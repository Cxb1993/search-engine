中文摘要 
 
本計畫以兩年的時間，投入蛋白質體三級結構預測(protein tertiary structure 
prediction)的研究。蛋白質結構目前已經確定的數量，遠小於已知的蛋白質序列
數量；結構預測益發重要，藉以提供生物學家蛋白質結構的線索來設計實驗，加
速找出蛋白質體結構。蛋白質體結構具有階層性，也就是說，二級結構是由一級
結構（序列）所組成，三級結構是由二級結構所組成，以此類推。因此二級結構
預測的準確度會影響三級結構的準確度。本計畫的第一年，我們持續改進本實驗
室前已發表的於Nucleic Acids Research 上的二級結構預測方法HYPROSP，提出
改良的HYPROSP II，此研究成果已發表於 2005 六月份的Bioinformatics期刊上。
本計畫的第二年，我們進一步改進第一年的研究成果- HYPROSP II，提出改良的
HYPROSP III，將準確度Q3提升至目前文獻中的最高，目前我們正在將研究成果
撰寫為論文。 
 
同時，由於區域結構預測(local structure prediction)有助於三級結構預測，無
論是 ab initio prediction, protein threading及 remote homology detection上都有所助
益，因此在本計畫的第一年我們也進行 local structure prediction 之研究。目前
local structure prediction 的問題在於正確率太低，以致於其應用受限。我們提出
knowledge-based 的預測方法，並測試在不同的 local structure alphabets 上，得到
較佳的準確度，此研究成果先於 2006年 Proceedings of Asia Pacific Bioinformatics 
Conference (APBC) 會議中發表，並在同年十一月下旬剛為 Journal of Bioinfor-
matics and Computational Biology 期刊所接受。 
 
三級結構預測在難度上比二級結構預測複雜許多，需要對蛋白質化學、物理
的特性有所瞭解。結構預測的方法，一般而言有三大類，第一類是 homology 
modeling，第二類是 protein threading，第三類是 ab initio 方法。目前 homology 
modeling 與 protein threading 主要都是利用資料庫搜尋，尋找可能的模板來建構
預測的結構。但是在沒有可用的模板情況下，預測的準確率不甚理想。因此我們
希望利用先前已經相當準確的二級結構預測以及區域結構預測的結果，再藉由所
預測的區域結構進一步反推回三級的結構，目前已有初步的成果。此外，著名的
三級結構預測競賽 CASP （Critical Assessment of Techniques for Structure Predic-
tion） 及 CAFASP（Critical Assessment of Fully Automated Structure Prediction）
包含許多預測項目，我們已經針對其中 domain prediction/ assignment 及 loop 
modeling 展開研究，目前獲得初步結果，將持續進行，日後並應用於 tertiary 
structure prediction. 
 
關鍵詞：蛋白質結構預測、蛋白質三級結構、蛋白質體三級結構預測、蛋白質折
疊、蛋白質區域結構、蛋白質區域結構預測、protein threading、ab initio 方法。 
 II
from the predicted local structure sequence. We have obtained some preliminary re-
sults. Furthermore, we started to study domain prediction/assignment and loop mod-
eling, which are tasks proposed in the famous CASP (Critical Assessment of Tech-
niques for Structure Prediction) and CAFASP (Critical Assessment of Fully Auto-
mated Structure Prediction). With respect to these two topics, we have obtained pre-
liminary results and will continue to enhance our methods and results.  
 
Keywords: protein structure prediction, protein tertiary structure, protein tertiary 
structure prediction, protein local structure, protein local structure prediction, homol-
ogy modeling, protein threading, ab initio prediction. 
 IV
1 前言 
 This is a two-year project proposal. The aim of this project is to design and de-
velop efficient and effective methods for protein tertiary structure prediction. 
 To identify functions of proteins is the ultimate goal for proteomics research. 
Proteins exhibit an enormous variety of structure that leads to a vast variety of differ-
ing functions. Knowing protein structure can help with identifying protein function. 
Protein structure prediction is an efficient way to find out protein structure. Currently, 
the number of proteins with known structure is far less than the proteins, and the gap 
is ever-increasing. Therefore, the importance of protein structure prediction cannot be 
overemphasized.  
 Protein structure is hierarchical, i.e., secondary structure consisting of primary 
structure (sequence), tertiary structure consisting of secondary structure, and similarly 
for quaternary structure. Nowadays, secondary structure prediction can reach about 
80% accuracy, and tertiary structure prediction still needs improvement. In order to 
improve tertiary structure prediction, in this project we have worked on improvement 
of secondary structure prediction and local structure prediction.  
We also know that secondary structure prediction itself is insufficient for predict-
ing protein tertiary structure. Protein folding must be considered. Interestingly, it is 
observed that apparently unrelated proteins adopt similar fold and these protein may 
have similar function. Currently, only 100 folds account for about half of all protein 
superfamilies in SCOP database. Therefore, we are pursuing study on protein fold 
recognition. We are trying to incorporate SCOP’s protein superfamily information to 
improve structure prediction. Besides fold recognition, homology modeling is a main 
method used for structure prediction that uses proteins with known structure to predict 
the structure of their homologous proteins. We are trying study refinement of homol-
ogy modeling.  
 
2 研究目的 
Knowing the structure of proteins is important for proteomic research and bio-
logical experiment design. However, the sequence-structure gap is ever-increasing, 
i.e., the number of proteins with known structure is far less than the protein sequences. 
Protein structure prediction is vital to bridge the gap. Therefore, we are interested in 
research on protein structure prediction.  
Since protein structure is hierarchical, the accuracy of tertiary structure prediction 
is affected by secondary structure prediction. In this project, we aim to improve our 
secondary structure prediction method, which a hybrid method based on knowledge 
 1
nearly 80%. The advantage of the neural network approach is that evolutionary in-
formation, amino acid and structure propensities as well as global sequence composi-
tions can all be taken into account. A drawback of this approach is that, it is unclear 
how the additional evolutionary information affects the prediction accuracy. The in-
side of neural network algorithms is hard to understand and to translate into useful 
knowledge. Machine learning approaches other than neural network are also used for 
secondary structure prediction [9], and they have different limitations. 
 
3.2 Protein Local Structure Prediction 
A protein’s local structure is a set of protein peptides that share common 
physiochemical and structural properties. Researchers usually cluster protein frag-
ments by different local criteria, such as solvent accessibility, residue burial [10], and 
backbone geometry [11], and represent these fragment clusters by an alphabet, 
called a structural alphabet. By using the structural alphabet we can encode a native 
protein into a set of discrete representations. Local structure prediction predicts the 
local structure expressed by a letter of the structural alphabet from the amino acid 
sequence, which improves the performance of both ab initio and fold recognition 
methods of tertiary structure prediction [12-14]. 
Various local structure libraries have been constructed, some of which focus on 
the reconstruction of protein tertiary structures. In such libraries, the number of let-
ters in each structural alphabet is large, e.g., 100 in Unger et al. [15], 40 and 100 in 
Micheletti et al. [16], 100 in Schuchhardt et al. [17], and 25-300 with a fragment 
length from 5 to 7 in Kolodny et al [18]. Although large alphabets can better ap-
proximate protein tertiary structures, predicting protein local structures from amino 
acid sequences is much more challenging.  
As a result, smaller structural alphabets have been proposed, associated local 
structure libraries have been constructed, and local structure prediction algorithms 
have been developed for use with these libraries. Bystroff et al. generated a library 
called I-site [19], which contains 13 structural motifs of different length. Prediction 
is based on profile-profile alignment between each structural motif and the 
PSI-BLAST [20] result of the input sequence. To improve prediction accuracy, the 
authors also proposed a new model called HMMSTR [21]. In this paper, we use the 
structural alphabet of HMMSTR, denoted by SAH, to test our method. A.G. de Brev-
ern et al [15] built their library, called Protein Blocks (PB), by clustering 5-mer pro-
tein fragments into a structural alphabet of 16 letters according to the torsion angle 
space. They used a Bayesian probabilistic approach for prediction. Karchin et al. [11] 
constructed an STR library, in which the structural alphabet consists of 13 letters ob-
tained from eight secondary structure states by dividing β-sheets into 6 types. They 
 3
structures with flexible stem from the predicted local structure sequence. 
 
4 研究方法 
4.1 Protein Secondary Sructure Prediction 
4.1.1 HYPROSP II 
As local structural libraries are frequently encoded in short segments of protein 
sequences [33,34], another line of prediction approach is to use local structure-based 
sequence databases. This motivated us to design a knowledge-based prediction algo-
rithm PROSP [35], which uses a peptide sequence-structure knowledge base and a 
voting scheme for prediction. In order to combine the strength of machine learning 
approaches, we proposed a hybrid prediction method called HYPROSP [35], which 
combines PROSP and PSIPRED. We used a quantitative measure called global match 
rate to determine which of PROSP and PSIPRED should be used to predict the struc-
ture of a target protein (i.e., a protein whose structure is unknown and targeted for 
prediction). The global match rate defined in HYPROSP is a global measure for the 
amount of structural information that a target protein can extract from the se-
quence-structure knowledge base (SSKB). Our experiments show that the prediction 
accuracy of PROSP has a significant positive correlation with the global match rate. 
The hybrid strategy of HYPROSP is as follows: if the global match rate of a target 
protein is at least 80%, we use PROSP to predict; otherwise, we use PSIPRED [8]. 
To compare the performance of different prediction methods, we use a prevailing 
accuracy measure Q3(p) defined as follows:  
%100
 of residues all ofnumber 
predictedcorrectly   of residues ofnumber  )(Q3 ×=
p
pp .      (2) 
where p is the target protein. HYPROSP made a slight improvement of Q3 (i.e., the 
average of Q3(p)) over PSIPRED in several datasets. However, there are two limita-
tions. First, the proportion of proteins with global match rate above 80% is often not 
large enough, so the improvement could be diluted. Second, as the prediction accuracy, 
Q3, of PSIPRED has also been improved from 76% as reported in EVA web site to 
79% using version 2.45 on the nrDSSP dataset, the marginal advantage of HYPROSP 
is minimal. To reduce the effect of these two limitations, we introduce two new con-
cepts: 1) we consider a new quantitative measure called local match rate as opposed 
to the global match rate defined in HYPROSP; 2) we propose a new hybrid strategy 
called HYPROSP II, which combines the results of PROSP and PSIPRED based on 
their confidence levels.  
Local match rate is defined on each position x of the target protein p. Let Qf (x) be 
the collection of all similar peptides qf ’s containing the position x. We define the lo-
 5
matched in SSKB. To improve this situation, we consider use smaller length to define 
peptides and generate another SSKB of shorter peptides. In Step 2 of PROSP predic-
tion algorithm, an increasing number of shorter similar peptides can be matched in 
SSKB of shorter peptides to vote for the structure of the target protein. However, 
short peptide sequences are likely to be associated with many incompatible structures, 
which could create more ambiguity in structure prediction (Wu et. al., 2004). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig 1. An example illustrated how the duplicates bias the prediction results in the vot-
ing stage of PROSP.  
 
4.1.2.2 The algorithm of PROSP II 
Based on the above motivation, PROSP II is proposed to improve the performance of 
PROSP. In PROSP II, we first construct two knowledge bases, 7-mer and 5-mer 
SSKBs. The 5-mer SSKB is constructed by the same procedure to construct 7-mer 
SSKB and choosing the w =5 and δ=2 (because the prediction results obtained by the 
6-mer SSKB are too similar to those of the 7-mer SSKB and the accuracy of the re-
sults from the 4-mer SSKB is not good enough.). In the construction of knowledge 
base, PSIBLAST is run using the same parameters setting and the same sequence da-
tabase as used in HYPROSP II (Lin et. al., 2005), wherej is 3 (three iterations), e is 10 
(E-value < 10) and the sequence database is NCBI nr (which contains 3,747,820 se-
quences).  
 
Once the two SSKBs are constructed, PROSP II uses each SSKB to takes similar 
procedure to predict the secondary structure of a target protein and then finally com-
bine both prediction results from the two SSKBs to obtain the final prediction result. 
 7
When at least one prediction method provides false high confidence, it is difficult to 
avoid making inaccurate prediction. Secondly, when PROSP and most machine 
learning-based prediction methods predict the structure of an amino acid in the target 
protein, they usually do not take the structure conformation of its neighbors. To im-
prove the performance of these hybrid strategies, we propose a neural network based 
system to serve as the decision fusion platform (DFP) for secondary structure predic-
tion. Not only the confidence scores of PSIPRED and PROSP II are used as the struc-
ture information, but also the statistical results of sequence alignments extracted from 
PSSM are involved to determine the secondary structure for each residue. The overall 
encoding scheme and the architecture of DFP are illustrated in Figure 2. 
 
PSSM PROSPII PSIPRED
A  R  N  D  C  Q  E  G  H  I  L  K  M  F  P  S  T  W  Y  V
1 M   -3 -3 -4 -5 -3 -3 -4 -5 -4  0  1 -3 10 -2 -5 -4 -3 -4 -3 -1
2 P    2 -3 -3 -1 -3 -1 -1 -1 -4 -2 -4 -2 -2 -5  4  2  4 -5 -4 -3
3 L   -4 -5 -6 -6 -4 -3 -5 -6 -5  3  5 -5  4  0 -5 -5 -3 -4 -3  2
l T   -1 -3 -1 -1 -4 -2 -3 -2 -1 -4 -3 -1 -3 -4 -4  3  6 -5 -4 -3
…
474 17 290 
336 21 314 
1 
0
235 
153 
214 
77 
10086
50049
0
0
12
0
83
94
… …
Normalized
60.7
50.0
47.5
33.3
2.2
3.2
0.3
0
37.1
46.8
52.2
66.7
10.4 089.6 0.05 0.02 0.01 1.00 0.95 0.02 0.02 0.05 0.27 0.05 0.02 0.27 0.12 0.05 0.12 0.02 0.27 0.27 0.05 0.27 
50.5 049.5 0.88 0.05 0.02 0.05 0.01 0.01 0.50 0.98 0.01 0.99 0.95 0.01 0.00 0.01 0.05 0.02 0.00 0.00 0.01 0.02 
87.4 012.6 0.05 0.02 0.01 0.98 0.88 0.98 0.01 0.12 0.12 0.02 0.12 0.02 0.27 0.27 0.27 0.05 0.27 0.05 0.05 0.88 
100000.27 0.05 0.02 0.05 0.02 0.01 0.12 1.00 0.05 0.73 0.50 0.02 0.01 0.02 0.05 0.05 0.01 0.02 0.05 0.05 
Window of 11 rows
Neural Network
286 inputs
3 Outputs
…
PSSM PROSPII PSIPRED
11×26=286 scaled inputs 
to Neural Network
Final 3-states Prediction
Fig. 2. An example of transforming the structure information on the DFP. If every 11 
(k = 5) rows sliced as an instance, there are 11*(20+3+3)=286 elements in the range 
of (x-5, x+5) rows for each position x. In the case of k=5, the architecture of the net-
work contains 286 input nodes and 3 output nodes. 
 
4.2 Protein Local Structure Prediction 
Previous studies indicate that accuracy is the main bottleneck of local structure 
prediction. In this study, we knowledge-based prediction method for local structure 
prediction to improve current accuracies. The proposed method is alpha-
bet-independent, i.e., it is not particularly designed for a specific structural alphabet. 
The knowledge base construction is basically the same as in HYPROSP II [36], where 
 9
 
4.2.2 Neural network method 
We use a standard feed-forward back-propagation neural network [37] with a 
single hidden layer. The layer contains 35 hidden units, which we found to be the 
most effective number for our training stage. The learning parameters of the hidden 
layer and the output layer are 0.075 and 0.05, respectively; and the sum of square 
errors is used during back propagation. 
Each protein sequence in the training set or test set is partitioned into peptides, 
using a sliding window of length 7. We also perform a PSI-BLAST search to obtain 
the profile of the sequence, which is called the Position-Specific Scoring Matrix 
(PSSM). Our neural network takes each peptide as input. Specifically, the input vec-
tor consists of the peptide’s corresponding segment of PSSM and its secondary 
structure. Hence, the length of each input vector is 161, i.e., 7×20 for the PSSM 
segment and 7×3 for its secondary structure. The output reports the results corre-
sponding to the amino acid located at the center of the peptide (the peptide center). 
The output is a vector of size n, i.e., the size of the underlying structural alphabet, 
and each entry represents the confidence score of the peptide center to be assigned a 
specific alphabet letter.  
Our neural network prediction method consists of two steps: 
Step 1: Perform secondary structure prediction on a target protein. 
Step 2: Use the neural network method to predict the local structure of each amino 
acid in the target protein. 
Unlike the proteins in the training set, target proteins do not have secondary 
structure information. Thus, in Step 1 we use HYPROSP II [36] to predict the sec-
ondary structure. The predicted secondary structure and PSSM, extracted by a slid-
ing window of length 7, constitute the input to the trained neural network. The letter 
with the highest confidence score in the output is then considered to be the local 
structure of the peptide center. Step 2 is repeated to predict all amino acids in the 
target protein. 
 
4.2.3 HYPLOSP: a hybrid method for protein local structure prediction 
As our knowledge-based method and neural network method have different 
strengths, one  may outperform the other, depending on the circumstances. To bet-
ter utilize their respective strengths, we propose a hybrid prediction method, 
HYPLOSP, which combines the prediction results of both methods at each position 
along the amino acid sequence based on their confidence scores. 
The knowledge-based method generates a set of voting scores, denoted {V1, 
V2,…, Vn} for each output letter. We define the confidence score of letter Ai as the 
 11
respectively, which is 2.0 and 1.1 better than that of PSIPRED. For local structures 
with match rate higher than 80%, the average Q3 improvement is 4.4 on the nrDSSP 
dataset. However, the average Q3 improvements of HYPROP over PSIPRED on these 
two datasets are both only 0.1%. Using local match rate improves the accuracy better 
than global match rate. There has been a long history of attempts to improve secon-
dary structure prediction. We believe HYPROSP II has greatly utilized the power of 
peptide knowledge base and raised the prediction accuracy to a new high. Our method 
could have a profound effect on the general use of knowledge base techniques for 
various prediction algorithms. The results of this part had been published in Bioin-
formatics in June 2006 as listed in [36]. 
 
5.1.2 HYPROSP III 
On EVAc4, HYPROSP III achieves 1.01% improvement of the average Q3 
higher than the best reported Q3 and competitive average SOV scores. On CB513, the 
average Q3 of HYPROSPIII is 0.85 % higher than the best reported Q3 when the av-
erage SOV of HYPROSP III is 0.86% higher than the best reported SOV. On RS126, 
HYPROSPIII also improves the average Q3 and SOV scores 1.12% and 0.84% re-
spectively from the best reported scores. Table 1 compares the performance achieved 
by HYPROSP III against that achieved by other model-based servers such as 
PSIPRED [8], PHD [1], SAM-T99 [38], PROFsec (B. Rost, to be published), SSPro 
[39] and YASSPP [40]. 
 
Table 1. The comparison of the performance of HYPROSPIII and the other methods 
on the three benchmarks: EVAc4, CB513 and RS126. 
(a) EVAc4 
 Q3 SOV Info CH CE CC
PHDpsi 74.52 70.69 0.35 0.67 0.69 0.53 
PSIPRED 77.62 76.05 0.38 0.70 0.74 0.56 
SAM-T99sec 77.64 75.05 0.39 0.68 0.72 0.58 
PROFsec 76.54 75.39 0.38 0.68 0.71 0.56 
YASSPP 79.34 78.65 0.42 0.72 0.75 0.61 
Errsig 0.82 1.16 0.02 0.02 0.02 0.02 
HYPROSPIII 80.35 78.66 0.44 0.70 0.62 0.62 
Errsig 0.84 1.20 0.02 0.02 0.02 0.01 
 
(b) CB513 
 Q3 SOV Info CH CE CC
PSIPRED 79.95 76.48 0.43 0.76 0.68 0.63 
 13
Errsig 0.40 0.62 0.01 0.04 0.01 0.01 
HYPROSPIII 81.84 78.72 0.47 0.74 0.65 0.65 
Errsig 0.37 0.60 0.01 0.01 0.01 0.01 
 
(c) RS126 
 Q3 SOV Info CH CE CC
HYPROSPII 81.59 76.59 0.46 0.72 0.66 0.64 
Errsig 0.84 1.36 0.01 0.02 0.02 0.02 
HYPROSPIII 82.12 77.08 0.43 0.61 0.70 0.75 
Errsig 0.73 1.35 0.02 0.02 0.02 0.02 
 
5.2 Protein Local Structure Prediction 
Our method has been extensively tested on three different structural alphabets: 
Structure Alphabet of HMMSTR (SAH), Protein Blocks, and STR. We use two pre-
vailing evaluation criteria, MDA (maximum deviation of backbone torsion angle) and 
QN, which is similar to Q3 in secondary structure prediction. All of the tests demon-
strate significant improvement to the previous studies. 
The experimental results of our methods using nrDSSP on the three alphabets 
are shown in Table III. It can be observed from the table that HYPLOSP improves 
the knowledge-based (KB) method and the neural network (NN) method. Even when 
KB and NN methods achieve a similar performance, HYPLOSP still improves their 
scores, e.g., it improves the MDA scores for SAH and PB by approximately 3-4%.  
Table 3. Cross-validation results of each method 
 QN MDA 
NN 59.53% 58.71%
KB 56.70% 58.31%SAH 
Hybrid 61.51% 62.69%
NN 59.54% 55.26%
KB 57.79% 54.57%PB 
Hybrid 63.24% 58.66%
NN 58.78%  
KB 58.96%  STR 
Hybrid 63.07%  
 
Since the global match rate of a protein indicates the amount of information ex-
tracted from the knowledge base, we examine the relation between the performance 
of each method and the global match rate of a protein. For prediction on each struc-
tural alphabet, we choose MDA for the SAH alphabet to illustrate the performance 
versus the global match rate. (Figures 3). We observe that the KB method is more 
 15
a QN of only 47.02% because this dataset has a low average global match rate. In 
contrast, NN has a QN of 58.26%. However, HYPLOSP yields a better QN (59.03%), 
which improves the NN result slightly.  
 
0
2
4
6
8
10
12
<5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45-50 55-60 60-65 65-70 70-75
Global Match Rate
Number
 
Fig. 4. The number of proteins versus the global match rate 
Table 4. Comparison of HYPLOSP with previous studies 
 QN MDA 
HMMSTR 53.04% 50.08%
HYPLOSP 57.44% 55.23%SAH 
Improvement 4.40% 5.15%
LocPred 41.93% 36.11%
HYPLOSP 55.17% 52.81%PB 
Improvement 13.24% 16.7%
SAM-T02 59.79%
HYPLOSP 59.03%
 
STR 
Improvement -0.76%
 
5.3 Protein Loop Structure Prediction 
We tested our approach with popular benchmark and constructed the loop con-
formations with ideal local structure descriptions from known protein structures. 
 
Table 5. Performance respect to each loop length 
Length 4 5 6 7 8 9 10 11 12 13 14 
RMSD 3.029 3.745 3.855 4.338 4.683 4.808 5.388 5.687 6.086 5.996 6.411
RMSDglobal 3.926 4.506 5.340 5.896 6.595 7.606 8.058 8.874 9.792 10.339 10.569
RMSDlocal 0.771 1.212 1.606 2.08 2.466 2.681 3.219 3.408 3.909 4.027 4.204
RMSDstem 3.168 4.112 4.098 4.695 4.960 5.029 5.938 6.127 6.581 6.541 6.765
RMSDstem-left 0.224 0.247 0.251 0.239 0.245 0.239 0.237 0.257 0.197 0.234 0.257
 17
based on residue burial. Proteins 2004, 55: 508-518. 
11. Karchin, R., Cline, M., Mandel-Gutfreund, Y. and Karplus, K. Hidden Markov 
models that use predicted local structure for fold recognition: Alphabets of back-
bone geometry. Proteins-Structure Function and Genetics 2003, 51: 504-514. 
12. Bystroff, C. and Shao,Y. Fully automated ab initio protein structure prediction 
using I-Sites, HMMSTR and Rosetta. Bioinformatics 2002, 18:54-61. 
13. Karplus, K., Karchin, R., Draper, J., Casper, J., Mandel-Gutfreund, Y., Diekhans, 
M. and Hughey, R. Combining local-structure, fold-recognition, and new fold 
methods for protein structure prediction. Proteins-Structure Function and Genetics 
2003, 53: 491-496. 
14. Simons, K. T., Kooperberg, C., Huang, E. and Baker, D. Assembly of protein ter-
tiary structures from fragments with similar local sequences using simulated an-
nealing and Bayesian scoring functions. Journal of Molecular Biology 1997, 268: 
209-225. 
15. Unger, R., Harel, D., Wherland, S. and Sussman, J. L. A 3D Building-Blocks Ap-
proach to Analyzing and Predicting Structure of Proteins. Proteins-Structure 
Function and Genetics 1989, 5: 355-373. 
16. Micheletti, C., Seno, F. and Maritan, A. Recurrent oligomers in proteins: An op-
timal scheme reconciling accurate and concise backbone representations in auto-
mated folding and design studies. Proteins-Structure Function and Genetics 2000, 
40: 662-674. 
17. Schuchhardt, J., Schneider, G., Reichelt, J., Schomburg, D. and Wrede, P. Local 
structural motifs of protein backbones are classified by self-organizing neural net-
works. Protein Engineering 1996, 9: 833-842. 
18. Kolodny, R., Koehl, P., Guibas, L. and Levitt, M. Small libraries of protein frag-
ments model native protein structures accurately. Journal of Molecular Biology 
2002, 323: 297-307. 
19. Bystroff, C. and Baker, D. Prediction of local structure in proteins using a library 
of sequence-structure motifs. Journal of Molecular Biology 1998, 281: 565-577. 
20. Altschul, S. F., Madden, T. L., Schaffer, A. A., Zhang, J. H., Zhang, Z., Miller, W. 
and Lipman, D. J. Gapped BLAST and PSI-BLAST: a new generation of protein 
database search programs. Nucleic Acids Research 1997, 25: 3389-3402. 
21. Bystroff, C., Thorsson, V. and Baker, D. HMMSTR: a hidden Markov model for 
local sequence-structure correlations in proteins. Journal of Molecular Biology 
2000, 301: 173-190. 
22. de Brevern, A. G., Etchebest, C. and Hazout, S. Bayesian probabilistic approach 
for predicting backbone structures in terms of protein blocks. Proteins-Structure 
Function and Genetics 2000, 41: 271-287. 
 19
38. Karplus, K., Barrett, C. and Hughey, R. Hidden Markov models for detecting re-
mote protein homologies. Bioinformatics 1998, 14: 846-856. 
39. Pollastri, G., Przybylski, D., Rost, B. and Baldi, P. Improving the prediction of 
protein secondary structure in three and eight classes using recurrent neural net-
works and profiles. Proteins-Structure Function and Genetics 2002, 47: 228-235. 
40. Karypis, G. YASSPP: Better kernels and coding schemes lead to improvements in 
protein secondary structure prediction. Proteins-Structure Function and Bioinfor-
matics 2006, 64: 575-586. 
41. Rost, B. and Eyrich, V. A. EVA: Large-scale analysis of secondary structure pre-
diction. Proteins-Structure Function and Genetics 2001, 192-199. 
42. Berman, H. M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T. N., Weissig, H., 
Shindyalov, I. N. and Bourne, P. E. The Protein Data Bank. Nucleic Acids Re-
search 2000, 28: 235-242. 
43. Ching-Tai Chen, Hsin-Nan Lin, Ting-Ying Sung and Wen-Lian Hsu, "A 
Knowledge-based Approach to Protein Local Structure Prediction," to ap-
pear in Journal of Bioinformatics and Computational Biology,  also in Pro-
ceedings of Asia Pacific Bioinformatics Conference (APBC), (2006). 
 
7 計畫成果自評 
在本計畫兩年的執行期間，我們設計一套知識庫的方法，將此方法應用在
蛋白質二級結構及區域結構的預測，分別獲得領域中最高的預測正確率；
這兩項研究成果都可以進一步幫助蛋白質三級結構的預測，因此我們藉由
所預測的 local structure 結果，進一步反推回原有的三級結構，目前也已
經有了初步的研究成果。本計畫研究成果共計發表兩篇期刊論文及一篇會
議論文，甚有一篇期刊論文正在撰寫當中，可謂研究成果豐富；此外為了
擴展研究成果的影響力，我們將計畫中關於二級結構預測的程式架設成 
web server，提供給有需要的研究社群使用。 
 
z Ching-Tai Chen, Hsin-Nan Lin, Ting-Ying Sung and Wen-Lian Hsu, "A Knowl-
edge-based Approach to Protein Local Structure Prediction," to appear in Journal 
of Bioinformatics and Computational Biology, also in Proceedings of Asia Pa-
cific Bioinformatics Conference (APBC), (2006). 
z Hsin-Nan Lin, Jia-Ming Chang, Kuen-Pin Wu, Ting-Yi Sung and Wen-Lian Hsu, 
"A knowledge-based hybrid method for protein secondary structure prediction 
based on local prediction confidence," Bioinformatics 21, 3227-3233, (2005). 
 
 
 21
tion. However, the accuracy of existing methods is limited. In this paper, we propose a knowledge-based prediction 
method that assigns a measure called the local match rate to each position of an amino acid sequence to estimate the 
confidence of our method. Empirically, the accuracy of the method correlates positively with the local match rate; 
therefore, we employ it to predict the local structures of positions with a high local match rate. For positions with a 
low local match rate, we propose a neural network prediction method. To better utilize the knowledge-based and neu-
ral network methods, we design a hybrid prediction method, HYPLOSP (HYbrid method to Protein LOcal Structure 
Prediction) that combines both methods. To evaluate the performance of the proposed methods, we first perform 
cross-validation experiments by applying our knowledge-based method, a neural network method, and HYPLOSP to a 
large dataset of 3,925 protein chains. We test our methods extensively on three different structural alphabets and 
evaluate their performance by two widely used criteria, MDA (Maximum Deviation of backbone torsion Angle) and 
QN, which is similar to Q3 in secondary structure prediction. We then compare HYPLOSP with three previous studies 
using a dataset of 56 new protein chains. HYPLOSP shows promising results in terms of MDA and QN accuracy and 
demonstrates its alphabet-independent capability. 
Keywords: knowledge-based prediction method, local structure prediction, neural networks, secondary structure, 
structural alphabet. 
Introduction 
A protein’s local structure is a set of protein peptides that share common physiochemical and struc-
tural properties. Researchers usually cluster protein fragments by different local criteria, such as sol-
vent accessibility, residue burial1, and backbone geometry2, and represent these fragment clusters by 
an alphabet, called a structural alphabet. By using the structural alphabet we can encode a native 
protein into a set of discrete representations. Local structure prediction predicts the local structure 
expressed by a letter of the structural alphabet from the amino acid sequence, which improves the 
performance of both ab initio and fold recognition methods of tertiary structure prediction11-13.  
Various local structure libraries have been constructed, some of which focus on the reconstruc-
tion of protein tertiary structures. In such libraries, the number of letters in each structural alphabet is 
large, e.g., 100 in Unger et al.6, 40 and 100 in Micheletti et al.7, 100 in Schuchhardt et al.8, and 25-300 
with a fragment length from 5 to 7 in Kolodny et al9. Although large alphabets can better approximate 
protein tertiary structures, predicting protein local structures from amino acid sequences is much 
more challenging.  
As a result, smaller structural alphabets have been proposed, associated local structure libraries 
have been constructed, and local structure prediction algorithms have been developed for use with 
these libraries. Bystroff et al. generated a library called I-site10, which contains 13 structural motifs of 
different length. Prediction is based on profile-profile alignment between each structural motif and the 
PSI-BLAST11 result of the input sequence. To improve prediction accuracy, the authors also proposed 
a new model called HMMSTR12. In this paper, we use the structural alphabet of HMMSTR, denoted 
by SAH, to test our method. A.G. de Brevern et al13 built their library, called Protein Blocks (PB), by 
clustering 5-mer protein fragments into a structural alphabet of 16 letters according to the torsion an-
gle space. They used a Bayesian probabilistic approach for prediction. Karchin et al.2 constructed an 
STR library, in which the structural alphabet consists of 13 letters obtained from eight secondary 
structure states by dividing β-sheets into 6 types. They then used a hidden Markov model 
(HMM)23,24 for local structure prediction. Yang et. al.16 used a local structure-based sequence profile 
database (LSBSP1) to predict the local structure of 4 states. Kuang et al.17 incorporated a neural net-
work which takes the result of LSBSP1 as input to enhance the model. 
The accuracy of local structure prediction depends on the definition of the underlying structural 
 23
2.1.  Knowledge-based approach 
2.1.1.  Construction of the sequence-structure knowledge base (SSKB) 
Our knowledge base contains both local structure information and secondary structure information of 
peptides. The former is expressed by a structural alphabet (discussed in Section 2.4), while the latter 
is obtained from the DSSP database19. For ease of exposition, we assume that we are given a protein 
dataset with known secondary structures and local structures based on a specific structural alphabet. 
The strength of a knowledge base depends on its size. Since the number of proteins with known 
secondary structures is relatively small, we amplify our knowledge base by finding homologous pro-
teins to inherit the structural information of the given dataset. To this end, we utilize PSI-BLAST to 
find proteins remotely homologous to a protein with a known structure, i.e. proteins in the DSSP da-
tabase, referred to as a Query protein in the PSI-BLAST output. When using PSI-BLAST, we set the 
parameter j to 3 (3 iterations), e to 0.001 (E-value < 0.001), and use the NCBI nr20 database as the 
sequence database. For each Query protein, PSI-BLAST generates a large number of homologous 
protein segments as well as their pairwise alignments, called high-scoring segment pairs (HSPs). In 
each HSP, the counterpart sequence aligned with the Query protein is denoted by Sbjct in the 
PSI-BLAST output.  
Since applying PSI-BLAST to a Query protein generates a large set of HSPs, we need to find 
the peptides in the Sbjct protein of each HSP that are similar to those of the Query protein so that they 
can inherit the structural information of the Query protein. We use a sliding window of length w to 
determine the peptides. In our experiments, we choose w = 7, which yields the best results among 
various lengths. Let p and q denote a pair of peptides in the Query protein and Sbjct protein, respec-
tively. We define the similarity score, S, of p and q as the number of positions that are identical or 
have a “+” sign in the sliding window. We call p and q similar if S ≧ 5. We distinguish two cases of 
similarity between p and q for constructing the knowledge base. Case 1: p and q are similar. We de-
fine the confidence score of q with respect to p as (S × A) / w. This enables us to measure the confi-
dence level of q inheriting the structural information of p, where A denotes the alignment score of the 
HSP reported in PSI-BLAST output. If there is no gap between p and q, we create a new record cor-
responding to peptide q, (q, secondary structure of q, local structure of q, confidence score of q), 
given by (q, secondary structure of p, local structure of p, confidence score of q with respect to p), to 
the knowledge base. When q is later found in another HSP and is similar to the counterpart peptide r 
that already exists in the knowledge base, i.e., q can also inherit the structural information from r, we 
simply update the record of q by including the structural information of r and adding the confidence 
score of q with respect to r. Case 2: p and q are dissimilar. Then we simply ignore this pair of peptides 
for the construction of knowledge base. 
Fig. 1 shows part of an HSP. The pair of peptides inside the box has a similarity score of 5, so 
the peptides are considered similar. The confidence score of the peptide in the Sbjct with respect to 
 25
dary and local structures are unknown and to be predicted). 
Step 2: Use similar peptides found in SSKB to vote for the local structure of each amino acid in the 
target protein. 
In Step 1, the parameters and the sequence database used in PSI-BLAST are the same as those 
used in the construction of the knowledge base. To define the similar peptides in Step 2, we use the 
same sliding window length of 7 and the same similarity score of 5 with no gap to define similar pep-
tides as before. We then match all the peptides of the target protein with similar peptides in SSKB, 
and use the local structure information of the matched peptides in SSKB to vote for the local structure 
of the target protein. Hereafter, we assume that the structural alphabet is a set of {A1, A2,…, An}. Let p 
be a peptide of the target protein. We associate each position, x, in p with n variables, called the voting 
score, denoted by Vpi(x), where i = 1,…, n; initially, Vpi(x) is set to be 0. In an HSP, let q be p’s coun-
terpart peptide with similarity score S and alignment score A. If q is similar to p and can be found in 
SSKB, the voting score of p is updated as follows. For each position, x, compute  
 Vpi(x)← Vpi(x)+ Cqi(x) × (S × A) / 7, i=1,…n, (2) 
where Cqi(x) is the normalized confidence score of q with the structural letter i given in SSKB. It is 
calculated as the confidence score of letter i divided by the total confidence score of all letters. The 
value turns out to be a fraction between 0 and 1 which represents the weight of each structural letter 
based on this SSKB entry.  The calculation is repeated for all similar peptides. The local structure of 
x in p is given by the letter corresponding to Max {Vp1(x), Vp2(x,),… , Vpn(x)}. 
2.1.3.  Confidence measure of the knowledge-based approach 
We use two measures, the local match rate and the global match rate, to represent the amount of in-
formation extracted from the knowledge base. At each position, x, of a target protein, we obtain from 
the HSPs a set of similar peptides, Q(x), that contains the position x. The local match rate at position x, 
denoted by LocalMatchRate(x), is defined by: 
 LocalMatchRate(x) = %100
|Q(x)|
|SSKBQ(x)| ×I . (3) 
A higher local match rate implies higher confidence in the result of the knowledge-based method 
for the position. Note that it is possible for a target protein to have high local match rates in some 
positions and low local match rates in others.  
 Unlike the local match rate, the global match rate is a measure that deals with the target 
protein as a whole. Let Q be the set of peptides obtained from the HSPs of the target protein, i.e., 
Q is equal to the union of all Q(x) of the protein. The global match rate of a protein, p, is defined 
as follows: 
 GlobalMatchRate(p) = %100×
|Q|
|SSKBQ| I . (4) 
 27
dary structure and PSSM, extracted by a sliding window of length 7, constitute the input to the trained 
neural network. The letter with the highest confidence score in the output is then considered to be the 
local structure of the peptide center. Step 2 is repeated to predict all amino acids in the target protein. 
2.3.  HYPLOSP: a hybrid method for protein local structure prediction 
As our knowledge-based method and neural network method have different strengths, one  may 
outperform the other, depending on the circumstances. To better utilize their respective strengths, we 
propose a hybrid prediction method, HYPLOSP, which combines the prediction results of both meth-
ods at each position along the amino acid sequence based on their confidence scores.  
The knowledge-based method generates a set of voting scores, denoted {V1, V2,…, Vn} for each 
output letter. We define the confidence score of letter Ai as the normalized voting score multiplied by 
the prediction confidence, LocalMatchRate: 
 Conf_KBi= Rate(x)LocalMatch
V
V
j
j
i ×∑  (5) 
The neural network also automatically generates a set of confidence scores between 0 and 1. To 
make both prediction methods have the same scale of confidence scores, we multiple the output score 
of NN method by 100 to define the confidence scores of the NN method, denoted by {Conf_NN1, 
Conf_NN2,…, Conf_NNn}3.  
Using Conf_NNi and Conf_KBBi, we determine the final predicted structure at position x to be Ak 
if  
 Conf_NNk + Conf_KBk = )__(}...,{ iin21i KBConfNNConfMax +∈ . (6) 
2.4.  Structural alphabets and encoding 
To evaluate our hybrid prediction method, we use three popular structural alphabets: 
the Structural Alphabet of HMMSTR, Protein Blocks, and STR, each of which has a 
relatively small number of letters, We use a non-redundant DSSP database as our 
dataset (explained in Section 3.1) and encode each amino acid of a protein sequence 
into structural letters based on its structural information. Such encoding is necessary 
to construct our knowledge base and the training stage in our neural network method. 
The definitions of the alphabets and their encodings are presented below. 
Structural Alphabet of HMMSTR (SAH)12. This alphabet, proposed by Bystroff et al. is composed of 
11 letters, ten of which represent conformation states in different Φ−Ψ angle regions of a trans pep-
tide; the other one corresponds directly to a cis peptide. Following Karchin’s approach2, we assign the 
cis residues to one of the other 10 regions according to their Φ−Ψ angles. (In our encoding scheme, 
SAH is comprised of 10 letters.) Table II shows the Φ−Ψ regions of the SAH alphabet. 
                                                 
3 For implementation convenience, we further normalize Conf_KB and Conf_NN in a range of 0 to 94 and assign a correspond-
ing ASCII character to each of them. 
 29
neighbored by only one parallel strand partner, and “Z” if it is neighbored by only one anti-parallel 
strand partner. If a β-strand does not have any neighboring strand partners, it is assigned ”E”.  
Experimental results 
3.1.  Datasets and experiment design 
We carried out two types of experiment. First, we used a large dataset to perform 10-fold 
cross-validation experiments on each structural alphabet to evaluate our knowledge-based method, 
neural network method, and the hybrid method, HYPLOSP. To generate the dataset, we downloaded 
25,288 proteins from the DSSP database (dated 9/22/2004), which were divided into 46,745 protein 
chains. We then used PSI-BLAST and pairwise sequence alignment to filter out protein chains with a 
pairwise sequence identity over 25%. Moreover, protein chains of length less than 80 were removed. 
Finally, we had a non-redundant DSSP dataset, called nrDSSP, containing 3,925 unique protein 
chains along with their secondary structures. To evaluate our prediction methods, we transformed 
nrDSSP into structural alphabets of our choice, as described in Section 2.4. In each experiment, the 
nrDSSP dataset was randomly divided into ten sets. One set was selected as the test set (containing 
predicted secondary structure information) and the other nine were combined as the training set (con-
taining observed secondary structure information) for training the neural network and construction of 
SSKB. This process was repeated for each set in turn to be used as the test set.  
Second, we used another dataset, containing new proteins of DSSP reported during the period 
October 2004 to May 2005, as the test set to compare HYPLOSP with the other two methods. This 
dataset consisted of fifty-six protein chains after filtering out chains with a sequence identity over 
25% to the nrDSSP. Furthermore, all 56 protein chains had a pairwise sequence identity of less than 
25%. We compared the HYPLOSP model, which was trained on the nrDSSP dataset, with the follow-
ing three servers: the HMMSTR12 server developed by Bystroff et al. for the SAH alphabet, the 
LocPred13, 23 server developed by de Brevern et al. for the PB alphabet, and the SAM-T024 server 
developed by Karplus et al. for the STR alphabet. Note that the LocPred server provided three models: 
Bayesian prediction, sequence families, and a new version of sequence families. We only compared 
HYPLOSP with the result of the last model, since it was the best of the three. 
We used the QN and MDA scores as performance measures. Specifically, QN was used for the 
three structural alphabets, while the MDA score was used for SAH and PB only. It was not used for 
STR, since it lacks torsion angle information.  
Our datasets and HYPLOSP’s results on the datasets are available at 
http://bio-cluster.iis.sinica.edu.tw/~caster/hyplosp/. 
3.2.  Cross-validation results of HYPLOSP 
The experimental results of our methods using nrDSSP on the three alphabets are shown in Table III. 
It can be observed from the table that HYPLOSP improves the knowledge-based (KB) method and 
the neural network (NN) method. Even when KB and NN methods achieve a similar performance, 
HYPLOSP still improves their scores, e.g., it improves the MDA scores for SAH and PB by approxi-
 31
counts for 52% of the dataset). As more protein structures are determined, the knowledge base will be 
enlarged. Thus, the number of proteins with higher global match rates will very likely increase, and 
the KB method and HYPLOSP can then be further improved. 
 
25
30
35
40
45
50
55
60
65
70
75
0-10 10-20 20-30 30-40 40-50 50-60 60-70 70-80 80-90 90-100
Global Match Rate
QN
Neural Network Knowledge Base HYPLOSP
 
Fig. 4. STR prediction: QN of the KB, NN methods and HYPLOSP with respect to the global match rate. 
3.3.  Benchmark comparison of HYPLOSP with previous studies 
To compare HYPLOSP with existing methods, we followed the methodology adopted in EVA24 by 
using new submissions to PDB25 and avoiding homologous proteins in the training stage since all of 
these methods may be trained on different datasets. So, the benchmark comparison is based on the 
second dataset, which contains 56 new protein chains with an average global match rate of only 25.49. 
The low match rate can be attributed to the following factors: (1) some proteins have only a few HSPs; 
(2) though there are enough HSPs for some proteins, only a few of their peptides match the SSKB. 
Fig. 5 shows the number of proteins versus the global match rate.  
The experimental results are shown in Table IV. For the SAH alphabet, HYPLOSP outperforms 
HMMSTR by 4.4% and 5.15% in terms of QN and MDA, respectively; for the PB alphabet, it 
achieves 13.24% and 16.7% improvement over QN and MDA, respectively; and for the STR alphabet, 
it yields a QN of 59.03%, which is 0.76% lower than the result of SAM-T02. In addition, for STR, we 
note that KB has a QN of only 47.02% because this dataset has a low average global match rate. In 
contrast, NN has a QN of 58.26%. However, HYPLOSP yields a better QN (59.03%), which improves 
the NN result slightly.  
 
0
2
4
6
8
10
12
<5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45-50 55-60 60-65 65-70 70-75
Global Match Rate
Number
 
Fig. 5. The number of proteins versus the global match rate 
 33
the experiments in the testing stage of the KB method, while the neural-network training stage is the 
same as in Section 2.2.2. We summarize the experimental results in Table VI.  
Although our secondary structure prediction method can achieve Q3 of 81.6%22, the perform-
ance gap between using the predicted SSE and the observed SSE is still larger than the gap between 
using the predicted SSE and not using SSE. Using the observed SSE (i.e., secondary structure predic-
tion would improve from the current accuracy of 81.6% to an ideal 100%) can enhance the perform-
ance significantly. In other words, an improvement in current secondary structure prediction can fur-
ther improve local structure prediction. 
Table VI. Prediction results of HYPLOSP using the observed secondary struc-
ture information  
QN MDA  
Observed SSE Predicted SSE Without SSE Observed SSE Predicted SSE Without SSE 
SAH 65.81% 61.51% 60.14% 68.85% 62.69% 58.29% 
PB 69.14% 63.24% 61.91% 64.41% 58.66% 54.84% 
STR 77.15% 63.07% 60.59%  
4.2  Effect of the hybrid mechanism 
Our hybrid mechanism defined in HYPLOSP yields better results than the KB and NN methods. To 
better understand the hybrid effect, we define the hybrid_benefit of a protein in terms of QN as fol-
lows:  
 hybrid_benefit = (QN of HYPLOSP)－(QN of NN),  (9) 
since the NN prediction accuracy is relatively stable. A negative hybrid_benefit means that the hybrid 
mechanism’s prediction results are worse than those of NN, which happens when the global match 
rate is low and the KB prediction results are poor. For example, the average global match rate of the 
second test dataset is approximately 25.49. Despite this low rate, about 89.29%, 80.36%, and 81.82%, 
respectively, of the proteins in this dataset have a positive hybrid_benefit with an average of 2.05%, 
1.56%, and 0.78%, respectively. In Fig. 6, we illustrate the relation between the hybrid_benefit of 
STR prediction and the global match rate. (We chose STR because HYPLOSP’s performance is 
slightly inferior on this alphabet.) The figure shows that there is a positive regression curve between 
the hybrid_benefit and the global match rate.  
Fig. 7 further demonstrates the relation between the average hybrid_benefit and the global match 
rate on nrDSSP. Clearly, there is a positive relation between the hybrid_benefit and the global match 
rate. This shows, once again, that when more protein structures are determined, the knowledge base 
will expand and the hybrid mechanism will derive better hybrid_benefit. In this case, the performance 
of HYPLOSP will improve. 
 35
NN-1 58.78%  
NN-2 59.19%
HYPLOSP 63.07%
 STR 
HYPLOSP-2 63.42%  
4.4.  Comparison with the standard two-stage neural network approach 
Since the two-stage neural network approach is frequently applied to secondary structure prediction, 
we examine the effect of replacing the 1-stage NN in HYPLOSP with a 2-stage NN. The resulting 
hybrid system is called HYPLOSP-2. The two-stage neural network architecture and HYPLOSP-2 are 
described below. 
A two-stage neural network for protein secondary structure prediction usually contains the first 
neural network that maps amino acid sequences (or profiles) to structures, and the second struc-
ture-to-structure neural network refines the results26,27. In our construction, the first neural network 
remains the same as described in Section 2.2.1, and the second neural network takes the output of the 
first network as input. The sliding window length which yields the best performance is 11, and the 
learning parameters are identical to section 2.2.2. The training and testing procedure of the second 
neural network are similar to Jones et al.27, but uses structural letters instead of SSE.  
The hybrid strategy of HYPLOSP-2 is basically the same as HYPLOSP. But the confidence 
score of neural network prediction is obtained from the output of the second network.  
We use the nrDSSP dataset to compare the performance of one-stage and two-stage neural net-
work prediction methods, HYPLOSP and HYPLOSP-2. The results are shown in Table VIII. We can 
observe HYPLOSP outperforms the two-stage NN method in every case, though the two-stage NN 
method produces slight improvements (0.33% to 1.59%) to the one-stage NN method (shown in Table 
III). Obviously, incorporating the knowledge-based method is more effective than adding a struc-
ture-to-structure neural network.  
We further examine the performance of HYPLOSP-2 and two-stage NN method. Incorporating 
the knowledge-based method to the two-stage NN method can still improve the performance (even if 
we do not fine-tune our hybrid mechanism). For example, HYPLOSP-2 improves the QN of the 
two-stage NN results by 0.57%, 1.25%, and 1.51% for SAH, PB, and STR.  
Conclusion 
Since existing local structure prediction methods are limited in performance, we use two different 
prediction methods: a knowledge-based method and a neural network-based method. To better utilize 
the advantages of these two methods, we propose a hybrid method, called HYPLOSP, which is al-
phabet-independent. We use three popular structural alphabets, SAH, PB, and STR, to evaluate the 
three methods and perform a 10-fold cross-validation test on nrDSSP containing nearly 4,000 protein 
chains. In addition, we also conduct a benchmark test that compares HYPLOSP with the prediction 
methods proposed by the authors of SAH, PB, and STR on a dataset of 56 protein chains. HYPLOSP 
shows promising results in terms of QN and MDA accuracy and also demonstrates its alpha-
bet-independent capability. As more protein structures are determined, the knowledge-based method 
and HYPLOSP can be further improved, as evidenced by the increase in the number of proteins with 
 37
25(17), 3389-3402 (1997). 
12. C Bystroff, V Thorsson, D Baker. HMMSTR: a hidden Markov model for local sequence-structure 
correlations in proteins. J. Mol. Biol. 301, 173-190 (2000). 
13. AG de Brevern, C Etchebest, S Hazout. Bayesian probabilistic approach for predicting backbone 
structures in terms of protein blocks. Proteins  41, 271-287 (2000). 
14. R Hughey, A Krogh. SAM: Sequence alignment and modeling software system, version3. Techni-
cal report UCSC-CRL-95-7. Santa Cruz, CA: University of California, Santa Cruz, Computer En-
gineering (1995). 
15. R Hughey, K Karplus, A Krogh. SAM: Sequence alignment and modeling software system, ver-
sion 3. Technical report UCSC-CRL-99-11. Santa Cruz, CA: University of California, Santa Cruz, 
Computer Engineering (1999). Available from http://www.soe.ucsc.edu/rsearch/compbio/ sam.html 
16. AS Yang, LY Wang. Local structure prediction with local structure-based sequence profiles. Bioin-
formatics 19(10), 1267-1274 (2003). 
17. R Kuang, CS Leslie, AS Yang. Protein backbone angle prediction with machine learning ap-
proaches. Bioinformatics 20, 1612-1621 (2004). 
18. T Tang, J Xu, M Li. Discovering sequence-structure motifs from protein segments and two appli-
cations. Pacific Symposium on Biocomputing (2005). 
19. W Kabsch. and C Sander. Definition of secondary structure of proteins given a set of 3D coordi-
nates. Biopolymers 22, 2577-2637 (1983). 
20. KD Pruitt, T Tatusova, DR Maglott. NCBI reference sequence (RefSeq): a curated non-redundant 
sequence database of genomes, transcripts, and proteins. Nucleic Acids Res. 33, 501-504 (2005). 
21. D.Rumelhart, G. Hinton, and R. Williams. 1988. Learning internal representations by error propa-
gation. In Neurocomputing, 675-695. Cambridge, MA: MIT Press 
22. HN Lin, JM Chang, KP Wu, TY Sung, WL Hsu. A knowledge-based hybrid method for protein 
secondary structure prediction based on local prediction confidence. Bioinformatics 21, 3227-3233 
(2005). 
23. AG de Brevern, C Benros, R Gautier, H Valadié, H Hazout, C Etchebest. Local backbone structure 
prediction of proteins. In silico Biology 4(3),381-6 (2004). 
24. B Rost, VA Eyrich. EVA: large-scale analysis of secondary structure prediction. Proteins 5, 
192-199 (2001). 
25. HM Berman, J Westbrook, Z Feng, G Gilliland, TN Bhat, H Weissig, IN Shindyalov, PE Bourne. 
The Protein Data Bank. Nucleic Acids Res. 28, 235-242 (2000). 
26. B Rost, C Sander. Prediction of protein secondary structure at better than 70% accuracy. J. Mol. 
Biol. 232, 584-599 (1993). 
27. DT Jones. Protein secondary structure prediction based on position-specific scoring matrices. J. 
Mol. Biol. 292, 195-202 (1999). 
 39
