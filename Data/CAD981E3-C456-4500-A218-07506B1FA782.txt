平台式系統晶片之節能記憶體架構 
“Energy-Efficient Memory Hierarchy for Platform-based SoC” 
計畫編號：NSC95-2221-E-002-360- 
執行期間：95 年 8 月 1 日 至 96 年 7 月 31 日 
主持人：楊佳玲 台灣大學資訊工程系副教授 
 
一、 中文摘要 
隨著製成技術的進步，漏電在單晶片系統
上造成之能源消耗的問題也越來越重要。在一
處理器中，快取記憶體所需之資源佔相當大部
份，因此，有許多針對快取記憶體以減少漏電
之機制被提出。然而，這些機制都會引起無法
預期之效能衰退，因此並不適用於需要絕對遵
守時間限制之硬性即時系統(hard real-time 
system)應用程式。在本計畫中，我們利用現有
之減少快取記憶體漏電之電路設計，提出第一
個適用於硬性即時系統之控制漏電機制。此考
量時間限制之減少快取記憶體漏電機制，利用
每個工作(task)之多餘時間(slack time)來決
定是否要將每個工作相對應之快取記憶體區塊
放入低漏電模式，並且保證每個工作可在其時
間限制內完成。實驗數據顯示，我們所提出之
漏電控制機制，與不管時間限制之漏電控制機
制相比，可達到幾乎相同之漏電減少量。 
關鍵字: 漏電， 快取記憶體 ，硬性即時系統 
 
英文摘要 
Leakage energy consumption is an 
increasingly important issue as the technology 
continues to shrink. Since on-chip caches 
constitute a major portion of the processor’s 
transistor budget, several leakage reduction 
schemes have been proposed to reduce cache 
leakage. However, these schemes introduce 
performance unpredictability thereby not suitable 
for hard real-time applications that require the 
timing constraint is met in all cases. In this paper, 
we propose the first approach to apply existing low 
leakage circuit techniques on hard real-time 
applications. The proposed timing-aware cache 
leakage control mechanism exploits task slack 
time to turn cache lines into the low-leakage state 
provided that the timing constraint is met. The 
experimental results show that the proposed 
control policy achieves comparable leakage 
reduction to the leakage control policy that 
aggressively turn cache lines into low leakage 
mode without considering the timing constraint.  
 
Keywords: leakage, cache, hard real-time system 
二、 計畫的緣由與目的 
Power consumption is becoming a critical 
design issue of embedded systems due to the 
popularity of portable devices such as cellular 
phones and personal digital assistants. As the 
technology continues to shrink, leakage power is 
becoming a dominant factor to overall CPU energy 
[13]. Reducing leakage energy can be done by 
exploiting task idle time to shut down the CPU 
completely [4, 5, 9, 10] or individual micro- 
architecture component, for example, caches [7, 19] 
and branch predictors [11]. Previous works on 
applying shutting down techniques to hard real- 
time systems only focus on turning off a CPU 
completely [4, 5, 9, 10]. We are not aware of any 
increased, the discrepancy of leakage savings 
between the proposed scheme and the simple 
policy decreases, and the leakage savings of the 
proposed method is approaching that of the simple 
policy. With 20% of static slack, our scheme 
achieves 1.3% more leakage savings than the 
simple policy. Joint use of drowsy caches and 
gated-Vdd also leads to more leakage savings. 
When the proposed scheme has opportunities to 
turn off the cache lines of a idle task, the proposed 
scheme achieves 2.8% more leakage reduction 
than the one with the drowsy caches only. 
 
三、 研究方法及成果 
In this section, we first introduce the system 
model we discussed in this project, and then we 
describe the proposed timing-aware leakage 
control policy.  
1. System Model 
The system consists of a task set of n periodic real 
time tasks. These tasks are independent tasks and 
preemptable. Tasks are denoted as T = {τ1, τ2… 
τn}, where T denotes the task set andτi denotes 
the i-th task of n tasks. Each τ i has its own 
period Pi and its WCET Wi. We assume a task’s 
deadline is its period. Tasks are scheduled using 
the EDF scheduling policy. A task with earlier 
deadlines gets higher priority. The scheduler has 
two queues: waiting queue (Qwaiting) and ready 
queue (Qready). The waiting queue contains the 
completed tasks, and the ready queue contains the 
running and preempted tasks. The task that is 
currently running is the active task, and the 
completed and preempted tasks are idle tasks. The 
schedualibitlity of a task set is tested by the CPU 
utilization U defined ∑
=
=
n
i ii
PWU
1
/ . If U is less 
than 100%, the task set is said to be schedulable.  
The baseline cache architecture that supports 
cache locking described in [11] is shown in Figure 
1. The lock_ctrl signal indicates whether a cache 
line can be replaced or not. We select instructions 
to be locked in the instruction cache based on the 
locking algorithm described in [11]. Each cache 
line is associated with leakage mode bits to select 
the supply voltage. A cache line can be turned into 
either the drowsy caches or state-destructive mode 
(i.e. the gated-Vdd circuit). We use the terms 
drowsy mode and state-preserving mode 
interchangeably in this report. 
Tag array Data array
Leakage mode
D
ecod
er
 
D
ecod
er
 
D
ecod
er
 
D
ecod
er
 
tag indexaddress =
Leakage mode
voltage
controller
voltage
controller
hit
lock_ctrl
 
Figure 1. Baseline cache architecture of the proposed 
scheme. 
 
2. Timing-aware leakage control 
The objective of the proposed leakage 
management scheme is to determine the drowsy 
window size for active tasks and the leakage mode 
for idle tasks, provided that the timing constraint is 
not violated. The details of the proposed scheme 
are described as the follows. 
 
2.1 Leakage Control Scheme for Active Tasks 
The leakage control scheme for active tasks is 
based on the Drowsy+Simple policy proposed in 
[7]. Different from Drowsy+Simple in [7] that 
uses fixed drowsy window size, the proposed 
leakage control scheme for active tasks adjust 
drowsy window size dynamically with hard 
real-time guarantee. The drowsy window size 
associated with L(Bi), which is the number of 
locked cache lines in basic block Bi. The WCAS 
size of each node Bi, which is denoted by AS(Bi), 
is the maximal number of locked cache lines that 
could be accessed from Bi. Therefore, AS(Bi) is 
calculated by  
AS(Bi) = max{L(Bi) + AS(Bj)}; )( ij BchildB ∈∀  
WCAS analysis is performed at compile time. 
To convey the WCAS size to the cache controller, 
which performs the leakage control, we use a store 
instruction to write the WCAS size to the cache 
controller, and the cache controller triggers drowsy 
window resizing on receiving a WCAS size. To 
prevent frequent drowsy window resizing, we 
merge basic blocks of a loop into one, and insert 
the store instruction at the loop entry point. As 
shown in Figure 3, B2, B3, B4 and B5 form a loop, 
and the active set size information is recorded on 
B2 only.  
 
2.1.2 On-line Phase 
Dynamic Slack Reclamation 
Dynamic slack is from variations of task 
execution time, and the collection of the dynamic 
slack time is performed by the OS when a context 
switch occurs. The dynamic slack reclamation 
process is similar to the one proposed in [15]. 
Before we detail dynamic slack reclamation, we 
first define five notations:  
 
CPU
iU  : the unused CPU budget of τi. 
 
rem
iW  :the remaining WCET ofτi. 
 Si: the slack time ofτi. 
  Ei: the execution time ofτi. 
 DS: dynamic slack time 
When a task arrives (i.e., removed from the 
waiting queue), CPUiU  and remiW  are initialized 
to (WCET + static slack) and WCET, respectively. 
During the execution ofτi., CPUiU  is consumed, 
and remiW  decreases. 
rem
iW  is updated by cache 
controller during task execution. Since the 
wake-up overhead of drowsy cache line does not 
estimated in WCET, at every cycle, remiW  is 
decreased by one when there is no drowsy cache 
hit. Note that we do not claim the slack time of 
preempted tasks as in [15]. In our scheme, a 
preempted task could utilize its slack to turn its 
cache lines into the low leakage mode during the 
idle period. Whenτi is preempted or completes, 
we first consume the dynamic slack (DS) from 
unused CPU budget of the tasks in Qwaiting with 
earlier deadlines. Then, we update CPUiU  of task
τi. DS is estimated by the following equation:  
∑ ∈= waitingk Q
CPU
KUDS τ   
If DS is greater than Ei, CPUiU  is not consumed. 
Otherwise, the CPU budget is updated using the 
following formula. 
CPU
iU  = 
CPU
iU - (Ei - DS) 
Therefore, the slack time that a task can use to 
compensate the wake-up overheads is: 
Si = ( CPUiU - remiW ) + DS 
 
Drowsy Window Resizing 
The process of drowsy window resizing is to 
decide the smallest drowsy window size such that 
the timing constraint is met. Drowsy window 
resizing is performed when a context switch 
occurs or when the active set changes. To decide 
the drowsy window size of the scheduled task, we 
have to find the smallest drowsy window size with 
the wake-up overhead that is not larger than the 
task’s available slack. Therefore, the drowsy 
window size is the smallest window size that 
satisfies the following inequality:  
  iiactiveremi SOHSwsizeW <×× )(/      ( 1 ) 
, where wsize denotes the window size, Sactive(i) 
denotes the WCAS size of task τ i, and OH 
mode, the performance overhead of accessing a drowsy line is set to 2 cycles according to [16]. 
Table 1. Simulated architecture parameters. 
Processor Core 
Instruction window 16-RUU, 16-LSQ 
Issue width 1 instruction per cycle, in-order issue 
Functional units 4 IntALU, 1 IntMult Div, 1 FPALU, 1 FP Mult Div 
Memory Hierarchy 
L1 I-cache Size 8KB, 2-way, 16B block size 
L2 cache Size 32KB, 4-way, 32B block size, 8-cycle access latency 
Memory 8-cycle access latency 
Energy Parameter 
Processor technology 0.07nm 
Supply voltage 0.9V 
Temperature 353 
  
Table 2. Task set characterization. 
Name Description Code size(byte) WCET(cycles) 
Small task set (Total code size 7608 bytes) 
Jfdctint JPEG integer implementation of the forward DCT 3296 19087 
Crc cyclic redundancy code example program 1400 142088 
Ludcmp Linear equations by LU decomposition 2336 16607 
Matmult Matrix multiplication 576 12555 
Medium task set (Total code size 9192 bytes) 
Qurt Computation of roots of quadradic equations 1200 4038 
Minver Matrix inversion 3656 11281 
Jfdctint JPEG integer implementation of the forward DCT 3296 18969 
Fft1 FFT Cooly-Turkey algorithm 1040 8685 
The detailed processor and memory hierarchy 
parameters are shown in Table 1. We implement 
two leakage control mechanisms, the 
Drowsy+Simple scheme proposed in [7], and the 
proposed timing-aware leakage control scheme 
(TALC). For the Drowsy+Simple scheme, we 
determined the drowsy window size through 
exhaustive simulations and chose the best one on 
the average, 1000-cycle [7]. The cache lines 
allocated to idle tasks are turned into the drowsy 
mode immediately when a context switch occurs. 
The benchmarks used in this work are from the 
SNU real-time benchmark suite [1]. The 
benchmark programs are C sources which are 
collected from numerical calculation programs and 
DSP algorithms. We mix multiple applications 
together to form two multi-tasking workloads, the 
small task set and the medium task set. Details of 
the workloads are listed in Table 2. The WCET of 
each task is measured with cache locking. To 
active set for drowsy window resizing, it could 
overestimate the wake-up delay. For the medium 
task set, the overestimation is more serious than 
the small one since the medium task has larger 
code size and longer worst-case execution path. A 
more precise active set analysis scheme could help 
alleviate this problem. We leave this as the future 
work. As slack time increased, the energy savings 
achieved by TALC approaches Drowsy+Simple. 
With 20% static slack, the proposed scheme has 
1.1% and 1.3% more leakage savings that 
Drowsy+Simple for the small and medium task set, 
respectively. This energy advantage provided by 
TALC over Drowsy+Simple comes from run-time 
drowsy window resizing. With 20% static slack for 
the small task set, the window size ranges from 13 
cycles to 979 cycles while Drowsy+Simple fixed 
the window size to 1000-cycle.  
Table 3. Leakage savings of TALC-drowsy and TALC-dual. 
Static slack TALC-drowsy TALC-dual Differences 
20% 90.9% 93.3% 2.4% 
30% 91.7% 94.2% 2.5% 
40% 92.9% 95.6% 2.7% 
50% 93.9% 96.6% 2.7% 
60% 94.2% 97.0% 2.8% 
 
To evaluate the effect of turning off cache 
lines of idle tasks completely, we create a new task 
set that has idle periods long enough for the 
state-destructive mode. To lengthen the idle period, 
we can increase both static and dynamic slack. To 
increase static slack, we set 20%, 30%, 40%, 50% 
and 60% static slack in this set of experiments. To 
increase dynamic slack, we prolong a task’s 
WCET by increasing the number of iterations 
executed by the task’s major subroutines on the 
worst-case execution path. The BCET/WCET ratio 
remains 0.95 as the original setup, and the a task’s 
dynamic slack increases with its WCET prolonged. 
The experimental results of this new task set are 
shown in Table 3. In Table 3, TALC-drowsy 
denotes the TALC scheme with the drowsy mode 
only, and TALC-dual denotes the TALC scheme 
with both the drowsy mode and the 
state-destructive mode. The results show that 
turning off cache lines of an idle task completely 
achieves up to 2.8% more leakage saving than that 
of TALC-drowsy.  
 
四、結論 
In this project, we propose a timing-aware 
cache leakage control scheme for hard real-time 
system. The basic idea of the proposed algorithm 
is to consume system slack by the performance 
overhead caused by activating the drowsy cache 
lines. The proposed scheme manages cache lines 
of active and idle tasks differently. The objective 
of the proposed leakage management method is to 
determine the drowsy window size for the active 
task, and the leakage mode for the idle task 
provided that the timing constraints is not violated. 
Experimental results show that, although our 
scheme achieves less leakage savings than 
Drowsy+Simple with tight schedule, our scheme 
provides the timing constraint is met in all cases 
while Drowsy+Simple has tasks miss deadlines. 
With task slack increased, the discrepancy between 
leakage savings of our scheme and Drowsy+ 
Simple decreases. With 20% static slack, our 
scheme even achieves 1.3% more leakage savings 
than Drowsy+Simple. This energy advantage 
provided by the proposed scheme comes from 
run-time drowsy window resizing. With the task 
set that has opportunities to put cache lines into 
state-destructive mode for idle tasks, the proposed 
scheme achieves 2.8% more leakage savings than 
and M. Stan. Hotleakage: A temperature-aware model of 
subthreshold and gate leakage for architects. 
三、攜回資料名稱之內容 
 
2007 Date 會議論文集光碟片一片。 
 
 
四、結語 
 
非常感謝國科會提供補助，使得我得以成行。也使得我們有機會與國
外同領域的學者交換 low power embedded system design發展及研究的心
得。 
 
 
 
 
[16, §5.5]. We consider systems with Pd(s) as a convex and increas-
ing function, e.g., Pd(s) ∝ sα for any α > 1.
The number of CPU cycles executed in a time interval is linear of
the processor speed. That is, the number of CPU cycles completed
in time interval (t1, t2] is
R t2
t1
s(t)dt, where s(t) is the processor
speed at time t. The energy consumed in (t1, t2] is
R t2
t1
P (s(t))dt.
We first target ideal processors, in which a processor may operate
at any speed in [Smin, Smax]. We also show the extension to cope
with non-ideal processors with discrete speeds. For non-ideal pro-
cessors, there are H available speeds indexed by s1, s2, . . . , sH in
an increasing order. For non-ideal processors, for brevity, sH+1 and
P (sH+1) are both assumed ∞, Smin is s1, and Smax is sH .
When needed, turning the processor into a dormant mode (or
turning the processor off) might further reduce the energy consump-
tion. However, turning off or waking up a processor takes time and
has energy overheads. For processors with non-negligible overheads
to be turned off, the overheads could be treated as part of the over-
heads to turn on the processor [6, 10]. We denote Esw (tsw, re-
spectively) as the energy (the time, respectively) requirement of the
switching overheads for the whole process on turning off the proces-
sor and then turning on the processor.
Task models Tasks considered in this paper are periodic and inde-
pendent in execution. A periodic task is an infinite sequence of task
instances, referred to as jobs, where each job of a task comes in a
regular period. Each task τi is associated with its initial arrival time
(denoted as ai), its computation requirement in CPU cycles (denoted
as ci), and its period (denoted as pi). The relative deadline of each
task τi is equal to its period pi. That is, the arrival time and dead-
line of the j-th job of task τi are ai + (j − 1) · pi and ai + j · pi,
respectively. We assume that all the tasks arrive at time 0, but ex-
tensions can be achieved easily for tasks with different arrival times.
Given a task set T, the hyper-period of T, denoted by L, is defined
as the minimum L so that L/pi is an integer for any task τi in T.
For example, L is the least common multiple (LCM) of the periods
of tasks inT when the periods of tasks are all integers. Without loss
of generality, we only consider tasks τis with cipi ≤ Smax, since it is
not possible to complete any task τj with cjpj > Smax in time.
This research explores systems with the possibility to reject a
task for execution with a specified cost (penalty) provided by system
designers. If a task is more important than another, its rejection cost
should be specified with a greater value. If a task instance of task
τi is not completed in time, the system receives χi penalty, where
χi > 0. (If a task can be rejected without penalty, we can reject the
task directly.) If a task is very important and cannot be rejected, its
rejection cost should be specified as ∞. If the rejection costs of all
the tasks are infinite, all the tasks are asked to be completed in time.
Problem definition This paper explores the problem on the min-
imization of the energy consumption of the system and the rejec-
tion cost at the same time. We pursue the objective on the linear
combination of the energy consumption and the rejection cost, i.e.,
(1 − α)E + αΠ, where α is a non-negative factor no more than 1
specified by the system designer, E is the energy consumption of
the system in the hyper-period, and Π is the total rejection penalty
of the task instances missing their deadlines in the hyper-period. If
energy consumption minimization is more important than task rejec-
tion penalty minimization, α should be specified as close to 0, and
vice versa.
For notational brevity, we normalize the rejection penalty of task
τi as αχi, the power consumption function P () as (1− α)P (), the
energy switching overheads as (1 − α)Esw. Hence, the objective
of the linear combination can be treated as the summation of the
(normalized) penalty and the (normalized) energy consumption.
The problem explored in this paper is defined as follows:
DEFINITION 1. Energy-eFFicient schEduling with rejeCting Tasks
(EFFECT):
Consider a task set T of N independent tasks over M identical
processors with a common power consumption function P (s). Each
periodic task τi ∈ T arrives at time 0 and is associated with a com-
putation requirement in ci CPU-cycles, a rejection cost (penalty) χi,
and a period pi, where the relative deadline of task τi is pi. The en-
ergy consumption and timing of the switching overheads are Esw
and tsw, respectively. The problem is to derive a schedule of T to
minimize the summation of the penalty (cost) of the task instances
that miss their deadlines and the energy consumption of the system
in the hyper-period L of tasks in T, in which a job of task τi is
executed entirely on a processor.
For brevity, for the rest of this paper, the objective function of the
EFFECT problem is called as energy-penalty (EP for abbreviation).
Hardness analysis Since most previous studies on multiprocessor
energy-efficient scheduling did not take task rejection penalty into
considerations, the schedulability of the derived schedules cannot be
guaranteed, e.g., [4, 9]. As shown in [6], it is NP-hard to derive
a schedule with the minimum energy consumption to complete all
the tasks in time without rejecting any real-time task. The following
lemma shows that the EFFECT problem is still NP-hard even if we
have the flexibility to reject some tasks for execution.
LEMMA 1. The EFFECT problem isNP-hard in a strong sense even
when Esw is 0, and all the tasks have the same rejection penalty.
Proof. It can be proved by a reduction from the leakage-aware
multiprocessor energy-efficient rejection problem [6] with the same
period p. The rejection cost of each task is a constant greater than
P (Smax) · p. The detail is omitted due to space limitation.
Due to the NP-hardness of the EFFECT problem, polynomial-
time approximation algorithms might be pursued for the provision of
approximated solutions with worst-case guarantees. A polynomial-
time β-approximation algorithm for the EFFECT problem must have
polynomial-time complexity of the input size and could derive a
solution with an objective value at most β times of an optimal
solution, for any input instance. However, in addition to the NP-
hardness of the EFFECT problem, the following theorem shows the
hardness on the approximability of polynomial-time algorithms.
THEOREM 1. There does not exist any polynomial-time approxima-
tion algorithm for the EFFECT problem unless P = NP .
Proof. This theorem can be proved by a gap reduction from the
NP-complete PARTITION problem: Given a set of N non-negative
numbers, denoted by o1, o2, . . . , oN , the PARTITION problem is to
answer whether there is a partition of these N numbers into two sets,
so that the sum of the numbers in each set is the same. Suppose for
contradiction that there is a polynomial-time (1 + )-approximation
algorithm, denoted by Algorithm A, with  > 0 for the EFFECT
problem. We will show that we can use Algorithm A to answer
the PARTITION problem in polynomial time, which contradicts the
assumption on P = NP .
To solve the PARTITION problem by applying Algorithm A, we
have to create an input instance for the EFFECT problem. For each
number oi, a unique task τi is created with ci as oi, pi as
PN
j=1 oj
2
,
and χi as (1 + )(
PN
j=1 oj), where P (s) = s
3 and Esw = 0.
Moreover, Smax is 1, and Smin is no more than 1. If the input
instance of the PARTITION problem admits a positive answer, the
optimal solution for the constructed input instance is
PN
j=1 oj . By
the construction, there exists no feasible solution with EP more thanPN
j=1 oj and no more than (1 + )
PN
j=1 oj . Since Algorithm A
is a (1 + )-approximation algorithm, Algorithm A guarantees to
derive a solution whose EP is
PN
j=1 oj . If the input instance of the
PARTITION problem does not admit a positive answer, the solution
answered by Algorithm A must be greater thanPNj=1 oj .
Since the construction of the input instance of the EFFECT prob-
lem takes O(N) time, and Algorithm A is with polynomial-time
Algorithm 3 : SGA
Input: T,M ;
1: sort tasks inT non-increasingly according to χi
ci
;
2: let y∗i be the value of yi of task τi after calling LEP(∅,∅, 0);
3: T† ← {τi | y∗i = 1}, T ← T \T†;
4: let (T†1,T
†
2, . . . ,T
†
M ) be the task partition of T† on M processors
derived from Algorithm LA+LTF in [6];
5: for m ← 1;m ≤ M ;m ← m + 1 do
6: while
P
τi∈T†m
ci
pi
> Smax do
7: let τj be the task with the minimum
χj
pj
inT†m;
8: T†m ← T†m \ {τj},T ← T ∪ {τj};
9: return (T†1,T
†
2, . . . ,T
†
M ,T
) as the task partition;
and tasks with lower χi
ci
for rejection. Let T† be the set of tasks
decided to be executed on these M processors. Initially, T† is ∅.
For scheduling the selected tasks on these M processors in poly-
nomial time, we apply Algorithm LA+LTF (Leakage-Aware Largest-
Task-First) in [6] to partition these tasks into M disjoint sets. Algo-
rithm LA+LTF sorts these selected tasks in a non-increasing order of
their loads, in which the load of a task τi is defined by its compu-
tation requirement divided by its period, i.e., ci
pi
. Then, Algorithm
LA+LTF assigns tasks according to the sorted order to the processor
with the least load so far.
The first algorithm is Algorithm SGA, stands for Standard Greedy
Algorithm. For each iteration, we consider the selection of task
τi according to the non-increasing order of χjcj for tasks τj in T.
Algorithm SGA applies Algorithm LEP for the determination. Let
(y∗1 , y
∗
2 , . . . , y
∗
N) be the vector of yis of tasks τis after calling
LEP(∅, ∅, 0). Algorithm SGA then first attempts to execute all the
tasks in T† ← {τi | y∗i = 1} on these M processors. By apply-
ing Algorithm LA+LTF to assign tasks in T† to M processors, we
can have a task partition (T†1,T
†
2, . . . ,T
†
M ). However,
P
τi∈T†m
ci
pi
might be greater than Smax, and, hence, we must reject some tasks
in T†. Algorithm SGA then repeatedly evicts the task with the mini-
mum
χj
pj
fromT†m until the schedulability is guaranteed on the m-th
processor. Algorithm SGA is summarized in Algorithm 3. The time
complexity is O((N + M) log(N + M)).
Algorithm EGA, stands for Enhanced Greedy Algorithm, is an
enhancement of Algorithm SGA. The difference is on the derivation
of (y∗1 , y∗2 , . . . , y∗N ) in Algorithm LEP. Instead of returning the result
when yi < 0 in Step 6 in Algorithm 1, the revised Algorithm
LEP continues the loop by setting yi to 0. The time complexity of
Algorithm EGA is the same as that of Algorithm SGA.
Algorithm ES+EGA (Enhanced Greedy Algorithm with Esti-
mated Schedule) applies Algorithm LA+LTF on the fly to verify
whether the execution of task τi can reduce the EP by evaluating the
EP of the derived schedule.1
Both Algorithms SGA and EGA evict those tasks τis with y∗i < 1,
and Algorithm ES+EGA evicts a task τi if executing τi and the se-
lected tasks has greater EP . However, execution of some of these
tasks with eviction on some selected tasks might reduce the EP. Al-
gorithm TE+EGA (Enhanced Greedy Algorithm with Task Eviction)
is the revision of Algorithm ES+EGA with the possibility of evictions
of tasks already in T†. If applying Algorithm LA+LTF to execute
T†∪{τi} is not a feasible solution or with greater EP than that to ex-
ecuteT†, Algorithm TE+EGA first finds the index m′, in whichT†
m′
is the task setT†m of the task partition ofT† derived from Algorithm
LA+LTF with the smallest
P
τj∈T†m
χj
pj
−P ∗(P
τj∈T†m
cj
pj
). That is,
m′ is the index, in which evicting all the tasks in T†m′ increases no
greater EP than any other index. Then, if Algorithm LA+LTF can
1 The pseudo-code of Algorithm ES+EGA is to eliminate the steps between
Step 6 and Step 10 in Algorithm 4.
Algorithm 4 : TE+EGA
Input: T,M ;
1: sort tasks in T non-increasingly according to χi
ci
;
2: T† ← ∅,T ← T;
3: for i ← 1; i ≤ N ; i ← i + 1 do
4: if applying Algorithm LA+LTF to executeT†∪{τi} has a feasible so-
lution with less EP than the EP to execute T† by applying Algorithm
LA+LTF then
5: T† ← T† ∪ {τi},T ← T \ {τi};
6: else
7: let (T†1,T
†
2, . . . ,T
†
M ) be the task partition of T
† on M proces-
sors derived from Algorithm LA+LTF;
8: let m′ be the index m with the smallest
P
τj∈T†m
χj
pj
−
P ∗(
P
τj∈T†m
cj
pj
);
9: if Algorithm LA+LTF can have a feasible task partition for task set
T† \T†
m′ ∪ {τi} with less EP than the EP by applying Algorithm
LA+LTF toT† then
10: T† ← T† \T†
m′ ∪ {τi},T ← T \ {τi} ∪T
†
m′ ;
11: return (T†1,T
†
2, . . . ,T
†
M ,T
), where T†m is the task set on the m-th
processor by applying Algorithm LA+LTF for T†;
have a feasible task partition for task setT† \T†
m′ ∪ {τi} with less
EP than the EP by applying Algorithm LA+LTF to T†, we update
T† as T† \ T†
m′ ∪ {τi}. The detail procedure is shown in Algo-
rithm 4. Algorithm TE+EGA has the same time complexity as Algo-
rithm ES+EGA, which is O(N(N + M) log(N + M)).
3.1.3 Extensions to non-ideal processors
Algorithms in Sections 3.1.1 and 3.1.2 are designed for ideal proces-
sors. With slight modifications, they can be applied to systems with
discretely available speeds. As shown in [11, 13], if a task is going
to execute for t time units to complete C cycles, we can execute the
task at two speeds sh and sh+1, in which sh < Ct ≤ sh+1, for th
and th+1 time units so that th+th+1 is t and thsh+th+1sh+1 is C.
Therefore, what we have to do is to re-define the power consumption
function P ∗ in Equation (1) as follows:
P ∗(s) =
8><
>:
 
sh+1−s
sh+1−sh P (sh)+
s−sh
sh+1−sh P (sh+1)
!
, when sh < s < sh+1,
P (s), when s = sh, for some h
s
s∗P (s
∗), otherwise.
(2)
All the algorithms in Sections 3.1.1 and 3.1.2 can be applied to non-
ideal processors according to the revision of P ∗(s) in Equation (2).
3.2 Systems with non-negligible switching overheads
For systems with non-negligible switching overheads, we first apply
the first-fit strategy to re-assigned the tasks selected for execution
to reduce the number of processors executed at the critical speed
[6]. Then, each processor determines its schedule independently
by applying the procrastination algorithm in [12]. Due to space
limitation, we only sketch the ideas here.
Suppose that at time instant t, there is no task instance in the
ready queue on a processor. By the procrastination algorithm [6, 12],
the processor is either turned off or idle at the lowest available speed.
The determination of the switching can be done by verifying whether
the idle interval is longer than max{tsw, Esw/P (Smin)}. If the
processor is turned off, the scheduler has to decide when to turn
on the processor, and the energy consumption in the idle interval is
Esw. Suppose that the procrastination schedule decides to turn off
the processor at time instant t, and turn on the processor at time
instant t∗ by applying the procrastination algorithm [12]. We then
evaluate whether there is a task instance which is decided to be
rejected in the off-line phase and be done before the time instant t∗.
If such a task instance exists and the EP obtained in the estimated
