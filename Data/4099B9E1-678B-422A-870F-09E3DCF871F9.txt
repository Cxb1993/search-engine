 
語意網路本體知識選擇、建立及整合技術之研究 
 
摘要 
    由於網際網路的廣泛使用，使得透過網路達到知識分享與交換的需求劇增。不同領域
間知識分享與交換的困難度因其定義與所用詞彙的差異也因而增加，導致欲同時取得多方
知識必須耗費大量的時間與成本。因此語意網的概念是被提出來用以協助電腦能具有閱讀
及理解不同領域網頁資訊的能力，其中又以知識本體被用來描述某個知識領域及與其相關
之詞彙，並供邏輯推理之用。因此，如何有效地處理不同領域的知識本體，並且使之結合
成為一有效、可重複使用及整合之知識本體倉儲將成為一個有趣且重要的研究課題。在本
論文中，我們因此試圖提出一個知識本體倉儲架構來幫助增加在網路上的智慧型應用。它
是整合了多個獨立的知識本體而形成一個更為有用的知識本體倉儲。使用者不僅可以獲得
來自於各別獨立的知識本體之資訊，更可從知識本體倉儲中取得更複雜詳細的知識。在知
識本體倉儲架構中的各個模組也被設計成能符合建立一個知識本體倉儲的需求。而這些模
組包含了分析，選擇，擷取，轉換，整合，知識本體倉儲，知識本體市集、應用模組及 OWL
為基準的本體資料挖掘系統。 
因此整個計畫所涵蓋的的研究內容很廣，我們以三年的時間來探討這些相關問題以及
發展適當的方法。首先，計畫第一年的重點放在對相關背景知識的收集了解及對使用者需
求模式及介面之探討，此外我們也設計了符合使用者需求的本體知識的選擇方法。接著在
計畫的第二年我們研究本體知識架構內路徑的比對技術及評分方法以能做適度評比，並藉
以設計從一本體知識中有效萃取及精簡知識的方法，此外我們也提出了初步的知識本體倉
儲架構。最後一年，我們發展架設一個以 OWL 語言為基礎的本體知識查詢系統，包含有問
句剖析器和推論系統。透過此系統，使用者將選出來的本體市集做更有效的查詢，並將結
果以 RDF 圖形的表示方式將答案輸出給使用者，以提供使用者參考並做決策。 
 
關鍵字：本體知識、語意網、本體知識選擇、本體知識整合、本體知識轉換、OWL 的本體
查詢系統 
 1. Introduction 
Along with the progress of information technology and the development of computer 
networks in recent years, retrieving required data from different sources has become easier than 
before. Some information techniques, such as database management, data warehousing, data 
mining, semantic webs, and among others, have been widely and efficiently applied by 
enterprises and organizations on data acquisition and analysis. Rules and other types of 
knowledge may be extracted from the raw data and can provide some help to decision makers 
while they are making their decisions. 
 
In the past, the adopted knowledge was usually confined to a particular domain in which the 
data were collected. Due to the wide usage of internet in this decade, the need of sharing and 
exchanging knowledge is dramatically increasing. It, however, also causes some difficulties in 
correctly using the knowledge since different definitions and terminologies may be adopted 
among different data sources. Considerable time and cost may be spent in obtaining knowledge 
from multiple sources. Recently, the concept of semantic webs has been widely used to help 
make machines read and understand the web pages in different domains. Especially, ontology 
plays an important role on performing semantic webs. It can be used to describe particular 
domain knowledge and related terms for effective logical inference.  
 
As mentioned above, semantic-web and ontology have become more and more important 
along with various applications on the internet. How to effectively deal with ontologies and to 
combine them in different resources for forming an efficient, reusable and integrated ontology 
warehouse is an interesting and challenging task. 
 
2. Research Objectives 
In this project, we thus attempt to propose a framework of ontology warehouse to achieve 
this purpose. It integrates several single ontologies to form a more useful ontology warehouse. 
Users can not only obtain information from each individual ontology, but can also get more 
sophisticated knowledge from the ontology warehouse. 
Modules on the framework are also designed to meet the requirements of building an 
ontology warehouse. These modules include analysis, selection, extraction, transformation, 
user queries. Views can be defined by query languages and can provide particular formats of 
query results to users. There are two kinds of views in a data warehousing system: materialized 
view and virtual view. A materialized view retrieves all of the necessary information from data 
sources according to the view definition and physically stores the extracted data in a data 
warehouse. A virtual view retrieves the information from other materialized views using the 
query language whenever the view contents are required. Each kind of views has its advantages 
and disadvantages, which can be referred to in [8][9][23][29][30]. OLAP also provides five 
operations: roll-up, drill-down, pivoting, slice and dice for processing diverse analysis 
requirement. 
 
3.2 Ontology 
The term “ontology” is very commonly seen in the field of artificial intelligence. It is often 
used to represent known or needed knowledge architecture [10][13][19][24]. It usually defines a 
common vocabulary and provides logical inference in a specific domain, thus enabling reuse and 
sharing of domain knowledge. Neches et al. defined an ontology as the basic terms and relations 
comprising a vocabulary [26]. Gruber took an ontology as a specification of a conceptualization 
[12]. Uschold and Gruninger thought an ontology might take a variety of forms, but it would 
necessarily include a vocabulary of terms and some specification of their meaning [32]. We could 
thus define an ontology as an explicit formal specifications of concepts, relationships and terms 
to represent domain knowledge.  
 An ontology usually has at least three components - classes, properties and instances 
[25][32]. A class describes the basic structure of a concept in a domain. In general, there are 
several kinds of relations such as generalization and association existing among the three 
components. Some ontology integration approaches were proposed in the past [3][25][28][31].  
 
3.3 OWL and OWL-QL 
The Web Ontology Language (OWL) [36] is the most popular for implementing semantic 
web applications. It is a makeup language for sharing and publishing data using ontology on the 
World Wide Web (WWW). It also can easily express the ontology needed in a particular domain. 
There are three sub-languages in OWL. They are OWL Lite, OWL DL and OWL Full [38]. 
Figure 1 shows the inclusion relation of them. 
4.1.1 Coarse-level Requirement Analysis 
The coarse-level analysis model is quite simple, only used for describing the main domains 
covered by the desired ontology and for selecting a fixed number of promising candidate source 
ontologies. The process for users to build an analysis model in the coarse level is shown in Figure 
2.  
Users
Coarse-Level Relation Table
Domain names
&
Relations 
Conversion
Coarse-Level Analysis Model
 
Figure 2: The process of building a coarse analysis model 
According to the objective of the desired ontology, the requirement analysis phase begins by 
asking users to input the desired domain names or the most important concept names into the first 
row of a table, called the coarse-level relation table. A simple example for building a coarse-level 
relation table is shown here. Assume in a medical field, a domain expert inputs three important 
domain names – disease, symptom and department of medicine. These three domain names will 
be put into the first row and then copied into the first column. The results are shown in Table 1. 
 
Table 1: The coarse-level relation table after domain names are input 
Department 
of Medicine
Symptom
Disease
Department 
of Medicine
SymptomDisease
 
Then the domain expert specifies the relations among the domain names. Each cell in the 
table allows only one relation, and thus the most principle one is filled. Assume the coarse-level 
relation table after the relationships are filled is shown in Table 2. 
Coarse-Level 
Analysis Model
Fine-Level Analysis Model
…
…. …
….
…
….
…
….
…
…
.
……
……
Users
Concept Designation
…
….
 
Figure 4: The process of building a fine-level analysis model 
In this process, each finer concept related to a domain name in the coarse level is further 
designated by users. Continuing the above example, assume a user inputs the concept “Internal 
Medicine” for the domain “Department of Medicine”. Assume he/she also inputs two concepts 
“Department of Intestines and Stomach” and “Department of Heart” under the concept “Internal 
Medicine“. These concepts will form the hierarchy as shown in Figure 5. 
Internal 
medicine
Department of 
intestines and 
stomach
Department
Of heart
Department 
Of
Medicine
 
Figure 5: The concept hierarchy built in the example  
In addition, the user can continuously use the implemented interface to add related or 
hierarchical concepts. A simple example for extending the coarse-level analysis model in the 
medical field is shown in Figure 6, where some more departments such as “surgical department” 
and “obstetrics and gynecology” department are added and the other two domains are refined. 
Department 
Of
Medicine
Surgical 
Department
Department of 
Internal Medicine
Department of 
Obstetrics and 
Gynecology 
Department of 
Intestines and 
Stomach
Department
Of Heart
…….
…….
…….
…….
Disease
Cold Gastric 
ulcer
Cardiopathy
…
…….
Prophylaxis 
and 
Therapy 
No 
smoke
Sport 
diagnosing
leading to
remedying
Symptom
Rhinorrhea
FeveringHeadache
……
……
 
Figure 6: The fine-level analysis model in the example 
4.1.4 Fine Selection 
The fine selection phase uses the fine-level analysis model further to find the best matched 
ontology for usage in a new application. Figure 8 shows the process of calculating the match 
degree between a relevant candidate ontology and the given fine-level analysis model. 
A Relevant 
Candidate Ontology  
Intra-Ontology Match Table
Ontology
Extraction
Vocabulary
Replacement
Fine-Level Analysis Model
Token
Extraction
Tokens
WordNet
Term n
…
Term 2
Term 1
Concept
m
…
Concept
2
Concept 
1
Fine-level  concept
Terms
in a selected 
source ontology
…
Matched Score
 
Figure 8: The process of calculating the match degree between a relevant candidate 
ontology and the given fine-level analysis model 
In the process, an intra-ontology match table as shown in Table 4 is used to keep the match 
degrees among the terms of a relevant candidate ontology and the concepts in the fine-level 
analysis model. If a term appearing in a relevant candidate ontology matches a concept, the 
corresponding cell is marked with a score. 
Table 4: The intra-ontology matching table 
Term n
…Term 3
Term 2
Term 1
Concept m…Concept 3Concept 2Concept 1
Fine-level  concept
Terms
in a selected 
source ontology
…
 
The token extraction procedure as in Phase 1 is used to cut out the tokens in a compound 
concept name for later match. The WordNet system is also used to replace the tokens by 
synonyms for flexible match.  
 
effective logical inference. Thus, an ontology warehouse will collect and integrate a set of 
ontologies from related or similar domains for a variety of applications. The source ontologies act 
as the knowledge sources for the ontology warehouse. The knowledge contained in different 
source ontologies may be different. For example, in an ontology warehouse for providing medical 
information, some ontologies about departments of medicine, diseases, symptoms, prophylaxis 
and therapy may be chosen as the sources. Each source ontology will be further processed to 
extract the relevant parts to be put into the ontology warehouse.  
 Extraction from Source Ontologies 
Part or all of each chosen source ontology is extracted for being integrated into the ontology 
warehouse according to its relevance to the application scope. Thus, effective extraction from 
source ontologies is a key to narrow down the size of an ontology warehouse. It first identifies the 
relevant part of the source ontology. It then uses the proposed extraction steps to filter out the 
desired part. The selection and extraction approaches were introduced in our first year project for 
finding appropriate ontology form different source ontologies. 
 Transformation and Integration 
The purpose of transformation is to make the extracted parts of different source ontologies 
have the same format such that their integration into the ontology warehouse can be easily 
achieved. The extracted ontologies have inevitably to be transformed into a consistent format 
before they are moved from the source ontologies to the ontology warehouse, because the 
knowledge structures and the terms used in different ontologies may be different. After the 
transformation, the extracted source ontologies can then be integrated into the ontology 
warehouse. The purpose of integration is then to integrate the extracted ontologies into a whole 
by removing redundant and unnecessary parts. The models of transformation and integration 
from management of the extracted ontologies from different sources are introduced in the project 
of this year, and they will be stated in the following subsections. 
 Ontology Warehouse 
An ontology warehouse stores the integrated ontology for different applications. It can not 
only effectively provide integrated ontology knowledge to users, but can also divide itself into 
different particular ontologies, called ontology marts, to help particular users increase their 
efficiency in ontology retrieval. A data warehouse involves all the integrated enterprise data and 
provides appropriate decision support to high-level managers, but a data mart contains only a 
specific scope of data interesting to particular users. An ontology warehouse plays the same role 
WordNet
Consistent Represented Ontologies
Value 
Transformation
Synonym
Templates 
Synonym
Grouping
Equivalent
Class 1
Equivalent
Class 2
Equivalent
Class 3
Equivalent
Class n…
…
Vocabulary
Replacement
…
Experts
Editing 
 
Figure 11: The process of value transformation 
The value transformation groups the values with the same meaning into an equivalent class. 
The attribute values appearing in the relevant parts of all the selected ontologies are checked 
through the assistance of WordNet or by the synonym templates assigned by users. The first value 
encountered is used as the name for the equivalent class. A synonym relation is built between the 
terms in a same equivalent class, and only the equivalent class name is used in the integration 
process performed later. The integration process can thus be simplifed. 
 
4.2.3 The Integration Module 
When the extracted ontologies have been transformed into a consistent structure and format, 
they can then be integrated into the ontology warehouse by merging the paths found previously. 
The purpose of integration is to remove redundant nodes and combine similar nodes on the paths. 
The integration proceeds by incrementally merging each transformed ontology into the result one. 
In each step, only two ontologies are merged into a resulting one. The process of integrating two 
transformed ontologies is shown in Figure 12. 
 
Ontology Warehouse
Users
Sub-Ontology 
Choices
Selecting
Data Marts  
Figure 13: The process of generating ontology marts from an ontology warehouse 
 
4.3. The Third Year of This Project 
 In the third year of this project, we have proposed a framework of query system which helps 
the end user to find the correct answers from the ontology marts. It consists of two sub-systems, 
including query parser and rule inference system. The framework of the proposed system 
architecture is shown in Figure 14. 
user
Query
Parser
OWL Query
Pattern
Inference
Engine
query
response
Rule Inference System
form
Ontology Warehouse
Users
Sub-Ontology 
Choices
Selecting
Data Marts
 
Figure 14: The proposed system framework 
The proposed query system is mainly designed on dealing with user’s query and generating 
answers. It mainly consists of the two sub-systems of query parser and rule inference system. The 
 ? car
owns
subject
property
object
 
Figure 15: The RDF Graph representing the triple in the example 
 
 Inference Engine 
The main function of the inference engine is to infer appropriate answers through the 
ontology marts according to the OWL query pattern. Continuing the above example, assume the 
fact about “Tom owns a car” stored in the integrated knowledge base is expressed in the OWL 
form as follows: 
 
<rdf:RDF> 
      <rdf:Description rdf:about="#Tom"> 
        <owns rdf:resource="#car"/> 
      </rdf:Description> 
    </rdf:RDF> 
  </owl-ql:premise> 
 
The inference engine will derive the answer as Tom in the following process: 
 
Query: (“Who owns the car?”) 
Query Pattern: (owns ?p car)  
Must-Bind Variables List: (?p) 
Answer: Tom 
 
Note that the symbol “?p” represents a variable, and the inference engine must reason 
about the variable and output it to users. Figure 16 represents the match process of the query 
in visualization of the RDF Graph. 
 
results in RDF graphs from the selected ontology mart. The system thus searches the pre-found 
ontology marts, and performs appropriate reasoning if necessary, to find the answers and output 
them to the user. 
  
6. Self-evaluation of the Research 
The semantic-web usage has become more and more popular due to the quick progress of 
the internet technology. In this project, we have proposed several frameworks based on ontology 
and OWL to meet the trend of the semantic web. Especially, we propose the concept of ontology 
warehouse to efficiently and effectively handle the issue. We have carefully designed each 
module in the frameworks. We have also published a lot of journal and conference papers from 
the project in the three years. The publication list is shown as follows. 
 
 Journal Papers 
1. T. P. Hong, C. M. Huang and S. J. Horng, “Mining linguistic mobility patterns for wireless 
networks", Lecture Notes in Computer Science, Vol. 3683, pp. 1352-1357, 2005. (SCI) 
2. T. P. Hong, W. C. Chang and J. H. Lin, "A two-phased ontology selection approach for 
semantic web", Lecture Notes in Computer Science, Vol. 3684, pp. 403-408, 2005. (SCI) 
3. Y. C. Lee, T. P. Hong and W. Y. Lin, "Mining association rules with multiple minimum 
supports using maximum constraints," International Journal of Approximate Reasoning, 
Vol. 40, No. 1, 2005, pp. 44-54. (SCI) 
4. C. L. Wang, G. Horng, Y. S. Chen and T. P. Hong, “An efficient key-update scheme for 
wireless sensor networks", Lecture Notes in Computer Science, Vol. 3991, pp. 1026 -1029, 
2006. (SCI) 
5. C. M. Huang, T. P. Hong and S. J. Horng, “Mobility knowledge discovery in wireless 
networks", WSEAS Transactions on Communications, Vol. 5, No. 8, pp. 1514-1520, 2006. 
(INSPEC) 
6. C. Y. Wang, S. S. Tseng and T. P. Hong, "Flexible online association rule mining based on 
the multidimensional pattern relation," Information Sciences, Vol. 176, No. 12, pp. 
1752-1780, 2006. (SCI) 
7. C. Y. Wang, S. S. Tseng and T. P. Hong, "Improved negative-border online mining 
approaches", Lecture Notes in Computer Science, Vol. 3918, pp. 483-492, 2006. (SCI) 
8. T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, "A GA-based fuzzy mining approach to 
achieve a trade-off between number of rules and suitability of membership functions", Soft 
Computing, Vol. 10, No. 11, pp. 1091-1101, 2006. (SCI) 
9. T. P. Hong, K. Y. Lin and S. L. Wang, “Mining fuzzy sequential patterns from quantitative 
transactions", Soft Computing, Vol. 10, No. 10, pp. 925-932, 2006. (SCI) 
26. C. S. Kuo, T. P. Hong and C. L. Chen, “An improved knowledge-acquisition strategy based 
on genetic programming", Cybernetics and Systems, Vol. 39, No. 7, pp. 672 – 685, 2008. 
(SCI) 
27. C. W. Lin, T. P. Hong, W. H. Lu and B. C. Chien “Incremental mining with prelarge trees", 
Lecture Notes in Artificial Intelligence, Vol. 5027, pp. 169 – 178, 2008. (EI) 
28. T. P. Hong, C. E. Lin, J. H. Lin and S. L. Wang, "Learning cross-level certain and possible 
rules by rough sets," Expert Systems with Applications, Vol. 34, No. 3, pp. 1698 - 1706, 
2008. (SCI) 
29. T. P. Hong, C. H. Chen, Y. C. Lee and Y. L. Wu, "Genetic-fuzzy data mining with 
divide-and-conquer strategy", IEEE Transactions on Evolutionary Computation, Vol. 12, 
No. 2, pp. 252 – 265, 2008. (SCI) 
30. T. P. Hong, C. M. Huang and S. J. Horng, “Linguistic object-oriented web usage mining", 
International Journal of Approximate Reasoning, Vol. 48, No. 1, pp. 47 – 61, 2008. (SCI) 
31. T. P. Hong, C. W. Lin and Y. L. Wu, “Incrementally fast updated frequent pattern trees", 
Expert Systems with Applications, Vol. 34, No. 4, pp. 2424 – 2435, 2008. (SCI) 
32. T. P. Hong, M. J. Chiang and S. L. Wang, "Mining fuzzy weighted browsing patterns from 
time duration and with linguistic thresholds", American Journal of Applied Sciences, Vol. 5, 
No. 12, pp. 1611 – 1621, 2008. (INSPEC) 
33. W. C. Chen, S. S. Tseng and T. P. Hong, "An efficient bit-based feature selection method", 
Expert Systems with Applications, Vol. 34, No. 4, pp. 2858 – 2869, 2008. (SCI) 
34. W. S. Lo, T. P. Hong and R. Jeng, "A framework of e-SCM multi-agent systems in fashion 
industry," International Journal of Production Economics, Vol. 114, No. 2, pp. 594 – 614, 
2008. (SCI) 
35. Y. C. Lee, T. P. Hong and T. C. Wang, "Multi-level fuzzy mining with multiple minimum 
supports", Expert Systems with Applications, Vol. 34, No. 1, pp. 459 - 468, 2008. (SCI) 
36. Y. H. Tao, T. P. Hong and Y. M. Su, "Web usage mining with intentional browsing data," 
Expert Systems with Applications, Vol. 34, No. 3, pp. 1893 – 1904, 2008. (SCI) 
 
 Conference Papers 
1. W. Y. Lin, T. P. Hong and S. M. Liu, "Evolution of appropriate migration rate and migration 
interval in multi-population genetic algorithms", The Seventeenth IMACS World Congress 
on Scientific Computation, Applied Mathematics and Simulation, 2005. 
2. C. H. Chen, T. P. Hong and V. S. M. Tseng, “Analyzing time-series data by fuzzy 
data-mining technique", The 2005 IEEE International Conference on Granular 
Computing, Vol. 1, pp. 112-117, 2005. 
3. T. P. Hong, C. M. Huang and S. J. Horng, “Mining linguistic mobility patterns for wireless 
networks", The Ninth International Conference on Knowledge-Based Intelligent 
Information and Engineering Systems, 2005. 
4. T. P. Hong, Y. C. Lee and M. T. Wu, "Using master-slave parallel architecture for GA-fuzzy 
data mining,” The 2005 IEEE International Conference on Systems, Man, and 
Seventeenth International Conference on Information Management, pp. 1283-1290, 2006. 
21. C. M. Huang, T. P. Hong and S. J. Horng, “Linguistic object-oriented web mining", The 
2006 North American Fuzzy Information Processing Society International Conference 
(NAFIPS), 2006. 
22. P. Y. Huang, T. P. Hong and C. Y. Kao, "A comparative analysis for PT-based hybrid group 
flexible flow-shop scheduling approaches", The 36th International Conference on 
Computers and Industrial Engineering, pp. 544-555, 2006. 
23. C. M. Huang, T. P. Hong and S. J. Horng, “Object-oriented web mining", The 36th 
International Conference on Computers and Industrial Engineering, pp. 5294-5303, 
2006. 
24. Y. C. Lee, T. P. Hong and T. C. Wang, "Mining multiple-level association rules under the 
maximum constraint of multiple minimum supports", The Nineteenth International 
Conference on Industrial & Engineering Applications & Other Applications of Applied 
Intelligent Systems, 2006. 
25. W. S. Lo, T. P. Hong, R. Jeng and J. P. Liu "Using data mining technology in supply chain 
management", The Fourth International Conference on Supply Chain Management and 
Information Systems (SCMIS), pp.916-920, 2006. 
26. C. Y. Chen, W. S. Lo, T. P. Hong, C. Y. Chen and H. J. Hsu, "Case study of global supply 
chain management in small and medium sized enterprise", The Fourth International 
Conference on Supply Chain Management and Information Systems (SCMIS), 
pp.967-973, 2006. 
27. P. Y. Huang, T. P. Hong and C. Y. Kao, "Multiple-pass-based heuristic algorithms for group 
flexible flow-shop scheduling problems", The 21st European Conference on Operational 
Research, 2006. 
28. W. S. Lo, S. Z. Lin, T. P. Hong and C. L. Chiang, "Using RFID technology in supply chain 
management", The International Congress on Logistics and SCM Systems, 2006. 
29. C. M. Huang, T. P. Hong and S. J. Horng, “Applying data mining to wireless networks", 
The Tenth WSEAS International Conference on Communications, 2006. 
30. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A less domain-dependent fuzzy mining 
algorithm for frequent trends", The 2006 IEEE International Conference on Fuzzy 
Systems, pp.4242-4247, 2006. 
31. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A cluster-based fuzzy-genetic mining 
approach for association rules and membership functions", The 2006 IEEE International 
Conference on Fuzzy Systems, pp.6971-6976, 2006. 
32. T. P. Hong, J. W. Lin and Y. L. Wu, “Maintenance of fast updated frequent pattern trees for 
record modification", The 2006 International Conference on  Innovative Computing, 
Information and Control, Vol. 1, pp.570-573, 2006. 
33. T. P. Hong, J. C. Wang and J. H. Lin, “Implementing an intelligent system of electronic 
medical records by multiple-agent technique", The Seventeenth Workshop on 
Object-Oriented Technology and Applications, 2006. 
34. C. M. Huang, T. P. Hong and S. J. Horng, “Discovering mobile users’ moving behaviors in 
wireless networks", The  Joint Conference of the Third International Conference on Soft 
Computing and Intelligent Systems and the Seventh International Symposium on 
Applications, 2006. 
49. 洪宗貝, 謝文家和謝朝和, "以具備突變運算子的粒子群優演算法規劃報告型細胞位置", 
The 2006 Multimedia and Networking Systems Conference, 2006. 
50. V. S. Tseng, C. H. Chen, C. H. Chen and T. P. Hong, “Segmentation of time series by the 
clustering and genetic algorithms", The Workshop on Foundations of Data Mining and 
Novel Techniques in High Dimensional Structural and Unstructured Data in The Sixth 
IEEE International Conference on Data Mining, 2006. 
51. S. L. Wang, R. Maskey, A. Jafari and T. P. Hong, “Efficient sanitization of informative 
association rules with updates,” The Second International Conference on Information and 
Automation, 2006. 
52. W. S. Lo, P. S. Huang, T. P. Hong, R. Jeng, H. J. Hsu and C. Y. Chen, "Knowledge 
Management in SCM with institutional perspective", The International Workshop on 
Institutional View of SCM, 2006. 
53. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A comparison of different fitness functions for 
extracting membership functions used in fuzzy data mining", The IEEE Symposium on 
Foundations of Computational Intelligence, 2007, Hawaii. 
54. C. W. Lin, T. P. Hong and W. H. Lu, “Using the Pre-FUFP algorithm for handling new 
transactions in incremental mining", The IEEE Symposium on Computational Intelligence 
and Data Mining, 2007, Hawaii. 
55. T. P. Hong and G. N. Shiu, “Allocating multiple base stations under general power 
consumption by the particle swarm optimization", The IEEE Swarm Intelligence 
Symposium, 2007, Hawaii. 
56. T. P. Hong and T. J. Huang, "Maintenance of generalized association rules for record 
deletion based on the pre-large concept", The 6th WSEAS International Conference on 
Artificial Intelligence, Knowledge Engineering and Data Bases (AIKED '07), 2007, 
Greece. 
57. P. Y. Huang, T. P. Hong and C. Y. Kao, "Multiple-passes scheduling for 
sequence-independent group flexible flow shops", The 17th International Conference on 
Flexible Automation and Intelligent Manufacturing (FAIM), 2007, USA. 
58. C. W. Lin, T. P. Hong, W. H. Lu and C. H. Wu, “Maintenance of fast updated frequent trees 
for record deletion based on prelarge concepts", The Twentieth International Conference 
on Industrial & Engineering Applications & Other Applications of Applied Intelligent 
Systems (IEA/AIE), 2007, Japan. 
59. C. R. Yeh, Y. H. Tao, T. P. Hong, W. Y. Lin, P. C. Chen, C. H. Wu, and C. W. Lin, 
“Adapting monopoly as an intelligent learning game for teaching dynamic competitive 
strategy", The 2007 International Conference on Electronic Commerce, Administration, 
Society, and Education (e-CASE), 2007, Hong Kong. 
60. C. H. Chen, T. P. Hong, V. S. M. Tseng and C. S. Lee, "A genetic-fuzzy mining approach 
for items with multiple minimum supports", The 2007 IEEE International Conference on 
Fuzzy Systems, pp. 1733-1738, 2007, UK. 
61. P. Y. Huang, T. P. Hong and C. Y. Kao, "Four hybrid heuristic algorithms for the group 
flexible flow-shop problem with sequence-independent setup time", The 24th International 
Manufacturing Conference (IMC), 2007, Ireland. 
Its Applications, pp. 9 - 15, 2007, Taiwan. 
77. K. J. Yang, Y. M. Chen and T. P. Hong, “Ontology-based inter-organizational knowledge 
sharing in collaborative business process environment", The Joint Conference of The 
Eighth Asia Pacific Industrial Engineering & Management System and The 2007 Chinese 
Institute of Industrial Engineers Conference & The Tenth Asia Pacific Regional Meeting 
of International Foundation for Production Research, 2007, Taiwan. 
78. W. S. Lo, T. P. Hong, R. Jeng, C. Y. Chen and H. J. Hsu, "Using ontology for supply chain 
management", The Joint Conference of The Eighth Asia Pacific Industrial Engineering & 
Management System and The 2007 Chinese Institute of Industrial Engineers Conference 
& The Tenth Asia Pacific Regional Meeting of International Foundation for Production 
Research, 2007, Taiwan. 
79. S. L. Wang and T. P. Hong, "One-scan sanitization of collaborative recommendation 
association rules", The 2007 National Computer Symposium, pp. 170 - 176, 2007, Taiwan. 
80. C. W. Lin, T. P. Hong and W. H. Lu, “Maintenance of the prelarge trees for record deletion", 
The Twelfth WSEAS International Conference on Applied Mathematics, pp. 105 - 110, 
2007, Egypt. 
81. T. P. Hong, "Some techniques and applications in data mining", 2008知識經濟與管理學術
研討會, 2008, Taiwan. 
82. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A brief survey of genetic-fuzzy data mining 
techniques", The 2008 International Conference on Advanced Information Technologies, 
2008, Taiwan. 
83. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A cluster-based genetic-fuzzy mining 
approach for items with multiple minimum supports", The Twelfth Pacific-Asia Conference 
on Knowledge Discovery and Data Mining, pp. 864 - 869, 2008, Japan. 
84. S. L. Wang, P. A. Stripe and T. P. Hong, "Modeling optimal security investment of 
information centers," The Workshop on Data Mining for Decision Making and Risk 
Management in The Twelfth Pacific-Asia Conference on Knowledge Discovery and Data 
Mining, pp. 293 - 304, 2008, Japan. 
85. S. L. Wang, J. D. Chen and T. P. Hong, “資訊中心機率型安全塑模與分析＂, The 
International Conference on Information Management (ICIM), 2008, Taiwan. 
86. C. H. Chen, T. P. Hong and V. S. M. Tseng, "A divide-and-conquer genetic-fuzzy mining 
approach for items with multiple minimum supports", The IEEE International Conference 
on Fuzzy Systems, pp. 1231 - 1235, 2008, Hong Kong. 
87. V. S. M. Tseng, C. H. Chen, P. C. Huang and T. P. Hong, "A cluster-based genetic approach 
for segmentation of time series and pattern discovery", The IEEE Congress on 
Evolutionary Computation, pp. 1949 - 1953, 2008, Hong Kong. 
88. J. H. Lee, C. C. Tuan and T. P. Hong, "Maximum channel reuse with Hopfield neural 
network-based static cellular radio channel allocation systems", The IEEE International 
Joint Conference on Neural Networks, pp. 3659 - 3666, 2008, Hong Kong. 
89. C. W. Lin, T. P. Hong, W. H. Lu and B. C. Chien, “Incremental mining with prelarge trees", 
The Thirteenth International Conference on Industrial & Engineering Applications & 
Other Applications of Applied Intelligent Systems (IEA/AIE), pp. 169 - 178, 2008, Poland. 
90. C. L. Wang, C. H. Chen, T. P. Hong and G. Horng, “A full connection and less memory 
IEEE International Conference on Systems, Man, and Cybernetics, 2008, Singapore. 
107. S. L. Wang, T. Z. Lai, T. P. Hong and Y. L. Wu, “Efficient hiding of collaborative 
recommendation association rules with updates", The Seventh International Conference on 
Machine Learning and Applications (ICMLA), 2008, USA. 
108. C. L. Wang, T. P. Hong, G. Horng and W. H. Wang, “A GA-based key-management scheme 
in hierarchical wireless sensor networks", The 2008 International Symposium on 
Intelligent Informatics (ISII), 2008, Japan. 
109. C. W. Lin, T. P. Hong, W. H. Lu and H. Y Chen, “An FUSP-tree maintenance algorithm for 
record modification", The Workshop on Foundations of Data Mining in The Eighth IEEE 
International Conference on Data Mining, 2008, Italy. 
 
[20] D. McGuinness, R. Fikes, J. Rice and S. Wilder, “An environment for merging and testing 
large ontologies,” The Seventh International Conference on Principles of Knowledge 
Representation and Reasoning, 2000, pp. 483-493. 
[21] G. Miller “Nouns in WordNet: a Lexical Inheritance System,” Five Thesiss on WordNet 
Princeton University, 1993. 
[22] G. Miller, R. Beckwith, C. Fellbaum, D. Gross and K.Miller, “Introduction to WordNet: an 
Online Lexical Database,” Five Thesiss on WordNet Princeton University, 1993. 
[23] I. Mumick, D. Quass and B. Mumick. "Maintenance of data cubes and summary tables in a 
warehouse," The ACM SIGMOD Conference, Tucson, Arizona, 1997, pp. 100-111. 
[24] N. F. Noy and D. L. McGuinness, Ontology Development 101: A Guide to Creating Your 
First Ontology, Standford Knowldege Systems Laboratory Technical Report, 2001. 
[25] N. F. Noy and M. A. Musen, “PROMPT: algorithm and tool for automated ontology 
merging and alignment,” The Seventeenth National Conference on Artificial Intelligence, 
Austin, Texas, 2000, pp.450-455.  
[26] R. Neches, R. E. Fikes, T. Finin, T. R. Gruber, T. Senator and W. R. Swartout, “Enabling 
technology for knowledge sharing,” AI Magazine, Vol. 12, 1991, pp. 36-56. 
[27] P. Ponniah, Data Warehousing Fundamentals : A Comprehensive Guide for IT Professionals, 
Wely-QED, 2001. 
[28] S. Pinto, A. Gómez-Pérez, and J. Martins, “Some issues on ontology integration,” The 
Workshop on Ontologies and Problem-Solving Methods, In International Joint Conference 
on Artificial Intelligence, Stockholm, Sweden, 1999, pp. 7.1-7.12. 
[29] D. Quass. "Maintenance expressions for views with aggregation," The Workshop on 
Materialized Views, In Association for Computing Machinery, Montreal, Canada, 1996, pp. 
110-118.  
[30] D. Quass, A. Gupta, I. S. Mumick, and J. Widom "Making views self-maintainable for data 
warehousing," The Conference on Parallel and Distributed Information Systems, Miami 
Beach, USA, 1996, pp. 158-169. 
[31] G. Stumme and A. Maedche, “FCA-MERGE: bottom-up merging of ontologies,” The 
Seventeenth International Conference on Artificial Intelligence, Seattle, USA, 2001, pp. 
225-234. 
[32] M. Uschold and M. Gruninger, “Ontologies: principles, methods and applications,” The 
Knowledge Engineering Review, Vol. 2, 1996, pp. 93-155. 
[33] M. Uschold and M. King, “Towards a methodology for building ontologies,” The Workshop 
on Basic Ontological Issues in Knowledge Sharing, In International Joint Conference on 
Artificial Intelligence, Montreal, Canada, 1995. 
[34] DARPA Markup Language (DAML+OIL), http://www.daml.org/. 
[35] Resource Description Framework (RDF), http://www.w3.org/RDF/. 
[36] Semantic Web technology, http://www.w3.org/2001/sw. 
[37] World Wide Web Consortium (W3C), http://www.w3.org/. 
神經網路式學習及資料群聚等文章。 
本體論：此部份探討本體論的技術及
其應用，主要是針對本體論的建構及應用
於查詢時所遭遇的問題做進一步的探討和
研究，有些研究並將其和資料挖掘結合。 
智慧型影像處理：此部份主要探討影
像處理及圖形識別等技術的應用，包含人
臉辨識、邊界偵測、監控偵測及遠端感測
等，均是影像處理領域熱門的問題。 
 
 
2. 電腦智能 
此部份較強調實際電腦智能的應用方
面，包含了人工智慧於工程及商業的應用
如智慧型控制、自然語言、多重智慧型代
理人、智慧型系統、智慧型錯誤檢測，及
智慧型推論系統等，均為相當重要且常見
的應用。另外針對困難的最佳化問題亦有
多篇論文發表。部份筆者較感興趣的內容
介紹如下: 
智慧型控制：智慧型控制即人工智慧
與控制系統的結合，為目前相當重要的一
個關鍵研究領域。此次發表的文章，包括
了模糊控制、神經網路控制、混合式控制、
強健控制、及控制應用等。 
最佳化問題：這個部分主要是對粒子
群優演算法做進一步的探討和研究，包括
提高演算法的效率或將其應用於各種問題
如資料挖掘、自動控制及工作排程上。 
 
3.其它相關技術 
此部份探討和智慧型工程系統間接相
關之技術，包括了智慧型網路運算及Web
的技術、隱私權保護、資訊安全等議題。
部份筆者較感興趣的內容介紹如下: 
智慧型網路運算及Web：此部份主要
是探討如何有效地促進網際網路之效能及
增廣其應用，並能有效地搜尋及收集網際
網路的資料，這對於網際網路發達的今日
資訊社會相當的重要。此部份的文章包括
了使用一些機器學習的方法去改善並增進
網路效能，垃圾郵件過濾，文件分類，網
路服務品質的探討，網路的流量問題和安
全性相關方面的議題做研究及相關實際系
統應用等。  
資訊安全：主要針對於如何去隱藏某
些資訊以達到資訊安全做進一步的探討，
比如金鑰管理和加解密相關方面的議題，
像是影像方面的加密演算和簽章方面的結
構等等。 
 
(三) 建議 
    1.此次承蒙國科會工程處計劃補助，
方得成行，在此先致上十二萬分的謝意，
也希望爾後國科會及教育部對於參加國際
知名會議之學者均能大力支持。 
    2.國內有不少研究此方面的學者，而
每年也都舉辦相關的國際或國內研討會，
藉此國內外學者能互相切磋交流，甚至彼
此合作，這是相當可喜的現象，也希望國
內的研討會能不斷舉辦下去，並朝世界知
名國際會議目標努力。 
     
(四) 攜回資料名稱及內容 
    1.會議論文集ㄧ冊。 
    2.會議內容介紹一冊。 
    3.其他相關會議之 call for paper資料。 
Proceedings of  the Sixth International Conference on Machine Learning and Cybernetics, Hong Kong, 19-22 August 2007 
2.1. Reducts 
Let I = (U, A) be an information system, where U = {x1, 
x2, …, xN} is a finite non-empty set of objects and A is a 
finite non-empty set of attributes called condition attributes 
[4]. A decision system is an information system of the form 
I = (U, A∪{d}), where d is a special attribute called 
decision attribute and d ∉ A [4]. For any object xi ∈ U, its 
value for a condition attribute a is denoted by fa(xi). The 
indiscernibility relation for a subset of attributes B is 
defined as: 
})()(,),{()( yfxfBaUUyxBIND aa =∈∀×∈=    (1) 
where B ⊆ A is any subset of the condition attribute set A 
[4]. If the indiscernibility relations from both A and B are 
the same (i.e. IND(B) = IND(A)), then B is called a reduct 
of A. That means the attributes used in the information 
system can be reduced to B, with the original 
indiscernibility information still kept. Furthermore, if an 
attribute subset B satisfies the following condition, then B is 
called a minimal reduct of A: 
    (2) ).()'('  and  )()( AINDBINDBBAINDBIND ≠⊆∀=
When a decision system, instead of an information 
system is considered, the definition of a reduct B can be 
modified as follows [6]: 
)()( )()(  ,,  , jijBiBji xdxdxfxfUxxAB =→=∈∀⊆∃ .  (3) 
where d(xi) denotes the value of the decision attribute of the 
object xi, fB(xi) denotes the attribute values of xi for the 
attribute set B. Similarly. If no subset of B can satisfy the 
above condition, B is called a minimal reduct in the 
decision system.  
Since finding minimal reducts has been proven as an 
NP-Hard problem, Li et al proposed the concept of 
“approximate” reducts [4]. The approximate reducts allow 
for some reasonable error degrees, but can greatly reduce 
the computation complexity. In this paper, we partition the 
attributes into k clusters according to the dependency 
between each pair of attributes. Each cluster can thus be 
represented by its representative attribute. All the other 
attributes within the same cluster can thus be removed. The 
whole feature spaces can thus be greatly reduced. Next, the 
relative dependency used to measure the dissimilarity of 
attributes is introduced. 
2.2. Relative dependency 
Han et al. developed an approach based on the relative 
dependency for finding approximate reduct [3]. The relative 
dependency is motivated by the operation “projection”, 
which is very important in the relational algebra. Given an 
attribute subset B ⊆ A and a decision attribute d, the 
projection of U on B is denoted by ΠB(U) and can be 
computed by the following two steps: (1) removing 
attributes in A－B, (2) merging all the remaining rows 
which are indiscernible [4]. Thus only one of the tuples 
with the same attribute values for B is kept and the others 
are removed.  
Han and Li thus defined the relative dependency 
degree of the attribute subset B with regard to the set of 
decision attributes D as follows: 
( )
( )U
U
DB
BD
B
∪Π
Π
=δ ,              (4) 
where  is the relative dependency degree of B on D,     
|Π
D
Bδ
B(U)| and |ΠB∪D(U)| are the numbers of tuples after the 
projection operations are performed based on B and on B∪
D, respectively. 
3. Calculation of attribute dissimilarity 
For most clustering approaches, the distance between 
any two objects is usually adopted as a measure for 
representing their dissimilarity, which is then used for 
deciding whether the objects belongs to the same cluster or 
not. In this paper, the attributes, instead of the objects, are 
to be clustered. The conventional distance measures such as 
Euclidean distance or Manhattan distance are thus not 
suitable since the attributes may have different formats of 
data, which are hard to compare. For example, assume there 
are two attributes, one of which is age and the other is 
gender. It is thus hard to compare the two attributes via the 
traditional distance measure. Below, a measure based on the 
concept of data dependency is proposed to achieve it. 
Given two attributes Ai and Aj, the relative dependency 
degree of Ai with regard to Aj is denoted by Dep(Ai, Aj) and 
is defined as: 
( )
( )U
U
AADep
ji
i
AA
A
ji
,
),(
Π
Π
=
,               (5) 
where ( )U
iA
Π  is the projection of U on attribute Ai. 
Note that the original relative dependency degree only 
considers the relative dependency between a condition 
attribute set and a decision attribute set. Here we extend it 
to estimate the relative dependency between any pair of 
attributes. Besides, it can be easily found that the extended 
relative dependency can be thought of as the similarity of 
2287 
Proceedings of  the Sixth International Conference on Machine Learning and Cybernetics, Hong Kong, 19-22 August 2007 
distance between the two attributes is thus computed as: 
67.1
)6.0,6.0(
1)( ==
Avg
PR, DMd  
Step 3: All non-center attributes are allocated to their 
nearest centers. Thus, cluster C1 contains {PR, CA, AL, DM} 
and cluster C2 contains {C++, JAVA, DB, DS}. 
Step 4: Calculate the distances between any two 
different attributes in the same clusters. The distances are 
shown in Table 2. 
Table 2. Distances between any two attributes within the 
same clusters 
1.25d(DB, DS)1.33d(AL, DM)
1.25d(JAVA, DS)1.67d(CA, DM)
1.67d(C++, JAVA)1.67d(PR, CA)
2d(JAVA, DB)1.67d(CA, AL)
1.25d(C++, DB)1.33d(PR, AL)
1d(C++, DS)1.67d(PR, DM)
DistanceAttribute pairDistanceAttribute pair
Within cluster C2Within cluster C1
 
Step 5: The radius of each cluster is calculated. Take 
cluster C1 as an example. It includes 4 attributes {PR, CA, 
AL, DM}. The distances between each pair of attributes in 
C1 are {1.67, 1.67, 1.33, 1.67, 1.67, 1.33}. The radius r1 is 
then calculated as: 
56.1
6
1.33 1.67 1.67 1.33 1.67 1.67
1 =
+++++
=r  
Step 6: The Near set of each attribute within a cluster 
is calculated. Take the attribute PR in cluster C1 as an 
example. Its distance from the other three attributes CA, AL 
and DM in the same cluster are calculated as 1.67, 1.33 and 
1.67. Near(PR) is thus {AL} since only AL is within the 
radius r1 (1.56), which is found from Step 5. Similarly, the 
Near sets of the other three attributes in cluster C1 are found 
as follows: 
Near(CA) =φ , 
Near(AL) = {PR, DM}, and 
Near(DM) = {AL}. 
Step 7: Since the attribute AL has the most number of 
attributes in its Near set among those in C1, AL then 
replaces the attribute DM as the new center of C1. Similarly, 
the original center DS has the most attributes in its Near set 
for C2, it is thus still the center of C2. 
Step 8: Steps 2 to 7 are repeated until the two clusters 
no longer change. The final clusters can thus be found as 
follows: 
C1 = {PR, CA, AL, DM}, with the center AL. 
C2 = {C++, JAVA, DB, DS}, with the center DS. 
6. Conclusions and future works 
In this paper, a measure of the dissimilarity between 
two attributes is proposed based on the relative dependency. 
The proposed attribute clustering algorithm consists of two 
major phases: reassigning the attributes to the clusters and 
updating the centers of the clusters. After the attributes are 
organized in several clusters by their similarity degree, we 
can select an attribute from each cluster to represent all 
attributes within the same cluster. 
 In the proposed algorithm, the number of cluster 
should be known in advance. This requirement results in the 
limitation of its applications. In the future, we will try to 
develop other new approaches to attribute clustering while 
the number of clusters is unknown. 
References 
[1] Q. A. Al-Radaideh, M. N. Sulaiman, M. H. Selamat and H. Ibrahim 
“Approximate reduct computation by rough sets based attribute 
weighting,” IEEE International Conference on Granular Computing, 
Vol. 2, pp. 383-386, 2005. 
[2] K. Gao, M. Liu, K. Chen, N. Zhou and J. Chen, “Sampling-based 
tasks scheduling in dynamic grid environment,” The 5th WSEAS 
International Conference on Simulation, Modeling and Optimization, 
pp.25-30, 2005. 
[3] J. Han, “Feature selection based on rough set and information 
entropy,” IEEE International Conference on Granular Computing, 
Vol. 1, pp. 153-158, 2005. 
[4] J. Komorowski, L. Polkowski, A. Skowron “Rough Sets: A 
Tutorial”, http://www.let.uu.nl/esslli/ Courses/ skowron/skowron.ps. 
[5] Y. Li, S. C. K. Shiu and S. K. Pal, “Combining feature reduction 
and case selection in building CBR classifiers,” IEEE Transactions 
on Knowledge and Data Engineering, Vol. 18(3), pp. 415- 429, 
2006. 
[6] Z. Pawlak, “Rough set,” International Journal of Computer and 
Information Sciences, pp. 341-356, 1982.  
[7] Z. Pawlak, “Why rough sets?,” Proceedings of the Fifth IEEE 
International Conference on Fuzzy Systems, Vol. 2, pp. 738-743, 
1996. 
[8] H. Q. Sun, Z. Xiong, “Finding minimal reducts from incomplete 
information systems,” The Second International Conference on 
Machine Learning and Cybernetics, Vol. 1, pp. 350-354, 2003. 
[9] A. Skowron, C. Rauszer, “The Discernibility Matrices and 
Functions in Information Systems”, Handbook of Application and 
Advances of the Rough Sets Theory, pp. 331-362, Kluwer 
Academic Publishers, Dordrecht, 1992. 
[10] J. Wroblewski, “Finding minimal reducts using genetic algorithms,” 
The Second Annual Join Conference on Information Sciences, pp. 
186-189, 1995. 
 
2289 
挖掘和資料探勘等等，均是目前相當熱門
之主題。部份筆者較感興趣的內容介紹如
下: 
資料探勘：資料探勘主要是從大型資
料庫中整理分析資料，以求出高階總結性
之規則作為決策依據。廣義而言，甚至可
將機器學習領域納入。此次會議發表的文
章均是將傳統資料挖掘技術再做更深入的
變化，以符合不同的需求。 
機器學習：包含遺傳演算法、類神經
網路式學習及資料群聚等文章。主要是用
來做分類法之使用和聲音辨識方面等應
用。 
智慧型影像處理：此部份主要探討影
像處理及圖形識別等技術的應用，包含人
臉辨識、邊界偵測、監控偵測及遠端感測
等，均是影像處理領域熱門的問題。 
 
2. 智慧系統與控制 
此部份較強調實際電腦智能的應用方
面，包含了人工智慧於工程及商業的應用
如模糊理論、模糊系統的模組和分析、知
識系統、混合式的系統模組化和設計、錯
誤偵測和辨識、最佳化問題和決策制定、
控制系統和應用、系統辨識、風險分析和
管理、模組化和模擬技術等，均為相當重
要且常見的應用。另外針對困難的最佳化
問題亦有多篇論文發表。部份筆者較感興
趣的內容介紹如下: 
智慧型控制：智慧型控制即人工智慧
與控制系統的結合，為目前相當重要的一
個關鍵研究領域。此次發表的文章，包括
了模糊控制、神經網路控制、混合式控制、
強健控制、及控制應用等。 
最佳化問題：這個部分主要是對粒子
群優演算法做進一步的探討和研究，包括
提高演算法的效率或將其應用於各種問題
如資料挖掘、自動控制及工作排程上。 
3.其它相關技術 
此部份探討和智慧型工程系統間接相
關之技術，包括了智慧型網路運算及Web
的技術、隱私權保護、資訊安全等議題。
部份筆者較感興趣的內容介紹如下: 
智慧型網路運算及Web：此部份主要
是探討如何有效地促進網際網路之效能及
增廣其應用，並能有效地搜尋及收集網際
網路的資料，這對於網際網路發達的今日
資訊社會相當的重要。此部份的文章包括
了使用一些機器學習的方法去改善並增進
網路效能，文件分類，網路服務品質的探
討，網路的流量問題及相關實際系統應用
等。  
資訊安全：主要針對於如何去隱藏某
些資訊以達到資訊安全做進一步的探討，
比如如何把網路服務做保密化和加解密相
關方面的議題，都有在此會議中探討。 
 
(三) 建議 
    1.此次承蒙國科會工程處計劃補助，
方得成行，在此先致上十二萬分的謝意，
也希望爾後國科會及教育部對於參加國際
知名會議之學者均能大力支持。 
    2.國內有不少研究此方面的學者，而
每年也都舉辦相關的國際或國內研討會，
藉此國內外學者能互相切磋交流，甚至彼
此合作，這是相當可喜的現象，也希望國
內的研討會能不斷舉辦下去，並朝世界知
名國際會議目標努力。 
     
(四) 攜回資料名稱及內容 
    1.會議論文集 CD一張。 
    2.會議內容介紹一冊。 
    3.其他相關會議之 call for paper資料。 
     
Next, we introduce the concept of prelarge itemsets. 
Hong et al. proposed an algorithm based on pre-large 
itemsets to handle the inserted transactions in 
incremental mining, which can further reduce the 
number of rescanning databases [5]. A pre-large 
itemset is not truly large, but may be large with a high 
probability in the future. Two support thresholds, a 
lower support threshold and an upper support 
threshold, are used to realize this concept. Considering 
an original database and some records to be modified 
by the two support thresholds, itemsets may fall into 
one of the following nine cases illustrated in Figure 1. 
Large 
itemsets
Pre-large 
itemsets
Original 
database
New
transactions
Small 
itemsets
Case 1      Case 2      Case 3  
Case 4      Case 5      Case 6
Case 7      Case 8      Case 9 
Positive 
difference
Item 
difference
Zero
difference
Negative
difference
Figure 1: Nine cases arising from and the 
original database and the modified records 
Cases 1, 2, 5, 6, 8 and 9 above will not affect the 
final large items. Case 3 may remove some existing 
association rules, and cases 4 and 7 may add some new 
association rules. If we retain all large and pre-large 
itemsets with their counts after each pass, then cases 3 
and 4 can be handled easily. Also, in the maintenance 
phase, the ratio of modified records to old transactions 
is usually very small. This is more apparent when the 
database is growing larger. It has been formally shown 
that an itemset in case 7 cannot possibly be large for 
the entire updated database as long as the number of 
transactions is smaller than the number f shown below 
[5]: 
¬ ¼dSSf lu )(  
where f is the safety number of modified records, Su is 
the upper threshold, Sl is the lower threshold, and d is 
the number of original transactions. 
3. The proposed pre-FUFP modification 
algorithm for handling modified records 
An FUFP tree is built in advance from the initial 
original database. Its initial construction is similar to 
that of an FP tree. The algorithm is described below. 
INPUT: An old database consisting of d records, its 
corresponding Header_Table storing the 
frequent items initially in descending order, its 
corresponding FUFP tree, a lower support 
threshold Sl, an upper support threshold Su, its 
corresponding pre-large table storing the set 
of pre-large items from the original database, 
and a set of t modified records. 
OUTPUT: A new FUFP tree after record modification 
by using the Pre-FUFP modification algorithm.  
STEP 1: Calculate the safety number f of modified 
records according to the following formula [5]:  
¬ ¼dSSf tu )(  .
STEP 2: Find all the items in the t records before and 
after modification. Denote them as a set of 
modified items, M.
STEP 3: Find the count difference (including zero) of 
each item in M for the modified records. 
STEP 4: Divide the items in M into three parts 
according to whether they are large, pre-large 
or small in the original database. 
STEP 5: For each item I in M which is large in the 
original database (appearing in the 
Header_Table), do the following substeps (for 
cases 1, 2 and 3):
Substep 5-1: Set the new count SU(I) of I in 
the entire updated database as:  
SU(I) = SD(I) +SM(I),
where SD(I) is the count of I in the 
Header_Table (from the original database) 
and SM(I) is the count difference of I from 
record modification. 
Substep 5-2: If SU(I)/dt Su, update the count 
of I in the Header_Table as SU(I), and put I in 
the set of Increased_Items and 
Decreased_Items, which will be further 
processed to update the FUFP tree in STEP 
10;  
Otherwise, if Su>SU(I)/dt Sl,, remove I from 
the Header_Table, connect each parent node 
of I directly to its corresponding child node in 
the FUFP tree, set SD(I) = SU(I), and keep I
with SD(I) in the pre-large table; 
Otherwise, item I is small after the database is 
updated; remove I from the Header_Table and 
connect each parent node of I directly to its 
corresponding child node in the FUFP tree. 
STEP 6: For each item I in M which is pre-large in the 
original database, do the following substeps 
(for cases 4, 5 and 6):
Substep 6-1: Set the new count SU(I) of I in 
the entire updated database as:  
SU(I) = SD(I)+SM(I). 
