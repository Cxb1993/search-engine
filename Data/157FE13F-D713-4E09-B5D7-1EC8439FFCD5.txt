for PV – insertion type, deletion type, and 
substitution type. Furthermore, a pronunciation 
variation network is constructed from the 
pronunciation lexicon and then the tri-phones (i.e., 
a sub-path) of this PV network are compared to the 
CMU pronunciation lexicon to extract the potential PV 
rules. Finally, the extracted PV rules and those 
rules defined by experts are combined into the 
pronunciation lexicon for the basic bi-lingual ASR. 
In the second year, the project presents a 
transformation-based approach to robust modeling of 
accented speech based on articulatory attributes for 
non-native speech recognition.  Firstly, a two-stage 
verification method is used to extract speech 
segments from the speech input with non-native 
accent.  Secondly, acoustic models of accented speech 
are transformed from normal models using linear 
transformation functions selected from a decision 
tree to deal with the problem of data sparseness.  
Thirdly, a discrimination function is applied to 
filter out the models with low recognition 
discriminability. 
In the third year, the project proposes a data-
driven-based method to construct a Chinese-English 
phone set for ASR. Articulatory features are 
integrated into similarity measure to estimate the 
similarities between different models. Similar phones 
are merged into the same phone unit. Furthermore, a 
machine translation-based language model is used to 
improve the ability in recognizing Chinese-English 
code-switched utterances. 
 
英文關鍵詞： bi-lingual automatic speech recognizer, pronunciation 
variation, accented speech, transformation function, 
articulatory feature, code-switching 
 
  I 
中文摘要 
本計畫於第一年建立一套具抗發音變異的中英混合辨識雛型系統。第一階段先對中英雙
語語料庫以國際音標(international phonetic alphabet, IPA)進行標記以建立音素集(phone set)，並
且利用訓練語料對所有的音素進行聲學模型的訓練，以建立一個中英雙語辨識雛型系統中各
個音素的聲學模型(acoustic model, AM)。而第二階段主要在於擷取發音變異規則，所謂的發
音變異亦即插入(insertion)、刪除(deletion)與取代(substitution)這三種情形。本計畫將中英混合
辨識雛型系統的發音辭典 (pronunciation lexicon)，經由建構發音變異網路 (pronunciation 
variation network, PVN)後，將發音變異網絡中的三連音(triphone)與 CMU 發音辭典中曾出現
的三連音做一個比較，找出機率較大的發音變異規則，將找到的發音變異規則與專家定義的
規則做一個結合並整合進最後的中英混合辨識器中的發音辭典。 
本計劃於第二年基於聚類狀態模型(Senone Model, SEM)，同時考慮聲學特性與發音特徵
(articulatory feature, 簡稱 AF)進行兩階段腔調語音驗證，利用轉換函式作為轉換並產生腔調語
音模型，以及考慮語言特性參數做腔調語音模型之預測，希望改善腔調語音對辨識正確率造
成的影響，並以語言特性參數對腔調語音做聲學特性上的分類，以彌補腔調語音訓練語料不
足的問題。藉由產生腔調語音聲學模型。 
本計畫於第三年以資料驅動的方式建立中英雙語的聲學模型集，在基於隱藏式馬可夫模
型語音辨識器的情況下，引入發音特徵來改善計算聲學模型間之距離時，資料稀疏所造成的
問題。我們根據發音特徵與聲學特性估算中文與英文的聲學模型的相似度，進而合併相似之
聲學模型以建立一中英雙語語音辨識器。在語言模型方面，我們使用以翻譯為基礎之語言模
型，使系統可辨識較為廣泛之語言轉換(code-switching)之語句。 
 
 
關鍵字：發音變異規則、中英雙語辨識系統、腔調語音、轉換函式、發音特徵、語言轉
換 
Abstract 
The project develops a Chinese-English basic bi-lingual automatic speech recognizer (ASR) 
with pronunciation variation (PV) in the first year. The first step is to annotate the Chinese-English 
bi-lingual corpus by the International Phonetic Alphabet (IPA) for phone set collection. Then, each 
phone is modeled by an acoustic model through training data in the basic ASR. The next step is the 
rule extraction for PV – insertion type, deletion type, and substitution type. Furthermore, a 
pronunciation variation network is constructed from the pronunciation lexicon and then the 
tri-phones (i.e., a sub-path) of this PV network are compared to the CMU pronunciation lexicon to 
  1 
一﹑前言 
 
隨著科技的進步，電腦不僅已經融入了人類的生活，同時更朝向人工智慧與自動化的方向發展，因此人機
互動是一個相當重要的課題。由於語音是人類溝通最自然方便的方式，近年來電腦網路多媒體普及，語音在人
機互動的介面更是扮演重要角色。自動語音辨識是語音應用技術的重要一環，目前有許多有關語音辨識之應用，
如：自動聽寫機、語音文件摘要、語音文件檢索等。並且全球化趨勢之盛行，多語語音常出現於會議紀錄及一
般對話等方面。對於會議紀錄及對話系統而言，多語語音自動辨識日顯重要。 
在台灣日漸國際化的今日，日常生活的對話之中，往往因為詞意表達的便利性，使得以中文為主的對話裡
不時地會夾帶些許英文字詞或語句，例如：商店名 (e.g., seven-eleven) 、人名 (e.g., A-rod) 或驚呼語 (e.g., Oh~ 
My God) 等等 ，針對這類的句子，在使用單一語言辨識器的人機介面中，要達到親切與便利的互動有其困難
度存在。然而，在中英夾雜的語句中，大部分的語者依然會使用中文之發音方式來發英語的音，使得發出的英
語單字或片語，和以英語為母語的語者所發出的音作比較，存在著差異，中文語者所發出的英語會帶有變異與
腔調。非母語語者所造成的腔調語音嚴重影響多語自動語音辨識的正確性。因此，發展一套具抗發音變異之中
英文雙語自發性語音辨識技術是一個迫切的研究主題。本計畫的一項重點即在於具腔調與變異之中英雙語自動
語音辨識之研究，從而改善因腔調語音與發音變異對於語音辨識器的影響。 
此外，如何建立一套富強健性雙語(或多語)語音辨識之聲學模型集亦為一重要研究課題，因要收集對於特
定國家語者口說之各種語言語料以建立聲學模型並不容易，特別是語言轉換的語料，經常會遭遇資料稀疏(data 
sparseness)的問題，使得訓練出之聲學模型不夠可靠。因此本計畫的另一項重點為具強健性中英雙語模型集之建
立。 
二﹑研究目的及文獻探討 
由於語音辨識技術發展日益純熟，語音辨識已可運用於許多實際應用上，例如美國麻省理工學院(MIT)所開
發的語音導覽系統 – CityBrowserII [1] 和美國電信電報公司 (AT&T) 的線上服務系統 – May I Help you [2]‧
此類的人機互動相關應用的是近年來日益重要的研究課題，尤其是在日趨國際化的台灣，多數語者不再僅使用
單一語言的情況下，發音變異和不流暢語留的問題導致自動語音辨識器的效能。本計劃針對此兩大課題來實現
具抗發音變異、不流暢語流之中英雙語自發性語音辨識系統。在第一年中，我們先探討有關於發音變異的方法
和建立具發音變異的發音辭典。在發音變異的相關文獻中，大致可分為幾類：發音辭典、聲學模型擴展(AM 
expansion)和以多路徑(multi-path)為基礎的方法。在發音辭典的相關方法中，發音變異的模型化(modeling)可再細
分為不明確 (implicit) 和 明確(explicit) 兩種類別 [3]。在明確類別中，發音變異將多重發音形式符號化並納入
最後的發音辭典中，例如使用多重路徑方式 [4][5]。相反地，不明確類別中採用了分類程序來實現，例如語音
規則(phonological rules) [6][14]、決策樹(decision tree)[6]和混淆矩陣(confusion matrix)[8]。 
近年來，已有不少關於多語語音辨識以及針對發音變異問題的研究，也各有其優缺點，近年來在多語語音
辨識上的研究逐漸成為焦點，有一些藉由解決發音變異現象來改善發音變異造成辨識率降低的研究漸漸被提出
來，而在描述模擬發音變異時需要找出哪些發音片段為發音變異音段，才能準確的模擬發音變異。因此，在找
尋發音變異規則上，大致有三種方式，第一種為知識為基礎的方式(Knowledge-based)，即透過專家定義，找出
發音變異的規則[9][10][11][12]，但其缺點是發音變異的情況有太多種，無法將所有發音變異規則都觀察到。 
第二種方法是利用資料分析的方式(Data-driven)，[13][14][15]使用標準辨識器辨識變異語料，建立混淆矩陣
找出發音變異規則，再進行發音變異的模擬。[16]利用本國(Native)語言的辨識器辨識標準的非本國(Non-native)
  3 
第一年整體計畫執行架構    
如圖所示，本計劃之系統架構中，除了使用兩個單一語言語料庫所訓練而得的音素模型(phone models)外，
本系統亦利用專家知識 (expertise)、音素混淆資訊 (phone confusion information) 和發音規則 (pronunciation rules) 
以建構發音變異規則 (PV rules)，以提升語音辨識器的整體效能。在第一年完成以下之功能：音素集的建立、潛
在發音變異規則的萃取和雙語語音模型的建立。此三部分主要之研究方法與結果詳述如下： 
一. 音素集的建立 
為了使得本系統的語音辨識器具有中英文雙語的辨識能力，本系統以國際音標 (IPA) 標定了兩種以單一語
言為主的語料庫，分別為以中文為主的 TCC-300 語料庫和以台灣口音英語為主的 EAT 語料庫。並從這兩種主
要語言的語料中，選定了中文 150 個次音節 (sub-syllable) 和英文 40 個音素做為辨識單位 (acoustic unit)。在這
兩種以單一語言為主的語料庫中，可將語料的譯文 (transcription) 分成三類詞句。如圖二所示，第一類為整個
語句的內容皆為英文，第二類則為整個語句中夾雜了中文與英文，第三類則為整個語音資料就是一個英文字詞。
在標定過程中，若遇到中英文皆存在的發音，例如中文的”歐”和英文的”Oh”，則在音素集裡的表是分別以”_M”
和”_E”的結尾區別中英文。然而，為了辨識的準確度，我們則採用三連音 (triphone) 作為發音辭典的辨識單位。
我們希望能對中文與英文各個音素搜集到大量的語料，如此便能將各音素的聲學模型訓練到比較可靠穩定的狀
態。同時也因為我們使用來訓練聲學模型的語料就是由國人說的國語與英語所建構而成。因此使用來訓練的語
音資料即會隱含了本國人說英語的腔調與變異情形，此類資訊也會被訓練至聲學模型當中，使得對含有腔調或
發音變異的語句具有較佳的辨識能力。 
二. 潛在發音變異規則的萃取 
當非英語為母語的語者(non-native English speaker)說英語時，大部分的語者會以其母語的發音方式來發英語
的音，使得發出的英語與以英語為母語的語者(native English speaker)所發出之英語，兩者之間存在著差異性，
主要是會帶有變異與腔調在語句當中，如圖 三至圖 五即為台灣人民受中文發音而常誤唸之英語發音。在本計
畫中，經由歸納前述範例中，所謂的發音變異可被定為標準發音(normal pronunciation)發生包括三種形式 - 插入
其他音素(insertion type)、刪除某個音素(deletion type)和取代某個音素(substitution type)，如所示圖六。所以除了
將前述帶有變異與腔調的語句拿來訓練以外，倘若我們另外將發音變異的規則找出來並且將其應用至辨識器的
辨識路徑的話，將可得到更好的辨識效果。此外，除了專家定義的發音變異，我們還希望能夠去找出更多可能
的發音變異情形，在此我們利用建構發音網絡，如圖 七所示，來達到這個目的。 
發音變異網絡的建構，如圖 七以英語詞句「attend」為例為例，其在 CMU 發音詞典所對應之發聲音標依
序為「AH、T、EH、N、D」等國際標準音標，係設定此英語詞句之發聲音標為基準，以偵測此等發聲音標之
每一發聲間距是否存在一插入變異發音，即從輸入發音與「AH」之間、「AH」與「T」之間、「T」與「EH」
之間、「EH」與「N」之間、「N」與「D」之間，「D」與結束發音之間，利用前述建構之發音模型偵測此等
發聲間距是否有插入變異發音存在。接著偵測每一發聲音標至次一發聲音標之間是否存在一刪除變異發音。然
而偵測時，除偵測兩相鄰之發聲音標之間是否存在刪除變異發音外，若發聲音標之後存在一插入變異發音時，
係偵測此發聲音標與接續的插入變異發音之間是否存在刪除變異發音。 
最後偵測每一發聲音標對應之替換變異發音以建構此變異發音網絡。然而為降低變異發音網路之複雜度，
可利用建立音素混淆矩陣 (phone confusion matrix) 並歸納其混淆資訊來達成此目的。音素混淆矩陣如圖 八所
示。音素混淆矩陣係先收集所有國際標準音標之發音，並計算每一國際標準音標因發音錯誤而形成其它國際標
準音標的發音機率，以建立此音素模糊矩陣。再以英語詞句之發聲音標為基準，取出一發音機率範圍之至少一
國際標準音標，並設定所選取的國際標準音標作為發聲音標的替換變異發音。音素模糊矩陣欄位內之英文對應
之發音機率如下，(A)=0%-10%，(B)=10%-15%，(C)=15%-20%，(D)=20%-25%，(E)=25%-30%，(F)=30%-35%，
(G)=35%-40%，(H)=40%-45%，(I)=45%-50%，(J)=50%-55%，(K)=55%-60%，(L)=60%-65%，(M)=65%-70%，
(N)=70%-75%，(O)=75%-80%，(P)=80%-85%，(Q)=85%-90%，(R)=90%-95%，(S)=95%-100%，(T)=100%。 
  5 
一. 腔調語音與發音變異語音片段之偵測 
對於具腔調語音的語料，當中存在標準的發音與帶有腔調的發音，而為了能夠準確的描述模擬腔調語音現
象，因此需要偵測出哪些音段可能有腔調語音的現象存在，並找出腔調語音的規則，而以往研究的做法大部分
是基於辨識器考慮聲學特性進行驗證，但往往由於辨識器不夠準確而造成錯誤偵測，因此，本論文提出透過兩
階段 senone 驗證的方式，對於具腔調語音的語料進行驗證並偵測腔調語音現象。 
提出以 senone 驗證找出具腔調語音現象之語音段的方式，主要分成兩個階段，分別為以聲學特性為基礎
之驗證(Acoustic-Based Verification)與以發音特徵為基礎之驗證(articulatory feature-Based Verification)： 
第一階段將考慮聲學特性基於辨識器進行驗證：如圖十一，對於一個 SEM所對應的音框序列稱之為 senone
語音段(Senone Segment)，一 senone 語音段若藉由考慮聲學特性，經由此階段進行驗證為具腔調語音，則將其稱
為候選之腔調語音段(candidate accented segment)。由於第一階段驗證腔調語音資料是基於辨識器考慮聲學特型
進行驗證，因此稱之為以聲學特型為基礎之驗證階段。 
第二階段，以發音特徵為基礎之驗證中，則考慮發音特徵進行驗證：此階段根據發音特徵參數，針對前一
階段驗證認為的候選之腔調語音段再次進行驗證。以發音特徵為基礎之驗證，經過第二階段驗證認為具有腔調
語音現象者，則將其視為腔調語音段(PV Segment) 。 
提出透過兩階段進行驗證方法的主要原因為，第一階段考慮聲學特性透過辨識器進行驗證，但由於無法保
證辨識器能夠具有百分之百的辨識正確率，因此，經由辨識器驗證時無法確認 senone 語音段是真的為腔調語音
或者是因為辨識器本身不夠準確而造成的錯誤偵測。而造成腔調語音時，其所屬的發音特徵與標準發音時的有
所不同，可以利用標準發音的發音特徵對具腔調語音現象之語料進行驗證。因此，在第二階段再進一步加以考
慮發音特徵(articulatory feature)透過事件偵測器(Event Detector)加以驗證是否為腔調語音段。若經聲學特性驗證
認為具腔調語音現象，但經發音事件驗證不具腔調發音現象，則認為是因辨識器所造成的錯誤偵測，因此經過
兩階段驗證認為具腔調語音現象之語音段，才認定為腔調語音段。 
二. 潛在發音變異規則的萃取 
本計畫於第二年之主要目標在於產生具有腔調語音的 SEM，利用新產生的 SEM進一步產生具腔調的 SPM，
將標準 SPM 與具腔調的 SPM 結合，以期達到降低因腔調語音對多語語音辨識之辨識率造成的影響。針對產生
腔調語音之 SEM，經由以高斯分佈為基礎的階層式轉換函式架構可以找出成對的高斯分佈，接著採用線性的假
設關係，將發生腔調的高斯分佈視為標準高斯分佈的線性組合和轉換，將成對的高斯分佈利用線性轉換的方式
來描述標準與腔調發音高斯混合的關係，我們定義標準高斯分佈（normal, 
1 2
( , , )
N
X x x x ） 
( )Y Xf   
藉由上式的線性轉換函式來轉換成為目標的發音變異高斯分佈（PV, 
1 2
( , , )
N
Y y y y ）。希望找出一個
線性轉換的關係，能夠將來源標準高斯分佈資料，透過這個線性轉換關係後，產生具發音變異的目標高斯分佈。
本論文採用式(1)當作線性轉換的函式，利用正常語音資料 X透過旋轉矩陣Ａ的轉換後，將旋轉的誤差用 R 表示。 
 Y AX R  (1) 
腔調語音之轉換函式的訓練上，採用了隱藏式馬可夫模型，藉由其時間軸上可考慮前後關聯的特性，使描
述出來的聲學模型更具有連慣性。在此為了更仔細的描述轉換的函式，除了引入隱藏式馬可夫模型，並同時考
慮標準高斯分佈與具發音變異的高斯分佈資料間的關聯性，也就是最大化  ,P X Y 的機率。則可以得到式
子，其定義如下，其中 為初始機率，a為轉移機率，b為觀察機率。 
 
  7 
之排列組合，即有七種可能。因此，一個標準 SPM 即可產生出七個預測腔調語音 SPM(Predicted PV SPM)。至
於，七個預測腔調語音的 SPM 是不是皆適合做為描述腔調語音的聲學模型，則需要再做進一步的模型驗證。 
對於預測產生的腔調語音的 SPM，必須去驗證其是否適合做為描述腔調語音的聲學模型，且希望經由驗證
後，認為適合留下的腔調語音 SPM，在原有標準 SPM 以及具腔調語音 SPM 中是具有足夠鑑別力
(Discrimination)，因此，在此使用ㄧ模型驗證方法，做為腔調語音 SPM 是否留下的評估方式。 
本計劃使用標準SPM對所有預測之腔調語音SPM進行驗證，亦是希望能夠先驗證預測產生之腔調語音SPM
對標準 SPM 的混淆程度，若驗證為視為適合留下的腔調語音 SPM。 
因此驗證中，定義了ㄧ鑑別函式(Discrimination Function)，如下式所示，計算鑑別程度(Discrimination 
Degree)，其定義如下： 
_ ( ) max{ ( )}
i
PV N
i i Y i m
m X
d g Y g Y 

  
 
其中 _PV N
id 為第 i 個預測腔調語音 SPM 之鑑別程度， iY 為對於第 i 個預測腔調語音 SPM之發音參數， iY 為
iY 之 SPM，而在此式中的 m 代表所有標準的 SPM。 
 
另外，再定義以下式子，做為一個門檻值 
_
,
( ) max { ( )}
i
i
normal N
i i X i m
m X m X
d g Y g Y 
 
  
 
_normal N
id 為對應於第 i個預測腔調語音 SPM之標準 SPM的鑑別程度， iX 為對應於第 i個預測腔調語音 SPM
之標準發音參數，而在此式中
m 代表除了 iX 之外的所有標準 SPM。 
在驗證中，若 _ _PV N normal N
i id d ，則表示預測的具腔調之 SPM 不易造成標準 SPM 的混淆，即代表第 i 個
預測腔調語音 SPM被驗證通過且為適合留下的聲學模型，將其稱為候選之腔調語音 SPM。 
 
第三年整體計畫執行架構    
圖十三為系統架構圖， 在訓練(Training)方面一開始會對語句內語言轉換語料抽取聲學特徵以及發音特徵作
為訓練聲學模型的訓練資料。接下來利用擷取到的聲學特徵訓練基於聲學特徵的隱藏式馬可夫模型；擷取到的
發音特徵訓練基於發音特徵的高斯混合模型。將基於聲學特徵的隱藏式馬可夫模型以及基於發音特徵的高斯混
合模型同時列入階層式三連音素模型合併方法(hierarchical triphone clustering method)的考量條件，最後取得本系
統之多語聲學模型集。在建立語言轉換的語言模型(CS language model)方面，透過中文語料和建立中英對照字典
建立出富強健性的語言轉換語言模型，使其可辨識更廣泛之中英夾雜語句。辨識(Recognition)方面，輸入語音區
段抽取聲學特徵，再透過本研究所建立之中英暨語言轉換多語自動語音辨識器進行辨識，取得該語音區段的辨
識結果。 
一. 特稱擷取與模型訓練 
為了合併產生語言轉換之多語聲學模型集，須先計算聲學模型間之相似度，因此須先擷取語料中之發音特
徵和聲學特徵作為訓練資料。本研究所使用的發音特徵取自於 Lee 等學者[29][30]所提出的 22 類發音特徵。由於 Lee 所
提出的 22 類發音特徵在其研究應用上，使用於多國語言的語料，因此我們認為其亦適用於用於夾雜不同語言之語言轉換語
料，22類特徵其內容如圖十四所示： 
本研究為了評估 Acoustic-based HMM 前後銜接的發音對於自身的影響，利用基於類神經網路的 22類發音
特徵偵測器擷取基於聲學特徵的隱藏式馬可夫模型的前、後銜接狀態之發音特徵，取得基於音框的發音特徵向
量(frame-based AF vector)。在取得基於音框的發音特徵向量之前，須先針對中英語句內語言轉換語料進行切割
(force alignment)，擷取出每個模型的前、後銜接的狀態之語音區段，將這些狀態的語音區段輸入至已建立好的
基於類神經網路的 22類發音特徵偵測器進行偵測，取得其發音特徵向量。切割模型前後銜接狀態之語音區段的
示意圖如圖 3-2 所示。 
  9 
 
0 0
1
( , ) ( || ) ( || )
2
S S
x s s y s sAC s ss s
D X Y w KL x y w KL y x
 
  
 
(4) 
其中 ( , )ACD X Y 代表 X和 Y 這兩個聲學模型之間的 KL-Divergence 距離，xs代表 X聲學模型狀態的指標，
ys代表 Y 聲學模型狀態的指標。X 和 Y 這兩個聲學模型各自包含 S 個狀態，將 X 和 Y 這兩個聲學模型的狀態一
對一對應，分別計算其狀態間的 KL-Divergence 距離 ( || )s sKL x y 和 ( || )s sKL y x 。取得 X 和 Y 每個狀態間的
KL-Divergence 距離後，乘上狀態的權重值進行加總，最後再做平均取得 X 和 Y 這兩個聲學模型之間的
KL-Divergence 距離。 
得到 X和 Y 的聲學模型之間的距離 ( , )ACD X Y 後，由於要考量前後發音對於聲學模型本身的影響，因此接
下來需計算 X和 Y 的發音特徵模型之間的距離 ( , )AFD X Y 。計算 X和 Y 的發音特徵模型之間距離須考量 X和 Y
各自前後銜接之發音特徵模型的距離，分別計算 X 和 Y 前銜接之發音特徵模型之距離 ( , )AFPreD X Y 和後銜接
之發音特徵模型之距離 ( , )AFSucD X Y ，將求得的前後銜接發音特徵模型的距離平均，作為 X 和 Y 的發音特徵
模型之間的距離 ( , )AFD X Y 。式(5)是 X和 Y 的發音特徵模型之間的距離的計算公式： 
 
 
1
( , ) ( , ) ( , )
2
AF AFPre AFSucD X Y D X Y D X Y 
 
                 (5) 
在計算發音特徵模型距離部份亦利用 KL-Divergence 計算發音特徵模型之間的距離。計算 X 和 Y 前銜接之
發音特徵模型之距離 ( , )AFPreD X Y 時，分別求取 X對 Y及 Y對 X的前銜接發音特徵模型之 KL-Divergence距離，
再將其加總平均作為 X 和 Y 前銜接之發音特徵模型之距離。計算 X 和 Y 後銜接之發音特徵模型之距離
( , )AFSucD X Y 時，分別求取 X對 Y 及 Y對 X 的後銜接發音特徵模型之 KL-Divergence 距離，再將其加總平均作
為 X和 Y 後銜接之發音特徵模型之距離。數學式(6)和(7)為計算前後銜接之發音特徵模型距離之數學公式： 
 
1
( , ) ( || ) ( || )
2
AFPre pre pre pre preD X Y KL X Y KL Y X 
 
      (6) 
 
1
( , ) ( || ) ( || )
2
AFSuc suc suc suc sucD X Y KL X Y KL Y X 
 
      (7) 
 
三. 合併三連音素模型 
當取得最短距離的三連音素模型 x 和 y 後，本研究使用階層式的架構合併相似之三連音素模型，首先會將
要合併的三連音素模型 x 和 y 進行狀態對應(state mapping)，從對應的狀態中計算 x 的第 s 個狀態中之第 i 個高
斯分布對於 y 的第 s個狀態中所有高斯分布的距離，取出最短距離的 mixture-pair，其數學式表示式如(8)所示，
其中 six 代表三連音素模型 x的第 s個狀態中之第 i 個高斯分布， sjy 代表三連音素模型 y的第 s個狀態中之第
j 個高斯分布，其中三連音素模型 y的第 s個狀態包含 m 個高斯分布。 ( , )si sjx y 代表經由最短距離計算取得的
mixture-pair。 
 
1
( , ) arg min ||
m
si sj si sj
j
x y KL x y


 
(8) 
利用求出之最短距離的 mixture-pair 產生新模型中第 i 個高斯分布，以下數學式(9)和數學式(10)分別是新模
型中每個高斯的平均向量以及共變異係數矩陣的計算方式。 
Xs Xsi Ys Ysj
Zsi
Xs Ys
m m
m m
 



  
(9) 
  11 
發音習慣造成的語音辨識效能不彰。本系統於第一年完成的模組包含 1)音素集的建立，針對中英雙語語料庫的
譯文擋收集兩種語言所對應的音素；2)潛在發音變異規則萃取，其目的在藉由比對非英語體系和英語體系語者
之間的差易找尋可能的發音變異規則；3)雙語語音模型之建立，依據語音辨識之發音辭典，建立中英雙語語音
辨識雛型系統，以期能降低語音辨識錯誤率。而關於本雛型系統的辨識率，可達到 93.12%的辨識正確率。 
本計劃於第二年建立一套系統性方法，可以產生富強健性之腔調模型，能夠提升對於腔調語音語句之辨識
率。在驗證腔調語音現象上，同時考慮聲學特性以及發音特徵，利用 SEM 進行腔調語音資料的驗證，且為了較
精準的模擬腔調語音現象，提出於 SEM模擬發音變異，並藉由擴增以 senone 為基礎的英文腔調語音聲學模型，
改善因腔調語音造成的辨識系統正確率降低的問題。另外，對於未能收集到的腔調語料問題，利用決策樹並考
慮語言的特徵參數進行分類，預測產生以 senone 為基礎之腔調語音模型。 
由實驗結果顯示，在針對腔調語音辨識的部分，可從數據得知由本計劃所建立的強健性多語語音聲學模型，
與傳統語音辨識器做比較，對於標準英文語料與腔調語音語料之字辨識率分別約提升 3.6%以及 3.1%的辨識正確
率，且針對標準音素與腔調音素之辨識率則分別提升 0.8%和 17%。 
本計畫於第三年結合聲學特性與發音特徵於計算模型間之距離上，在建立雙語或多語模型集時期望能夠降
低資料稀疏之問題，並且能夠精確的計算出模型與模型間於此兩種特性上之相似度，使合併後之模型具有最佳
的辨識效能。實驗結果顯示，於 CECOS 中英雙語轉換語料上，本研究所提出之合併方法的詞辨識率可達 84.27%，
與傳統的直接合併中英雙語的音素模型集和以 IPA 為主的模型集所建立之聲學模型方法相比，詞辨識率分別提
升了 3.85% 和 3.05%。更與傳統只考慮聲學特性相似度之資料驅動為基礎之方法的詞辨識率相比提高了 2.74%，
可以得知使用聲學特性與發音特徵之距離計算，會比其他傳統方法來得好，證實本研究引入發音特徵確實能夠
提升對於語句內語言轉換內語料之準確率，改善資料稀疏所造成的影響。此外，本研究所使用之基於翻譯為基
礎之中英混合語句語言模型，能增進模型之強健性，在維持辨識準確率的情況下，使語言模型可辨識更廣泛之
中英夾雜語句。 
 
五、誌謝 
  本研究承蒙國科會 NSC 98-2221-E-006 -139 -MY3 補助特此致謝。 
 
六、參考文獻 
[1] Jingjing Liu, Yushi Xu, Stephanie Seneff, and Victor Zue, “Citybrowser Ⅱ: A multimodal restaurant guide in 
Mandarin”, in Chinese Spoken Language Processing, 2008. 
[2] AT&T(2002) How May I Help You? [Online]. Available: http://www.research.att.com/~algot/hmihy/ 
[3] T. Hain, “Implicit Modeling of Pronunciation Variation in Automatic Speech Recognition,” Speech Recognition, 
vol. 46, pp. 171-188, 2005 
[4] A. Hämäläinen, L. Bosch, and L. Boves, “Modeling Pronunciation Variation Using Multi-path HMMs for 
Syllables”, in Proc. of International Conference on Acoustic, Speech, and Signal Processing (ICASSP), pp. 
781-784, 2007. 
[5] A. Hämäläinen, L. Bosch, and L. Boves, “Pronunciation Variant-based Multi-path HMMs for Syllables”, in Proc. 
of International Conference on Spoken Language Processing (INTERSPEECH), pp. 1579-1582, 2006. 
[6] M. Wester, "Pronunciation Modeling for ASR-knowledge-based and Data-driven Methods," Journal of Computer 
Speech and Language, vol. 17, pp. 69-85, 2003 
[7] T. J. Hazen, I. Lee Hetherington, Han Shu and Karen Livescu, "Pronunciation Modeling using a Finite-State 
Transducer Representation", in Proc. of ISCA Workshop on Pronunciation Modeling and Lexicon Adaptation, 
  13 
Phonetic Association, 1993.  
[25] P.-Y. Shih, J.-F. Wang, H.-P. Lee, H.-J. Kai, H.-T. Kao and Y.-N. Lin, “Acoustic and Phoneme Modeling Based 
on Confusion Matrix for Ubiquitous Mixed-Language Speech Recognition,” IEEE SUTC, pp.500-506, 2008.  
[26] C.-L. Huang and C.-H. Wu, “Phone Set Generation Based on Acoustic and Contextual Analysis for Multilingual 
Speech Recognition,” in Proc. of ICASSP, vol.4, pp.IV-1017-IV-1020, 2007.  
[27] B. Mak and E. Barnard, “Phone clustering using the Bhattacharyya distance,” in Proc. of the International 
Conference on Spoken Language Processing (ICSLP), pp. 2005-2008, 1996.  
[28] J. Goldberger and H. Aronowitz, “A Distance Measure Between GMMs Based on the Unsented Transform and its 
Application to Speaker Recognition,” in Proc. of Eurospeech, pp. 1985-1988, 2005. 
[29] S. M. Siniscalchi, T. Svendsen and C.-H. Lee, “Toward A Detector-Based Universal Phone Recognizer,” in Proc. 
ICASSP, 2008. 
[30] S. M. Siniscalchi, T. Svendsen, and C.-H. Lee, "A penalized logistic regression approach to detection based phone 
classification," in Proc. Interspeech, pp. 2390-2393, 2008. 
 
附圖: 
 
 
圖一：英文雙語語音辨識流程示意圖 
 
  15 
 
圖 五：常誤唸之英語發音 (三) 
 
 
圖六：發音變異之種類與實例 
 
 
圖 七：發音變異網絡之結構 
 
 
圖 八：音素混淆矩陣 
 
  17 
 
圖十二：轉換函式預測模型示意圖 
 
 
圖十三：富強建性之資料驅動為基礎建構之雙語模型集系統架構示意圖 
 
 
 
AFs (22) 
Vowel Fricative Nasal Voiced 
Approximant Coronal Labial Back 
Stop Glottal Low Vocalic 
Dental High Mid Continuant 
Velar Anterior Retroflex Round 
  19 
計劃成果自評 
在這個為期三年的計畫中，第一年的計畫重點為建立具抗發音變異之中英雙語自發性語音辨識技術；第二年
的重點為產生抗腔調之語音辨識模型以增進語音辨識器的效能；第三年則是提出一結合聲學與發音特性之資料
驅動為基礎之音素模型集建立方式、並且收集中英雙語語料，以發展出一套完整的中英雙語語音辨識雛型系統，
可轉譯中英雙語語音訊號以及語音辨認結果。 
在第一年的計畫執行方面，我們已建立出一套具抗發音變異之雙語語音辨識雛型系統。此系統透過中英雙語
發音音素之收集與制訂初步的發音辭典，再透過潛在發音變異規則的萃取，可有效萃取出可能的發音變異，並
且達到高辨識正確率。此中英雙語語音辨識技術之研究與發現有一定之學術及產業價值。 
在第二年的計畫執行方面，我們建立出一套系統性方法並產生抗腔調語音之語音辨識模型。此系統透過兩階
段之腔調語音驗證找出語料中之腔調語音，再透過正常發音與對應腔調語音之線性關係找出線性轉換函式，可
依此轉換函式推論出語料中尚未收集到之腔調語音模型，並且配合模型鑑別力檢測提升辨識正確率。此中英雙
語語音辨識技術之研究與發現有一定之學術價值。 
在第三年的計劃執行方面，我們亦已達成預期目標，依據所提出之整合聲學與發音特性之以資料驅動為基礎
之音素模型集建立方式建立出一套中英雙語語音辨識器。此方法考慮了三連音本體之聲學特性與前後狀態之發
音特性來進行三連音素之間相似度之計算，再透過階層式架構合併相似之三連音素模型，此外本研究亦使用一
中英雙語轉換之語言模型以降低雙語語料收集不齊全所造成之資料稀疏之問題。此中英雙語轉換語音辨識技術
可提升國內產業在語音辨識上之技術層次，進而開發相關產品。 
  21 
英文： 
The goal of this technique is to develop a Chinese-English basic 
bi-lingual automatic speech recognizer with pronunciation variation  
in the first year. The first step is to annotate the Chinese-English 
bi-lingual corpus by the International Phonetic Alphabe for phone set 
collection. Then, each phone is modeled by an acoustic model through 
training data in the basic ASR. The next step is the rule extraction for 
PV – insertion type, deletion type, and substitution type. In this project, 
a pronunciation variation network is constructed from the 
pronunciation lexicon and then the tri-phones of this PV network are 
compared to the CMU pronunciation lexicon to extract the potential 
PV rules. Finally, the extracted PV rules and those rules defined by 
experts are combined into the pronunciation lexicon for the basic 
bi-lingual ASR. 
In the second year, this project focuses on generating accented 
acoustic models for recognition of non-native speech with accents.  In 
accented speech segment extraction, acoustic features and articulatory 
attributes are considered.  For the extracted accented speech segment, 
the linear transformation functions between normal phones and 
accented phones are constructed.  A decision tree is constructed based 
on the articulatory attributes and acoustic features and used to select a 
suitable transformation function to generate accented models with 
unseen data.  Finally, discrimination verification is performed to 
remove the accented models with low discriminability. By combing 
normal phone models and accented phone models, the accuracy of 
ASR can be improved in recognizing accented and normal speech. 
In the third year, this project develops a Chinese-English 
code-switching speech recognition system based on the proposed 
Chinese-English phone set construction method. The proposed method 
integrates acoustic and articulatory features into distance estimation in 
similar phone model clustering. The project also builds a 
Chinese-English code-switching language model to overcome data 
sparseness problem. The accuracy of ASR can be improved in 
recognizing Chinese-English code-switching utterances. 
可利用之產業 
及 
可開發之產品 
數位內容產業、數位教學產業。 
多國語言語音文件(會議、新聞、教學錄音)自動語音轉譯系統 
技術特點 
此技術於潛在發音變異規則萃取部分，將中英混合辨識雛型系
統的發音辭典，經由建構發音變異網路後，將發音變異網絡中的三
連音與 CMU發音辭典中曾出現的三連音做一個比較，找出機率較
大的發音變異規則，再從句子中擷取出發生機率較高的發音變異之
後，將找到的發音變異規則與專家定義的規則做一個結合並整合進
最後的中英混合辨識器中的發音辭典。 
此技術於兩階段式發音變異與腔調之驗證部分 – 使用聲學
特性驗證與發音特徵驗證，將語料中之腔調語音擷取出來，並根據
正常語音與其相對應之腔調或變異語音之間之現性轉換關係，配合
決策樹去預測語料中尚未收集到之可能腔調模型，將所預測之腔調
模型與正常發音模型做結合，可提升多語辨識器對於腔調語音之辨
 1 
國家科學委員會補助出席國際學術會議報告 
100年9月5日 
報告人: 吳宗憲教授 
國立成功大學 資訊工程學系 
計畫編號 NSC 98-2221-E-006 -139 -MY3  
會議時間：100年8月27日~ 8月31日  會議地點：義大利 佛羅倫斯 
      中文會議名稱： 二O一一年國際語音通訊會議 
英文會議名稱: Interspeech2011 
 
 
會議摘要 
 
    本人依計畫參加於義大利  佛羅倫斯舉行之2011 國際語音通訊會議
(Interspeech2011)出國其目的如下：  
     1) 發表三篇Interspeech會議論文； 
     2) 參加 ISCA Group Meeting。  
 
一、會議大要 
「二O一一年國際語音通訊會議」係國際語音通信學會 (International Speech 
Communication Association, ISCA)主辦之全球年會，此一會議為聲學語音處理處理
方面最大且最被肯定會議之一。本年在義大利佛羅倫斯Firenze Fiera國際會議中
心舉行，參與人數超過1000人以上。本會議分多個領域主題，並以9個會場同時
進行，共47個口頭報告場次，每篇論文發表20分鐘。海報論文發表38個場次，
每篇論文張貼120分鐘。共有846篇論文發表。 
本次大會主席由Piero Cosi及Renato De Mori教授擔任，議程主席由Robero 
Pieraccini 及Pino Di Fabbrizio 教授擔任。大會於8月28~31日每天早上一場主題
演講(Keynote Session)或圓桌會議，分別為 
 
Keynote 1: Prof. Julia Hirschberg, Columbia University, USA 
Speaking More Like You: Entrainment in Conversational Speech 
 
Keynote 2: Tom M. Mitchell, Carnegie Mellon University, USA 
Neural Representation og Word Meaning 
 
Keynote 3: Alex ‘Sandy’ Pentland, MIT 
Honest Signals 
 
Keynote 4: Gabriele Miceli, Bjorn Granstrom, Hiroshi Ishiguro 
Future and Applications of Speech and Language Technologies for the Good 
Health of Society 
 
二、參與會議 
本人8月26日晚上搭飛機出國，並於8月27日抵達義大利羅馬，再搭火車至
佛羅倫斯之旅館，並熟悉大會場地之狀況，準備研討會之論文發表工作。大會
於 8月 28日舉行內容相當充實。本人此次參加 2011 國際語音通訊會議
Dear Chung-Hsien Wu 
 
We are glad to announce that we have finalized the Interspeech 2011 program.  
Your paper 
Paper ID: 91755 
Title:    "Interactional Style Detection for Versatile Dialogue Response  Using Prosodic and 
Semantic Features" has been included for presentation in 
 
Session Sun-Ses2-P4: "Spoken Dialogue & Spoken Language Understanding Systems" 
Date: Sunday, August 28, 2011 
Time: 13:30 
Room: Faenza 2 - Pala Congressi 
 
The format of this session is poster. Furhter detailed information will be available on the confeence 
web site. 
 
We wish to remind you that at least one author has to register to the conference before June 15th in 
order to get your paper published in the proceedings.  If you have not registered yet, please go to 
 
http://www.interspeech2011.org/conference/registration.php. 
 
We also would like to kindly remind you that, being Florence a worldwide touristic attraction, it is 
advisable to do early travel arrangements and hotel reservations. To hotel near the conference 
venue and make your online reservation go to 
 
http://www.firenzefiera.it/en/florence-useful-information/hotels-and-restaurants 
 
(remember to check "Palazzo dei Congressi" as the destination in the 
drop-down menu). 
 
We hope to see you in Florence, 
 
Best regards, 
 
Roberto Pieraccini and Pino Di Fabbrizio 
Interspeech 2011 Technical Chairs 
recognition models (SERMs), prosodic information-based 
recognition models (PERMs), and dialogue topic 
categorization model (DTCM) using MaxEnt, SVM, and LDA, 
respectively. DTCM is employed to characterize the key 
words for topic categorization. SERMs and PERMs are 
utilized to model semantic labels and prosodic information for 
emotional state detection, respectively. Then, the same 
training instances are used again to estimate the confidence 
scores for each emotional state and each dialogue topic. To 
integrate the three base-level measures, an ANN is used to 
train the IS detection model. Finally, given a test instance, the 
confidence scores will be obtained from the three base-level 
models. The output node of the ANN with the highest score is 
used to output the detected IS. The detailed training procedure 
of each base-level classification will be described in the 
following sections. 
2.1. Semantic Label-based Emotion Recognition 
In a conversational system, speakers can express their 
emotional state in different ways. For example, 
I am so glad I finished the annoying job 
and  
I finished the annoying job 
represent the same emotion – Happy. However, the word 
“glad” does not appear in the latter sentence. To solve this 
problem, the SLRM [11] was employed for emotional state 
recognition based on textual information. First, the training 
data are divided into four categories – happy, angry, sad, and 
neutral. Then, for each emotional state, synonym mapping is 
employed to generate the synonyms of the recognized words 
using the Chinese knowledge base, HowNet [12]. Three 
categories of semantic labels (SL), including specific SLs, 
negative SLs, and disjunctive SLs, are extracted. Examples of 
the synonym set with the semantic labels are given in Table 1. 
Most of the specific SLs are verb and can express the 
speaker’s emotional state without emotional keyword such as 
“glad”. For two other categories of SLs, negative SLs are used 
to represent negative semantics. The atomic sentences 
before/after disjunctive SLs will be retained for further process. 
In order to handle the conversational speech, several additional 
semantic labels (e.g. President) with their corresponding 
synonym set (e.g., OBAMA and BUSH) are adopted in this 
work. Before employing the maximum entropy (MaxEnt) 
model to classify the emotional state based on the semantic 
label sequence, emotional association rules (EAR) selected 
based on a priori algorithm [13] are utilized for frequent 
trigger pair selection. As an illustrative example, the input text  
I earn a lot of money 
belonging to happy emotion will be converted into the 
semantic labels “OBTAIN” and “MONEY” as earn is the 
synonym set with the semantic label of “OBTAIN” and 
“MONEY” being the synonym set with the semantic label of 
MONEY. If labels “OBTAIN” and “MONEY” form a 
frequent pair, this pair is an EAR. Finally, the MaxEnt model 
is employed to classify emotional state em based on the 
semantic label sequences S. The probability-related score is 
given by 
1( , ) exp{ ( , )}
( )MaxEnt m k k mk
f e f e
Z


S S
S
 (1)  
where fk(em,S) is the observation function with the weight k ; 
Z(S) is a normalization term for the summation of the  
observation functions of all training data and can be defined as: 
 
Figure 1: Framework of Interaction Style Detection. 
Table 1. Some Examples of Semantic Labels 
Specific SLs Synset in HowNet 
ACHIEVE VACHIEVE, FULFILL, END, FINISH  
OBTAIN OWN, OBTAIN, RECIEVE, EARN 
LOSE OWNNOT, LOSE, INDEBT 
 
Negative SLs Synset in HowNet 
NEGATIVE CANNOT, UNNECESSARY, NEVER  
 
Disjunctive SLs Synset in HowNet 
Disj_Extract FINALLY, BUT  
Disj_Remove OTHERWISE, ALTHOUGH 
 
 
( )= exp{ ( , )}k k memZ f eS S  (2)  
Finally, given a word sequence, each semantic label-based 
emotion recognizer will output its corresponding confidence 
score. 
2.2. Prosodic Information-based Emotion 
Recognition 
In PIER, a front-end procedure extracts the prosody-related 
features that are commonly used in emotion recognition. 
Specially, the maximum, minimum, mean, and standard 
deviation of the pitch, energy, formant-related features, as well 
as speech rate are extracted to represent the acoustic 
expression of the utterance. These extracted values constitute a 
feature vector representation for an utterance, which is denoted 
by F. 
For each emotional state em, a corresponding support 
vector machine Vm , m=1,…,M is used to separate the feature 
vectors of the target emotional-state instances from the feature 
vectors of the non-target emotional-state instances. Note that 
the SVMs are trained by maximizing the margin (F) of the 
vectors in a training data set. Furthermore, we use the Platt’s 
formula [14]  
1( , )
1 exp{ ( ) }SVM j
f e
  

 
F
F
 (3) 
to convert the binary SVM output to data likelihood, where the 
parameters  and  are determined by maximizing the data 
likelihood of the training set. Finally, given an instance, each 
SVM-based emotion recognizer will output its corresponding 
confidence scores.  
1346
Table 2. Average recognition accuracy (ARA) of 
emotion recognition 
Approaches PIER SLER Text 
ARA(%) 78.16 80.92 60.74 
Table 3. Number of utterance of each IS in each 
emotional state. 
 IC CTC GTG BTS 
Angry 595 63 23 7 
Happy 31 3 21 371 
Neutral 126 132 1169 679 
Sad 0 490 16 5 
Table 4. Recognition performance of topic detection 
with different number of latent topics 
#(Topic) 6 7 8 9 
Correct Rate (%) 78.97 79.25 81.23 76.82 
Table 5. Example words of some latent topics in eight 
topics (N=8). 
Topic 1 Topic 2 Topic 3 Topic 4 
Politician Everyone Now McDonald 
President Smart Today KFC 
Lawmaker Problem Weekend Buffet 
Budget Gossip Male Steak 
Weapon Know Female Vegetable 
 
 
Figure 2: MLP-based interactional style detection with 
different number of hidden nodes 
based IS detection with various numbers of hidden nodes is 
shown in Figure 2. The best one can achieve 82.67% detection 
accuracy with eight topics and twenty hidden nodes. However, 
the results with fifteen hidden nodes can obtain acceptable 
performance for different number of latent topics. 
4. Conclusions 
This work presents an approach to interactional style detection 
using prosodic features and textual information. In order to 
deal with the synonym problem, four categories of semantic 
labels are extracted from HowNet and conversational data. 
Then, MaxEnt is employed to model the relationship between 
emotional states and the semantic label sequence derived from 
the word sequence. For prosodic information, SVM is utilized 
to train a prosodic information-based emotion recognizer. 
Besides emotion, words are also the key role in IS detection. 
Hence, LDA is employed to obtain the keywords in the latent 
topics. Finally, an artificial neural network is adopted for IS 
detection considering the scores estimated from the above 
classifiers. In order to evaluate this work, more than 1800 
sentences in the domain of daily life chatting were collected. 
Finally, the proposed approach can achieve 82.67% 
recognition accuracy with twenty hidden nodes used in the 
MLP-based IS detector. Moreover, the prosodic and semantic 
information based emotion recognizer can achieve 80.92% and 
78.16% accuracy, respectively. Eight topics could obtain the 
best topic detection result. In our future work, although the 
proposed approach can work well on IS detection, more 
training data are needed for fair evaluation. 
5. References 
[1] D. A. Dahl, “Expanding the Scope of the ATIS task: The ATIS - 
3 corpus,” in Proc. DARPA Human Language Technology 
Workshop, pp.43-48, 1994. 
[2]  J. Liu, Y. Xu , S. Seneff , and V. Zue, “CityBrowser II: A 
Multimodal Restaurant Guide In Mandarin,” in Proc. ISCSLP, 
pp. 1-4, 2008. 
[3] N. Roy , J. Pineau , S. Thrun, "Spoken Dialogue Management 
Using Probabilistic Reasoning," in Proc. Annual Meeting of the 
Association for Computational Linguistics (ACL), pp. 93-100, 
2000.
[4]  R. Wallace, Artificial Linguistic Internet Computer Entity 
(A.L.I.C.E.) [Online] Available: 
http://www.alicebot.org/aiml.html 
[5]  R. Carpenter, Jabberwacky, [Online] Available: 
http://www.jabberwacky.com/  
[6]  C.-H. Wu and G.-L. Yan, “Speech Act Modeling and 
Verification of Spontaneous Speech with Disfluency in a Spoken 
Dialogue System,” in IEEE Trans. on Speech and Audio 
Processing, Vol.13, May, pp. 330-344, 2005. 
[7]  L. V. Berens, “Understanding Yourself and Others: An 
Introduction to Interaction Styles,” Telos Publications, 2008.  
[8]  D. Jurafsky, R. Ranganath, D. MacFarland, “Extracting Social 
Meaning: Identifying interactional style in spoken conversation,” 
in Proc. NAACL HLT, pp. 638-646, 2009. 
[9]  M. A. Walker, J. E. Cahn, S. J. Whittaker, “Improvising 
Linguistic Style: Social and Effective Bases for Agent,” in Proc. 
International Conference on Autonomous Agents, pp. 96-105, 
1997 
[10]  D. Gabor, How to Start a Conversation and Make Friends, 
Simon and Schuster, 2001 
[11] C.-H. Wu and W.-B. Liang, “Emotion Recognition of Affective 
Speech Based on Multiple Classifiers Using Acoustic-Prosodic 
Information and Semantic Labels,” IEEE Trans. on Affective 
Computing, Vol. 2, No. 1, pp. 1 – 12, 2011. 
[12] Z. Dong, and Q. Dong, HowNet [Online] Available: 
http://www.keenage.com/ 
[13] R. Agrawal, T. Imielinski, and A. N. Swami, "Mining 
association rules between sets of items in large databases," in 
Proc. the ACM Special Interest Group on Management of Data 
(SIGMOD), pp. 207-216, 1993.
[14]  C. Platt, “Probabilities for SV Machines,” Advances in Large 
Margin Classifiers, pp. 61-74, MIT Press, 2000. 
[15] D. M. Blei, A. Y. Ng,, and M. I. Jordan, "Latent Dirichlet 
Allocation," Journal of Machine Learning Research, pp. 993–
1022, 2003. 
[16] N. Fraser and G. N. Gilbert, “Simulating Speech Systems,” 
Computer Speech and Language, Vol. 5, No. 1, pp. 81–99, 1991. 
[17]  P. Boersma, and D. Weenink, Praat: doing phonetics by 
computer (version 5.1.05s),  [Computer program] Retrieved May 
1, 2010, from http://www. praat.org/ 
[18]  X.-H. Phan, L.-M. Nguyen, and S. Horiguchi, “Learning to 
Classify Short and Sparse Text & Web with Hidden Topics from 
Large-scale Data Collections,” in Proc. International World 
Wide Web Conference, pp.91-100, 2008. 
[19] S. J. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and 
P. Woodland, “The HTK Book, version 3.4.” Cambridge 
University Press, 2006.. 
1348
98 年度專題研究計畫研究成果彙整表 
計畫主持人：吳宗憲 計畫編號：98-2221-E-006-139-MY3 
計畫名稱：具抗發音變異、不流暢語流之中英雙語自發性語音辨識技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 4 4 100%  
博士後研究員 1 1 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
已發表期刊論文如下 
[1] Chung-Hsien Wu*, Hung-Yu Su and Han-Ping Shen, ’Articulation-Disordered 
Speech Recognition using Speaker-Adaptive Acoustic Models and Personalized 
Articulation Patterns,’ ACM Trans. Asian 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
在這個為期三年的計畫中，第一年的計畫重點為建立具抗發音變異之中英雙語自發性語音
辨識技術；第二年的重點為產生抗腔調之語音辨識模型以增進語音辨識器的效能；第三年
則是提出一結合聲學與發音特性之資料驅動為基礎之音素模型集建立方式、並且收集中英
雙語語料，以發展出一套完整的中英雙語語音辨識雛型系統，可轉譯中英雙語語音訊號以
及語音辨認結果。 
在第一年的計畫執行方面，我們已建立出一套具抗發音變異之雙語語音辨識雛型系統。此
系統透過中英雙語發音音素之收集與制訂初步的發音辭典，再透過潛在發音變異規則的萃
取，可有效萃取出可能的發音變異，並且達到高辨識正確率。此中英雙語語音辨識技術之
研究與發現有一定之學術及產業價值。 
在第二年的計畫執行方面，我們建立出一套系統性方法並產生抗腔調語音之語音辨識模
型。此系統透過兩階段之腔調語音驗證找出語料中之腔調語音，再透過正常發音與對應腔
調語音之線性關係找出線性轉換函式，可依此轉換函式推論出語料中尚未收集到之腔調語
音模型，並且配合模型鑑別力檢測提升辨識正確率。此中英雙語語音辨識技術之研究與發
現有一定之學術價值。 
在第三年的計劃執行方面，我們亦已達成預期目標，依據所提出之整合聲學與發音特性之
