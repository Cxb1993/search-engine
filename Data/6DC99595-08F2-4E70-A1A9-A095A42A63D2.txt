In the first year of this proposal, we developed two new packet classification 
schemes. We implemented all the proposed and existing classification algorithms in 
IXP2400 and analyzed them completely. The results show that the proposed packet 
classification algorithm is faster and smaller than other schemes. By using our scheme, 
it is possible to put all the rule table data structure in fast SRAM. A portion of SRAM 
is allocated to implement the L1-like cache memory. This L1 cache design gives our 
scheme a great improvement. 
In the second year of our proposal, we proposed a new TCAM architecture to 
improve the speed of packet classification. However, directly converting all 
classification rules into prefixes will consume a very large TCAM space and high 
power consumption because of the port range problem. The port range problem is 
resulted from the fact the source port and destination port fields have arbitrary ranges. 
The proposed range split algorithms solve the port range problem by using a small 
rule structure for each prefix. Using this new architecture and split algorithms, we can 
solve the table increasing problem and speed up the classification speed. 
Keywords: packet classification, TCAM, network processor, Gray code, cache 
 
2. 文獻探討 
The simplest classification algorithm is a linear search of each rule of a classifier. 
For a large number of rules this approach implies a long query time, but is very 
efficient in terms of memory. The linear search date structure is simple and is readily 
updated as rules change. Srinivasan et al. propose a algorithm for two dimensions, 
called Grid of Tries. Srinivasan et al. show that on a classifier with 20,000 rules in 
two dimensions, the scheme requires about 9 memory accesses per query in the worst 
case and about 2MB for storage. The authors found that the scheme cannot be easily 
extended to more than two fields, and so proposed a generalized scheme called 
Cross-Producting. This scheme build a table of all possible field interval combination 
(cross-products) and precompute the highest priority rule matching of each 
cross-product. Search can be done quickly by doing separate lookups on each field, 
combining the results together into a cross-product, and indexing into the 
cross-product table. Unfortunately, the size of this table grows astronomically with the 
number of rules. Srinivasan et al. also propose another algorithm called Tuple-Space 
Search, the scheme partition the rules of a classifier into different tuple categories 
based upon the number of specified bits in the rules. The scheme then uses hashing 
among rules within the same category. The main disadvantage of this scheme is the 
use of hashing makes the time complexity of searches and updates non-deterministic. 
Lakshman and Stiliadis propose a hardware-optimized scheme called Lucent Bit 
Vector scheme. The query process of this scheme first searches each dimension 
separately to yield the set of rules that match the packet in that particular dimension. 
These sets are then intersected efficiently using bitmaps to yield the set of rules that 
match in all dimensions. The drawbacks of this scheme are this scheme need parallel 
hardware assist and is impractical for large classifiers due to large memory consuming. 
Gupta and McKeown propose a scheme called Recursive Flow Classification (RFC), 
this scheme is very fast but whose memory needs are also large. AQT, HyperCuts, 
FIS-trees, segment tree, and HiCut, these trie-based packet classification algorithms 
are not only having poor performance but also needing a lot of memory. Table 1 
shows the search time and storage complexity of previous packet classification 
algorithms. 
Table 1: Comparison of the complexity of previously proposed packet 
classification algorithms on a classifier with N rules and d W-bit 
wide dimensions, where l means queries on an l-level FIS-tree, and 
w means the memory width. The results assume that each rule is 
stored in O(1) space and takes O(1) time to determine whether it 
matches a packet. 
Algorithm Worst-case time 
complexity 
Worst-case storage 
complexity 
Linear Search O(N) O(N) 
Hierarchical Tries O(Wd) O(NdW) 
Set-Pruning Tries O(dW) O(N
ddW) 
the cache miss penalty thus we try to utilize the matched part of the miss matched 
cache entry. Thus, our cache has contribution to the overall performance even when 
the cache is not hit. Our project does not focus on how to improve the cache hit ratio 
just like pervious studies, instead, we focus on how to utilize the cache entry when 
cache miss. We though this another direction of cache scheme for packet 
classification. 
 
3.2 A 2-Level TCAM Architecture for Ranges 
As the demand for high-quality Internet increases, emerging network 
applications are spurring the need for faster, feature rich, and cost-effective routers. 
Multifield packet classification in routers has been a computation-intensive data path 
function for software implementation. Therefore, solutions for packet classification 
based on hardware design, such as Ternary Content Addressable Memory (TCAM), 
are necessary to sustain gigabit line processing rate. Traditionally, TCAMs have been 
designed for storing prefixes. However, multifield packet classification usually 
involves two fields of arbitrary ranges that are TCP/IP layer 4 source and destination 
ports. Storing ranges in TCAMs relies on decomposing each individual range into 
multiple prefixes, which leads to range-to-prefix blowout. To reduce the total number 
of prefixes needed to represent all ranges, this paper proposes a 2-level TCAM 
architecture and two range-to-prefix conversion schemes. In the first proposed scheme, 
designed for disjoint ranges, the maximum number of entries needed in TCAM is 2m 
－1 for m disjoint ranges. In the second proposed scheme, designed for contiguous 
ranges, only m TCAM entries are needed. In a general case of n arbitrary ranges, all 
ranges can first be converted into disjoint ranges or contiguous ranges and then the 
proposed algorithms can be applied. As a result, only 4n－3 TCAM entries are 
needed for the disjoint ranges and only 2n＋1 TCAM entries are needed for 
contiguous ranges. This paper also proposes insertion and deletion algorithms to 
accommodate incremental changes to the range sets. The experiments made show that 
the proposed range-to-prefix conversion schemes perform better than the existing 
schemes in terms of the number of required TCAM entries and execution time for 
range update operations. 
 
3.3 Efficient Multidimensional Packet Classification with Fast Updates 
We propose a new packet classification scheme based on the binary range and 
prefix searches. The basic data structure of the proposed packet classification scheme 
for multi-dimensional rule tables is a hierarchical list of sorted ranges and prefixes 
that allows the binary search to be performed on the list at each level to find the best 
matched rule. We also propose a set of heuristics to further improve the performance 
4. 實驗結果 
In this network processor project, we show the experimental results of the 
proposed schemes in this project. First we compare the performance of HBPS with 
EGT and HiCuts. Then we compare the performance of HBPS with the proposed 
cache schemes. 
 
 
 
Figure 1: Packet Processing Rate of HBPS and other Well-Known Schemes. 
 1ME 2ME 3ME 4ME 5ME 6ME 
HBPS 1.81 3.62 5.42 6.42 6.42  6.42 
EGT 0.68 1.33 1.96 2.28 2.30  2.30 T1000H 
HiCuts 1.19 2.36 3.43 4.07 4.21  4.22 
HBPS 1.74 3.49 5.20 6.38 6.39  6.39 
EGT 0.65 1.28 1.90 2.20 2.23  2.23 T1000L 
HiCuts 1.21 2.41 3.54 4.29 4.44  4.45 
HBPS 1.81 3.62 5.42 6.42 6.42  6.42 
EGT 0.68 1.33 1.96 2.28 2.30  2.30 T1000HN 
HiCuts 1.19 2.36 3.43 4.07 4.21  4.22 
HBPS 1.74 3.48 5.20 6.38 6.39  6.39 
EGT 0.65 1.28 1.90 2.20 2.23  2.23 T1000LN 
HiCuts 1.21 2.41 3.54 4.29 4.43  4.45 
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1ME 2ME 3ME 4ME 5ME 6ME
5D1000H
M
illi
on
 Pa
ck
et 
Pr
oc
ess
ing
 R
ate
HBPS EGT HiCuts
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1ME 2ME 3ME 4ME 5ME 6ME
5D1000L
M
illi
on
 Pa
ck
et 
Pr
oc
ess
ing
 R
ate
HBPS EGT HiCuts
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1ME 2ME 3ME 4ME 5ME 6ME
5D1000HN
M
illi
on
 Pa
ck
et 
Pr
oc
ess
ing
 R
ate
HBPS EGT HiCuts
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1ME 2ME 3ME 4ME 5ME 6ME
5D1000LN
M
illi
on
 Pa
ck
et 
Pr
oc
ess
ing
 R
ate
HBPS EGT HiCuts
delay is not included in the experiments. In columns 4, 5, 8, and 9, we define the 
number of TCAM operations to be the number of prefixes inserted in or deleted from 
TCAM. We assume that the times taken for inserting and deleting a prefix into/from 
TCAM are the same without differentiating the lower-level TCAM operations such as 
TCAM write and disable operations. The number of TCAM write and disable 
operations will be calculated based on the CAO_OPT TCAM update algorithm 
proposed in [1] when we evaluate the performance of inserting/deleting a prefix 
into/from TCAM. 
Table 2 shows the numbers of required TCAM entries that can be used as a 
measure of power consumption. CONT performs best with Firewall rule tables, as 
shown in Table 2a, because all the elementary intervals are valid intervals and 
completely cover the entire address space. The number of prefixes needed for 
TrieNode and LCP is almost double that of CONT. As expected, DIRECT performs 
much worse than other schemes. Table 2b shows the results for the range sets 
generated by us. When the number of original ranges is less than or equal to 1,000, a 
lot of elementary intervals are default intervals which are not used in DISJ. Thus, 
DISJ outperforms CONT. 
 
 
 
Table 2: Numbers of Required TCAM Entries: (a) Destination Port Ranges of 
Firewall Rule Tables and (b) Randomly Generated Range Sets 
source and destination ports of these rule tables were extracted and evaluated in the 
experiments based on the storage requirements in SRAM and TCAM memories. The 
results were given in terms of expansion factors, defined to be the ratio of the number 
of expanded rules by some range encoding scheme to the number of original rules. 
The evaluated schemes include the direct range-to-prefix conversion (DC), the 
Direct range-to-ternary string conversion using Gray code (SRGE) [8], the elementary 
interval-based scheme using Buddy code (EIDC), the proposed elementary 
interval-based scheme using binary reflected Gray code (EIGC), the database 
independent range pre-encoding (DIRPE) [10], the bitmap intersection scheme 
(Bitmap) [9], PPC style-II or III which performs the best (PPC) [11], and the 
proposed scheme using the concept of perfect BRGC range set (P-BRGC). In DIRPE, 
the source port field uses strides 2, 2, 3, 3, 3, 3, and the destination port field uses 
strides 2, 2, 2, 2, 2, 3, 3, as suggested in [10]. Thus, TCAM entry widths of source and 
destination port fields are 34 and 29 bits, respectively. Table 4 shows the performance 
results for the three real-life rule tables. Since many original rules called prefix rules 
whose port values in both their source and destination fields are already prefixes, they 
can be stored in TCAM directly without the need for any encoding scheme. The 
performance on the non-prefix rules has a primary impact on the overall performance 
of any encoding scheme. Therefore, experiments on the non-prefix rules were also 
conducted, the results of which are shown in Table 4. The TCAM size in Kbits (Kb) is 
calculated by (number of TCAM Entries × sum of source and destination port entry 
sizes) / 1024. The SRAM size in Kbytes (KB) is calculated by (65536 × sum of source 
and destination port entry sizes) / (8×1024). Figure 4 also shows the results in bar 
charts. 
5. 結論 
We proposed a set of efficient algorithms and architectures for multi-dimensional 
packet classification. We implement them by using network processor and accelerate 
them by TCAMs. The experimental results showed that our scheme outperforms other 
existing schemes not only in terms of classification speed but also in terms of TCAM 
usage. The results of this project have been published in many journals and 
conferences. We was satisfied with the outcome of this project. 
 
6. 參考文獻 
[1] D. Shah and P. Gupta, “Fast Updates on Ternary-CAMs for Packet Lookups and 
Classification,” IEEE Micro, vol. 21, no. 1, pp. 36-47, Jan./Feb. 2002. 
[2] EGT and EGT-PC, http://www.ial.ucsd.edu/classification/. 
[3] A. Feldmann and S. Muthukrishnan, “Tradeoffs for Packet Classification,” IEEE 
INFOCOM, pp.1193-1202, Mar.2000. 
[3] P. Gupta, S. Lin, and N. McKeown, "Routing Lookups in Hardware at Memory 
Access Speeds," IEEE INFOCOM, pp. 1240–47, Apr. 1998. 
[4] P. Gupta and N. McKeown, “Packet Classification on Multiple Fields,” ACM 
SIGCOMM, pp.147-160, Aug.1999. 
[5] P. Gupta and N. McKeown, “Packet Classification using Hierarchical Intelligent 
Cuttings,” Hot Interconnects VII, Stanford, CA, Aug.1999. 
[6] P. Gupta and N. Mckeown, “Algorithms for packet classification,” IEEE Network, 
vol.15, no. 2, pp. 24-32, 2001. 
[7] Hypercut, http://www.arl.wustl.edu/~hs1/project/hypercuts.htm. 
[8] A. Bremler-Barr and D. Hendler, “Space-Efficient TCAM-based Classification 
Using Gray Coding,” Proc. IEEE INFOCOM, 2007. 
[9] T. Lakshman and D. Stiliadis, “High-Speed Policy-Based Packet Forwarding 
Using Efficient Multi-Dimensional Range Matching,” Computer Communication 
Review, vol. 28, no. 4, pp. 203–214, Oct. 1998. 
[10] K. Lakshminarayanan, S. Venkatachary, and A. Rangarajan,“Algorithms for 
Advanced Packet Classification With Ternary CAMs,” Proc. ACM SIGCOMM, 
2005. 
[11] J. Lunteren and T. Engbersen, "Fast and Scalable Packet Classification," IEEE 
Journal on Selected Areas in Communications, Vol. 21, No. 4, May 2003. 
[12] D. Taylor and J. Turner, “ClassBench: A Packet Classification Benchmark,” Proc. 
IEEE INFOCOM, 2005. 
[13] Intel Corporation, “IXP1200 Network Processor Datasheet”, 2000.  
[14] RadiSys Corporation, “ENP2505 Hardware Reference”, 2002. 
[15] Intel Corporation, “Development Tools User’s Guide”, March 2002. 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-006 -163 -MY2 
計畫名稱 以 TCAM 及網路處理器為基礎的封包分類方法的研究及開發 
出國人員姓名 
服務機關及職稱 
張燕光  成功大學資訊工程系 副教授 
會議時間地點 2008/3/25 ~ 3/28 日本沖繩 
會議名稱 
(中文) 高等資訊網路與應用 
(英文) The IEEE 22nd International Conference on Advanced Information 
Networking and Applications (AINA 2008) 
發表論文題目 
Paper 1: Dynamic Cache Invalidation Scheme in IR-based Wireless 
Environments 
Paper 2: Improved TCAM-Based Pre-Filtering for Network Intrusion 
Detection Systems 
Paper 3: Multi-Character Processor Array for Pattern Matching in Network 
Intrusion Detection System 
 
一、參加會議經過 
今年第 25 屆 AINA 國際學術會議，在日本沖繩宜野灣市的 Convention Center 會議
中心舉辦，除了主要的 AINA 會議之外，尚包含 16 個 workshop 共同於同一地點一起舉辦，
會議期間為 25 號星期二至 28 號星期五，每天早晚四個時段，各個時段約同時有 6~12 個
session 於不同的會議廳發表論文，26號和 27號的早上為 keynote speaker 的演講時間，
分別邀請 Prof. Leonard Barolli (Title: Synchronization is Coming Back, But is 
it the Same?) 及 Prof. Shigeki Yamada (Title:Cyber Science Infrastructure (CSI) 
for Promoting Research Activities of Academia and Industries in Japan) 來作一
個專題演講。 
本人連同一位博士班研究生及二位碩士班研究生，在這次會議中，一共發表三篇論
文，第一篇論文探討在無線網路中，如何作資料快取一致性的問題，並達到可省電的效
能。第二篇及第三篇是研究在入侵偵測系統中，如何達到有效率並且可快速的特徵比對，
並提出以 TCAM 架構的方式來運作，報告論文的時間為 28 號星期五早上一場及下午一場。
除了我們的研究外，本人亦參與數個感興趣及相關的 session 進行討論，特別是無線網
路及特徵比對的議題。覺得各國的研究方法及步驟，部份可值得學習進而觀摩改善。 
 
二、與會心得 
本人這次參與會議，除了發表我們所作的研究論文，也與其他不同領域的教授交換
了相當多的意見，了解目前其他領域的研究情況，Keynote speech 的內容，更是讓我們
了解目前業界的情況，因此積極的發表論文，參與國際學術會議仍是一項非常需要努力
的工作。另外此行共同前往的研究生，也讓其提昇國際視野，了解研究是需要彼此觀摩，
Dynamic Cache Invalidation Scheme in IR-based Wireless 
Environments 
Yeim-Kuan Chang, I-Wei Ting and Tai-Hong Lin 
National Cheng Kung University, Tainan, Taiwan 
{ykchang, p7893113}@mail.ncku.edu.tw 
Abstract--Traditional cache invalidation schemes are not 
suitable to be employed in wireless environments due to the 
affections of mobility, energy consumption, and limited 
bandwidth. Cache invalidation report (IR) is proposed to deal 
with the cache consistency problem. However, the main 
drawback of IR-based schemes is the long latency of data access 
because the mobile hosts (MHs) need to wait next IR interval 
for cache invalidation when the cache hit happens. In this paper, 
we propose a Dynamic Invalidation Report (DIR) to reduce the 
latency of data access when the MHs query data. DIR contains 
an early cache validation mechanism by utilizing the validation 
messages. Therefore, the MHs can verify their cached data as 
soon as possible. Next, we design a predictive method to 
dynamically adjust IR interval to further reduce the latency 
called DIR-AI (DIR with Adjustable Interval) scheme. Finally, 
we evaluate the performance of the DIR and DIR-AI and 
compare them with the existing invalidation report schemes by 
using NS2 (Network Simulator). The experimental results show 
that DIR reduces averagely 54.3% and 34.3% of latency; DIR-
AI reduces averagely 57.35 and 38.6% of latency compared 
with TS (TimeStamp) and UIR (Updated IR) schemes 
respectively. 
Keywords: cache consistency, dynamic, invalidation report, 
latency, wireless networks. 
1. Introduction 
In recent years, the demand for advanced 
technologies of mobile computing is growing up 
significantly. Many wireless communication protocols 
provide different services for accessing resources in the 
wireless networks. People can obtain their desired 
application service from Internet at anytime and 
anywhere. Figure 1 shows a typical wireless architecture. 
Mobile Support Station (MSS) or called Base Station (BS) 
is a powerful hardware with large transmission range for 
communicating with mobile hosts (MHs). A server is a 
service component connected to MSS. MHs are mobile 
users which carry devices such as notebooks, PDA’s, or 
cell phones who would like to get information from MSS. 
A client is a process residing in MH that cooperates with 
server. A database connected to MSS stores data items 
needed by server and clients. The main task of a server is 
to respond the requests from clients, retrieve data from 
database, and send data to clients. 
In most data access model (client-server model), 
data items are delivered in an on-demand basis. It means 
server only responses the request from the clients. When 
the client-server model is applied in the wireless 
networks, MSS broadcasts data via the wireless channel 
to all MHs within its transmission coverage. Therefore, 
broadcast scheme is frequently used by many wireless 
communication protocols for data access. A broadcast 
data delivery system uses one or more servers to satisfy 
the requests of users. We first introduce two basic 
architectures for broadcast delivery systems: push and 
pull [7]. Then we present the advantages of combining 
cache techniques with the broadcast system and the 
challenge of solving the cache consistency problem in the 
wireless networks. 
In a push-based system, data transmission is 
independent of the actual request patterns. The action of 
transmitting data is initialized by the server. Clients only 
wait for their requested data passively. Server broadcasts 
data to clients continuously and repeatedly. When a user 
requests a data item, it needs to wait until the requested 
data item is broadcast by the server. In broadcast disk 
access model [1], the server broadcasts all data items 
repeatedly. The broadcast channel can be thought of as a 
disk (broadcasting repeatedly is like disk spinning). MHs 
can get their desired data by listening broadcast channel 
in a certain time interval as shown in Figure 2. The 
fundamental optimization problem is to design broadcast 
programs for the server such that we can minimize the 
average waiting time of accessing a data item. 
In a pull-based system, the specific data item 
requested by a user can be observed by the servers. The 
action of transmitting data is initialized by the client. The 
servers are always waiting for clients’ requests and ready 
to respond the requested data to clients as shown in 
Figure 3. The servers know the exact number of pending 
requests for each data item. We first distinguish the 
differences between pull-based broadcast systems and the 
traditional on-demand pull systems. The traditional on-
demand pull scheme is a unicast communication protocol. 
The requested data is sent through the routing path 
between the source and destination. The data can only be 
received at the source. However, in the pull-based 
broadcast system, MSS sends data by wireless channel. 
The MHs in the transmission range of MSS can share the 
wireless channels and receive the broadcast data. The 
fundamental optimization problem is to design a 
scheduling algorithm for the servers so that we can 
minimize the average response time. 
The main advantage of broadcast scheme is to save 
more request uplink bandwidth and power consumption 
of MHs. When a data item (hot data) is queried by many 
Figure 1: The mobile environments. 
MSS or BS 
database 
mobile hosts
Ti-2
Cache hit IR
UIR
Ti-2,1 Ti-2,2 Ti-1 Ti-1,1 Ti-1,2 Ti 
Figure 6: Latency of cache hit in UIR scheme. 
Latency
Ti-2
Cache hit IR
Ti-2,1 Ti-2,2 Ti-1 Ti-1,1 Ti-1,2 Ti
Figure 5: Latency of cache hit in TS scheme. 
Latencycached data is valid or not. This scheme is scalable to operate invalidation process of MHs. Because MHs can 
only wake up in each IR interval to receive the IR 
message, much power consumption is saved by MHs.  
In the stateful schemes [8], server has the knowledge 
of status of cached data in MHs. When a data object is 
updated, server can immediately send the invalidation 
report messages to MHs. MHs do not need to wait for the 
next IR interval to verify the cache. Compared to the 
stateless schemes, the stateful schemes can reduce the 
latency of the data objects. But server needs to maintain 
complex data structures for the states of the MHs’ data 
cache. Also, MHs will waste power in active mode in 
order to wait for the IR message at anytime. In wireless 
networks, the power concern is the most important 
consideration. Therefore, we focus on the discussion of 
stateless schemes and briefly present two popular 
schemes as follows. 
2.3 TimeStamp (TS) 
The TS [2] algorithm is a stateless IR-based 
invalidation method. The server generates IRs and 
broadcasts them periodically at time Ti = iL (i is a time 
index; L is an IR cycle). An IR is a history that records 
the data items that have been changed in the last w 
seconds, L ≤  w. When an MH receives an IR, for each 
data item in its cache, said j, it compares the timestamp of 
j(tjc) in the cache to the timestamp of j(tj) in the 
invalidation report. If tjc ≤  tj , the data item j will be 
removed from the cache. 
MHs record a query list Qi ＝｛  j | j has been 
queried in the interval [Ti-1 ,Ti]｝.When an MH receives 
an IR, it compares tjc with tj. If some queried data items 
are valid, they are returned to the requester (i.e., tjc ≥  tj) 
and the request is removed from the query list. The 
remaining query list will be requested via the uplink after 
all actions are done. The MH also keeps a variable Tl that 
indicates the last time it received an invalidation report. If 
the difference between current report timestamp and Tl is 
bigger than w, the entire cache is dropped. 
2.4 UIR (Updated IR) 
The UIR algorithm [6] concentrates on reducing the 
waiting latency and is independent of the cache 
invalidation strategies combined with IR-based 
algorithms to deal with long disconnection problem. UIR 
uses a technique similar to the (1,m) indexing [3] to 
reduce the query latency. In this strategy, the server 
broadcasts an invalidation report every L time units. UIR 
also broadcasts a smaller version of the invalidation 
report, called Update Invalidation Report, every L/m   
time units, where m is the number of times the smaller 
reports will be broadcast in each interval. Because the IR 
contains the update history of the past w broadcast 
seconds, replicating the complete IR m times will 
consumes large bandwidth and client’s power. 
The common drawback of TS and UIR is the long 
latency when the clients query the data item and the local 
cache returns the cache hit. It needs to wait next IR 
interval to check the data consistency as Figure 5 and 
Figure 6.  
3. Proposed Dynamic Invalidation Report 
Scheme 
3.1 Motivation 
In a wireless environment, many new factors affect the 
performance of cache invalidation scheme. 
1. Limited bandwidth: The bandwidth in wireless 
networks is much less than that in wired networks. If the 
clients always want to keep the cache consistency with 
the server, the bandwidth will be consumed more due to 
the communication of the cache consistency messages. 
2. Mobility and Disconnection: When an MH moves 
out of the transmission range of the server, it will 
disconnect from the server. Thus, it can not receive any 
invalidation messages during the disconnection time. 
When an MH reconnects to server, its cache might be 
inconsistent because it loses invalidation messages sent 
by server. Therefore, cache invalidation algorithms 
should consider the problem of reconnecting operation.   
3. Power consumption: The MHs can always verify 
their cache consistency with the server at anytime. 
However, it will waste more power consumption which is 
the most importance concern in wireless networks. Thus, 
in cache invalidation scheme, reducing power 
consumption is always a goal for MHs. 
3.2 Assumptions 
We utilize the pull-based on-demand schemes and 
push-based broadcast schemes to develop a dynamic 
invalidation report scheme in order to obtain their 
performance advantages. We assume they are mixed with 
each other in our schemes. Server broadcasts hot data 
repeatedly and continuously and transmits some cold data 
on an on-demand basis if it is a pull-based service. The 
typical usage of uplink is to request data objects when 
cache miss happens. All the requests will be observed by 
the server. The server uses the requests to generate 
statistic information to schedule data items in the push 
situation. We also assume that the update operations only 
act on the server side for simplicity. The server is 
stateless but it can exactly know the number of requests 
at anytime. 
Invalidation reports are broadcast by the server as in 
TS scheme. The contents of IRs are the updated 
information for pass w seconds. And a timestamp is 
Figure 8: Operations of adjusting IR interval by server.
VHRi, VHRi-1 : defined in section 3.4 
LS : the low bound indicates the system load , 
minimum value of Ti 
Mingrep :the minimum guarantee service response 
time, high bound of Ti 
Threshold_h : high threshold of VHRi  
Threshold_l : low threshold of VHRi  
Ti: the time that server construct a IR  
( i is an integer that denotes the ith IR interval ) 
 
At interval time Ti, server decides to adjust IR 
interval or not 
  
IF (Ti < SL) or (Ti > Mingrep)   
THEN Ti+1 = Ti + Li //Not adjust next IR interval 
Return 
 
IF VHRi< = Threshold_l 
THEN  // decrease next IR interval  
Ti+1 =  Ti + f( VHRi, VHRi-1, Li )  
 
IF VHRi>  Threshold_h  
THEN // increase next IR interval 
Ti+1 =  Ti + f( VHRi, VHRi-1, Li ) 
`mechanism according to the receiving message of server 
and client.  
3.4 DIR with Adjustable interval 
Based on the early cache validation mechanism, we 
also propose an improved method to further reduce the 
latency. For a certain time interval, said L (the cycle time 
to broadcast an invalidation report), the server can 
maintain counters of the positive EREP and negative 
EREP messages (called the “missed DREP”), which are 
sent after cache miss happens. To clarify the description 
of our scheme, the valid bit of “positive EREP” is set to 
“1” and the valid bit of “negative EREP” is set to “0”. We 
define a parameter called “virtual hit ratio” (VHR) to be 
the ratio of the number of positive EREP messages to the 
total amount of three messages. This VHR parameter 
shows to the server that if the VHR is high, most of 
clients’ queries in pass L seconds can finish their process 
just by sending early validation messages. In contrast, if 
the VHR parameter is quite low, the clients’ queries have 
to wait for data coming in pass L seconds. We expect that 
the VHR parameter is high. In this case, the invalidation 
reports may not work very well because most of caches’ 
consistency is kept by EREQ/EREP messages. On the 
other hand, the low VHR indicates that usage of most of 
EREQs is actually no use, but just a waste of power 
consumption. 
A low VHR arises under two conditions: 1) The 
update rate is very high on the server. 2) The updated 
data items were queried in pass L seconds. These 
conditions impact the performance of the early validation 
mechanism much more than the high hit ratio condition 
does. Thus, we focus on these conditions and make the 
server adjusting the IR broadcast interval dynamically. 
Reducing the IR interval improves the performance 
because, for a push-based service data, a client has to 
wait for the next IR. If the next IR comes in quickly, the 
response time will also be short. The idea is 
straightforward. We decrease IR interval when the virtual 
hit ratio is low and increase it when virtual hit ratio is 
high. Under the low hit ratio situation, reducing the IR 
interval reduces the latency of cache invalidation. 
Alternatively, when hit ratio is quite high, adjusting 
interval actually also reduces the server’s load while 
keeping low latency. Note that we do not discuss how the 
server schedule IRs. The scheduling problem is out of 
scope in the paper. 
We first formally define the notations and VHR 
mentioned above: 
Vt  :  the number of “positive EREP” in the tth IR interval.  
It  :  the number of “negative EREP” in the tth IR interval.  
Mt :  the number of DREQ received by the server in the tth 
IR interval. DREQ is the data request message. 
PMt :  the number of DREQ send after cache miss in the 
tth IR interval. Thus, PMt ＝ Mt － It. 
VHRt : the virtual hit ratio in the server’s view (computed 
in the end of tth IR interval) as Equ. (1) 
VHRt  ＝ 
PMtItVt
Vt
++
 ＝ MtVt
Vt
+                     (1)                    
The virtual hit ratio indicates the performance of 
early cache validation mechanism. We expect that the 
more cache-hit data can be verified as “positive” and used 
immediately.  
Due to the affection of temporal locality, server 
references the VHRs from the recently IR intervals can 
obtain better benefit. Thus, we first let the server keeps 
two VHR value from the latest two IR intervals: VHRi and 
VHRi-1, VHRi and VHRi-1 are calculated by the server in 
the end of ith and (i-1)th IR interval respectively. In the 
end of each IR interval, the server computes VHR and 
also predicts the (VHRi+1p) for the next IR interval. The 
VHRi+1p is predicted by simply using the concept of 
exponential average as Equ. (2). 
               VHRi+1p ＝αVHRi +  (1－α) VHRi-1.                     (2) 
The parameter α controls the relative weight of recent 
hit ratio and past history in our prediction. If α＝0, then 
VHRi+1p ＝VHRi-1, and recent hit ratio does not included. 
If α＝1, then VHRi+1p ＝VHRi, and only the most recent 
hit ratio VHRi is included. In general, we can reference 
the VHR record in each past IR interval from the history 
and expand the formula Equ. (2) to Equ. (3) for VHRi+1p 
by substituting for past VHR  as follows: 
VHRi+1p ＝ αVHRi + (1 － α)αVHRi-1 + (1 － α)2αVHRi-
2+ ...+ (1－α)jαVHRi-j+ ...+ (1－α)i+1αVHR0.                    (3) 
The VHR0 is defined as a constant. Both (1－α) and α 
are less than or equal to 1, so each successive term has 
less weight than its predecessor. 
Figure 9 (a): Latency of TS scheme. 
Latency of TS scheme
6
8
10
12
14
16
18
20
5 10 20 30 40
Number of clients
Av
era
ge
 la
ten
cy
 (s
ec.
)
query rate=10
query rate=20
query rate=40
query rate=80
Figure 9 (b): Latency of UIR scheme. 
Latency of UIR scheme
6
8
10
12
14
16
18
20
5 10 20 30 40
Number of clients
Av
era
ge
 la
ten
cy
 (s
ec.
)
query rate=10
query rate=20
query rate=40
query rate=80
Figure 9 (c): Latency of DIR scheme. 
Latency of DIR scheme
2
3
4
5
6
7
8
9
10
5 10 20 30 40
Number of clients
Av
era
ge
 la
ten
cy
 (s
ec.
)
query rate=10
query rate=20
query rate=40
query rate=80
Figure 9 (d): Latency of DIR-AI scheme. 
Latency of DIR-AI scheme
2
3
4
5
6
7
8
9
10
5 10 20 30 40
Number of clients
Av
era
ge
 la
ten
cy
 (s
ec.
)
query rate=10
query rate=20
query rate=40
query rate=80
methods. When a client gets a cache hit, it needs to wait 
for an IR to validate the queried data. So, if the data is 
invalid, many clients use the uplink at the same time, and 
these messages become the server’s overhead, therefore 
increases the latency. 
Although DIR is also a constant-interval algorithm, 
it employs the early cache validation mechanism. The 
queries have not to be pended until the client receives the 
next IR, so it can reduce the latency obviously. The 
latency with query rate 10 is suddenly increasing when 
the number of clients is 20 to 30. This behavior is similar 
to that in TS and UIR schemes because the overhead of 
the server increases. In a wireless environment, the 
bandwidth will decrease when the communication signal 
becomes week. And this condition can be raised when the 
communication distant increases or channel conflicts. 
Even though that is always a barricade, the latency 
decreases. In the other situation, the latency in DIR is 
about 5~6.1 seconds.  
The performance of DIR-AI is much different from 
others. We set the low high bounds of the IR broadcast 
interval that indicate the system load and the minimum 
guarantee service response time. The low bound depends 
on the system capacity and the high bound depends on 
the service type. The low bound is set to 10 while the 
high bound is 60. By adjusting the interval, the latency is 
reduced more than the pure DIR scheme in many 
situations. 
In fact, whether an algorithm performs well or not is 
associated deeply with the value of Threshold_l and 
Threshold_h. The values of both parameters can be 
decided by experience when request and update 
information is enough. At the point when the number of 
clients is 5 and the query rate is 80, and latency time is 
just about 6.7 seconds. When there are only 5 clients and 
the query rate is quite large, the value of VHRt (defined in 
section 3) is quite absolute. That is, the sample space is 
too deficient for the server to make the interval 
adjustment perform well. Therefore, to use the interval 
adjusting scheme with 100% efficiency, a premise has to 
be made that the number of queries in an interval can’t be 
too little. But as the identity of the DIR scheme, the 
latency is improved even if the interval become large 
since the hit and valid data could be used immediately by 
early validation mechanism. 
To further evaluate the average response time, we 
make several experiments with each algorithm. The 
compared results are shown in Figure 10 (a), (b), (c), and 
(d). The simulation is made in different combinations 
where the mean query rate is 20/40. The result shows that 
the latency reduces by using early validation messages. 
When the server applies “the virtual hit ratio” to adjusting 
the interval, the performance is further improved. The 
main part of the reduced latency is the validation time in 
DIR while both the validation time and the data getting 
time is reduced in DIR with adjusting interval scheme. To 
highlight the performance difference, we choose the 
mean updated rate of the server to be only 5 and 10. That 
is, the server updates the data items in the database 
frequently during the simulation time. This decision is 
mainly for emphasizing the usage of interval adjusting 
method of the server, since the TS and UIR schemes are 
affected more deeply when the mean update rate is high. 
Actually, if the value of the “virtual hit ratio” (VHRi) is 
always stable, the performance of DIR-AI scheme will be 
close to the DIR. However, in real world, the value of 
VHRi might be unstable. Thus, the choice of Threshold_l 
and Threshold_h play an important role in the algorithm. 
The decision can be made by server’s experience 
statistics and the value can be changed dynamically 
according to the system state. To decide the Threshold_l 
and Threshold_h, we run 10 times of all algorithms that 
the update rate is 10/20/80/320/640, the query rate is 
2.5/5/10 and we get an average VHRi of about 0.52. So, 
Improved TCAM-based Pre-Filtering for 
Network Intrusion Detection Systems 
 
Yeim-Kuan Chang, Ming-Li Tsai and Cheng-Chien Su 
Department of Computer Science and Information Engineering 
National Cheng Kung University 
{ykchang, p7694157, p7894104}@mail.ncku.edu.tw 
 
 
Abstract—With the increasing growth of the Internet, the 
explosion of attacks and viruses significantly affects the 
network security. Network Intrusion Detection System 
(NIDS) is developed to identify these network attacks by a 
set of rules. However, searching for multiple patterns is a 
computationally expensive task in NIDS. Traditional 
software-based solutions can not meet the high bandwidth 
demanded in current high-speed networks. In the past, the 
pre-filtering designed for NIDS is an effective technique that 
can reduce the processing overhead significantly. A FNP-
like TCAM searching engine (FTSE) [5][6] is an example 
that uses an 2-stage architecture to detect whether an 
incoming string contains patterns. 
In this paper, we propose two techniques to improve the 
performance of FTSE that utilizes ternary content 
addressable memory (TCAM) as pre-filter to achieve gigabit 
performance. The first technique performs the w-byte suffix 
pattern match instead of using w-byte prefix. The second 
technique finds the matching results from all groups rather 
than first group. We finally present the simulation result 
using Snort pattern set and DEFCON packet traces. 
I. INTRODUCTION 
With the growth of the Internet, large number of viruses 
and malicious probes spread every day. Many network 
hosts are vulnerable to attacks. Traditionally, networks 
have been protected using firewalls that monitor and filter 
network traffic. Firewalls usually examine the packet 
headers to determine whether the packets are allowed to 
go through or dropped. For example, if a packet attempts 
to connect a disallowed port, such as port number 80 
(http), the connection is rejected. However, firewalls are 
not effective to protect network from worms and viruses. 
Today, the most commonly used defense strategy is to use 
end-host based solutions that rely on security tools, such 
as antivirus software. The main drawback of these 
approaches is the inability to protect thousands of hosts in 
less than an hour.    
Network intrusion detection systems (NIDS) are 
utilized to detect malicious attacks and protect Internet 
system. The intrusion detection systems are growing in 
popularity because they can provide an efficient protection 
against the attacks. A NIDS differs from a firewall in that 
it needs to scan both the headers and the payloads of each 
incoming packet for thousands of suspicious patterns. By 
inspecting both packet headers and payloads to identify 
attack signatures, NIDS is able to discover malicious 
attacks or hackers that intend to intrude.  
Because most of the known attacks can be represented 
with patterns or combinations of multiple sub-patterns, 
pattern matching has become a performance bottleneck in 
NIDS. Current NIDS pattern databases contain thousands 
of patterns, resulting in a difficult computational task. 
Traditionally, software-based NIDS may be overloaded 
when the packet arrival rate becomes high. To keep up 
with the high-speed networks, hardware-based NIDS 
implementation is needed.  
Snort [9] is an open source light-weight NIDS which is 
designed to filter packets by pre-defined rules. Snort 
contains a set of rules with corresponding actions. Each 
Snort rule consists of the rule header and the rule options. 
The rule header contains the action, protocol, source and 
destination IP addresses, and the source and destination 
ports. The rule options contain alert messages and 
signatures which would be inspected to determine if the 
incoming packets match the rule. A sample Snort rule that 
detects mountd access is shown in Figure 1. When the new 
viruses or malicious attacks are discovered, corresponding 
rules are added to Snort. Because of its free availability 
and efficiency, Snort is quite commonly used and there 
are very large and current databases of signatures 
maintained on the Internet.  
 
The rest of the paper is organized as follows: In section 
2, we review and summarize the related works. In section 
3, we propose two approaches to improve TCAM pre-
filter techniques. The simulation results are presented in 
section 4. Finally, we present the conclusions in section 5. 
II. RELATED WORKS 
In the past few years, several interesting algorithms and 
techniques have been proposed for multiple-pattern 
matching. Current software-based NIDS cannot meet the 
bandwidth requirement of a multiple-gigabit network. 
Hence, several hardware-based solutions have been 
proposed to solve the problem. In this section, we divide 
the common pattern matching solutions for intrusion 
detection system into four categories: software-based, 
FGPA-based, bloom filter based, and TCAM based. The 
detail of each solution will be discussed as follows. 
A. Software-based solutions 
Knuth-Morris-Pratt (KMP) [4] and Boyer-Moore (BM) 
[2] are the most well-known single-pattern matching 
alert tcp any any -> 192.168.1.0/24 111 
(content:"|00 01 86 a5|"; msg: "mountd access";)
Figure 1. Sample Snort rule 
match in TCAM, the sliding window will be shifted by w 
bytes. Figure 3 shows an example of matching process. 
For a pattern P=’ABCDEFG’, TCAM with width of four 
is organized as in Figure 2. We first search the first 4 
bytes of payload in TCAM and find a match in group G3. 
Therefore, the sliding window is shifted by one byte. By 
repeating this search process, we find a match with five 
TCAM lookups and then report the pattern ID to exact 
matching module. 
Figure 4 shows the hardware architecture of FTSE 
which consists of two parts: TCAM pre-filter module and 
exact matching module. The incoming data stream is first 
filtered through TCAM pre-filter module, which matches 
w-byte prefixes of patterns. If a match occurs in group G0, 
the corresponding ID of the partial matching pattern is 
sent to the exact matching module for performing the 
exact matching between the potential pattern and input 
data stream. The controller determines the shift value of 
sliding window according to lookup results in TCAM. 
The full patterns are stored in 'Pattern Table'. 
To verify the effectiveness of FTSE, we analyze the 
TCAM memory requirement and theoretical expected 
shift value which is defined to be the expected number of 
skipped characters per cycle. If we set TCAM width to w, 
then the (w-i)-byte prefix of each pattern is stored in group 
Gi for 0 ≤ i ≤ w – 1. Suppose we have a total of N patterns 
and the number of prefix-patterns in group Gi is Ni. The 
total number of TCAM entries is ΣNi. So the total TCAM 
memory requirement is w×ΣNi bytes.  
To calculate the average shift value, we need to 
calculate the matching probability in each group of the 
TCAM. For a random w-byte input stream in TCAM, the 
matching probability of an entry in group Gi is  
,
)2(
)( 8 iw
i
i
NGP −≅  
where the number of don’t-care bytes in Gi is i and each 
character is 8 bits The probability of mis-match in one 
TCAM lookup is  
.)(1 ∑− iGP  
Therefore, the average shift value Savg is  
( ) ( ),)()(1 ∑∑ ×+×−= iGPwGPS iiavg  
Increasing the TCAM width greatly reduces the 
probability of false positive and increases the average shift 
value. The performance of FTSE depends on the average 
shift value which is the expected number of TCAM 
lookups needed to match the sliding window against the 
patterns in TCAM. However, it results in a huge amount 
of TCAM memory. Thus, choosing w is a tradeoff 
between cost and performance.  
According to our observation, the probability of finding 
a match in group Gw-1 is greater than that in other groups. 
As we mentioned earlier, the average shift value can be 
increased by reducing the probability of finding a match in 
TCAM. On the other hand, if we can increase the mis-
match probability in one TCAM lookup that results 
shifting w bytes in the sliding window, the total number of 
TCAM lookups for packet payload should be reduced. In 
the following subsections, we provide two approaches for 
improving the efficiency of TCAM pre-filter. 
B. Suffix matching 
As stated, the don’t-care bytes in TCAM entries 
increase the probability of finding a match in TCAM. For 
example, the probability of matching an entry '***A' in 
one TCAM lookup is 1/28 which is large. The average 
shift value will decrease while the number of entries in 
group Gw-1 increases. But, in this case, the number of 
TCAM lookups will also increase before we can find a 
match in group G0 and thus we can send the partial 
matching result to the exact matching module. In 
summary, it is better to get a mis-match in TCAM as often 
as possible, so that the sliding window can skip w bytes 
and the exact matching module does not get involved. For 
this reason, our improved schemes try to avoid using the 
don’t-care bytes in TCAM as much as possible. 
Our first improved approach matches w-byte suffixes of 
patterns instead of prefix matching. We divide the TCAM 
entries into w groups from G0 to Gw-1, similar to FTSE. If 
the patterns are shorter than w, they will be stored in 
TCAM in the same manner as FTSE. Otherwise, for a 
pattern of length m denoted by P[1…m], group Gi stores 
the w-byte suffix of sub-pattern P[1…m – i] for i = 0 to 
w – 1. If the sub-pattern P[1…m – i] is shorter than w, we 
also pad it with don’t-care bytes and store it in group Gi.  
Sub-patterns that belong to the same group should be 
organized according to their lengths in descending order. 
It is because TCAM only reports the first matching result 
among multiple matches. In this way, patterns whose 
lengths are longer than or equal to 2w-1 occupy one 
TCAM entry in all groups without padding any don’t-care 
byte. Figure 5 shows an example of our improved TCAM 
layout for a pattern P=’ABCDEFG’ and TCAM width 
AAAABABCAABCDEFG 
AAAABABCAABCDEFG 
AAAABABCAABCDEFG 
AAAABABCAABCDEFG 
AAAABABCAABCDEFG match G0 
match G2, shift 2 bytes
match G3, shift 3 bytes
match G3, shift 3 bytes
match G1, shift 1 bytes
Figure 3. An FTSE example that needs five TCAM lookups.
Figure 2. TCAM layout for a pattern P=’ABCDEFG’ with TCAM 
width w=4. 
G0 ABCD 
G1 *ABC 
G2 **AB 
G3 ***A Matching 
ID
Pattern Table
Exact 
Matching 
Module  
Controller 
Buffer
TCAM
Packet 
Payload
Figure 4. The hardware architecture of FTSE. 
We can solve this problem by both increasing TCAM 
width and restricting the number of groups less than w. 
With a large w and a small number of entries in a group, 
we can greatly reduce the probability of finding a match in 
TCAM. Let w' be our new TCAM width and g be the 
reduced number of groups in TCAM. Since each pattern is 
divided into g×w'-byte sub-patterns in series, our approach 
requires g× w'×N bytes of TCAM memory approximately 
and can process g characters per TCAM lookup, where N 
is the total number of patterns. Although reducing the 
number of groups in TCAM also restricts the performance, 
the simulation results shows that our proposed scheme is 
better than FTSE. 
IV. EXPERIMENTS 
In this section, we present the experimental results of 
our two improved approaches. We utilized Snort pattern 
sets and DEFCON [10] packet traces to evaluate the 
effectiveness of the proposed algorithms and original 
FTSE.  
We used the Snort [9] version 2.4 for our experiments. 
The total number of unique patterns in Snort is 2535. The 
pattern length distribution is shown in Figure 10. The 
average pattern length is 17 bytes and the maximum 
pattern length is 364 bytes.  
We use five DEFCON8 packet traces for our TCAM 
pre-filter simulations. “RootFu!” is a contest run at 
DEFCON each year. “Capture the Capture the Flag” 
(CCTF) is a project by “The Shmoo Group” to sniff and 
log all the data on the “RootFu!” network. Our packet 
traces can be derived from the CCTF project held in 
DEFCON. Table I shows the matching statistics of these 
five DEFCON8 packet trace. The highest pattern-match 
rate in our experiments is in the packet trace 4 and the 
lowest one is in the packet trace 2. 
The throughput in our TCAM pre-filter architecture 
will be increased as the average shift value becomes high. 
It is because the total number of TCAM lookups can be 
decreased. Increasing the TCAM width significantly 
reduces the amount of false positive and increase the 
average shift value. However, TCAM memory is one of 
the more expensive components in this architecture. 
Reducing the amount of required TCAM memory is a 
major concern. Therefore, we simulate our improved 
TCAM pre-filter approaches by using several TCAM 
widths. 
Table II shows the TCAM memory size requirement 
with TCAM width 4, 8, 12, and 16. We observe that our 
suffix matching approach had less TCAM memory 
requirement when TCAM width is 12 or 16. It is because 
that we eliminate certain inclusive entries from group 1 to 
group w-1.  
For every packet trace, we measure the average shift 
values, as shown in Table III to Table VII. With the 
TCAM width increasing by four sequentially, the average 
shift value increases slowly. The results shows that our 
suffix approach had better average shift value than FTSE 
in shorter TCAM width, 4 especially. It is because the 
number of short patterns becomes low. By comparing with 
different packet traces, we can get higher throughput than 
FTSE when pattern-match rate of packet trace is low.  
Since the DEFCON traces tend to match patterns in 
nature, the pattern-match rate is higher than normal 
packets. We believe our suffix approach can be improved 
if the packet traces are normal ones instead of DEFCON 
traces.  
For our second approach with multi-character 
processing, we perform the experiments by using different 
sets of TCAM width w' and the reduced number of groups 
0
20
40
60
80
100
120
140
160
180
1 8 15 22 29 36 43 50
pattern length
nu
mb
er 
of 
pa
tte
rns
+
Figure 10. Length distribution of patterns in the Snort databases.
Trace Name Total payload size (bytes) 
# of matched 
events Percentage
Defcon8_trace1
(28153903) 148,878,041 15,110,119 10.1% 
Defcon8_trace2
(28160701) 13,434,830 781,811 5.8% 
Defcon8_trace3
(29124449) 86,117,534 6,306,690 7.3% 
Defcon8_trace4
(29142147) 78,488,243 8,972,176 11.4% 
Defcon8_trace5
(29190000) 159,480,998 11,834,997 7.4% 
Table I. DEFCON8 packet traces. 
Table IV. TCAM width impact on 
Avg. shift value for DEFCON8 
trace2 
Table V. TCAM width impact on 
Avg. shift value for DEFCON8 
trace3 
Table III. TCAM width impact on 
Avg. shift value for DEFCON8 
trace1. 
 FTSE Suffix 
4 12 14 
8 80 89 
12 218 195 
16 432 332 
Table II. TCAM memory size (KB)
 FTSE Suffix 
4 3.075 3.092 
8 4.847 4.858 
12 6.011 6.014 
16 6.796 6.796 
Table VI. TCAM width impact on 
Avg. shift value for DEFCON8 
trace4 
Table VII.TCAM width impact on 
Avg. shift value for DEFCON8 
trace5 
 FTSE Suffix 
4 2.693 2.707 
8 4.032 4.012 
12 4.744 4.712 
16 5.144 5.144 
 FTSE Suffix 
4 2.619 2.641 
8 3.859 3.842 
12 4.477 4.451 
16 4.819 4.819 
 FTSE Suffix 
4 2.965 2.985 
8 4.650 4.664 
12 5.665 5.693 
16 6.333 6.322 
 FTSE Suffix 
4 2.921 2.955 
8 4.529 4.539 
12 5.470 5.473 
16 6.061 6.062 
Multi-Character Processor Array for Pattern 
Matching in Network Intrusion Detection System   
Yeim-Kuan Chang, Ming-Li Tsai and Yu-Ru Chung  
Department of Computer Science and Information Engineering 
National Cheng Kung University 
{ykchang, p7694157, p7695101}@mail.ncku.edu.tw 
 
Abstract—Network Intrusion Detection System (NIDS) is a 
system developed for identifying attacks by using a set of 
rules. NIDS is an efficient way to provide the security 
protection for today’s internet. Pattern match algorithm 
plays an important role in NIDS that performs searches 
against multiple patterns for a string match. Pattern 
matching is a computationally expensive task. Traditional 
software-based NIDS solutions usually can not achieve a 
high-speed required for ever growing Internet attacks. In 
order to satisfy high-speed packet content inspection, 
hardware-implementable pattern match algorithm is 
required. In this paper, we propose a hardware-based 
pattern match architecture that employs a multi-character 
processor array. The proposed multi-character processor 
array is a parallel and pipelined architecture which can 
process multiple characters of the input stream per cycle. 
The proposed architecture can reduce a lot of unnecessary 
computations and thus it is power efficient. We use Snort 
pattern sets and DEFCON packet traces to perform our 
simulations. Our experiment results show that, with a 3-
character processor array, we can reduce 83% of the 
computations compared with the brute force approach. 
Keywords: intrusion detection, pattern matching, Snort, 
processor array 
I. INTRODUCTION 
With the growth of the Internet, large number of 
viruses and malicious probes spread every day. Many 
network users are vulnerable to attacks. Traditionally, 
networks have been protected using firewalls that monitor 
and filter network traffic. Firewalls usually examine the 
packet headers to determine whether or the packets are 
allowed to pass through or dropped. For example, if a 
user attempts to connect to a disallowed port, such as port 
number 80 (http), the connection is rejected. However, 
firewalls are not effective to protect networks from 
worms and viruses. Today, the most commonly used 
defense strategy is to use end-host based solutions that 
rely on security tools, such as antivirus software. The 
main drawback of these approaches is the inability to 
protect thousands of hosts in less than an hour. 
Network intrusion detection systems (NIDS) are 
utilized to detect malicious attacks and protect Internet 
system. The intrusion detection system are growing in 
popularity because they provide an efficient protection to 
attacks. The NIDS differs from a firewall in that it needs 
to scan both the headers and the payloads of each 
incoming packet for thousands of suspicious patterns. By 
inspecting both packet headers and payloads to identify 
attack signatures, NIDS is able to discover whether 
malicious attacks or hackers are attempting to intrude.  
Because most of the known attacks can be represented 
with patterns or combinations of multiple sub-patterns, 
pattern matching has become a performance bottleneck in 
intrusion detection system. Current NIDS pattern 
databases consist of thousands of patterns and thus 
searching against them is a computationally expensive 
task. Traditionally, software-based NIDS may be 
overloaded when the packet arrival rate becomes high. To 
keep up with the high-speed networks, a hardware-based 
NIDS implementation is generally needed. 
The rest of the paper is organized as follows: In section 
2, we review and summarize the related works. In section 
3, we present the proposed multi-character processor array 
design. The performance evaluation results are presented 
in section 4. Finally, we present the conclusions. 
II. RELATED WORK 
In the past few years, several interesting algorithms 
and techniques have been proposed for multiple-pattern 
matching in the content of network intrusion detection. 
Current software-based NIDS cannot meet the bandwidth 
requirement of a multiple-gigabit network. Hence, several 
hardware-based solutions have been proposed to solve the 
problem. In this chapter, we divide common pattern 
matching solutions for intrusion detection system into 
four categories: software-based, FGPA-based, Bloom 
filter based, TCAM based, and brute force approaches.  
A. Software-based solutions 
Knuth-Morris-Pratt (KMP) [8] and Boyer-Moore (BM) 
[3] are the most well-known single-pattern matching 
algorithms. Assume the length of the pattern is m and the 
length of text is n. The KMP algorithm [8] utilizes a 
precomputed table to prevent redundant comparisons. 
KMP reduces the worst case running time from O(n×m) 
to O(n+m). The precomputed table, or π-table, tells the 
automata which pattern character to match against next. 
That is, the π-table indicates that, when a mismatch 
occurs, we know which position of the pattern to continue 
the matching process without starting the matching over 
from the beginning of the pattern. The time complexity of 
BM algorithm is sub-linear of O(n/m) in the best case, 
and O(n) in the average case . However, the performance 
of the BM algorithm depends on the characters in the 
input string and the pattern. The worst-case complexity to 
find all occurrences in a text needs approximately 3×n 
comparisons regardless whether the text contains a match 
or not. Hence the worse-case time complexity is O(n×m). 
Current Snort implementations use Boyer-Moore 
algorithm because BM has the best average-case 
performance. The Aho-Corasick(AC) [1] is designed for 
multiple-pattern matching. By pre-processing the patterns 
and we build a finite state machine in AC. AC algorithm 
can process the input text in a linear time, so the 
searching complexity of the algorithm is linear in the 
Fang Yu et al.[11]proposed a TCAM-based pattern 
matching algorithm for handling both short patterns and 
long patterns. If a pattern is shorter than TCAM width, 
then we put it into TCAM and pad it with don’t care bits. 
A pattern longer than TCAM width will split into several 
sub-patterns: the first TCAM width prefix pattern and 
remaining suffix patterns. There are three data structures 
to be stored in memory for matching long patterns: 
pattern table, partial hit list, and matching table 
  To match a pattern, a window of characters from 
input string is looked up in the TCAM. Then, the result is 
stored in a temporary table. The window is moved 
forward by a character and the look up is executed again. 
At every stage, the approximate partial match table entry 
is taken into account to verify if a complete pattern 
matched 
III. PROPOSED METHOD 
A. Proposed Processing Element Design 
  In this section, we improve the brute force multi-
character architecture design by using the concept of 
processor array. In the proposed multi-character processor 
array architecture of degree n, each processing element 
(PE) can handle n characters per clock cycle. The 
proposed processor array architecture is based on the 
following observations: instead of enabling all PEs each 
of which is compared against a window of input data, we 
only enable some PEs that are necessary to obtain the final 
match signal. As a result, unnecessary comparisons are 
significantly reduced and design complexity is also 
simplified.  
B. Details of Processing Element 
Our processing element is designed to detect a match of 
n-byte sub-pattern against packet payload. There are n 
possible cases that PE should indicate a match for the 
input string by using the matching results at the current 
and last clocks. Consider the example in Figure 2 in which 
each PE of degree 3 stores pattern ‘ABC’. The initial value 
of two inputs, ‘Enable’ and ‘Match’, are set to true. There 
are three cases that PE should indicate a matching signal: 
(1) A window of input string is ‘ABC’ at the current 
clock cycle, 
(2) A window of input string are ‘BC*’ at the current 
clock cycle and ‘**A’ at the previous clock cycle, 
where ‘*’ means ‘don’t care’. 
(3) A window of input string are ‘C**’ at the current 
clock cycle and ‘*AB’ at the previous clock cycle. 
In case (1), the input string exactly matches the pattern 
‘ABC’ and then a match signal is output immediately. In 
the cases (2) and (3), the input string matches k-byte 
prefix and (m–k)-byte suffix of pattern ‘ABC’ at the 
preceding clock and at the current clock, respectively. For 
simplifying our description later, case (1) is called ‘exact 
PE match’ and the other two cases are called ‘partial PE 
match’. In order to keep the previous comparison result, 
shift registers are used to delay the signals of partial PE 
matching results by one clock cycle. 
The PE of degree n consists of n2 8-bit comparators and 
n×(n – 1)/2 SRL16 shift registers. Each n-byte pattern is 
stored in the corresponding comparators of a PE. If pattern 
is shorter than n, we assign a designated value, known as 
wildcard, to the remaining bytes. If pattern is longer than n, 
we cut it into multiple n-byte sub-patterns. It should be 
noted that the last sub-pattern may be less than n bytes.  
Figure 3 shows the block diagram of our processing 
element, which has three inputs and two outputs. A PE is 
active if and only if its input ‘Enable’ is set. The n-bit 
matching result is used to identify the n possible matching 
offsets between then-byte input string and sub-pattern.  
To illustrate how the n-bit match output is set, we assume 
that the n-byte input string is W = w1…wn and the n-byte 
sub-pattern in PE is PE = p1…pn. At clock cycle i, the bit 
k of the n-bit match output denoted by Match_out[k] is set 
to 1 if w1…wn–k =  pk+1…pn and Match_in[k] = 1 for k ∈ 
{1, …, n – 1} and Match_out[n] is set to 1 if  w1…wn =  
p1…pn and Match_in[n] = 1. Notice that the match input 
signal Match_in[k] resulting from wn–k+1…wn=  p1…pk at 
clock i – 1 was delayed one clock by the shift register and 
thus can be used at clock i.  
CLK 
Match 
Match 
Enable
Enable 
PE 
n bytes 
n bits 
Input 
string 
Figure 3. The proposed processing element. 
Enable Enable
Match
Input  
string 
8
Match 
8
8
detect k-byte 
fi h
A
A
B
B
C
C
C
B
A
 
Figure 4. Details of PE with degree 3. 
clock  
P
E 
Match  
Match 
111 
(3) 
(1) 
i-1 
A B *
* *
i
C * * 
B 
C 
C 
AB 
* (2) 
Enable 1 
Input 
string 
Figure 2. Three possible cases that PE should indicate a 
match for pattern P=’ABC’ at clock cycle i 
A 
The details of our computation saving technique working 
on processor array are described as follows: 
Given a pattern P = p1 p2 ……pm, and input text T = t1 
t2 ……ts. We assume multi-character degree of our PE is n. 
the pattern P is divided into ⎡m/n⎤  sub-patterns, and those 
sub-patterns are stored in the ⎡m/n⎤ corresponsive PEs 
(PE1, PE2, ….., PE⎡m/n⎤). The inputs ‘Enable’ and ’Pre-
Match’ of all PEs are unset except the first one before 
starting to search. In the search phase, there are four cases 
when we compare a window W = w1 w2 …..wn of input text 
against sub-pattern of PEg at clock cycle i, where we 
assume the location of w1 is b ( w1 = tb ) and  1 ≤ g ≤ ⎡m/n⎤. 
(1) Exact match:  
The window W matches sub-pattern PEg exactly, where 
w1 w2 ...wn = pg1 pg2 ...pgn. It implies that a real pattern 
match may occur if the input ‘Pre-Match’ of PEg is true 
and the subsequent m–(g×n) bytes of input string are equal 
to p(g+1)1 ...p(g+1)n,…, p(m/n)1 ...p(m/n)n. Therefore, if the input 
‘Match’ of PEg is true in this case, the outputs ‘Enable’ 
and ‘Match’ of PEg are set. On the contrary, the outputs of 
PEg are unset. 
(2) k-byte prefix match:  
The k-byte suffix of W match k-byte the prefix sub-
pattern, where w(n-k+1) w(n-k+2) …..wn = pg1 pg2 ……pgk. It 
implies that a real pattern match may occur if subsequent  
m–(g×n)+(n–k) bytes of input string are equal to pg(k+1)…pgn 
p(g+1)1 ...p(g+1)n,…, p(m/n)1 ...p(m/n)n. Because those k-byte 
matching results are delayed one clock by shift registers, 
the partial PE match may occur at next clock cycle. 
Therefore, we must enable next PE even if the input 
‘Match’ of PEg is false. 
(3) (n-k)-byte suffix match:  
The (n-k)-byte prefix of W matches (n-k)-byte suffix of 
sub-pattern, where w1 w2…wn-k = pg(n-k+1) pg(n-k+2) …pgn. 
This case is similar to case (1). It implies that a real 
pattern match may occur if the input ‘Match’ of PEg is 
true and the subsequent m–(g×n) bytes of input string are 
equal to p(g+1)1 ...p(g+1)n,…, p(m/n)1 ...p(m/n)n. Therefore, if the 
input ‘Match’ of PEg is true in this case, the outputs 
‘Enable’ and ‘Match’ of PEg are set. On the contrary, the 
outputs of PEg are unset. The difference between cases (1) 
and (3) is the final matching result will delay one clock.   
(4) Mismatch:  
If the cases (1), (2), and (3) were not discovered, a real 
pattern match will not occur from current input string. 
Therefore, the next PE is unset to save unnecessary 
computations.  
If the cases (1), (2), and (3) were not discovered, a real 
pattern match will not occur from current input string. 
Therefore, the next PE is unset to save unnecessary 
computations.  
IV. Performance Evaluation 
We evaluate the efficiency of our multi-character 
processor array architecture using two metrics: area cost 
and computation-saving rate. We used the Snort rule set 
and DEFCON8[13] packet traces for our experiments. We 
used the Snort [12] version of 2.4 for our experiments. 
The total number of unique patterns in Snort is 2535. The 
pattern length distribution is shown in Figure 7. The 
average length of patterns is 17 bytes and the maximum 
length of patterns is 364 bytes.  
 Since our PE design is scalable, we considered three 
multi-character degrees: 3, 4, and 5 to evaluate our 
architecture. As we know that increasing the multi-
character degree of PE increases both the throughput and 
area cost per PE. Thus, we can find a balance between 
performance and cost. 
A. Analysis of area cost 
In this section, we analyze the area cost by real Snort 
patterns. Suppose we have a total of k patterns, each with 
mi bytes. If we set the multi-character degree of PE is n, 
then each pattern will be cut into ⎡mi/n⎤ PEs. Each PE 
consists of n2 8-bits character comparators and n(n–1)/2 
SRL16 shift registers. Therefore, we need total ∑ ⎡mi/n⎤×
n2 comparators and ∑ ⎡mi/n⎤×n(n–1)/2 shift registers in 
our architecture. 
Our Snort rule set contains 2535 unique patterns and 
total 44,416 characters. The average length of patterns is 
17 bytes and the maximum length of patterns is 364 bytes. 
For our experiments, the value of n should be smaller than 
six to achieve the best benefit of cost and performance. If 
we use 3 as multi-character degree of PE, we need total 
15,733 PEs in our entire system and have seven PEs on 
0
20
40
60
80
100
120
140
160
180
1 8 15 22 29 36 43 50
nu
m
be
r o
f p
at
ter
ns
pattern length
Figure 7. Distribution of the string lengths 
in the Snort database 
n=3 n=4 n=5
Computation 
reduction rate 83% 78% 74% 
Table 3. Computation reduction rates for different multi-
character degree. 
 Trace 1 Trace 2 Trace 3 Trace 4 Trace 5
Computation 
reduction rate 83% 83% 83% 83% 83% 
Table 2. Computation reduction rates of various traces
with n = 3. 
 n=3 n=4 n=5 
# of comparators per PE 9 16 25 
# of shift registers per PE 3 6 10 
# of PEs required for Snort 15,733 12,020 9,882 
Avg # of PEs for a pattern 6.2 4.7 3.9 
Table 1. Area cost for different multi-character degrees.
