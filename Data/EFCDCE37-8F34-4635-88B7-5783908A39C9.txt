整合資料，將整合後的資料提供給使用者
進行資料探勘。但是隨著資訊科技的進
步，使得資料量成長迅速、資料變化快速、
即時需求提高的資料串流(Data Streams)環
境興起。在資料串流環境下，串流資料
(Stream Data)不同於以往的資料集，它有著
快速變化、大量、無窮盡、無法完全儲存
等特性。因此，有許多研究針對串流資料
的特性提出新的序列樣式探勘技術[10, 21, 
25, 35] ，透過這些技術能夠提供使用者即
時的探勘結果。 
近年來，越來越多的新興應用開始朝
向多重資料串流(Multiple Data Streams)的
監測，希望藉由探勘多重資料串流做更進
一步的分析。例如：在加護病房裡，針對
病患使用的醫療儀器都會產生資料串流，
經由分析這些串流資料將有助於醫生更全
面瞭解病人身體狀況。因此，在多重資料
串流環境下進行探勘是當前資料探勘研究
的新議題之ㄧ。 
多重資料串流序列樣式探(Sequential 
Pattern Mining in Multiple Data Streams)的
應用相當廣泛，舉例來說，在股市方面，
我們可以將微軟、奇摩、Google 等不同股
票輸入的漲跌資料各視為一個資料串流，
並以一天為週期探勘不同股票的漲跌相對
發生的順序，尋找是否微軟的漲停會在同
一時間或不同時間帶動奇摩與Google上漲
的序列樣式。但是，在搜尋多重資料串流
探勘相關文獻後，發現在關聯規則、分類、
分群等技術上已有不少研究，卻在序列樣
式探勘技術上鮮少找到相關研究。因此，
本研究計畫針對多重資料串流序列樣式探
勘之技術與應用進行研究，希望不僅能夠
提出效能佳的探勘技術更能實際應用到現
實世界中。 
關於多重資料串流序列樣式探勘技術
之研究，Oates and Cohen [32] 提出的
MSDD (Multi-Stream Dependency Detection)
演算法是用來尋找在不同串流之中，特定
事件發生於固定時間範圍內的規則。然而
Chen et al. [14] 認為MSDD尋找的規則只
能算是序列樣式的一個特殊案例，所以提
出MILE (Mining in Multiple Streams)演算
法來尋找完整的序列樣式探勘結果。然而
MILE演算法是以一次性(One-time Fashion)
的探勘方式來對一段期間內的多重資料串
流進行探勘，因此，當有新的串流資料輸
入時，它只能針對新的串流資料進行探
勘，而無法將新舊探勘結果整合以得到更
精確的探勘結果。針對上述問題，本研究
計畫提出ICspan (Incremental Mining of C-
losed Sequential Patterns in Multiple Data 
Streams)演算法，以漸進的方式不斷探勘封
閉序列樣式，希望能夠提供更精確的探勘
結果並減少探勘過程的空間與時間耗費。 
然而在多重資料串流環境中，隨著現
實資料串流環境的變化，其資料量將變得
龐大並且資料複雜度將會越來越高，但是
MILE演算法和ICspan演算法只能處理單一
時間單一資料並不足以因應多重資料串流
環境的變化，另一方面，這兩個演算法所
探勘出來的序列樣式可能會存在於不同的
串流中但被視為同一個串流之序列樣式。
針對上述問題，本研究計畫提出IAspam 
(Incremental Across-streams Sequential Pat-
terns Mining)演算法，以因應更複雜的多重
資料串流環境。它能夠處理單一時間多個
資料，並且能夠探勘出跨串流 (Across 
Streams)序列樣式。它將串流資料轉換成位
元映射方式進行探勘，希望能夠更進一步
提昇探勘效能。 
由於股票資料具有網路公開、數量龐
大、更新快速等特點，所以無疑是具有代
表性與實用性的應用。由於影響股價的因
素眾多，使得股票市場難以預測，有些分
析因加入過多複雜及無法客觀量化的資訊
 2
形就如同重複掃描相同部分的資料。所以
Yan et al. [41] 提出CloSpan (Closed Se-
quential Pattern Mining)演算法來解決此問
題。CloSpan演算法是利用PrrefixSpan演算
法的投射方式產生頻繁序列樣式，並在投
射的過程中以雜湊表(Hash Table)來記錄探
勘出的封閉(Closed)序列樣式。所以當有頻
繁序列樣式產生時，可透過搜尋雜湊表來
判斷此頻繁序列樣式是否為封閉序列樣
式。由於頻繁長(Long)序列所包含序列之組
合數量相當多，導致探勘過程的空間與時
間大量消耗。因此，利用探勘封閉序列樣
式的概念來減少消耗。雖然此方式找出的
序列樣式會減少許多，但仍然可由封閉序
列樣式推導出所有的頻繁序列樣式，使得
探勘結果不會失去正確性。 
到目前為止所提到的演算法，都是所
謂的一次性探勘的方式，意指探勘整個序
列資料庫(Sequence Database)並從中獲得
探勘結果。如有新資料加入必須重新探
勘，才能得到正確的探勘結果。可是大部
分的應用中，序列資料庫是漸進式
(Incremental)地更新資料，使得一次性的序
列樣式探勘演算法不適用於這種情況。在
序列資料庫的應用中更新序列資料的方法
有兩種。第一種是插入(INSERT)模式，意
指插入新的序列。此模式有一個重要的特
性，就是由原來的 (Original)序列資料庫
(Denoted as DB)與新序列資料(Denoted as 
Δdb) 所 組 成 的 新 序 列 資 料 庫
(DB’=DB∪Δdb)裡的頻繁序列樣式，必須
在 DB或 Δdb或DB’是頻繁的。第二種是附
加(APPEND)模式，是指附加項目集或項目
到原有的序列之中。這個模式比前一種模
式較為複雜，因為此模式不僅可能在Δdb
裡有附加項目(Appended Items)所產生的新
局部地(Locally)頻繁序列樣式，也可能有局
部地非頻繁序列與在DB內相同之非頻繁
序列的支持度相加而產生的頻繁序列樣
式。實際的應用可能結合兩種方法更新序
列資料庫，但Cheng et al. [15]提出的
IncSpan (Incremental Mining of Sequential 
Patterns)演算法將插入模式視為附加模式
的特例。也就是把插入的序列當作交易
(Transactions)附加到原來的序列資料庫的
空序列(Empty Sequence)之中，把問題簡化
成只處理附加模式所可能發生的情況。
IncSpan演算法為使漸進過程更有效率提出
幾個新想法：(1)維持(Maintaining)一個如同
候選序列的接近頻繁的(Almost Frequent)
序列集合，這裡使用緩衝儲存半頻繁樣式
(Buffer Semi-frequent Patterns)的概念。此概
念用一個緩衝比例(Buffer Ratio)的參數將
接近頻繁的序列樣式儲存起來。因為他們
認為漸進的更新過程中，大多數的頻繁樣
式都是由半頻繁樣式轉變而成，所以透過
事先記錄這些樣式在漸進的過程中可以減
少重新產生樣式的時間。(2)反向序列比對
(Reverse Pattern Matching)用於比對序列樣
式與修剪(Prune)搜尋空間。(3)投射分享
(Shared Projection)被設計用來減少一些序
列的投射。有了這些概念，令IncSpan演算
法更有效率與適應性(Scalable)。 
除了前面所談到的幾個序列樣是探勘
演算法之外，亦有其他的序列樣式探勘演
算法被提出，如Aseervatham et al. [5]提出
的bitSPADE演算法、Chang et al. [12]所提
出的FSPAN演算法、Chen et al. [13]提出的
多階層序列樣式探勘、和Wang et al. [39, 40]
所提的BIDE演算法、…等。 
 
2.2. 資料串流探勘 
資料串流環境與傳統資料庫不同之處
在於資料會隨著時間不斷地產生，也因為
資料量的龐大，資料庫無法無限制地儲存
資料。資料串流具下列特性[25]： 
 4
再適用的(Useful)舊資訊。 
同年 Chen et al. [14]提出 MILE 演算法
來改良[19, 31]兩篇論文的缺點，此演算法
是處理一段週期內的多重資料串流資料。
在時間序列資料串流環境中，他們將輸入
的資料視為一串連續且有時間順序的標記
(Token)。標記中包含了串流辨識號碼、時
間點、數值，且每一個資料串流皆是由相
同串流識別號碼的標記所組合而成的，例
如：股市交易中，串流識別號碼可能是一
支股票的編號；醫療方面，串流識別號碼
可能是一個醫療儀器的編號。另外，在多
重資料串流中同一時間點內的所有標記都
是同步(Synchronously)記錄，並且針對這些
同步記錄的多重資料串流，進行跨資料串
流的序列樣式探勘。而 MILE 是以
PrefixSpan 為基礎改良而成，它的作法是透
過記錄已產生的序列樣式，並利用雜湊
(Hash)方式來加快搜尋，使得演算法的效率
提高。但因為它是以 PrefixSpan 為基本架
構並記錄產生的序列，因此導致過多記憶
體的消耗。此外，MILE 屬於一次性探勘演
算法，所以無法保留之前探勘結果。 
接著 Raissi et al. [35] 提出 SPEED 
(Sequential Patterns Efficient Extraction in 
Data Streams)演算法來尋找資料串流中的
最大(Maximal)序列樣式。此演算法不僅提
出新的資料結構來維護頻繁序列樣式，還
加入快速的刪減策略，令使用者可以在任
何時間尋找任意(Arbitrary)時間區段(Time 
Interval)的最大序列樣式。 
然而Ho et al. [25] 提出的IncSPAM 
(Incremental Mining of Sequential Patterns 
using SPAM)演算法不同於之前所提之方
法，它是利用漸進式的方式探勘資料串流
中的序列樣式。在此演算法中不僅利用位
元映射作法來計算序列樣式的支持度，還
加入CBASW (Customer Bit-vector Array 
with Sliding Window)的觀念，以提升計算
序列樣式的支持度與序列樣式順序的速
度。此外，為了處理過期(Out-of-date)資料
所導致的誤測(False Positive)問題，也改良
Chang and Lee [9] 所 提 出 的 遞 減 率
(Decay-rate)概念，作為遞減機制來判斷資
料的重要性，令演算法能適應資料串流探
勘經常遇見的概念移轉(Concept Drift)問
題。 
 
三、 研究方法 
  在本節中，我們分別針對多重資料串
流序列樣式探勘和多重資料串流之跨串流
序列樣式探勘做詳細的方法說明與分析，
並建置一個行動序列樣式探勘系統，將所
提出之演算法運用至該系統中。 
 
3.1 多重資料串流序列樣式探勘 
3.1.1 SPAMDAS 研究架構 
 
圖 4 SPAMDAS 的整體流程 
由於之前關於多重資料串流序列樣式
探勘的研究無法保留先前的序列樣式探勘
結果，會導致探勘的結果不夠精確。因此，
本研究計畫提出 SPAMDAS (Sequential 
 6
以三個時間點的長度當作序列樣式的週期
長度，並將移動框架內的時間點切割為長
度相同的三段，希望透過比對不同基礎框
架的資料找出頻繁序列樣式。因此，在探
勘一個移動框架的過程中，首先我們視一
個基礎框架為一個客戶的交易資料，而同
一時間點的資料會被當作一筆交易資料，
如圖 7 所示。接著將用圖 6 與圖 7 的兩個
範例圖示進行說明。圖 6 中，基礎框架內
的T1、T2、T3三個格子與圖 7 上方的T1、
T2、T3 三個時間點相對應，而圖 7 中左方
的S1、S2、S3 則代表不同的串流。基礎框
架的每一個時間點T都是由不同串流S輸入
的數值所組合而成的集合，如同前面所述
我們會將這個集合視為一筆客戶交易資
料。另外，基礎框架裡的三個時間點，所
代表的是時間的順序性。這些順序性加上
時間點內的資料，我們可以把基礎框架視
為一個客戶的交易序列資料，例如圖 7 中
第一個基礎框架的序列資料表示為
<(11,33,1) (55,86,81) (7,8,22)>。透過比對在
同一個移動框架中，不同基礎框架內所含
的序列資料，從中尋找出頻繁的序列樣
式。我們設定最小支持度為 100 % (mini-
mum support = 100%)，所以在這個移動框
架裡找出的頻繁序列樣式為 <(11,33) 
(22)>。 
 
圖 7 基礎框架範例 
 
3.1.4 漸進式探勘 
除了使用移動框架模式取樣外，在多
重資料串流環境探勘精確的序列樣式，還
需面臨串流資料無法永久保存導致探勘結
果失去精確性的問題，因此需要導入漸進
式探勘的觀念。我們可以將之前所取樣過
的資料視為先前的資料庫(以 SW 表示)，而
當新的移動框架取樣資料(以 SW’表示)進
來時，先對 SW’進行序列樣式探勘並利用
漸進式的方式與 SW 的結果作整合。但是
因為多重資料串流的資料量過於龐大，為
了避免在序列樣式探勘的階段，由於序列
辭彙樹(Lexicographical Sequence Tree)的分
支及節點(Node)產生過多，造成消耗記憶
體空間與搜尋序列辭彙樹的時間的問題，
我們也融入探勘封閉序列樣式的概念，利
用雜湊表作為封閉序列樣式之索引，以提
高建立及維護序列辭彙樹的效率。 
Definition 1 (序列樣式支持度)：假設 α為一
個序列樣式，α的支持度等於包含 α的交易
的個數除以所有交易的個數。 
Definition 2 (頻繁序列樣式)：假設 α為一個
序列樣式，α為頻繁序列樣式之充分必要條
件為α的支持度要大於或等於最小支持度。 
Definition 3 (半頻繁序列樣式)：假設 α為一
個序列樣式，α為半頻繁序列樣式之充分必
要條件為 α 的支持度要小於最小支持度，
並大於或等於緩衝比例 r 乘以最小支持度。 
Definition 4 (頻繁封閉序列樣式)：假設 α
為一個序列樣式，α為頻繁封閉序列樣式之
充分必要條件為 α 為頻繁序列樣式且不存
在任何一個超序列樣式其支持度和α相同。 
Definition 5 (半頻繁封閉序列樣式)：假設 α
為一個序列樣式，α為半頻繁封閉序列樣式
之充分必要條件為 α 為半頻繁序列樣式且
不存在任何一個超序列樣式其支持度和 α
相同。 
在漸進式更新封閉序列樣式的過程
中，會因為加入新的資料而影響原來的序
列樣式探勘結果，可能會出現下列情況： 
1. 頻繁封閉序列樣式依然是頻繁封閉序
列樣式。 
 8
z SF 為 SW 之中所找到的半頻繁封閉序
列樣式，而 SF’為新找到的半頻繁封閉
序列樣式。 
z supsw(p)為序列樣式p在SW的支持度。 
z supsw’(p)為序列樣式p在SW’的支持
度。 
z supappend(p)為序列樣式p在所有被附加
項目或項目集(itemsets)的序列的集合
中的支持度。 
ICspan 演算法的步驟分為兩個部份。
第一部分是新序列樣式的尋找(圖 8 的 line 
1-4)。這個部分是要找出新資料裡的封閉序
列樣式，所以對長度為 1 的序列樣式進行
投射，以找出新的頻繁封閉序列樣式。第
二部分則是新舊序列樣式的整合(圖 8 的
line 5-13)。這個部分是對第一部分找到的
新樣式與先前記錄之樣式進行投射，以尋
找是否會有新舊項目組合而成的序列樣
式。以下是 ICspan 演算法詳細步驟說明： 
Step 1 (line 1)：設 F’與 SF’為空集合。 
Step 2 (line 2)：掃描此次移動框架的取樣資
料，從中找出長度為 1 且支持度大於
或等於 min_sup 或 r*min_sup 的序列
樣式，並將結果分別加入到 F’或 SF’。 
Step 3 (line 3-4)：對於所有在 F’ or SF’中長
度為 1 的新序列樣式進行投射，轉換
為封閉序列樣式。 
Step 4 (line 5-11)：對於之前所記錄的頻繁
或半頻繁序列樣式做漸進地更新與投
射，轉換為封閉序列樣式。 
Step 4.1 (line 6)：判斷序列樣式 p 在 SW 的
支持度與在 SW’的支持度相加是否大
於或等於最小支持度，假如判斷為
是，代表其為頻繁序列樣式並執行步
驟 4.2 與 4.3。 
Step 4.2 (line 7)：判斷序列樣式 p 是否包含
其它序列樣式或被其它序列樣式所包
含，並進行更新以確保記錄的皆為頻
繁封閉序列樣式。 
Step 4.3 (line 8-9)：判斷序列樣式 p 在所有
被附加項目或項目集的序列樣式集合
中 的 支 持 度 是 否 大 於 或 等 於
(1-r)*min_sup，如果是則表示可能有非
頻繁序列樣式變成頻繁序列樣式，則需
進行投射產生封閉序列樣式。 
Step 4.4 (line 10-11)：判斷序列樣式 p 在 SW
的支持度與在 SW’的支持度相加是否
小 於 min_sup ， 並 大 於 或 等 於
r*min_sup。假如是則表示此序列樣式 p
為半頻繁序列樣式，並且進行更新以確
保記錄的皆為半頻繁封閉序列樣式。 
Step 5 (line 12)：將 F’存回 F；SF’存回 SF。 
Step 6 (line 13)：輸出 F 與 SF。 
 
圖 9 第一次執行 ICspan 的取樣資料 
以下將用範例來說明 ICspan 演算法。
首先，我們設定 、 。
圖 9 顯示第一次執行 ICspan 的取樣資料。
步驟 1 會先設定兩個空集合 F’和 SF’來儲
存這次所找到的新序列樣式。步驟 2 是對
於移動框架新取樣的資料尋找長度為 1 的
頻繁或半頻繁序列樣式，在圖 9 的範例中
所找到頻繁序列樣式為<A>、<a>、<3>，
半頻繁序列樣式為<f>、<d>、<6>，並將找
到的序列樣式分別加入到 F’或 SF’之中。
步驟 3 會針對 F’或 SF’中長度為 1 的新序
列樣式進行投射，轉換為頻繁封閉序列樣
式 <(A, a)(3)> 與半頻繁封閉序列樣式
<(6)(3)>、<(A, a)(F)>、<(A, a)(d, 3)>。步驟
4 的部份由於沒有之前探勘的序列樣式，所
以會直接跳到步驟 5 更新 F 與 SF，最後步
驟 6 輸出 F 與 SF。表 1 顯示第一次執行
100%min_sup = 5.0=r
 10
  在多重資料串流環境中，隨著現實資
料串流環境的變化，資料量將變得龐大並
且資料複雜度將會越來越高，但是現有的
演算法只能處理單一時間單一資料(Item)
並不足以因應多重資料串流環境的變化。
然而基於商業因素考量，業者可能需要找
到不同地點所產生的串流之間的關聯性以
提供使用者更完善的服務。 
因此，本研究計畫提出ASPAMDAS 
(Across-streams Sequential Pattern Mining in 
Multiple Data Streams)方法來解決這幾個
問題。ASPAMDAS主要分為資料取樣與漸
進式探勘兩個階段。首先，在資料取樣階
段中，利用移動框架模式對串流資料取
樣，取樣的資料量以框架大小為單位。然
後，在漸進式探勘階段中，將移動框架中
的顧客交易資料轉換為位元映射來尋找跨
串流序列樣式，以提升探勘效率。接著再
針對新舊跨串流序列樣式進行更新或刪除
以產生精確的跨串流序列樣式探勘結果。
圖 11 為ASPAMDAS的整體流程。 
 
3.2.2 多重資料串流環境 
ASPAMDAS 所採用的多重資料串流
環境描述如下： 
z 每一個資料串流在一個時間點會產生
一筆顧客交易資料。 
z 資料串流中的顧客交易資料包括顧客
序號與交易資料。 
z 顧客交易資料的先後順序以資料進入
時間點區分。 
z 同一個時間點的各個串流資料會被同
步處理。 
z 多重資料串流的資料量比單一資料串
流更為龐大。 
z 一個串流集合表示為S＝{s1, s2, ..., 
sj, ..., sm}，m表示串流的數量，  
(
js
mj ≤≤1 )代表一個資料串流。 
ASPAMDAS 所採用的多重資料串流
環境架構引用於 MILE[14]，在現實應用中
相當普遍，例如：股市交易、天氣預測、
行動通訊等。以行動通訊環境為例，如圖
12 所示，每個使用者會在不同的地點發送
服務訊號，而使用者所做的服務請求皆會
藉由基地台(Base Station)傳送至處理器。每
個地點所傳出的資料都被視為是一個資料
串流，處理器將有可能同時接受到多個資
料串流，因此便可同時在這些資料串流中
探勘序列樣式與移動路徑。 
 
圖 12 多重資料串流環境之應用範例 
 
3.2.3 資料取樣 
 
圖 13 資料串流之移動框架位移 
在資料串流環境中，由於即時性知識
需求提高和資料串流特性的限制，相對
地，越新的資料便顯得越重要，因此我們
使用移動框架來擷取新的串流資料，並將
處理完的舊串流資料丟棄。如圖 13 所示，
移動框架只會保留最新 N 筆的交易資料，
 12
必要條件為 β是頻繁的串流序列。 
圖15顯示我們所提出的漸進式跨串流
序列樣式探勘演算法(稱之為 IAspam 演算
法)，演算法的參數定義如下: 
Algorithm: IAspam 
Input ： SW’, SW, min_sup, CID, S, I, 
k-item, CS, L-tree 
Output：FASP, StP 
1. For each transaction data of SW’ do  
     store into CS as <S, I> and order by 
CID; 
2. For each I of CS do  
     turn into bit-vector matrix; 
update time point column of k-item; 
3. For each 1-item do 
 add 1-item and supSD(1-item) into 
L-tree ; 
 If supSD(1-item) ≧ min_sup then 
 generate CP with I-step and S-step 
and map to bit-vector matrix; 
4. For each CP do  
     If supsw’(CP) ≧ min_sup then 
       If CP exists in L-tree then  
update supsw(CP); 
       else add FASP and StP into L-tree; 
5. For each FASP in L-tree do 
     If sup(FASP) not updated then 
       decay sup(FASP) in L-tree; 
       If sup(FASP) ≦ min_sup then 
eliminate FASP; 
6. return; 
圖 15 漸進式跨串流序列樣式探勘演算法 
z SW’為目前移動框架的資料，而 SW 為
之前移動框架的資料。 
z min_sup 為最小支持度。 
z S 為串流編號、I 為項目集、CID 為顧
客編號、SD 為串流資料。 
z k-item 為項目名稱為 k 之項目。 
z CP 為候選序列。 
z CS 為顧客序列表，用來暫時儲存顧客
資料。 
z supsw(p)為序列樣式p在SW的支持度、
supsw’(p)為序列樣式p在SW’的支持
度。 
z supSD(p)為序列樣式p在SD的支持度。 
z L-tree 為序列辭彙樹。 
z FASP 為頻繁跨串流序列樣式、StP 為
串流序列。 
z sup(FASP)為頻繁跨串流序列樣式在
L-tree 的支持度。 
IAspam 演算法的步驟分為三個部份。
第一部份是轉換成位元映射表示法(圖 15
的 Step 1-2)，將顧客交易資料轉換成位元
映射矩陣。第二部份是找出新資料裡的跨
串流序列樣式(圖 15 的 Step 3)，對 1-item
進行 I-step與S-step以產生候選序列並對映
至位元映射矩陣中。第三部份是新舊序列
樣式的整合(圖 15 的 Step 4-5)，對第二部分
找到的新樣式與先前記錄之樣式進行比
對，以更新序列樣式，並且遞減未被更新
的序列樣式支持度。以下是 IAspam 演算法
詳細步驟說明： 
Step 1：將框架內的顧客交易資料儲存至顧
客序列表，並以顧客編號排序，資
料格式為<S, I>，S為串流編號，I 為
項目集。 
Step 2：將顧客序列表轉換成顧客位元映射
矩陣，判斷每個時間點的項目集是
否包含 k-item，如果是則將位元 1
儲存至 k-item 中與項目相同時間
點的欄位，否則儲存位元 0。 
Step 3：計算每個 1-item 在同一個串流中的
支持度，並將 1-item 及其支持度儲
存至 L-tree 中。判斷 1-item 在各串
流中的支持度是否大於或等於
min_sup，如果是則利用 I-step 與
 14
Support, w_sup)方程式(3)為序列樣式的支
持度乘上權重值所得到的。當跨串流序列
樣式的加權支持度小於最小支持度時便會
被刪除。 
    supsup_ ×= weightw     (3) 
以下將以範例說明IAspam演算法，表 5
顯示第一次執行IAspam的取樣資料(W1)以
顧客編號(CID)排序而成的顧客序列表。顧
客序列表是一個暫存資料表，每個顧客的
交易序列資料皆是依據流入的時間順序來
排序。由於移動框架大小設為 3，因此會擷
取資料串流中三個時間點的顧客交易資
料，以<Si,α>表示，其中Si為串流編號，α
為項目集。表 5 中的<S1,(a,c)>即表示(a,c)
來自於S1 串流。在CID為 2 的第一欄以及
第三欄的空集合即表示此顧客分別在這兩
個時間點沒有交易行為。 
表 5  W1 之顧客序列表 
CID 顧客交易序列 
1 <S1, (a,c)>    <S3, (b,d)>   <S2, 
(c,d)> 
2 <∅>        <S2, (c,d)>    <∅> 
3 <S2, (b,c,d)>  <S1, (a,c,d)>  <S3, 
(b,c,d)> 
4 <S3, (b,c)>   < >        <S1, 
(a,b,c)> 
∅
表 6  W2 之顧客序列表 
CID 顧客交易序列 
1 <S3, (b,d)>  <S2, (c,d)>    <S2, 
(a,c,d)> 
2 <S2, (c,d)>   <∅>        <∅> 
3 <S1, (a,c,d)>  <S3, (b,c,d)>  <S1, 
(c,d)> 
4 < >        <S1, (a,b,c)>  <S3, 
(a,d)> 
∅
在探勘完 W1 的取樣資料後，移動框
架會往前移動一個時間點，此時在 W2 框
架中可擷取到一筆新的串流資料。如表 6
所示，在 CID 為 1、3、4 的顧客交易序列
中 分 別 新 增 <S2,(a,c,d)> 、 <S1,(c,d)> 與
<S3,(a,d)>，這三筆顧客交易資料是 W2 所
擷取的新串流資料，而 CID 為 2 的顧客交
易序列中未出現新的顧客交易資料，即表
示這位顧客在該時間點沒有任何交易行
為。 
圖 16 顯示 W1 之顧客序列表所轉換成
的位元映射矩陣，位元映射矩陣中的 a、b、
c、d 代表各個項目，CID=1 代表顧客 1。
在顧客 1 中，項目 a 欄位的(1,0,0)表示表 1
中的顧客交易序列所轉換出來的位元列表
(Bit List)。位元列表的第一個 1 代表第一個
時間點有出現項目 a 的交易資料，第二個 0
和第三個 0 則代表第二、三個時間點沒有
出現項目 a 的交易資料。在圖 16 中顯示在
CID=1 下方的串流記錄<S1,S3,S2>表示第
一個時間點的交易資料來自於 S1、第二個
時間點的交易資料來自於 S3、第三個時間
點的交易資料來自於 S2。 
 
圗 16  W1 之位元映射矩陣 
將顧客序列表轉換成位元映射矩陣之
後，我們必須先計算 1-項目的支持度，以
產生候選序列。如圖 17 所示，其為設定在
min_sup=2 的情況下，所計算出的每個 1-
項目在各個串流中的支持度。c 項目的支持
度在 S1 中為 3、在 S2 中為 2、在 S3 中為
2，皆大於或等於 min_sup，因此 c 項目在
 16
 ∅
 
圖 20 W1 之序列辭彙樹範例 
∅
 
圖 21 W2 之序列辭彙樹範例 
在圖 21 中，方框標示部分為被更新後
的支持度，表示該跨串流序列樣式在 W2
中亦為頻繁，因此支持度由 3 更新為 2。圓
圈標示部分為加權後的支持度，在
decay-rate 設定為 0.9 的情況下，支持度由
2 遞減為 1.8，因此該跨串流序列樣式在支
持度小於 min_sup 的情況下會被刪除，連
結(Link)以虛線表示。 
表 7 IAspam 第一個移動框架的執行結果 
頻繁跨串流序列樣式 串流序列 
<(a,c)(b,d)> <(S1)(S3)> 
表 8 IAspam 第二個移動框架的執行結果 
頻繁跨串流序列
樣式 串流序列 
<(a,c)(d)> <(S1)(S3)> 
 
3.3 股票探勘系統 
3.3.1 系統架構 
本節將介紹本研究的系統架構(如圖
22)。首先收集所需的資料，進行前置處理
後；再將資料執行資料清潔與轉換後，成
為我們所需的資料串流，接著執行多重資
料串流序列樣式探勘，以 IAspam 和 ICspan
兩個演算法，進行序列樣式探勘，產出封
閉序列樣式及跨串流序列樣式，以此來預
測股價走勢及個股間漲跌關係，最後針對
所完成的系統，進行系統評估。 
 
3.3.2 樣本選取 
 
圖 22 系統架構圖 
本研究以台灣股市為研究對象，所選取
的股票為台灣證券交易所裡台灣50指數的
這 50 檔股票，台灣 50 指數涵蓋臺灣證券
市場中市值排名前50大的台灣龍頭上市公
司做代表，其與大盤的關連性高達九成，
 18
差異，當預測值為 a (大漲)，實際為 b (小
漲)時，差異為 1，最大差異為 4，加總預
測與實際間的差異值，做為評估預測精確
度的基礎。 
表 8 狀態欄位說明 
值 意義 說明 
a 大漲 漲幅超過 3% 
b 小漲 漲幅 3% 以內 
c 平盤 今日收盤價同前一日收盤價 
d 小跌 跌幅 -3% 以內 
e 大跌 跌幅超過 -3% 
表 9 轉換後的資料串流 
名稱  同證券代碼  
說明  每一檔股票即為  一個資料串流  
序號 欄位名稱 資料型態 
1 證券代碼 nvarchar(255) 
2 年月日 datetime 
3 昨收(元) float 
4 今收(元) float 
5 股價漲跌 float 
6 百分比 float 
7 狀態 float 
 
圖 26 轉換後的資料串流示意圖 
 
表 10 預測某股票未來股價走勢示意圖 
探勘結果 股價走勢 
<(b)(b)(a)> 小漲Æ小漲Æ大漲 
<(d)(d)> 小跌Æ小跌 
表 11 探勘不同股票間的關聯示意圖 
探勘結果 各股關聯 
<(S1,S3)> 
<(d,b)(a)> 
S1 股票小跌Æ小漲後 
S3 股票會大漲 
 
四、 績效評估 
   此章節共分為三個部份，第一部份為
ICspan 演算法之結果比較與分析，第二部
份是 IAspam 演算法之結果比較與分析，第
三部份是股票探勘系統結果與分析。 
 
4.1 ICspan 演算法 
我們是在 Visual Studio 2005 的編譯環
境下，使用 C++ STL 來撰寫程式。所有的
實驗都在表 12 描述的實驗環境下執行。實
驗資料方面，我們使用 SQL Sever 2005 提
供的 data generator，產生均勻分佈(Uniform 
Distribution)與高斯分佈(Gaussian Distribu-
tion)的 synthetic datasets。表 13 是人造資料
產生參數的描述。舉例來說，以 S9T200V4
的表示方式是指串流數量為 9 個、時間點
的資料有 200 筆、每一個串流資料的值域
為 4 個值的多重資料串流環境。 
表 12 實驗環境 
項目 規格內容 
處理器 Pentium D 3 GHz 
記憶體 1 GB 
硬碟 80 GB 
作業系統 WINDOWS XP 
程式語言 C++ STL 
 
 20
0
500
1000
1500
2000
2500
3000
6 7 8 9 10
min_sup (%)
nu
mb
er 
of 
seq
ue
nti
al
pa
tte
rns
ICspan MILE
 
圖 28 序列樣式數量的比較 
 
(二) ICspan 演算法的分析 
第二部分的實驗是對於 ICspan 演算法
在不同環境與參數的測試，我們利用不同
的條件來瞭解演算法的執行效能，例如：
資料量、基礎框架長度、資料分佈等條件。 
(1)資料量 
0
50
100
150
6 7 8 9 10
min_sup (%)
tim
e (
s)
S9T200V4 S9T2000V4 S9T20000V4
 
圖 29 各種資料量的執行時間 
圖 29 與 圖 30 顯 示 我 們 在
S9T20000V4、S9T2000V4、S9T200V4 三
個不同的多重資料串流環境下，執行
ICspan 的執行時間與記憶體使用量比較。
圖 29 的縱軸為執行時間以秒為單位，圖 30
的縱軸為記憶體的最大使用量以 kilobytes 
(KB)為單位。由圖 29 與 30 中，我們發現
ICspan 當最小支持度低於 7%時執行效能
較差，但是在其餘最小支持度的執行效能
都還算良好。其原因可能是符合條件的序
列樣式數量過於龐大，令轉換封閉序列樣
式時間拉長。 
0
500
1000
1500
2000
2500
6 7 8 9 10
min_sup (%)
me
mo
ry 
us
ag
e (
KB
)
S9T200V4 S9T2000V4 S9T20000V4
 
圖 30 各種資料量的記憶體使用量 
 (2)基礎框架長度 
圖 31 顯示我們在 S9T2000V4 的多重
資料串流環境中，以不同基礎框架長度執
行 ICspan 的執行時間比較。圖中的折線
BW = 3 是表示 basic window 長度等於 3，
其餘折線同此類推。圖 26 顯示 BW = 3 的
執行時間小於 BW = 5 與 BW = 7，而 BW = 
5 在最小支持度 8%以上時的執行時間略高
於 BW = 7。前者的原因為基礎框架長度越
長會加長序列樣式令探勘時間增加，而後
者卻是因為基礎框架長度改變，而導致序
列資料轉換為封閉序列樣式的時間縮短。 
0
50
100
150
200
6 7 8 9 10
min_sup (%)
tim
e (
s)
BW = 3 BW = 5 BW = 7
 
圖 31 基礎框架長度 
 (3)資料分佈 
圖 32 顯示我們在 S9T2000V4 的多重
資料串流環境下，以高斯分佈與均勻分佈
兩種資料分佈執行 ICspan 的執行時間比
較。圖中顯示在最小支持度大於或等於
25%時的執行時間皆能接受，而當最小支
持度低於 25%時，高斯分佈的執行時間會
隨著最小支持度不斷地降低出現急遽上升
的現象。其原因為高斯分佈的資料出現機
率不同所造成。 
 22
02
4
6
8
0.01 0.02 0.03 0.04 0.05 0.06
min_sup (%)
Ti
me
 (s
)
IAspam T-Map-Mine
 
圖 33 各種支持度之執行時間比較 
(2) 記憶體使用量 
圖 34顯示 IAspam與T-Map-Mine在記
憶體使用量的比較，環境設定與執行時間
比較相同。縱軸為記憶體最大使用量以
kilobytes (KB)為單位。在各種最小支持度
下，IAspam 的記憶體使用量與 T-Map-Mine
相當，其原因為 IAspam 的記憶體使用量會
受到項目總類個數與顧客量的影響，而
T-Map-Mine則會受到顧客資料的雜亂程度
而影響 T-Map-Tree 分支程度，資料越亂則
樹會越大。 
0
200
400
600
800
1000
1200
0.01 0.02 0.03 0.04 0.05 0.06
min_sup (%)
M
em
or
y u
sa
ge
 (K
B)
IAspam T-Map-Mine
 
圖 34 各種支持度之記憶體使用量比較 
 
(二) IAspam 演算法的分析 
第二部分的實驗是對於 IAspam 演算
法在不同環境與參數的測試，我們利用不
同的條件來瞭解演算法的執行效能，例
如：時間點個數、框架大小、顧客量、項
目種類個數等條件。 
(1) 時間點個數 
    圖 35 顯 示 我 們 在 S6T100I3 、
S6T1000I3、S6T10000I3 三個不同的多重資
料串流環境下，執行 IAspam 的執行時間比
較。圖 35 的縱軸為執行時間以秒為單位。
在圖 35 中，我們可以發現當最小支持度小
於 0.03 時，T10000 的執行效能較差。其原
因可能是處理更大量資料時，由於支持度
較低，必須處理更多的跨串流序列樣式，
因此便會花費較多的執行時間。 
0
100
200
300
400
500
600
0.02 0.03 0.04 0.05 0.06 0.07 0.08
min_sup (%)
Ti
me
 (s
)
S6T100I3 S6T1000I3 S6T10000I3
 
圖 35 各種時間點個數的執行時間 
(2) 框架大小 
0
20
40
60
80
100
0.02 0.03 0.04 0.05 0.06 0.07 0.08
min_sup (%)
Ti
me
 (s
)
W=6 W=9 W=12
 
圖 36 各種框架大小的執行時間 
  圖 36 顯示我們在 S6T1000I3 的多重資
料串流環境中，以 W6、W9 與 W12 執行
IAspam 的執行時間比較，其中 W 代表框
架大小。圖 36 的縱軸為執行時間以秒為單
位。在圖 36 中，我們可以發現 W6 的執行
時間效能優於 W9 與 W12，W9 的執行時
間效能優於 W12。其原因可能是框架越小
所處理的資料量較少，雖然移動次數多於
較大的框架，而可能會花費較多的移動時
間，但結果顯示執行時間的多寡是取決於
框架的處理資料時間而非框架的移動時
間，因此越大的框架會花費越多的執行時
間。 
(3) 顧客量 
圖 37 顯 示 我 們 在 S6C50I3 、
S6C100I3、S6C500I3 三個不同多重資料串
 24
1.05
1.07
1.09
1.11
1.13
1.15
1.17
1.19
60 90 120 150 180 210 240 270 300 330
框架大小：天
差異總和
預測次數
 
圖 40 各框架大小之精確度 
  由圖 40 得知，採用 210 天期精確度較
高，接著我們以表 17 期間的資料對個股走
勢進行預測，細看該期間分年月的精確度
(如圖 41)，來瞭解整個期間中，精確度的
改變。結果顯示探勘初期精確度較差，但
經由漸近式更新後，精確度即慢慢提升。 
表 17 資料範圍 
探勘期間  框
架 訓練資料  測試資料  
210 2007/11/19~2009/07/28 2008/09/23~2010/05/31
0.9
1
1.1
1.2
1.3
1.4
200
8/0
9
200
8/1
1
200
9/0
1
200
9/0
3
200
9/0
5
200
9/0
7
200
9/0
9
200
9/1
1
201
0/0
1
201
0/0
3
201
0/0
5
年/月
差異總和
預測天數
 
圖 41 分年月之精確度 
(3) 分年比較 
  採用 210 天期，透過分年比較來瞭解
各時期預測精確度(如圖 42)，是否會因為
不同年度期間，而影響其精確度。結果顯
示精確度約落在 1.06 上下，精確度並沒因
不同年度而有明顯的差異。 
1.03
1.06
1.09
1.12
1.15
1.18
1998 2000 2002 2004 2006 2008 年
差異總和
預測次數
 
圖 42 各時期(分年)之精確度 
 
(二) 不同股票間漲跌關係 
(1) 執行時間 
圖 43 顯示不同天期的執行時間。橫軸
為不同框架大小以天為單位，縱軸為時間
以秒為單位；框架愈大，需處理的資料量
愈多，所需的執行時間亦跟著增長，表示
成本也跟著增加。 
0
20
40
60
80
100
120
60 90 120 150 180 210 240 270 300 330
框架大小：天
執行時間
(秒)
 
圖 43 不同框架大小之執行時間 
(2) 精確度 
1.06
1.08
1.1
1.12
1.14
1.16
60 90 120 150 180 210 240 270 300 330
框架大小：天
差異總和
預測次數
 
圖 44 各框架大小之精確度 
不同天期精確度整理(如圖 44)。橫軸
為不同的框架大小以天為單位，縱軸為差
異總和÷預測天數；愈低表示差異愈小，精
 26
55
167 178
69
0
50
100
150
200
ICspan IAspam 連漲天數 規則配對
執行時間
(秒)
 
圖 48 不同方法之執行時間 
(2) 精確度 
各方法之精確度整理(如圖 49)。縱軸
為平均誤差；愈低表示差異愈小，精確度
愈高。ICspan 與 IAspam 平均誤差稍優於連
漲天數與規則配對。 
1.06 1.07
1.9
1.69
0
0.5
1
1.5
2
ICspan IAspam 連漲天數 規則配對
平均誤差
 
圖 49 不同方法之精確度 
 
五、結論與建議 
本研究計畫提出了 ICspan 演算法，以
漸進的方式不斷探勘序列樣式來解決
MILE 探勘演算法 (Chen, Wu and Zhu, 
2005)，無法保留先前的序列樣式探勘結
果，導致探勘的結果不夠精確的問題。這
個概念在序列樣式越多的清況下越能減少
記憶體空間的耗費，讓 ICspan 演算法可以
提供更精確的序列樣式結果給使用者參考
與決策，ICspan 演算法能夠有效減少序列
樣式的記錄進而降低記憶體使用量，以及
在資料不斷輸入的情況下能夠維持良好的
探勘效率。 
基於商業因素考量，業者可能需要找
到不同串流之間的關聯性以提供使用者更
完善的服務，先前的研究只能處理單一時
間單一資料，並不足以因應多重資料串流
環境的變化，所探勘出來的序列樣式可能
會存在於不同的串流中但被視為同一個串
流之序列樣式。IAspam 演算法與一般多重
資料串流序列樣式探勘演算法的差異在
於，它是探勘介於串流之間的跨串流序列
樣式，而跨串流序列樣式是一種新類型的
序列樣式。 
最後建置一個股票探勘系統，運用多
重資料串流技術可處理大量資料、即時動
態產出分析結果的特色於系統中，將每一
檔股票視為一個資料串流，在不同股票間
進行多重資料串流序列樣式探勘。在以歷
史股價預測未來走勢的部份中，結果顯示
框架大小 210 天精確度最高，耗用成本又
相對低；在不同股票間漲跌關係中，瞭解
關係密切及差異極大的個股，根據產業輪
動的狀況，決定投資標的物。用以探勘股
價資料，能幫助投資人掌握即時股市行
情，增加獲利機會。 
 
參考文獻 
1. 葉明威，“應用資料探勘技術分析股
市漲跌型態之研究＂，中山大學資訊
管理研究所碩士論文，2002 年。 
2. C.C. Aggarwal, J. Han, J. Wang, P.S. Yu, 
“A Framework for Clustering Evolving 
Data Streams,” in Proceedings of the 
29th International Conference on Very 
Large Data Bases, Vol. 29 , Berlin, 
Germany, September 2003, pp. 81-92. 
3. C.C. Aggarwal, J. Han, J. Wang, P.S. Yu, 
“On Demand Classification of Data 
Streams,” in Proceedings of the 10th 
ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data 
Mining, Seattle, WA, USA, August 2004, 
pp. 503-508. 
4. R. Agrawal and R. Srikant, “Mining 
Sequential Patterns,” in Proceedings of 
the 11th International Conference on 
Data Engineering, Taipei, Taiwan, ROC, 
March 1995, pp. 3-14. 
 28
pp. 59-66.  
17. B. Dai, J. Huang, M. Yeh, M. Chen, 
“Adaptive Clustering for Multiple 
Evolving Streams,” IEEE Transaction 
on Knowledge and Data Engineering, 
Vol. 18, No. 9, December 2006, pp. 
1166-1180 
18. B. Dai, J. Huang, M. Yeh, M. Chen, 
“Clustering on Demand for Multiple 
Data Streams,” in Proceedings of the 4th 
IEEE International Conference on Data 
Mining, Washington, DC, USA, No-
vember 2004, pp. 367-370. 
19. G. Das, K.-I. Lin, H. Mannila, G. Ren-
ganathan, and P. Smyth, “Rule Discov-
ery from Time Series,” in Proceedings 
of the 4th International Conference of 
Knowledge Discovery and Data Mining, 
New York, USA, August 1998, pp. 
16-22. 
20. P. Domingos and G. Hulten, “Mining 
High-Speed Data Streams,” in Proceed-
ings of the 6th ACM SIGKDD Interna-
tional Conference on Knowledge Dis-
covery and Data Mining, Boston, MA, 
USA, August 2000, pp. 71-80. 
21. C.I. Ezeife and M. Monwar, “SSM: A 
Frequent Sequential Data Stream Pat-
terns Miner,” in Proceedings of the 
2007 IEEE Symposium on Computa-
tional Intelligence and Data Mining, 
Hawaii, USA, April 2007, pp. 120-126. 
22. V. Ganti, J. Gehrke, R. Ramakrishnan, 
“Demon: Data Evolution and Monitor-
ing,” IEEE Transactions on Knowledge 
and Data Engineering, Vol. 13, No. 1, 
January 2001, pp. 50-63. 
23. C. Giannella, J. Han, J. Pei, X. Yan and 
P.S. Yu, “Mining Frequent Patterns in 
Data Streams at Multiple Time 
Granularities,” in Proceedings of the 
NSF Workshop on Next Generation 
Data Mining, Marriott, Inner Harbor, 
Baltimore, November 2002, pp. 
191-210. 
24. J. Han, J. Pei, B. Mortazavi-Asl, Q. 
Chen, U. Dayal, and M.-C. Hsu, 
“Freespan: Frequent Pattern-Projected 
Sequential Pattern Mining,” in Pro-
ceedings of the 6th ACM SIGKDD In-
ternational Conference on Knowledge 
Discovery and Data Mining, Boston, 
Massachusetts, USA, August 2000, pp. 
355-359. 
25. C.C. Ho, H.F. Li, F.F. Kuo and S.Y. Lee, 
“Incremental Mining of Sequential Pat-
terns over a Stream Sliding Window,” in 
Proceedings of the 6th IEEE Interna-
tional Conference on Data Mining - 
Workshops, Hong Kong, China, De-
cember 2006, pp. 677-681.  
26. G.. Hulten, L. Spencer, and P. Domingos, 
“Mining Time-changing Data Streams,” 
in Proceedings of the 7th ACM SIGKDD 
International Conference on Knowledge 
Discovery and Data Mining, San Fran-
cisco, California, USA, August 2001, pp. 
97-106. 
27. S. Ju and C. Chen, “MMFI: An Effec-
tive Algorithm for Mining Maximal 
Frequent Itemsets,” in Proceedings of 
2008 International Symposiums on In-
formation Processing, Moscow, Russia, 
May 2008, pp. 144-148. 
28. H.F. Li, C.C. Ho, F.F. Kuo, S.Y. Lee, “A 
New Algorithm for Maintaining Closed 
Frequent Itemsets in Data Streams by 
Incremental Updates,” in Proceedings of 
 30
tions on Knowledge and Data Engi-
neering, Vol. 19, No. 8, August 2007, pp. 
1042-1056. 
41. X. Yan, J. Han, and R. Afshar, 
“CloSpan: Mining Closed Sequential 
Patterns in Large Datasets,” in Pro-
ceedings of 2003 SIAM International 
Conference Data Mining on Data Min-
ing, San Francisco, CA, USA, May 
2003, pp. 438-457. 
42. J. Yang, W. Wang, PS. Yu, J. Han, 
“Mining Long Sequential Patterns in a 
Noisy Environment,” in Proceedings of 
the 2002 ACM SIGMOD International 
Conference on Management of Data, 
New York, USA, June 2002, pp. 
406-417. 
43. C. Yang and J. Zhou, “HClustream: A 
Novel Approach for Clustering Evolv-
ing Heterogeneous Data Stream,” in 
Proceedings of the 6th IEEE Interna-
tional Conference on Data Mining - 
Workshops, Hong Kong, China, De-
cember 2006, pp. 682-688. 
44. M. Yeh, B. Dai, M. Chen, “Clustering 
over Multiple Evolving Streams by 
Events and Correlations,” IEEE Trans-
actions on Knowledge and Data Engi-
neering, Vol. 19, Issue 10, January 2007, 
pp. 1349-1362. 
45. M.J. Zaki, “Efficient Enumeration of 
Frequent Sequences,” in Proceedings of 
the 7th International Conference on In-
formation and Knowledge Management, 
Bethesda, Maryland, USA, November 
1998, pp. 68-75. 
46. Y. Zhu and D. Shasha, “StatStream: 
Statistical Monitoring of Thousands of 
Data Streams in Real Time,” in Pro-
ceedings of the 28th International Con-
ference on Very Large Data Bases, 
Hong Kong, China, August 2002, pp. 
358-369. 
 32
Incremental Mining of Closed Sequential Patterns in Multiple Data
Streams
Ching-Ming Chao *, Wei-Ting Chen
Department of Computer Science and Information Management, Soochow University,
56, Kueiyang St., Sec. 1, Taipei 100, Taiwan
*Corresponding Author: chao@csim.scu.edu.tw
ABSTRACT
Sequential patterns mining searches for the relative sequence of events, allowing users
to make predictions on discovered sequential patterns. The application of the tech-
nique is considerably prevalent among commercial transactions, meteorology and
health care…etc. Due to IT progress in recent years, data has changed rapidly, growth 
in the amount of data explodes and real-time demand increase, leading to a so-called
data stream environment. In this environment, data cannot be fully stored and inepti-
tude in traditional mining techniques has led to the emergence of data streaming min-
ing technology. With application of this mining technology, a database mining which
could not store massive amount of data can even provides users with real-time mining
results.
Multiple data streams are a branch off the data stream environment. In the study of
multiple data streams, sequential pattern mining is still one of the many important is-
sues. Nonetheless, the previously proposed MILE algorithm from the study has a li-
mitation to preserving the previous minding sequential pattern when a new data is en-
tered due to the concept of one-time fashion mining. To address this problem, we
propose an ICspan algorithm to continue mining sequential patterns through an in-
cremental approach and to acquire a more accurate mining result. In addition, due to
the algorithm’s constraint in closed sequential patterns mining, the generation and
records for sequential patterns will be reduced, leading to the reduction of memory
usage and to effectively increase execution efficiency.
Keyword: Multiple Data Streams, Data Stream Mining, Sequential Pattern Mining,
Closed Sequential Patterns, Incremental Mining
1. Introduction
Due to widespread use of the network, increasing hardware processing speed, and
expanding disk storage capacity, the amount of data stored on the computer and net-
work is increasing enormously. Therefore, how to discover useful knowledge from
large amounts of data becomes very important for the businesses. The objective of
data mining is just to discover knowledge from large amounts of data, which enables
more accurate mining result and to reduce special and time consumption in the mining
process. Nonetheless we have the following difficulties to overcome during the study
process. First of all, the data amount in multiple data streams environment is more
enormous relative to a single data stream, while the more sequential patterns will also
been generated. Under the limitation for memory capacity, an effective processing for
unnecessary sequential patterns is required. As a result, under the data stream envi-
ronment, data could not be stored indefinitely, and to assure continuous data entry,
preserve the accuracy of sequential pattern mining results.
The remainder of this paper is organized as follows. Section 2 analyzes some relevant
studies. Section 3 explains the study methods, with multiple data streams, data sam-
pling and ICspan algorithm will also be reviewed. Section 4 focuses on the experi-
ment and analysis on ICspan algorithm. Section 5 concludes this paper and suggests
some directions for future work.
2. Related Work
This section will review on relevant literature starting from introducing traditional
sequential pattern mining algorithm to sequential patterns mining algorithm in data
stream environment, providing description and analysis on these mentioned algo-
rithms.
2.1 Sequential Pattern Mining
The concept of sequential mining was proposed by Agrawal and Srikant (1995) in
1995. Despite such easy-to-understand ApriorAll algorithm, the result was also quite
inefficient. Consequently Srikant and Agrawal (1996) proposed GSP (Generalized
Sequential Patterns) mining approach to improve weakness from AprioriAll and to
increase sequential pattern mining efficiency, while providing a better scalability in
the linear extension of data sequence number. Later, Zaki (2001) proposed SPADE
(Sequential Pattern Discover Using Equivalence Classes) to search sequence patterns
through lattice-theoretic approach. In comparison to GSP solving calculation of fre-
quent sequential patterns by using repeated scanned database and complex hash
structure, SPADE algorithm uses the characteristics of combination to divide the ini-
tial problem into smaller sub-problems, allowing issues to be applied with lattice
search techniques efficiently and solved by simple join operations, whereas only 3
times scanning is required to discover all sequential patterns.
Despite the higher efficiency resulted from the previously mentioned method, a mas-
sive amount of candidate sequences is required to be examined, resulting in a bottle-
neck particularly when encountering a large sequential database, whereas there is a
considerable number or length of sequential patterns mining. In response to this prob-
sequence within the DB. The actual application might combine the two methods to
update sequence database. However Cheng et al. (2004) proposed IncSpan (Incre-
mental Mining of Sequential Patterns) algorithm to take INSERT mode as an excep-
tion to APPEND mode. Simply put, taking inserted sequences as transactions and to
append into the empty sequence of the original sequence database, while simplifying
the problem into processing possible scenarios. To further streamline the incremental
process, the IncSpan algorithm is brought up with several new ideas: (1) Maintaining
an almost frequent se quence set similar to a candidate sequence, applying with a
concept of buffer semi-frequent patterns. This concept implies storing the almost fre-
quent sequential patterns through the parameters of a buffer ratio. Due to most fre-
quent patterns have been transformed by semi-frequent patterns in a incremental up-
dating process, through pre-recording of these sequential patterns in the incremental
process will reduce the time to re-generate patterns. (2) Reverse pattern matching is
used to cross match sequential patterns and pruning searched space. (3) Shared pro-
jection is designed for reducing some sequential projections. Hence, IncSpan algo-
rithm becomes more effective and scalable on the basis of these concepts.
2.2 Data Stream Mining
Data stream environment differs from traditional data base in that data will be gener-
ated throughout time, whereas due to the huge scale of data, database could not store
data infinitely. Data stream consists of the following attributes (Ho et al., 2006):
Unbounded size of input data.
Usage of main memory is limited.
Input data can only be handle once.
Fast arrival rate.
System cannot control arrival ordering for data.
Analytical result is instantaneously generated upon user demand.
Deviation for analyzed result must be covered within the user tolerance range.
Due to the one-pass feature of data stream, we need to note on how to sample data. If
we use the wrong sample mode, a data failing to meet demand becomes a noise to in-
terfere with the analysis result and will lead to a lower mining efficiency. The use of
different temporal spans in data stream will have different results, thus we will discuss
the features of different temporal spans (Zhu and Shasha, 2002):
Landmark window mode: This mode will define a certain timepoint as landmark,
proceeding mining process based on the latest data of the current timepoint from
the landmark, as referred to figure 1.
ciation Rule, Classification and Clustering, as well as sequential pattern mining.
First of all, the eISeq method proposed by Chang and Lee (2005) will be discussed.
This method could catch the latest change for the sequential data stream in a short
time frame; while through a decaying mechanism to gracefully discarding the
non-useful old data.
In the same year, Chen et al. (2005) propose the MILE algorithm to implement weak-
nesses from the two papers (Das, Lin, Mannila, Renganathan and Smyth, 1998; Oates
and Cohen, 1996), which was used to process multiple data streams within a time
cycle. In the time-series data stream environment, input data is considered a series of
consecutive and time ordering token. Tokens include stream identification number,
timepoint, and values, while each data stream is composed by tokens from the iden-
tical stream identification number. For example, in stock market, stream identification
number may act as a stock identification; while in the medical treatment, stream iden-
tification number could represent the model number of a medical instrument. In addi-
tion, all tokens for multiple data streams at the same timepoint are all synchronously
recorded, and the sequential pattern mining across data stream will be conducted in
response to these multiple data streams in synchronized records. While MILE is re-
formed method based on PefixSpan, raising efficiency for algorithm through record-
ing already generated sequential patterns and expediting searching by hash method.
However due the algorithm records generated sequences based on the fundamental
framework of PrefixSpan, therefore resulting in a high consumption of memory. In
addition, due to MILE is a one-time fashion mining algorithm, previously mining re-
sults could not be preserved.
Subsequently, Raissi et al. (2006) proposed SPEED (Sequential Patterns Efficient Ex-
traction in Data Streams) algorithm to search for the maximal sequential pattern in the
data stream. This algorithm maintains frequent sequential patterns based on new data
structure in addition to augmenting prompt decaying strategies, allowing users to
search for the maximal sequential pattern in arbitrary time interval at any given time.
Nonetheless, Ho et al. (2006) proposed IncSPAM (Incremental Mining of Sequential
Patterns using SPAM) algorithm which differs from previously mentioned methods in
which it applies incremental method for mining sequential patterns from the data
stream. In this algorithm, bitmap representation is used for calculating the supports of
sequential patterns, in addition to implementation through the concept of CBASW
(Customer Bit-Vector Array with Sliding Window), in order to raise the supports for
calculating sequential patterns and the speed of sequential pattern ordering. Further-
more, to process the false positive problems from out-of-date data, the concept of de-
cay-rate proposed by Chang and Lee (2004) will be improved and used to determine
the importance of data through a decay mechanism, allowing algorithms to accom-
Stream
Data
Stream
Data
Stream
Data
Sliding Window
Sampling
Basic Window
Division
Closed sequential
pattern mining for
new data
Combination and
update for new and
old closed sequential
patterns
Multiple Data Stream Environment
Data
Sampling
Incremental
Mining
Closed Sequential
Patterns
Figure 3. Overall process of SPAMDAS.
SPAMDAS uses multiple data streams environment that in real world is quite normal.
For examples, in the stock market where the input stock fluctuation information for
different stocks such as Microsoft, Yahoo and Google could be taken as a data stream,
as referred to figure 4. Then we based on a season cycle, mining the relative occurring
sequence of the fluctuation of different stocks, subsequently search if sequential pat-
tern for the fluctuation of Microsoft stock driving the rising of Yahoo and Google
stocks at the same or different time.
the timepoint of sliding window in three equal lengths, in anticipation of discovering
frequent sequential pattern through matching different basic window. Consequently, in
the process of mining a sliding window, we first take a basic window as transactions
of one customer, while the data at the same timepoint will be treated as a transaction
data, as showed in figure 6. The proceeding figure 5 and 6 will show the procedures.
In figure 5, the three lattices, T1, T2, and T3 correspond with the three timepoints, T1,
T2, and T3 in figure 6, while the center left S1, S2, and S3 each represents a different
stream. Every timepoint T in the basic window is a set composed of input values from
different streams, equal to the previous set we take as a transaction of one customer.
In addition, the three timepoints in basic window represents the time ordering. With
these orderings joined with data at the timepoint, we could think of the basic window
as transactions of one customer, for example, in figure 6, the sequence data in the ba-
sic window are represented as<(11,33,1) (55,86,81) (7,8,22)>. Thereupon through
matching the sequence data of different basic windows in the same sliding window,
frequent sequence patterns will be searched. We will set the minimum support =
100%, therefore the frequent sequence pattern discovered in this sliding window will
become <(11,33) (22)>.
S1
S2
S3
T1 T2 T3 T6T5T4 T7 T8 T9
11 55 7
33
1
86
81
8
22
355411
33
36
27
22
25
245 62 22
11
33
9 45
326
Time
Stream
Basic Window
( transactions of one customer)One Transaction
Figure 6. A basic window model.
3.3 Incremental Mining
In addition to sampling through sliding window models, mining accurate sequential
pattern in multiple data streams environment will face with stream data unable to be
preserved permanently, leading to a loss of accuracy, whereas the concept of a incre-
mental mining is introduced. We could take the previously sampled data as the pre-
vious database (represented as SW), and on that occasion of a new entry from a slid-
ing window data sampling (represented as SW’), SW’ wil be processed with sequen-
tern.
9. Non-frequent closed sequential pattern becomes semi-frequent closed sequential
pattern.
In situations 1 though 4, due to our preservation of sequential pattern information
from the lexicographical sequence tree, we could modify the preserved information
directly for any update in new data. In situation 5 and 6, the support of sequential pat-
tern is smaller than the support of semi-frequent sequential pattern, will delete the se-
quential pattern information from the lexicographical sequence tree. In situation 7
where new data bringing new entry, in other words the sequence data not exist in lex-
icographical sequence tree, consequently will find out the new data when scanned. In
situation 8, notice that for a non-frequent closed sequential pattern to become a fre-
quent closed sequential pattern, all of the previously existing sub-sequential patterns
of this sequential pattern must be frequent sequential pattern and without any ultra
sequential pattern. Simply put, if one of the sub-sequential pattern is non-frequent in
the lexicographical sequence tree then this sequential pattern does exist. Situation 9 is
similar to situation 8 whereas for a non-frequent closed sequential pattern to become a
semi-frequent closed sequential pattern, all of the previously existing sub-sequential
patterns in the lexicographical sequence tree must be frequent or semi-frequent closed
sequential patterns without containing any super sequence. Otherwise the sequence
will not be accounted for a semi-frequent closed sequential pattern.
Figure 7 shows the parameter definitions for the incremental closed sequential pattern
mining algorithm (as known as ICspan algorithm) as the following:
SW’ is the data for the curent sliding window, whereas SW is the data of the
previous sliding window.
r is the buffer ratio(0 r 1), used for determining the number of
semi-frequent closed sequential patterns.
min_sup is the minimum support.
F is the frequent closed sequential patern in SW, whereas F’ is the newly found 
frequent closed sequential pattern.
SF is the semi-frequent closed sequential patern found in SW, whereas SF’ is the 
newly discovered semi-frequent closed sequential pattern.
supsw(p) is the support for sequential pattern p in SW.
supsw’(p) is the support for sequential patern p in SW’.
Sup(p) is the support for sequential pattern p.
supappend(p) is the support for sequential pattern p in the sequential set of all
appended items or itemsets.
Algorithm: ICspan
Step 4.2 (line 7): Determine if sequential pattern p includes other sequential patterns
or is included under other sequential pattern. Update to assure recorded se-
quential patterns are frequent closed sequential patterns.
Step 4.3 (line 8-9): Determine if the support for sequential pattern p in all appended
items or item sets in the sequential pattern set is larger or equal to
(1-r)*min_sup. If the support is determined to be true, then it is possible that a
non-frequent sequential pattern has becomes a frequent sequential pattern,
which requires projection in order to generate closed sequential patterns.
Step 4.4 (line 10-11): Determine if the support for sequential pattern p in SW added
with support in SW’ is smaler minimum support and larger or equal to the 
r*min_sup. If the support is determined to true then this sequential pattern p is
a semi-frequent sequential pattern and requires update to assure the recorded
sequential patterns are all semi-frequent closed sequential patterns.
Step 5 (line 12):Store F’ back to F; store SF’ back to SF.
Step 6 (line 13): Output F and SF.
The following example will be used to explain the ICspan algorithm. First of all, we
will set 100%min_sup and 5.0r . Figure 8 will show data sampling from the
first execution of ICspan. Step 1 wil set the two empty sets, F’ and SF’, to store the 
newly found sequential pattern. Step 2 searches for new data sampling from the slid-
ing window which are frequent or semi-frequent sequential patterns with length = 1.
The frequent sequential patterns found in the example of figure 8 are <A>,<a> and
<3> the semi-frequent sequential patterns found in the example are <f>,<d> and <6>,
then add each of the found sequential paterns to F’ or SF’. Step 3 wil project the al 
of new sequential paterns in F’ or SF’ with length = 1 and transform into frequent 
closed sequential patterns <(A, a)(3)> and semi-frequent closed sequential patterns
<(6)(3)>, <(A, a)(F)>, and<(A, a)(d, 3)>. Due to lack of previously mined sequential
patterns in Step 4, the step will go straight to Step 5 to update F and SF. Lastly Step 6
output F and SF. Table 1 shows the result of the first ICspan execution.
S1
S2
S3
T1 T2 T3 T6T5T4 T7 T8 T9
A C F
a
1
b
2
d
3
DFA
a
5
e
6
d
36 3 4
A
a
E B
fc
Time
Stream
Figure 8. Data sampling of the first ICspan execution.
Frequent closed sequential patern F’ < 3>
Semi-frequent closed sequential pattern
SF’
<a> <D> <f> <e> <(A, 3)(6)>
Table 3. Results F’ and SF’ for second execution to step 4.3.
Frequent closed sequential patern F’ <3>
Semi-frequent closed sequential pattern
SF’
<a> <D> <f> <e> <(A, 3)(6)>
Table 4. Final output results.
Frequent closed sequential pattern F <3>
Semi-frequent closed sequential pattern
SF
<A> <a> <6> <(A, a)(3)>
4. Performance Evaluation
4.1 Experimental Environment and Data
The program has been written by C++ STL in the Visual Studio 2005 compiling envi-
ronment. All experiments have been conducted in the experimental environment de-
scribed in Table 5. We used the data generator from SQL Server 2005 to generate
synthetic datasheets a Uniform distribution and Gaussian distribution. Table 6 de-
scribes the generated parameters of the synthetic data. For example, S9T200V4 is ex-
pressed by 9 stream numbers and 200 data at timepoint, with the value range for each
data stream as much as 4 times more from the multiple data streams environment.
Table 5. Experimental environment.
Item Specification
Processor Pentium4 3.00GHz
Memory 1 GB
Hard Drive 80 GB
O.S WINDOWS XP
Programming Language C++ STL
length for the sequential pattern set derived from every ICspan algorithm. By adding
the total length of sequential patterns, the higher the value means better accuracy.
Figure 10 shows the comparison of accuracy for ICspan and MILE. The vertical axis
from the figure indicates the total amount of the sequential pattern length whereas the
horizontal axis indicates the minimum support is represented by percentage. The
MILE line represents the total amount of sequential pattern lengths found by MILE,
whereas ICspan line represents the sequential pattern set derived from ICspan with the
sequential pattern lengths corresponding to MILE. Figure 10 shows under various
minimum supports, the total amount of sequential pattern lengths found by ICspan is
larger than that of MILE. Because ICspan will integrate the results of the new and old
sequential pattern mining by using a incremental method, the mining result of ICspan
will be more accurate.
Figure 10 Comparison of Accuracy
(2) Number of Sequential Patterns
Figure 11 shows the comparison of the number of generated sequential patterns for
MILE and ICspan in S9T2000V4 environment. The min_sup from the figure indicates
the minimum support is represented by percentage while sequential patterns represent
the number of generated sequential patterns. In order to compare with MILE, the buf-
fering parameter r is set to 1 for ICspan algorithm. Under various minimum supports,
the recorded sequential pattern amount for ICspan is much less than that of MILE.The
reason for the difference lies on ICspan looking for closed sequential patterns only,
leading to the reduction of many undesired sequential patterns.
Figure 12. Execution time for various data size.
Figure 13. Memory usage for various data size.
(2) Basic Window Length
Figure 14 shows the comparison of the ICspan execution time for various base win-
dow lengths in the multiple data streams environment in S9T2000V4. The line BW =
3 indicates the basic window length is equal to 3, whereas the same indication applies
to the other lines. Figure 14 indicates the execution time for BW = 3 is smaller than
that of BW = 5 and for BW = 7, whereas BW = 5 has an execution time slightly high-
er than BW = 7 at the minimum support of 8%. As the base window length increases
for the former, the sequential patterns also increase the mining time; while due to the
change of the basic window length for the later, shortening the time to transform from
sequential data to closed sequential patterns.
short-term data is easily interfered by abrupt events. Long-term data has less possibil-
ity to be interfered, and instead it is easier to find out the periodicity of occurring
events, allowing people to be readily interpreting the periodical changes to events. For
this reason, in order to find out the sequential pattern mining results from long-term
data, we have put forward this ICspan algorithm, applying an incremental mining
method to conduct a direct sequential mining to the new data and to integrate with
former mining results for generating new sequential patterns. In addition, we have
implemented the method of closed sequential pattern mining to reduce the number of
sequential pattern records. This concept will reduce waste of memory space in in-
creasing sequential patterns. Finally, the ICspan algorithm is collaborated with sliding
window mode to obtain a comprehensive sampling of all historical data, resulting in
more accurate sequential patterns for reference and decision-making to all users. The
experiment results also support that the ICspan algorithm will effectively reduce the
sequential pattern records and consequently reducing the memory usage, while main-
taining a sound mining efficiency under continuous data entries.
The objective of the future work could consider on the multiple data streams envi-
ronment from single timepoint with single data towards single timepoint with multiple
data, or searching for possible results of concept drifting from sliding window mode
with consider on the incidental events on short-term data. In addition, the algorithm
could be applied to practical applications in mobile communication, sensor network
and financial investment…etc. In the example of mobile communication application,
we could start mining user behaviors within the same base station coverage for base
stations in different positions, and to examine if the user behaviors have relevance or
ordering in base stations at different locations.
Acknowledgment
The authors would like to express their appreciation for the financial support from the
National Science Council of Republic of China under Project No. NSC
98-2221-E-031-003.
REFERENCES
Agrawal, R. and Srikant, R., Mining Sequential Patterns, Proceedings of the 11th In-
ternational Conference on Data Engineering, Taipei, Taiwan, 1995.
Chang, J. and Lee, W. 2005. Decaying Obsolete Information in Finding Recent Fre-
quent Itemsets over Data Stream, IEICE Transaction on Information and Sys-
tems, 87(6), 1588-1592.
Chang, J.H. and Lee, W.S. 2005. Efficient Mining Method for Retrieving Sequential
Patterns over Online Data Streams, Journal of Information Science, 31(5),
Streams in Real Time, Proceedings of the 28th International Conference on
Very Large Data Bases, Hong Kong, China, 2002.
Incremental Mining of Across-streams Sequential Patterns in Multiple Data Streams
Ching-Ming Chao and Yan-Ting Lin
Department of Computer Science and Information Management
Soochow University, Taipei, Taiwan
chao@csim.scu.edu.tw
Abstract Sequential pattern mining is to discover se-
quential patterns that frequently happen with time se-
quence. There are many user behaviors in our daily life
that can generate data streams. As the characteristics of
data streams are different from those of traditional data-
bases, traditional data mining algorithms are inapplicable
in data stream environment. In recent years, several algo-
rithms have been proposed for mining sequential patterns
in data stream environment. Existing algorithms can only
handle a single item at a time. Besides, the sequential pat-
terns mined may cross different data streams but are re-
garded as existing in a single data stream. To solve these
problems, we propose the IAspam algorithm for mining
sequential patterns in multiple data streams. IAspam not
only can handle a set of items at a time but also can in-
crementally mine across-streams sequential patterns. In
the process, stream data are converted into bitmap repre-
sentation for mining. Experimental results show that the
IAspam algorithm is effective in execution time when
processing large amounts of stream data.
Keywords: multiple data streams, data stream mining,
sequential pattern mining, incremental mining
1. Introduction
In the era of knowledge economy, knowledge is the
driving force behind the growth of industry. How to ac-
quire valuable knowledge is very important. Data mining
can be used to discover useful knowledge or patterns from
a large amount of data. Sequential pattern mining is to
discover sequential patterns that frequently happen with
time sequence. For example, in terms of web page access,
the access patterns of web surfers can be explored to pre-
dict the next possibly accessed web page for advance
access to expedite the web access.
There are many user behaviors in our daily life that
can generate data streams such as the signal data of mo-
bile communication and browsing records of web pages.
Data streams have the following characteristics [1]: (1)
unlimited data input, (2) limited memory capacity, (3)
one-time processing of input data, (4) fast data arrival, (5)
inability of system to halt data inflow. As the characteris-
tics of data streams are different from those of traditional
databases, traditional data mining algorithms are inap-
plicable in data stream environment. In recent years, sev-
eral algorithms [2-7] have been proposed for mining se-
quential patterns in data stream environment, enabling
data stream mining without their storage in a database.
Existing algorithms can only handle a single item at
a time and are incapable of coping with the changing en-
vironment of multiple data streams. Besides, the sequen-
tial patterns mined may cross different data streams but
are regarded as existing in a single data stream. To solve
these problems, we propose the IAspam algorithm for
mining sequential patterns in multiple data streams. IAs-
pam can not only handle a set of items at a time but also
incrementally mine across-streams sequential patterns.
2. The Proposed Approach
In multiple date streams environment, data are large
in size and complicated. However, existing mining algo-
rithms are incapable of coping with the changing envi-
ronment of multiple data streams. Based on commercial
consideration, mobile operators may need to find out the
association among streams produced at different locations
to provide their customers with better services. In this
paper, therefore, we propose an approach to incremental
mining of across-streams sequential patterns in multiple
data streams, which mainly consists of two stages: data
sampling and incremental mining. In the stage of data
sampling, we adopt the sliding window model to sample
stream data. In the stage of incremental mining, we con-
vert customer transaction data in the sliding window into
bitmap representation to find across-streams sequential
patterns in order to improve mining efficiency. After that,
new and old across-streams sequential patterns are either
updated or eliminated to produce precise mining results.
2.1 Multiple Data Streams Environment
The multiple data streams environment adopted by
our approach is described as below:
Each data stream produces a customer transaction
data at a time point.
A customer transaction data includes the customer
identification and the transaction information.
Customer transaction data are ordered by the time
point at which the data arrives.
All the stream data at the same time point will be
processed simultaneously.
The data amount of multiple data streams is much
larger than that of single data stream.
A set of data streams is represented as S {s1, s2,…,
sj,…, sm}, in which m stands for the number of data
streams.
supSD(p) represents the support of p in SD.
sup(FASP) represents the support of FASP in L-tree.
Algorithm: IAspam
Input:SW’, SW, min_sup, CID, S, I, k-item, CS, L-tree
Output: FASP, StP
1. For each transaction dataof SW’do
store into CS as <S, I>;
2. For each transaction sequence of CS do
convert into bit-vector and store into bitmap matrix;
3. For each 1-item do
add 1-item and supSD(1-item) into L-tree;
If supSD(1-item) min_sup then
generate CP with I-step and S-step and
map to bitmap matrix;
4. For each CP do
If supsw’(CP) min_sup then
If CP exists in L-tree then update supsw(CP);
else add FASP and StP into L-tree;
5. For each FASP in L-tree do
If sup(FASP) not updated then
decay sup(FASP);
If sup(FASP) min_sup then delete FASP;
6. return;
Figure 2 IAspam algorithm
The algorithm consists of three parts. The first part is
the conversion into bitmap representation (Step 1-2).
Customer transaction data will be converted into a bitmap
matrix. The second part is mining across-streams sequen-
tial patterns in new data (Step 3). I-step and S-step are
conducted on the 1-item to generate candidate sequences,
which will be mapped to the bitmap matrix. The third part
is the integration of new and old sequential patterns (Step
4-5). The newly mined patterns will be compared with
previous patterns to update sequential patterns and decay
the support of the sequential patterns not updated. Below
is the detailed explanation of the steps of the algorithm.
Step 1: Store customer transaction data in current
sliding window into the customer sequence table, in
which data are ordered by customer identification. Data
format is <S, I>.
Step 2: Convert each transaction sequence of the
customer sequence table into a bitmap vector of the cus-
tomer bitmap matrix. Judge if the itemset at each time
point contains k-item. If yes, store bit 1 into the column of
that time point of k-item; otherwise, store bit 0.
Step 3: Count the support of each 1-item in each
stream and store each 1-item and its supports into L-tree.
Judge if every support of a 1-item is greater than or equal
to the minimum support. If yes, conduct I-step and S-step
to generate candidate sequences and map them to a bit-
map matrix. In the mapping process, the candidate item-
sets generated by I-step will be used to search for the ‘1’
bits that are at the same time point and in the same stream
but not in the same item, and count the support. Whereas,
the candidate sequences generated by S-step will be used
to search, in different items and at different time points,
for the ‘1’bits whose prefix subsequences are at the same
time point and in the same stream and the ‘1’bits whose
suffix subsequences are at the same time point and in the
same stream, and count the support.
Step 4: Judge if the support of each candidate se-
quence in SW' is greater than or equal to the minimum
support. If yes, check to see if the frequent sequential pat-
tern exists in the lexicographical tree. If yes, update the
support of this sequential pattern. Otherwise, insert a fre-
quent across-streams sequential pattern (FASP) and a
stream sequence (StP) into the lexicographical tree.
Step 5: Judge if the support of each FASP in the lex-
icographical tree has been updated. If not, decay the sup-
port of FASP and judge if the decayed support is less than
the minimum support. If yes, delete FASP to get rid of the
FASP that has not been updated for a long time.
Step 6: Output FASP and StP.
Sequence extension is to conduct I-step before S-step.
I-step is to insert a new item into an itemset. For example,
after going through I-step, <(a,b)> and <(c)> will become
<(a,b,c)>, which can be expressed as <(a,b)> I <(c)> =
<(a,b,c)>. S-step is to append a new itemset to an itemset.
For instance, after undergoing S-step, <(a,b)> and <(c)>
will become <(a,b)(c)>, which can be expressed as
<(a,b)> S <(c)> = <(a,b)(c)>. According to the Apriori
property, the supersequences of an infrequent sequence
cannot be frequent. Consequently, it is not necessary to
conduct I-step and S-step on infrequent sequences and
hence no surplus candidate sequences will be generated.
Each leaf node of a lexicographical tree records a
frequent across-streams sequential pattern plus its support
and stream sequence. The purpose of constructing a lex-
icographical tree is to store frequent across-streams se-
quential patterns in order to insert new sequential patterns
or to combine or update new and old sequential patterns.
The nodes of a lexicographical tree have a lexicographical
order. The order of constructing a lexicographical tree is
from top to bottom and from left to right.
After the construction of the lexicographical tree, the
across-streams sequential patterns in the tree will be up-
dated with new mining results. Sequential patterns that
have not been updated for a long time may become obso-
lete. Therefore, it is necessary to decay their supports to
delete those sequential patterns that have not been up-
dated for a long time. The decay equations are as follows:
weight = dp, 0 d 1 (1)
p = w –u (2)
In Equation (1), d stands for the decay rate that is de-
to 2. The support within a red circle is a weighted support,
which is decayed from 2 to 1.8 under the condition that
the decay rate is set to 0.9. The sequential pattern will be
deleted because its support is less than the minimum sup-
port. <(a,c)(d)> will be the only frequent across-streams
sequential pattern and its stream sequence is <(S1)(S3)>.
3. Performance Evaluation
Our experiment consists of two parts. In the first part,
we compare IAspam and T-Map-Mine [7] in terms of ex-
ecution time and memory usage. In the second part, we
test IAspam under various conditions, such as the number
of time points, the window size, the number of customers,
and the number of item varieties, to evaluate its execution
performance. Experimental data are generated by the
synthetic data generator by IBM Almaden Research Cen-
ter. Table 1 shows the parameters of experimental data.
Table 1 Parameters of experimental data
Parameter Parameter Description
S Number of data streams
T Number of time points
I Average number of items per transaction
C Number of customers
D Number of item varieties
3.1 Comparison of IAspam and T-Map-Mine
(1) Execution Time
Figure 7 shows the comparison of execution time in
the S5T100C100I1 multiple data streams environment.
The vertical axis is execution time in second and the ho-
rizontal axis is the minimum support in percentage. As
seen in Figure 7, the execution time of IAspam is less
than that of T-Map-Mine under various minimum supports.
This is because T-Map-Mine must repeatedly search
T-Map-Tree and RST for each customer data as well as
insert new nodes and increase the support, which will take
more time in the mining process.
Figure 7 Comparison of execution time
(2) Memory Usage
Figure 8 shows the comparison of memory usage in
the same environment. The vertical axis is the maximum
usage of memory in kilobyte (KB). As seen in Figure 8,
the memory usage of IAspam is about the same as that of
T-Map-Mine under various minimum supports. The
memory usage of IAspam will be affected by the number
of item varieties and the number of customers. The
branching of T-Map-Tree of T-Map-Mine will depend on
the complexity of customer transaction data. The more
chaotic the data is, the bigger the T-Map-Tree becomes.
Figure 8 Comparison of memory usage
3.2 Analysis of IAspam
(1) Number of Time Points
Figure 9 shows the execution time of IAspam in
three different environments: S6T100I3, S6T1000I3, and
S6T10000I3. The vertical axis is execution time in second.
The execution performance of T10000 becomes worse
when the minimum support is less than 0.03. This may be
because, due to low supports, processing massive amount
of data needs to process more across-streams sequential
patterns, which requires a longer execution time.
Figure 9 Various numbers of time points
(2) Window Size
Figure 10 shows the execution time of IAspam with
three different window sizes: W=6, W=9, and W=12 in
the S6T1000I3 environment. The execution time of W=6
is less than that of W=9 and W=12, and the execution
time of W=9 is less than that of W=12. This may be be-
cause when the window size is smaller, there are less data
to be processed. Although the moving frequency of
smaller windows is higher than that of bigger windows,
which leads to more moving time, the experimental result
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
