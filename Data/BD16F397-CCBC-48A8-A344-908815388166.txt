 2 
pages into web directories maintained by 
Google and Yahoo can involve as many as 
thousands of topics. In structural classification 
of protein families, the number of classes is 
about 700. In handwritten Chinese character 
recognition, the number of classes (or 
characters) is several tens of thousands. None 
of previous research has addressed the issues 
of large time-complexity arising from 
multi-class classification. In this project we 
have developed time-efficient approaches that 
can handle applications involving large 
number of classes. We apply our approach to 
web page classification that often involved 
hundreds or even thousands of classes.  
 
2. Problem Foumulation 
 
To handle multi-class classification by SVM, 
the multi-class problem is usually reduced to 
multiple binary problems. Several 
well-known approaches[2] have been 
proposed and widely used, such as 
one-against-all, one-against-one[13] and 
error-correcting output coding[8]. Let M  be 
the number of classes, and N be the number 
of examples in training data. In one-against-all, 
we will train M  SVMs in which the i th 
SVM is trained, using instances in the i th 
class as positive examples and instances in all 
other classes as negative examples. In 
one-against-one, we will train 2/)1( -MM  
SVMs, one for each pair of classes. In output 
coding, each class is assigned a unique binary 
string of length L . We refer to these strings 
as “codewords”. Then, L  SVMs are learned; 
one for each bit position in these binary 
strings. 
 
Consider a M -class multi-class problem 
reduced to L  binary problems. A unified 
representation proposed in [2] is to associate 
each class with a row of a coding matrix 
( ) LMA ´  with { }1,0,1 -ÎijA . Each binary 
problem is associated with a column of A , 
which is used to train a SVM, using classes 
associated with 1 as positive classes, classes 
associated with -1 as negative classes; 
classes associated with 0 are ignored.  
 
Given a LM ´  coding matrix A , in general, 
L  grows as a function of the number of 
classes. The learning time and classification 
time of SVM depends heavily on the number 
of training examples and the number of 
classes as well. Learning of SVM for each 
column grows at least quadratic to the number 
of training examples in classes associated 
with non-zero entries in that column. The 
whole learning process can take exhaustive 
amount of time when the number of classes is 
huge, say, tens of thousands. In this project, 
we develop a class-selection approach that 
selects a subset of non-zero entries from 
coding matrix A  to form a reduced matrix 
A¢  to reduce the learning time to an 
acceptable level while the classification 
accuracy is maintained. We aim to find 
approximate solutions to the following 
problems. 
 
DEFINITION: Class Selection Problem.  
Given a coding matrix A  and a positive 
integer K , we are ask to select K  of the 
non-zero entries from A  to form a reduced 
coding matrix A¢  that achieves the best 
classification accuracy, i.e. for all reduced 
coding matrix A ¢¢  with K  non-zero entries 
selected from, 
)()( AAcuracyAAccuracy ¢¢³¢ . 
 
We also study the following planning 
problem. 
 
DEFINITION: Planning Problem. Given an 
integer K , we are asked to find a coding 
matrix A  with K  non-zero entries that 
achieves the best classification accuracy, i.e. 
for all coding matrix A¢  with K  non-zero 
entries, )()( AAccuracyAAccuracy £¢ . 
 
To classify a new instance, one usually needs 
to run all the classifiers, and combine their 
predictions to form the final prediction. To 
reduce the classification time, we propose the 
following approach: for each instance, we 
perform classifier selection that selects only a 
small subset of SVMs to join the prediction. 
 4 
classifiers, such as naive Bayes and linear 
classifiers, to derive the confusion matrix. 
Preliminary experiment shows that the linear 
classifier produced by Widraw-Hoff(WH) 
algorithm[19] is very time-efficient, and 
achieve very good performance. WH is an 
online algorithm. It runs through the training 
examples one at a time, updating a weight 
vector at each step. WH uses a learning rate 
0>h  to control how quickly the weight 
vector is allowed to change, and how much 
influence each new example has on it. In the 
experiment, we set  )4/(1 2l=h , where l  is 
the maximum value of x  in the training set. 
 
Classifier Selection for Multi-Class 
Classification 
We apply class selection to reduce the number 
of candidate classes examined by SVM. The 
objective is to reduce the classification time 
of SVM. In particular, we apply WH to select 
r  most possible classes, and use SVMs 
corresponding to candidate classes to join the 
prediction. Observe that WH achieves very 
high top r  measure for small r , say 5=r . 
It is highly possible that the target class is one 
of the candidates selected by WH. 
 
WH-SVM is outlined follows. On receiving 
an instance x , WH-SVM first invokes WH 
that in turns, computes the cosine similarity 
between x  and the prototype vector of each 
class, and returns a list of classes sorted by 
the computed similarity values. WH-SVM 
then passes the top r  classes ranked by WH 
to SVM. For each of the top r  ranked 
classes, SVM evaluates the decision function 
of that class on x . SVM returns the class 
with the highest decision value(score). Notice 
that if none of the r  classes passed to SVM 
has score (computed by SVM) larger than a 
predefined threshold, WH-SVM simply takes 
the top 1 class ranked by WH as the final 
prediction. 
 
4. Experimental Results 
 
Experiment on data sets collected from web 
directories shows that our approaches 
significantly reduce the running time of SVM 
while maintaining and sometimes improving 
their classification accuracy. In 
one-against-all, our experiments shows that 
we can make about 90% of the entries become 
zero, while maintaining similar classification 
accuracy, on one data set collected from 
Yahoo and Opendfind directories. Our 
approach outperforms a simple approach that 
randomly selects non-zero entries from A . 
On the data set from Openfind directory, the 
training time of SVM with linear kernel by our 
approach is about 326 sec while 
one-against-all takes 6,656 sec, for 10,059 
training instances categorized into 99 classes. 
Our approach saves about 95% of the training 
time. The classification time is reduced from 
80 sec to 23.33 sec for classifying 4,391 test 
instances, while the classification accuracy is 
significantly improved from 0.576 to 0.629. 
The approach has been developed to improve 
one-against-one SVM, and achieves 
significant improvement. 
 
5. Conclusion 
 
In this project, we have developed class 
selection approach which is used to develop 
one-against-k strategy for learning multi-class 
SVM, and to cascade linear classifiers to 
reduce the classification time of SVM. 
Experiment shows that our approach 
significantly improves both learning time and 
classification time of traditional SVM. 
 
References 
 
1. D. K. Agarwal. Shrinkage estimator generalizations of 
proximal support vector machines. In Proc. 8th Int. 
Conf. Knowledge Discovery and Data Mining, 
Edmonton, Canada, 2002. 
2. E. L. Allwein, R. E. Schapire and Yoram Singer. 
Reducing multi-class to binary: a unifying approach for 
margin classifiers. Journal of Machine Learning 
Research, 1:113-141, 2000. 
3. J. L. Balczar, Y. Dai, and O. Watanabe.  A random 
sampling technique for training support vector machines. 
In Proc. 13th Int. Conf. Algorithmic Learning Theory, 
Washington D.C., 2001. 
4. A. Berger. Error correcting output codes for text 
categorization. In proceedings of Workshop on 
Machine Learning for Information Filtering, 1999. 
5. Chih-Chung Chang, Chih-Jen Lin. LIBSVM -- A 
Library for Support Vector Machines. 
