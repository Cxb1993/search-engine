都是在空間（special）維度做處理，但是動畫資料多加了一個動作（時間）的維度，因此，
簡化處理不可忽視這份重要的資訊，所以，我們提出了如下的新的特徵值計算方式： 
 
2= * *( )ij ij ij ijw A lξ Δ ,  DOD =DSDij ij ijξ+  
 
在上述的公式裡， ijA 代表要簡化的頂點（vertex）周圍之三角型面積（area）， 代表了將
簡化頂點與周圍頂點的相對距離，藉此我們希望可以了解一個表面形變的程度，也就是把
一個表面的形變程度當成程式簡化時特徵值計算考量的項目之一。在下圖我們可以看到與
前人方法的比較。我們提出的演算法不僅提供較好的三角形簡化形狀，並且也可以再行遍
較多的區域保留較多的三角形。 
ijlΔ
 
  
 
三、 三維簡化形變模型之拓璞變化 
上一節介紹的演算法在面對骨架動畫上處理的很好，但是面對另外一種型態的動畫─極端形
變，例如三維形變（3D morphing），就無法發揮這麼大的用處。主要問題在於如果使用單
一的拓璞資訊（connectivity），則在某些畫格需要很大量的頂點去描述時，由於減化時不一
定有考慮到這些後面畫格的資訊，則簡化過後的動畫會在某些影格變的很失真。所以現下
大多利用改變最少的連結關係，也就是動態的改變連結關係來進行三維動態模型的播放。
如果三維動態模型的播放過程中需要大量的改變連結關係，那麼使用者便會覺得這個動畫
的連續性很差，會有一直閃來閃去的感覺。 
 
以下面的二個圖做比較，右圖是一個正弦波（sin wave）畫在平面上的圖，因為正弦波會一
直向外擴張，所以我們希望只有在下一個畫格也需要改變連結關係的地方，會有比較細緻
（即面積較小）的三角形，以達成簡少閃爍（popping）的情況，其餘的部分則保持著簡化
（即面積較大）的三角形即可，而左圖則沒有做任何偵測，整個模型都是用同一套網格簡
化的方法，所以即使是不太需要改變的平面部分，還是會被簡化成面積較大的三角形，造
成三角形的個數變多，在繪製三維動態模型時有著速度較慢的缺點。 
 
(textures)資訊與輪廓切割(silhouette clipping)資訊，將簡化模型的影像結合上述兩種資訊，
便可產生視覺上近似於原始模型的影像。 
 
由於視點(viewpoint)的不同，會對眼睛看到貼圖上的顏色產生影響，在不同的角度、不同
的光線下，都會讓從視點看到的貼圖顏色不一樣。因此，我們必須先紀錄如下圖所示原始
物體在此生成影像的視點下之模型向量圖(normal map)，紀錄此原始模型每個頂點的向量
值。有了向量值，我們就可以計算每個頂點與光線、視點的夾角，以得到該點正確的貼圖
顏色，使得產生的影像近似於原始模型之影像。 
 
 
 
所謂輪廓(silhouette)，指的是在幾何上，將三維模型投影到二維平面上，產生的邊緣，而
這些邊緣是由三維模型上的哪些邊(edges)所構成的，則需要我們去找出來。在三維模型
上，所有的邊都有會相交於兩個面(faces)，構成我們定義輪廓邊(silhouette edge)的條件就
是，從使用者視點看過去，一個邊所相交的兩個面為相反方向的面，這個邊就稱為輪廓邊，
如下圖所示。 
 Not Silhouette Silhouette 
 
在我們原始的模型中，找出輪廓邊的最簡單方式，就是把搜尋模型上的每個邊，找出與它
相交的兩個面之方向，再與使用者視點的方向來做比對，若相交的兩個面方向對於使用者
視點來說，剛好其一為正面、其一為反面，則此邊就標記成輪廓邊，反之則不做標記，而
判斷正反面利用內積即可快速求得。這個方法能夠很明確的解決找尋輪廓邊的問題，然而
在原始三維模型上，這個模型可能非常細緻，包含了非常多的三角形，儘管現在的處理器
能力很強，仍然會造成大量的負擔，因此我們設計了一些算法來降低計算時間。 
 
TYPE VALUE
Front Facing Section 0001
Back Facing Section 0010
Neither 0100
 
View 
Orientation 
Cluster Grid 
 
首先是將三維向量空間分割，並且將所有的邊進行編碼(encodes)分類，分成一個一個群組
一個二維的影像，所以我們就可以使用成熟的網路傳輸技術，來輕易和我們所要得到的結
果做結合。 
 
首先，在得到我們所要的幾何影像之前，最重要的一個步驟，稱為參數化
（parameterization），意思是找到個一對一的映成關係 ，使得 23: RRF ↔
 
ii qpF =)( ，其中  ⎩⎨
⎧
=
=
),(
),,(
iii
iiii
vuq
zyxp
 
舉例來說，下左圖是一個貓頭的三維幾何模型模型，下右圖則是這個模型投影到一個二維
平面的結果。 
 
 
 
由於這個二維的參數化平面是和原本的三維幾何模型有著一對一的映成關係，因此在二維
平面上的所有點，都對映著一個三維的座標(x, y, z)，因此我們利用簡單地公式 
 
⎪⎪
⎪⎪
⎩
⎪⎪
⎪⎪
⎨
⎧
×
++
=
×
++
=
×
++
=
255
255
255
222
222
222
iii
i
i
iii
i
i
iii
i
i
zyx
zB
zyx
yG
zyx
xR
 
 
就可以將這個座標轉變成影像上的顏色，而這個從平面轉變過來的影像，就是我們所要的
幾何影像，以下是產生幾何影像的簡單流程。 
 
後，我們就將其還原成三維幾何模型，那就會是個很粗糙的三維幾何模型，而當我們依序
收到更多的模型時，我們就可以還原成更精細的模型，如此一來，也就達到了我們串流的
目的。下圖是利用 JPEG 2000 格式所形成幾何影像的三維幾何模型，在網路串流不同時間
軸下所得到的變化圖，左邊是較粗糙的模型，而右邊是完整傳輸完畢的結果。 
 
 
 
下圖所表示的是我們提出有關以 JPEG 2000 為基礎的網格串流的系統架構圖，為了傳送三
維幾何模型，首先將三維幾何模型參數化且將其編碼為 JPEG 2000 的影像，接著透過 JPIP
的通訊協定將 JPEG 2000 的幾何影像傳到給使用者，為了支援視點獨立和多重解析度
（multi-resolution）這兩個關鍵的特性，所以我們修改了一個已存在的 JPIP 伺服器之傳送
序列，如此一來，系統不只能呈現原本的視點獨立、可調變品質（quality-scalable），以及
多重解析度的影像串流，同時也支援視點獨立和多重解析度的 JPEG 2000 幾何影像串流，
而在客戶端將幾何影像還原為三維幾何模型時，就同樣有著視點獨立和多重解析度的性
質。更進一步來說，為了支援三維動態模型，不只能將靜態的三維幾何模型編碼成以 JPEG 
2000 為基礎的網格串流，同時可以將三維動態模型以 Motion JPEG 2000 為基礎，編碼成形
變網格（deforming mesh）串流。 
 
Encode Decode
 
 
因為三維幾何模型可以使用 JPEG 2000 之編碼串流來做編碼，又一個三維動態模型可以將
其轉換成形變網格的形式，所以此時就可以使用 Motion JPEG 2000 之編碼串流來對三維動
態模型做編碼。當要將一個三維動態模型轉成 Motion JPEG 2000 幾何影片（geometry 
video），首先當然是將此三維動態模型的每個畫格的三維幾何模型轉換成 JPEG 2000 幾何
影像，如同之前所描述，要將三維幾何模型轉成幾何影像，必須先找到一個適合的切割方
法並將其投影在二維平面上，為了處理三維動態模型，假如此模型在動畫的每個畫格都是
封閉表面（closed manifold），就能夠將所有畫格裡適合的邊給切割開來，也就是說，為了
保持在 Motion JPEG 2000 裡所有畫格的一致性，所以在動畫中所有畫格裡的三維幾何模型
都必須盡量有著相同的切割路徑，接著就是將邊界對映到一個正方形上，為了保持一致性，
對於每個畫格而言，在正方形四個角的頂點應該是相同的，最後將所有畫格中的三維幾何
JPIP Server JPIP Client Image / Video / Mesh 
Streaming transmission
新目前使用者的位置與方向，來達到與整個場景、物件、其他使用者互動的情況。然而，
這個場景可能很大，物件可能非常多，其實並不需要對每一個其他的物件及使用者做互動，
只需要讓該使用者在可視範圍以及可互動的範圍內的物件做改變就好，而這個範圍我們稱
為視野範圍（Area Of Interest, AOI），這是一個以使用者的位置為中心的圓形。爲了簡化開
發模型，我們設定在場景中的物件都是固定的，其位置及方向不變，且在基本模型中，我
們只考慮單一使用者，也就是使用者只能看到固定的物件，並不會看到場景中其他使用者。 
 
對於每個固定的三維物件，我們假設它所有的內容，包含三角形及其他資訊，都能被分割
成基礎片段（base pieces）和精細片段（refine pieces），在使用者端一開始只有基礎片段的
資料，隨著點對點傳輸慢慢修補物件的內容，使其更完整。而分割三維物件的技術是採用
漸進式模型（Progressive Meshes, PM)以及幾何影像（Geometry Image, GI），並利用上節所
提到的方法利用 JPEG、GIF、PNG 的漸進式編碼（progressive encodings），來分割物件的
貼圖。所有的物件及場景內容，都是存放在伺服器中，而客戶端藉由串流的方式，經過伺
服器或是其他客戶端，來取得所需要的內容。只要能得到視野範圍內的基礎片段資料，就
能快速的產生影像及漫遊，減少使用者等待的時間。 
 
對一個使用者來說，在三維串流中最重視的就是視覺品質（visual quality），然而這個品質
是主觀的，每個人的看法不同，因此我們對此有較詳細的定義，就是客戶端能”多快”拿到”
多少”資料。我們定義了一個填滿比例（fill ratio），這個比例若為 100%，就代表最好的視
覺品質，我們所呈現出來的影像跟所有內容都存在自己電腦中的成像相同，也就是說客戶
端擁有所有三維物件的所有內容以及資訊。此外，我們定義了兩種測量方式，一種是基礎
延遲（base latency），代表客戶端能夠得到基礎片段所花費的時間；另一種是完成延遲
（completion latency），則是要下載完整物件資料所需花費的時間，基礎延遲是使用者能看
到基礎片段生成的影像所造成的時間延遲，而完成延遲則是使用者能看到完整看到整個物
件完整內容之成像的延遲。所以，對於客戶端的系統而言，三維串流的最佳目標就是能夠
最大化填滿比例，以及最小化基礎延遲和完成延遲。 
 
對於伺服器來說，最重要的就是整個系統的擴充性，能夠服務多少人。藉由分散處理程序、
分散傳輸到所有的客戶端，讓客戶端能負責越多事情，就可以減少伺服器的負擔，也就能
服務更多人。在傳輸上，最好是大部分的資料內容傳輸，都是客戶端與客戶端在傳送，這
部份可藉由伺服器頻寬使用率來做測量。在處理程序上，最好是所有對於使用者視覺部份
以及要傳輸什麼資料，都是在客戶端上解決，理想上伺服器只需要處理當客戶端有找不到
的資料，才來伺服器作請求。因此對於伺服器而言，目標就是能夠最小化伺服器的處理器
以及頻寬之使用度。 
 
為了能夠達到上述的需求及目標，我們必須要盡量利用客戶端的資源。我們訂出了兩個最
重要的議題： 
 
z 分散式決定可視範圍（distributed visibility determination） 
可視範圍的決定應該不需要伺服器端的介入，也不需要任何場景中的全域知識（global 
knowledge）。雖然一開始是由伺服器來持有所有的場景知識（包含物件的位置、場景描
述等），然而我們必須把這些場景描述能夠分散到所有的客戶端，讓客戶端能夠有效率
的分散處理這些相互影響的情況。 
z 場景分格 
我們採用了一個小的虛擬小村莊（3D Studio Max 格式的場景檔案, X3D），並且將其中
的物件抽出來，與場景檔案不同，而每一個村莊就是一個場景格，我們將這些小村莊
乘以 100 倍，當作我們的整個虛擬環境。 
z 三維物件分割 
我們將物件的場景敘述只包含了一個唯一的編號以及其邊界區塊（bounding box），而
將三維模型存成如上節所述一般的 JPEG 2000 的幾何影像。 
z 請求資料排序 
在加入一個點對點網路之後，客戶端必須先請求它的場景敘述以決定它的可視範圍，
得到場景敘述後，就可以產生一個請求片段列表（piece request list），列出所需要的資
訊。 
z 客戶端選擇 
一旦網路層收到了一個請求片段列表，點與資料片段選擇功能就會想辦法能滿足這些
請求。首先會搜尋視野範圍內的其他客戶端，是否能提供相關的三維物件內容，若有
就與這些鄰居建立連線開始傳輸，若沒有找到所需要的內容，則會持續查詢所有網路
上的結點，直到找到伺服器來請求所需要的資料。 
 
七、 點對點之三維遊戲串流 
儘管我們有了點對點之三維串流技術，三維模型檔案大小對於網路的傳輸還是造成了嚴重
的負擔，所以現今的三維網路遊戲，仍然是必須先安裝其程式，將這些三維模型先存放在
使用者的電腦之中，這部份可能還包含必須自行下載遊戲程式，造成了冗長的等待時間。
本節內容就是為了讓使用者可以更快速的進行遊戲，不需要在一開始就安裝所有遊戲的模
型、內容，而是在遊戲進行中，慢慢經由點對點方式，利用點對點之三維串流技術，將所
有需要的資料下載到電腦之中。 
 
使用者一開始看到的人物、場景皆為較簡略的模型，隨著時間進行，模型會漸進式的變成
更精確的模型，最後完成最詳細的模型，而這些過程都會在遊戲進行中完成，使用者不會
受到模型更新的影響。此節內容包含了三個部份：轉換模型格式、簡化模型及紀錄、運用
Ogre 生成影像。 
 
z 轉換模型格式 
首先我們所用的遊戲 Ember，使用的模型儲存格式為副檔名.mesh 的形式，為二元編碼
檔案，對於我們要做的處理相當不方便，因此我們第一步是將.mesh 檔案轉換格式，轉
成可讀的一般檔案。我們選擇了.obj 的格式作為我們模型的標準檔案，一來.obj 為主流
的儲存格式之一，另一方面對於我們之後簡化的步驟處理上較為簡單。因此我們使得
遊戲中的所有三維模型都轉換成.obj 儲存格式。 
z 簡化模型及紀錄 
考慮到了一開始遊戲的安裝檔案越小越好，我們將所有的模型做切割，分為基本檔（base 
file），以及後續編號的部分檔（part file），基本檔是簡化後的模型，頂點較少，讓安裝
檔案的大小降低，而部份檔考慮到網路點對點傳送的封包大小，一次是以 1000 個頂點
為單位，慢慢加入簡化後的模型，使簡化模型能逐漸增加頂點數量，產生更精細的模
型，最後會變成原始模型，這邊我們稱為漸進式模型。我們將基本檔保留 1000 個頂點，
理。以下則是本系統的演算法： 
 
(1) 模型簡化（mesh simplification）：假使原始三維模型的三角形數量過多便會造成計算時
間拉長，所以我們先用上述第二節的方法進行簡化三維模型的步驟。 
(2) 檢查三維模型的類型：如果三維模型是一個非封閉（non-close）模型，我們就進行簡單
的修補流程，即是將其非封閉的地方修補完成。修復方法有很多種，例如對非封閉的
地方分別做三角化（triangulation），或是使其變成一個封閉模型。  
(3) 特徵點對應（feature matching）：系統需要使用者分別在兩個三維模型之間分別指定對
應的特徵點，目的在於找出兩個三維模型之間對應關係，並切割出對應的共同基本領
域（common base domain）。其中，共同基本領域的好壞會對三維形變過程的品質和視
覺效果產生很大的影響，所以必須注意尋找合適的特徵點。 
(4) 產生共同基本領域（common base domain creation）：使用 Dijkstra 演算法找出兩兩特徵
點之間的最短路徑。但是在兩條對應的最短路徑可以加入兩個共同基本領域之前，必
須先檢查下面三個條件： 
a) 相交（intersection）：新加入的路徑不可與已經存在的路徑相交。 
b) 順序（cyclical order）：檢查此新加入路徑的兩端點的特徵點所射出的路徑的順序在
此兩三維模型間是否一致（consistent）。 
c) 同側（same-sidedness）：檢查新加入路徑的兩端點的特徵點跟路徑的相對位置在此
兩三維模型間是否一致。 
(5) 交錯參數化（cross parameterization）：此一步驟使用 [JU05] 的方法，建立補片間的對
應關係。 
(6) 形變：以上步驟都完成後三維模型形變所需工作便已完成，結果如下圖所示。 
 
 
 
九、 研究成果 
本研究之主要部份成果已分別於 2006 年的 ACM SIGGRAPH/Eurographics Symposium on 
Computer Animation、Computer Graphics International、 2007 年的 IEEE International 
Conference on Consumer Electronics、IEEE Transactions on Consumer Electronics 與 2008 年的
IEEE International Conference on Computer Communications 中以 Progressive deforming 
meshes based on deformation oriented decimation and dynamic connectivity updating 
[HUAN06b]、Skeleton-driven animation transfer based on consistent volume parameterization 
[CHAN07]、3d model streaming based on a jpeg 2000 image [LIN07a]、3d model streaming 
based on jpeg 2000 [LIN07b] 與 FLoD: a framework for peer-to-peer 3d streaming [HU08] 為
題進行發表，作者分別為黃輔中、陳炳宇與莊永裕 (Fu-Chung Huang, Bing-Yu Chen, and 
Yung-Yu Chuang)、張硯拓、陳炳宇、羅婉琪與黃建賓 (Yen-Tuo Chang, Bing-Yu Chen, 
Wan-Chi Luo, and Jian-Bin Huang)、林念賢、黃庭豪與陳炳宇 (Nein-Hsiung Lin, Ting-Hao 
Huang, and Bing-Yu Chen) 以及胡舜元、黃庭豪、張少榛、宋偉綸、江振瑞與陳炳宇 
streaming geometry in VRML. IEEE Computer Graphics and Applications, Vol. 19, 
No. 2, p.68 – p.78, 1999. 
HOPP96 Hugues Hoppe. Progressive meshes. ACM SIGGRAPH 1996 Conference 
Proceedings, p.99 – p.108, 1996. 
HOPP97 Hugues Hoppe. View-dependent refinement of progressive meshes. ACM 
SIGGRAPH 1997 Conference Proceedings, p.189 – p.198, 1997. 
HU08 Shun-Yun Hu, Ting-Hao Huang, Shao-Chen Chang, Wei-Lun Sung, Jehn-Ruey 
Jiang, and Bing-Yu Chen. FLoD: a framework for peer-to-peer 3d streaming. In 
Proceedings of IEEE International Conference on Computer Communications 2008, 
p.1373 – p.1381, 2008. 
HUAN06a Fu-Chung Huang, Bing-Yu Chen, and Yung-Yu Chuang. Progressive deforming 
meshes based on deformation oriented decimation. ACM SIGGRAPH 2006 
Conference Abstracts and Applications, 2006. 
HUAN06b Fu-Chung Huang, Bing-Yu Chen, and Yung-Yu Chuang. Progressive deforming 
meshes based on deformation oriented decimation and dynamic connectivity 
updating. In Proceedings of ACM SIGGRAPH/Eurographics Symposium on 
Computer Animation 2006, p.53 – p.62, 2006. 
IBAR03 Lawrence Ibarria and Jarek Rossignac. Dynapack: space-time compression of the 
3D animations of triangle meshes with fixed connectivity. Proceedings of ACM 
SIGGRAPH/Eurographics Symposium on Computer Animation 2003, p.126 – p.135, 
2003. 
JU05 Tao Ju, Scott Schaefer and Joe Warren. Mean value coordinated for closed triangular 
meshes. ACM Transactions on Graphics (SIGGRAPH 2005 Conference 
Proceedings), Vol. 24, No. 3, p.561 – p.566, 2005. 
KRAE04 Vladislav Kraevoy and Alla Sheffer. Cross-parameterization and compatible 
remeshing of 3d models. ACM Transactions on Graphics (SIGGRAPH 2004 
Conference Proceedings), Vol. 23, No. 3, p.861 – p.869, 2004. 
LENG99 Jerome E. Lengyel. Compression of time-dependent geometry. ACM Interactive 3D 
Graphics 1999 Conference Proceedings, p.89 – p.95, 1999. 
LIN07a Nein-Hsiung Lin, Ting-Hao Huang, and Bing-Yu Chen. 3d model streaming based 
on a jpeg 2000 image. Proceedings of IEEE 2007 International Conference on 
Consumer Electronics, 2007. 
LIN07b Nein-Hsiung Lin, Ting-Hao Huang, and Bing-Yu Chen. 3d model streaming based 
on jpeg 2000. IEEE Transactions on Consumer Electronics, Vol. 53, No. 1, p.182 – 
p.190, 2007. 
SHAM01 Ariel Shamir and Valerio Pascucci. Temporal and spatial level of details for 
dynamic meshes. ACM Virtual Reality Software and Technology 2001 Conference 
Proceedings, p.77 – p.84, 2004. 
TELE01 Eyal Teler Teler and Eyal Teler Lischinski. Streaming of complex 3d scenes for 
remote walkthroughs. Computer Graphics Forum (Eurographics 2001 Conference 
Proceedings), Vol. 20, No. 3, p.17 – p.25, 2001. 
WANG08 Tse-Hsien Wang, Chun-Tse Hsiao, Bing-Yu Chen, and Pei-Zhi Huang. Controllable 
motion textures. ACM SIGGRAPH 2008 Conference Abstracts and Applications, 
赴國外研究心得報告 
                                                             
計畫編號 NSC 95-2221-E-002-273-MY2 
計畫名稱 三維動態模型之網格簡化與具有體積保持的動作編輯之研究 
出國人員姓名 
服務機關及職稱 
國立台灣大學 資訊管理學系暨研究所 副教授 陳炳宇 
出國時間地點 2008 年 7 月 12 日至 23 日於日本東京 
國外研究機構 日本東京大學 
 
工作記要： 
 
七月十二日 
搭乘日本航空公司班機前往日本東京。 
七月十三日 
與東京大學西田友是教授夫婦餐敘，並與西田教授洽談目前正在執行的財團法人交
流協會「グローバルイルミネーションを考慮した 3 次元アニメーションモデルと
動的なシーンのリアルタイムレンダリング」國際合作計畫。 
七月十四日 
搭乘筑波特急電車自東京前往東京大學柏校區（千葉縣柏市柏之葉）參訪西田友是
教授所領導的研究團隊，並參與該研究團隊之定期會議。會中主要由數位碩博士班
的研究生介紹他們目前所做的研究，本人也就「My Recent Research Topics」為題做
一最近的研究簡介。 
七月十五日 
前往東京大學與來自於北海道大學的土橋宜典教授以及西田友是教授的博士班樂永
灝同學就「Portal-to-Vertex Light Transfer for Interactive Indoor Global Illumination with 
High-Frequency Environment Lighting」 （如附件一）為題進行討論，這篇論文曾經
投稿到 Pacific Graphics 2008 國際會議，雖然最後並沒有被錄取，但由於已經打下了
良好的基礎，審查委員的意見也相當正面，因此我們也決定將重新修改整篇論文並
再次投稿到 SCI 國際期刊 IEEE Transactions on Visualization and Computer Graphics。 
七月十六日 
與東芝（Toshiba）公司坂東洋介先生就「Extracting Depth and Matte using a 
Color-Filtered Aperture」 （如附件二）進行討論，該論文已投稿至 ACM SIGGRAPH 
Asia 2008 國際會議。因此，本次討論的目的主要是針對此一主題之後續研究進行研
討。 
Pacific Graphics 2008
T. Igarashi, N. Max, and F. Sillion
(Guest Editors)
Volume 27 (2008), Number 7
Portal-to-Vertex Light Transfer for Interactive Indoor Global
Illumination with High-Frequency Environment Lighting
paper 1063
Abstract
We present an interactive rendering system for indoor scenes while considering global illumination due to en-
vironment light sources, issuing through portals, such as windows or ventilators. Due to the occlusion, the light
transfer through a portal contains a lot of high-frequency components, and its calculation had been difficult to
be handled by previous methods. We decompose the transfer function between the environment light source and
each vertex of the objects in the scene into a transfer function between the environment light source and a portal,
and another one between the portal and each vertex. The low-frequency components and the high-frequency ones
of the latter transfer function are computed separately, using different representations, to obtain efficient render-
ing. Using our method, it is possible to interactively manipulate the viewpoint, the material of portals (e.g. the
transparent glass or frosted glass), and the environment light source (e.g. the sunset or sunny sky).
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and RealismRadiosity
1. Introduction
In recent years, image-based lighting [Deb98] has been re-
alized to be a powerful lighting model, making images to be
rendered more impressive, thus has been paid a lot of atten-
tion. Many pre-computation based methods have been pro-
posed for real-time rendering of static scenes taking image-
based lighting into account, e.g. [SKS02, NRH03, TS06].
These methods were aimed at rendering static objects just
under environment light sources, which can be treated as an
outdoor scene, and the aim is achieved successfully.
On the other hand, the aim of this paper is to present a
solution for rendering static indoor scenes lit by environ-
ment sources at interactive frame rates. Such a solution is
important since most indoor scenes have windows, and there
are frequent opportunities to render frames taking into ac-
count environment light sources. Some examples of the in-
door scenes would be a church, a room, or a car, especially
a church is usually lit by mainly natural sky light including
the sun light through windows (Figure 1). The lighting prob-
lem of an indoor scene is more difficult than the problem of
an outdoor one, because in an indoor scene, the light trav-
els through windows or ventilators, which makes the occlu-
sion very complex and contains many high-frequency com-
ponents. Another important factor is that the indirect illu-
mination is more dominant in indoor scenes, thus should be
Figure 1: Sibenik cathedral rendered using our method at
around 12 fps. Using our method, we can obtain good visual
effects due to the direct light issuing through the windows
(bottom right), as well as the inter-reflected light.
taken into account for high quality rendering. Therefore this
lighting problem needs to be explicitly handled.
Rather than to develop novel bases, we show that a rather
simple approach, that uses different sets of basis functions
to represent either the low-frequency and the high-frequency
submitted to Pacific Graphics (2008)
???
paper 1063 / Portal-to-Vertex Light Transfer 3
v
v
= +
Figure 3: Light transfer between the environment light
source and a vertex v, is decomposed into the transfer from
the environment light source to the portal on the ceiling and
that from the portal to the vertex v.
variations of the instant radiosity are intensively presented
e.g. [WABG06, HPB07]. Recently, [DSDD07, DKTS07] ex-
tended radiosity and proposed a solution for interactive
global illumination without imposing any static conditions
or pre-computation. A drawback of these methods is that
they do not scale well to the number of polygons in the
scene.
3. Portal-to-Vertex Light Transfer
In our method, we focus on the light transferred through por-
tals (i.e. windows or ventilators). By explicitly considering
portals, the light transfer through a portal is decomposed into
the light transfer from the environment light source to the
portal and that from the portal to the vertices of the objects
in the scene, as shown in Figure 3. Employing this decom-
position, it is also possible to manipulate the material of the
portals at run-time, for example applying transparent glass
or frosted glass materials, or changing the colors of the por-
tals.
A portal can be either simply a rectangle or a curved sur-
face, and is generally, represented as a collection of triangles.
Each triangle is regarded as an area light source with direc-
tional radiance distribution. As we assume the environment
light is coming from infinitely far away, the directional radi-
ance distribution on such an area light source is independent
from the location on the source.
3.1. Light transfer between a single source and a vertex
The light transfer from an area light source s towards a vertex
v, exiting in direction ωo can be computed as,
Lv(ωo) =
Z
Ω
Z
A
Ls(x,ω)Tsv(x,ω,ωo)dxdω, (1)
where Ω is the upper hemi-sphere aligned at the normal of s,
ω is an arbitrary direction exiting from s, Lv and Ls are the
directional radiance distribution of v and s, respectively, Tsv
denotes the global light transfer between v and s, A denotes
the region of the light source s, and x denotes a location on
the light source. In our case, the directional radiance distri-
bution is independent from x, thus Ls is simply a function of
ω, thus we rewrite Ls as Ls(ω), then Ls(ω) can be moved out
of the integral over A. By letting
T ′sv(ω,ωo) =
Z
A
Tsv(x,ω,ωo)dx, (2)
which can be pre-computed, Eq.(1) then becomes
Lv(ωo) =
Z
Ω
Ls(ω)T ′sv(ω,ωo)dω. (3)
By projecting both Ls and T ′sv onto basis functions, such that,
Ls(ω)≈
k−1
∑
i=0
liΨi(ω), (4)
where li is the coefficient for the i-th basis function Ψi, and
T ′sv(ω,ωo)≈
k−1
∑
i=0
k−1
∑
j=0
t′i jΨi(ω)Ψ j(ωo), (5)
where t′i j is the coefficient for the combination of the basis
functions Ψi and Ψ j . If we use orthonormal basis functions,
then by letting the vector Ls = {li}, and the matrix T= {t′i j},
the coefficient vector Lv = {l′j} of Lv is given by, Lv = TLs.
Finally, Lv(ωo) is given by evaluating the basis function at
direction ωo, that is,
Lv(ωo)≈
k−1
∑
j=0
l′jΨ j(ωo). (6)
If the vertex is on a diffuse surface, then Eq.(3) degener-
ates to
Lv =
Z
Ω
Ls(ω)T ′′sv (ω)dω, (7)
and T ′′sv is projected as
T ′′sv (ω)≈
k−1
∑
i=0
t′′i Ψi(ω). (8)
Eq. (6) would then become a simple dot product, that is,
Lv ≈
k−1
∑
i=0
lit′′i . (9)
By letting the directional distribution of the environment
light source be LEnv(ω), for a portal made of transparent
glass, the directional distribution on the portal can be ex-
pressed as (Figure 3),
Ls(ω) = kpLEnv(−ω), (10)
where kp is the transparency of the portal. For a portal made
of frosted glass, the distribution can be expressed using a
convolution,
Ls(ω) = kpG⊗LEnv(−ω), (11)
where G is a filter, e.g. a Gaussian kernel, representing the
frosting nature of the portal.
submitted to Pacific Graphics (2008)
paper 1063 / Portal-to-Vertex Light Transfer 5
(a) (b)
Figure 5: Hatched lines show the interior of the scene, and
bold lines indicate the portals. (a): a photon is emitted from
the portal into the direction of the solid arrow, but can not
contribute to the scene because the environment light coming
from the same direction (the dotted arrow) will be occluded.
This is what we call the backward occlusion, which can be
detected by tracing a ray from the position of the photon
into the backward direction (dashed arrow). (b): also, the
contributions due to several portals should not be double-
counted, if such contributions come from the same direction
and pass through the portals. This case can also be detected
by tracing a ray backward. If the ray hits another portal,
then the photon is discarded.
glossy inter-reflection is not significant in usual cases, and
can be ignored for further speed up.
After the incoming radiance distribution at a vertex is ob-
tained via the final gathering, we compute the outgoing ra-
diance distribution for non-diffuse vertices by evaluating the
BRDF on each pair of the incoming and outgoing directions.
This step can be accelerated by a factor of 2 by storing the
BRDF into a table. We enforced the surface albedo to be 1.0,
which will be taken into account during rendering to handle
textures. Finally, we expand outgoing radiance distribution
for non-diffuse vertices with spherical harmonics.
All the computation steps are summarized in Figure 6.
Note that the process for the pre-computation can be eas-
ily parallelized. For the process to render photon maps and
estimate the radiance at each vertex on a diffuse surface, we
can process several photon maps at one time, and in the final
gathering step, we can process several vertices at one time.
Direct lighting component. For the direct lighting compo-
nent, we directly sample the contribution for each direction
at each vertex. We discretize both the ranges of directions
ω and ωo into the directions on a cube with the resolution
required, e.g. 6×32×32.
First, we select a triangle belonging to a portal accord-
ing to the area of the triangle. Then, we shoot rays from the
vertex towards the triangle. For those non-occluded rays, we
then evaluate the contribution taking into account the solid
angle of viewing the triangle and the backward occlusion.
The whole transfer function between all the directions of
ω and ωo would include many components with values of
0. We first quantize the value of the transfer function into
8 bits, and then discard those components with values of 0.
PreparePhotonMap()
For i = 1 to n2
RenderPhotonMap(i);
For each vertex v
RadianceEstimation(i,v);
End For
End For
FinalGathering()
For each vertex v
For i = 1 to n2
For a = 1 to NFG
GatherEnergy(i,v,a)
For b = 1 to NDIR
EvaluateBRDF(a,b)
End For
End For
ExpandRadianceDistribution(i,v)
End For
End For
Figure 6: Algorithm for the pre-computation. NFG denotes
the number of final gathering rays, and NDIR denotes the
number of discretized directions of ωo.
More sophisticated way to store the coefficients would be to
use wavelets [NRH03], however, if the portal is quite small,
it is sufficient to use our approach.
5. Rendering
We represent the environment light source in two ways.
First, we express the directional distribution of the environ-
ment light source as spherical harmonics for the computation
of the indirect lighting component. Second, we express the
distribution directly in the discretized directions, at the same
resolution as the pre-computed direct lighting component.
The indirect lighting component at a vertex is computed
according to Eq.(9) for diffuse vertices, and to Eq.(6) other-
wise. These computations are carried out on GPU. The com-
putation for the direct lighting component is performed on
CPU, because the computation requires conditional branches
to skip the components with the values of 0, and this is
carried out more efficiently on CPU. Finally, the colors are
modulated with textures.
6. Results
First, we show some simple experimental results using the
box scene. We then show the application of our method us-
ing two scenes: the Sibenik cathedral, and a car. The pre-
computation and rendering are all conducted using a PC with
an Intel Core 2 Extreme QX9650 CPU, 2GB memory, and
an NVidia GeForce 8800 Ultra GPU.
First, we show four examples of the box scene for dif-
submitted to Pacific Graphics (2008)
paper 1063 / Portal-to-Vertex Light Transfer 7
(a) (b) (c) (d)
Figure 9: The box scene with different portals. (a): small, (b): large, (c): blueish, and (d): made of frosted glass.
(a) (b) (c) (d) (e)
Figure 10: (a): the direct lighting component, (b): the indirect lighting component. (c): full global illumination using our
method, (d): reference image using pure Monte Carlo method, and (e): false color shows the difference between (c) and (d).
Table 1: Statistics of the scenes ("Box" indicates the box
scene with the glossy bunny). T (dir/ind) denotes the times
for pre-computation of the direct and indirect lighting com-
ponents in minutes. Fps denotes the rendering performance.
M(dir/ind) denotes the required storage sizes for the direct
and indirect lighting components in MB.
Scene Box Sibenik Car
#Triangles 69,410 185,676 329,070
#Vertices 44,763 188,394 182,470
T (dir/ind) 12 / 11 56 / 14 60 / 23
Fps 70 12 12
M(dir/ind) 32.9 / 110.2 54.3 / 27.9 77.8 / 34.2
source can be efficiently handled. An implementation of fast
pre-computation is also presented and several examples to
demonstrate the importance of the environment lighting and
the efficiency of our method for rendering indoor scenes are
also displayed in the paper. The glossy surfaces and complex
scenes are also able to be handled well. Using our method,
rendering at interactive frame rates, manipulating the view-
point, light sources, and the material of portals are achieved.
For the future work, we are planning to explore the way
for subdividing the geometry of scenes, making the number
of resulting triangles as small as possible, as well as the illu-
mination is accurately captured.
References
[CG85] COHEN M. F., GREENBERG D. P.: The hemi-
cube: a radiosity solution for complex environments. In
SIGGRAPH ’85 (1985), pp. 31–40.
[Chr99] CHRISTENSEN P. H.: Faster photon map global
illumination. J. Graph. Tools 4, 3 (1999), 1–10.
[Deb98] DEBEVEC P.: Rendering synthetic objects into
real scenes: bridging traditional and image-based graph-
ics with global illumination and high dynamic range pho-
tography. In SIGGRAPH ’98 (1998), pp. 189–198.
[DKNY95] DOBASHI Y., KANEDA K., NAKATANI H.,
YAMASHITA H.: A quick rendering method using ba-
sis functions for interactive lighting design. Computer
Graphics Forum 14, 3 (1995), 229–240.
[DKTS07] DONG Z., KAUTZ J., THEOBALT C., SEIDEL
H.-P.: Interactive global illumination using implicit visi-
bility. In Pacific Graphics ’07 (2007), pp. 77–86.
[DSDD07] DACHSBACHER C., STAMMINGER M.,
DRETTAKIS G., DURAND F.: Implicit visibility and
antiradiance for interactive global illumination. ACM
Trans. Graph. 26, 3 (2007), 61.
[GKMD06] GREEN P., KAUTZ J., MATUSIK W., DU-
RAND F.: View-dependent precomputed light transport
using nonlinear gaussian function approximations. In I3D
’06 (2006), pp. 7–14.
[HPB06] HA ˘SAN M., PELLACINI F., BALA K.: Direct-
to-indirect transfer for cinematic relighting. ACM Trans.
Graph. 25, 3 (2006), 1089–1097.
[HPB07] HAŠAN M., PELLACINI F., BALA K.: Matrix
row-column sampling for the many-light problem. ACM
Trans. Graph. 26, 3 (2007), 26.
[Jen96] JENSEN H. W.: Global illumination using photon
maps. In Proc. EGWR ’96 (1996), pp. 21–30.
[Kaj86] KAJIYA J. T.: The rendering equation. In SIG-
GRAPH ’86 (1986), pp. 143–150.
submitted to Pacific Graphics (2008)
Online Submission ID: 0344
Extracting Depth and Matte using a Color-filtered Aperture
(a) (b) (c) (d)
Figure 1: (a) Top: camera lens with color filters placed at the aperture. Bottom: filter arrangement. (b) Photograph taken through the
color-filtered aperture. Note that the background colors are misaligned. (c) Estimated depth (the darker, the nearer). (d) Extracted matte.
Abstract
This paper presents a method for automatically extracting a scene
depth map and the alpha matte of a foreground object by capturing
a scene with RGB color filters placed at a camera lens aperture. By
dividing the aperture into three regions through which only light in
one of the RGB color bands can pass, we can acquire three shifted
views of a scene in the RGB planes of a captured image in a sin-
gle exposure. We develop a color alignment measure for evalu-
ating correspondence between the RGB planes which are recorded
with different bands of wavelength. Based on this measure, we esti-
mate disparities between the RGB planes to reconstruct depth, and
also extract a matte of an in-focus foreground object by leverag-
ing the fact that the captured background color is misaligned. We
show results for outdoor scenes and/or hairy foreground objects to
demonstrate the portability of our device and the effectiveness of
our method.
Keywords: color filters, depth estimation, alpha matting
1 Introduction
Rapid progress in the field of computational photography has
brought forth new types of cameras and imaging systems capable
of capturing additional scene properties that conventional photog-
raphy misses. These properties, when combined with computation,
extend the ability of imaging applications in many ways: increased
dynamic range and resolution, depth-guided editing, post-exposure
refocusing, variable lighting and reflectance, to name but a few.
While elaborate imaging systems and optical elements continue to
emerge, one of the recent trends in this field is to make a system
compact, or even portable [Ng et al. 2005; Georgeiv et al. 2006],
and to simplify optical elements to be attached to the conventional
camera [Levin et al. 2007; Veeraraghavan et al. 2007]. The ability
to easily switch from being a computational camera to the con-
ventional one to capture regular photographs is also claimed as
an advantage [Green et al. 2007; Liang et al. 2007]. This trend
will serve as a driving force for making computational photography
more commonplace and affordable for ordinary users.
To boost this trend, this paper proposes a method for automatically
extracting a scene depth map and an alpha matte of a foreground ob-
ject with a conventional camera body and a slightly modified cam-
era lens with RGB color filters placed at the aperture. By dividing
the aperture into three regions through which only light in one of
the RGB color bands can pass, we can acquire three shifted views of
a scene in the RGB planes of a captured image in a single exposure,
which enables depth reconstruction. While this idea has already
been proposed previously [Amari and Adelson 1992; Chang et al.
2002], we realize this idea in a hand-held camera without the need
for additional equipment other than color filters. We also devise a
better correspondence measure between the RGB planes which are
recorded with different bands of wavelength. Moreover, we pro-
pose a method for matte extraction, which is an entirely novel ap-
plication of a color-filtered aperture. By capturing a scene so that a
foreground object is in focus, we can deliberately misalign the RGB
planes of a scene background while keeping the focused object un-
affected. This color misalignment cue serves to constrain the space
of possible mattes that would otherwise contain erroneous mattes
when foreground and background colors are similar.
The proposed imaging system is portable, and can easily be restored
to the conventional camera by replacing the lens. It only requires
off-the-shelf color filters for camera as additional optical elements.
We show results for outdoor scenes and/or hairy foreground objects
to demonstrate the effectiveness of our method, with several image
editing examples such as viewpoint interpolation, post-exposure re-
focusing, and composition over different backgrounds.
2 Related Work
This section reviews several research areas that are closely related
to our work. We refer readers to [Raskar et al. 2006] for an exten-
sive survey on computational photography.
Color-filtered aperture. The idea of using color filters in an aper-
ture to estimate depth has been proposed previously. Amari and
Adelson [1992] used a squared intensity difference measure for
high-pass filtered images to estimate disparities. As they discussed
in their paper, however, this measure was insufficient to compensate
1
???
Online Submission ID: 0344
Figure 4: Example of a photograph taken with our lens, and its
separeted RGB planes. The white lines are superimposed in order
to make the background color shift visible. The in-focus toy dog has
little color shift.
4 Depth Estimation
The RGB planes Ir, Ig , and Ib of a captured image correspond to
three views of a scene. If we take a virtual center view (cyclo-
pean view) as a reference coordinate system, R, G, and B planes
are shifted to rightward, upward, and leftward according to the ar-
rangement of the aperture color filters. Therefore, letting d be a
hypothesized disparity at (x, y), we need to measure the quality of
a match between Ir(x+d, y), Ig(x, y−d), and Ib(x−d, y).
Clearly, we cannot expect these three values to have similar in-
tensities because they are recorded with different bands of wave-
length. To cope with this issue, we exploit the tendency of col-
ors in natural images to form elongated clusters in the RGB space
(color lines model) [Omer and Werman 2004]. We assume that
pixel colors within a local window w(x, y) around (x, y) belong
to one cluster, and we use the magnitude of the cluster’s elon-
gation as a correspondence measure. More specifically, we con-
sider a set S(x, y; d) of pixel colors with hypothesized disparity
d as S(x, y; d) = {(Ir(s+d, t), Ig(s, t−d), Ib(s−d, t)) | (s, t) ∈
w(x, y)}, and search for d that minimizes the following color align-
ment measure:
L(x, y; d) = λ0λ1λ2/σ
2
rσ
2
gσ
2
b , (1)
where λ0, λ1, and λ2 (λ0 ≥ λ1 ≥ λ2 ≥ 0) are the eigenval-
ues of the covariance matrix Σ of the color distribution S(x, y; d),
and σ2r , σ2g , and σ2b are the diagonal elements of Σ. Note that the
dependence on (x, y; d) of the right-hand side of Eq. 1 is omitted
for brevity. L(x, y; d) is the product of the variances of the color
distribution along the principal axes, normalized by the product of
the variances along the RGB axes. It gets small when the clus-
ter is elongated (i.e., λ0 ≫ λ1, λ2) in an oblique direction with
respect to the RGB axes, meaning that the RGB components are
correlated. L(x, y; d) is in the range [0, 1], with the upper bound
given by Hadamard’s inequality [Gradshteyn and Ryzhik 2000].
To illustrate the effect of this measure, we use a sample image
shown in Fig. 5(a), taken with a conventional camera lens. Since
its RGB planes are aligned, the true disparity is d = 0 everywhere,
and colors within the local window indicated by the red rectangle in
Fig. 5(a) actually form an elongated cluster, as shown in Fig. 5(c). If
we deliberately misalign the RGB planes by d = 1, 3, and 5 pixels,
the distribution becomes more isotropic, as shown in Figs. 5(d-f),
and the color alignment measure becomes larger, as shown under
the corresponding plots.
Now that we can evaluate the quality of a match between the RGB
planes, we can find the disparity d that minimizesL(x, y; d) at each
pixel (x, y), from a predetermined set of disparity values (-5 to 10
in our implementation). As local estimates alone are prone to error,
we use the standard energy minimization framework using graph-
cuts [Boykov et al. 2001] to impose spatial smoothness constraints.
 0
 0.5
 1 0
 0.5
 1
 0
 0.5
 1
B
R
G
(a) (b) (c) d = 0, L = 0.003
 0
 0.5
 1 0
 0.5
 1
 0
 0.5
 1
B
R
G  0
 0.5
 1 0
 0.5
 1
 0
 0.5
 1
B
R
G  0
 0.5
 1 0
 0.5
 1
 0
 0.5
 1
B
R
G
(d) d = 1, L = 0.11 (e) d = 3, L = 0.39 (f) d = 5, L = 0.54
Figure 5: (a) Sample photograph taken with a conventioanl cam-
era lens. (b) Magnified crop from the local window indicated by the
red rectangle in (a). (c-f) Plots of the pixel colors within the local
window in the RGB space. The values d and L shown below each
plot are the simulated diparity and the value of Eq. 1.
5 Matting
Matting is a problem of solving for foreground opacity α(x, y) at
each pixel (x, y) in the following matting equation.
I(x, y) = α(x, y)F (x, y) + (1− α(x, y))B(x, y), (2)
which models an observed image I(x, y) as a convex combination
of a foreground color F (x, y) and a background color B(x, y). As
we capture an image so that a foreground object is in focus, we
assume that α(x, y) is aligned between the RGB planes. More pre-
cisely, regions with fractional alpha values should be within the
depth-of-field of the lens. This assumption is similar to [McGuire
et al. 2005]. Slight violation of this assumption however does not
lead to severe degradation of extracted mattes, as will be shown in
Sec. 6.
Solving Eq. 2 based only on the observation I(x, y) is an under-
constrained problem, since we have only three measurements (RGB
components of I(x, y)) for seven unknowns (α(x, y), RGB com-
ponents of F (x, y), and those of B(x, y)) at each pixel. Therefore,
we incorporate additional constraints taking into account the fact
that foreground colors and background colors are differently mis-
aligned due to the color-filtered aperture. We first develop a consisi-
tency measure of estimated colors with the disparity map obtained
in Sec. 4. We then present how to solve for α(x, y) based on that
measure.
5.1 Measuring Consistency with a Disparity Map
Suppose that we have an estimate of foreground color F (x, y) and
that of background color B(x, y). This subsecion develops a mea-
3
Online Submission ID: 0344
(a) (b) (c) (d)
(e) (f) (g) (h)
Figure 7: Matting process illustrating the effect of the color consistency measure. (a) Input image (cropped and magnified from Fig. 11(a)).
(b) Alpha value α5(x, y) at fifth iteration. Major errors are indicated by the arrows. (c) Foregournd color F 5(x, y) based on (b). (d)
Background color B5(x, y) based on (b). (e) Foreground color consistency cF5(x, y) computed for (c). Large values indicate that the
background colors erroneously included in (c) around those regions. The blue regions are “strictly foreground” and “strictly background”
regions, and the consistency values are undefined. (f) Background color consistency cB5(x, y) computed for (d). Large values indicate that
the foreground colors erroneously included in (d) around those regions. (g) Updated matte α6(x, y) after one iteration. (h) Updated matte
α10(x, y) after five iterations.
(a) (b) (c) (d) (e)
Figure 8: Comparison of correspondence measures between the RGB planes. Larger intensities indicate larger disparities. Top row: results
for the toy dog image in Fig. 4. Bottom row: results for the woman image in Fig. 1. (a) Our method (local estimate). (b) Amari and Adelson
[1992] (local). (c) Chang et al. [2002] (local). (d) Our method (after graph-cut optimization), (e) Kim et al. [2003] (based on mutual
information with iterative graph-cut optimization).
a priori knowledge of the intensity relationships. However, some
portions of the foregound objects were not detected (Fig. 8(e)).
Next we show our matting results. Fig. 9(a) shows the extracted
matte for the toy dog image in Fig. 4. The hairy silhouette was ex-
tracted successfully. We can use this matte to refine the boundary of
the foreground and background regions in the depth map as shown
in Fig. 9(b), by compositing the foreground and background dis-
parity maps shown in Figs. 6(b-c). Fig. 1(d) shows the result matte
for an image of a person which was captured outdoors. A good
matte was obtained despite the similar foreground and background
colors. In Fig. 10, we applied the previous natural image matting
methods [Levin et al. 2008; Wang and Cohen 2007] with the trimap
given by our method. Of course this is not a fair comparison be-
cause the previous methods are designed for color-aligned images,
but the artifacts seen in Fig. 10 are indicative of the importance of
our color consistency measure in suppressing them. For fairer com-
parison, we used a ground truth matte shown in Fig. 13(a) obtained
by capturing a foreground object in front of a simple background
and by using Bayesian matting method [Chuang et al. 2001], fol-
lowed by manual touch-up where needed. We created a synthetic
“natural” image by compositing over a new background image,
as shown in Fig. 13(b). We also created a color-misaligned ver-
sion of them by shifting the background colors by 3 pixels be-
fore composition. We applied the previous methods to the color-
aligned synthetic image, and our method to the color-misaligned
one. Though not perfect, our method produced a better matte as
shown in Figs. 13(c-e).
As our camera is portable, and only a single exposure is required,
it is easy to capture moving objects such as animals, as shown in
Fig. 11. Fig. 11 also shows a portion of the foreground object (the
hip of the sheep) is slightly out of the depth-of-field of the lens,
violating the assumption that α(x, y) is aligned between the RGB
planes in Eq. 2. However, degradation of the extracted matte around
the region was small, as shown in Fig. 11(d).
5
