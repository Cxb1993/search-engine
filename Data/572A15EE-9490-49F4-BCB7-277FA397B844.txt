reconstructed images. In comparisons with other view 
estimation methods, the proposed method can provide 
lower computational complexity and time consumption, 
while yielding better reconstructed video quality. 
英文關鍵詞： Multiview Video Coding, Distributed Coding, Depth 
Image Interpolation 
 
2 
 
壓縮效能是重要議題。在立體電視(3DTV)[1]或是自由視角電視(Free-Viewpoint 
Television, FTV)[2]等應用中，多視角視訊(Multi-view Video, MVV)編碼系統扮演了
一個相當重要的角色，但由於三維影像的資料量遠大於二維影像，使得多視角視訊
編碼(Multi-view Video Coding, MVC)在如何適應網路時變性、終端多樣性前提下，
需滿足節省傳輸頻寬、低複雜度、低延遲的設計上，面臨更大的挑戰。目前所發展
的 MVC 系統，將不同視角視訊內容透過聯合編碼的方式傳送單一碼流至解碼端，
利用運動估計(motion estimation)方法和視角間視差(disparity)的估計，將時間域和視
角間的關聯性去除。關於三維視訊有另一個討論結構稱為多視角視訊結合深度影像
(Multi-view Video plus Depth, MVD)。其特點是經由編碼傳送的影像畫面和其深度圖
於解碼端接收後，以深度影像繪圖法(Depth Image Based Rendering, DIBR) [3]模擬人
眼成像原理達到真實的三維視訊感受，或是以虛擬視角影像合成法(Virtual View 
Synthesis)[4]產生視角間的虛擬畫面給使用者互動的隨選自己喜愛的視角如 FTV，減
少編碼端大量的視訊資料傳送進而減少傳輸頻寬。為了將相關的資訊源獨立編碼且
在解碼端能夠根據彼此的相關性來聯合解碼，Slepian-Wolf 理論[6]的分散式資訊源
編碼(Distributed Source Coding, DSC)提出了此問題的解答，並且在之後的 Wyner 和
Ziv 兩位學者延伸此理論至失真的信息源編碼[7]，透過解碼端的輔助資訊(Side 
Information, SI)來重建解碼的畫面，因此分散式視訊編碼(Distributed Video Coding, 
DVC)也受到愈來愈多人的關注；而整合 DVC 與 MVV 的分散式多視角編碼系統
(Multi-view Video with Distributed Video Coder, MDVC)也成為廣泛討論的一環。本計
劃以 DVC 編碼理論為基礎，平衡編解碼端的複雜度，並結合 MVV 畫面的深度資訊
而提出之改良多視角與分散式視編碼演算法(Multi-view Video Distributed Video plus 
Depth Coding, MDVDC)，進一步提高輔助資訊之可靠度進而提升視訊重建的品質。 
二、文獻探討 
典型的多視角視訊編解碼架構如錯誤! 找不到參照來源。所示，將不同視角的
視訊內容透過聯合編碼的方式傳送單一碼流至解碼端，利用區塊估計與視角內插將
時間域和視角間的關聯性去除，在解碼端藉由接收到的殘餘值(residual)和運動向量
(motion vector)來聯合解碼出完整的畫面，編碼端付出了高運算複雜度的代價來達到
高壓縮的效能，且必須在編碼端提供攝影機之間的通訊，因此我們考慮能夠降低編
碼端運算複雜度的架構；分散式視訊編碼(Distributed Video Coding, DVC)將運算複
雜度由編碼端轉移到解碼端的特性提供了解決方法。MVC 外另一個代表三維視訊
(3D video)是單一視角畫面和視訊影像深度圖(Depth Map)於解碼端以深度影像繪圖
(Depth Image Based Rendering, DIBR)產生虛擬視角影像方法，缺點是虛擬影像繪圖
產生封閉(occlusion)、空洞(pinhole)問題。文獻[5] 提出改善繪圖品質 MVV 結合深
度影像架構(Multi-view Video plus Depth, MVD)，處理的資料包括 MVV 並對應的影
像深度圖。典型的三維視訊傳輸系統包含 MVD 格式流程圖如錯誤! 找不到參照來
源。。 
4 
 
的輔助資訊影像將能夠提高整體編碼的效能與品質。本計劃提出的系統中，在解碼
端討論的是 GOP=1，也就是中間視角只傳送 WZF，在解碼端藉由左、右視角和深
度資訊所產生的 SI 來重建中間視角的影像。在這種應用於解碼端產生虛擬視角影像
合成技術(visual view synthesis)而建立輔助資訊的方法有二種，分別介紹如下： 
(1) 視差補償視角預測： 
視差補償視角預測(Disparity Compensation View Prediction)錯誤! 找不到參照
來源。類似於運動補償時間域內插(Motion Compensation Temporal Interpolation, 
MCTI)，將原始左、右視角影像透過 H.264/AVC 編解碼後於解碼端合成虛擬的影像
畫面做為輔助資訊，利用不同視角間存在的關聯性進行區塊估測，得出虛擬中間視
角的輔助資訊。其架構可以適用於編碼結構為不同畫面群(Group of Pictures, GOP)
長度的內插演算法。然而此方法產生的輔助資訊品質較 MCTI 來的差，原因在於攝
影機之間視差遠大於時間域前後畫面的微小運動差距。 
(2) 透視轉換模型： 
透視轉換是一個可以描述轉換平面上影像點之間的可逆性轉換關係，這裡我們
假設單一視角攝影機的成像平面是透過投影矩陣 P 將三維空間中的平面投影至二
維平面上，並給定一成像平面座標；我們可以將該二維平面座標轉換到一個三維世
界座標的關係；而此二維座標對應於另一個視角攝影機的成像平面座標同樣可以藉
由在三維世界座標上的對應點來得到，而整合此關係的矩陣就稱為透視轉換
(homography)矩陣錯誤! 找不到參照來源。，而對於此透視轉換矩陣的求法，我們
會採用視角間使用尺度不變特徵轉換(Scale Invariant Feature Transform, SIFT)工具
[14]找出的匹配關鍵點(key points)，搭配一致性隨機取樣(Random Sample Consensus, 
RANSAC)演算法[15]將錯誤的對應點排除後，求出最佳的透視轉換矩陣。 
故本計劃將提出一個新的多視角視訊結合深度資訊編碼(Multi-view Distributed 
Video plus Depth Coding)稱之為 MDVDC 的技術，融合深度透視轉換和視差間區塊
補償預測稱做 Depth Disparity Homo-fusion，將輔助資訊以僅有的資源重建得更精
確，以達到更好的 PSNR 編碼效能。  
三、 系統架構 
本計劃提出加入影像的深度資訊於分散式技術的多視角視訊編碼系統，如圖
3-1 所示。在編碼端擷取到的左、右視角的視訊影像和其經由深度估測後的兩視角
深度圖一併傳入 H.264 Intra mode (H.264I)來編碼，稱 I-frame 而中間畫面會依照不
同的 GOP 結構分成 I-frame(Key-frame, KF)和 WZ-frame(WZF)，此部分將在後續章
節中討論。若畫面為 WZF 則進入分散式編碼系統(DVC)來編碼，KF 同樣以 H.264I
編碼。三個視訊串流於編碼端採獨立的編碼方法將位元流傳送至解碼端。於解碼端，
聯合的 DVC 解碼器綜合參考左、右視角視訊及其深度資訊和中間視角依照 GOP 架
構編碼完的 KF 解碼出 WZF 的部分。 
6 
 
再搭配一致性隨機取樣 RANSAC 演算法[15]出最佳的透視轉換矩陣。至於設定門
檻值是因為影像深度估測時依照攝影機的位置與畫面中物體遠近關係設定灰階值區
域，而背景區域深度灰階值較不一致且輪廓較不清晰完整，如此對應回原本影像找
尋特徵點，造成太少特徵點匹配問題。因此，門檻值以下的廣大背景區域對應回視
角間原始畫面找尋特徵對應點，當作最基礎的轉換關係點；門檻值以上則以背景基
礎轉換關係點一層一層的加上分段後的深度區間透過影像對應特徵點，進入矩陣找
尋函式，找出每層的深度轉換矩陣。得到左、右視角的深度區段透視矩陣 ( 1 5 )
lH 、
( 1 5 )
rH
後，原始影像
C
loI 、
C
r oI ，和其灰階的深度圖
D
loI 、
D
r oI 可以使用相對應的深度區間透視
矩陣進行透視轉換得到 ˆ
l
I  、 ˆ
r
I  ，最後再與當前 GOP 架構下解碼完可用的
Key-frame
k e yI 作區塊比對預測或是像素點合成得到輔助資訊 
in t
mI 。本計劃所提出之
Depth Disparity Homo-fusion方法是參考[14]中 fusion 1的演算法，其概念是加
入左、右視角做完深度對應透視轉換後合成一輔助資訊，並與左右視差補償估測後
的畫面一併當作 SI 候選影像，比較兩候選畫面計算每點像素值與左、右視角 KFs
相減的差值，選擇差值較小的像素點即為最終的輔助資訊像素點。此方法包括區塊
估測和像素點的內插合成技術，重建出來的影像有兩種方法的優點能有效稍加提升
輔助資訊的品質。但又如之前所說，利用到的 KFs只有兩張資源較少，雖然減少 H.264
編碼時用到的位元數，導致輔助資訊的品質不夠好使得聯合解碼端重建的 WZF PSNR
不如預期。 
   在錯誤! 找不到參照來源。中描述了深度轉換流程，其中包括的深度值分段找尋
轉換矩陣、深度值轉換、 深度值對應原始亮度影像、內插填補部分像素轉換重疊時
造成空洞，以及使用原始亮度影像轉換圖補償在不同深度轉換時發生的輪廓效應並
畫面偏移時的黑邊。因為左右視角的轉換方式是一樣的，接下來以左視角為例分別
描述每個流程。計算深度轉換矩陣，需要有左視角和中間視角的亮度影像和深度影
像
LLC D 、
in t
mC 。利用深度影像特性將畫面中的物體遠近，以一個門檻值來界分兩
視角的背景影像和前景影像， 並對應回原本的該亮度影像像素位置利用 SIFT 工具
找尋特徵對應點，而因為背景深度較前景深度沒有明顯的物體輪廓且不清晰完整，
若以深度分段對應回亮度圖找尋特徵點，造成太少特徵點匹配問題。所以將門檻值
以下的廣大區域一併對應回亮度影像找尋特徵對應點稱做基礎對應點，也就是說它
包含畫面背景深度主要的轉換趨勢；而門檻值以上每隔幾個單位細分深度區間並加
上背景深度區域，因此對應回原本亮度圖找出眾多對應點包含深度背景來的基礎對
應點和主要的前景物件特徵對應點，以確保所得到的轉換矩陣建構在有背景畫面的
轉換趨勢上，換言之越高的深度區間包括的特徵點越廣，轉換趨勢也更穩定。 
 
8 
 
4.1 輔助資訊(SI)品質 
 
 
 
 
本節中所討論的三種重建 SI 影像的方法已經在第二節中說明，呈現的結果會比較 3
種不同 Key-frame 品質下的重建 PSNR 值，如圖 4-1 到錯誤! 找不到參照來源。2 所
示，分別表示測試影片為 Breakdancer 與 Ballet 在不同重建 SI 下的平均 PSNR，DCVP
於兩個影片都是最差的，原因在於單純使用區塊運動估測補償左、右視差，造成強
烈的區塊效應，且若是在時間域複雜度偏高的 Breakdancer 影片更顯得嚴重。相較
之下 Disparity Homo-fusion 針對每個像素選擇最適合的產生 SI 方法，改進 DCVP
的缺點，把比較區塊效應的像素更精確的再篩選一次，於 Ballet 影片較 DCVP 方法
高 0.3dB；且於 Breakdancer 也能抵抗高運動複雜度，因此比 DCVP 好約 2.5dB。而
以本計劃提出深度對應轉換方法 Disparity Depth Homo-fusion，以深度區間對應轉換
更能較單純一張亮度圖透視來的細緻精確，比 Disparity Homo-fusion 好 0.3~0.7dB。
並發現當視差特性越大深度對應轉換效果更為顯著。 
4.2 編碼效能之速率-失真(Rate-Distortion) 
本節將討論 MDVDC 架構於 GOP=1 下三種 SI 重建方法最後解碼完 WZFs 的
PSNR 品質和所需要的位元數。Ballet 影片因為運動複雜度較低，WZFs 的 PSNR 值
變動較平緩如錯誤! 找不到參照來源。，而應用本論文提出深度轉換的方法 Disparity 
Depth Homo-fusion結合區塊估測和像素候選點比較解碼後的WZFs較其他兩個方法
都要好，比 DCVP、Disparity Homo-fusion 分別高了約 1~2dB。Breakdancer 複雜度
較高，PSNR 變動幅度較大如錯誤! 找不到參照來源。所示，整體來看所提出的方
法仍然較其他來的好約 0.1~0.3dB。接著討論編碼位元數，以表 4-1、表 4-2，分別
記錄每個 QP 下所需的解碼位元數，和其 SI 和 WZFs 的 PSNR。在 GOP=1 的格式裡，
討論中間視角 DVC 的壓縮效能時，左右視角的 KFs 消耗位元數不包括其中，所以
只需要討論渦輪解碼器的查核位元(parity bits)。以 Ballet 而言，在 QP=20 查核位元
數降至最低的 66Kbits，較其他兩種方法減少了 4~6Kbits。但相較變動大的
Breakdancer，SI 的品質並沒有好到擺脫掉 Disparity Homo-fusion，因此解碼所需位
元數很接近，但都比 DCVP 少約 7K bits。最終我們發現 SI 品質高不僅能有效提升
WZFs 的品質也能減少渦輪解碼器所需的查核位元，主要原因在於 SI 能左右虛擬通
道破壞模擬和 WZFs 重建。 
 
圖4-1 SI影像品質的比較(“Ballet”) 
 
圖4-1 SI影像品質比較(“Breakdancer”) 
10 
 
一張影像所花費的時間，以 2 種測試影片的平均值來看，所提出的 MDVDC 架構，
編碼一張影像需要 5.74 ms，H.264I 需要 29.595 ms，而 H.264 inter no motion 需要
67.075 ms，在 H.264Inter 模式則是需要 99.12ms，可知本系統能達成編碼端低複雜
度的需求，降低運算量時間與 H.264I 相比減少 6 倍的時間、與 H.264 inter no motion
相比減少了 12 倍的時間，再與 H.264 Inter 相比減少 18 倍的時間。 
        Encoding     
           Time 
  Sequence 
Even frame GOP = 12 
MDVC H.264 intra H.264 inter no 
motion 
H.264 
inter Breakdancer 6.06 29.83 68.18 106.38 
Ballet 5.42 29.36 65.97 91.86 
Average 5.74 29.595 67.075 99.12 
Unit：Millisecond (ms) / frame 
 
 
表 4-3、編碼一張影像平均時間  
4.4  主觀效果評量 
本節展示 2 種不同測試影片分別使用 QP=26 的 KF 品質來搭配二種重建 SI 方
法所得到的影像，與原始畫面比較，討論內插重建的結果。圖 4-5 是比較 Ballet 影
像，DCVP 使用區塊視差補償對於大量的背景部分造成錯誤估測，而 Depth H-fusion
是結合 DCVP 和透視轉換對於像素點來比較左、右視角最相近點進行合成，畫面較
呈現模糊且皆有重疊的部分原因是在左、右視角的像素點皆大多選擇透視轉換後的
後選點，因此在視差最大的前景物件重疊現象更為明顯，但相較 DCVP 背景部分有
較好的重建品質。提出的 Depth H-fusion 方法則有最穩定的重建品質。 
 
圖4-5 比較Ballet使用不同重建方法之SI影像( QP=26 ) 
圖 4-6 視差較小的 Breakdancer，DCVP 背景的錯誤補償導致 PSNR 較低 
 
12 
 
381-395, Jun. 1981. 
[16] Joint Video Team (JVT) of ISO/IEC MPEG & ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and 
ITU-T SG16 Q.6), “JSVM Software Manual,” JSVM 9.18, 19 Jun. 2009. 
七、 計畫成果自評 
本計畫執行已達成原計畫提案書所列工作項目:(1)研究如何在解碼端參考左
右視角畫面的深度資訊，利用對應矩陣(Homography Matrix)投影方式，或是採用
影像深度繪圖(Depth Image-Based Rendering)方式，產生相對應的左右兩個虛擬畫
面資訊，最後再合成所要的畫面。因此當觀察者任意切換所要觀察的視角時，透
過此一即時合成演算法能快速產生出虛擬的畫面資訊，以提供所需視角的視訊。
(2)研究以深度資訊在 MDVC 架構編解碼，避免高複雜度的運算並且降低碼率。
研究人才在多視角視訊以及多媒體編碼的技術上獲得很真實的視訊信號處理經
驗。本計畫相關的具體成果說明如下： 
(1) 綜合評估：本計畫產出相當多具有學術與應用價值的成果，並執行相關的產學
計畫，目前已發表兩篇 IEEE 期刊論文，除準備投稿期刊論文，並有提出專利。
另外也培育高科技人才，整體成效良好。本年度計畫相關的學術論文發表如下： 
[1] J.-J. Chen, S.-C. Lee, C.-H. Sun, and J.-J. Jhuang, “The multiple description video codec 
with a residual distributed video coding,” IEEE Trans. Circuits & Systems for Video 
Technology, vol. 22. no. 5, pp. 754-768, May 2012. (SCI impact factor: 2.548, Ranking: 
24/244)  [SCI, EI] 
[2] J.-J. Chen, C.-R. Su, and W. Eric L. Grimson, Jun-Lin Liu, and De-Hui Shiue, “Volume 
image foreground segmentation method by dual multi-scale morphological reconstructions 
and retrieval applications,” IEEE Trans. Image Processing vol. 21, Issue 2, pp. 828-843, 
Feb. 2012. (SCI impact factor: 2.848, Ranking: 18/244) [SCI, EI] 
[3] C.-R. Su and J.-J. Chen, “Content-based image retrieval on reconfigurable peer-to-peer 
networks,” IEEE Workshop Multimedia Signal Processing, Sept. 2012. [EI] 
[4] Hsin-Teng Sheu, Jie-Ci Yang, Yu-Feng Hsu and J.-J. Chen, “A statistical method for 
generic image segmentation,” in Proc. Computer Vision Graphics and Image Processing, 
Sec. E6-1, Aug. 12-24, 2012. 
[5] Chun-Chi Chang, Han-Yen Yu and J.-J. Chen, “An active face recognition based 
human-machine interface of IPTV recommender,” in Proc. Computer Vision Graphics and 
Image Processing, Sec. C5-5, Aug. 12-24, 2012. 
[6] J.-J. Chen et al., “An IPTV based program recommender based on group profiles,” in Proc. 
Computer Vision Graphics and Image Processing, Sec. Poster & Demo, Aug. 12-24, 2012.  
(2) 產學計畫(計畫主持人：陳建中) 
[1]  “依群體收視喜好之機動節目推薦系統”，工業技術研究院資訊與通訊研究所 FY100
分包學術機構研究計畫,金額：500,000 執行期間：100/1/1-100/12/31. 
(3) 專利提案 
[1] Chung-Hsien Kuo, Jiann-Jone Chen, Yan-Yen Yu, and Huw-Yi Lin, “Human-environment 
interactive system and portable device using the same,” US Patent in application  2011 
[2] 郭重顯、陳建中、游函諺、林華毅，“人境互動系統與應用其之可攜式裝置”。  
中華民國專利申請中  2011 (NTUST:0990134TW 事務所案號 37373-TW-PA) 
如分散式編碼法來有效的壓縮影像特徵，進一步則利用分散式資料庫的架構開發
高效影像檢索引擎。 
 
第二天早上的議程為多媒體系統的 poster session，主要的主題有電腦圖像技
術應用到可攜式裝置上、以多執行緒實現 JPEG2000 編解碼系統、運用雲端技術
於 virtual machine 的應用上、以 snake 技術應用於影像切割、3D motion 
estimation、dual channel noise reduction for sparse representation 等。接下來的議程
為 media sharing for social networks，相關的內容有應用於 youtube 上的推薦表
列、無線多媒體的安全管理、互動式視覺搜尋技術、EXTRACTING SOCIAL 
VALUES AND GROUP IDENTITIES FROM SOCIAL MEDIA TEXT DATA 等，這
些主題屬於觀念式的分享，比較少有技術性的描述，但是對於未來 social network
上的媒體互動，有很好的平台與安全管理的議題。會議到中午，主辦單位當天下
午沒有安排論文報告，而是安排的當地的遊覽活動，參觀洛機山脈的瀑布以及路
易斯湖(lake louise)，據傳是當年建築鐵路的工頭發現了路易士湖，是接近雪山源
頭最近的高山湖泊，整體景致相當壯觀與清幽。加拿大在維護自然資源的努力相
當用心，因此雖然班夫的居民只有八千人，每年可以吸引三百萬至五百萬的觀光
客。本參觀活動由 session chair 親自帶隊，除了幫忙解說也和與會人員討論與聊
天。 
 
第三天中午的 panel discussion，由三位學者專家引言，探討多媒體信號處理
的過去五年的主題，以及未來的發展趨勢，主要的研究能量應會著重在 web-scale
的多媒體應用、雲端多媒體應用、虛擬實境之多視角視訊處理等。另外香港理工
大學的 Prof. Oscar C. Au 針對研究生找尋好的研究題材，有很多獨到的見解，與
會的學生與老師都很熱烈討論這個問題。雖然沒有得到什麼結論，但是大家的意
見經驗交流，難能可貴。第三位專家講到如何選定最有潛力的研究主題，他用圖
示個概念說明如何從少數研究主題中找到共同所必須的關鍵技術，他並舉例聲音
信號的處理乃是電腦、人機介面、以及多媒體通信等記個主要領域的交集。概念
簡單卻相當有說服力。我的論文排在第三天的海報展示發表。本人的發表論文題
目是「Content-based image retrieval on reconfigurable peer-to-peer networks」為本
實驗室多年的研究成果，將影像檢索技術有效的在點對點網路上實現，可以將檢
索的規模從傳統的主從式擴大到點對點網路規模，因此比較符合現在網路連線普
遍的情況。在展示期間，來與我討論的先進很多，整整兩個小時幾乎都有人與我
交流，本來也從這些不同領域專家學者的交流中，學到許多不同的思維，獲益良
多。Poster session 結束後，接著是最後一個議程，主要的研究方在 social network
應用上相關的規劃與技術開發，因為這個主題牽涉很多領域的知識，因此引起很
多討論。例如頻寬與傳輸的考量，資料存取權限與加密的規劃、以及介面設計等。 
 
三、 與會心得 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/10
國科會補助計畫
計畫名稱: 以分散式架構處理多視角視訊資料之研究(II)
計畫主持人: 陳建中
計畫編號: 100-2221-E-011-156- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計畫之執行，研究團對另有組隊參加全國微電腦競賽及校園軟體競賽，獲得
第二名及第三名的佳績。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
