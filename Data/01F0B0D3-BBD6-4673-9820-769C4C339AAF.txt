 I 
 
摘要 
在本研究中，我們設計了兩個系統。於第一部份，我們描述我們所提的數位
影像防震系統。數位影像防震會通常捨棄邊緣的像素而輸出一個較小的影片。此
處我們呈獻一個新的數位防震演算法，可以藉由像素充填技術來維持影片中原有
的影像大小。我們所提的演算法能藉由直接估計轉換鏈中相對於一張固定畫面的
全域運動來消除積累的誤差。為了節省全域運動估計的計算量以及處理大動作，
我們採取了特徵比對。實驗結果顯示，我們所提的演算法能製作出穩定且對準得
較好的全幀影片。於第二部份，我們描述一個視訊物件切割技術來支援影像合成。
視訊物件切割是多媒體編輯與分析中不可或缺的作業。一般來說，使用者提供一
些關於前景與背景的提示，然後物件就可以被切割出來。大多數前人的方法不是
高計算量就是高度依賴使用者互動，而那些假設背景不動的方法其應用也有限。
我們提出了一個新的視訊物件切割系統，結合了基於 MRF 的輪廓追蹤演算法與
graph-cut 影像切割演算法。輪廓追蹤能在畫面之間傳遞目標物件的形狀，而
graph-cut 能使該形狀更完美以及提高視訊切割的準確度。實驗結果顯示，我們的
切割系統有效率，而且所需的關鍵畫格以及使用者互動皆更少。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 III 
 
摘要................................................................................................................................ I 
Abstract ......................................................................................................................... II 
Part I ............................................................................................................................... 1 
1. Introduction ........................................................................................................ 1 
1.1. Related Work ........................................................................................... 1 
1.2. Proposed Approach ................................................................................. 2 
2. Overview of the Proposed Algorithm ................................................................ 3 
3. Global Motion Estimation.................................................................................. 3 
3.1. Feature Matching and Local Motion Vectors ......................................... 4 
3.2. Model Parameters Estimation ................................................................. 5 
3.3. Error Detection and Handling ................................................................. 6 
4. Motion Compensation ........................................................................................ 7 
5. Frame Composition ............................................................................................ 7 
6. Experimental Results ......................................................................................... 8 
7. Conclusion and future work ............................................................................... 9 
References ............................................................................................................ 11 
Part II ........................................................................................................................... 12 
1. Introuduction ................................................................................................ 12 
2. Related Work ................................................................................................... 13 
2.1 Segmentation.......................................................................................... 13 
2.2 Matting ................................................................................................... 13 
3. System Overview ............................................................................................. 14 
4. MRF-Based Contour Tracking ......................................................................... 15 
5. Graph-Cut Image Segmentation ...................................................................... 18 
5.1 Graph Formulation ................................................................................. 19 
5.2 Energy Functions ................................................................................... 19 
5.3 Temporal Consistency ........................................................................... 20 
6. Experiment Results .......................................................................................... 21 
6.1 Evaluation of Contour Tracking ............................................................ 21 
6.2 Evaluation of Energy Functions ............................................................. 22 
7. Conclusion ....................................................................................................... 25 
References ............................................................................................................ 26 
Published Paper List .................................................................................................... 29 
 2 
 
 
After image warping, the undefined pixels must be filled to preserve the image 
size. The digital inpainting technique described in [10], [11] recovers the undefined 
pixels by solving partial differential equations numerically. However, it has high 
complexity and inconsistent performance between frames. Some approaches [12], [13] 
fill the undefined pixels by sampling spatio-temporal volume patches. Such 
approaches require a long video sequence to increase the success rate. Matsushita et al. 
[7] proposed motion in-painting that warps neighboring frames and propagates optical 
flow to stitch multiple images. Although the optical flow may compensate the GME 
failure in some cases, the inaccuracy of the optical flow algorithm may still cause se-
rious artifacts.  
1.2. Proposed Approach   
Accurate global motion estimation is required for digital image stabilization 
systems to generate full-frame stabilized videos. Therefore, the proposed approach 
aims at improving the accuracy of GME and the performance of image stitching. In 
previous work, the global motion between non-adjacent frames is computed by 
 4 
 
represented by 
 
Let (x, y) and (x’, y’), respectively, be the coordinates of a point in the current 
frame and the target frame. We have 
 
The advantage of the perspective model is that it can describe the global motion more 
accurately than geometric and affine model since it has more degrees of freedom.  
3.1. Feature Matching and Local Motion Vectors   
We use local motion vectors to estimate the global motions of neighboring 
frames. To find local motion vectors between frames, we use a two-stage feature 
matching algorithm [5]. In the first stage, it scans the current image and the associated 
target image to detect feature points and generates feature descriptors. In the second 
stage, the matching algorithm finds the corresponding feature point in the target 
image for each feature point in the current image. The local motion vectors are 
computed from the pairs of corresponding feature points. Note that once feature 
descriptors are extracted from the current image, they can be repeatedly utilized to 
find the motion vectors for different target images. In comparison to the block-based 
methods, which require performing whole GME again when the current or target 
image is changed, the computational cost is saved in the proposed approach.  
In the proposed algorithm, we adopt the SIFT [5] algorithm to detect feature 
points and generate feature descriptors. The features detected by these algorithms are 
highly invariant to illumination changes and image transformations. Thus, the 
proposed algorithm is stable even for frames with large motion.  
For each feature point extracted from the current frame, the corresponding 
feature point can be found by searching the nearest neighbor in the feature descriptor 
space. A KD-tree [14], [4] data structure is used to index feature descriptors for each 
frame in video sequence to accelerate searching speed. With the help of the KD-tree, 
the cost of searching target can be significantly reduced to a small fraction of the 
exhaustive search. 
 6 
 
 
3.3. Error Detection and Handling   
Motion blur in the video may suppress the number of feature points generated by 
the feature detection algorithm. A camera with acceptable shutter speed can generate 
videos without extreme blur. However, for low-end cameras or when shooting in dark 
environments, shutter speed becomes slow. In such cases, the feature matching 
algorithm would fail due to low feature repeatability.  
Two heuristic methods are utilized to detect errors of the feature matching 
algorithm. One uses the ratio of the number of inliers to the number of all motion 
vectors to measure the quality of GME. If the ratio is lower than 80%, we consider the 
global motion to be corrupted.  
The other method is based on the assumption that the global motions between 
adjacent frames are relatively moderate. In the motion compensation stage, the 
translational components of Tt,s are compared with the ones of Tt,s−1. If any dimension 
of transitional components is larger than a threshold, the global motion Tt,s is 
considered to be corrupted as well. The threshold is defined as 
 
where W and H is the width and height of the image and R is a constant number that 
affects the performance of the proposed algorithm. If R is too large, some GME errors 
may not be detected. If R is too small, the proposed algorithm may suffer from the 
accumulation error due to the correction of the global motions. In the proposed 
algorithm, R is empirically set to 0.05. 
The corrupted global motion parameters can be corrected by utilizing other 
transformations that are related to the current frame and the target frame. For example, 
the corrupted transform Tt,s can be replaced by Tt,kTk,s where k can be searched from 
 8 
 
 
where I is the frame transformed from Is by Ts,tSt. 
After compositing the current image with the neighboring images, the undefined 
pixels may still exist. The number of undefined pixels would be reduced, if we 
increase the neighboring range to cover the entire sequence, but it is computational 
costly. We propose backward composition that utilizes the results of the previous 
frames. It composites the current frame with the previous stabilized full-frames within 
a certain range. Because the previous frames are also composited from their previous 
frames within certain range, an undefined pixel may acquire the value which actually 
belongs to the frame that appears long time ago. Fig. 6 compares the composition 
result without and with backward composition. It can be seen that the undefined 
pixels are fewer when backward composition is applied.  
To deal with remaining undefined pixel, the proposed algorithm diffuses the 
color from the known pixels to the undefined pixels in the fast marching order [15]. 
Because the area of the undefined pixels is small in this step, and the fast marching 
diffusion fills the undefined pixels smoothly, the artifact is hardly sensed by the user.  
 
6. Experimental Results   
We tested the proposed algorithm on various video sequences. It was run on 
Pentium 4, 3.2GHz CPU, at 1 frame/sec by using the SIFT algorithm and 3 frames/sec 
by using the SURF [16] algorithm instead for CIF sequences.  
Raw video sequences were obtained by a DV camera, undergoing translation, 
rotation, and zooming during the video shooting. Fig. 7 shows the result without and 
with error handling. The serious distortion due to blur (shown in the 2nd and 4th 
 10 
 
 
 
 
 
 
 
 12 
 
Part II. Video Segmentation 
1. Introuduction 
Object extraction is a critical task in multimedia analysis and editing. One of its 
important applications is digital composition, in which the object of interest is 
extracted from a video clip and pasted to a new background.  Most video effects in 
movie involve this task. In film making, this task is usually accomplished by chroma 
keying, where principal subjects are first captured against a background consisting of 
a single color and then extracted by a special equipment. As the popularity of digital 
camera increases, users may be more likely to composite video clips at home. 
However, the chroma keying technique needs an expensive workshop and does not 
work for natural video. 
For nature image object extraction, many excellent systems have been proposed 
that allow users to efficiently extract objects either by coarsely tracing the object 
boundary  [1], [2] or by roughly labeling regions that are definitely foreground and 
definitely background [3]–[5]. However, when it comes to video, there are serious 
challenges to such interactive approach. First, it is time-consuming to interact with the 
system frame by frame for the entire video. In addition, the frame by frame 
segmentation results in unnatural artifacts due to the lack of temporal consistency 
between frames [6]. Systems that automatically segment dynamic objects without 
human intervention have been proposed [7]–[9], assuming that the motion of the 
desired object is different from that of the background and extracting objects by 
motion clustering. But such systems often fail to extract meaningful regions, because 
the desired object may not have apparent motion relative to the background. Recently, 
some interactive approaches have been proposed [6], [10]; however, they require 
time-consuming preprocessing to achieve acceptable efficiency. 
In this paper, we propose a novel interactive video object segmentation system 
aiming at applications that demand high quality video object extraction. The system 
allows the user to specify keyframes, then it starts to automatically segment the 
remaining frames. By incorporating automatic contour tracking in the system, the user 
no longer needs to specify hints frame by frame. Moreover, with the tracked contour, 
the system is able to focus on the region near the object boundary and process those 
regions far from the boundary before the segmentation, thereby achieving 
computational efficiency without preprocessing. As a result, the user can check 
whether the resulting video is satisfactory on the fly as the segmentation proceeds. 
The rest of the paper is organized as follows. Section II gives a review of related 
work, and Section III provides an overview of the proposed approach. Section IV 
 14 
 
pixel. 
Many matting methods have been proposed for dealing with natural images 
instead of the images produced by the chroma keying technique. Most matting 
methods for natural images often require a trimap specified by the user that roughly 
partitions an image into three regions: foreground, background, and unknown. Then, 
the composition equation is solved for the unknown region through optimization 
techniques with some assumptions about pixel color distributions [2], [16], [17]. 
The image matting techniques have also been exploited for video object 
extraction. Chuang et al. [18] extended the Bayesian matting method to video by 
using optical flow to interpolate trimaps from keyframes. However, it is quite labor 
intensive to provide trimaps periodically. Wang at el. [6] extended the 2D border 
matting methods designed for images to work with 3D video volumes by preserving 
the smoothness of segmentation across both space and time. In [10], the coherent 
matting method, which uses the binary segmentation as a prior, is adopted to produce 
the alpha matte of the object. In [15], a fast weighted-distance-based technique for 
image and video segmentation and matting is proposed. 
3. System Overview 
The user first indicates an initial keyframe of the input video and uses an 
interactive still image segmentation tool to segment the key-frame, then the system 
automatically segments the target object frame by frame. During the automatic 
segmentation process, the user is allowed to interrupt the system and edit unsatisfied 
frames by modifying the object boundary through a boundary editing tool. After 
editing, the user may continue the automatic segmentation to process the remaining 
frames. Our system framework, which is based on the Markov random field (MRF) 
and shown in Fig. 1, consists of two components: MRF-based contour tracking and 
graph-cut image segmentation. The former predicts the contour of current frame by 
extracting the object contour of previous segmented frame and then estimating the 
motion of polygon vertices. The predicted contour is used as the initial constraint of 
the latter component, where the graph-cut algorithm is applied to the regions near the 
predicted contour so that only the pixels near object boundary are considered in the 
graph-cut algorithm. Note that, after the second component is applied, the segmented 
frame is fed back to the contour tracking component in the next loop to extract new 
object contour. Therefore, topology changes to vertices and edge connections are 
automatically handled by the image segmentation and the contour extraction 
processes. 
 16 
 
           (2) 
where the energy operator E(·)= −log P(·). The first term E(It|Dt, Ct−1) on the 
right-hand side is the likelihood energy determined from the observed pixels. It is 
used to estimate the vertex motion and is defined as 
 
                 (3) 
 
where wL and wG are the weights of the coefficients L and G, respectively. L is a 
measure of the block difference between the current frame and the previous frame, 
          (4) 
where 
                  (5) 
is a mask function, Wt−1v is a 11×11 window in frame t−1 centered at the vertex v, zp is 
the position of pixel p, αp is the label of pixel p, and . The evaluation of L is a 
block-matching process masked by the labels of the previous frame because only the 
foreground object is tracked. We run this block-matching process on GPU to 
accelerate its speed. To avoid the contour from shrinking inward due to the mask 
function, another coefficient G is used to guide the contour vertices to lie at the image 
edges. This coefficient is defined as 
                (6) 
where zv is the position of vertex v, and gtc is the Sobel gradient of the RGB channel c 
of the image It. 
The second term E(Dt|Ct−1) is the prior energy, which is independent of the 
observed pixels. We use it to maintain the contour shape. It is defined as 
                    (7) 
where wF and wS are the weights of the coefficients F and S, respectively. F is used to 
penalize the large vertex motion and constrain the contour velocity: 
 18 
 
 
 
5. Graph­Cut Image Segmentation 
Let  be the estimated contour in the current frame. After performing contour 
tracking,  is obtained by deforming  according to the estimated motion 
vectors . Since the estimated contour is an approximation of the object boundary 
in the current frame, the graph-cut algorithm is employed to segment the target object 
using the estimated contour. The pixels are labeled as foreground or background by 
minimizing the Gibbs energy 
             (11) 
where E1 is the data energy whose value depends only on pixel p, E2 is the link energy 
that describes the relationship between pixel p and its neighbor q, Np is the neighbor 
set of the pixel p, and λ is a coefficient specifying the importance of E2 relative to E1. 
In this algorithm, we need to determine the graph formulation (ie. Np) and the energy 
 20 
 
obtained by referring to the background and foreground histograms, respectively. We 
assume that the color distribution of each frame does not vary significantly over time, 
thus the histogram bins are updated through exponential moving averaging (EMA) 
once a frame has been processed. The data energy terms U2 and U3 are the 
nonparametric energy functions used to model the spatially variation of the 
foreground and the background color distributions. We use the k-nearest neighbor 
algorithm (kNN) to estimate the energy and assume that pixels with similar colors and 
near each other are more likely to have the same labels. Therefore, we consider each 
uncertain pixel p, as well as all the training samples (pixels in the previous frame), a 
5D spatial-color point, 
             (14) 
where (rp,gp,bp) and (xp,yp) are, respectively, the RGB color and the location of pixel p, 
and ws is a weight that determines the relative importance of the spatial coordinates. 
As a result, D2,X is computed as the distance sum of nearest neighbors in the 5D space. 
The method for the computation of D3,X is the same as that for D2,X, but its training 
samples are the foreground seeds ΩF and background seeds ΩB in the current 
processing frame. The selection of foreground seeds (described in Section V-A) is 
performed by considering only w1U1(F)+ w2U2(F) since U3 is not known until the 
foreground seeds are selected.  
2) Link energy: The link energy E2 is used to make the segmentation boundary lie in 
the high contrast region. We use the reciprocal link energy function proposed in [3]. 
However, we normalize the image contrast according to the local color variance of a 
11 × 11 window to boost the contrast of object boundaries that have similar color 
distributions on both sides and to suppress the image edges near the object 
boundaries. 
5.3 Temporal Consistency 
To make the segmentation consistent between frames, we add temporal edges 
that connect the pixels of the current frame with those of the previous frame. To find 
the temporal neighbor of each unlabeled pixel in the current frame, we need to 
determine the backward motion vector of each pixel. This vector is used to locate the 
position of the corresponding pixel in the previous frame. Previous methods [6], [18] 
recommend the use of optical flow algorithms, which involve extensive computations 
to estimate the motion of each pixel. Since the motion of the contour has already been 
estimated through contour tracking, the motion of each pixel can be approximated by 
interpolating the contour motion. In our implementation, the rigid deformation 
proposed in [23] is used for the interpolation because natural objects are often locally 
rigid [24]. 
 22 
 
the object boundary in the current frame. Thus, the use of contour tracking enables the 
graph-cut algorithm to locate the object boundary more precisely. Fig. 5 shows 
additional results to illustrate that the proposed system is capable of dealing with 
object motion as well as significant shape variation. 
6.2 Evaluation of Energy Functions 
We compare the proposed data energy functions with the MM-based energy 
function [6], [10] and the k-means-based energy function [3]. To obtain the GMM and 
k-means energy functions, 10 Gaussian mixtures and 80 clusters are used.  
Fig. 6 shows the results for the energy functions. The brighter regions indicate 
the presence of serious background penalty (for labeling pixels as background pixels). 
We do not show the foreground penalty because it can be obtained by subtracting the 
background penalty from one, see Eq. 13. It can be seen that the global energy 
functions U1, GMM, and k-means erroneously cause some portions of the background 
to have high background penalty. In contrast, the energy function U2 shows more 
reasonable results: Almost the entire background has low background penalty and the 
foreground has high background penalty. The result for the energy function U3 is 
similar to that for U2 except that it is noisier. We also show the result for the hybrid 
energy function E1. It can be see that, as a cue of segmentation, E1 may be more 
useful than the GMM and k-means.  
Fig. 7 shows a visual comparison between the proposed, GMM, and k-means 
energy functions. The GMM energy function has the poorest performance; the 
k-means energy function is much better, but still causes some apparent artifacts. In 
contrast, the proposed data energy function provides the most accurate segmentation.  
Figs. 8 shows the segmentation and composition results for the dancer, and 
walker sequences. A border matting algorithm [4] is applied after segmentation to 
produce natural composition results. In the dancer sequence, the target object has a 
significant shape variation. In the walker sequences, there are large global motions 
and strong edges in the background. Despite of these factors, the target objects are 
correctly extracted. 
 24 
 
 
 26 
 
results from graph-cut is proposed. Our contour tracking can effectively track the 
shape of the target object in between the key-frames, and the proposed energy 
function is more effective than the GMM and k-means energy functions. Furthermore, 
our system does not require any pre-processing, and the user interface of the system is 
simple and intuitive. 
Our current system has the following limitations.  First, it assumes only one 
contour for the target object. If the target object contains holes, the system depends on 
the graph-cut algorithm to extract them. Therefore, if the color distribution of the 
background within the holes is not sufficiently distinct from that of the target object, 
the system may fail. This problem may be solved by adding another type of contour 
for the holes, but the segmentation algorithm should be carefully designed to avoid 
errors when tracking holes that later disappear in the video sequence. 
Another limitation arises when the target object has very fast shape variation. If 
the boundary of the target is quite straight in the previous frame but seriously arched 
in the current frame, the image segmentation algorithm may not  correctly extract the 
boundary of the target object, because the real boundary has moved beyond the 
unlabeled region. In addition to adding new key-frames and increasing the range of 
the unlabeled region, this problem can be alleviated by using splines, such as the 
Bezier curve, instead of polygons, to represent the contour. However, the 
computational cost would increase significantly due to the complexity of spline 
parameter determination. This issue requires further investigation.  
 
References   
[1] E. N. Mortensen and W. A. Barrett, “Intelligent scissors for image composition,” 
in Proc. of SIGGRAPH, 1995, pp. 191–198.  
[2] J. Wang, M. Agrawala, and M. F. Cohen, “Soft scissors: an interactive tool for 
realtime high quality matting,” ACM Trans. Graph, vol. 26, no. 3, p. 9, 2007.  
[3] J. Jia, J. Sun, C.-K. Tang, and H.-Y. Shum, “Drag-and-drop pasting,” ACM Trans. 
Graph., vol. 25, no. 3, pp. 631–637, Jul. 2006.  
[4] M. Gleicher, “Image snapping,” in Proc. of SIGGRAPH, Aug. 1995, pp. 183–190.  
[5] Y. Li, J. Sun, C.-K. Tang, and H.-Y. Shum, “Lazy snapping,” ACM Trans. Graph, 
vol. 23, no. 3, pp. 303–308, 2004.  
[6] C. Rother, V. Kolmogorov, and A. Blake, “Grabcut: interactive foreground 
extraction using iterated graph cuts,” ACM Trans. Graph, vol. 23, no. 3, pp. 309–314, 
2004.  
[7] C. Wang, Q. Yang, M. Chen, X. Tang, and Z. Ye, “Progressive cut,” in Proc. of 
ACM Multimedia.  
 28 
 
matting,” IEEE Trans. PAMI, vol. 30, no. 2, pp. 228–242, Feb. 2008.  
[26] J. Wang and M. F. Cohen, “Optimized color sampling for robust matting,” in 
Proc. of IEEE CVPR, 2007, pp. 1–8.  
[27] Y.-Y. Chuang, A. Agarwala, B. Curless, D. H. Salesin, and R. Szeliski, “Video 
matting of complex scenes,” ACM Trans. Graph, vol. 21, no. 3, pp. 243–248, Jul. 
2002.  
[28] C.-H. Teh and R.-T. Chin, “On the detection of dominant points on digital 
curves,” IEEE Trans. PAMI, vol. 11, pp. 859–872, 1989.  
[29] A. Agarwala, A. Hertzmann, D. H. Salesin, and S. M. Seitz, “Keyframebased 
tracking for rotoscoping and animation,” ACM Trans. Graph, vol. 23, no. 3, pp. 
584–591, 2004.  
[30] R. S. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, 
M. Tappen, and C. Rother, “A comparative study of energy minimization methods for 
markov random fields,” in Proc. of IEEE ECCV, 2006, pp. II: 16–29. 
[31] G. Vogiatzis, P. H. S. Torr, and R. Cipolla, “Multi-view stereo via volumetric 
graph-cuts,” in Proc. of IEEE CVPR, 2005, pp. II: 391–398.  
[32] S. Schaefer, T. McPhail, and J. Warren, “Image deformation using moving least 
squares,” ACM Trans. Graph, vol. 25, no. 3, pp. 533–540, 2006.  
[33] T. Igarashi, T. Moscovich, and J. F. Hughes, “As-rigid-as-possible shape 
manipulation,” ACM Trans. Graph, vol. 24, no. 3, pp. 1134–1141, 2005. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 30 
 
[14] Y.-H. Huang, C.-Y. Chang, and H. H. Chen, “Fast H.264 mode decision using 
previously coded information,” IEEE Int. Symp. on Multimedia, Berkeley, CA, 
Dec. 2008.  
[15] T.-H. Huang, C.-K. Liang, S.-L. Yeh, and H. H. Chen, “JND-based enhancement 
of perceptibility for dim images,” IEEE Int. Conf. Image Process., San Diego, Oct. 
2008.  
[16] C.-C. Cheng, C.-K. Liang, Y.-C. Lai, H. H. Chen, L.-G. Chen, “Analysis of belief 
propagation for hardware realization,” IEEE Workshop on Signal Process. 
Systems, Washington, D.C., Oct. 2008.  
[17] Y.-H. Yang, P.-T. Wu, C.-W. Lee, K.-H. Lin, W. H. Hsu, H. H. Chen, 
“ContextSeer: Context search and recommendation at query time for shared 
consumer photos,” ACM Multimedia Conf., Vancouver, Canada, Oct. 2008  
[18] M.-L. Wong, Y.-L. Lin, and H. H. Chen, “A hardware-oriented intra prediction 
scheme for high definition AVS encoder,” Picture Coding Symp., Lisbon, Portugal, 
Nov. 2007.  
[19] C.-K. Liang, G. Liu, H. H. Chen, “Light field acquisition using programmable 
aperture camera,” IEEE Int. Conf. Image Proc., 233-236, San Antonio, TX, Sept. 
2007. 
[20] P.-S. Tsai, C.-K. Liang, and H. H. Chen, “Image quality enhancement for 
low-backlight TFT–LCD displays,” IEEE Int. Conf. Image Proc., 473-476, San 
Antonio, TX, Sept. 2007.  
[21] C.-C. Su, J. J. Yao, and H. H. Chen, “H.264/AVC-based multiple description 
coding scheme,” IEEE Int. Conf. Image Proc., 265-268, San Antonio, TX, Sept. 
2007.  
 
Trip Report (Oct. 8-20, 2008)  
Homer Chen 
My activities in this trip includes  
1)  Attended the IEEE International Conference on Image Processing, Oct. 12-15. 
2) Visited Dr. Bimal Mathur and Craig Reinhold of Red Digital Cinema, Oct. 9-10. 
3) Visited Qualcomm, Oct. 16. 
4) Visited Prof. Jay Kuo of USC, Oct. 17. 
The International Conference on Image Processing (ICIP), sponsored by the IEEE Signal 
Processing Society.   ICIP is the premier forum for the presentation of technological advances 
and research results in the fields of theoretical, experimental, and applied image and video 
processing. It brought together leading engineers and scientists in image and video processing 
from around the world. Research frontiers in fields ranging from traditional image processing 
applications to evolving multimedia and video technologies were presented in the technical 
sessions of this conference.  
The main purpose of this trip for me was to present our paper entitled “JND-Based perceptibility 
enhancement of dim images” on Oct. 13.  The paper received great attention from the audience.  
In particular, researchers from Sharp, Bosch, and Qualcomm came to have further discussion 
with us after our presentation.  The presentation also inspired a further discussion meeting at 
Qualcomm. I was also invited to chair a session on enhancement and denoising on Oct. 12.    
 
The meeting with Drs. Mathur and Reinhold was to discuss possible solutions to speed up the 
switching of aperture patterns for our light-field camera.   The meeting with Prof. Kuo was to 
discuss my career plan.   
 
  
  
