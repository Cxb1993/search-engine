 2
行政院國家科學委員會專題研究計畫成果報告 
計畫編號：NSC 96-2221-E-150-063 
執行期限：96 年 8 月 1 日至 97 年 7 月 31 日 主持人：黃惠俞   
執行機構及單位名稱：國立虎尾科技大學資訊工程系 
計畫參與人員：魏台軍(國立中正大學資工所)， 
                郭益誠(國立虎尾科技大學光電與材料科技所) 
 
 
一、中文摘要 
 
由於數位科技技術不斷的提升，同時
也帶給人類前所未有的生活模式。人類的
生活已離不開數位科技，科技發展儼然是
未來生活的一個基石，眾多的科技商品之
研發皆是為了帶給人類更舒適的生活品
質。雖科技不斷的進步，但要實現具有人
類判別及認知的智慧系統，尚存在著落差
亟需解決及克服。而本研究主要是利用人
類的智識為根基及相關演算法、技術等而
建置一個可以接近人類識別的主動式註
解媒體內容物之系統，例如:影像中有”
草”，”植物”，”花”等物件。 
首先利用知識及語意建立物體本體特
質，設計一套註解的機制。本計劃欲建構
兩項目(1)影像檢索系統之架設：使用者可
利用物體本體論結構進行影像搜尋。透過
學習機制建立影像之低階特徵與高階特性
之間的關係作最佳的應對，並用物體之本
體論對物體用簡單的單字加以描述。而後
再與資料庫內之影像作比對等一系列的處
理。(2)影像註解：藉由人類預先學習的知
識所建之物體本體論架構，以簡易語辭對
影像內容物給予註解。 
 
關鍵詞：影像註解 
Abstract 
 
Owing to rapidly advance in the digital 
technology, it makes the technical and 
comfortable life different to the past form 
for humans. Hence, the digital technology 
and human life are relative and compact 
together. The development of many 
technological products is to achieve the high 
quality life of humans. Thus, increasingly 
researches are focused on this scheme how 
to use the technology to implement U-life 
for humans. However, the gap is still existed 
and must be overcome between human 
intelligence and science technology. Hence, 
in this research, we try to build an automatic 
image annotation system combined the prior 
knowledge of human beings, semantics, and 
the relative technologies to achieve the 
digital cognition machine.  
In this approach, we explore the use of 
prior knowledge and semantics to construct 
object ontology and further develop an 
annotation mechanism for image. Two goals 
of our approach are presented. The first is an 
image retrieval system construction that 
allows users to search image based on object 
ontology from image database. By means of 
machine learning, it can find the best 
mapping results between the low-level 
image features describing the color, shape, 
position of regions and high-level concept 
forming a simple word termed object 
ontology. The second is an image annotation. 
Based on the object ontology and prior 
knowledge of humans, we present an 
attempt to bridge the semantic gap between 
low-level features of image and the 
high-level concepts by humans. Each image 
of database can obtain the appropriate and 
simply words to annotate via our approach.  
 
Keywords: image retrieval, image 
annotation 
 
二、緣由與目的 
數位相機及攝影機的使用已是相當普
及化也越來越容易操作，對於照片的取得
更是方便，大大改變了以前舊式的照片儲
 4
3. 在兩個相鄰區域之顏色必是可區分。 
利用顏色特徵 hue (H), value (V), and 
chroma (C) 三 成 員 進 行 homogenous 
color-texture regions 的分割處理，在進行
相同屬性的歸類，並以少量的顏色代表。
在顏色代表是以量化的處理方式進行，並
以以自我組織圖(SOFM)之分類法取得合
適群組個數進行像素點的標記處理[2-4]。 
 
 
 
圖一 系統圖 
 
（B）低階特徵處理： 
1. 低階特徵定義 
• Color feature[5]：顏色特徵是最常被採
用在影像搜尋上，常用色彩空間有
RGB，LAB，LUV，HSV，HVC。基
於人類視知覺對色彩的認知是相當直
覺的反應，而 HSV 或 HVC 最接近人
類的反應模式，因此本研究將採用
HVC 色彩空間模式進行顏色特徵處
理。 
• Texture feature：對於處理影像分割
上，紋理特徵提供了相當重要的訊息
描述影像的內涵，例如雲彩，樹木等。
因此，就影像註解處理上，紋理在定
義高階語意是相當重要的特徵之一。
我們在紋理特徵是利用邊界直行圖描
述子，分析邊界的空間分佈。根據上
一步驟所分割出的區域進行 4 個方向
(垂直，水平，450, 1350)邊界計算，
則每個區域既有 4 bins 邊界資訊。 
• Shape:自然影像物體區域比較難以使
用明確的形狀表達，因此我們系統將
以粗略的形體(例如：矩形)呈現外形
特徵。 
• Spatial location：此特徵可用來解決因
色彩及紋理特徵相似時無法區分類別
的一項有效的資訊。在此項特徵中我
們將區域之間相對空間位置定義其空
間相關性，以左，右，上，下表示空
間特徵。 
•  
2. 索引處理 
為了使用物件為基本之陳述方式進行搜尋
之詢問的處理方式更有效，將低階特徵做
索引處理。首先將低階特徵 color feature, 
texture feature, shape, and spatial feature 分
別設定一組描述子代表 (IH, IV, IC), (Is) 
(Iedge), (Ix, Iy)。如此，任一個區域既可用一
描述子 Ik = (IH, IV, IC, Is, Iedge, Ix, Iy) k,在 kth 
區域表示。而這將利用到設定區域之模糊
語意描述子提供低階特徵與高階內涵之間
相關聯性的對應，以利後續處理。 
 
(2). 相 似 度 量 測  (similarity 
measurement) 
以區域為基礎(region-based)的影像註
解系統，相似度的估測有兩層。第一層區
域層主要計算兩個區域之低階特徵之差。
第二層是影像層主要計算在包含多個區域
的兩影像之相似度。由於以計算區域之相
似度，就一對一區域比對而言，首先假設
 6
 
圖三 原始圖 
 
 
圖四 區域分割及語辭 
 
表一 語意描述相對於圖四。*符號表示不正確的描述。 
Figure 4 Semantic Description 
(a)  Reg. 1 This region is a Sky and located in the Up-Horizontal of image.  
 Reg. 2 This region is a Flowers and located in the Bottom-Biased-Right of image. 
(b) Reg. 1 *This region is a Sunset and located in the Biased-Up of image. 
 Reg. 2 This region is a Sky and located in the Center-Biased-Right of image. 
 Reg. 3 This region is a Trees and located in the Biased-Bottom of image. 
 
 
目前對於物體區域的表示是以較簡易的矩
形方式呈現，由於影像中自然景物通常是
不規則的形狀，比較難以真實的外型呈現
其外觀形狀，因而改採用制式的矩形形狀
取代真實外觀，如圖四之所顯示的矩形區
域。而語意的表達也僅以用簡單的物件語
辭及空間相對位置對所分割表示出的區域
給予適當的註解。對於真實的人類認知尚
有在進一步改善的空間，未來也將朝著真
實物體外觀呈現及更貼近人類語意認知的
範疇加以改善。 
 
五、成果自評 
本次計畫的執行主要預達成的目標成果是
基於物體本體論實現對影像之註解系統；
其主要利用人類的智識建立物件的語辭，
並適當的對照片加以註解，方便於管理數
位影像資料庫並可快速的根據所需而找尋
其照片。 
 就目前而言，完成的成果如上節所顯
示部分實驗結果，同時本系統也面臨一些
困難如下所述：顯著物體分割不佳及物體
本體高階語意定義無法包含全部的物件。
因而僅能處理有限的物體個數，本系統僅
MOVIE CLASSIFICATION USING VISUAL EFFECT FEATURES
Hui-Yu Huang
National Formosa University
Department of Computer Science
and Information Engineering,
Yunlin, 632 Taiwan,
e-mail: hyhuang@nfu.edu.tw
Weir-Sheng Shih and Wen-Hsing Hsu
National Tsing Hua University
Department of Electrical Engineering
Hsinchu 300, Taiwan
ABSTRACT
In this paper, we propose an approach to category the ﬁlm
kinds using low-level features and visual effect features. This
approach aims to category the ﬁlms into genres. Our current
domain of study is the movie preview. A ﬁlm preview often
emphasizes the theme of a ﬁlm and hence provides suitable
information for classiﬁcation process. In our approach, we
classify ﬁlms into three broad categories: Action, Dramas,
and Thriller ﬁlms. Four computable video features (average
shot length, color variance, motion content and lighting key)
and visual effects are combined in our approach to provide
the advantage information to demonstrate the movie category.
Our approach can also be extended for other potential appli-
cations, including browsing and retrieval of videos on the in-
ternet, video-on-demand, and video libraries. Experimental
results show the visual effect feature play an important role
for classifying the ﬁlm kinds.
Index Terms— movie genres, shot detection, visual fea-
tures
1. INTRODUCTION
With recent advances in digital video coding and transmis-
sion, larger and larger amounts of digital videos are becom-
ing from various source in every day. The advent of digital
television and internet is yet another motivating factor for au-
tomated analysis of digital video. Although digital video can
be labeled at the production stage, there is still need for auto-
matic classiﬁcation of videos. Manual annotation or analysis
of video is an expensive and arduous task that will not be able
to keep up with this rapidly increasing volume of video data
in the near feature. Hence, people need some technologies
such as video database, video browsing, indexing, and data
mining to manage the videos and discover knowledge from
videos.
Application of scene-level classiﬁcation would allow de-
parture from the prevalent system of movie ratings to a more
ﬂexible system of scene ratings. For instance, a child would
be able to watch movies containing a few scenes with exces-
sive violence, if a pre-ﬁlter system can prune out scenes that
have been rated as violent. In order to effective classiﬁcation
of movie protecting the children download from the internet,
a ﬁlm classiﬁcation processing is needed to develop.
Pickering and Ruger [1] investigated the application of a
variety of content-based image retrieval method to video re-
trieval. Low et al. [2] presented an overview in developing an
integrated and content-based solution for computer-assigned
video parsing, abstraction, retrieval and browsing. The most
important feature of this approach is the use of low-level vi-
sual features as a representation of video content and auto-
matic abstraction process. In order to bridge low-level media
features and high-level semantics, there are many algorithms
proposed to bridge low-level features to object level search,
and then connect the object level to event level. Chua and
Ruan [3] designed a system to support the entire process of
video information management: segmenting, logging, retriev-
ing, and sequencing of video data. It is mainly developed a
semiautomatic tool to divide video sequences into meaningful
shots.
The rest of the paper is organized as follows. The pro-
posed method is described in Section 2. Experimental results
are given in Section 3. Finally, the paper is concluded in Sec-
tion 4.
2. PROPOSED METHOD
In this section, we present the ﬁlm classiﬁcation method us-
ing the movie preview within the feature-based paradigm. For
clustering video categories, we collect all movies which were
played in Taiwan from 2004 to 2006 at [4], there are many
genres can be found such as Action, Adventure, Comedy,
Crime, Drama, Animation, Thriller, War, etc. According to
these data, the Action, Drama and Thriller genres almost oc-
cupy 88% in every year. Hence, if we can classify those of
categories, the great part of movies will be indicated. Based
on this viewpoint, we identiﬁed three major genres: Action,
Drama (including comedy, drama, romance) and Thriller (or
2951-4244-1222-6/07/$25.00 ©2007 IEEE SiPS 2007
Fig. 2. Slow moving effect from movie Silent Hill. (a) Fade
in and Fade out. (b) Dissolve.
Fig. 3. Fast moving effect. (a)A ﬂash occur in a short time
from movie Silent Hill.(b)A short shot from movie i Robot.
ments, we set if there are at least 2 Abrupt Cuts occurred in
0.2 seconds, it possesses a fast moving effect. Two fast mov-
ing effects are shown in Fig. 3.
Abrupt Cut: The total brightness difference will almost
equal to brightness difference at change frame, because there
has only one rapidly change in these durations.
Fade: Film minima brightness can be found in these dura-
tions because the fade must have to change between original
shot to black frames. Besides that, brightness change is grad-
ually increased or decreased.
Dissolve: This technique is hard to determine by bright-
ness. The brightness change can be gradually increased, de-
creased, or unchanged. If a shot change is detected and cannot
recognize as Abrupt Cut or Fade, we set this shot change as
Dissolve. We only want to ﬁnd fast moving effect and slow
moving effect, the fast moving effect is deﬁned as Abrupt Cut
occurred in 0.2 seconds, slow moving effect is including Fade
and Dissolve. Thus, we design a rule to classify these two
change modes (denoted as CH) which present Abrupt Cut
Fig. 4. Film classiﬁcation construction diagram (A two-layer
classiﬁer structure).
and Gradual Change (including Fade and Dissolve). From the
change mode, the neighboring frame’s brightness is denoted
B, the brightness difference between two frames is denoted
Bd, the classiﬁcation rule is deﬁned as:
CH ∈ Abrupt Cut, if 0.8 < T,
CH ∈ Gradual Change, otherwise, (7)
where T = (max(B) − min(B))/max(abs(Bd)) < 1.2.
This threshold boundary is an empirical value for our testing
data. With this classiﬁcation rule and the visual effect deﬁni-
tion, we can extract the fast moving effect and slow moving
effect.
In addition, we will give slow moving effect and fast mov-
ing effect to a visible value in order to represent the visual ef-
fect characteristics. By using the frame numbers which were
detected as fast/slow moving, we calculate the distance be-
tween two neighboring frames and then obtain a new vector.
Quantizing this vector, we calculate the relative histograms
normalized to 1. Then, we can obtain two new features which
are deﬁned as fast moving effect distribution (fv) and slow
moving effect distribution (sv). In our experiments, we set
the total length of visual effect distribution as 100. We will
further estimate the values of these two features denoted Fme
and Sme, and deﬁned as
Fme =
n∑
l=1
l · fv(l), Sme =
n∑
l=1
l · sv(l), (8)
where l is indicated the length index of visual effect distribu-
tion, and n is set 100. If one kind of visual effect values is
never occurred in a ﬁlm, we set its visual effect value as 100.
The visual effect value can serve as the expected value of how
often this visual effect occurred in a ﬁlm. The less the value,
the more occurrence the visual effect.
Motion content: The visual disturbance of a scene can be
represented as the motion content present in it. The motion
content represents the amount of activity in a ﬁlm. Obviously,
action ﬁles would have higher values for such a measure, and
297
Fig. 7. Distribution of visual effect value.
Fig. 8. Distribution of lighting key.
Drama ﬁlm. For color variance, it is a strong correlational
structure with respect to genres. However, in our experiments,
it is hard to crisply distinguish ﬁlm genres. In other words, the
color information is not a critical condition.
Fig. 7 shows the result of visual effect values. The Action
and Thriller ﬁlms tend to lower the fast moving effect value
because there will have more shots about ﬂash or short action
when the fast moving effect occurs. That is, these frames hap-
pened these effects are centralized to the neighboring frames.
Fig. 8 shows the distribution of lighting key feature. From
Fig. 8, the lighting factor in these ﬁlms is an insigniﬁcant fea-
ture, hence, it cannot effectively categorize the ﬁlm cluster.
The Abrupt Cut detection result is presented in Table 1. In
addition, we use two parameters, precision and recall [7], to
evaluate the performance of visual effect detection. Precision
and recall are denoted the accuracy of detection and the abil-
ity of detection, respectively, deﬁned by
Recall =
Nc
Na
× 100%, (12)
Precision =
Nc
Nd
× 100%, (13)
where Nc is the number of correct detected, Na is the number
of actual occurred, Nd is the number of detected. The Grad-
Table 1. Abrupt Cut detection result
Nc Na Nd Precision(%) Recall(%)
467 484 486 96.09 96.49
Table 2. Gradual Change detection result
Nc Na Nd Precision(%) Recall(%)
173 176 232 74.57 98.01
ual Change (including Fade and Dissolve) detection result is
given in Table 2. In order to obtain the classiﬁcation result,
we choose two-thirds ﬁlms for each category randomly for
training the classiﬁcation tree, and we will calculate the deci-
sion threshold Tfm, Tsm, and Tl by Eq. (10), the rest of ﬁlms
are used as the test data. And we repeat this step six times for
all of ﬁlms. The accuracy of this classiﬁer can be measured
by precision [8] as:
precision = t/w, (14)
where t is the number of samples that actually belongs to this
ﬁlm genre and is correctly classiﬁed. w is the number of all
samples that is classiﬁed as this ﬁlm genre. We have to no-
tice that the precision is different in which the estimation of
classiﬁcation performance and detection performance.
We perform the decision rules every time after estimating
the threshold, then it will test the decision rules using other
ﬁlms in test set. The experimental result shows the average
precision is 73.33%. The decision accuracies for each video
category are close to the precision of decision rules.
On the other hand, we think average shot length is an im-
portant feature and will affect to classify processing of the
ﬁlm, but this feature needs a non-linear classiﬁcation method.
Besides that, we also want to verify the performance of mo-
tion content, color variance, and visual effect values. Thus,
we make another experiment with two neural networks and
compare its experimental result. It is indicated the feature
set Fa and Fb with the neural network mechanism. Fa is
denoted the average shot length, lighting, fast moving effect
value, and slow moving effect value. Fb is denoted the aver-
age shot length, lighting, motion content and color variance,
the feature set Fb was proposed in [6].
The neural network can only classify one type of ﬁlm
once, hence we build three networks to perform three gen-
res of ﬁlms: Action, Thriller, and Drama. We choose 2-layer
back-propagation neural network [9] method be a non-linear
classiﬁer. For every time we build a classiﬁer, we choose two-
thirds ﬁlms from the genre as positive (set as 1), and the rest of
ﬁlms as negative (set as 0). For instance, while we are train-
ing a network to classify Action ﬁlm, we choose two-thirds
ﬁlms of Action, a third ﬁlms of Thriller and a third ﬁlms of
Drama. This result is shown in Fig. 9. Table 3 is presented
299
表 Y04 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                     九十六年十一月十七日 
報告人姓名 黃惠俞 
Hui-Yu Huang 
 
服務機構
及職稱 
國立虎尾科技大學 
資訊工程學系 
助理教授 
    時間 
會議 
     地點 
2007 年 10 月 17 日至 10 月
19 日  
中國 上海市  
本會核定
補助文號
NSC96-2221-E-150-063 
會議 
名稱 
 (中文) 2007 IEEE 訊號處理研討會 
(英文) 2007 IEEE Workshop on Signal Processing Systems 
發表 
論文 
題目 
 (中文) 利用視覺特徵之電影分類 
 (英文) Movie classification using visual effect features 
報告內容應包括下列各項： 
一、參加會議經過 
我的論文被選為以 oral 方式進行發表，並安排在十月十八日早上第二場 Multimedia 
Signal Processing & Analysis section (AM11:10~PM1:10)。大會承襲以往會議舉行的模式同一時
段僅有一場會議進行。由於會議地點在大陸，國內並沒有直達之飛機，因而需要在香港轉由
上海航空公司之飛機飛抵目的地上海-浦東國際機場，這是一個近年才興建完成的國際機場。
對於第一次踏上大陸參加大型會議的我格外新奇。會議舉行之地點是位於浦東裕景大飯店
(The Eton Hotel)。會議開幕式準時開始於十六日(三)早上 8:00，主席 Dr.Wenjun Zhang 僅
簡單致歡迎詞並介紹本次研討會特色及歡迎各位學者專家等到上海。隨即進行第一場的 oral 
presentation 及相關的 poster presentation。此次大會議程安排每天有三場 sections、三至四場不
等的 poster sections、及每天皆有一場的 invited talk 進行，整個會議時程的安排相當充實且緊
湊。 
 
二、與會心得 
由於有多次出國參加國際會議的經驗，而至大陸參加國際會議則是本人第一次，所以格外
新鮮事事都充滿新奇，也因為文化相同語言相通而倍感親切。上海是一個國際都會充滿著新
舊的文化衝擊及各種類型的建築，在這對中國近代而言一個佔有歷史地位的地方舉行會議，
附件三
 
表 Y04 3
當踴躍且可觀，且在每一位演講者結束後之 Q ＆ A 時間來賓提出了問題非常的踴躍及熱烈
往往超過既定的時間，會後的討論更充滿在整個場內外。 
 
  我的 oral section 的時間被安排在會議進行的第二天(十八日)上午約 11 點 45 分左右報告，
本 section 共有 6 篇論文進行報告。由於時間嚴重落後，因而所有 Q ＆A 皆在這 section 結束
發問，會後的討論更是熱絡。累積多年參於國際會議之經驗，使自己越來越能得心應手並充
裕的準備所需之所有資料，當然最大的收穫是可得知近來的研究趨勢及學術應用在科技方面
上發展的寶貴的經驗。不外乎又再一次給自己學習的機會，本人秉著戰戰兢兢的態度及學習
的精神參與此次會議。在這將為期三天的會議上多方面的受益。對我而言，經驗累積個人實
力，為自己的未來奠定穩固根基增加研究能力，往後將更加努力於研究方面及多參加國際間
相關研究舉行之會議，拓展自己的視野，增廣自己的科技見聞，也適時的將自己的研究分享
於外。因會議舉行地點靠近黃浦江之外灘，大會的晚宴更是安排在東方明珠塔內之餐廳，可
在用餐之處讓各位前來開會的學者親見上海夜景的驚豔及充滿異國建築之美景，讓人驚歎不
已。 
  考察參觀活動(無是項活動者省略) 
無 
三、建議 
無 
四、攜回資料名稱及內容 
本次會議主要攜回僅有光碟片一片及大會議程時間表（program）。光碟片內包含所有 oral, 
poster papers 全文論文共 299 篇。其主要目錄依日期分列如下: 
 
Wednesday, October 17 
Keynote: Magdy Bayoumi, University of Louisiana at Lafayette, 
USA: Wireless Sensors Networks & DSP: Marriage Made in Heaven 
WL1: Wireless Communications 
WL2: MIMO Systems 
WP1: Channel Coding and Multimedia Signal Processing 
WP2: Network and Communication - I 
WP3: DSP Architectures I 
