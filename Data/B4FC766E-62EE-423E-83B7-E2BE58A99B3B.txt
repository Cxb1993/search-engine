二、研究成果報告
1. Introduction
In the field of computer graphics, the models of 3D objects
are often represented in the form of polygons, because polygon is
a very simple and efficient data structure to use. For representing
more realistic visual effects, numerous polygons are often used for
modeling 3D objects in most CG scenes and VR environment.
Manipulating and rendering these polygons burden the computer
system with very high computational costs. Obviously, the more
polygons are adopted, the higher rendering cost is required.
Although the technologies for accelerating the rendering speed
keep improving, the needs for rendering highly complicated 3D
scenes in real-time are still not fully satisfied.
The main purpose of LOD (Levels of Detail) is to reduce these
computational costs at the rendering stage by switching among the
multi-resolutions of 3D models according to the current needs of
system and environment. The fundamental concept is to compress
the complexity of scene by reducing the amounts of polygons,
according to the relative distance of objects with camera, and the
requirements of system and environment. However, the
appearance of object will become coarser certainly while the
resolution of model is reduced. Therefore, simplifying the
complexity of models and preserving the shape of objects at the
same time is a very important research issue for LOD.
In this paper, we propose a perception-oriented error metrics
for mesh simplification of LOD based on the adaptive 3D skeleton
structures of models. The rest of this paper is organized as follows.
In section 2, background and related works are explained. In
section 3, we describe our system configuration and algorithms. In
section 4, experiment results are shown. Finally, we conclude in
section 5.
Fig 2. Three LOD models with different resolutions are
represented.
2. Background and Related Works
There are many previous research works on LOD reported.
Many of the early research focus on the LOD problem of terrain.
Depending on how they build the data structure of terrain, these
LOD methods can be briefly divided into two approaches: one
uses regular grids to represent terrain, and the other represents
terrain as a network of triangles. Both of these two approaches are
good for constructing the LOD of terrain. Obviously the data
structure of regular grids is simpler and more systematic, but the
data structure of triangles network is more flexible. In comparison
with the former, the latter reconstruct triangles in a more efficient
manner. Most of them are based on a revised Delaunay
triangulation or constrained Delaunary triangulation algorithm.
However, neither of the above two approaches is appropriate for
simplifying the 3D models of general objects.
Other early approaches pre-calculate discrete LODs for each
object in the preprocessing stage, so that the distant object will
adopt coarser resolution. At run-time, it picks up the appropriate
LOD of each object according to the pre-determined criterions.
The advantage of discrete LOD is its simplicity. Also, it fits the
modern graphics hardware well. However, it suffers visual pops
and other problems for simplifying massive models.
Contrary to the discrete LOD, Continuous LOD creates
auxiliary data structure from which a desired level of detail can be
extracted at run time. One of the most representative continuous
LOD methods is the Progressive Mesh, which is proposed by
Hoppe [10][11]. In PM, the sequence of collapsing is pre-
calculated and recorded so that the computing time is greatly
reduced.
Later, Varshney etc. integrate lots of interesting concepts
such as local illumination, screen-space projection, visibility
culling, and silhouette boundaries into a system to solve the model
simplification problem [16]. These advanced approaches take
current viewing parameters into account to select the best
representation for the current view. Therefore, view-dependent
LOD methods have better granularity than continuous LOD
because they allocate polygons where they are most needed.
Among the various related works, the Quadric Error Metric
(QEM) algorithm is one of the most representative view-
dependent LOD methods [12]. QEM algorithm was published by
Michael Garland and Paul S. Heckbert in Carnegie Mellon
University. QEM is a kind of greedy algorithm. On the steps of
model simplification, QEM simplify the lowest cost of concept of
QEM, the quadric error calculation is represented by the sum of
squared distances to its planes. In order to reduce the computation
cost, these fundamental quadrics are summed together, and an
entire of plane is represented by a single matrix Q. The algorithm
itself can be quickly summarized as shown below,
1. Compute the error matrices, Q, for all the initial vertices.
2. Select all valid pairs.
3. Compute the optimal contraction target v for each valid
pair (v1; v2 ). The error vT (Q1CQ2 ) v of this target
vertex becomes the cost of contracting that pair.
4. Place all the pairs in a heap keyed on cost with the
minimum cost pair at the top.
5. Iteratively remove the pair .v1; v2 / of least cost from the
heap, contract this pair, and update the costs of all valid
pairs involving v1.
LOD and model simplification have developed for a long
time, and the simplification with perception consideration have
balls are then connected by a straight line. The connection of
domain balls represents the topological connectivity of the
original model.
3.1.3 Because a straight line which connects two domain points
may be not sufficient for describing the surface curvature variation
of the shape. Therefore, the force field method is applied to adjust
the skeleton path to follow the model's varied form. The force
field method is based on the repulsive force field inside the model.
Finally, the skeleton of model can be saved for further researches.
DCG is a robust algorithm that can successfully extract
skeleton structure from many different types of 3D models.
Furthermore, there are several advantages for us to choose DCG
algorithm to construct 3D skeletons.
First, it is quite intuitive for us to use DCG skeleton to
segment the model into featuring regions as a combination of main
body and branches, since DCG classifies the skeleton nodes as
joint points, end points, and connection points (figure 4). The end
point that connects to only one adjacent node is the boundary of
the skeleton. The point which connects to two adjacent points is
the connection point. The joint point which connects to more than
two points represents the main body of the model. Also, the
hierarchical structure of these points provides us very important
information to describe the shape of model.
Second, the domain ball approach helps us to perform
clustering of vertices. All of the vertices inside, or on the
boundary surface of a domain ball belong to the same skeleton
node, which is the center of the domain ball. Vertices located on
the boundary between two adjacent domain balls will be assigned
a higher weighting value than other vertices.
Fig 4. The skeleton structure of Teddy bear
The importance of skeleton to our algorithm is described as
follows.
1. Dividing the model into several regions. Each region
belongs to one of the three types of subdivision including of joint
part, connection part, and end part. The amount of the regions in
the model equals to the numbers of the nodes of skeleton.
2. The joint parts represent the main body of the model. To
preserve the main shape of the model, it is necessary to reserve the
joint parts.
3. The end parts represent the terminal part of branch. The
end point on the model surface of each branch can be easily
determined by utilizing skeleton.
4. Reservation points can be computed for each connection
parts. Two optimal reserved points for a branch are then
determined. For each branch part of model, the above-mentioned
two reservation points and one end point are associated with
higher weighting values to preserve the perception-oriented
features.
5. The volumetric feature of main body is preserved by
adding the weighting values of the volumetric points. A
volumetric point is the intersection points of the model surface
and a ray which is shot from the viewer to a joint point of skeleton.
3.2 Perception-Oriented Error Metrics
On the basis of skeleton, we analyze the model from the view
point of perception model. Weighting value of each vertex is
adjusted not only according to the geometric and topological
information, but also the above-mentioned perception-oriented
considerations.
We modify the QEM algorithm to compute the quadric error
and implement our LOD methods. The importance of a vertex is
decided by the Q matrix. If we adjust the contents of a Q matrix, it
will then affect the sequence of edge contraction. In the previous
step, we have found out the significant featuring regions and
points via 3D skeleton. Therefore, we can retain these points
during simplifying the model by assigning certain vertices with
higher weighting value and adjusting the Q matrix.
In QEM algorithm, it selects all valid vertices pairs of model
and places all the pairs in a heap for choosing the minimum cost
pair. By always removing the pair with minimum cost, the model
is simplified in a manner of greedy method. However, the way
which QEM determine its error metrics, Q, only focus on the
geometry of model. In another word, the way of collapsing vertex-
pair with the minimum cost is not guaranteed to be appropriate for
shape-preserving.
In our algorithm, we re-evaluate the weighting values of Q
metrics by re-considering the meaning of each vertex from
different view points of human perception model. As mentioned
above, according to the information that is provided by skeleton
structure, we can segment the model into several featuring regions
which is also coincident with the topology of model. Every region
has different properties. The relationships between regions
represent the topology of model. Obviously, it is better to have
these relationships being kept during the model simplification.
Therefore, in the operation of edge contraction, our system will try
to delay the selection of the edge as the valid candidate, if collapse
this edge will caused to lose the topological relationships of the
model.
In QEM, after an edge is contracted, the two end-points of
this edge are replaced by a new point with lower quadric error. In
our method, P-QEM, the same replacing operation is also
performed, but the quadric error of each survived point is adjusted
during model simplification. In order to guarantee that the end
regions could be retained, we would raise the weighting to the
terminal vertices.
However, when the model is simplified extremely, it is
difficult to preserve the shape feature of these branches by just
equally overweighting their vertices. To achieve the above goal,
an approximate triangle for each branch has to be retained.
Therefore, at the beginning of the simplification stage, we have to
determine a vertices set which reserves an approximate triangle for
each branch. To implement this idea, we first determine the
terminal vertex of each branch, which is the nearest vertex to the
intersection point between the model surface and the extending
ray of terminal skeleton. Please note that this terminal vertex is
view-independent. The other two vertices of this approximate
triangle are determined later in a view-dependant manner, as
explained in the following step [D].
Fig 7. The end points of skeleton and reserved terminal vertices
[D] Reservation triangle for branch
As mentioned in step [C], we want to reserve the shape
feature of branches when the model is simplified in low resolution.
In contrast with the conventional QEM algorithm which does not
guarantee the feature of shape could be reserved, we reserved a
least one approximate triangle for each branch as simplifying
process is performed in low resolution. Depending on the DCG
skeleton, a set is pre-determined, which consists of vertices belong
to the boundary between the branch and main-body. Then,
according to the sequence of edge-collapsing and the silhouette
information, the inner priorities among these reserved vertices are
determined on the fly. Consequently, a pair of vertices in this set is
retained. The approximate triangle which represents the branch of
model is then formed by the terminal vertex which is preserved by
step [C], and the pair of reserved vertices which is preserved by
step [D].
Table 3. Pseudo-codes for finding important points of branches
Fig 8. The skeleton nodes (red, yellow, blue), and reserved points
(green) of teddy bear
[E] Volumetric points
We also assign the volumetric points, which are related to the
skeleton, with higher weighting values during simplification. To
implement this idea, we shoot several rays from the viewer to the
joint points of skeleton. The thickness of model is retained by
adding the weighting values of the vertices which is located as the
nearest point to the intersection between the surface and the ray.
Fig 9. The volumetric points
Fig 14. The comparison of horse models. The four legs of horse
are preserved by adopting our method.
4.1 Comparison between Perception-Oriented
and traditional QEM
We design a diagrammatical questionnaire with 5 sets of
questions to verify our work. This test is performed by 80 visitors.
According to the statistics of this survey, we give the graphs of
comparison between perception-oriented QME and conventional
QEM in this section. The outcome of each set is shown in figure
14, figure 19, and figure 20, separately.
The statistic show that, when the solution of starfish reduces
to 10 faces, almost all of the visitors can recognize the simplified
model of PQEM as a starfish, because the silhouette is reserved in
a perception-oriented manner. On the other hand, they fail to
recognize the same model of QEM due to the silhouette is
destroyed.
In this survey, another question for bunny asks whether
visitors can recognize what the model is, as the original model is
simplified to contain only 20 facets. The outcome shows that
almost half of visitors recognize successfully in the case of PQEM
but fail in the case of QEM simplification, as shown in figure 16.
Another question asks visitors to determine the possible original
model intuitively, according to a simplified model provided by
three-point views in figure 15. The statistic result is shown in
figure 16.
In the case of another question as shown in figure 18, when
the model of mouse is simplified to 174 faces, the survey result
shows that 3/4 visitors prefer the QEM simplified result. However,
when the model is further simplified to 128 faces, more than half
of visitors think perception is better. In addition to the above
questionnaire, other comparisons for different models between our
method and the conventional QEM approach are also performed.
Fig 15. Three-point of views of bunny with 20 facets
Fig 18. The comparison of mouse
Fig 19. The comparison of bull
6. Acknowledgement
Special thanks to Professor Ming Ouhyoung, Bing-Yu Chen
and other members of graphics group, CSIE, NTU, for their kindly
help on the DCG algorithm.
7. Reference
[1] P. Heckbert and M. Garland. “Survey of Polygonal Surface
Simplification Algorithms”, In SIGGRAPH 97 Course Notes:
Multiresolution Surface Modeling.
[2] M. Garland. “Multiresolution Modeling: Survey & Future
Opportunities”, Eurographics ’99, Stateof the Art Reports,
September 1999.
[3] Jonathan D. Cohen, “Concepts and Algorithms for Polygonal
Simplification”, SIGGRAPH 99 Course Tutorial #20: Interactive 
Walkthroughs of Large Geometric Datasets. Page(s):C1-C34.
1999. also in SIGGRAPH 2000 Course Tutorial.
[4] Luebke, D.P. “A developer's survey of polygonal
simplification algorithms,” Computer Graphics and Applications,
IEEE Volume 21, Issue 3, Page(s):24-35, May/Jun 2001.
[5] Tan Kim Heok, Daman, D. “A review on level of detail”, 
Computer Graphics, Imaging and Visualization, 2004. CGIV 2004,
Page(s):70–75, July 2004.
[6] W.J. Schroeder, J.A. Zarge, and W.E. Lorensen, “Decimation 
of Triangle Meshes”,ACM SIGGRAPH Computer Graphics, Volume 26, Issue
2, Page(s):65-70, July 1992.
[7] Jarek Rossignac, Paul Borel. “Multi-Resolution 3D
Approximations for Rendering”, Modeling in Computer Graphics.
Springer-Verlag1993. Page(s):455-465.
[8] David Luebke, Carl Erikson. “View-Dependent
Simplification of Arbitrary Polygonal Environments”, Proceedings
of SIGGRAPH 97. Page(s):199-208.
[9] K.L. Low and T.S. Tan, “Model Simplification Using Vertex-
Clustering”, Proc. ACM Symp. Interactive 3D Graphics, 
Page(s):75-82, 1997.
[10] H. Hoppe, “Progressive Meshes”, SIGGRAPH ’96 Conf. 
Proc., H. Rushmeier, ed., Page(s):99-108, Aug. 1996.
[11] H. Hoppe, “View-Dependent Refinement of Progressive
Meshes”, SIGGRAPH ’97 Conf. Proc., T. Whited, ed., 
Page(s):189-198, Aug. 1997.
[12] M. Garland, P.S. Heckbert, “Surface Simplification Using 
Quadric Eror Metrics”, SIGGRAPH ’97 Conf. Proc., T. Whited, 
ed., Page(s):209-216, Aug. 1997.
[13] Bernd Hamann, “A data reduction scheme for triangulated
surfaces”, Computer Aided Geometric Design, Volume 11 ,Issue
2, Page(s):197–214, 1994
[14] Fu-Che Wu, Wan-Chun Ma, Rung-Huei Liang, Bing-Yu
Chen, Ming Ouhyoung, “Domain Connected Graph: the Essential
Skeleton of a 3D Shape”, http://graphics.csie.ntu.edu.tw/DCG/
[15] Nathaniel Williams, David Luebke, Jonathan D. Cohen, Mike
Kely, and Brenden Schubert, “Perceptually Driven Simplification
of Lit, Textured Meshes”, Proceedings of 2003 ACM Symposium
on Interactive 3D Graphics, Page(s):113-121, 2003.
[16] David Luebke, Martin Reddy, Jonathan Cohen, Amitabh
Varshney, Benjamin Watson, Robert Huebner, “Level of Detail 
for 3-D Graphics”, Morgan Kaufmann, 2002.
[17] C. H. Lee, A. Varshney, and David Jacobs. Mesh Saliency.
ACM SIGGRAPH 2005.
