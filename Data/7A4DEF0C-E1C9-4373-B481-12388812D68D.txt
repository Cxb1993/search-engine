2 
 
over 94% of the streaming data were from P2P peers. However, our random back-off publishing 
strategies for cached video segments have synchronization problems, which lead to incomplete 
publishing results. New publishing strategy needs to be developed, and large-scale of experiments 
need to be conducted. 
1. Introduction 
With the increasing prevalence of broadband Internet access, multimedia streaming service has 
been a very popular Internet application in recent years, but the system scalability has always been 
an issue to solve. In the early developments of media streaming applications, client-server 
architecture suffers from scalability problems; as the number of users increases, the servers are 
quickly overloaded [1]. Content delivery networks (CDNs) with strategically placed proxies have 
been developed to balance the loading of the servers, but they are too costly for general applications 
[2]. IP multicast being probably the most efficient vehicle, its deployment, however, is very limited 
due to many practical issues, such as the lack of IP multicast supporting infrastructures and the lack 
incentives for network operators to carry streaming data traffic [3]. Application-level multicast, by 
constructing an overlay network with unicast connections between peer nodes in the system, has 
been proposed to deal with the scalability issue and used in many Internet applications. 
Currently, there are mainly two types of streaming services: live streaming and VoD (video on 
demand) streaming. Live streaming is similar to watching broadcast TV programs; users tune to a 
selected channel, and the users tuning to the same channel synchronously receive the same content. 
By contrast, in VoD streaming, a user selects to watch any video clip any time at the user’s will. 
Therefore, contents delivered to VoD users are asynchronous even for the users watching the same 
video clip.  
Another type of streaming service is time-shift streaming service, which provides the ability for 
a user to watch contents broadcasted in the past; it is like a digital video recorder (DVR) but the user 
does not need to program the DVR in advance. P2P time-shift streaming can be taken as a special 
case of P2P VoD streaming. For P2P VoD streaming, video programs are pre-generated and their 
lengths are known. The video programs are originally stored in dedicated VoD servers. By contrast, 
in P2P time-shift streaming, the video contents are generated constantly and transmitted to live 
program viewers. There may not be dedicated servers to store the video contents for time-shift 
viewers to retrieve them at later time. Therefore, the live program viewers need to coordinately cache 
the video contents, and thus an efficient peer cache management strategy is needed. 
However, there are very few researches on streaming systems that provide both live streaming 
and time-shifted streaming. In this report, we present the design and implementation of a P2P system 
providing both live streaming and time-shift streaming functionality. We also propose a distributed 
cache management strategy to store video contents for time-shift users. In addition, we performed 
experiments on the PlanetLab platform to study the performance and characteristics of our system. 
We believe that our work provides valuable knowledge of P2P live/time-shift streaming system for 
further development on time-shift streaming services. 
4 
 
time. One of the design issues of P2P VoD service is what a peer should cache to share the load of 
the VoD servers and how to find such cached contents from the peers. In P2Cast [9], peers watching 
the same video clip within a time interval form a session in a single-tree fashion, each peer caches 
the beginning part of the video program and a newly joined peer can be patched with the cached 
beginning part from its parent’s buffer. In P2Vod [10], peers form generations, where in each 
generation, peers have synchronized buffer start. A newly joined peer tries to join a generation, or 
form a new generation appended to an older generation. Generations are numbered, from G1 as the 
oldest generation and Gn as the youngest generation. Nodes in these generations excluding the server 
form a video session. In a session, if there is no client that still has the first block of the video, the 
session will be closed, and a new video session is created for newly joined clients. Both P2Cast and 
P2Vod only support start-from-beginning VoD viewing. oStream [11] provides peers the ability to 
watch from arbitrary positions, but since the system inserts new peers into the system, video 
disruption is noticeable on the child nodes of the new peers. 
BASS [12] applied BitTorrent protocol to download video content, with the VoD server to 
support emergency contents, which are too close to the playback deadline but are not arrived yet. 
Their simulation results indicate that this mechanism reduces 34% of the bandwidth of the server 
when the users’ average outgoing bandwidth is about the same as video bit-rate. However, the 
required bandwidth from the server still increases linearly as the number of users increases. 
PONDER [13] also adopts a mesh-based approach similar to BitTorrent, and applies new 
mechanisms to accommodate VoD service. While BitTorrent treats all data units, called chunks, with 
equal importance, PONDER partitions the video into equal sized sub-clips, each of which contains 
hundreds of chunks. The sub-clip close to the playback deadline is given a higher priority to 
download, so that the urgent data can be downloaded first. PONDER also gives up the tit-for-tat 
incentives of BitTorrent; peers are served based only on their needs without considering their 
contributions. This maximizes the amount of data that can be downloaded before the playback time. 
PONDER achieves 70% saving of server bandwidth with users’ average outgoing bandwidth being 
about 80% of video bit-rate, and up to 93% saving for users’ average outgoing bandwidth being 
112% of the video bit-rate. 
2.3 P2P live streaming with time-shift streaming support 
To the best of our knowledge, P2TSS [14], LiveShift [15] and an IPTV variation [16] are the 
few existing researches on providing both live streaming and time-shift streaming. P2TSS presents 
two distributed cache algorithms: Initial Play-out Position Caching (IPP) and Live Stream Position 
Caching (LSP). It allows peers to decide which video block to be cached locally to share with other 
peers. Their simulation results indicate that P2TSS achieves low server stress by utilizing the peer 
resource. However, in IPP, the video availability is not uniform for each video block, while in LSP, 
though the availability is uniform for each video block, it requires extra bandwidth and more 
connections for each peer to fill its distributed streaming cache. 
LiveShift is a software prototype. It is a live streaming system based on a multiple-tree overlay. 
As a peer receives the video contents and the video segment reaches a pre-defined size, the segment 
6 
 
bootstrap server, provider and viewer. The bootstrap server maintains a list of available channels and 
a list of participating nodes of each channel, in order to bootstrap the newly joined nodes. A provider 
is also a source node of live streaming data, and it registers its channel information with the bootstrap 
server. When a viewer joins the system, it first obtains the information of available channels and 
participating nodes from the bootstrap server, and then retrieves the desired video contents for live or 
time-shift streaming. 
Signaling
Live streaming
TS streaming
Channel 1
Live Streaming
Channel 1
Time-shift Streaming
Channel 
Provider
Live 
Viewer
Time-shift 
Viewer
Live 
Viewer
Live 
Viewer
Time-shift 
Viewer
Bootstrap 
Server
DHT
 
Figure 3-1  The system architecture. 
 
Figure 3-2 depicts the block diagram of a node; a node can be a channel provider or a viewer. 
The player-buffer relationship depends on the type of the node. For a provider, the player encodes the 
original video stream into packet stream, and the stream data is put in the buffer for data transmission 
and genertaing its buffering status. For a viewer, the video content is also put in the buffer for data 
transmission, generating buffering status and playback. To share video content among peers, the 
buffered content can be transmitted to other peers for live streaming or time-shift streaming. The live 
streaming part handles the content transmission for live streaming, and cooperates with the time-shift 
streaming part to cache and publish the contents. Transmissions are carried out over TCP connections 
to avoid data losses in the network layer. Kademlia DHT is used to publish the cahed content, and its 
messages are transmitted over UDP packets. 
8 
 
UDP packets are then sent to the video provider, via local loopback interface. The video provider 
measures each packet’s duration. Since it is inefficient to track each packet individually, continuous 
packets received in a second are packed into a video block. Furthermore, in order to support 
time-shift streaming, 10 consecutive blocks, with the starting block’s timestamp aligned to 10’s 
multiples, are packed into a video file for local storage purpose. The file is named after the 
information given by the channel provider, along with a timestamp. For example, a video file with 
name “ProviderName_Channel1_20100620182520” stands for 10 blocks of Channel 1 provided by 
ProviderName, with timestamp 2010/06/20 18:25:20. Figure 3-4 shows the structures of a video 
block and a video file. 
Duration Packet Length Packet length of UDP packet payload
Timed PacketTimestamp Timed Packet Timed Packet ... Timed Packet
Block Length Block Block Length Block ... Block Length Block
Packet with timing information:
Packing timed packet to block:
Packing blocks to file for storage:  
Figure 3-4  The structures of a video block and a video file. 
2.2 Live streaming framework based on DONet/Coolstreaming 
We adopted the design of the latest DONet/Coolstreaming as the live streaming framework to 
deliver live contents. In the following, we present the characteristics of the latest 
DONet/Coolstreaming, and our modifications as well. 
1. Node hierarchy 
Each live steaming node maintains three levels of nodes known to it: members, partners and 
parents. Members give the node a partial view of currently active nodes in the system, but no 
connection is established between the node and the known members. Connections are established 
between partners to exchange block availability information. Parent-child relations are formed when 
connections are established for video block transmission. Apparently a node’s parents and children 
are a subset of its partners set. 
2. Multiple sub-streams 
The video stream is encoded and packed into continuous blocks, each of which is one-second 
long and time-stamped. The blocks are decomposed into S sub-streams, by grouping blocks whose 
timestamps have the same modulo of S. By dividing the stream into multiple sub-streams, each 
sub-stream can be retrieved from different parent nodes independently, which means a node can 
retrieve data from up to S nodes. Figure 3-5 shows a video stream divided into four sub-streams with 
S=4. In our design, the video stream is divided into 8 substreams. 
10 
 
is over a threshold, the node replaces the sub-stream that has the largest difference by subscribing to 
the partner whose timestamp of the substream is the nearest to the average. As shown in the lower 
part of Figure 3-6, the node compares the receiving status in its buffer with a partner’s buffer, and 
can discover that its sub-stream 2 is lagging behind the partner’s sub-stream 2 by three blocks. If the 
lagging range is larger than a certain threshold, which may indicate the parent node is overloaded, 
the parent re-selection procedure is triggered, and a new parent node will be selected to support the 
lagging sub-stream and the original subscription is cancelled. The new parent node can be selected 
from the current partners if there’s any, or from current parents with better buffering status, if there’s 
no available partners. 
...13951
...141062
...151173
...161284
Sub-stream 1
Sub-stream 2
Sub-stream 3
Sub-stream 4
Current Node’s Buffer
...13951
...141062
...151173
...161284
Sub-stream 1
Sub-stream 2
Sub-stream 3
Sub-stream 4
Some Partner’s Buffer
Block not received
Block received
 
Figure 3-6  Comparing sub-stream status in parent re-selection. 
 
3.4 Distributed cache management strategy 
The goal of our distributed cache management strategy is to effectively keep a desired number 
of replicas for the cached contents. The strategy is composed of two parts: publishing/re-publishing 
policy and content caching based on probability. 
1. Content publishing/re-publishing policy 
After a video file is collected for future time-shift viewers, the node publishes the ownership 
information on the DHT. However, the provider node caches all video contents but never publishes 
the ownership information. The provider node would act as a backup node; its cached contents can 
only be accessed at emergency. For example, when a block is 5 seconds to the time-shift playback 
12 
 
This means that each node can perform at least one republishing operation before the record is 
out-dated. The algorithm for random caching and random back-off for publishing is listed as follows.  
01 while(waitngForBlocks) 
02  block = node.receiveBlock(); 
03  buffer.put(block); 
04  if(block and nearby blocks can be dumped) 
05   viewerKnowledge = MAX(parent.size()+partner.size(), viewerCount); 
06   rand = a random integer generated between (0, viewerKnowledge] 
07   if(rand < replicasRequired) 
08    dump blocks to local storage; 
09    random back-off for DHT publishing; 
10    fileOwnerList = DHT.get(filename); 
11    remove out-dated entry and this node’s entry in fileOwnerList 
12    if(DHT.get(filename).size() < replicasRequired) 
13     FileOwnerList.add(this node); 
14     DHT.put(filename, fIleOwnerList); 
15    else 
16     delete dumped file 
17    end if 
18   end if 
19  end if 
20 end while 
Algorithm 3-1  Random caching and random back-off for publishing. 
 
2. Caching based on probability 
To distribute the responsibility of caching streaming contents and keep a desired number of 
replicas in the system, we adopted a probability algorithm to determine whether a file should be 
cached or not. Assume that the system wants to keep R replicas, and the system has N viewers. It is 
clear that each node should cache the received content with a probability of R/N. Since R is a 
constant, the discovery of N is the issue here. 
To estimate N, first, a local knowledge based on the design of DONet/Coolstreaming is used. 
Since each node keeps connections with its partners and parents, these nodes must be active nodes in 
the system. Therefore, the node has the first parameter as the value of the number of partners plus the 
number of parents. In addition, the number of the current active viewers can be obtained by a 
modified node-startup procedure. When a node joins the system, heartbeat messages are periodically 
sent to the bootstrap server to update the membership cache, and the number of active viewers is 
piggybacked to the node in the reply messages. With the two values, N is selected as the larger one of 
the two. The local knowledge helps the node to react fast to the change of active nodes, especially 
when the size of viewer is small, since they would form an almost fully-connected mesh structure; 
and the number of the current active viewers helps the node to make better decisions when the size of 
14 
 
this node’s messaging port number, channel provider’s name and channel description. The 
bootstrap server replies with whether the registration is successful or not. 
(2) Channel List 
A viewer requests for the available channels registered at the bootstrap server. The options 
include the node’s messaging port number, channel provider’s name and channel description. 
The bootstrap server replies with a list of available channels’ provider names and channel 
descriptions. 
(3) Channel Join 
For live streaming, this message is used for channel joining procedure; the options include 
this node’s control port number, channel provider’s name and channel description. The 
bootstrap server replies with a list of currently active nodes in the channel. For time-shift 
streaming, the message is used for DHT joining procedure, where the bootstrap server replies a 
DHT bootstrap node for the DHT bootstrap procedure. 
(4) Buffermap Exchange 
The message is used for buffer map information exchanges between nodes. The options 
includes this node’s control port number and buffer map. The recipient replies with its buffer 
map. 
(5) Sub-stream Subscription 
This message is used for sub-stream subscription. The options include this node’s 
messaging port number, block transmission port number, subscribing timestamp and its buffer 
map of subscribing sub-stream. The recipient replies with the subscription result. 
(6) Sub-stream Un-subscription 
This message is used for sub-stream un-subscription. The options include this node’s 
messaging port number and the index of the un-subscribing sub-stream. The recipient replies 
with the un-subscription result. 
(7) Time-shift Block Request 
This message is used for time-shift streaming viewer nodes to request a block from other 
nodes. The options include the node’s messaging port number, block transmission port number 
and its requesting timestamp. The recipient replies with the requested result and (1) if it has the 
block, the requested block is sent to the requesting node, or (2) if it does not has the block, it 
informs the node to request from the source. 
4. Performance Measurements 
4.1 Experiment Environment 
To evaluate the system performance, we performed experiments on PlanetLab, an open global 
research network [20]. The streaming provider was located in the Internet Communication 
Laboratory, NCTU. 48 PlanetLab nodes were used as live streaming viewers, and 16 PlanetLab 
nodes were used as time-shift streaming viewers; most of them were located in the United States. 
16 
 
i.e., received all blocks that they need. The average continuity index is 98.5%. However, Nodes 19, 
26, 29, and 40 experience large numbers of lost blocks; this is due to the underlying TCP errors. Note 
that Node 19 experiences a large number of delayed blocks. The reason is still unclear to us. 
 
Figure 4-1  The distribution of startup delay of live streaming nodes. 
 
 
Figure 4-2  The distribution of end-to-end delay of live streaming nodes. 
18 
 
Fig. 4-5 depicts the distribution of the number of replicas of each cached files on all the live 
streaming nodes and the time-shift streaming nodes. The red bars indicate the number of files that 
have the corresponding number of replicas cached by all the nodes. The green bars indicate the 
number of files that have the corresponding number of replicas successfully published on the DHT. 
The results indicate that our publishing and re-publishing algorithms need to be improved. While 
most video files have more than 10 replicas cached in all the nodes, only a small percentage of the 
files are successfully published 10 times on the DHT. This indicates that re-publishing processes may 
need a longer time to stabilize than we have expected. 
 
Figure 4-5  The distribution of replica counts on all the nodes and on the DHT. 
 
Table 4-2  The sources of time-shift streaming blocks. 
Node Index TS 01 TS 02 TS 03 TS 04 TS 05 TS 06 TS 07 TS 08 TS 09 TS 10 TS 11 TS 12 TS 13 TS 14 TS 15 TS 16 
From peer nodes 3227 2577 3789 0 4893 4830 4178 2742 6014 5356 2904 5330 2516 3524 3667 1796 
From the provider 68 40 48 0 203 276 119 61 167 113 25 417 14 106 15 18 
Failed to get 54 31 32 0 55 23 66 37 75 43 22 20 13 25 12 14 
Emergency 14 0 6 0 148 244 53 8 92 70 3 397 1 1 3 4 
No peer owner 0 9 9 0 0 9 0 16 0 0 0 0 0 80 0 0 
 
 Table 4-2 lists the number of blocks received from different sources for the time-shift 
streaming nodes in the experiment. All time-shift streaming nodes (TS 01-TS 16) have received a 
total number of 59033 blocks. 57343 (97.14%) of the received blocks were served by peer nodes. 
The provider served another 1690 blocks, 1044 of which were emergency handling, 522 were unable 
to obtain from peers, and 123 were not cached by peers. The results indicate that P2P time-shift 
streaming service is feasible since 97% of the video blocks are provided by the peers. 
5. Conclusion and Future Work 
In this report, we had implemented a P2P live/time-shift streaming system and presented a 
20 
 
[9] Yang Guo, et al., “P2Cast: Peer-to-peer Patching Scheme for VoD Service” Multimedia Tools 
and Applications, vol. 33, pp. 109-129, 2007 
[10] T.T. Do, K.A. Hua, and M.A. Tantaoui, “P2VoD: providing fault tolerant video-on-demand 
streaming in peer-to-peer environment” Communications, 2004 IEEE International Conference 
on, vol. 3, pp. 1467-1472, Jun. 2004 
[11] Yi Cui, Baochun Li, and K. Nahrstedt, “oStream: asynchronous streaming multicast in 
application-layer overlay networks” Selected Areas in Communications, IEEE Journal on,  
vol. 6, no. 1, Jan. 2004 
[12] C. Dana et al., “BASS: BitTorrent Assisted Streaming System for Video-on-Demand” 
Multimedia Signal Processing, 2005 IEEE 7th Workshop on, pp. 1-4. Nov.2005 
[13] Yang Guo et al., “PONDER: Performance Aware P2P Video-on-Demand Service” Global 
Telecommunications Conference, 2007. GLOBECOM '07. IEEE, pp. 225-230, Nov. 2007 
[14] S. Deshpande, and J. Noh, “P2TSS: Time-shifted and live streaming of video in peer-to-peer 
systems” Multimedia and Expo, 2008 IEEE International Conference on, pp.649-652. Jun. 2008 
[15] F.V. Hecht et al., “LiveShift: Peer-to-Peer Live Streaming with Distributed Time-Shifting” 
Peer-to-Peer Computing , 2008. P2P '08. Eighth International Conference on, pp. 187-188, Sept. 
2008 
[16] D. Gallo et al., “A Multimedia Delivery Architecture for IPTV with P2P-Based Time-Shift 
Support” Consumer Communications and Networking Conference, 2009. CCNC 2009. 6th IEEE, 
pp. 1-2. Jan. 2009 
[17] P. Maymounkov and D. Mazi`eres, “Kademlia: A peerto- peer information system based on the 
XOR metric.” Electronic Proceedings for the 1st International Workshop on Peer-to-Peer 
Systems, Mar. 2002 
[18] Plan-x, http://www.thomas.ambus.dk/plan-x/routing/ 
[19] VideoLAN – VLC Media Player, http://www.videolan.org/vlc/ 
[20] PlanetLab, http://www.planetlab.org 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：張明峰 計畫編號：98-2221-E-009-083- 
計畫名稱：點對點實況及移時播放之影音服務 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 0%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
