?? 
?????????????????????????????????
???????????????????????????????????
???????????????????????????????????
??? 
?????????????????????????????????
????????????????????????????????给??
???????????????????????????????????
???????????????????????????????? 
?????????????????????????????????
???????????????????????????????????
?????(1) ?????(2) ?????(3) ?????(4) ?????
??????????????????????????????? 
 
???? 
?????????????????????????????????
???????????????????????????????????
???????????????????????????????????
??????????????????????????????????
2 
???????????????????????????????????
????(???????)???????????????????????
??????????????? 
?????????????????????????????????
???????????????????????????????????
???????????????????????????????????
??????(Graph)?????????????????????????
?????? 
?????????????????????????????????
???????????????????????????????????
???????????????????????????????????
???????????????????????????????????
????? 
?????????????????????????????????
????????????????????????????????????
????????????????? 
 
4 
The advantage of this method is that it is simple 
and fast. The algorithm is described in the 
following.  
 
i.  Do median operation on the background to 
produce an average background matrix Bave. 
 
∑ ∑−+
−−=
−+
−−=


=
2/)1(
2/)1(
2/)1(
2/)1(
2 ),,(
1),,(
ni
nii
nj
njj
ave kjiBn
kjiB 
 (c) 
ii.  Form the average matrix Aave with the same 
window size using the image with moving 
object A. 
Figure 1. (a) Input video frame  (b) Extracted 
silhouette  (c) Smoothed silhouette 
 
3. Skeleton Extraction  ∑ ∑−+
−−=
−+
−−=


=
2/)1(
2/)1(
2/)1(
2/)1(
2 ),,(
1),,(
ni
nii
nj
njj
ave kjiAn
kjiA   
 To obtain pose information from the silhouette, a 
representative skeleton with key points is desired. 
In order to have a more efficient system, a 
thinning algorithm is chosen for producing such 
a skeleton. Here, the Z-S algorithm is used [6]. 
The Z-S algorithm adopts a peeling approach. It 
is fast and it can avoid the break-line problem 
[6]. But generally speaking there are still 
common problems for the thinning algorithm. 
First, it can produce loops, corners, and 
redundant line segments (Figure 2). In part (a) of 
Figure 2, the lower-right corner produces a noise 
branch. On the other hand, a loop is formed 
between the hands and the body in part (b) of 
Figure 2. Also, it is sensitive to noise.  
iii.  Subtract background image Bave from the 
image with moving object Aave to get matrix 
C.  
iv.  Add absolute differences of R, G, and B 
intensities in each pixel in the resulted 
matrix C to form a new two dimensional 
matrix D.    
 )3,,()2,,()1,,(),( jiCjiCjiCjiD ++=
 
v.  Find the maximum value in matrix D. 
vi.  Multiply each pixel by a constant value so 
that the maximum value of D becomes 255. 
vii.  If any number in D is greater than 
Th_Object, set matrix Obj to 1. Otherwise, 
set it to 0.   

 >=
else
ObjectThjiDif
jiObj
,0
_),(,1
),(  
         
 
In this research, the video clips of standing 
long jumps were taken in a studio with a black 
background. By doing so, the light sources can 
be controlled and are more stable. An example of 
extracted object by the above-mentioned 
algorithm is shown in Figure 1(b). The result is 
further smoothed by a median filter and the 
smoothed silhouette is in Figure 1(c).  
(a) (b) 
 
Figure 2. (a) Corner problem  (b) Loop 
 
To conquer the above-mentioned problems, 
methods in [7] are used to produce a better 
skeleton. First, the thinning result is converted 
into a graph. Then, the adjacent junction vertices 
are removed from the skeleton graph. An 
adjacent junction vertex is the vertex that has 
more than one junction vertices in its eight 
neighbors. This procedure can remove redundant 
vertices to ensure that the degree of each node in 
the graph is less than or equal to 4 to simplify 
the graph structure.  Furthermore, there will 
always be only one path between any two 
vertices. Next, to avoid loops in the skeleton 
graph, a “maximum” spanning tree is built. It is 
similar to the minimum spanning tree, but 
maximum length instead of minimum length is 
chosen while the tree grows. Loops will be cut 
with this procedure. One example is shown in 
Figure 3. In part (b) of Figure 3, the loop is cut 
near the lower-left corner and is marked in green. 
 
 
(a) 
 
(b) 
6 
cases. So a jumping stage flag is necessary. The 
structures of one BN and the DBN are shown in 
Figure 7. Part (a) of Figure 7 presents a BN for 
the “standing & hand swung forward” pose. 
There are totally 22 defined poses in our work. 
Part (b) of Figure 7 shows the influence of the 
current jumping stage and the previous pose on 
the determination of the current pose.  
 
 
(a) 
   
 
(b) 
Figure 7. (a) Bayesian network structure (b) 
Dynamic Bayesian network structure  
In the Training Phase:    
When the first frame enters, we reset the 
jumping stage to “before jumping” and the 
current pose to “standing & hand overlap.” In 
the later frames, the previous pose is the 
predicted pose in the previous frame. Similar 
poses from the “before jumping” stage and the 
“landing” stage can be distinguished by the 
jumping stage flag. At every training phase, we 
input the locations of Head, Hand and Foot, 
respectively. Next, the path from Head to Foot is 
used as the torso, and the waist location can be 
estimated. The waist location is set to be in the 
middle of the torso. Then the waist is put as the 
origin and the locations of all key points (Head, 
Chest, Hand, Knee or Foot) on the eight areas of 
the plane can be determined (Figure 6). These 
locations are combined as the feature vector. 
Once the feature vector is received, the DBN can 
update the relation strength between the current 
pose and the previous pose. 
 
In the Testing Phase: 
When the first frame enters, we do the same 
thing as when the first frame is entering the 
training phase. And we set the lowest point to be 
Foot because no matter what pose it is Foot is 
always the lowest point. Then we try to assign 
body parts to other key points and to combine 
them as the feature vector to infer the most 
probable pose. Once the feature vector gets the 
maximum probability, we say that the pose 
represented by this feature vector is the most 
fitted pose.  
One thing needs to be discussed next is that 
different poses in the training samples do not 
appear equally. For example, “Standing & hand 
swung forward” appears most of the time. But 
“Knee and foot extended & Hand raised 
forward” and “Waist bended & Hand raised 
forward” may appear much less frequently than 
“Standing & hand swung forward.” Thus a 
threshold called Th_Pose is set for each pose 
other than “Standing & hand swung forward” to 
emphasize more on these poses. Otherwise, 
“Standing & hand swung forward” would 
dominate the decision making. If the probability 
of certain pose is greater than Th_Pose, we say 
that the pose appears. When moving to the next 
frame, the current pose will be input to the next 
frame as the previous pose.  
With a proper training on the Bayesian 
networks and the dynamic Bayesian network, the 
trained networks can find the most probable pose 
for each frame. The poses in consecutive frames 
can then be used to identify incorrect 
movements.  
 
5. Experimental Results 
 
The DBN is trained by 10 video clips and tested 
by three other ones. The major portion of one 
tested video clip is shown in Figure 8 to 
demonstrate the thinning results. Among the ten 
poses in the figure, eight can be classified 
correctly by the dynamic Bayesian network. The 
two poses surrounded by a dashed square cannot 
be determined and are marked as Unknown. The 
reason might be that the number of training 
samples is not enough.  The probabilities of 
these poses are not large enough to be accepted. 
Moreover, each time an Unknown appears, the 
previous pose for the next frame will be set to 
the pose that is recognized most recently for 
each Bayesian network. From our experience, 
this is really useful in dealing with this problem.  
 
8 
Dynamic Bayesian Network,” IEEE 
Transactions on Multimedia, Vol. 8, No. 1, 
749-760, Feb. 2006. 
[9] David Edwards, Introduction to Graphical 
Modelling, 2nd edition, Springer, 2007.
 
 
?????? 
?????????????????????????????????
???????????????????????????????????
???????????????????????????????????
???????????????????????????????????
??????????????????????????????? 
???????????????????????????????
(Real-time)?????????????(1)?????????????(2)
???????????????????????????????????
???????????????????????????????????
????????????????????????????????????
?????????? 
 
10 
Protein Secondary Structure Prediction by the 
Gamma Neural Model 
  
Hui-Huang Hsu1, Jian-Tsang Chang1, and Chun-Jung Chen2 
1Department of Computer Science and Information Engineering, Tamkang University 
2Department of Computer Science, Chinese Culture University 
Taipei, Taiwan, R.O.C. 
email: h_hsu@mail.tku.edu.tw 
  
 
Abstract - One way to understand a protein’s function is to 
infer from its structure. However, it is not easy to obtain the 
three-dimensional structure directly. So to predict 
substructures like alpha helices, beta sheets, and coils from the 
protein sequence is the first step to decode the mystery of a 
protein. It is straightforward and was investigated, but 
improvement is still needed in this problem. In this paper, the 
gamma neural model for context analysis is used. Also, a new 
encoding method for the protein sequence is proposed and 
discussed. The results are compared with the 
traditionally-used time -delay neural networks and another 
encoding method in the field. The gamma neural model not 
only can reduce both space complexity and time complexity 
but also improves the overall prediction accuracy. The 
proposed encoding method, on the other hand, shows its 
usefulness in improving the prediction rate of beta sheets.  
  
I.  INTRODUCTION 
  
The comp osition of proteins in an organism controls its 
functioning. There are hundreds or even thousands of proteins 
in an organism. To understand the function of respective 
protein and even the interaction between proteins is desired. 
The field is named as proteomics. It examines the functioning 
of proteins in an organism [1].  
A protein’s function can be determined by its structure. 
X-ray crystallographic and NMR are two techniques to 
visualize the three dimensional structure of the protein. 
However, they are both expensive and time -consuming. It 
takes weeks or even months to decide a protein’s structure 
from either of the two techniques. Another problem with the 
techniques is the resolution. Details might be missing in the 
results. On the contrary, with the advances of bio -techniques 
in the past decade, the primary structure, i.e., the sequence of 
the protein in amino acids, can be found by high-throughput 
methods. It is fast and cheap to determine a protein’s amino 
acid sequence. So it is desirable that the protein structure can 
be inferred simply from the protein sequence.  
We can say that in nature the primary structure determines 
the secondary structure and the secondary structure decides 
the tertiary structure and then the quaternary structure. One 
more force or interaction is placed to the protein to have the 
next higher level structure when the four levels of structures 
are considered. The function of a protein resides in its tertiary 
and quaternary structures. However, it is a nontrivial task to 
predict the tertiary structure or quaternary structure of a 
protein. Different methods have been tried. Here we only 
focus on the neural network techniques with the sequence 
information of the protein. It is desired that the secondary 
structure can be predicted from the sequence and the tertiary 
structure then can be determined from the secondary structure. 
So to decide the secondary structure from the sequence is the 
first step.  
There are three major secondary structures: alpha helices, 
beta sheets, and coils. They are the substructures of the 
tertiary structure. The protein sequence is composed of 20 
amino acids. Each amino acid is named with a unique 
one-letter code. A terminus code is added to indicate the two 
ends of the sequence. Thus twenty-one-bit binary numbers can 
be used to encode the amino acids in the sequence. The 
time-delay neural network (TDNN) is generally used to 
classify each position of the amino acid sequence into the 
three substructures. However, it is hard to decide a proper 
window size for the TDNN. In this paper, the gamma neural 
model that is adaptable in memory depths is tested for further 
improvement of the prediction accuracy [2][3]. Also, to gain 
more information on the input to the neural network, 
chemistry properties of the amino acids are taken into 
consideration for encoding the sequence.  
The rest of the paper is organized as follows. Section 2 
briefly introduces the definitions of the prediction rates, the 
data set, and the common encoding method. Section 3 
presents the proposed encoding method. Section 4 describes 
the gamma neural network for protein secondary structure 
prediction. Section 5 shows and discusses the experimental 
results. And a brief conclusion is drawn in Section  
  
II.  METHODOLOGY 
  
A. Prediction accuracy  
  
The most popular method to judge the accuracy of protein 
secondary structure prediction is the three-state per-residue 
accuracy (Q3). Its equations are as follows: 
 
%1003 ´
++
=
all
c
N
CCC
Q ba   
%100´=
a
a
a N
C
Q  
%100´=
b
b
b N
C
Q   
TH-I2-3 SCIS&ISIS2006 @ Tokyo, Japan (September 20-24, 2006)
- 202 -
window. It is similar to the time-delay neural network besides 
replacing the tapped delay units at the input layer with gamma 
memories. In this section, we will describe how to use the 
gamma neural network for protein secondary structure 
prediction. The details of the gamma neural model can be 
found in [3]. Our aim is to speed up the neural network 
training and hopefully improve the prediction accuracy with 
the strength of adaptable memory depths. Fig. 1 shows the 
structure of the extended gamma neural model for context 
analysis . It can be seen from the figure that the gamma neural 
network replaces the fixed window in the time-delay neural 
network with the causal gamma memory and the anti-causal 
gamma memory to encode the input sequential data. The 
feedforward structure of the rest of the network stays the 
same.  
   
  
 
Fig.1. The gamma neural model for predicting protein secondary structure 
  
 
Fig.2. The memory bank 
 
  
The feature of the gamma neural network is the 
introduction of gamma memories. There are two types of 
gamma memories for context analysis: the causal part and the 
anti-causal part. The gamma memory is a set of memory 
banks, and each memory bank is a tapped delay line with 
locally recurrent connections.  The operation of the memory 
bank is as follows. 
  
M0(i) = I(i). 
Mk(i) = (1-µ)Mk(i-1)+µMk-1(i-1)               (2) 
  
Where I is  coded protein sequence, M is the output of the 
gamma memory, i is the index of the current position, and k = 
1,2,… ,K. µ is the memory parameter of this memo ry bank, 
(1-µ) is  the feedback quantity, K is the order of this memory 
bank, the memory depth is K/µ, and the memory depth for 
each input variable is the sum of the memory depth of the 
causal memory bank and the memory depth of its anti-causal 
counterpart  [2]. We can modify the number K and/or adapt the 
memory parameter µ  to adjust the memory depth of the 
memory bank. Fig. 2 shows the detail of the memory bank. 
The main steps in the training phase are as follows: 
1. Choose the amino acid sequence to train the gamma 
neural network. 
2. Put the preceding encoded amino acids into the causal 
gamma memory (each digit  in the binary string 
- 204 -
