I 
 
 
前瞻異質多核心系統開發工具研發子計畫二： 
多核心連結網路架構之研究與設計 
 
 
計畫編號：NSC 98-2220-E-007-003- 
執行期間：中華民國 96年 8月 1日至 99年 7 月 31 日 
主持人 ：許雅三教授 
             (Email：yshsu@ee.nthu.edu.tw) 
執行單位：國立清華大學電機系 
 
中文摘要 
在這個三年期的研究計畫之中，我們針對多核心連結網路開發了不同類型的模擬
器。針對探索設計空間(Design Space Exploration)而設計的模擬器提供已經設計好
的網路元件模型，讓設計者可以快速的建立不同架構的連結網路，並且比較這些
架構在效能上的差異；專為網狀(Mesh)拓樸網路而設計的模擬器則提供了更接近
真實硬體的效能報告，讓設計者能精確的調校細部的設計，同時，透過這個網狀
拓樸模擬器，我們提出了一個動態可重組網路(Dynamic Reconfigurable NoC)，其
能偵測網路交通，並且自動建立暫態線路交換連線。 
 
關鍵詞：多核心連結網路、模擬器、探索設計空間、動態可重組網路 
 
Abstract 
In this three-year research, we’ve developed different simulators for multi-core 
interconnection network. A simulator which is targeted for design space exploration 
provides predefined models of network components. It helps designers to construct 
various types of interconnection network rapidly. Therefore, the designers are able to 
compare different networks. Another simulator has also been built for mesh topology 
network. It has more realistic performance parameters, very close to the real hardware. 
Accordingly designers are able to fine-tune their designs. At the same time, through 
the mesh simulator, we’ve proposed a dynamic reconfigurable NoC. It monitors the 
network traffic trend and constructs temporary circuit-switched connections to 
improve performance. 
 
Keywords：Multi-core Interconnection Network, Simulator, Design Space Exploration, 
Dynamic Reconfigurable NoC 
1 
 
一、 前言 
隨著半導體製程的改進，現今的產品已經能將數十到數百顆智慧財核心(IP Cores)
整合成一顆系統單晶片(System-on-Chip)。藉由這種多核心的晶片架構，產品能
提供更多的運算能力來解決複雜的應用問題。然而，隨著核心數目的上升，核心
之間的資料交換量也會跟著上升，如果系統不能及時的傳遞這些資料，則運算效
能會大幅的下降。因此，連結架構的設計將明顯的影響系統的效能。目前的趨勢
是使用單晶片連結網路(Network-on-Chip)來作為系統的連結架構，而本研究會針
對如何決定晶片連結網路來開發模擬器，讓設計者能在設計初期得到效能的評估
結果。 
二、 研究目的 
傳統上，晶片最常使用的連結架構是匯流排(Bus)與點對點連線(Point-to-point 
link)。前者的頻寬是固定的，當核心數目上升時，單一匯流排的頻寬將不足以支
持總資料的傳輸量；而後者則是在核心之間拉上點對點的傳輸線，雖然能解決頻
寬不足的問題，但是，當核心數目到達數百甚至數千的時候，線路的複雜度將會
劇烈的上升。因此，目前的趨勢是將封包交換網路的概念運用在系統單晶片之上，
主要是因為封包交換網路提供了高延展性(Scalability)的好處，這個特性特別適用
於單晶片系統之上，而這樣的連結網路就被稱之為單晶片連結網路。 
單晶片連結網路在設計上需要考慮非常多的參數，包括網路的拓樸
(Topology)、路由演算法(Routing Algorithm)、緩衝器(Buffer)的大小等等，這些參
數將影響最後系統傳輸資料的效能。因為網路的運作非常複雜，不容易只依靠設
計者的經驗來決定這些參數，另一方面，在硬體成本的考量之下，設計者也不可
能一味的採用最複雜、效能最好的元件。因此設計者需要一套方法能夠在考量設
計的複雜度以及所花費的硬體成本下找到最適合當前應用的連結網路架構。 
為了解決上面提到的問題，在這個三年的研究計畫之中，我們主要的目的在
於建立連結網路的模擬器，藉由高階程式語言利於閱讀與修改的特性，模擬器能
讓使用者輕易的描繪出各種可能的連結網路架構。然後在經過快速的模擬之後，
模擬器能提供設計者充分的性能資訊，使其能在系統設計初期就先得到各種網路
架構的大略效能。如此，設計者能夠在初期就精確的選擇出合適的連結網路架構，
提升整體系統的效能。另一個目的則是藉由開發模擬器獲得連結網路的設計經驗
與知識，並且提出新穎的連結網路架構。 
  
3 
 
四、 研究方法 
在第一年的時間之中，本研究先致力於建立模擬器設計的相關知識，藉由建立簡
單但是完整的模擬器，我們可以得到建構模擬器時可能會遭遇到的困難，並且將
這些經驗作為之後研究的基石。為了方便驗證與考量模擬速度，本研究所設計的
模擬器都是以 SystemC 語言所建立。研究首先探討單晶片網路最基本的元件：
緩衝器(Buffer)。由於不論任何拓樸的單晶片網路都會使用之，我們可以預期若
緩衝器的效能可以被改善，則整體網路的效能也將獲得改善。在確定緩衝器的設
計之後，我們開始擴展、設計不同拓樸的單晶片網路，由於多數的模擬器都是採
用網狀拓樸(Mesh Topology)，缺乏設計空間的多樣性，在初期有限的時間之中，
我們先以環狀(Ring)、網狀(Mesh)以及圓環體狀(Torus)三種拓樸的設計為主。在
確認三種拓樸網路的設計是正確且可靠之後，我們能夠針對不同的參數作模擬，
可調整的參數包含了緩衝器的深度，虛擬通道的數目，以及採用均勻交通
(Uniform Traffic)或不均勻交通(Non-uniform Traffic)等。這些項目的模擬結果與設
計模擬器的過程能有助於我們抉擇單晶片網路的架構。 
 
 第二年的研究之中，我們從前一年開發模擬器及相關研究中發現，模擬器對
於單晶片連結架構模型的設計彈性極差，通常只會選取某一種主要的網路架構，
而設計者就只能基於這個架構調整少部分的參數來作模擬。例如我們為了要模擬
三種不同拓樸的單晶片網路，基本上就必須設計出三種不同的單晶片網路模擬器，
且使用者就只能採用這三種網路模擬；一旦設計人員需要另外評估不規則拓樸的
單晶片網路時，就必須重新設計網路模型，這種設計的方式相當的花費時間與精
力。但是網路架構大部分是由相似的元件所構成，如果不能有效的利用已經撰寫
過的部分是非常可惜的事情。因此，在仔細的分析網路架構之後，我們將網路切
割成不同層級的系統元件，讓模擬器提供各種網路架構會共同使用到的系統元件，
則設計人員在評估不同類型網路的時候，就只需要使用與組合預先被設計好的元
件模型，就能夠快速的建立出其所需要的網路架構，這樣的設計方式能大幅的拓
展設計空間。 
另一方面，由於網路架構的效能往往取決於交通的樣式(Traffic Pattern)，因
此，設計者必須要提供模擬器未來可能會運行的應用，但是在系統設計的初期，
軟硬體可能都處於尚未確定的階段，設計者可能只知道應用程式可能有的行為與
各工作(Task)之間的大致交通量，因此不容易向模擬器描述交通樣式。基於這個
假設，我們提出了稱為 PACMDL 的應用描述語言，其能夠以較通用模型語言
(UML)簡單的方式來描述各種應用程式的工作圖(Task Graph)。如此，模擬器能
夠得到更精確的交通應用，讓使用者得到各種網路架構基於這些應用的效能評估
報告。 
 
計畫的最後一年，我們延續模擬單晶片網路所得到的結果，針對擴展性最高
5 
 
data_0 data_1 data_2 data_3
read
write
data_in
0
1
4
full
empty
data_out
1
0
Z
r w
data_0 data_1 data_2 data_3
read
write
data_in
1
1
4
full
empty
data_out
0
0
0
rw
data_4 data_1 data_2 data_3
read
write
data_in
0
1
4
full
empty
data_out
1
0
Z
r w
Original FIFO behavior
Cycle 1: 
Write data (4) into the 
FIFO
Cycle 2: 
Write data (4) into the 
FIFO
Read data from the 
FIFO
Cycle 3: 
Write data (4) into the 
FIFO
Commands in each cycle
 
圖 1：傳統雙阜緩衝器的行為 
data_0 data_1 data_2 data_3
read
write
data_in
0
1
4
full
empty
data_out
1
0
Z
r w
1
0
0
New FIFO behavior
Cycle 1: 
Write data (4) into 
the FIFO
Cycle 2: 
Write data (4) into the 
FIFO
Read data from the 
FIFO
Commands in each cycle
data_4 data_1 data_2 data_3
read
write
data_in
1
1
4
full
empty
data_out
r w
 
圖 2：改良的雙阜緩衝器行為 
 
圖 3：傳統與改良式雙阜緩衝器的延遲時間比較 
7 
 
 
圖 4：藉由預先實作好的元件，設計者可以建立起以上的單晶片連結架構，包含
封包交換網路(網狀、雙層環狀、多層式網路、二元拓樸網路)以及匯流排。(DC：
Data-link-layer Channel) 
 
 
圖 5：平行程式的工作圖與各工作被分配到節點的關係 
 
 
圖 6：各種連結網路架構在兩種不同配置下的總執行時間 
 
Mapping I Mapping II
Mesh 88082 94576
Double Ring 88067 88050
Binary Tree Network 89019 106141
Multi-stage Network 94484 88266
0
20000
40000
60000
80000
100000
120000
Cycles
Total Exec Time 
9 
 
 
圖 7：旁路電路控制器(Dynamic Bypass Circuit)可自動偵測交通趨勢並調整拓樸
交換機(Topology Switch [9])上的組態，建立一個跨越路由器(Router)的旁路捷徑 
 
 
(a) 
 
(b) 
圖 8：不同網路架構下，封包產生率與延遲時間的關係圖，(a)轉置(Transpose)交
通 (b)位元補數(Bit-Complement)交通 
 
 
表格 1：各種合成交通下，能自動建立旁路捷徑的網路較傳統封包交換網路在延
遲時間上的改進幅度 
Traffic Name Saturation Point (flit/cycle/node) 
Improvement before 
Saturation Point (%) 
Transpose 0.15 18.08 
Bit-Complement 0.17 7.65 
Bit Reversal 0.15 10.98 
Perfect Shuffle 0.24 7.22 
Butterfly  0.255 9.89 
Average Improvement 10.76 
0 0.05 0.1 0.15 0.2 0.25
0
20
40
60
80
100
Latency versus load
Load(in flits/node/cycle)
La
te
nc
y(
cy
cl
e)
 
 
Baseline
Proposed_NoC
0 0.05 0.1 0.15 0.2 0.25
0
50
100
150
Latency versus load
Load(in flits/node/cycle)
La
te
nc
y(
cy
cl
e)
 
 
Baseline
Proposed_NoC
11 
 
Computing, v.62 n.7, p.1121-1141, July 2002 
[4] "Noxim: Network-on-Chip Simulator," http://sourceforge.net/projects/noxim, 
2008 
[5] "NIRGAM: A Simulator for NoC Interconnect Routing and Application 
Modeling, " http://nirgam.ecs.soton.ac.uk/home.php, Sept 2007 
[6] "Garnet: A Detailed Interconnect Model Inside a Full-System Simulation 
Framework," http://www.princeton.edu/~niketa/garnet.html, Feb 2008 
[7] A. B. Kahng, Bin Li, Li-Shiuan Peh, K. Samadi, "ORION 2.0: A fast and 
accurate NoC power and area model for early-stage design space exploration," 
Design, Automation & Test in Europe Conference & Exhibition, DATE '09. 
pp.423-428, April 2009 
[8] Qiaoyan Yu, P. Ampadu, "A Flexible Parallel Simulator for Networks-on-Chip 
With Error Control," IEEE Transactions on Computer-Aided Design of 
Integrated Circuits and Systems, vol.29, no.1, pp.103-116, Jan 2010 
[9] M. B. Stensgaard and J. Sparso, "ReNoC: A Network-on-Chip Architecture with 
Reconfigurable Topology," Second ACM/IEEE International Symposium on 
Networks-on-Chip, pp.55-64, April 2008 
[10] W. J. Dally and B. P. Towles, Principles and Practices of Interconnection 
Networks. Elsevier, Inc., 2004. 
From the above example, we can recognize that the 
FIFO has to change its state from a full state to a non-
full state in cycle 2, and then the data can be written in 
cycle 3. There will be an extra cycle wasted on state 
switching. Since buffers are limited resources in NoC 
designs, they are easy to fill. By using this FIFO 
behavior, it will induce an extra cycle delay every time 
when the buffers are full. In order to fix this problem, 
we proposed a design concept that eliminates the extra 
cycle delay as Figure 2 shows. 
 
 
Figure 1. Conventional FIFO behavior 
 
 
Figure 2. Cycle stealing buffer 
 
The same as previous example, a write event is 
declared in cycle 1, but the data is not written into the 
FIFO since the FIFO is full. In cycle 2, there is a new 
read event declared, so there are two events: write and 
read ready to be processed. Instead of waiting for the 
state transition from a full state to a non-full state, we 
can let both events be processed at the same cycle, that 
is, to read out the old data (data_0) and then write in 
the new data (data_4), just like what a single Flip Flop 
works. By using this concept, we can reduce the extra 
cycle wasted on an unnecessary state switching. 
This design concept must be implemented carefully. 
Because in cycle 2, two events are processed at the 
same time, which means the original data (data_0) has 
been flashed in this cycle. If there is a data miss in the 
receiver, a conventional FIFO design can just shift 
back the pointer and read the data again in cycle 3, but 
the cycle stealing buffer design can’t. Designers should 
pay more efforts on solving these kinds of issues. 
Although we have increased the buffer design 
complexity, the network operation frequency will not 
change. Since the buffer is still less complex than other 
network components, it will not be the bottleneck of 
the network operation frequency. 
 
3. Physical channel management scheme 
 
3.1. Conventional virtual channel multiplexing 
 
A conventional virtual channel multiplexing is the 
flow control that can be easily adopted in wormhole 
based on-chip networks using virtual channels. It 
equally assigns the physical channel resource to those 
virtual channels across it. Each virtual channel uses the 
physical link for one clock cycle only and has to wait 
until all other virtual channels have used the link. 
These virtual channels share the physical channel in a 
round robin fashion. Since the transfer speed of each 
virtual channel is fixed, no extra control is needed in 
this implementation method. 
To implement the conventional virtual channel 
multiplexing, a multiplexer and a de-multiplexer are 
added on different ends of a physical channel and a 
selector is added to implement the selection function, 
as Figure 3 shows. 
 
 
MUX
selector
DE-
MUX
physical
channel
virtual channel 2
virtual channel 3
virtual channel 1
virtual channel 4
virtul channels
 
Figure 3. Virtual channel implementation 
 
In conventional virtual channel multiplexing, 
although the flexibility and efficiency of physical 
channels has increased; a blocked packet stored in a 
certain virtual channel’s buffer won’t occupy the whole 
resource of the physical channel. However, the speed 
of each virtual channel also decreases significantly 
according to the number of virtual channels [1]. This 
decrease in speed will become a bottleneck for using 
virtual channel. 
To solve this problem, we need to analyze the 
transfer state of virtual channels in different kinds of 
situations. 
 
3.2. The transfer state of virtual channels 
 
First, we assume an ideal case that all of the virtual 
channels across one physical channel are ready to send 
packets, and these packets will not be blocked in the 
the state of each buffer must be sent to the left hand 
side node, too. By this network node architecture, we 
can realize the cycle stealing buffer concept proposed 
earlier. 
 
PO
data_3
PItrans_3
PIfull_3
PO
req_3
PIreq
PIdata
PO
full
PO
trans
 
Figure 4. A network node block diagram for 
ring network adopting cycle stealing buffers 
 
4.2. Network nodes adopting cycle stealing 
buffers and physical channel management 
scheme 
 
To realize the physical channel management 
scheme in ring topology network, we virtualize the 
escape channels which break the cyclic dependency 
and add two modules in each network node as Figure 5 
shows. 
 
PO
data_3
PItrans_3
PIfull_3
PO
req_3
PIreq
PIdata
PO
full
PO
trans
 
Figure 5. A network node design for ring 
network adopting cycle stealing buffers and 
physical channel management scheme 
 
The physical channel management controller can 
select one owner from the virtual channels which can 
transfer data through the physical channel, and skip 
those unable to transfer. Thus, the physical channel 
resource won’t be wasted on such unnecessary 
assignments, and the utilization rate of physical 
channel can be higher than just equally assigning the 
physical channel resource to every virtual channel 
across it. In addition, the controller can even assign 
more physical channel resources to the virtual channels 
holding more traffic load. The de-selector is a de-
multiplexer device and can pass the incoming data to 
their destination buffers by the information from 
sender’s physical channel management controller 
module. 
 
5. Applying these methods to wormhole 
based torus topology network 
 
In this section, the cycle stealing buffers and 
physical channel management scheme are adopted in a 
wormhole based torus topology network 
 
5.1. Network nodes adopting cycle stealing 
buffers 
 
The main difference of designing a network node 
between ring and torus topology networks is its 
number of physical channels. A network node for the 
torus based network has two more channels than that 
for the ring based network. Therefore, the most 
network components are designed similar to those in a 
ring topology network node. In order to solve deadlock 
anomaly in the tours topology network, extra escape 
channels are added to connect the neighbor nodes in 
both dimensions. The block diagram of the network 
node is shown in Figure 6.  
 
 
Figure 6. A network node design for 3X3 
torus adopting cycle stealing buffers 
 
 
 
5.2. Network nodes adopting cycle stealing 
buffers and physical channel management 
scheme 
 
To realize the physical channel management 
scheme in 3X3 torus topology network, we virtualize 
the escape channels in each dimension of the network 
and add two modules in each network node as Figure 7 
shows. 
 
Table 1. Network parameters for 
performance evaluations  
Topology Ring 3X3 torus 
Number of Nodes 9 
Switching Wormhole 
Routing Deadlock prevention 
DOR [7] & 
Deadlock 
prevention 
Arbitration 
method 
Weighted 
round robin Round robin 
FIFO size 4 Flits 
Message size 1 packet 
Packet size 4 Flits 
Flit size 1 Phit 
Simulation cycle 100000 
 
 
 
Figure 8. Performance comparisons 
between two ring networks adopting 
different buffer architectures 
 
From this figure, we can see that the average flit 
latency of the network adopting cycle stealing buffer 
architecture is smaller than the network adopting 
conventional FIFO architecture, especially when the 
applied load grows larger. However, with a very small 
applied load, each buffer in the network will never be 
full and these two networks have nearly the same 
average flit latency. 
We can see that there is a large gap in the maximum 
accepted traffic per node between different buffer 
architectures as the applied load increases. The 
network adopting conventional FIFO architecture 
saturates at a smaller applied load than the other. When 
the applied load exceeds the saturation point, the 
accepted traffic starts decreasing and finally reaches a 
steady state value smaller than the maximum accepted 
traffic. However, this problem won’t happen in the 
network adopting the cycle stealing buffer architecture. 
 
 
 
Figure 9. Performance comparisons 
between two 3X3 torus networks adopting 
different buffer architectures 
 
We change the network topology to 3X3 torus and 
again use the setting listed in the Table 1. The 
simulation results of average flit latency and accepted 
traffic for the networks adopting different buffer 
architectures in different applied load per node are 
plotted in Figure 9. 
  
Figure 11. Performance comparisons of 
different 3X3 torus networks adopting 
different methods to realize multiple virtual 
channels across one physical channel  
 
9. Conclusions 
 
In this paper, the cycle stealing buffer architecture 
is proposed, which eliminates the unnecessary cycles 
in a read/write event. From simulation results, we 
clearly prove that this buffer architecture can improve 
the network performance with only a small extra cost 
on hardware, especially when there are fewer physical 
channel resources, such as ring topology network.  
In addition, the physical channel management 
scheme is proposed, which management the physical 
channel resource for multiple virtual channels across 
one physical channel. From simulation results, we 
clearly prove that this physical channel management 
scheme is an efficient solution for multiple virtual 
channels to share one physical channel resource and it 
is also easy to be adopted. 
 
Acknowledgements 
 
This work is supported by NSC under grant 96-
2220-E-007-032 and SoC Technology Center, ITRI 
under grant of 62-96032. 
 
References 
 
[1] J. Duato, S. Yalamanchili, and L. Ni, Interconnection 
Networks – An Engineering Approach. Morgan 
Kaufmann, 2002. 
[2] L .Benini and G. Demicheli, “Networks on Chips: A New 
SoC Paradigm”, Computer, vol.35, no.1, pp. 70-78, 
Jan.2002. 
[3] P. Magatshack and P. G. Paulin, “System-on-Chip 
beyond the Nanometer Wall”, Proc. Design Automation 
Conf. (DAC), pp.419-424, June 2003. 
[4] M. Oka and M. Suzuoki, “Designing and Programming 
the Emotion Engine”, IEEE Mirco, Vol. 19, No.6, 
November-December 1999, pp. 20-28. 
[5] D. pham, et al., “Overview of the Architecture, Circuit 
Design, and Physical State Circuits”, IEEE Journal of 
Solid-state Circuits, Vol. 41, No.1, January 2006, pp 
179-196. 
[6] Kyeong Keol Ryu, Eung Shin, and Vincent J. Mooney, 
“A Comparison of Five Different Multi processor SoC 
Bus Architectures”, Digital Systems, Design, 2001. 
Proceedings. Euromicro Symposium on 4-6 Sept. 2001, 
pp.202-209. 
[7] L. De Coster,N. Dewulf, and C. T. Ho, ”Efficient Multi-
Packet Multicast Algorithms on Meshs with Wormhole 
and Dimension-Ordered Routing”, Proc. 1995 Int'l Conf. 
Parallel processing, Vol.3 IEEE CS Press, Los 
Alamitics, Calif.. 
[8] SystemC User’s Guide Version 2.0. 
III. DESCRIPTION OF NOCSEP 
The behavior of an application and the generation of its 
traffics are explained in Section III-A. The middle-layer 
modeling of Nocsep is explained in Section III-B. Section III-
C briefly explains how OONoC[11] can be used to achieve 
wide-range design space exploration. 
A. Application Modeling 
In Nocsep, we introduce a quasi-formal description 
language called “Parallel Application Communication 
Mechanism Description Language” (PACMDL), that can 
describe the task-graph features of any parallel application. It 
is based on the researches of Kahn-process-network 
(KPN)[18], the improved-KPN[12] and the innovative 
communication architecture modeling in [5]. PACMDL is 
totally compatible with KPN, which has been studied for years 
and used in practice for many important applications like 
Motion-JPEG [12]. Any KPN-based application can be 
translated into PACMDL without any information loss. 
PACMDL is used as part of Nocsep’s application modeling 
and more details will be given later. 
As shown in Fig. 1, there are three parts in Nocsep’s 
application modeling: thread modeling, task modeling, and 
PACMDL-coding. Nocsep uses thread modeling (“Thread” in 
brief) as an application traffic generator. Thread in Nocsep is 
simpler than the thread in operating system (OS). It is a 
module that wraps different traffic generation mechanisms and 
provides the following features: “traffic priority”, “traffic 
content”, “Thread priority”, and “the method of Thread 
mapping”, etc.  
Any traffic generation mechanism in Nocsep is called task 
modeling (“Task” in brief), which may include computation 
workloads or communication workloads. There are three kinds 
of Tasks supported by Nocsep: random generation, PACMDL-
driven generation, and event-triggered generation. Random 
generation produces statistical traffic patterns. An infinite 
first-in-first-out (FIFO) buffer is used, so random traffic 
patterns are preserved from real-time behaviors of NoC. The 
traffic of event-triggered generation yields to some events fed 
to Threads, such as “a data-reading request”, “a signal coming 
from other processing elements”, etc. Many of the simple 
inputs and outputs can be simulated as this kind of Thread. 
The traffic produced by PACMDL-driven generation is 
controlled by a textual file in PACMDL provided by users. 
PACMDL describes not only the features of a parallel 
program, but also the strategy of Task grouping and Thread 
mapping.  
Node modeling (“Node” in brief) is a modeling similar to 
a processing element. Nodes will process traffics through their 
computation cores or communication cores. More details of 
Nodes will be explained later. 
Fig. 1 shows an example application with three Task 
groups, each of which consists of computation Tasks or 
communication Tasks and is assigned to one Thread. The 
 
Figure 1 Application modeling 
 
Figure 2 Traffic layering  
execution conditions are satisfied. The conditions are similar 
to those in step 2.  
Traffic Layering: Nocsep provides a detailed traffic 
layering shown in Fig. 2. In this figure, the traffic flows across 
many layers and Nocsep components maintain the traffic 
pattern. There are several kinds of components in Nocsep: 
Task components, Thread components, Node components, the 
adaptor modeling (“Adaptor” in brief) components, NoC 
components, and hardware modeling components. All 
components in Fig. 2 will be explained later. 
The layering is based on three kinds of channels: process 
channels, real network channels and lowest-level physical 
channels. The process channel is an application-level channel 
which can be implemented as a Thread-to-Thread channel, a 
Node-to-Node channel, or an Adaptor-to-Adaptor channel. For 
example, in Fig. 2 the process channel is implemented as an 
Adaptor-to-Adaptor channel. The real network channel is the 
NoC-provided logical transmission resource which can be 
composed of different lowest-level physical channels. The 
lowest-level physical channel is the real implementation of a 
physical transmission resource. The channel concept is 
important in Nocsep for separating the designs of different 
network layers. More details of such a kind of channel-
oriented hardware modeling are explained in [11].  
There are four kinds of traffic formats, including messages, 
streams, transfer packages and physical channel units (Phits). 
In Fig. 2 messages are generated by Tasks, and further 
transformed into streams in Node, streams into transfer 
packages such as packets, flow control units (Flits),  BUS 
transaction units in Adaptor, or packages into Phits in physical 
channel modeling (“PC” in brief). 
B. Middle Layer Modeling 
One of the contributions of Nocsep is the modeling of the 
middle layer between an application and a NoC. Nocsep 
provides two kinds of models: Node and Adaptor. These two 
models emphasize only the middle-layer behaviors that 
significantly affect the traffic, so we skip some trivial details. 
Node: A Node is a pseudo-component mixing the 
structures of a processing element and the handling of an OS 
process. Fig. 3 shows its concept. The “request table” in this 
figure is a table temporarily holding all entering messages. 
Each slot of this table holds only Tasks of the same “Task 
priority” and the same “Thread ID”, both of which are 
statically assigned to this slot before simulation. The “kernel 
manager” is a concept similar to the OS threading. It selects 
one message from the request table, selects one core, handles 
this message by that core and reports its status to its source 
thread through events.  
The computation core handles computation messages. For 
example, one 30-operation computation message will take 30 
cycles in the core whose computing power is one operation 
per cycle. The communication core transforms communication 
messages into streams and sends streams to an Adaptor which 
will be explained later. The Node in Nocsep is very flexible. 
Each step of message handling is given a parameterized 
latency. The number of kernel managers, the number of 
computation cores and the number of communication cores 
are all parameterized. Moreover, the computation core 
supports a simple context switch. 
Adaptor: The Adaptor is a hardware component between 
a Node and a NoC. Fig.4 shows its concept. For sending, a 
Node’s communication core has to allocate two kinds of 
resources in Adaptor– stream managers and buffer space. The 
stream manager is a NoC controller to send out or receive 
stream data. The Buffer is the storage space in which the 
stream data is composed or decomposed. After these resources 
are allocated, the core builds up a package queue in its 
 
Figure 5 The system for PACMDL-task driven simulation 
 
Figure 6 NoC topologies  
(The number in Adaptor stands for ‘node ID’.  
All sub-components is explained in [11]) 
 
Task 
0
Task 
1
Task 
2
Task 
3
Task 
4
Task 
5
Task 
6
Task 
7
idle
Task 
0
Task 
1
Task 
2
Task 
5
Task 
4
Task 
3
Task 
6
Task 
7
idle
Task 
0
Task 
5
Task 
4
Task 
6
Task 
1
Task 
3
Task 
7
idle
Task 
2
Task 
0
Task 
7
Task 
3
Task
2
idle
Task 
5
Task 
6
Task 
4
Task 
1
Case I (MAP-1) Case II (MAP-2)
Case III (MAP-3) Case IV (MAP-4)
 
Figure 7 Mapping Methods in Mesh case 
 
Figure 8 Application Case Studies 
 
The gray blocks are computation tasks, marked with computation operation count. The arrows are 
communication tasks, marked with the data amount (in Byte). Each computation task will be triggered by the 
incoming communication traffic. After the computation task is finished, it triggers its output communication 
tasks. The multiple outputs of one task will be finished one by one. The task of multiple inputs will be triggered 
after all inputs are received. Tasks are forced to be triggered once at first and then triggered by run-time traffic. 
 
 
[21] Halambi, Ashok, et al. “EXPRESSION: A Language for architecture 
exploration through compiler/simulator retargetability.” date, pp.485, 
Design, Automation and Test in Europe (DATE '99). 1999. 
[22] Chang, Chi-Fu. “Scalable on-chip interconnect design & exploration 
framework for embedded multi-pacdsp platform.” Techenical Report,   
ITRI, Taiwan,  2008. 
  
Figure 9 Case 1 – Total execution time 
 
 
Figure 10 Case 1 – Average latency 
 
 
Figure 11 Case 1 – Average throughput 
vc 
=1
vc 
=2
vc 
=3
vc=4
mapping case I 315051 315051 313432 313230
mapping case II 313051 313051 311613 311464
mapping case III 314051 314051 312517 312324
mapping case IV 319051 319051 317044 316752
306000
308000
310000
312000
314000
316000
318000
320000
Cycles
vc 
=1
vc 
=2
vc 
=3
vc=
4
mapping case I 266 266 264 264
mapping case II 264 264 263 262
mapping case III 265 265 263 263
mapping case IV 270 270 268 268
258
260
262
264
266
268
270
272
Cycles per 
message 
per Thread
vc 
=1
vc 
=2
vc 
=3
vc=4
mapping case I 23 23 23 23
mapping case II 23 23 23 23
mapping case III 23 23 23 23
mapping case IV 22 22 22 22
20
20.5
21
21.5
22
22.5
23
23.5
24
24.5
25
Bits per 
cycle per 
Thread
  
 
Figure 12  Case 2– AP-1 with Mapping I & II 
 
Figure 13 Case 2– AP-2 with Mapping I & II 
 
 
 
Figure 14 Case 2– AP-3 with Mapping I & II 
Mapping I Mapping II
Mesh 315051 313051
Double Ring 312551 515349
Binary Tree Network 313051 591556
Multi-stage Network 543975 521132
0
100000
200000
300000
400000
500000
600000
700000
Cycles
Mapping I Mapping II
Mesh 88082 94576
Double Ring 88067 88050
Binary Tree Network 89019 106141
Multi-stage Network 94484 88266
0
20000
40000
60000
80000
100000
120000
Cycles
Mapping I Mapping II
Mesh 377191 384266
Double Ring 380320 390699
Binary Tree Network 377252 382931
Multi-stage Network 369950 367085
355000
360000
365000
370000
375000
380000
385000
390000
395000
Cycles
II. DYNAMIC RECONFIGURABLE NOC 
The architecture of the dynamic reconfigurable NoC is 
shown in Figure 1.  Each router is wrapped by a static 
topology switch [6] and the proposed Dynamic Bypass 
Circuit. We adopt the topology switch as the bypass 
infrastructure. By carefully setting up the multiplexers on the 
topology switch, two separate links can be connected 
together. Hence, packets can bypass the entire router pipeline 
via the topology switch. In the following paragraphs, this 
short-cut is called bypass. For example, as shown in Figure 
1. , a packet takes 2 hops to travel from node 5 to node 3 in 
the original packet-switched network. If one sets up the 
topology switch on node 4, the west link of node 5 can be 
connected to the east link of node 3 directly. Therefore, the 
packet can reach the destination in 1 hop. 
Although the topology switch improves the performance 
of the network, it requires an agent to determine and set up 
its configuration according to different traffic patterns. In 
this research, this cumbersome work is accomplished by 
Dynamic Bypass Circuit. It monitors the traffic following 
through this node and makes the decision on when and how 
to set up the static topology switch at run-time. Taking into 
consideration that a control circuit requires additional 
hardware cost, each node will construct only one bypass. 
That is, Dynamic Bypass Circuit can choose one of the 
trends as the bypass configuration. Here, the trends are 
defined as the paths that the packets go through a single node. 
An example is shown in Figure 2. , node 3 sends a small 
amount of packets to node 1, node 2 and node 7 send a large 
amount of packets to node 5 and node 4 respectively. After a 
period of sensing, Dynamic Bypass Circuits on node 5 and 
node 6 construct the bypasses spanning from east to west, 
whereas Dynamic Bypass Circuit on node 1 constructs a 
bypass spanning from east to south, while the other nodes 
don’t have bypasses. Owing to the bypass on node 1, the 
packets traveling from node 3 to node 1 are now misrouted 
to node 5. The misrouted packets need an additional hop to 
go from node 5 to node 1. It seems the bypass does harm to 
the system. However, the bypass benefits packets traveling 
from node 2 to node 5. Therefore, the bypass can still be 
advantageous to the system. 
 
 
Figure 1.  The architecture of the dynamic reconfigurable NoC. 
 
Figure 2.  Dynamic Bypass Circuit on each node senses the traffic and 
constructs the bypass automatically. 
 
Figure 3.  The diagram of Dynamic Bypass Circuit. (VC: Virtual Channel) 
 
 
Figure 4.  The timeline of Dynamic Bypass Circuit. 
III. DYNAMIC BYPASS CIRCUIT 
A. Architecture of Dynamic Bypass Circuit 
The schematic diagram of Dynamic Bypass Circuit is 
shown in Figure 3.  It consists of Traffic Sensing Module and 
Bypass Controller. The function of Traffic Sensing Module is 
to decide the configuration of topology switch. It monitors 
the routing decisions from the routing logic. Then, after the 
calculation, it returns a configuration. The monitoring 
continues even after the bypass construction. If the bypass 
does harm to the performance, it informs Bypass Controller 
to cancel the bypass by Cancel_Flag.  
Bypass Controller configures the topology switch 
according to the received instruction. It maintains the 
sequence of flits during the process of bypass construction 
and destruction. 
B. Timeline 
The timeline of Dynamic Bypass Circuit is shown in 
Figure 4.  The first phase is Traffic Trend Sensing. The main 
job here is to let Traffic Sensing Module monitor the traffic 
trends and decide a bypass configuration. Dynamic Bypass 
Circuit stays in this phase until the configuration is 
Allowed 180‐degree Turns  The valid Bypass Paths 
 
Figure 5.  Allowed 180-degree turns and the valid Bypass Paths for 
nonminimal north-last routing algorithm 
 
TABLE I.  AN EXAMPLE OF RECORDED TRAFFIC TRENDS 
Input port N E C 
Output port S E W E 
Transmitted 
Packets 12 2 5 1 
 
 
D. The Inducements of Bypass Destruction 
Because the application is changing, sometimes a bypass 
may be unsuitable for the traffic. At this moment, the bypass 
needs to be torn down. In this research, there are two ways to 
detect the inappropriateness of the bypass. According to the 
initiator, the destruction is divided into the passive 
destruction and the active destruction. 
The passive destruction happens if the bypass blocks 
some packets on current node. This is due to the fact that the 
physical link is occupied by the topology switch. If a packet 
enters the router and wants to go through the same output as 
the one of the bypass, the packet is stuck at the current node. 
This will dramatically increase packet latency. Therefore, as 
long as Traffic Sensing Module finds that a packet is blocked, 
it informs the local Bypass Controller to cancel the bypass 
(by Passive_Cancel_Flag). At the same time, to reflect the 
performance degradation, the Budget_Value of the current 
configuration will be reduced by 1. If the budget value is still 
larger than 0, Traffic Sensing Module assumes that the 
configuration will still benefit the system. Therefore, it 
retains the configuration. The bypass can be reconstructed 
without traffic sensing as soon as the blocking situation lifted. 
In contrast, Dynamic Bypass Circuit needs to determine a 
new configuration after the active destruction. It occurs 
when the bypass misroutes the majority of packets. Each 
Traffic Sensing Module has the identification number (ID 
number) of its default neighbors. As the bypass is 
constructed, Traffic Sensing Module obtains its current 
neighbor IDs through the bypass. Whenever Traffic Sensing 
Module receives the routing decision, it compares Manhattan 
distances from the default neighbor and the current neighbor 
to the destination. By doing this, Traffic Sensing Module is 
able to check whether the neighbor bypass prolongs the 
distance or not. If the number of packets been misrouted is 
larger than the empirical threshold, 5, Traffic Sensing 
Module informs its neighbor to cancel the bypass (by 
Active_Cancel_Flag).  
IV. NORTH-LAST-WEAVE ROUTING ALGORITHM 
Although we have selected north-last routing algorithm 
for dynamic reconfigurable NoC, the original north-last 
routing algorithm may not function well without a suitable 
selection strategy. The selection strategy influences the 
traffic distribution as well as the possibility of bypass 
construction. Some researches [8][9] indicate that straight 
lines and minimum congestion strategies provide better 
performance for the adaptive routing algorithms. However, 
neither of the strategies is suitable for dynamic 
reconfigurable NoC. Minimum congestion balances the 
traffic by sending packets to the less congested link. 
However, it also balances the number of transmitted packets 
along each trend. Hence, there is no significant trend exist. 
The strategy raises the difficulty of Traffic Trend Sensing.  
In contrast, the straight lines strategy postpones taking 
the turn. If the destinations of certain nodes are close, 
packets will be guided to travel along the same dimension 
first. This leads to the similar situation as in XY routing 
algorithm that the traffic pairs are twisted. As a result, it is 
difficult to construct a bypass without influencing the 
existing traffic. The example can be seen in Figure 6. (a) and 
(b). Under north-last routing algorithm, node 1, 2, 4 and 5 
are sending packets to node 6. Figure 6.  (a) and (b) show the 
traffic under straight lines strategy which chooses x 
dimension first and y dimension first respectively. The trends 
on one node tend to have the same output port. Hence, they 
are not suitable to be chosen as bypass configuration. As one 
can see from these figures, a bypass can be only constructed 
on node 0 in (a) and node 8 in (b). 
Since the above two selection strategies have some 
defects on dynamic reconfigurable NoC, we like to bring out 
a new solution to enhance the north-last algorithm. The 
solution should neither scatter the traffic as in minimum 
congestion nor overly centralize the trends as in straight 
lines. The idea is to assign different output direction to 
packets at their source node. Then, even if packets are 
transferred to the destination along the straight line path, 
there are still places to construct bypasses. Following the 
previous example, Figure 6.  (c) shows the traffic under this 
idea. The source nodes in black choose y dimension first and 
the source nodes in white choose x dimension first. By doing 
this, the traffic is distributed among two dimensions evenly. 
The bypasses can be constructed on node 0, node 4, node 5, 
and node 8. 
For simplicity’s sake, we impose the restriction on 
routing function instead of selection strategy. Based on the 
observation, we propose north-last-weave routing algorithm 
as in Figure 7.  The nodes are divided into two subsets, 
Horizon-first and Vertical-first, as the two colors in chess 
board. In the source nodes, if there are multiple output 
candidates, the horizon-first nodes pick the one along 
dimension X first, while the vertical-first node pick the one 
along dimension Y. Hence, two adjacent source nodes will 
select different output directions, even if their destinations 
are close. This helps disentangle the overlapped traffic. 
Packets stay in the straight line path to destination. As a 
result, the traffic pairs are merged easily if their source nodes 
are in the same subset. Here, the Select() function is a 
priority-fixed selection strategy. The left parameter has 
higher priority. In order to shorten the pseudo code, certain 
output port is derived from XY routing algorithm.  
0 0.05 0.1 0.15 0.2 0.25
0
20
40
60
80
100
Latency versus load
Load(in flits/node/cycle)
La
te
nc
y(
cy
cl
e)
 
 
NL-Min-Congestion
NL-Weave-Normal
NL-Weave-Dynamic
 
Figure 8.  The latency under Transpose pattern 
0 0.05 0.1 0.15 0.2 0.25
0
50
100
150
Latency versus load
Load(in flits/node/cycle)
La
te
nc
y(
cy
cl
e)
 
 
NL-Min-Congestion
NL-Weave-Normal
NL-Weave-Dynamic
 
Figure 9.  The latency of Bit-Complement pattern 
path as possible. Hence, the north-last-weave algorithm 
balances traffic load and improves performance. 
The traffic distribution of Bit-Complement is somewhat 
like random traffic. It has evenly distributed traffic pairs. 
However, by using the north-last-weave routing algorithm, 
Dynamic Bypass Circuit can still find the trends in the 
network. The latency improvement of the proposed NL-
Weave-Dynamic to the NL-Weave-Normal is 7.65% before 
the saturation point ( 0.17 flit/node/cycle ).  
C. Time Variant Traffic Patterns 
In this part, we use time variant traffic pattern to evaluate 
the networks. The traffic is called Transpose-Hot Spot which 
is the mix of the Transpose pattern and hot spot pattern. This 
traffic runs Transpose in most of the time, but it changes to 
hot spot during 5000th ~ 6000th cycle and 7000th ~ 7500th 
cycle. Under hot spot pattern, the source nodes send packets 
to the four nodes at the center of the mesh (node 27, node 28, 
node 35 and node 36) with an additional 6 percent of 
probability. The source nodes send the random traffic in the 
else 94 percent of probability.  
The latency improvement of NL-Weave-Dynamic to NL-
Weave-Normal is 13.43% which is still decent but a little bit 
lower than the improvement under pure transpose traffic. It is 
because that the system pays at least 15 % of time to run the 
hot spot traffic pattern and the dynamic reconfigurable NoC 
does not show significant advantage for random traffic like 
pattern. 
VI. CONCLUSIONS 
In this paper, we propose the architecture of Dynamic 
Bypass Circuit and north-last-weave routing algorithm to 
realize the dynamic reconfigurable NoC. The proposed 
Dynamic Bypass Circuit reconfigures the network for 
different applications at run-time. Therefore, users need not 
have to handle the mapping between applications and the 
network. The proposed north-last-weave routing algorithm 
balances traffics by guiding packets to different directions at 
their source nodes. Therefore, Dynamic Bypass Circuit can 
find more clear trends and places to construct bypasses.  
Our simulation results show the dynamic reconfigurable 
NoC which adopts Dynamic Bypass Circuit outperforms the 
traditional packet-switched network. The improvement is 
18.08% under Transpose traffic and on average 13.05% 
under three different traffic patterns. This confirms that our 
proposed architecture and routing algorithm can provide the 
ability to configure the network automatically at run-time. 
REFERENCES 
[1] W.J. Dally and B. Towles, "Route packets, not wires: on-chip 
interconnection networks," Design Automation Conference, pp. 684- 
689, June 2001 
[2] M. Modarressi, Sarbazi-Azad H and A. Tavaakkol, "Performance and 
power efficient on-chip communication using adaptive virtual point-
to-point connections," 3rd ACM/IEEE International Symposium on 
Networks-on-Chip, pp.203-212, May 2009  
[3] Shih-Hsun Hsu, Yu-Xuan Lin and Jer-Min Jou, "Design of a Dual-
Mode NoC Router Integrated with Network Interface for AMBA-
based IPs," Solid-State Circuits Conference, pp.211-214, Nov. 2006 
[4] K. Goossens, J. Dielissen and A. Radulescu, "AEthereal network on 
chip: concepts, architectures, and implementations," Design & Test of 
Computers, vol.22, no.5, pp. 414- 421, Sept.-Oct. 2005 
[5] N. Enright Jerger, Li-Shiuan Peh and M. Lipasti, "Circuit-Switched 
Coherence," Second ACM/IEEE International Symposium on 
Networks-on-Chip, pp.193-202, April 2008 
[6] M.B. Stensgaard and J. Sparso, "ReNoC: A Network-on-Chip 
Architecture with Reconfigurable Topology," Second ACM/IEEE 
International Symposium on Networks-on-Chip, pp.55-64, April 2008 
[7] G.J. Glass and L.M. Ni, "The Turn Model for Adaptive Routing," The 
19th Annual International Symposium on Computer Architecture, 
pp.278-287, May 1992 
[8] Loren Schwiebert; Renelius Bell; , "Performance tuning of adaptive 
wormhole routing through selection function choice," Journal of 
Parallel and Distributed Computing, v.62 n.7, p.1121-1141, July 2002 
[9] G. Ascia, V. Catania, M. Palesi and D. Patti, "Implementation and 
Analysis of a New Selection Strategy for Adaptive Routing in 
Networks-on-Chip," IEEE Transactions on Computers, vol.57, no.6, 
pp.809-820, June 2008 
[10] NIRGAM: A Simulator for NoC Interconnect Routing and 
Application Modeling, http://nirgam.ecs.soton.ac.uk/home.php, Sept, 
2007 
innovation continues tremendously rapidly according to Moore’s law. Along with design rule innovation 
and hardware architecture innovation such as multiple core, the latest LSI becomes functionally huge, but 
spatially small, low power consumption. This ideal LSI derived from not only seeds driven approach 
within hardware innovation but needs driven approach, that is, software led innovation. At the beginning 
era of Personal Computer, CPU speed innovation was technology leader. At that time, the goal of software 
technology is how to utilize the latest hardware speed. Then the innovation of software created many 
applications such as “Word Processing”, “Spread Sheet”, “Personal Database” and so on. When digital AV 
based consumer electronics was planned, phase change of innovation scenario was occurred in some 
technical fields which have real time processing requirement such as CODEC technology. In this field, 
software algorithm established first and refined within software innovation by using simulation. And 
software created the requirements for hardware, after that, hardware responded the requirements and 
created balanced combination of CPU and DSP architecture. In Panasonic, this architecture was named 
“UniPhier” which represents unified integrated platform for wide variety of consumer electronics products 
such as DTV, DVD, Blu-ray, cellar phone and so on. In the “Uniphier”, hardware balancing point between 
CPU and DSP is changed according to software’s requirement. Currently, CPU sometimes includes some 
real time processing process which was originally done by DSP because software requires complex but 
rapid conditional process of DRM. In other words, UniPhier’s hardware architecture is designed by 
UniPhier’s software based on the consumer electronics market’s requirements. Then, what is next phase of 
mutual interaction between innovation of software and that of hardware? Nowadays, open source software 
has the power from both technical and business point of view. The development style of open source 
software assumes define de facto hardware architecture such as PC architecture. But currently open source 
software also assumes some non PC architecture. It means that the movement of open source software 
might sometimes never allow various hardware designs. Innovation style in open source software world 
will continue to generate more and more software assets, more rapidly than expected. After we have big 
amount of software asset, the software itself might be constraint of hardware innovation because it is much 
more expensive to convert all of software asset to new hardware architecture. In order to keep innovation 
of hardware, software virtualization technology will be important and the virtualization technology should 
be assisted by hardware architecture. Giving consumer innovated products continuously in the future, he 
would like to design new consumer electronics software architecture based on open source software and 
virtual machine. 
3. Day three’s keynote is given by Marco Cornero, ST-Ericsson, Italy. The title of his talk is “MPSoc's in 
Mobile phones: parallel computing for the masses”. His major information to us is: With the quick 
adoption of ARM multi-core solutions, the Mobile industry has suddenly accelerated the shift towards 
multi-processing that is happening right now. This is in addition to the GPGPU trend that has renewed the 
interest in parallel processing in the PC and supercomputing worlds, which it is quickly reaching the 
mobile space as well, and the move towards many-cores architectures for multimedia acceleration. In other 
words, multi-processing everywhere, soon in the hands of the masses, enabling really exciting and 
compelling applications, such as augmented reality, in battery-powered devices. In his talk he shares his 
experience of driving such as fundamental shift from the perspective of an early industrial adopter, 
including motivations, vision, results and the day-to-day mix of practical issues and theoretical challenges 
that we are facing in these exciting times.  
4. The fourth keynote speech is given by Sebastian Steibl, Intel, Germany. His talk is about a single chip (48 
cores) cloud computer architecture that Intel has recently released. He presents the design of the 
四、 其他 
Will try to invite key overseas speakers in the area to Taiwan to encourage and exchange research activities if 
funding can be secured. 
 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：許雅三 計畫編號：98-2220-E-007-003- 
計畫名稱：前瞻異質多核心系統開發工具研發--子計畫二：多核心連結網路架構之研究與設計(3/3)
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 2 50%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 2 2 50%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
