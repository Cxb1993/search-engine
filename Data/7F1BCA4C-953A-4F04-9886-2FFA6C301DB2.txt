2 
 
音特徵參數重建法。語音特徵參數補償法是希望能夠將
受雜訊干擾語音特徵參數補償至未受雜訊干擾的語音特
徵參數，具有代表性的技術包括有最小化對數頻譜振幅
之均方誤差估測法 (Minimum Mean Square Error Log 
Spectral Amplitude Estimator, MMSE-LSA)[4]、頻譜消去
法(Spectral Subtraction, SS)[5]，向量泰勒展開式(Vector 
Taylor Series, VTS)[6]等。此外，近來有一些使用到以
編碼為基礎之演算法的新興技術被提出來[7-14]，在此
類方法中他們可能使用單一的編碼簿(Codebook)或是一
組的編碼簿，來描繪雜訊干擾所造成的失真效應，並且
每個編碼詞(Codeword) 都有其相應的補償向量，而這些
補償向量可在訓練階段被估測出，然後運用於測試階
段，以補償雜訊語音特徵向量。其中，常見的使用單一
的編碼簿的方法，有如編碼詞相關之倒頻譜正規化法
(Codeword Dependent Cepstral Normalization, CDCN)[7]
與機率最佳化過濾法 (Probabilistic Optimum Filtering, 
POF)[8]等；而常見的使用一系列的編碼簿的方法，有
如隨機向量特徵映射法 (Stochastic Vector Mapping, 
SVM)[11, 12]與複合環境模型為基礎之線性正規化法
(Multi-Environment Model-Based Linear Normalization, 
MEMLIN) [14]。 
另一方面，特徵參數轉換法其設計理念，是找到一
個轉換方式將特徵向量投射到更有鑑別性的向量空間，
期望能降低雜訊干擾與通道效應。較具有代表性的技術
有 線 性 鑑 別 分 析 (Linear Discriminant Analysis, 
LDA)[15] 、異質性線性鑑別分析(Heteroscedastic Linear 
Discriminant Analysis, HLDA)[16, 17]、異質性鑑別分析
(Heteroscedastic Discriminant Analysis, HDA)[18]和特徵
空間最小音素錯誤特徵訓練法(Feature-space Minimum 
Phone Error Training, fMPE)[19]等。此外，另一個有趣
且重要的研究主題是特徵參數重建法，而遺失特徵理論
(Missing Feature Theory, MFT)是它的代表性技術之一[20, 
21]。使用遺失特徵理論來重建特徵參數向量，一般而
言有兩個主要的步驟：第一步是先確認哪一個特徵參數
向量維度的值是不可靠的或遺失的；第二步是在語音辨
識階段時重建不可靠參數的值或是單純地忽略它。 
以上三個分類的大部分技術通常可以達到很高的效
能，只要在事前知道實際雜訊造成的失真，或是假設乾
淨語音與其相對應的噪音存在固定或是已知的關係。不
過，其中一些方法在線性失真下有良好表現，然而，在
非線性情況下時卻未必能有一致的效能，這現象的可能
原因之一是由於雜訊對於語音訊號產生的干擾並非都是
線性關係。 
因此，從另一觀點出發的強鍵性方法是基於語音特
徵中具抗噪性的機率分布資訊而非特徵參數本身來進行
語音特徵參數補償，這類方法過去已在各種辨識含有多
種雜訊的自動語音辨識研究中證實其強健能力。在所有
這一派的典型的方法中，倒頻譜平均消去法(Cepstral 
Mean Substraction, CMS)[22, 23]是一項簡單且有效的技
術；其主要功效在於消減通道效應造成的非時變性
(Time-invariant)失真。此外，倒頻譜平均消去法亦有許
多的延伸，像是倒頻譜正規化法 (Cepstral Mean and 
Variance Normalization, CMVN)[24]；其不只針對平均做
正規化，亦延伸至變異數的正規化。儘管這兩項技術在
補償通道效應造成的失真與一些加成性噪音引發的邊際
效應上展現了強健能力，但因其線性特性，仍使得它們
在處理多種類噪音環境下的辨識效能上有明顯的下降
[25]。 
因此，近年來有許多的研究致力於探索更有效的特
徵向量正規化法。例如，不僅是針對特徵向量機率分布
的第一與第二階動差施行正規化(像是CMN與CMVN)，
有些研究更進一步延伸CMN與CMVN的精神，針對特
徵向量的機率分布的第三階動差[26]甚至更高階動差進
行正規化[27, 28]。另一方面，亦有學者嘗試將已經在
影像處理中行之有年的統計圖等化法 (Histogram 
Equalization, HEQ)應用於語音辨識之特徵參數正規化上
[29-33]。統計圖等化法除了試圖去匹配訓練語料與測試
語料之語音特徵參數的平均數和變異數之外，更希望能
讓訓練語料和測試語料在正規化後的機率分布特性能夠
相同。雖然，這類方法已證明具有可以減少訓練語料與
測試語料不匹配問題的能力，但它們仍有可以改善的空
間 [33] 。 例如，統 計圖等化法 (Table-lookup HEQ, 
THEQ)[29]或分位差統計圖等化法(Quantile-based HEQ, 
QHEQ)[32]雖能提昇語音辨識系統的辨識效能，但此二
種方法在執行等化過程，可能需耗費大量的記憶體空間
或者是處理器運算時間；因此，可想而知，若要將它們
運用在運算能力有限的行動裝置上，往往是不太可行
的。另一個問題是：雜訊不僅會改變語音特徵的分布，
甚至會因雜訊的隨機性行為而注入不確定性。然而，絕
大部分的統計圖等化法只能處理訓練語料與測試語料不
匹配的問題，只有少部分研究能處理上述因雜訊的隨機
性行為而造成的不確定性[31]。 
基於上述的觀察，吾人在本三年期研究計畫發展出
一種語音強健技術，它不僅能繼承上述的兩個方向發展
它們自身的優勢，亦能改善各自所隱含的限制，以達到
互補的效果。在過去三年的研究時期裡，吾人提出了以
分群為基礎之多項式擬合統計圖等化法(Cluster-based 
Polynomial-fit Histogram Equalization, CPHEQ)[34]，使
用語音特徵與其相對應的分布特性進行語音特徵補償。
CPHEQ繼承以上兩個方向的優點，並純粹以資料導向
(Data-driven)的方式，使用資料擬合技術來近似真實的
機率分布，而不需要對語音特徵機率分布有不符合事實
4 
 
其中T 為所有訓練語料的音框個數， tx 為雙聲源語料
中乾淨語料所擷取出的語音特徵參數， ty 為雙聲源語
料中雜訊語料所擷取出的語音特徵參數，而  iyCDF 則
是 ty 所對應的累積密度函數。因此只需利用 kma 對式(4)
作偏微分令其為零，即可透過解聯立方程式求得每個多
項式轉換函數  kG 的係數。 
但由式(3)可看出，在求取新的語音特徵參數時，必
須將  iyCDF 代入每一群集 k 的多項式轉換函數  kG ，
然後再將多項式轉換函數輸出的特徵值乘上當看到 ty
後群集 k 發生的事後機率。在實作上，當分群數變多，
所需的運算時間會隨之增加；因此，吾人提出的另一種
做法是，利用最大事後機率的概念重新定義式(3)成如
下： 
    
     

 




 




K
k
t
M
m
m
tkm
K
k
tktt
ykyCDFa
yCDFGyky
1 0
1
|   
|~


  (5) 
且 
   

 
otherwise
|'maxarg if
   
0
1
| ' tkt
ykpk
yk   (6) 
其 中  tyk | 為 克 羅 內 克 函 數 (Kronecker Delta 
Function)，判斷的依據是計算在給定給定雜訊語音特徵
向量 ty 下，發生在不同高斯分布的事後機率，唯具有
最大事後機率的高斯分布 k 其  tyk | 值設為 1，否則
設為 0。 
因此，在實作上，對於每一個雜訊語音特徵向量
ty 而言，吾人須先計算在給定 ty 情況下，哪一個高斯
分布具有最大的事後機率，再將 ty 每一維特徵參數所
對應的累積密度函數(CDF)輸入至該高斯分布所對應的
多項式轉換函數  kG 即可得補償後語音特徵，而非像
式(3)需將累積密度函數代入每個高斯分布所對應的多
項式轉換函數。因此，新的(簡化後)多項式轉換函數的
係數估測方式可表示如下： 
    

 

  
 
1
0
2
0
2 |
T
t
t
M
m
tkmtk ykyCDFaxE   (7) 
然而式(4)與式(7)最大的差異只是在於群集指派(Cluster 
Assignment) 方式的不同，式 (4) 屬於軟性指派 (Soft 
Assignment)，每個雙聲源訓練語料樣本對於每個群集的
均方誤差皆有貢獻，貢獻程度取決於該雜訊語音特徵落
在對應群集的事後機率，而式(7)屬於硬性指派(Hard 
Assignment)，因為每一個訓練樣本僅會落在某一群集
內，而其它群集對於語音特徵的補償並不具任何影響。 
3. 多項式擬合統計圖等化法 
在本研究，吾人亦嘗試 CPHEQ 的一種變形做法，稱之
為多項式擬合統計圖等化法（Polynomial-Fit Histogram 
Equalization, PHEQ）。對於 PHEQ，吾人假設用於補償
雜訊語音特徵的轉換函數只有單一個(全域)，因此轉換
函數只須使用乾淨語料即可求得。此項假設源自於
HEQ 的基本理念：找到一個轉換數能將測試語句的語
音特徵向量的每一維的統計分布分別轉換至先前已從訓
練語句中所求得的對應參考分布。這樣一來會使得
PHEQ 實質上的精神與傳統的 HEQ 相當接近。更詳細
地說，PHEQ 在實作上，只使用單一的全域轉換函數來
獲得雜訊特徵參數 
ty 補償後的復原值 tx~ ，因此式(3)可
以簡化成： 
     

M
m
m
tmtt yCDFayCDFGx
0
~    (8) 
其中係數 ma 僅由乾淨的訓練特徵參數 tx 和最小化均方
誤差求得，其均方誤差定義為： 
   
 


 
1
0
2
0
2
T
t
M
m
m
tmt xCDFaxE   (9) 
其中T 為所有乾淨訓練特徵向量。在測試階段，對測試
語句的每一維度特徵參數分別做升冪排序以獲得其對應
的累積密度函數值，再將累積密度函數值帶入  G 獲得
其對應的補償後的特徵參數值。 
接下來，吾人進一步探討多項式擬合統計圖等化
法(PHEQ)與查表式統計圖等化法(THEQ)以及分位差統
計圖等化法(QHEQ)三種方法之間記憶體空間需求和計
算複雜度的差異。一般而言，THEQ 的查表表格的記錄
點數不可過小；在小詞彙語音辨識應用上，THEQ 的特
徵向量每一維度一般需要 1,000 個甚至更多的參考點
數，因此，總合所有維度的參考點數約需要 1Mbyte 的
記憶體空間來記錄表格供查詢(Table Look-up)。但是，
在較複雜的應用上，如大詞彙連續語音辨識(應用在廣
播新聞)，則需要較大的記憶體空間來儲存包含較多參
考點數的記錄表格供查詢，以達到較高的辨識率。然
而，QHEQ 只需少數的分位差(通常為含 4 個參考點數
的記錄表格)就足以涵蓋特徵向量的轉換資訊。同樣
地，PHEQ 則取決於回歸函數的項數，舉例來說，若多
項式的階數為 7，則約需 2.5Kbytes 的記憶體空間來儲
存多項式的係數。 
6 
 
派與軟性指派的辨識結果並無顯著差異，其主要因素可
能是在式(3)中，需計算在給定雜訊語音特徵參數 ty 下
會落在第 k 個高斯分布的事後機率，而此事後機率有可
能只會被某一個高斯分布所支配著，這意謂著其餘的高
斯分布對此訓練樣本的誤差貢獻幾近可忽略，所以使得
式(3)與式(5)擁有大致相同的效用。因此，本研究往後
的 CPHEQ 實驗數據皆是使用硬性指派，以便降低運算
時間。 
在下一個實驗裡，吾人觀察使用不同階數的多項式
轉換函數與不同群集個數的高斯分布混合模型對於辨識
效能的影響。由於多項式階數可能產生端點行為(End 
Behavior)之特性，會使得使用偶數階數的多項式可能無
法滿足累積密度函數端點行為(Monotonically increasing)
的特性，所以在此研究中只考慮使用奇數階數。實驗結
果如圖 1 所示。其中，使用單一轉換函數的情況也列出
以利比較。由圖 1 可看到，隨著分群數增加，平均詞錯
誤率會隨著降低，在分群數較少時，高階的多項式轉換
函數有較好的效果；相反地，當分群數較多時，低階的
多項式轉換函數會有較好的效果，其主要原因可能是雙
聲源訓練語料樣本有限，當分群數太多，會使得每一群
集內的訓練樣本數相對減少，此時若使用高階的多項式
轉換函數，將造成此函數過度擬合(Over-Fit)訓練樣本，
多項式轉換函數的估測也容易受到異常值(Outlier)的影
響，而失去多項式回歸的一般化(Generalization)能力，
此情形亦可解釋為大家所熟知的「維度的詛咒 (The 
Cures of Dimensionality)」。吾人將使用單一多項式轉換
函數的最好結果與梅爾倒頻譜係數(MFCC)基礎實驗結
果做比較，發現 CPHEQ 能相對地降低 41%的詞錯誤
率。進一步地來看，當使用多個多項式轉換函數(例如
1,024 時)，則將會再額外提供 35%的詞錯誤率下降。另
外，在不同的訊噪比下，相對於梅爾倒頻譜係數(MFCC)
基礎實驗的增進結果如圖 2 所示；由表可發現，除了乾
淨的測試語料情況，在不同的訊噪比(SNRs)下，使用越
多的多項式轉換函數能有越多的效能提升。 
接下來的一個實驗裡，吾人將探討以分群為基礎之多
項式擬合統計圖等化法結合不同特徵擷取方式的效能，
因此吾人嘗試結合兩種不的語音特徵；此兩種語音特徵
參數，是分別利用線性鑑別分析(LDA)[15]或異質性線
性鑑別分析 (HLDA)[16] 加上最大相似度線性轉換
(MLLT)[40, 41]作用在梅爾對數濾波器組輸出值(18 個維
度)之後產生。語音特徵參數擷取的方式為：對每個音
框的特徵向量，採用該向量加上其前後各取四個音框的
特 徵 向 量 形 成 162 維 度 的 特 徵 向 量 (Feature 
1-st 3-rd 5-th 7-th 9-th
Polynomial Order
14
15
16
17
18
19
20
21
22
23
24
25
40
60
80
A
ve
ra
ge
 W
or
d 
Er
ro
r R
at
e(
%
)
GMM_1
GMM_32
GMM_64
GMM_128
GMM_256
GMM_512
GMM_1024
 
圖 1：以分群為基礎之多項式擬合統計圖等化法
(CPHEQ)使用不同分群數與搭配不同多項式階數的平均
錯誤率(%)。 
 
Clean 20 dB 15 dB 10 dB 5 dB 0 dB -5 dB
0
4
8
12
16
20
24
28
32
36
40
44
-2
2
6
10
14
18
22
26
30
34
38
42
46
A
bs
ol
ut
e 
W
or
d 
Er
ro
r R
at
e 
Im
pr
ov
em
en
t (
%
)
GMM_32
GMM_64
GMM_128
GMM_256
GMM_512
GMM_1024
圖 2：以分群為基礎之多項式擬合統計圖等化法
(CPHEQ) 在不同分群數與不同訊噪比下相對於梅爾倒頻
譜係數基礎實驗增進的平均錯誤率(%)。 
32 64 128 256 512 1024
Number of Mixtures
12
14
16
18
20
22
24
26
28
A
ve
ra
ge
 W
or
d 
Er
ro
r R
at
e 
(%
)
HLDA_MLLT
LDA_MLLT
CPHEQ
HLDA_MLLT+CPHEQ
LDA_MLLT+CPHEQ
 
圖 3：以分群為基礎之多項式擬合統計圖等化法
(CPHEQ)結合多種特徵參數表示法的平均詞錯誤率。
  
8 
 
看，相對於以分群為基礎之多項式擬合統計圖等化法，
多項式擬合統計圖等化法是具有優勢的。 
儘管如此，由上述的實驗中吾人可發現，使用過少
的多項式轉換函數時，將不足以描述雜訊的特性。因
此，吾人利用簡單的線性插值法，將多項式擬合統計圖
等化法(PHEQ)與以分群為基礎之多項式擬合統計圖等
化法(CPHEQ)做結合，以彌補這項缺點；在此，結合的
插值權重值設定為 0.5，實驗結果如圖 4 所示。由圖 4
可發現，線性結合兩者所產生的語音特徵參數能有較好
的辨識效能；其中又以在以分群為基礎之多項式擬合統
計圖等化法使用較少的多項式轉換函數(如 32、64 和
128)情況下擁有更為顯著的表現。從上述實驗結果顯示
出結合兩者(CPHEQ+PHEQ)的確擁有互補的效果。 
4.4 與多種語音特徵參數正規化法之比較 
吾人將前面所提及的兩種特徵參數正規化法(即以分群
為基礎之多項式擬合統計圖等化法(CPHEQ)與多項式擬
合統計圖等化法(PHEQ))與多種特徵參數正規化法作在
乾淨語料訓練模式[37]下比較。表 5 呈現 CMS[22]、
CMVN[24]、THEQ[29]、QHEQ[32]和 SPLICE[9]在測
試集 A、B 與 C 的平均詞錯誤率詳細結果；同時，梅爾
倒頻譜係數基礎實驗(MFCC)、以分群為基礎之多項式
擬合統計圖等化法(CPHEQ)與多項式擬合統計圖等化法
(PHEQ)也列出來比較。高斯混合模型內的高斯分布皆
為 1,024 個。由表 5 可發現，CPHEQ 的效能表現皆優
於 SPLICE 及任一特徵參數正規化法，而 PHEQ 的效能
表現僅略亞於 SPLICE，但皆優於其它以統計圖等化法
為基礎的正規化法(THEQ 和 QHEQ)。當進一步觀察
CPHEQ 在各個測試集的表現，則可得知 CPHEQ 不僅
在測試集 A 更在測試集 B 中，皆能有效地降低詞錯誤
率；這也表示 CPHEQ 即使是在雜訊特性未出現在訓練
語料中的情況下，依然保有良好的辨識效果。儘管如
此，CPHEQ 在測試集 C 的表現卻較測試集 A 與測試 B
來得差，主要原因是因為在測試集 C 中的測試語料含
有與訓練語料不同的通道效應影響，所以在執行分群的
指派(如式(6))動作時，估測事後機率  tYkp | 會產生誤
差，致使其效能不及在測試集 A 或測試集 B 來得好。
此問題可利用倒頻譜消去法(CMS)[22, 23]解決通道效應
的影響；因此，不論是在訓練高斯混合模型前或是分群
指派前，所有語音特徵參數可先經過倒頻譜消去法處
理，移除通道效應的影響，再接續從事模型訓練或分群
指派。CPHEQ 結合 CMS(標示為 CMS+CPHEQ)後，在
測試集 C 的詞錯誤率便從 20.28%降低到 16.78%。若進
一步觀察表 5 最末欄的辨識結果，吾人可發現 CMS 雖
能增進訓練高斯混合模型與估測事後機率，但卻無法作
用於最後的特徵參數補償。 
為證實在從事特徵參數正規化時，同時使用語音特
徵參數與其相對應的分布特性能較僅使用特徵參數的資
訊的方法有較好的辨識效能。吾人因此也探討以分群為
基礎之多項式擬合特徵參數補償法 (Cluster-based 
Polynomial Feature Compensation, CPFC)。此方法是直
接針對特徵參數而非特徵數相對應的分布特性(如累積
密度函數(CDF)值)做補償；因此，式(3)與式(4)為適用
於此方法，其式中的累積密度函數值  tyCDF 應修改為
特徵參數值 ty 本身。實驗結果如圖 5 所示，在此
CPHEQ 也列出以利比較(其為在最佳設定下的結果)。
觀察圖 5 吾人可歸納出以下三點結論，1）當雜訊語音
特徵 ty 視為一種自變量時，一階多項式函數擁有最佳
的辨識結果；2）當混合高斯模型內的高斯分布數不足
以描述雜訊特徵空間時，將無法適當的描繪乾淨語音與
雜訊間的關係，某種程度來說，這現象意味著簡單的線
性方式不足以補償非線性的失真；3）相較於僅使用特
1 32 64 128 256 512 1024
Number of Mixtures
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
40
60
80
100
A
ve
ra
ge
 W
or
d 
Er
ro
r R
at
e 
(%
)
1-st
3-rd
5-th
7-th
9-th
CPHEQ
 
圖 5：以分群為基礎之多項式擬合特徵參數補償法在不同分
群數與不同多項式階數的平均詞錯誤率(%)。 
表5：梅爾倒頻譜係數基礎實驗與多種強健性方法在乾
淨語料訓練模式的平均詞錯誤率(%)之比較。 
 測試集 A 
測試集
B 
測試集
C 
平均 
MFCC 41.06 41.52 40.03 41.04 
CMS 32.40 27.16 34.15 30.65 
CMVN 22.73 19.60 31.70 23.27 
THEQ 22.70 19.78 26.67 22.33 
QHEQ 23.08 22.03 24.08 22.86 
PHEQ 20.92 18.12 25.68 20.75 
SPLICE 17.03 17.13 26.90 19.04 
CPHEQ 14.35 14.04 20.28 15.41 
CMS+CPHEQ 14.33 13.03 16.78 14.30 
 
10 
 
多項式函數階數，便即可足以描繪轉換函數；在相反
的情況時，則需要較高階數的多項式函數。再者，
CPHEQ 的效能表現不能優於一些現有著名(較複雜)的
強健性方法(此現象的原由來自於 CPHEQ 的簡單特
性，其詳細描述在第 4.5 節)；即使如此，無論是單獨
使用或是與其他較複雜的強健性方法作結合，CPHEQ
皆擁有一定的能力來處理雜訊造成的失真。表 8 從各種
層面比較 CPHEQ 與現有著名的強健性方法，諸如計算
複雜度、記憶體需求和雜訊的估測等。這些方法本身
皆擁有自己的價值與缺點存在。 
以下是本研究計畫所歸納的結論，1）CPHEQ 的計
算複雜度主要取決於高斯混合模型的分布數量，而相
較於其他三種方法，其擁有最低的計算複雜度；2）F-
VTS 與 CPHEQ 兩者皆使用機率模型進行特徵參數的補
償，因此皆只需要少量的記憶體空間需求，以存放模
型的參數；3）AFE 與 MLMWF 的顯著的辨識效果是需
絕對地仰賴於正確的 VAD 與可靠的雜訊估測，但卻不
像 CPHEQ 與 SPLICE 需要額外使用雜訊或環境的模
型，以捕捉雜訊特性。另外，F-VTS 與 CPHEQ 是需要
利用雜訊或環境的模型進行特徵向量的補償。最後，
因為雙聲源語料的取得並非如此容易，而使得雙聲源
語料的需求限制了 CPHEQ 的應用性。解決這項難題的
可能方法之一是藉由引進 VTS 補償的概念[39, 45]，其
原理是利用環境模型進行轉換函數的估測，這議題也
是本研究計畫未來展望的中程目標。另一方面，
CPHEQ 可以有效率的求取訓練語料累積密度函數的反
函數，這意味著相較於傳統以統計圖等化法為基礎的
方法，CPHEQ 在記憶體與計算時間的消耗上擁有絕佳
優勢。 
本計畫的研究成果目前已經發表於[35, 33, 34, 47, 
48]，其中[48]為最為主要的代表著作。另外，吾人目
前亦正在發展新式的統計圖等化法，設法打破語音特
徵向量的每一維度間彼此是獨立的假設，以及嘗試與
其它強健性方法的進一步結合[49, 50]。 
 
6. 參考文獻 
[1] Y. Gong, “Speech Recognition in Noisy Environments: A 
Survey,” Speech Communication, 16(3), 1995, pp. 261-291. 
[2] J. C. Junqua, J. P. Haton and H. Wakita, “Robustness in 
Automatic Speech Recognition,” Kluwer, 1996. 
[3] X. Huang, A. Acero, H. Hon, “Spoken Language Processing: 
A Guide to Theory, Algorithm and System Development,” 
Prentice Hall, 2001. 
[4] Y. Ephraim and D. Malah, “Speech Enhancement Using a 
Minimum Mean-Square Log-Spectral Amplitude Estimator,” 
IEEE Transaction on Acoustic, Speech and Signal Processing, 
33(2), 1985, pp. 443-445. 
[5] S. F. Boll, “Suppression of Acoustic Noise in Speech Using 
Spectral Subtraction,” IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 27(2), 1979, pp. 113-120. 
[6] P. Moreno, “Speech Recognition in Noisy Environment,” 
Ph.D. Dissertation, ECE Department, Carnegie Mellon 
University, Pittsburgh, PA, 1996. 
[7] A. Acero, “Acoustical and Environmental Robustness for 
Automatic Speech Recognition,” Ph. D. Dissertation, 
Carnegie Mellon University, 1990.  
[8] L. Neumeyer and M. Weintraub, “Probabilistic Optimum 
Filtering for Robust Speech Recognition,” in Proc. of IEEE 
International Conference on Acoustic, Speech and Signal 
Processing, 1994, pp. 417-420. 
[9] L. Deng, A. Acero, M. Plumpe and X. Huang, “Large 
Vocabulary Speech Recognition under Adverse Acoustic 
Environments,” in Proc. of the International Conference on 
Spoken Language Processing, 2000, pp. 806-809. 
[10] J. Droppo, L. Deng and A. Acero, “Evaluation of the 
SPLICE Algorithm on the Aurora2 Database,” in Proc. of the 
表 8：廣泛比較以分群為基礎之多項式擬合統計圖等化法與其他強健性方法。 
 
 AFE MLMWF F-VTS CPHEQ 
計算複雜度 高 中等 高 低 
記憶體需求 N/A N/A 低 低 
使用乾淨語音模型 否 否 高斯混合模型 否 
使用雜訊語音模型 否 否 單一高斯 高斯混合模型 
估測雜訊需求 是 是 否 否 
聲音變化偵測器 是 是 否 否 
雙聲源語料 否 否 否 是 
 
12 
 
Recognition Systems under Noisy Conditions,” in Proc. of 
the ISCA ITRW ASR, 2002, pp. 181-188. 
[38] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw, 
G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev and P. 
Woodland, “The HTK Book (for HTK Version 3.3),” 
Cambridge University Engineering Department, Cambridge, 
UK, 2005. 
[39] J. Droppo and A. Acero, “Environmental Robustness,” in 
Springer Handbook of Speech Processing, J. Benesty, M. M. 
Sondhi, Y. Huang (Eds.), Chapter 33, 2008, pp. 653-679. 
[40] M. J. F. Gales, “Maximum Likelihood Linear 
Transformations for HMM-based Speech Recognition,” 
Technical Report, CUED/FINFENG/TR291, Cambridge 
University, 1997. 
[41] R. A. Gopinath, “Maximum Likelihood Modeling with 
Gaussian Distributions,” in Proc. of IEEE International 
Conference on Acoustics, Speech, Signal processing, 1998, 
pp. 661-664. 
[42] B. Chen, J. -W. Kuo, W. -H. Tsai, “Lightly Supervised and 
Data-Driven Approaches to Mandarin Broadcast News 
Transcription,” in Proc. of IEEE International Conference on 
Acoustics, Speech, Signal processing, 2004, pp. 777-780. 
[43] D. Macho, L. Mauuary, B. Noé, Y. M. Cheng, D. Ealey, D. 
Jouvet, H. Kelleher, D.  Pearce and F. Saadoun, “Evaluation 
of a Noise-Robust DSR Front-End on Aurora Databases,” in 
Proc. of the International Conference on Spoken Language 
Processing, 2002, pp. 17-20. 
[44] M. B. Islam, K. Yamamoto and H. Matsumoto, “Mel-Wiener 
Filter for Mel-LPC Based Speech Recognition,” IEICE 
Transactions on Information and Systems, vol. 90, 2007, pp. 
935-942. 
[45] V. Stouten, “Robust Automatic Speech Recognition in Time-
Varying Environments,” Ph.D. Dissertation, Katholieke 
Universiteit Leuven, 2006. 
[46] L. Mauuary, “Blind Equalization in the Cepstral Domain for 
Robust Telephone Speech Recognition,” in Proc. of 
European Signal Processing Conference, 1998, pp. 359-363. 
[47] S. -H. Lin, Y. -M. Yeh and B. Chen, “Investigating the Use of 
Speech Features and their Corresponding Distribution 
Characteristics for Robust Speech Recognition,” IEEE workshop 
on Automatic Speech Recognition and Understanding, pp. 87-92, 
2007. 
[48] Shih-Hsiang Lin, Berlin Chen, Yao-Ming Yeh, “Exploring the 
use of speech features and their corresponding distribution 
characteristics for robust speech recognition,” IEEE 
Transactions on Audio, Speech and Language Processing, 
17(1), 2009, pp. 84-94. 
[49] W.-H. Chen, S.-H. Lin, B. Chen, “Exploiting Spatial-
Temporal Feature Distribution Characteristics for Robust 
Speech Recognition,” in Proc. of the Annual Conference of 
the International Speech Communication Association, 2008, 
pp. 2204-2207. 
[50] B. Chen, W.-H. Chen, S.-H. Lin, W.-Y. Chu, “Robust speech 
recognition using spatial-temporal feature distribution 
characteristics,” submitted to Pattern Recognition Letters, 
2010. 
14 
 
Association (Interspeech 2009), pp. 1847-1850, Brighton, 
U.K., September 6-10, 2009. 
[16] Shih-Hsiang Lin, Yueng-Tien Lo, Yao-Ming Yeh, Berlin 
Chen, "Hybrids of supervised and unsupervised models for 
extractive speech summarization," the 10th Annual 
Conference of the International Speech Communication 
Association (Interspeech 2009), pp. 1507-1510, Brighton, 
U.K., September 6-10, 2009. 
[17] Hung-Shin Lee, Berlin Chen, "Likelihood Ratio Based 
Discriminant  Analysis for Large Vocabulary Continuous 
Speech Recognition," ROCLING XXI: Conference on 
Computational Linguistics and Speech Processing 
(ROCLING 2009), September 1-2, 2009. (in Chinese)  
[18] Kuan-Yu Chen, Berlin Chen, "On the Use of Topic Models 
for Large Vocabulary Continuous Speech Recognition," 
ROCLING XXI: Conference on Computational Linguistics 
and Speech Processing (ROCLING 2009), September 1-2, 
2009. (in Chinese)   
[19] Berlin Chen, "Latent topic modeling of word co-occurrence 
information for spoken document retrieval," the 34th IEEE 
International Conference on Acoustics, Speech, and Signal 
Processing (ICASSP 2009), pp. 3961-3964, Taipei, Taiwan, 
April 19-24, 2009.   
[20] Hung-Shin Lee, Berlin Chen, "Empirical error rate 
minimization based linear discriminant analysis," the 34th 
IEEE International Conference on Acoustics, Speech, and 
Signal Processing (ICASSP 2009), pp. 1801-1804, Taipei, 
Taiwan, April 19-24, 2009. 
[21] Hsuan-Sheng Chiu, Kuan-Yu Chen, Chun-Jen Lee, Berlin 
Chen, "Position Information for Language Modeling in 
Speech Recognition," the 6th International Symposium on 
Chinese Spoken Language Processing (ISCSLP 2008), pp. 
101-104, Kunming, China, December 16-19, 2008. 
[22] Hung-Shin Lee, Berlin Chen, "Improved Linear Discriminant 
Analysis Considering Empirical Pairwise Classification 
Error Rates," the 6th International Symposium on Chinese 
Spoken Language Processing (ISCSLP 2008), pp. 149-152, 
Kunming, China, December 16-19, 2008. (Best Student 
Paper Award) 
[23] Wei-Hau Chen, Shih-Hsiang Lin, Berlin Chen, "Exploiting 
Spatial-Temporal Feature Distribution Characteristics for 
Robust Speech Recognition," the 9th Annual Conference of 
the International Speech Communication Association  
(Interspeech 2008), pp. 2204-2207, Brisbane, Australia, 
September 22-26, 2008.  
[24] Hung-Shin Lee, Berlin Chen, "Linear Discriminant Feature 
Extraction Using Weighted Classification Confusion 
Information," the 9th Annual Conference of the International 
Speech Communication Association (Interspeech 2008), pp. 
2254-2257, Brisbane, Australia, September 22-26, 2008. 
[25] Shih-Hsiang Lin, Yi-Ting Chen, Hsin-Min Wang, Berlin 
Chen, "A Comparative Study of Probabilistic Ranking 
Models for Spoken Document Summarization," the 33rd 
IEEE International Conference on Acoustics, Speech, and 
Signal Processing (ICASSP 2008), pp. 5025-5028, Las 
Vegas, USA, March 30 - April 4, 2008. 
[26] Shih-Hung Liu, Fang-Hui Chu, Shih-Hsiang Lin, Hung-Shin 
Lee, Berlin Chen, "Training Data Selection for Improving 
Discriminative Training of Acoustic Models," IEEE 
workshop on Automatic Speech Recognition and 
Understanding (ASRU 2007), pp. 284-289, Kyoto, Japan, 
December 9-13, 2007. 
[27] Shih-Hsiang Lin, Yao-Ming Yeh, Berlin Chen, 
"Investigating the Use of Speech Features and their 
Corresponding Distribution Characteristics for Robust 
Speech Recognition," IEEE workshop on Automatic Speech 
Recognition and Understanding (ASRU 2007), pp. 87-92, 
Kyoto, Japan, December 9-13, 2007. 
[28] Yi-Ting Chen, Shih-Hsiang Lin, Hsin-Min Wang, Berlin 
Chen, "Spoken Document Summarization Using Relevant 
Information," IEEE workshop on Automatic Speech 
Recognition and Understanding (ASRU 2007), pp. 189-194, 
Kyoto, Japan, December 9-13, 2007. 
[29] Shih-Hung Liu, Fang-Hui Chu, Berlin Chen, "Improved 
MPE-Based Discriminative Training of Acoustic Models for 
Mandarin Large Vocabulary Continuous Speech 
Recognition," ROCLING XIX: Conference on 
Computational Linguistics and Speech Processing 
(ROCLING 2007), September 6-7, 2007. (in Chinese)  
[30] Shih-Hsiang Lin, Yao-Ming Yeh, Berlin Chen, "Cluster-
based Polynomial-Fit Histogram Equalization (CPHEQ) for 
Robust Speech Recognition," the 8th Annual Conference of 
the International Speech Communication Association 
(Interspeech 2007), pp. 1054-1057, Antwerp, Belgium, 
August 27-31, 2007. 
[31] Yi-Ting Chen, Hsuan-Sheng Chiu, Hsin-Min Wang, Berlin 
Chen, "A Unified Probabilistic Generative Framework for 
Extractive Spoken Document Summarization," the 8th 
Annual Conference of the International Speech 
Communication Association (Interspeech 2007), pp. 2805-
2808, Antwerp, Belgium, August 27-31, 2007. 
[32] Yi-cheng Pan, Hung-lin Chang, Berlin Chen, Lin-shan Lee, 
"Subword-based Position Specific Posterior Lattices (S-
PSPL) for Indexing Speech Information," the 8th Annual 
Conference of the International Speech Communication 
Association (Interspeech 2007), pp. 318-321, Antwerp, 
Belgium, August 27-31, 2007. 
[33] YT Yen, B Chen, CL Shyu, YC Li, CY Hsu, "Applying 
Clinical Ontology for Biomedical Information Retrieval," the 
12th World Congress on Health (Medical) Informatics 
(Medinfo 2007), Brisbane, Australia, August 20-24, 2007. 
 
II. GENERALIZED LIKELIHOOD RATIO 
DISCRIMINANT ANALYSIS 
A. Background 
Conceptualized from statistical hypothesis testing [12], the 
likelihood ratio test (LRT) is a celebrated method of obtaining 
test statistics in situations where one wishes to test a null 
hypothesis 0H  against a completely general alternative 
hypothesis 1H . In the paper, 0H  generally represents a 
statistical fact that we would not like to accept. For example, 
in discriminative feature extraction tasks, the parameter space 
(e.g., for class mean vectors and covariance matrices) 
described by 0H  contains less class-discrimination 
information. If Ω denotes the complete parameter space, and 
ω denotes the parameter space restricted by the null 
hypothesis 0H , the LRT criterion for the null hypothesis 
0H  against the alternative hypothesis 1H  is 
ΩL
LLR
sup
sup ω
=  (1) 
where L denotes the likelihood of the sampled data, and 
SLsup  denotes the likelihood computed with the maximum 
likelihood (ML) estimated parameter set S. 
The logic behind the LRT criterion lies in that, if H0 is 
perfectly true with no extra confidence measure being 
considered, the ML estimates over Ω should also occur within 
the parameter set that is consistent with ω. Accordingly, 
ωLsup  and ΩLsup  should be close to each other. On the 
other hand, if 0H  is apparently false, the ML condition will 
happen to occur at a point of Ω that is not in ω, which means 
that ωLsup  will be far smaller than ΩLsup  ideally. 
B. Problem Formulation 
For speech processing, various kinds of LRT statistics have 
been used across many tasks, such as speaker verification [13], 
phonetic disambiguation [14], voice activity detection (VAD) 
[15], etc. In this paper, we, however, do not intend to strictly 
follow the LRT procedure for dimensionality reduction of 
acoustic features. More specifically, we do not set the goal at 
testing whether the null hypothesis is true or false, but instead, 
at seeking a projected subspace, where the (most confusing) 
null hypothesis is as unlikely as possible to be true. To this 
end, we design the following statistical hypotheses: 
0H : The class populations are the same. 
1H : The class populations are different. 
Moreover, the projected subspace spanned by the column 
vectors of the transformation matrix )( nddn <ℜ∈ ×Θ , must 
satisfy the condition that the likelihood of all acoustic features 
generated by the null hypothesis being as small as possible. In 
light of such a perspective, the objective function of 
generalized likelihood ratio discriminant analysis (GLRDA) 
can be generically formulated by  
.
)(sup
)(sup
)(
space paremeter   complete  the
same.    theare  spopulation  class
  e  that  thspace parameter   the
GLRDA Θ
Θ
Θ
L
L
J =  (2) 
Finally, the transformation matrix Θ can be derived by 
minimizing ).(GLRDA ΘJ  
C. The Homoscedastic Case 
Suppose the sampled data is a collection of N independent 
labelled pairs ),( ii lx , where }),...,1{(1 Nini ∈ℜ∈ ×x  is a 
acoustic feature vector, and },...,1{ Cli ∈  is a class label. 
Each class },...,1{ Cj ∈  with the sample size jn  is modelled 
by a Gaussian distribution with mean vector jμ  and 
covariance matrix jΣ . The log-likelihood of the data in the 
transformed subspace is given by  
,
|~|log)~~Tr(
)~~(~)~~(
2
),(
)},{},{,(log)(log
1
1
1
1

=
−
−




+
+−−
−=
=
C
j jjj
jj
T
jjj
jj
N
ndNg
pL
ΣSΣ
μmΣμm
ΘΣμxΘ
 (3) 
where )2log()2(),( πNddNg −= , jm  and jS  denote the 
sample mean and the sample covariance of class j, 
respectively, and the tilde variables refer to the transformed 
versions of the original variables. In general, the parameter 
space, where the class populations are the same, can be 
characterized by the estimates of the class mean vectors.  
Therefore, in the homoscedastic case that all classes are 
assumed to share the same covariance matrix, the hypotheses 
of GLRDA can be stated by 
homo
0H : For each class j, ΣΣ =j  and μμ =j . 
homo
1H : For each class j, ΣΣ =j  and jμ  is unrestricted. 
where homo0H  describes an extreme situation that if it is true, 
then the distributions of all class populations will become 
almost indistinguishable resulting in less class-discrimination 
information offered by the parameter space. Therefore, the 
goal of GLRDA is set to find out the most appropriate 
projected subspace that makes the likelihood of the null 
hypothesis homo0H  be as small as possible. 
The following proposition will show that, in a maximum 
likelihood sense, the classical LDA is merely a special case of 
GLRDA under the above two competing hypotheses homo0H  
and homo1H . 
Proposition 1: The derivation of the LDA transformation 
by maximizing the criterion  
||
||)(LDA ΘSΘ
ΘSΘΘ
W
T
B
T
J =  (4) 
is equivalent to that obtained by minimizing the objective 
function of homoscedastic GLRDA 
,
)(sup
)(sup
)(
homo
1
homo
0homo
GLRDA Θ
ΘΘ
H
H
L
L
J =  (5) 
where nnB ×ℜ∈S  and nnW ×ℜ∈S  denote the between-class 
and within-class scatter matrices, respectively [16]. 
Proof: For computational convenience, (5) can be 
reformulated to a logarithmic form: 
),(logsup)(logsup)(log homo
1
homo
0
homo
GLRDA ΘΘΘ HH LLJ −=  (6) 
where the two log-likelihood functions can be respectively 
expressed by (cf. (3)) 
Therefore, we get the objective function of the heteroscedastic 
GLRDA (H-GLRDA): 
.
)()(
)(1
log
2
)(
1
heter
0
1
heter
0
GLRDA-H

=
− 



−
−+
−
=
C
j
T
j
T
j
T
TT
j
T
jn
G
μΘmΘΘSΘ
μΘmΘ
Θ
 (20) 
The derivative of )(GLRDA-H ΘG  is given by 
,
)~~trace(1
~)~~()(
1
1
11
GLRDA-H 
=
−
−−
+
+−
−=
∂
∂ C
j jj
jjjjj
jn
G
BS
SΘBBSΘS
Θ
Θ  (21) 
where Tjjj ))(( heter0heter0 μmμmB −−= . 
Since 0)(GLRDA-H =′ ΘG  has no analytical solution for the 
stationary points, we can use a gradient descent-based 
procedure for the minimization of )(GLRDA-H ΘG . 
E. Some Discussions 
Table I summarizes the Statistics of GLRDA under various 
hypotheses. Furthermore, not only can GLRDA be reduced to 
LDA, of which the transformation matrix is derived through 
the statistics of homo0H  against homo1H , but also we can prove 
that GLRDA is a generalization of HLDA. 
Proposition 2: The transformation matrix of HLDA can be 
derived by minimizing the ratio of the maximum likelihood 
described by homo0H  to that described by heter1H . In other 
words, GLRDA is a generalization of HLDA. 
Proof: Based on [6], the objective function of HLDA can 
be expressed as 
.||log||log
2
||log
2
)(
1
)()(HLDA
ΘΘSΘ
ΘSΘΘ
Nn
NJ
C
j
dj
T
d
j
dnT
T
dn
+
−−=

=
−−
 (22) 
Since here, )( nn×Θ  is a full-rank matrix and can be 
dissembled to ][ )( dnd −ΘΘ , we can show that (cf. [18]) 
||log||log||log
||log||log||log
||||||
)()(
)()(
)()(
dT
T
dT
T
dnT
T
dn
dnT
T
dndT
T
dT
T
dnT
T
dndT
T
dT
T
ΘSΘΘSΘΘSΘ
ΘSΘΘSΘΘSΘ
ΘSΘΘSΘΘSΘ
−=
+=
×=
−−
−−
−−
(23) 
and 
.||log
2
||log
2
||log
2
||log
2
||log
||log
2
||log
T
T
T
T
N
NNNN
NN
S
ΘSΘΘ
ΘSΘΘ
−=
−−−=
−
 (24) 
Substitute the result in (23) into (22) and consider the result in 
(24), an another form of HLDA thus can be reached 
.||log
2
||log
2
||log
2
)(
)(logsup)(logsup
1
HLDA
homo
0
heter
1
    
ΘΘ
ΘSΘΘSΘ
SΘ
HH
L
dT
T
d
L
C
j
dj
T
d
j
T
Nn
NJ



−−−
−=

=
 (25) 
Obviously, when the constant term ||log)2( TN S−  is being 
ignored, maximizing (25) is equivalent to minimizing the 
objective function of GLRDA, of which the two competing 
hypotheses are homo0H  and heter1H . 
■ 
From Proposition 2, it can be found that the major 
difference between HLDA and heteroscedastic GLRDA lies in 
the descriptions of the null hypothesis 0H : In HLDA, 0H  
is more strictly defined in a homoscedastic fashion. However, 
in heteroscedastic GLRDA, the parameter space described by 
the null hypothesis heter0H  is in fact larger (or more general) 
than that by HLDA, which would make the GLRDA-derived 
feature space be more amenable to class discrimination. 
III. AN EXTENSION OF GLRDA 
The presupposition that the most confusing condition (i.e., 
the null hypothesis 0H ) occurs only when all class means are 
entirely indistinguishable might be too rigorous to tally with 
the realistic speech recognition applications. For example, 
Table II shows the top 5 most confusing class (phone) pairs, 
as well as their corresponding error numbers in frames, 
obtained from the error analysis of a Mandarin LVCSR task 
(see Section IV-A) that employed the LDA-transformed 
speech features [9-11]. As is evident, the dominating or most 
confusing phone pair is (“in”, “ing”), since the number of 
sample items (in frames) that originally belong to “in” and 
“ing” but are misallocated to “ing” and “in”, by the 
recognizer , is the largest among all phone pairs. Intuitively, 
these statistics motivate us to think of a better way to 
represent the adverse condition for classification (i.e., 0H ) 
TABLE I 
The statistics of GLRDA under various hypotheses. 
Statistical Hypotheses ML Estimators 
(Relevant) Maximum 
Log-likelihood 


=
=
μμ
ΣΣ
j
jH homo0  TSm
~,~  |
~|log
2
T
N S−  

 =
edunrestrict:
homo
1
j
jH μ
ΣΣ  Wj Sm ~,~  |~|log
2
W
N S−  


= μμ
Σ
j
jH
edunrestrict:heter
0  heter,0heter0
~,~ jΣμ  
=
+−
C
j
jj
jn
1
|~~|log
2
SB


edunrestrict:
edunrestrict:heter
1
j
jH μ
Σ  jj Sm ~,~  
=
−
C
j
j
jn
1
|~|log
2
S  
TABLE II 
The top 5 most confusing phone pairs in the Mandarin LVCSR system.
K Class (Phone) pair Errors (in Frames) 
1 “in” “ing” 66,353  
2 “an” “eng” 42,550  
3 “i” silence 31,796  
4 “u” silence 29,082  
5 “e” silence 26,134  
Total number of errors 2,400,806 
We also report the experimental results obtained by 
additionally conducting a heteroscedastic feature decorrelation, 
namely maximum likelihood linear transform (MLLT) [22], 
immediately after the abovementioned transformations. 
MLLT was designed to obtain a projection that can 
approximately diagonalizes the class covariances by 
maximizing the likelihood of the projected data, so as to 
approximate the performance of the system that uses full 
covariance HMM modeling. 
In the experiment on heteroscedastic GLRDA (denoted by 
H-GLRDA), since the mean vector heter0μ  in the objective 
function (20) cannot be directly derived from the sample, we 
thus adopt two kinds of approximation for heter0μ . One is to 
represent the mean vector in the original space using the 
following equation (cf. (15)): 
,
1
1
1
1
1heter
0 
=
−
−
=
− 



=
C
j
jjj
C
j
jj nn mSSμ  (28) 
while the other is to merely use the arithmetic mean of all 
sampled data to approximate heter0μ : 
.1
1
heter
0 
=
=
N
i
i
N
xμ  (29) 
We see in Table III that H-GLRDA with the weighted mean 
as the estimate of heter0μ  achieves a higher character accuracy 
(CA), and the same setting will be held for CI-GLRDA. 
To go a step further, Table IV compares the performance of 
H-GLRDA and CI-GLRDA with that of various existing 
LDA-based approaches. As can be seen, without using MLLT 
for feature decorrelation (Column 2), H-GLRDA and CI-
GLRDA both yield much lower accuracies than the existing 
approaches. The major reason would be that the 
transformation matrices derived by the optimization of (20) 
and (26) in essence do not have the effect of approximately 
diagonalizing the transformed covariance matrix for each 
class. They, on the contrary, both demonstrate moderate 
improvements over LDA, HLDA and HDA when MLLT has 
been applied, as illustrated in the right-most column of Table 
IV. It is also noteworthy that the improvement made by H-
GLRDA is less pronounced than that by CI-GLRDA. This 
may confirm our expectation that, with the assistance of 
empirical class confusion information, a proper design for the 
null hypothesis would be beneficial to the final classification 
performance. 
V. CONCLUSIONS 
In this paper, we have proposed a new and more general 
framework, namely generalized likelihood ratio discriminant 
analysis (GLRDA), for discriminative feature transformation 
on the basis of the likelihood ratio test. Not only can GLRDA 
be successfully applied to ASR, but it is also expected to be 
feasible for other pattern recognition tasks. Besides, GLRDA 
can work in conjunction with empirical class confusion 
information for better recognition performance. 
VI. ACKNOWLEDGEMENT 
This work was supported by the National Science Council, 
Taiwan, under Grants: NSC96-2628-E-003-015-MY3, 
NSC98-2221-E-003-011-MY3, and NSC97-2631-S-003-003. 
REFERENCES 
[1] B. D. Ripley, Pattern Recognition and Neural Networks. New York: 
Cambridge University Press, 1996. 
[2] X. Wang and K. K. Paliwal, "Feature extraction and dimensionality 
reduction algorithms and their applications in vowel recognition," 
Pattern Recognition, vol. 36, pp. 2429-2439, 2003. 
[3] X.-B. Li, et al., "Dimensionality reduction using MCE-optimized LDA 
transformation," in Proc. ICASSP 2004. 
[4] D. Povey, et al., "fMPE: discriminatively trained features for speech 
recogntion," in Proc. ICASSP 2005. 
[5] R. A. Fisher, "The statistical utilization of multiple measurements," 
Annals of Eugenics, vol. 8, pp. 376-386, 1938. 
[6] N. Kumar and A. G. Andreou, "Heteroscedastic discriminant analysis 
and reduced rank HMMs for improved speech recognition," Speech 
Communication, vol. 26, pp. 283-297, 1998. 
[7] K. Demuynck, et al., "Optimal feature sub-space selection based on 
discriminant analysis " in Proc. Eurospeech 1999. 
[8] G. Saon, et al., "Maximum likelihood discriminant feature spaces," in 
Proc. ICASSP 2000. 
[9] H.-S. Lee and B. Chen, "Linear discriminant feature extraction using 
weighted classification confusion information," in Proc. Interspeech 
2008. 
[10] H.-S. Lee and B. Chen, "Improved linear discriminant analysis 
considering empirical pairwise classification error rates," in Proc. 
ISCSLP 2008. 
[11] H.-S. Lee and B. Chen, "Empirical error rate minimization based linear 
discriminant analysis," in Proc. ICASSP 2009. 
[12] W. J. Krzanowski, Principles of Multivariate Analysis: A User's 
Perspective. New York: Oxford University Press, 1988. 
[13] C.-H. Lee, "A Tutorial on Speaker and Speech Verification," in Proc. 
NORSIG 1998. 
[14] Y. Liu and P. Fung, "Acoustic and phonetic confusions in accented 
speech recognition," in Proc. Interspeech 2005. 
[15] J. M. Górriz, et al., "Generalized LRT-based voice activity detector," 
IEEE Signal Processing Letters, vol. 13, pp. 636-639, 2006. 
[16] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd ed. 
New York: Academic Press, 1990. 
[17] N. A. Campbell, "Canonical variate analysis with unequal covariance 
matrices - generalizations of the usual solution," Mathematical Geology, 
vol. 16, pp. 109-124, 1984. 
[18] M. Sakai, et al., "Linear discriminant analysis using a generalized mean 
of class covariances and its application to speech recognition," IEICE 
Transactions on Information and Systems, vol. E91-D, pp. 478-487, 
2008. 
[19] J. D. Foley, et al., Computer Graphics: Principles and Practice in C, 
2nd ed.: Addison-Wesley, 1995. 
[20] B. Chen, et al., "Lightly supervised and data-driven approaches to 
mandarin broadcast news transcription," in Proc. ICASSP 2004. 
[21] H.-S. Chiu and B. Chen, "Word topical mixture models for dynamic 
language model adaptation," in Proc. ICASSP 2007. 
[22] R. A. Gopinth, "Maximum likelihood modeling with Gaussian 
distributions for classification," in Proc. ICASSP 1998. 
TABLE III 
The CA results (%) of H-GLRDA with various mean estimates. 
H-GLRDA Without MLLT With MLLT 
Weighted Mean (28) 62.34 74.88 
Arithmetic Mean (29) 58.68 74.45 
TABLE IV 
Comparison among the CA results (%) of H-GLRDA, CI-GLRDA, 
and various LDA-based approaches. 
Methods Without MLLT With MLLT
LDA 71.46 74.33 
HLDA 70.28 74.88 
HDA 71.36 74.53 
H-GLRDA 62.34 74.88 
CI-GLRDA 63.62 75.26 
distribution which corresponds to that of the training (or 
reference) speech. Accordingly, HEQ attempts not only to 
match the means and variances of the speech features, but 
also to completely match the speech feature distributions of 
training and test data. Put another way, HEQ normalizes all 
the moments of the probability distributions of the speech 
features. In practice, the equalization can be conducted either 
in a non-parametric way, such as the table-lookup histogram 
equalization (THEQ) [3-5], or in a parametric way, such as 
the quantile-based histogram equalization (QHEQ) [6].  
2.1 Table-lookup Histogram Equalization (THEQ) 
The cumulative histogram of each feature dimension d  of all 
training data is computed and divided into a set of equally-
probable bins, where the mean ( )iyd  of each bin i  is taken as 
one of the representative outputs of the transformation 
function )( dxF  [3-5]. That is, each feature vector component 
dx  of the test utterance is replaced by the mean of a specific 
bin in the cumulative histogram of the training speech data 
that corresponds to the same bin position of dx  in the 
histogram of the test data. However, normalization of the test 
data alone results in only moderate gain of performance 
improvement. It is usually necessary to normalize the training 
data in the same way to avoid mismatch and to achieve good 
performance [9]. Moreover, because a set of cumulative 
histograms of all speech feature vector dimensions of the 
training data have to be kept in memory for the table-lookup 
of restored feature values, such an approach usually needs 
huge disk storage consumption and the table-lookup is also 
time-consuming. 
2.2 Quantile-based Histogram Equalization (QHEQ) 
In [6], a parametric type of histogram normalization, referred 
to as the quantile based histogram (QHEQ) approach, has 
been proposed. QHEQ attempts to calibrate the CDF of each 
feature vector component of the test data to that of the 
training data in a quantile-corrective manner instead of full-
match of the cumulative histogram as that done by the table-
lookup approach described above. A transformation function 
)( dxH  is applied to each feature vector component dx to 
make the CDF of the equalized feature match that observed in 
training: 
 ( ) ,1)(
,,
, ⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
−+⎟⎟⎠
⎞
⎜⎜⎝
⎛=
Kd
d
d
Kd
d
dKdd Q
x
Q
xQxH
d
αα
γ
 (1) 
where K  is the total number of quantiles; KdQ ,  is the K -th 
quantile of a specific feature vector dimension d  calculated 
from the utterance; and dα  and dγ  are transformation 
coefficients. For each feature vector dimension d , dα  and dγ  
are optimized using the following equation: 
 { } { } ( )( ) ,minarg, 11 2,,, ⎟⎟⎠⎞⎜⎜⎝⎛ ∑ −= −=Kk trainkdkddd QQHdd γαγα  (2) 
where trainkdQ ,  is the k -th quantile of the same feature vector 
dimension calculated from the training speech. QHEQ allows 
the estimation of the transformation function performing 
merely on the basis of a single test utterance (or eventually, a 
short utterance), without using additional adaptation data [5]. 
However, in order to find the optimal transformation 
coefficients for each feature vector dimension, an exhaustive 
grid search is required, which in fact is time-consuming. 
2.3 Polynomial-fit Histogram Equalization (PHEQ) 
Recently, we have presented an efficient method exploring 
the use of data fitting to approximate the inverse of the CDFs 
of the speech feature vector components for HEQ, named 
polynomial-fit histogram equalization (PHEQ) [8]. PHEQ 
makes use of data fitting (or so-called least squared error 
regression) to estimate the inverse functions of the CDFs of 
the training speech. For a specific speech feature vector 
dimension d  of the clean training utterances, given the pair of 
the CDF value ldc ,  of the vector component ldx ,  and ldx ,  
itself at the frame l , the linear polynomial function ( )ldcG ,  
with output ldx ,~  can be expressed as: 
 ( ) ( ) ,~
0
,,,, ∑== =
M
m
m
ldmdldld caxcG  (3) 
where the coefficients mda ,  of each dimension d  can be 
estimated by minimizing the sum of squared errors expressed 
in the following equation: 
 ( ) ,2
1 0
,,,
2 ∑ ⎟⎠
⎞⎜⎝
⎛ ∑−=
= =
N
l
M
m
m
ldmdldd caxE  (4) 
where N  is the total number of training speech feature 
vectors. During the training phase, the polynomial functions 
of all dimensions are obtained by minimizing the sum of 
squared errors expressed in Eq. (4). During the recognition 
phase, for each feature vector dimension, the feature vector 
components of the test utterance are simply sorted in 
ascending order of their values to obtain the approximate 
CDF values, which can be then taken as the inputs to the 
inverse function to obtain the corresponding restored 
component values. 
3. Proposed HEQ Method (STHEQ) 
It is true that the independence assumption of speech feature 
components made by HEQ would be invalid when the feature 
vector components of different dimensions are not completely 
de-correlated. On the other hand, since speech signals are 
slowly time-varying, the contextual relationships of 
consecutive speech frames might provide additional 
information clues for feature normalization. With these 
observations in mind, in this paper we devise an alternative 
HEQ method that can exploit both spatial and temporal 
feature distribution characteristics simultaneously for speech 
feature normalization (demoted as STHEQ). Given the feature 
vector sequence Ll XXX ,,,1 KK  of a speech utterance, where ( )lDldlTl xxxX ,,,1 ,,,, KK= , the corresponding CDF values ldc ,  of 
the feature vector components ldx ,  of each dimension d  at 
each frame l  are first computed, resulting in an CDF vector 
sequence Ll CCC ,,,1 KK  of the utterance being constructed, 
where ( )lDldlTl cccC ,,,1 ,,,, KK= . The resultant CDF vector lC  is 
further concatenated with its K  preceding and K  succeeding 
vectors to form a spliced CDF vector lS  of ( )DK 12 +  
dimensions. Then, the restored feature vector lXˆ  of each lX  
can be obtained by applying a linear transformation on lS : 
,ˆ l
T
l SX A=     (5) 
presented in Figure 2, it is evident that the improvement made 
by STHEQ over the other HEQ methods is steadily increased 
as the noise condition becomes worse. These results indeed 
confirm our expectation that the spatial and temporal 
relationships between speech feature vector components of 
different dimensions and consecutive frames might provide 
helpful information clues for speech feature normalization.  
In the meantime, we are actively working on the ways to 
further improve the performance of STHEQ, including trying 
different methods for CDF estimation [10], investigating the 
possibility of using the other objective functions for deriving 
the distribution-to-feature transformation, etc. We are also 
investigating the application of STHEQ to more complicated 
ASR tasks. 
5. Conclusions 
In this paper, we have proposed a new HEQ based method 
that exploits spatial-temporal feature distribution 
characteristics for speech feature normalization. A 
distribution-to-feature linear transformation was designed for 
such a purpose. The performance of the presented method 
have been extensively tested and verified by comparison with 
the other HEQ methods. The experiment results have 
demonstrated that for clean-condition training, our method 
can yield significant word error rate reduction over the 
baseline system, and also considerably outperforms the other 
HEQ methods compared in this paper.  
6. References 
[1] Y. Gong. “Speech recognition in noisy environments: A 
survey,” Speech Communication, vol. 16, 1995. 
[2] A. Viikki and K. Laurila, “Cepstral Domain Segmental 
Feature Vector Normalization for Noise Robust Speech 
Recognition,” Speech Communication, vol. 25, 1998. 
[3] S. Dharanipargda and M. Padmanabhan, “A Nonlinear 
Unsupervised Adaptation Technique for Speech 
Recognition,” in Proc. ICSLP 2000. 
[4] S. Molau, et al., “Matching Training and Test Data 
Distributions for Robust Speech Recognition,” Speech 
Communication 41(4), 2003. 
[5] A. de la Torre et al., “Histogram Equalization of Speech 
Representation for Robust Speech Recognition,” IEEE 
Trans. on Speech and Audio Processing 13, 2005. 
[6] F. Hilger and H. Ney, “Quantile Based Histogram 
Equalization for Noise Robust Large Vocabulary Speech 
Recognition,” IEEE Trans. on Audio, Speech and Language 
Processing 14, 2006. 
[7] J. C. Segura et al., “Cepstral domain segmental nonlinear 
feature transformations for robust speech recognition,” IEEE 
Signal Processing Letters 11(5), 2004. 
[8] S.H. Lin et al., “Exploiting Polynomial-Fit Histogram 
Equalization and Temporal Average for Robust Speech 
Recognition,” in Proc. Interspeech 2006. 
[9] S. Molau et al., “Histogram Normalization in the Acoustic 
Feature Space,” in Proc. ICASSP 2002. 
[10] E. Alpaydin. Introduction to Machine Learning. MIT Press, 
2004 
[11] R.O. Duda and P.E. Hart. Pattern Classification and Scene 
Analysis. John Wiley and Sons, New York, 1973. 
[12] H.S. Lee and B. Chen, “Linear Discriminant Feature 
Extraction Using Weighted Classification Confusion 
Information,” in Proc. Interspeech 2008. 
[13] N. Kumar. Investigation of Silicon-Auditory Models and 
Generalization of Linear Discriminant Analysis for 
Improved Speech Recognition. Ph.D. Thesis, John Hopkins 
University, Baltimore, 1997. 
[14] H. G. Hirsch and D. Pearce, “The AURORA Experimental 
Framework for the Performance Evaluations of Speech 
Recognition Systems under Noisy Conditions,” in Proc. 
ISCA ITRW ASR 2000. 
[15] S. Young et al., “The HTK Book Version 3.3,” 2005. 
[16] S.H. Lin et al., “Cluster-based Polynomial-Fit Histogram 
Equalization (CPHEQ) for Robust Speech Recognition,” in 
Proc. Interspeech 2007. 
Table 1. The average WER results (%) of STHEQ with 
respect to different numbers (the values of K  described in 
Section 3) of preceding and succeeding frames used to form 
the spliced CDF vectors. 
 Set A Set B Set C Average
K=0 20.6 19.41 20.93 20.31 
K=1 18.33 16.98 19.34 18.22 
K=2 18.02 16.98 18.97 17.99 
K=3 19.04 17.95 19.43 18.80 
K=4 18.05 17.34 20.13 18.51 
K=5 18.14 17.35 20.50 18.66 
 
 
Table 2. The average WER results (%) of three HEQ 
methods, as well as the ETSI system, CMVN, and the 
combination of HLDA and CMVN. 
 
 Set A Set B Set C Average
MFCC 41.06 41.52 40.03 41.04
THEQ 22.76 21.16 23.47 22.47
QHEQ 23.53 21.90 22.36 22.64
PHEQ 20.98 20.17 21.43 20.75
ETSI 38.69 44.25 28.76 38.93
CMVN 27.73 24.60 27.17 26.37
HLDA-CMVN 21.63 21.37 21.59 21.52
 
0
10
20
30
40
50
60
70
Clean 20 15 10 5 0
SNR Levels
WER (%)
THEQ
QHEQ
PHEQ
STHEQ
 
Figure 2: The detailed WER results (%) of STHEQ and the 
other HEQ methods at different SNR levels. 
表 Y04 
 
的研究者交流討論，對於我們的研究發現了許多之前從未察覺的思考方向，也啟發了一
些新的想法。計畫主持人也與 IBM、亞洲微軟研究院、以及新加坡的學者洽談派遣博士
班研究生前往實習研究 (擔任 Intern)的可能性，初步以明年(民國 99年)暑假期間派遣博
士班研究生到 IBM實習研究為規劃方向。 
 
在會場中我們也遇到台大李琳山教授、中研院王新民教授、成大吳宗憲與簡仁宗
教授)以及香港中文大學的 Helen Meng教授，我們也與其中幾位聚在一起，暢談與會經
驗。我們於當地 9月 10日搭機返回台北，此次的出國開會與論文發表任務算是圓滿達。
 
二、與會心得 
此次會議有下列一些 Sessions是計畫主持人感興趣的： 
Monday 7 September 2009 
- Mon-Ses2-O3: Systems for LVCSR and Rich Transcription 
- Mon-Ses3-O4: Systems for Spoken Language Translation 
- Mon-Ses3-O1: Automatic Speech Recognition: Language Models I 
Tuesday 8 September 2009 
- Tue-Ses1-O1: ASR: Discriminative Training  
- Tue-Ses2-O3: ASR: Spoken Language Understanding 
- Tue-Ses2-P3: ASR: Decoding and Confidence Measures 
Wednesday 9 September 2009 
- Wed-Ses1-O3: Automatic Speech Recognition: Adaptation II   
- Wed-Ses1-S1: Special Session: Lessons and Challenges Deploying Voice Search 
- Wed-Ses2-O2: Applications in education and learning 
Thursday 10 September 2009 
- Thu-Ses1-P3: Automatic Speech Recognition: Language Models II   
- Thu-Ses2-P2: ASR: Acoustic Model Features 
- Thu-Ses2-P4: ASR: new paradigms II 
 
綜觀今年 Interspeech2009議程，本人認為在語音辨識與理解、語言模型等研究，有
許多新的方法與結果被提出，這些都是會後值得進一步去深入瞭解的。而對於此次代表
台師大資工系所發表的兩篇論文，本人也預計將其中的方法與實驗內容做延伸，投稿到
IEEE Transactions on Audio, Speech and Language Processing。 
 
總之，經由此次的與會，讓計畫主持人的研究視野不論是廣度與深度都開拓了不
少。相信對今後在台師大資工系所從事的相關學術研究，定能帶來莫大的幫助。 
 
三、考察參觀活動　 無。 
 
四、攜回資料名稱及內容 
1. Interspeech2009 CDROM: 為一光碟，其包含了會議簡介、會程安排、及主要是會
議中所發表文章之全文。 
 
 
 
 
五、其他　  
Improved Speech Summarization with Multiple-Hypothesis Representations and 
Kullback-Leibler Divergence Measures 
Shih-Hsiang Lin and Berlin Chen 
Department of Computer Science & Information Engineering 
National Taiwan Normal University, Taipei, Taiwan 
{shlin, berlin}@csie.ntnu.edu.tw 
 
Abstract 
Imperfect speech recognition often leads to degraded 
performance when leveraging existing text-based methods for 
speech summarization. To alleviate this problem, this paper 
investigates various ways to robustly represent the recognition 
hypotheses of spoken documents beyond the top scoring ones. 
Moreover, a new summarization method stemming from the 
Kullback-Leibler (KL) divergence measure and exploring both 
the sentence and document relevance information is proposed 
to work with such robust representations. Experiments on 
broadcast news speech summarization seem to demonstrate 
the utility of the presented approaches. 
Index Terms: speech summarization, multiple recognition 
hypotheses, KL divergence, relevance information 
1. Introduction 
Extractive summarization produces a summary by selecting 
salient sentences from an original document according to a 
predefined target summarization ratio. The wide spectrum of 
extractive summarization approaches that have been 
developed so far may roughly fall into three main categories 
[1-2]: 1) approaches based on the sentence structure or 
location information, 2) approaches based on proximity or 
significance measures, and 3) approaches based on sentence 
classification.  
For the first category, the important sentences can be 
selected from the significant parts of a document, e.g., 
sentences can be selected form the introductory and/or 
concluding parts. However, such approaches can be only 
applied to some specific domains or document structures. In 
contrast, approaches based on proximity or significance 
measures attempt to select salient sentences based on the 
statistical features of the sentences or the words in the 
sentences, such as the term frequency (TF), the inverse 
document frequency (IDF), the N-gram scores, and the topic 
or semantic information.  The associated methods based on 
these features have gained much attention of research. Besides, 
a number of classification-based methods using statistical 
features and/or sentence structure (or position) information 
also have been developed, such as the Gaussian mixture 
models (GMM), the Bayesian classifier (BC), the support 
vector machine (SVM) and the conditional random fields 
(CRFs). In these methods, important sentence selection is 
usually formulated as a binary classification problem. A 
sentence can either be included in a summary or not. These 
classification-based methods need a set of training documents 
together with their corresponding handcrafted summaries (or 
labeled data) for training the classifiers (or summarizers). 
However, manual annotation is expensive in terms of time and 
personnel. Even if the performance of unsupervised 
summarizers is not always comparable to that of supervised 
summarizers, their easy-to-implement and portable property 
still makes them attractive [3]. 
Although most of the above approaches can be equally 
applied to both text and spoken documents, the latter presents 
unique difficulties, such as speech recognition errors, 
problems with spontaneous speech, and the lack of correct 
sentence or paragraph boundaries. It has been shown that 
speech recognition errors are the dominating factor for the 
performance degradation of speech summarization when using 
recognition transcripts instead of manual transcripts, whereas 
erroneous sentence boundaries cause relatively minor 
problems [4, 5]. A straightforward remedy, apart from the 
many approaches improving recognition accuracy, might be to 
develop more robust representations for spoken documents. 
For example, multiple recognition hypotheses, beyond the top 
scoring ones, are expected to provide alternative 
representations for the confusing portions of the spoken 
documents [6, 7]. Moreover, the use of subword units, as well 
as the combination of words and subword units, for 
representing the spoken documents should be beneficial for 
speech summarization. 
In this paper, we investigate various ways to robustly 
represent the recognition hypotheses of spoken documents, 
including  the use of the confusion network (CN) [8] and the 
position specific posterior lattice (PSPL) [6], for the 
summarization purpose. Moreover, a new summarization 
method originating from the Kullback-Leibler (KL) 
divergence measure [9] and exploring both the sentence and 
document relevance information is proposed to work with 
such robust representations. 
2. Summarization Method 
Extractive summarization produces a concise summary by 
selecting salient sentences or paragraphs from an original 
document according to a predefined target summarization ratio. 
Conceptually, it could be cast as an ad hoc information 
retrieval (IR) problem, where the document is treated as an 
information need and each sentence of the document is 
regarded as a candidate information unit to be retrieved 
according to its relevance (or importance) to the information 
need. Therefore, the ultimate goal of extractive summarization 
could be stated as the selection of the most representative 
sentences that can succinctly describe the main concepts of the 
document. In the past several years, the language modeling 
approaches have been introduced to IR problems and 
demonstrated with good empirical success [9]; this modeling 
paradigm has also been adopted for speech summarization 
recently [2].  
In this paper, we present a novel summarization model, 
stemming from the KL-divergence measure, for important 
sentence selection, which models the relationship between the 
sentences of a document to be summarized and the document 
itself from an information-theoretic perspective. To this end, 
two different language models are estimated: one for the 
whole document and the other for each sentence. We assume 
that words in the document are simple random draws from a 
lattice. Since it is likely that more than one path contains the 
same word, to compute the expected count of a word w  at a 
specific position l  in the lattice, one would need to sum over 
all possible paths in a lattice that contain w  at l . This 
computation can be accomplished by employing a modified 
forward-backward algorithm. For the forward search, the 
forward probability  tw,  is split into several more subtle 
probability masses  ltw ,,  according to the length of partial 
paths that start from the start node and end at w ; while the 
procedure of the backward search remains unchanged. Finally, 
the posterior probability of a given word w  occurring at a 
given position l  in a lattice can be easily calculated [6]. 
3.3. Pruning and Expected Count Computation 
After the construction of CN or PSPL, a simple pruning 
procedure is adopted to remove the unlikely word hypotheses 
(i.e., words with lower posterior probabilities) [6]. For each 
cluster (or position) l , the pruning procedure first finds the 
most likely word entry in it. Then, those word entries that 
have log posterior probabilities lower than that of the most 
likely one minus a predefined threshold   are then removed 
from l . Finally, we can compute the expected frequency 
count of each word w  in a given speech segment o : 
       l w ll wwPwc LAT|,Ε o  (5) 
where lw  is an arbitrary word that occurs in cluster (or at 
position) l ; LAT denotes CN (or PSPL);  LAT|wwP l   
denotes the posterior probability of word w  in cluster (or at 
position) l .  
4. Experiments 
4.1. Experimental Setup 
All the summarization experiments were conducted on a set of 
205 broadcast news documents compiled from the MATBN 
corpus [3]. Three subjects were asked to create summaries of 
the 205 spoken documents for the summarization experiments 
as references for evaluation. A development set consisting of 
100 documents were defined for tuning the parameters (or 
settings) while the remaining documents were taken as the 
held-out evaluation set. The average Chinese character error 
rate obtained for the spoken documents is about 30%.  
To assess the goodness of the automatic generated 
summaries, we use the ROUGE evaluation [11], which is 
based on N-grams co-occurrences statistics between automatic 
summary and a set of reference (or manual) summaries. More 
precisely, we adopted the ROUGE_2 measure, which uses 
word bigrams as the matching units. The summarization 
results were evaluated by using several summarization ratios 
(10%, 20%, and 30%), defined as the ratio of the number of 
words in the automatic (or manual) summary to that of words 
in the manual transcript of the spoken document.  
4.2. Experimental Results 
We first evaluate the utility of using CN and PSPL for 
representing spoken documents. The vector space method 
(VSM) is employed as the default summarization method, 
since it usually achieves quite comparative results as 
compared to other unsupervised methods [2-3]. VSM 
represents each sentence and the whole document in vector 
form, where each dimension specifies the product of the TF 
and IDF scores associated with a word in the sentence (or 
document). Sentences that have the highest cosine or 
proximity scores to the whole document will be included in 
the summary.  
In Table 1, Row “Text” shows the results obtained by 
using manual transcripts of the spoken document and 
sentences for sentence ranking, while Rows “1-best”, “CN” 
and “PSPL” are the results obtained by using the 1-best ASR 
transcripts, CN and PSPL representations, respectively. Also 
noteworthy is that when performing the ROUGE evaluation, 
for both CN and PSPL, only the top scoring word sequence 
derived from them was used to compare to the manual 
summaries. As can be seen, there are significant performance 
gaps between summarization using the manual transcripts and 
the 1-best ASR transcripts; however, summarization using 
either CN or PSPL indeed can provide substantial 
performance boosts over the 1-best ASR transcripts. Moreover, 
PSPL seems to outperform CN for the purpose of speech 
summarization. On the other hand, the resulting summary 
sentences can also be present in speech form (besides text 
form) to bypass the problem caused by speech recognition 
errors [12]. In order to simulate such a scenario as well as to 
assess the performance of the proposed approaches on it, we 
therefore align the ASR transcripts of the summary sentences 
to their respective waveform segments to obtain the correct 
(manual) transcripts for evaluation. The corresponding results 
are shown in the parentheses of Table 1, which reveal that 
with aid of PSPL, we can achieve almost the same 
performance level as that using manual transcripts, when the 
summarization ratio is lower (e.g., 10%) and the resulting 
summary is present in speech form. 
In the next set of experiments, we evaluate the 
performance of the proposed KL-divergence summarization 
method, as well as its integration with various representations 
of spoken documents; the corresponding results are illustrated 
in Table 2. The KL-divergence method dose not seem to 
outperform VSM (cf. Table 1) when the sentence and 
document models were estimated merely based on the 1-best 
ASR transcripts. One possible explanation is that the 
recognition errors contained in the 1-best ASR transcripts 
would seriously hurt the accuracy of model estimation. On the 
contrary, the KL-divergence method can yield superior results 
when compared to VSM, if either CN or PSPL is adopted for 
representing the spoken sentences and the spoken document. 
From the results shown in Tables 1 and 2, we can confirm that 
speech summarization can benefit greatly by the introduction 
of CN and PSPL for robust spoken sentence and document 
representations. 
VSM Summarization Ratio 10% 20% 30% 
Text 0.313 0.405 0.464 
1-best  0.175 (0.252) 
0.212 
(0.310) 
0.261 
(0.374) 
CN 0.198 (0.285) 
0.232 
(0.327) 
0.259 
(0.371) 
PSPL 0.234 (0.313) 
0.260 
(0.368) 
0.285 
(0.401) 
Table 1: The Rogue_2 summarization results achieved by the 
vector space method under different summarization ratios. 
 
 
KL-
divergence 
Summarization Ratio 
10% 20% 30% 
Text 0.360 0.407 0.454 
1-best  0.163 (0.242) 
0.209 
(0.307) 
0.251 
(0.363) 
CN 0.215 (0.316) 
0.237 
(0.340) 
0.265 
(0.382) 
PSPL 0.246 (0.339) 
0.263 
(0.370) 
0.293 
(0.415) 
Table 2: The Rogue_2 summarization results achieved by the KL-
divergence method under different summarization ratios. 
Hybrids of Supervised and Unsupervised Models for Extractive Speech 
Summarization 
Shih-Hsiang Lin, Yueng-Tien Lo,Yao-Ming Yeh, Berlin Chen 
Department of Computer Science & Information Engineering 
National Taiwan Normal University, Taipei, Taiwan 
{shlin, berlin}@csie.ntnu.edu.tw 
 
ABSTRACT 
Speech summarization, distilling important information and 
removing redundant and incorrect information from spoken 
documents, has become an active area of intensive research in the 
recent past. In this paper, we consider hybrids of supervised and 
unsupervised models for extractive speech summarization. 
Moreover, we investigate the use of the unsupervised summarizer 
to improve the performance of the supervised summarizer when 
manual labels are not available for training the latter. A novel 
training data selection and relabeling approach designed to leverage 
the inter-document or/and the inter-sentence similarity information 
is explored as well. Encouraging results were initially demonstrated. 
Index Terms— Speech summarization, hybrid summarizer, 
unsupervised training 
1. INTRODUCTION 
Speech summarization, which aims at extracting important 
information and removing redundant and incorrect information from 
spoken documents, enables us to efficiently review spoken 
documents and understand their associated topics quickly [1-2]. It 
hence has the side effect of improving the efficiency for searching 
or organizing large volumes of spoken documents. The research of 
text summarization dates back to the early 1950s. In the past decade, 
the focus of the summarization research has been extended from 
text to spoken documents. Generally, the summarization techniques 
can be classified as either extractive or abstractive. Extractive 
summarization produces a summary by selecting salient sentences 
or paragraphs from an original document according to a predefined 
target summarization ratio. Abstractive summarization, on the other 
hand, provides a fluent and concise abstract of a certain length that 
reflects the key concepts of the document. This requires highly 
sophisticated techniques, including semantic representation and 
inference, as well as natural language generation [3]. Thus, in recent 
years, researchers have tended to focus on extractive summarization. 
Aside from traditional ad-hoc methods, such as those based on 
document structure and style information [4], linguistic information 
[5], proximity [6] or significance measures [7] to identify salient 
sentences or paragraphs, machine-learning approaches with either 
supervised or unsupervised learning strategies have gained much 
attention and been applied with empirical success to many 
extractive summarization tasks [8]. For supervised machine-learning 
approaches, the summarization task is usually cast as a two-class 
(summary/non-summary) sentence-classification problem: A 
sentence with a set of indicative features is input to the classifier (or 
summarizer) and a decision is then returned from it on the basis of 
these features [8]. Representative supervised machine-learning 
summarizers include, but not limited to, Bayesian classifier, support 
vector machine (SVM), and conditional random fields (CRF). The 
major shortcoming of these summarizers is that a set of handcrafted 
document-reference summary exemplars are required for training 
the summarizers; however, manual annotation is expensive in terms 
of time and personnel. Moreover, such summarizers trained on a 
specific domain might not be directly applicable to another one. The 
other potential problem is the bag-of-instances assumption 
implicitly made by most of these summarizers. That is, sentences 
are classified independently of each other, with little consideration 
of the dependence relationships among the sentences or the global 
structure of the document. 
Another school of thought attempts to conduct document 
summarization using unsupervised machine-learning approaches, 
getting rid of the demand for manually labeled training data. For 
example, the graph-based methods, such as TextRank [9] and 
LexRank [10], conceptualize the document to be summarized as a 
network of sentences, where each node represents a sentence and 
the associated weight of each link represents the lexical or topical 
similarity relationship between a pair of nodes. Document 
summarization thus relies on the global structural information 
conveyed by such conceptualized network, rather than merely 
considering the local features of each node (sentence). Put simply, 
sentences more similar to others are deemed more salient to the 
main theme of the document. Moreover, we have recently proposed 
a probabilistic generative framework for speech summarization, 
which can perform the summarization task in a purely unsupervised 
manner [11]. Each sentence of a spoken document to be 
summarized is treated as a probabilistic generative model or a 
language model for generating the document, and sentences are 
selected according to their likelihoods.  
Even though the performance of unsupervised summarizers is 
usually worse than that of supervised summarizers, their domain-
independent property still makes them attractive. Therefore, we 
expect that researches conducted along the aforementioned two 
directions could complement each other, and it might be possible to 
inherit their individual merits to overcome their inherent limitations. 
In this paper, we also investigate the use of unsupervised 
summarizer to improve the performance of supervised summarizer 
when manual labels are not available for training the latter. A novel 
training data selection and relabeling approach designed to leverage 
the inter-document or/and the inter-sentence similarity information 
is explored as well.  
 
2. EVALUATION CORPUS 
 
All the experiments were conducted on a set of 205 broadcast news 
documents compiled from the MATBN corpus [8, 11]. For each 
news article, three manual summaries are provided as references. A 
development set consisting of 100 documents were defined for 
tuning the parameters or settings while the remaining documents 
were taken as the held-out evaluation set. The average Chinese 
character error rate obtained for the spoken documents is about 
30% and sentence boundaries are determined by speech pauses. 
To evaluate the quality of the automatic generated summaries, 
we used the ROUGE evaluation approach [12], which is based on 
N-grams co-occurrences statistics between automatic summary and 
a set of reference (or manual) summaries. More precisely, we 
In the next set of experiments, we examine the contributions 
that different kinds of features (cf. Table 1) made to the 
performance of SVM for the SD case. As illustrated by Figure 1, 
the summarization performance can be improved steadily by 
including a substantial number of indicative features. The acoustic 
features make more substantial contributions to the performance 
than the lexical features and the relevance features. Combining the 
acoustic features and structural features together outperforms 
combing lexical features and relevance features together. These 
results show that lexical cues might not be the dominating 
predictors when summarization is carried out with erroneous 
recognition transcripts. The results also reveal that relevance 
features are more effective than lexical features. 
In the third set of experiments, we attempt to combine SVM 
with WTM. The combination can be conducted in two alternative 
ways. The first is to directly take the score obtained by WTM as an 
additional feature to augment the feature set defined in Table 1 
(denoted by “Augmented”). The second is to compute the sentence 
prior probability  iSP  of WTM on the basis of the decision 
probability  ii XSP |S  provided by SVM (denoted by 
“Combined”). Therefore,  iSP  can be estimated using a variety of 
features instead of being simply set to uniform. As reported in 
Table 3, both these two combinations significantly boost the 
summarization performance especially at the summarization ratio 
of 10%, as compared to the baseline performance of SVM or WTM 
shown in Table 2. These results seem to justify our postulation that 
supervised and unsupervised summarizers may complement each 
other. 
 
4. TRAINING SVM WITH LABELS DERIVED BY 
WTM 
 
As stated in the previous sections, the supervised summarizer such 
as SVM suffers from several limitations, including the need of 
document-reference summary pairs for either model training or task 
migration. In contrast, the unsupervised summarizer such as WTM 
usually considers the relevance of a sentence to the whole 
document, which might be more robust across different 
summarization tasks. In this paper, we investigate the use of WTM 
to improve the performance of SVM under the condition that the 
handcrafted document-reference summary pairs are not available 
for training the latter. Nevertheless, as will be discussed in Section 
4.3, it was experimentally observed that the performance of SVM 
trained simply with the document-summary labels derived 
automatically from WTM would be worse than the original 
performance of WTM. Therefore, how to filter out unreliable 
automatic labels or collect more reliable automatic labels for 
training SVM without supervision is deemed to be an important 
issue for reducing the performance gap. To this end, we propose a 
training data selection and relabeling approach, which leverages 
either the inter-document or the inter-sentence similarity 
information, to filter out unreliable labels or collect more reliable 
automatic labels for training SVM without supervision.  
 
4.1. Inter-Document Similarity Information  
 
The inter-document similarity (IDS) of a sentence iS  is defined by 
the average similarity of documents in the relevant news document 
set R  of iS  , where R  is obtained by taking iS  as a query and 
posing it to an information retrieval (IR) system to obtained a list of 
M  most relevant documents from a contemporaneous news 
document repository [11]. Our assumption is that the relevant news 
documents retrieved for a summary sentence might have the same 
or similar topics because a summary sentence is usually indicative 
for some specific topic related to the document. In contrast, the 
relevant text documents retrieved for a non-summary sentence 
might cover diverse topics. In other words, the IDS information 
estimated based on the similarity of documents in the relevant news 
document set might be a good indicator for determining the 
importance of a sentence. Consequently, we can select or collect 
more reliable summary/non-summary sentences for training the 
supervised summarizers based on such information. The average 
similarity of documents in the relevant news document set R  for a 
sentence iS  is computed by 
      ,1
 

  





MM
DD
DD
SavgSim
lD
uDlD
uD ul
ul
i
R R


 (5) 
where lD

 is the TF-IDF vector representation of the document lD , 
and M  is the number of documents in the retrieved relevant news 
document set R .  
 
4.2. Inter-Sentence Similarity Information 
 
As opposed to the IDS information, the inter-sentence similarity 
(ISS) of a sentence iS  is derived from the concept of the centrality 
among all sentences in a document. More specifically, if a sentence 
iS  is more similar to other sentences in a document, it might be a 
representative sentence and can be used to depict the main theme of 
 Summarization Ratio 
 10% 20% 30% 
TD 
SVM 0.548 0.625 0.632 
WTM 0.359 0.483 0.517 
SD 
SVM 0.329 0.361 0.350 
WTM 0.205 0.247 0.282 
Table 2: The results achieved by different summarizers under 
different summarization ratios. 
 
 Summarization Ratio 
 10% 20% 30% 
TD 
Augmented 0.596 0.646 0.635 
Combined 0.599 0.652 0.649 
SD 
Augmented 0.338 0.366 0.354 
Combined 0.344 0.368 0.356 
Table 3: The results achieved by combing SVM and WTM. 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Ac St Le Re
Ac
+S
t
Le
+R
e
Ac
+S
t+L
e
Ac
+S
t+R
e
Ac
+S
t+L
e+
Re
10% 20% 30%Summarization Ratio:
 
Figure 1: The summarization results of the SD case achieved 
by SVM with different features and their combinations. 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/11/22
國科會補助計畫
計畫名稱: 強健性語音特徵擷取技術之研究
計畫主持人: 陳柏琳
計畫編號: 96-2628-E-003-015-MY3 學門領域: 自然語言處理與語音處理 
研發成果名稱
(中文) 以分群為基礎之多項式擬合統計圖等化法
(英文) Cluster-based Polynomial-fit Histogram Equalization
成果歸屬機構
國立臺灣師範大學 發明人
(創作人)
陳柏琳,林士翔
技術說明
(中文) 本研究計畫提出結合語音特徵的頻域-時域語音分佈特性(Spatio-Temporal 
DistributionCharacteristics)之語音特徵正規化強健技術架構，以包括語音分
群技術估計之語音特徵累積機率分佈函數(Cumulative Distribution Function, 
CDF)、多項式函數為基礎之轉換函數等，做為雜訊語音處理之依據。此技術是一
項全新的嘗試，有別於一般統計圖等化法(Histogram Equalization Methods)。 
本研究計畫的語音辨識實驗是以Aurora-2 語識庫為題材；實驗結果顯示，在乾
淨語識訓識模式下，吾人所提出的方法相較於基礎實驗結果能顯著地識低詞錯誤
識，並且其成效也較其它傳統基於統計圖等化之語音強健方法識的好。 
(英文) In this research project, we present a robust feature extraction framework building on top 
of polynomial regression, which has the merit of directly characterizing the relationship 
between the speech features and their corresponding probability distributions to 
compensate the noise effects. Two methods derived from such a framework are also 
extensively investigated and compared with the existing robustness methods. 
Experiments conducted on the Aurora-2 standard noise-robust ASR task show that for 
clean-condition training, our methods achieve considerable word error rate reductions 
over the baseline system, and also significantly outperform the conventional histogram 
equalization methods compared in this research.
產業別 其他專業、科學及技術服務業
技術/產品應用範圍 Speech Recognition, Speech Signal Processing
技術移轉可行性及
預期效益
增加語音辨識之強健性、提升語音辨識率
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 計畫主持人目前擔任國際語音通訊協會(ISCA)下 SIG-CSLP 之 Community 
Relations Group 成員；擔任國內計算語言學學會(ACLCLP)之兩屆理事
(2007-2011)、兩屆學術委員會主任委員(2007-2011)；擔任兩屆在國內自然語
言和語音處理領域相當重要 ROCLING 會議(2007 &amp； 2008)之議程主席、國
際會 ISCSLP2010 的 Special Session Chair。 
2. 近五年(2005~)計畫主持人發表 12 篇國際期刊論文與 32 篇國會議論文。其
中 2010 於國際非常重要會議 ACL 發表長篇論文(2010 台灣學者只有一篇長篇論
文被 ACL2010 接受)，計畫主持人並於 ACL2010 擔任 Session Chair. 
3.計畫主持人亦致力於「研究教育」之職責，積極鼓勵所指導的師大碩博士研
究生參加相關學術活動；鼓勵學生參與國際主流語音處理會議汲取新知、培養
其從事語音處理研究之志趣與視野，深切期望能藉此有機會為國內語音研究領
域注入更多年輕新血。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
