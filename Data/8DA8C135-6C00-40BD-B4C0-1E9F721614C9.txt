中文摘要 
 
本計畫針對 Watch List 即 Open-Set 人臉辨識的問題，提出兩項解決方案，一為 Fusion 
Facial Model、另一為 Facial Trait Codes。 前者利用 HMM (Hidden Markov Model)和
SVM (Support Vector Machine)相互結合，而形成一個新的融合分類器。利用 HMM 擷取
一人臉內的單一個體變化量 (intrapersonal variation)，再結合 SVM 可量測多個體的人臉
之間的差異量 (interpersonal variation)，故不僅可將各個體的人臉獨特的特徵變異量擷取
出，並可強化不同人臉之間的特徵差異。本計畫提出之 Fusion Classifier 可維持 SVM 在處
理不同視角下人臉辨識的低誤拒率(FRR)，並可同時利用SVM大幅降低誤受率(FAR)。Facial 
Trait Codes 利用大小與比例不同的矩形視窗掃描人臉，擷取人臉中不同尺寸與分佈的特徵
區塊。由同一矩形視窗在不同人臉中所擷取到的影像特徵，進行局部分類處理。再由不同
矩形視窗內已分類後的局部特徵，進行人臉辨識有效性擷取。再將擷取出的特徵區塊與各
區塊內的類別進行人臉特徵編碼 (Facial Trait Code)，並利用人臉特徵編碼進行人臉辨識。 
 
 
英文摘要 
 
Two approaches are proposed for face recognition in watch-list or open-set scenarios, 
which are considered the most challenging cases in face recognition. The first approach 
combines a HMM and a SVM to form a fusion classifier. The HMM part captures the 
evolution of facial features across a subject’s face without referencing to the faces of 
others. Because of the captured subject-oriented features, the HMM retains certain 
robustness against pose variations, yielding low FRR, on the price of high FAR because 
it is built upon within-class variation only. The SVM part is developed following a 
special design able to substantially diminish the FAR and further lower down the FRR. 
The second approach, called Facial Trait Codes (FTC), first extracts the distinctive trait 
patterns (DTP). The extraction of DTP involves clustering and boosting for maximizing 
the discrimination between faces. The extracted DTP’s can be symbolized and used to 
make up facial trait codes. A given face can be encoded at some prescribed facial traits to 
render a facial trait code with each symbol in its codeword corresponding to the closest 
DTP. Both the fusion model and FTC are experimentally proven effective for watch-list 
face recognition.  
 
 
關鍵詞: Face recognition、open-set face identification、fusion classifier、
hidden Markov models (HMM)、support vector machines (SVM) 
 
 
一張由搜尋組內取得人臉影像，識別系統必定會決定一位相對應的註冊者。 
3. 名單搜尋 (Watch List) 效能：搜尋組內含有許多非註冊者的人臉影像，辨識系統必
須決定任意一張由搜尋組內取得的人臉影像是否為註冊者，若為註冊者，則須決定
對應註冊者的身分。 
    名單搜尋 (Watch List) 效能測試遠比臉部確認和臉部識別富挑戰性，FRVT 2002 評鑑
單位由原搜尋組中任意取出一張人臉影像，參試者必須回答註冊組中是否存有與此人符合
的臉部模型，如有，必須一併輸出人名。以在註冊組含 25 人為例，搜尋正確率僅達 77%。
如將註冊組擴大至 100 人，搜尋組含 200 人的 400 張影像，在 FAR1%下，搜尋正確率下降
到 68%，由此可見名單搜尋的高難度。事實上，名單搜尋的效能直接影響了臉部辨識科技
是否可應用於犯罪防治，從 FRVT 2002 的測試結果看來，似乎仍有一段相當值得努力的空
間。本研究的目的即在設計適用於名單搜尋式的臉部辨識方法。本研究所提出的方法有二： 
1. 利用 HMM (Hidden Markov Model)和 SVM (Support Vector Machine)相互結合，
而形成一個新的融合分類器。利用 HMM 擷取一人臉內的單一個體變化量 
(intrapersonal variation)，再結合 SVM 可量測多個體的人臉之間的差異量 
(interpersonal variation)，故不僅可將各個體的人臉獨特的特徵變異量擷取出，並
可強化不同人臉之間的特徵差異。本計畫提出之 Fusion Classifier 可維持 SVM 在處
理不同視角下人臉辨識的低誤拒率(FRR)，並可同時利用 SVM 大幅降低誤受率
(FAR)。 
2. 利用大小與比例不同的矩形視窗掃描人臉，擷取人臉中不同尺寸與分佈的特徵區塊。
由同一矩形視窗在不同人臉中所
擷取到的影像特徵，進行局部分
類處理。再由不同矩形視窗內已
分類後的局部特徵，進行人臉辨
識有效性擷取。再將擷取出的特
徵區塊與各區塊內的類別進行人
臉特徵編碼 (Facial Trait Code)，
並利用人臉特徵編碼進行人臉辨
識。 
 
人臉辨識的效能常以光源、視角與
表情三大參數進行評估。本研究專注於
不同視角下的人臉辨識，並以名單搜尋式的臉部辨識為測試的評比標準，並以 CMU PIE 資
料庫為主要的測試樣本。CMU PIE 資料庫提供 13 種不同的人臉角度，其中有 8 種為側面
或角度過大的影像，本研究暫不考慮，本研究考慮的範圍包括正面人臉、向上 20 度、向下
20 度、往左往右各 30 度等五種視角，如上圖所示。 
 
第三篇論文摘要 
The Facial Trait Code (FTC) is proposed to encode human facial images. The FTC is 
motivated by the discovery of the basic types of local facial features, called facial trait 
bases, which can be extracted from a large number of faces. In addition, the fusion of 
these facial trait bases can accurately capture the appearance of a face. Extraction of the 
facial trait bases involves clustering and boosting approaches, leading to the best 
discrimination of the faces in the FTC face set. The extracted facial trait bases are 
symbolized and make up the n-ary facial trait codes. A given face can be then encoded at 
the patches specified by the traits to render an n-ary facial trait code with each symbol in 
its codeword corresponding to the closest trait base. Encoding and decoding can be 
made application-oriented using different FTC face sets. We applied the FTC to three 
interesting problems: (1) Synthesis of male (female) faces from female (male) faces, and 
(2) Normalization of the illumination condition over a given face, and (3) A typical face 
identification problem with a hard matching and soft matching schemes. All have 
yielded satisfactory results.   
 
文獻探討：(本節全以英文敘述) 
    The most well known methods for face recognition includes PCA (Principal Component 
Analysis)、LDA (Linear Discriminant Analysis)、EBGM (Elastic Bunch Graph Matching)、Kernel 
Methods、AAM (An Active Appearance Model)、3-D Morphable Model、SVM (Support Vector 
Machine)、HMM (Hidden Markov Models)、and Boosting & Ensemble. Each of the above will be 
briefly reviewed below.  
 
PCA (Principal Component Analysis) 
PCA-based method is derived from Karhunen-Loeve's transformation. Given an 
s-dimensional vector representation of each face in a training set of images, Principal 
Component Analysis (PCA) tends to find a t-dimensional subspace whose basis vectors 
correspond to the maximum variance direction in the original image space. This new subspace is 
normally lower dimensional (t<<s). If the image elements are considered as random variables, 
the PCA basis vectors are defined as eigenvectors of the scatter matrix [1~4]. 
 
LDA (Linear Discriminant Analysis) 
Linear Discriminant Analysis (LDA) finds the vectors in the underlying space that best 
discriminate among classes. For all samples of all classes the between-class scatter matrix SB and 
the within-class scatter matrix SW are defined. The goal is to maximize SB while minimizing SW, 
in other words, maximize the ratio det|SB|/det|SW| . This ratio is maximized when the column 
vectors of the projection matrix are the eigenvectors of (SW-1 × SB) [5~10].  
 
EBGM (Elastic Bunch Graph Matching) 
All human faces share a similar topological structure. Faces are represented as graphs, with 
nodes positioned at fiducial points. (exes, nose...) and edges labeled with 2-D distance vectors. 
Each node contains a set of 40 complex Gabor wavelet coefficients at different scales and 
very accurate (strong) classifier. Viola and Jones build the first real-time face detection system by 
using AdaBoost, which is considered a dramatic breakthrough in the face detection research. On 
the other hand, the papers by Guo et al. are the first approaches on face recognition using the 
AdaBoost methods [33~38].  
 
計畫成果自評： 
 
本計畫原訂目標為利用高畫質與 3D 人臉影像進行 Watch List 人臉辨識方法之研究，
雖因補助金額有限，不足以採購 3D 人臉掃描器。但透過陳祖翰教授提供的 CMU PIE 人臉
資料庫，本計畫依然完成原計畫書規劃之幾項主要研究項目，即利用 2D 人臉影像，推動
可應用於 Watch List 式之人臉辨識方法的研究。在壹年的執行期內，陸續完成 Fusion 
Classifier (融合分類器)與 Facial Trait Code (人臉特徵編碼)的部份方法設計與相關實驗。本
計畫成果可概述如下： 
 
1. Fusion Classifier 挑戰在不同視角下的人臉辨識演算法。利用一串聯機制將 HMM 
(隱藏式馬可夫模型)與 SVM (支持向量機)結合，串聯機制包括兩者所擷取的特徵，
及兩者模型訓練和應用於辨識的部份。本串聯機制保持了兩者個別應用於人臉辨識
時的優點，並利用相互之間的互補性，彌補了兩者個別的缺點。本串聯式臉部辨識
系統可有效結合自我差異(intra-personal variation)與人我差異(inter-personal 
variation)，並由此結合產生低 FAR 與低 FRR 之人臉辨識器。 
2. Facial Trait Code 利用大量的人臉資料，將許多人臉局部特徵進行局部分類；再將
這些局部分類後的特徵，利用其在完整人臉分類的能力，擷取較具人臉分類的權重
組合。再利用這些具有人臉分類權重的局部特徵的類別，進行人臉編碼，並利用編
碼的結果進行人臉註冊與辨識。 
3. 上述二項均有相關論文發表於國際與國內研討會，並且二項目前成果的延伸與詳述
版本均在進行中，準備進行相關領域期刊的投稿。 
 
若以下表為成果自評標準:  
 
等級 特優 優 佳 可 欠佳 
評判標準 
完成原計畫
大部份規劃
工作，作品得
到國際(如知
名期刊、專利
等)之肯定，
或引發廣泛
產業關注。 
 
完成原計畫
大部份規劃
工作，作品得
到廣泛的肯
定，如領域內
國際頂尖研
討會或相關
專利等。 
完成原計畫
大部份規劃
工作，作品得
到 些 許 肯
定，如領域內
一般研討會
等。 
完成原計畫
大部份規劃
工作，但作品
尚未得到其
它單位肯定。 
未完成原計
畫大部份規
劃工作。 
 
因本計畫產生數篇國內外研討會論文，其中一篇發表於 CVPR (IEEE International 
 可供推廣之研發成果資料表 
▓ 可申請專利  ▓ 可技術移轉                                      日期：98 年 9 月 10 日 
國科會補助計畫 
計畫名稱：以高畫質影像及三維臉部掃瞄進行 Watch List 人臉辨識 
計畫主持人：徐繼聖        
計畫編號：NSC 97-2218-E-011-004- 學門領域：圖型辨識 
技術/創作名稱 串聯式臉部辨識系統 
發明人/創作人 徐繼聖 
技術說明 
中文：本發明利用一串聯機制將 HMM(隱藏式馬可夫模型)與 SVM 
(支持向量機)結合，進行快速且精準的人臉辨識。串聯部份包括兩
者所擷取的特徵部份，及兩者模型訓練和應用於辨識的部份。本串
聯機制保持了兩者個別應用於人臉辨識時的優點，並利用相互之間
的互補性，彌補了兩者個別的缺點。本串聯式臉部辨識系統可有效
結合自我差異(intra-personal variation)與人我差異(inter-personal 
variation)，並由此結合產生低 FAR 與低 FRR 之人臉辨識器。 
英文：A cascade architecture is proposed to fuse HMM (Hidden 
Markov Model) and SVM (Support Vector Machine) for face 
recognition. The fusion is on feature extraction, training, and 
recognition. The proposed method keeps the advantages of 
HMM and SVM when each applies for face recognition, while 
dampening the associated disadvantages using the cascade 
architecture. Because both the intra-personal and inter-personal 
variations are considered in this architecture, it yields a classifier 
with low FAR and FRR.  
可利用之產業 
及 
可開發之產品 
1. 人臉門禁控制系統 
2. 人臉登錄系統 
3. 人臉搜尋與犯罪防制相關系統產品 
技術特點 
1. 快速人臉登錄註冊，以 800MHz CPU 與 RAM 256MB 的
Windows OS 為例，個人登錄註冊時間少於一秒(人臉偵測時間
除外) 
2. 快速人臉辨識，以上述為例，個人辨識時間少於 0.1 秒 
3. 以 100 人在一般室內環境下，FAR 0.005 時，FRR 達 0.008 
推廣及運用的價值 
本技術可推廣至門禁控制、安全監控、犯罪防制、電腦與光學產品
之加值軟體等相關系統之設計與製造商，提昇現有產品功能，並加
強產品國際競爭力。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研發成果
推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
附件 1 
Fusion Classifier for Open-Set Face Recognition 
with Pose Variations 
Gee-Sern Jison Hsu
Abstract—A fusion classifier composed of two modules, one 
made by a hidden Markov model (HMM) and the other by a support 
vector machine (SVM), is proposed to recognize faces with pose 
variations in open-set recognition settings. The HMM module cap-
tures the evolution of facial features across a subject’s face using 
the subject’s facial images only, without referencing to the faces of 
others. Because of the captured evolutionary process of facial fea-
tures, the HMM module retains certain robustness against pose vari-
ations, yielding low false rejection rates (FRR) for recognizing faces 
across poses. This is, however, on the price of poor false acceptance 
rates (FAR) when recognizing other faces because it is built upon 
within-class samples only. The SVM module in the proposed model 
is developed following a special design able to substantially dimin-
ish the FAR and further lower down the FRR. The proposed fusion 
classifier has been evaluated in performance using the CMU PIE 
database, and proven effective for open-set face recognition with 
pose variations. Experiments have also shown that it outperforms 
the face classifier made by HMM or SVM alone.  
Index Terms—Face recognition, open-set identification, hidden 
Markov model, support vector machines.
T
I. INTRODUCTION 
H
va
preted
IS paper aims at open-set face recognition with pose 
riations. Open-set face recognition can be better inter-
 using a gallery set and a probe set. A gallery set con-
tains the subjects enrolled to the system with one or a few 
facial images per subject, and a probe set refers to the facial 
images unseen to the system and presented to the system for 
recognition. The images in both sets are disjoint. When both 
sets have the same individuals, it is known as closed-set iden-
tification, and each probe face has one and only one matched 
subject in the gallery. Many former algorithms were evalu-
ated using this scenario. The closed-set identification is often 
not the case in real life, but the open-set recognition is. In 
open-set recognition, the probe set is larger than the gallery 
set, and those in the probe set but not in the gallery act as 
imposters trying to break in the gallery. In such a scenario, 
one must first determine whether a probe face exists in the 
gallery, and if it exists, one will have to identify what the 
matched subject is from the gallery set. Open set face recog-
nition is considered more general, and thus more difficult, 
than closed-set identification because it actually adds in a 
detection task on top of an identification task. It is reported in 
[1] that in an open-set scenario a small size of gallery is eas-
ier to recognize than a large size.  
Poses, illumination conditions, and expressions are gen-
erally acknowledged as three challenging parameters in face 
recognition. Quite a number of methods for recognizing faces 
across poses use 3D approaches [1]-[7], and among them the 
3D morphable model [1, 2] may be the most well-known and 
considered an effective tool for handling poses. However, the 
3D approaches suffer from intensive computation, possible 
imprecise alignments, and undesirable artifacts generated on 
the model-based virtual views. Therefore, many researchers 
have been working on approaches with perspectives different 
from 3D ones.    
A geometry assisted probabilistic approach is reported in 
[8], which approximates a head with a 3D ellipsoid model, so 
that any face image is a 2D projection of such a 3D ellipsoid 
at a certain pose. Linear object classes (LOC) are introduced 
in [9, 10], which are formed by the prototypical views to a 
specific class of objects, as faces for example. LOC has the 
properties that the virtual views of any object of the same 
class under uniform affine 3D transformations can be gener-
ated if the corresponding transformed views are known for 
the set of prototypes. If a training set consists of frontal and 
rotated views of a set of prototypical faces, any rotated view 
of a new face can be generated from a single frontal view. 
Two issues have limited the application of LOC in practice, 
one is the finding of correspondence between the model and 
an image, and the other is the completeness of available ex-
amples for building the prototypes. The virtual view genera-
tion problem is reformulated as a prediction problem in [11], 
and solved by linear regression. This method is inspired by 
the idea that the linear mapping between non-frontal patches 
and frontal patches maintains better than that of the global 
case in the case of coarse alignment.  
This paper reports a model that fuses a Hidden Markov 
Model (HMM) with a Support Vector Machine (SVM), each 
of which has been applied to face identification and recogni-
tion, and each has some specific advantages and weakness 
[12]-[19]. The proposed fusion model can keep their advan-
tages and compensate for their weakness. It has been re-
ported and also observed in our experiments that HMM is 
good for closed-set face identification with pose variations, 
but poor for open-set face recognition [20] with unacceptable 
false acceptance rates (FAR). The poor FAR is primarily 
caused by the HMM built upon within-class samples only. 
To exploit the HMM’s strength in recognizing faces across 
poses and effectively diminish the FAR it induces, the pro-
posed fusion model connects a HMM module with a SVM 
module following a special architecture. When enrolling a 
subject to the gallery, the HMM module, built from the sub-
ject’s facial images, searches for those in the gallery whose 
faces look similar to the subject’s face, and uses these similar 
faces as part of the training sample for building the SVM 
Gee-Sern Jison Hsu is with the National Taiwan University of Science and 
Technology (phone: +886-2-2730-3234, email: jison@mail.ntust.edu.tw,) 
World Academy of Science, Engineering and Technology 56 2009
563
The sign of d(x) shows on which side of the separating 
hyperplane x is located, and its magnitude gives the distance 
of x away from the hyperplane. The larger the distance, the 
more reliable the classification result.    
The feature vectors in [16, 17] are generated by Eigenface 
and Fisherface decompositions, and those in [18] are normal-
ized gray-valued pixel vectors formed by the face images 
after histogram equalization and lighting intensity subtrac-
tion. SVM is good at extracting the information for the dis-
crimination of unclassified features, e.g., the features from 
PCA; however, it is prone to be over-trained with well-
classified features, e.g., the features from LDA [17].  
SVM aims at binary classification, but can be extended to 
multiple classification with two schemes. One is the one-to-
the-rest scheme. If there are n classes in the training set, the 
negative samples of one specific class are the conglomerate 
of all the rest n-1 classes. The other is pairwise approach, 
each classifier only involves two out of the n classes, so there 
will be n (n -1)/2 classifiers in total.     
The major disadvantage of the two schemes is the expen-
sive computation due to the huge amount of training data. In 
the one-to-the-rest scheme, the support vectors of each class 
require the training upon the whole data set; and in the pair-
wise approach, there are too many classes to train. Training 
upon a large number of data poses a serious threat to the ap-
plication scope of SVMs [22].      
III. FUSION OF HMM AND SVM
Aiming at open set face recognition, we propose a fusion  
model, which keeps the advantages of both HMM and SVM 
methods, and effectively overcomes the weakness of each. 
The fused model is composed of a HMM module and a SVM 
module. In the enrollment stage, a subject’s HMM module is 
built from the subject’s facial images, and acts as a filter to 
select those in the gallery whose faces look similar to the 
subject’s face. These similar faces are then used as part of the 
training sample for building the SVM module, substantially 
reducing the amount of training samples for the SVM, and 
also effectively suppressing the HMM-induced FAR in the 
recognition stage. Furthermore, in the recognition stage the 
similarity between a probe face and each subject in the gal-
lery can be readily measured by the subject’s HMM module, 
and the rank-n pool can be quickly determined which in-
cludes n subjects whose faces are considered similar to the 
probe face. The way of developing the HMM modules is 
similar to those reported in [12]-[15], the originality of this 
work is on (1) the fusion of the HMM module and the SVM 
module so that both can be integrated, and (2) the training of 
SVM using a generic negative sample set and a subject-
oriented negative sample set.  
A. Features for the Fusion Model 
The features for the HMM module must preserve the 
variations of local facial features captured by the window 
moving from left to right and top to bottom. Similar to the 
work in [10, 11, 12], the DCT coefficients taken from each 
patch captured by a moving square window are used in this 
work. Only the low frequency parts, i.e., those in the upper 
triangular of each patch’s DCT coefficients, are extracted. 
The DCT coefficients from the overlapping patches are good 
for building the HMM module. But they are inappropriate for 
building the SVM module, because of the high dimensional-
ity of the feature space formed by these coefficients. Con-
sider a case with a 64x64 face and a 8x8 patch overlapped 
with its neighbors for 3 pixels, if the largest 15 DCT coeffi-
cients are taken from each patch, it will result in a feature 
vector of 5415 in dimension. This size of dimension can 
paralyze a computer when it is running a SVM session with a 
large number of training samples.  
A solution to the above is to downsample the features and 
use some subset of the features. Assuming that each patch in 
the above example overlaps its neighbors for 2 pixels only 
and just the largest 9 DCT coefficients are taken, a feature 
dimension of 900 is attained. In our experimental study, we 
tried 5 DCT coefficients from each patch, and the accuracy 
degraded at a negligible degree but came with faster training 
due to the reduced feature dimension of 500. 
B. Development of the SVM module in the Fusion Model 
We propose a special design to the generation of the SVM 
module for each subject when enrolling to the gallery set. 
This design consists of the following steps: 
1. For each subject, the positive sample set is formed by 
the subject’s own face images, but the negative sample 
set is composed of a generic (or white) set and a sub-
ject-oriented set of face images.  
2. The generic negative set aims at carrying a wide spec-
trum of face variations across individuals, poses, illu-
minations and other factors. The generic negative set 
can be made, or approximated, by selecting the repre-
sentative samples from a large face database using self-
organizing maps (SOM’s) and principal component 
analysis (PCA). Each subject in the gallery set shares 
this same generic negative set, but has a subject-
oriented (or tailor-made) negative sample set.   
3. Using the easy-to-be-misclassified samples to strategi-
cally carve the SVM hyperplane, the subject-oriented 
negative sample set is meant to reduce the false accep-
tance rate (FAR) and improve the recognition rate. This 
sample set is formed by the face images of those in the 
gallery who look similar to the enrolling subject, and 
the similarity is measured by the subject’s HMM mod-
ule. The subject’s HMM module can select a rank-m
pool of m similar faces to form the subject-oriented 
negative samples. This subject-oriented negative set and 
the generic negative set constitute the complete nega-
tive training set for building the subject’s SVM module. 
We have tested a few kernels, and decided to use the 
radial basis function (RBF) because it gives better per-
formance than others.  
bKy
Ky
d
s
s
n
i
iiin
i
iiii
 ¦
¦  
 
1
1
)(
)(
1)( xx,
x,x
x D
D
(2)
World Academy of Science, Engineering and Technology 56 2009
565
A. Test Protocols for Performance Evaluation 
Two different test protocols were considered. One studied 
the impact of different number of images available for train-
ing each subject’s fusion model in the gallery set, and the 
other studied the performance variation with different gallery 
sizes. Details are as follows: 
Protocol-1. Different number of facial poses available for 
each subject to be enrolled to the gallery: for the fusion mod-
el we ran two different test scenarios. The Test-1 scenario 
started with 2 poses, frontal (F) and left-sided (L); and then 3 
poses, frontal (F), left-sided (L), and right-sided (R); then 4 
poses, F, L, R, and upward (U); and finally with all 5 poses, 
F, L, R, U, and downward (D). The Test-2 scenario again 
starts at 2 poses, but with F and U; then 3 poses, F, U, and D; 
and then 4 poses, F, U, D, and L. In each scenario, we ran-
domly selected 34 subjects out of the 68 available from the 
PIE database for enrolling to the gallery. For each subject in 
the gallery, we randomly selected the facial image samples 
from one out of the seven selected illumination conditions for 
building the fusion model. The samples in the rest six illumi-
nation conditions were used to compute the FRR (false rejec-
tion rate). The samples of the other 34 individuals were used 
to compute the FAR (false acceptance rate). The randomized 
selection scheme was repeated for 12 times, and the average 
rate was reported. The samples used in the Test-1 scenario 
were also used to build a SVM model and a HMM model for 
performance comparison. To attain a fair comparison, the 
thresholds in all algorithms were adjusted to make the FAR 
at 0.005, except for the HMM model. If the FAR of the 
HMM was set at 0.005, the FRR would have been over 0.9; 
therefore, the FAR was set to 0.15.  
Protocol-2. Different galley sizes and probe sizes: the gal-
lery size and probe size refer to the number of individuals in 
the gallery and probe sets, respectively. Given an upper 
bound of 68 subjects in the PIE database, we tested the gal-
lery sizes of 10, 20, and 34 with the probe sizes of 20, 40, 
and 68, respectively. Those in the gallery set were also in the 
probe set, but with different sets of images. The data partition 
was same as in the Protocol-1, but aims at open-set settings: 
for each gallery size, an equal size of imposters are there 
trying to break in. For each gallery size, we repeated the test 
with a random selection on the subjects for enrollment, and 
then a random selection of one out of the seven illumination 
conditions to provide training samples. As this protocol aims 
at the performance of the proposed fusion model handling 
galleries of different sizes in open-set settings, we used all 5 
poses available from the selected illumination condition for 
enrollment.  
B. Sample Preprocessing and Test Results 
All faces were aligned by the eyes, and normalized to 
64x64 pixels in size according to the distance between the 
eyes. Each facial image was converted from the original col-
or image into an 8-bit gray-scale image. We subtracted the 
best-fit linear plane from each image to reduce possible illu-
mination impacts, and then equalized its intensity histogram.  
To make the generic negative set for the SVM module, we 
collected a large number of face images from other bench-
mark databases, including FRGC [24, 25], AR [26], and 
XM2VTS [27], and some from the internet. Our collection 
had 8,156 facial images with different poses, expressions, 
ethnic backgrounds, and under various illumination condi-
tions. 626 representative ones were extracted using a self-
organizing map (SOM) with facial features extracted by PCA 
(Principal Component Analysis).   
Rank-5 candidate pool was used in all experiments, for ei-
ther training or testing, i.e., n, the number of similar faces 
selected to train the HMM module is 5, and the number of 
candidates selected to validate a probe face is also 5. For the 
proposed fusion model, 8x8 squares overlapped for 4 pixels 
have been chosen along with the major 15 coefficients from 
each square’s DCT map served as the features for the HMM 
module. As mentioned in Section III-A, part of these features 
were used by the SVM module to form the feature vector of 
dimension 500. The HMM-only for comparison used the 
same features as those used in the fusion model, but the 
SVM-only used the 666 low-frequency DCT coefficients 
extracted from each image.   
Fig. 4. Seven similar illumination conditions were selected, and 
the yellow box shows the facial area considered in our experi-
ments.
All test results are the average of 10 randomized selections 
of the gallery sets with the associated probe set. The per-
formance for Protocol-1 is shown in Fig. 5 where the two 
ways of pose variations, Test-1 and Test-2, are compared 
with its SVM-only and HMM-only counterparts. With a pre-
selected FAR at 0.005, the FRR of the fusion model with 
Fig. 5. Performance of the fusion model varies with the number 
of images available for enrollment to the galley set, compared 
with the HMM and the SVM algorithms (FAR=0.005).  
World Academy of Science, Engineering and Technology 56 2009
567
tern Recognition, (CVPR) 2005, vol.1, pp. 502 – 509.  
[9] T. Vetter and T. Poggio, “Linear object classes and image synthesis 
from a single example image,” PAMI, vol.19, no.7, pp.733–742, 1997. 
[10] T. Vetter, “Synthesis of novel views from a single face image,” Int’l J. 
Computer Vision (IJCV), vol.28, no.2, 1998, pp.103–116. 
[11] X. Chai, S. Shan, X. Chen and W. Gao, “Locally Linear Regression for 
Pose-Invariant Face Recognition,” IEEE Trans. Image Processing, vol. 
16, Iss.7, pp. 1716-1725, 2007.  
[12] A.V. Nefian and M.H. Hayes III, “Hidden Markov Models for Face 
Recognition,” Proc. ICASSP, 1998, pp. 2721-2724.  
[13] A.V. Nefian and M.H. Hayes III, “An Embedded HMM Based Ap-
proach for Face Detection and Recognition,” Proc. ICASSP, vol.6, 
1999, pp. 3553-3556.  
[14] S. Eickeler, S. Müller, and G. Rigoll, “Improved Face Recognition 
using Pseudo 2-D Hidden Markov Models,” in Workshop on Advances 
in Facial Image Analysis and Recognition Technology (AFIART), Frei-
burg, Germany, 1998.  
[15] F. Samaria and S. Young, “HMM-based architecture for face identi-
fication,” Image and Vision Computing, 12(8), pp. 537-543, 1994. 
[16] P.J. Phillips, “Support Vector Machines Applied to Face Recognition,” 
in Advances in Neural Information Processing Systems, vol.11, M.J. 
Kearns et. al., eds., MIT Press, 1999.    
[17] K. Jonsson, J. Matas, J. Kittler, and Y.P. Li, “Learning Support Vectors 
for Face Verification and Recognition,” Proc. IEEE Int’l Conf. on Au-
tomatic Face and Gesture Recognition (FG), 2000, pp. 208-213.   
[18] B. Heisele, P. Ho, and T. Poggio, “Face Recognition with Support 
Vector Machines: Global versus Component-Based Approach,” Com-
puter Vision and Image Understanding (CVIU), vol.91, no. 1/2, pp. 6-
21, 2003. 
[19] B. Heisele, T. Serre and T. Poggio, “A Component-based Framework 
for Face Detection and Identification,” IJCV, vol.74, no.2,  pp. 167-
181, 2007.  
[20] P.H. Lee, Y.W. Wang, J. Hsu and Y.P. Hung, “Facial Features Extracted 
by 2-D HMM for Face Recognition with Pose Variations,” Proc. of 
IAPR Conference on Machine Vision Applications (MVA), pp. 392~395, 
2007.   
[21] L. R. Rabiner, "A Tutorial on Hidden Markov Models and Se-lected 
Applications in Speech Recognition," Proc. of IEEE, vol. 77, no. 2, pp. 
257-286, 1989.     
[22] C.J.C. Burges, "Simplified Support Vector Decision Rules," Proc. 13th 
Int Conf. on Machine Learning, pp. 71-78, 1996. 
[23] T. Sim, S. Baker, and M. Bsat, "CMU pose illumination and expres-
sion(PIE) database," PAMI, IEEE Trans, vol.25,  NO.12, Dec 2003. pp. 
1613 – 1618, 2003. 
[24] Frgc    P.J. Phillips, P.J. Flynn, T. Scruggs, K.W. Bowyer, J. Chang, K. 
Hoffman, J. Marques, M. Jaesik, W. Worek, "Overview of the Face 
Recognition Grand Challenge," CVPR 2005, vol.1, 20-25, pp.947-954. 
[25] P.J. Phillips, P.J. Flynn, T. Scruggs, K.W. Bowyer, W. Worek, " Preliminary 
Face Recognition Grand Challenge Results," Proc. 7th Int’l Conf Automatic 
Face and Gesture Recognition, pp. 15-24, 2006.  
[26] A.M. Martinez and R. Benavente, "The AR Face Database," CVC Technical 
Report #24 , June 1998. 
[27] K. Messar, J. Matas, and J. Kittler, "XM2VTSDB: The Extended M2VTS 
Database, " Proc. 2nd Int’l Conf. Audio and Video-based Biometric Person 
Authentication (AVBPA’99’),  pp. 2 – 14, 1999.
World Academy of Science, Engineering and Technology 56 2009
569
Figure 1. Illustration of Facial Trait Code.
encode it using the numbers of the patterns at all facial traits
that best describes the appearance of this face. These num-
bers constitute the facial trait code for this face. An illustra-
tive example is given in Figure 1, where three facial traits
are shown in each of the three faces. According to FTC, the
faces can be encoded into [1, 2, 1], [3, 7, 4], and [9, 9, 6],
respectively.
Putting the facial trait patterns into codes has the follow-
ing advantages:
1. A face can be effectively denoted by a finite length of
codeword in which each symbol gives not just a spe-
cific facial trait with a fixed size and location, but also
the pattern in this facial trait that best describes the face
in that specific facial trait. This implies that the FTC
offers an effective descriptor to a given face.
2. Face identification and verification problems can be
formulated as code matching problems, and thus some
merits from coding perspective can be preserved. Er-
ror correcting is one such merit needed in the develop-
ment of the FTC, and more details will be given sub-
sequently.
3. Coding consists of encoding and decoding. The for-
mer transforms a face into a codeword, and the lat-
ter, according to the proposed FTC, transforms a code-
word to a subject in the gallery for face identification.
This is, however, just an option in the FTC decoding.
Our on-going research show that FTC decoding can be
made to generate faces with different levels of simi-
larity and some caricature faces that preserve certain
features of a real face. This implies that the FTC may
also be useful for other applications, for example in
animation or entertainment areas.
1.1. Related Works
A few works were proposed that put together coding and
facial recognition. Kittler et al [13] and Windeatt et at [23]
applied an Error Correcting Output Coding (ECOC) ap-
proach for face verification. This work shows that ECOC
can decompose a multi-classification problem into a set of
complimentary binary classification problems solvable by,
for example, MLP (Multi-Layer Perceptron) binary classi-
fiers. The input to the binary classifiers are the holistic fa-
cial features extracted using Principal Component Analysis
(PCA) and Linear Discriminant Analysis (LDA). The out-
put of the binary classifiers defines the ECOC feature space,
where the patterns of different faces are claimed to be well
separated. Followed by [13], Xie and Kumar proposed Face
Class Code (FCC) [24] to encode each subject’s facial class
into a binary string. They designed classifiers to discrimi-
nate ’1’ or ’0’ for each bit in the binary string for the deter-
mination of the class label. [24] aimed to fix the computa-
tionally expensive one-classifier-per-subject problem when
the number of subjects is large. Given an N -subject recog-
nition task that generally requiresN binary classifiers, FCC
only creates log2(N) binary classifiers.
The proposed FTC is distinctive from all of the previous
works upon coding for facial recognition in following as-
pects. Firstly, the codes are developed from unperceivable
and local features. Secondly, each symbol in a codeword
denotes some specific pattern existing in a facial trait, i.e., a
local rectangle patch of a certain size and location. Thirdly,
the FTC is an n-ary codeword instead of the binary ones in
all the previous works. This paper begins with the spec-
ification of the FTC in Section 2 that elaborates how the
facial trait codes are determined and specified from a large
collection of faces. When the specification of the FTC is
determined, one can encode a given facial image and de-
code a FTC codeword for face recognition, and the details
are given in Section 3. The experimental setup and the
databases used for the performance evaluation is given in
Section 4. This paper ends at a conclusion in Section 5.
2. Extraction of Facial Traits and Their Pat-
terns
Given a facial image, one can specify a local patch by
a bounding box {x, y, w, h}, where x and y are the 2-D
pixel coordinates of this bounding box’s upper-left corner,
and w and h are the width and height of this bounding box,
respectively. If this bounding box is moved from left to right
and top to bottom in the face with a step size of∆x and∆y
pixels in each direction, one can obtain many patches with
the same size but different locations. If w and h can further
change from some small values to large values, we will end
up with some exhaustive set of local patches across the face.
Some similar set of such an exhaustive collection of local
1614
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
Given N patches and their associated PPMi’s stacked
to form a L × L × N dimensional array, there are L(L−1)2
N -dimensional binary vectors along the depth of this array
because each PPMi is symmetric matrix and one can con-
sider the lower triangular part of it. Let vp,q (1 ≤ q <
p ≤ L) denote one of the N -dimensional binary vectors,
then vp,q reveals the local similarity between the p-th and
the q-th subjects in terms of these N local patches. More
unities in vp,q indicates more differences between this pair
of subjects, and on the contrary, more zeros shows more
similarities.
The binary vector vp,q motivates our application of the
Error Correcting Output Code (ECOC) [6] to this research.
If each subject’s face is encoded using the most discrim-
inant patches, or the facial traits, then the induced set of
[vp,q]1≤q<p≤L can be used to define the minimum and max-
imum Hamming distance among all encoded faces in the
corresponding code space. The vp,q with the least (most)
of unities gives the minimum (maximum) Hamming dis-
tance. To maximize the robustness against possible recogni-
tion errors in the decoding phase, we propose an Adaboost
algorithm to maximize the dmin, the minimum Hamming
distance, for the determination of the facial traits from the
overall patches. This algorithm is summarized in Algorithm
1.
Algorithm 1 Extraction of Facial Trait Patterns
Require: PPMi, i = 1 ∼M
Ensure: selected N facial traits that yield maximum dmin
F = {the set ofM patches}; Fˆ = {∅}
C(p, q) = 0; ω(p, q) = 1, where p = 1 ∼ L and q =
1 ∼ L
for t = 1 to N do
Normalizing the weight ω(p, q) = ω(p,q)∑
p,q ω(p,q)
.
For every element fi ∈ F , α(fi) =∑
p,q PPMi(p, q)ω(p, q)
Select fˆt = argmaxi α(fi)
Update C(p, q) = C(p, q) + PPMi(p, q)
Calculate dmin(dmax), which is the mini-
mum(maximum) element in C(p, q)
ω(p, q) =
 L if C(p, q) = dmin0 if C(p, q) = dmax1 otherwise
Update sets F = F − fˆt and Fˆ = Fˆ
⋃
fˆt.
end for
In Algorithm 1, F is the set of the overall patches, ini-
tially contains M patches. Fˆ is the set of selected facial
traits, which will finally reachN traits in total. C is a L×L
dimensional array where C(p, q) gives the number of ones
in vp,q. ω is a weight array with the same dimension as that
of C, and ω(p, q) is the weight for the subject pair p and q.
In each run, the patch able to maximize the updated dmin is
selected as one new facial trait.
The N facial traits with their trait patterns symbolized
from 1, 2, ..., N define the basic structure of the proposed
Facial Trait Code. Each codeword in the FTC is of lengthN
and n-ary where n is the largest number of the trait patterns
found in one single trait. And the smallest distance between
codewords is dmin. In summary, given a set of frontal faces,
we can define N facial traits,
∑N
j=1 kj trait patterns, and∏N
j=1 kj faces (or FTC codewords).
3. FTC Encoding, Decoding, and Application
to Face Recognition
The following facial sets need to be defined so that one
can apply the proposed FTC for face recognition.
1. Trait Extraction Set: A large collection of neutral
frontal faces which best covers both genders and a
wide range of ages, races, and possibly other param-
eters. Those with variations caused by poses, illumi-
nations, and expressions are excluded. The facial trait
patterns and thus the facial trait codes are defined upon
such a set. This set specifies (1) the number of facial
traits, and thus the codeword length for a given face;
(2) the patterns in each facial trait, and thus the range
of each symbol in a codeword; (3) the location and the
size of each facial trait.
2. Trait Variation Set: This is the set that encompasses
the images with the variations excluded in the trait ex-
traction set. This set does not alter any specifications
given by the trait extraction set, but substantially en-
riches the spectrum of each predetermined trait pattern
by adding in samples with variations.
3. Gallery Set: This is the set that those enrolled used to
register their faces to a face recognition system. It is
allowed in the FTC and other algorithms that images
with the aforementioned variations can be included.
4. Probe Set: This is a disjoint set from the gallery set,
and is used to test the recognition rate and other perfor-
mance indices. This set includes the images of those
enrolled but taken at different time and conditions, and
also the images of those not enrolled. It often includes
images with a tremendous amount of the aforemen-
tioned variations.
With a pre-selected length of the FTC codeword, N , the
trait extraction set defines N facial traits of different sizes,
orientations, and locations, and also the patterns in each fa-
cial trait. Each facial trait pattern is tagged with a number,
which will be used as a symbol in the FTC codeword. In
1616
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
(a)
Figure 4. FTC identification performance with different numbers
of facial traits.
2. Subset Identification: To study the identification per-
formance with different sizes of datasets, three subsets
were randomly selected from Sg . Sg1 had 100 sub-
jects, Sg2 had 200 subjects, and Sg3 had 400 subjects.
The three corresponding probe sets, Sp1, Sp2, and Sp3
were also selected from Sp. This test was repeated for
more than 20 times to obtain some average statistics.
3. Subset Verification: The three probe sets Sp1, Sp2, and
Sp3 were used along with three disjoint but equal-sized
probe sets, Sq1, Sq2, and Sq3, respectively, to match
against Sg1, Sg2, and Sg3. Sq1, Sq2, and Sq3 were
used as imposters who were trying to break in using
false identities. This test was also repeated for more
than 20 times.
4. Generalization Test: This test was designed to eval-
uate the generalization performance of the FTC. The
840 subjects were randomly selected to form three dis-
joint sets, V1 had 440 subjects, V2 had 200 subjects,
and V3 had the rest 200 subjects. V2 was further par-
titioned into two disjoint subsets V2a and V2b, which
had the same subjects but with images taken at differ-
ent time. V1 was used as the trait extraction set and
trait variation set in the FTC, and also as the basis ex-
traction set for the approaches as PCA and LDA.When
the basis components were defined byV1, the faces in
V2a, V2b and V3 would be decomposed accordingly.
V2a was then used as the gallery set,V2b as the probe
set, and V3 as another probe set formed by imposters
only. V2a andV2b were used in identification test, and
withV3 added in for verification test. This test was re-
peated for at least 20 times for 4 different gallery sizes,
20, 40, 80, and 160.
With the first setupOverall Identification, figure 4 shows
the the FTC’s performance with different numbers of facial
traits, and thus with different length of codeword. It can be
seen that the identification rate increases with the number of
the facial traits, and it reaches some upper bound between
64 and 128 traits. If 127 traits were selected, we would end
Figure 5. An example of the FTC codebook.
up with a [127, 840, 41] FTC3, which could correct up to
41 error digits in the 127 symbol positions. The BCH code
proposed in Kittler01 in our experiment could only correct
27 error bits. We also found that the number of trait patterns
in each different trait was typically from 15 to 30. A few
major facial traits with their patterns are shown in figure 5.
Figure 6 (a) and (b) gives results for the Subset Identi-
fication and Subset Verification, respectively. The verifica-
tion performance was evaluated as the Hit Rate when the
False Acceptance Rate was 0.001. The most difficult test
scenario was given by theGeneralization Test, and the iden-
tification and verification rates are shown in Figure 7 (a) and
(b), respectively. In both Figure 6 and 7, we use 127 facial
traits for the proposed FTC method and a 127-bit BCH code
for the algorithm Kittler01.
Our experiments have shown that the ECOC algorithm
Kittler01 performs well in most setups except the most
challenging Generalization Test. This is because its bina-
rization at each bit is determined solely by the BCH cod-
ing method, resulting in dichotomies irrelevant to the ap-
pearance of a human face. It sure fails in the generaliza-
tion test. In all our experiments, FTC outperformed other
methods, including Ahonen06, which drew some attention
in the face recognition community recently. FTC performed
better than Ahonen06 in the subset(generalization) test for
23.9%(15.8%) higher in the verification rate.
Table 2 summarizes the performance of all algorithms
evaluated in our experiments in terms of average identifica-
tion/verification rates. Heisele03 were not feasible for the
study of the generalization performance, and thus excluded
in this table.
3We denote a [N,M, dmin−1
2
] FTC as a code with M valid n-ary
codewords of lengthN . And the smallest distance between theseM code-
words is dmin.
1618
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
larger out-of-plane rotations. One better solution is to use
3-D facial data. It is worthy of mention that the proposed
algorithm can be extended to incorporate 3-D facial data
straightforwardly. The whole algorithm remains the same,
except that the raw features become range data instead of
image intensities.
Acknowledgement
This work is supported in part under the grant 97-EC-17-
A-02-S1-032.
References
[1] J. Ahlberg. Facial feature extraction using deformable graphs
and statistical pattern matching. In in Swedish Symposium on
Image Analysis, SSAB, 1999.
[2] T. Ahonen, A. Hadid, and M. Pietikainen. Face description
with local binary patterns: Application to face recognition.
In PAMI, pages 2037–2041, 2006.
[3] E. Bart, E. Byvatov, and S. Ullman. View-invariant recogni-
tion using corresponding object fragments. In Proceedings
of the Seventh European Conference on Computer Vision,
pages Vol II: 152–165, 2004.
[4] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces
vs. fisherfaces: Recognition using class specific linear pro-
jection. PAMI, 19(7):711–720, 1997.
[5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for
support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
[6] T. G. Dietterich and G. Bakiri. Solving multiclass learning
problems via error-correcting output codes. Journal of Arti-
ficial Intelligence Research, 2:263–286, 1995.
[7] M. Figueiredo and A. Jain. unsupervised learning of finite
mixture models. PAMI, 24:381–396, 2002.
[8] V. Guruswami and A. Sahai. Multiclass learning, boosting,
and error-correcting codes. In Proceedings of the twelfth
annual conference on Computational learning theory, pages
145 – 155, 1999.
[9] B. Heisele, P. Ho, J. Wu, and T. Poggio. Face recognition:
component-based versus global approaches. CVIU, 91(1):6–
12, 2003.
[10] B. Heisele, S. Thomas, P. Sam, and T. Poggio. Hierar-
chical classification and feature reduction for fast face de-
tection with support vector machines. Pattern Recognition,
36(9):2007–2017, 2003.
[11] Y. Ivanov, B. Heisele, and T. Serre. Using component fea-
tures for face recognition. In FGR’04, page 421, 2004.
[12] M. J. Jones and P. Viola. Face recognition using boosted
local features. Technical report, 2003.
[13] J. Kittler, R. Ghaderi, T. Windeatt, and J. Matas. Face veri-
fication using error correcting output codes. In CVPR, vol-
ume 1, pages I–755– I–760, 01.
[14] R. Liao and S. Z. Li. Face recognition based on multiple
facial features. In In Proc. of the 4th IEEE Int. Conf. on
Automatic Face and Gesture Recognition, pages 239–244.
Dekker Inc, 2000.
[15] A. Martinez and R. Benavente. The ar face database. Tech-
nical Report 24, CVC, 1998.
[16] K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre.
Xm2vtsdb: The extended m2vts database. In AVBPA, 1999.
[17] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang,
K. Hoffman, J. Marques, J. Min, andW.Worek. Overview of
the face recognition grand challenge. In CVPR, pages 947–
954, 2005.
[18] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss. The
FERET evaluation methodology for face-recognition algo-
rithms. PAMI, 22(10):1090–1034, 2000.
[19] M. Savvides, R. Abiantun, J. Heo, S. Park, C. Xie, and B. Vi-
jayakumar. Partial and holistic face recognition on frgc-ii
data using support vector machine. In Proceedings of the
2006 Conference on Computer Vision and Pattern Recogni-
tion Workshop (CVPRW06), pages 48–48, 2006.
[20] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination,
and expression (PIE) database of human faces. Technical Re-
port CMU-RI-TR-01-02, Carnegie Mellon University, 2001.
[21] M. Turk and A. Pentland. Eigenfaces for recognition. Jour-
nal of Cognitive Neuroscience, 3(1):71–86, 1991.
[22] P. Viola andM. Jones. Rapid object detection using a boosted
cascade of simple features. In CVPR, volume 1, pages 511–
518, 2001.
[23] T. Windeatt and G. Ardeshir. Boosted ecoc ensembles for
face recognition. In VIE, pages 165 –168, 2003.
[24] C. Xie and B. V. Kumar. Face class code based feature
extraction for face recognition. In Fourth IEEE Workshop
on Publication Automatic Identification Advanced Technolo-
gies, pages 257–262, 2005.
[25] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld.
Face recognition: A literature survey. ACM Comput. Surv.,
35(4):399–458, 2003.
Appendix: Error-Correcting Property of the
ECOC
FTC inherits the error-correcting capability from the
ECOC. An (N,L, dmin) ECOC C is a set of L binary vec-
tors of dimensionN , called codewords, such that the Ham-
ming distance between every pair of distinct codewords is
at least dmin. This code can correct at least (dmin−12 ) er-
ror bits. Since every codeword has distance at least dmin
from every other codeword, the closed Hamming balls of
radius (dmin−12 ) around each codeword are disjoint. Hence
if a binary vector y differs from some codeword x ∈ C in at
most (dmin−12 ) bit positions, then x is still the unique clos-
est codeword in C to y [8]. Consequently, an ECOC with
larger dmin is able to correct more error bits. It also means
that the L codewords in C are well separated in the Ham-
ming space of C.
1620
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
