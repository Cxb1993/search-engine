2 
摘要 
本計劃對鳥類鳴叫聲音之自動辨識做一深入之研究，以輔助調查鳥類族群之生態、
棲地之變化，並能減少對生態的影響。首先將輸入之鳥類聲音中之每一音節切取出來，
然後我們提出結合倒頻譜濾波法(cepstral filtering)及調變頻譜過濾演算法(modulation 
spectrum filtering)來濾除聲音訊號之雜訊。接著以整個音節之二維倒頻譜係數、動態二
維梅爾倒頻譜係數及 MPEG-7 之聲音頻譜封包(Normalized Audio Spectrum Envelope, 
NASE)之調變係數為此音節之特徵向量，然後以主軸分析演算法(Principal Component 
Analysis, PCA)來降低特徵向量之維度，對於每一種鳥類聲音，我們使用高斯混合模型
(Gaussian mixture model, GMM)來描述並得到同一種鳥類聲音之代表特徵向量群組，再
以線性區別分析演算法(Linear Discriminant Analysis, LDA)來提升辨識之正確率，最後將
各種特徵向量整合在一起，進一步提高辨識率，當結合三種特徵向量時可得到之最佳辨
識正確率為 83.90%。 
 
一. 報告內容 
1. 前言 
全球生物多樣性資訊機構(GBIF)已在 2001 年正式成立，其目的在配合生物多樣性
國際公約之要求，推動全球各國成立生物多樣性資訊交換中心，以進行生物多樣性資料
之蒐集、整理與保存，使各國生物多樣性之資訊可與全球其他國家分享，促進生物多樣
性之保育、利用、管理、研究及教育。國科會於 2001 年起推動建立「台灣生物多樣性
國家資訊網」計畫 (Taiwan Biodiversity National Information Network, TaiBNET)，目前已
完成本地生物多樣性專家名錄及台灣物種名錄兩個資料庫。此外國科會正在推動之「數
位典藏國家型科技計畫」，其重點在將典藏之生物標本或文獻解說等基本資料予以數位
化，農委會多年來也持續在推動全省分年分區生態之調查與時間空間分佈資料之數位化
工作。從事生態調查之工作通常是由專家或具備豐富野外調查經驗之人員來執行，一般
而言，都是依據視覺上(外形、顏色等)及聽覺上(聲紋)之特徵來辨識動物種類，但是對
於某些動物，其習性隱匿不易觀察，若想見其行蹤，則是難上加難，例如八色鳥是國際
鳥類聯盟公佈的全球遭受威脅鳥種之一，估計全球數量可能不到一萬隻，在台灣也十分
稀少，而鳴叫聲是其互相溝通聯繫的重要工具，通常我們在野外比較容易聽到其叫聲而
不易見其形體，此外生物的叫聲早已進化成與特定之物種相關，也就是不同之物種之聲
音會有所不同，因此利用生物的鳴叫聲來辨識生物種類是相當自然且有效可行的方法，
4 
 
2.1 聲音訊號強化 
本計劃擬提出之聲音訊號強化演算法是結合倒頻譜濾波法(cepstral filtering)及調變頻譜
演算法(modulation spectrum filtering)來濾除聲音訊號之雜訊。首先，我們將時間域之鳥
類聲音訊號以傅立葉轉換得到其頻譜，接著對頻譜係數取對數可得到對數頻譜(log 
magnitude spectrum)，然後將對數頻譜做餘弦轉換可得到其倒頻譜係數，之後對倒頻譜
係數做低通濾波計算，再做反餘弦轉換得到回復之對數頻譜，此一步驟之目的是對聲音
頻譜做平滑化(smoothing)以避免通道失真之影響，然後我們將以調變頻譜濾波法來消除
環境雜訊，首先，對回復之對數頻譜之同一頻率係數沿著時間軸做餘弦轉換成調變頻譜
(modulation spectrum)，保留調變頻譜 1~16 赫茲(Hertz, Hz)範圍內的係數，再做反餘弦轉
換可得到一個乾淨的頻譜，將此頻譜之係數正規化至[0, 1]，再與原始之頻譜相乘，即可
對原始聲音訊號作強化以去除雜訊，圖二顯示原始有雜訊之鳥類鳴聲之頻譜及強化後之
頻譜。其詳細的步驟如下： 
步驟 1. 取音框 (frameing)大小為 512，重疊一半。 
步驟 2. 傅立葉轉換 
NkenwnxkX
N
n
n
N
kj
qq <≤=∑−
=
−
0  ][][][
1
0
2 ，π  
其中 N 為音框大小，令 xq[n]表示第 q 個音框之第 n 個訊號值，Xq[k]為第 q 個音
框之第 k 個傅立葉係數，w[n]為漢明視窗(Hamming window)之第 n 個係數值： 
Nn
N
nnw <≤−−= 0  ),1
2cos(46.054.0][ π  
步驟 3. 取對數頻譜 
][log][ 10 kXkM qq =  
其中 Mq[k]為對數頻譜係數 
步驟 4. 餘弦轉換 
對所有對數頻譜係數做餘弦轉換以得到倒頻譜係數： 
1-0  ,
2
)12(cos][][2][
1
0
Nn
N
nkkMn
N
nC
N
k
qq ≤≤⎟⎠
⎞⎜⎝
⎛ += ∑−
=
πμ  
其中 Cq[n]為倒頻譜係數，μ[n]之定義如下： 
6 
 
    
(a)       (b) 
    
(c)       (d) 
圖二 (a)原始有雜訊之鳥類鳴聲之頻譜 (b)做倒頻譜運算後之頻譜 (c)做調變頻譜處理
後之頻譜 (d)強化後之頻譜。 
 
2.2 特徵擷取 
 對於切割出來之每一鳥類聲音之音節，我們將自倒頻譜域(cepstral domain)中擷取二
維 梅 爾 倒 頻 譜 係 數 (Two-dimensional Mel-scale Frequency Cepstral Coefficients, 
TDMFCC）及動態二維梅爾倒頻譜係數(Dynamic TDMFCC, DTDMFCC)，以及自頻譜域
(spectral domain)擷取 MPEG-7 之聲音頻譜封包(Normalized Audio Spectrum Envelope, 
NASE)為此音節之特徵向量。 
 
2.2.1 MPEG-7 之聲音頻譜封包之調變係數 
在MPEG-7標準中[18]，是以對數之頻率間格來描述音訊訊號的頻譜圖。由於人類對於頻
率的敏感度是成對數對應關係，所以我們使用對數頻率來取頻率間距，如此可以兼顧簡
潔性與描述性。聲音頻譜封包(audio spectrum envelope, ASE)在MPEG-7標準裡普遍用於
表示原始聲音訊號中每一頻帶之功率頻譜，主要是描述介於loEdge (預設62.5Hz)與
hiEdge (預設為16000Hz)間的頻譜圖資訊，將介於loEdge與hiEdge間的頻率再分成多個頻
8 
同頻帶之NASE係數沿著時間軸之方向做離散餘弦轉換(DCT)，便取得到此一鳥類音節之
NASE調變係數，但我們只取低頻部份作為特徵值，圖四為求取NASE調變係數特徵之流
程圖。 
 
圖四 計算正規化之聲音頻譜封包調變係數之流程圖 
 
此一特徵之產生步驟如下： 
步驟 1: 取音框 (Framing) 
將每一個音節切割成一個一個的音框，大小為 512，而且為了讓每個音框的
差異性不大，我們又讓每個音框重疊一半。 
步驟 2: 乘上漢明視窗(Hamming Windowing)  
每個音框都乘上一個漢明視窗，漢明視窗式子如下。 
10  ),
1
2cos(46.054.0][ −≤≤−−= NnN
nnw π  
步驟 3: 快速傅立葉轉換(FFT)  
將音訊訊號從時域轉換成頻率域 
    0   ,][~][
1
0
2
NkenskX
N
n
n
N
kj
l <≤=∑−
=
− π
 
N：音框大小, ][~ ns ：離散訊號, l：音框索引值 
Bird Syllable 
Framing 
Normalized Audio Spectrum 
Envelope (NASE) 
DCT along the time axis for 
each NASE coefficient 
NASE Modulation Coefficients 
Hamming Windowing 
FFT 
Audio Spectrum 
Envelope(ASE) 
10 
步驟 6: 離散餘弦轉換(Discrete Cosine Transform) 
令 ),( flM 為對所有 ),( flNASE 沿著時間軸之方向做離散餘弦轉換所得到之
調變頻譜係數，式子如下： 
Ff Ll πil/Li,fNASE
L
l,fM
L
i
<≤<≤= ∑−
=
0,0,)2cos()(1)(
1
0
 
另外，在選取 ),( flM 參數當作特徵時，我們只取時間軸的前五個索引值，也
就是 NASE 調變頻譜係數區塊大小為 8×5；此外，令 lR′為對 lR 作離散餘弦轉
換，亦只取前五個係數作為特徵，所以取 9×5 共 45 個特徵作為 NASE 之調變
頻譜特徵。 
 
2.2.2 二維梅爾倒頻譜係數 
二維倒頻譜係數(Two-dimensional cepstrum, TDC)已被用於語音辨識上[19-21]，主要
原因是二維倒頻譜係數能夠表現出倒頻譜係數隨著時間的變化，對於描述相鄰音框特徵
的關聯性是一個不錯的方法，另外也能表現一個音節裡聲音頻譜圖裡之靜態和動態特
性，也就是說可以表現出一個音節整體的頻率變化和細微的頻率變化；另外，二維倒頻
譜係數還能夠同時解決音節長度不同的問題，因為在二維倒頻譜係數中真正有意義的是
分佈於低頻的係數，所以真正對語音辨識有幫助的是分佈在低頻的係數，而分佈在高頻
的係數在語音辨識上是比較沒有意義的。因此我們擬採用二維梅爾倒頻譜係數來表示每
一個隨時間改變其特性之鳥類鳴叫聲音，不只提供了梅爾倒頻譜係數的特性，也描述了
梅爾倒頻譜係數隨著時間改變的特性。其做法是對各個音框之每一頻帶的對數能量頻譜
值(logrithmic spectra)做二維離散餘弦轉換，由於二維離散餘弦轉換具有可分離特性
(separability)，因此我們可以先對一音節內之每一音框計算其梅爾倒頻譜係數為此音框
其特徵向量，再將這些梅爾倒頻譜係數依時間排成一矩陣之方式，針對同參數的梅爾倒
頻譜係數做離散餘弦轉換，即可得到二維梅爾倒頻譜係數矩陣，其示意圖如圖五所示。 
12 
化，利用梅爾三角帶通濾波器將聲音訊號分成一個個頻帶，並算出每個頻帶的
能量: 
   JjAkE
K
k
kjj <≤=∑−
=
0  ,)(
1
0
φ , 
J 為三角帶通濾波器之個數, Ak為 X[k]的振幅: 
   2/0  ,|][| 2 NkkXAk <≤= , 
而φj為第 j 個濾波器: 
   
⎪⎩
⎪⎨
⎧
≤≤−−
≤≤−−
≥≤
=
j
h
j
c
j
c
j
h
j
h
j
c
j
l
j
l
j
c
j
l
j
h
j
l
j
IkIIIkI
IkIIIIk
IkIk
k
),/()(
),/()(
or  ,0
][φ  
在這裡， jlI , jcI 和 jhI 分別代表第 j 個濾波器之低頻索引值，中間頻率索引值，
和高頻索引值: 
   
)( Nf
fI
s
j
lj
l = , )( Nf
fI
s
j
cj
c = , )( Nf
fI
s
j
hj
h = , 
fs為取樣頻率， jlf , jcf , jhf 為第 j 個濾波器的低頻、中頻和高頻值，而每個濾
波器的低頻、中頻和高頻值。 
步驟 6. 二維離散餘弦轉換(Two-Dimensional Discrete Cosine Transform) 
 我們利用二維離散餘弦轉換具有可分離特性，先對一音節內之每一音框計算其
梅爾倒頻譜係數： 
   ,)(log))5.0(cos(
1
0
10∑−
=
+=
J
j
j
i
m EjJ
mC π  10 −≤≤ Lm  
其中 imC 代表第 i 個音框之第 m 個梅爾倒頻譜係數，L 代表的是梅爾倒頻譜係數
的個數。我們共用了 25 個三角濾波器，所以 J=25，而梅爾倒頻譜係數的長度
為 15(L=15)。再將所有音框之梅爾倒頻譜係數依時間排成一矩陣之方式，針對
同參數的梅爾倒頻譜係數再做一次離散餘弦轉換，即可得到二維梅爾倒頻譜係
數矩陣(TDMFCCs)： 
   ,))5.0(cos(
1
0
∑−
=
+=
M
k
k
m
k
m CkM
mTDMFCC π  10 −≤≤ Lm  
其中 M 是一音節內之音框個數。因為在二維倒頻譜係數中真正對聲音辨識有幫
助的是分佈在低頻的係數，因此我們只取前前幾個較低頻之二維梅爾倒頻譜係
14 
    NkenwnxkX
N
n
n
N
kj
qq <≤=∑−
=
−
0  ][][][
1
0
2 ，π  
其中 N 為音框大小，令 xq[n]表示第 q 個音框之第 n 個訊號值，Xq[k]為第 q 個
音框之第 k 個傅立葉係數，w[n]為漢明視窗(Hamming window)之第 n 個係數
值： 
Nn
N
nnw <≤−−= 0  ),1
2cos(46.054.0][ π  
步驟 4: 三角帶通濾波器(Triangular band-pass filter)  
利用三角帶通濾波器將聲音訊號分成一個個頻帶，並算出每個頻帶的能量: 
∑−
=
=
1
0
)(
K
k
kjj AkE φ ， Jj ≤≤0 。 
步驟 5: 計算迴歸係數 
令 Ei(j)表示第 i 個音框之第 j 個三角帶通濾波器輸出值，將所有音框之三角帶
通濾波器之輸出值依時間順序排列，計算其迴歸係數 ai(j): 
∑
∑
−=
=
−+ −
=
0
0
0
2
1
))()((
)( n
nn
n
n
nini
i
n
jEjEn
ja , Jj ≤≤0 。 
步驟 6: 離散餘弦轉換(Discrete Cosine Transform) 
對這些迴歸係數乘上不同的餘弦值，求出動態梅爾倒頻譜係數 )(mCi′ : 
,))((log))5.0(cos()(
1
0
10∑−
=
+=′
J
j
ii jajJ
mmC π  10 −≤≤ Lm      
步驟 7: 對同索引值係數沿時間軸做離散餘弦轉換 
令 )(mCC q′ 為對所有 )(mCi′ 沿著時間軸做離散餘弦轉換得到的動態二維梅爾倒
頻譜係數，式子如下: 
∑−
=
′−=′
2
1
)/)cos(2(
2
1)(
M
i
iq MiqmCM
mCC π ,  
其中 q 表時間軸， 21 −≤≤ Mq ，M 為音節音框總數。另外，在選取 )(mCi′ 參
數當作特徵時，本計劃只要取時間軸的前五個索引值，也就是動態二維梅爾
倒頻譜係數區塊大小為 15×5。 
 
16 
重新排序 
步驟 5：設定臨界值 T(表示所要保留的資訊量程度)，以計算轉換後維度 d 
∑∑
==
×≥
D
i
i
d
i
i T
11
λλ  
其中λi表示第 i 大之 eigenvalue，D 為轉換前之維度 
步驟 6：以所保留之 d 個 eigenvector 對所有資料作線性轉換 
'
ixEy
T=   
其中 E 為此 d 個較大 eigenvector 構成之轉換矩陣。 
 
2.4 代表向量生成 
由於鳥類鳴叫聲音相當豐富多變化，因此就算有兩個音節是從同一種鳥類聲音中所
切割出來的，所擷取出來的特徵向量也可能會有明顯的不同，所以對於每一種鳥類聲
音，我們將使用高斯混合模型(Gaussian mixture model, GMM)來描述，也就是說屬於同
一種鳥類聲音之不同音節可以分成幾個小群(高斯分佈)，而屬於同一小群之不同音節其
特徵向量會較相似。 
傳統上，使用高斯混合模型於分類辨識，對不同類別之資料需分別建立其高斯混
合模型，一般傳統上高斯混合模型之參數預測是使用 EM (Expectationn Maximization)演
算法[24]。此演算法主要是用來預測高斯混合模型中多變數機率分佈函數之參數值，其
目的是要找到最佳之參數Θ使得 )|( ΘXp 最大，其中 X = {x(t), t = 1, 2, …, N}為訓練資
料之特徵向量集合，N 為訓練資料之數目； } ..., ,2 ,1|,),({ Mrp rrr =≡Θ Σμθ ， )( rp θ 為
在高斯混合模型中第 r 個高斯分佈之事前機率(Prior Probability)， rμ 為平均值向量， rΣ
為共變異數矩陣(Covariance matrix)，M 為高斯混合模型中高斯分佈之群數。EM 演算法
之詳細步驟如下： 
步驟 1: 執行 K-Means 演算法 
首先，依據高斯混合模型中所指定之高斯分佈群數執行 K-Means 演算法分群，
以每群之平均值向量作為每個高斯分佈之平均值向量之初始值，且將共變異數
矩陣之初始值設為單位矩陣。 
步驟 2: Expectation-Step 
對所有資料計算其屬於高斯混合模型中每一高斯分佈之機率比值作為預測
值，其公式如下： 
18 
))()(()( ASAASAtrAJ B
T1
W
T
F
−=  
其中， WS 和 BS 分別代表的是同類別之散佈矩陣(within-class scatter matrix)和不同類別之
散佈矩陣(between-class scatter matrix)，而同類別之散佈矩陣的公式如下: 
∑∑
= =
−−=
C
j
n
i
T
j
j
ij
j
iW
j
S
1 1
))(( μxμx  
而 jix 代表在類別 j 中的第 i 個特徵向量， jμ 為第 j 類的平均向量(mean vector)，C 為類
別的數目， jn 為類別 j 裡的特徵向量個數。而不同類別之散佈矩陣公式如下: 
   ∑
=
−−=
C
j
T
jjjB nS
1
))(( μμ μμ  
μ為所有類別的平均向量。線性區別分析演算法的目的是要去求出能夠使不同類別之散
佈矩陣和同類別之散佈矩陣的比值為最大值轉換矩陣(transformation matrix) optA ，而其維
度大小為 dn× : 
.
)(
)(
maxarg
ASAtr
ASAtrA
W
T
B
T
opt
A
=
 
此一轉換矩陣，可經由求出 B1W SS − 的特徵向量(eigenvectors)來得到，而 optA 之 d 個行向量
為前 d 個最大特徵值(eigenvalue)值所對應之特徵向量。在此我們是取(鳥鳴聲種類數目
-1)作為 d 值。 
在我們決定出最佳的轉換矩陣 optA 後，我們以 optA 將每一正規化(normalized)後之 n
維的特徵向量轉換為 d 維之向量。令 jf 為類別 j 裡維度為 n 的特徵向量，轉換成維度為
d 的向量之公式如下: 
j
T
optj A fx =  
 
2.6 辨識階段 
在輸入一個測試音節之後，我們先計算其 MPEG-7 之聲音頻譜封包之調變係數
(MNASE)、二維梅爾倒頻譜係數(TDMFCC)和動態二維梅爾倒頻譜係數(DTDMFCC)等
特徵向量，然後對各個特徵值作正規化，接著以主軸分析演算法來降低降低特徵向量維
度，最後利用線性區別分析演算法再進一步降低特徵向量維度且提高不同類別間特徵向
量之距離，辨識時以歐基里德距離(Euclidean distance)來計算測試特徵向量和每一鳥類鳴
叫聲所建立之高斯混合模型中高斯分佈的向量平均值之間的距離，取最小距離作為代表
20 
PCA LDA Recognition
Ax
Bx
xv
TT
B
T
A ] ,[ xxx =v
 
圖七 先結合特徵向量再進行辨識之流程 
 
PCA
Recognition
PCA
Ax
Bx
PCAA  ,x
CA , PBx
TT
APB
T
APA ] ,[ C ,C , xxx =v
xv LDA
 
圖八 先將個別特徵向量維度以 PCA 降低後再結合進行辨識 
 
PCA LDA
Recognition
PCA LDA
Ax
Bx
LDAA  ,x
LDAB  ,x
xv
TT
LDAB
T
LDAA ] ,[  , , xxx =v
 
圖九 先將個別特徵向量維度以 LDA 降低後再結合進行辨識 
 
3. 實驗結果與討論 
在實驗中所使用之鳥類鳴聲資料，總共有 28 種台灣鳥類，其訓練資料之鳥類錄音
與測試資料之鳥類錄音皆為在不同環境下使用不同錄音設備之錄音。首先，將所有鳥類
22 
0.90 其辨識正確率為 76.01%，使用結合方法三(Fusion-3)之最佳辨識正確率中為取 PCA
門檻值 0.93, 0.95 或 0.96 其辨識正確率為 81.11%；最後使用結合方法四(Fusion-4)之最佳
辨識正確率中為取 PCA 門檻值 0.95 其辨識正確率為 83.90%，綜合以上實驗數據來看以
結合三種特徵向量並使用結合方法四(Fusion-4)對於鳥類鳴聲之辨識有較佳之結果。 
 
表二 28 種鳥類訓練音節與測試音節數目 
Bird Name Training Syllable Test Syllable 
大冠鷲 10 4 
小卷尾 229 37 
小啄木 17 25 
小翼鶇 296 29 
小彎嘴畫眉 120 22 
火冠戴菊鳥 194 57 
白耳畫眉 98 14 
白喉笑鶇 100 37 
白腹秧雞 172 15 
灰鷽 70 8 
竹鳥 31 31 
岩鷚 122 53 
青背山雀 140 14 
冠羽畫眉 49 12 
紅頭山雀 61 24 
栗背林鴝 230 18 
烏頭翁 131 30 
深山竹雞 123 27 
深山鶯 51 8 
筒鳥 284 45 
黃山雀 222 27 
黃腹琉璃 76 12 
煤山雀 149 34 
鳳頭蒼鷹 32 16 
頭烏線 32 18 
鵂鶹 61 14 
藍腹鷴 23 10 
藪鳥 20 5 
 
 
 
24 
 
二. 參考文獻 
[1] J. Kogan and D. Margoliash, “Automated recognition of bird song elements from 
continuous recordings using dynamic time warping and hidden Markov models: a 
comparative study”, Journal of the Acoustical Society of America, Vol. 103, No. 4, pp. 
2187-2196, Apr. 1998. 
[2] A. L. McIlraith and H. C. Card, “Birdsong recognition with DSP and neural networks”, 
in Proceedings of IEEE Conference on Communications, Power, and Computing, Vol. 2, 
pp. 409-414, May 1995. 
[3] A. L. McIlraith and H. C. Card, “A comparison of backpropagation and dtatistical 
classifiers for bird identification”, in Proceedings of IEEE International Conference on 
Neural Networks , Vol. 1, pp. 100-104, June 1997. 
[4] A. L. McIlraith and H. C. Card, “Birdsong recognition using backpropagation and 
multivariate statistics”, IEEE Trans. on Signal Processing, Vol. 45, No. 11, pp. 
2740-2748, Nov. 1997. 
[5] A. L. McIlraith and H. C. Card, “Bird song identification using artificial neural 
networks and statistical analysis”, in Proceedings of Canadian Conference on Electrical 
and Computer Engineering, Vol. 1, pp. 63-66, May 1997. 
[6] 張勇富, “以語料分析為主的鳥音辨識系統研究”, 國立東華大學碩士論文, 中華民
國九十二年七月. 
[7] S. E. Anderson, A. S. Dave, and D. Margoliash, “Template-based automatic recognition 
of birdsong syllables from continuous recordings”, Journal of the Acoustical Society of 
America, Vol. 100, No. 2, pp.1209-1219, Aug. 1996. 
[8] A. Harma, “Automatic identification of bird species based on sinusoidal modeling of 
syllables”, in Proceedings of IEEE International Conference on Acoustics, Speech, and 
26 
EI). 
[17] Edgar E. Vallejo, Martin L. Cody, and Charles E. Taylor, “Unsupervised Acoustic 
Classification of Bird Species Using Hierarchical Self-organizing Maps”, 
Springer-Verlag Berlin Heidelberg 2007, ACAL 2007, LNAI 4828, pp. 212–221, 
2007. 
[18] H. G. Kim, N. Moreau, and T. Sikora, “Audio classification based on MPEG-7 spectral 
basis representation”, IEEE Trans. On Circuits and Systems for Video Technology, vol. 
14, no. 5, pp. 716-725, May 2004 
[19] Y. Ariki, S. Mizuta, M. Nagata, and T. Sakai “Spoken-word recognition using dynamic 
features snalysed by two-dimensional cepstrum”, IEE Proceedings on Communications, 
Speech and Vision, Vol. 136, No. 2, pp. 133-140, April, 1989. 
[20] H. F. Pai and H. C. Wang, “A study of the two-dimensional cepstrum approach for 
speech recognition”, Computer Speech and Language, Vol. 6, pp. 361-375, 1992. 
[21] C. T. Lin, H. W. Nein, and J. Y. Hwu, “GA-based noisy speech recognition using 
two-dimensional cepstrum”, IEEE Trans. Speech and Audio Processing, Vol. 8, No. 6, 
pp. 664-675, November, 2000. 
[22] S. Furui, “Speaker-independent isolated word recognition using dynamic features of 
speech spectrum”, IEEE Trans. Speech and Audio Processing, Vol. ASSP-34, No. 1, pp. 
52-59, February, 1986. 
[23] R. Duda, P. Hart, and D. Stork, Pattern Classification. New York:Wiley, 2000. 
[24] D. A. Reynolds and R. C. Rose, “Robust text-independent speaker identification using 
Gaussian mixture speaker models,” IEEE Trans. Speech Audio Processing, vol. 3, no. 1, 
pp. 72–83, Jan. 1995. 
 
 
 
