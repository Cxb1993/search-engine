 2
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價值（簡
要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適合在學術期
刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
▓ 達成目標 
說明： 
延續之前的相關研究，本研究案完成了一項以機率模型建構學生學習模型的研究，並且發表一系列之相關學術會
議論文和包含一篇 IJAIED 之兩篇長篇期刊論文。在認知與教學方面，藉由此研究案的資助，我們陸續發表於國
際間卓俱聲望的學術研討會，例如 ACL 和 COLING。 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：▓已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ▓無 
技轉：□已技轉 □洽談中 ▓無 
其他：（以 100 字為限） 
   主要論文發表如下。 
1. C.-L. Liu. Selecting Bayesian-network models based on simulated expectation, Behaviormetrika, 36(1), 1‒25. 
2009.  
2. C.-L. Liu. A simulation-based experience in learning structures of Bayesian networks to represent how students 
learn composite concepts, International Journal of Artificial Intelligence in Education, 18(3), 237‒285. 2008.  
3. C.-L. Liu, M.-H. Lai, Y.-H. Chuang, and C.-Y. Lee. Visually and phonologically similar characters in incorrect 
simplified Chinese words, Proceedings of the Twenty Third International Conference on Computational 
Linguistics (COLING'10), posters, 739‒747. 2010.  
4. C.-L. Liu, K.-W. Tien, M.-H. Lai, Y.-H. Chuang, and S.-H. Wu. Capturing errors in written Chinese words, 
Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics (ACL'09), 
short papers, 25‒28. 2009.  
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價值（簡
要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以 500 字為限）
本研究於 2007 年底提案之時，即已著手進行以機率式模型建構學生學習模型之相關研究，而且也正著手進行
關於漢字錯字分析的基礎工作。經過兩年的努力之後，我們完成了以機率式模型建構學生學習模型的探索，瞭解了
其中的困難之處，並且將所得的經驗發表於一系列之國際學術會議以及兩篇國際學術期刊論文之中。 
語言教學方面，我們從 2007 年的初步探索，歷經 2008 年和 2009 年多位同學的持續努力，我們完成了以倉頡
詳馬來捕捉形體相近漢字的相關研究，並且完成了數個實際的應用，過程中的相關成果均發表於 ACL 年會之中。
至 2010 年，我們將相關研究經驗擴大到簡體漢字，相關結果也發表於 COLING 年會。 
在認知科技方面，我們的成果仍屬初階，目前只獲得少量的眼動資料，雖然已經啟動相關研究，但是仍然沒有
建立精確的眼動軌跡與漢字閱讀歷程的關係。這一部份的成果現在只發表在 2010 年 TAAI 年會的國際議程中。我們
希望能夠在這一方面繼續努力。 
除了發表論文之外，這一研究計畫對於社會最實際的貢獻在於搭配 2010 到 2011 年的後續計畫中，我們實際地
建立了一個可以輔助孩童學習漢字的軟體，並且實際在台北市某國小實地測試，而且證明該軟體對於學習成效的明
顯助益。 
由於計畫主持人於 2010 年中出國短期訪問直到 2011 年一月初才返國，基於須要準備不少工作，並且有一些計
畫工作的延遲，所以計畫工作遲到 2010 年下半年方才結束，導致報帳與相關結案作業均相當延遲。 
 4
about how the texts for reading comprehensions were chosen by analyzing the linguistic structures of the texts, and the results were 
reported in DC2 and DC5.  
DC4 reported how we may infer the emotion carried by short Chinese sentences. 
DC3, DC6, and DC8 were related to techniques of machine translation (MT). DC3, in particular, was related how we may apply 
MT techniques for translating English test items into their Chinese counterparts.  
DC9 was a piece of work that related to a previous NSC research project of ours. It reported an information retrieval system for 
Chinese indictment documents. 
DC10 provided our experience in using machine learning techniques in game design. It was one of our first attempt to build a 
game, and was educational for us to build the game for learning Chinese characters. 
中文關鍵詞 
貝氏網路、機率式建模、學生學習歷程、中文錯字、改錯字試題、句子重組、心理語言學、中文字學習遊戲 
英文關鍵詞 
Bayesian Networks, Probabilistic Modeling, Learning Style Modeling, Incorrect Chinese Characters, Word Correction Tests, 
Scrambled Sentence Tests, Psycholinguistics, Games for Learning Chinese Characters 
報告內容：包括前言、研究目的、文獻探討、研究方法、結果與討論（含結論與建議）…等。 
由於本研究案發表許多論文，如果要瞭解個別研究工作的參考文獻、研究方法與結果等細節，煩請參考下面表列之各論
文。個別計畫的工作內容與關連性已經在前面的摘要中概述。 
計畫執行期間之內所發表之論文 
期刊論文 
J1. Chao-Lin Liu, Jen-Hsiang Lin, and Yu-Chun Wang. Applications of NLP techniques to computer-assisted authoring of test items 
for elementary Chinese, US-China Education Review, 7(3), 42ԟ52. David Publishing Company, USA, March 2010.  
J2. Chao-Lin Liu. Selecting Bayesian-network models based on simulated expectation, Behaviormetrika, 36(1), 1ԟ25. The 
Behaviormetric Society of Japan, Japan, April 2009.  
J3. Chao-Lin Liu. A simulation-based experience in learning structures of Bayesian networks to represent how students learn 
composite concepts, International Journal of Artificial Intelligence in Education, 18(3), 237ԟ285. IOS Press, The Netherlands, 
September 2008.  
國際學術會議論文 
IC1. Shih-Hung Wu, Yong-Zhi Chen, Ping-Che Yang, Tsun Ku, and Chao-Lin Liu. Reducing the false alarm rate of Chinese 
character error detection and correction, Proceedings of the First CIPS-SIGHAN Joint Conference on Chinese Language 
Processing (CLP'10), 54ԟ61. Beijing, China, 28-29 August 2010.  
IC2. Chao-Lin Liu, Min-Hua Lai, Yi-Hsuan Chuang, and Chia-Ying Lee. Visually and phonologically similar characters in incorrect 
simplified Chinese words, Proceedings of the Twenty Third International Conference on Computational Linguistics 
(COLING'10), posters, 739ԟ747. Beijing, China, 23-27 August 2010.  
IC3. Chao-Lin Liu, Chih-Bin Huang, Ying-Tse Sun, and Wei-Ti Kuo. Computer assisted creation of items for scrambled sentence 
tests, Proceedings of the Seventeenth International Conference on Computers in Education (ICCE'09), 117ԟ121. Hong Kong, 
China, 30 November-4 December 2009.  
IC4. Chao-Lin Liu, Kan-Wen Tien, Min-Hua Lai, Yi-Hsuan Chuang, and Shih-Hung Wu. Phonological and logographic influences 
on errors in written Chinese words, Proceedings of the Seventh Workshop on Asian Language Resources (ALR7), the Forty 
Seventh Annual Meeting of the Association for Computational Linguistics (ACL'09), 84ԟ91. Singapore, 2-7 August 2009.  
IC5. Chao-Lin Liu, Kan-Wen Tien, Min-Hua Lai, Yi-Hsuan Chuang, and Shih-Hung Wu. Capturing errors in written Chinese words, 
Proceedings of the Forty Seventh Annual Meeting of the Association for Computational Linguistics (ACL'09), short papers, 
25ԟ28. Singapore, 2-7 August 2009.  
IC6. Chao-Lin Liu, Kan-Wen Tien, Yi-Hsuan Chuang, Chih-Bin Huang, and Juei-Yu Weng. Two applications of lexical information 
to computer-assisted item authoring for elementary Chinese, Lecture Notes in Computer Science 5579: Proceedings of the 
Twenty Second International Conference on Industrial Engineering & Other Applications of Applied Intelligent Systems 
(IEA/AIE '09), 470ԟ480. Tainan, Taiwan, 24-27 June 2009.  
國內學術會議論文 
DC1. Chia-Ling Lee, Yu-Chi Chang, Chia-Ying Lee, and Chao-Lin Liu. 結合認知理論之電腦輔助漢字教學遊戲, Proceedings of 
the 2010 Taiwan Academic Network Conference (TANET'10), CD-ROM. Tainan, Taiwan, 27-29 October, 2010. (in Chinese)  
 6
在期中報告中我們已經附上當時已經發表的論文，以下我們再附上
兩篇更新的論文。分別是前面論文表列中的 IC2 和 IC3。
Chinese characters for computing visually simi-
lar characters. Evidence observed in psycholin-
guistic studies offers a cognition-based support 
for the design of Liu et al.’s approach (Yeh and 
Li, 2002). In addition, the proposed method 
proves to be effective in capturing incorrect tra-
ditional Chinese words (Liu et al., 2009a-c). 
In this paper, we work on the errors in simpli-
fied Chinese words by extending the Cangjie 
codes for simplified Chinese. We obtain two lists 
of incorrect words that were reported on the In-
ternet, analyze the major reasons that contribute 
to the observed errors, and evaluate how the new 
Cangjie codes help us spot the incorrect charac-
ters. Results of our analysis show that phonolog-
ical and visual similarities contribute similar por-
tions of errors in simplified and traditional Chi-
nese. Experimental results also show that, we can 
catch more than 90% of the reported errors. 
We go over some issues about phonological 
similarity in Section 2, elaborate how we extend 
and apply Cangjie codes for simplified Chinese 
in Section 3, present details about our experi-
ments and observations in Section 4, and discuss 
some technical issues in Section 5.  
2 Phonologically Similar Characters 
The pronunciation of a Chinese character in-
volves a sound, which consists of the nucleus and 
an optional onset, and a tone. In Mandarin Chi-
nese, there are four tones. (Some researchers in-
clude the fifth tone.) 
In our work, we consider four categories of 
phonological similarity between two characters: 
same sound and same tone (SS), same sound and 
different tone (SD), similar sound and same tone 
(MS), and similar sound and different tone (MD).  
We rely on the information provided in a lex-
icon  (Dict, 2010) to determine whether two cha-
racters have the same sound or the same tone. 
The judgment of whether two characters have 
similar sound should consider the language expe-
rience of an individual. One who live in the 
southern and one who live in the northern China 
may have quite different perceptions of “similar” 
sound. In this work, we resort to the confusion 
sets observed in a psycholinguistic study con-
ducted at the Academic Sinica. 
Some Chinese characters are heteronyms. Let 
C1 and C2 be two characters that have multiple 
pronunciations. If C1 and C2 share one of their 
pronunciations, we consider that C1 and C2 be-
long to the SS category. This principle applies 
when we consider phonological similarity in oth-
er categories. 
One challenge in defining similarity between 
characters is that the pronunciations of a charac-
ter can depend on its context. The most common 
example of tone sandhi in Chinese (Chen, 2000) 
is that the first third-tone character in words 
formed by two adjacent third-tone characters will 
be pronounced in the second tone. At present, we 
ignore the influences of context when determin-
ing whether two characters are phonologically 
similar.  
Although we have confined our definition of 
phonological similarity to the context of the 
Mandarin Chinese, it is important to note the in-
fluence of sublanguages within the Chinese lan-
guage family will affect the perception of phono-
logical similarity. Sublanguages used in different 
areas in China, e.g., Shanghai, Min, and Canton 
share the same written forms with the Mandarin 
Chinese, but have quite different though related 
pronunciation systems. Hence, people living in 
different areas in China may perceive phonologi-
cal similarity in very different ways. The study in 
this direction is beyond the scope of the current 
study.  
3 Visually Similar Characters 
Figure 1 shows four groups of visually similar 
characters. Characters in group 1 and group 2 
differ subtly at the stroke level. Characters in 
group 3 share the components on their right sides. 
The shared component of the characters in group 
4 appears at different places within the characters. 
Radicals are used in Chinese dictionaries to 
organize characters, so are useful for finding vi-
sually similar characters. The characters in group 
1 and group 2 belong to the radicals “田” and “ ”, 
respectively. Notice that, although the radical for 
group 2 is clear, the radical for group 1 is not 
obvious because “田” is not a standalone compo-
nent.  
However, the shared components might not be 
the radicals of characters. The shared compo-
nents in groups 3 and 4 are not the radicals. In 
Figure 1. Examples of visually similar characters
decompose individual Chinese characters. The 
Chinese Document Lab at the Academia Sinica 
proposed a system with 13 operators for describ-
ing the relationships among components in Chi-
nese characters (CDL, 2010). Lee (2010b) pro-
pose more than 30 possible layouts.  
The layout of a character affects how people 
perceive visual similarity between characters. 
For instance, c16 in Table 1 is more similar to c17 
than to c18, although they share “ ”. We rely on 
the expertise in Cangjie codes reported in (Lee, 
2010a) to split the codes into parts. 
Table 2 shows the extended codes for some 
characters listed in Table 1. The “ID” column 
provides links between the characters listed in 
both Table 1 and Table 2. The “CC” column 
shows the Chinese characters. The “LID” column 
shows the identifications for the layouts of the 
characters. The columns with headings “P1”, 
“P2”, and “P3” show the extended Cangjie codes, 
where “Pi” shows the ith part of the Cangjie 
codes, as indicated in Figure 2. 
We decide the extended codes for the parts 
with the help of computer programs and subjec-
tive judgments. Starting from the original Cang-
jie codes, we can compute the most frequent sub-
strings just like we can compute the frequencies 
of n-grams in corpora (cf. Jurafsky and Martin, 
2009). Computing the most common substrings 
in the original codes is not a complex task be-
cause the longest original Cangjie codes contain 
just five symbols.   
Often, the frequent substrings are simplified 
codes for popular components in Chinese charac-
ters, e.g., “ ” and “ ”. The original codes for “ ” 
and “ ” are “戈弓女” and “弓人一”, but they are 
often simplified to “戈女” and “弓一”, respec-
tively.  When simplified, “ ” have the same 
Cangjie code with “戉”, and “ ” have the same 
Cangjie code with “马” and “鱼”. 
After finding the frequent substrings, we veri-
fy whether these frequent substrings are simpli-
fied codes for meaningful components. For mea-
ningful components, we replace the simplified 
codes with complete codes. For instance the 
Cangjie codes for “许” and “讦” are extended to 
include “弓” in Table 2, where we indicate the 
extended keys that did not belong to the original 
Cangjie codes in boldface and with a surrounding 
box. Most of the non-meaningful frequent sub-
strings have two keys: one is the last key of a 
part, and the other is the first key of another part. 
They were by observed by coincidence. 
Although most of the examples provided in 
Table 2 indicate that we expand only the first 
part of the Cangjie codes, it is absolutely possible 
that the other parts, i.e., P2 and P3, may need to 
be extended too. c19 shows such an example. 
Replacing simplified codes with complete 
codes not only help us avoid incorrect matches 
but also help us find matches that would be 
missed due to simplification of Cangjie codes. 
Using just the original Cangjie codes in Table 1, 
it is not easy to determine that c18 (“经”) in Table 
1 shares a component (“ ”) with c16 and c17 (“劲” 
and “颈”). In contrast, there is a chance to find 
the similarity with the extended Cangjie codes in 
Table 2, given that all of the three Cangjie codes 
include “弓人一”.  
We can see an application of the LIDs, using 
“劲”, “颈” and “经” as an example. Consider the 
case that we want to determine which of “颈” 
and “经” is more similar to “劲”. Their extended 
Cangjie codes will indicate that “颈” is the an-
swer to this question for two reasons. First, “劲” 
and “颈” belong to the same type of layout; and, 
second, the shared components reside at the same 
area in “劲” and “颈”.  
3.3 Similarity Measures 
The main differences between the original and 
the extended Cangjie codes are the degrees of 
details about the structures of the Chinese cha-
racters. By recovering the details that were ig-
nored in the original codes, our programs will be 
ID CC LID P1 P2 P3 
5 许 2 戈弓女 人十  
6 讦 2 戈弓女 一十  
7 计 2 戈弓女 十  
10 购 10 月人 心 戈 
11 沟 10 水 心 戈 
12 构 10 木 心 戈 
13 员 5 口 月人  
14 圆 9 田 口 月人 
15 勋 2 口月人 大尸  
16 劲 2 弓人一 大尸  
17 颈 2 弓人一 一月人  
18 经 3 女女一 弓人 一 
19 恸 4 心 一一戈 大尸 
Table 2. Examples of extended Cangjie codes 
different characters. 
We searched the Internet for reported errors 
that were collected in real-world scenarios, and 
obtained two lists of errors. The first list3 came 
from the entrance examinations for senior high 
schools in China, and the second list4 contained 
errors observed at senior high schools in China. 
We used 160 and 524 errors from the first and 
the second lists, respectively, and we refer to the 
combined list as the Ilist. An item of reported 
error contained two parts: the correct word and 
the mistaken character, both of which will be 
used in our experiments. 
4.2 Preliminary Data Analysis 
Since our programs can compare the similarity 
only between characters that are included in our 
lexicon, we have to exclude some reported errors 
from the Ilist. As a result, we used only 621 er-
rors in this section.  
Two native speakers subjectively classified the 
causes of these errors into three categories based 
on whether the errors were related to phonologi-
cal similarity, visual similarity, or neither. Since 
the annotators did not always agree on their clas-
sifications, the final results have five interesting 
categories: “P”, “V”, “N”, “D”, and “B” in Table 
3. P and V indicate that the annotators agreed on 
the types of errors to be related to phonological 
and visual similarity, respectively. N indicates 
that the annotators believed that the errors were 
not due to phonological or visual similarity. D 
indicates that the annotators believed that the 
errors were due to phonological or visual similar-
ity, but they did not have a consensus. B indi-
cates the intersection of P and V.  
Table 3 shows the percentages of errors in 
these categories. To get 100% from the table, we 
can add up P, V, N, and D, and subtract B from 
the total. In reality there are errors of type N, and 
Liu and his colleagues (2009b) reported this type 
of errors. Errors in this category happened to be 
missing in the Ilist. Based on our and Liu’s ob-
                                                 
3www.0668edu.com/soft/4/12/95/2008/2008091357140.htm
 ; last visited on 22 April 2010. 
4 gaozhong.kt5u.com/soft/2/38018.html; last visited on 22 
April 2010. 
servations, the percentages of phonological and 
visual similarities contribute to the errors in sim-
plified and traditional Chinese words with simi-
lar percentages.  
4.3 Experimental Procedure 
We design and employ the ICCEval procedure 
for the evaluation task. 
At step 1, given the correct word and the cor-
rect character to be intentionally replaced with 
incorrect characters, we created a list of charac-
ters based on the selection criterion. We may 
choose to evaluate phonologically or visually 
similar characters. For a given character, ICCEv-
al can generate characters that are in the SS, SD, 
MS, and MD categories for phonologically simi-
lar characters (cf. Section 2). For visually similar 
characters, ICCEval can select characters based 
on SC1, SC2, and SC3 (cf. Section 3.3). In addi-
tion, ICCEval can generate a list of characters 
that belong to the same radical and have the same 
number of strokes with the correct character. In 
the experimental results, we refer to this type of 
similar characters as RS. 
At step 2, for a correct word that people origi-
nally wanted to write, we replaced the correct 
character with an incorrect character with the 
characters that were generated at step 1, submit-
ted the incorrect word to Google AJAX Search 
 P V N D B 
Ilist 83.1 48.3 0 3.7 35.1
Table 3. Percentages of types of errors
Procedure ICCEval
Input:  
ccr: the correct character; cwd: 
the correct word; crit: the selec-
tion criterion; num: number of re-
quested characters; rnk: the cri-
terion to rank the incorrect 
words; 
Output: a list of ranked candidates 
for ccr 
Steps: 
1. Generate a list, L, of charac-
ters for ccr with the specified 
criterion, crit. When using SC1, 
SC2, or SC3 to select visually 
similar characters, at most num 
characters will be selected. 
2. For each c in L, replace ccr in 
cwd with c, submit the resulting 
incorrect word to Google, and 
record the ENOP. 
3. Rank the list of incorrect words 
generated at step 2, using the 
criterion specified by rnk. 
4. Return the ranked list. 
erated with different crit at step 1, contained the 
incorrect character in the reported errors. In the 
Ilist, there were 516 and 3006  errors that were 
related to phonological and visual similarity, re-
spectively. Using the characters generated with 
the SS criterion, we captured 426 out of 516 
phone-related errors, so we showed 426/516 = 
82.6% in the table. 
Results in Table 4 show that we captured 
phone-related errors more effectively than visual-
ly-similar errors. With a simple method, we can 
compute the union of the characters that were 
generated with the SS, SD, MS, and MD criteria. 
This integrated list suggested how well we cap-
tured the errors that were related to phones, and 
we show its effectiveness under “Phone”. Simi-
larly, we integrated the lists generated by SC1, 
SC2, SC3, and RS to explore the effectiveness of 
finding errors that are related to visual similarity, 
and the result is shown under “Visual”. 
4.5 Experimental Results: Ranking Tests 
To put the generated characters into work, we 
wish to put the actual incorrect character high in 
the ranked list. This will help the efficiency in 
supporting computer assisted test-item writing. 
Having short lists that contain relatively more 
confusing characters may facilitate the data prep-
aration for psycholinguistic studies. 
At step 3, we ranked the candidate characters 
by forming incorrect words with other characters 
in the correct words as the context and submitted 
the words to Google for ENOPs. The results of 
ranking, shown in Table 5, indicate that we may 
just offer the leading five candidates to cover the 
actual incorrect characters in almost all cases.  
The “Total” column shows the total number of 
errors that were captured by the selection crite-
rion. The column “Ri” shows the percentage of 
all errors, due to phonological or visual similarity, 
that were re-created and ranked ith at step 3 in 
ICCEVAL. The row headings show the selection 
criteria that were used in the experiments. For 
instance, using SS as the criterion, 70.3% of ac-
tual phone-related errors were rank first, 7.4% of 
the phone-related errors were ranked second, etc. 
If we recommended only 5 leading incorrect cha-
                                                 
6The sum of 516 and 300 is larger than 598 because 
some of the characters are similar both phonologically 
and visually. 
racters only with SS, we would have captured the 
actual incorrect characters that were phone re-
lated 81.6% (the sum of R1 to R5) of the time. 
For errors that were related to visual similarity, 
recommending the top five candidates with SC3 
would capture the actual incorrect characters 
87.1% of the time. Since we do not show the 
complete distributions, the sums over the rows 
are not 100%. In the current experiments, the 
worst rank was 21. 
We also used PMI to rank the incorrect words. 
Due to page limits, we cannot show complete 
details about the results. The observed distribu-
tions in ranks were not very different from those 
shown in Table 5. 
5 Discussion 
Compared with Liu et al.’s analysis (2009b-c) 
for the traditional Chinese, the proportions of 
errors related to phonological factors are almost 
the same, both at about 80%. The proportion of 
errors related to visual factors varied, but the av-
erages in both studies were about 48%. A larger 
scale of study is needed for how traditional and 
simplified characters affect the distributions of 
errors. Results shown in Table 4 suggest that it is 
relatively easy to capture errors related to visual 
factors in simplified Chinese. Although we can-
not elaborate, we note that Cangjie codes are not 
good for comparing characters that have few 
strokes, e.g., c1 to c4 in Table 1. In these cases, 
the coding method for Wubihua input method 
(Wubihua, 2010) should be applied. 
Acknowledgement 
This research was supported in part by the research 
contract NSC-97-2221-E-004-007-MY2 from the Na-
tional Science Council of Taiwan. We thank the ano-
nymous reviewers for constructive comments. Al-
though we are not able to respond to all the comments 
 Total R1 R2 R3 R4 R5
SS 426 70.3 7.4 2.9 0.4 0.6
SD 151 25.6 2.7 0.6 0.0 0.4
MS 9 1.4 0.4 0.0 0.0 0.0
MD 8 1.6 0.0 0.0 0.0 0.0
SC1 235 61.3 10.3 4.3 2.0 0.3
SC2 213 53.7 11.0 3.7 2.3 0.3
SC3 263 66.7 12.7 5.7 1.7 0.3
RS 4 1.3 0.0 0.0 0.0 0.0
Table 5. Ranking the candidates 
KONG, S.C., et al. (Eds.), ICCE2009; ©2009 Asia-Pacific Society for Computers in Education. 
 
Computer Assisted Creation of Items for 
Scrambled Sentence Tests 
Chao-Lin Liu Chih-Bin Huang Ying-Tse Sun Wei-Ti Kuo 
Department of Computer Science, National Chengchi University, Taiwan 
{chaolin,96753014,93703038,94703041}@nccu.edu.tw 
 
Abstract. We apply techniques of natural language processing to support the creation of special 
scrambled sentences that allow only specific word orders. The scrambled sentences are useful for 
students to practice their knowledge about grammars. It takes two steps to create a test item for 
scrambled-sentence tests. We create a set of grammatical alternative sentences from the target sen-
tence, and make sure that students will rebuild the target sentence by pegging some of the words in 
the target sentence. The proposed methods can automatically and effectively peg words to single out 
a specific sentence from a set of sentences. We also employ the Stanford parser and propose a practi-
cal heuristic principle to help teachers exclude a potentially large number of alternative grammatical 
orderings of a set of words in the scrambled sentence.  
Keywords: scrambled sentence tests, computer assisted item generation, grammar learning, 
natural language processing 
1 Introduction 
Techniques that were originally designed for natural language processing (NLP) have proved to be instru-
mental for applications for computer assisted language learning (CALL) [1, 3, 7]. In this paper, we report an 
application of NLP techniques for grammar learning. 
Word orders are important in conveying the correct meaning in almost all languages, so learning the cor-
rect word orders are crucial even for beginning learners of languages (cf. some examples at 
http://www.manythings.org/ss/  and http://www.msrossbec.com/scrambleintro.shtml). Placing students in 
actual conversations is common for students to practice knowledge of word orders. However, some students 
might not be ready for such challenging field tests, so a less stressful environment is necessary for those who 
are not completely ready for direct conversations. 
Constructing sentences from scrambled sentences offers an alternative for students of intermediate com-
petence. In these practices, sentences are segmented and scrambled to create a set of words or phrases, and 
students have to reconstruct the original sentences with the given text segments. Taking advantage of the 
information contained in parse trees, Liu et al. [5] segmented the sentences into different numbers of pieces 
to make the test items adaptive to students’ competence levels. 
A sentence can be segmented at coarse or fine levels, and a student may find two or more grammatical 
orders of the resulting segments. When a teacher prefers to avoiding multiple answers to a test item, just 
segmenting the sentences based on parse trees become insufficient, and more techniques are in need.  
The safest way to make sure that there is only way to build a sentence from a set of words is to rule out all 
but the target sentence by pegging some words. Given a set of sentences, it is not very difficult to find such 
words to be anchored. Our experience show that the task of generating all grammatical permutations of a set 
of words turns out to be more challenging that it appears [1, 4].  
In Section 2, we present more background information about the work on scrambling sentences for 
grammar learners. In Section 3, we propose methods to select and peg some words for a set of sentences to 
achieve a unique ordering. In Section 4, we attempt to employ the Stanford parser 
(http://nlp.stanford.edu:8080/parser/) to find all grammatical arrangements of a set of words and phrases. 
With such a capability, we will reduce the burden of teachers. In Section 5, we report the results when we 
repeated the work reported in the previous section with a categorical parser. In Section 6, we propose a prac-
tical heuristic to solve the discussed problems.  
KONG, S.C., et al. (Eds.), ICCE2009; ©2009 Asia-Pacific Society for Computers in Education. 
48 arrangements. The algorithm returned the indexes for “the”, “new”, and “bike”. In fact, this is not the 
only choice, and pegging words in {“new”, “bike”, “that”} will also make the original sentence the only an-
swer among the 48 arrangements. If the original sentence was “Is this new bike better than that old car”, 
Word-Pegging will recommend {“is”, “this”, “new”, “bike”}. If we modify and allow Word-Pegging to find 
alternative ways of pegging the words, {“this”, “bike”, “better”, “old”} can be such an alternative. 
A conceivable method to offer flexibility to teachers is that we modify Word-Pegging to compute all pos-
sible ways of pegging the words and provide these choices to the teacher. For instance, allowing the teachers 
to choose whether we should peg words in {“this”, “new”, “bike”}, {“new”, “bike”, “that”}, or {“new”, 
“that”, “car”}. This will allow the teachers to make the resulting test item fit specific teaching needs. How-
ever, there can be many such choices. 
A more practical alternative to make Word-Pegging more flexible is to allow the teachers to specify a set 
of words that they want to peg. However, we do not include the algorithm due to page limits.  
4 Seeking Admissible Arrangements 
Recommending the tokens to peg to achieve unique answers resolves the first problem discussed in Section 2. 
The algorithms we presented and discussed in Section 3 will be 
sufficient when it is possible for the teachers to provide an ex-
haustive list of grammatical arrangements like those listed in 
Table 1. However it is not always easy for a human teacher to 
enumerate such grammatical permutations and provide the set 
of S as input to our algorithms. It will be more convenient for 
the teachers if our system can help them enumerate all possible 
arrangements of the words of the sentence provided by the 
teacher.  
An intuitive approach for generating the grammatical enu-
merations for a given sentence that contains m tokens is to com-
pute all m! permutations and check their grammaticality. Doing 
so requires a categorical parser, which we do not have at this 
moment. Instead we employ the Stanford parser to parse the 
permutations.  
A more challenging barrier is that parsing all of the m! per-
mutations is feasible only for relatively small values of m be-
cause of computational costs. It took about 5.5 hours to parse 
the 9! (=362880) permutations of the sentence in Figure 1, 
when we ran the Stanford parser (version 2008-10-26) on a ma-
chine with 3.0G INTEL Core 2 Duo E8400 CPU and 2.0 G 
RAM running Windows XP SP3.  
In fact, we did not receive categorical decisions for the gram-
maticality of the permutations from the Stanford parser. It 
would accept all of the permutations and provided the probabili-
ties of the permutations being grammatically acceptable.  
In general, these estimated probabilities are not good for 
comparing the acceptability of sentences of different lengths, 
but they serve as a good indication for sentences of the same 
length. In our case, we have an additional advantage that we are 
comparing the acceptability of the permutations of the same set 
of tokens.   
Hence, we submitted the permutations to the Stanford parser, 
collected the probabilities of these permutations, and ranked 
them accordingly. Table 1 shows the ranks of the 48 legal ar-
rangements in the 8! (=40320) permutations. We created the 
40320 permutations by treating “more than” as a token in the 
sentence in Figure 1. The permutation “That is better than this 
old new car bike” received the highest probability in all permu-
tations, where a noun is functioning as an adjective. Neverthe-
less, we did not treat it as an acceptable candidate in Table 1.  
Given the ranking information, our job is to choose the  
permutations that have the leading probabilities among all of 
these 40320 permutations as the input to the Word-Pegging 
algorithm. If we choose the top 100 arrangements, and run the 
Word-Pegging algorithm, the algorithm will recommend us to 
Table 1. Alternative sentences with ranks
48 Arrangements (rank) 
this new bike is better than that old car (16)
that new bike is better than this old car (8) 
this old bike is better than that new car (14) 
this new car is better than that old bike (15) 
that old bike is better than this new car (6) 
this old car is better than that new bike (13) 
that new car is better than this old bike (7) 
that old car is better than this new bike (5) 
is this new bike better than that old car (1181)
is that new bike better than this old car (1177)
is this old bike better than that new car (1179)
is this new car better than that old bike (1180)
is that old bike better than this new car (1175)
is this old car better than that new bike (1178)
is that new car better than this old bike (1176)
is that old car better than this new bike (1174)
this bike is better than that new old car (38) 
this bike is better than that old new car (37) 
that bike is better than this old new car (19) 
that bike is better than this new old car (20) 
this car is better than that new old bike (36) 
this car is better than that old new bike (35) 
that car is better than this new old bike (18) 
that car is better than this old new bike (17) 
this new old bike is better than that car (94) 
this old new bike is better than that car (92) 
that old new bike is better than this car (59) 
that new old bike is better than this car (60) 
this new old car is better than that bike (93) 
this old new car is better than that bike (91) 
that new old car is better than this bike (58) 
that old new car is better than this bike (57) 
is this bike better than that new old car (839) 
is this bike better than that old new car (838) 
is that bike better than this old new car (833) 
is that bike better than this new old car (834) 
is this car better than that new old bike (836) 
is this car better than that old new bike (828) 
is that car better than this new old bike (831) 
is that car better than this old new bike (825) 
is this new old bike better than that car (840) 
is this old new bike better than that car (837) 
is that old new bike better than this car (832) 
is that new old bike better than this car (835) 
is this new old car better than that bike (830) 
is this old new car better than that bike (829) 
is that new old car better than this bike (827) 
is that old new car better than this bike (826) 
KONG, S.C., et al. (Eds.), ICCE2009; ©2009 Asia-Pacific Society for Computers in Education. 
Table 3. Using the chart parsers to filter sentences 
C300 C450 C600 C985 ID SP I N I N I N I N 
1.1 25 1 1 1 1 
1.2 12 1 32 1 78 1 96 1 240
2.1 4 1 1 1 1 
2.2 9 0 0 0 0 
2.3 10 1 1 1 1 
2.4 6 1
120
1
120 
1 
120 
1 
120
6.1 1 0 0 0 0 
6.2 19 0
0 
0
0 
0 
0 
0 
0 
7.1 3 1 1 1 1 
7.2 9 0
192
0
192 
0 
240 
0 
240
10  0 0 12 4050 12 4050   
Except the very last row, the “I” column shows 
whether the sentence with a particular ID was ac-
cepted by the chart parser. We use “1” and “0” to 
indicate acceptance and rejection, respectively. The 
IDs are exactly the IDs shown in Table 2, except the 
last row. The “N” column shows the number of sen-
tences (out of the k! permutations) that were 
accepted by the chart parser. The “SP” column 
shows the ranks of the sentences when we submitted 
the k! permutations to the Stanford parser. 
The very last row shows how the 48 sentences in 
Table 1 were accepted by the chart parsers. When we 
used only 300 rules, no sentences in Table 1 were 
accepted because of missing rules. When we used more than 300 rules, the parsers accepted only 12 sen-
tences from Table 1, and accepted 4050 permutations (from 40320 permutations) as grammatical sentences. 
The statistics in the “SP” column confirm that it is not easy to find the best  candidate sentences to cap-
ture the correct sentences. However, the chart parsers that used different numbers of rules did not always 
find the correct sentences either. The incorrect sentences could be accepted, and the correct sentences could 
be rejected. Increasing the number of rules allowed the parsers to accept more sentences, but may not help 
the acceptance of correct sentences. 
To make the categorical parsers really useful, we may need to improve the grammar rules. This could be 
achieved by finding a better source of the rules – either by domain experts or by machine learning techniques. 
6 A Heuristic: POS-based Pegging 
If we do assume that only learners of the beginning levels will be interested in the practices of sentence re-
construction, we have a good reason to assume that the correct sentences are probably not very long and not 
very complicated. With these assumptions, we can do part-of-speech (POS) tagging of the correct sentence 
first, and peg words that have the same POS tags. Details about effectiveness and problems of using this 
heuristic will be discussed in an extended paper. 
7 Summary 
We have gone through an investigation of the techniques that are useful for the creation of test items for re-
constructing scrambled sentences. It is not difficult to determine how to peg tokens in a set of candidate 
permutations to force unique solutions. It is not as easy to help the teachers to foresee all of the possible an-
swers to the scrambled sentence tests. Generating the set of grammatical sentences efficiently has been a 
challenging task. We investigated the applications of the Stanford parser and a primitive categorical parser, 
and proposed a useful heuristic. Although the results are not conclusive, the current experimental results 
show encouraging improvements against the state of the art. 
Acknowledgments 
We were supported in part by the grant NSC-97-2221-E-004-007 from the National Science Council of Taiwan. 
References  
[1] Becker, T., Joshi, A. K., & Rambow, O. (1991). Long-distance scrambling and tree adjoining gram-
mars, Proc. of the 5th Conf. on European Chapter of the ACL, 21–26. 
[2] Burstein, J. & Leacock, C. Editors. (2005). Proc. of the 2nd Workshop on Building Educational Ap-
plications Using Natural Language Processing, The 43rd Annual Meeting of the ACL. 
[3] Heift, T. & Schulze., M. (2007). Errors and Intelligence in Computer-Assisted Language Learning. 
NY, USA: Routledge. 
[4] Joshi, A. K., Becker, T., & Rambow, O. C. (2002). Complexity of scrambling: A new twist to the 
competence-performance distinction. In Abeille, A. and Rambow, O. C., editors, Tree-Adjoining 
Grammars. Stanford: CSLI. 
[5] Liu, C.-L. Lin, J.-H., & Huang, C.-B. (2008). A platform for authoring test items for elementary Chi-
nese with techniques of natural language processing, presented in the 2008 CAERDA Int’l Conf.  
[6] Manning, C. D. & Schultz, H. (1999). Foundations of Statistical Natural Language Processing. The 
MIT Press. 
[7] Tetreault, J., Burstein, J., De Felice, R. Editors. (2008). Proceedings of the Third Workshop on Inno-
vative Use of NLP for Building Educational Applications, The 46th Annual Meeting of the ACL 
 8
ICCE 的論文有多種層面，有探討學生的學習模型者，有探討不同學科的教育軟體者，參與者有
教育學家，有資訊軟硬體學家；展現了跨領域研究的特性。比起計畫主持人於 2008 年在美國所參加
的 AERA 和 NCME 兩個年會性質有所不同。AERA 大多是教學學者來參與，可以聽到許多的教學經
驗，NCME 則是測驗學家的會議。 
截至目前為止，ICCE 或許只能算是還在復興階段，但是透過與會者的認真，個人相信這一個研
討會將逐漸建立聲譽，成為電腦輔助教學領域中的一個重要會議。 
在這一個會議之中遇到兩個來自政治大學教育學院結伴而來的女研究生，相對於我自己在(後
來)2010 年無法找到學生到北京報告 COLING 論文，實在是覺得慚愧。 
三、考察參觀活動(無是項活動者略) 
無 
四、建議 
有一些領域其實不要在意論文的技術深度，而是要問那一些技術的應用意義。這樣才能夠掌握
真實的應用。 
五、攜回資料名稱及內容 
六、其他 
 10
國科會補助專題研究計畫項下赴大陸地區出差報告 
                                    日期：100 年 4 月 2 日 
一、 大陸研究過程 
這一次到中國北京的語言大學交流，是一些巧合與善意的組合。計畫主持人在投稿 TALIP 論文
的過程中，論文評審指出我們所進行的文獻回顧有重要的遺漏，漏掉了北京語言大學和上海交通大
學的一些相關研究。由於計畫主持人在 2010 年下半年在美國密西根大學短期訪問，如果要透過網路
去購買相關的文獻，必須先在中國擁有相關的帳號，因此不得其門而入。基於限時之內必須回應
TALIP 論文評審的修改要求，因此冒昧直接聯絡語言大學的資深院長宋柔教授索取論文，不意宋教
授不只善意回應，並且於極短時間之內轉寄論文。雖然計畫主持人的 TALIP 論文最後獲得接受發表，
不見得與得以閱讀和引用宋教授的相關著作絕對相關，但是這樣的協助與善意，加上研究工作的直
接相關醞釀了計畫主持人於返國之後，再前往北京與宋教授面談的契機。 
計畫主持人於一月五日晚上返回國內，結束半年的短期訪問，在收作整理之後，即申請前往北
京語言大學訪問，於獲得國科會核可之後即前往北京。由於研究計畫的限制，同時農曆春節將屆，
因此只暫留於北京語言大學一天。 
計畫主持人於廿五日下午抵達北京，於廿六日訪問北京語言大學，於廿七日中午即搭機返國。 
二、 研究成果 
北京語言大學是北京市幾所大學中很特殊的大學，接受去中國學習中文的學生是主要的任務，
去語言大學學習的學生不見得只是學習中文，還有可能是去北京語大學習特殊專業的，例如學習資
訊科技的研究生。目前在北京語大學習的外國學生來自超過一百個國家，學生總人數接近一萬人。 
北京語大的資訊學院規模不大，主要的任務是從事與語文學習相關的資訊科技，總共不及十位
專業是資訊科技的教授，儘管如此，國內也少有類似規模專研語文教學相關科技的團隊。團隊之中，
有專研語音科技、有專研機器翻譯科技、有專研書寫輔助科技的教授、甚至有專研中文斷詞科技的
教授。不小的團隊把中文學習科技切分成數個非常小的個別領域。 
計畫主持人此行主要是分享所發表的 TALIP 論文，除聽取相關的評論之外，並且與宋柔教授先
前所做關於簡體漢字的類似研究相互交流。在交換意見之後，發現還有不少可以繼續深入探討的研
究議題。 
三、 建議 
在中文學習成為極大市場過程中，台灣的產學界是否考慮以合作代替競爭與中國的產學界建立
藍海關係？ 
四、其他 
計畫編號 NSC-97-2221-E-004-007-MY2 
計畫名稱 機率式建模技術與自然語言的標記、認知和教學 
出國人員
姓名 劉昭麟 
服務機構
及職稱 國立政治大學資訊科學系教授 
出國時間 2011 年 01 月 25 日至 2011 年 01 月 27 日 出國地點 中國北京市語言大學 
 4
ICCE 的論文有多種層面，有探討學生的學習模型者，有探討不同學科的教育軟體者，參與者有
教育學家，有資訊軟硬體學家；展現了跨領域研究的特性。比起計畫主持人於 2008 年在美國所參加
的 AERA 和 NCME 兩個年會性質有所不同。AERA 大多是教學學者來參與，可以聽到許多的教學經
驗，NCME 則是測驗學家的會議。 
截至目前為止，ICCE 或許只能算是還在復興階段，但是透過與會者的認真，個人相信這一個研
討會將逐漸建立聲譽，成為電腦輔助教學領域中的一個重要會議。 
在這一個會議之中遇到兩個來自政治大學教育學院結伴而來的女研究生，相對於我自己在(後
來)2010 年無法找到學生到北京報告 COLING 論文，實在是覺得慚愧。 
三、考察參觀活動(無是項活動者略) 
無 
四、建議 
有一些領域其實不要在意論文的技術深度，而是要問那一些技術的應用意義。這樣才能夠掌握
真實的應用。 
五、攜回資料名稱及內容 
六、其他 
 6
國科會補助專題研究計畫項下赴大陸地區出差報告 
                                    日期：100 年 4 月 2 日 
一、 大陸研究過程 
這一次到中國北京的語言大學交流，是一些巧合與善意的組合。計畫主持人在投稿 TALIP 論文
的過程中，論文評審指出我們所進行的文獻回顧有重要的遺漏，漏掉了北京語言大學和上海交通大
學的一些相關研究。由於計畫主持人在 2010 年下半年在美國密西根大學短期訪問，如果要透過網路
去購買相關的文獻，必須先在中國擁有相關的帳號，因此不得其門而入。基於限時之內必須回應
TALIP 論文評審的修改要求，因此冒昧直接聯絡語言大學的資深院長宋柔教授索取論文，不意宋教
授不只善意回應，並且於極短時間之內轉寄論文。雖然計畫主持人的 TALIP 論文最後獲得接受發表，
不見得與得以閱讀和引用宋教授的相關著作絕對相關，但是這樣的協助與善意，加上研究工作的直
接相關醞釀了計畫主持人於返國之後，再前往北京與宋教授面談的契機。 
計畫主持人於一月五日晚上返回國內，結束半年的短期訪問，在收作整理之後，即申請前往北
京語言大學訪問，於獲得國科會核可之後即前往北京。由於研究計畫的限制，同時農曆春節將屆，
因此只暫留於北京語言大學一天。 
計畫主持人於廿五日下午抵達北京，於廿六日訪問北京語言大學，於廿七日中午即搭機返國。 
二、 研究成果 
北京語言大學是北京市幾所大學中很特殊的大學，接受去中國學習中文的學生是主要的任務，
去語言大學學習的學生不見得只是學習中文，還有可能是去北京語大學習特殊專業的，例如學習資
訊科技的研究生。目前在北京語大學習的外國學生來自超過一百個國家，學生總人數接近一萬人。 
北京語大的資訊學院規模不大，主要的任務是從事與語文學習相關的資訊科技，總共不及十位
專業是資訊科技的教授，儘管如此，國內也少有類似規模專研語文教學相關科技的團隊。團隊之中，
有專研語音科技、有專研機器翻譯科技、有專研書寫輔助科技的教授、甚至有專研中文斷詞科技的
教授。不小的團隊把中文學習科技切分成數個非常小的個別領域。 
計畫主持人此行主要是分享所發表的 TALIP 論文，除聽取相關的評論之外，並且與宋柔教授先
前所做關於簡體漢字的類似研究相互交流。在交換意見之後，發現還有不少可以繼續深入探討的研
究議題。 
三、 建議 
在中文學習成為極大市場過程中，台灣的產學界是否考慮以合作代替競爭與中國的產學界建立
藍海關係？ 
四、其他 
計畫編號 NSC-97-2221-E-004-007-MY2 
計畫名稱 機率式建模技術與自然語言的標記、認知和教學 
出國人員
姓名 劉昭麟 
服務機構
及職稱 國立政治大學資訊科學系教授 
出國時間 2011 年 01 月 25 日至 2011 年 01 月 27 日 出國地點 中國北京市語言大學 
97年度專題研究計畫研究成果彙整表 
計畫主持人：劉昭麟 計畫編號：97-2221-E-004-007-MY2 
計畫名稱：機率式建模技術與自然語言的標記、認知和教學 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 10 10 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 8 8 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 1 100% 
人次 
 
期刊論文 3 3 100%  
研究報告/技術報告 0 0 100%  
研討會論文 6 6 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
