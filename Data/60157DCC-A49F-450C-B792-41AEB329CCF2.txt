序列樣式探勘(Sequential Pattern Mining)
等。本研究計畫主要針對分群技術做深入
的研究。分群是指自動地將資料區隔為幾
個特性接近的資料群集，主要的功能是找
出群集與群集之間的差異，同時也可以將
同一個群集中成員們的相似性找出來。例
如:一群住在附近的人，他們購買相同的汽
車與家電，而另一群從事相同行業的人，
他們的家庭成員人數與年收入接近，透過
觀察這些資料是為何被群集在一起的，可
以更了解資料之間的關係，以及這些關係
將如何影響預測的結果。 
由於無線網路環境快速發展，使無線
與有線網路整合已達到一定的水準，並且
可以提供無接縫的(seamlessly)連線服務，
許多企業與個人透過無線網路來傳遞資
訊。除此之外，最近幾年來行動、手持式
設備之計算能力與記憶體增加，導致無所
不在的運算(Ubiquitous Computing)典範出
現。行動使用者可以透過手機或 PDA 在任
何時間、任何地點 (anytime、anywhere)收
發電子郵件、處理文件或召開行動會議
等。許多應用所產生的資料通常分散在不
同地點且具有時效性。資料若不立即進行
分析，則其過了一段時間後就會失去應有
的價值。例如:股票分析、入侵偵測與車輛
碰撞預防等。如何在任何時間、任何地點
透過行動設備探勘出有趣的樣式，並且即
時回應給使用者將面臨重大的挑戰，因此
產生無所不在的資料探勘議題[13,14]。 
近年來，隨著網際網路、通訊科技與
感測設備(Sensor Device)的快速進步，資料
成長的速度以每天數以萬計甚至沒有上限
的速度大量增加，並且以串流(Stream)的形
式傳遞。這些連續的串流資料具有大量且
潛在無限、即時性、快速改變和不可預測
的特性[2,11]。無所不在的資料探勘比傳統
的資料探勘有較多資源限制，例如:有限的
計算能力與記憶體大小。因此，無所不在
的資料探勘並無法有效處理資料串流，當
串流快速抵達時會導致探勘失敗。因此，
無所不在的資料串流探勘(Ubiquitous Data 
Stream Mining)是當前資料探勘研究的新
議題之ㄧ。 
過去關於無所不在的資料串流分群探
勘的研究，主要是利用 AOG (Algorithm 
Output Granularity)方法[8]做為分群探勘的
基礎，其藉由合併分群結果來降低輸出細
粒度(granularity)，使演算法能夠適應可獲
取的資源。雖然 AOG 方法能夠在資源限制
的環境中持續地進行探勘，卻忽略探勘結
果的精確度，當群集很少時會導致精確度
下降。因此針對上述問題本研究計畫提出
一個名為 RA-HCluster 的方法，其建置在
行動設備中漸進式探勘串流資料，並且根
據目前設備可獲取的資源來調整演算法的
參數與壓縮串流中的資料，使行動設備即
使在較低的記憶體資源下仍然可以持續地
進行分群探勘且有一定程度的精確度。 
 
二、 文獻探討 
 本節將探討相關文獻，我們會分別介
紹資料串流取樣、資料串流分群探勘、資
源感知探勘技術以及無所不在的資料串流
分群探勘，並對提到的演算法進行描述與
分析。 
 
2.1 資料串流取樣 
由 於 資 料 串 流 具 有 一 次 通 過
(one-pass)、連續不斷(continuity)及無窮盡
(infinity)的特性，我們需要對串流資料進行
取樣(sample)，而不是針對全部的串流資料
進行處理。如果使用錯誤的取樣模式，會
使不符合需求的資料如同雜訊(noise)干擾
著分析的結果，還會導致探勘效率降低。
目前已有許多資料串流取樣模式被提出，
 2
VFML (Very Fast Machine Learning)演算
法，其每次執行時會計算出一個樣本數量
與誤差值，並將此兩個數值給予學習器當
作損失函數(loss function)。接著，利用損
失函數檢驗演算法每次執行時使用的資料
量所產生的誤差值是否在允許的範圍內，
其目的在於使VFML演算法能夠使用更少
的資料來達到同樣的探勘結果，且誤差值
不超過上限。此外，他們更運用此方法發
展以K-means為基礎的VFKM (Very Fast 
K-Means)和決策樹分類的VFDT (Very Fast 
Decision Trees)技術，這兩個技術已經被應
用並且使用在人造與真實網路的串流資料
上。VFKM的主要概念是使用霍夫丁門檻
值(Hoeffding Bound)來限定K-means演算法
每次執行的資料量，使VFKM所使用的樣
本資料達到最少。此外，VFKM保證比使
用全部串流資料分群探勘出來的結果沒有
顯著的差異，而且執行效率優於傳統的
K-means演算法。Guha et al. [12]則提出
LSEARCH 演算法，其設計為一次性
(single-pass)方法與利用K-median技術來分
群串流資料，並且透過可利用的記憶空間
將樣本資料點分為2個群集，接著重覆先前
的步驟直到將2個群集分成k個群集。實驗
結果證明LSEARCH演算法只需使用少量
的記憶體空間，並且與使用固定資料量的
K-median演算法相比有較好的執行效率。
針對分群大量演變的串流資料的問題，
Aggarwal et al. [1]提出CluStream分群架
構，其解決一般資料串流分群演算法當串
流資料隨著時間快速演變，導致群集品質
變差的問題。此架構分成二個部分，線上
部分(on-line)儲存關於資料串流的概要統
計量，而離線部份(off-line)根據使用者的需
求(例如：時間點、群集數量)與線上部分所
儲存的概要統計量當作輸入，並且使用金
字塔時間框架(Pyramidal Time Frame)結合
微分群(Micro-clustering)的方法來分群。實
驗結果證明CluStream具有較高效率與精確
度。 
 
2.3 資源感知探勘技術 
由於資料串流的特性，大部分的一般
探勘演算法只能一次或少於一次處理串流
資料，因此一些輸入端的適應技術紛紛被
提 出 ， 例 如 : 抽 樣 (Sampling) 、 聚 合
(Aggregation)、減少負荷(Load Shedding)以
及概要資料結構(Synopsis Data Structure)
等[2]。這些適應技術的目的在於減少所要
處理的資料量，以部分、聚合的資料來代
表串流整體資料。但上述的方法有一些問
題: (1)抽樣與聚合方法無法處理隨著時間
演變的串流資料 (2)減少負荷方法會喪失
一些重要資訊 (3)概要的資料結構通常無
法代表所有串流資料的特徵。資源感知
(Resource-Aware)探勘技術最早是由 Gaber 
et al. [8]提出的 AOG (Algorithm Output 
Granularity)方法，其不同於前面提到的輸
入端的適應技術，AOG 主要是在輸出端做
處理。圖 2 為 AOG 處理程序。AOG 處理
程序主要分為三個階段，分別為探勘、適
應及知識整合階段。探勘階段主要是執行
探勘演算法，當其執行完一次後則會透過
適應階段來調整門檻值，適應資料速率與
可獲取的記憶體。接著會判斷記憶體是否
耗盡，如果是則透過知識整合階段來合併
探勘的結果，否則回到探勘階段繼續執
行。另一方面，串流中的資料特性有可能
會隨著時間而改變，此現象被稱之為概念
漂移(Concept Drifting)。針對資料串流所產
生的概念漂移問題， Zhu et al. [21]提出
Granularity Adaptive Estimation 分群架構，
其使用核心密度估計(Kernel Density Esti-
mation)處理串流中的雜訊值，根據串流中
資料的密度來分群，且提出負荷去除(Load 
 4
持續地進行。雖然這些方法能夠在資源限
制的環境中進行探勘，卻降低探勘的精確
度。因此，本研究計畫提出RA-HCluster 
(Resource-Aware High Quality Clustering)方
法來解決此問題。RA-HCluster主要分為線
上維護與離線分群兩個階段。在線上維護
階段中，主要是維護並儲存串流中資料點
的統計資訊，使離線分群階段能夠利用這
些資訊來做探勘，而不是探勘串流中的每
一筆資料，得以減少計算的時間與複雜
度。首先，利用移動視窗(Sliding-Window)
模式對串流資料取樣，取樣的資料是以視
窗大小為單位。接著，針對移動視窗中的
資料計算統計值，形成所謂的微群集
(Micro-cluster)，並且漸進式更新統計值。
除此之外，我們在微群集合併的處理中加
上相關係數的計算，以改善微群集合併所
導致的精確度下降的問題。最後，使用階
層式摘要框架(Hierarchical Summary Frame)
來儲存每個微群集的群集特徵向量，同時
根據可獲取的資源來調整摘要框架的層
級。當資源不足時，可透過聚合目前的摘
要框架到較高的層級來減少需要處理的資
料量，進而達到縮減資源消耗的目的。在
離線分群階段中，主要是根據目前記憶體
來調整線上維護階段的設定，並且利用階
層式摘要框架所儲存的統計資訊來進行分
群。首先，我們利用資源監控模組(Resource 
Monitor Module)來計算記憶體使用量與記
憶體剩餘率，並且判斷記憶體是否不足。
當記憶體不足時，則會透過演算法輸入細
粒度(Algorithm Input Granularity)來調整移
動視窗的大小與階層式摘要框架的層級。
最後，利用我們所提出的資源感知以門檻
值 為 基 礎 的 K-means 演 算 法
(Resource-Aware Threshold-based K-means 
Algorithm, RAT K-means)來進行分群探
勘。當記憶體不足時，則減少距離門檻值
(distance threshold)，使分群演算法減少所
需要處理的資料量。相反地，當資源充足
時，則增加距離門檻值，以提升分群結果
的精確度。圖 3 為RA-HCluster方法的整體
流程。 
 
圖 3 RA- HCluster 的整體流程 
 
3.1 線上維護 
3.1.1 資料取樣 
資料串流取樣採用移動視窗模式，圖 4
顯示移動視窗取樣範例，其中 Stream 代表
一個資料串流，而 t0、t1、…、t9分別代表
一個時間點。假設使用者設定移動視窗的
大小為 3，代表一次處理串流中的 3 筆資
料，因此移動視窗第一次取出 3 筆資料，
其分別為 A、B、C，並且時間點為 t1、t2、
t3。在處理完視窗內的資料點後，視窗會往
右移動一次來處理接下來的 3 筆資料。在
這個範例中，視窗總共移動了三次，因此
總共取樣了 9 筆資料，其時間點分別從 t1
到 t9。表 1 顯示圖 4 所取樣的串流資料。 
 6
表2 產生的3個微群集  L 是用來表示目前階層式摘要框架的
層級，每個層級是由多個框架(Frame)
所組成，且每一個框架儲存一個微群
集的群集特徵向量。 
 
 每個框架可以用  esji ttF , 來表示， st 代
表開始儲存時的時間戳記， et 代表儲
存結束時的時間戳記，i、j分別標示層
級與框架編號。 
 
接著，我們會設定一個最大半徑邊界
 (Maximal radius boundary)，當串流中有
新的資料點p到達時，如果p與其最近的微
群集中心點mi所計算出來的平方距離
小於),( imp2d  則將 p併入微群集Mi
中，否則將p建立為一個新的微群集。如果
目前微群集的個數大於使用者所設定的個
數時，則必須將其中兩個微群集合併。在
合併處理上，我們不僅計算每個微群集的
距離相似度，並且藉由皮爾森相關係數
 (Pearson Correlation Coefficient)來找出最
相近的兩個微群集來進行合併，以改善合
併時精確度降低的問題。 
 在階層式摘要框架L=0以上的層級都
會附加細節係數(Detail Coefficient)欄
位，其用來儲存資料之間的差值，目
的是為了後續資料解析的處理。 
],[ 0
1
0 tttF ],[ 21
2
0 tt ttF  ],[ 312
3
0 tt ttF 
],[ 20
1
1 tttF ],[ 412
2
1 tt ttF  ],[ 614
3
1 tt ttF 
],[ 413
4
0 tt ttF  ],[ 514
5
0 tt ttF  ],[ 615
6
0 tt ttF 
],[ 40
1
2 tttF
圖5 階層式摘要框架 
  
3.1.3 建立階層式摘要框架 資料聚合與資料解析的處理是利用哈
爾小波轉換(Haar Wavelet Transform)，其是
一種壓縮資料的方法，具有計算快速與容
易理解的特性，並且普遍使用在資料探勘
的領域上[4,15]。此轉換可以視為一連串計
算平均值與差值的處理。其計算式定義如
下: 
產生微群集之後，接著透過我們提出
的階層式摘要框架(Hierarchical Summary 
Frame)來儲存每個微群集的群集特徵向
量，產生階層式摘要框架的層級 L=0，表
示目前階層式摘要框架位於第 0 個層級。
離線分群階段則會利用目前階層式摘要框
架的層級所儲存的群集特徵向量當作虛擬
點來進行分群。另外，階層式摘要框架具
有資料聚合(Data Aggregation)與資料解析
(Data Resolution)兩種功能。當記憶體不足
時，則會透過資料聚合將底層詳細的資料
聚合成上層較概括的資料，以降低分群時
記憶體空間與計算時間的耗費。若記憶體
充足時，則會透過資料解析將上層的資料
轉換回底層詳細的資料。 
 在區間  中使用小波轉換來聚合框架
可以表示成   

 1i
iF
W ，其 iF 表示
框架。 
 k 個 小 波 轉 換 可 以 表 示 成
 





 





k
i
i
k
i
ii
k
W
W
1
1

 。 
圖 6 為階層式摘要框架範例，其聚合
區間  設定為 2，表示資料聚合的程序每次階層式摘要框架的表示方法如圖5所
示: 
 8
因此在情境 2 我們將視窗大小從 5 縮減為
2。 
458.0,8.0
100
20100,20,100  wRUN mmm
254.0,4.0
100
60100,60,100  wRUN mmm
圖7 調整移動視窗大小範例 
 
接著，我們將目前階層式摘要框架的
層級進行資料聚合的處理，由於調整階層
式摘要框架的層級會影響分群探勘的精確
度，所以只有當 < 20% 時才會執行此程
序。另外，當 <20%代表記憶體充
足，則會進行資料解析的處理。資料聚合
與資料解析詳細處理的步驟已在3.1.3節闡
述過。 
mR R1 m
 
3.2.2 分群  
分群探勘是使用我們提出的 RAT 
K-means 演算法 (Resource-Aware Thresh-
old-based K-means Algorithm)，如圖8所
示。演算法的輸入為群集數量k、距離門檻
值 d (distant threshold)、記憶體使用量最低
邊界 以及目前階層式摘要框架的層級
所儲存的群集特徵向量當作虛擬點  
(Pseudo-Point)。演算法的輸出為最後產生
的k個群集 。RAT K-means演算法的步驟
分為三個部份。第一部分是群集的產生(圖
8的line 4-10)。這個部分是要將每個虛擬點
歸屬到與其距離最短的群集中心，以產生k
個群集。第二部分是調整距離門檻值
mLB
ix
kC
d (圖
8的line 11-14)。這個部分是根據目前記憶
體剩餘率來調整 d ，d 越小則表示演算法
越容易將虛擬點視為離群值並丟棄，以減
少記憶體的使用量。第三部分是判斷群集
是否穩定(圖8的line 15-31)。這個部分是
對第一部分產生的群集重新計算新的群集
中心，並且利用樣本變異數(Sample Vari-
ance)與距離總偏移值(Total Deviation)來判
斷分群結果是否穩定，若是則輸出分群結
果，否則回到第一部分繼續執行。 
RAT K-means演算法的參數定義如下: 
 k 為使用者定義的群集數量。 
 d 為使用者定義的距離門檻值。 
 mLB 為使用者定義的記憶體使用量最
低邊界。 
 }1{ nixx i  為目前階層式摘要
框架的層級所儲存的虛擬點。 
 }1{ kjCC j  為演算法最後產
生的 k 個群集的集合。 
 mN 為記憶體總共的大小。 
 }1{ kjcc j  為群集中心點的集合。 
 ),( 為虛擬點 xi 與群集中心點
cj的尤拉距離。 
2
ji cxd
 }1,1),({ 2 kjnicxdD jii 
為虛擬點 xi 與每個群集中心點的尤拉
距離的集合，初始值為ψ。 
 ][ iD 為虛擬點 xi 與距離其最近
的群集中心點的尤拉距離。 
Min
 mU 為記憶體使用量。 
 mR 為記憶體剩餘率。 
 E 為距離總偏移值。 
 2S 為樣本變異數。 
 )( jC 為第 j 個群集所包含的虛擬
點的數量。 
count
 E為新的群集中心點所計算出來的距
離總偏移值。 
 2Sˆ 為新的群集中心點所計算出來的樣
本變異數。 
 
 10
刪除 所包含的虛擬點與其群集中
心；否則利用
jC
jc 重新計算樣本變異數
與距離總偏移值2Sˆ E，接著判斷 與2Sˆ
E是否皆減少，若是則利用新的群集
中心點 取代舊的群集中心點 ；否
則輸出群集 ，並且刪除 所包含的
虛擬點與其群集中心。 
jc jc
jC jC
重複執行Step 3到Step 5，直到每個群
集的群集中心點不再改變，或樣本變異數
與距離總偏移值其中之一不再減少，則結
束RAT K-means演算法的處理程序。 
d舉例說明，首先我們設定 k=2、 =10
與 =30%，並且目前階層式摘要框架的
層級所儲存的虛擬點為 A(2,4)、B(3,2)、
C(4,3)、D(5,6)、E(8,7)、F(6,5)、G(6,4)、
H(7,3)、I(7,2)。Step 1 計算記憶體大小為
40MB。Step 2 隨機選取 2 個虛擬點 B 與 F
當作群集中心點。Step 3 計算每個虛擬點與
群集中心點 B 與 F 的尤拉距離，其分別為
2.24 、 1.41 、
4.47 、 7.07 、
3.6 、 4.12 、
4 、 4.12 、
2.83 、 1.41 、
2.83 、 1 、
2.24、 3.16。接著，
找出每個虛擬點與距離其最近的群集中心
點的尤拉距離，其分別為 2.24、
1.41 、 1.41 、
2.83 、 1 、
2.24、 3.16。由於上
述距離皆小於
m
), BA
), BD
), BG
), B
), FC
), FE
, FH
), BC
), FE
, FH
LB
(2
(2
(2
(2 I
(2
(2
(2
(2
(2
(2









d
d
d
d
d
d
)d
d
d
)d
)
)


)
)F
), BA
)
)F
, Bd
, Bd
)Bd
)2 F
, Fd
,G
,I
(2
, Fd
,G
,I
(2 C
(2 E
,(2 H
,(A
(2 D
(2d
)F
d
(2 D
(2d
)F
d
(2d
(2d

d ，因此產生兩個群集
=[A,B,C]與C =[D,E,F,G,H,I]。Step 4 計算
記 憶 體 使 用 量 為 30MB ， 求 出
。由於 < ，
因此透過10
1
m
C
R
2
40 25
25
.0
1(
/)3040( mR LB
%)
m
10  ，將 d 減少為
2.5。Step 5 計算群集 與 的樣本變異數
與距離總偏移值
1 2C C
2S E ，其分別為 =2、
=3.65、 =7、 =26。接著，計算群集
與 的新的群集中心點為 =(3,3)、
2
1S
2
2S
1C
2c
1E 2E
2C 1c
 =(6.5,4.5) 。 Step 6 由 於 且1c1c
2c2c  ，因此利用 1c與 重新計算樣本變
異數 與距離總偏移值
2c
2Sˆ E ，其分別為
=1.367、21Sˆ 1E =4、 =3.2、 =14.5。Step 
7 由於群集 與 的 與 皆小於 與
，因此用新的群集中心點取代舊的群集
中心點，將 、c 設為(3,3)、(6.5,4.5)並且
回到 Step 3 計算每個虛擬點與群集中心點
與 的 尤 拉 距 離 ， 其 分 別 為
1.41、 1、 1、
3.6 、 6.4 、
3.6 、 3.16 、
4 、 4.12 、
4.5 、 4.3 、
3.53 、 2.12 、
2.92 、 0.5 、
0.5 、 1.58 、
2.5。接著，找出每個虛擬點與距
離其最近的群集中心點的尤拉距離，其分
別 為 1.41 、 1 、
1 、 2.12 、
2.92 、 0.5 、
0.5 、 1.58 、
2.5。由於 2.92 大於目
前的
2
2Sˆ
2C
2
(2 Bd
d
2

d
)


)
)
)
B

)
E
E
, 1c
), 1c
)1c
, 2c
, 2c
, 2cF
), 2c
(2d
)2c
, 2cF
), 2c
)2
1
)
C
1c
,( 1cA
2ˆ
)1
(2 Ed
(2 G
,(2 I
(2 Bd
(2 D
(2d
(2 H
,(D
(2d
(2 H
,( cE
S
c
d
d
d
2
d
2
2S
), 1c

1E
1c
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
(2d
2c
), 1c
), 1c
), 1c
), 1c
), 2c
), 2c
), 2c
), 2c
), 2c
d
)1c
), 2c
), 2c
), 2c









2



A
D
F
H
A
C
E
G
I
,C
E
G
I
,
d
d
(2 C





), 1c



d 為 2.5，因此將虛擬點 E 刪除，產生
兩個群集 =[A,B,C]與 =[D,F,G,H,I]。
Step 4 計算記憶體使用量為 10MB，求出
1C
/)10
2C
75.04040( mR 。由於 > 且mR mLB mR1 >20%，因此不調整d 。Step 5 計算
群集 與 的樣本變異數 與距離總偏
移值
1C 2C
2S
E ，其分別為 =1.367、 =3.01、
=4、 =14.5。接著，計算群集 與 的
新的群集中心點為
2
1S
1c
2
2S
1C1E 2E 2C
 =(3,3)、 =(6.2,4)。
Step 6 由於 1
2c
1c c ，則輸出C 最後的分群結
果 =[A,B,C]；而
1
1C 2c2c  ，因此利用 2c重
新計算樣本變異數 與距離總偏移值2Sˆ
 12
串流處理效率的比較，橫軸為資料處理所
經過的時間(elapsed time)以秒為單位，而縱
軸為每秒可處理的資料點數量。如圖 10 所
示，在串流資料剛到達時，RA-HCluster 須
計算群集特徵向量，並產生微群集，因此
一開始的資料處理效率並不佳，在時間過
了 20 秒 之 後 ， 微 群 集 建 立 完 成 ，
RA-HCluster 的處理速率逐漸增加且趨於
穩定；而 RA-VFKM 會計算 Hoeffding 
Bound 來限定取樣的大小，因此一開始處
理效率較好，但隨著時間過去，RA-VFKM
所處理的資料點數量漸漸減少。如圖 11 所
示，在較長的執行時間下，RA-HCluster 只
有剛開始的串流處理效率較差，但其執行
到 60秒時的處理效率與RA-VFKM相差不
大，並且執行到 80 秒之後處理效率漸漸超
過 RA-VFKM。 
 
圖10 串流處理效率比較 
 
 
圖11 在較長的執行時間下串流處理效率
比較 
 
(二) 精確度 
在精確度評估的實驗中，我們使用平
均距離變異總和 (average of the sum of 
square distance, Average SSQ)[1]來衡量分
群結果的精確度。假設在目前時刻 Tc前的
期間 中，總共有 個資料點，對於在 的
每一個資料點 找出距離其最近的群集中
心 ，並計算出 與 之間的距離
。因此目前時刻 Tc前的期間 h 所
計 算 出 來 的 平 均 距 離 變 異 總 和
等於 h 中所有 個資料
點與其群集中心之間的距離總和除以群集
個數。Average SSQ 的值越小表示精確度越
高。圖 12 與圖 13 的橫軸為串流匯入的速
率(例如: Stream 為 100 表示資料串流以每
秒 100 個資料點的速率匯入)，而縱軸為平
均距離變異總和(Average SSQ)，並且分別
顯示使用消費者推薦資料集與人工資料集
在各種資料串流速率下，探勘精確度變化
的情形。如圖 12 與圖 13 所示，除了串流
匯入的速率從 50 到 100 時 RA-HCluster 與
RA-VFKM 的 Average SSQ 幾乎相同之
外，RA-HCluster 在串流匯入的速率大於
200 以 上 ， 其 精 確 度 皆 明 顯 地 高 於
RA-VFKM。此外，由於圖 13 是使用相同
分配的人造資料，因此 Average SSQ 相差
不 會 太 大 ， 但 我 們 還 是 可 以 看 出
RA-HCluster所計算出來的Average SSQ較
少。原因在於 RA-HCluster 利用微群集之
間的相似程度作為合併的依據，並且在離
線分群階段透過樣本變異數來找出更密集
的群集，以增加分群的精確度。而
RA-VFKM 是藉由增加誤差值
h
ni
,i cn
Average
hn
)h
h
in
,Tc
c
(2
in nic
)nid
 (SSQ hn
 來減少樣
本大小與合併分群結果來達到持續探勘的
目的，因此導致分群的精確度降低。 
 
圖 12 探勘精確度比較(Consumer  
recommendations data set) 
 14
時間過了 40 秒之後，記憶體使用量逐漸減
少且趨於穩定。RA-VFKM 只有剛開始執
行時的記憶體使用量低於 RA-HCluster，在
執行 37 秒之後皆使用較多的記憶體。 
 
圖 16 各種執行時間的記憶體使用量比較
(Synthetic data set B100kC10D5) 
 
五、 結論與建議 
行動手持設備越來越普及的情況下，
如何能有效地利用其有限的資源來探勘資
料串流是很重要的議題。因此，本研究計
畫針對無所不在的資料串流環境提出
RA-HCluster 方法，其利用資源感知技術調
整演算法的參數與階層式摘要框架的層
級，使行動設備能夠持續地進行分群探
勘，並且克服傳統的資料串流分群演算法
當記憶體不足時導致探勘精確度下降或是
探勘中斷的問題。另外，我們加入計算微
群集間的相關係數的方法，使分群處理能
夠將更相關的資料點歸屬到相同的群集之
中，進而提升分群結果的精確度。實驗結
果證明 RA-HCluster 不僅精確度高於
RA-VFKM，而且能夠保持較少且穩定的記
憶體使用量。 
由於本研究計畫是在行動設備中針對
單一的資料串流進行探勘，因此未來的研
究方向可以考慮處理多重資料串流環境。
另外，在資源感知技術方面可以加入電
力、CPU 使用率及串流速率等因素，使演
算法更能有效地針對行動設備目前的環境
與串流的特性做調整。在實務應用上可以
考慮應用在車輛碰撞偵測、醫療保健及股
票分析等方面。 
 
參考文獻 
1. C.C. Aggarwal, J. Han, J. Wang, and P.S. 
Yu, “A Framework for Clustering 
Evolving Data Streams,” in Proceedings 
of the 29th International Conference on 
Very Large Data Bases, Berlin, Ger-
many, September 2003, pp. 81-92. 
2. B. Babcock, S. Babu, R. Motwani, and J. 
Widom, “Models and Issues in Data 
Stream Systems,” in Proceedings of the 
21th ACM SIGMOD Symposium on 
Principles of Database Systems, Madi-
son, Wisconsin, U.S.A., June 2002, 
pp.1-16. 
3. D. Barbara, “Requirements for Cluster-
ing Data Streams,” The ACM SIGKDD 
Explorations Newsletter, January 2002, 
pp. 23-27. 
4. B.R. Dai, J.W. Huang, M.Y. Yeh, and 
M.S. Chen, “Adaptive Clustering for 
Multiple Evolving Streams,” in Pro-
ceedings of the IEEE Transactions on 
Knowledge and Data Engineering, Vol. 
18, No. 9, Los Angeles, U.S.A., Sep-
tember 2006, pp. 1166-1180. 
5. M. Datar, A. Gionis, P. Indyk, and R. 
Motwani, “Maintaining Stream Statis-
tics over Sliding Windows,” in Pro-
ceedings of the 13th Annual ACM-SIAM 
Symposium on Discrete Algorithms, San 
Francisco, USA, January 2002, pp. 
635-664. 
6. P. Domingos and G. Hulten, “A General 
Method for Scaling Up Machine Learn-
ing Algorithms and its Application to 
Clustering,” in Proceedings of the 18th 
 16
 18
19. D.N. Phung, , M.M. Gaber, and U. 
Roehm, “Resource-aware Distributed 
Online Data Mining for Wireless Sensor 
Networks,” in Proceedings of the Inter-
national Workshop on Knowledge Dis-
covery from Ubiquitous Data Streams, 
Warsaw, Poland, September 2007. 
20. R. Shah, S. Krishnaswamy, and M. M. 
Gaber, “Resource-Aware Very Fast 
K-Means for Ubiquitous Data Stream 
Mining,” in Proceedings of 2nd Interna-
tional Workshop on Knowledge Discov-
ery in Data Streams, Porto, Portugal, 
October 2005, pp.40-50. 
21. W. Zhu, J. Pei, J. Yin, and Y. Xie, 
“Granularity Adaptive Density Estima-
tion and on Demand Clustering of Con-
cept-Drifting Data Streams,” in Pro-
ceedings of the 8th International Con-
ference on Data Warehousing and 
Knowledge Discovery, September 
2006, pp. 322-331. 
22. Y. Zhu and D. Shasha, “StatStream: 
Statistical Monitoring of Thousands of 
Data Streams in Real Time,” in Pro-
ceedings of the 28th International Con-
ference on Very Large Data Bases, 
Hong Kong, China, August 2002, pp. 
358–369. 
 
Resource-Aware High Quality Clustering in Ubiquitous Data Streams  
Ching-Ming Chao 
Department of Computer Science & Information Management, Soochow University, Taipei, Taiwan 
chao@csim.scu.edu.tw 
Guan-Lin Chao 
Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan 
guanlinchao@gmail.com 
Abstract. Data stream mining has attracted much research attention from the data mining community. 
With the advance of wireless networks and mobile devices, the concept of ubiquitous data mining has 
been proposed. However, mobile devices are resource-constrained, which makes data stream mining a 
greater challenge. In this paper, we propose the RA-HCluster algorithm that can be used in mobile 
devices for clustering stream data. It adapts algorithm settings and compresses stream data based on 
currently available resources, so that mobile devices can continue with clustering at acceptable accuracy 
even under low memory resources. Experimental results show that not only is RA-HCluster more 
accurate than RA-VFKM, it is able to maintain a low and stable memory usage. 
Key words: Data mining, Data streams, Clustering, Ubiquitous data mining, Ubiquitous data stream 
mining 
1   Introduction 
Due to rapid progress of information technology, the amount of data is growing very fast. How to identify useful 
information from these data is very important. Data mining is to discover useful knowledge from large amounts of data. 
Data generated by many applications are scattered and time-sensitive. If not analyzed immediately, these data will soon 
lose their value; e.g., stock analysis [1] and vehicle collision prevention [2]. How to discover interesting patterns via 
mobile devices anytime and anywhere and respond to the user in real time faces major challenges, resulting in the 
concept of ubiquitous data mining (UDM). 
With the advance of sensor devices, many data are transmitted in the form of streams. Data streams are large in 
amount and potentially infinite, real time, rapidly changing, and unpredictable [3] and [4]. Compared with traditional 
data mining, ubiquitous data mining is more resource-constrained, such as constrained computing power and memory 
size. Therefore, it may result in mining failures when data streams arrive rapidly. Ubiquitous data stream mining thus 
has become one of the newest research topics in data mining. 
Previous research on ubiquitous data stream clustering mainly adopts the AOG (Algorithm Output Granularity) 
approach [5], which reduces output granularity by merging clusters, so that the algorithm can adapt to available 
resources. Although the AOG approach can continue with mining under a resource-constrained environment, it 
sacrifices the accuracy of mining results. In this paper, we propose the RA-HCluster (Resource-Aware High Quality 
Clustering) algorithm that can be used in mobile devices for clustering stream data. It adapts algorithm settings and 
compresses stream data based on currently available resources, so that mobile devices can continue with clustering at 
acceptable accuracy even under low memory resources 
The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 presents the RA-HCluster 
algorithm. Section 4 shows our experimental results. Section 5 concludes this paper. 
coefficients is included in the process of merging micro-clusters to improve the problem of declining accuracy caused 
by merging micro-clusters. Finally, a hierarchical summary frame is used to store cluster feature vectors of 
micro-clusters. The level of the hierarchical summary frame can be adjusted based on the resources available. If 
resources are insufficient, the amount of data to be processed can be reduced by adjusting the hierarchical summary 
frame to a higher level, so as to reduce resource consumption. 
In the offline clustering component, algorithm settings are adapted based on currently available memory, and 
summary statistics stored in the hierarchical summary frame are used for clustering. First, the resource monitoring 
module computes the usage and remaining rate of memory and decides whether memory is sufficient. When memory is 
low, the size of the sliding window and the level of the hierarchical summary frame are adjusted using the AIG 
(Algorithm Input Granularity) approach. Finally, clustering is conducted. When memory is low, the distance threshold is 
decreased to reduce the amount of data to be processed. Conversely, when memory is sufficient, the distance threshold 
is increased to improve the accuracy of clustering results. 
3.1 Online Maintenance  
3.1.1 Data Sampling 
The sliding window model is used for data stream sampling. Figure 2 shows an example of sliding window 
sampling, in which Stream represents a data stream and t0, t1, …, t9 each represents a time point. Suppose the window 
size is set to 3, which means three data points from the stream are extracted each time. Thus, the sliding window first 
extracts three data points A, B, and C at time points t1, t2, and t3, respectively. After the data points within the window 
are processed, the window moves to the right to extract the next three data points. In this example, the window moved a 
total of three times and extracted a total of nine data points at time points from t1 to t9. Table 1 shows the sampled 
stream data. 
 
Fig. 2. Example of sliding window sampling. 
Table 1.  Sampled stream data. 
Data point Age Salary (in thousands) Arrival timestamp 
A 36 34 t1 
B 30 21 t2 
C 44 38 t3 
D 24 26 t4 
E 35 27 t5 
F 35 31 t6 
G 48 40 t7 
H 21 30 t8 
I 50 44 t9 
 
data points for each micro-cluster. The value of is between -1 and 1. A greater means a greater level of change; that is, 
the degree of correlation between two micro-clusters is greater. 
   
       













222222
11
1
YnYXnX
YXnXYn
YYXX
n
YY
n
XX
(1) 
 
3.1.3 Hierarchical Summary Frame Construction 
After micro-clusters are generated, we propose the use of a hierarchical summary frame to store cluster feature 
vectors of micro-clusters and construct level 0 (L = 0), which is the current level, of the hierarchical summary frame. In 
the offline clustering component, the cluster feature vectors stored at the current level of the hierarchical summary 
frame will be used as virtual points for clustering. In addition, the hierarchical summary frame is equipped with two 
functions: data aggregation and data resolution. When memory is low, it performs data aggregation to aggregate 
detailed data of lower level into summarized data of upper level to reduce the consumption of memory space and 
computation time for clustering. But if there is sufficient memory, it will perform data resolution to resolve summarized 
data of upper level back to detailed data of lower level. 
The representation of the hierarchical summary frame is shown in Figure 3. 
 
],[ 0
1
0 tttF ],[ 21
2
0 tt ttF  ],1
1F
[ 32
3
0 tt ttF
],[ 20
1
1 tttF ],[ 412
2
tt tt  ], 6141 tt tF [
3 t
],[ 413
4
0 tt ttF  ,[ 14
5
0 t ttF  ],[ 615
6
0 tt ttF 
],[ 0
1
2 tF
]5t
4tt
 
Fig. 3. Hierarchical summary frame. 
 L is used to indicate a level of the hierarchical summary frame. Each level is composed of multiple frames and each 
frame stores the cluster feature vector of one micro-cluster. 
 Each frame can be expressed as  esji ttF , , in which st is the starting timestamp, et is the ending timestamp, and i and 
j are the level number and frame number, respectively. 
 A detail coefficient field is added to each level above level 0 of the hierarchical summary frame, which stores the 
difference of data and is used for subsequent data resolution. 
The process of data aggregation and data resolution utilizes the Haar wavelet transform, which is a data 
compression method characterized by fast calculation and easy understanding and is widely used in the field of data 
mining [10]. This transform can be regarded a series of mean and difference calculations. The calculation formula is as 
follows: 
 The use of wavelet transform to aggregate frames in the interval can be expressed as   

 1i
iF
W
 , in which  
represents the frame. 
 k wavelet transforms can be expressed as 
458.0,8.0
100
20100,20,100  wRUN mmm
254.0,4.0
100
60100,60,100  wRUN mmm
 
Fig. 5. Example of window size adjustment. 
First, we adjust the size of the sliding window. A larger window size means a greater amount of stream data to be 
processed, which will consume more memory. Thus, we multiply the window size w by the remaining rate of 
memory to obtain the adjusted window size. As gets smaller, so is the window size. Figure 5 shows an example 
of window size adjustment, with the initial window size w set to 5. In scenario 1, the memory usage is 20 and the 
computed is 0.8. Then, through 
mR
mR
mR
mU
58.0  wRm  we obtain the new window size of 4, so we reduce the window size 
from 5 to 4. In scenario 2, the memory usage is 60 and the computed is 0.4. Then, 
through we obtain the new window size of 2, so we reduce the window size from 5 to 2. 
mU mR
54.0 wRm
Next, we perform data aggregation to adjust the level of the hierarchical summary frame. This process will be done 
only when  < 20% because it will reduce the accuracy of clustering results. On the other hand, we will perform data 
resolution when  < 20%, which indicates there is sufficient memory. The process of data aggregation and data 
resolution has been described in details in Section 3.1.3. 
mR
 mR1 
3.2.2 Clustering 
Figure 6 shows the proposed clustering algorithm. The algorithm inputs the number of clusters k, the distance 
threshold d , the lowest boundary of memory usage , and the cluster feature vectors stored in the current level of 
the hierarchical summary frame as virtual points x. The algorithm outputs the finally generated k clusters C. The steps 
of the algorithm are divided into three parts. The first part is for cluster generation (line 4-10). Every virtual point is 
attributed to the nearest cluster center to generate k clusters. The second part is for the adjustment of distance threshold 
mLB
d  (line 11-14). The adjustment of d is based on the current remaining rate of memory. A smaller d implies that 
virtual points are more likely to be regarded as outliers and discarded in order to reduce memory usage. The third part is 
for determination of the stability of clusters (line 15-31). Recalculate cluster centers of the clusters generated in the first 
part and use the sample variance and total deviation to determine the stability of clusters. Output the clusters if they are 
stable; otherwise repeat the process by returning to the first part. 
The parameters of the clustering algorithm are defined as follows: 
 k is the user-defined number of clusters. 
 d is the user-defined distance threshold. 
 mLB is the user-defined lowest boundary of memory usage. 
 }1{ nixx i  is the set of virtual points stored in the current level of the hierarchical summary frame. 
 }1{ kjCC j  is the set of k clusters generated by the algorithm. 
 
mN is the total memory size 
 }1{ kjcc j  is the set of k cluster centers. 
 ),(2 ji cxd is the Euclidean distance between virtual point xi and cluster center cj. 
 }1,1),({ 2 kjnicxdD jii  is the set of Euclidean distances between virtual point xi and each cluster 
center, with the initial value of . 
Input: k, d , ,mLB x  
Output:  C
1. compute mN ; 
2. c Random (x); 
3. Repeat 
4.   For each xxi   do 
5.     For each cc j   do 
6.       )},( ; { 2 jiii cxdDD 
7.     If ][ iDMin < d  then 
8.       }{ ij  s.t. ][ ; j xCC  ),(2 iji DMincxd 
9.     Else 
10.        delete ix ; 
11.   compute mU ; 
12.   mmmm NUNR /)(  ; 
13.   If mR < mLB  then  mRddd  1 ; 
14.   If  mR1 <20% then  ddd mR ; 
15.   For each jC  do 
16.     


ji Cx
ji cxE
2)( ; 
17.     1)( ;   /22 


  

j
Cx
ji CcountcxS
ji
18.      )( ; /)( j
Cx
ij Ccountxc
ji



19.      If jj cc   then 
20.         


ji Cx
ji cxE
2)( ; 
21.         1)( ;   /ˆ 22 


  

j
Cx
ji CcountcxS
ji
22.         If ( 2Sˆ < 2S ) and ( E < E ) then  
23.           jj cc  ; 
24.         Else 
25.           output jC ; 
26.           ji Cx   }{ ixxx  ; 
27.           }{ jccc  ; 
28.      Else  
29.        output jC ; 
30.         }{ ixxji Cx  x  ; 
31.        }{ jcc ; c 
32. Until  )( jj ccjC  or )ˆ( 22 SS  or )( EE   
33. return; 
Fig. 6. Clustering algorithm. 
We use the ACM KDD-CUP 2007 consumer recommendations data set as the real data set. This data set contains 
480,000 customer data, 17,000 movie data, and 1 million recommendations data recorded between October 1998 and 
December 2005. We use 200,000 recommendations data for the experiments. Furthermore, in order to use a variety of 
number of data points and dimensions to carry out the experiments, we use Microsoft SQL Server 2005 with Microsoft 
Visual Studio Team Edition for Database Professional to generate synthetic data sets, which are in uniform distribution. 
Table 4 shows the description of generating parameters of synthetic data. All data points are sampled evenly from C 
clusters. All sampled data points show a normal distribution. For example, B100kC10D5 represents that this data set 
 Fig. 8. Comparison of stream processing efficiency for a longer elapsed time. 
4.2.2 Accuracy 
We use the average of the sum of square distance (Average SSQ) to measure the accuracy of clustering results. 
Suppose there are data points in the period before the current time Tc. Find the nearest cluster center for each 
data point in the period and compute the square distance between and . The 
for the period h before the current time Tc equals to the sum of all square distances between every 
data point in  and its cluster center divided by the number of clusters. A smaller value of Average SSQ indicates a 
higher accuracy. 
hn
), h
h nic
nichn
(T
h
h ),(2 nii cnd in
 SSQAverage c
 
Fig. 9. Comparison of accuracy with consumer recommendations data set. 
 
Fig. 10. Comparison of accuracy with synthetic data set. 
Figure 9 and Figure 10 show the comparison of accuracy between RA-HCluster and RA-VFKM with the consumer 
recommendations data set and synthetic data set, respectively. The horizontal axis is the data rate (e.g., data rate 100 
means that stream data arrive at the rate of 100 data points per second) and the vertical axis is the Average SSQ. As 
shown in Figure 9 and Figure 10, the accuracy of RA-HCluster is about the same as that of RA-VFKM only when the 
memory usage in megabytes (MB). The experimental data is a synthetic data set B100kC10D5. As shown in Figure 13, 
even though RA-HCluster uses more memory in the beginning, it then decreases the memory usage by reducing the 
input granularity. After 40 seconds, therefore, the memory usage is decreased and stabilized. In contrast, RA-VFKM 
uses less memory than RA-HCluster only in the beginning, and it uses more memory after 37 seconds. 
 
Fig. 12. Comparison of memory usage by data size 
 
Fig. 13. Comparison of memory usage by execution time 
5   Conclusion 
In this paper, we have proposed the RA-HCluster algorithm for ubiquitous data stream clustering. This algorithm 
adopts the resources-aware technique to adapt algorithm settings and the level of the hierarchical summary frame, 
which enables mobile devices to continue with mining and overcomes the problem of lower accuracy or mining 
interruption caused by insufficient memory in traditional data stream clustering algorithms. Furthermore, we include the 
technique of computing the correlation coefficients between micro-clusters to ensure that more related data points are 
attributed to the same cluster during the clustering process, thereby improving the accuracy of clustering results. 
Experimental results show that not only is the accuracy of RA-HCluster higher than that of RA-VFKM, it can also 
maintain a low and stable memory usage. 
Acknowledgment 
The authors would like to express their appreciation for the financial support from the National Science Council of 
Republic of China under Project No. NSC 99-2221-E-031-005. 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/05
國科會補助計畫
計畫名稱: 無所不在的資料串流探勘技術與應用之研究
計畫主持人: 趙景明
計畫編號: 99-2221-E-031-005- 學門領域: 資料庫系統及資料工程
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
