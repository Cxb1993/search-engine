(2) Video resizing in spatial domain: to alter the 
size of a video by discarding its relatively 
unimportant portions 
2nd year, there are two tasks: 
(1) Video content analysis for building efficient 
indexing structures: to facilitate future video 
content access 
(2) Video summarization: to represent a video by a 
series of static frames 
英文關鍵詞： Video Length Alteration, Video Size Alteration, Video 
Content Analysis, Video Summarization 
 
2 
國科會多年期專題研究計畫成果報告 
 
 
頁數 
目錄                 2 
 
報告內容                3 
   前言                3 
研究目的               3 
文獻探討               5 
研究方法               9 
結果與討論             17 
 
參考文獻               18 
 
計畫成果自評              24 
 
附件  出席國際學術會議心得報告         25 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4 
伸縮比例是否應調整等，也將是研究的重點。 
3. 視訊內容分析與檢索： 
由於視訊資料的大量累積，視訊內容檢索也日形重要，例如監控錄影所取
得的資訊，電視台大量的歷史新聞影片等，若沒有提供適當的檢索功能下
將無法發揮應有的功效。一種可能的做法是針對視訊的內容做自動或半自
動式的分析或分類，例如可以採用「物件式」(object-based)的方法將影片
的前景分離出來，再加上註解(annotation)，並針對物件的形狀，顏色，材
質等建立各種適於搜尋的資料結構，以利未來的資訊檢索。注意到資訊檢
索也可搭配前述的視訊長度調整來進行，以使得資訊的儲存更為精簡有效
率。 
4. 視訊內容摘要： 
視訊內容摘要與視訊長度縮短之不同在於摘要的呈現方式可為動態的亦
可為靜態的。在動態呈現方式上可以利用視訊長度縮短的方式呈現，亦可
為視訊內容加上各類註解或說明，使得視訊內容更精簡與易於瞭解。在靜
態呈現方面，則是利用少數的關鍵畫面(key frames)來呈現視訊的內容，此
時關鍵畫面的選取變得十分重要，而畫面上同時亦可附加各類註解或說明
來豐富呈現的內容。 
 
整體而言，本計劃的目標是希望提供一個「智慧型視訊處理」之架構，而為達
此目標所需之關鍵技術則至少包含以下幾項：一、影像或視訊分析，二、影像
或視訊處理，三、影像或視訊合成，以下分項討論之。 
1. 影像或視訊分析： 
在絕大部分的視訊處理之前，某種程度的內容分析通常無法避免。對視訊
而言，可以視做一連串的影像，而視訊分析通常分為兩大部分，空間的
(spatial)及時間的(temporal)。所謂空間的係指對視訊的每張構成影像做個
別特徵的分析，例如視訊中的背景，人物內容等；而所謂時間的則是對視
訊的構成影像之間的關係做分析，例如畫面中人物的動作等。而此處與本
計畫有關的在空間方面的特徵計有臉部，視覺焦點(region of interests or 
ROI)，嘴型，字幕等；在時間方面則以動作為主。例如在視訊中常常需將
人臉找出來，以因應未來的檢索需求，或者在縮短視訊時，有人臉的部分
在處理上需相對保守。又如在沒有字幕的情況下做影片中的聲訊分析時，
透過嘴型辨識的輔助也可能大幅提高聲訊辨識的精準度。另外在針對時間
方面的行為分析也可以提高視訊縮短時的重要依據；再內容上缺乏變化的
時段可以大幅縮減其長度。 
2. 影像或視訊處理： 
於內容分析之後，即可針對所得之特徵內容做適當之處理，例如改變視訊
大小或長短，與利用不同的風格來呈現視訊。在改變視訊大小方面，可以
應用的是近年來十分熱門的影像大小改變(image resizing)技術，此部分將
在相關研究處詳述。至於改變視訊長短方面，進來亦成為許多研究的重
點，我們目前已有些初步的想法，將在之後研究方法處詳述。一般而言，
在改變視訊大小及長度時皆可採用改變影像大小之方法，然而卻有兩因素
6 
  
                             圖一(引自[1]) 
將視訊長度縮減的方式之一為將不重要的資訊捨去，最簡單的方式是利用
「快轉」(fast forward)，也就是在視訊中利用「均勻取樣」 (uniform sampling)
的方式。然而這樣的作法為人詬病的是可能會忽略掉ㄧ些重要資訊。故有
研究者者開始採用「非均勻取樣」 (non uniform sampling)，也就是根據畫
面的重要性來決定如何取樣，例如人物活動較密集處應該動態提高其取樣
頻率，反之，則可以降低取樣頻率，例如 Divakaran 等人[6]，Nam 等人[16]，
Bennett 等人[2]，Petrovic 等人[20]，以及 Jojic 等人[9] 皆採用這類
做法。以另ㄧ類作法是採用由 Avidan 等人所提出的 Seam Carving 影像大
小更改之方法[1]，見圖一，開此類想法的先河，亦可以被延伸應用到視訊
上。此處我們先解釋 Seam Carving 的想法，因為本計畫所提出的視訊長度
縮間方法基本上亦屬此類做法。在圖一中，我們先找出一條垂直的(vertical)
或水平的(horizontal)縫(seam)，如圖中兩條紅線所示，而此處所謂的垂直(水
平)縫，係指一從影像上(左)端到影像下(右)端的連接路徑(connected path)，
亦即路徑上的任兩像素(pixel)必互為對方的平面上的八個鄰居之一，而且
路徑與影像中的每一列(行)恰相交於一個像素。由此可知，藉由刪除此垂
直或水平的縫，即可達到縮小輸入影像寬度(width)或高度(height)的目的。
然而，為使得影像改變大小後對品質影響最小，在選擇縫時需考慮其內
容。一般而言，我們大概是用梯度(Gradient)大小來代表一個像素的重要性
或能量(energy)大小，若其梯度大小較小，表示該像素是座落於較平坦的區
域，故將移除造成影響較小，而移除ㄧ條縫的影響則定義為在縫上所有像
素的能量總和。至於如何有效率地找出最小能量的縫則可以利用「動態規
劃」(dynamic programming)的方式來達成。若要縮小影像的寬度或高度不
只一個像素，則可以重複上述的過程直到達成目標大小為止。至於要放大
影像也可以用類似的做法，透過加上垂直縫及水平縫來達成，但要注意的
是如果持續地加入縫，其結果可能造成在某些影像較無變化的部分被過分
地拉大造成變形，故[1]的做法是同時找出前 k 條能量較小的縫一次一齊加
入，並將一個較大的放大過程切割為數個步驟，而且在每個步驟內限制放
大的比例來避免變形的發生。 
8 
了考慮梯度(gradient)外，也考慮了 Itti 等人所提出的影像的 salience 
map[14]，而所謂的 salience map 係考慮了影像中的顏色，對比，亮度等資
訊而將影像中較顯明之部分賦予較高之重要性。Liu 等人則提出另ㄧ種作
法，在考量內容的重要性及人物的運動的情況下，藉著自動 pan 及 scan 的
方式來進行輸入視訊的切取(cropping)或伸縮(scale)。 
3. 視訊內容分析與檢索： 
       
                                圖四(引用自[17]) 
視訊內容分析通常不外乎以下幾種方式：根據物件，例如 Viola 等人[39]，
Yang 等人[55]及 Wei 等人[99]等人的人臉偵測法，其他許多相關的人臉偵
測方法亦可見 Yang 等人的 survey[55]，Kjeldsen 等人的皮膚偵測[40]；根
據「動作」(motion)，例如 Ngo 等人[17]的「移動註解演算法」(motion 
annotation algorithm)，藉著分析每段鏡頭內的移動並從視訊的各類剖面中
去分析其移動軌跡，參見圖四；根據畫面顏色，例如 Farin 等人[7]的做法，
利用 K-Mean 分群法，並將使用者事先提供的「領域知識」  (domain 
knowledge)也納入整合考量；根據字幕，如 Zhu 等人[88]，Kobla 等人[89]
以及 Qi 等人[92]的方法。更詳細的內容可參閱 Brezeale 等人[86]所寫的視
訊自動分類相關研究的文獻回顧。一旦內容分析確立後，即可採用各式架
構建立「索引」(index)或資料結構，以便未來的搜尋，相關的做法可以參
考 Smoliar 人[87]，Hauptmann 等人[90]，Wold 等人[94]，以及 Kobla 等人
10 
                            圖六 
有關視訊長度縮短，我們將從兩類方式著手，第一是 Avidan 等人所
提的 Seam Carving 的 方 式 [1] ，第二是 Wang 等人 所提的
Scale-and-Stretch 方式[28]。在 Seam Carving 方面，如之前相關文獻
處所言，在 2D 影像上尋找據最小能量縫的做法依[1]之作法可採用
「動態規劃」(dynamic programming)的方式，見圖六(a)，此處以尋找
最小能量之垂直縫為例，說明如下。我們由 y 軸為倒數第二大的列開
始做起，對此列的每一個像素(x,y)，尋找它在前一列(y 值較小的一列)
中累積至目前最小能量的鄰居，即(x-1,y-1)，(x,y-1)或(x+1,y-1)(在遇
到左右邊界時鄰居數會縮減為兩個)，依此選擇則會形成ㄧ由(x,y)至
前一列的路徑(path)，並將加上(x,y)像素能量後的值累積於(x,y)上，
作為之後繼續計算的依據。依此方式計算後，對影像中最上(y 值最大
的)ㄧ列，其每ㄧ像素皆有一向下對應的路徑，及其路徑的總能量，
故最小能量的路徑即可決定。但不幸的是，這樣的作法無法直接推廣
到求 3D 的視訊的最小能量 sheet，而其原因可見圖六(b)：想像在 Y=y
的平面上的一條路徑上的兩點(x-1,y,t-1)與(x,y,t)，其中(x-1,y,t-1)在
Y=y-1的平面上其所選擇能量最小的鄰居可能為(x-1,y-1,t-2)，而(x,y,t) 
在 Y=y-1 所選擇的卻可能是(x,y-1,t+1)，但很明顯的卻與(x-1,y-1,t-2)
不相鄰，因而導致錯誤的情形發生。為解決此問題，Chen 等人提出
了使用 graphcut 的方式[5]，雖然可將最小能量的 sheet 求出，但卻衍
生了三個問題。其一，這樣的做法可能有違反「連接性」(connectivity)
的問題，以下圖七即為一 2D 的例子。在圖七中，黑色圓點代表像素，
而藍色線為其「切割」(cut)，而依照[5]的做法，紅色方框所框住的像
素為對應所要移除的縫上的點，而箭號所示即為兩個不連接的像素。
其二，此 grapcut 的演算法使用的是圖論(graph theory)中解決「最大
流量/最小切割」(maximal flow/minimal cut)的演算法，相對於 2D 動
態規劃所費的「線性時間複雜度」(linear time complexity)，graphcut
的「時間複雜度」(time complexity)相對高得多。其三，其所消耗的
空間複雜度(space complexity)也高得多，而這對處理資訊量天生比較
大的視訊來說問題顯得更是棘手。 
12 
                             圖九 
這樣的做法雖上無法證明其最佳性，但可見其時間複雜度為線性，空
間複雜度為線性，而且更有處理「超核心運算」(out-of-core computation)
的能力。所謂超核心運算，係指運算時所需記憶體大小超過系統的主
記憶體大小，而這在視訊資訊變得愈來愈大的今日將變得更容易發
生。而我們所提出的做法，在此方面享有其優勢，茲以圖九之 2D 影
像範例說明之。令影像的高度為 W1，則我們可以訂出一個寬度為
W1+W+W1=2W1+W 的「工作視窗大小」(working window size)，此
處之 W 為一可變大小，取決於可用記憶體大小是否可容納一張
(2W1+W)×W1 的影像。我們的做法是在「目前工作視窗」(current 
working window)中一次處理一個工作視窗大小的資訊，而這也是目
前的記憶體能承載的大小，處理完畢時再將目前工作視窗往下移動
W1+W 的距離，以處理下一個視窗大小的資訊，直到所有的資訊處
理完畢為止。現在考慮任一條從目前工作視窗底部長出的縫，則其縫
的終點共有三種情形：一、結束在目前工作視窗之前，則此縫必為已
處理過之情形(此部分之後再做解釋)或者碰到左邊界故不需處理；
二、結束在目前工作視窗之上，則當然可以處理；三、結束在未來工
作視窗之上，則我們可在這些縫碰觸到目前工作視窗右邊界時將其處
理暫時「凍結」(suspend)，等待移至下一個工作視窗時再做處理。現
在考慮下一工作視窗之情形，注意到此新的工作視窗與上一個工作視
窗有 W1 寬度的重疊，這使得在新的工作視窗中所考慮的新的縫其最
左端的結束點必定落在新工作視窗內，此乃由於縫必頇為連接
(connected)的緣故。這樣一來我們只需考慮在新增 W+W1 寬度內的
新縫即可。注意到這樣做的好處是要尋找「全域的最佳解」(global 
optimum)時只需要一次的線性掃描(linear scan)，作法是在目前工作視
窗內只保留最佳解，即縫的最小能量值，及其縫的結束位置，至於縫
上每ㄧ像素的前一列的鄰居為誰等資訊可以寫入硬碟，待找到全域最
佳解時再作縫的重建之用。此作法的另一個好處是也可以支援平行計
算，亦即可以使用不同的機器針對不同的工作視窗同時做計算，最後
再做結果的整合。同樣的做法可以推廣至視訊，只不過工作視窗大小
需調整為 4W1+W，而每次工作視窗向又移的大小調整為 2W1+W，
主要原因是要同時考慮在 X 及 T 方向的連接的 sheet 的最大位移量。 
14 
個要注意的是，若是放大倍數太高，也可能造成扭曲，如圖十七(d)
所示，而解決辦法是將大倍數的放大過程拆解成數個步驟，並在每個
步驟限制放大的程度，例如 50%，已達成較佳的效果，如(e)所示。 
2. 視訊大小調整： 
                   
                         圖十一(引用自[107]) 
在視訊大小調整方面，仍然可用 Seam Carving 或 Scale-and-Stretch 的方式
來做。但視訊與影像在處理上有一最大的不同之處：即「時間方向的一致
性」(temporal coherence)。舉例而言，若每一張視訊的影像各自做影像大
小的調整，不論是用 Seam Carving 或 Scale-and-Stretch 的方式，即使調整
後的影像大小一樣，但因為在每張影像上所修改的部分不見得相同，將造
成修改後的視訊在撥放時會有閃爍(flickering)的問題。對此，目前我們先
針對 Seam Carving 的做法設計了一個對策：先將所有(或屬於同一場景)的
能量加總至第一張影像，接者在第一張影像上做 2D 的 Seam Carving，然
後將所得的縫作用在視訊的每一張影像上：也就是所有的影像都會做一致
的改變，因而去除了閃爍的現象。當然這樣做是否為最佳解還有待持續及
深入的研究。至於在使用 Scale-and-Stretch 來處理視訊大小調整方面，除
了在兼顧 temporal coherence 外，重點是找出視訊中對應的能量定義，目前
能想到的是，除了考慮原本 2D 影像的「梯度」 (gradient)及「顯明圖」
(salience map)的資訊，同時也要將視訊物件中的動作(motion)納入考量，例
如考慮 MPEG 的「動作估計」(motion estimation)或「動作補償」(motion 
compensation)，或是所謂的「光流」 (optical flow)，如 Aires 等人的研究
[107]，以及 Beauchemin 等人的研究[108]等，來維持時間方向的一致性。
在圖十一(引用自[107])的例子中箭號即為每個對應像素在相關視訊畫面中
的移動(motion)方向。 
 
在本計畫的第二年，共計要執行 1. 視訊內容分析與檢索，2.視訊內容摘要，茲
將其相關研究方法概述如下： 
1. 視訊內容分析與檢索：                          
在視訊檢索方面，除了將實作根據人臉[39,55,99]，膚色[40]，動作[17]，
字幕[88,89,92]等來分類視訊等方式外，也將根據 Nadi 人[83]，Li 等人[84]
以 及 Arivazhagan 等 人 [85] 的 材 質 分 類 / 分 割 (texture 
classification/segmentation)方式納入考量，其實考量材質做法其來有自，
16 
的摘要，亦即提供靜態畫面。一般的做法有幾種：從視訊中挑選一些代表
性畫面，或者由視訊中的畫面組合成一張或數張畫面。此處的研究重點在
於以下兩方面：第一是如何由視訊中找出重要的畫面，第二是如何由重要
畫面中找出重要的部分。在找出重要畫面方面，一般也有許多做法，例如
可以參考視訊壓縮時找出關鍵畫面(key frame)的作法。簡單地來說，以
MPEG 為例，其中有所謂的 I(ntra) frame，P(rediction) frame，B(idirectionally 
predicted) frame，而所謂的 Intra frame 代表的該 frame 的壓縮為 spatial 
compression，亦即其壓縮所考慮的範圍僅包含該畫面本身，而不會如 P 
frame 及 B frame 等其壓縮作法是 temporal compression，亦即會同時考量
時間上相鄰的其他畫面。在實務上選擇 key frame 的做法可以考量兩件事：
資訊量及資訊壓縮效能。在考量資訊量方面，ㄧ張畫面可以當作 key frame
應代表其中所包含資訊量較高，至於衡量方式則可以採用「熵」(entropy)，
ㄧ般定義如下： 


n
i
ii xpxpEntropy
1
2 )(log)( ， 
其中 p(xi)為符號 xi 出現的機率；在考量資訊壓縮效能方面，若要衡量某ㄧ
key frame 的資訊壓縮效能，所要計算的是，以此 key frame 當做 I frame，
並對之後所產生的 P frame 及 B frame 作「動作補償」(motion compensation)
等預測(prediction)及內插(interpolation)所計算出的總壓縮效果做評估，以
決定 key frame 的取捨。至於 key frame 的張數如何決定可由使用者決定或
者系統可以將依資訊量與資訊壓縮效能所計算的結果從高到低的順序排
列列出。注意到以上的 key frame 選取過程應該搭配視訊的段落偵測(shot 
detection)及場景偵測(scene detection)，亦即應該每個段落或者每個場景分
開處理。 
    
                         圖十四(引用自[115]) 
至於如何由重要的畫面中找出重要的部分則可以透過以下做法，人臉偵測
18 
針對以上困難，目前其解決途徑目前規劃如下：(1)可能先從既有文獻的分類方式著手，例
如[113]所採用的顏色分類等，未來或許再另行研究之，因為實言之，各類屬性定義的完整
性，例如材質，都將是未來可以繼續研究的有趣課題。(2)可以考慮不要採用具最高頻率的
屬性，而是用ㄧ種機率分佈(probabilistic distribution)的概念來代表，則只要用兩種機率分佈
夠接近我們就可認為其相似。 
4. 視訊內容摘要： 
目前可預期的困難計有以下各項：(1)執行時間恐過長。(2)畫面中重要部分如何決定？(3) 
欲將數個畫面中之重要部分共同呈現於一個畫面但將導致位置重疊。 
針對以上困難，目前其解決途徑目前規劃如下：(1)藉由GPU 的協助。(2)同時考慮salience 
map 及motion estimation 作為視訊能量或重要性之定義。(3)參考Pavic 等人的作法[109]， 
將視訊中的畫面依視角的需要做調整，使得由各畫面重要部份的重疊減至最小。 
 
參考文獻 
[1] S. Avidan and A. Shamir. Seam Carving for Content-Aware Image Resizing. In 
SIGGRAPH ’2007, 2007.  
[2] E. P. Bennett and L. McMillan. Computational Time-Lapse Video. In SIGGRAPH ’2007, 
2007. 
[3] Y. Boykov and M. P. Jolly. Interactive Graph Cuts for Optimal Boundary & Region 
Segmentation of 
Objects in N-D Images. In ICCV ’2001, 2001. 
[4] Y. Boykov and V. Kolmogorov. An Experimental Comparison of Min-Cut/Max-Flow 
Algorithms for Energy Minimization in Vision. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 26(9):1124–1137, 2004. 
[5] B. Chen and P. Sen. Video Carving. In Eurographics ’2008, 2008. 
[6] A. Divakaran, K. A. Peker, R. Radhakrishnan, Z. Xiong, and R. Cabasson. Video 
Summarization using Mpeg-7 Motion Activity and Audio Descriptors. Technical Report 
TR2003-34, MERL - Mitsubishi Electric Research Laboratories, 2003. 
[7] D. Farin, W. Effelsberg, and P. H. N. de With. Robust Clustering-Based Video 
Summarization with Integration of Domain-Knowledge. In ICME ’2002, pages 89–92, 2002. 
[8] X. Hua, L. Lu, and H. Zhang. Ave: Automated Home Video Editing. In ACM 
Multimedia ’2003, pages 490–497, 2003. 
[9] N. Jojic, N. Petrovic, and T. S. Huang. Scene Generative Models for Adaptive Video Fast 
Forward. In ICIP ’2003, pages 619–622, 2003. 
[10] H. Kang, Y. Matsushita, X. Tang, and X. Chen. Space-Time Video Montage. In 
CVPR ’2006, pages 1331–1338, 2006. 
[11] C. Kim and J. Hwang. An Integrated Scheme for Object-Based Video Abstraction. In ACM 
Multimedia ’2000, pages 303–311, 2000. 
[12] V. Kwatra, A. Schodl, I. Essa, and A. Bobick. Graphcut Textures: Image and Video 
Synthesis Using Graph Cuts. In SIGGRAPH ’2003, 2003. 
[13] F. Liu and M. Gleicher. Video Retargeting: Automating Pan and Scan. In ACM 
20 
[32] Y. F. Zhang, S. M.. Hu, R.. R. Martin, Shrinkability Maps for Content-Aware Video 
Resizing. In Pacific Graphics ’2008, pages 1797-1804, 2008. 
[33] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid Mothod 
in Image Processing. RCA Engineer, 29(6):33–41, 1984. 
[34] A. Agarwala, M. Dontcheva, M. Agrawala, S. Drucker, A. Colburn, B. Curless, D. Salesin, 
and M. Cohen. Interactive Digital Photomontage. In SIGGRAPH ’2004, pages 294–302, 2004. 
[35] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image Inpainting. In 
SIGGRAPH ’2000, 2000. 
[36] R. Bornard, E. Lecan, L. Laborelli, and J. Chenot. Missing Data Correction in Still Images 
and Image Sequences. In ACM Multimedia ’2002, pages 355–361, 2002. 
[37] A. A. Efros and T. K. Leung. Texture Synthesis by Non-Parametric Sampling. In ICCV ’99, 
1999. 
[38] H. Gu, G. Su, and C. Du. Feature Points Extraction from Faces. In Image and Vision 
Computing NZ, 2003. 
[39] P. Viola and M. J. Jones, Robust real-time face detection. International Journal of 
Computer Vision, 57:137-154, 2004. 
[40] R. Kjeldsen and J. Kender. Finding Skin in Color Images. In FG’ 96 (2nd International 
Conference on Automatic Face and Gesture Recognition), 1996. 
[41] V. Kwatra, A. Schodl, I. Essa, and A. Bobick. Graphcut Textures: Image and Video 
Synthesis Using Graph Cuts. In SIGGRAPH ’2003, 2003. 
[42] J. Lalonde, D. Hoiem, A. a. Efros, C. Rother, J. Winn, and A. Criminisi. Photo Clip Art. In 
SIGGRAPH ’2007, 2007. 
[43] A. Levin, A. Zomet, S. Peleg, and Y. Weiss. Seamless Image Stitching in the Gradient 
Domain. In ECCV ’2004, pages 377–389, 2004. 
[44] T. Leyvand, D. Cohen-Or, G. Dror, and D. Lischinski. Digital Face Beautification. In 
SIGGRAPH 2006 Technical Sketch, 2006. 
[45] T. Leyvand, D. Cohen-Or, G. Dror, and D. Lischinski. Data-Driven Enhancement of Facial 
Attractiveness. In SIGGRAPH ’2008, 2008. 
[46] E. N. Mortensen and W. A. Barrett. Intelligent Scissors for Image Composition. In 
SIGGRAPH ’1995, pages 191–198, 1995. 
[47] M. M. Oliveira, B. Bowen, R. McKenna, and Y. Chang. Fast Digital Image Inpainting. In 
VIIP (International Conference on Visualization, Imaging and Image Processing) ’2001, 2001. 
[48] S. Peleg. Elimination Seams from Photomosaics. In Conference on Pattern Recognition 
and Image Processing, pages 426–429, 1981. 
[49] P. Perez, M. Gangnet, and A. Blake. Poisson Image Editing. In SIGGRAPH ’2003, pages 
313–318, 2003. 
[50] J. Sun, L. Yuan, J. Jia, and H. Shum. Image Completion with Structural Propagation. In 
SIGGRAPH ’2005, pages 861–868, 2005. 
[51] M. Uyttendaele, A. Eden, and R. Szeliski. Eliminating Ghosting and Exposure Artifacts in 
Image Mosaics. In CVPR ’2001, pages 509–516, 2001. 
[52] J. Wang, M. Agrawala, and M. Cohen. Soft Scissors: An Interactive Tool for Realtime 
22 
Complex Scenes, In SIGGRAPH ’2002, pages 243-248, 2002. 
[74] D. B. Goldman, B. Curless, D. Salesin and S. M. Seitz. Schematic Storyboards for Video 
Visualization and Editing. In SIGGRAPH ’2002, pages 862-871, 2006. 
[75] Y. Li, J. Sun, C. Tang and H. Shum. Lazy Snapping. In SIGGRAPH ’2004, pages 303-308, 
2004.  
[76] J. Wang, M. Agrawala and M. Cohen. Soft Scissors: An Interactive Tool for Realtime High 
Quality Matting. In SIGGRAPH ’2007, 2007. 
[77] J. Assa, Y. Caspi and D. Cohen-Or, Action Synopsis: Pose Selection and Illustration. In 
SIGGRAPH ’2005, pages 667-676, 2005.  
[78] J. Assa, D. Cohen-Or, I. Yeh and Y. Lee. Motion Overview of Human Actions. In 
SIGGRAPH Asia ’2008, 2008. 
[79] Y. Chuang, B. Curless, D. Salesin, and R. Szeliski. A Bayesian Approach to Digital Matting. 
In CVPR ’2001, vol. II, pages 264 - 271. 
[80] M. Gleicher. Image Snapping, In SIGGRAPH ’95, pages183-190, 1995. 
[81] A. Rav-Acha, Y. Pritch and S. Peleg. Making a Long Video Short: Dynamic Video Synopsis. 
In  
CVPR ’2006, pages 435-441, 2006. 
[82] M. Bertalmio, G. Sapiro, V. Caselles and C. Ballester. Image Inpainting. In 
SIGGRAPH ’2000, pages 417-424. 
[83] D. A. A. Nadi and A. M. Mansour. Independent Component Analysis (ICA) for Texture 
Classification. In In 5
th
 International Multi-Conference on Systems, Signals and Devices, 2008. 
[84] S. Li, J. T. Kwok, H. Zhu and Y. Wang. Texture Classification using the Support Vector 
Machines. Pattern Recognition, 36:2883-2893, 2003. 
[85] S. Arivazhagan and L. Ganesan. Texture Classification using Wavelet Transform. Pattern 
Recognition Letters, 24:1513-1521, 2003. 
[86] D. Brezeale and D. J. Cook. Automatic Video Classification: A Survey of the Literature. 
IEEE Transactions on Systems, Man, and Cybernetics, 38(3):416-430, 2008. 
[87] S. W. Smoliar and H. Zhang. Content Based Video Indexing and Retrieval. IEEE 
Multimedia, vol. 1, pp. 62, Summer. 1994. 
[88] W. Zhu , C. Toklu and S.-P. Liou “Automatic News Video Segmentation and Categorization 
Based on Closed-Captioned Text,” Proc. IEEE Int. Conf. Multimedia Expo (ICME 2001), p. 
829. 
[89] V. Kobla , D. DeMenthon and D. Doermann “Identifying Sports Videos using Replay, Text, 
and Camera Motion Features,” Proc. SPIE Conf. Storage Retrieval Media Databases, 2000, p. 
332. 
[90] A. G. Hauptmann , R. Jin and T. D. Ng “Multi-Modal Information Retrieval from Broadcast 
Video using OCR and Speech Recognition,” Proc. 2nd ACM/IEEE-CS Joint Conf. Digit. Libr. 
(JCDL 2002), p. 160. 
[91] R. S. Jasinschi and J. Louie “Automatic TV Program Genre Classification Based on Audio 
Patterns,” Proc. IEEE 27th Euromicro Conf., 2001, p. 370. 
[92] W. Qi , L. Gu , H. Jiang , X.-R. Chen and H.-J. Zhang “Integrating visual, audio and text 
24 
[111] I. Posner, D. Schroeter and P. Newman. Using Scene Similarity for Place Labeling. In 
Proc 10th International Symposium on Experimental Robotics, 2006. 
[112] I. Ulrich and I. Nourbakhsh. Appearance-based place recognition for topological 
localization. In ICRA (IEEE International Conference on Robotics and Automation) ’2000, 2000. 
[113] C. Yang and L. Peng, Automatic Mood-Transferring between Color Images. IEEE 
Computer Graphics and Applications, 28(2): 52-61, 2008. 
[114] C. Yang and H. Yang, Realization of Seurat’s Pointillism via Non-Photorealistic Rendering. 
The Visual Computer, 28(5):303-322, 2008. 
[115] C. Rother, L. Bordeaux, Y. Hamadi and A. Blake. AutoCollage. In SIGGRAPH ’2006, 
pages 847-852, 2006. 
[116] M. Ashikhmin, Synthesizing Natural Textures, In Proceedings of the 2001 Symposium on 
Interactive 3D Graphics, pages 217-226, 2001. 
[117] O. N. Gerek and Y. Altunbasak. Key Frame Selection from MPEG Video Data. In Visual 
Communications and Image Processing ’97, Vol. 3024, Pages 920-925, 1997. 
[118] J. Chou and C. Yang. Face-off: Automatic Alteration of Facial Features. Technical Report, 
2008. 
[119] V. Blanz, K. Scherbaum, T. Vetter and H. Seidel. Exchanging Faces in Images. In 
Eurographics ’2004, 2004. 
[120] C. Bregler, M. Covell and M. Slaney. Video Rewrite: Driving Visual Speech with Audio. 
In SIGGRAPH ’1997, pages 353-360, 1997. 
[121] V. Blanz, C. Basso, T. Poggio and T. Vetter. Reanimating Faces in Images and Video. In 
Eurographics ’2003, 2003. 
[122] V. Blanz and T. Vetter. A Morphable Model for the Synthesis of 3D Faces. In 
SIGGRAPH ’1999, pages 187-194, 1999. 
[123] Y. Chuang, D. Goldman, K. C. Zheng, B. Curless, D. Salesin andR. Szeliski. Animating 
Pictures with Stochastic Motion Textures. In SIGGRAPH ’2005, pages 853-860, 2005. 
[124] S. Varadarajan, T. Chiueh, Sase: Implementation of A Compressed Text Search Engine, in 
Proceedings of USENIX Internet Technologies and Systems, 1997. 
[125] C. Chang, N. Izumi, N. Yanagihara and C. C. Lee. New Motion Estimation Algorithm for 
DV to MPEG1 Compression Domain Conversion. In ICCE International Conference on 
Consumer Electronics, pages 158-159, 1999. 
 
計畫成果自評 
目前本計畫所規劃之視訊各項研究皆已依計畫完成，其中第一年計劃所研究之視訊長度及
大小調整部分已投稿至期刊審稿中，而第二年計畫所研究之視訊內容分析與檢索及視訊內
容摘要部分之相關研發技術正撰寫為一技術報告以準備期刊投稿中。 
 
 
 
 
26 
System”，他提出了一個十分有趣的觀點：認為學習閱讀的本身，不但增加了知識，
也對一個人腦中與視覺方面有關的功能，有正面的提升效果。該場演講也激發了現場
眾多來賓的許多熱烈討論。 
 
鑒於如今與會的人士仍主要以歐美人士為主，而國內的產、官、學、研界參與的人數仍偏
少。我想國內的學者應多參與如此般的國際會議，一方面拓展視野，一方面也可結識國際
間的研究同好，交流彼此想法，對其研究當更有助益。在另外一方面，由於國內教授升等
時僅參考期刊論文的做法似乎也降低了大家出國的意願，相關的做法或可也有所調整。此
外於會後於法國也順道做一番遊覽，對當地留下不錯的印象。 
Prof. Stan Dehaene所講述的主題：“How Learning to Read Changes the Visual System”，他
提出了一個十分有趣的觀點：認為學習閱讀的本身，不但增加了知識，也對一個人腦中
與視覺方面有關的功能，有正面的提升效果。該場演講也激發了現場眾多來賓的許多熱
烈討論。 
 
鑒於如今與會的人士仍主要以歐美人士為主，而國內的產、官、學、研界參與的人數仍
偏少。我想國內的學者應多參與如此般的國際會議，一方面拓展視野，一方面也可結識
國際間的研究同好，交流彼此想法，對其研究當更有助益。在另外一方面，由於國內教
授升等時僅參考期刊論文的做法似乎也降低了大家出國的意願，相關的做法或可也有所
調整。此外於會後於法國也順道做一番遊覽，對當地留下不錯的印象。 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：楊傳凱 計畫編號：98-2221-E-011-099-MY2 
計畫名稱：智慧型視訊處理 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 9 9 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
