中文摘要:  
 
  目前在資料網格方面的研究著重在資料分享機制
的建立與維護。藉由機制的建立，使用者才能分享
資料與控管權限。但我們仍需解決像是資料複製的
份數，複製後的資料要如何取得才有利，要取哪一
份才不會造成存取瓶頸，複製後的資料要如何擺放
才能有效提升效能等問題。 
  本計畫探討幾個重要問題：（一）如何決定複製資
料的擺放位置以確保複製資料之間的工作量平衡，
（二）如何決定最少的資料複製份數，（三）如何同
時兼顧工作量平衡與服務品質，（四）做工作排程時
如何兼顧資料傳輸時間以縮短工作執行時間。本計
畫針對資料網格提出資料複製與工作排程的決策模
型，並在此模型環境下提出快速演算法並配合實際
製作以解決上述問題。 
 
關鍵詞：資料伺服器配置、資料備份配置、工作排
程、資料網格、網格計算 
 
Abstract: 
 
This project focuses on studying the challenges im-
posed by data-intensive computation on cluster, Grid, 
Cloud and distributed systems, and on devising 
state-of-the-art solutions to overcome these challenges. 
We have identified three critical issues in improving 
the performance of data intensive computation: (1) 
Proper allocation, management and scheduling of 
resources (data servers, data replicas and I/O devices) 
to increase availability, scalability and efficiency of 
resource use, (2) Once the locations of resources have 
been determined, data access efficiency can be further 
improved by careful scheduling and optimization of 
data transfers between data site and computing site, 
and (3) Since in data-intensive computation, the time 
for data  for data transfer may be substantial and may 
vary depending on network bandwidth, an effective 
job scheduling strategy should take both computation 
and data transfers of multiple jobs into consideration, 
and properly interleaves data transfers and job execu-
tion to maximize the overall performance. We 
introduce the concept of data-aware job scheduling for 
this purpose. 
 
Keywords: Data server placement, data replica place-
ment, data-aware job scheduling, data grid, grid 
computing. 
 
Introduction  
The data needs of scientific as well as commercial 
applications from a diverse range of fields have been 
increasing exponentially over the recent years. This 
increase in the demand for large-scale data processing 
has necessitated collaboration and sharing of data 
collections among institutions and use of distributed 
resources owned by collaborating parties. My research 
in recent years focuses on studying the challenges 
imposed by data-intensive computation on cluster, 
Grid, Cloud and distributed systems, and on devising 
state-of-the-art solutions to overcome these challenges. 
We have identified three critical issues in improving 
the performance of data intensive computation: (1) 
Proper allocation, management and scheduling of 
resources (data servers, data replicas and I/O devices) 
to increase availability, scalability and efficiency of 
resource use, (2) Once the locations of resources have 
been determined, data access efficiency can be further 
improved by careful scheduling and optimization of 
data transfers between data site and computing site, 
and (3) Since in data-intensive computation, the time 
for data transfer may be substantial and may vary 
depending on network bandwidth, an effective job 
scheduling strategy should take both computation and 
data transfers of multiple jobs into consideration, and 
properly interleaves data transfers and job execution to 
maximize the overall performance. We introduce the 
concept of data-aware job scheduling for this purpose. 
data that a server has to handle is minimized (called 
the MinMaxLoad problem)? Furthermore, if we fix the 
workload that a server can handle, what is the mini-
mum number of replicas required to satisfy all data 
requests and where should we place them (called the 
FindR problem)? We show that there exists an optimal 
solution to the FindR problem. We devise a fast algo-
rithm called LazyUpdate that finds the optimal 
solution to FindR in O(nlogn) time, where n is the 
number of tree nodes. With the LazyUpdate algorithm, 
we can also devise an algorithm that finds the optimal 
solution to MinMaxLoad with k replicas. The algo-
rithm finds the replica set by “guessing” the maximum 
workload D through a binary search. If the upper 
bound on the workload of every node is U, then the 
total workload is bounded by O(nU). Clearly, after 
O(logn+logU) calls of LazyUpdate, we will be able to 
find the smallest value of D such that k replicas are 
sufficient. We also show that when the clients are 
allowed to specify QoS requirement in terms of a 
distance constraint for their data requests, we can still 
find optimal solutions to FindR and MinMaxLoad with 
dynamic programming. These results provide a theo-
retical foundation for replica placement in hierarchical 
Grid with the goal of balancing server workload. 
 
In [CWL08], we investigate server/replica place-
ment in more general Grid models. We show that 
server/replica placement under the constraint of data 
access cost, replica deployment cost and QoS re-
quirement is NP-Complete even in the simple, 
hierarchical Grid model. We develop a fast algorithm 
called Greedy Cover to solve the problem. The results 
of simulation experiments show that Greedy Cover 
finds near-optimal solutions and is very scalable. In 
[CWL08], the Grid nodes are organized as an arbitrary 
graph, and the placement algorithm needs to take data 
access cost, replica deployment cost, QoS requirement 
and server capacity constraint into consideration. This 
model captures real-world scenarios more accurately. 
The placement problem under this model is 
NP-complete. We propose a realistic cost model that 
estimates the combined effect of all the cost con-
straints. Based on the cost model, we propose two 
algorithms called greedy remove and greedy add to 
approximate the optimal solution. Experiment results 
show that both algorithms find a near-optimal solution 
in acceptable time; nonetheless, there is room for 
improvement of the time complexity.  
 
1.2 Server Placement for Parallel I/O in Cluster 
Systems 
 
Parallel I/O technologies help increase parallelism 
in reading and writing a data file by striping the data 
file across multiple disks. In traditional parallel ma-
chines, parallel I/O operations are usually performed 
by dedicated I/O nodes. This is reasonable because 
these parallel systems usually have large number of 
light-weight computing nodes without support for I/O.  
A cluster, however, usually consists of limited number 
of general-purpose nodes each having local disks and 
I/O support. This motivated us to investigate dynamic 
configuration of parallel I/O system. The central issue 
in this problem is how to properly select a subset of 
nodes in the cluster for I/O based on the data transfer 
pattern of the application program. We call this prob-
lem “server placement for parallel I/O”. Chou et. al. 
addressed a similar problem for bus-based clusters. To 
our knowledge, our work is the first attempt to tackle 
this problem on modern switch-based clusters.  
 
In one of our earlier works, we have observed that 
load balance on the I/O nodes is the key optimization 
for improving parallel I/O performance on 
switch-based clusters. Based on the observation, we 
formulate server placement as a weighed bipartite 
matching with the goal to minimize the maximum 
centralized optimal, augmenting path algorithm in a 
distributed environment. HDLWF is based on a distri-
buted, two-step scheme that determines appropriate 
execution order of data requests through a small 
number of rounds of bidding between clients and I/O 
servers.  Our experimental results indicate that 
HDLWF yields schedules close to the centralized 
optimal solution, and in some cases within 3% of the 
optimal solution. 
 
2.2 Optimization for Multicast Data Transfers  
 
There have been numerous research results on op-
timization of multicast. Most of the results focus on 
regular network topologies such as mesh and hyper-
cube. It is difficult to implement efficient multicast on 
irregular networks because such networks lack the 
attractive mathematical properties that exist in regular 
networks. In an earlier work, we propose an 
agent-based approach for scheduling multiple multi-
cast on switch-based networks with irregular 
topologies. Our approach assigns an agent to each 
subtree of the root of the routing tree such that the 
agents can exchange information efficiently and 
independently.  The entire multicast problem is then 
recursively solved with each agent sending the mes-
sage down the subtree to those switches that it is 
responsible for.  In this way, communication is 
localized by the assignment of agents to subtrees.  
This idea can be generalized to multiple multicast 
since the order of message passing among agents can 
be interleaved for different multicasts.  The key to the 
performance of this agent-based approach is the mes-
sage-passing scheduling between agents and the 
destination processors. In [LLWK07], we propose an 
optimal scheduling algorithm to solve this problem. 
We first devise a testing algorithm to determine 
whether a given problem instance can finish within a 
given total multicast time T. Then, the optimal T and 
the optimal schedule can be decided by a binary search 
on T.  Experiment results show that our agent-based 
approach outperforms the reported fastest algorithm, 
SPCCO, in multiple multicast, and is competitive to 
SPCCO in single multicast.  
 
3. Data-Aware Job Scheduling  
 
Traditional scheduling research considers job sche-
duling as independent of data access (i.e. data access 
time is negligible). However, in distributed systems 
and data-intensive computation, data access typically 
requires long latency and thus cannot be ignored. An 
effective scheduler should take both computation and 
data transfers of multiple jobs into consideration, and 
properly interleaves data transfers and job execution to 
maximize the overall performance. We call such new 
job scheduling data-aware job scheduling.  In this 
line of work, we study job scheduling with shared data, 
replicated data, or shared data access bandwidth, and 
propose novel and effective solutions for each of them. 
 
In [WCL09] we analyze the computational com-
plexity of the job scheduling problem in which the 
jobs share data, with and without storage capacity 
constraint. We found that if there is an upper bound on 
the storage capacity (the maximum number of data 
items that a data server can store and handle), the 
problem is NP-complete even when each job depends 
on at most two data items. On the other hand, if there 
is no upper bound on the storage capacity, we show 
that there exists an efficient algorithm that gives 
optimal job schedule when each job depends on at 
most two data items. We also provide an effective 
heuristic algorithm that produces good schedule for 
cases where there is no limit on the number of data a 
job may access. Experimental results indicate that this 
heuristic algorithm gives near optimal solutions. We 
found two interesting comparison with previous work. 
(SCI).   
[LLWK07] P. Liu, Y.-F. Lin, Jan-Jan Wu and Z.H. 
Kang, "An Optimal Scheduling Algorithm for an 
Agent-Based Multicast Strategy on Irregular Net-
works," The Journal of Supercomputing, volume 42(3), 
pages 283-302, Dec. 2007. (SCI) (Note: Partial result 
of this work was published in the International Con-
ference on Grid and Pervasive Computing 2006 
(GPC’2006), and received Best Paper Nomination.) 
[LW10] W.-C. Liao and Jan-Jan Wu, “Replica-Aware 
Job Scheduling in Distributed Systems”, International 
Conference on Grid and Pervasive Computing, May 
2010 (acceptance rate 29%). 
[WCL09] Jan-Jan Wu, E.-J. Chou and P. Liu, “Com-
putation and Communication Schedule Optimization 
for Data-sharing Tasks on Uni-processors”, accepted 
by Journal of Systems Architecture, 2009 (SCI)  
[WLC10] Jan-Jan Wu, P. Liu, Y.-C. Chung, “Metadata 
partitioning for large-scale distributed storage sys-
tems”, IEEE International Conference on Cloud 
Computing (IEEE CLOUD), Miami, USA, July, 2010 
(acceptance rate 20%).   
[WLL08] Jan-Jan Wu, Yi.-Fang Lin and Pangfeng Liu, 
“Optimal Replica Placement in Data Grid Environ-
ments with Locality Assurance”, Journal of Parallel 
and Distributed Computing (JPDC), Vol.68, No.12, pp. 
1517-1538, Dec, 2008. (SCI) 
[WLWW09] Jan-Jan Wu, Yi.-Fang Lin, Da.-Wei. 
Wang, Chien-Min Wang, “Optimizing Server Place-
ment for Parallel I/O in Switch based Clusters”, 
Journal of Parallel and Distributed Computing 
(JPDC), Vol.69, No.3, pp. 266-281, March 2009.  
[WSLC10] Jan-Jan Wu, Shu-Fan Shih, Pangfeng 
Liu ,Yi-Min Chung (my co-advised MS student), 
“Optimizing Server Placement in Distributed Systems 
in the Presence of Competition”, accepted by Journal 
of Parallel and Distributed Computing (JPDC), 2010. 
(SCI)  
migration, Cloud security, Cloud data management and Cloud management. The Application and Industry 
Track gives insights in practical issues in developing business and service oriented Cloud systems, including 
Cloud security and accounting, Cloud evaluation, Scalable Cloud, Cloud middleware, Cloud composition, 
Cloud framework, Cloud performance study, Cloud applications and case studies.   
 
The keynote speeches are all very interesting, but I enjoyed the panel discussion most.  
The Panel discussion entitled “Trends of services and cloud computing” is informative and inspiring. 
The three panelists Andrzej Goscinski, Deakin University,  Hermant K. Jain, University of Wisconsin 
at Milwaukee, and Dr. Min Luo, IBM Software Group, all gave in-depth discussion on this topic.  
The moderator first outlined a number of desirable goals of cost-effect services and cloud computing, 
including scalability, availability, energy efficiency, usability, application development, performance 
and real-time, reliability/dependability, accuracy, privacy/security, trustworthiness.  
 
Prof. Goscinski at Deakin University outlines the top 10 technologies for 2010. The first is 
virtualization, the second is business intelligence, and Cloud computing has become the third. Analysts 
believe tat 35% of global enterprise IT budgets will be spent on Cloud services. Furthermore, with the 
move of HPC to clouds the percentage will go up. The implication is – Cloud Computing is here to 
stay.  Cloud computing challenges: virtualization, security/privacy/trust, resource 
metering/pricing/billing, QoS/SLA/provisioning on demand, utility and risk management, reliability, 
energy efficiency, scalability, programming environment and application development (Should we go 
for completely new programming model or stay with legacy codes?). About future of service 
computing. In Prof. Goscinski’s opinion, everything will be provided as a service. There is a need for 
standardization even beyond Web services to ensure interoperability, usability and for making Web 
services stateful. There is also a need for automatic publication/exposure of resource state via WSDL 
documents. There will be less attention to service construction and more attention on how to publish 
and discover required services, how services workflows are created dynamically, and how service 
agreements are brokered for both individual services. About the future of cloud computing: 
Virtualization will form a basis of all cloud environment. Cloud computing will all but replace the 
existence of the Internet. Conclusion: We can achieve what the analysts believe! 
 
Professor Jain at University of Wisconsin at Milwaukee looks at this problem from the analogous 
between Cloud computing and electric utility. At the beginning, he questioned: Is this just a passing 
fad or will cause paradigm shift? What will it take to sustain this trend? What are the opportunities and 
challenges?  Can cloud adopt electric utility model?  His answer is not yet, because Cloud 
computing is more complex -- lack of standards, variety of interface requirements, variety of 
performance measurements required.  Comparison to electric utility: IaaS is closer, PaaS is much 
more complex, SaaS can be looked as appliances and equipments using electricity. But the need and 
interfaces of applications has many variations and complexity. Therefore, the technology issues will be 
mostly the same as the opening remarks given by the moderator. Business issues: new measures are 
required to evaluate service and QoS. Conclusion: Change is mind set.  
 
□(5)論文壁報發表 
□(6)評論人 
□(7)主辦人 
     
2. 學術交流、合作事項: 
  
3.  重要殊榮、論文獲獎、研究成果獲得獎項: 
  
註：請申請人印出本表親自簽名，並由所屬單位主管審閱及簽章後，連同第一頁出國報
告表交由各單位承辦人掃瞄為 PDF 檔，上傳至本院，方為完成報告繳交。 
      填表人簽章                      所屬單位主管審閱簽章  
      填表日期 
第二頁 
The goal is to minimize the number of transitions between
servers. To this end, we propose a dynamic programming ap-
proach that partitions the metadata tree so that the maximum
cost among servers is minimized. We also conduct extensive
experiments. The results indicate that dynamic programming
is every effective in minimizing the workload imbalance
among metadata servers.
The remainder of the paper is organized as follows.
Section II contains a review of related works. In Section III,
we formally define the proposed model and the partitioning
problem. In Section IV, we present our dynamic program-
ming approach. Section V details the experiment results, and
Section VI contains some concluding remarks.
II. RELATED WORK
A significant number of works in the literature are de-
voted to tree partitioning. The objectives of these works
can be classified into three categories – to maximize the
minimum values of partitioned subtrees, to minimize the
maximum values of partitioned subtrees, or to partition a
tree according to certain constraints. Hereafter, we refer
to them as max-min, min-max, and constrained objective
functions respectively. For the max-min objective function,
a tree is partitioned into several parts, each of which is given
a value. The goal is to partition the tree so that the minimum
value among the parts is maximized. Perl and Schach [13]
proposed an O(k2rd(T )+kn) time algorithm that partitions
a tree T by removing k edges, where rd(T ) is the number
of edges in the radius of T ; and Frederickson [5] proposed
a linear time algorithm that partitions a tree by removing k
edges so as to maximize the minimum-weight component.
For the min-max objective function, a tree is also par-
titioned into several parts, but the goal is to minimize the
maximum value of the parts. For example, Becker et. al. [3]
proposed an O(k3rd(T )+kn) time algorithm that partitions
a tree T by removing k edges, where rd(T ) is the number of
edges in the radius of T . Kanne and Moerkotte [9] proposed
a partitioning model called sibling partition, which considers
the subtrees that are siblings as one part, and proposed a
linear time algorithm that optimally partitions an ordered,
labeled, weighted tree such that each partition does not
exceed a fixed weight limit.
Under the third type of approach, a tree is partitioned
according to two constraints. The first constraint involves
finding the minimum number for partitions for a tree with
multiple weight functions so that, for each weight function,
the sum of the weights of the parts is less than a given
threshold. It has been shown that, even for binary trees or
for two weight functions, the problem is NP-Complete [6].
Hamacher et. al. [8] proposed a worst-case running time
O(n3 logn) algorithm to solve the partitioning problem if a
tree is binary and the number of weight functions is two. The
second constraint involves partitioning a tree in which both
the nodes and edges have weights. The goal is to minimize
the sum of the weights of the cutting edges in the tree, while
ensuring that the sum of all the nodes in each partitions
is bounded by a given threshold. Lukes [10] proposed an
O(W 2n) time algorithm to solve this problem, where W is
the maximum number of partitions in an optimal solution
and n is the number of nodes.
Our objective function is different from those in previous
works, which evaluate each partition separately. By contrast,
in our model, the evaluation of one partition depends on
other partitions. We elaborate on this aspect in the next
section.
III. SYSTEM MODEL
In this section, we formally define our system model and
the optimization problem. First, we consider a general model
for answering queries; then we focus on the metadata tree
partitioning problem.
We use a graph to model a query answering process. Let
G be a graph and P be a set of paths on it. The graph can
be thought of as a query answering system, and the paths
are the reasoning process of a query. A query enters the
system at the node where the path starts, travels along the
path one node at a time until it reaches the node where it can
be answered. During this process, the system may need to
consider additional information; however, a query can still
be represented by a path on the graph.
Since using one server to answer all queries is time
consuming, we use k servers to distribute the query answer-
ing workload. Specifically, we distribute the workload by
labeling each node v with a number from 1 to k, so that
each server is responsible for the nodes that have the same
label as the server id.
1
1
1
1
12
2
2
2
b
a
c
d
e
Figure 2. An example of a query graph. A query starts from node a,
passes through nodes b, c, d, and stops at node e, where it is answered.
The numbers next to the nodes are the assigned labels.
Since it is expensive to transfer a query from one server to
another, we denote an edge as a jump edge if it connects two
nodes that are labeled with different numbers (i.e. managed
by different servers). We define the cost of a query to be
j+1, where j is the number of jump edges the query passes
v to the root r. Then, the cost of the queries can be defined
as c(v, J) = q(v)× (j(v, J)+1), where the additional jump
edge is the implicit edge at the root r. Let Tv denote the
subtree rooted at v. Similarly the cost of the subtree Tv,
denoted by c(Tv, J), is the sum of the costs of the nodes in
Tv.
Let consider node v in Figure 3. We assume that the jump
edge set J has only one edge (w, v) since the edge at the
root is implicit. The number of queries that end at v is 5,
i.e., q(v) = 5. In the figure, number of jump edges in J
from v to r is 1, i.e., j(v) = 1. Therefore, the cost of node
v is c(v, J) = 5× (1 + 1) = 10; and the cost of subtree Tv
is c(Tv, J) = (5 + 2 + 2)× (1 + 1) = 18.
We formally define metadata tree partitioning problem as
follows. Given a metadata tree T and the number of queries
q, find a set of k jump edges J so that the maximum cost
among all k + 1 subtrees is minimized. Formally, we have
the following objective function, where r is the root of T
and F (J) is the forest set of k + 1 subtrees when T is
partitioned by J .
min
J
max
t∈F (J)
c(t, J) (1)
The proposed query tree partitioning model is very differ-
ent from the traditional tree partitioning models discussed in
Section II. In traditional tree partitioning, the cost function
is defined on a subgraph and depends on one subtree only.
By contrast, in our model, the cost function of a subgraph
depends on the nodes in that subgraph and on how other
parts of the tree are partitioned.
IV. DYNAMIC PROGRAMMING
First, we define some additional notations as shown in
Table I. Recall that Tv denotes a subtree rooted at a node v.
We use T ′v,J to denote the subtree in Tv that only connects
to v by normal edges when the jump edge set is J . In other
words, if we remove the jump edge set J from E, T ′v,J is
the set of nodes in Tv that are still connected to v. Moreover,
after we chose a jump edge set J , the subtree Tv will be
partitioned into a forest of trees Fv,J , which includes T ′v.
We use F ′v,J to represent the forest F
′
v,J = Fv,J − T ′v,J ;
that is, the trees that are separated from T ′v by J . For ease
of presentation, we drop the subscript J and use T ′v, Fv, and
F ′v when the context clearly indicates the jump set J .
Figure 4 illustrates a subtree rooted at node v. The subtree
T ′v,J consists of nodes v, w, u, and z because they are
connected to v by normal edges. The forest Fv,J consists
of T ′v,J , Tx, and Ty, since T
′
v,J is connected to the other
two trees by jump edges. As a result, the forest F ′v,J only
consists of Tx and Ty only,
A. Cost function
We now describe the core function of our dynamic
programming approach. The function C(v, q, j, k) is the
jump edgejump edge
T’v,J
v
w
x y z
u
Figure 4. A metadata subtree rooted at v. The subtree has two jump edges,
and the forest Fv,J is comprised of three trees T ′v,J , Tx, and Ty .
T = (V, E) The metadata tree
r The root of T
J Jump edge set
q(v) The number of queries that end at node v
j(v, J) The number of jump edges in J that are along the
path from v to r
c(v, J) The cost of node v under jump edge set J
Tv The subtree rooted at v
c(Tv, J) The cost of Tv under jump edge set J
T ′v,J (or T
′
v ) The subtree in Tv that only connects to v by normal
edges under jump edge set J
Fv,J (or Fv) Forest derived by using J to partition Tv
F ′v,J (or F
′
v) Fv,J excluding T
′
v,J
Table I
A NOTATION SUMMARY
minimum possible cost from trees in forest F ′v , such that
J has j jump edges along the path from v to the root r, T ′v
has at most q queries, and Tv contains k jump edges. Note
that the queries in T ′v are not counted in the C function
because we only count the contribution of the forest F ′v.
Next, we describe the recursive calculation of the C
function. For simplicity, we assume that the metadata tree T
is binary, since we can convert any general tree into a binary
tree with a constant factor size increase. We discuss this
aspect further in Section IV-F. To explain the calculation,
we consider a node v and its two children w and x. There
are three cases: v is connected to its children by one jump
edge and two jump edges, and v is not connected by any
jump edge.
B. One Jump Edge
First, we consider the case where v is connected to one
of its children by one jump edge. Without loss of generality,
we assume that v is connected to w by a jump edge and to
x by a normal edge. In this case C(v, q, j, k) would be the
best solution considering both Tw and Tx.
The contribution of Tw will be from T ′w or F ′w. The former
can be calculated from the number of queries in T ′w, since
w is connected to v by a jump edge and w is not in T ′v.
The latter is C(w, qw , j + 1, kw) if we assume that qw is
the number of queries in T ′w and kw is the number of jump
edges in Tw.
j jump edges to the root
C(x,q−q(v)−qw,j,k−kw)
C(q,qw,j,kw)
qw quires
kw jump edges
k−kw−1 jump edges
q−q(r)−qw queries
v
w x
Figure 7. A metadata subtree rooted at node v, which has two normal
edges to its children.
C(v, q, j, k)
= C2(v, q, j, k), when q = q(v)
= min(C0(v, q, j, k), C1(v, q, j, k)), otherwise
The solution to our metadata tree partitioning problem
can be stated as shown in Equation 8. The numeral 1 in
C′ indicates the implicit jump edge at r, which is the root
of the tree, and qr is the possible bound on the number of
queries in T ′r induced by the jump edge set.
C′(r, 1, k) = min
qr
(max(qr, C(r, qr, 1, k))) (8)
F. General Trees
Next, we generalize the dynamic programming techniques
to general trees. We transform the subtree of a parent p and
its children c1, . . . , cn into a binary tree such that is p the
root and c1, . . . , cn are the leaves. Moreover, we will add
n−2 dummy nodes, and set the number of the query function
q to 0 for those nodes. Note that we do not select the n− 2
edges that connect the dummy nodes and their parents as
jump edges. During the dynamic programming phase, this
constraint can only be enforced by applying the “zero-jump-
edge” case for nodes with dummy children (as described in
Section IV-D). The dynamic programming techniques can
now be applied to the binary tree.
G. Time Complexity
Our analysis of the complexity of the proposed dynamic
programming technique focuses on the computation of the
C function because it dominates the execution time. We
assume that the total number of queries per node is Q and
the maximum total number of queries in the tree is O(nQ),
where n is the number of nodes. Consequently, there are
O(n2Qk2) entries of C to compute. For each entry we need
to consider three cases, as described in Sections IV-B, IV-C,
and IV-D.
Actually, case of zero jump edges discussed in Sec-
tion IV-D dominates the execution time. To compute
C(v, q, j, k) according to Equation 7, we need to enumerate
all the possible qw and kw, i.e., the number of queries and
the number of jump edges in a subtree. These two quantities
are bounded by nQ and k respectively. Combined with the
fact that we have O(n2Qk2) C function values to compute,
we conclude that the total execution time is bounded by
O(n3Q2k3).
H. Binary Search
We now derive an optimization for the calculation of
Equation 7 when there are zero jump edges. That is, we
want to reduce the time required to compute Equation 7,
which we rewrite as follows:
C0(v, q, j, k) = min
qw ,kw
(max(C(w, qw , j, kw),
C(x, q − q(v)− qw, j, k − kw))) (9)
k
qw
C(w, qw, j, 0)
C(x, q−q(v)−qw, j, k)
k
Figure 8. The computation pattern of Equation 9
An important observation about the definition of C is
that C is a decreasing function of q. Therefore, Equation 9
actually computes the minimum of the maximum of one
increasing function, and one decreasing function. Now let
us focus on the case where kw is 0, as shown in Figure 8.
Equation 9 now becomes:
min
qw
(max(C(w, qw , j, 0), C(x, q − q(v)− qw, j, k))) (10)
Equation 10 can be computed easily in O(log q) time.
This can be computed by a binary search on qw to de-
termine where the two functions have almost the same
function value. As this technique can be applied to any
k, Equation 9 can now be computed in O(k(log(nQ)) =
O(k(log n+logQ)) time. Therefore, the total time complex-
ity is O(n2Qk2 × k(logn + logQ)) = O(n2Qk3(log n +
logQ))
Theorem 2: Given a binary metadata tree T comprised of
n nodes, the number of query function q, a constant k, there
exists an algorithm that can find a set, J , of k jump edges in
O(n2Q2k3(logn+ logQ)) time, where Q is the maximum
number of queries assigned to a node by the q function, so
that the maximum cost of all k + 1 subtrees is minimized.
Note that Theorem 2 is also valid for general trees
because, according to the construction in Section IV-F,
the number of tree nodes increases by a constant factor.
Moreover, in practice, the maximum number of queries per
 0
 1e+09
 2e+09
 3e+09
 4e+09
 5e+09
 6e+09
 7e+09
 8e+09
 0  2  4  6  8  10
# 
of
 lo
ok
up
s 
of
 ta
bl
e 
C
k (# of jump edges)
Metadata-tree partition in algorithm and # of jump edges
DP w/ binary search
DP w/o binary search
Figure 11. The effect of the number of jump edges on the number of
table lookups in a complete tree.
 0
 5e+06
 1e+07
 1.5e+07
 2e+07
 2.5e+07
 3e+07
 3.5e+07
 4e+07
 0  10  20  30  40  50  60  70  80  90  100
N
um
be
r 
of
 L
oo
ku
ps
Maximum depth
Metadata-tree partition in different depth with same number of nodes
DP w/ binary search
DP w/o binary search
Figure 12. The effect of the tree shape on the number of table lookups.
each tree is roughly ten. Then, we use the two algorithms to
select 10 jump edges for the trees, and record the number
of table lookups in the algorithms for different tree depths.
When the tree depth is 100, i.e., the tree is completely
skewed, we find that the number of table lookups is almost
the same in the two implementations. The reason is that,
since the tree is completely skewed, optimization by the
binary search is not feasible. We also observe that the binary
search method is most effective when the tree depth is
around 20. Overall the binary search method performs much
better than the naive implementation approach.
VI. CONCLUSION
We have proposed an optimal partitioning algorithm for
metadata trees. The paper focuses on how to partition a
hierarchical metadata structure so that the query workload
is evenly distributed among multiple metadata servers. We
take the number of transitions from one server to another as
the cost of a query, and propose a dynamic programming ap-
proach that partitions the metadata tree so that the maximum
cost among servers is minimized. In addition, we propose
a binary search based optimization techniques to further
reduce the time complexity of dynamic programming.
The results of extensive experiments indicate that dynamic
programming is very effective in minimizing the workload
imbalance. The effects of the number of tree nodes, the
number of jump edges, the maximum depth of the tree,
and the maximum number of queries reaching a node are
analyzed. We show that our dynamic programming method,
enhanced by a binary search, performs much better than a
naive exhaustive search approach.
REFERENCES
[1] Amazon. S3: Simple storage service, 2010.
[2] Chaitanya Baru, Reagan Moore, Arcot Rajasekar, and Michael
Wan. The sdsc storage resource broker. In In Proceedings of
CASCON, 1998.
[3] Ronald I. Becker, Stephen R. Schach, and Yehoshua Perl. A
shifting algorithm for min-max tree partitioning. J. ACM,
29(1):58–67, 1982.
[4] Bram Cohen. Incentives build robustness in bittorrent, 2003.
[5] Greg N. Frederickson. Optimal algorithms for tree parti-
tioning. In SODA ’91: Proceedings of the second annual
ACM-SIAM symposium on Discrete algorithms, pages 168–
177, Philadelphia, PA, USA, 1991. Society for Industrial and
Applied Mathematics.
[6] Michael R. Garey and David S. Johnson. Computers and
Intractability; A Guide to the Theory of NP-Completeness. W.
H. Freeman & Co., New York, NY, USA, 1990.
[7] Google. Gdrive on-line data storage, 2010.
[8] Anja Hamacher, Winfried Hochsta¨ttler, and Christoph Moll.
Tree partitioning under constraints - clustering for vehicle
routing problems. In Proceedings of the 5th Twente workshop
on on Graphs and combinatorial optimization, pages 55–69,
Amsterdam, The Netherlands, The Netherlands, 2000. Elsevier
Science Publishers B. V.
[9] Carl-Christian Kanne and Guido Moerkotte. A linear time
algorithm for optimal tree sibling partitioning and approxima-
tion algorithms in natix. In VLDB ’06: Proceedings of the
32nd international conference on Very large data bases, pages
91–102. VLDB Endowment, 2006.
[10] J. A. Lukes. Efficient algorithm for the partitioning of trees.
IBM Journal of Research and Development, pages 217–224,
1974.
[11] Nirvanix. Cloud storage solutions for the enterprise, 2010.
[12] B. Pawlowski, C. Juszczak, P. Staubach, C. Simith, D. Lebel,
and D. Hitz. Nfs: Design and implementation. In Proc. Usenix
Summer Technical Conf., 1994.
[13] Yehoshua Perl and Stephen R. Schach. Max-min tree parti-
tioning. J. ACM, 28(1):5–15, 1981.
[14] D. Roselli, J. R. Lorch, and T. E. Anderson. A comparison
of file system workloads. In Proc. Ann. Usenix Technical
Conference, June 2000.
[15] M. Satyanarayanan, J.J. LKistler, P. Kumar, M.E. Okasaki,
E.H. Siegel, and D.C. Steere. Coda: A highly available file
system for distributed workstation environments. IEEE Trans.
Computers, 39(4), 1990.
[16] Powering Cloud Storage. Parascale cloud storage, 2009.
96年度專題研究計畫研究成果彙整表 
計畫主持人：吳真貞 計畫編號：96-2628-E-001-004-MY3 
計畫名稱：資料格網環境下之資源管理最佳化 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 6 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 7 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
