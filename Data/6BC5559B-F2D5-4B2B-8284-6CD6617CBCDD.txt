 
 
 
行政院國家科學委員會補助專題研究計畫成果報告 
The research published the following five SCI papers 
Ching-Liang Su, “The Extraction and Comparison of Finger 
Edges for Person Identification,” JOURNAL OF INTELLIGENT 
MANUFACTURING, USA, April 2006, pp. 233-241, Vol. 17, No. 
2, ISSN 0956-5515, NSC 93-2213-E-212-011, EI, SCI (impact factor 
0.595) 
Ching-Liang Su, “Original Finger Image Extraction by 
Morphological Technique and Finger image Comparisons for 
Persons’ Identification,” Journal of Intelligent and Robotic 
Systems, Netherlands, January 2006, pp. 1-14, vol. 45, No. 1, 
ISSN 0921-0296, NSC 93-2213-E-212-011, EI, SCI (impact factor 
0.254) 
Ching-Liang Su, “Hand Image Recognition by the Techniques of 
Hand Shape Scaling and Image Weight Scaling,” EXPERT 
SYSTEMS WITH APPLICATIONS, May 2007, ISSN 0957-4174, 
NSC 95-2221-E-212 –003, EI, SCI (Accepted, impact factor 0.688) 
Ching-Liang Su, “Finger Extraction, Finger Image Automatic 
Registration, and Finger Identification by Image Phase Matching,” 
Applied Mathematics and Computation, October 2006, ISSN 
0096-3003, NSC 95-2221-E-212-003, EI, SCI (Accepted, impact 
factor 0.688) 
Ching-Liang Su, “Hybrid Finger Geometry Signal Processing and 
Finger Shape Comparisons for Person Identification,” 
INFORMATICA, Vol. 18, No. 3, 1–10, 2007, ISSN 0868-4952, 
NSC 95-2221-E-212-003, EI, SCI (impact factor 0.456) 
 
 
 
 
In this research, the new technique is used to extract the index, middle and the ring 
fingers to do the person’s identification. The orientations of the index, middle and ring fingers are 
calculated. The geometry features of index, middle and ring fingers are compared to identify 
different person. The technique – database SQL searching and manipulating, image dilating, 
object-position-finding, image shifting, image rotating, and image interpolating is used to do the 
person’s recognition. The hand are fixed each time when a picture is taken and one can assume 
that each time when the index, middle or the ring finger image is taken, the acquired index, 
middle or the ring finger image is the same as the previous acquired index, middle or ring finger 
image. Since the pictures are the same, after the index, middle or ring fingers are extracted from 
the hand image, one can use the acquired index, middle or ring finger image to identify different 
persons.  
This report consists of five sections. Section 2 shows the process to acquire the 
hand-image. Section 3 explains the process to extract the index, middle, and ring fingers. Section 
4 performs the image registration and image subtraction. Section 5 concludes this report.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3. The extraction of the index, middle, and ring fingers. 
 
By using the geometry feature of the fingertips and finger roots, one can extract the index, 
middle, and ring fingers. The extracted results are shown in figure 3.1.  
 
4. Image registration and image subtraction. 
 
The object’s centroid can be defined as (ic, jc). In equations 4.1 and 4.2, idistance represents the 
i-distance of a specific pixel g(i, j) to the object’s centroid (ic, jc) and  jdistance represents the 
j-distance of a specific pixel g(i, j) to the object’s centroid (ic, jc). Weightij represents the gray 
level of a specific pixel g(i, j). By using equation 4.3, the object’s orientation can be obtained. In 
equation 4.4, θdifference represents the difference of the orientations between two images. After 
performing the function of equation 4.4, a specific pixel in location (i, j) would be rotated to 
position (if, jf). After the image rotating, shifting, and interpolating, two finger images are 
overlapped. Since both finger images are overlapped, image subtraction can now be applied to 
compare the difference of these two finger images. By the subtracted result, one can identify the 
different persons. Figures 4.1 shows the image subtractions of the finger images.  
 
5. Results and conclusions. 
 
In this research, one person needs to place his hand at three different positions for the photos 
taken. Totally three photos are taken for each person. In this research, totally ten persons 
participate in the experimental test. These ten persons are divided into two groups. Each group 
has 5 persons. Since one person has been taken three different photos, totally fifteen photos are 
taken for each group. To each group, one hundred and five comparisons are conducted to find the 
accurate-identification-rate of the developed identification algorithm. Within those one hundred 
and five comparisons, fifteen comparisons are conducted for self-comparison – since one person 
taken three different photos. The data of these fifteen comparisons are shown inside the 
rectangular boxes in figure 5.1. The other ninety comparisons are conducted for comparisons 
between two different persons’ images. These data are also shown in the figure 5.1. The 
comparisons of the same person’s finger images yield much less values; however, the 
comparisons of the different person’s finger images yield much greater values. For the first group 
test, the error rate for the index finger is 22%, the middle finger is 9%, and the ring finger is 11%. 
For the second group, the error rate for the index finger is 35%, the middle finger is 29%, and the 
ring finger is 15%. 
 
 
 
 
 
 
 
 
 
References 
1. Karen O Egiazarian, S. Gonzalez Pestana, “Hand shape identification using neural networks,” The International 
Society for Optical Engineering, v 4667, 2002, p 440-448 
2. Miguel A. Ferrer, Carlos M. Travieso, and Jesus B. Alouso, “Using hand knuckle texture for biometric identifications,”
IEEE A&E Systems Magazine, June 2006 
3. Chin-Chuan Han, “A hand-based personal authentication using a coarse-to-fine strategy,” Image and Vision 
Computing, Volume 22, 2004, pp 909–918  
4. 
 
Chin-Chuan Han, Hsu-Liang Cheng, Chih-Lung Lin and Kuo-Chin Fan, “Personal authentication using palm-print 
features,” Pattern Recognition, 36, 2003, pp 371-381 
5. Bing He, Zheng-Ding Qiu, Dong-Mei Sun “Secure authentication system incorporating hand shapes verification and 
cryptography techniques,” IEEE Region 10 Annual International Conference, Proceedings/TENCON, v 1, 2002, p 
156-159  
6. 
 
D. G. Joshi, Y. V. Rao, S. Kar, V. Kumar, “Computer vision based approach to personal identification using finger 
crease pattern,” Pattern Recognition, 31, 1998, pp 15-22 
7. 
 
Ajay Kumar, David C. M. Wong, Helen C. Shen, “Personal Verification Using Palmprint and Hand Geometry 
Biometric,” Lecture Notes in Computer Science, Springer-Verlag Heidelberg, Volume 2688, 2003, pp 668-678 
8. 
 
Wenxin Lia, David Zhang, and Zhuoqun Xub, “Image alignment based on invariant features for palmprint 
identification,” Signal Processing: Image Communication, Volume 18, Issue 5, May 2003, Pages 373-379 
9. Chih-Lung Lin and Kuo-Chin Fan, “Biometric Verification Using Thermal Images of Palm-Dorsa Vein Patterns,” 
IEEE Transactions on Circuits and Systems for Video Technology, vol. 14, no. 2, Februaty 2004  
10. 
 
Guangming Lua, David Zhang, and Kuanquan Wanga, “Palmprint recognition using eigenpalms features,” Pattern 
Recognition Letter, Volume 24, Issues 9-10, June 2003, Pages 1463-1467 
11. YingLiang Ma, Frank Pollick and W. Terry Hewitt, “Using B-Spline Curves for Hand Recognition,” Proceedings of the 
17th International Conference on Pattern Recognition (ICPR’04), 2004 
12 Sotiris Malassiotis, Niki Aifanti, and Michael G. Strintzis, “Personal Authentication Using 3-D Finger Geometry,”
IEEE Transactions on Information Forensics and Security, vol. 1, no. 1, March 2006  
13. Aya Mitome and Rokuya Ishii, “A Comparison of Hand Shape Recognition Algorithms,” The 29th Annual Conference 
of the IEEE Industrial Electronics Society, Nov 2-6 2003  
14. 
 
Cenker Oden, Aytul Ercil, and Burak Buke, “Combining implicit polynomials and geometric features for hand 
recognition,” Pattern Recognition Letter, Volume 24, Issue 13, September 2003, Pages 2145-2152 
15. 
 
R. Sanchez-Reillo, Sanchez-Avila, and Gonzales-Marcos, “Biometric identification through hand geometry
measurements,” IEEE Trans Pattern Anal Mach Intell, Volume 22, No. 10, 2000, Pages 1168-1171 
16. Ching-Liang Su, “Technique for Person’s Identification: Using the Extracted Index Finger Image to Identify 
Individuals,” Journal of Intelligent and Robotic Systems, Netherlands, July 2003, pp. 337-354, vol. 37, No. 3 
17. Dong-Mei Sun, Zheng-Ding Qiu “Automated hand shape verification using HMM,” The 7th International Conference 
on Signal Processing Proceedings (ICSP'04), 2004, p 2274-2277  
18. Wei Xionga, Kar-Ann Toha, Wei-Yun Yaua, and Xudong Jiangb, “Model-guided deformable hand shape recognition 
without positioning aids,” Pattern Recognition, Volume 38, 2005, Pages 1651 –1664 
19. Wei Xiong, Changsheng Xu, and Sim Heng Ong, “PEG-FREE HUMAN HAND SHAPE ANALYSIS AND 
RECOGNITION,” ICASSP, 2005, Pages 77 –80 
20. Erdem Yoruk, Helin Dutagaci, and Bulent Sankur, “Hand biometrics,” Image and Vision Computing 24 (2006) 
483–497  
21. Erdem Yörük, Ender Konuko˘Glu, and Bülent Sankur, “Shape-based hand recognition,” IEEE Transactions on Image
Processing, vol. 15, no. 7, July 2006  
22. Jane You, Wenxin Li, and David Zhang, “Hierarchical palmprint identification via multiple feature extraction,” Pattern 
Recognition, Volume 35, Issue 4, April 2002, Pages 847-859 
 英文： 
1. Put the hand on one certain position. 
2. Control the illumination. 
3. Take the hand image picture. 
4. Find the positions of the entire fingertips and entire 
finger-to-finger-valleys. 
5. Extract the index, middle, ring, thumb, and ring fingers, separately.
6. Extract the palm image with the entire fingers truncated. 
7. Use the inertial invariant technique to find the position and 
orientation of the finger images.   
8. Align the images to the same position by performing the image 
shifting and image rotation. 
9. Interpolating the non-integer point of the rotated image. 
10. Finger image scaling and magnifying. 
11. Place the entire finger images in one image file. 
12. Extract the finger image separately from the hybrid image file. 
13. Perform Image Small Window Convolution and image comparison 
to the finger images to identify the finger images. 
14. Expand the signal salient feature by the wavelet transform. 
15.Using the correlation technique to identify the finger images. 
Sorting the subtraction result and perform the statistic evaluation to 
identify different persons 
可利用之產業 
及 
可開發之產品 
Company for implement the biometric identification machine 
技術特點 
Automatically extract the finger image and automatically identify the 
image. 
推廣及運用的價值
Implement the person identification machine 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
234 J Intell Manuf (2006) 17:233–241
Fig. 1 The designed shelves for
taking the hand images
Fig. 2 Obtain the thinned-edges
of the hand images
Extract the finger images
Figure 1 shows the shelves for taking the hand-images. The
illuminations are from the left lateral and the bottom of the
shelves. By adjusting the lights one can control the illumina-
tions to the hand-image. In the middle shelf, several pegs are
used to peg the person’s hand to a certain position. This will
make the hand inertial when the hand-image is taken. Figure 2
shows the hand images and the extracted hand-edge images.
Figure 3 shows the position-features of the fingertips and fin-
ger-to-finger-valleys. By using these position-features, one
can extract the entire—thumb, index, middle, ring, and small
fingers. The entire images are shown in Figs. 4, 5, and 6. The
finger extracting technique is shown in reference Su (2003).
Geometry descriptor
For every pixel (i, j) in the object, after the transformation
of the geometry descriptor, the final destination of (i, j) is
(i f , j f ). The geometry descriptor will transfer and interpo-
late the geometry features of every pixel inside the object
to another feature-domain. The original geometry feature of
the finger image will be preserved, after the object is trans-
fer to the new feature-domain. In the new feature-domain,
every object will have the same straight orientation and every
object’s centroid is aligned to position (64, 64). Since every
Fig. 3 Features of the fingertips and finger-to-finger-valleys
object has the same orientation and every object is aligned
to the same centroid, one can perform the image subtrac-
tion to perform the person’s identification. Equation 1 shows
the function of the geometry descriptor. In Eq. 1, “θ” repre-
sents the object’s orientation and “Centroid” represents the
object’s centroid. By using Eqs. 2–4, one can find the param-
eter θ . In Eqs. 2 and 3, idistance represents the i—distance
of a specific pixel g(i, j) to the object’s centroid, jdistance
represents the j—distance of a specific pixel g(i, j) to the
object’s centroid, and Weighti j represents the weight of a
specific pixel g(i, j).
236 J Intell Manuf (2006) 17:233–241
Fig. 9 The result of the
finger-edge after performing the
geometry descriptor
Fig. 10 The result of the
finger-edge after performing the
geometry descriptor
Fig. 11 The result of the
finger-edge after performing the
geometry descriptor
(i, j, θ, centroid) →
Geometry
Descriptor
(i f , j f ) (1)
Var1 =
N∑
j=1
N∑
i=1
(i−distance)2 · Weighti j (2)
Var1 =
N∑
j=1
N∑
i=1
2 · idistance · jdistance · Weighti j (3)
θ =
Sin − 1
∑N
j=1
∑N
i=1 2·idistance· jdistance·Weighti j√
Var22+Var12
2
(4)
Hybrid geometry signals and image subtraction
Figures 12 and 13 show the hybrid geometry signals. The
hybrid geometry signals are the composed signals of—the
thumb, index, middle, ring, and small fingers. The signals
of—the thumb, index, middle, ring, and small fingers are
added together to generate the hybrid geometry signals. Since
the separate finger cannot perform well to perform the per-
son’s identification, the hybrid geometry signals would pos-
sess more fingers’ geometry features to perform the person’s
identification. Finger 14 shows the various hybrid geometry
signals. As mentioned earlier, because of the illumination and
the fingers stretch at different positions, one cannot precisely
extract the fingertip and finger-root. For coping these errors,
the images are fine-tuning to different positions to obtain the
better recognition rate—during the image subtraction stage.
238 J Intell Manuf (2006) 17:233–241
Fig. 16 Sub-pattern template convolution to find the minimum pixel
value
First, the image is shifted at different positions—to which
are—left four pixels, left three pixels, left two pixels, . . .
etc, right four pixels. Also, the image is shifted—top four
pixels, top three pixels, top two pixels, . . . etc, bottom four
pixels. When the images are fine-tuning to one certain po-
sition, the image is also fine-tuning to different orientations
for comparison. The rotated degrees are—counter-clockwise
2 degrees, counter-clockwise 0.8 degrees, counter-clockwise
0.6 degrees, . . .etc, to clockwise 1.8 degrees. Figure 15 shows
the rotated-images, which are rotated from −2.0 degrees to
+1.8 degrees.
After the image rotation, the software will perform the im-
age subtraction. The image subtractions are shown in
Figs. 16 and 17 In the top of Fig. 16, one can find a 5 by 5 sub-
pattern template. This 5 by 5 sub-pattern template will extract
the sub-pattern images of two persons’ geometry hybrid sig-
nals and, consequently, the image convolution is performed
to these two 5 by 5 sub-pattern template images. Fine-tuning
of the image is also performed in this stage and the min-
imum value of the convolution results will be recorded in
the resultant-image. The subtract result image is shown in
the right-hand side picture of Fig. 16. By calculating the
difference of the subtracted-result, one can recognize differ-
ent fingers.
Results and conclusions
In Fig. 17, one can find the original hand-edge images and
the hybrid finger geometry signals. The image subtraction
is performed to the hybrid finger geometry signals and the
results are shown in the right-hand side pictures of the Fig.
17. Figure 17 also shows the value of the difference of the
two subtracted-images. Figure 17 shows four different cases
of image subtractions. All these four cases involve the im-
age subtraction of one person’s hand-image subtracting to
the same person another hand-image. After performing the
image subtraction, the differences of these four cases are—
886850, 825250, 763852, and 834800. Figures 18 and 19
show the partial comparison data of the image difference after
performing the image subtraction. The data inside the rectan-
gular boxes in Figs. 18 and 19 are the image subtraction-result
of one person’s hand-image subtracting to the same person’s
another hand image. The other data, which is not circled
by the rectangular boxes, show the difference between two
different persons’ hand images subtractions. In Fig. 20, one
can find that the error range of the subtracted value of differ-
ent person comparison are 834725–674625. When conduct-
ing one person’s hand-image subtracting to the same person
another hand-image, the error range of the subtracted-value
are 1135550– 834800. In this research, 57 persons’ hand
images are taken. Each person takes three different hand im-
ages. As shown in Fig. 1, the illuminations are adjusted to
provide different illuminations to each hand image. After the
hand images are taken, several problems are found in the
hand images—the middle finger and the ring finger of sev-
eral people hands are stick together or the thumb finger of one
person’s hand-image are stretching too wide. The algorithm
developed in this research cannot correctly extract the middle
and ring fingers correctly when the middle and ring fingers
are sticking together and the algorithm will not extract the
wider-stretched thumb finger either. Since these two prob-
lems will cause the algorithm unable to extract the thumb,
middle, and ring fingers correctly, several images are dis-
carded from the test. Actually, the hand-photo-images used
in this system are only 120, which are belonged to 40 persons.
Regard these 120 photos as whole and run these 120 pho-
tos in one batch—which will take a very long time to get
the job done. Due to the software glitch, the running process
might break before the job completed. Consequently, one
might endlessly run and run the procedure again and again
and every time from the square one. Furthermore, the data-
base might have no enough memory to accommodate the
entire obtained-data. Thus, in this system, five persons are
designated as one group, i.e., five people are regarded as a
group and these five persons are tested in one step. Since one
person has been taken three different photos, totally 15 photos
are tested in each step. As mentioned earlier, in this system,
there are 40 persons participating the test. Thus, the system
totally will run eight different batches. Totally, 120 hand-
photos are tested. For each hand-photo-image, the following
steps are performed (1) the hand-edge is found, (2) the fin-
gers are extracted separately, (3) geometry descriptor is per-
formed to each finger, (4) mixed-finger-signal is generated,
(5) image shifting and rotation are performed, (6) genuine
and imposter comparisons are conducted. To each group, one
photo is compared against the other 14 photos. Totally, 105
comparisons are conducted to test the accuracy of the devel-
oped identification algorithm. Within those 105 comparisons,
15 comparisons are conducted for genuine-comparison, the
comparisons of the same person’s fingers — since one person
taken three different photos. The other ninety comparisons
are conducted for imposter-comparisons — the comparisons
of different person’s fingers. In this research, there are eight
different test batches. Thus, there are 120 genuine tests and
720 imposter tests. Figure 21 shows the comparison results
240 J Intell Manuf (2006) 17:233–241
Fig. 19 The comparison-data
Range ofthe
subtracted-value
Error range of
the subtracted-
value
Error
(Times)
Error
rate
Different person
comparison
Same person
comparison
2022075
~
674625
1135550
~
544150
834725
~
674625
113550
  ~
834825
123
12
840:123
840:12
120 different photographs; 840 comparisons; 834800 asthethreshold
Fig. 20 Experimental data
Fig. 21 Response character of
genuine and imposter
comparisons-120 different
photographs; 840 comparisons;
834800 as the threshold
Subtracted-Value
2022075
1135550
544150
834800
834725
674625
Threshold Threshold
Correct
(Times)
597
Error
(Times)
123
Imposter Comparison
Error
(Times)
12
Correct
(Times)
108
Imposter Comparison
(Total Times)
720
Genuine Comparison
(Total Times)
120
Genuine Comparison
Original Finger Image Extraction by Morphological
Technique and Finger Image Comparisons for
Persons’ Identification
j
CHING-LIANG SU
Department of Industrial Engineering & Technology Management, Da Yeh University,
112 Shan-Jeau Road, Da-Tsuen, Chang-Hua, Taiwan 51505; e-mail: cls2@mail.dyu.edu.tw
(Received: 14 August 2004; in final form: 16 June 2005)
Abstract. This research uses the object extracting technique to extract the index, middle and ring
fingers from the hand images. The algorithm developed in this research can find the precise
locations of the different fingers’ fingertips and the finger-to-finger-valleys. After finding the
positions of the fingertips and finger-valleys, the index, middle and ring fingers can be extracted
from the hand images by using morphological technique. The extracted index, middle and ring
fingers contain many useful geometry features. One can use these features to do the person’s
identification. The orientations of the index, middle and ring fingers are found in this research.
Image rotating, image shifting, and image interpolating techniques are used to align different
persons’ index, middle and ring fingers. Image subtraction is used to exam the difference of two
index, middle and ring finger images. In this research so far only use the index, middle and the ring
fingers as the features to identify different persons.
Key words: dilating, finger shape extraction, finger shape identification, morphology, object curve
comparison.
1. Introduction
In the past 20 years, researchers invested a lot of effort to develop different
techniques to identify the hand images. This past work includes Y hand geometry
[2, 5, 12, 16], middle finger crease pattern matching [6], various finger size
measurements [7, 14], various finger lateral view size measurements [14], vein
pattern [9], eigenpalm [10], implicit polynomials [13], algebraic invariants [13],
Karen invariant computation [13], line interception and slope comparisons [17],
control point selection [8, 17], coarse to fine strategy [3], B-Spline [11], wa-
tershed transform [9], HMM [16]; However, some are very sensitive to the noise
[6, 17]; some have very complicated mathematical models [13] and some have
very complicated neural training algorithms. In this research, the new technique
is used to extract the index, middle and the ring fingers to do the person’s
identification. The orientations of the index, middle and ring fingers are cal-
j This work was supported by National Science Council under grant NSC 93-2213-E-212-011.
Journal of Intelligent and Robotic Systems (2006) 45: 1Y14 # Springer 2006
DOI: 10.1007/s10846-005-9007-3
are fixed with interval of five pixels. When A is moved, the distance BC between
points B and C is calculated. Consequently, various distances BC of points B
and C will be found. By checking the center picture in Figure 1, one can
find the distances BC are less than 13 Y since A is located in the positions around
the finger-to-finger-valley. Various distances BC are compared; the point A with
the minimum distance of BC will represent the index-middle-fingers-valley. The
center and right-hand-side pictures in Figure 1 are extracted from Microsoft
Access database. The center picture shows how to find the index-middle-fingers-
valley. The right-hand-side picture shows how to find the middle-ring-fingers-
valley.
In this research, the finger-ends are used to identify different persons. Figure 2
shows by using the finger root Y (i.e., finger-to-finger-valley) as the beginning
searching point and performing the bottomYup searching, one can obtain the dif-
ferent fingers’ finger-ends. When the algorithm performs the bottom-up searching
and when the algorithm detects the straight finger edge, the algorithm will mark
this point as the extracting point Y which late the algorithm will use this point to
extract finger-ends. In this research, when the distance BC is the greater than 20,
the algorithm will mark this point as the beginning point of the straight-finger-
edge. As contrast to Figure 1, one can find that when the finger edge is straight,
the distances BC between points B and C would be greater than those distances
of BC Y when A is located in the positions of the bending finger edge in
Figure 1. The center and right-hand-side pictures in Figure 2 are also extracted
from the Microsoft Access database. By using the beginning points, which are
Figure 2. Find the beginning points with straight finger edges.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 3
The extracted middle finger images are shown in Figures 5Y7, respectively. Di-
lating circles with radii 2, 3, and 4 generate the results of Figures 5Y7, re-
spectively. The error rates for Figures 5Y7 are 4.7%, 2.8%, and 4.7%. The error
rates are calculated by using the misidentification cases divided by the total test
cases. Since the dilating circle with radius 3 generates more accurate result, the
circle with radius 3 is used in this research to extract the original finger images.
Figure 5. The extracted middle finger images with dilating radius 2.
Figure 6. The extracted middle finger images with dilating radius 3.
Figure 7. The extracted middle finger images with dilating radius 4.
Figure 8. The extracted ring finger images with dilating radius 3.
Figure 9. The extracted index finger images with dilating radius 3.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 5
the second group, the error rate for the original middle finger is 2.8%, the
original ring finger is 5.7%, and the original index finger is 1.9%. In the previous
research [15], the middle, ring, and index finger edges are extracted from
different persons for conduct the persons’ identification too. The subtracted-re-
sults of the middle, ring, and index finger edges are shown in Figures 13Y15,
Figure 11. The subtracted-results of different persons’ ring-finger images.
Figure 12. The subtracted-results of different persons’ index-finger images.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 7
By the experimental test, one can conclude that the original finger images
extracted by the dilated finger edges will produce the more accurate identification
rates.
4. Fine-tuning the Extracted Images to Increase the
Identification-Accuracy-rates and Speed Up the
Identification Process
For one specific person, one should expect one unique shape of the finger image
is extracted from the person’s different hand photos. However, the illumination
Figure 15. The subtracted-results of different persons’ index-finger images.
Figure 16. Shift the image to different positions for comparison.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 9
the topYbottom position shifting are top two pixel, top one pixel, no move,
bottom one pixel, and bottom two pixels. The shifting positions are shown in
Figure 16.
When the images are shifted at one certain position, the image is also rotated
to different orientations for comparison. The rotated degrees are counter-clock-
wise 2-, counter-clock-wise 0.8-, counter-clock-wise 0.6-. . . clock-wise 1.8-. The
image rotating figures are shown in Figure 17. After this fine-tuning, the
misidentification rate is drop from 30% to 20%, for the pure thinned middle
finger edge test in Figure 13.
As mentioned in Section 3, in this research, totally 10 persons participate
in the experimental test. In each group, totally 15 photos are taken. For each
group, totally 105 comparisons are required for each photo to be compared to the
other 14 remaining photos. Figure 18(a) shows 225 comparisons, which contains
120 trivial comparisons Y (since C2
15 is equal to 105). Figure 18(b) shows the
essential 105 comparisons. The algorithm, developed in this research, only
perform the 105 essential comparisons and the other non-essential comparisons
are skipped.
As checking Figure 19(a), one can find that the extracted middle finger image
only occupies a small portion of the image. In order to speed up the comparison
process, the algorithm only needs to compare the useful image to identify the
person. The useful image range Y Imin, Imax, Jmin, and Jmax are found in this
research and are saved in the end of the image’s original BMP file. The content
of the BMP file with saved Y Imin, Imax, Jmin, and Jmax is shown in Figure 19(b).
Figure 19. Save the image range into the BMP file. (a) The useful image occupies only a
small portion of the image. (b) The BMP file records the image’s range Y Imin, Imax, Jmin, and
Jmax.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 11
respectively, the algorithm will locate the beginning points, which begin with the
straight-finger-edges. The picture is shown in Figure 21(b). By further pro-
cessing, the point with minimum distance near the thumb-index-fingers-valley
will be located. This point will be treated as the thumb-index-fingers-valley. The
result is shown in Figure 21(c).
6. Results and Conclusions
As mentioned in Section 3, the original finger images extracted by the dilated
finger edges will produce more accurate results than the pure thinned finger
edges. If one can more effectively control the zoom-in and zoom-out effect of the
camera lens and if one also can control the illumination well when one person’s
hand image is taken, the algorithm might reduce the misidentification error to
below 2%. In order to cope the above lens and illumination problems, one might
consider the 3-D laser scanner to acquire the hand images.
References
1. Editorial: Hand-based biometrics, Biom. Technol. Today 11(7) (July 2003), 9Y11.
2. Egiazarian, K. O. and Pestana, S. G.: Hand shape identification using neural networks, SPIE
4667 (2002), 440Y448.
Figure 21. Find the thumb-index-fingers-valley. (a) Find the finger tips. (b). Locate the
beginning points, which begin with the straight edges. (c) Find the point with the minimum
distance, which will be treated as the thumb-ndex-fingers-valley.
FINGER IMAGE EXTRACTION BY MORPHOLOGICAL TECHNIQUE AND FINGER IMAGE 13
te
e
ia
Yeh
– th
tio
fea
her
by
magnifying, the ﬁnger-image will possess more salient feature. Image subtraction is used to exam the diﬀerence of the two images.
 2007 Elsevier Ltd. All rights reserved.
2003; Sun & Qiu, 2004; Xionga, Toha, Yaua, & Jiangb,
2005; Xiong, Xu, & Ong, 2005), middle ﬁnger crease
(You, Li, & Zhang, 2002), control point selection (Lia,
Zhang, & Xub, 2003; You et al., 2002), coarse to ﬁne strat-
In the previous research, when performing the hand
geometry matching (Editorial, 2003; Egiazarian & Gonz-
2003). In this research, the developed recognition algorithm
automatically calculate and check the ﬁnger-edge energy
response signals and selected the high energy response sig-
nals to ﬁnd the ﬁnger-to-ﬁnger valleys of the hand image
automatically. The algorithm developed in this research
ﬁnds the ﬁnger-to-ﬁnger valleys more accurately and more
* Tel.: +886 4 851 1888x4121; fax: +886 4 851 1270.
E-mail address: cls2@mail.dyu.edu.tw
Expert Systems with Applications
ARTICLE IN PRESSpattern matching (Joshi, Rao, Kar, & Kumar, 1998), vari-
ous ﬁnger size measurements (Kumar, Wong, & Shen,
2003; Sanchez-Reillo, Sanchez-Avila, & Gonzales-Marcos,
2000), various ﬁnger lateral view size measurements (San-
chez-Reillo et al., 2000), vein pattern (Lin & Fan, 2004),
eigenpalm (Lua, Zhang, & Wanga, 2003), implicit polyno-
mials (Oden, Ercil, & Buke, 2003), algebraic invariants
(Oden et al., 2003), Karen invariant computation (Oden
et al., 2003), line interception and slope comparisons
alez Pestana, 2002; Han et al., 2003; He, Bing, Qiu,
Zheng-Ding, & Sun, 2002; Mitome & Ishii, 2003; Su,
2003; Sun & Qiu, 2004; Xionga, Toha et al., 2005; Xiong,
Xu et al., 2005), in order to recognize the hand image, every
time the hand needs to place in a precise certain ﬁxed posi-
tion – thus the camera can capture the same hand image. In
this research the hand needs not to place in a precise certain
ﬁxed position. Wavelet technique is used to ﬁnd the ﬁnger-
to-ﬁnger valley in the past research (Han, 2004; Han et al.,Keywords: Finger shape information extraction; Finger shape computation and identiﬁcation
1. Introduction
In the past 20 years, researchers invested a lot of eﬀort
to develop diﬀerent techniques to identify the hand images.
This past work includes – hand geometry (Editorial, 2003;
Egiazarian & Gonzalez Pestana, 2002; Han, Cheng, Lin, &
Fan, 2003; He, Qiu, & Sun, 2002; Mitome & Ishii, 2003; Su,
egy (Han, 2004), B-Spline (Ma, Pollick, & Terry Hewitt,
2004), watershed transform (Lin & Fan, 2004), HMM
(Sun & Qiu, 2004); however, some are very sensitive to
the noise (Joshi et al., 1998; You et al., 2002); some have
very complicated mathematical models (Oden et al.,
2003) and some have very complicated neural training
algorithms.Hand image recognition by the
and image w
Ching-L
Department of Industrial Engineering and Technology Management, Da
Abstract
This research uses the object extracting technique to extract the
The algorithm developed in this research can ﬁnd the precise loca
ﬁngers contain many useful geometry features. One can use these
used to transfer geometry features of these ﬁnger images to anot
the ﬁnger image has more salient feature. Image is also magnifying0957-4174/$ - see front matter  2007 Elsevier Ltd. All rights reserved.
doi:10.1016/j.eswa.2007.05.040
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040chniques of hand shape scaling
ight scaling
ng Su *
University, 112 Shan-Jeau Road, Da-Tsuen, Chang-Hua 51505, Taiwan
umb, index, middle, ring, and small ﬁngers from the hand images.
ns of the ﬁngertips and the ﬁnger-to-ﬁnger-valleys. The extracted
tures to do the person identiﬁcation. The geometry descriptor is
feature-domain for image-comparison. Image is scaled to make
the basis of distance-multiplying-pixel-gray-level. After the image
www.elsevier.com/locate/eswa
xxx (2007) xxx–xxx
Expert Systems
with Applicationsby the techniques of hand shape scaling ..., Expert Systems with
EnergyðknÞ
PalmOutline Pixel Numberq
n¼1
¼ Distance
PalmOutline Pixel Numberq
n¼1
kðknqÞ to ðknþqÞk
ð2:1Þ
3. Geometry descriptor and image subtraction
For every pixel (i, j) in the object, after the transforma-
tion of the geometry descriptor, the ﬁnal destination of
C.-L. Su / Expert Systems with Applications xxx (2007) xxx–xxx 3
ARTICLE IN PRESSbetween (kz+q and kzq) is shorter than the distance
between (ky+q and kyq) too. In Eq. (2.1), knq represents
the (n  q)th pixel residing in the obtained palm’s edge
image. The variable n varies from 1 to palm-edge-total-pix-
els subtracting q. The response energy Energy(kn) is
weighted by the distance of the two points knq and kn+q.
As one can see from Fig. 4, one can ﬁnd Energy(kn) will
have the less values in the ﬁngertip or the ﬁnger-to-ﬁnger
Fig. 4. Show the positions of various feature points.valley areas. In the straight-line area of the ﬁnger edge,
Energy(kn) will have the greater values. In this research,
q is set to 10. By this concept, one can extract the entire ﬁn-
gertips and ﬁnger-to-ﬁnger-valleys. Furthermore, one can
extract the entire ﬁngers. Since the ﬁngertip and the ﬁn-
ger-roots might cause some errors during the ﬁnger-recog-
nition process, the ﬁngertips and ﬁnger-roots are taken out.
The ﬁnal ﬁnger images are shown in Fig. 5. In the previous
research, the wavelet transform (Han, 2004; Han et al.,
2003) is used to ﬁnd the positions of ﬁnger-to-ﬁnger-val-
leys. This paper purposed method work more eﬃciently
and correctly to ﬁnd the position of the ﬁngertips and ﬁn-
ger-to-ﬁnger-valleys.
Fig. 5. The hand image an
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040(i, j) is (if, jf). The geometry descriptor will transfer and
interpolate the geometry features of every pixel inside the
object to another feature-domain. The original geometry
feature of the ﬁnger image will be preserved, after the
object is transfer to the new feature-domain. In the new
feature-domain, every object will have the same straight
orientation and every object’s centroid is aligned to posi-
tion (64,64). Since every object has the same orientation
and every object is aligned to the same centroid, one can
perform the image subtraction to perform the person’s
identiﬁcation. Eq. (3.1) shows the function of the geometry
descriptor. In Eq. (3.1), ‘‘h’’ represents the object’s orienta-
tion and ‘‘Centroid’’ represents the object’s centroid. By
using Eqs. (3.2)–(3.4), one can ﬁnd the parameter h. In
Eqs. (3.2) and (3.3), idistance represents the i-distance of a
speciﬁc pixel g(i, j) to the object’s centroid, jdistance repre-
sents the j-distance of a speciﬁc pixel g(i, j) to the object’s
centroid, and Weightij represents the weight of a speciﬁc
pixel g(i, j). The images in the second row of Fig. 6 shows
the resultant images after the geometry descriptor is
applied to the ﬁnger images. During the image-comparison
stage, to allow the ﬁnger geometry to possess more salient
feature, the ﬁngers are scaled to various sizes. The images
are shown in the third row of Fig. 6.
In this research, in order to allow the ﬁnger geometry to
possess more salient feature when performing the ﬁnger
image-comparison, the ﬁngers are also magniﬁed to vari-
ous images. The images are named ‘‘distance-weighted-
images’’. One can obtain the ‘‘magniﬁed ﬁnger-image’’ by
processing the ﬁnger images by the following steps:
Assume one existing pixel T, T 2 ﬁnger image, and
assume f(i, j) is the gray level of point (i, j), and also assume
f(i, j) 2 ﬁnger image. One use the symbol kT  f(i, j)k to
represent the distance between point T and point (i, j)
and one denote the distance as d. The construction ofd the extracted ﬁngers.
by the techniques of hand shape scaling ..., Expert Systems with
comparison. The rotated degrees are – counter-clockwise
20, counter-clockwise 15, counter-clockwise 10, coun-
ter-clockwise 5, etc., to clock-wise 20. Fig. 13 shows the
rotated-images, which are rotated from 5 to +5.
As mentioned earlier, for reducing the number of ﬁles in
the system, one person’s entire ﬁnger images are placed in
one ﬁle to perform the process of the image shifting and
image-rotation. After the image shifting and rotation, the
individual ﬁnger image needs to be extracted from the
hybrid geometry signal separately to perform the pattern
recognition. The hybrid geometry signals and the extracted
ﬁnger images are shown in Figs. 14 and 15. After obtaining
the individual ﬁnger image, one can perform the image sub-
traction to the ﬁnger images and to perform the pattern
recognition. The image subtractions are shown in Figs.
16–19. In the top of Fig. 16, one can ﬁnd a 5 by 5 sub-pat-
tern-template. This 5 by 5 sub-pattern-template will extract
the sub-pattern images of two persons’ hand geometry sig-
nals and, consequently, the image convolution is performed
to the 5 by 5 sub-pattern-template images. Image-ﬁne-tun-
ing is also performed in this stage and the minimum value
of the convolution results will be recorded in the resultant-
image. The subtract result image is shown in the right-hand
side picture of Fig. 16. By calculating the diﬀerence of the
Fig. 8. The images before and after image-magnifying.
C.-L. Su / Expert Systems with Applications xxx (2007) xxx–xxx 5
ARTICLE IN PRESSbetter recognition rate. First, the image is shifted at diﬀer-
ent positions – to which are – left four pixels, left three pix-
els, left two pixels, etc., to right four pixels. Also, the image
is shifted – top four pixels, top three pixels, top two pixels,
etc., to bottom four pixels. The picture is shown in Fig. 12.
When the images are ﬁne-tuning to one certain position,
the image is also ﬁne-tuning to diﬀerent orientations forFig. 9. The results of the ﬁnger images after performing the g
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040subtracted-result, one can recognize diﬀerent persons. In
Figs. 17–19, one can ﬁnd the hybrid geometry signals and
the extracted ﬁnger images. The image subtraction is per-
formed to the extracted ﬁnger images and the subtracted-
results are shown in the right-hand side pictures of Figs.
17–19. Fig. 17 shows the palm image, Fig. 18 shows thumb
ﬁnger, and Fig. 19 shows the index ﬁnger.eometry descriptor, imagescaling, and image-magnifying.
by the techniques of hand shape scaling ..., Expert Systems with
ði; j; h;Centroid )
Geometry Descriptor
ðif ; jfÞ ð3:1Þ
Var1 ¼
XN
j¼1
XN
i¼1
ðidistanceÞ2 Weightij

XN
j¼1
XN
i¼1
ðjdistanceÞ Weightij ð3:2Þ
Var2 ¼
XN
j¼1
XN
i¼1
2  idistance  jdistance Weightij ð3:3Þ
h ¼
Sin1
PN
j¼1
PN
i¼12idistancejdistanceWeightijﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var22þVar12
p
2
ð3:4Þ
Fig. 13. The image is rotated to diﬀerent positions.
Fig. 14. Separating-operation to separate the diﬀerent ﬁngers.
Fig. 15. Separating-operation to separate the diﬀerent ﬁngers.
5º0º
Subtract
TemplateSubPattern _
SubPattern _Template SubPattern_Template
TemSubPattern_
Fig. 16. Sub-pattern template convolutio
C.-L. Su / Expert Systems with Applications xxx (2007) xxx–xxx 7
ARTICLE IN PRESS
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040=
sultSubtract Re
MinimumGetnConvolutioplate _⇒n to ﬁnd the minimum pixel value.
by the techniques of hand shape scaling ..., Expert Systems with
ent
C.-L. Su / Expert Systems with Applications xxx (2007) xxx–xxx 9
ARTICLE IN PRESScorrectly, several images are discarded from the test. Actu-
ally, the hand-photo-images used in this system are only
120, which are belonged to 40 persons.
Regard these 120 photos as whole and run these 120
photos in one batch – which will take a very long time to
get the job done. Due to the software glitch, the running
process might break before the job completed. Conse-
Fig. 19. Image subtraction of diﬀerquently, one might endlessly run and run the procedure
again and again and every time from the square one. Fur-
thermore, the database might have no enough memory to
accommodate the entire obtained-data. Thus, in this sys-
tem, ﬁve persons are designated as one group, i.e. ﬁve peo-
ple are regarded as a group and these ﬁve persons are tested
in one step. Since one person has been taken three diﬀerent
photos, totally ﬁfteen photos are tested in each step. As
mentioned earlier, in this system, there are 40 persons par-
ticipating the test. Thus, the system totally will run eight
diﬀerent batches. Totally, 120 hand-photos are tested.
Comparisons of the same
person's index fingers
13606 9344 32648 35920 32057 19731 19951 20
9075 36050 39984 35514 16802 18862 17
32330 36490 31081 17812 20023 17
15355 6971 39351 40742 41
14295 45178 45510 47
39852 41553 40
8055 11
11
Fig. 20. The comparison-dat
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040For each hand-photo image, the following steps are per-
formed – (1) the hand edge is found, (2) the ﬁngers are
extracted separately, (3) geometry descriptor is performed
to each ﬁnger, (4) image shifting and rotation are per-
formed, (5) image magnifying are performed, (6) genuine
and imposter comparisons are conducted. To each group,
one photo is compared against the other 14 photos.
orientations and diﬀerent positions.Totally, 105 comparisons are conducted to test the accu-
racy of the developed identiﬁcation algorithm. Within
those one hundred and ﬁve comparisons, ﬁfteen compari-
sons are conducted for genuine comparison, the compari-
sons of the same person’s ﬁngers – since one person
taken three diﬀerent photos. The other ninety comparisons
are conducted for imposter-comparisons - the comparisons
of diﬀerent person’s ﬁngers. In this research, there are eight
diﬀerent test batches. Thus, there are 120 genuine tests and
720 imposter tests. Fig. 20 shows the partial comparison
data of the image diﬀerences after performing the image
553 21518 22666 24366 23012 23208 24767
742 27583 27046 25050 19072 19944 23275
886 21973 22145 22144 21070 21091 23797
120 27115 28159 34670 42610 45765 41568
317 29593 29584 34317 45483 49224 43513
999 27351 26778 32646 42803 45542 42197
227 28340 28836 28194 12836 12500 17419
585 29746 31098 30431 14746 15300 18198
30076 31020 30898 16496 12981 20415
11496 18355 32092 34529 31320
14401 32070 34369 31665
28564 31604 27848
10206 11121
14594
a of index ﬁnger images.
by the techniques of hand shape scaling ..., Expert Systems with
App
ARTICLE IN PRESS.20 Imposter
C.-L. Su / Expert Systems withperson’s another ring ﬁnger image. The other data, which is
not surrounded by the rectangular boxes, show the sub-
tracted-results of the diﬀerences of two diﬀerent persons’
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
15.6
11.5
18.2
11.4
12.9
10.1
13.4
.
.
.
14
15
16
17
18
10
11
12
13
19 Comparison
16.1
10.1
11.4
Imposter
Comparison
Fig. 24. The gap between the imposter and the genuine comparisons for
middle ﬁnger.
.
.
.
.
.
.
.
First
middle finger
FAR
.
.
.
90
95
100
0
5
10
Second
middle finger
Third
Middle finger
Fourth
Middle finger
Fifth
Middle finger
FRR
Accuracy Rate
Fig. 25. The accuracy rate, FRR, and FAR for the middle ﬁnger.
Comparisons of the same
person's ring fingers
8 912 7236 1 3 0 9 1 1 4 4 7 2 1 3 9 1 0 2 0 4 0 9 2 0 3 4 0 1 8 8
8304 1 6 2 2 7 1 4 8 8 4 1 5 5 5 4 1 8 6 8 5 1 7 8 9 8 1 6 4
1 3 3 5 7 1 4 8 9 6 1 3 9 2 0 1 7 8 6 5 1 8 0 3 4 1 6 5
1 2 0 8 3 8 7 1 4 2 5 6 6 2 2 5 6 9 5 2 4 0
8 3 4 0 2 8 6 9 0 2 9 0 3 1 2 7 3
2 7 1 0 4 2 7 5 6 9 2 6 0
8 8 2 3 7 9
79
Fig. 26. The comparison-da
Please cite this article in press as: Su, C. -L. , Hand image recognition
Applications (2007), doi:10.1016/j.eswa.2007.05.040.17
lications xxx (2007) xxx–xxx 11ring ﬁnger images. Fig. 27 shows the summarized-results
of Fig. 26. Fig. 27 shows the gap between the imposter
and genuine comparisons. Fig. 27 depicts ﬁve diﬀerent per-
5 8 1 8 8 2 6 1 5 4 5 5 1 6 1 3 5 2 3 2 5 8 2 4 2 7 2 2 3 9 7 0
0 8 1 7 2 3 2 1 6 4 4 0 1 4 0 7 3 1 9 5 8 2 2 0 7 3 0 2 0 1 4 6
1 6 1 7 5 8 6 1 4 7 4 3 1 5 1 3 2 2 1 2 9 0 2 2 1 3 2 2 1 8 3 0
2 7 2 1 7 9 1 2 1 4 4 8 2 2 0 8 7 3 0 1 1 2 3 1 0 1 4 3 0 7 7 4
8 7 2 6 7 9 7 2 5 3 4 1 2 4 4 3 6 3 1 8 8 9 3 2 9 4 0 3 2 2 3 1
3 7 2 4 9 7 3 2 3 6 6 1 2 3 4 4 5 3 1 5 6 1 3 2 1 4 2 3 1 9 3 6
6 9 1 0 9 5 1 1 2 8 2 0 1 2 3 0 9 1 2 1 1 4 1 2 6 1 2 1 3 5 2 3
44 9 1 5 0 1 1 3 2 2 1 1 0 5 5 9 0 8 6 1 0 1 3 0 1 0 7 2 8
9 2 2 0 1 0 1 1 2 9 4 3 4 1 0 0 9 9 9 9 9 4 1 1 3 7 6
8 5 9 8 1 0 0 0 1 1 3 9 7 0 1 4 4 3 0 1 4 5 8 4
8 3 5 1 1 3 6 1 2 1 4 4 1 1 1 4 2 6 5
1 2 9 2 9 1 3 5 0 4 1 3 4 3 0
7 1 7 5 6 8 8 6
6 5 9 8
ta of ring ﬁnger images.
.
.
.
.
.
.
First
Person
Imposter
Comparison
Genuine
Comparison
Imposter
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
13.0
8.9
13.9
9.0
9.4
8.8
12.0
.
.
.
Imposter
Comparison
10.0
7.1
9.9
14
15
16
10
11
12
13
7
8
9
Imposter
Comparison
Fig. 27. The gap between the imposter and the genuine comparisons for
ring ﬁnger.
.
.
.
.
.
.
.
First
Ring finger
FAR
.
.
.
90
95
100
0
5
10
Second
Ring finger
Third
Ring finger
Fourth
Ring finger
Fifth
Ring finger
FRR
Accuracy Rate
Fig. 28. The accuracy rate, FRR, and FAR for the ring ﬁnger.
by the techniques of hand shape scaling ..., Expert Systems with
Ching-Liang Su
the most salient feature of the resultant images.
eigenpalm [10], implicit polynomials [13], algebraic invariants [13], Karen invariant computation [13], line
q National Science Council, Taiwan, supported this work under grant NSC 95-2221-E-212-003.
E-mail address: cls2@mail.dyu.edu.tw
Applied Mathematics and Computation 188 (2007) 912–923
www.elsevier.com/locate/amc0096-3003/$ - see front matter  2006 Elsevier Inc. All rights reserved. 2006 Elsevier Inc. All rights reserved.
Keywords: Finger shape information extraction; Image phase matching; Identiﬁcation intelligence
1. Introduction
During the past 20 years, researchers invested a lot of eﬀort to develop diﬀerent techniques to identify hand
images. This past work includes hand geometry [1,2,4,5,12,15–18], middle ﬁnger crease pattern matching [6],
various ﬁnger size measurements [7,14], various ﬁnger lateral view size measurements [14], vein pattern [9],Department of Industrial Engineering and Technology Management, Da Yeh University, 112 Shan-Jeau Road,
Da-Tsuen, Chang-Hua 51505, Taiwan
Abstract
In this research, a new technique is used to extract the thumb, index, middle, ring, and small ﬁngers and to perform a
person’s identiﬁcation. To allow the ﬁnger geometry to be more salient when performing the ﬁnger image comparison, the
ﬁngers are scaled to various sizes. For reducing the number of ﬁnger-image-ﬁles in the system, a person’s entire ﬁnger-
images are placed in one ﬁle. The hand is ﬁxed each time when a picture is taken and one can assume that each time when
the hand image is taken, the acquired ﬁnger images are the same as the previously acquired ones. Since the pictures are the
same, after the ﬁngers are extracted from the hand image, one can use the acquired ﬁngers to identify diﬀerent people. In
this research, the developed algorithm of the auto-registration technique can ﬁnd the precise location of the ﬁnger image –
including the centroid of the ﬁnger image and the orientation of the ﬁnger image. The ﬁnding of the position and the ori-
entation of the ﬁnger image are conducted automatically and without any further human eﬀort. After ﬁnding the positions
of the ﬁnger images, image rotating, image shifting, and image interpolating techniques are used to align diﬀerent ﬁnger
images to the same position and the same orientation for comparison. The extracted ﬁnger image contains many useful
geometrical features. One can use these features to do ﬁnger image identiﬁcation. Since the entire ﬁnger images are aligned
to the same position and the same orientation, the image phase-matching technique is used to examine the diﬀerence
between two ﬁnger images. The image phase-matching technique involves complex number manipulation and also ﬁndsFinger extraction, ﬁnger image automatic registration,
and ﬁnger identiﬁcation by image phase matching qdoi:10.1016/j.amc.2006.10.074
Fig. 2.1. The designed shelves for taking the hand images.
Fig. 2.2. The 128 by 128 gray scale hand images.
Fig. 2.3. The extracted hand edge images.
kx ρ ρ+
kx
kx−
ky ρ
ρ
ρ ρ
+
ky
ky−
kz+
kz
kz−
Fig. 2.4. The positions of various feature points.
914 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923
916 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923The images in the third row of Fig. 3.1 show the resultant images after the geometry descriptor is perform
to the ﬁnger images. To allow the ﬁnger geometry to possess more salient features when performing the ﬁnger
image comparison, the ﬁngers are scaled to various sizes. The images are shown in the fourth row of Fig. 3.1.
Fig. 3.2 shows the overlapped geometry signals. The overlapped geometry signals are the overlapped signals
of the palm, thumb, index, middle, ring, and small ﬁngers; i.e. the signals of the palm, thumb, index, middle,
ring, and small ﬁngers are added together to generate the overlapped geometry signals. Since one ﬁnger-image
needs one ﬁle to record the ﬁnger image, more ﬁnger-images need more ﬁles to record these ﬁnger-images. In
this overlapped scheme, several ﬁngers’ images are recorded in one ﬁle. Since several ﬁnger images are
Fig. 3.1. The results of the ﬁnger-images after performing the geometry descriptor and image scaling.recorded in one ﬁle, the total number of the ﬁles in the system would be reduced. Since the total number
of the ﬁles is reduced, the system complexity is reduced when performing the person identiﬁcation. Fig. 3.3
shows the various overlapped geometry signals.
Fig. 3.2. The overlapped geometry signal.
4. Image phase matching
Eq. (4.1) shows the function which transfers the function f(x,y) to function F(u,v) and Eq. (4.2) shows the
function which transfers the function g(x,y) to function G(u,v). Fig. 4.1 shows the various f(x,y) and its
corresponding F(u,v) images. Eq. (4.3) shows the relationship of image phase matching between – F(u,v),
conjugated G(u,v), f(x,y), and g(x,y). Symbol  represents the image phase matching. Fig. 4.2 shows the phase-
matching images of the two images f(x,y) and g(x,y). By checking Fig. 4.2, one can ﬁnd that by using the
image phase-matching technique, one can correctly identify the position of g(x,y). Fig. 4.3 shows the various
phase-matching images which are generated by the images f(x,y) and g(x,y). The top image is the palm image,
the center image is the thumb image, and the bottom image is the index ﬁnger image. The algorithm developed
in this research will examine the most salient pixel in the phase-matching image in Fig. 4.3 to identify whether
or not image f(x,y) and image g(x,y) are the same images. Fig. 4.4 shows the extracted palm images and the
phase-matching images of the two extracted palm images, Fig. 4.5 shows the extracted thumb images and the
phase-matching images of the two extracted thumb images, and Fig. 4.6 shows the extracted index ﬁnger
images and the phase-matching images of the two extracted index ﬁnger images. As mentioned before, the
algorithm developed in this research will identify the most salient pixel of the phase-matching image to identify
whether or not image f(x,y) and image g(x,y) are the same images.
F ðu; vÞ ¼ 1
128
X127
x¼0
X127
y¼1
f ðx; yÞ  exp½j2pðuxþ vyÞ=128; ð4:1Þ
Gðu; vÞ ¼ 1
128
X127
x¼0
X127
y¼1
gðx; yÞ  exp½j2pðuxþ vyÞ=128; ð4:2Þ
F ðu; vÞ  Gðu; vÞ ¼ f ðx; yÞ  gðx; yÞ: ð4:3Þ
918 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923Fig. 4.1. The f(x,y) and F(u,v) images.
920 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923have insuﬃcient enough memory to accommodate the entire obtained data. Thus, in this system, ﬁve persons
are designated as one group, i.e. ﬁve people are regarded as a group and these ﬁve persons are tested in one
step. Since one person has three diﬀerent hand images being taken, totally 15 hand images are tested in each
step. As mentioned earlier, in this system, there are 40 persons participating in the test. Thus, the system will
Fig. 4.4. Image phase matching of diﬀerent orientations and diﬀerent positions.
Fig. 4.5. Image phase matching of diﬀerent orientations and diﬀerent positions.
922 C.-L. Su / Applied Mathematics and Computation 188 (2007) 912–923.
.
Imposter
Comparison
Imposter
Comparison
Imposter
Comparison
27.1
.
.
.
19
20
21
22
Imposter
Comparison
22.1 23.2various ﬁnger images. By analyzing the entire test result of this research, one can conclude that the accuracy
rate is 95%. The false accepted rate is 2.5% and the false rejected rate is 3%.
References
[1] Hand-based biometrics, Editorial, Biometric Technology Today, 11(7) (2003) 9–11.
[2] Karen O. Egiazarian, S. Gonzalez Pestana, Hand shape identiﬁcation using neural networks, The International Society for Optical
Engineering 4667 (2002) 440–448.
.
.
.
.
.
First
Person
Genuine
Comparison
Genuine
Comparison
Second
Person
Imposter
Comparison
Genuine
Comparison
Third
Person
Genuine
Comparison
Fourth
Person
Genuine
Comparison
Fifth
Person
16.8
13.6
12.5
14.5
18.3
11.5
15.3
.
.
.
14
15
16
17
18
10
11
12
13
Fig. 5.2. The gap between the imposter and the genuine comparisons for an index ﬁnger.
.
.
.
.
First
person
Second
person
Third
person
Fourth
person
Fifth
person
FAR
.
.
.
0
5
10
FRR
Index finger
FAR
.
.
.
0
5
10
FRR
Middle finger
FAR
.
.
.
0
5
10
FRR
Ring finger
Fig. 5.3. The FRR and FAR for the various ﬁnger images.
