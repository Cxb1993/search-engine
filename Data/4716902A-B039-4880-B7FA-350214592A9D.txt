1 
 
行政院國家科學委員會專題研究計畫成果報告 
新世代自動語音辨識技術之研究-第二階段 
語音事件整合、證據確認與後處理 
Speech Event Integration, Evidence Verification and Post Processing 
計畫編號：NSC 97-2221-E-002-134-MY3 
執行期限：99年08月01日至100年07月31日 
主持人：李琳山 國立台灣大學電機工程學系暨研究所 
E-mail: lslee@gate.sinica.edu.tw 
 
計畫中文摘要 
本計畫工作內容在整體計畫中橫跨三個階段或功
能方塊(第二級－語音事件之整合及確認、第四級
－語音辨識之強化及後處理、第五級－外加已有知
識)，在此計畫當中，我們在三個功能方塊中都有
相當的進展。我們藉由音素分群建立新的階層式
多層感知器，並且引入帶有長時間訊息的加伯濾
波器，用以找出一些語音事件的訊息，並設法強
化語音辨識能力，在廣播新聞辨識中獲得相當的
改善。一般串接模型中以單一多層感知器學習全
面性的音素分類，很難區分混淆的音素，本計畫
將全面性音素分類問題拆解為一組針對性的階層
式分類，將複雜的音素分類問題分而治之。同時
我們也針對中文語言的韻律特性，將語音辨識系
統的聲學單位以韻調因素重新定義，輔以多層感
知器以及加伯濾波器的成果，能夠對中文語音辨
識系統進一步加以改良。相對於傳統梅爾倒頻譜
系數(MFCC)所訓練的聲學模型，使用此計畫中技
術改良之系統，錯誤率最大能獲得30.5%的相對進
步。 
 
關鍵詞：事件整合、證據確認、後處理、群聚階
層式多層感知器、大字彙連續語音辨識 
 
計畫英文摘要 
This project includes works in three stages in the 
overall project (Stage II – Event Integration and 
Evidence Verification, Stage IV – Output En-
hancement and Post Processing, Stage V – External 
Existing Knowledge). In this project, we have very 
good progress in all three of these function stages. 
By constructing a new hierarchical multilayer per-
ceptron through phoneme clustering, and introduc-
ing Gabor features which carries long term infor-
mation, we can extract better features from the 
acoustic signals of acoustic events, and further im-
prove the recognition performance. In this project, 
the recognition system for broadcast news has been 
greatly improved. In general approach, it is hard to 
classify confusing phonemes since a single multi-
layer perceptron is trained to classify all types of 
phonemes. Here we divide the problem of classify-
ing all phonemes into several problems for smaller 
subsets, and build a hierarchical classifier to im-
prove the performance of each subset. At the same 
time, the prosodic structure of Mandarin is investi-
gated so that we can redefine the acoustic units in 
the system according to the tonal characteristics. 
With the hierarchical structure and Gabor features 
mentioned, the system for tonal Mandarin recogni-
tion can be improved further. Compared with the 
system with general approach using MFCC feature 
along, the system proposed in this project can 
achieve 30.5% relative improvement in error rate. 
 
Index Terms - Event Integration, Evidence Verifica-
tion, Post Processing; Clustered Hierarchical Mul-
ti-Layer Perceptron, CHMLP; LVCSR 
 
1. Introduction 
In this project, we focused on improving the 
general tandem system structure which combines 
the discriminative power of multilayer perceptron 
and trying to introduce new acoustic features into 
the recognition system. The whole progress can be 
mainly classified into three stages according to the 
time order, that is, the first year results, the second 
year results and the third year results. For each year, 
we presented the content, the main idea, proposed 
approach in detail, and performance evaluation in 
sub-sections. For the results of first year, we modi-
fied the structure of the multilayer perceptron used 
in tandem system, and presented it in section 2. For 
the results of second year, we proposed a bottom-up 
approach based on the system built in the first year, 
and presented it in section 3. For the results of third 
year, which is the last one of this project, Gabor fil-
3 
 
characterizes the similarity between each pair of 
phones. The distance d (i, j) between phones i and j 
is defined based on the posterior probabilities esti-
mated from a monolithic MLP as below. 
 
 (1) 
       (2) 
and 
       (3) 
 
Where ci is phone class i, ito  is the t-th observa-
tion vector of phone i in the training set, ni is the 
total number of such observation vectors, and P (cj 
| ito ) is the posterior probability for phone j given 
the observation vector ito  of phone i as obtained 
from the monolithic MLP. It is clear that P (cj | ci) 
represents the averaged posterior probability that the 
observation vector of phone i is confused as phone j , 
and vice versa, and wi and wj are used to give higher 
weights to more reliable posterior probabilities ob-
tained with more data. Clearly, the more likely it is 
for phone i to be classified as phone j, the smaller 
the phonetic distance d (i, j) is. Using (1), we con-
struct an N × N phonetic distance matrix for N 
phones. The distance d (i, j) is symmetric, which 
makes for easier clustering in the next step. 
Table 1 shows an example submatrix of the 
above phonetic distances for four specific phones. It 
is obvious that /p/ and /t/ are phonetically close, as 
are /m/ and /n/; but the former two phones are very 
different from the latter two. 
 
 
Table 1: An submatrix of phonetic distances. 
 
In this way phonetic distance is based directly 
on phone confusability, as determined from the 
output of a monolithic MLP. It is thus reasonable to 
expect that the hierarchy obtained using this dis-
tance metric better reflects confusability among dif-
ferent MLP-classified phones, as compared to hier-
archies based on other criteria or knowledge. 
 
2.2.2. Hierarchical clustering algorithm 
To construct the clustered phone hierarchy us-
ing the phonetic distance defined in 2.1, we exploit 
the hierarchical agglomerative clustering (HAC) 
algorithm to tie the closest phones together.  
The distance between two clusters Ci and Cj is 
first defined as the average distance between all 
phone pairs respectively belonging to the two clus-
ters, as in (4) below, i.e. the average-linkage ag-
glomerative algorithm. 
 
  (4) 
 
where 
ic
n  is the number of different phones 
in the cluster Ci, and a and b are two phones respec-
tively in clusters Ci and Cj. The resulting HAC algo-
rithm is summarized below: 
 
step 1.  Regard each phone ci as a cluster Ci. 
step 2.  Find a pair of closest clusters Ci and Cj 
with minimum D(i, j) . 
step 3.  Merge Ci and Cj into a new cluster. 
step 4.  If the stop criterion is not satisfied (for 
example, there are still too many clusters, 
or the minimum distance is larger than a 
specific threshold), go to step 2. 
 
The criteria listed above in step 4 were actually 
used to decide the number of leaf clusters and the 
entire hierarchical structure. The final hierarchical 
structure is labeled by two integers here, the first for 
total number of leaf clusters, the second for total 
number of levels. Two examples are shown in Fig. 2, 
CHMLP 3/2 and CHMLP 3/3 for 3 leaf clusters and 
2 and 3 levels in Fig. 2 (a) and (b) respectively. 
 
2.2.3. Clustered hierarchical tandem system 
For the tandem system here, we construct a 
clustered hierarchical classifier consisting of the 
higher-level MLPs and leaf MLPs, given the struc-
ture mentioned in Section 2.2. 
 
2.2.3.1. Higher-level MLP 
The higher-level MLP, MLP( l )- k , separates a 
given cluster on level l into its child clusters on lev-
el l +1. Thus, given an observation vector to , the 
higher-level MLP can be used to estimate the poste-
rior probability P (Cj | to ) of to  belonging to the 
child cluster Cj. Since phonetic distances between 
phones in different child clusters are significantly 
larger than those between phones within the same 
cluster, higher-level MLPs are able to accurately 
distinguish different leaves 
5 
 
for training both the monolithic and clustered hier-
archical MLPs. All of the experiments in Table 2 
were trained with a window of 9 successive frames, 
1000 hidden nodes, and task-dependent output 
nodes. In all cases, it was found that adding hidden 
nodes yielded a negligible impact on recognition 
performance. All of the results here used a trigram 
language model. 
 
2.3.2. MLP feature model in GMM/HMM mod-
eling 
We applied the tandem architecture for integra-
tion of MLP and HMM by using the output vectors 
of MLP as extra HMM features. As the original 
MLP output features are too sharp to be modeled as 
Gaussian mixture distributions, we used a log 
transform to make the MLP features more Gaussi-
an-like. PCA was further used to reduce the dimen-
sions from 36 to 25, preserving 95% of the variance 
[3, 4]. 
MFCC features and MLP features were con-
catenated in the conventional fashion, resulting in 
64-dimension feature vectors. We use the concate-
nated features to train the right-context-dependent 
initial/final models. As features with more dimen-
sions changed the range of the Gaussian mixture 
likelihood, a proper weight was adopted to make the 
range more reasonable [3]. 
 
2.3.3. LVCSR results 
We have compared several results for different 
structures and different numbers of leaf clusters (2, 
3, 4, and 6). In each case, the clustered hierarchical 
tandem system performed better than the general 
tandem system, row (2) as shown in Table 2. With 
the same number of leaf MLPs, two-level hierar-
chical structures (row (3)-(6)) outperformed struc-
tures with even higher-level MLPs (row (7)-(9)). 
Fig. 4 compares the cases of different leaf clusters 
with a two-level hierarchical structure. The optimal 
structure was the two-level hierarchical structure 
with three leaf clusters CHMLP in row (4), the im-
provements for which were about 6% relative to 
monolithic MLP and 13.7% relative to MFCC. In 
the row (10), the phone set was decomposed into 
voiced and unvoiced phones and the same training 
procedure for the two-level hierarchical classifier 
was applied. 
 
 
Table 2: Character error rate (CER) results for dif-
ferent cluster configurations. n: leaf cluster count, m: 
level count (Sect. 2.2). “Impr. (1), (2)”is relative 
character error rate reduction as compared to the 
baseline 1 and baseline 2 respectively. 
 
 
Fig. 4: Analysis of two-level structure for different 
numbers of leaf clusters. Single-cluster case is for 
monolithic structure. 
 
To investigate the classification capability 
against confusing phones given the best-performing 
configuration (CHMLP 3/2 in row (4) of Table 2), 
we evaluate the frame error rate (FER) or the per-
centage of frames for which the following is incor-
rect: 
 
  (7) 
 
where ito  is the t-th observation vector of 
phone i , Ck is k-th cluster k=1,2,3. Only the phones 
belonging to the same cluster are considered. The 
results for CHMLP 3/2 are listed in the right column 
of Table 3, are compared to the corresponding frame 
error rate for the corresponding phones but in a 
monolithic MLP listed in the middle column of Ta-
ble 3. This table shows that hierarchical MLP out-
performs monolithic MLP. 
 
 
7 
 
the averaged posterior probability that the observa-
tion vector of phone i is confused as phone j, and 
vice versa, and wi and wi are used to give higher 
weights to more reliable posterior probabilities ob-
tained with more data. This distance d(i,j) is sym-
metric. 
To construct the clustered phone hierarchy using the 
phonetic distance defined above, we exploit the hi-
erarchical agglomerative clustering (HAC) algo-
rithm to tie the closest phones together. The distance 
between two clusters Ci and Cj is defined as the av-
erage distance between all phone pairs respectively 
belonging to the two clusters, as in (11) below, 
based on the average-linkage agglomerative algo-
rithm, 
 
       (11) 
 
where nC is the number of different phones in the 
cluster C, and a and b are two phones respectively in 
clusters Ci and Cj.. The resulting HAC algorithm is 
straightforward. We first regard each phone ci as a 
cluster Ci, and then find a pair of closest clusters Ci 
and Cj with minimum D(Ci,Cj) and merge them into 
a new cluster. This process continues until the stop 
criterion is satisfied. 
 
3.2.2. Clustered Hierarchical MLP (CHMLP) 
The result of the above algorithm is a clustered 
hierarchical MLP (CHMLP) as shown in Fig. 1, 
where MLP(l)-k is the k-th MLP in level l of the 
hierarchy, and phone set [(l)-k] is classified by 
MLP(l)-k [21]. 
 
3.2.2.1. Higher level MLPs and Leaf MLPs 
A clustered hierarchical classifier consists of 
higher-level MLPs and leaf MLPs as shown in Fig. 
1. The higher-level MLP, MLP( l )- k , separates a 
given cluster on level l into its child clusters on lev-
el l +1. Thus, given an observation vector , the 
higher-level MLP is to estimate the posterior proba-
bility P(Cj |  ) for  belonging to the child cluster 
Cj. On the other hand, different phones in a leaf 
cluster at the lower end of the hierarchy are easily 
confused. We thus train a specific leaf MLP for 
each cluster to distinguish between these competing 
phones [21]. 
 
3.2.2.2. Integration of higher-level and leaf MLPs 
The process for integrating the clustered hier-
archy MLP (CHMLP) structure is shown in Fig. 5. 
The posterior probability that each observation vec-
tor  belongs to each phone class ci can be ob-
tained by multiplying the outputs of the leaf cluster 
with the output of the higher level MLP immediate-
ly above the leaf cluster including ci as in (12). 
 
  (12) 
 
where Cj is the leaf cluster including the phone class 
Ci, P(ci| ,Cj) is estimated by the MLP for the leaf 
cluster Cj, and P(Cj| ) is obtained from high-level 
MLPs; 
 
  (13) 
 
where Ck is the cluster immediately above the clus-
ter Cj, etc. All these posterior probabilities P(Ci| ) 
in (12) are then used as input to HMM. 
 
3.2.2.3. CHMLP with Bottom-up Processing 
CHMLP (BU) 
In the above structure of CHMLP, the high-
er-level MLPs are to perform cluster-level discrim-
ination. However, when the number of leaf clusters 
becomes large, the phonetic distances between some 
phones belonging to different clusters may not be 
large enough any longer, and it may become diffi-
cult for the higher-level MLPs to provide good dis-
crimination for clusters including such similar 
phones. This leads to the concept of bottom-up pro-
cessing approach proposed here as shown in Fig. 6. 
It is based on exactly the same CHMLP structure as 
discussed above, except processed in a bottom-up 
manner. 
Complete MFCC features for each speech 
frame at time t,  is used as the input of each leaf 
MLP at level l, MLP(l)-n for a phone cluster Cj, to 
classify all phones ci in the phone set [(l)-n]. The 
output is then the posterior probabilities P(ci | ,Cj ). 
The parent MLP immediately above, MLP (l-1)-m 
for the parent phone cluster Ck, then takes all these 
output posterior probabilities P(ci | ,Cj ) from all its 
child MLPs (child clusters) as inputs, giving outputs 
P(ci | , Ck) for all phones ci belonging to the 
cluster Ck, where  is the set of all posterior 
probabilities used as the input of MLP(l-1)-m, i.e. 
P(ci | ,Cj ),  Cj and Cj Ck for the case in 
Fig. 6. The outputs P(Ci | , Ck) are then used 
as the inputs of the next higher level MLP, etc. This 
9 
 
generate word graph in the first pass, and then the 
posteriors from MLP and trigram language model 
were used in rescoring. 
 
3.3.2 Baseline Results 
For fair comparison, we performed a series of 
five baseline experiments with results listed in Table 
5. The MFCC baseline is in column (1) of Table 5. 
We also constructed tandem system with a conven-
tional monolithic MLP (MLP (mono)) as well as 
with conventional cascade two-stage monolithic 
MLPs (2-stage MLP (mono)), with results listed in 
columns(2)(3) of Table 5 respectively. In the latter 
case, the posteriors estimated from the first MLP 
were used as the input for the second stage MLP. 
We also implemented the rescoring process on top 
of the baseline tandem system in column (3), with 
the last term of rescoring in the right hand side of 
(14) obtained from either a conventional monolithic 
MLP (RSC by MLP (mono)) or a conventional 
two-stage monolithic MLP (RSC by 2-stage MLP 
(mono)), with results respectively listed in columns 
(4) and (5) of Table 5. From the character error rate 
(CER) listed in Table 5, incremental improvements 
step by step by the approaches can be easily ob-
served. 
 
 
Table 5. Character error rate (CER) for the five 
baseline systems. (2)(3) tandem with conventional 
monolithic MLPs, (4)(5) with rescoring. 
 
3.3.3. Recognition Results 
In Table 6, the results for CHMLP(BU) pro-
posed in this paper and those with rescoring are 
demonstrated. Since in the previous work it was 
found CHMLP with 2-level structures performed 
the best, here only three structures of CHMLP with 
2 levels were compared, where CHMLP(m/n) 
means hierarchy of n levels and m leaves. So listed 
in rows (a)(b)(c) of Table 6 are CHMLP of 2 levels 
with 3,4 and 6 leaves. In columns labeled (2) and (3) 
of Table 6 are the results for Tandem systems with 
the previously proposed CHMLP [14] and the new 
approach of CHMLP(BU) proposed here, respec-
tively to be compared with columns (2) and (3) in 
Table 5. For columns labeled (2) the comparison is 
between using conventional monolithic MLP and 
using CHMLP, while for columns labeled (3) the 
comparison is between using conventional 2-stage 
monolithic MLPs and the proposed 2-level hierar-
chical CHMLP(BU). In both cases it is clear 
CHMLP and CHMLP (BU) perform better. It is also 
clear by comparing columns labeled (3) with (2) in 
Table 6 that CHMLP (BU) proposed here is always 
better than CHMLP proposed previously. Also 
listed in columns labeled (4) and (5) of Table 6 are 
the results for rescored CHMLP(BU) Tandem sys-
tems, respectively using a conventional monolithic 
MLP or a conventional 2-stage monolithic MLP for 
rescoring, to be compared to columns (4) and (5) in 
Table 5.Again the new approach proposed here 
performed better. It is also clear that in each row of 
Table 6 the performance was improved step by step 
from left to right (columns labeled (2) to (5)). 
The above results are also shown in Fig. 7, in 
which each set of 4 bars are the 4 results for col-
umns labeled (2)(3)(4)(5) in Tables 5 and 6. The 
first set is for the baselines in Table 5, while the 
next three sets for the three rows in Table 6 for three 
CHMLP structures. An important observation here 
is that for the previously proposed CHMLP (labeled 
(2)), the best results was obtained with 3 leaves 
(CHMLP (3/2)), and more leaf MLPs wasn’t able to 
offer better accuracy. As mentioned previously, too 
many leaves may cause confusion in higher level 
MLPs. However, with the bottom-up processing 
proposed here (next 3 bars in each set labeled (3) (4) 
(5)), the accuracy were continuously improved as 
more leaf MLPs were used, so the above problem of 
CHMLP has been solved with bottom-up processing 
proposed here. The frame error rates (FER) of 
CHMLP (BU) (labeled (3)) are shown in Table 7. It 
can be found the decrease of FER with increase of 
leaf MLPs is consistent with the decrease in CER in 
Tables 5 and 6. 
 
 
Table 6. Character error rate (CER) for approaches 
proposed here to be compared with those in Table 5 
respectively. (2) with previously proposed 
CHMLP(3) with bottom-up processing, (4)(5) with 
rescoring.  
 
 
Table 7. Frame error rate (FER) of CHMLP(BU) 
approaches in columns labeled(3) in Table (5) and 
(6). 
 
To investigate the classification capability 
against confusing phones in the same leaf clusters, 
11 
 
spectro-temporal, pitch, and cepstral features) when 
used with tonal acoustic units. 
 
4.2. Proposed Approach 
Here we introduce the three types of features and 
the framework of using tonal acoustic units in our 
Tandem recognition systems. 
 
4.2.1. MFCC features 
The cepstral features we use here are MFCCs 
obtained with a 25ms window and a 10ms shift. The 
39-dimension feature vectors include c0 to c12 plus 
derivatives. Each vector is then concatenated with 
its previous and following four feature vectors as 
the input for MLP. 
 
4.2.2. Pitch features 
Smoothed log-pitch estimate plus its first and 
second derivatives are extracted [37]. Nine neigh-
boring frames are concatenated for a 27-dimension 
vector as the input of MLP (referred to as pitch fea-
tures F0 below). 
 
4.2.3. Spectro-temporal features 
We use 2-D Gabor filters to extract the spec-
tro-temporal modulation information. The impulse 
response of Gabor filter G(t,f) is 
 
( ) ( )
( ) ( )],exp[
22
exp
2
1),(
00
2
2
0
2
2
0
ttiffi
ttffftG
tf
tftf
−+−×








−−
+
−−
×=
ωω
σσσσpi
  (18) 
 
where σt, σf, ωt, and ωf are the four parameters that 
decide the shape of each filter. Per previous work 
[36], we selected the parameter set to divide the 
temporal modulation frequency from 1 to 16 Hz and 
the spectral modulation frequency from zero to two 
cycles per octave equally on a logarithmic scale [31], 
which was found to most closely correspond to hu-
man speech recognition [38]. These Gabor filter pa-
rameters were further divided into four streams, 
each corresponding to one Gabor filter bank, to 
cover from low to high spectro-temporal modulation 
frequency bands [31, 36]. As shown in the upper 
part of Fig. 8, we convolve the log mel-spectrogram 
of speech signals with the four filter banks and thus 
obtain four streams of spectro-temporal features for 
MLP training. 
 
4.2.4. Tandem systems 
In the lower part of Fig. 8, we show how to es-
timate the posteriors for the tonemes using features 
mentioned above with the aid of artificial neural 
networks (ANN). Each stream of spectro-temporal 
features are respectively concatenated with pitch 
features F0 mentioned above at the frame level. 
Each stream of the augmented features are fed into 
an MLP with the tonemes as its training target. The 
MLP output is a vector of posteriors; each element 
corresponds to the probability of a specific toneme 
given the input features for the present time frame. 
We then merge the four streams of posteriors by 
taking the geometric mean over the four vectors 
frame by frame. We denote the resulting vectors as 
F0+Gabor posteriors and use them in the following 
system. We similarly augment the MFCC features 
with F0 and train an MLP. The output vectors are 
denoted as F0+MFCC posteriors. Due to the com-
plementarity between Gabor and MFCC features 
investigated previously [36], we further merge the 
F0+Gabor and F0+MFCC posteriors by concatenat-
ing the two, obtain an even better estimation for 
posteriors, and denote them as F0+Gabor+MFCC 
posteriors. In addition, Gabor, MFCC and Ga-
bor+MFCC (merging the two by concatenation) 
posteriors are also obtained where we trained the 
MLP without pitch feature F0 for comparison. 
 
Mel filter bank
spectrogram
logConvolution
with 4 Gabor 
filter banks
F0
●
●
●
●
●
●
●
●
●
●
●
●
F0 F0 F0
MLP MLP MLP MLP
posteriors posteriors posteriors posteriors
 
Fig. 8: The generation of F0+Gabor posteriors. 
 
The posteriors are first transformed by a loga-
rithm function which maps the range of the value 
between zero and one to negative real. We then use 
linear discriminant analysis (LDA) transformation 
for dimension reduction and decorrelation. We re-
tain 95% of the total variance. Both steps are em-
ployed for better Gaussian modeling in the follow-
ing HMM system. As pointed out in the previous 
study [35, 36], we further perform the mean and 
13 
 
targets to tonemes helps learning more tonal infor-
mation at the front-end ((i) and (j)). Without such 
information, what the tonal HMM units can do in 
discriminating the tones is limited. However, the 
improvement from the tonemes for MLP and tonal 
RCDIFs for HMM units are additive. 
Finally, we augment the MLP input features 
with F0 and report the result in (k) to (m). Adding 
the pitch features offered consistent improvement 
over (h) to (j); the improvements from (h) to (k) and 
(j) to (m) are statistically significant while (i) to (l) 
are not. Additional to the information from 
mel-spectrogram where the MFCC and Gabor fea-
tures are extracted, the F0 provide clues directly 
from speech signals and they are beneficial. In our 
experiments, the system integrating pitch, cepstral 
and spectro-temporal features and modeling tone 
and phoneme variation simultaneously (tonemes for 
MLP and tonal RCDIFs for HMM), (m), achieved 
the best performance, 4.1% absolute or 19.3% rela-
tive CER improvement compared to the conven-
tional Tandem system baseline, (b). 
 
4.3.3. Analysis 
Here, we investigate the frame-level tone and 
phoneme classification accuracy for the posteriors 
with different MLP input features. Firstly, we show 
the frame accuracy of the four tones and their total 
in Fig. 9. The six bars in each tone type correspond 
to the accuracy for six toneme posterior sets (used in 
(h) to (m) in Sec. 3.2): MFCC, Gabor, Ga-
bor+MFCC, F0+MFCC, F0+Gabor, and 
F0+Gabor+MFCC. To calculate the frame-level 
tone accuracy, we excluded consonants and summed 
the posterior probabilities of all tonemes having the 
same tone but varied phoneme parts, frame by frame. 
Thus, only four probabilities were obtained for the 
four tones although there are 75 tonemes. We then 
classified the tone following the maximum a poste-
riori (MAP) criterion on the tone posteriors. 
 
 
Table 9. CER for each recognition system with var-
ied features and training targets. 36 phonemes or 75 
tonemes for MLP target, and 147 RCDIFs, or 257  
tonal RCDIFs as HMM units 
 
 
Fig. 9: The frame-level tone accuracy of different 
features used with MLP for tonemes 
 
 
Fig. 10: The frame-level phoneme accuracy for 
different MLP targets and input features. 
 
There is a common trend for all the four tones. 
In the first three bars, the Gabor features resulted in 
the best accuracy while MFCC the worst. Because 
the 2-D Gabor filters have different sloped pattern 
in spectrogram according to their parameters and 
respond to different spectro-temporal modulation, 
the Gabor features are more sensitive than cepstral 
features to frequency component changing across 
time and frequency band. Therefore, Gabor features 
retain more information needed for tone classifica-
tion and improve accuracy. Owing to that the per-
formance of MFCC features was much worse than 
Gabor, and combining the two sets of posteriors 
slightly deteriorated the performance. In the last 
three bars, we additionally appended F0 to features 
in the first three. With the additional pitch features, 
the accuracy is consistently improved in every tone 
and every feature set. As mentioned above, even 
though Gabor filters parameterize considerable 
spectro-temporal modulation components, the 
mel-spectrogram, where the Gabor features are ex-
tracted, still somewhat smooth the spectrogram. The 
F0 features provide additional information from the 
15 
 
Workshop, 2004 
[7] Zhu, Q., Stolcke, A., Chen, B., and Morgan, N, “Im-
proved MLP structures for data-driven feature extraction 
for ASR”, Interspeech 2005 
[8] Schwarz, P., Matejka, P., Cernock J., “Hierarchical struc-
tures of neural networks for phoneme recognition”, 
ICASSP 2006 
[9] Antoniou, C., “Modular neural networks exploit large 
acoustic context through broad-class posteriors for con-
tinuous speech recognition”, ICASSP 2001 
[10] Fritsch, J. “ACID/HNN — Clustering hierarchies of neu-
ral networks for context-dependent connectionist acoustic 
modelling”, ICASSP 1998 
[11] Sivadas, S. and Hermansky, H., “Hierarchical tandem 
feature extraction”, ICASSP 2002 
[12] Fabio Valente, Jithendra Vepa1, Christian Plahl, Christian 
Gollan, Hermansky, H., Ralf Schlüter, “Hierarchical neu-
ral networks feature extraction for LVCSR system”, In-
terspeech 2007 
[13] X. Lei, M.-Y. Hwang, and M. Ostendorf, “Incorporating 
tone-related MLP posteriors in the feature representation 
for Mandarin ASR,” Interspeech 2005 
[14] M.-Y. Hwang, Gang Peng, Wen Wang, Arlo Faria, Aaron 
Heidel, Mari Ostendorf, “Building a highly accurate 
Mandarin speech recognizer,” ASRU 2007 
[15] H. Misra, H. Bourlard, and V. Tyagi, “New entropy based 
combination rules in HMM/ANN multi-stream ASR”, 
ICASSP 2003. 
[16] Fosler-Lussier, E. and Morris, J. “Crandem systems: 
Conditional random field acoustic models for hidden 
macrov models”, ICASSP 2008 
[17] Siniscalchi, S., Schwarz, P and Chin-Hui, L. 
“High-accuracy phone recognition by combining 
high-performance lattice generation and knowledge-based 
rescoring”, ICASSP 2007 
[18] Hermansky H. and Fousek P., “Multi-resolution rasta 
filtering for tandem-based ASR.,” Interspeech 2005 
[19] Valente, F. and Hermansky, H. ”Hierarchical and parallel 
processing of modulation spectrum for ASR application”, 
ICASSP 2008 
[20] Ketabdar, H and Bourlard, H. “Hierarchical integration of 
phonetic and  lexical knowledge in phone posterior es-
timation” ICASSP 2008 
[21] S-Y. Chang and L-S Lee. ”Data-driven clustered hierar-
chical tandem system for LVCSR”, Interspeech 2008 
[22] Frantisek, G and Fousek, P. “Optimizing Bottle-Neck 
features for LVCSR” ICASSP 2008 
[23] T. Lee, W. Lau, Y.-W. Wong and P.-C. Ching, “Using 
tone information in Cantonese continuous speech recog-
nition,” ACM Trans. Asian Language Info. Process, vol. 1, 
pp. 83–102, 2002. 
[24] H.-L. Wang, Y. Qian, F. K. Soong, J.-L. Zhou and J.-Q. 
Han, "A Milti-Space Distribution (MSD) Approach to 
Speech Recognition of Tonal Languages,” in Proc. Inter-
speech, 2006. 
[25] L.-W. Cheng, and L.-S. Lee, “Improved Large Vocabu-
lary Mandarin Speech Recognition by Selectively Using 
Tone Information with a Two-stage Prosodic Model,” in 
Proc. Interspeech 2007. 
[26] H.-C. Huang and F. Seide, “Pitch tracking and tone fea-
tures for Mandarin speech recognition,” in Proc. ICASSP, 
2000. 
[27] E. Chang, J.-L. Zhou, S. Di, C. Huang and K.-F. Lee, 
“Large vocabulary Mandarin speech recognition with dif-
ferent approaches in modeling tones,” in Proc. ICSLP, 
2000. 
[28] F. Valente, M. Magimai.-Doss, C. Plahl, S. Ravuri and W. 
Wang, “A comparative large scale study of MLP features 
for Mandarin ASR,” in Proc. Interspeech, 2010. 
[29] D.A. Depireux, J.Z. Simon, D.J. Klein and S.A. Shamma, 
“Spectro-temporal response field characterization with 
dynamic ripples in ferret primary auditory cortex”, J. 
Neurophysiology, vol. 85, pp. 1220¬–1234, 2001. 
[30] M. Kleinschmidt and D. Gelbart, “Improving word accu-
racy with Gabor feature extraction”, in Proc. ICSLP, 
2002. 
[31] S. Zhao and N. Morgan, “Multi-stream spectro-temporal 
features for robust speech recognition”, in Proc. Inter-
speech, 2008. 
[32] S. Ganapathy, S. Thomas and H. Hermansky, “Robust 
spectro-temporal features based on autoregressive models 
of Hilbert envelopes”, in Proc. ICASSP, 2010. 
[33] X. Domont, M. Heckmann, F. Joublin and C. Goerick, 
“Hierarchical spectro-temporal features for robust speech 
recognition”, in Proc. ICASSP, 2008. 
[34] S. Thomas, N. Mesgarani and H. Hermansky, “A Multi-
stream Multiresolution Framework for Phoneme Recogni-
tion”, in Proc. Interspeech, 2010. 
[35] S. Ravuri and N. Morgan, “Using Spectro-Temporal Fea-
tures to Improve AFE Feature Extraction for ASR”, in 
Proc. Interspeech, 2010. 
[36] S.-W. Li, L.-C. Sun and L.-S. Lee., “Multi-stream spec-
tro-temporal and cepstral features based on data-driven 
hierarchical phoneme clusters”, in Proc. ICASSP, 2011 
[37] X. Lei, M.-H. Siu, M.-Y. Hwang, M. Ostendorf and T. 
Lee, “Improved Tone Modeling for Mandarin Broadcast 
News Speech Recognition,” in Proc. Interspeech, 2006. 
[38] T. Chi, Y. Gao, M.C. Guyton, P. Ru and S.A. Shamma, 
“Spectro-temporal modulation transfer functions and 
speech intelligibility,” J. Acoust. Soc. Am., 
106:2719¬–2732, 1999. 
[39] B. Meyer and B. Kollmeier, “Complementarity of 
MFCC, PLP and Gabor features in the presence of 
speech-intrinsic variabilities”, in Proc. Interspeech, 2009. 
    本次會議於美國加州柏克萊的 Claremont 舉行。大會投稿論文約 200 篇，接
受發表論文 88 篇。四天會議中共 3 個 keynotes、2 個 tutorial、4 個 award sessions
以及 80 個 poster/demo session，同時在會場內進行，各個議程分別就語音及語言
處理當前最新技術與應用，以及各個前瞻性主題做深入與完整探討。 
二、與會心得 
此次與會包含許多學術界及科技界重要人士，算是重要的國際會議，與會
學者皆為語音/語言處理相關領域之專家，討論之熱烈，可想而知。除了會議進
行時的討論外，連中場 Coffee Break 的時間，亦可看到許許多多學者仍不斷針對
他們的疑問與作者進行交流，討論之熱絡，是參加國內研討會時，並不多見的
景象。 
SLT 堪稱是語音及語言處理最重要的學術會議之一，論文審查過程嚴謹，議
題也涵蓋當今語音及語言處理技術之範疇，為目前語音辨識等研究領域上最前
瞻與創新的成果。 
三、考察參觀活動(無是項活動者略) 
四、建議 
希望國科會能繼續支持鼓勵國內專家學者出國參加國際會議，以開拓國
際化的視野，引進更多國際上的研究題材，並提升我國形象及實力。 
五、攜回資料名稱及內容 
會議資料。
Fig. 1. A simpliﬁed partial list of the word-based PAT tree
used to computed the branching entropy
Fig. 2. Sistrings of ”is hidden Markov model a key term”
strings of symbols called Patricia tree (PAT tree) [11], with a
simpliﬁed example is shown in Fig.1. It is a specialized struc-
ture based on trie that is used to store strings, and the symbol
we used here is the word. We show a simpliﬁed partial list of
the PAT tree constructed by semi-inﬁnite strings (Sistrings)
of sentences in the lecture corpus (slides and transcriptions),
where semi-inﬁnite strings are sequences of words starting at
any position of the corpus and continuing to the right [12].
Several examples of such semi-inﬁnite strings are listed in
Fig.2. We segment each in the corpus (for example, ”hidden
Markov model...”) to its Sistrings (”hidden Markov model...”,
”Markov model...” and ”model...”) and use these Sistrings to
build the PAT tree. However, some works had been reported
on PAT tree-based Chinese key phrase extraction using mu-
tual information [13], but in this work, we proposed another
approach, branching entropy, to extract key phrases.
The right branching entropy of a patternX of two or more
words considered is deﬁned as
p(xi) =
fxi
fX
, (1)
Hr(X) = −
n∑
i=1
p(xi) log2 p(xi), (2)
where X is the pattern of interest (e.g., ”hidden Markov”),
and xi is a child pattern of X (e.g., ”hidden Markov model”,
”hidden Markov chain” for ”hidden Markov”), and fX and
fxi are the frequency counts of X and xi respectively. Thus
p(xi) is the probability of having xi given X , and Hr(X) is
therefore the right branching entropy of X , where n is the
number of different child patterns xi of X .
When a pattern ”hidden Markov model” is a key phrase,
not only its frequency count is high, but most patterns of ”hid-
den Markov” are all followed by the word ”model” (so ”hid-
denMarkov” has a lowHr(X)), while the patterns of ”hidden
Markov model” are followed by many different words such as
”is”, ”can”, ”to”, ”with”... (so ”hidden Markov model” has a
high Hr(X)). In this way we can use the right branching en-
tropyHr(X) to identify the right boundary of a key phrase (it
is to the right of ”model” rather than the right of ”Markov” in
the above example) by setting thresholds for Hr(X).
Similarly we can construct a ”reverse PAT tree” using re-
verse sequences of words in the sentences of the corpus (e.g.
”... model Markov hidden ...”) and deﬁne a left branching en-
tropy Hl(X) for each reverse pattern X . This left branching
entropy Hl(X) can be used in exactly the same way to iden-
tify the left boundary of a key phrase (e.g. the left boundary
of the phrase ”hidden Markov model” is to the left of ”hid-
den” rather than the left of ”Markov”, because ”hidden” is
preceded by many different words, while ”Markov” is almost
always preceded by ”hidden”.)
The above procedure requires searching through the
whole corpus to calculate the left/right branching entropy
for every pattern in the corpus. This is quite time consuming
if the corpus is large, while PAT tree allows efﬁcient key
phrase extraction in this way. In the test, we can compute the
average Hr(X) for all X and average Hl(X) for all X , and
then extract patterns X whose Hr(X) and Hl(X) are both
higher than the average values to be the key phrases.
3. FEATURE SETS FOR KEYWORD EXTRACTION
Three different sets of features are used here: prosodic fea-
tures, lexical features and semantic features. They are sum-
marized here in this section.
3.1. Prosodic Features
Substantial works demonstrated that prosodic information is
useful for summarization of spoken documents, which im-
plied that prosodic features can help extract salient sentences.
It was argued that prosodic features in conference lectures are
less useful because of the speaker variability [6]. Since the
course lectures considered here usually consist of speech pro-
duced by a single instructor, the prosodic features may help.
It was also claimed that presenters usually use prosodic varia-
tion, changes in pitch, intensity and speaking rate for tagging
important contents in their speech [7].
In this work, the prosodic features were obtained from
the phonetic units segmented by HTK forced alignment [14].
For each term, only the prosodic features for it when it was
produced at the ﬁrst time in the lectures were used. A total of
twelve prosodic features was used and presented below.
3.1.1. Duration Related Features
Because different phonetic units have quite different dura-
tions, we ﬁrst compute the average duration of each phonetic
unit using the whole course lectures. For each phonetic unit
in a term, we then normalize its duration by its average value.
Finally, for each term, we use the maximum, minimum, mean
254
3.3.3. Latent Topic Entropy (LTE)
Latent Topic Entropy (LTE), EN(ti), for a given term ti can
be calculated in equation (6) from the distribution P (Tk|ti)
estimated in equation (4) [9],
EN(ti) = −
K∑
k=1
P (Tk|ti) logP (Tk|ti). (6)
Lower EN(ti) indicates that the distribution of P (Tk|ti)
is more focused on fewer latent topics and therefore ti carries
more topical information and is more likely to be a key term.
Table 1. All features used in this work
Feature Name Feature Description
Pr
os
od
ic
Fe
at
ur
es
Duration I maximum of normalized duration
Duration II minimum of normalized duration
Duration III mean of normalized duration
Duration IV range of normalized duration
Pitch I F0’s maximum value
Pitch II F0’s minimum value
Pitch III F0’s mean value
Pitch IV F0’s range
Energ I maximum energy value
Energy II mimimum energy value
Energy III mean energy value
Energy IV range of energy value
L
ex
ic
al
Fe
at
ur
es TF tfi
IDF idfi
TF-IDF tfidfi
Left Context lcvi
Left Context Nor lcvni
PoS the PoS tags
Se
m
an
tic
Fe
at
ur
es
LTP I variance of LTP
LTP II standard deviation of LTP
LTP III mean of LTP
LTP IV LTP I / LTP III
LTP V LTP II / LTP III
LTS I variance of LTS
LTS II standard deviation of LTS
LTS III mean of LTS
LTS IV LTS I / LTS III
LTS V LTS II / LTS III
LTE term entropy for latent topics
4. LEARNING METHODS FOR KEYWORD
EXTRACTION
4.1. Unsupervised Learning
We use Sti(Tk) in equation (7) to construct a feature vector
for each term ti,
vi = (Sti(T1), Sti(T2), ..., Sti(TK)), (7)
where K is the number of latent topics. Therefore, vi repre-
sents the position of the term in the latent semantic space.
We use the K-means algorithm to cluster the terms based
the above vectors vi and extract all exemplars of the clusters
as the key terms. An exemplar is the vector closest to the
median of all vectors in the same cluster. We assume that
terms close to a key term in the latent semantic space usually
form a cluster, and the key term is close to the median of the
cluster [19].
4.2. Supervised Learning
We use all the features in Section 3 and two learning methods
to train the classiﬁers, the AdaBoost and neural network.
4.2.1. AdaBoost
AdaBoost (adaptive boosting) was proposed to obtain a highly
accurate classiﬁer by combining many basic classiﬁers [20].
Given a training set {xn, yn}Nn=1, where xn is the feature vec-
tor of a training sample, yn is the desired label (+1 for key
term and −1 for non-key term), and N is the total number of
training terms. We use decision stump as the basic hypothesis
hs,i,θ.
hs,i,θ(x) = sign (s · (x)i − θ), (8)
where x is the feature vector, (x)i is the i-th component of x,
i ∈ {1, 2, . . . , d}, s ∈ {+1,−1}, and θ ∈ R is a threshold.
Initially, we assign the same weights to all decision
stumps (un = 1N for all n) in equation (9). In each itera-
tion l, we then choose the best hypothesis hl in equation (9),
compute the conﬁdence weight αl in equation (10) for it, and
re-estimate un as in equation (11),
hl = arg min
hs,i,θ
N∑
n=1
un · yn = hs,i,θ(xn), (9)
αl =
1
2
ln
1− l
l
, (10)
u′n = un · exp (−αtynht(xn)), (11)
where l is the weighted error for the iteration l. After many
iterations, we can get combined hypothesis H , which is pa-
rameterized as
H(x) = sign (
T∑
l=1
αlhl(x)), (12)
where T is the total number of iterations.
4.2.2. Neural Network
We implement the backpropagation algorithm for 3-layered
neural network (b-J-1) with tanh-type neurons [21], in which
b is dimension of the feature vectors, and J is the number
of neurons in the second layer. Stochastic gradient descent
is used for many iterations to ﬁnd the ﬁnal hypothesis that
minimizes the error of the training data.
256
is useful, giving an F-measure ranging from about 20% to
about 42%. Row (d) indicates intergrating prosodic and lex-
ical (Pr+Lx) features is better than using either one alone in
rows (a) and (b), so the two sets of features are additive. Row
(e) shows using semantic (Sm) features in addition helped sig-
niﬁcantly. Therefore, all the three sets of features are useful.
Table 3. Keyword extraction (not including key phrase) using
different sets of features (%)
Features Precision Recall F-measure
(a) Pr 21.92 19.75 20.78
(b) Lx 33.57 59.26 42.86
(c) Sm 37.80 33.70 35.63
(d) Pr+Lx 48.15 48.15 48.15
(e) Pr+Lx+Sm 63.08 51.25 56.55
Pr: Prosodic, Lx: Lexical, Sm: Semantic
5.3.3. Overall Results on Manual and ASR Transcriptions
Finally, the overall results for both key phrases and keywords
by combining the results in Table 2 are listed in Table 4. The
baseline to be compared is the conventional method, of us-
ing TF-IDF without considering the key phrases as a different
type of key terms. From this table, we ﬁnd that using branch-
ing entropy to extract key phrases is very useful for both man-
ual and ASR transcriptions. Since it improved the F-measure
from 23.38% to 51.95% (manual transcriptions) or 43.51%
(ASR transcriptions). In other word, once a pattern (e.g. ”in-
formation theory”) is extracted by the branching entropy, the
probability that it is a key term becomes high. On the other
hand, the K-means exemplar approach proposed here is also
reasonably good, considering the fact that it didn’t use anno-
tations from the subjects. The best result is the supervised
learning using neural network, which offered an overall F-
measure of 67.31% for manual transcriptions and 62.70% for
ASR transcriptions after combining with the key phrases from
branching entropy.
Table 4. Overall performance (for key phrase from branch-
ing entropy plus keywords) using different approaches from
manual or ASR transcriptions (%)
Approach Precision Recall F-measure
M
an
ua
l
Baseline 23.38 23.38 23.38
U TF-IDF 51.95 51.95 51.95Exemplar 55.84 55.84 55.84
S AdaBoost 56.61 69.48 62.39NN 67.10 67.53 67.31
A
SR
Baseline 20.78 20.78 20.78
U TF-IDF 43.51 43.51 43.51Exemplar 52.60 52.60 52.60
S AdaBoost 51.11 66.19 57.68NN 60.61 64.94 62.70
U: unsupervised, S: supervised
6. CONCLUSIONS
In this paper we propose a set of unsupervised and supervised
approaches for key term extraction from spoken documents,
and use a corpus of course lectures for experiments. We di-
vide the key terms into two types: key phrases and keywords,
and develop different approaches to extract them in order.
Very encouraging results were obtained in the experiments.
7. REFERENCES
[1] A. Hulth and et al, ”Automatic keyword extraction using do-
main knowledge,” in Computational Linguistics and Intelli-
gent Text Processing, 2004.
[2] Y.H. Kerner, Z. Gross, A. Masa, ”Automatic extraction and
learning of keyphrases from scientiﬁc articles,” in Computa-
tional Linguistics and Intelligent Text Processing, 2005.
[3] P. Turney, ”Learning algorithms for keyphrase extraction,” in
Information Retrival, 1999.
[4] F. Liu, F. Liu, Y. Liu, ”Automatic keyword extraction for the
meeting corpus using supervised approach and bigram expan-
sion,” in SLT, 2008.
[5] F. Liu and et al, ”Unsupervised approaches for automatic key-
word extraction using meeting transcripts,” in NAACL, 2009.
[6] J.J. Zhang, H.Y. Chan, P. Fung, ”Improving lecture speech
summarization using Rhetorical Information,” in ASRU, 2007.
[7] J. Hirschberg, ”Communication and prosody: functional as-
pects of prosody,” in Speech Communication, 2002.
[8] S. Knog, L. Lee, ”Improved summarization of chinese spoken
documents by Probabilistic latent semantic analysis (PLSA)
with further analysis and integrated scoring,” in SLT, 2006.
[9] S. Knog, L. Lee, ”Improved spoken document summariza-
tion using probabilistic latent semantic analysis (PLSA),” in
ICASSP, 2006.
[10] S. Kong, M. Wu, C. Lin, Y. Fu, L. Lee, ”Learning on demand
- course lecture distillation by information extraction and se-
mantic structuring for spoken documents,” in ICASSP, 2009.
System available at http://speech.ee.ntu.edu.tw/˜RA/lecture/.
[11] D.R. Morrison, ”PATRICIA-practical algorithm to retrieve in-
formation coded in alphanumeric,” in Journal of ACM, 1968.
[12] D.E. Knuth, ”The art of computer programming: sorting and
searching,” Vol. 3. Addison-Wesley, Mass, 1973.
[13] T. Ong, ”Updateable PAT-tree approach to Chinese key phrase
extraction using mutual information: a linguistic foundation
for knowledge management,” in Second Asian Digital Library
Conference, 1999.
[14] S. Young and et al, ”The HTK Book (for HTK Version 3.0),”
Cambridge University, 2000.
[15] Entropic, Inc., ”ESPS/waves+ with EnSig 5.3 Release Notes.”
[16] W. Lin, ”Tone recognition for ﬂuent mandarin speech and its
application on large vocabulary recognition,” in Master’s the-
sis of NTU, 2004.
[17] H. Schmid, ”Probabilistic part-of-speech tagging using deci-
sion trees,” in New Methods in Language Processing, 1994.
[18] T. Hofmann, ”Probabilistic latent semantic analysis,” in Uni-
versity in AI, 1999.
[19] Z. Liu, P. Li, Y. Zheng, M. Sun, ”Clustering to ﬁnd exemplar
terms for keyphrase extraction,” in ACL and AFNLP, 2009.
[20] Y. Freund, R.E. Schapire, ”Experiments with a new boosting
algorithm,” in ICML, 1996.
[21] S. Haykin, ”Neural networks: a comprehensive foundation,”
3rd edition, 2008.
258
二、與會心得 
此次與會人數超過千人，是相當大型國際會議，與會學者皆為訊號語音
處理相關領域之專家。除了會議進行時的討論外，連中場 Coffee Break 的時
間，亦可看到許許多多學者仍不斷針對他們的疑問與作者進行交流，討論熱
絡。每天一場的大型 Plenary (Keynote) speech, 均邀請相關領域的學界或業界
菁英給予不同主題的演講, 收穫豐富。而所有被接受的論文均以 Lecture 發表
或 Poster 展示。在 Lecture 議程方面, 討論極為熱烈, 可以得到許多對訊號處
理上的不同想法。而 Poster 展示方面, 面對面的互動, 提供了最直接的研究
討論與意見交換。 
在這次會議當中，訊號處理領域最重要的潮流當屬 compressed sensing。
此問題的來源希望探討自然界常見訊號的本質(如語音訊號)。根據 Shannon 
capacity，在有限 bitrate 下能表示的訊號頻寬有限，但很多自然界的訊號在訊
號頻譜空間的分佈並不是均勻的，因此如果可以對訊號有足夠的假設，那麼
可以用低於 Nyquist rate 來完整的傳遞訊號或信息。在語音研究中，常見的
做法是從大量的語音訊號中，在 sparse representation 的假設下找到一組展開
訊號空間的基底。而這組具有 sparsity 性質的少量基底即具有信號的代表性。
有些研究使用在將基底對應到音素單位之語音辨識、標型發現、或者機器學
習理論中的 regularization、也有手寫文字辨識的標型發現等等。 
其它在語音處理的重點包含了語音辨識的語者調適，有不少篇的論文在
討論 Microsoft Redmond 之前所提出 subspace Gaussian mixture modeling 
IMPROVED SPOKEN TERM DETECTION USING SUPPORT VECTOR MACHINES
BASED ON LATTICE CONTEXT CONSISTENCY
Hung-yi Lee †1, Tsung-wei Tu #, Chia-ping Chen †, Chao-yu Huang #, Lin-shan Lee †#2
Graduate Institute of Communication Engineering, National Taiwan University †
Graduate Institute of Computer Science and Information Engineering, National Taiwan University #
tlkagkb93901106@yahoo.com.tw1, lslee@gate.sinica.edu.tw2
ABSTRACT
We propose an improved spoken term detection approach that
uses support vector machines trained with lattice context consis-
tency. The basic idea is that the same term usually have similar
context, while quite different context usually implies the terms are
different. Support vector machine can be trained using query context
feature vectors obtained from the lattice to estimate better scores for
ranking, and signiﬁcant improvements can be obtained. This pro-
cess can be performed iteratively and integrated with the pseudo rel-
evance feedback in acoustic feature space proposed previously, both
offering further improvements.
Index Terms— Spoken Term Detection, Query Context Consis-
tency, Support Vector Machine
1. INTRODUCTION
Spoken term detection (STD) refers to the retrieval from a large spo-
ken document archive of a list of spoken segments containing the
term requested by the user. This technology is crucial to accessing
multimedia content, including audio signals. In general, there are
two stages in STD [1]. The audio content is ﬁrst recognized and
transformed into transcriptions or lattices using a set of acoustic and
language models. The retrieval engine searches through the recogni-
tion results and then based on the entered query returns to the user a
list of possible relevant spoken segments. The returned segments are
usually ranked by the relevance scores derived from the recognition
output. As a result, STD performance depends heavily on the acous-
tic and language models used in recognition. However, in practice
the relatively poor performance of STD is due to the limited robust-
ness of the available acoustic and language models, in particular with
respect to the various topics represented in the audio content on the
Internet, as well as the variety of speakers under different conditions
in varying environments.
Recently it was proposed that by adjusting the relevance scores
of the spoken segments using either pseudo relevance feedback
(PRF) [2, 3, 4] or user relevance feedback [2, 5, 6], STD perfor-
mance may be made less dependent on recognition results that have
relatively high error rates. The acoustic models used in recognition
can be adapted using user relevance feedback [5, 6]. Also, instead of
relying solely on the recognition output, information obtained from
the acoustic features such as the MFCCs can be used to improve
retrieval performance [3]. These works notably take into account
only acoustic-level information and ignore the linguistic context of
the query.
A major problem in STD is the uncertainty in speech recogni-
tion, in particular the confusion among similarly pronounced words.
The use of context information has been proposed to verify the pres-
ence of spoken terms [7]. Occurrences of a given term are usually
characterized by similar context, while widely-varying contexts typ-
ically denote different terms. A previously mentioned example [7]
is shown in Fig. 1 to illustrate this concept. For the user query
“mouse”, spoken segment A with the phrase “the mouse trap” is
relevant, whereas spoken segment B with the phrase “a house boat”
is irrelevant. However, as the term “house” may be easily misrec-
ognized as “mouse”, spoken segment B may be retrieved as a false
alarm. If the system knows (for instance via language model con-
straints) that “mouse” is unlikely to have “boat” as its right context,
as in the phrase “a mouse boat”, the corresponding path score in the
lattice for “a mouse boat” will be lower, and with it the relevance
score of segment B. Thus we use the query context, that is, the
context of the query terms in the recognition results, to reﬁne the
relevance score of the spoken segments [7]. We obtain the query
context directly from the recognition lattices and use this informa-
tion with support vector machines (SVM) [8, 9, 10]. No external
training corpora are needed. While it is true that language models to
some extent already take into account query context, in real appli-
cations, it is difﬁcult to obtain in-domain text data, that is, language
model training data that matches the test domain. Also, lattice-based
query context can yield information about recognition errors; lan-
guage models trained on pure text, however, cannot properly model
query context that is generated by such recognition errors.
2. PROPOSED APPROACH
Fig. 2 shows the framework of the proposed approach. In ﬁrst-pass
retrieval (Section 2.1), conventional STD technologies rank the spo-
ken segments X based on the relevance scores derived from the
Recognition 
Output
(lattice)
User Enters Query “mouse”
Oracle 
Transcription
mousethe
…  the mouse trap  …
Spoken Segment A
trap
…  a house boat  …
Spoken Segment B
a
mouse b
True Positive
oat
False Alarm
Fig. 1: Example [7], showing that query context information has the
potential to discriminate relevant and irrelevant spoken segments.
5648978-1-4577-0539-7/11/$26.00 ©2011 IEEE ICASSP 2011
Cosine similarity SVM
α = 1 α = 10 α = 1 α = 10
Baseline 0.4819
N=5 0.4647 0.4359 0.5121 0.5185
N=10 0.4830 0.4541 0.5183 0.5437
N=15 0.4835 0.4590 0.5198 0.5506
N=20 0.4825 0.4543 0.5222 0.5523
N=25 0.4821 0.4582 0.5237 0.5516
N=30 0.4796 0.4571 0.5247 0.5514
Max RI(%) 0.33 - 8.88 14.61
Table 1: Comparison between different context consistency models
using f1Q(X) with different values of α in Eq. (2). Max RI represents
maximum relative improvement.
A negative sign is needed here because d(X) as in Fig. 2 is always
positive. The value of r′2(X) is then linearly normalized between 0
and 1 as r2(X).
2.5. Re-ranking and Iterations
The context consistency score is then integrated with the original
relevance score SQ(X) mentioned in Section 2.1 for re-ranking:
SˆQ(X) = SQ(X)r1(X)
α or SQ(X)r2(X)α, (2)
where α is a constant.
The above process can be conducted iteratively. That is, the
retrieval results re-ranked by SˆQ(X) in Eq. (2) can be taken as the
ﬁrst-pass retrieval results, and the above process can be repeated over
the re-ranked results again.
2.6. Integration with Pseudo Relevance Feedback in Acoustic
Feature Space
The retrieval results re-ranked as proposed above can be further
taken as the ﬁrst-pass results for pseudo relevance feedback (PRF)
in the acoustic feature space [3]. The top M spoken segments are
selected as the pseudo relevant spoken segment set1, and the system
calculates the dynamic time warping distances of each segment and
theM pseudo relevant segments over the hypothesized query occur-
rence region based on their corresponding MFCC sequences. The
distances are then transformed into similarity measures to be used to
further re-rank the segments.
3. EXPERIMENTS
3.1. Experimental Setup
We used as the test data 33 hours of recorded lectures of a course
offered at National Taiwan University. The acoustic model was
trained using the maximum likelihood criterion with 4602 state-tied
triphones spanned from 37 monophones using a corpus of noiseless
Mandarin read speech, including 24.6 hours of data produced by
100 males and 100 females. 39-dimension MFCCs were used as
the feature. There were ﬁve states per triphone and 24 mixtures per
state. A lexicon with 10.7K words was used, and a trigram language
model was trained on a news corpus. The character accuracy was
50.26%. We used mean average precision (MAP) as the retrieval
1The size of the pseudo relevant set for context consistency (N ) and PRF
in the acoustic feature space (M ) need not be the same.
f1Q f
2
Q f
3
Q f
4
Q
Baseline 0.4819
N=5 0.5185 0.5196 0.5528 0.5222
N=10 0.5437 0.5503 0.5552 0.5522
N=15 0.5506 0.5663 0.5653 0.5672
N=20 0.5523 0.5678 0.5662 0.5697
N=25 0.5516 0.5667 0.5634 0.5673
N=30 0.5514 0.5676 0.5528 0.5685
Max RI(%) 14.61 17.83 17.49 18.22
Table 2: Comparison of different query contextual feature represen-
tations with SVM and α = 10. Max RI represents maximum relative
improvement.
evaluation measure, for which we manually selected 80 single-word
testing queries. For SVM we used the tool SVMlight 2 with the
default parameters. The baseline (ﬁrst-pass retrieval) MAP was
0.4819.
3.2. Experimental Results
3.2.1. Different context consistency scores
First, we compared the context consistency scores derived using the
two approaches proposed in Section 2.4. α in Eq. (2) was set to ei-
ther 1 or 10. The top or bottom N spoken segments in the ﬁrst-pass
retrieval results were taken as the pseudo relevant and irrelevant spo-
ken segments, and N was set to 5, 10, 15, 20, 25, or 30. Note that
this means that it was possible for a spoken segment to belong to
both the pseudo relevant and irrelevant sets. In this experiment we
used the f1Q(X) context feature vector. The experimental results
in Table 1 show that the context consistency scores derived using
SVM outperformed those based on cosine similarity. The SVM con-
text consistency scores r2(X) improved the retrieval performance
regardless of the choice of α. For α = 10 and N = 20, the relative
MAP improvement using SVM was 14.61%. In the following exper-
iments, we used SVM context consistency scores and set α equal to
10.
3.2.2. Different query context feature representations
In this section, we compared the different query context feature vec-
tor representations described in Section 2.2. The experimental re-
sults in Table 2 show that f2Q(X), which takes into account the con-
text of the query throughout the entire segment, outperforms f1Q(X)
(considers immediate query context only). It is not surprising that
f3Q(X) (both left and right immediate query context) also outper-
formed f1Q(X). f
4
Q(X), the concatenation of f
2
Q(X) and f
3
Q(X),
slightly outperformed f2Q(X) in all cases and outperformed f
3
Q(X)
except for N = 5 and N = 10. N = 20 yielded the best results
for all types of feature vectors. The maximum relative improvement
(18.22%) was obtained using f4Q(X) for N = 20.
3.2.3. Iterations
In this experiment, we used the f4Q(X) feature vector and set N
equal to 20 to re-rank the ﬁrst-pass results iteratively using SVM-
based context consistency. The results are shown in Fig. 4, for which
the vertical axis is MAP, and the horizontal axis is the number of
2http://www.cs.cornell.edu/People/tj/svm_light/
5650
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/31
國科會補助計畫
計畫名稱: 語音事件整合、證據確認與後處理
計畫主持人: 李琳山
計畫編號: 97-2221-E-002-134-MY3 學門領域: 自然語言處理與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
ISCA Fellow 2010 
Meritorious Service Award, IEEE Signal Processing Society, 2011 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
