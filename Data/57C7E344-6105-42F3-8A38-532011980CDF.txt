 1 
行政院國家科學委員會專題研究計畫期中進度報告 
結合嵌入式Linux的SoC軟體快速雛型系統設計平台之研究與開發(3/3) 
計畫編號: NSC 94－2213－E－194－003 
執行期限: 94年 08 月 01 日 至 95 年 07 月 31 日 
主持人: 陳添福        國立中正大學資工所 
 
中文摘要 
隨著晶片所能容納的元件越來越多，因為本身的溝通
架構(communication architecture)所消耗掉的功率已經
佔了整體所能消耗的功率裡相當大的比率。在晶片系
統裡有著各式各樣不同的組成元件，而他們在功能及
溝通的需求上都不相同，所以，設計一個溝通拓撲
(communication topology)時，就應該針對這些元件不
同的溝通情況以及工作負載來做最佳化的調整設計。
在本計劃中，提出一個創新的連結架構，這個架構可
以利用十字型轉換開關動態的建構出兩個元件中一個
專屬的溝通路徑。我們將會介紹針對不同的應用，根
據profile的一些溝通特徵，如何去建構出匯流排溝通
架構的設計策略。根據應用的溝通情況，我們設計了
兩種匯流排操作模式。我們使用MPEG4 decoder和
JPEG的工作負載來做實驗，而實驗結果顯示出，如
果我們可以知道嵌入式軟體的應用，我們將可以善加
控制十字型轉換開關以及產生適當的匯流排的架構以
得到較佳的效能及較低的功率消耗。 
另外，隨著目前行動多媒體系統越來越普及 , 在行動
多媒體系統中電池壽命成為相當重要的因素。 由於
MPEG影片對於效能需求的變動性 , 我們可以有效的
來利用閒置的處理時間以達到能量的節省。 本計劃
提出了一個針對即時的影像解碼的極平順控制的機
制。 我們利用極平順演算法來找出一個合理並且越
平順越好的排程來達到大量能量的節省。 我們也利
用預測錯誤累加器來減低output buffer overflow 與 
underflow 的機會。 另外, 我們利用兩級最接近的電
壓/頻率等級來最佳化最佳平順控制在影像解碼中的
效能。 實驗結果顯示這個機制可以大量減少在影像
解碼的能源消耗並且提供即時特性保證與減少 output 
buffer overflow 與 underflow 的機會。 
 
關鍵字 
bus, SoC, Network on Chip(NoC), bus topology, 
Dynamic Voltage Scaling (DVS), MPEG Decoding, 
energy consumption, QoS 
 
 
ABSTRACT 
As the number of cores on a chip increases, power 
consumed by the communication structures takes 
significant portion of the overall power-budget. We first 
propose a novel interconnect architecture, which uses 
crossroad switches to dynamically construct a dedicated 
communication path between any two cores. We then 
present two application-specific bus operation schemes 
(Normal mode and lease line mode). Each switch may 
operate in a "lease line", which can dynamically offer a 
dedicated path between two high-communicative cores 
for a specific period according to the application 
characteristics. Finally, we present a concept of wrappers 
to help designers use our crossroad architecture as 
communication backbone. We take the JPEG and 
MPEG4 reference codes as our case studies and 
experimental results show the power consumptions can 
be saved if we dynamically control NoC switches when 
the behavior of the embedded software is well-known. 
As mobile multimedia systems become more and 
more popular, battery life become critical factor in 
mobile multimedia system. Because of the variability of 
MPEG stream, we can achieve energy saving by fully 
utilizing the idle processing time. This paper introduces 
an aggressively smoothing control mechanism for on-
line video decoding. We use the aggressively smoothing 
algorithm to find a flexible schedule that is as smooth as 
possible so as to achieve significant energy savings. We 
also use prediction error counter to decrease the 
opportunity of the output buffer overflow and underflow 
that is caused by prediction accuracy. We also exploit 
two closed discrete voltage/frequency level to maximize 
the performance of aggressively smoothing control in 
video decoding. Simulation results show that this 
mechanism significantly reduces energy for video 
decoding and also provides strong real-time guarantees 
and prevents output buffer overflow. 
 
1. INTRODUCTION 
In this research we propose the concept of virtual 
segment, which isolate the communication path by 
crossroad switches. In our design, only one sender, one 
receiver and one or more than two switches are involved 
at each data communication, which will result in 
minimum power consumption. For example, a core flow 
graph for application suite is shown as Figure 1(a), each 
node in the core flow graph represents a core, while each 
weighted edge represents the communication frequency 
between a pair of cores. Figure 1(b) shows the power-
aware core placement (PACP) results. When core 1 
communicates to core 2, signals will be propagated 
throughout the wires and switches that are connected 
between the two cores. In this case, the communication 
signals will not propagate to switches S2 and S3 which 
are not involved in the communication path. Thus, it can 
save power consumption. Some applications have high 
point-to-point communications. If it is not well 
controlled, large inter-communications across switches 
could consume large energy of NoCs. It can save power 
consumption by allocating highly communicate cores in 
local communication blocks, as shown in Figure 1(b). 
The new bus communication architecture is discussed in 
[A2, A3, A4]. 
 During execution of applications, there are some 
switches busy and there are some switches idle. For 
example, the interconnect topology is shown as Figure 
 3 
2. CROSSROAD INTERCONNECT 
ARCHITECTURE 
The basic communication unit of the architecture is the 
crossroad communication block (CCB) as shown in 
Figure 4(a). A communication block comprises a 
crossroad switch with an arbiter, a group of cores, and 
four bus links for data transmissions. More than one 
route can go through the switch at the same time as long 
as its input and output ports in the switch are not 
occupied by another route. Every crossroad switch only 
takes care of the requests from four different ways (up, 
down, left and right), so the control of the crossroad 
switch is scalable, and the design complexity is low. In 
this case, users can construct different structure styles 
(topology) of the crossroad communication architecture 
based on the requirements (power, performance, area) of 
their SoCs. 
 
 
Figure 4: The crossroad architecture and the CCB 
  
 Only the edge sides of the crossroad switches can be 
connected with cores. If two switches connect to each 
other, the bus line between the two switches cannot plug 
any cores, because it only provides data path between 
two CCBs. The global arbiter is used for specific 
purposes, such as deadlock-free algorithms, routing 
algorithms, contention-free algorithms, etc. We can 
design several algorithms to coordinate all switches to 
enhance the performance or low-power communication 
depending on the characteristics of different applications. 
Figure 5 shows how CCBs communicate to each other. If 
the Master 0 want to communicate to other CCBs, the bit 
that select crossroad direction will “open” the CMOS 
between CCBs and enable the transmission. 
 
 
Figure 5: Communication between CCBs 
 
2.1 Segmenting a bus 
In the crossroad interconnect architecture, each pair of 
cores has its dedicated communication path, called 
virtual bus segment (VBS), generated dynamically 
between two communicating cores. A virtual bus 
segment includes one or more than two switches on the 
path, and the physical wires for the address, data and 
control signals. When a master communicates to a slave 
in a virtual segment, the signals will be propagated 
throughout the wires connecting to the master and the 
slave. The communication signals will not propagate to 
other switches, modules and bus lines that are not 
involving in the segment. Thus, it can save power 
consumption. 
 
2.2 Features of crossroad bus architecture 
The switch in a CCB only takes care of the control and 
the data exchanges between four ways, so the 
architecture is scalable with fully configurable. When a 
master requests a slave in the local block, it passes the 
signals to the target slave. If a master requests a slave in 
another remote block, the local arbiter will act as a 
master in the remote block and sends the request to the 
remote switch. This feature makes the overall 
architecture more scalable and highly configurable. We 
can employ with a placement algorithm [A3] to explore 
the best solutions of the intercommunication for purposes 
of high performance, low power consumption and low 
cost. 
 
 
Figure 6: Three available combinations for parallel 
communications 
 
     In shared-bus architectures, every data transfer is 
broadcast, meaning the data must reach each possible 
receiver at great energy cost. Because the power 
consumption of each bus segment is proportional to the 
number of devices connecting to the segment in splitting 
bus architecture. In our design, only one sender, one 
receiver and one or more than two switches are involved 
at each data communication, data exchanges among 
devices will result in minimum power consumption. So 
the power consumption can be optimized by localization. 
Due to the full programmability, the placement of all 
cores will be optimized by profiling the communication 
traffic characteristics of applications, as shown in Figure 
1(b). In this case, it can save more energy consumption 
due to the reduction of long-distance communications. 
We design an "overpass" mechanism that can 
construct multiple segments to let different masters 
communicate with their target slaves at the same time, as 
shown in Figure 6(a). Figure 6(b) shows the three 
possible combinations of parallel communications in a 
local CCB. If those master-slave pairs are different, they 
may use different channels to communicate in the 
crossroad bus architecture. This behaviour is suitable for 
multiprocessor or multithreading, where each processor 
or thread can work in their local regions and 
communicate to other regions by coordinating crossroad 
switches in each block. It results in more communication 
parallelism than conventional shared-bus architectures. 
Figure 7 shows the circuit design of the switch in the 
CCB. 
 5 
4. THE COMPATIBILITY OF DIFFERENT IPS 
Users should inform the tool (wrapper generator) that 
these IPs should use what special communication mode 
if needed. And then the generator will know about the 
information as follows: 
1. The IP is AMBA mode or Wishbone mode. 
This will generate the suitable arbiter. That means we 
will consider the ports in AMBA or Wishbone. 
2. The IP uses some particular protocols or not. 
This will let us know the ports that are responsible for 
those particular protocols are used or not. And we will 
decide the functionality of the arbiter or some control 
units based on the information. Some information of 
wrappers is shown in Table 5. 
  
5. MPEG DECODING 
In this section, we will discuss the basic issue of MPEG. 
The MPEG video compression standard [B13] define a 
video stream as a sequence of frames and it compress 
video by exploiting temporal redundancy and spatial 
redundancy. A MPEG stream usually consists of three 
type frames: 
• I frames (Intra frames): frames in this type do not have 
any dependency with other frames. 
• P frame (Predicted frames): frames in this type are 
predicted frames and must refer recently I frames or P 
frames. 
• B frame (Bidirectional frames): frames in this type are 
predicted frames and must refer the closest previous 
and following I frames or P frames. 
In the common MPEG stream, I frames usually have 
the largest size, and B frames have the smallest size. I 
frames also spend more cycles than P frames and B 
frames, while B frames usually spend least. The frame 
decoding cycles can be predicted using the type of frame 
and the size of frame, and using the method to predict the 
decoding cycles have proved that it will less than 25% 
prediction errors [B4]. 
Another important issue in MPEG decoding is realtime 
constraint. The MPEG decoder should follow frames per 
second (fps) constraint. That is to say that when decoder 
decodes a frame it has a deadline time. If decoder can not 
finish decoding a frame in the deadline time, the frame 
must be dropped. In different MPEG clips, they may 
have the different fps. 
 
5.1 DYNAMIC VOLTAGE SCALING (DVS) 
The major low-power techniques using in system level 
include DVS and DPM (Dynamic Power Management). 
In DVS, it allows processor to dynamically scale 
frequency and voltage at run time when processor’s 
workload change. Lower the voltage of processor will 
create the opportunity for quadratic energy savings, but 
will also decrease the frequency of processor. Therefore, 
DVS must depend on workload variability to adjust 
adequate performance. In this way we could not only 
meet the performance of application but also save energy. 
Furthermore, DVS is suitable for realtime applications 
that have low workload during deadline period [B3]. In 
DPM, it is a dynamic power reduction technique that 
depend on application’s run time behavior to shut down 
unused hardware components [B5, B23]. 
The modern processors have supported DVS and have a 
set of supply voltage levels. For example, Transmeta’s 
Crusoe processor has 16 voltage/frequency levels and 
realizes DVS mechanism to save energy by LongRun 
power management [B1]. In DVS, when voltage change 
to another voltage that will cause overhead. In the 
previous research, they have implemented DVS systems 
and showed that voltage scaling takes about 70 us to 140 
us[B3, B19].In [B18], their processor is based on the 
ARM8 core, and have supply voltage between 1.1V and 
3.3V, and have respective frequency between 10 MHz 
and 100 MHz, and voltage transition overhead take about 
25 us for 10 MHz to 100 MHz. 
 
5.2 RELATED WORK 
 
5.2.1 PREDICTION-BASED DVS APPROACHES 
Prediction-based DVS approaches means that they 
predict the frame decoding cycles by certain prediction 
mechanism and prediction interval. As summary by 
Eriko et al [B17], prediction interval means how often 
the predictions are made and how much frames the 
decided decoding speed is used for. We classify three 
types of prediction interval, including per-GOP, per-
frame and intra-frame [B6]. Prediction mechanism 
means how to predict the decoding cycles or decoding 
time of an incoming frame or GOP. Currently, most of 
approaches use the linear relationship between frame 
type, frame size, frame decoding cycles (time) [B13]. In 
this prediction method, some are fixed method, and 
others are dynamic method. Fixed method means that the 
rule for predicting decoding cycles would not change 
during decoding. Dynamic method means that the rule 
for predicting decoding cycles would adjust during 
decoding. By permute different prediction interval and 
different prediction mechanism we can classify many 
groups of prediction-based DVS approaches. 
 
• Per-GOP interval and dynamic equation methods: 
Donghwan et al [B22] proposed a dynamic method that 
updates the new prediction rule by history information 
and the original prediction information. 
• Per-frame interval and fixed equation methods: 
Pouwelse et al. implemented a StrongARM based system 
for power-aware video decoding [B12, B20, B21]. 
• Intra-frame interval and dynamic equation methods: 
Kihwan et al. [B6] proposed a dynamic equation that is 
based on frame-based history and they separate a frame 
into two parts. A frame-dependent part varies greatly and 
a frame-independent part has constant decoding time for 
each frame type. Thus they use equation to predict the 
decoding time of frame-dependent part and use frame-
independent part to compensate for the prediction error. 
 
5.2.2 CONTROL THEOREM APPROACHES 
Lu et al. [B16] propose an on-line feedback approach to 
control the decoding speed. This approach is based on 
Dead-zone based control algorithm that is a decoder with 
a display buffer and feedback control. In this theme, they 
always try to control the number of decoded frame in the 
buffer within a region specified by [Bl,Bh], where Bl is 
the lower threshold of the number of frames in the buffer 
and Bh is the higher threshold of the number of frames in 
the buffer. If the number of frames in the output buffer is 
 7 
possible. Intuitively, the constant decoding rate schedule 
ˆ S is optimally smooth, but it may be not a feasible 
schedule. 
 
 
Figure 13: Various decoding speed schedule 
 
6.4.2 THE AGGRESSIVELY SMOOTHING 
ALGORITHM 
Salehi et al. [B9] proposed an Optimal Smoothing 
Algorithm to reduce network transmission rate 
variability in video-on-demand(VOD) domain. But it can 
not directly used in our online video decoding. Because 
we do not know the precise decoding cycle for each 
frame and the input buffer is limited so that the algorithm 
can not get the decoding information of overall incoming 
frames. In the section we would show how we solve 
these problems so that the optimal smoothing algorithm 
can use for our on-line video decoding. Before we 
introduce aggressively smoothing algorithm, we must 
explain the input of the algorithm. The input of the 
algorithm is illustrated by figure 14. The red line is L(t) 
that is the low bound cycles at time t. The blue line is the 
U(t) that is the upper bound cycles at time t. L(t) would 
increase in a fixed interval that is the playback cycle time, 
because the display device would demand the decoded 
frames for display with a constant playback rate. Why 
does L(t) increase with different cycles? That is because 
that each frame has different decoding cycles. U(t) 
increases at different time with L(t), because output 
buffer would increase a capacity until display devise 
complete to transfer the decoded frame data. U(t) would 
increase with different cycles, and the reason is the same 
with L(t). 
 
 
Figure 14: The meaning for the input graph of 
aggressively smoothing algorithm 
 
The L(t) and U(t) are the inputs of aggressively 
smoothing algorithm, below we would illustrate how we 
get L(t) and U(t). First, we must obtain the frame 
decoding cycle prediction formula for each type of frame 
that is illustrated in section 3.3, and we can obtain frame 
decoding cycles for each frame by using the frame 
decoding cycle prediction formula. Then, we can obtain 
L(t) by accumulate the frame cycles and by the 
information that deadline time is a multiple of the display 
interval. We can also obtain U(t) by accumulate 
decoding cycles for next n frames, where n is the output 
buffer capacity, and by the timing information that the 
deadline time and the data transfer time is known. Now 
we begin to introduce the aggressively smoothing 
algorithm in detail. We summary two important policies 
of aggressively smoothing algorithm. 
 
1. Find constant decoding rate segments which are as 
long as possible. 
2. When the rate must be increased or decreased to 
ensure feasibility, we change the rate as early as possible. 
 
 
Figure 15: An illustration for policies of aggressively 
smoothing algorithm 
 
Figure 15 illustrates the construction of feasible schedule 
S. First, we find the longest feasible constant decoding 
rate segment, so in the figure is a segment to (a). Because 
the output buffer would occur underflow at (a), we must 
increase the decoding rate before (a). By policy 2, we 
would increase the decoding rate at the earliest (b) to let 
the decoding rate change as small as possible. Point (b) is 
the earliest point because if we increase the decoding rate 
before (b), it would cause output buffer overflow. The 
process is repeated, we try to 
find the longest feasible constant decoding rate segment 
from point (b). Because the new segment starting from (b) 
would cause the output buffer overflow at (c), the 
decoding rate must decrease before (c). Then we 
decrease the decoding rate as easily as possible at (d). 
Finally, we can find a feasible schedule S. 
 
Before we illustrate the details of aggressively smoothing 
algorithm, we illustrate related definitions and concepts. 
Now we explain how to examine a decoding frequency 
would cause output buffer starves or overflow. In Figure 
16, for example, we have three decoding frequency f1, f2 
and f3. How can we determine these frequencies that 
 9 
• Case 2 (Line 22): if the fmin is too large to avoid 
causing output buffer overflow at the output buffer 
release time of check frame, the constant speed segment 
must end before the output buffer release time of check 
frame (the case is illustrated in Figure 23). The segment 
must decrease the decoding rate in the future, so it must 
end as early as possible to smooth the decreasing rate. 
The segment end at the earliest time tl and assign the 
lowest feasible decoding rate fmin for the segment. Then 
the algorithm would try to find a new constant speed 
segment from tl. 
 
 
Figure 18: Algorithm for compute fmin 
 
 
Figure 19: The method to find fmax in a period time 
 
• Case 3 (Line 27): if we have used the total frame 
information in input buffer, we do not know we must 
increase or decrease the decoding rate in the future 
(Figure 24). We would choose the lowest feasible 
decoding rate for the segment because we do not know 
we must increase or decrease the decoding rate in the 
future and whether we choose the lowest or the highest 
feasible decoding rate, we may choose incorrectly. And 
another reason that choosing the lowest feasible 
decoding rate would save more energy than choosing the 
highest feasible decoding rate. Then the algorithm would 
reschedule for the decoding rate of new segment at time 
tl. 
 
Figure 20: Algorithm for compute fmin 
 
 
Figure 21: The aggressively smoothing algorithm 
 
• Otherwise, there exits a feasible constant speed 
segment between time ts and the output buffer release 
time of check frame. The process will repeat to find 
suitable decoding rate until the entire MPEG stream is 
segmented. 
 
Figure 22: Case 1 in the aggressively smoothing 
algorithm 
 11
 
Figure 28: Snapshot 4 on decoding 
 
 
Figure 29: The illustration for two closed discrete 
voltage/frequency level 
 
5. EXPERIMENT AND PERFORMANCE 
EVALUATION 
Experimental environment is described as follows: 
1. Simplescalar simulator (sim-cache) 
2. Linux RedHat8.0 workstation 
3. SunOS 5.8 workstation 
4. Modelsim SE 5.8d 
5. Synopsys Design Analyzer 
6. NanoSim Version U-2003.03-SP2 
The benchmark is JPEG and MPEG4 reference codes. 
     We design a simulation tool that starts from 
application specifications, continues through the 
mapping of the application onto topologies and selection 
of topology. It further profiles the application's 
characteristics and generates a switch-mode table to 
dynamically control switches at run time. It also collects 
test patterns for final simulation of the design. First, we 
use the simplescalar [A1] to simulate applications and 
collect the DAG information. Then the Writer reader 
analyzer will analyze the relation about the writer cores 
and reader cores, and it will generate a core flow graph 
for low-power placement, a switch-mode table for 
dynamic switching, and test patterns for final simulation. 
Finally, we can get power and performance results to 
evaluate our design. After profiling, we will get the 
communication status of cores, and we will generate the 
simulation patterns for Verilog simulator. We use 0.18 
tech. to do synthesis. The experimental results of the 
Crossroad, WishBone and segmented bus are shown in 
Table 1 and 2. Because the original segmented bus is 1-D 
architecture, we modified it to 2-D type as shown in 
Figure 30 for fair comparison. The experimental results 
for performance of different placement strategy are 
shown in Table 3. Table 4 shows Area and power 
estimates for original switch design and switch design 
with bus controller. Table 5 shows some information of 
wrappers. There are some experimental results about the 
Crossroad are shown in Figure 31. It includes execution 
time reduced for different placement with delay 
consideration, Power ratios of different placement, 
Applicability of lease-line mode and the power affection 
of lease-line mode. 
 
 
Figure 30: The 2-D segmented bus and crossroad 
architecture 
 
0
10
20
30
40
50
60
70
80
90
100
MPEG4-
decoder
cjpeg-
small
cjpeg-
large
djpeg-
small
djpeg-
large
Ratio of
execution time
(%)
Initial Placement
PACP Placement
 
(A) 
0
0.2
0.4
0.6
0.8
1
Ratio of
power
MPEG4
decoder
cjpeg-
small
cjpeg-
large
djpeg-
small
djpeg-
large
Reference
PACP
 
(B) 
0
20
40
60
80
100
Applicability of
lease-line mode
(%)
MPEG4
decoder
cjpeg-
small
cjpeg-
large
djpeg-
small
djpeg-
large
Reference Switch 1
PACP Switch 1
Reference Switch 2
PACP Switch 2
 
(C) 
 13
Figure 32: related characteristics of video workload 
 
level to optimize the DVS performance. This scheme is 
used to provide the lower bound of energy 
consumption for on-line scheme.In this model, we 
assume the output buffer size is 1024 KByte and input 
buffer size is 64 KByte. 
 
 
Figure 33: Energy consumption comparison for different 
DVS approaches. All of the energy is normalized by the 
energy consumption of the Oracle scheme. 
 
Figure 33 shows the relative energy consumption and 
Figure 34 present the performance loss in terms of 
quality of service (QoS). It is observed that our 
aggressively smoothing control off-line consumes the 
least energy among the four approaches that consumes 
38.7%-80.3% and average 57.2% energy compared to 
the Oracle scheme, and our aggressively smoothing 
control on-line is very closed to our off-line scheme with 
39.2%-81.2% and average 58.5% energy compared to 
the Oracle scheme. Another observation is that our 
aggressively smooth control on-line is always better than 
the Dead-zone based control scheme which consumes 
62.2%-118.0% and average 88.7% energy compared to 
the Oracle scheme. 
 
Figure 34: QoS comparison with different DVS 
approaches 
 
While our aggressively smoothing control on-line 
approach improves energy consumption for MPEG 
decoding, it may in return degrade the quality of service 
because the prediction equation is not absolutely 
accuracy. Our aggressively smoothing control on-line 
approach performs the worst, but it is noted that the 
average QoS rate is still 97.6%. Thus the quality loss is 
very slight. In Figure 35, it shows the performance of our 
counter mechanism for QoS degradation. The original 
scheme suffer greatly QoS degradation in several 
benchmarks. In the coastguard and foreman, the QoS rate 
is only 25.7% and 32.3% respectively. One reason is that 
these benchmarks has the worse prediction accuracy for 
decoding cycle estimation. Another reason is that when 
over-estimation or under-estimation occur consecutively, 
it will accumulate the prediction error cycles and cause a 
series of overflow or underflow in the later frames of this 
constant speed segment. If a benchmark has this two 
features, it is possibly to suffer greatly QoS degradation 
in the original without counter scheme. In our 
aggressively smoothing control on-line approach with 
counter mechanism, the average QoS rate is 97.6%. In 
our aggressively smoothing control on-line approach 
without counter mechanism, the average QoS rate is 
77.1%. Thus, our proposed counter mechanism can 
greatly improve QoS rate. In Figure 36, it illustrates the 
voltage switch number comparison for different DVS 
schemes. It is expected that the Oracle scheme has the 
highest voltage switch number. Next is the dead-zone 
