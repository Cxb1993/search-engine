 2
目錄 
 
一、 中文摘要               3 
二、 英文摘要               4 
三、 計畫緣由與目的             5 
四、 結果與討論              6 
五、 計畫成果自評              15 
六、 參考文獻               15 
七、 附錄（已發表論文）            16 
 
 4
二、英文摘要 
 
The current state-of-the-art approach to automatic speech recognition (ASR) is based on the 
data-driven concept, which learns speech by constructing acoustic and language models 
exclusively from corpora. Unfortunately, even the best ASR system nowadays gives much 
higher error rates in some rather simple tasks, compared to human speech recognition. 
Though some specifically-suitable applications do exist, ASR performance is still far away 
from users’ expectation. Until more recently, many researchers suggest that we should go 
back to bring in the linguistic and phonological knowledge, which has been largely ignored 
during this decade. The next generation ASR should follow a paradigm of integrating both 
knowledge-based and data-driven techniques. To make this paradigm a reality, a shared 
platform for system design and assessment must be set up for bringing in more researchers 
together to this area. The first phase of this collaborative project attempted to set up a shared 
platform for ASR research in Taiwan. Several universities and research organizations joined 
together to carry out this collaborative project so as to promote the fundamental research of 
the next generation ASR. The major research items of the second phase include (1) detection 
of speech attributes and events, (2) event integration and evidence verification, (3) 
knowledge-bases, corpora, models, tools, and the shared platform, (4) output enhancement 
and post processing, and (5) integration of known linguistic and acoustic knowledge into the 
next generation ASR. Under this collaborative mode, the knowledge-bases, corpora, models, 
and tools generated will be opened to public so as to bring in more researchers to the ASR 
area. This gives a decisive chance to promote the research and development of speech 
technology in Taiwan. 
As an indispensable part of the whole collaborative project, this subproject focuses on: 
(1) establishment of phone-labeled speech corpora for benchmark test, (2) study on acoustic 
features for automatic phone segmentation and attribute and event detection, (3) development 
of automatic audio segmentation, (4) development of automatic phone segmentation, (5) 
study on feature selection, and (6) study on classifiers. 
 
Keywords: speech recognition, speech attributes, speech events, automatic labeling, 
automatic audio segmentation, speech corpus 
 
 6
四、結果與討論 
 
本計畫的主要工作項目包括：(1)自動音素分段(automatic phone segmentation)技術改良，
以加速語料庫標記的進度；(2)國語語料庫的音素自動標記及人工修訂；(3)語音事件偵
測技術改良； (4)基於偵測之自動語音辨識系統 (detection-based automatic speech 
recognition system)的建置與改良；(5)韻律特徵(Prosodic Feature)對語音特徵偵測影響評
估。 
 
(1) 自動音素分段技術改良 
我們在第一階段三年期計畫提出一個基於HMM強迫校準(forced alignment)及SVM邊界
調整的兩階段自動音素分段技術，本計畫針對HMM強迫校準及音素邊界調整分別提出
改良的技術。 
 
A.HMM自動音素分段技術改良 
就 HMM 聲學模型訓練而言，我們在第一階段三年期計畫所提出的最小邊界誤差
(minimum boundary error, MBE)訓練法相較於傳統的 ML 訓練方式已有相當大的突破
[1-2]。假設有 R 句訓練語料的特徵向量序列，即  ROOO ,..,1 。最小邊界誤差的目標函
式定義為所有語句期望邊界誤差之總和，即： 

 
 R
r
S
r
c
r
i
rr
iMBE rri SSEROSPF
1
),()|(Φ       (1) 
其中 riS 是一條邊界序列； rΦ 為所有可能邊界序列所形成的集合； )| rri OSP 為 riS 在給
定 rO 時的事後機率； ),( rcri SSER 則定義為 riS 與實際邊界 rcS 的誤差，即序列中每個音素
邊界誤差之和。顯然地，要達到最小此目標函式的目的，須調高邊界誤差較小之邊界序
列的事後機率，調低邊界誤差較大之邊界序列的事後機率，進而使整體的邊界誤差期望
值達到最小。然而，最小化邊界誤差的整體期望值卻有可能造成過度訓練
(over-training)，使模型無法提供較佳的一般性(generalization)。 
我們用下面的例子來說明。假設  321 ,, SSSΦ 包含三條邊界序列，且
5.5),( 1 cSSER 、 3.2),( 2 cSSER 及 7.6),( 1 cSSER ，若事後機率分佈我們只考慮 OSP iA | 及  OSP iB | 兩組，則從表一可見，在此例中，最小邊界誤差訓練法將偏好 BP ，
因為 BP 的邊界誤差期望值為3.65小於 AP 的4.36；然而，若我們將此三條邊界序列以邊界
誤差值由小至大排列可得到 312 ,, SSS ，而以 AP 及 BP 由大至小排列可分別得到 312 ,, SSS 及
132 ,, SSS ，反倒是 AP 的排序與邊界誤差較為一致(consistent)；換句話說，當考慮到排序
的一致性時，我們將偏好選擇 AP 。 
 
表一、最小邊界誤差訓練法與最小排序錯誤訓練法的比較。 
 ),( ci SSER )|( OSP iA )|( OSP iB
1S  5.5 0.30 0.20 
2S  2.3 0.45 0.45 
3S  6.7 0.25 0.35 
邊界誤差期望值 4.36 3.65 
對序錯誤數 0 1 
 
考慮排序的一致性是否合理？承襲上例，若測試時所產生的序列集合Φ中未包含
2S ，將導致 BP 挑選出 3S ，使得邊界誤差達到6.7，高於以 AP 選出 1S 的5.5。這雖然僅僅
是一個特例，卻說明了考慮排序的一致性將有助於選出較小邊界誤差的邊界序列。故我
 8
另外，我們也利用排序提升法(Rank Boost)實作非線性排序模型 RANKBOOSTKM和
RANKBOOSTDT，它們具備區段線性的分類能力。 
實驗結果如表二所示。邊界調整模型在 HMM 自動音素分段的初始邊界左右各 5 毫
秒範圍內尋找最佳的邊界。SVMKM是採用 K-均數分群的結果，而 SVMDT則是決策樹分
群的結果，兩種皆使用非線性的徑向基底核函數(radial basis kernel function)，並利用分
類模型作訓練。LSVMKM和 LSVMDT則使用線性支向機分類模型作訓練。我們發現非線
性支向機分類器優於線性支向機。RANKLSVMKM和 RANKLSVMDT使用線性排序支向
機作訓練。我們發現線性排序模型優於線性分類模型，而和非線性分類模型不相上下。
非 線 性 排 序 模 型 RANKBOOSTKM 和 RANKBOOSTDT 又 優 於 線 性 排 序 模 型
RANKLSVMKM和 RANKLSVMDT。 
 
表二、TIMIT 語料下平均邊界誤差(ms)及在不同容忍誤差下邊界的準確率(%)。 
調整方式 平均邊界誤差 (毫秒) 
不同容忍誤差下邊界的準確率(%) 
≦10ms ≦20ms 
未調整 7.14 81.57 93.73 
LSVMKM 6.84 83.51 93.85  
LSVMDT 6.89 83.44 93.79 
SVMKM 6.75 84.00 94.33 
SVMDT 6.83 83.70 94.12 
RANKLSVMKM 6.72 83.89 94.17 
RANKLSVMDT 6.76 83.90 94.01 
RANKBOOSTKM 6.66 84.20 94.14 
RANKBOOSTDT 6.66 84.13 94.11 
 
(2) 國語語料庫的音素自動標記及人工修訂 
我們從 MATBN 公視新聞口語語料庫選取將近 1.5 小時的語句，利用所提出的自動音素分
段技術進行自動標記。由於人工修訂非常費力耗時，我們僅能就其中 20 分鐘的語料完
成人工修訂。 
 
(3) 語音事件偵測技術改良 
在第一階段三年期計畫的第三年，我們已探討三種不同語音特徵系統的音素辨識率[6]，
發現支配音韻特徵系統(Government Phonology (GP) feature system)能提供後端語音事件
整合辨識最豐富的語音資訊，從而達到最佳的辨識效果。因此，在第二階段三年期計畫
我們以支配音韻特徵做為語音事件偵測之主要研究對象。支配音韻特徵系統是透過聲學
頻譜的分析，將語音音素中各種互補的獨立成分抽取出來。此方法共取得 11 項獨立成
分(即"A", "I", "U", "E", "S", "h", "H", "N" 與 "a", "i", "u")，透過此 11 項獨立成分之組合
可表示任何語音音素。 
針對 GP 語音特徵系統，我們進一步設計兩種新的語音事件偵測器架構，並與第一
階段三年期計畫期間採用的架構進行比較。三種架構均使用多層感知器(multi-layer 
perceptron, MLP)作為語音事件機率模型之核心，分別簡介如下： 
 
1、 傳統語音特徵事件偵測器： 
此架構為第一階段三年期計畫期間使用之語音事件偵測器架構，可當作其它兩種新
設計架構之比較基礎。在此多層感知器的設計上，輸入向量使用 13 維梅爾倒頻譜係數，
輸出則為 GP 系統 11 個語音特徵事件之事後機率所形成之 11 維向量。我們採用循環架
 10
表三、傳統語音特徵事件偵測器架構下，支配音韻(GP)語音特徵事件偵測之音框正確率。 
語音特徵 音框正確率 語音特徵 
音框正
確率 
A 85.0 % H 92.6 %
I 89.6 % N 97.3 %
U 86.2 % a 96.1 %
E 85.8 % i 94.2 %
S 90.8 % u 95.3 %
H 94.6 %   
 
表四、語音特徵暨音素事件偵測器架構下，支配音韻(GP)語音特徵事件偵測之音框正確率。 
語音
特徵 
音框正確
率 語音特徵 
音框正確
率 
A 85.9 % H 93.6 % 
I 89.5 % N 97.3 % 
U 87.0 % a 96.3 % 
E 86.6 % i 94.3 % 
S 91.2 % u 95.7 % 
H 94.9 %   
 
表五、基於長時資訊之語音特徵暨音素事件偵測器架構下，支配音韻(GP)語音特徵事件偵測之音框正確
率。 
語音特
徵 
音框正確
率 語音特徵 
音框正確
率 
A 88.2 % H 94.1 % 
I 91.0 % N 97.6 % 
U 89.0 % a 96.8 % 
E 88.5 % i 95.2 % 
S 92.4 % u 96.2 % 
h 95.4 %   
 
(4) 基於偵測之自動語音辨識系統的建置與改良 
在前項語音事件偵測研究工作中，我們針對 GP 語音特徵系統的語音事件偵測器提出了
兩個新的架構，並有效而穩定的提升了語音特徵事件偵測正確率。基於此研究成果，我
們進一步以此兩組改進後的語音事件偵測結果來辨識更高層次的語言資訊，以驗證前端
語音事件偵測對後端高層次辨識效能的影響。我們首先嘗試進行音素的辨識，作為更高
層次語言資訊辨識研究之基礎。 
我們使用條件隨機域(conditional random fields, CRF)來整合語音事件偵測結果，進
行音素辦識。對兩組改進後的語音事件偵測結果，我們分別建立對應的條件隨機域模
型，用以觀察不同語音事件偵測結果對自動語音辨識正確率的影響。在條件隨機域模型
的設定上，我們使用二元值特徵(binary-valued feature)，即僅以各語音事件是否發生做為
模型之輸入；在訓練上，傳統主要訓練方法分為兩種：一、使用由訓練語料提供之正確
語音事件發生之音框標記做為訓練資料(記為 oracle train, OT)；二、使用前述多層感知器
(MLP)直接對訓練語料進行偵測，所產生帶有若干偵測錯誤的語音事件發生音框標記做
為訓練資料(記為 detection-based train, DT)。 
在第一階段三年期計畫的第三年我們已經探討過前述兩種訓練方法，在第二階段三
年期計畫我們提出一個新的訓練方法，使用多層感知器(MLP)對訓練語料進行時間重新
對準(Realign)，所產生帶有語音特徵之非同步起始資訊、同時亦保有一定正確性之語音
事件發生之音框標記做為訓練資料(記為 alignment-based train, AT)。 
 語音特徵之非同步起始現象是連續語音的特性。在連續語音中，語者並非依音素序
列逐一完整發聲，而是流暢地將音素序列之發聲串連起來，形成語流。當語者在對連續
 12
方法中，我們使用語音事件偵測器將訓練語料提供之正確語音特徵事件在時間上進行重
新校準。如圖五所示，重新校準後的訓練資料在音素序列之邊界附近保有自然語流之語
音特徵非同步起始現象，同時在其它時間點上又能保持符合語音特徵系統所定義之音素
與語音特徵對應關係。我們預期使用此方法訓練之條件隨機域模型較前兩種傳統方法更
能妥善處理語音事件偵測結果，從而提升整體音素辨識效能。 
 實作上，我們使用前節所提出之語音事件偵測器所產生之語音特徵與音素事件之事
後機率搭配維特比(Viterbi)演算法，分別對音素序列與語音特徵事件序列進行時間的重
新對準。 
 在測試時，條件隨機域模型均使用多層感知器所偵測之語音特徵事件音框標記做為
輸入，而以音素序列做為辨識結果輸出。實驗語料同樣使用 TIMIT 英文語料庫，表六列
出三種條件隨機域模型訓練方式下基於偵測之音素辨識結果，其中準確率%(accuracy) = 
正確率%(correction) – 插入率%(insertion rate)。表六同時亦列出傳統基於 HMM 之音素
辨識的結果做為比較。 
 
表六、三種條件隨機域模型訓練方式下，自由音素辨識(free phone decoding)結果比較。 
事件偵測器類型 辨識系統 正確率 
(%) 
準確率
(%) 
基於 HMM 之自動語音辨識 69.02 63.45 
語音特徵暨音素
事件偵測器 
OT CRF 70.55 34.38 
DT CRF 57.30 56.14 
AT CRF 64.87 62.32 
基於長時資訊之
語音特徵暨音素
事件偵測器 
OT CRF 75.38 47.47 
DT CRF 62.76 61.46 
AT CRF 69.83 66.97 
 
透過比較表六中前端使用「語音特徵暨音素事件偵測器」與「基於長時資訊之語音
特徵暨音素事件偵測器」辨識系統之整體表現，我們首先可以發現前端語音事件偵測器
之效能對語音辨識系統整體表現有相當關鍵之影響。儘管使用長時資訊之語音事件偵測
器僅在語音特徵事件音框正確率數據上進步約 1~2%，然而對整體音素辨識而言，卻能
達到 4~10%的提升。此一發現可以做為訂定後續研究方向之參考。 
從表六我們也可以看出，傳統 OT 訓練所得之條件隨機域模型擁有高正確率，但由
於無法妥善處理自然語音中語音特徵之非同步起始現象，因此造成在音素邊界處的錯誤
插入(Insertion Errors)增加，使準確率大幅下滑。而傳統 DT 訓練所得之條件隨機域模型
由於使用與辨識環境相匹配之訓練資料進行訓練，因此並沒有出現 OT 訓練法中出現大
量錯誤插入情形，但由於訓練語料中帶有之前端事件偵測器錯誤減弱了模型的音素辨識
鑑別力，辨識結果儘管較 OT 訓練法所得之模型好，仍遠不及傳統 HMM 辨識系統。 
 相較之下，新提出之 AT 訓練法所得之條件隨機域模型在音素準確率上有相當程度
之提升。在前端使用語音特徵暨音素事件偵測器時，使用 AT 訓練法相較 DT 訓練法準
確率能絕對提升 6.18%；而在前端使用基於長時資訊之語音特徵暨音素事件偵測器時，
則能提升 5.51%絕對值，所得之準確率 66.97%並已大幅超越傳統 HMM 辨識系統 63.45%
的準確率。 
 
(5) 韻律特徵(Prosodic Feature)對語音特徵偵測影響評估 
在前一階段研究結果中，我們發現讓條件隨機域模型學習語音特徵在音素邊界的非同步
起始現象對音素辨識結果影響重大。同時我們亦發現當使用長時聲學特徵資訊於語音事
件偵測以減少此語音特徵非同步起始現象之程度時，整體音素辨識準確率會大幅提升。
 14
MFCC + 音強(I) 60.0098 
MFCC + 語速(R) 59.7697 
MFCC + P + I + R 60.8396 
 
Articulatory Feature Asynchrony Analysis and Compensation in  
Detection-Based ASR 
I-Fan Chen1 and Hsin-Min Wang1,2
1Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan 
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
{ifanchen, whm}@iis.sinica.edu.tw 
 
Abstract 
This paper investigates the effects of two types of 
imperfection, namely detection errors and articulatory feature 
asynchrony, of the front-end articulatory feature detector on 
the performance of a detection-based ASR system. Based on a 
set of variable-controlled experiments, we find that 
articulatory feature asynchrony is the major issue that should 
be addressed in detection-based ASR. To this end, we propose 
several methods to reduce the asynchrony or the effects of 
asynchrony. The results are quite promising; for example, 
currently, we can achieve 67.67% phone accuracy in the 
TIMIT free phone recognition task with only 11 binary-valued 
articulatory features. 
Index Terms: articulatory feature asynchrony, detection-
based ASR, speech recognition 
1. Introduction 
In recent years, detection-based automatic speech recognition 
(ASR) has become a popular research topic in fields related to 
ASR. By simulating human speech recognition (HSR), 
detection-based ASR systems attempt to reduce the gap 
between HSR and ASR. Basically, the framework of 
detection-based ASR can be divided into two key components: 
a front-end knowledge attribute detector and a backend 
knowledge integrator [1]. The front-end process collects a 
wide variety of speech-related knowledge attributes to form 
knowledge sources; and the backend process integrates the 
attributes into higher-level speech units, such as phones, 
syllables, words, and sentences. If the front-end detector 
targets articulatory features (AFs), two factors may affect the 
system's overall recognition accuracy: the errors produced by 
the detector and the asynchrony of AFs. 
The asynchrony of AFs is a phenomenon caused by 
variations in natural speech production; i.e., the onset times of 
different AFs shift differently in different contextual 
conditions in speech. Because of this phenomenon, it is 
generally believed that one of the reasons why AFs are more 
suitable than phones for ASR is their ability to describe the 
contextual information in speech. A number of studies have 
focused on this phenomenon. For example, in [2], AF 
asynchrony was introduced to the articulatory feature 
recognizer by applying an embedded training technique; while 
in [3], the authors used the pattern of AF asynchrony in the 
AF space to probe for acoustic differences in the onset of 
Dutch singulars and plurals. However, it is still unclear what 
role this phenomenon plays in a phone or word recognition 
task, which maps multiple asynchronous AF streams into a 
single phone or word stream. 
In our previous research [4], we found that the 
imperfection of the front-end articulatory feature detector 
(AFDT) can cause a substantial decline in recognition 
accuracy in detection-based ASR. Here, we investigate the 
effects of two types of the above imperfection, namely AFDT 
errors and AF asynchrony, on the backend integrator's 
performance. To compare the relative importance of the two 
factors, we design a set of variable-controlled experiments. 
The results show that AF asynchrony plays a more important 
role in detection-based ASR than AFDT errors. Based on this 
finding, we propose a number of ways to improve the system 
performance. We believe that our experiment results provide 
further insight into how to design a high-performance 
detection-based ASR system. 
2. Front-end articulatory feature detection 
and AF asynchrony analysis 
2.1. Front-end articulatory feature detection 
The AFDT's detection target is the Government Phonology 
(GP) feature set [5], a phonological feature system in which 
speech sounds are destructed into a set of primes and can be 
represented by fusing these primes structurally. We select the 
GP feature set because the results of our previous study show 
that it is the most effective for building a high-performance 
detection-based ASR system [4]. In this work, the GP feature 
set contains 11 AFs, including 8 AFs (namely "A", "I", "U", 
"E", "S", "h", "H", and "N" ) defined in the original GP feature 
system [5] and 3 AFs ("a", "i", and "u") added by King and 
Taylor [6]. 
In addition to the 11 AFs, our front-end AFDT 
simultaneously detects TIMIT 61 phones for AF asynchrony 
analysis. The objective of phone detection is to remove the 
bias introduced by the inconsistency between human 
alignment and machine alignment. AF asynchrony is defined 
as the shift between a phone's boundary and the boundaries of 
its associated AFs. Thus, the different criteria used for AF 
alignment and phone alignment might lead to inaccurate 
articulatory feature asynchrony measurements. Even though 
the TIMIT corpus provides human aligned phone labels, to 
generate unbiased AF asynchrony data, we need to re-align 
the phone sequence in speech using our front-end AFDT. 
Following the work in [6], we use a single time delay 
recurrent neural network [7] to implement the front-end AFDT. 
The inputs of the neural network are 12 Mel-frequency 
cepstral coefficients (MFCCs) plus energy, which are 
extracted by a 25-ms Hamming-windowed frame with 10-ms 
shifts. The neural network outputs a 72-dimensional vector 
representing 11 GP AFs and 61 TIMIT phones. The value of 
each element in the vector ranges from 0 to 1, which can be 
treated as a posterior probability. The values of the AFs are 
then discretized1 with a threshold to form binary-valued AF 
                                                                 
 
1  Theoretically, CRFs can handle real-valued input; however, we 
discretize the AFDT's results because the CRF++ toolkit used in our 
experiments only supports binary-valued input. Moreover, it is easier 
to define the boundaries of AFs, which are important for asynchrony 
measurement, by using the discrete AF detection results.
recognition performance in detection-based ASR. The derived 
knowledge will help us to further refine our detection-based 
ASR system. 
4. Experiments 
4.1. Experiment setup 
We conduct experiments on the TIMIT acoustic-phonetic 
continuous speech corpus, but the dialect utterances (SA1 and 
SA2) are not used. The database is divided into three parts: a 
training set (3296 utterances), a development set (400 
utterances), and a test set (1344 utterances). The training and 
development sets are subsets of the standard TIMIT training 
set, while the test set is the standard TIMIT test set. In the 
experiments, all the models are trained by the training set, and 
the configurations and parameters associated with the models 
are assigned empirically based on the development set. The 61 
phones defined in TIMIT are used as recognition units; 
however, for the performance evaluation, the recognized 
TIMIT 61-phone results are mapped to the CMU/MIT 39-
phone set. No language models are used in recognition. 
4.2. The baseline detection-based ASR systems 
First, we evaluate the baseline detection-based ASR systems, 
in which the CRF models are trained by oracle data training 
(OT) and detected data training (DT). We consider two test 
conditions: the ideal case and the real case. In the ideal case, 
the test data is set to match the CRF model's training 
conditions. In the OT-trained CRF system, the AFs converted 
from the human phone labels of the testing data are input to 
the CRF model. In this way, we can measure the performance 
upper bound of the CRF-based system. In the real case, the 
inputs to the CRF models are the AFs detected automatically 
by the AFDT. The phone recognition results of these baseline 
systems are shown in Table 1, where Corr (correct rate) and 
Acc (accuracy) are obtained by HTK's HResults tool; and Acc 
= Corr – insertion rate. The second row shows that if there are 
no AFDT errors and AF asynchrony, the OT-trained CRF 
system can achieve a very high phone recognition 
performance. Clearly, the GP11 AF set demonstrates high 
potential for detection-based ASR; therefore, we may consider 
this performance upper bound our ultimate goal in this 
research. In the real case, since the detected AFs inevitably 
contain errors and asynchrony, though with a high correct rate, 
the OT-trained CRF system suffers from serious phone 
insertion errors, as shown in the third row of Table 1. 
Although the performance of the DT-trained CRF system is 
quite stable in terms of the correct rate and accuracy, the 
values are not satisfactory. 
Table 1. The phone recognition results of the oracle data 
trained (OT) CRF model and the detected data trained (DT) 
CRF model. 
Test Data Type System Corr Acc 
Ideal (upper bound) OT CRF 98.31 98.28 
OT CRF 70.55 34.38 Detected (real case) DT CRF 57.30 56.14 
4.3. The AFDT aligned data trained system 
To identify the degrees of AFDT errors and AF asynchrony 
that cause the substantial difference in the performance of the 
OT-trained CRF model between the ideal case and the real 
case, we applied AFDT aligned data training (AT) in the 
experiments. Table 2 shows the ideal and real performances of 
the AT-trained CRF system, where the ideal performance 
(upper bound) is obtained by feeding the individual frame 
alignments of the 11 AF sequences of the test data (obtained 
by the alignment methods described in Section 2.2) to the AT-
trained CRF model. Comparing the upper bound performance 
of the AT-trained CRF model in Table 2 with that of the OT-
trained CRF model in Table 1, it is clear that the introduction 
of AF asynchrony to the backend CRF integrator has a major 
impact on the performance (26.82% drop in the correct rate 
and 27.97% drop in accuracy). The effect of the front-end 
AFDT errors can be measured by the difference between the 
performances of the AT-trained CRF model in the ideal and 
real cases. Specifically, the differences are only 6.62% for the 
correct rate and 7.99% for accuracy. We also compare the real 
case performance of the DT-trained CRF model (the fourth 
row in Table 1) with that of the AT-trained CRF model (the 
third row in Table 2). Clearly, the CRF model trained with AF 
asynchrony information alone outperforms the CRF model 
trained with both AF asynchrony and AFDT errors. The above 
evidences show that AF asynchrony plays a much more 
important role than AFDT errors in detection-based ASR. In 
other words, we can conclude that, to build a high-
performance detection-based ASR system, it is better to use 
AFDT aligned data training for the backend CRF integrator; 
and employ as many methods as possible to reduce or conceal 
AF asynchrony from the front-end AFDT. 
Table 2. The phone recognition results of the AFDT aligned 
data trained (AT) CRF model. 
Test Data Type System Corr Acc 
Ideal (upper bound) AT CRF 71.49 70.31 
Detected (real case) AT CRF 64.87 62.32 
4.4. AF asynchrony compensation 
In this section, we perform several preliminary experiments to 
determine whether AF asynchrony can be reduced or 
concealed. AF asynchrony compensation can be applied in the 
front-end AFDT or the backend CRF integrator. 
4.4.1. AF asynchrony compensation in the AFDT using 
long-term information 
Contextual variation in natural speech is one of the major 
reasons for AF asynchrony [2]. One way to reduce the output 
asynchrony is to let the AFDT learn the contextual variation 
directly. This can be done by introducing long-term 
information in speech to the AFDT. It has already been shown 
that such information is helpful for speech recognition [12]. 
We believe that using long-term speech information in the 
AFDT makes the detection of AFs more stable than using only 
local speech information; hence, the AF boundary shift around 
the phone boundary can be reduced. 
Table 3. The mean AF-phone boundary distances (in frames) 
of GP11 AFs generated by the original AFDT (using MFCC 
as input) and the AFDT using long-term information. 
AFDT sys a A E h H i I N S u U
MFCC 0.94 4.27 1.18 0.94 0.59 1.07 2.44 1.74 0.56 1.88 7.16
Long Term 0.75 3.57 0.94 0.77 0.38 0.91 2.36 1.15 0.43 1.32 4.55
 
Following the approach in [12], we use the Mel-frequency 
filter bank vectors in a 310-ms window jointly as input to 
implement an AFDT with long-term speech information. 
Table 3 shows the statistics of the AF asynchrony of the 
original AFDT and the AFDT with long-term information. 
Clearly, the introduction of long-term information reduces the 
AF asynchrony for all 11 AFs significantly. 
PHONE BOUNDARY REFINEMENT USING
RANKING METHODS
Hung-Yi LO∗†, Hsin-Min WANG∗
∗Institute of Information Science, Academia Sinica, Taipei
†Department of Computer Science and Information Engineering, National Taiwan University, Taipei
{hungyi, whm}@iis.sinica.edu.tw
Abstract—The HMM/SVM-based two-stage framework has
been widely used for automatic phone alignment. The two-
stage method uses SVM classifiers to refine the hypothesized
boundaries given by the HMM-based Viterbi forced alignment.
However, there are two drawbacks in using the classification
model for detecting the phone boundaries. First, the training data
contains only information about the boundary and far away non-
boundary signal characteristics. Second, the classification model
suffers from the class-imbalanced training problem. To overcome
these drawbacks, we propose using ranking methods to refine
the hypothesized boundaries. We train multiple phone-transition-
dependent rankers by using K-means-based and decision-tree-
based clustering. Both Ranking SVM and RankBoost are
evaluated. The results of experiments on the TIMIT corpus
demonstrate that the proposed ranking method outperforms
the classification method. The best accuracy achieved is 84.20%
within a tolerance of 10 ms. The mean boundary distance is 6.66
ms.
Index Terms—automatic phone segmentation, ranking method,
ranking SVM, RankBoost
I. INTRODUCTION
Annotated speech corpora are indispensable to various ar-
eas of speech research, e.g., speech recognition and speech
synthesis. Phoneme level annotation is especially important
for fundamental speech research. However, the development
of a large high-quality, manually labelled speech corpus
requires lots of human effort, and is time-consuming. To
reduce the human effort and speed up the labelling process,
many attempts have been made to utilize automatic phone
alignment approaches to provide initial phone segmentation
for subsequent manual segmentation and verification [5]–[10],
[12].
The most popular method of automatic phone alignment
is to adapt an HMM-based phone recognizer to align a
phoneme transcription with a speech utterance. Empirically,
phone boundaries obtained in this way should contain few
serious errors, since HMMs in general capture acoustic prop-
erties of phones; however, small errors are inevitable because
HMMs are not sensitive enough to detect changes between
adjacent phones. Consequently, researchers have proposed
a HMM/SVM-based two-stage framework [6], which uses
support vector machine (SVM) classifiers to refine the hy-
pothesized boundaries given by the HMM-based Viterbi forced
alignment.
In order to provide training data for the SVM classifier,
current researches [6], [7], [12] use the feature vectors as-
sociated with the true phone boundaries as positive training
samples and the randomly selected feature vectors at least 20
ms away from the true boundaries as negative training samples.
However, using a classification model for detecting the phone
boundaries has two drawbacks. First, the training data contains
only information about the boundary and far away non-
boundary signal characteristics. We expect that the refinement
task can be better modelled by treating the instances extracted
from the true boundaries as high preference instances, the
nearby instances as medium preference instances, and the far
away instances as low preference instances. Second, modelling
the refinement task as a classification problem will suffer
from the class-imbalanced training problem, since the training
data contains a lot of negative instances but only a limited
amount of positive instances. As a result, general classification
algorithms will be biased to predict all instances to be negative
on such a class-imbalanced dataset, since they are learned to
minimize the number of incorrectly classified instances.
Fig. 1. The difference between using a classification model and using a
ranking model for phone boundary refinement.
In this paper, we propose using ranking methods [3],
[4] to refine the hypothesized phone boundaries given by
the HMM-based Viterbi forced alignment. For each true
boundary, we generate a from-high-to-low preference list, i.e.,
the instance extracted from the true boundary is associated
with the highest preference and, for the remaining instances,
the preference degree decreases as the distance to the true
boundary increases. These preference lists are used to train a
ranking model. In our approach, a phone-transition-dependent
ranker is applied to detect the true phone transition boundary
around each hypothesized boundary given by the initial HMM-
is, the frame with the largest score will be selected to replace
the initial boundary.
B. RankBoost
RankBoost finds a highly accurate ranker by combining
many weak rankers, despite that each of them is only moder-
ately accurate. The typical weak ranker is a threshold function:
ℎ(𝒙) =
{
1 if 𝒙𝑗 > 𝜃
0 if 𝒙𝑗 ≤ 𝜃, (3)
where 𝒙𝑗 is the 𝑗-the element of 𝒙. The RankBoost algorithm
iteratively trains many weak rankers. The training procedure
maintains a weight distribution matrix 𝐷𝑡 over 𝒳 ×𝒳 in each
iteration and determines the parameters 𝜃 and 𝑗 in the ranker ℎ𝑡
to minimize a weighted pairwise mis-ordered error 𝑟 according
to 𝐷𝑡,
𝑟 =
∑
𝒙𝑖,𝒙𝑗
𝐷𝑡(𝒙𝑖,𝒙𝑗)(ℎ𝑡(𝒙𝑗)− ℎ𝑡(𝒙𝑖)). (4)
After each iteration, the weight is updated by
𝐷𝑡+1(𝒙𝑖,𝒙𝑗) =
𝐷𝑡(𝒙𝑖,𝒙𝑗) exp(𝛼𝑡(ℎ𝑡(𝒙𝑗)− ℎ𝑡(𝒙𝑖)))
𝑍𝑡
, (5)
where ℎ𝑡(𝒙𝑖) is the prediction score of ranker ℎ𝑡 on the
instance 𝒙𝑖, {𝒙𝑖 ≻ 𝒙𝑗} is an ordered pair, 𝑍𝑡 is a normaliza-
tion factor to make 𝐷𝑡+1 a distribution. The parameter 𝛼𝑡 is
calculated by 12 ln(
1+𝑟
1−𝑟 ). The output score for the input vector
𝒙 is 𝑓(𝒙) =
𝑇∑
𝑡=1
𝛼𝑡ℎ𝑡(𝒙).
The ranking methods in general require a higher training
time complexity than the classification methods. However,
there is an efficient solution for training RankBoost [3]. Its
time complexity depends on the number of instances but not
on the number of ordered pairs.
C. Generation of Ordered Pairs for Training
The training data for both Ranking SVM and RankBoost
is represented by a set of correct-ordered pairs of instances.
For each true boundary, we first extract feature vectors from
a speech segment centered at it. Let the instances in the
segment be (𝒙1, . . . ,𝒙𝑇−1,𝒙𝑇 ,𝒙𝑇+1, . . . ,𝒙𝑁 ), where 𝒙𝑇 is
extracted from the true boundary. Then, we generate four
ordered ranking lists for this segment:
1) (𝒙𝑇 ,𝒙𝑇−1, . . . ,𝒙2,𝒙1)
2) (𝒙𝑇 ,𝒙𝑇+1, . . . ,𝒙𝑁−1,𝒙𝑁 )
3) (𝒙𝑇−1,𝒙𝑇−2, . . . ,𝒙2,𝒙1)
4) (𝒙𝑇+1,𝒙𝑇+2, . . . ,𝒙𝑁−1,𝒙𝑁 )
Each ordered ranking list can produce multiple ordered pairs
by coupling the first instance in the list with every one of the
remaining instances in the same list. For example, the first
list produces {𝒙𝑇 ≻ 𝒙𝑇−1}, {𝒙𝑇 ≻ 𝒙𝑇−2}, . . . , and {𝒙𝑇 ≻
𝒙1}. Then, the ordered pairs are gathered together to form the
training set. We have tried to generate more ordered ranking
lists for each segment, e.g., the lists starting from 𝒙𝑇−2 and
𝒙𝑇+2. However, the computational time increases substantially
but the prediction performance does not improve.
V. PHONE TRANSITION CLUSTERING
Ideally, we should be able to train a ranker or classifier for
each type of phone transition. However, this is not feasible
because the training data is always limited. Maintaining a
balance between the available training data and the model’s
complexity is critical to the training process. Furthermore,
since many phone transitions have similar acoustic characteris-
tics, we can partition them into clusters so that the training data
can be shared and the phone transitions with little training data
can be covered by the rankers or classifiers of the categories
they belong to. We implement phone transition clustering in
two ways: by K-means clustering and by decision-tree-based
clustering.
A. K-means-based Clustering
The K-means-based phone transition clustering is described
as follows:
1) For each specific phone transition case, we gather all the
augmented vectors associated with the human-labelled
phone boundaries, and compute the mean vector.
2) For each one of the four phone transition classes, namely
sonorant to non-sonorant, sonorant to sonorant, non-
sonorant to sonorant, and non-sonorant to non-sonorant,
we apply the K-means algorithm to cluster the phone tran-
sitions according to their mean vectors. Note that only the
phone transitions with enough instances are considered in
this step. The number of clusters is determined according
to the cross-validation accuracy that the resulting rankers
or classifiers achieve in the training data.
3) We assign the phone transitions, which are ignored in
Step 2 due to sparse instances, to the nearest clusters
according to the distances between their mean vectors
and the cluster centers.
B. Decision-tree-based Clustering
The drawback of K-means clustering is that it can not
cover phone transitions that do not occur in the training
data. In contrast, decision-tree-based clustering can generalize
to unseen phone transitions and take advantage of linguistic
knowledge during clustering. Here, all the questions have the
form “Is the left phone of the transition a member of the set
X and the right phone a member of the set Y?” The sets X
and Y range from broad phonetic classes, such as sonorant,
stop, and unvoiced classes, to distinct phonemes, such as {r}
and {s}. In total, 397 phonetic sets are used.
VI. EXPERIMENTS
A. Experiment setup
Our experiments were conducted on the TIMIT acoustic-
phonetic continuous speech corpus. TIMIT contains a total of
6,300 sentences, comprised of 10 sentences spoken by each of
630 speakers from 8 major dialect regions in the United States.
The TIMIT suggested training and testing sets contain 462 and
168 speakers, respectively. We discard the dialect sentences
(SA1 and SA2 utterances) and utterances with phones shorter
[10] S. S. Park and N. S. Kim. On using multiple models for automatic
speech segmentation. IEEE Trans. on Audio, Speech, and Language
Processing, 15:2202–2212, 2007.
[11] J.-L. Shen, J.-W. Hung, and L.-S. Lee. Robust entropy-based endpoint
detection for speech recognition in noisy environments. In Proc. ICSLP,
1998.
[12] T. G. Toledano, L. A. H. Gomez, and L. V. Grande. Automatic phonetic
segmentation. IEEE Trans. on Speech and Audio Processing, 11:617–
625, 2003.
抵桃園機場。 
 
二. 與會心得與建議 
 
由 IEEE Signal Processing Society 所舉辦的聲學語音信號處理國際研討會(ICASSP)是國
際上關於信號處理及其相關應用研究規模最大，最負盛名的學術研討會。語音處理研究
領域的學者均視其為年度最重要的研討會，所有知名學者專家幾乎都會與會發表最新研
究成果及互相討論、交換研究心得。此次會議台灣地區與會人數很多，除中研院外，包
括台大、清大、交大、成大、師大、北科大等都有多位師生出席。根據大會統計，台灣
地區投稿論文數佔第 8 位，僅次於美、中、法、日、德、英和加拿大，成績相當可觀。
 
三. 攜回資料名稱及內容 
 
會議論文集隨身碟 
 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：王新民 計畫編號：97-2221-E-001-022-MY3 
計畫名稱：新世代自動語音辨識技術之研究－第二階段--自動標音及語音資料庫確認 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 4 4 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 1 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 1 1 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
