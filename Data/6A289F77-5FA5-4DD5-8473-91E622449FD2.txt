seeing a key posture. In other words, each key 
posture is related to one or more actions. Taking 
this idea, every key posture in database has weights 
for all actions, called weighting vector. The 
weighting vectors of matched key postures in recent 
frames give action scores for every action. Because 
this method has no property of the order of key 
postures, the center point trajectory is analyzed to 
distinguish actions with same key postures but have 
different orders. 
The feature of our approach is that the order of key 
postures is not utilized. The composition of key 
postures and center point trajectory are used to 
recognize human actions. This method is robust 
against the error result of pattern matching, and the 
stationary temporal situations are also considered. 
Besides, in pattern matching process, a feature 
called ’distance vector’ [41] which was applied in 
gait recognition is modified and utilized as the 
pattern feature. This feature keeps the 
characteristic of human exterior contours well. 
Seventeen common human actions and static postures 
are recognized, and 5 subjects are tested in 4 
viewpoints. The recognition rate of our approach is 
93.13%, and the experiment result shows that our 
approach has potential to recognize more actions. 
英文關鍵詞： Actions Recognition, Background Segmentation, Key 
Postures, Center Point Trajectory, Action Scores, 
Weighting Vector 
 
  i 
中文摘要 
 
本計劃主要目標為建構一套「以影像為基礎之多目標智慧型動作辨識」系統，以迎合高齡化社會
需求，應用於居家看護、監控系統等。本年度計畫我們提出了一個室內人類動作辨識系統。一般而言，
動作是由一組有順序的關鍵姿態組成。而我們所提出的方法最大特色在於，擱置關鍵姿態彼此之間的
順序性，改以目標物的重心移動方向作為參考，並將不同動作以數個關鍵姿態的「組成」拆解，以關
鍵姿態的匹配結果作為動作辨識的依據。 
首先，藉由圖形匹配，將輸入的人類剪影和資料庫內事先儲存的各種動作的關鍵姿態作比對，找
出一些最相像的關鍵姿態。而每個關鍵姿態都會有連結不同動作的可能性，所以我們將每個關鍵姿態
都對各個動作賦予不同的權重分數，形成「權重向量」。隨著時間將匹配到的關鍵姿態之權重向量分
數總和後，就是各個動作的權重分數。由於權重分數並沒順序性，因此另外分析了重心的位移情況以
及其垂直方向，用來分辨一些由相同關鍵姿態組成但順序不同的動作。 
由於只考量動作與各個關鍵姿態間的「組成」，再輔以重心的移動軌跡來進行動作辨識，所以對
偶發的匹配錯誤有較佳的強健性，並且能更完整的考量到靜態姿態的描述。此外，在圖形匹配的程序
中，我們新嘗試了一種已用於步伐分析但尚未被拿來應用在圖形辨識的特徵，此特徵能保留姿態外觀
的輪廓特性。到目前為止，我們以 17 個室內常見的動作為辨識目標，並且針對五個受測者以及四種
視角進行測試，實驗結果顯示本方法的辨識率可高達 93.13%。除了強健性，本方法亦具有高度可擴
充性，可允許開發者自由地增加欲進行判斷的動作。 
 
關鍵字：動作辨識、背景相減、關鍵姿態、重心軌跡、動作分數、權重向量 
  iii 
目錄 
 
中文摘要............................................................................................................................................................. i 
Abstract ............................................................................................................................................................. ii 
目錄................................................................................................................................................................... iii 
List of Figures .................................................................................................................................................. iv 
List of Tables .................................................................................................................................................... vi 
Chapter 1 Introduction .............................................................................................................................. 1 
1.1 Motivation .................................................................................................................................. 1 
1.2 Problem Definition ..................................................................................................................... 1 
1.3 Proposed Approach .................................................................................................................... 2 
1.4 System Overview ....................................................................................................................... 3 
Chapter 2 State of the Art .......................................................................................................................... 4 
2.1 Human Body Models ................................................................................................................. 4 
2.1.1 Modeling Human Body with Priori Human Model .......................................................... 4 
2.1.2 Modeling Human Body without Priori Human Model .................................................... 6 
2.2 Human Representation in Approaches without Human Model .................................................. 7 
2.3 Actions Classification ................................................................................................................ 9 
2.3.1 Template Matching Approaches ......................................................................................... 9 
2.3.2 Spatial-Temporal Approaches ...........................................................................................11 
2.3.3 State Space Approaches..................................................................................................... 12 
2.4 Summary .................................................................................................................................. 15 
Chapter 3 Moving Object Detection and Key Postures Matching ....................................................... 16 
3.1 Moving Object Detection ......................................................................................................... 16 
3.2 Feature Extraction .................................................................................................................... 20 
3.2.1 Size Normalization ............................................................................................................. 21 
3.2.2 Distance Vector .................................................................................................................. 23 
3.3 Classification of Key Postures in Database .............................................................................. 24 
3.4 Key Postures Matching Criteria ............................................................................................... 24 
Chapter 4 Actions Recognition ................................................................................................................ 29 
4.1 Weighting Vectors of Key Postures .......................................................................................... 29 
4.2 Moving Direction of the Center Point ...................................................................................... 32 
4.3 Actions Recognition Criteria .................................................................................................... 35 
Chapter 5 Experiment Result .................................................................................................................. 36 
Chapter 6 Conclusion and Future Work ................................................................................................ 49 
Appendix ......................................................................................................................................................... 50 
References ....................................................................................................................................................... 59 
國科會補助計畫衍生研發成果推廣資料表 ................................................................................................. 62 
國科會輔助專題研究計畫成果報告自評表 .............................................................. 錯誤! 尚未定義書籤。 
 
  v 
shown here. ................................................................................................................................... 37 
Figure 5-5 The considered viewpoints in our research. .................................................................................. 38 
Figure 5-6 Snapshots of the walking video (side view). ................................................................................. 38 
Figure 5-7 Snapshots of kneel crawling video (side view). ............................................................................ 39 
Figure 5-8 Snapshots of the sitting down and stand up video (side view). .................................................... 39 
Figure 5-9 Snapshots of the jumping video (side view). ................................................................................ 40 
Figure 5-10 Snapshots of the squatting down and stand up video (side view). ................................................ 40 
Figure 5-11 Snapshots of the crawling video (side view). ............................................................................... 41 
Figure 5-12 Snapshots of the sitting down and stand up video (45 degree viewpoint). ................................... 42 
Figure 5-13 Snapshots of the walking video (45 degree viewpoint). ............................................................... 42 
Figure 5-14 Snapshots of the sitting up and lying down video (45 degree viewpoint). ................................... 43 
Figure 5-15 Snapshots of the kneel crawling video (45 degree viewpoint). .................................................... 43 
Figure 5-16 Snapshots of the crawling video (45 degree viewpoint). .............................................................. 44 
Figure 5-17 Snapshots of the jump video (45 degree viewpoint). .................................................................... 44 
Figure 5-18 Snapshots of the squatting down and standing up video (45 degree viewpoint). ......................... 45 
Figure 5-19 Snapshots of the bending down and bending up video (45 degree viewpoint). ............................ 45 
Figure 5-20 Snapshots of the running video (side view). ................................................................................. 48 
 
  1 
Chapter 1 Introduction 
 
1.1 Motivation 
 
Intelligent surveillance system is the system which automatically monitors people in a specific 
environment. Traditionally, the task of monitoring is done by people, but such task is very tedious and 
labor-intensive. The intelligent surveillance system can significantly promote the manpower utilization 
efficiency. For such the system, the core part is the human action recognition algorithm. In our research, an 
indoor human actions recognition approach is proposed, which can recognize 11 basic indoor human actions 
and 6 static postures. 
 
1.2 Problem Definition 
 
While talking about “human actions”, there are several similar words to “action”, such as ”movement”, 
“motion”, “posture”, “activity”, “behavior”, and so on. When people communicate with each other in daily 
life, some of these words are interchangeable. However, there still exists certain variance. To avoid 
ambiguous situation, some words must be defined in this section. 
In the beginning, based on the number of images needed to recognize, the words “posture” and 
“motion” are defined as follows: 
 Posture: a posture is defined as a static human exterior which can be judged in only one frame. In 
other words, the posture is composed by one image only. 
 Motion: a motion is composed by several postures, and two or more frames are needed to 
recognize a motion. In other words, the motion is composed by more than one image. 
 
Some variances still exist among the “motion”. There were many different taxonomies which have been 
proposed in the past. In our research, the taxonomy proposed by Moeslund et al. [1, 2] is used. By their 
taxonomy, “motion” is further divided into three classifications: “action primitive”, “action” and “activity”. 
 Action Primitive: an action primitive means the atomic human movements, and these can be 
described at limb level. For example, “shaking left hand”, “kicking with right leg”, and so on. 
 Action: an action means the whole body movements, such as “running”, “walking”, “sitting down”, 
and so on. 
 Activity: an activity means the movements contain subsequent actions, and gives them a semantic 
interpretation. For example, “jumping hurdles” is an activity contains actions “running” and 
“jumping”. 
 
In our research, single human actions and postures in singe viewpoint in the indoor environment are 
discussed. The interactions between persons or objects are not discussed. Table 1.1 shows the actions 
recognized in our human actions recognition system. These actions are chosen because they are common 
actions in the indoor environment.  
 
 
 
 
 
 
 
 
 
  3 
approach. The center point trajectory of all actions and static postures are divided into four classes in our 
approach, and each of which has a weight for each action and static posture, too. The weights of center point 
trajectory are multiplied to the action scores of key postures part, and then the final scores of all actions and 
static postures are completed. The action is determined by choosing the action (or static posture) with the 
maximum score, and that score must be larger than a threshold. The details will be shown in chapter 3 and 
chapter 4. Figure 1.1 shows the flow chart of our approach. 
 
 
Figure 1-1 The overall flow chart of our approach. 
 
1.4 System Overview 
 
The organization of our research is as follow. Chapter 2 is an overview of the related researches on 
human actions analysis. Chapter 3 consists of the details of the first stage of our human actions recognition 
system. This stage contains the background segmentation process, size normalization, feature extraction, and 
similarity measurement processes. Chapter 4 introduces the second stage of our system, that is, how to 
connect the sequential matched key postures (the result of the first stage) with human actions: the weighting 
vectors of key postures and center point trajectory. Chapter 5 shows the simulation result. Chapter 6 is the 
discussion and conclusion of our human actions recognition system. And the key postures stored in database 
and the corresponding weighting vectors are shown in Appendix. 
 
 
  5 
torso, hip, arms, and legs. Various body constraints were imposed on the model parameters for the basic 
analysis of the gait. Their approach was computationally expensive, because it searched through all possible 
combinations of 3D configurations, given the known 2D projection, and required accurate extraction of 2D 
stick figures. Bharatkumar et al [5] also used stick figures to model the lower limbs of the human body, 
where joints such as the hip, knee, and ankle were considered. Huber’s human model [6] is a refined version 
of the stick figure models. Joints were connected by line segments with a certain degree of constraint that 
can be relaxed by “virtual springs”. Therefore, this designed articulated kinematic model behaves 
analogously to a mass-spring-damper system. Motion and stereo measurements of joints are confined to a 
3D space called “Proximity Space”. The human head serves as the starting point for tracking all Proximity 
Space locations. Finally, particular gestures were recognized based on the Proximity Space states of the 
joints associated with the head, torso, and arms. 
 
Figure 2-2 A 2D contour human model [7]. 
 
Leung and Yang [7] applied a 2D ribbon model to recognize postures of a human performing gymnastic 
movements. What they emphasized is to estimate motion solely from the outline of a moving human subject. 
Their system consists of two major processes: extraction of human outlines and interpretation of human 
motions. The 2D ribbon model is comprised of two components: the “basic” body model and the “extended” 
body model. The basic body model outlines the structural and shape relationships between the body parts, 
shown in Figure 2-2. The extended model contains three patterns: the support posture model, the side view 
kneeling model, and the side horse motion model. 
 
 
Figure 2-3 A volumetric human model [7]. 
 
Elliptical cylinders (shown in Figure 2-3) are one of the commonly used volumetric models for 
modeling human body. Hogg [7] and Rohr [8] used the cylinder model originated by Marr and Nishihara [9]. 
Human body is represented by 14 elliptical cylinders. Each cylinder is described by three parameters: the 
  7 
The blobs were grouped based on the magnitude and direction of the pixel velocity, which was obtained 
using techniques similar to the optical flow method. The velocity of each part was considered to converge to 
a global average value over several frames. This average velocity corresponded to the motion of the whole 
human body. This observation led to identification of the whole subject via region grouping of blobs with a 
similar smoothed velocity. 
The model-based approaches are intuitive to recognize human actions. However, the human bodies are 
high complexity architectures, the action recognition result is sensitive to the model establish result.  
 
2.2 Human Representation in Approaches without Human Model 
 
Human representation in approaches without human model is mainly derived from silhouette. The first 
step is to get human silhouettes by using background subtraction methods. After obtaining human silhouettes, 
there are many different methods to extract useful features to actions recognition. [37] used the so called 
active contours feature to represent human postures. [39, 40] developed the “star skeleton” descriptor to 
analysis human gait motion. These will be introduced more detail in the following context. 
 
 
Figure 2-5 Left picture: Segmentation of a gray-level image through the traditional-snake; middle picture: GVF-snake; right 
picture: Comparison between the final shape of the GVF-snake and the real human shape [37]. 
  
Buccolieri [37] give a method using the active contours as the image features and take this as the NN 
input to model a system in recognizing the human postures. The system architecture consists of five 
sequential modules that include the moving target detection process, two levels of segmentation process for 
interested element localization, features extraction of the object shape and a human posture classification 
system based on the Radial Basis Functions (RBF) neural network. Moving objects are detected by using an 
adaptive background subtraction method with an automatic background adaptation speed parameter and a 
new fast Gradient Vector Flow snake (shown in Figure 2-5) algorithm for the elements segmentation is 
proposed. In [37], the three layer RBF neural network is used, and the input of the RBF neural network is 
the Euclidean distance from the snake center to the active contour which is shown in Figure 2-6. The 
Euclidean distance is calculated as follows: 
 
 
Figure 2-6 Left image shows the active contour at the end of the deformation, and right graphics represent the relative Euclidean 
  9 
 
Figure 2-8 The boundary is “unwrapped” as a distance function from the centroid. This function is then smoothed and external 
points are extracted [40]. 
 
2.3 Actions Classification  
 
Action classification can be divided into three main approach based on the actions classification 
method: template matching approaches, spatial-temporal approaches, and state space approaches. These are 
discussed in the following text. 
 
2.3.1 Template Matching Approaches 
 
The template matching methods are the methods that combine every frame together first, and then 
analysis or match it with stored representations of known movements. A. F. Bobick and J. Davis [13] 
analyzed human actions in an image sequence by motion-energy images (MEI) and motion-history images 
(MHI). The MEI is a binary image that contains an image sequence information. Let ( , , )D x y t  be a binary 
image sequence indicating regions of motion, and then MEI ( , , )
t
E x y t  is defined as: 
 
1
0
( , , ) ( , , )
i
E x y t D x y t i




   (2.3) 
where τ is critical in defining the temporal extent of a movement. However, MEI can only offer the 
information of where there is motion. To represent how motions the image is occurring (that is, the order of 
motions), MHI was presented. MHI is a gray-level image that the intensity of each pixel is a function of the 
temporal history of motions at that point. The MHI ( , , )
t
H x y t  is defined as: 
 
if ( , , ) 1
( , , )
max(0, ( , , 1) 1) otherwise
D x y t
H x y t
H x y t


 
 
 
 (2.4) 
Note that MEI can be got by adjusting the threshold above zero. Examples of MEI and MHI are shown in 
Figure 2-9 and Figure 2-10. After establishing multi scaled MEIs and MHIs of the input image sequence, 
they calculated Hu moments of each image. 
  11 
 
Figure 2-10 The comparison of MEI and MHI [13]. 
 
2.3.2 Spatial-Temporal Approaches 
 
The spatial-temporal (ST) methods are based on filtering a video volume using a large filter bank. The 
responses of the filter bank are further processed to derive action specific features. Chomat et al [18] 
modeled a segment of video as a ( , , )x y t  spatial-temporal volume and computed local appearance models 
at each pixel using a Gabor filter bank at various orientation and spatial scales and a single temporal scale. A 
given action is recognized using a spatial average of the probabilities of individual pixels in a frame. 
However, this method is not applicable to variations in execution rate because that actions are analyzed at a 
single temporal scale. As an extension to Chomat’s method, Zelnik-Manor and Irani [19] extracted local 
histograms of normalized ST gradients at several temporal scales. The sum of the chi-square metric between 
histograms was used to match an input video with a stored exemplar. Filtering with the Gaussian kernel in 
space and the derivative of the Gaussian on the temporal axis followed by adjusting threshold of the 
responses and accumulation into spatial histograms was found to be a simple yet effective feature for actions 
in far-field setting [20]. These approaches are fast and easy to implement because of efficient algorithms for 
convolution. In most applications, the appropriate bandwidth of the filters is not known a priori, thus a large 
bank at several spatial and temporal scales is required for effectively capturing the action dynamics.  
 
Figure 2-11 The spatial-temporal interest points [22]. 
  13 
interactions between nearby subsequences occurring in time. In many actual situations, a sequence has 
higher correlation between closer subsequences than between distant subsequences [27]. 
 Hidden Markov Model (HMM) is one of the most popular models in state space approaches [28]. 
HMM has been successfully used in speech recognition [28], and in the early 1990s it has been also applied 
in the research fields of image processing and computer vision such as handwritten character recognition, 
human actions recognition, sign language recognition, and facial expression identification. 
Hidden Markov Model comes from the Markov chain. The Markov chain consists of a finite set of 
states, and these states are interconnected with probability distributions in such a way that any state can be 
reached from any other state. Basically, HMM also consists of a finite set of states, and what difference 
between HMM and Markov chain is that HMM can not acquire the information of the states of event directly. 
HMM must utilize the observation sequences to infer the event states indirectly. The basic concept of Figure 
2-13 shows the basic concept of HMM.  
 
 
Figure 2-13 The process flow of HMM [29]. 
 
A complete HMM model is characterized by the following [28]:  
 N: the number of states in the HMM model. Although that the states in HMM are hidden, there is 
often some physical significance attached to the set of states in many practical applications. The 
individual states are denoted as 1 2{ , , , }NS S S S , and the state at time t is denoted as tq . 
 M: the number of distinct observation symbols per state. The individual symbols are denoted as 
1 2
{ , , , }
M
V v v v . 
 { |1 , }ijA a i j N   : the state transition probability distribution form state i to state j, where 
1
( | )
ij t j t i
a P q S q S   . 
  15 
2.4 Summary 
 
Because of the complexity of human actions in real life, there are many approaches to deal with human 
actions recognition. Table 2.1 lists some approaches of literatures and our approach.  
 
Table 2.1 The comparison of some previous approaches (refer to [50]) and our approach. 
 
 
 
  17 
 
The first stage of Two-StaBaS detects the coarse foreground by using Improved Running Average 
Background (IRAB) and a shadow remove algorithm. The moving object detection algorithm is: 
 
1 ,if
_
0 ,if
C B th
F candidate
C B th
  
 
 
 (3.1) 
and 
 ( _ )F F candidate SE SE     (3.2) 
where C is the current image, B is the background, th is a threshold value which is generated by an 
auto-thresholding algorithm, F is the temperate foreground (i.e. the moving object), SE is the morphological 
structuring element,   and   are erosion operation and dilation operation respectively. The last step in 
the first stage of Two-StaBaS is shadow remove. In shadow remove algorithm, firstly the color space of  
pixels in the temperate foreground region of the input image is transformed from RGB space to HSV (Hue, 
Saturation and Value) space, and then the shadow detection method is presented in Eq. (3.3): 
 
, ,
min( ,360 )
H H H H H H
S S S S
V
V L V H
V
AD C B C B TH
AD C B TH
C
TH TH
B

     

  

  

 (3.3) 
where ( , , )
H S I
C C C  represents the hue, saturation, and value of the current frame, ( , , )
H S I
B B B  represents 
the hue, saturation, and value of the background, 
H
AD  and 
S
AD  means the absolutely difference between 
the current frame and background in the hue and saturation space respectively, 
H
TH , 
S
TH , 
,V L
TH  and 
,V H
TH  are the threshold manual assigned, and 
, ,
0 1
V L V H
TH TH   . After above IRAB operation,  the 
first stage result is obtained. Su [25] was not satisfied with this result, therefore the second stage of 
Two-StaBaS was proposed.  
 In the second stage of Two-StaBaS, firstly a few operations of dilation are imposed on the result of the 
first stage, and the new region is called Moving Object Candidate (MOC) region. Distance transform is 
applied on the binary image of MOC region, result in a distance map. Then the normalized distance map is 
used to put different weights on the difference map. After imposing some morphological operations and 
thresholding, the second stage DMO region is extracted. The details of the second stage of Two-StaBaS are 
shown in Figure 3.2, and some experiment results are shown in Figure 3.3 and Figure 3.4. Note that the 
results of Two-StaBaS are generally larger than the object region of ground truth. 
 
  19 
 
Figure 3-4 The foreground in our experiment. The left picture is the current image and the right picture is the binary moving 
object image. 
 
After obtaining the binary moving object image, a bounding rectangle box is established to box the 
moving object, and the boxed region is the ROI region, take Figure 3.5 for example. In Figure 3.5, L1 means 
the left edge of the bounding box, R1 means the right edge of the bounding box, U1 means the upper edge of 
the bounding box and D1 means the lower edge of the bounding box. The X, Y projection histogram of the 
foreground binary image are used to find the four edges of the bounding box. The X projection histogram 
contains the information of the foreground pixel number in each column and the Y projection histogram 
contains the foreground pixel number in each row. After establish the X, Y projection histograms, the 
foreground pixel numbers are checked from the index having the maximum foreground pixel number to the 
boundaries of the image. If the foreground pixel number of index i  and 1i   are smaller than a threshold, 
then the index i  is set to be the boundary of the bounding box.  
 
 
Figure 3-5 The bounding box and the ROI. 
 
The threshold values of the left and right boundaries are equal to 10% of the maximum foreground 
pixel number among all columns in X projection histogram, and the threshold value of the upper boundary is 
equal to 10% of the maximum foreground pixel number among all rows in Y projection histogram but which 
of the lower boundary is zero. The reason to set these as the threshold values is that for the actions 
recognized in our system, the human hands are redundant, so a high boundary threshold value is set to cut 
the human hands (take Figure 3.6 for example). Because the legs are important, the threshold value of the 
lower boundary is zero to obtain complete leg information. 
 
  21 
3.2.1 Size Normalization 
 
There are six pre-set normalization HW ratios in our system: 4, 3, 2, 1, 0.5 and 0.25. The corresponding 
normalization sizes of the normalization HW ratios are shown in Table 3.1. These normalization HW ratios 
are chosen experimentally. While the input human silhouette is bounded with a bounding box, the HW ratio 
of that box will be calculated, and then the input human silhouette will be resized to the closest 
normalization HW ratio. However, for some HW ratios which are not obviously close to one normalization 
HW ratio (for example, the distances between 2HW ratio   and 1.6,1.5,1.4HW ratio     are nearly the 
same to the distances between 1.6,1.5,1.4HW ratio     and 1HW ratio  ). To overcome these situations, 
for input HW ratios which are located on the “middle” of two sequential normalization HW ratios, each of 
which will be normalized to both two sequential normalization HW ratios. See Figure 3.8, if the HW ratio of 
input human silhouette is located in region A, C, E, G, I and K, the input human silhouette will be resized to 
the image with HW ratio 0.25, 0.5, 1, 2 ,3 and 4, respectively. However, if the input HW ratio is located in 
region B, D, F, H and J, the input human silhouette will be resized to the images with HW ratios {0.25, 0.5} , 
{0.5,1} , {1, 2} , {2, 3}  and {3, 4} , respectively (take Figure 3.9 for example). The “middle” region is 
defined as: 
 0.3 ( ) 0.7 ( )
L H L L H L
nHW nHW nHW HW ratio nHW nHW nHW         (3.4) 
Where 
L
nHW  means the lower one of two sequential normalization HW ratios and 
H
nHW  means the 
higher one of two sequential normalization HW ratios. 
 
 
Table 3.1  Corresponding normalization sizes (unit: pixel) of normalization HW ratio. 
Normalization HW ratio 
Corresponding Normalization Size 
( [High Width] ) 
4 [256 64] 
3 [192 64] 
2 [128 64] 
1 [128 128] 
0.5 [64 128] 
0.25 [64 256] 
 
 
Figure 3-8 Size normalization criterion for input image. 
  23 
3.2.2 Distance Vector 
 
After size normalization, a feature called “distance vector” is extracted from the normalized human 
silhouette. The distance vector was firstly proposed by M. Ekinci [41], and it was used for gait recognition. 
Because this feature keeps the characteristics of human exterior contours, it is used as the pattern feature in 
our key postures matching process. 
 
Figure 3-10 The top, bottom, left and right distance vectors. 
 
The concept of the distance vector is shown in Figure 3.10. There are four distance vectors for each 
normalized human silhouette image: the top distance vector, bottom distance vector, left distance vector and 
right distance vector, and each of which is an 1D vector. The lengths of the top and bottom distance vector 
are equal to the width of the bounding box, and the lengths of the left and right distance vector are equal to 
the height of the bounding box. The data in the left/right distance vector is generated by calculating the pixel 
number right/left forward from the left/right edge to the first foreground pixel row by row, and the data in 
the top/bottom distance vector is generated by calculating the pixel number down/up forward from the 
top/bottom edge to the first foreground pixel column by column. Therefore, the maximum value of the 
left/right distance vector is equal to the width of the bounding box (the situation that there are no foreground 
pixel in that row), and the maximum value of the top/bottom distance vector is equal to the height of the 
bounding box (the situation that there are no foreground pixel in that column). To keep the same maximum 
value, the left and right distance vectors are divided by the width of the bounding box and the top and 
bottom distance vectors are divided by the height of the bounding box. That is:  
  (1), (2), ( )TopDV topdv topdv topdv nW    ,  (3.5) 
  (1), (2), , ( )BottomDV bottomdv bottomdv bottomdv nW      (3.6) 
  (1), (2), , ( )LeftDV leftdv leftdv leftdv nH      (3.7) 
  (1), (2), , ( )RightDV rightdv rightdv rightdv nH      (3.8) 
and 
  25 
 However, before aligning the four distance vectors to an 1D distance vector, the weight of each distance 
vector should be adjusted. When the length of a distance vector is small, it is sensitive to a little variation of 
human pose. Take standing people for example (Figure 3.12). In this case, the top and bottom distance 
vectors are the vectors with shorter length and the left and right distance vectors are the one with longer 
length. It is obviously that the top distance vector of a human standing up straight and the top distance vector 
of a humpback human are very different, but it is not change severely for left and right distance vectors. 
Therefore, the weight of each distance vector is adjusted according to the original height and width of the 
original ROI (that is, before size normalization). The weight adjustment process is: 
 
 
 
 
W
Top DV
Top DV
H
  (3.14) 
 
 
 
W
Bottom DV
Bottom DV
H
  (3.15) 
 
 
 
W
Left DV
Left DV
W
  (3.16) 
 
 
 
W
Right DV
Right DV
W
  (3.17) 
and 
  [           ]
W W W W
Feature Vector Top DV Bottom DV Left DV Right DV  (3.18) 
Where H is the original height of the ROI and W is the original width.  
 
 
Figure 3-12 An example for explaining the purpose of the weight adjustment. 
  27 
 
Figure 3-13 An example of key postures matching. One key posture is matched in this example. 
 
 
 
Figure 3-14 An example of key postures matching. Two key postures are matched in this example. 
 
  29 
Chapter 4 Actions Recognition 
 
After key postures matching process, the matched key posture(s) in each frame is obtained. To achieve 
action recognition, the matched key postures of more than one frame is needed. The following task is to 
associate the matched key postures with the correct human action.  
In our system, an Action Scores (AS) is established. The AS is: 
 
1
2
( )
( )
( )
( )
M
as t
as t
AS t
as t
 
 
 
 
 
 
    
 (4.1) 
Where M is the number of the actions and static postures recognized in our system, ( )
i
as t  is the score of 
action i at time t. 
This AS records the scores of all actions, and action recognition is achieved by choosing the action with 
maximum score and this score must be larger than a threshold. Question is, how to get the score for each 
action? There are two factors: the weighting vector of each key posture and the weighting vector of the 
center point motion. Figure 4.1 shows the overall flow chart of our system, and the action recognition 
process is bounded with a dot line. The details will be discussed in the following text. 
 
Figure 4-1 The flow chart of overall system. The action recognition process (which is discussed in this chapter) is bounded with a 
blue dot line. 
 
4.1 Weighting Vectors of Key Postures 
 
Figure 4.2 shows the idea of our method. While a person see a human posture picture, he will 
automatically associate this posture to some possible actions. It can be regard as that the person gives 
  31 
 The weighting vector of each key posture is assigned manually. The rules of the weighting vector 
design are shown in Table 4.1.  
 
Table 4.1 The rules of the weighting score vector design. 
For actions 
 If the key posture belongs to only one specific action i 
Give maxScore weight for action i and zero for other 
actions. 
 If the key posture belongs to one specific action i, but it also 
appears in some other actions j 
Give maxScore weight for action i, and 0.5*maxScore for 
actions j, and zero for other actions. 
 If the key posture belongs to no specific action, but it 
appears in some actions j 
Give 0.5*maxScore weight for action j, and zero for other 
actions. 
 The maxScore = 1. 
For static postures 
 If static posture i is associated when the key posture 
consecutively appears for a long time 
Give maxScore weight for static posture i and zero for 
other static postures.. 
 If no static posture is associated when the key posture 
consecutively appears for a long time 
Give zero weight for all static postures. 
 The maxScore = 1. 
 
As mentioned before, the weighting vector of key postures at frame t (WKP(t)) is generated by 
summing up all weighting vectors of matched key postures at that frame. Because actions are composed by 
several frames of key postures, several weighting vectors in several frames must be considered. If the 
weighting vectors of key postures in all frames are added together directly, the score of each action is 
accumulated and then it will diverge as time passes by, and it makes the system out of balance. Besides, it 
does not make sense to take all past information to analysis human actions. For these reasons, the WKP(t) 
are firstly multiplied by a decay factor (DF) and then are added with the weighting vector of key postures at 
recent frame. The trapezoid function is chosen as the decay factor. That is: 
 
[ , , , ]
0 ,if 
, if 
( ) ( ) 1 , if 
, if 
0 , if 
trap
a b c d
x a
x a
a x b
b a
DF x f x b x c
d x
c x d
d c
d x

 
  


   
 
  

 
 (4.4) 
Where a, b, c, d are parameters of the trapezoid function. Note that the decay factor also indicates the fact 
that how many recent frames are considered for action recognition. In our system, 0a  , 0b  , 3c  , and 
7d  , shown in Figure 4.3. The x-axis means nth recent time. For example, 0x   means the current time, 
1x   means the first recent time, 2x   means the second recent time, and so on. This trapezoid function 
indicates that our system take the information of up to 7 recent times to analysis human actions, and the 
weights of first four times are 1 and it starts to decay from the forth recent time ( 3x   in Figure 4.3) to 
eighth recent time ( 7x   in Figure 4.3). The information at times before the eighth recent time is not 
considered, that is, gives zero weights for them. Briefly to say, only recent 7 frame information is 
considered. 
  33 
downward, and moving horizontally (shown in Table 4.2). Note that the motionless actions are the static 
postures. The first question is whether the human is moving. In the indoor environment, it is possible that 
people keep the same postures for a long time, that is, motionless. For example, sleep (lying for a long time) 
and sitting on a chair to read a book (sit for a long time). Therefore, if the human in the image is motionless 
or moving is firstly judged. If the human is motionless, the recognition result is “static posture” instead of an 
“action”. The conditions of motionless and moving are shown later. 
Table 4.2 The actions (static postures) of each center point motion result. 
Motionless Moving Upward Moving Downward 
Moving Only 
Horizontally 
Standing Bending Up Bending Down Walking 
Sitting Standing Up Sitting Down Kneel Crawling 
Kneeling Sitting Up Squatting Down Crawling 
Lying Jumping Lying Down  
Squatting    
Bending    
 
If the human is moving, the moving direction of his center point is then analyzed. Basically, the moving 
direction of the center point includes horizontal direction and vertical direction. The horizontal direction is 
divided into leftward, rightward, and neither leftward nor rightward, and the vertical direction is divided into 
upward and downward. By observation, there are only actions which consist of the same key postures but 
are different in vertical moving direction. Therefore, the horizontal direction is not important in our 
approach, and the moving actions are classified into only 3 classes (moving upward, moving downward, and 
moving horizontally). 
The Instantaneous Displacement (ID) and Net displacement in 7 frames (ND) are calculated to 
determine the human motion situation. The ID means the displacement of the center points of two sequential 
frames, and the ND means the displacement of the current center point and the center point of the 7th 
previous frame. That is: 
 
 ( ) ( ), ( )
( ) ( 1), ( ) ( 1)
( 1) ( 2), ( 1) ( 2)
( 5) ( 6),
hori vert
hori hori vert vert
hori hori vert vert
hori hori
ID t ID t ID t
cp t cp t cp t cp t
cp t cp t cp t cp t
cp t cp t cp

   
     

  
 
     
 
          
                                       
 ( 5) ( 6)
vert vert
t cp t
 
 
 
 
 
   
 (4.6) 
 
( ) [ ( ), ( )]
[ ( ) ( 6), ( ) ( 6)]
hori vert
hori hori vert vert
ND t ND t ND t
cp t cp t cp t cp t

    
 
           
 (4.7) 
Where ( )horicp t  and ( )vertcp t  mean the horizontal and vertical positions of the center point at frame t. 
Table 4.3 shows the motionless and moving conditions. 
 
Table 4.3 The condition for each center point motion result. 
Motionless 
 The absolute values of horizontal and vertical parts 
of net displacement of the center point in most 
recent 7 frames ( ( )horiND t  and ( )vertND t ) are 
both less than a pre-set tolerance. 
 All absolute values of the elements of ( )horiID t  
and ( )vertID t  are less than a pre-set tolerance. 
Moving  
 The center point motion does not satisfy all 
motionless conditions. 
  35 
section 4.3). Because the purpose of the center point weight is to distinguish the actions with the same key 
postures but with contrary vertical displacement (that is, these actions get equivalent scores from key 
postures matching part), the weights of center point motion is quite small (0, 1, or the values a little larger or 
smaller than 1). For example, sitting down and standing up get equivalent scores from key postures 
matching part, but sitting down is the action with downward direction and standing up is the action with 
upward direction, their scores of key postures matching part are multiplied (see section 4.3) with different 
weights, and then the final action scores of sitting down and standing up become different.  
 The center point trajectories of the actions with horizontal direction are not only horizontal in 45 degree 
viewpoint. However, because the magnitudes of action scores of key postures are much larger than the 
weights of center point motion, the final scores of these actions are still much larger than those actions with 
upward or downward direction. In other words, the scores of key postures part is dominant, so for the 
situation that actions with horizontal direction in 45 degree viewpoint (for example, walking in 45 degree 
viewpoint), the result is not affected. 
 
4.3 Actions Recognition Criteria 
 
As mentioned before, all actions and static postures have their own score at each frame. That is, action 
scores are established (Eq. 4.1), and the scores of all actions and static postures are determined by key 
postures matching and the center point motion. The action scores of the key postures part are multiplied with 
the weights of the center point motion, that is: 
 
1 1 1
2 2 2
( ) ( ) ( )
( ) ( ) ( )
( )
( ) ( ) ( )
M M M
as t wcp t askp t
as t wcp t askp t
AS t
as t wcp t askp t
   
   
    
   
   
   
                
 (4.12) 
where ( )
i
wcp t  is the weight of the center point motion for action i ( 1,2, ,i M ) at frame t which is 
determined in section 4.2, and ( )
i
askp t  is the action scores of the key postures part at frame t which is 
calculated in (Eq. 4.5). Finally, the action is determined by choosing the maximum score in that frame, 
besides this maximum score must be larger than a threshold. That is: 
 max  and 
i Action Action
i
Action as as Threshold   (4.13) 
Note that the threshold value determines the number of frames needed to recognize human actions. The 
number of frames needed to recognize human actions (frame number) is: 
 Action
Thresold
frame number =
3 maxScore
 
  
 (4.14) 
In our design, 1maxScore   , 7
Action
Threshold  . Therefore, at least 3 frames are needed to recognize 
actions. If there is no score of action which is larger than the threshold, no result is shown. 
  37 
0 10 20 30 40 50 60 70 80
0
2
4
6
8
10
12
14
16
18
20
Frame
A
c
ti
o
n
 S
c
o
re
s
 
 
Bending Down
Bending Up
Standing
Bending
 
Figure 5-2 The action scores of the test video shown in Figure 5.1. Note that only four action scores are shown here. 
 
 
Figure 5-3 Snapshots of the sitting up and lying down video. 
 
0 20 40 60 80 100 120 140
0
2
4
6
8
10
12
14
16
18
20
Frame
A
c
ti
o
n
 S
c
o
re
s
 
 
Sitting Up
Lying Down
Sitting
Lying
 
Figure 5-4 The action scores of the test video shown in Figure 5.3. Note that only four action scores are shown here. 
 
 
 
  39 
 
Figure 5-7 Snapshots of kneel crawling video (side view). 
 
Figure 5-8 Snapshots of the sitting down and stand up video (side view). 
 
  41 
 
Figure 5-11 Snapshots of the crawling video (side view). 
 
Table 5.2 The recognition rate of the side viewpoint situation. 1 person in database, and 5 subjects are tested. 
Actions Recognition Rate Number of Total Frame 
Sitting Down & Standing Up 87.38% 1515 
Walking 100% 576 
Sitting Up and Lying Down 94.26% 1264 
Kneel Crawling 99.37% 1049 
Crawling 83.5% 1029 
Jumping 90.58% 388 
Squatting Down & Standing Up 92.96% 983 
Bending Down & Bending Up 92.55% 796 
Average / Total Frames 92.58% 7600 
 
Following Figure 5.12~5.19 show some simulation results of each action in the 45 degree viewpoint, 
and the recognition rate in 45 degree viewpoint is shown in Table 5.3. Note that the recognition rates are 
calculated in frame by frame way.  
 
 
  43 
 
Figure 5-14 Snapshots of the sitting up and lying down video (45 degree viewpoint). 
 
 
Figure 5-15 Snapshots of the kneel crawling video (45 degree viewpoint). 
  45 
 
Figure 5-18 Snapshots of the squatting down and standing up video (45 degree viewpoint). 
 
 
 
Figure 5-19 Snapshots of the bending down and bending up video (45 degree viewpoint). 
 
 
  47 
 Recall Rate and Precision Rate 
The specific recognition situation of “walking” and “kneel crawling” are shown in Table 5.6 and Table 
5.7. For action “walking”, the recall rate is 96.24% and the precision rate is 94.60%. For action “kneel 
crawling”, the recall rate is 98.24% and the precision rate is 96.48%. 
 
Table 5.6 The recognition situation for “walking”. (unit: frame) 
 Actual Action 
Obtained 
Action 
 Walking Not Walking 
Walking 946 54 
Not Walking 37 14892 
Recall Rate 96.24% 
Precision Rate 94.60% 
 
Table 5.7 The recognition situation for “kneel crawling”. (unit: frame) 
 Actual Action 
Obtained 
Action 
 Kneel Crawling Not Kneel Crawling 
Kneel 
Crawling 
2634 96 
Not Kneel 
Crawling 
47 13152 
Recall Rate 98.24% 
Precision Rate 96.48% 
 
 Expansibility 
The advantage of our approach is that it can be application to recognize more actions. To justify this 
advantage, 28 key postures of “running” of side view are added into database and test that if this action can 
be correctly recognized. Experiment result shows that our approach can correctly recognize the additional 
action (running). Besides, the original actions “walking” and “humping” which are similar actions to 
“running” are tested. The average correct rates of before and after adding “running” are shown in Table 5.6, 
and it proves that our approach can recognize more actions. Figure 5.20 shows the snapshots of running 
videos. 
 
Table 5.8 The recognition rates of “walking” and “jumping” before and after adding the “running” action 
(side view). 
Actions 
Before Adding 
“Running” 
After Adding 
“Running” 
Number of Total 
Frames 
Walking 100% 100% 576 
Jumping 90.58% 90.54% 388 
Running  100% 243 
 
 
  49 
Chapter 6 Conclusion and Future Work 
 
A new approach for human actions recognition which combines the weighting vectors of key postures 
and center point trajectory is proposed. In pattern matching process, the HW ratio of the input human 
silhouette is used first to coarsely classify the input image into one or two of six classes, and then the 
modified distance vector is extracted as the feature of the pattern. Cross-correlation coefficient is utilized as 
the similarity measurement, and one to three key postures are matched in each frame. The weighting vectors 
of matched key postures in recent 7 frames are multiplied with the decay factor and added together to form 
the action score of key postures part. In the other side, the center point trajectory in recent 7 frames is 
calculated and classified into moving or motionless, and each situation has weights for all actions. Finally 
the weights of the center point trajectory and the action scores of key postures part are multiplied together to 
form the final scores of all actions and static postures, and the action (static postures) with the maximum 
score and that score is larger than the action determination threshold is chosen as the action recognition 
result.  
Our approach has the advantage of expansion ability, that is, it can be applied to different actions 
anyone wants to recognize.  
The main feature of our approach is that the center point trajectory is used to substitute for the order of 
key postures. Our approach has advantages of expansibility (adding more actions if someone wants), good 
robustness for the variation of action duration, and stationary temporal situation consideration (static 
postures), and it is robust against the error result of pattern matching. 
 For our future work, these basic human actions and static postures can be associated to human activities 
(behaviors). Besides, multiple human actions recognition system can be achieved by combining a tracking 
algorithm. 
  51 
 
91          
 
101          
 
111      
    
 
121          
 
131 
 
  
 
     
 
141 
         
 
151          
 
161          
 
171          
 
181   
    
   
 
191          
 
201          
 
  53 
K
ey
 P
os
tu
re
s 
N
um
be
r 
W
al
ki
ng
 
B
en
di
ng
 D
ow
n 
B
en
di
ng
 U
p 
S
it
ti
ng
 D
ow
n 
S
it
ti
ng
 t
o 
S
ta
nd
in
g 
S
qu
at
ti
ng
 D
ow
n 
S
qu
at
 t
in
g 
to
 S
ta
nd
in
g 
K
ne
el
 C
ra
w
li
ng
 
C
ra
w
li
ng
 
S
it
ti
ng
 U
p 
L
yi
ng
 D
ow
n 
Ju
m
pi
ng
 
S
ta
nd
in
g 
S
it
ti
ng
 
K
ne
el
in
g 
L
yi
ng
 
S
qu
at
ti
ng
 
B
en
di
ng
 
30 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
31 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
32 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
33 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
34 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
35 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
36 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
37 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
38 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
39 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
40 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
41 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
42 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
43 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
44 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
45 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
46 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
47 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
48 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
49 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
50 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
51 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
52 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
53 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
54 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
55 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
56 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
57 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
58 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
59 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
60 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
61 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 
62 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
63 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
  55 
K
ey
 P
os
tu
re
s 
N
um
be
r 
W
al
ki
ng
 
B
en
di
ng
 D
ow
n 
B
en
di
ng
 U
p 
S
it
ti
ng
 D
ow
n 
S
it
ti
ng
 t
o 
S
ta
nd
in
g 
S
qu
at
ti
ng
 D
ow
n 
S
qu
at
 t
in
g 
to
 S
ta
nd
in
g 
K
ne
el
 C
ra
w
li
ng
 
C
ra
w
li
ng
 
S
it
ti
ng
 U
p 
L
yi
ng
 D
ow
n 
Ju
m
pi
ng
 
S
ta
nd
in
g 
S
it
ti
ng
 
K
ne
el
in
g 
L
yi
ng
 
S
qu
at
ti
ng
 
B
en
di
ng
 
98 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 
99 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 
100 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 
101 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 0 0 0 0 0 0 
102 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
103 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
104 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
105 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
106 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
107 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
108 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
109 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
110 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
111 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
112 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
113 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
114 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
115 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
116 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
117 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
118 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
119 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
120 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
121 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
122 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
123 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
124 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
125 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
126 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
127 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
128 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
129 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 
130 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
131 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
  57 
K
ey
 P
os
tu
re
s 
N
um
be
r 
W
al
ki
ng
 
B
en
di
ng
 D
ow
n 
B
en
di
ng
 U
p 
S
it
ti
ng
 D
ow
n 
S
it
ti
ng
 t
o 
S
ta
nd
in
g 
S
qu
at
ti
ng
 D
ow
n 
S
qu
at
 t
in
g 
to
 S
ta
nd
in
g 
K
ne
el
 C
ra
w
li
ng
 
C
ra
w
li
ng
 
S
it
ti
ng
 U
p 
L
yi
ng
 D
ow
n 
Ju
m
pi
ng
 
S
ta
nd
in
g 
S
it
ti
ng
 
K
ne
el
in
g 
L
yi
ng
 
S
qu
at
ti
ng
 
B
en
di
ng
 
166 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 
167 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 
168 0 0.5 0.5 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
169 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
170 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
171 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
172 0 0.5 0.5 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
173 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
174 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
175 0 0.5 0.5 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
176 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 0 1 0 0 0 0 
177 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 
178 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 0 1 0 0 0 0 
179 0 0.5 0.5 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
180 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
181 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
182 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
183 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
184 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
185 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
186 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 
187 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 
188 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
189 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
190 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 
191 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
192 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
193 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 
194 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
195 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
196 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 
197 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 
198 0 0 0 0.5 0.5 0.5 0.5 0 0 0 0 0 1 0 0 0 0 0 
199 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 
  59 
References 
 
[1] T. B. Moeslund, A. Hilton, and V. Kruger, “A Survey of Advances in Vision-Based Human Motion 
Capture and Analysis,” Computer Vision and Image Understanding, vol. 104, no. 2-3, pp. 90-126, 
2006. 
[2] R. Poppe, “A Survey on Vision-Based Human Action Recognition,” Computer Vision and Image 
Understanding, vol. 28, no. 6, pp. 976-990, 2010. 
[3] J. K. Aggarwal and Q. Cai, “Human Motion Analysis: A Review,” Proceedings IEEE Nonrigid and 
Articulated Motion Workshop, pp. 90-102, Jun 1997. 
[4] Z. Chen and H. J. Lee, “Knowledge-Guided Visual Perception of 3-D Human Gait from A Single 
Image Sequence,” IEEE Transactions on System, Man and Cybernetics, vol. 22, no. 22, pp. 336-342, 
Mar/Apr 1992. 
[5] A. G. Bharatkumar, K. E. Daigle, M. G. Pandy, C. Qin, and J. K. Aggarwal, "Lower Limb Kinematics 
of Human Walking with the Medial Axis Transformation," Proceedings of the 1994 IEEE Workshop on 
Motion of Non-Rigid and Articulated Objects, pp.70-76, Nov 1994. 
[6] E. Huber, "3-D Real-time Gesture Recognition Using Proximity Spaces," Proceedings of the 3rd IEEE 
Workshop on  Applications of Computer Vision,   pp.136-141, Dec 1996 
[7] D. Hogg, “Model-Based Vision: a Program to See a Walking Person,” Image and Vision Computing, 
vol. 1, no. 1, 1983. 
[8] K. Rohr, “Towards Model-Based Recognition of Human Movements in Image Sequences,” Computer 
Vision, Graphics, and Image Processing, pp. 94-115, 1994. 
[9] D. Marr and H. K. Nishihara, “Representation and Recognition of the Spatial Organization of 
Three-Dimensional Shapes,” Proceedings Royal Society London, vol. B, pp. 269-294, 1978. 
[10] R. F. Rashid, “Towards a System for the Interpretation of Moving Light Displays,” IEEE Transactions 
on Pattern Analysis and Machine Intelligence, vol. 2, no. 6, 1980. 
[11] A. Shio and . Sklansky, “Segmentation of People in Motion,” Proceedings of IEEE Workshop on 
Visual Motion, IEEE Computer Society, pp. 325-332, 1991. 
[12] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder: Real-Time Tracking of the 
Human Body,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 
780-785, July 1997. 
[13] A. F. Bobick and J. W. Davis, "The Recognition of Human Movement Using Temporal 
Templates," IEEE Transactions on  Pattern Analysis and Machine Intelligence, , vol.23, no.3, 
pp.257-267, Mar 2001. 
[14] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri, "Actions as Space-Time 
Shapes," Computer Vision, 2005. Tenth IEEE International Conference on , vol.2, no., pp.1395-1402 
Vol. 2, 17-21 Oct. 2005. 
[15] P. Turaga, R. Chellappa, V. S. Subrahmanian, and O. Udrea, "Machine Recognition of Human 
Activities: A Survey," IEEE Transactions on  Circuits and Systems for Video Technology, vol.18, 
no.11, pp.1473-1488, Nov. 2008. 
[16] A. Mokber, C. Achard, and M. Milgram, “Recognition of Human Behavior By Space-Time Silhouette 
Characterization,” Pattern Recognition Letters, vol. 29, no. 1, pp. 81-89, Jan. 2008. 
[17] R. Poppe, “Vision-Based Human Motion Analysis: An Overview,” Computer Vision and Image 
Understanding, vol. 108, no. 1-2, pp. 4-18, 2007. 
[18] O. Chomat and J. L. Crowley, “Probabilistic Recognition of Activity Using Local Appearance,” 
Proceedings of IEEE Conference Computer Vision and Pattern Recognition, vol. 02, pp. 104-109, 
1999. 
[19] L. Z. Manor and M. Irani, “Event-Based Analysis of Video,” Proceedings of IEEE Conference 
Computer Vision and Pattern Recognition, vol. 02, pp. 123-130, 2001. 
[20] H. Zhong, J. Shi, and M. Visontai, “Detecting Unusual Activity in Video,” Proceeding of IEEE 
Conference Computer Vision and Pattern Recognition, pp. 819-826, 2004. 
  61 
[42] J. B. Arie, W. Zhiqian, P. Pandit, and S. Rajaram, "Human Activity Recognition Using 
Multidimensional Indexing," IEEE Transactions on Pattern Analysis and Machine Intelligence, , 
vol.24, no.8, pp. 1091- 1104, Aug 2002. 
[43] X. Feng and P. Perona, “Human Action Recognition By Sequence of Movelet Codewords,” 
Proceeding of 3D Data Processing Visualization and Transmission, pp. 717-723, 2002. 
[44] A.A. Efros, A.C. Berg, G. Mori, and J. Malik, "Recognizing Action at a Distance," Proceedings. Ninth 
IEEE International Conference on  Computer Vision, pp.726-733, Oct. 2003. 
[45] C. Schuldt, I. Laptev, and B. Caputo, "Recognizing Human Actions: a Local SVM 
Approach," Proceedings of the 17th International Conference on  Pattern Recognition, vol.3, pp. 32- 
36, Aug. 2004. 
[46] V. Kellokumpu, M. Pietikainen, and J. Heikkila, “Human Activity Recognition Using Sequences of 
Postures,” IAPR Conference on Machine Vision Applications, 2005. 
[47] C. Sminchisescu, A. Kanaujia, L. Zhiguo, and D. Metaxas, "Conditional Models for Contextual 
Human Motion Recognition," IEEE International Conference on  Computer Vision, vol.2, 
pp.1808-1815, Oct. 2005. 
[48] D. Weinland, R. Ronfard, and E. Boyer, “Motion History Volumes for Free Viewpoint Action 
Recognition,” IEEE Workshop Modeling People and Human Interaction, 2005. 
[49] A. Yilmaz and M. Shah, “Action Sketch: A Novel Action Representation,” IEEE Computer Society 
Conference on  Computer Vision and Pattern Recognition, vol.1, pp. 984- 989, June 2005. 
[50] L. Wang and D. Suter, "Learning and Matching of Dynamic Shape Manifolds for Human Action 
Recognition," IEEE Transactions on  Image Processing,, vol.16, no.6, pp.1646-1661, June 2007. 
 
 
 
  63 
技術/產品應用範圍 
居家監控系統、居家安全及看護系統、金融保全系統、一般消費
性電子產品等等領域應用上。 
技術移轉可行性及預期
效益 
  本研究所設計的演算法可輕易、快速地將技術移植於居家監控
系統、居家安全及看護系統、金融保全系統、一般消費性電子產
品等等領域應用上。所建立的系統僅需要單一影像攝影機，無頇
額外感測器（三維加速規、壓電器或是壓力規），系統硬體成本
具競爭力。且裝設架構簡易方便，體積小維護容易，且不影響裝
設地點原來之規劃及作息。 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：陳永耀 計畫編號：97-2221-E-002-165-MY3 
計畫名稱：以影像為基礎之多目標智慧型動作辨識 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 Brussels, 
Belgium, 
pp.2021-2024, 
Sep., 2011. 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
