 I
一、 研究計畫中英文摘要 
（一） 計畫中文摘要 
在最近幾十年來，排程可能性與排程問題在即時系統研究的社群中，始終受到高度的注意。
針對一組任務群來進行即時排程已經被仔細地研究，而其對應的可排程性分析也已被發展
出來。由於精確地分析一組任務群的可排程性（稱為精確可排程性分析）需要大量的時間，
因此如何在精確性與效率之間取得平衡便引起了廣泛的探討。很多有效率但不精確（也就
是充分但非必要）的可排程性分析在文獻中被討論。然而，如何精確而有效率地分析一組
任務群的可排程性仍然是一個重要的議題。 
 本研究計畫為期一年。我們著重在減少著名的 Audsley 分析法在對一組任務群進行精
確可排程性分析時所需耗費的時間。Audsley 分析法能有效地針對『一組以 Rate Monotonic
排程法（一種最佳的固定優先權排程法）來排程的任務群』進行精確可排程性分析。藉由
在每個分析循環中，恰當地將任務群分成兩組子任務群，並以不同方式來處理這兩組子任
務群，使得『分析任務群可排程性』所需的循環次數大幅減少。在這個一年期的研究計畫
中，我們也透過實驗來評估所提出演算法的能力，並與一些相關的研究進行比較。本計劃
的研發成果不但可以提供即時系統於精確可排程性分析上的創見與洞悉力，並且可以大幅
縮短精確可排程性分析的所需時間。所有參與學生也於相關的即時系統發展上獲得很好的
研究經驗。 
（二） 計畫英文摘要 
Feasibility and schedulability problems have received considerable attention in the real-time 
systems research community in recent decades. Real-time scheduling for task sets has been 
studied, and the corresponding schedulability analysis has been developed. Due to the 
considerable overheads required to precisely analyze the schedulability of a task set (referred to 
as exact schedulability analysis), the trade-off between precision and efficiency is widely studied. 
Many efficient but imprecise (i.e., sufficient but not necessary) analyses are discussed in the 
literature. However, how to precisely and efficiently analyze the schedulability of task sets 
remains an important issue. 
This is a one-year project. We focus on reducing the runtime overhead of the Audsley’s 
Algorithm. The Audsley’s Algorithm was known as an effective exact schedulability analysis for 
task sets under rate-monotonic scheduling (one of the optimal fixed-priority scheduling 
algorithms). By properly partitioning a task set into two subsets and differently treating these two 
subsets during each iteration, the number of iterations required for analyzing the schedulability of 
the task set can be significantly reduced. During this one-year project, the capability of the 
proposed algorithm was evaluated and compared to related works as well. The results of this 
project provides not only good insights in exact schedulability analysis but also significant 
advance to the efficiency of exact schedulability analysis. Students who are involved in this 
project develop excellent experiences in real-time system research. 
 2
來觀察此排程是否能讓任務群可以在期限內完成工作。 
許多學者都針對如何精確而有效率地分析一組任務群在固定優先權排程下的可排程性
進行研究。其中，Bini 與 Buttazzo [4] 提出一個可調式的可排程性分析架構，可以透過調整
參數在執行時間與精確性之間取得平衡；Bril [5] 修改由 Audsley [2] 所提出的著名精確可
排程分析 (稱為 Audsley’s Algorithm) 的初始設定，使分析的過程可以提早進入主要的部
分。不同於 Bril 所提出的方法僅能侷限於改善初始設定，本研究計畫能在分析過程中持續
略過不必要的分析。藉由在每個分析循環中，恰當地將任務群分成兩組子任務群，並以不
同方式來處理這兩組子任務群，我們能夠使『分析任務群可排程性』所需的循環次數大幅
減少。 
五、 研究方法 
5.1 任務模型與定義 
本計畫所探討之精確可排程分析，是專注在單處理器上、以 RM 排程演算法對週期性任務
進行排程的系統。一個週期性的任務通常可以用四個參數來定義：任務開始時間、工作出
現週期、工作最高所需執行時間，與相對完成期限。為了不模糊焦點，我們將任務模型簡
化如下：假設所有任務的第一個工作都在同一個時間點開始執行，並且所有任務的工作出
現週期與相對完成期限是一致的。 
定義一 任務模型與任務群：一個週期性任務 iτ 是由一連串同一屬性的工作所構成，這些工
作有規律的起始執行時間（即每個週期固定出現一個工作），Ji,j代表第 i 個任務的第 j 個工
作。我們用 Tn = { 1τ = (c1, p1), 2τ = (c2, p2), . . . , nτ = (cn, pn)}來表示一組任務群，ci與 pi分別
是任務 iτ 的工作最高所需執行時間與工作出現週期。 iτ 的工作利用率以 ui 來表示，定義為
ci/pi。所有的工作一出現時即可準備執行，並且必須在下個工作出現前完成。在不失一般性
的原則下，我們假設 nppp ≤≤≤ ...21 。 
定義二 Rate monotonic (RM) 排程演算法：RM 排程演算法是一種最佳的固定優先權排程
演算法，這表示只要有一組任務群在某種固定優先權排程下，其所有的工作皆可以在期限
內完成，則這組任務群用 RM 演算法來排程，所有的工作也都可以在期限內完成。RM 演
算法的優先權給定方式是：工作出現週期越小的任務，優先權越高。 
定義三 Audsley’s Algorithm (AA)：根據 AA，任務 iτ 的最慢完成時間可以藉由反覆套用公
式： ⎡ ⎤∑ −=+ ⋅+= 11 )()1( /ik kklil cprcr 直到 r(l+1)收斂到一個實數 ri或是超過任務 iτ 的完成期限時即
終止，此處 ∑ == ij jcr 1)0( 。如果一個任務的最慢完成時間不超過其完成期限，則稱此任務可
排程；否則，則稱此任務不可排程。注意此處任務群會先依優先權排序，由最高優先權的
任務開始依序往低優先權的任務測試其可排程性。一旦有任務不可排程，則測試終止並宣
告此任務群不可排程。如果一組任務群的所有任務皆可排程，則我們稱此任務群可排程。 
 4
若且惟若 FEA 判定任務群 T 可排程。 
定理二：FEA 需要 O(n)的時間來找出下一個時間點 r(l)。 
0
10
20
30
40
50
60
70
80
90
100
75 80 85 90 95 100
Total Utilization (%)
Ite
ra
tio
n 
R
at
io
 (%
) Ratio = 0.4
Ratio = 0.1
Ratio = 0.8
Ratio = 0.2
(a) Comparison in the Iteration Ratio
Ratio = 0.4
Ratio = 0.1
Ratio = 0.8
Ratio = 0.2
0
10
20
30
40
50
60
70
80
90
100
R
un
tim
e 
R
at
io
 (%
)
75 80 85 90 95 100
Total Utilization (%)
(b) Comparison in the Runtime Ratio  
圖 5.1：在不同的 Ratio 值之下的改善狀況 
六、 結果與討論 
當系統要對一任務群進行可排程性分析時，會由最高優先權的任務開始，依序往低優先權
的任務檢查，直到確認所有的任務皆能夠在時限內完成，則回報此系統可排程，若在過程
中有某一任務無法及時完成，則終止可排程性分析，並判定此系統不可排程。一般系統考
量到效率問題，會先以較有效率的『充分但非必要分析』來判定任務群是否可排程，當某
任務被判定為不可排程時，再改以較費時的『精確可排程分析』，亦即 Audsley’s Algorithm 
(AA) 或我們所提出的 Fast Exact Analysis (FEA)，來判定任務是否真的不可排程。在實驗
中，我們亦套用了 Bril [5] 的作法在 AA 及 FEA 上，在實驗結果中分別以 Enhanced AA 與 
Enhanced FEA 來表示。圖 6.1 顯示了任務群對處理器的總使用率分別在 75%~100%時，任
務無法通過『充分但非必要分析』，而需要以『精確可排程分析』來判定其可排程性的平均
比例。 
 
圖 6.1：任務群中無法通過充分但非必要分析的任務平均比例 
6.1 實驗結果 
 6
對處理器的總使用率達到 100%時，FEA 平均所需的執行時間可比 AA 減少約 55.1%。由此
實驗我們可以得知，我們所提出的精確可排程分析在系統負載極重時 (95%~100%) 能夠大
幅減少精確可排程分析所需的時間，這對許多需要線上 (on-line) 即時進行可排程分析的系
統來說，是相當重要的。 
參考文獻 
[1] Audsley, N.C., Burns, A., Richardson, M., Wellings, A. 1991. Hard realtime scheduling: the 
deadline-monotonic approach. In Proceedings of the 8th IEEE Workshop on Real-Time Operating 
Systems and Software, May, pp. 133–137. 
[2] Audsley, N.C., Burns, A., Richardson, M., Tindell, K., Wellings, A., 1993. Applying new 
scheduling theory to static priority preemptive scheduling. Software Engineering Journal 8 (5), pp. 
284–292. 
[3] Bini, E., Buttazzo, G.C., Buttazzo, G., 2001. A hyperbolic bound for the rate monotonic 
algorithm. In Proceedings of the 13th IEEE Euromicro Conference on Real-Time Systems, June, 
pp. 59–66. 
[4] Bini, E., Buttazzo, G.C., 2002. The space of rate monotonic schedulability. In Proceedings of 
the 23rd IEEE Symposium on Real-Time Systems, December, pp. 169–178. 
[5] Bril, R.J., Verhaegh, W.F.J., Pol, E.J.D., 2003. Initial values for on-line response time 
calculations. In Proceedings of the 15th IEEE Euromicro Conference on Real-Time Systems, July, 
pp. 13–22. 
[6] Han, C.C., Tyan, H.Y., 1997. A better polynomial-time schedulability test for real-time fixed 
priority scheduling algorithms. In Proceedings of the 18th IEEE Symposium on Real-Time 
Systems, pp. 36–45. 
[7] Using the RDTSC Instruction for Performance Monitoring. 
http://developer.intel.com/drg/pentiumII/appnotes/RDTSCPM1.htm 
[8] Lehoczky, J.P., Sha, L., Ding, Y., 1989. The rate monotonic scheduling algorithms: exact 
characterization and average behavior. In Proceedings of the 10th IEEE Symposium on 
Real-Time Systems, December, pp. 166–171. 
[9] Liu, C.L., Layland, J.W., 1973. Scheduling algorithms for multiprogramming in a 
hard-real-time environment. Journal of the ACM 20 (1), pp. 46–61. 
計畫成果自評 
本計劃的目的在於改進著名精確可排程分析 (Audsley’s Algorithm，AA) 的效能，藉由
在分析過程中持續略過不必要的分析，使得完成分析的時間大幅縮短。其基本想法是每次
在推導 r(l+1)時，恰當地將任務群分成兩組子任務群，並以不同方式來處理這兩組子任務群，
盡可能跳得遠一些，使得原本 AA 所需的分析循環次數得以減少。我們所提出演算法的時
間複雜度為 O(n)。由實驗結果可以得知，我們所提出的 FEA 大幅減少了精確可排程分析所
需的分析循環次數，其減少的比例隨著任務群對處理器的總使用率增加而提高。當任務群
出席國際學術會議心得報告 
                                                             
計畫編號 96-2218-E-011-012- 
計畫名稱 固定優先權排程演算法之高效率精確可排程分析 
出國人員姓名 
服務機關及職稱 
謝仁偉 國立嘉義大學資訊工程系 助理教授 
(現為國立台灣科技大學資訊工程系 助理教授) 
會議時間地點 
August 27-29, 2007 
Portland, Oregon, USA 
會議名稱 The 12
th ACM/IEEE International Symposium on Low Power Electronics and 
Design 
發表論文題目 Energy-Efficient and Performance-Enhanced Disks Using Flash-Memory Cache
 
一、參加會議經過 
第 12屆 ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED’07) 
於 8月 27日至 8月 29日，在美國奧勒崗州的波特蘭舉行；這次的國際學術會議總共有來自
14個國家 192篇的論文投稿，regular及 short papers僅有 57篇被接受 (30%的接受率)，若再
包含 17 篇 posters，則接受率為 39%。這 57 篇論文分別安排在三天共 12 個場次，來進行發
表。而所有被接受的論文，也都會刊登在會議所發行的 proceeding上。 
會議第一天，大會安排了 Robert Chau (Senior Fellow, Intel Corporation) 演講，講題為
『Nanotechnology for Low-Power and High-Speed Nanoelectronics Applications』；下午的 invited 
special session 則邀請了 Shekhar Borkar (Fellow, Intel Corporation) 及 Bill Dally (Stanford 
University) 來談 on-chip interconnection architectures的未來發展；會議第二天中午，大會邀請
了 UC Berkeley的 David Patterson教授來進行演講，講題為『The Parallel Computing Landscape: 
A Berkeley View』；會議最後一天則是 Luiz Barroso (Distinguished Engineer, Google) 來談伺服
器上的 power consumption issues。 
 在這一次的國際學術會議中，regular paper的報告者有 30分鐘的時間來進行英文的口頭
報告，而 short paper的報告時間則為 20分鐘，這也包含了提供給在場與會的學者專家提問相
關的問題的時間。申請者所投稿之論文『Energy-Efficient and Performance-Enhanced Disks 
Using Flash-Memory Cache』，是被安排於會議的第三天 (8/29) 的第一個場次 (Session 12，
AM8:00~AM10:00) 報告投稿之論文。申請者順利於時間內完成論文的口頭報告，並且與在場
的學者專家討論相關研究問題。而申請者所報告的論文則是被編排於 Proceedings of the 2007 
International Symposium on Low Power Electronics and Design中的第 334頁至第 339頁。 
二、與會心得 
參加這次國際會議最大的收穫，除了瞭解到國際上關於省電方面最新的研究與技術之外，就
是能夠與大師級的學者專家 (例如 David Patterson) 近距離接觸。而大會所安排的多場精彩的
演講亦讓申請者受益非淺。在會議過程中，申請者對台大楊家玲教授所發表的『Post-Placement 
Leakage Optimization for Partially Dynamically Reconfigurable FPGAs』甚感興趣，亦與楊教授
討論進一步研究發展之可能性。會後，申請者亦利用此次難得的機會，與在場來自世界各國
Energy-Efficient and Performance-Enhanced Disks Using
Flash-Memory Cache∗
Jen-Wei Hsieh
Department of Computer
Science and Information
Engineering
National Chiayi University,
Chiayi, Taiwan 60004, ROC
jenwei@mail.ncyu.edu.tw
Tei-Wei Kuo
Department of Computer
Science and Information
Engineering
Graduate Institute of
Networking and Multimedia
National Taiwan University,
Taipei, Taiwan 106, ROC
ktw@csie.ntu.edu.tw
Po-Liang Wu
Department of Computer
Science and Information
Engineering
National Taiwan University,
Taipei, Taiwan 106, ROC
b91029@csie.ntu.edu.tw
Yu-Chung Huang
Genesys Logic, Inc. Taipei,
Taiwan 231, R.O.C.
YC.Huang@genesyslogic.com.tw
ABSTRACT
This work explores the unique characteristics of flash memory
in serving as a cache layer for disks. The experiments show
that the proposed management scheme could save up to 20%
energy consumption while reduce the read response time by
the two third and the write response time by the five sixth
of their counterparts. The estimated lifetime of the flash-
memory cache is significantly improved as well.
Categories and Subject Descriptors
C.0 [Computer Systems Organization]: General; B.3.2
[Memory Structure]: Design Styles—Cache memory
General Terms
Management
Keywords
Flash memory, cache, energy efficient, performance
1. INTRODUCTION
Flash memory recently gains a lot of attention in serving
as a storage-system alternative (e.g., [1, 3, 4, 8]) or as caches
for hard disks. In particular, Windows ReadyBoost [6] lets
users use a removable flash memory device to improve system
∗Supported in part by research grants from Taiwan, ROC
National Science Council under Grants NSC95-2219-E-002-
014 and NSC 95R0062-AE00-07.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ISLPED’07, August 27–29, 2007, Portland, Oregon, USA.
Copyright 2007 ACM 978-1-59593-709-4/07/0008 ...$5.00.
performance, while ReadyDrive [6] enables Windows Vista
PCs equipped with a hybrid hard disk (a new type of disk
with integrated non-volatile flash memory) to boot up faster,
resume from hibernate in less time, preserve battery power,
and improve disk reliability.
However, flash memory does have several unique character-
istics that introduce challenges to the management issues. A
NAND flash memory is organized in terms of blocks, where
each block is of a fixed number of pages. Data must be writ-
ten to the free space of flash memory. When a flash memory
page is written, the space is no longer available unless it is
erased. As a result, out-place-update is usually adopted in
the management. A block is the basic unit for erase opera-
tions, while reads and writes are processed in terms of pages.1
The typical block size and the page size of a NAND flash
memory are 16KB and 512B, respectively.2 After the pro-
cessing of a large number of page writes, the number of free
pages on flash memory would be low. Garbage collection are
needed to reclaim invalid pages scattered over blocks (due to
out-place update) so that they could become free pages. A
flash-memory block has a limitation on erasures, block erased
over 106 times might suffer from frequent write errors. “Wear-
levelling” is usually adopted to erase blocks evenly so that a
longer overall lifetime is achieved.
One of the most pioneering work in adopting flash memory
as a disk cache is done by Marsh et al. [5]. Due to the state of
the art at that time, the study had 20MB NOR flash memory
as cache for a 40MB hard disk, which implies that an efficient
lookup mechanism to locate the cache space of given Logical
Block Addresses (LBA’s) for large capacity flash memory was
not considered. Another issue of adopting flash memory as a
disk cache is its robustness, since flash memory suffers from
worn-out effect. Different from the past work, this work is
motivated by the needs of management in caching data for
1Note that terms “page” and “block” used here are different
from those used for disks. As can be seen, the term “page”
refers to a unit that is smaller than the unit referred to by
the term“block.”
2Some flash memory adopts 128KB blocks and 2KB pages.
When a write request arrives, it is checked to see if its LBA
exists in any corresponding caching buffer. The correspond-
ing caching buffer of the LBA is then derived by searching
over associated buffers. If no such a caching buffer exists,
a new caching buffer is allocated and attached to the corre-
sponding entry of the entry table. We always try to cache the
data in the primary block first. If the corresponding page of
the primary block is occupied by old-version data of the LBA,
the page will be invalidated. We then try to cache the data
in the first available page of the overflow block. An overflow
block is allocated for the caching buffer if it does not exist.
If there exists an overflow block, it must be checked to see
if any free page is available. A garbage collection is invoked
to reclaim invalid pages of the primary and overflow blocks
if there is no free page left in the overflow block. When the
data is cached in the overflow block, any page for the old ver-
sion of the data is invalidated. After garbage collection, the
data would be written to the primary and overflow blocks,
as described above. Note that the corresponding page of the
data in the primary block might still be occupied because of
a hash collision. In other words, an overflow block might still
be needed. Finally, the data are cached in the overflow block.
2.3 Garbage Collection and Data Replacement
2.3.1 Garbage Collection
When there is no free page in an overflow block, garbage
collection should start to recycle pages occupied by invalid
pages of the overflow block and its corresponding primary
block. If the disk is not spinning down or idle during the
garbage collection, data in the blocks that correspond to write
requests should be written back to the disk. The strategy of
the proposed garbage collection is based on two major ideas:
(1) If the disk is spinning down or idle, the system should
avoid writing data cached in the blocks back to the disk when-
ever possible. (2) When valid pages of the two blocks are
written back to the caching buffer (with new primary and
overflow blocks), they are written back in an LRU fashion.
That is, valid pages in the overflow block are written back to
the buffer earlier than those in the primary block, and valid
pages in the overflow block are written back to the buffer
from the bottom to the top of the overflow block.
A new primary block is allocated and associated with the
caching buffer, and an overflow block is not allocated until
necessary. If the disk is spinning down, then all of the data
that correspond to writes must be kept in the cache when-
ever possible. Otherwise, the data should be written to the
corresponding disks, and the rest valid pages of the previous
primary and overflow blocks (correspond to reads) are written
back to the new primary and overflow blocks of the caching
buffer in an LRU fashion. The previous primary and overflow
blocks are then inserted into a queue to erase.
2.3.2 Replacement Strategy
The entry table of caching buffers, that changes over time,
is used to do bookkeeping for data in the cache. Whenever
there exists any problem in allocating a new block, we must
execute a replacement strategy to recycle one or more caching
buffers and their associated blocks. The basic idea is to pick
up the LRU caching buffer for replacement to avoid any cache
miss! Blocks of the flash memory are considered as a circular
array, and a free pointer always points to a free block, as
shown in Figure 2.(a). Whenever a free block is needed, the
free block pointed by the pointer is returned, and the pointer
moves to the next free block one-by-one along the circular
array. Examples in the allocation of free blocks are as shown
in Figure 2.(b).
Entry Table
Caching Buffer
Caching Buffer
: Allocated
: Free
: In Use
Flash Memory Blocks
Pointer of
Free Blocks
Allocation
(Find the First Free Block)
Entry Table
Caching Buffer
Caching Buffer
Caching Buffer
Caching Buffer
Caching Buffer
1
2
3
4 5
: Order of Allocation Requests1 5~
Flash Memory Blocks
1 2 3 4 5
Pointer of
Free Blocks
(a) Before Allocation. (b) After Allocation.
Figure 2: Allocations of Free Blocks.
To speed up the seeking of any free block and to help in
the locating of the LRU caching buffer, an access map, that
is an array of bits, is introduced to keep the access record.
Each bit in the access map corresponds to a unique block
in the circular array. When any block of a caching buffer is
accessed, the corresponding bit is set to 1. A replacement
pointer that initially equals to the free pointer moves along
the circular queue whenever there is any need to locating
an LRU caching buffer or to recycle used blocks. When the
replacement pointer moves, it stops at the bit with value 0.
The caching buffer corresponding to the block is considered
as the LRU buffer and recycled. If the replacement pointer
moves on a bit with value 1, the bit is set as 0, and the pointer
moves to the next bit, as shown in Figure 3.
Entry Table
Accessed
Caching Buffer
Caching Buffer
Accessed
Accessed
A
E F
1
2
D
B C
Flash Memory Blocks
1 0 0 1 1 0 0 1 0 0 0
: Order of Allocation Requests1 2~
Access Map
A E B D C F1 2
Replacement Pointer
Entry Table
Caching Buffer
Caching Buffer
A
E F
1
2
D
B C
Flash Memory Blocks
1 0 0 0 0 0 0 1 0 0 0
: Order of Allocation Requests1 2~
Access Map
A E B D C F1 2
Replacement Pointer
Replaced!
Caching Buffer
Caching Buffer
Caching Buffer
Corresponding Caching
Buffer Replaced
(a) Before Replacement. (b) After Replacement.
Figure 3: The Access Map and Replacement.
2.4 Rebuilding Procedure of the Entry Table
When a computer shut down normally, there exists many
strategies in accelerating the rebuilding of the entry table.
This section illustrates a simple procedure in rebuilding the
entry table by scanning blocks on the flash memory without
any auxiliary information when the system crashed.
0
500,000
1,000,000
1,500,000
2,000,000
2,500,000
3,000,000
3,500,000
512MB
K=1
1024MB
K=1
2048MB
K=1
4096MB
K=1
1024MB
K=2
1024MB
K=4
1024MB
K=8
1024MB
Direct
Mapped
1024MB
Set
Associative
Implementation Strategies
Sa
ve
d E
ne
rgy

 
(Jo
u
le)
7.9%
8.97%
14.65%
19.94%
10.84%
12.34%
15.7%
5.54%
8.42%
Figure 5: Comparison of Energy Efficiencies.
efficiency it can achieve. As shown in Figure 5, we could
save about 20% energy consumption while adopting a 4GB
flash-memory cache for a 80GB disk.
3.2.3 The Number of Block Erasures
Since flash memory has a limitation on the block-erasure
count, the distribution of erase counts over flash-memory
blocks was definitely a major evaluation metric. The number
of erasures over each flash-memory block is separately ac-
cumulated. According to erase counts, flash-memory blocks
were sorted into groups. The number of groups and the cov-
ered range of erase counts for each group implied the quality
of achieved wear-levelling, from which the life cycle of a flash-
memory cache can be estimated.
A large cache size improved not only the idle time but
also the quality of wear-levelling. Erasures over flash-memory
blocks were amortized when the cache size became large. Dif-
ferent from idle times, the impact of the cache size on the
distribution of erase counts was more predictable. When the
size of flash-memory cache was double, the peak in the dis-
tribution roughly grew into double, and the range of erase
counts roughly became half. On the other hand, although
a large stretch factor was beneficial to idle times, it greatly
deteriorated the quality of wear levelling. When K became
larger, the range of erase counts over flash-memory blocks ex-
panded. In addition, total erasures over flash-memory blocks
boosted as well. Table 2 lists total erasures for a 23-day trace
in the experiment. Since both a direct mapped cache and a set
associative cache do not take out-place-update nature of flash
memory into consideration, deviations of erase counts among
flash-memory blocks were large. In addition, their suffered
erasure overheads were also enormous, as shown in Table 2.
Note that when K was set large in the proposed strategy,
the performance gap on idle times can even widen while the
erasure overhead was still superior to a direct mapped cache
or a set associative cache.
Total Erasure
512MB, K = 1 15,662,090
1024MB, K = 1 14,995,330
2048MB, K = 1 14,265,300
4096MB, K = 1 14,317,930
1024MB, K = 2 73,866,860
1024MB, K = 4 272,317,700
1024MB, K = 8 406,628,000
1024MB, Direct Mapped 418,747,600
1024MB, Set Associative 296,467,000
Table 2: Comparison of Total Erasures.
In the simulation over the 23-day trace, the maximum erase
counts among all flash-memory blocks for various implemen-
tation strategies are listed in Table 3. Based on these infor-
mation, the life cycle of the flash-memory cache under differ-
ent implementation strategies could be estimated. The flash-
memory cache under the proposed implementation strategy
(for cache size = 1024MB and K = 1) could last over 203
years, while a direct mapped cache could only work for 2.4
months and a set associative cache did not function well af-
ter 7 months.
Maximum Erasure Estimated Product
Counts (23-day) Lifetime
512MB, K = 1 570 110.6 years
1024MB, K = 1 310 203.3 years
2048MB, K = 1 180 350 years
4096MB, K = 1 150 420 years
1024MB, K = 2 2,780 22.67 years
1024MB, K = 4 26,600 28.4 months
1024MB, K = 8 59,000 12.8 months
1024MB, Direct Mapped 314,500 2.4 months
1024MB, Set Associative 121,000 6.25 months
Table 3: The Estimated Product Lifetime.
3.2.4 The Read/Write Response Time
Since flash memory is a kind of EEPROM, the flash-memory
cache has intrinsic limitation in improving the performance
of data accessing. In addition to the penalty of disk access
during a cache miss, the flash memory cache suffered from
the erasure overhead when the utilization of the cache space
was high. Without a proper space management, read/write
requests could suffer from a series of page reads, page writes,
block erasures, and even disk accesses. In the proposed im-
plementation strategy, the garbage collection was properly
designed such that block erasures could be postponed un-
til a system idle time. To illustrate the read/write perfor-
mance, the simulation adopts the access parameters of Sam-
sung K9F6408U0A 8MB NAND Flash Memory and Western
Digital Caviar WD800JB 80GB 7200RPM 8MB IDE Ultra ATA-
100 Hard Drive. Their performance characteristics are listed
in Table 4.
Read Write Erase
K9F6408U0A 36.55µs 226.65µs 2ms
Caviar WD800JB 13.1ms 13.1ms N/A
Table 4: Performance Characteristics.
Figure 6 (a) and (b) compares average read/write response
times among different cache sizes in terms of a day. As the
cache size got larger, a better read/write response time can
be achieved. Figure 6 (c) and (d) shows impacts of the stretch
factor over the average read/write response time. When the
stretch factor became larger, the average read/write response
time deteriorated quickly. As shown in the figure, when K =
8, the average write response time even got worse than the
disk without any flash-memory cache. This was because a
great deal of erase operations were introduced.
Figure 6 (e) compares average read response times among
different implementation strategies in terms of a day. As
shown in the figure, the proposed strategy could save up to
two third of the read response time and save one third of the
read response time in average. On the other hand, a direct
mapped cache did not improve the read response time in most
cases, while the average read performance of a set associative
