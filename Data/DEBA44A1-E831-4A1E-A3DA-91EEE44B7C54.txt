 
 
template matching, binary image, vertical 
projection, and artificial teaching etc., to 
increase the recognition accuracy. In the 
recognition of car license plate characters, the 
paper [14] proposed a template matching 
method to achieve the goal in which a neural 
network based intelligent algorithm is 
proposed. The authors are satisfied with the 
performance of recognition accuracy and 
computation speed. Moreover, Messelodi and 
Modena [15] first used vertical and horizontal 
projection histogram to detect the region of 
character, and then find the characteristics of 
each character. Finally, template matching and 
probability classification are used to recognize 
the characters. 
  
II. System structure 
There are two parts in the system structure, 
one is the computer system and the other is the 
robot system. The computer system deals with 
the picture capture, image processing and 
decision making (see the left of Fig.1). The 
robot system includes the wheeled robot, a 
robot arm, the micro-controller, motors and 
sensors (see the right of Fig. 1).  
 
 
Fig. 1. The system structure. 
 
In the computer system, there is a 
micro-camera set on the tip of the robot arm, in 
which the image is captured. Those images are 
the environment around elevator which may 
contain color pattern around the buttons, the 
door of elevator, and elevator’s button, etc. 
The micro-camera is with the size 1.4cm x 
1.4cm x 1.8cm and the weight 8g (see Fig.2). 
The control center is a notebook computer 
which takes charge the images processing and 
pattern recognition, decision making and 
motion control and computation of the arm. 
The notebook computer is of the type Lenovo 
X200 RY9, with a 2.4GHz CPU and 2GB 
RAM. Its weight is only 1.5 kg.  
 
 
Fig. 2. Micro-camera. 
 
In the robot system, there are a 
micro-controller (BASIC Stamp 2p40 module), 
a touch sensor, a power supply and a 
transformation interface. The main tasks of the 
BS2p40 are to control the rotation degrees of 
the robot arm, give the motion command to the 
wheeled robot’s wheels, and communicate 
with notebook computer. BS2p40 has 40 I/O 
pins and is shown in Fig. 3. 
 
 
Fig. 3. The micro-controller BS2p40. 
 
The touch sensor (see Fig. 4) is set on the tip 
of the robot arm too. When the arm extends 
and pushes the button of the elevator, the 
sensor can sense the touch and respond the 
command to BS2p40 to shrink the arm. If the 
sensor senses nothing, the arm will not shrink 
back for a while. 
The power supply is a set of series four 
lithium cells which is 1.48V/2.6Ah and 
supplies the power to the micro-controller, 
 2
 
 
the desired floor. 
 
 
Fig. 8. The color patterns around the buttons 
 
Since the RGB color mode is easily affected 
by the light change in the environment, the 
transformation of (1) from RGB to YCbCr 
models is needed. The reason is that YCbCr 
model is more robust to the light change in the 
around environment than RGB model.  
 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
+
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−−
−−=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
128
128
16
),(
),(
),(
0706.03647.04392.0
4392.02902.01451.0
098.05059.02549.0
),(
),(
),(
yxB
yxG
yxR
yxC
yxC
yxY
r
b
 (1) 
 
Then based on a serial image processes, 
opening operation, erosion and dilation, the 
green and blue color patterns are detected to 
locate the positions of buttons. Furthermore, 
let us set three thresholds to determine whether 
the light behind the button is on or off. When  
 
      , ,  2550 ≤≤ Y(x,y) 137122 ≤≤ (x,y)Cb
and   , 150135 ≤≤ Y(x,y)
 
then the arm deems that the light is on; 
otherwise, the light is off.  
Next, the buttons of the elevator should be 
recognized. Those buttons include up and 
down, floor numbers 1, 2, 3…. The number 
pattern recognition is performed by the 
following procedure.  
 
Step 1. Transforming the captured color 
image to gray level image using the 
transformation 
 
B0.114G0.587R0.299G g ×+×+×= ,  
where denotes the gray pixel value. gG
Step 2. Using average threshold method (2) 
and (3) to transform the gray image to the 
binary image (see Fig. 9). Let  be the 
area size of the processed image, and   be 
the average gray value. Then we have 
gArea
gA
 
g
h w
g
Area
whG∑∑
= ==
239
0
274
44
g
),(
  A  .        (2) 
 
Based on the average value , the threshold is 
set as 
gA
gg ATh β= , whereβ  is the experience 
value by trial and errors. Then 
 
  ,
otherwise  0,
G  if ,255 g
⎩⎨
⎧ ≥= gAN(x,y)         (3) 
 
   
(a)                     (b) 
Fig. 9. (a) The gray image and (b) the binary 
image 
 
Remark: Instead of using a fixed threshold, 
we used the average threshold to get the binary 
image. The reason is that the fixed threshold 
value will not change corresponding to the 
light variation of the environment or the 
different qualities of the captured image. 
Step 3. Use medium filter and opening 
operation to the binary image such that the 
noise is decreased as few as possible. 
Step 4. Capture the area of each character of 
the button by using connected component 
method and delete the pixels outside the area 
(see Fig. 10(a)). 
 
 
(a) Area of character  4
 
 
 
[1] K. Furuta, K. Kosuge and N. Mukai, 
“Control of Articulated Robot Arm with 
Sensory Feedback: Laser Beam Tracking 
System,” IEEE Transactions on Industrial 
Electronics, vol. 35, no. 1, pp. 31–39, 
February 1988. 
[2] R. B. White, R. K. Read, M. W. Koch and 
R. J. Schilling, “A Graphics Simulator for 
a Rob,” IEEE Transactions on Education, 
vol. 32, no. 4, pp. 417-429, November 
1989. 
  
(a) 
 
[3] Z. Li, T. J. Tarn and A. K. Bejczy, 
“Dynamic Workspace Analysis of 
Multiple Cooperating Robot Arms,” IEEE 
Transactions on Robotics and Automation, 
vol. 7, no. 5, pp. 589-596, October, 1991. 
21 αα θθθ +=A
[4] M. Koga, K. Kosuge, K. Furuta and K. 
Nosaki, “Corrdinated  Motion Control of 
Robot Arms Based on the Virtual Internal 
Model,” IEEE Transactions on Robotics 
and Automation, vol. 8, no. 1, pp. 77-85, 
Feburary 1992. (b) 
 [5] S. R. Munasinghe, M. Nakamura,S. Goto 
and N. Kyura, “Optimum Contouring of 
Industrial Robot Arms Under Assigned 
Velocity and Torque Constraints,” IEEE 
Transactions on Systems, Man, and 
Cybernetics-Part C Applications and 
Reviews, vol. 31, no. 2, pp. 159-167, May 
2001. 
   
[6] C. W. Kennedy and J. P. Desai, 
“Modeling and Control of the Mitsubishi 
PA-10 Robot Arm Harmonic Drive 
System,” IEEE Transactions on 
Mechatronics, vol. 10, no. 3, pp. 263-274, 
June 2005. 
(c) 
    
[7] .O M. && Efe, “Fractional Fuzzy Adaptive 
Sliding-Mode Control of a 2-DOF 
Direct-Drive Robot Arm,” IEEE 
Transactions on System, Man, and 
Cybernetics-Part B: Cybernetics, vol. 38, 
no. 6, pp. 1561-1570, December 2008.  
[8] 劉咏龍（徐國鎧；吳育德教授指導），“適
應性視覺伺服控制之機械手臂應用 ,” 
碩士論文，國立中央大學電機工程研究
所，2003. 
[9] 吳政昌（張文中教授指導），“結合力與
視覺回授之機械手臂姿態控制,” 碩士
(d) 
Fig. 12. Serial motion pictures for pushing the 
button 
 6
