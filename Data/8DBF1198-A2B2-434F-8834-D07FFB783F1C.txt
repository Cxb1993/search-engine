2 
摘要 
彩色濾光片(Color Filter)目前之缺陷檢測雖有自動化機台輔助，但需仰賴人工進行缺陷辨
識，較無效率且易誤判。故本研究導入機器視覺並藉由統計觀點之灰階共生矩陣(Gray-Level 
Co-occurrence Matrix, GLCM) 進行影像處理及缺陷特徵擷取，來解決樣本影像本身之旋轉、
位移、放大倍率…等尺度不一之問題，最後透過倒傳遞類神經網路(Back-Propagation Neural 
Network, BPN)、支持向量機(Support Vector Machine, SVM)等分類(Classification)技術進行缺陷
辨識分類，以期提昇彩色濾光片缺陷檢測效能。經實驗分析，針對十一種 CF 缺陷及良品共
十二類樣本，於四種不同角度下(0°, 45°, 90°, 135°)取 GLCM 各 22 個特徵，以 BPN 辨識，測
試正確率達 90.91%；而以 SVM 辨識，其辨識正確率平均達 96.05%，優於 BPN。故本研究透
過統計觀點之灰階共生矩陣法擷取出缺陷特徵，可順利克服彩色濾光片缺陷樣本尺度不一問
題並有效正確辨識缺陷類型。 
關鍵詞：彩色濾光片、缺陷分類、灰階共生矩陣、倒傳遞類神經網路、支持向量機 
Abstract 
Most of the color filter (CF) manufactures classify the surface defects of the finished goods by 
human inspectors. Such inspection is inefficient and inspectors may be mistaken about defect types. 
To improve the inspection efficiency, the research develops an automatic defect classification 
system for CF. The study utilizes the gray-level co-occurrence matrix (GLCM) to extract the texture 
features of a CF image to overcome the problems of shift variant, scale variant, and illumination 
variant. The co-occurrence matrix is formed using a set of offsets sweeping through 180 degrees (i.e. 
0, 45, 90, and 135 degrees) at the same distance (10 pixels). For one degree, 22 features are 
extracted. The extracted features are imported into the back-propagation neural network (BPN) and 
support vector machine (SVM) to further classify the defect types. For 12 CF defect types, the 
recognition accuracy is 90.91% for the BPN classifier, and 96.05% for the SVM classifier.  Overall 
speaking, the proposed method can achieve good performance for CF defect classification. 
Keywords: Color Filter (CF), Defect Classification, Gray-Level Co-occurrence Matrix 
(GLCM), Back-propagation Neural Network (BPN), Support Vector Machine 
(SVM) 
一、前言 
基於現今液晶顯示器（LCD）面板產業的快速發展，其各生產線上之品質檢驗也相形重
要。在 LCD 製造過程中，成本最高之零組件為彩色濾光片(Color Filter, CF)。目前針對彩色
濾光片微觀缺陷檢測雖有自動化機台可進行缺陷框選擷取及判定，然而在實務上，檢測機台
雖能檢出缺陷位置，但缺陷種類往往無法正確辨識；故最終判定仍於成品階段進行人工全面
檢測，除曠日費時外亦容易造成漏檢或誤判情況發生。此外，就樣本本身其取像之環境、光
源、放大倍率、角度、影像大小…等往往為未知情況下，我們也希望可以在製造過程中藉由
導入機器視覺的自動缺陷檢測技術來取代人工檢測，及時發現缺陷產生，並立即作修補矯正、
重工或停機等，將傷害及損失降到最低，使產品良率相對獲得提昇。 
4 
(Macro-defects)與微觀缺陷(Micro-defects)。Nakashima（1994）針對彩色濾光片之巨觀缺陷與
微觀缺陷進行明確定義並檢測之，其使用影像相減與光學傅立葉濾波器二種方法來偵測缺
陷。吳木杏（2000）亦於彩色濾光片表面缺陷檢測分析中採用上述概念，以影像相減法尋找
表面缺陷。但上述二方法皆有其限制，影像相減技術需考量參考影像之決定與影像定位
（Registration）校準等問題，當參考樣本因選擇不佳或定位不準確時，缺陷偵測誤判機率將
大幅提高。 
對於 TFT-LCD 之微觀缺陷，如微小之粉塵（Particle）、孔洞（Pinhole）、刮痕（Scratch）
及指觸（Fingerprint）…等，曾彥馨（2003）對其四類缺陷利用二維傅立葉轉換分析，將傅立
葉頻譜中規律性紋路之響應予以去除，並以反傅立葉轉換還原影像，最終使用統計管制界限
判定缺陷所在，並以二值化方式突顯之。陳一斌（2001）提出 TFT 彩色濾光片缺陷檢測系
統的研製。針對 TFT 產業所研發的大面積缺陷檢測系統介紹其缺陷檢測系統的功能與相關
技術。其中包含了整體 TFT 彩色濾光片視覺缺陷檢測系統的組成架構、光源設計、視覺影
像及缺陷檢測等相關技術的探討。Lee et al.（2004）等人提出以 Saliency map 為基礎之缺陷
檢測演算法於灰階影像中，利用 Saliency map 三項特徵：顏色、方向、密度等，找出 TFT LCD 
面板中亮點、顆粒及線缺陷等缺陷，更可應用於模糊不清不易分辨的影像資訊中找出缺陷。 
關於 TFT-LCD 之巨觀缺陷，如 MURA、SIMI…等，針對色彩不均之缺陷偵測，Saitoh 
(1999) 提出以遺傳基因演算法 (Genetic  Algorithm) 尋邊之方法，此作者後續再提出以彩色
影像偵測 LCD 亮點缺陷之論文(Saitoh 2001)；劉祥吉 (2002) 則利用機器視覺輔助人工判定，
並運用變異數分析及指數加權平均管制圖來偵測顯示不均之現象；陳志忠（2000）將 TFT-LCD
模組面板劃分為數個區塊並計算各區塊平均亮度，再經由各區塊平均亮度中，比較出最高及
最低亮度值，透過該比例，進而判斷此模組是否具有亮度不均勻之缺陷；除了亮度均一性與
畫素缺陷之檢測外，作者也發展十字標記定位演算法。錢志豪 (2002) 發展一以機器視覺為
基礎之 TFT-LCD 顯像色彩偏差缺陷之品質檢測系統。Lee and Yoo（2004）針對 TFT-LCD 之
巨觀性缺陷－區域 Mura 進行檢測，以統計廻歸手法將影像和背景估算出來進行目標切割，
之後基於視覺對灰階亮度之突顯效率作辨識是否為缺陷。Zhang and Zhang（2005）建構出一
模糊類神經網路（Fuzzy neural network, FNN）對於 TFT-LCD 的 MURA 進行數量的估算，而
其實驗結果顯示 FNN 可以有助於解決像是 MURA 之高複雜性辨識問題。陳亮嘉等人（2005）
則提出奇異值分解法（Singular Value Decomposition, SVD）對於 TFT-LCD 之 MURA 缺陷進
行影像處理，將影像上的規律紋路予以濾除。Taniguchi et al.（2006）提到為了要檢測 MURA，
就像是在一張均勻的面板表面為了能夠看到清晰之不規則亮度變化，故必須保持設備之品
質。郭家成(2006)利用離散餘弦轉換(DCT)之係數去分析並能重建不具 MURA 之原始背景影
像，利用影像相減分離出 MURA 缺陷。陳與周（2007）利用離散小波轉換(DWT)突顯 MURA
缺陷，可偵測小面積之 MURA。Chen（2008）則利用二維快速傅利葉轉換(FFT)，並設計二
個濾波（2D high-pass and 2D band stop mask）以濾除 LCD 上之規則紋路，以凸顯巨觀缺陷並
偵測之。 
由於彩色濾光片基板中含重複出現的紋路特徵，本研究也針對紋路分析（Texture 
Analysis）之機器視覺技術稍做探討。紋路分析方法可分空間域（Spatial Domain）及頻率域
（Spectral Domain）。空間域方法主要是直接利用感測影像之灰階訊號進行紋路分析，最具代
表為相關矩陣法（Co-occurrence Matrix）（Haralick, et al., 1973），此法乃以原始影像中像素點
（Pixels）在特定鄰近位置之灰階變化之機率密度函數來建立空間灰階相關矩陣，藉此相關矩
6 
相互關係矩陣又稱共生矩陣或灰階共生矩陣，此為基於觀察紋路影像中所有成對像素點
其明亮度值（Gray-Level）共同發生次數。在計算過程中還須考慮兩個重要條件：一為兩個像
素點間距離 d，另一則為兩像素點間角度 θ。更精確地來說，假設有一紋路影像 I，其大小為
N×N。影像中有兩個像素點其座標值為 I (k, l )和 I (m, n)，它們的明亮度值分別為 i 及 j，兩個
像素點間距離為 d，定義 P (i, j | d,θ) 代表影像中之一成對像素點 I (k, l ) 和 I (m, n) 在距離
值為 d、角度為 θ，發生個別明亮度值為 i 與 j 的次數。本研究中我們考慮了四種角度：水平
角度 θ= 0°、右對角度 θ= 45°、垂直角度 θ= 90°、左對角度 θ=135°。未正規化前之灰階明亮度
相互關係矩陣定義如下： 
  
基於明亮度相互關係矩陣，本研究利用 Haralick et al.（1973）提出了 14 個用於紋路分析
的紋路特徵及 Soh and Tsatsoulis（1999）Clausi （2002）所提特徵值共 22 項特徵值搭配不同
角度進行運算擷取之。茲將特徵值計算公式中之符號定義如下： 
 , | ,p i j d  ：正規化的灰階明亮度相互關係矩陣中，明亮度值級數分別為 i 與 j 所對應到的元
素值。  
   
N
djiPdjip  ,|,,|,                  (1) 
 其中 N 為兩明亮度值共同發生的總次數。 
 ipx ：為在正規化的灰階明亮度相互關係矩陣中的第 x 行的所有機率值總和，即 
    


Ng
j
x djipip
1
,|,                   (2) 
 jpy ：為在正規化的灰階明亮度相互關係矩陣中的第 y 列的所有機率值總和，即 
    


gN
i
x djipjp
1
,|,                   (3) 
Ng：為此影像的明亮度值級數；Ng = (最大明亮度值-最小明亮度值) +1。 
   
 
 
Ng
i
Ng
j
yx djipkp
1 1
,|,                  (4) 
 其中 gNk 2,,3,2  ， kji   
   
 
 
g gN
i
N
j
yx djipkp
1 1
,|,                  (5) 
 其中  1,,1,0  gNk  ， kji  ||  
 
以下列舉其中主要紋路特徵值： 
1. Angular Second Moment（二階度動量） 
   
i j
djipF 21 ,|,                  (6) 
8 
五、結果與討論 
本研究於影像前處理至後續缺陷特徵值擷取皆藉由 MATLAB 7.1 編寫所需程式並配合
Super PCNeuron 5.0（SPCN 5.0）之類神經網路軟體及 BSVM2.06 來進行後續 CF 缺陷分類辨
識。以下依研究流程圖中所述之三種實驗驗證方式討論結果。 
1. 對於同一彩色濾光片之缺陷特徵於 4 種角度（0 度、45 度、90 度與 135 度）擷取情況下
135 度訓練樣本正確辨識率最高；其次是 45 度；而經 BW 比率刪減資料維度後卻導致辨識
率降低。 
2. 將合併四種不同角度下特徵值資料置於倒傳遞類神經網路分類辨識判斷，其訓練辨識結果
正確率達 88.30%，測試辨識結果正確率達 90%。可知當樣本於取像環境不定、光源不一、
放大倍率未知且在無定位矯正的情況下仍可以正確辨識其類別。 
3. 將十二類樣本於不同輸入角度下所擷取之 22 項特徵一併匯入分析即共 88 項特徵值為輸
入，由實驗可知以 BPN 分類，正確率在訓練方面達 100%，測試方面正確率達 90.91%。而
進一步刪減維度各保留前 20、前 40、前 60BW 值高之特徵投入倒傳遞進行比較，更可發
現當僅採用前 40 筆 BW 值較高的特徵值時更可獲得優異的分類表現在訓練方面達
99.55%，測試方面正確率達 96.36%，故推論刪減過多維度或保留過多皆不盡理想。 
4. 透過支持向量機中 linear、polynomial、RBF 與 sigmoid 等四種不同核心函數實驗，可得到
linear 核函數擁有較高正確分類能力，其正確率平均為 96.05%，而由不同實驗之測試資料
與訓練 model 交叉實驗可得到 95%以上正確率。 
6. 以分類正確率最高之 SVM 核函數 linear 來看，在不同類別與不同角度下，最高正確率可
達 98.33%，最低為 92.08%正確率，可知當樣本於取像環境不定、光源不一、放大倍率未
知且在無定位矯正的情況下仍可以正確辨識其類別。故，運用此統計觀點之灰階共生矩陣
來擷取缺陷特徵是可行且具可靠性的。 
五、結論 
本研究利用廠商所提供之影像進行 CF 缺陷離線分類辨識，所提出的以灰階共生矩陣擷
取特徵並搭配倒傳遞類神經網路與支持向量機的缺陷分類方式，經實驗驗證可克服樣本取像
光源、角度、放大倍率…等尺度不一問題，而其中 SVM 分類效果又較 BPN 佳。為使研究成
果能實際應用於業界，提供以下數點研究方向： 
1. 目前為離線進行試驗測試，往後可建構即時線上辨識系統。 
2. 對於缺陷類別雖可正確辨識，後續研究可依其缺陷面積與實際缺陷範圍及透過判定規則給
予後續修正回饋。 
3. 面對彩色濾光片製程越趨精密且缺陷類型不斷增加之時，未來面對未知缺陷類型也可嘗試
不同分類方式，如使用無監督式學習網路如自適應共振理論網路(Adaptive Resonance 
Theory, ART)等進行研究探討。 
4. 可增加或修改不同之特徵值，來獲得更優異分類效果並面對未來更精密更複雜之缺陷類
型。 
10 
Soh, L. and Tsatsoulis, C. (1999), Texture Analysis of SAR Sea Ice Imagery Using Gray Level 
Co-Occurrence Matrices, IEEE Transactions on Geoscience and Remote Sensing, Vol. 37, No. 
2, pp. 780-795. 
Taniguchi, K., Ueta, K., and Tatsumi, S. (2006), A mura detection method, Pattern Recognition, 39, 
1044-1052. 
Zhang, Y., and Zhang, J. (2005), A fuzzy neural network approach for quantitative evaluation of 
Mura in TFT-LCD, Neural Networks and Brain, 2005 ICNN&B’05. International Conference, 
1, 424-427. 
[論文發表情形] 
1. 江育民，林冠良，2009，“應用灰階共生矩陣於彩色濾光片瑕疵檢測之研究”， 第十四屆人
工智慧與應用研討會，台中。  
2. 江育民，林冠良，林曜璋，2009，“彩色濾光片微觀疵檢測之研究”，第九屆全國 AOI 論壇、
展覽與競賽(2009 Taiwan AOI Forum, Show and Contest)，台北。 
3. 江育民，林冠良，2009，“應用灰階共生矩陣於彩色濾光片瑕疵檢測”，台灣作業研究學會
2009 年學術研討會暨年會。 
4. 江育民，林冠良，翁若棉，2009，“應用灰階共生矩陣與支持向量機於彩色濾光片瑕疵檢
測之研究”，中國工業工程學會 98 年度年會暨學術研討會，台中。 
5. 江育民，翁若棉，2009，“太陽能電池外觀瑕疵檢測”，中國工業工程學會 98 年度年會暨學
術研討會，台中。 
6. Yu-Min Chiang and Huei-Min Chiang, 2009, “Customer segmentation and marketing decision 
using data mining techniques”, Management International Conference, Sousse, Tunisia, 
November 25-28. 
7. Huei-Min Chiang, Tai-Yue Wang, and Yu-Min Chiang, 2009, “Solving the multi-label text 
categorization problems—application of radial basis function neural network”, Management 
International Conference, Sousse, Tunisia, November 25-28. (Nominees for Best Paper 
Awards)  
8. Yu-Min Chiang* and Yao-Chang Lin, “Using Gray-Level Co-Occurrence Matrix and for Defect 
Classification of Color Filter”, submitted to The Second POMS-HK International Conference, 
January 6~7, 2011, HongKong. 
9. Yu-Min Chiang* and Kuan-Liang Lin, “Using Gray-Level Co-Occurrence Matrix and 
Back-Propagation Network for Defect Classification of Color Filter” (working paper). 
10. Yu-Min Chiang* and Yao-Chang Lin, “Developing an Automatic Defect Classification System 
for LCD Color Filters” (working paper). 
 
後，首先由突尼西亞 National Center for Computing (CNI)單位的負責人 Tahar Hfaiedh博士發表
專題演講，講題為“Diversity as a source of creativity and innovation”， Hfaiedh博士提到了目前
多樣化的Web2.0的網路環境、部落格、社會網路、…等工具對個人之知識管理以及創造力的
影響，連帶帶動了組織的創新，管理者除應維持一個適合員工發揮創新的工作文化外，也應
強化領導技能，在教育方面也應加強創造創新的能力培養。緊接著由斯洛維尼亞 Ljubljana大
學的 Žiga Turk 教授發表專題演講(參閱圖一)，講題為“The Case for a Creative Europe”，他以
ABCD四個問題來描述西方所面臨的問題與挑戰，A為 abundance，即農業、工業及資訊產品
或服務的豐富度；B為 BRICS, globalization，金磚四國的崛起及全球化的影響；C為 climate 
change，氣候變遷及能源的影響；D為 demography，人口成長與 GDP成長間的關聯性。Turk 
教授認為將來產品設計除考慮功能性(Function)外，更應考慮其意義(meaning)，即創造力及藝
術方面的表現；另外他也提到了世界是平(flat)的概念，因此無論在教育或政策方面均應有所
對應措施，強調 empowerment(活化、賦權)，培育大量優質人力。在短暫的休息後，隨後進行
論文發表，同一時段分別有四至五個場次同時進行，每個場次時間為一個小時，各安排四至
五篇論文發表，所有的發表場次於 11月 27日下午三點結束。大會在議程結束後隔日並安排
了一 EI Gem的參觀活動。 
筆者的發表場次為11月27日下午的F5(14:00~15:00)場次，場次主題為研究方法(Research 
Methods II)，由同樣來自台灣的成功大學王泰裕教授擔任主持人。筆者在此次會議中上台發
表 的 論 文 題 目 為 “Customer segmentation and marketing decision using data mining 
techniques”(參閱圖二)，該文主要探討資料探勘技術於顧客分群及行銷決策之應用，論文中乃
使用台灣南部一大型零售業者之顧客交易資料庫，模式一(顧客分群)中乃先萃取出每個顧客
的 R(Recency)、F(Frequency)及 M(Monetary)三項指標後，利用自組織映射圖(Self-Organizing 
Map, SOM)類神經網路來將顧客分群，由於 SOM網路所區隔出的群數過高(16群)，不適合企
業發展相對應的行銷策略，故本研究再利用 RFM價值矩陣將顧客分群合併成四個區隔：現在
關係密切的顧客、過去關係密切的顧客、現在關係疏離的顧客、以及過去關係疏離的顧客；
模式二(顧客價值預測)中則結合倒傳遞網路(BP)、支持向量機(SVM)、決策樹(Decision tree)
之多重分類器模式，以人口統計變數和地理變數為輸入值，來預測顧客價值的高低，以供企
業擬定行銷策略參考依據。在報告後有不少與會之歐洲當地的學者專家們針對分析物品個
 
圖二  MIC2009會議筆者發表概況 
 
突尼西亞為北非的回教國家，為對此國家風俗地理民情有進一步的認識，筆者與其他同
行的台灣老師們亦安排了一參訪之旅，對突尼西亞的沙漠景觀及原住民族之特殊洞穴屋有進
一步的體驗與認識。突國人民非常友善，治安良好，境內有多處世界文化遺產，是非常具有
人文地理特色的國家。 
 
二、與會心得 
1. 本次學術會議中參與學者背景多來自管理學院，參與此次會議，藉由與國內外學者的交談
以及論文發表中，令筆者更為了解管理意涵及範疇，也啟發了不少可行的創新研究方向，
甚至是與外校學者跨院系的整合研究等，令筆者受益匪淺。 
2. 此次會議由於安排了一主編會議，令與會學者有當面與主編溝通之機會，相信對所有與會
學者們未來論文發表有所助益。 
3. 此次會議台灣學者參加者僅有六位，但其中清大侯建良教授與其博士班學生吳友仁先生的
論文“An Integrated Model for Employee Performance Estimation and Trend Analysis of 
Distribution Centers”獲選為最佳論文，而筆者與成大王泰裕教授、南榮江蕙民副教授所共
同發表的論文“Solving the Multi-Label Text Categorization Problems-Application of Radial 
Basis Function Neural Network”也入選最佳論文獎，相信令來自其他各國的學者留下深刻印
Creativity, Innovation and Management
Proceedings of the 10th International Conference
Sousse, Tunisia, 25–28 November2009
CUSTOMER SEGMENTATION AND MARKETING DECISION USING 
DATA MINING TECHNIQUES 
 
Yu-Min Chiang, I-Shou University, Taiwan 
ymchiang@isu.edu.tw  
 
Huei-Min Chiang, Nan Jeon Institute of Technology, Taiwan 
hueimin@mail.njtc.edu.tw 
 
 
ABSTRACT 
 
Nowadays, the issues of customer relationship management (CRM) have attracted many 
concerns and business operation model has gradually turned from product-focused to 
customer-centric. The capability to collect customer data has been expanded enormously and 
provides enterprises huge amount of data. The interesting knowledge or the high-valued 
information about the customer can be extracted by data mining. By following the market 
segmentation strategy, an enterprise could increase the expected profits. However, the 
customer’s basic data including some demographic variables and geographic variables are 
easier to obtain than the behavior data of customers. The customer value may be predicted 
through the customer’s basic data. Followed by taking marketing activities to those 
customers with high value, the enterprises could avoid unnecessary marketing cost. As a 
result, the study first utilizes the self-organization map (SOM) neural network with the RFM 
model and customer relation matrix to segment customers, and applies Taguchi method to 
improve the quality of clustering. Secondly, the research constructs a marketing decision 
model which utilized the demographic and geographic variables as input of three individual 
classifiers - BP network, support vector machine (SVM), and decision tree - to predict a new 
customer’s value. Empirical study shows that the proposed clustering method can fast group 
customers with good clustering quality, and the addressed customer value prediction method 
can provide the decision support for enterprise’s marketing strategy. 
 
Keywords:  Customer segmentation, customer value, data mining, neural networks, RFM 
model 
 
1201
The purposes of the research are to propose an effective and systematic customer 
segmentation approach, and to test the feasibility of using the customer’s basic data including 
demographic variables and geographic variables in marketing decision. To that purpose, the 
customer data including the basic data and the purchasing transaction data of a warehouse in 
southern Taiwan was used for performance evaluation. The study first utilizes the self-
organization map (SOM) neural network with the RFM model and customer relation matrix 
to segment customers. Secondly, the research constructs a marketing decision model which 
utilizes the demographic and geographic variables as input of three individual classifiers – 
back-propagation (BP) network, support vector machine (SVM), and decision tree - to predict 
a new customer’s value. In order to improve the accuracy of prediction, this study combines 
three classifiers to predict new customer’s value. 
 
This paper is organized as follows: first, the issues of data mining techniques, customer 
segmentation, and combination of multiple classifiers are presented. The following section is 
devoted to describe the analyses methodology. In the next section, the performance of 
proposed customer segmentation and customer value prediction methods is reported. The 
final section summarizes the finding of the research and outlines some suggestions for future 
research. 
 
LITERATURE REVIEW 
 
Data Mining 
 
Data mining is the exploration and analysis of large quantities of data in order to discover 
meaningful patterns and rules [1]. Given the enormous size of databases, DM is the 
technology for knowledge discovery in databases. DM is an interdisciplinary field that 
combines statistics, database management, computer science, artificial intelligence, machine 
learning, and mathematical algorithms. This technology provides different methodologies for 
decision-making, problem solving, analysis, diagnosis, integration, learning, and innovation 
[15]. Berry and Linoff [1] defined six common DM tasks: classification, estimation, 
prediction, affinity grouping or market basket analysis, clustering, and profiling. A popular 
application of data mining with CRM is customer segmentation. The purpose of segmentation 
is to identify behavioral segments and to tailor products, services, and marketing messages to 
each segment [8, 21]. Data mining should be embedded in a business CRM strategy that 
spells out the actions to be taken as a result of what is learned through DM. 
 
 
1203
combination links single classifier in a sequence. The output of a classifier is passed to the 
classifier in the next position in the sequence. While parallel combination approach considers 
all the output of the classifiers and integrates them by a combination algorithm.  
 
Previous methods for parallel classifiers combination include majority vote, naïve Bayes, 
behavior-knowledge space method, Borda count, and neural network. Schiele [19] deemed 
that combining only few classifiers can obtain good performance, and combined the suitable 
number of classifiers can increase the robustness of classifier. He also mentioned combining 
complementary classifier can raise the robustness and the accuracy thus the total classifier 
can be attained. Xu et al. [23] used Bayesian formalism, voting principle, and Dempster-
Shafer belief theory to combine different classifiers. Wang et al. [22] proposed a Kohonen 
self-organizing neural network to integrate the results of 5 other neural network classifiers. 
They used radar recognition problem as demonstration. The proposed combination method is 
insensitive to classifier correlation. Sboner et al. [18] combined linear discriminant algorithm, 
k-NN and decision tree by majority votes to classify 152 skin images. To summarize, it is 
practicable to combine simple classifier rather than design a single complex classifier.  
 
METHODOLOGY 
 
This research intends to apply data mining techniques to segment customers based the 
transaction data, and to construct a multiple classifier system to predict the value of a new 
customer by utilizing the demographic and geographic variables of customer’s basic data. 
The architecture of the research is outlined in Figure 1. The following subsections will 
introduce the approaches adopted in the study. 
 
1205
SOM Clustering 
 
The study utilizes the SOM network to segment customers first and applies the Taguchi 
method to optimize the network parameters. The input layer of SOM has three nodes that 
represent the behavior variables of customers. They are recency, frequency, and monetary, 
respectively. Each node in output layer represents the customer segmentation, and the 
corresponding weights mean the coordinates of the center point of the segmentation. The 
SOM training algorithm is summarized to the following steps. 
 
Step 1: Initialize weights to small random values; set the neighborhood to be large, and set a 
learning rate. 
Step 2: Stimulate the network with a given input vector that consists of RFM values. 
Step 3: Calculate the Euclidean distance between the input vector and the weight of each 
output node and select the output node with minimum distance to be a winning node. 
Step 4: Update weights for the winning node and the nodes within its neighborhood. 
Step 5: Repeat from Step 2 to Step 4 until the stopping criterion is met. In the research, the 
stopping criterion is the iteration numbers. 
 
The parameters of the SOM network such as the learning rate and the topology function will 
greatly influence the clustering performance. To find a suitable parameter setting, the 
research further adopts the Taguchi parameter design method to get an optimal parameter 
combination in a few experiments.  
 
Customer Relationship Matrix Analysis 
 
The paper constructs a customer relationship matrix based on the concept of the customer 
value matrix proposed by Marcus [18] and Chiang and Wei [3] to analyze the SOM 
clustering result. The average number of purchases (F) and the average time period (R) since 
the last purchase for the whole customer base can be used to further segment customers into 
four groups (see Figure 2). By using the customer relationship matrix, we can segment 
customers into suitable categories. 
 
1207
study, the input variables are the customer’s basic data. Before inputting them to the network, 
the variables should be normalized. The next layer is called hidden layer because it is 
connected neither to the inputs nor to the outputs of the network. Each neuron in the hidden 
layer is fully connected to all neurons in the input layer and calculates its output by 
multiplying the value of each input by its corresponding weight, adding these up, and 
applying the transfer function. The output layer is to represent the output variable of the 
network. Neurons in the output layer are fully connected to all neurons in the hidden layer. 
Two neurons are designed in the output layer; one represents Category 1 and the other 
represents Category 2. The back-propagation learning is to adjust the connected weights to 
minimize the error that is the difference between the network output and the target (actual 
result). The error is fed back through the network. The adjustment process will continue until 
the error converges to an acceptable value or executing the specified number of learning 
cycles. In consideration of network parameters setting, the study applies the Taguchi method 
to determine the best network parameters.  
 
Support Vector Machine 
 
Support Vector Machines are powerful pattern recognition techniques that have been 
successfully applied to classification and regression. They have outperformed many other 
machine learning methods such as k-nearest neighbors and artificial neural networks because 
they own the properties such as good generalization performance, robustness in the presence 
of noise, ability to deal with high dimensional data, and fast convergence. 
  
We now provide a brief introduction of SVM here. More detailed information can be found in 
Kumar [12]. In the two class classification problem, suppose we are given a training set of 
examples miX ∈R ( 1,...,i n= ), with corresponding labels { 1, 1}id ∈ − + ( 1,...,i n= ), where 1 
and -1 stand for the positive class (C1) and negative class (C2). For a linearly separable set of 
data, there exists a set of separating hyperplanes ( ) 0w X b⋅ + = , where w is a weight vector 
and b is the bias or threshold. SVM finds a unique separating hyperplane between the two 
classes in input space that maximizes the margin between the hyperplane and the classes, as 
shown in Figure 3. The total margin can be expressed as 2/||w||, and the data points lie on the 
margin are called support vectors. 
 
To have good generalization, the total margin should be as large as possible. Therefore, the 
objective is then to minimize the squared Euclidean norm of w, ||w||2, subject to the 
constraints:  ( ) 1 0i id w X b⋅ + − ≥  ( 1,...,i n= ). The weight vector is generally expressed in 
1209
 
 
Figure 3: Illustration of support vector machines 
 
Decision Tree 
 
A decision is a simple structure where non-terminal nodes represent tests on one or more 
attributes and terminal nodes represent decision outcomes. Decision trees are powerful and 
popular for both prediction and classification. Decision trees are easy for us to understand 
and can be transferred into rules thus they are very attractive. The path from root node to the 
terminal node forms the classification rules. Figure 4 shows a general tree structure. Decision 
trees are constructed using only those attributes best able to differentiate the concepts to be 
learned. Repeatedly split the data at each node into smaller and smaller groups in such a way 
that each new generation of nodes has greater purity than its ancestors with respect to the 
larger variable [1]. At the start of the process, there is a training set consisting of pre-
classified records, the target. The tree is built by splitting the records at each node according 
to a function of a single input variable. The remaining training set instances test the accuracy 
of the constructed trees. If the decision tree classifies the instances correctly, the process 
terminates.  
 
Class 1
Total Margin 
2/||w|| 
support vectors 
Class 
( ) 0w X b⋅ + =   
 
( ) 1w X b⋅ + =   
 
( ) 1w X b⋅ + = −  
Margin
1/||w|| 
1211
(dimensions of map, distance function, topology function, and learning rate), each has three 
levels. The optimal parameter combination can be obtained by performing the Taguchi 
experimental design.  
 
The 4388 customers were divided by SOM into 12 groups. The study examined RFM values 
carefully and adopted the customer relationship matrix to segment customers into four 
clusters. The segmentation result is depicted in Figure 5. The cluster named active customer 
with close relationship include 4 groups separated by SOM: 5, 6, 9, 10. The cluster named 
active customer with close relationship include 4 groups separated by SOM: 5, 6, 9, 10. Other 
customer segmentations are as follows: 
y Potential customer with close relationship:  Group 11. 
y Active customer with distant relationship: Groups  1, 2, 3. 
y Potential customer with distant relationship: Groups 4, 7, 8, 12. 
 
 
Figure 5: Results of customer segmentation 
 
Since the customer values of the four segmentations are different, the enterprise should make 
a marketing decision carefully. The study suggests advertising to those customers with high 
value and saving the marketing cost of low value customers. In terms of marketing, the 
research defines two categories of customers. The first category is defined as the marketing 
target. By carefully examined the monetary data of each group in the segmentation of active 
customer with close relationship, we found it is above the average value in the 5th, 9th, and 
10th group. Therefore, the research suggested view them as the marketing target. Other 
groups belonging to the second category are low profitable or even cause negative profit. The 
enterprise should avoid taking marketing activities to those customers in the second category.  
 
 
 
1213
CONCLUSIONS AND FUTURE WORKS 
 
The study proposed a systematic approach which utilizes the SOM neural network together 
with the customer relationship matrix to target customers. The enterprises therefore can 
allocate the marketing resources more efficiently and strength the customer relationship. In 
addition, the research applied multiple classifiers to categorize customers into two categories 
by the customer’s basic data. One category is the potential high-value customers and 
organizations should take marketing activities on customers in this group. The other one is 
the customers with relatively low-value and needn’t to take much marketing effort. The 
empirical analyses proved that the performance of the multiple classifier system is better than 
the individual classifier. The results are useful for business to estimate a new customer value 
and apply a suitable marketing strategy to that customer. 
 
Though the proposed customer segmentation and purchase behavior mining methods perform 
well, it leaves some room for improvement. Other variables used to segment markets such as 
psychographic variables and behavioral patterns may used to improve accuracy of the 
customer value prediction. Attention should also be given to evaluate the effects of different 
marketing strategies.  
 
REFERENCES 
 
1. Berry, M.J.A and Linoff, G. (1997). Data Mining Techniques: For Marketing Sale and 
Customer Support, New York: John Wiley & Sons, Inc. 
2. Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J. (1984). Classification and 
Regression Trees, Monterey, CA: Wadsworth International Group. 
3. Chiang, Y.M. and Wei, N.C. (2006). The Application of Data Mining Technique to 
Customer Segmentation and Purchasing Behavior, Management International Conference, 
Portoroz, Slovenia, November, 1593-1601. 
4. Chung, K.Y., Oh, S.Y., Kim, S.S. and Han, S.Y. (2004). Three representative market 
segmentation methodologies for hotel guest room customers, Tourism Management,25, 
429-441.  
5. Dillon, W.R. (1984). Multivariate analysis, John Wiley & Sons Inc.  
6. Frawley, W.J., Paitetsky-Shapiro, G. and Matheus, C.J. (1991) Knowledge Discovery in 
Databases: An Overview, Knowledge Discovery in Databases, AAAI/ MIT Press, 1-30. 
7. Gunter, S. and Bunke, H. (2004). Feature selection algorithms for the generation of 
multiple classifier systems and their application to handwritten word recognition, Pattern 
Recognition Letters, 25(11), 1323-1336. 
1215
Creativity, Innovation and Management
Proceedings of the 10th International Conference
Sousse, Tunisia, 25–28 November2009
SOLVING THE MULTI-LABEL TEXT CATEGORIZATION 
PROBLEMS—APPLICATION OF RADIAL BASIS FUNCTION 
NEURAL NETWORK 
 
Dr. Huei-Min Chiang, Institute of Technology, Taiwan  
hueimin@mail.njtc.edu.tw 
Dr. Tai-Yue Wang, National Cheng Kung University, Taiwan 
tywang@mail.ncku.edu.tw 
Dr. Yu-Min Chiang, I-Shou University, Taiwan 
ymchiang @ isu.edu.tw 
 
ABSTRACT 
The pervasiveness of information available on the Internet means that increasing 
numbers of documents must be classified. In addition to domain experts, automatic text 
categorization systems also perform document classification. Document classification is not 
only undertaken by domain experts, but also by automatic text categorization systems. 
Therefore, a text categorization system with a multi-label classifier is necessary to process 
the large number of documents.  
This work presents a novel automatic text categorization model based on the Radial 
Basis Function Neural Network (RBFNN). This model utilizes valuable discriminative 
information in the training dataset and incorporates background knowledge during model 
learning. A novel procedure for adapting RBFNN to multi-label text categorization is also 
presented. The Reuters 21,578 News dataset is utilized as an example to demonstrate the 
application of the proposed model and to compare its performance with that of other models. 
The analytical results vindicate that the performance of the proposed model is comparable to 
that of other models for different performance indices. 
Keywords: Radial basis function neural network (RBFNN), Orthogonal Least Squares (OLS), 
threshold function, multi-label 
445
SPAM email. Jiang showed that the RBFNN is a competitive alternative to well-known text 
classifiers such as the naive Bayes and SVM models.   
The RBFNN has several shortcomings. For instance, randomly choosing the center point 
before determining the weights of the network is unsatisfactory. Hence, Chen et al. (1991) 
proposed an alternative learning procedure based on the orthogonal least squares method to 
overcome these shortcomings. Chang (2007) then proposed an algorithm combining the 
orthogonal least squares, second-order derivative for a network, pruning and the Bayesian 
model to improve reverse engineering for linear systems.   
This study applies the RBFNN to a multi-label text categorization problem. Using the 
proposed algorithm, this study will present the least-mean square (LMS) base RBFNN 
system to calculate the connected weights of the network.  
 
SOLVING THE MULTI-LABEL TEXT CATEGORIZATION PROBLEMS BY 
RBFNN 
 
The proposed multi-label text categorization system consists of two subsystems, the text 
preprocessing subsystem and text classification subsystem. The text preprocessing subsystem 
is to preprocess the original text, select the feature, representing the text and weigh the term. 
And the text classification subsystem uses training data to train the classifier and uses test 
data to test the classifier. The detailed description is discussed in the following sections. 
The document preprocessing subsystem 
In the text preprocessing, document tags are first deleted, and individual terms in the 
content of each tag are identified. Feature selection is then employed to select the terms that 
have appeared at a particular frequency in each class. And, stop words, including verbs, 
adverbs and adjectives, are screened out because they are irrelevant and render the 
classification task inefficient. This preprocessing also reduces the dimensions of text 
preprocessing. 
447
output of the RBFNN. Finally, the output classes can be determined by setting the threshold 
function. The text classification subsystem is called the Random-OLS RBFNN.  
Term weights from 
training Term weights for testing
Selecting candidates 
of potential center 
points for the RBFNN
Seeking appropriate 
center points of 
RBFNN by OLS
Calculating the 
weights of RBFNN by 
LMS
Calculating the new 
weights of the RBFNN 
Determining the 
classes of  the 
document by setting 
threshold function
Calculating the new 
outputs of the RBFNN 
 
 
Figure 1: Flowchart of text classification subsystem 
RBFNN classifier 
The RBFNN has three layers: an input layer, hidden layer of nonlinear processing 
neurons and output layer. The input for the network is a vector (x), and the output (y) from 
the network is calculated by 
449
In the RBFNN, the dimensions of the hidden space are directly related to the capacity of 
the network to approximate a smooth input-output map (Mhaskar, 1996; Niyogi and Girosi, 
1996). Hence, selecting appropriate center points of the basis functions improves system 
efficiency. To locate the appropriate center points of for the RBFNN, one can use the 
randomly selected rq and the OLS method (Chen, 1991).  
The computational procedure of the OLS algorithm for choosing the RBFNN center 
points is as follows.  
Step 1.  
a) Compute  
   i
i Rs =)(1   
( ) ( )( ))(1)(1)(1)(1 iTi
Ti
i
ss
Dsg =  
[ ] ( )( ) ( ) ( )( ) DDssgNerror Tii Ti 1121i)(1 =  where D is desired output matrix for rqi ≤≤1 . 
⎩⎨
⎧=
calss i  tobelongnot  does data  theif  1-
class i  tobelongs data  theif    1
ijD  
b). Find 
[ ]{ },1max][ )(1)1(1 rqiNErrorNError ii ≤≤=  
c). Select  
( )
1
1
11 i
i Rss ==  
Step j ( 2≥j ).   
a). Compute 
( )ssRs Tmi
T
mi
mj =)(α  , jm <≤1  
m
j
m
i
mji
i sRs
j ∑
−
=
−=
1
1
)()( α  
( ))()()()( ijTijTijij ssdsg = , for rqi ≤≤1 , 1,..., −≠≠ ji iiii  
451
where ( ) ⎟⎟⎟⎠
⎞
⎜⎜
⎜
⎝
⎛ −−=−=
2
22
exp σ
ij
i
cX
uXRR , Qj ,...,2,1= , mi ,...,1= , and m is the final 
selected number of center points.  
Notably, D is the target output value matrix QncRD ×∈ , and is defined in Eq. (5):  
⎩⎨
⎧=
calss i  tobelongnot  does data  theif  1-
class i  tobelongs data  theif    1
ijD                                  (5) 
Note: Qjnci ,...,1,,...,1 == ,  
where nc is the class number, and Q is the number of training term weights.  
 The network output, y, is then expressed as follows: 
DRRRWy )( +==                                                          (6) 
QncRy ×∈  
Estimating the new outputs of the RBFNN and determining the output classes 
 In the testing phase, one must compute the new weights and outputs of the RBFFN for 
test documents before determining the classes to which a test document belongs.    
1. Estimating the new weights of the RBFNN and forecasting new network output 
The selected center points (Section 2.4.2) are also used to calculate the new weights of 
the RBFNN for test documents using the following equation:  
newDnewRnewDnewRnewRnewRnewW TT +− == 1)(                              (7) 
where newD is the target value of test documents. 
The term weights for test documents and new weights of the RBFNN for test documents 
are input to calculate the new outputs of the RBFNN. The new network output can be 
obtained by Eq. (8). 
( ) ( ) 0
0 1
.. wcxnewRnewWxznewWpredy
m
i
m
j
jijjj +−==∑ ∑
= =
                          (8)      
where x belongs to test term weights, and i=1,…,tQ, where tQ is the number of test term 
weights, cj represents the selected appropriate center points, and mj ,...,1= .  
2. Setting the threshold function and determining output classes 
One cannot separate relevant classes from irrelevant classes using the outputs of neural 
networks only. Therefore, the output of the ranking algorithm can be utilized   to separate 
453
(4) Average precision (Avgprec) 
(5) Ranking Loss 
The detailed description and computation on these indices can found the related 
literature (Salton, 1991; Schapire & Singer, 2000). 
 
NUMERICAL EXPERIMENT 
 
A numerical experiment using Reuter’s 21,578 documents was performed to 
demonstrate application of the proposed model. The documents from Reuters are in Standard 
Generalized Markup Language (SGML), numbered and have TOPIC and BODY tags. The 
data contain 10 topics, as presented in Table 2, and are named class 1, 2,..,10 accordingly. 
The content in the TOPIC tag included more than one topic; thus, each document was 
assigned to one or more topics.  
 
Preprocessing the 21,578 Reuters documents 
The content enclosed in <BODY> </BODY> was analyzed as the main content of 
documents, and was divided into different words. Insignificant words were filtered out. The 
keywords were as follows. 
Step 1: Count the frequency of words in each document. 
Step 2: Eliminate numbers and punctuation from each document. 
Step 3: Eliminate the stop words (306). 
Step 4: Select the top 100 words in each class. 
In total, 751 keywords and 8,606 documents were collected. Thus, the training dataset 
contains 6,454 documents and the test dataset contains 2,152 documents.   
The input matrix, which is called term weights, was a collection of n documents and 
contained the IDFTF _  weights, in which )/log(_ ii nNnIDFTF ×= , where N  is the 
total number of documents in the collection, and in  is the number of the documents in 
which the term it  appeared. The input matrix corresponds was the input data for classifiers. 
455
Avgprec 0.9912 0.8437 0.9937 0.8980 0.9925 0.9441 
Hamming Loss 0.0113 0.0620 0.0087 0.0443 0.0113 0.0275 
One-error 0.0125 0.2049 0.0093 0.1394 0.0125 0.0757 
Ranking-Loss 0.0048 0.1173 0.003 0.0629 0.0034 0.038 
Actually selected center 
point # 
174 147 174 1001 139 327 
Exact Match Ratio 0.8559 0.8352 0.9930 0.7500 0.8369 0.8542 
Avgprec 0.9755 0.9733 0.9998 0.9206 0.9652 0.9669 
Hamming Loss 0.0174 0.0181 0.0009 0.0416 0.0207 0.0197 
One-error 0.0414 0.0446 0.0005 0.1041 0.0590 0.0499 
Random-OLS 
RBFNN (1500) 
Ranking-Loss 0.0074 0.0086 0.0001 0.0553 0.0118 0.0166 
Actually selected center 
point # 
1500 1500 1500 1500 1500 1500 
Exact Match Ratio 0.9670 0.9684 0.9670 0.6989 0.9679 0.9138 
Avgprec 0.9919 0.9926 0.9919 0.8724 0.9933 0.9684 
Hamming Loss 0.0122 0.0110 0.0122 0.0545 0.0117 0.0203 
One-error 0.0116 0.0107 0.0116 0.1654 0.0098 0.0418 
Random- 
RBFNN (1500) 
Ranking-Loss 0.0049 0.0038 0.0049 0.0967 0.0033 0.0227 
Actually selected center 
point # 
162 150 452 859 642 453 
Exact Match Ratio 0.8429 0.8318 0.9359 0.9768 0.9638 0.9102 
Avgprec 0.9662 0.9665 0.9930 0.9962 0.9957 0.9835 
Hamming Loss 0.0199 0.0212 0.0075 0.0030 0.0043 0.0112 
One-error 0.0567 0.0553 0.0121 0.0056 0.0070 0.0273 
Random-OLS 
RBFNN (2000) 
Ranking-Loss 0.0114 0.0113 0.0023 0.0018 0.0021 0.0578 
Actually selected center 
point # 
2000 2000 2000 2000 2000 2000 
Exact Match Ratio 0.9726 0.9670 0.9698 0.9721 0.9675 0.9700 
Avgprec 0.9918 0.9913 0.9918 0.9912 0.9879 0.9908 
Hamming Loss 0.0140 0.0152 0.0135 0.0134 0.0170 0.0146 
One-error 0.0102 0.0121 0.0121 0.0125 0.0181 0.0130 
Random- 
RBFNN (2000) 
Ranking-Loss 0.0062 0.0051 0.0043 0.0049 0.0067 0.0054 
Actually selected center 
point # 
132 338 354 606 132 512 
Exact Match Ratio 0.8188 0.8783 0.9061 0.9614 0.8188 0.8767 
Avgprec 0.9543 0.9813 0.9875 0.9952 0.9593 0.9755 
Hamming Loss 0.0236 0.0150 0.0117 0.0046 0.0236 0.0157 
One-error 0.0664 0.0316 0.0218 0.0074 0.0664 0.0387 
Random-OLS 
RBFNN (2500) 
Ranking-Loss 0.0145 0.006 0.0036 0.0018 0.0145 0.0081 
Actually selected center 
point # 
2500 2500 2500 2500 2500 2500 
Exact Match Ratio 0.9930 0.8188 0.9954 0.9935 0.7430 0.9087 
Avgprec 0.9960 0.9976 0.9979 0.9968 0.8617 0.9700 
Random- 
RBFNN (2500) 
Hamming Loss 0.0049 0.0040 0.0029 0.0042 0.2124 0.0457 
457
represented with term weights. Some points from all training term weights were then 
randomly selected for identifying appropriate center points of the RNFNN using the OLS 
method. The selected center points were also used to calculate the neural weight of the 
RBFNN for test documents. Test term weights were input into the trained network to obtain 
the output vector. A threshold function finally helped determine the example belonged to 
which the classes.  
In the future, a comparative experimental study of the multi-label problem will use other 
datasets. Additionally, the proposed method will be applied for web classification tasks, such 
as scene classification or video surveillance.   
 
REFERENCES 
1. Boutell, M.R., Luo, J., Shen, X. & Brown, C.M. (2004). Learning multi-label scene 
classification, Pattern Recognition, 37(9), 1757-1771. 
2.  Broomhead D. S. & Lowe D. (1988). Multivariable functional interpolation and adaptive 
networks, Complex systems, 2, 321-355. 
3.  Chang, S. K. (2007). Bayesian Orthogonal Least Squares (BOLS) algorithm for reverse 
engineering of gene regulatory networks, BMC Bioinformatics, 8, 1-15. 
4.  Chen, S., Cowan, C. F. N. & Grant, P. M. (1991). Orthogonal Least Squares Learning 
Algorithm for Radial Basis Function Networks, IEEE Transactions on Neural Networks, 
2(2), 302-309. 
5.  Elisseeff, A., & Weston, J. (2002). A kernel method for multi-labelled classification, 
Advances in Neural Information Processing Systems, 14, 681-687.  
 6.  Erol, R., Oğulata, S. N., Şahin, C. & Alparslan, Z. N. (2008). A Radial Basis Function 
Neural Network (RBFNN) Approach for Structural Classification of Thyroid. Journal of 
Medical Systems, 32(3), 215-220. 
7.  Freund, Y. & Schapire, R.E. (1997). A decision-theoretic generalization of on-line 
learning and an application to boosting, Journal of Computer and System Sciences, 
55(1), 119–139. 
459
國科會補助計畫衍生研發成果推廣資料表
日期 2010年10月31日
國科會補助計畫
研發成果名稱
發明人
(創作人)
技術說明
技術移轉可行性及
預期效益
技術/產品應用範圍
產業別
計畫名稱:
計畫主持人:
計畫編號: 學門領域:
(中文)
(英文)
成果歸屬機構
(中文)
(英文)
彩色濾光片缺陷檢測與分類系統之研發(I)
江育民
98 -2221-E -214 -035 - 生產系統規劃與管制
彩色濾光片缺陷自動分類系統
Automatic Defect Classification System for Color Filters
義守大學 江育民,林冠良,翁若棉
本技術藉由統計觀點之灰階共生矩陣(Gray-Level Co-occurrence Matrix,
GLCM) 進行影像處理及缺陷特徵擷取，來解決樣本影像本身之旋轉、位移、放
大倍率…等尺度不一之問題。針對彩色濾光片白缺陷（White Defect）、黒缺
陷（Black Defect）、突起缺陷（Tokki Defect）…等共十一種缺陷類型及良
品，於四種不同角度下(0°, 45°, 90°, 135°)取GLCM各22個特徵，並經特徵篩
選過程，最後透過倒傳遞類神經網路(Back-Propagation Neural Network,
BPN)、支持向量機(Support Vector Machine, SVM)等分類(Classification)技
術進行缺陷自動分類，分類正確率可達95%以上，符合CF實務生產所需。藉由此
技術，可提昇彩色濾光片缺陷檢測效能。
The research develops an automatic defect classification system for
CF. The study utilizes the gray-level co-occurrence matrix (GLCM) to
extract the texture features of a CF image to overcome the problems
of shift variant, scale variant, and illumination variant. The co-
occurrence matrix is formed using a set of offsets sweeping through
180 degrees (i.e. 0, 45, 90, and 135 degrees) at the same distance
(10 pixels). For one degree, 22 features are extracted. The extracted
features are imported into the back-propagation neural network (BPN)
and support vector machine (SVM) to further classify the defect
types. For 11 CF defect types, the recognition accuracy is greater
than 95%.  Overall speaking, the proposed method can achieve good
performance for CF defect classification and is suitable for
光學及精密器械製造業；電機及電子機械器材業；檢測維護業；研究發展服務業；工
業檢驗業
1.彩色濾光片缺陷檢測。
2.LCD缺陷檢測。
3.紋理分類。
此技術乃與既有之彩色濾光片廠商配合研發，可移轉至彩色濾光片廠商以及推廣至
LCD產業。利用此缺陷自動分類技術，可取代現有以人力進行缺陷分類之方式，節省
成本及提高生產效率。
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
＇＇Solving the multi-label text categorization problems—application 
of radial basis function neural network ＇ ＇  一 文獲 Management 
International Conference 的最佳論文獎提名。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
