 1 
行政院國家科學委員會專題研究計劃成果報告 
 
多型態 X 光片自動化身份辨識系統之研製 
 
計劃類別: 個別型 
計劃編號:  NSC 99-2221-E-126-005 
執行期間:  99 年 8 月 1 日 至 100 年 7 月 31 日 
計劃主持人:  林芬蘭 靜宜大學資訊工程學系所 
計劃參與人員:  賴彥豪 郭俊宏 蔡雅珊 王豫勇  
成果報告類型:  精簡報告   
執行單位: 靜宜大學 
中華民國 100 年 10 月 25 日 
 3 
文獻探討  
 
A. 牙齒身份辨識: 在近幾年來的研究已經有不錯的成果。Hofer 和 Marana [1]利用牙齒治
療的資訊，將牙齒治療依據其位置和距離形成一個牙齒編碼 (dental code)，並利用此
編碼進行辨識。Zhou 和 Abdel-Mottaleb [2]利用牙齒的邊界作為特徵值，透過連續 2
至 3 顆的牙齒邊界來計算相對應的 Hausdorff distance，作為比對的依據。Nomir 和
Abdel-Mottaleb [3]利用牙齒的關鍵點(salient point)作為特徵值，並計算每個關鍵點到
邊界的距離，形成一個特徵向量，作為比對的依據。Chen 和 Jain [4]利用計算牙齒邊
界和牙齒治療的空間域特徵值作為比對依據，來提升辨識的正確率。Nomir 和 Abdel-
Mottaleb [5] 利用不同層次的融合技術(1)特徵萃取層之融合、(2)比對技術層之融合、
(3)比對決策層之融合，進行辨識率之分析，求得牙齒辨識系統的最佳比對方式。 
 
B. X 光影像分類:  Zhou & Abdel-Mottaleb 曾在身分辨識系統中[2]，提出了一個可分類
咬翼片影像的分類方法。 由於全景影像可提供完整的口腔內所有牙齒的資訊，所以
全景影像的牙齒個數也會來的比其他三類多。用邊緣偵測法找出牙齒的垂直邊緣，並
計算牙齒的垂直邊緣像素的個數，全景影像也會來的比其他三類多。另外，咬翼片影
像及全景影像在向上梯度或向下梯度的水平邊緣偵測時，都會有水平邊緣。而牙根間
下顎影像只有在向上梯度的水平偵測才有水平邊緣。牙根間上顎影像只有在向下梯度
的水平偵測才有水平邊緣。因此以垂直邊緣與水平邊緣像素的個數使用貝氏分類器進
行分類 
 
C. 牙齒分隔:  Jain和Chen [4] 提出針對牙齒咬翼X光影像半自動化的偵測方式，首先透過
水平方向的積分投影，找出水平方向的亮度積分分布圖中所有的波谷位置，再透過手
動的方式訂出可能的Gap valley位置，接著計算出每個波谷為Gap valley之機率， 找出
最大機率的波谷位置將影像分成上下顎二個部份。最後再利用垂直方向的積分投影將
每個牙齒分離成一個獨立的區塊。Nomir和Abdel-Mottaleb [3] 提出全自動化的分隔方
式，首先將牙齒X光影像利用Canny 邊界偵測法配合iterative thresholding及adaptive 
thresholding得到去掉背景的牙齒大致輪廓後，再利用水平及垂直積分投影來分離出上
下顎及各個獨立的牙齒。Zhou和Abdel-Mottaleb[2]提出利用牙齒邊界數量的多寡以及
邊界的方向位置，透過貝氏分類器將牙齒X光片分成咬翼、牙根尖、全景三種影像。
再針對咬翼X光片影像，利用形態學運算子增強影像與主動輪廓線(active contour 
model)進行牙齒切割。 
 
參考文獻  
 
1. M. Hofer, A. N. Marana, Dental biometrics: human identification based on dental work 
information, Brazilian Symposium on Computer Graphics and Image Processing, pp.281-
286, 2007.  
2. J. Zhou, M. Abdel-Mottaleb, A content-based system for human identification based on 
bitewing dental X-ray images, Pattern Recognition, Vol. 38, pp. 2132-2142, 2005.  
 5 
 
 
 
 
 
| | | |
1 1
1
2
C N
QTeeth QTeeth
QTeeth QTeeth QTeeth
C N
QTeeth QTeeth
i i
R n R n
W n
R i R i
 
 
 
   
 
 
 
 
 
圖 1. 是比較兩顆不同切割結果的牙齒沒有經過權重指派後和有經過權重指派後牙齒對位
的成果。  
                                     
 
圖 1                                                                                  圖 2 
 
B. 全自動高效能的牙齒分隔法 
牙齒分隔在牙齒生物特徵辨識系統中是一個很重要的前處理動作，牙齒分隔結果的準確
性，會直接影響到牙齒特徵萃取的精度，進而影響到身份辨識的結果。我們提出的牙齒 x
光影像的有效牙齒分隔演算法如下： 
 
步驟一： 分隔上下顎 
1. 將影像在正負 20 度內做小角度旋轉，找出每個旋轉角度影像的灰階 horizontal integral 
projection 的最低波谷位置， 由此位置畫一橫線。 
2. 以此橫線為基底，在其周圍訂出多個條狀區域，分別找出每個條狀區域 horizontal 
integral projection 的最低波谷位置，由此位置可得到各區域內的分隔線段。， 
3. 利用 spline function，將各個區域分隔線段的中點位置連線起來，得到一條分隔上下顎
的曲線。  
 
步驟二：分隔各顆牙齒  
1. 對上（下）顎的影像做一個旋轉的動作（-10~10 度），並在每個旋轉角度下各作一次
vertical integral projection，取得在每個旋轉角度下的波谷數量資訊，找出有最多波谷數
目的角度影像。 
2. 利用變動視窗法來分隔 vertical integral projection。 
3. 找出每個視窗內的最低波谷位置，由此位置畫一縱線。 
4. 以此橫線為基底，在其周圍訂出多個條狀區域，分別找出每個條狀區域 vertical integral 
projection 的最低波谷位置，由此位置可得到各區域內的縱向分隔線段。 
 7 
步驟 一: 萃取影像中各顆補牙，判斷其為上顎或下顎位置。 
步驟 二: 分別計算各顆補牙之大小特徵、與第一顆補牙之距離特徵、正規化形狀傅利葉描
述子之輪廓特徵。 
步驟 三: 以輪廓特徵比對找出比對影像中每顆補牙相對於資料庫影像中最相似的對應補牙
位置。 
步驟 四: 依補牙編號由小至大，將每顆補牙之特徵向量串接成一個序列編碼。 
步驟 五: 不考慮兩特徵向量編碼長度差異的成本，而直接根據位置、大小、與編碼序列中
第一個補牙的距離及輪廓特徵上不同的差異，計算總差異成本。 
步驟 六: 將總差異成本由小至大排序。  
  
結果與討論  
 
在本年度計劃中，我們提出了基於牙齒邊界權重之牙齒對位法，以及提出一個以多顆補牙
為特徵之身份辨識法，來提高自動化牙齒身份辨識之準確率。我們並提出全自動高效能的
牙齒分隔法，以提高牙齒切割的準確率，以及全景、咬翼片、與牙根尖三類型之 X 光影
像分類演算法，以便後續以多類型牙齒影像進行身份辨識，更加提高自動化牙齒身份辨識
之準確率。 
 
研究成果/ 論文發表  
 
(1) P. L. Lin1, Y. H. Lai2, P. W. Huang, “Dental Biometrics: Human Identification based on 
Teeth and Dental Works in Bitewing Radiographs,” pattern recognition 45, pp. 934-946, 
2012. NSC2221-98-E-126-009, NSC2221-99-E-126-005 
(2) P. L. Lin, Y. H. Lai, C. H. Kuo, “Dental Identification based on Teeth and Dental Works 
Matching for Bitewing Radiographs,” International Forum on Medical Imaging in Asia 
(IFMIA), Okinawa, Japan, 2011. NSC2221-99-E-126-005 
(3) Chun-Hung Kuo and P.L. Lin*, “An Effective Dental Work Extraction and Matching Method 
for Bitewing Radiographs,” The 2010 International Computer Symposium, Dec. 16 –18, 
2010, NSC2221-98-E-126-009, NSC2221-99-E-126-005. 
(4) P. L. Lin1, Y. S. Cho2, P. W. Huang, “An Effective and Automatic Tooth isolation Method 
for Bitewing Dental X-Ray Images,” submitted. 
(5) Po-whei Huang, Cheng-I Chen, Phen-Lan Lin, “A Heuristic Approach to Multi-focus Image 
Fusion,” CGVCVIP, 2011, July, 21-24, Rome, ltaly. 
(6) Phen-Lan Lin, Cheng-I Chen, Po-whei Huang, “Brain Medical Image Fusion Based on IHS 
and LOG-GABOR with Suitable Decomposition Scale and Orientation for Different 
Regions,” CGVCVIP, 2011, July, 21-24, Rome, ltaly. 
 9 
本年度的研究成果已經有 1 篇論文即將被刊登在國際知名期刊 Pattern Recognition (部份成
果結合 98 年度成果), 1 篇發表在國際研討會議，另有 2 篇論文分別投搞至國際知名期刊與
國際研討會議。碩士論文 3 本，博士論文一本。無論在學術價值上或是實際應用上都是相
當的豐碩，並且未來能夠有繼續發展的潛力。 
 2 
“Brain Medical Image Fusion Based on IHS and LOG-GABOR with Suitable Decomposition Scale and 
Orientation for Different Regions” 
“A Heuristic Approach to Multi-focus Image Fusion 
“Super-resolution Imaging: the Use Case of Optical Astronomy” 
“Segmentation and Discrimination of Breast Tumors in Ultrasonic Images Using an Ensemble Classifier and 
Application to a Diagnostic Support System” 
除了我們的 2 篇之外，有一篇是探討天文星空影像的 enhancement，報告者以一段小影片提到他們正
進行如何將不甚清晰的星空影像透過 noise reduction and deconvolution 的技術將整個星座甚至其周邊
之星星群明顯的顯示出來，相當有趣。而我們所提出的腦部影像融合結果也引起其中與會學者的關
注，由於我們的融合影像結果以影像的觀點來看，效果頗佳，與會學者建議我們與醫生討論，是否吻
合醫生的需求。 
Session Computer Vision: 
 此一 Session 中，所發表的論文共有 5 篇： 
“A Fast and Space-Efficient Discrete Focal Stack Transform” 
“ 3D Reconstruction of Small Solar System Bodies Using Photoclinometry by Deformation” 
“Pont-based Similarity Estimation Between 2.5D Visual Hulls and 3D Objects” 
“Evaluation of Two Reconstruction Systems Based on Stereovision” 
“Approximation of 3D Data with Piecewise Series Expansions for Surface Inspection” 
其中第一篇 discrete focal stack transform 是我們較為陌生的技術，而第 3 篇之 2.5D visual hulls and 3D 
objects 之相似性估算也是相當有趣的技術，讓我們得到了許多寶貴的資訊，有助於提升我們往後的研
究。 
Session Image Processing: 
 在此一 Session 中，所發表的論文共有 6 篇： 
“Texture Compression in Memory and Performance-Constrained Embedded System” 
“Accurate Multi-Modal Image Registration Using Compression” 
“Integration Data Hiding with JP3D Compression on Volumetric Medical Images” 
“Gaussian Mixture Background Model with Shadow Information”  
“A Similarity Measure Based on Geometric Consistency for Image Retrieval Using Local Descriptors”  
“Computing the Euclidean Distance Transform on the Epuma Parallel Hardware” 
其中第二篇 Accurate Multi-Modal Image Registration Using Compression 與第五篇 A Similarity Measure 
Based on Geometric Consistency for Image Retrieval Using Local Descriptors” 對於我們目前正在研究的
牙齒 x 光片身份辨識相當有幫助，我們目前的比對也是有用到 Geometric Consistency，因此可加進此
方法來加以比較比對的優劣。 
三、考察參觀活動(無是項活動者略) 
四、建議 
國內學者可主動爭取 session organizer 以增加在國際學術舞台主導性及曝光度。 
 4 
 From: CGVCVIP 2011 <secretariat@cgv-conf.org> 
Date: Wed, Feb 23, 2011 at 3:37 AM 
Subject: CGVCVIP 2011: paper accepted 
To: Phen-lan Lin <lan@pu.edu.tw>  
Cc: author.notifications@gmail.com, Cheng-i Chen <chenchengi2009@gmail.com>, Po-whei Huang 
<powhei.huang@msa.hinet.net> 
 
 
Dear Author 
We are pleased to inform you that your submission to the IADIS Computer Graphics, Visualization, Computer 
Vision and Image Processing (CGVCVIP) 2011 Conference has been accepted as a "Full Paper". 
 
Please, make the suggested corrections to your paper (see details below), use the correct format available at 
http://www.cgv-conf.org/submissions.asp (very important: if this format is not followed we cannot accept 
your contribution and it won't be published in the proceedings). Make sure that your final submission has the 
number of pages allowed for this category which is 8 pages (additional pages up to 4 will be charged as 
specified in the registration form). Also note that your final submission must be a WORD file since 
proceedings are produced in WORD, 
 
Also, login to your author area available at 
http://www.conf-system.org/confman_cgvcvip2011/author_menu.asp with your login and password, and 
1 - submit the final version - in word or rtf version please until 25 March 2011 (please   submit from your 
author area link only), 
2 - access the copyright form, fill it out and send it in order for your contribution to be published (until 25 
March 2011), 
3 - print an invitation letter (if required) for you or any of your co-authors, 
4 - register for the conference (deadline for this procedure is also 25 March 2011 - if not registered the paper 
won't be published in the proceedings. This deadline also corresponds to the early registration rates for this 
call - select the early registration option from the rates part of the registration form), 
5 - View hotel information and book hotel at http://www.cgv-conf.org/hotel.asp , 
6 - Check the guidelines for presenters available at http://www.cgv-conf.org/guidelines.asp . 
 
Hope to see you in Rome, Italy in July. 
 
For any information please contact us. Thank you. 
 
Best regards, 
 
Yingcai Xiao, The University of Akron, USA 
IADIS CGVCVIP 2011 Program Chair 
 
 6 
 
 
 
 
 Originality: 5 - Average 
 Significance: 6 - Good 
 Technical: 6 - Good 
 Relevance: 6 - Good 
 Classification: 6 - Good 
 Comments: This papers introduces a novel algorithm for multifocus image fusion. The main contribution of 
this paper is the algorithm to compute the decision maps used to guide the fusion process. Validation and 
pseudo code are provided. 
 Positive Points: The paper is in general well written. And the description of the algorithm is very clear. I 
particularly liked the decisive points-based criterium as activity level measurement. 
 
The number of methods compared against your technique is quite impressive as well. 
 Negative Points: Figures 1,2 and 3 are far from their references in the text. Figure 3 should be horizontal. 
This paper wouldn't be longer than six pages. 
 
The claim "the size of a focus-measuring window at each individual pixel location is not fixed; instead, it is 
automatically and dynamically adjusted" as contribution should be revised: for instance, Redondo (author of 
[8]) has already explored the idea in "Multifocus Fusion with Multisize Windows". 
 
 
 
=========================================================== 
FROMCGVCVIP 2011 SECRETARIAT@CGV-CONF.ORG  
TOPHEN-LAN LIN <PHEN.LAN.LIN@GMAIL.COM> 
 
CCAUTHOR.NOTIFICATIONS@GMAIL.COM, 
PO-WHEI HUANG <POWHEI.HUANG@MSA.HINET.NET>, 
CHENG-I CHEN <CHENCHENGI2009@GMAIL.COM>, 
CHIN-HAN CHAN <CHANCHINHAN@GMAIL.COM> 
 
DATEWED, FEB 23, 2011 AT 7:40 PM 
SUBJECTCGVCVIP 2011: PAPER ACCEPTED 
 8 
 
YINGCAI XIAO, THE UNIVERSITY OF AKRON, USA 
IADIS CGVCVIP 2011 PROGRAM CHAIR 
 
PIET KOMMERS, UNIVERSITY OF TWENTE, THE NETHERLANDS 
PEDRO ISAÍAS, UNIVERSIDADE ABERTA (PORTUGUESE OPEN UNIVERSITY), PORTUGAL 
MCCSIS 2011 GENERAL CONFERENCE CO-CHAIRS 
 10 
One important type of image fusion is to deal with multi-focus visible images. Due to the limitation of depth of field of optical 
lens in a camera, only those objects within the depth of field of the camera are in focus while other objects are out of focus [1]. 
Therefore, there is a need to obtain an image that is in focus everywhere by fusing the images taken from the same view point with 
different focal settings to extend depth of field. Multi-focus image fusion is particularly useful to forensic applications. For example, 
tool impressions and fingerprints (footprints) are usually recorded by close-up photography. In close-up photography, adjusting 
focus is necessary and can be done by changing the distance between the specimen and the focal lens. Since the surface of the 
specimen is usually not smooth and the depth of field of the camera lens is limited, it is very difficult to focus every portion of the 
specimen at one time. By taking several multi-focus images with close-up photography, we can obtain a composite image 
containing a better and complete description for the specimen by image fusion. Similar technique can be applied to AOI (automatic 
optical inspection) in industrial applications. 
2. THE PROPOSED FUSION SCHEME 
In this paper, we only concentrate on fusion for visible and multi-focus images. We propose a new fusion scheme based on a set 
of decisive locations with focus weights contributed from all source images. This scheme uses a new focus measure to identify the 
right pixels in the source images to be included in the fused image. Two novel ideas are embedded in the design of our fusion 
scheme: (1) only those decisive locations in the source images are considered as critical for focus-measuring calculation; (2) the size 
of a focus-measuring window at each individual pixel location is not fixed; instead, it is automatically and dynamically adjusted. 
Based on the above two ideas, our scheme can achieve a better fusion result by calculating the focus measure more precisely at each 
pixel location of the source images and choosing the right pixel from the in-focus region of a source image for the fused image. 
Therefore, the misjudgment of including wrong pixels to the fused result is reduced significantly.  
Assume that we have a set of multi-focus images I1, I2, …, IN of the same scene. We also assume that any area in the image of 
this scene has its best focus in some source image Ii (1 iN). Therefore, the goal of our fusion scheme is to find the best pixel at 
each location of the resulting image from the corresponding locations of all source images. To achieve this goal, we need a 
focus-measuring mechanism to help us judge which pixel of the same corresponding location will be chosen from all source images 
for the resulting image. Let 
iM (x, y) be the focus-measuring mechanism for the pixel at location (x, y) in source image Ii. The 
higher the value of iM (x, y), the better the pixel Ii(x, y) is in-focus. Let the size of the source images be CR , then the resulting 
image can be represented by F, where }...,,2,1,),,(),(,1,1|),({ NjiandjiyxMyxMCyRxyxIF jii  . 
We have the following two observations for the focus measure: (1) the edge information in the surrounding area of a pixel can 
help us judge how well that pixel is in-focus because it is extremely difficult to judge whether a pixel is well-focused without 
looking at its surrounding area; (2) an defocused area may contain fake edge points which lead to misjudgment of focus measure 
using existing methods reported in the literature.  
According to the above observations, we design our fusion scheme based on the concepts of decisive locations, effective 
focus-measuring window, focus weight, and focus measure to be defined as follows. Given a set of multi-focus gray-level images I 
= {I1, I2, …, IN}, the set of decisive locations obtained from the first b bit-planes of image Ii is defined as )()(
11
k
i
N
i
k
i
b
k
i EEbD

 , 
where 
k
iE  is the set of edge points detected from the k-th bit-plane of image Ii. The set of global decisive locations with respect to 
a set of multi-focus images I = {I1, I2, …, IN} is )(
1
bDD i
N
i
 . 
 12 
Input: A set of multi-focus images I1, I2, …, IN 
Output: A fused image Ifused  
1. Find the set of decisive locations )(bDi from each image Ii. Let )(
1
bDD i
N
i
 . 
2. For each location (x, y) in the resulting image Ifused  
(a) Call “Finding effective focus-measuring window” to get window Sz(x, y). 
(b) In each source image Ii, compute the focus measure ),( yxM
i
z
for pixel at location (x, y) using the focus-measuring window 
Sz(x, y). 
(c) Assume that the pixel at location (x, y) of source image Ik has the maximum focus measure among all pixels at this same 
location in all N source images. Then, let Ifused(x, y) = Ik(x, y). 
3. Return Ifused. 
3. EXPERIMENTAL RESULTS 
In physics, increasing in the magnification of a microscope makes the depth of field of a microscope become narrow. As a 
result, an observed object or area with an uneven surface under the microscope will be rendered with partial blur in an image. Figure 
1-3 show three examples of such a phenomenon. Figure 1(a) and 1(b) respectively focus on the body and the wing of a bee while 
Figure 2(a) and 2(b) focusing on different parts of eye of a bee. Figure 3(a) and 3(b) shows the images of the same liver tissue with 
different parts in focus. The fused results of the three examples are shown in Figure 1(c)-3(c), respectively.  
We also compared the performance of our fusion scheme with other eight existing fusion methods based on 15 reference 
images of size 256256 in term of PSNR. The PSNR is defined as 
dB
MSE
PSNR
2
10
255
log10  
where   







ni
i
mj
j ijij
cc
nm
MSE
0 0
2' )()
1
( , m, n are the width and the height of image, respectively, and c, c’ are the pixel intensities 
of the two images under comparison. Assume that the reference image has intensity cij and the fused image has intensity c’ij. The 
reference images are blurred on different parts to emulate the defocused areas. Then, the two blurred images are fused to generate 
the fused image. A method with better performance should have higher PSNR. The eight methods are abbreviated as PCA [10], LAP 
[2][3], CON [11], OWD [7][10], NOWD [4], MWF [5], DSCP [6], and LOG [8], respectively. Among these methods, DSCP is 
block-based while others including ours are pixel-based approach. We tested LAP, CON, OWD and NOWD methods using 
decomposition level l = 1, 2 and 3, respectively. Then we selected the best performance from these three levels as the performance 
of each method. MWF was also tested by using decomposition level l= 1, 2, and 3, respectively, with the fusion rule same as the one 
used by Chanda [5]. The best performance among these levels was regarded as the performance of MWF. The performance of LOG 
method was obtained by choosing the best PSNR from level-3 decomposition with 3, 4, and 5 orientations. Note that for those 
multi-resolution methods including LOG, WMF, NOWD, OWD, CON, and LAP, the averaging strategy was applied as the fusion 
rule for the approximation image (i.e. the base image), and choosing maximum coefficient or maximum wavelet energy as the 
fusion rule for detailed images. To grade the performance of DSCP method, we used DCT strategy to obtain the best fusion result as 
proved by Lin [6].  
The result of performance comparison for these nine fusion methods is shown in Table 1. It is very clear that our proposed 
method outperforms other eight fusion methods in all 15 images. The PSNR value obtained from our fused results is 52.37 on 
average. The second best is probably the LOG method which has 46.67 as its average PSNR. It is worthwhile to mention that 
 14 
 
   
(a) (b) (c) 
Figure 3. (a) and (b)：Two partially focused images of liver tissue. (c) Our fused result. 
 
Table 1. Performance comparison of our method with other eight methods on 15 reference images in terms of PSNR 
Methods Ours LOG DSCP MWF NOWD OWD CON LAP PCA 
S-01 49.96 41.01 42.98 36.42 42.93 41.68 41.98 41.96 33.97 
S-02 51.74 45.56 40.53 37.30 45.64 43.12 42.17 42.21 34.63 
S-03 50.62 42.58 40.75 37.39 45.06 42.44 41.93 41.87 33.95 
S-04 52.42 46.89 45.82 38.99 46.10 44.86 44.88 44.81 37.61 
S-05 55.25 50.59 46.03 39.75 47.90 45.31 43.68 43.65 36.01 
S-06 51.76 42.75 40.08 38.89 46.27 44.58 43.19 43.14 36.25 
S-07 49.26 47.95 44.43 39.76 46.31 45.23 45.52 45.47 37.39 
S-08 50.33 48.00 42.16 35.78 43.87 41.77 41.52 41.48 33.99 
S-09 50.90 50.87 44.65 37.58 44.45 43.11 43.71 43.68 36.20 
S-10 51.09 44.20 44.60 37.19 42.47 42.07 44.43 44.36 36.30 
S-11 52.94 44.65 44.00 37.63 45.17 44.07 44.39 44.28 37.78 
S-12 51.02 48.82 46.67 40.47 47.15 46.31 46.55 46.51 40.60 
S-13 53.79 48.94 46.11 41.83 48.41 46.57 46.57 46.55 40.18 
S-14 54.23 46.27 49.83 39.83 46.34 45.10 46.58 46.42 39.60 
S-15 60.30 50.92 49.81 38.75 44.73 44.34 46.25 46.01 38.77 
Average 52.37 46.67 44.56 38.50 45.52 44.04 44.22 44.16 36.88 
REFERENCES 
[1] Aslantas, V., 2007. A depth estimation algorithm with a single image. In Optics Express, Vol. 15, No. 8, pp. 5024-5029. 
[2] Burt, P.J. and Adelson, E.H., 1983. The Laplacian Pyramid as a Compact Image Code. In IEEE Trans. Communications, Vol. com-31, No. 
4. 
[3] Burt, P.J. and Adelson, E.H., 1985. Merging images through pattern decomposition. Applications of Digital Image Processing VIII,  
A.G.Tescher, ed., Proc. SPIE 575, pp.173-181. 
[4] Chibani, Y, Houacine, A., 2003. Redundatnt versus orthogonal wavelet decomposition for multisensor  image fusion. In Pattern 
Recognition, Vol. 36, pp. 879-887. 
[5] De, I., Chanda, B., 2006. A simple and efficient algorithm for multifocus image fusion using morphological wavelets. In Signal 
Processing, Vol. 86, No. 5, pp. 924-936. 
[6] Lin, P.L., Huang, P.Y., 2008. Fusion methods based on dynamic-segmented morphological wavelet or cut and paste for multifocus images. 
In Signal Processing, Vol. 88, No. 6, pp. 1511-1527. 
[7] Pajares, G., Manuel de la Cruz, J., 2004. A wavelet-based image fusion tutorial. In Pattern Recognition, Vol. 37, pp. 1855-1872. 
[8] Redondo, R. et al, 2009. Multifocus image fusion using the log-Gabor transform and a multisize Windows technique. In Information 
Fusion 10, Vol. 2, pp 163-171. 
[9] Rockinger, O., 1996. Pixel - Level Fusion of Image Sequences using Wavelet Frames. Proceedings of the 16 th Leeds Applied Shape 
Research workshop, Leeds University Press. 
[10] Smith, M.I., Heather, J.P., 2005. Review of image fusion technology in 2005. Proceedings on  Defense and Security Symposium, Orlando, 
FL. 
[11] Toet, A. et al, 1989. Merging thermal and visual images by a contrast pyramid. In Optical Engineering. Vol. 28, No. 7, pp. 789-792. 
 16 
spatial information with as high resolution as possible. 
2.1 Analysis of fusion method IHS and Log-Gabor wavelet  
Image fusion using log-Gabor filter has been shown to produce promising results recently both in fusing multi-focus images [8] 
and in multi-sensor images [4]. The method that fuses MRI and PEI images [4] is as follows. At first, the PET image is transformed 
to intensity, hue, and saturation components. Both the I components of PET and the MRI image are decomposed with a fixed scale 
and angle to high and low frequency coefficients. The high frequency coefficients of MRI and the low frequency coefficients of PET 
are inverse log-Gabor wavelet transformed to a fused intensity component. Finally, the fused intensity component, the hue 
component, and the saturation component of PET are inverse IHS transformed to a fused image of PET and MRI. Figs. 1 (c) and (d) 
show the result using IHS&LG fusion with a decomposition scale of 4 and 3, respectively. Notice that in Fig. 1(c), which is the 
result of using scale 4, the structure information is well preserved but the blue shade is noticeably lighter than the original PET, as 
shown in Fig. 1(a). On the contrary, blue shade in Fig. 1(d) that is the result using scale 3 is very close to that in the original PET, 
but the structural information in the areas in shades yellow and red is not as significant as that in Fig. 1(c).  
Noticing that using higher decomposition scale in log-Gabor wavelet results in a fused image with clearer structure and using 
lower decomposition scale results in a fused image with less color distortion. Thus, we devise a revision of IHS&LG fusion method 
by dividing the brain image into regions of high and low activity, respectively, and decomposing each region with different scale to 
solve the aforementioned problem. 
2.2 Image separation based on brain cells activity 
In IHS model, the angles of colors yellow, red, and magenta are in the range of [-30°, 90°] in the hue plane. Since the yellow-, 
red-, and white-regions in PET scan images contain highly active brain cells, while the blue- and cyan-regions contain less active 
brain cells, the hue angle ranges for high and low brain activity pixels are [0.92, 1] and [0, 0.25] after normalization. 
The separation of PET and MRI images into high activity region and low activity region can be briefly described as follows.  
1. Apply IHS transform to the PET image and obtain the color of each pixel based on its angle value of the hue component.  
2. For each pixel in the PET image, assign it to high activity region if its hue angle is within the range of [0.92, 1] or [0, 0.25]; 
otherwise, assign it to low activity region.  
3. For each pixel in the MRI image, assign it to high or low activity region if the pixel at the same location in the PET image is in 
high or low activity region. 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
 
(e) 
Figure 1. Log-Gabor wavelet fusion analysis with different decomposition scales. (a) PET image (b) MRI image (c) the log-Gabor wavelet fusion 
result with decomposition scale= 4 (d) the log-Gabor wavelet fusion result with decomposition scale=3 (e) the log-Gabor wavelet fusion result 
with decomposition scale=4 for high activity region and 3 for low activity region. 
2.3 Fusion based on IHS and log-Gabor wavelet with suitable decomposition scales for different 
regions (IHS&LG+) 
Once the input images are separated into two regions of high and low activity, respectively, the intensity of each activity region 
is fused, respectively, as follows. 
1. The intensity component of the PET image and the intensity of the MRI image are respectively decomposed into high and low 
frequency coefficients by log-Gabor wavelet transform with a suitable scale for each region. As mentioned in Section 3.1, our 
investigation indicated that decomposition scale 4 is suitable for high activity regions of brain images, and decomposition scale 
3 is suitable for low activity regions.  
2. The high-frequency coefficients of the intensity of each MRI sub-band and the low-frequency coefficients of the intensity 
component of each PET sub-band are inverse log-Gabor wavelet transformed to result in a fused intensity component. Note that 
using high-frequency coefficients of the intensity of MRI and low-frequency coefficients of the intensity of PET can preserve 
the high spatial resolution information in the MRI image while retaining the cell activity information in the PET scan image. 
3. The fused intensity components of both activity regions along with both the hue and the saturation components of the PET 
image are inverse IHS transformed to result in a PET and MRI fused image.  
Fig. 1(e) shows the fusion result of images in 1(a) and 1(b) using decomposition scale 4 for the high activity region and scale 3 
 18 
the MRI structural information at current pixel location. After trial and error, we found that T = Ip,k can result in visually optimal 
fused image. 
3.2 Anatomical structure enforcement for the fused image by IHSLG+ 
Anatomical information is particularly rich in regions containing GM and CSF pixels in MRI images. Thus, we need to locate 
the regions containing GM and CSF pixels in the fused image first then add back the missing anatomic information into these 
regions.  
3.2.1 Allocation of GM and CSF 
Observing MRI images, we found that GM and CSF pixels are visually darker than WM pixels. Thus, intensity can be used to 
differentiate GM and CSF pixels from WM pixels. Our GM and CSF pixel allocation procedure is as follows. 
1. Obtain the histogram of both high and low activity regions in the MRI image, respectively.  
2. Calculate p% number of pixels Npk (k represents either high or low activity region) in each of high and low activity region. 
3. From each histogram, locate the smallest intensity Lk, that the total number of pixels whose intensity is less than Lk is greater 
than Npk.. Set Lk as the threshold for separating GM and CSF pixels from WM pixels.  
4. Apply the threshold Lk to each activity region in the MRI image and classify each pixel to either GM/CSF or WM.  
3.2.2 Anatomical structure enforcement  
After allocating all the GM and CSF pixels, we adjust the fused intensity after color adjustment (If,k) of each GM or CSF pixel in 
the regions of high and low activity using  
Qyx
wyxIyxIyxIyxI kmkfkfkf


),(
)),(),((),(),( ,,,,                                         where Q represents 
the set of GM and CSF pixels, and w= 0 ~ 1 is the weight for proper intensity adjustment. When w = 0, no adjustment is required, 
and when w = 1, the fused intensity of this pixel will be replaced by the intensity of the pixel at this location in the MRI image. 
Through different w, various amount of structural information is added to result in different fusion image. 
4. EXPERIMENTAL RESULTS AND ANALYSIS 
The test data consist of three sets of brain images downloaded from the web site (http://www.med.har- 
vard.edu/AANLIB/home.html), where a color PET image and a high resolution MRI image are in each set. The images in Dataset -1, 
-2, and -3 are normal axial, normal coronal, and Alzheimer’s disease images, respectively. All three sets of images are fused firstly 
by our proposed fusion method IHSLG+, and then by IHSLG+ with color adjustment and anatomical structure enforcement, 
IHSLG++.  
4.1 Performance assessment 
The assessment of fusion image quality can either be subjective or objective. A subjective assessment is best performed by brain 
doctors to check if all valuable information is contained in a fused image visually. On the other hand, an objective assessment is 
based on various statistics of the fused image. In this study, we use the following metrics for objective assessment.  
4.1.1 Spectral discrepancy (SD) 
The spectral discrepancy measures the spectral quality of a fused image by [3] 
'
1 1
1
( , ) ( , )      , ,
3
QP
k k k
x y
R G B
D f x y f x y k R G B
P Q
D D D
D
 
  

 


                                                                                                   
where fk(x, y) and fk’(x, y) are the pixel values of the fused and original PET images at position (x, y), respectively, P = 256, and Q = 
256. A tiny discrepancy means an acceptable fusion result.  
4.1.2 Overall performance (OP) 
Overall performance is an assessment that integrates both spectral discrepancy and average gradient. The smaller the OP is, the 
better quality of the image is. OP can be computed by [1, 7] 
BGRb
AGDP
OP bb ,,,
3



                                                                                                                   
4.1.3 Mutual information (MI) 
Mutual information (MI) [1, 7] is another common measure to assess the similarity between a fused image and each of its source 
images. It is a fundamental concept of information theory to compute the statistical dependence between two random variables. 
Given two random variables, A and B, with marginal probability distributions, pA(a) and pB(b), and joint probability distribution pAB 
 20 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
 
(e) 
Figure 6. Comparison of the fused Alzheimer’s disease brain images. (a) PET image (b) MRI image (c) IHS&RIM (d) IHS&LG+ (e) IHS&LG++ 
with p=0.15,w=0.6 
Tables 1-3 list the data of spectral discrepancy, overall performance, and mutual information between the images fused by 
IHS&RIM, IHS&LG+, IHS&LG++, respectively, and the source multi-spectral image. From Table 1, it is clear that all three images 
fused by IHS&LG+ have less spectral discrepancies than images fused by IHS&RIM, and all three images fused by IHS&LG++ 
have the least spectral discrepancies. Similarly, Table 3 shows that all three images fused by IHS&LG+ have smaller overall 
performance values than images fused by IHS&RIM, and all three images fused by IHS&LG++ have the least overall performance 
values. Table 2 shows that all three images fused by IHS&LG++ have the largest mutual information values. Images 2 and 3 fused 
by IHS&LG+ have larger mutual information values than the images fused by IHS&RIM, but the MI of image 1 fused by 
IHS&LG+ is slightly smaller than that of image 1 fused by IHS&RIM. 
Table 1. The spectral discrepancies between the fused image and the PET image 
Fusion method Dataset 1 Dataset 2 Dataset 3 
IHS+RIM 7.7061 7.9031 6.0118 
IHSLG+ 6.5838 7.7461 5.7270 
IHSLG++(p=0.05,w=0.7) 4.6002 4.7612 4.5666 
IHSLG++(p=0.10,w=0.7) 4.7345 4.9118 4.9916 
IHSLG++(p=0.15,w=0.6) 4.6744 4.8017 5.1621 
Table 2. Performance comparison based on overall performance 
Fusion method Dataset 1 Dataset 2 Dataset 3 
IHS+RIM 2.3457 1.6104 0.9765 
IHSLG+ 1.2594 1.6585 0.8580 
IHSLG++(p=0.05,w=0.7) 0.3968 0.8597 0.5860 
IHSLG++(p=0.10,w=0.7) 0.3223 0.8100 0.5823 
IHSLG++(p=0.15,w=0.6) 0.2594 0.7787 0.7086 
Table 3. Performance comparison based on mutual information 
Fusion method Dataset 1 Dataset 2 Dataset 3 
IHS+RIM 0.6541 0.6551 0.623 
IHSLG+ 0.6484 0.7517 1.1188 
IHSLG++(p=0.05,w=0.7) 0.8817 0.9426 1.2188 
IHSLG++(p=0.10,w=0.7) 0.8828 0.9413 1.2141 
IHSLG++(p=0.15,w=0.6) 0.8799 0.9340 1.2027 
5. CONCLUSION 
In this paper, we presented two methods for PET and MRI images fusion based on IHS and log-Gabor wavelet. In the first 
method IHS&LG+, both PET and MRT images are decomposed into high and low activity regions. The intensity channels of high 
and low activity region images are then decomposed to high and low frequency coefficients using log-Gabor wavelet transform with 
different scale factors for high and low activity regions, respectively. The high frequency coefficients of each activity region in the 
MRI image and the low frequency coefficients of each activity region in the PET image are inverse log-Gabor transformed to result 
in a fused intensity. Finally the fused intensity along with the hue and saturation components of the PET image is inverse IHS 
transformed to result in a fused image. In the second method IHS&LG++, adjustments to the fused intensity at various pixels is 
applied for further recuing color distortion and enforcing anatomical structure. The experimental results in normal axial, normal 
coronal, and Alzheimer’s disease brain images demonstrate that all three images fused by IHS&LG+ are with less color distortion 
and about the same structural information as the images fused by IHS&RIM, and all three images fused by IHS&LG++ are with 
both color and anatomical structural information closest to PET and MRI images both visually and quantitatively. 
REFERENCE 
[12] Chen, Y., Blum, R.S., 2005. Experimental Tests of Image Fusion for Night Vision. International Conference on Information Fusion (7th), 
pp.491-498. 
[13] Chibani, Y. Houacine, A., 2003. Redundant versus orthogonal wavelet decomposition for multisensor image fusion. In Pattern 
Recognition, Vol.36, pp.879-887. 
[14] Daneshvar, S, Ghassemian, H, 2010. MRI and PET image fusion by combining IHS and retina-inspired models. In Information Fusion, 
Vol.11, pp.114-123. 
[15] Fischer, S. et al, 2007. Self-Invertible 2D Log-Gabor Wavelets. In International Journal of Computer Vision, Vol.75, No.2, pp.231-246. 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/29
國科會補助計畫
計畫名稱: 多型態牙齒X光片自動化身份辨識系統之研製
計畫主持人: 林芬蘭
計畫編號: 99-2221-E-126-005- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
