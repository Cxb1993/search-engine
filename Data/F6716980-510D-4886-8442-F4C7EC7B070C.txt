 2
____________________________________ 
行政院國家科學委員會專題研究計畫期末報告 
由部落格自動偵測興趣並建議購物願望清單 
Interest Detection and Wish-List Suggestion from a Blog Space 
計畫編號：NSC 96-2221-E-019 -038 -MY2 
執行期限：96 年 08 月 01 日至 98 年 07 月 31 日 
主持人：林川傑 國立台灣海洋大學資訊工程學系 
計畫參與人員： 陳威宇、詹嘉丞、郭育旻、趙品銜、徐榮彬、
劉奕緯、楊宗豫 
 
一、中文摘要 
 
部落格 (blog) 已是這幾年來網路上最
熱門的話題，近幾年來已有大量的網路使用
者投入部落格的建立和寫作風潮之中。而且
學術研究中，如何將技術應用在部落格上，
獲取有利的資訊，也成為熱門研究題目。畢
竟能掌握大多數的網路使用者，就能掌握網
路市場先機。 
本計畫擬以兩年的時間，研究如何由一
個部落格網站中包含的各項資訊，包括部落
格主人的個人基本資料、他所發表的在部落
格上的文章等等，自動為這位部落格主人建
構出他的購物願望清單，亦即他在近期內可
能希望購買或有興趣購買的商品清單。這種
自動建議願望清單的系統，不論對使用者或
是業者都有幫助。而發展過程中所需克服的
技術，也對學術研究有所貢獻。 
 
關鍵詞：部落格、願望清單、興趣偵測、自
然語言處理 
 
Abstract 
 
Blog (weblog) has been a hot topic in 
recent years. The number of users building 
their own blogs has grown rapidly. It is also hot 
to apply technologies on blogs to develop 
useful systems. Large amount of users and 
huge amount of data means a great opportunity 
in the market on the Internet. 
This two-year project plans to study how 
to automatically suggest a wish list for a 
blogger by all kinds of information available in 
his blog, including personal profile, contents of 
articles in his blog, and so on. A wish list is a 
list of products for which he may like to buy 
recently. Such a wish-list-suggestion system is 
useful both for bloggers and companies. 
Research for developing such a system is also 
helpful for researchers. 
 
Keywords: blog, wish list, interest detection, 
natural language processing 
 
二、緣由與目的 
 
部落格 (blog) 已是這幾年來網路上最
熱門的話題。只要你有電腦，能連接上網際
網路，會打字，你就可以在網路世界的一個
角落盡情寫作，分享自己的生活點滴、專業
興趣、心情及創作。而龐大的網友族群皆可
來閱讀你的部落格，和你做經驗交流。更可
將別人的文章引用至自己的部落格上，加速
了部落客 (blogger, 部落格的主人) 之間的
串連，更讓這股寫作與經驗分享的風潮帶到
最高點。 
根據美國部落格搜尋引擎Technorati1指
出，部落格以每天新增 2 萬 3,000 個的驚人
速度成長著。美國網路調查機構Perseus也指
出，全球部落格的數量 5 年內數量成長超過
1,000 倍，預估全球部落格數量到 2006 年年
底將達到 5,340 萬個。而在臺灣最大的兩個
部落格空間，無名小站2宣稱有 300 萬以上的
會員，Xuite3也早在去年中突破 100 萬個使用
者。由此可知，部落格使用者數量不可小覷。 
也因此，部落格一向為商業界所注意。
龐大的使用人群象徵著龐大的商機。然而要
1 http://www.technorati.com/search/ 
2 http://www.wretch.cc/ 
3 http://www.xuite.net/ 
 4
____________________________________ 
「ASUS」、「Acer」等各廠牌。 
(四) 過細的子分類：像是「T 恤」還分成「短
袖 T 恤」、「長袖 T 恤」、「連帽 T 恤」。
我們認為興趣與商品的關聯應該不會分
到這麼細，因此希望能刪除掉這種過細
的子分類。 
(五) 其他類：有的類別標籤是與「其他」同
義，像是「其他主機板」、「其他棒壘
球用品」，這也應該刪除。 
 
針對上述種種情形，我們採用了底下幾個方
法，期望能自動刪去大部份不要的字串，也
希望不會有誤刪的情形。 
 
(1) 所有包含 "其他" 這個子字串的類別名
稱全部刪除，例如「其他主機板」、「其
他棒壘球用品」。 
(2) 當一個其他類標籤在整個商品分類架構
中出現過 5 次以上，它在各層的兄弟類
別也全部刪除。例如「其他廠牌」出現
了 24 次，它的兄弟節點們正是各個廠牌
名稱，就可依此刪除。材質等節點亦然。 
(3) 若有一個父節點的子節點們有半數以上
是以英數字開頭的，就將這些子節點刪
去。這方法可以刪去如「1GB」、「2GB」
這種規格字串。 
(4) 若有一個父節點的子節點們與父節點的
名稱常有相同的結尾字串，就將這些子
節點刪去。這方法可以刪去如「短袖 T
恤」、「長袖 T 恤」這種過細的子分類。 
(5) 有一些特殊類別的子分類也全數刪除。
這些特殊類別常有大量非商品型的子分
類，又不易以前述方式刪除。目前以人
工找出 9 類，包括「圖書與雜誌」(子分
類均是書籍內容的領域像「自然科學」、
「藝術」)、「房屋出售」(子分類均是地
名像「新竹」、「台南」) 等等。 
(6) 所有被刪除的節點，其子節點也一併刪
除。 
 
還有一種情形需要處理。在奇摩拍賣的分類
中，有些標籤其實包含了兩種以上的商品。
例如「電子字典/翻譯機」或「汽車與機車」。
有些合併情形較為複雜，像「立燈(落地燈)」
應該被拆成「立燈」和「落地燈」兩個字串，
但是「床罩(單)」則應被拆成「床罩」和「床
單」。 
經過觀察後，我們整理出一些常見的分
隔字串來切分，包括「/」、「、」、「與」
以及左右圓括弧等，以程式自動地將這些標
籤拆成多個商品字串。為避免切分錯誤帶來
的困擾，長度僅一個中文字的字串都會刪去。 
最後得到 1,342 筆不同的商品名稱。提
出以上的自動處理方式，目的是希望未來也
能直接套用在一個不同的商品分類架構上，
得以快速地整理出所需的商品列表。 
 
3.2 建立興趣資料庫 
 
由於人類的興趣非常廣泛，同樣地很難
以人力窮舉出所有的興趣字串。在缺乏高品
質的興趣類別資來源時，我們打算先觀察人
類會如何描述自己的興趣。 
在許多部落格網站中，通常都會提供部
落格主人的個人基本資料，其中常包含興趣
欄位。我們於是嘗試收集部落客所使用的興
趣描述來做為候選集。 
我們拜訪了無名小站的部落格5空間，分
析了 2,386 位部落客的個人名片。名片有三
個欄位和興趣有密切關係，分別是「興趣」、
「喜歡的」和「討厭的」。部落客在填寫這
些欄位時並不嚴謹，有的欄位保留空白，更
有興趣與喜歡事物交錯填寫的情形。統計結
果，大約僅有 72%左右的人會填寫興趣和喜
歡事物欄位。 
上述步驟所得到的字串，是在一個欄位
中的整個字串，像是 
 
看勝運ˋ看電影ˋ看文章ˋ逛街 
☆打扮 ☆shopping ☆減肥 
每天睡覺睡到自然醒~~~ 
我討厭ㄉ都是你喜歡的...= = 
交朋友囉~只要能夠認識你就很開心囉~ 
 
這樣的字串，裡面包含多個興趣或物品，用
各式各樣奇怪的分隔方式隔開。有的敘述是
將興趣放在句子中說出，像是「交朋友囉」
多了語氣詞「囉」，「每天睡覺睡到自然醒」
其實就是指「睡覺」。有的敘述甚至和興趣
無關，像是「只要能夠認識你就很開心囉」
5 http://www.wretch.cc/blog/ 
GoogleCount 
snippet 
圖一、Google 查詢結果範例 
將興趣 x 至搜尋引擎查詢所得到 M 段
snippets 組合成該興趣 x 的上下文資訊，商品
y至搜尋引擎查詢所得到N段 snippets組合成
該商品 y 的上下文資訊。我們希望以興趣 x
的上下文資訊與商品 y 的上下文資訊的相似
度來代表興趣 x 與商品 y 的關聯度。 
串、所有商品字串，以及所有興趣及商品配
對的字串，都變成查詢句至 Google 搜尋引擎
查詢，並以其回傳結果的預估網頁個數做為
數據，如圖一圓框所示之 GoogleCount。查詢
時會將選項調成完整字串比對，以避免查詢
字串被斷開成數個詞造成查詢錯誤。 
 底下介紹以向量空間模型計算相似度的
方法，並且介紹兩大類詞彙權重計算公式。 4.2 上下文資訊相似度 
  
4.2.1 向量空間模型 另一種想法是，一個興趣與一個商品高
度關聯時，不見得一定要常常同時出現在文
章中。他們個別出現的文章領域應該會很相
似，這就給了我們利用上下文資訊計算相關
度的想法。 
 
IR中最常被使用的文字相似度計算方式
是建立向量空間模型 (vector space model)，
並以 cosine similarity (向量夾角餘弦值) 做
為相似度計算公式。將所有已知詞彙各視為
向量空間的一個維度，定義每個詞彙 termi
在一篇文章 d 之中的權重值 (term weight)，
就可將文章 d 表達成空間中的一個向量，其
投影在各維度的大小即各詞彙在文章 d 中的
權重。 
在估算一對興趣和商品之間的關聯度
時，若是手邊有大量文件集合，我們可以從
中找出出現興趣的文件集和出現商品的文件
集，再以文件之間的相似度來評量文件是否
屬於同一主題領域，進而估算這對興趣和商
品的關聯度。 
兩篇文章的相似度 cosine similarity 公式
定義如下： 
然而若沒有此類大型文件集，利用搜尋
引擎回傳結果網頁中的簡短摘要也是可行，
範例如圖一中方框所示的 snippets。簡短摘要
通常為該網頁中包括所查詢關鍵詞的文句片
段，因此可做為關鍵詞上下文資訊的來源。 
 




t
j ki
t
i ji
t
i kiji
kj
kj
ww
ww
dd
dd
1
2
,1
2
,
1 ,,  
 6
 8
個
人喜
用者不夠多的問題，以
至於
4.4 聯度實驗結果及討論 
一組實驗是計算詞彙共現度和交互資
訊量
組實驗是先蒐集各興趣及各商品的
上下
網站的標籤。
以各
4.4.2 效能評估方式 
有什麼樣興趣的人應該推薦給他何種
商品
的效
種評估方式是先請人建立參考標準
答案
估方法需要建立參考標準答
案。
是召回率 (recall, R)、精
確率
出了各個實驗的系統結果與參考
答案
策略
都是向量空間模型，詞彙權重分別是
 
表四、興趣商品關聯度效能評估 
策略 F 
Delicious 以及黑米書籤標記的對象是
愛的網頁，而且大部份人都會用較短的
字串做為標籤，確實較符合我們實驗的需
求。可惜 Delicious 使用者來自世界各地，標
籤以英文為主，中文標籤的量不若預期的
多。此外，若是以程式自動讀取，又有被視
為網路攻擊而被拒絕的危險。最後我們僅抓
取了與各興趣共同出現的最高頻標籤 20
個，再看是否有商品字串出現其中，大大減
低實驗的成功率。 
黑米則似乎有使
查詢結果資訊量也偏少。有超過 20 個興
趣不曾成為標籤，超過半數不曾與商品字串
共同成為標籤。所以實驗結果無法反應這方
法的真正效能。 
 
關
4.4.1 實驗資料 
 
第
，需要查詢包含各興趣與各商品的網頁
數量，這裡直接使用 Google 的搜尋服務即可
獲得。 
第二
文資訊，再建立向量空間模型來計算兩
段上下文資訊之間的相似度。同樣地我們使
用 Google 的搜尋服務，以查詢結果網頁所提
供的簡短摘要做為上下文資訊。每個查詢 
(興趣或商品) 最多都蒐集前 1,000 名，以實
際 Google 的回傳結果為準。 
第三組實驗蒐集的是社群
興趣做為查詢關鍵詞，分別查詢
Delicious 以及黑米書籤，得到與該興趣共同
出現的其他標籤名稱。黑米書籤蒐集得
115,824 筆，但是其中只有 242 個標籤是商品
字串。由 Delicious 蒐集所得的標籤總數則為
29,906 筆，有 1,006 筆是商品字串。 
 
 
 
具
，在經過幾次不斷地討論後，發現結果
可能因人而異。也就是說，關聯度中等或較
低的商品，會不會出現在購物清單中會和個
人的興趣強烈程度有關。如果此人強烈投入
該興趣，那麼所有相關商品他都有購買機
會；然而若他只是具有一般興趣而已，就只
有高度相關的商品能刺激他的購買慾望。 
為了研究目的，我們還是提出兩種不同
能評估方式，嘗試由不同面向來探討這
件事。 
第一
，以召回率 (recall) 和精確率 (precision)
做為評估公式。第二種則是改建立興趣商品
間的關聯強度，再以系統結果的平均關聯強
度來評估。 
第一種評
標記者先熟記所有興趣以及所有商品，
再針對每個興趣，挑出他認為關聯度較高的
商品，數量不限。 
第一種評估公式
 (precision, P) 和 F-measure (F)。針對一
個興趣 x 來看，令參考答案提出了 a 個對應
商品，系統提出了 b 個對應商品，兩者的交
集個數是 c。召回率定義為 c / a，意即參考答
案中有多少比例能被系統找到。精確率定義
為 b / a，意即系統提出的答案中有多少比例
是正確的。F-measure 為兩者的調和平均數
2PR/(P+R)，因為召回率和精確率常是一高
一低互相影響，不易只看單一指標來比較系
統好壞。 
表四列
比較的 R, P, F 效能評估。因為各組實驗
都需要設定提出答案的門檻值，相關分數高
於門檻值的商品才會被建議。表四中的數據
已經是各組實驗 F-measure 值最好的情形，
最後一欄是這時所需設定的門檻值。 
在表四的實驗數據中，三種最好的
etf’idf、(e1)tf’idf 和 log-likelihood value，三
者效能差不多。書籤的效果比單純計算網頁
頻率要好，可惜因為統計資料量太少，無法
達到預期的效果。 
 
 
 門檻值 P R 
共現度 3.41 4.1913200 5.41
MI 0 5.24 0.63 1.13
VSM1 0.58742 19 6.01 4.31 6.79
 10
5.1.2 興趣字串比對 
一類做法是計算各興趣在文章中的出
現頻
5.1.3 實驗測試資料 
驗資料的部落格文章選自無名小站的
部落
現代
站部落格的網頁版面中，並無法
直接
其
一是
者的所有部
落格
5.1.4 部落格文章與興趣關聯度評估 
於無法請使用者本人來協助評估文章
相關
部落格文章與興趣關聯度效能評
估的
 
表六、部落格文章與興趣關聯度效能評估 
 
 
 
另
率，直接在文章中比對興趣字串的出
現。此外，在第 3.2 節中提過，某些顯然同
義的興趣字串已經被分群，算成同一種興
趣。在現在這個做法中，同一種興趣的不同
寫法都會至文章中搜尋，頻率則加總做為這
一種興趣的關聯度分數。 
 
 
 
實
格空間。在第 3.2 節曾提到，已經蒐集
了 2,386 位部落客的無名帳號和個人資料。
下一步是希望由這些人之中選出一些人出來
下載他們的文章以進行實驗，並且希望他們
以填寫問卷的方式協助評估系統的效能。 
可惜的是，大部份人都沒有回信。而且
人使用部落格，多是抒發心情的日記功
能。對於這類較穩私的文章被公開實驗，都
較有疑慮。於是我們只能以模擬的方式來進
行評估。 
無名小
得知該位部落客總共曾發表過多少文
章。首先先拜訪各使用者部落格首頁的「所
有文章列表」頁面，即可取得該使用者所發
表過的文章的鏈結網址。如此也可以統計各
使用者發表文章的篇數，選取使用者時可在
頻繁使用者和低頻使用者間取得平衡。 
下載部落格文章會碰到兩種因難處。
鏈結失效的問題，因為大量下載文章仍
是耗時耗事的工作，過久的文章在實驗期間
會被作者移除。其二同樣是網站對於程式下
載的限制，無名仍會以疑似病毒攻擊為理由
拒絕頻繁及大量的下載行為。 
最後我們下載了 31 位使用
文章，發表篇數從低於 10 篇到高於 400
篇的人都有。文章總數是 1,849 篇。 
 
 
 
由
興趣的建議效能，實驗改採標記者模擬
的方式來進行評估。標記者在熟記各種興趣
後，閱讀每一篇文章，並且依一般常理推論
該篇文章的可能相關興趣。標記者並不知道
文章的作者是誰，各作者的文章也分散呈
現，以避免標記者學得作者習慣而使標記產
生偏頗。 
表六是
實驗數據。第一欄表示系統對於每篇文
章提出排名前 N 名的興趣的個數，其他欄位
則是參考答案中有多少比例被系統提出來。
三個實驗分別是 VSM5、VSM6 以及字串比
對方式所得的結果。 
N VSM5 VSM6 字串比對 
1 03.37 03.37 36.96 
2 05.40 05.06 52.14 
3 06.75 07.43 58.75 
4 09.45 10.13 59.92 
5 12.50 12.50 61.08 
6 14.86 13.51 61.86 
7 16.89 15.20 63.03 
8 17.22 15.87 63.42 
9 18.91 17.22 63.42 
10 19.59 18.91 63.81 
 
由表六可知，VSM5 和 VSM6 效能仍是
差不
5.2 測作者興趣 
用第 5.1 節所介紹的系統為一位部落
客的
(1) 每篇文章找出前 K 名相關興趣，每個興
K 名相關興趣，興趣的
多，不過 VSM5 略高。光以字串比對方
式就能有六成的召回率，是不錯的策略。第
5.2 節會提到這兩種策略可在不同情形找到
對應的興趣。 
 
偵
 
利
每篇文章找出相關的興趣後，這位部落
客的個人興趣就可以由他的文章來投票決
定。投票的方式有以下幾種方式： 
 
趣的出現都算一票，以票數來代表各興
趣的重要性。 
(2) 每篇文章找出前
權重依照名次遞減。以累積的權重來代
表各興趣的重要性。 
 12
是以研究的角度來看，仍可實驗各階
段各
察
部落
呈現給使用
者，
估，
消費網站合作，這點可
做為
四、計畫成果自評 
體而言，研究內容已完成大部份計畫
所列
、參考文獻 
 Zhou and Minyi Guo (2009). "A 
Kol
Kum
Mis
Mis
Ni,  
Oka irotake Abe and Kazuhiko Kato (2006). 
Ray ul and Roger Garside (2000). "Comparing 
Sah an (2006). 
Ten  and Hsin-Hsi Chen (2006). "Detection 
 
 
若
種策略的組合，或是採用多策略投票等
方式，看看何種組合能達到最好效能。 
購物清單的最好評估方式，是直接觀
客的購買行為。如果能夠與消費網站結
合，分析各部落客實際購買或是瀏覽過的商
品，就可以比對本系統所提的購物清單是否
符合部落客過往的購物經驗。 
而另一方向則是將購物清單
或者將購物清單所建議分類的待售商品
廣告給使用者，再去評估使用者是否因為購
物清單而購買了不在近期購物經驗的商品。 
由使用者自行依自己的購買意願來評
也是一種可行的方式。不過因為購物的
慾望常常是潛在的，消費者常會因為提示 
(例如廣告) 而激起購物需求。所以自我評量
不一定代表真實心理狀態。可惜現階段沒有
找到協助實驗者。 
可知評估需要與
未來規劃進行的研究方向。 
 
 
整
的工作項目，且已建立可直接應用之資
料庫。檢討中亦提出未來與業界合作研究的
可能性，現階段所建立的實驗環境也適合下
一年度的實驗進行。 
 
五
Guan, Hu, Jingyu
Class-Feature-Centroid Classifier for Text 
Categorization." In Proceedings of WWW 2009, 
p201-210. 
ari, Pranam, Tim Finin, and Anupam Joshi (2006) 
“SVMs for the Blogosphere: Blog Identification and 
Splog Detection,” Proceedings of AAAI Spring 
2006 Symposia on Computational Approaches to 
Analyzing Weblogs, 92-99. 
ar, Ravi, Jasmine Novak, Prabhakar Raghavan, and 
Andrew Tomkins (2003) “On the Bursty Evolution 
of Blogspace,” Proceedings of the 12th international 
conference on World Wide Web (WWW 2003), 
Budapest, Hungary, 568-576. 
hne, Gilad, David Carmel, and Ronny Lempel (2005) 
“Blocking Blog Spam with Language Model 
Disagreement,” Proceedings of the First 
International Workshop on Adversarial Information 
Retrieval (AIRWeb '05), Chiba, Japan. 
hne, Gilad and Maarten de Rijke (2006) “Deriving 
Wishlists from Blogs - Show Us Your Blog and 
We'll Tell You What Books to Buy,” Proceedings of 
the 15th international conference on World Wide 
Web (WWW 2006), Edinburgh, Scotland, 925-926. 
Xiaochuan, Xiaoyuan Wu and Yong Yu (2006).
"Automatic Identification of Chinese Weblogger 
Interests Based on Text Classification."  In 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
p.247-253. 
, Mizuki, H
"Extracting Topics From Weblogs Through 
Frequency Segments." In Proceedings of WWE 
2006. 
son, Pa
Corpora using Frequency Profiling." In Proceedings 
of the 2000 IEEE/WIC/ACM International 
Conference on Web Intelligence, p.1-6. 
ami, Mehran and Timothy D. Heilm
"Web-base Kernel Function for Measuring the 
Similarity of Short Text Snippets." In Proceedings of 
WWE 2006. 
g, Chun-Yuan
of Bloggers' Interest: Using Textual, Temporal, and 
Interactive Features." In Proceedings of the 2006 
IEEE/WIC/ACM International Conference on Web 
Intelligence, p.366-369. 
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
Overview of the NTCIR-7 ACLIA Tasks:  
Advanced Cross-Lingual Information Access 
  
Teruko Mitamura*   Eric Nyberg*   Hideki Shima*   Tsuneaki Kato†    
Tatsunori Mori‡  Chin-Yew Lin#   Ruihua Song#   Chuan-Jie Lin+  
Tetsuya Sakai@   Donghong Ji◊   Noriko Kando**  
*Carnegie Mellon University   †Tokyo University   ‡Yokohama National University 
#Microsoft Research Asia   +National Taiwan Ocean University  @NewsWatch, Inc.   
◊Wuhan University  **National Institute of Informatics 
teruko@cs.cmu.edu 
 
 
Abstract 
This paper presents an overview of the ACLIA 
(Advanced Cross-Lingual Information Access) task 
cluster.  The task overview includes: a definition of and 
motivation for the evaluation; a description of the 
complex question types evaluated; the document sources 
and exchange formats selected and/or defined; the 
official metrics used in evaluating participant runs; the 
tools and process used to develop the official evaluation 
topics; summary data regarding the runs submitted; and 
the results of evaluating the submitted runs with the 
official metrics. 
 
1.  Introduction 
 
Current research in QA is moving beyond factoid 
questions, so there is significant motivation to evaluate 
more complex questions in order to move the research 
forward. The Advanced Cross-Lingual Information 
Access (ACLIA) task cluster is novel in that it evaluates 
complex cross-lingual question answering (CCLQA) 
systems (i.e. events, biographies/definitions, and 
relationships) for the first time. Although the QAC4 task 
in NTCIR-6 evaluated monolingual QA on complex 
questions, no formal evaluation has been conducted in 
cross-lingual QA on complex questions in Asian 
languages until now. 
As a central problem in question answering 
evaluation, the lack of standardization has been pointed 
out [1], which makes it difficult to compare systems 
under a certain condition. In NLP research, system 
design is moving away from monolithic, black box 
architectures and more towards modular architectural 
approaches that include an algorithm-independent 
formulation of the system’s data structures and data 
flows, so that multiple algorithms implementing a 
particular function can be evaluated on the same task. 
Following this analogy, the ACLIA data flow includes a 
pre-defined schema for representing the inputs and 
outputs of the document retrieval step, as illustrated in 
Figure 1. This novel standardization effort made it 
possible to evaluate cross-lingual information retrieval 
(CLIR) task called IR4QA (Information Retrieval for 
Question Answering) in a context of a closely related 
QA task. During the evaluation, the question text and 
QA system question analysis results were provided as 
input to the IR4QA task, which produced retrieval 
results that were subsequently fed back into the end-to-
end QA systems. The modular design and XML 
interchange format supported by the ACLIA architecture 
make it possible to perform such embedded evaluations 
in a straightforward manner. More details regarding the 
XML interchange schemes and so on can be found on 
the ACLIA wiki [6]. 
 
 
Figure 1. Data flow in ACLIA task cluster 
showing how interchangeable data model 
made inter-system and inter-task 
collaboration possible.  
 
The modular design of this evaluation data flow is 
motivated by the following goals: a) to make it possible 
for organizations to contribute component algorithms to 
an evaluation, even if they cannot field an end-to-end 
system; b) to make it possible to conduct evaluations on 
a per-module basis, in order to target metrics and error 
analysis on important bottlenecks in the end-to-end 
system; and c) to determine which combination of 
algorithms works best by combining the results from 
various modules built by different teams. In order to 
evaluate many different combinations of systems 
effectively, human evaluation must be complemented by 
development of automatic evaluation metrics that 
correlate well with human judgment.  Therefore, we 
― 11 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
among participants and submission of results to be 
evaluated: 
 
• Topic format: The organizer distributes topics in 
this format for formal run input to IR4QA and 
CCLQA systems.  
• Question Analysis format: CCLQA participants 
who chose to share Question Analysis results 
submit their data in this format. IR4QA 
participants can accept task input in this format. 
• IR4QA submission format: IR4QA participants 
submit results in this format. 
• CCLQA submission format: CCLQA 
participants submit results in this format. 
• Gold Standard Format: Organizer distributes 
CCLQA gold standard data in this format. 
 
For more details regarding each interchange format, 
see the corresponding examples on the ACLIA wiki [6]. 
 
3.  CCLQA Task 
 
Participants in the CCLQA task submitted results for 
the following four tracks: 
• Question Analysis Track: Question Analysis 
results contain key terms and answer types 
extracted from the input question. These data are 
submitted by CCLQA participants and released to 
IR4QA participants. 
• CCLQA Main Track: For each topic, a system 
returned a list of system responses (i.e. answers 
to the question), and human assessors evaluated 
them. Participants submitted a maximum of three 
runs for each language pair. 
• IR4QA+CCLQA Collaboration Track 
(obligatory): Using possibly relevant documents 
retrieved by the IR4QA participants, a CCLQA 
system generated QA results in the same format 
used in the main track. Since we encouraged 
participants to compare multiple IR4QA results, 
we did not restrict the maximum number of 
collaboration runs submitted, and used automatic 
measures to evaluate the results. In the obligatory 
collaboration track, only the top 50 documents 
returned by each IR4QA system for each 
question were utilized. 
• IR4QA+CCLQA Collaboration Track 
(optional): This collaboration track was identical 
to the obligatory collaboration track, except that 
participants were able to use the full list of 
IR4QA results available for each question (up to 
1000 documents per topic). 
 
In the CCLQA task, there were eight participating 
teams (see Table 2), supplemented by an Organizer team 
who submitted simple runs for baseline comparison. The 
number of submitted runs is shown in Table 3 for the 
CCLQA main and Question Analysis tracks, and in 
Table 4 for the IR4QA+CCLQA collaboration tracks. 
 
Table 2. CCLQA Task Participants. 
Team Name Organization 
ATR/NiCT National Institute of Information and 
Communication Technology
Apath Beijing University of Posts & Telecoms 
CMUJAV Language Technologies Institute, Carnegie 
Mellon University
CSWHU School of Computer Science, Wuhan 
University
Forst Yokohama National University 
IASL Institute of Information Science, Academia 
Sinica
KECIR Shenyang Institute of Aeronautical 
Engineering
NTCQA NTT Communication Science Labs
Organizer 
(baseline)
ACLIA CCLQA Organizer
 
Table 3. Number of CCLQA runs submitted, 
followed by number of Question Analysis 
submissions in parenthesis. 
Team Name CS-CS EN-CS CT-CT JA-JA EN-JA 
ATR/NiCT 3 3    
Apath 2 (1) 1 (1)    
CMUJAV 3 (1) 3 (1)  3 (1) 3 (1) 
CSWHU 2 (3)     
Forst    1 1 
IASL 2  3   
KECIR 1 (1) 2    
NTCQA    2 1 
Organizer (baseline) 1 1  1 1 
Total by lang pair 14 (6) 10 (2) 3 7 (1) 6 (1) 
Total by target lang 24 (8) 3 13 (2) 
 
Table 4. Number of IR4QA+CCLQA 
Collaboration runs submitted for obligatory 
runs followed by optional runs in 
parenthesis. 
Team Name CS-CS EN-CS CT-CT JA-JA EN-JA
ATR/NiCT  6    
Apath 2 (2)     
CMUJAV 20 (20) 14 (14)  14 (14) 11 (11)
Forst     11 
KECIR (20) (18)    
NTCQA    (14)  
Total by lang pair 22 (42) 20 (32) 0 14 (28) 22 (11)
Total by target lang 42 (74) 0 36 (39) 
 
3.1 .  Answer Key Creation 
 
In order to build an answer key for evaluation, third 
party assessors created a set of weighted nuggets for 
each topic. A "nugget" is defined as the minimum unit 
of correct information that satisfies the information need. 
In the rest of this section, we will describe steps taken to 
create the answer key data. 
 
3.1.1 . Answer-bearing Sentence Extraction  
 
A nugget creator searches for documents that may 
satisfy the information need, using a search engine. 
During this process, a developer tries different queries 
that are not necessarily based on the key terms in the 
― 13 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
evaluating definition questions, and in the TREC 2006-
2007 QA tracks for evaluating "other" questions.  
A set of system responses to a question will be 
assigned an F-score calculated as shown in Figure 2. We 
evaluate each submitted run by calculating the macro-
average F-score over all questions in the formal run 
dataset. 
In the TREC evaluations, a character allowance 
parameter C is set to 100 non-whitespace characters for 
English [4]. We adjusted the C value to fit our dataset 
and languages. Based on the micro-average character 
length of the nuggets in the formal run dataset (see 
Table 5), we derived settings of C=18 for CS, C=27 for 
CT and C=24 for JA.  
 
Let 
r sum of weights over matched nuggets 
R sum of weights over all nuggets 
HUMANa  # of nuggets matched in SRs by 
human 
L total character-length of SRs 
C character allowance per match 
allowanc
e 
CaHUMAN ×  
 
Then 
recall  
R
r
=  
precision  
⎪⎩
⎪
⎨
⎧ <
= otherwise
if1
L
allowance
allowanceL
 
)(βF  
recallprecision
recallprecision
+×
××+
= 2
2 )1(
β
β  
 
Figure 2.  Official per-topic F-score definition 
based on nugget pyramid method. 
 
Note that precision is an approximation, imposing a 
simple length penalty on the SR. This is due to 
Voorhees’ observation that "nugget precision is much 
more difficult to compute since there is no effective way 
of enumerating all the concepts in a response" [5]. The 
precision is a length-based approximation with a value 
of 1 as long as the total system response length per 
question is less than the allowance, i.e. C times the 
number of nuggets defined for a topic. If the total length 
exceeds the allowance, the score is penalized. Therefore, 
although there is no limit on the number of SRs 
submitted for a question, a long list of SRs harms the 
final F score. 
The )3( =βF  or simply F3 score has emphasizes 
recall over precision, with the β  value of 3 indicating 
that recall is weighted three times as much as precision. 
Historically, a β  of 5 was suggested by a pilot study on 
definitional QA evaluation [4]. In the more recent TREC 
QA tasks, the value has been to 3. Figure 3 visualizes 
the distribution of F3 scores versus recall and precision. 
 
 
Figure 3.  F3 score distribution parameterized 
by recall and precision. 
 
As an example calculation of an F3 score, consider a 
question with 5 gold standard answer nuggets assigned 
weights {1.0, 0.4, 0.2, 0.5, 0.7}. In response to the 
question, a system returns a list of SRs which is 200 
characters in total. A human evaluator finds a conceptual 
match between the 2nd nugget and one of SRs, and 
between the 5th nugget and one of SRs. Then,  
 
39.0
7.05.02.04.00.1
7.04.0
=
++++
+
=recall  
24.0
200
242
=
×
=precision  
37.0
39.024.09
39.024.010)3( =
+×
××
==βF  
 
The evaluation result for this particular question is 
therefore 0.37.  
 
4.2 . Automatic Evaluation Metrics 
ACLIA also utilized automatic evaluation metrics for 
evaluating the large number of IR4QA+CCLQA 
Collaboration track runs. Automatic evaluation is also 
useful during developing, where it provides rapid 
feedback on algorithmic variations under test. The main 
goal of research in automatic evaluation is to devise an 
automatic metric for scoring that correlates well with 
human judgment. The key technical requirement for 
automatic evaluation of complex QA is a real-valued 
matching function that provides a high score to system 
responses that match a gold standard answer nugget, 
with a high degree of correlation with human judgments 
on the same task. 
The simplest nugget matching procedure is exact 
match of the nugget text within the text of the system 
response. Formally, the assessor HUMANa  in Figure 2 is 
replaced by EXACTMATCHa  as follows: 
 
),(Imax sna EXACTMATCH
Nuggetsn SRss
EXACTMATCH ∑
∈
∈
=      (1) 
 
⎩
⎨
⎧
=
otherwise:0
level text surfacein   contains:1
),(I
ns
snEXACTMATCH
    (2) 
― 15 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
the answer texts for the selected topic. The users type in 
the corresponding answer nugget and click “Add” to 
save the update.  
 
5.1.3 . Nugget Voting for Pyramid Method 
 
Figure 16 shows the nugget voting interface, which is 
used to identify vital nuggets from among the set of 
nuggets extracted using the nugget extraction tool.   (See 
details in Section 3.1.3). 
The user first selects a topic title from a list of 
previously completed titles in the Topic Development 
task. The user examines the topic data for the selected 
topic, and toggles the check boxes next to nuggets which 
they judge to be vital.  
 
5.2 . Download and Submission 
 
EPAN is used by each participant to upload their 
submission file for each run submitted. EPAN is also 
used to download intermediate results submitted by 
other participants, as part of an embedded evaluation, 
For example, ACLIA participants were able to 
download the results from Question Analysis and 
IR4QA in order to conduct an embedded CLIR 
evaluation. 
 
5.3 . Evaluation  
 
EPAN provides interfaces for supporting the core 
human-in-the-loop part of evaluation: relevance 
judgment for IR4QA and nugget matching for CCLQA. 
In each task, items to be evaluated belong to a pool 
created by aggregating the system responses from all 
systems, based on run priority. For the three runs 
submitted by each team in each ACLIA task, we created 
three pools of system responses. For the CCLQA task, 
the first pool (corresponding to run 1) was evaluated by 
independent third-party assessors hired by NII. The 
second and third pools (corresponding to runs 2 and 3) 
were evaluated by volunteers including members of the 
participant teams. Details of the CCLQA results are 
provided in Section 6.1. For the embedded IR4QA 
collaboration track, the system responses were evaluated 
automatically; details are provided in Section 6.2. 
 
 
 
6.  Evaluation Results  
 
In this section, we will present official evaluation 
results for the CCLQA main track, IR4QA collaboration 
track, and Question Analysis track. 
 
 
 
 
6.1 . CCLQA Main Track 
 
The official human evaluation results for CCLQA are 
shown in Table 8 through Table 12 for each language 
pair. Runs in Tables 13 through 17 were judged by 
volunteers including members of participant teams. We 
evaluated up to 50 system responses per run per 
question. 
Organizer runs are generated from a sentence 
extraction baseline system, sharing the same architecture 
as CMUJAV but with a minimally implemented 
algorithm that does not take into account answer types. 
The run has been motivated by the SENT-BASE 
algorithm introduced in TREC 2003 definition subtask 
as a baseline [4] that worked surprisingly well, i.e. 
ranked 2nd out of 16 runs. In the question analysis stage, 
the system translates the entire question string with 
Google Translate for crosslingual runs. Then, the system 
extracts all noun phrases as key terms. Subsequently in 
the retrieval stage, the system retrieves documents with 
Indri’s simplest query form, “#combine()”. Finally, in 
the extraction phrase, starting from the highest ranked 
document, the baseline system selects sentences that 
contain one of the key terms, until a maximum of 50 
system responses have been gathered. 
 
6.1.1 Official Runs 
 
Table 8. EN-CS official human evaluation. 
EN-CS Runs DEF BIO REL EVE ALL 
ATR/NiCT-EN-CS-01-T㩷 0.2216㩷 0.3158 0.2335㩷 0.1454 0.2211
CMUJAV-EN-CS-01-T㩷 0.2129㩷 0.2678 0.1884㩷 0.1346 0.1930
KECIR-EN-CS-01-T㩷 0.2493㩷 0.2563 0.1584㩷 0.1364 0.1895
Apath-EN-CS-01-T㩷 0.1694㩷 0.1165 0.1188㩷 0.0706 0.1140
Organizer-EN-CS-01-T㩷 0.1358㩷 0.1417 0.1052㩷 0.0793 0.1108
 
Table 9. CS-CS official human evaluation. 
CS-CS Runs DEF BIO REL EVE ALL 
CSWHU-CS-CS-01-T 0.4752 0.6012 0.4592 0.2662 0.4329
ATR/NiCT-CS-CS-01-T 0.2415 0.3376 0.2429 0.1430 0.2316
IASL-CS-CS-01-T 0.1536 0.3245 0.2548 0.1043 0.2034
CMUJAV-CS-CS-01-T 0.2326 0.2498 0.2321 0.1219 0.2027
Apath-CS-CS-01-T 0.1800 0.1662 0.2067 0.1298 0.1702
Organizer-CS-CS-01-T 0.1360 0.1248 0.1101 0.0640 0.1044
 
Table 10.  CT-CT official human evaluation. 
CT-CT Runs DEF BIO REL EVE ALL 
IASL-CT-CT-01-T 0.3020 0.4075 㩷 0.2509 㩷 0.1650 0.2666 㩷
 
Table 11.  EN-JA official human evaluation. 
EN-JA Runs DEF BIO REL EVE ALL 
CMUJAV-EN-JA-01-T 0.3772 0.1250 㩷 0.1641 0.0433 㩷 0.1627 㩷
Organizer-EN-JA-01-T 0.1938 0.1187 㩷 0.1253 0.0439 㩷 0.1133 㩷
Forst-EN-JA-01-T 0.1785 0.1403 㩷 0.1103 0.0516 㩷 0.1123 㩷
NTCQA-EN-JA-01-T 0.1699 0.0932 㩷 0.0476 0.0023 㩷 0.0676 㩷
 
Table 12.  JA-JA official human evaluation. 
JA-JA Runs DEF BIO REL EVE ALL 
CMUJAV-JA-JA-01-T 0.4201 0.1900 0.2332 0.0937 0.2201 㩷
NTCQA-JA-JA-01-T 0.2888 0.1788 0.2209 0.0915 0.1873 㩷
Organizer-JA-JA-01-T 0.2537 0.1527 0.1458 0.0916 0.1525 㩷
Forst-JA-JA-01-T 0.2313 0.1598 0.1161 0.0786 0.1366 㩷
― 17 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
 
7.  Further Analysis 
 
In this section, we present further analysis that was 
performed following the human and automatic 
evaluations of the run data. 
 
7.1 . Reliability of Automatic Evaluation 
 
This subsection discusses the correlation between the 
automatic and human evaluation metrics presented in 
Section 4.2, for three different nugget matching 
algorithms (see Table 23). 
 
Table 23.  Per-run and per-topic correlation 
between automatic nugget matching and 
human judgment 
Algorithm Token 
Per run 
( 40N = ) 
Per topic 
( 10040N ×= )
Pearson Kendall Pearson Kendall 
EXACTMATCH CHAR 0.4490 0.2364 0.5272 0.4054 
SOFTMATCH CHAR 0.6300 0.3479 0.6383 0.4230 
BINARIZED CHAR 0.7382 0.4506 0.6758 0.5228 
 
We compared per-run (# of data points = # of human 
evaluated runs for all languages) and per-topic (# of data 
points = # of human evaluated runs for all languages 
times # of topics) correlation between scores from 
human-in-the-loop evaluation and automatic evaluation. 
The Pearson measure indicates the correlation between 
individual scores, while the Kendall measure indicates 
the rank correlation between sets of data points.   
The results show that our novel nugget matching 
algorithm BINARIZED outperformed SOFTMATCH 
for both correlation measures, and we chose 
BINARIZED as the official automatic evaluation metric 
for the CCLQA task. 
The plots in Figure 4 compare the per-run human vs. 
automatic scores, measured using F3 with the different 
nugget matching algorithms. We can roughly observe 
that the distribution of points in the BINARIZED plot 
falls roughly between the distributions of 
EXACTMATCH and SOFTMATCH. 
The plots in Figures 5 and 6 compare the official 
human-in-the-loop and automatic metric scores using 
three different nugget matching algorithms. Scores 
generated by the human-in-the-loop and automatic 
metrics appear to correspond, although there are some 
exceptions such as can be seen in runs ATR/NiCT-CS-
CS-01-T, Apath-CS-CS-01-T, Forst-EN-JA-01-T and 
Forst-JA-JA-01-T. We plan to analyze the possible 
causes for these outlying data points. 
 
EXACTMATCH  
BINARIZED SOFTMATCH  
 
Figure 4. Per topic plot for Official (human-in-the-
loop) vs. three kinds of automatic metric scores.  
 











	

	

	



	

	
	


































































 !








 !






"
#


$







"
#


$








% 
&'
 
(
)
* 
+(
)
(&
+*
,
"
--
(

.)/

%'
0
0
 
!*
.&
&
1)
(&
+*
,
%0,
"--(.)2
 &- ' (!
30+4*5 +5' (!
 
Figure 5. Human-in-the-loop vs three automatic 
metric scores on CS CCLQA Main track data 
 






	

6


	

	

	



	

	












7


















"
#


$








2&
+,
 






"
#


$








2&
+,
 








7







 
% 
&'
 
(
)
* 
+(
)
(+
&*
,
"
--
(

.)/

%'
0
0
 
!*
.&
&
1)
(&
+*
,
%0,
"--(.)2
 &- ' (!
30+4*5 +5' (!
 
Figure 6. Human-in-the-loop vs three automatic 
metric scores (JA CCLQA Main track data)
― 19 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan






	







	

6





































































 !








 !






"

#


$







"

#


$








89
):
)&
-)
,
(
&+
*,
%0,
"--(.)2
 "--(.)*(..
"--(.);+*(,&0 89):)&-),
 
Figure 8. Official scores and Avg. # System 
Response for EN-CS and CS-CS runs.    
 

	

	

	



	

	
	

	

	

	



	













7



















"

#


$








2&
+,
 






"

#


$







2&
+,
 








7









89
):
)&
-)
,
(
&+
*,
%0,
"--(.)2
 "--(.)*(..
"--(.);+*(,&0 89):)&-),
 
Figure 9. Official scores and Avg. # System 
Responses for EN-JA and JA-JA runs. 
 
7.5 . Crosslingual vs Monolingual Runs 
 
To compare the distribution of scores between 
crosslingual and monolingual, we plotted official scores 
from all officially evaluated runs in Figure 12 and 
Figure 13 for CS and JA respectively. Interestingly, 
crosslingual (crosses) outperforms monolingual (circles) 
for many topics in CS, but the same is not true for JA. 
 
8.  Conclusion 
 
This paper presented an overview of the ACLIA 
(Advanced Cross-Lingual Information Access) task 
cluster at NTCIR-7, with a specific focus on the 
CCLQA evaluations.  We described the official metrics 
used in evaluating participant runs; the tools and process 
used to develop the official evaluation topics; summary 
data regarding the runs submitted; and the results of 
evaluating the submitted runs with the official metric. 
Novel aspects of the evaluation included cross-lingual, 
complex QA evaluation for Chinese and Japanese 
corpora, and an embedded evaluation of information 
retrieval technologies used for QA.  We utilized 
automatic evaluation metrics for the embedded 
evaluation, and analyzed both per-topic and per-run 
correlation between human-in-the-loop and automatic 
evaluation. We also analyzed performance on topics 
shared across the different language tracks, and found 
that for some topics, crosslingual QA performance was 
better than monolingual performance.  
We hope that the results of the NTCIR-7 ACLIA task 
will contribute to continued rapid progress in 
Information Retrieval and Question Answering. 
 
Acknowledgements 
 
We thank Jim Rankin (CMU) for helping with the 
development and maintenance of the EPAN tool set.  
We also thank Xinhua, Zaobao, CIRB and Mainichi for 
providing the corpora that were used for the evaluations. 
We also thank Fred Gey (UC Berkeley) Kui-Lam Kwok 
(Queens College) for their valuable advice. We also 
greatly appreciate the efforts of all the ACLIA 
participants.  
 
References 
 
[1] Lita, L.V. Instance-Based Question Answering. Doctoral 
Dissertation, Computer Science Department, Carnegie 
Mellon University. 2006. 
[2] Sakai, T., N. Kando, C.-J. Lin, T. Mitamura, H. Shima, D. 
Ji, K.-H. Chen, E. Nyberg. Overview of the NTCIR-7 
ACLIA IR4QA Task, Proceedings of NTCIR-7, 2008.  
[3] Lin, J., and D. Demner-Fushman. Will pyramids built of 
nuggets topple over?, Proceedings of the main conference 
on Human Language Technology Conference of the North 
American Chapter of the Association of Computational 
Linguistics. 2006. 
[4] Voorhees, E. M. Overview of the TREC 2003 Question 
Answering Track, Proceedings of TREC 2003, 2004. 
[5] Voorhees, E. M. Overview of the TREC 2004 Question 
Answering Track, Proceedings of TREC 2004, 2005. 
[6] The ACLIA wiki, http://aclia.lti.cs.cmu.edu 
[7]  Indri search engine: http://www.lemurproject.org 
[8] Lin, J., and D. Demner-Fushman. Methods for 
Automatically Evaluating Answers to Complex Questions, 
Information Retrieval, 9(5):565-587, 2006. 
[9] Asahara, M., Y. Matsumoto. Japanese Named Entity 
Extraction with Redundant Morphological Analysis, 
Proceedings of NAACL/HLT 2003, 2003. 
[10] Fukumoto, J., T. Kato, F. Masui, T. Mori. An Overview of 
the 4th Question Answering Challenge (QAC-4) at NTCIR 
Workshop 6, Proceedings of NTCIR-6, 2007. 
 
― 21 ―
Proceedings of NTCIR-7 Workshop Meeting, December 16–19, 2008, Tokyo, Japan
0.0 0.2 0.4 0.6 0.8 1.0
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
Official Score
To
pi
c
 
0.0 0.2 0.4 0.6 0.8 1.0
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
Official Score
To
pi
c
 
Figure 12. Official CS scores plotted for each 
topic where circles are for monolingual and 
crosses are for crosslingual runs. 
Figure 13. Official JA scores plotted for each 
topic where circles are for monolingual and 
crosses are for crosslingual runs. 
― 23 ―
