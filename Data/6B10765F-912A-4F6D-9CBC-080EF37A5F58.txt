transform could cover long-term and the short-term 
music features which can represent the time-varying 
behavior of music signals. In addition, the dynamic 
music frame analysis will be applied to train an 
optimized non-linear decision rule for music genre 
classifier based on support vector machine (SVM). 
Experimental results show that the proposed new music 
genre classification algorithm could achieve the 
average 98% classification accuracy rate for the six 
different music genres. Compared to other methods, 
these two new technologies developed in this project 
can determine the precise LSP frequencies and 
accurate music genre classification with the lowest 
computational complexity. It is worth to further 
implement these two technologies via DSP or VLSI 
architectures. 
英文關鍵詞： Line Spectral Pairs (LSP), Tschirnhaus Transform, 
Music Genre Classification. 
 
 2
行政院國家科學委員會專題研究計畫成果報告 
高鑑別性聲音特徵參數擷取演算法之SoC IP 設計 
SoC IP Design for Discriminative Sound Features Extraction Algorithms 
計畫編號：NSC 97-2221-E-366 -010 – MY3 
執行期限：98 年 8 月 1 日至 99 年 7 月 31 日 
主持人：陳璽煌    執行單位﹕樹德科技大學 資訊工程系 
 
中文摘要 
在本計畫執行期間內，主要完成了新型線頻譜對
(LSP)特徵參數快速演算法設計以及音樂曲風特徵分類等
兩大項研究工作，首先在新型線頻譜對特徵參數演算法
設計方面，本計畫完成了運用契爾恩豪斯(Tschirnhaus)轉
換實現 10 階版本線頻譜對新型快速演算法設計，藉由契
爾恩豪斯轉換來避開複數與根號求解等複雜運算，並利
用 k-means 演算法來建構一套低記憶體用量且具有高精準
度的三角函數對應表給契爾恩豪斯轉換使用。實驗結果
顯示，運用 k-means 演算法所建構的三角函數對應表可進
一步提高線頻譜對演算法之精確性，同時降低實作時記
憶體的使用量。其次，在音樂曲風特徵值的研究方面，
本計畫提出一項新型結合小波(Wavelet)轉換之動態音框
分析技術，可針對曲風多變的音樂達成不錯的分類效
果。實驗結果顯示，在搭配支撐向量機作為分類器的情
況下，可針對六種不同曲風的音樂達成 98%的分類準確
性。與其它演算法相比較，本計畫所完成的兩項新技術
均可運用最少的計算時間與運算成本便可完成高精確度
的線頻譜對係數求取與音樂曲風分類，非常適合進一步
研究將其運用至 DSP 或 VLSI 實作上。 
 
關鍵辭：線頻譜對、契爾恩豪斯轉換、音樂曲風分類 
 
Abstract 
This project developed two new technologies for the 
calculation of line spectrum pair frequencies and the 
classification of music genres during the project operation 
period. The first technology is the fast 10-order line spectrum 
pairs (LSP) frequencies calculation method. This method 
makes use of the k-means algorithm and Tschirnhaus 
transform to determine the accurate LSP frequencies. By the 
use of k-means algorithm, the proposed method can build up 
precise cosine lookup tables with the minimum memory size. 
The main advantage of the proposed method is that it has the 
rapid root determination of a quartic equation. This method 
can solve the given quartic equation without using complex 
number operations and hence result in a considerable 
computational saving. The second technology is the new 
music genre classification algorithm. This new music genre 
classification algorithm is implemented by the use of dynamic 
music frame analysis, wavelet packet transform, and support 
vector machine. The dynamic music frame analysis combined 
with wavelet packet transform could cover long-term and the 
short-term music features which can represent the time-
varying behavior of music signals. In addition, the dynamic 
music frame analysis will be applied to train an optimized 
non-linear decision rule for music genre classifier based on 
support vector machine (SVM). Experimental results show 
that the proposed new music genre classification algorithm 
could achieve the average 98% classification accuracy rate for 
the six different music genres. Compared to other methods, 
these two new technologies developed in this project can 
determine the precise LSP frequencies and accurate music 
genre classification with the lowest computational complexity. 
It is worth to further implement these two technologies via 
DSP or VLSI architectures. 
Index Terms—Line Spectral Pairs (LSP), Tschirnhaus 
Transform, Music Genre Classification. 
Part I. Calculation of LSP frequencies  
I. Introduction 
Line spectrum pairs (LSP) or line spectral frequencies (LSF) 
introduced by Itakura [1] are a very popular parameter set for 
representation of the linear predictive coding (LPC) speech 
model. Most of the current speech codecs and other speech 
processing related fields are based on LSFs or LSPs [7]. The 
LSP are determined from the direct form coefficients of the 
linear predictive analysis filter A(z). Generally, the LSP 
computation begins to decomposed a linear predictive 
analysis filter A(z) into a pair of symmetric and anti-
symmetric real characteristic polynomials. These two 
polynomials are called the LSP polynomials. If these two LSP 
polynomials have the characteristic of minimum phase system 
[2], then it can be shown that the roots of these polynomials, 
namely the LSP, are interlaced on the unit circle. For real-
time speech coding applications, the LSP calculation must be 
performed within 10 to 30 ms [9]. Therefore it is an important 
topic to develop a procedure that can reduce the complexity 
while can maintain accuracy of LSP calculation. 
Many methods have been proposed to compute the LSP 
frequencies [2, 3, 4, 8, 10]. For example, Soong and Juang [2] 
adopted a Discrete Cosine Transform (DCT) to evaluate the 
cosine functions on a fine grid using the bisection method. In 
addition, they considered it a better approach to solve LSP 
through closed-form formulas when a 4-degree polynomial is 
further reduced from those cosine functions. However, the 
above method needs large evaluations of trigonometric 
functions. Later Kabal and Ramachandran [3] presented an 
efficient method for LPC to LSP conversion. In [3], the LSP 
 4
and Problem P2 could be solved by 



 n
i li
n
i jili
jl
w
xw
q
1 ,
1 ,,
,
, for 1 l  k, 1 j  m.                      (10) 
Finally the basic iteration steps to solve problem P is 
summarized as follows: 
1. Choose an initial Q0 and solve ),( 0QWP to obtain w 0 . 
set t = 0. 
2. Let tWW ˆ  and solve ),ˆ( QWP  to obtain Q t + 1 . if 
),ˆ( tQWP = ),ˆ( 1tQWP , output Wˆ , tQ  and stop the 
iteration. Otherwise, go to step 3. 
3. Let 1ˆ  tQQ  and solve )ˆ,( QWP  to obtain wt + 1 . if 
)ˆ,( QWP t = )ˆ,( 1 QWP t , output tW , Qˆ  and stop the 
iteration. Otherwise, let t = t+1 and go to step 2. 
TABLE I 
Performance comparison of the cosine look-up tables with size of 32, 
64, and 128. 
Size n Iterative times Average estimated error  
n = 32 t = 646   = 0.007814 
n = 64 t = 499   = 0.003907 
n = 128 t = 397   = 0.001951 
Since P(, ) is non-convex and the sequence P(, ) 
generated by the above iterative algorithm is strictly 
decreasing, the k-mean algorithm will converge to a local 
minimum point after a finite number of iterations [11]. In this 
report, three look-up tables for cosine values with different 
sizes are built. These three different table sizes are 32, 64, and 
128. Because cosine function has symmetrical property, each 
look-up table covers radian range from 0 to /2. Table I 
shows the performances of these three look-up tables. 
The average estimated error   listed in table i is given by 
n
xx
x
k



2/
0
ˆcoscos

 , for x = 0 to /2.                  (11) 
where the resolution of x is /3600 and kxˆ  is selected from 
look-up table which |ˆ| kxx   is minimized. It is observable 
that the increase of table size will result in more accurate 
average estimated value. Furthermore, due to the constraint 
criterion is loose when the table size n is increased, the 
iterative time therefore will decrease from n = 32 to 128. 
IV. The Guideline to Solve 10-order LSP 
To evaluate 10-order LSP polynomials via Tschirnhaus 
transform, one has to derive a quartic equation from the 1st 
derivative of 5-degree LSP polynomials. Then the extremes of 
5-degree LSP polynomials can be found by applying 
Tschirnhaus transform to these 1st derivatives. Since LSP 
polynomials are in the range of [-1, +1] [3, 4], one can define 
initial value formula as 
2/)1(1 '11  xx FAbsInit                                    (12) 
3,,1for ,          
2/)( '' )1(
'
)1()1(

 
i
FFAbsFInit xiixixix            (13) 
2/)1(1 ' 45  xx FAbsInit                                (14) 
where Abs( ) means absolute value operation, '1xF , 
'
2xF , 
'
3xF , 
and '4xF  are the roots of the 1st derivative of 5-degree LSP 
polynomials. After determining initial values, the accurate 
LSPs, namely roots of LSP polynomials, can be obtained by 
using Newton method. Newton method is defined as: 
)(
)(
'1
n
n
nn xf
xfxx                                                   (15) 
where f(xn) is 5-degree LSP polynomial, f(xn) is the first 
derivative of f(xn), xn is present of value or initial value, xn+1 is 
the new value calculated by Newton method, n is the 
recursion number of Newton method. The convergence 
criterion of Newton method is defined to terminate the 
iterative procedure. Generally, the  value of size can be 
determine the accuracy of root, that is  
   1  nn xx                                                       (16) 
V. Simulation results 
The proposed algorithm was tested using 1000 speech 
files spoken by 50 males and 50 females. These speech files 
are selected from Aurora database [6] and the simulation 
program was implemented by VC++ on an Intel Pentium 
1.7GHz CPU. These test speech files were sampled at 8 kHz 
and the LPC vectors were calculated as shown in [2] with 30 
ms Hamming window, autocorrelation method, and 15 Hz 
bandwidth expansion. There are totally 88000 sets of 10-order 
LPC coefficients.  
 
Figure 1. Plot of EP0(x)=0 and EP0(x)=0. 
Figure 1 plots an example of 5-degree LSP polynomials 
where EP0(x) = x5  1.1192x4  0.4961x3 + 0.6355x2 + 0.0463x 
 0.0596 and EQ0(x) = x5  0.2484x4  0.9970x3 + 0.1892x2 + 
0.1553x  0.0261 are presented in solid line and dashed line, 
respectively. Starts and diamonds are denoted EP0(x) and 
EQ0(x) of initial value positions whereas circles and squares 
are shown EP0(x) and EQ0(x) of accurate rooting positions. The 
real root values of EP0(x) and EQ0(x) are {0.9584, 0.8135, 
0.3409, 0.3466, 0.6471} and {0.9411, 0.4471, 0.1637, 
0.4356, 0.8679}. 
 6
[2] F. K. Soong and B. H. Juang, “Line spectrum pair (LSP) 
and speech data compression,” in Proc. ICASSP-84, pp. 
1.10.1–1.10.4, Mar.1984. 
[3] P. Kabal and R. P. Ramachandran, “Computation of line 
spectral frequencies using Chebyshev polynomials,” IEEE 
Trans. Acoust., Speech, Signal Processing, Vol. ASSP-34, 
pp. 1419–1426, Dec 1986. 
[4] J. Rothweiler, “On polynomial reduction in the 
computation of LSP frequencies,” IEEE Trans. Speech 
Audio Processing, vol. 7, pp. 592–594, Sept. 1999. 
[5] See http://planetmath.org/encyclopedia/QuarticFormula. 
[6] See http://www.elda.org/article52.html. 
[7] W. C. Chu, Speech Coding Algorithm: Foundation and 
Evolution of Standardized Coders, Wiley-Interscience, 
2003.  
[8] S. H. Chen, Yaotsu Chang, and J. C. Ruan, “An efficient 
computation of LSP frequencies using modified complex-
free Ferrari formula,” Journal of Signal Processing 
Systems, Vol. 52, No. 2, pp.153-163, Aug. 2008. 
[9] D. Petrinovic, “Calculation of the line spectrum 
frequencies using the quotient-difference scheme,” in Proc. 
MELECON 2000, Vol. 2, pp. 831-834, 2000. 
[10] S. H. Chen; Yaotsu Chang; and C. J. Yu Syuan, “The 
computation of line spectrum pair frequencies using 
Tschirnhaus transform,” in Proc. ISCAS 2009, pp. 2333 - 
2336, 2009. 
[11] Zhexue Huang, “Extensions to the k-means algorithm for 
clustering large data sets with categorical values,” Data 
Mining and Knowledge Discovery, Vol. 2, pp. 283-304, 
1998. 
[12] J. MacQueen, “Some methods for classification and 
analysis of multivariate observations,” in Proc. the 5th 
Berkeley Symposium on Mathematical Statistics and 
Probability, pp. 281–297, 1967. 
Part II. Music genre classification 
I. Introduction 
With the rapidly expansion of digital music contents, the 
music information retrieval (MIR) system has been one of 
popular research topics in the past years. The automatic 
analysis of music signals is one of the most important parts of 
MIR [4]. In general, the first step of automatic music analysis 
is to make use of several characteristics that can capture the 
information about music content. Among these characteristics, 
music genre information is regarded as a principal one. The 
music genres are the top-level descriptors or labels used by 
music dealers and librarians for categorizing and describing 
the vast universe of music [2]. It can be used to describe 
music as well as to structure music database [3]. Usually, 
music genres have the properties that are related to the 
instrumentation and rhythmic structure of music. However, 
due to music genres do not have strict definitions, most of 
current musical genre annotations are performed manually [1, 
2]. It goes without saying that the manual classification of 
music genres is quite time-consuming and demanding. 
Therefore, many different approaches of automatic music 
genre classification have been proposed in recent years. 
Most of these proposed systems are based on pattern 
recognition techniques. Each music signal is represented by 
numerical features that are then used to train a music classifier 
via traditional statistical models such as Gaussian mixture 
models (GMM) and K-nearest neighbors (KNN) [2]. On the 
other hand, various content-based timbral features were 
proposed for music genre classification. These timbral 
features could be categorized into short-term and long-term 
features [2]. The short-term features, which can represent the 
spectrum of music, include spectral centroid, spectral rolloff, 
mel-frequency cepstral coefficient (MFCC), and etc. The 
long-term features, which can characterize either the variation 
of spectral shape or beat information, include low-energy [1], 
and beat histogram, and etc [2][5]. 
Based on the above discussions, this project proposed a 
novel content-based music genre classification method using 
dynamic music frame analysis combined with wavelet packet 
transform of timbral features and support vector machine 
(SVM). The proposed dynamic music frame analysis consists 
of both the long-term and short-term timbral features. They 
are log energy and MFCC with different frame length and 
overlap. In addition, the proposed method uses more recent 
powerful classification method, namely SVM, as a classifier 
instead of traditional statistical model. Various experimental 
results show that the use of SVM can perform significant 
improvements in music classification accuracy 
II. Wavelet packet transform 
)(kg
8
3jU
9
3jU
10
3jU
11
3jU
12
3jU
13
3jU
14
3jU
15
3jU
6
2jU
7
2jU
5
2jU
4
2jU
3
1jU
2
1jU
1)( jUxf 
)(kg
)(kg
)(kg
)(kg
)(kg
)(kg
)(kh
)(kh
)(kh
)(kh
)(kh
)(kh
)(kh
 
Figure 1. The structure of three-level wavelet packet 
transform 
Wavelets [12], [13] are a widely used technique that have 
also been applied to speech and audio feature extraction. The 
wavelet transform and wavelet packet transform discussed 
here are implemented via a filter bank structure. A wavelet 
packet transform was first introduced by Coifman et al. [14]. 
Figure 1 is the structure of three-level wavelet packet 
transform where )(kh and )(kg are the analysis lowpass and 
highpass filter. In addition, the symbol 2 denotes the down-
sampling by 2.   )()2( 1 naknhka i
n
i                                                (1) 
  )()2( 1 nakngkd i
n
i                                                (2) 
where  kai and  kdi are called the approximation and detail 
coefficients of the wavelet decomposition of  nai 1 , 
respectively. 
The idea of wavelet packet is the same as that of wavelet. 
The only difference is that wavelet packet offers a more 
complex and flexible analysis. In the wavelet packet, both of 
the details as well as the approximation are split. The 
spectrum characteristics of the wavelet packet tree for 3-level 
decomposition are shown in Figure 2. In this report, the 
 8
expanded version of speech sounds. Therefore, the dynamic 
music frame analysis used to represent music genre can be 
based on the features proposed for speech recognition or 
music-speech discrimination. This research selects two 
common usage features, namely MFCC and log energy, with 
different frame length as the timbral features for music genre 
classification. 
A. MFCC 
It is shown that MFCC can capture the acoustic 
characteristics for speech recognition, music classification, 
and other audio/speech related applications [7-8]. According 
to the previous psychophysical studies, human perception of 
the frequency content of sounds follow a subjectively defined 
nonlinear scale called the "mel" scale [9] defined as, 
)
700
1ln(1125mel
ff 
                                     
(6) 
where f is the actual frequency in Hz. This leads to the 
definition of MFCC and its calculation process is given as 
follows.  
Let s(n), n = 1~N, be a music signal frame that is pre-
emphasized and Hamming-windowed [8, 9]. First, the time 
domain signal, s(n), is transferred into frequency domain by 
an M point discrete Fourier transform (DFT). The resulting 
energy spectrum can be represented as 
2
1
2
2 )(|)(| 



 

M
n
M
nkj
enskS

                              
(7) 
where 1  k  M. Then, the triangular filter banks, whose 
frequency bands are linearly spaced in the mel scale, are 
imposed on the spectrum obtained in (2). The outputs e(l), l = 
1~Q, of the mel-scaled band-pass filters can be calculated by 
a weighted summation between respective filter response 
Hi(k), i = 1~M, and the energy spectrum |S(k)|2 as 



M
k
i kHkSie
1
2 )(|)(|)(
                   
(8) 
where Hi(k) is defined as 






















)1(
)1()(
)()1(
)1(
)()1(
)1()(
)1(
)1(
for,0
for,
)(
)(
for,
)(
)(
for,0
)(
ib
ibib
ibib
ib
ibib
ibib
ib
ib
i
fk
fkf
ff
kf
fkf
ff
fk
fk
kH
         (9) 
In (4), fb(i) are the boundary points of the filters and are 
depended on the sampling points Fs and the number of points 
N in DFT: 








 
1
(mel)low(mel)high
mel(low)
1
mel)( M
ff
iff
F
Nf
s
ib
.     (10) 
Here, fmel(low) and fmel(high) are respectively the low and high 
boundary frequencies for the entire filter bank. 1mel
f  is the 
inverse to (1) transformation, formulated as 



  


 1700 11251mel
melf
ef                                             (11) 
Figure 6 shows the original as well as normalized mel 
space triangular filter bank with M = 32. Finally, discrete 
cosine transform (DCT) is taken on the log filter bank 
energies, log[e(l)], and the MFCC coefficients Cm can be 
written as,  




 

 
1
0 2
12cos)]1(log[2
Q
p
m Q
pmpe
Q
C        (12) 
where 0  m  M-1.  
 
0 0.5 1 1.5 2 2.5
x 10
4
0
0.5
1
Frequency (Hz)
Triangular filter bank
0 0.5 1 1.5 2 2.5
x 10
4
0
0.005
0.01
0.015
Frequency (Hz)
Triangular filter bank (normalized)
 
Figure 6. Original (upper one) and normalized (lower one) 
mel-space triangular filter bank (M=32) 
The summary of MFCC calculation process is illustrates 
given in Figure 7. 
 
Figure 7. The block diagram of MFCC calculation process. 
B. Log Energy 
The log energy is usually cooperated with MFCC for 
applications of speaker recognition and audio segmentations 
[10]. The definition of log energy used in this report is 
defined in Equation (13). 


 

1
0
2][log
N
n
nsE
                                     
(13) 
 10
the Gaussian function, and parameter d is the degree of the 
polynomial. The performances of proposed music genre 
classification using these three kernel functions are given in 
Table III.  
TABLE I. The four common kernel functions used in SVM. 
Function type Definiton 
Linear function xxxxK T),(  
Exponential radial basis 
function (ERBF) 
)2/||exp(),( 2xxxxK 
Polynomial function dxxxxK )1,(),(   
The dynamic music frame analysis proposed in this report 
consists of MFCC and log energy with different frame length 
to cover short-term as well as long term features of music 
genre. In addition, the MFCC used in this report has variable 
mel-scale triangular filter banks from 16 to 64. 
The training set used in this report is listed in Table II. 
There are totally 45 music songs applied to SVM training. 
Each music track is digitized using 44.1 KHz sampling rate 
with 16-bit resolution. For each music track in the set for 
training, several timbral feature vectors can be obtained. 
Assume that nT is the number of the obtained MFCC and log 
energy vectors. The training set T is then defined to be the nT 
 (M+1) array with row vectors being these M-order MFCC 
vectors.  
Let T(i, j)denote the (i, j)-position of T. Use this array T to 
construct another nT  (M+1) array T’ whose (i, j) position 
T’(i, j) is defined to be 
jjiTjiT  ),(),( , where 
 i Tj njiT /),(  is the mean of column j. Next, one can 
normalize T’ by computing 
j
N mjiTjiT /),(),(  , where jm  
is the maximum of the absolute value of elements in column j. 
Thus, each timbral feature vector will have similar weights 
after the normalization process.  
TABLE II. The training set used in this report. 
Genre Track 
Classical Piano concerto no.1 in b flat minor 
The nutcracker 
Swan lake 
Trio sonata for organ 
Brandenburg concerto no.2 
Dance Keen on disco / missy eva  
Good friend / dj dove feat.   
Sunshine in the rain / cherry  
Time for dance / dj dove feat. Mr. V 
Apologize / dj elvis 
Jumping all over the world / scooter 
Discotheque / yamboo 
Big girls don't cry / nick skitz feat. Danni 
Lullaby Angels/ robbie williams 
Better man/ robbie williams 
Feel/ robbie williams 
Love somebody/ robbie williams 
Sing/ carpenters 
Superstar/ carpenters 
Yesterday once more/ carpenters 
Please mr. Postman/ carpenters 
Bossa Catupiry  
Litoral de sol  
O samba  
Clea  
O corso  
Orixa  
O guarda-chuva azul  
Taiyo no kodomotachi 
Piano Endings - michael jones 
Sunrise - kostia 
Dark eyes - wayne gratz 
Beloved - david lanz 
Water circles - mia jang 
Minor truths - fred simon 
Poetic justice - sheila larkin 
Bethel - paul cardall 
Blue Cantaloop (flip fantasia)/us3 
Steppin' into tomorrow/madlib 
Won't you open up your senses/horace silver 
Lemon/troublemakers 
Midnight blue/kenny burrell 
My funny valentine/chet baker 
Blue train/john coltrane 
Moanin/art blakey & the jazz messengers 
VI. Experimental Results 
The experimental results of the proposed music genre 
classification method are achieved by using 300 songs over 6 
genres, i.e., classical, dance, lullaby, Bossa Nova, piano, and 
blue note. Among these 300 songs, the 45 songs as listed in 
Table II are used in SVM training process and the remaining 
255 songs are applied to evaluate classification performance 
of the proposed method. All of these songs are noisy-free and 
are sampled at 44.1 KHz with 16-bit resolution. Each testing 
song consists of 9 seconds long music. The accuracy rate used 
in this report is defined as 
%100
frames ofnumber Totoal
frames corrected ofNumber                (16) 
Table III. The average accuracy rates of the proposed music genre 
classification method using 45 training songs and SVM with linear, 
polynomial, and exponential radial basis kernel functions. 
Kernel function Accuracy rate 
Linear function 93.6% 
Polynomial function 96.4% 
Exponential radial basis function 99.8% 
 
Table IV. Performances of the proposed music genre classification 
method using SVM with ERBF. 
Genre type Average accuracy rate 
Classical 98.2 % 
Dance 98.5 % 
Lullaby 97.4 % 
Bossa 96.1 % 
Piano 98.9 % 
Blue 98.6 % 
Total average  98.0 % 
 
Table III shows the average accuracy rates of the 
proposed music genre classification method using 45 training 
songs and SVM with linear, polynomial, and exponential 
 12
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價值（簡要敘述成果所代表之
意義、價值、影響或進一步發展之可能性）、是否適合在學術期刊發表或申請專利、主要發現或其他有關價
值等，作一綜合評估。 
 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■ 達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明：承蒙國科會對於本專題計畫的補助，計畫主持人得以在計畫第三年執行期間內順利完成本項研究
計畫，並完成 8 篇相關研究論文發表，其中 3 篇為 SCI 期刊論文，5 篇為國際研討會論文，詳細
的論文發表題目與發表地點請參閱”2. 研究成果在學術期刊發表或申請專利等情形”，另外還有一
篇準備投稿至 SCI 期刊的論文正在修稿中，預計 2012/012 前便可寄出。計畫主持人已和巴西聖保
羅大學物理與資訊系 University of São Paulo, Institute of Physics at São Carlos, Department of Physics 
and Informatics，Rodrigo Capobianco Guido 教授從事國際學術合作研究，預計將此演算法擴充至葡
萄牙語，並測試其效能。 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
[1] Shi-Huang Chen, Rodrigo Capobianco Guido, T. K. Truong, and Yaotsu Chang, “Improved Voice Activity 
Detection Algorithm Using Wavelet and Support Vector Machine,” Computer Speech and Language, Vol. 24, 
No. 3, pp. 531-543, July 2010. (SCI/EI)  
[2] M. A. Lacerda, Rodrigo Capobianco Guido, P. Zulato, J. Ribeiro, and Shi-Huang Chen, “A Wavelet-based 
Speaker Verification Algorithm,”Accepted by International Journal of Wavelets, Multiresolution and 
Information Processing, Vol. 8, No. 6, pp. 905-912, Nov. 2010. (SCI/EI) 
[3] T. C. Lin, T. K. Truong, Shi-Huang Chen, L. J. Wang, and T. C. Cheng, "Simplified 2-D Cubic Spline 
Interpolation Scheme Using Direct Computation Algorithm," IEEE Trans. on Image Processing, Vol. 19, No. 
11, pp. 2913-2923, Nov. 2010. (SCI/EI) 
[4] Shi-Huang Chen and Ming-Lung Hsu, “The Use of k-Means Algorithm to Compute the Line Spectrum Pair 
Frequencies with Tschirnhaus Transform,” the Sixth International Conference on Intelligent Information 
Hiding and Multimedia Signal Processing (IIH-MSP 2010), Oct. 15-17, 2010, Darmstadt, Germany. 
[5] Shi-Huang Chen and YuRu Wei, "A Study on Speech Control Interface for Vehicle On-Board Diagnostic 
System," The Fourth International Conference on Genetic and Evolutionary Computing (ICGEC 2010), Dec. 
13-15, 2010, Shenzhen, China.  
[6] Shih-Hao Chen, Shi-Huang Chen, Rodrigo Capobianco Guido, "Music genre classification algorithm based 
on dynamic frame analysis and support vector machine," IEEE International Symposium on Multimedia (ISM 
2010), Dec. 13-15, 2010, Taichung, Taiwan. 
[7] Po-Chuan Lin, Shi-Huang Chen, Jhing-Fa Wang, Kuo-Yuan Lu, Jun-Yu Chen, and Zheng-Wei Sun, “An 
SOC Design for Far-Field Sudden Sound Localization,” 2011 International Conference on e-Commerce, e-
Administration, e-Society, e-Education, and e-Technology (e-CASE & e-Tech 2011), Jan. 18-20, Tokyo. 
[8] Shi-Huang Chen, Jun-Yu Chen, and Kuo-Yuan Lu, "The use of cloud speech recognition technology in 
vehicle diagnosis applications," the Fifth International Conference on Innovative Mobile and Internet Services 
in Ubiquitous Computing (IMIS-2011), June 30th-July 2nd, 2011 Korean Bible University (KBU), Seoul, 
Korea. 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉■洽談中 □無 
 14
國科會補助計畫衍生研發成果推廣資料表 
                                                                       日期：100 年 8 月 20 日 
國科會補助計畫 
計畫名稱：高鑑別性聲音特徵參數擷取演算法之 SoC IP 設計 
計畫主持人：陳璽煌 
計畫編號：NSC 97-2221-E-366 -010 - MY3  
領域：微電子學門 
（中文）契爾恩豪斯(Tschirnhaus)轉換之超大型電路及嵌入式系統演算法 
技術/創作名稱 
（英文）The VLSI architecture and embedded algorithm for Tschirnhaus transform
成果歸屬機構 樹德科技大學 發明人 (創作人) 陳璽煌, 徐明龍 
(中文)本技術運用 k-means 演算法建構一套有效率的快速三角函
數查表系統，可應用於契爾恩豪斯(Tschirnhaus)轉換運算之新型快
速演算法設計，契爾恩豪斯轉換之主要功能為多項式求解，並可
用於線頻譜對求解之運算，由於契爾恩豪斯轉換內部運算需使用
三角函數，本技術即是針對此一需求設計一套有效率的快速三角
函數查表系統，該快速三角函數查表系統具有記憶體使用量少且
精確度高的特點，將可大幅減少契爾恩豪斯轉換運算所需的時間
以及運算器數量，非常適合運用至 DSP 或 VLSI 實作上。 
技術說明 (英文)This new technology makes use of k-means algorithm to 
develop an efficient fast triangle table look-up system. Such a triangle 
table look-up system could be applied to the calculation of Tschirnhaus 
transform. The proposed efficient fast triangle table look-up system has 
the advantages of low memory usage as well as high calculation 
accuracy. Therefore, by the use of the proposed efficient fast triangle 
table look-up system, the Tschirnhaus transform could have 
considerable computational saving and memory size reduction. The 
proposed Tschirnhaus transform with the fast triangle table look-up 
system is quite suitable to implement via DSP or VLSI architectures. 
產業別 IC 設計、嵌入式系統開發、DSP 系統開發 
技術/產品應用範
圍 
行動電話產業、網路電話產業、語音聲音檢索系統等。 
技術移轉可行性
及預期效益 
本技術運用 k-means 演算法建構一套有效率的快速三角函數查表
系統，可應用於契爾恩豪斯(Tschirnhaus)轉換運算之新型快速演算
法設計，本技術所設計的快速三角函數查表系統具有記憶體使用
量少且精確度高的特點，將可大幅減少契爾恩豪斯轉換運算所需
的時間以及運算器數量，非常適合運用至 DSP 或 VLSI 實作上。
目前尚未有此項新型演算法的專利被提出，可提出相關專利申
請，並向業界推廣。 
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/11/08
國科會補助計畫
計畫名稱: 高鑑別性聲音特徵參數擷取演算法之SoC IP設計
計畫主持人: 陳璽煌
計畫編號: 97-2221-E-366-010-MY3 學門領域: 積體電路及系統設計
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計畫完成了整合語音(Speech)與音訊(Audio)特徵參數之單一 SoC IP 演算法
架構設計 , 並使用小波轉換之語音特徵參數來改善 3G 行動通訊
AMR-NB/AMR-WB 語音編碼器的語音檢測模組(VAD)效能, 同時運用契爾恩豪森
(Tschirnhaus)轉換完成一套新型快速無複數運算之線頻譜對求解演算法, 計
畫成果可應用於 1.語音暨音訊編碼/識別等相關應用, 2.可攜式聲音類型識別
與聲音種類驗證系統, 3.音訊數位內容檢索 
 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
