  I
中文摘要 
近十年來，由於人形機器人擁有擬人的外形、具親和力的設計及在人類生活環
境中的智能等要素，使得人形機器人成為眾所期盼的對象;而為了滿足消費者的需
求，近幾年來，已開發出各式各樣的人形機器人。本研究計畫屬於二年期的研究計畫，
其目的在於創造一個能與人對打乒乓球之人形機器人。 
第一年期主要著重於一個會打乒乓球之人形機器人之建構、嵌入式以 DSP為基
礎之控制系統之研發，以及機器人雙眼機械頭之視覺伺服控制器之設計。而第二年期
的主要目標，在於發展一個眼/手的智慧型協調控制之設計架構，藉以使人形機器人
能與人對打乒乓球。因此，我們以先前本實驗室所開發之家用機器人為基礎，重新建
構其頭部與身軀，使其更為靈巧，以便能勝任打乒乓球之人形機器人。本研究所完成
之人形機器人，主要是由一輪式移動平台、一個三自由度的腰部、一雙分別為七自由
度的左右機械手臂、一雙分別為七自由度的左右機械手，以及一個七自由度的雙眼機
械頭所組成。為了機器人能撿拾起乒乓球，或於對手發球時能以球拍追蹤著對手所握
持之球，提出了一個眼/手協調控制之設計架構，此一架構主要是利用自我組織可逆
映射類祌經網路，學習影像資訊、雙眼機械頭及機械手臂控制命令之間的關係，配合
灰色預測對於空間目標物進行視覺追蹤；並透過柯漢寧類神經網路來學習空間表示式
變化與機械手臂控制命令間的關係，進而驅動機械手臂來追蹤此目標物。而在機器人
的打擊運動上，我們提出應用倒傳遞類神經網路來預測球的軌跡，以決定拍擊的瞬間
行為及機械手臂的贅餘自由度結構之運動控制策略。 
最後，以實驗來驗證理論的推導與打乒乓球人形機器人的性能。實驗結果顯示，
人形機器人能成功地打擊進入機器人場地的乒乓球，並返回對手場地。 
 
關鍵詞：人形機器人、乒乓球、嵌入式、以數位訊號處理器為基礎、眼/手協調、自
我組織可逆映射類祌經網路、灰色理論、柯漢寧類神經網路、倒傳遞類神經
網路 
  III
目錄 
中文摘要 ............................................................................................................................ I 
英文摘要 ............................................................................................................................ II 
目錄 .................................................................................................................................... III 
 
第一章 緒論.................................................................................................................... 1 
1.1 前言 ................................................................................................................... 1 
1.2 研究目的 ........................................................................................................... 1 
1.3 文獻回顧 ........................................................................................................... 2 
 
第二章 人形機器人之設計與逆向動力學之計算........................................................ 5 
2.1 人形機器人頭部及腰部之設計與建構 ........................................................... 5 
2.1.1 雙眼機械頭之設計與建構.................................................................... 5 
2.1.2 人形機器人腰部之設計與建構............................................................ 7 
2.2 以 DSP為基礎之控制系統之開發.................................................................. 8 
2.3 人形機器人逆向動力學之推導 ....................................................................... 9 
 
第三章 機器人伺服控制器之設計................................................................................ 13 
3.1 滑動模式控制 ................................................................................................... 13 
3.2 模糊滑動模式控制 ........................................................................................... 18 
3.3 人形機器人模糊滑動模式控制器之設計 ....................................................... 21 
3.4 實驗 ................................................................................................................... 23 
 
第四章 機器人之手/眼協調控制策略.......................................................................... 25 
4.1 手/眼協調控制的概念 ..................................................................................... 25 
4.2 視覺追蹤系統控制器之設計 ........................................................................... 26 
4.2.1 SOIM類神經網路................................................................................. 26 
4.2.2 SOIM網路的可逆性............................................................................. 29 
4.3 柯漢寧類神經網路 ........................................................................................... 30 
 - 1 - 
第一章 緒論 
1.1 前言 
隨著電子科技、電腦與控制技術之快速發展，近十年來各種仿生機器（機器狗、
機器貓、機器猿、機器魚、機器蛇、人形機器人等）應運而生。由於其應用範圍包括
娛樂、生活援助、家庭、運動、災害救援，甚至高科技領域等，因此吸引了各國研究
人員積極地投入研究。其中，由於人形機器人具有類似人類的動作與智慧，其自由度
大、靈巧性高，不但易與人們互動，而且亦能與工作環境配合，近年來已漸漸成為機
器人研究領域中最熱門項目之一。 
因此，機器人不再只是被要求能完成人類所不願意做或重複性的工作，而是希
望機器人能更進一步的與人互動，且更具有親和力;於是一個仿人類外形，且能深入
人類的生活，為人類做更多的事的人形機器人由此而生。人形機器人的建構，在機構
上需模組化及小型化，在控制系統上需具有即時網路通訊的分散式控制，而在感測系
統上需集合多樣感測器。 
 
1.2 研究目的 
自從日本 HONDA公司在 1998年公開發表其人形機器人的研究成果後，這個領
域開始引發熱烈討論，其亦將是機器人學在本世紀的一個重要研究方向。人形機器人
顧名思義為一具有類似人類的外形與功能的機器人，就外觀而言，一定有頭部與手
臂，如何令其眼睛與手臂能像人一樣能互相協調地去完成任務，將是一個非常重要的
研究課題。本研究計畫的目的在於研究人形機器人之智慧感測與決筞，以及手/眼協
調運動控制的機制，以建立未來個人機器人的基礎技術。本計畫所研究的對象是輪式
人形機器人，該輪式人形機器人將改良自本實驗室所設計的第一代家用人形機器人，
除了更改其雙眼機械頭的設計與增加其腰部的自由度外，並重新設計與規劃控制系
統，藉以減輕系統的重量與提昇系統的運算速度，而在實驗測試上，此一新的輪式人
形機器人將與人類進行乒乓球的對打，以驗證系統整合與控制策略的有效性。 
 - 3 - 
為期五年的人形機器人計畫。由於本田公司的不斷研發，最新型的阿西莫
(ASIMO;2000)機器人，已可依據地形狀況決定自己如何動作，因此能夠於上下樓梯
及斜坡進行時保持身體直立，以時速 1.6公里走向目的地。日本 Sony在 1999年設計
出來的 Aibo狗，差一點就取代狗，成為人類最好的朋友;之後，該公司以同樣的運算
處理技術為基礎，設計了一具人形機器人「SDR-4X」。而 2001年 9月日本的 Fujitsu
也曾發表一具會以雙足走路的人形機器人「HOAP-1」。「SDR-4X」與「HOAP-1」皆
屬娛樂性質，HOAP-1走起路來像個喝醉酒的小孩，而 SDR-4X本來就是用來娛樂大
家，顯然在唱歌伴舞方面要強得多。 
綜觀國外的人形機器人研究，本田汽車公司從 1986 年至今有 20 年的歷史，而
日本早稻田大學從 1973年至今已有 33年的歷史。由於二足式的人形機器人所需的研
發時間相當長，而且，有些場合以輪式移動平台取代雙足就足夠了，例如: Hadaly-1、
Hadaly-2及WENDY，都是上半身似人形而下半身為一移動平台，這些機器人的結構
設計多數著重於模仿人類頭部與雙眼方面的研究，以及透過雙臂與人類進行互動工作 
[12][13]。因此，本計畫特選定輪式人形機器人為研究對象，以作為朝向二足式人形
機器人邁進的第一步。 
以機器人與人類對打乒乓球，最早是由美國貝爾實驗室(At&T Bell Lab.) 於 1988
年所提出，該實驗室發展出一套能與人類進行乒乓球對打的機器人系統 [2][3][4]，其
中使用的乒乓球桌比人類所使用的桌子要狹小，且加入乒乓球必須通過的金屬線框於
兩端及中央，比賽是由一個小型的機器人和人類進行對打，所使用的機器人為一部六
自由度的 Puma 260機械手臂，而二部攝影機則固定地架設於機械手臂的兩側。控制
系統分為四個部分且分佈於多部電腦中：一個 3D 視覺系統(3D Vision System)能每
1/60秒決定球的位置，一個軌跡分析器(Trajectory Analyzer)能決定且推斷球的運動軌
跡，一個專家控制器(Expert Controller)能經由乒乓球的打擊策略來產生機器人所要的
打擊軌跡，以及伺服系統(Servo System)能使機器人隨著軌跡移動。雖然系統能成功
地與人類進行對打，但仍受限於某些條件，例如:人類必須使球穿過所規劃的金屬框
線，而且由於攝影機的涵蓋視野有限，因此，桌面的寬度與長度皆小於一般的桌球桌。
為了屏除這些限制條件，本研究計畫擬以所設計的輪式人形機器人與人類進行乒乓球
 - 5 - 
第二章 人形機器人之設計與逆向動力學之計算 
為了使機器人能融入人類生活環境中，機器人以擬人形活動於人類生活環境，將
有助於增加機器人對人類的親和力。然而，完全仿似人類外形的機器人，在製作上存
在著一定程度的困難性，因此，適當簡化機器人外形，可使機器人具有足夠親和力且
在製作上容易完成。本計畫在硬體上，改良自本實驗室所設計的第一代家用人形機器
人-輪式人形機器人，除了更改其雙眼機械頭的設計與增加其腰部的自由度外，並重
新設計與規劃控制系統，新的人形機器人其上半身擁有一個具三自由度的腰部的軀
體、一個七自由度的雙眼機械頭、各七自由度之左右機械手臂，以及各七自由度的左
右機械手，而在下半身則為一輪式移動平台。本文 2.1節介紹人形機器人之頭部、腰
部之硬體設計，2.2節說明人形機器人新的控制系統，2.3節則推導機器人之逆向動力
學計算。 
 
2.1 人形機器人頭部及腰部之設計與建構 
2.1.1 雙眼機械頭之設計與建構 
在人類的眼球中，視網膜的感測神經元分為兩大部分：錐狀體神經元和柱狀體神
經元，錐狀體神經元負責視覺中央區域的感測，柱狀體神經元負責在視覺中央兩側區
域的感測。因此，人類的視覺具有廣角的視野及高度的解析力，並且在視覺的中央部
分的感測有相當高密度，但在兩側的視覺感測其密度則相對較低。為了在新設計的機
械頭上實現與人類相當的視覺感測機制，必須使攝影機具有廣角的視野及高解析度。 
基於上述的理念，我們設計了一個擁有七自由度的雙眼機械頭，如圖 2.1所示。
在機構設計上，使用商用軟體 ProE 進行設計及分析，以決定所需的馬達動力及驅動
性能，表 2.1為此雙眼機械頭的運動參數。 
 - 7 - 
眼部機構將架於第四軸連桿上，這四個自由度的設計，主要是為了使眼部更能藉由頸
部之彎曲及伸展，來觀察靠近身體的物體。 
 
2.1.2人形機器人腰部之設計與建構 
機器人的輪式移動平台可使機器人自由的移動於平面空間中。輪式移動平台在大
範圍的空間移動對機器人有相當的效果，但如果機器人須側身擊球或撿拾起在輪式移
動平台周圍的物體，則僅賴輪式移動平台將無法完成此任務。為了增加機器人身軀之
靈巧度，我們建構一個具三自由度的腰部，如圖 2.2所示。 
 
圖 2.2三自由度機械人腰部 
在腰部之結構設計上，採用 roll-pitch- yaw的架構，並令三個旋轉軸相交於一點，
而機械人腰部的運動參數則列於表 2.2。圖 2.3顯示完成後的整體機器人架構。 
表 2.2 三自由度機械人腰部之運動參數 
淨重 18 Kg 
軸 下極限(deg) 上極限(deg) 最大角速度(deg/s) 
Roll -90 90 90 
Pitch -45 45 90 
Yaw -35 35 90 
 - 9 - 
 
圖 2.4 人形機器人控制系統之硬體架構 
 
2.3人形機器人逆向動力學之推導 
為了描述相鄰兩根連桿間平移與旋轉的關係，必須在每根連桿上定義座標系。本
計畫採用 Modified Denavit-Hartenberg 表示法，定義以機器人的腰部為基底座標系
( wb wb wbO x y z− )之人形機器人整體座標系統，如圖 2.5所示。有了座標系統的定義後，
即可決定機器人之連桿參數，並完成順向運動學的推導。 
而在動力學的分析上，主要探討機構運動與致動器所施加的力與力矩之間的關
係。一般常用來推導動力學的演算法包括了 Newton-Euler 法和 Lagrangian 法。
Newton-Euler法是直接利用 Newton第二定律來推導，根據力與動量來描述動態行為;
以 Newton-Euler 法所得到的方程式，包括了作用在相鄰兩連桿間的拘束力，必須透
過額外計算來消除這些項，才能得到關節力矩以及由關節角度所描述機構運動的關
係。而 Lagrangian法則是根據功和能量來描述系統的動態行為。Newton-Euler的計算
法則為遞迴式計算，其計算方程式具有模組之結構，對於微處理器之計算及程式之撰
寫更有效率。因此本計畫採用 Newton-Euler遞迴式來計算機器人系統的逆向動力學。 
 
 - 11 - 
1
cos sin cos sin sin
sin cos cos cos sin
0 sin cos
i i i i i
i
i i i i i i
i i
R
θ θ α θ α
θ θ α θ α
α α
−
⎡ ⎤⎢ ⎥= −⎢ ⎥−⎢ ⎥⎣ ⎦
  
0
ˆ 0
1
iz
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
 
向外疊代計算： 
1 1 ˆ
i i
i i i iR zt
θω ω− − ∂= • + ∂  (2.1) 
2
1
1 1 1 2?
i ii i i i
i i i i iR R z zt t t t
ω ω θ θω−− − −∂ ∂ ∂ ∂⎛ ⎞= • + • × +⎜ ⎟∂ ∂ ∂ ∂⎝ ⎠  (2.2) 
( )1 11 11 1 1i i ii i ii i i i iv vR p pt t tω ω ω− −− −− − −∂ ∂ ∂⎡ ⎤= × + • × +⎢ ⎥∂ ∂ ∂⎣ ⎦  (2.3) 
( )( ) ( )i ici i ic i i i c iv vp pt t tω ω ω∂ ∂ ∂= × + • × +∂ ∂ ∂  (2.4) 
( )
i
i i c iF m v= •  (2.5) 
( )( ) ( )ii c i i c i iN I Itω ω ω∂= • + × •∂  (2.6) 
向內疊代計算： 
1
1 1
i
i i i if R f F
−
− −= +  (2.7) 
( )1 1 1 11 1 ( 1) 1i i i ii i i i c i i i i in N R n p F p R f− − − −− − − −= + • + • + × •  (2.8) 
1 1 1ˆ
T
i i in zτ − − −=  (2.9) 
初始條件如下： 
向外疊代： [ ]1 0 0 0 Tiω − =  
[ ]1 0 0 0 Ti
t
ω −∂ =∂    
[ ]1 0 0 Tiv g
t
−∂ = −∂  
向內疊代： [ ]0 0 0 Tif =  
[ ]0 0 0 Tin =  (沒有外力及外力矩) 
使用此法來計算機器人逆向動力學時，以連桿接著下一連桿的方式來進行計算，
所以整體機器人的動力學在計算向外疊代時，需分成四組遞迴迴路，分別如下所示: 
  - 13 - 
??? ??????????? 
??????? PD ?????????????????????????
?????????????????????????????????????
????????????????????????????????????
?????????????????????????????????????
????????????????????(Chattering)????????????
?????????????????????????????????????
?????????????????????????????????????
???????????????????????????????? 
??? 3.1???????????????????? 3.2????????
?????????????????????????????????????
????????????? 3.3??????????????????????
?????3.4 ??????????????????????????????
??? 
 
3.1 ?????? 
???????????????????????????? 0),( =tS X ??
?????????????????????????????????????
??????????????????????????????? 
00
2
1 2
<⋅⇔< SS
dt
dS    (3.1) 
?????????? Lyapunov???????? 0)],([)( 2 >= tSV XX ? 
02 <=
dt
dSS
dt
dV  
????????????????????(Asymptotic Stability)? 
 
  - 15 - 
????????????????????????????????????
???????????????????????? ),( tS X ????? 
???????????????? )(tu ?? 0<⋅ SS  ?? 
0~
1
)()( <







+−++⋅ ∑
=
−
n
i
inin
d xi
n
xdbufS λ  (3.6) 
?????????????????? )(tu ?????????? 3.1??? 
 






+ ∫t
n
de
dt
d
0
)( ττλ
PLANT
SdX
X
u∆e
equ
u
+
+
+
−
u∆
K
K−
S








+−
− ∑
=
−
n
i
nin
d ei
n
xf
b 1
)1()(ˆ
ˆ
1 λ
 
? 3.1 ???????? 
 
)sgn(
),(ˆ
~ˆ
),(ˆ
1)(
1
)()( S
tb
Kx
i
n
xf
tb
tu
n
i
inin
d XX
−







+−
−
= ∑
=
−λ  (3.7) 
?? 
2/1
maxmin )(),(ˆ ββ=tb X ? ),( tb X ???? 


<−
>
=
0;1
0;1
)sgn(
S
S
S  
???? )(tu ??(3.1)?? 
  - 17 - 
 
?????? )(tu ??? 3.2(a)????????? )(tu ???????????
?????????????????? 3.2(b)????????????????
?????????????????????????????????????
???????????? 
K−
K
S
)(tu
K−
K
S
)(tu
∆−
∆
 
? 3.2(a) ?????????  ? 3.2(b)????????? 
 
??????????????? )(tu ? ),( tS X ?????????????
??????????? 3.3???? )(tu ??????????????????
?????????????????????????????????? 
????????????? )(tu ???? 
)(
),(ˆ
~ˆ
),(ˆ
1)(
1
)()(
Φ
−







+−
−
= ∑
=
−
Ssat
tb
Kx
i
n
xf
tb
tu
n
i
inin
d XX
λ  (3.11) 
?? 



Φ≤
Φ
Φ<−
Φ>
=
Φ
),(
),(1
),(1
)(
tSifS
tSif
tSif
Ssat
X
X
X
 
????????????? )(tu ?????????? 3.4????????? 
  - 19 - 
???? 
 






+ ∫t
n
de
dt
d
0
)( ττλ Fuzzy LogicController
PLANT
SdX
X
u∆e
equ
u
+
+
+
−








+−
− ∑
=
−
n
i
nin
d ei
n
xf
b 1
)1()(ˆ
ˆ
1 λ
 
? 3.5 ??????????? 
 
?????????? ),( tS X ??????????? 0),( =tS X ??????
????????? 0<⋅ SS  ????????? 0)()( <∆⋅ kSkS ????????
????????? )(kS ? )(kS∆ ??????????????????????
???????? )(kS ????????? )(kS∆ ???????? )(ku∆ ????
?????????? 3.6??? 
Fuzzy Control Rule
&
Fuzzy Inference
1k
2k
3k
S ′
S ′∆
)(kS
)(kS∆
u∆u′∆
 
? 3.6 ????????? 
 
? )(kS ? )(kS∆ ? )(ku∆ ??????????? S ′? S ′∆ ? u′∆ ??? S ′? S ′∆
? u′∆ ?????????? 
Positive Big (PB) 
  - 21 - 
3.3 ????????????????? 
??????????????????????????? 
τhqH =+  (3.13) 
??????????? 
τhqH =+ ˆˆ   (3.14) 
??????? 
hHτ ˆˆ += u  (3.15) 
? 
hHHHq 11 ∆+= −− u)ˆ(  (3.16) 
?? 
hhh −=∆ ˆ  
[ ]nLLLH "211 =−  
[ ]nHHHHHH ∆∆∆=−=∆ "21ˆ  
?? i??????? 
nidqqqS
t
iiiii ,,2,1;)(~~2~ 0
2 " =

++= ∫ ττλλ  (3.17) 
?? 
diii qqq −=~  diq ????
?? 
? 
( )∑ −−−∆+∆+=
++−=
j
iiiidi
T
ijj
T
ii
iiiidiii
qqqhLuHLu
qqqqS
~~2
~~2
2
2
λλ
λλ


 
? ieqi uuu ∆+= ? iiidieq qqqu ~~2 2λλ −−=            (3.18) 
  - 23 - 
Fuzzy
Sliding Mode
Controller
huH ˆˆ +
u
mk
n
2
AD /
d
s
d
s
d
s iii
qqq  ,, e τ v
++
is
q
isq Driver
Robotic 
SystemEncoderVelocity
Estimate
−
 
? 3.8(a) ????????????????????? 
 
diq
iq~
iq~
iiiidi qqq ~~2
2λλ −− 
)(kSi
Time
Delay
)1( −kSi
ieq
u
2ik
3ik
1ik
Look-up
Table
−
+
)(kSi∆
)(kSi′
)(kSi′∆
)(kui′∆



++
∫t ii
iii
dq
qq
0
2 )(~
~2~
ττλ
λ 
iu∆
+
+
iu
 
? 3.8(b) ? i??????????? 
 
3.4 ?? 
??????????????????????????????????
?????????????????????????????????????
?????????????????????????????????????
?????????????????????????????????????
???????????????????????????? 3.9???????
?????????????????????? 3.10 ????????????
??????????????? 0.2?? 
-  - 25 -
??? ?????/??????? 
??????????????????????????????????
?????????????????????????????????????
?????????????????????????????????(???
?)???????????????????????????????????
?????????????????????????????????????
??????????????????????? ECL(End-point Closed Loop)?
?????????????????????????????????????
?????????????????????????????????????
? ? ? ? (Dynamic Image-based Look-and-move Structure) ? ? ? ? SOIM 
(Self-organizing Invertible Map)????????????????????????
?????????????????????????????????????
????????????????????????????????????? 
??? 4.1 ???????????????????/?????????
????? 4.2???????????????? 4.3????????????
? 4.4???????????? 4.5??????????/??????????
?????? 
 
4.1 ?/???????? 
??????????????????????????????????
??????????????????? SOIM???????????????
??????????????????????????????? I (Spatial 
Representation)???????????????????????????????
??????????????????? SOIM???????????????
????????? I???? 
-  - 27 -
u1 v1 u2 v2
u1 v1 1 1− u 1 1− v u2 v2 1 2− u 1 2− v
J K
Visual Signals 5θ 6θ 7θ
Motor Signals
5I 6I 7I
Spatial Representation
Weights
ZJ
L
Weights
WJ
L
Weights
WK
R
ZK
R
Weights
F1
F2
Input 
Layer
Hidden
Layer
Output 
Layer
 
? 4.1 SOIM???????? 
 
????????(Input Layer)????????????????????
1 1( , )
LX u v= ? 2 2( , )RX u v= ??????????????????????????
????? 1u ? 1v ? 2u ? 2v ??????(Normalized)? 0? 1????????
????? 5 6 7( , , )Q θ θ θ= ??????????????????? 5θ ??? Tilt
????????? 6θ ? 7θ ????????? Vergence??????????
??? 0? 1??? 
???????????????????????????????
(Fuzzy Adaptive Resonance Theory Network) [15]???????????????
???????????(Unsupervised Learning)???? ART1???????
?? ART1 ???????(Binary)????? fuzzy ART ????????
(Binary)????(Analog)????????????? 0? 1??????? 
-  - 29 -
?????? LJiz ? RJiz ?????????????( I )????????????
(????????????[16])????????????( Lz ? Rz )??? SOIM
???? 
Pulse 
Generator
Binocular 
Head
Camera 
Motion
Image 
ProcessingSOIM
Stationary  
Point
Spatial 
Representation I
Previous Spatial 
Representation ′I
+
-
Random 
Input
X u vL = ( , )1 1
X u vR = ( , )2 2
Q
Joint 
Position
 
? 4.2 SOIM?????? 
 
4.2.2 SOIM??????(Invertible Property) 
???????????????????????????????????
????? SOIM??????????? I ?SOIM?????????????
?????????????????????????????????????
???????????????? dX ???????????????????
(4.1)????????? 
c L R
i i J i K iq I z z′ ′= − −   (4.3) 
???J ′?K ′???????????? dX ?????????????????? 
SOIM????????? 4.3?????????? SOIM????????
????????????????? 1 1 2 2( , , , )dX u v u v= ????????????
???????????? dX ?????? ( , )L RJ Kz z′ ′ ??????????????
?????????? SOIM ?????????? I ???????? I ???
-  - 31 -
J
5I∆ 7I∆6I∆ rxP
r
yP rzP
Change in Internal Representation Current Robot Configuration
r
xP∆
r
yP∆ rzP∆
Change in Robot Configuration
Kohonen
Layer
Weights
Weights
Jw
Jz
 
 
? 4.4 ??????????? 
 
?????????????(Kohonen Network Layer)???????????
? jw ?????? (Euclidean Distance) jd ?????????????? 
1/ 2
4
2 2
2 , ,
( ) ( )rj i ij k kj
i k x y z
d I w p w
= =
 
= ∆ − + −  ∑ ∑  (4.4) 
??????????(4.5)?????????????????????????
????? , ,r r rx y zp p p ∆ ∆ ∆ ? 
, ,( )
r
jk k jk k x y zz p zγ =∆ = ∆ −   (4.5) 
???γ ?????? 
 
-  - 33 -
????? X ik
( ) ( )1 ? j???????(4.9)?? 
 
{ }α ( ) ( )( , )j kX i1 ? 
i N
k m
j n
=
=
=
1 2
1 2
1 2
, ,...,
, ,...,
, ,...,
 (4.9) 
 
?? 
α ( ) ( )( , ) ( )0 1 1X i X ik k= ? 
α α α( ) ( ) ( ) ( ) ( ) ( )( , ) ( , ) ( , )1 1 0 1 0 1 1X i X i X ik k k= − − ? 
… 
α α α( ) ( ) ( ) ( ) ( ) ( )( , ) ( , ) ( , )j k
j
k
j
kX i X i X i
1 1 1 1 1 1= − −− −  
 
????GM( , )11 ??????????????????????? 
 
dx
dt
ax u
( )
( )
1
1+ =  (4.10) 
 
 ( ) ( ( ) )( ) ( )x t x u
a
e u
a
at1 1 0= − +−  (4.11) 
 
???????????????????(Numerical Calculation)????????
?GM( , )11 ????????????????(4.10)???? 
 
∆x t ax t u( ) ( )( ) ( )1 1+ =  (4.12) 
 
-  - 35 -
 
4.5?/??????????? 
???????????????/?????????4.5??? 
FSMCInversedSOIM
Grey 
Predictor
Spatial Representation of Target  It
Desired Point on
the Image plane
SOIM
Desired Error
∆Id
∆I
+
-
-
-
+
+
Image Capture
Image
Processing
(ball feature)
Image Capture
Image
Processing
(paddle feature)
FSMC
-
+
SOIM
Kohonen
Network
∆I-∆Id ∆P Inverse
Kinematic
θ∆
Spatial Representation of Target  Ir
Id
+
-
? 4.5 ??????????? 
SOIM????????????????????????????????
?????????????????????????????????????
?????????????????? 2360 240( )pixels× ???????????
??? Encoder?????SOIM??????(Training Data)??????????
???????????????????????????????? 9000 ??
????????????? 4???????????????????????
?????????? SOIM???????????? SOIM?????????
???(Vigilance Parameter) 0.88L Rρ ρ= = ????? 0.25γ = ??? 0.002α = ???
?????????????????????????????????????
????????????? 4.6 ???????????????? I?????
????0.03??? 
 
-  - 37 -
由於神經元權值初始化時均假設沒有事前的資訊可供參考，所以這裡是先將權
值以均勻分佈（Uniform Distribution）於三個空間表示式的區間[0,1]及工作空間
[-200,200,-150,150,-150,150]。在學習階段中，隨著學習次數的增加，神經元參數的調
整幅度也會慢慢的減小，所以權值分佈也會漸趨成型。圖 4.8、圖 4.9 分別表示神經
元權值初始化與 5000次訓練後的分佈情形。 
0 200 400 600 800 1000 1200 1400 1600 1800 2000
0
10
20
30
40
50
60
70
80
Learning Process (x10 Steps)
E
rr
or
 (
m
m
)
Learning Trajectory of 8x8x8 Neuron Setting
 
圖 4.7 柯漢寧類神經網路之學習誤差曲線 
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
I△
5
I
△
6
Initial situation
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
I△
6
I
△
7
Initial situation
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
I△
5
I
△
6
After 5000 learning steps
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
I△
6
I
△
7
After 5000 learning steps
 
圖 4.8 神經元之空間表示式的初始及 5000次學習後之權值分布 
-  - 39 -
臂上之球拍中心與球保持一定距離來進行追蹤。實驗中機械頭先鎖定目標物，再驅動
機械手臂至目標物位置。圖 4.10(a)為左、右影像平面上球與機械手臂之球拍中心之影
像變化，圖 4.10(b)為 I∆ 對時間之變化圖。對於靜態目標物而言，雙眼機械頭可以很
快鎖定目標物，並驅動機械手臂至目標物位置。 
0 50 100 150 200 250 300 350
0
50
100
150
200
u-axis (pixel)
v-
ax
is
 (p
ix
el
)
Tracking moving target on left image
center of the paddle
ball
0 50 100 150 200 250 300 350
0
50
100
150
200
u-axis (pixel)
v-
ax
is
 (p
ix
el
)
Tracking moving target on right image
center of the paddle
ball
0 20 40 60 80 100 120 140
-200
-100
0
100
200
300
u/v Coordinate Variation on Left Image
step (one step:16.67ms)
pi
xe
ls ball on u-axis
ball on v-axis
paddle on u-axis
paddle on v-axis
0 20 40 60 80 100 120 140
-200
-100
0
100
200
300
u/v Coordinate Variation on Right Image
step (one step:16.67ms)
pi
xe
ls ball on u-axis
ball on v-axis
paddle on u-axis
paddle on v-axis
 
圖 4.10(a) 球及球拍在左、右眼影像平面之位置變化（球為靜止不動）(左上：球及球
拍在左眼影像平面之位置變化，左下：球及球拍在左眼影像平面 u及 v方向之位置變
化，右上：球及球拍在右眼影像平面之位置變化，右下：球及球拍在右眼影像平面 u
及 v方向之位置變化) 
-  - 41 -
0 50 100 150 200 250 300 350
0
50
100
150
200
u-axis (pixel)
v-
ax
is
 (p
ix
el
)
Tracking moving target on left image
0 50 100 150 200 250 300 350
0
50
100
150
200
u-axis (pixel)
v-
ax
is
 (p
ix
el
)
Tracking moving target on right image
0 20 40 60 80 100 120 140
-200
-100
0
100
200
300
u/v Coordinate Variation on Left Image
step (one step:16.67ms)
pi
xe
ls
0 20 40 60 80 100 120 140
-200
-100
0
100
200
300
u/v Coordinate Variation on Right Image
step (one step:16.67ms)
pi
xe
ls
center of the paddle
ball
center of the paddle
ball
ball on u-axis
ball on v-axis
paddle on u-axis
paddle on v-axis
ball on u-axis
ball on v-axis
paddle on u-axis
paddle on v-axis
 
圖 4.11(a) 球及球拍在左右眼影像平面之位置變化（球為動態移動）(左上：球及球拍
在左眼影像平面之位置變化，左下：球及球拍在左眼影像平面 u及 v方向之位置變化，
右上：球及球拍在右眼影像平面之位置變化，右下：球及球拍在右眼影像平面 u及 v
方向之位置變化) 
0 20 40 60 80 100 120 140
-0.2
0
0.2
0.4
0.6
0.8
step (one step:16.67ms)
I△ 5
I△ 6
I△ 7
 
圖 4.12(b) I∆ 對時間之變化圖（球為動態移動） 
  - 43 -
m
r2
機器人場地 對手場地
打擊平面 球路預測之虛擬平面
r1h1
h2
s1
s2
o2o1
 
? 5.1 ???????? 
?????????????( 1 2,s s )?????????????( m )????
?????????????????????????????????????
( 1 2,r r )????????????????( 1 2,h h )???????????????
( 1 2,o o )????????( 1 2,s s )??????????? 
??????????????????????????? 5.2??? 
打擊程序
(A)
返回程序
(B)
預備程序(C)更新預測參數(C1)
球路預測
(C2)
運動軌跡
規畫
(C3)
 
? 5.2 ?????? 
(1)????( A )?????????????????????????????
???? 
  - 45 -
????????????????????????( hrdt )?????( hrdp )?
?? 
 
5.2 ?????? 
?Matsushima [11]??????????????????????????
?????????????????????????????????????
????????????????????????????????? 60Hz?
????????????????????? 60Hz?????????????
???????????? 
?????????????( m )???( 1h )??????????????
?? 
1( , , , )bmy bmx bmy bmzdt f p v v v=  (5.1) 
1( , , , )bmy bmx bmy bmzdx f p v v v=  (5.2) 
1( , , , )bmy bmx bmy bmzdy f p v v v=  (5.3) 
1 1( , , , )bh x bmy bmx bmy bmzv f p v v v=  (5.4) 
1 1( , , , )bh y bmy bmx bmy bmzv f p v v v=  (5.5) 
1 1( , , , )bh z bmy bmx bmy bmzv f p v v v=  (5.6) 
?? h mdt t t= − ?????( m )???( 1h )????? bhx bmxdx p p= − ? bhy bmydy p p= − ?
???????( ), ,bmx bmy bmzp p p ? ( ), ,bmx bmy bmzv v v ??????( m )????????
( )1 1 1, ,bh x bh y bh zv v v ?????( 1h )???????????????????? z??
????????( m )???( 1h )????? z????????????? 
??????????????????????????????????
?????????????????????????????????????
????????????????????????(Back Propagation Neural 
  - 47 -
* *
1    ( , , , )
h m
m bmy bmx bmy bmz
t t dt
t f p v v v
= +
= +  (5.7) 
* *
1       ( , , , )
bhx bmx
bmx bmy bmx bmy bmz
p p dx
p f p v v v
= +
= +  (5.8) 
* *
1       ( , , , )
bhy bmy
bmy bmy bmx bmy bmz
p p dy
p f p v v v
= +
= +  (5.9) 
????????????????? 300 ??????( m )???( 1h )??
??????? 200????????????????????????????
?????????????(the Gradient Steepest Descent Method)????????
?????????????????????????????????????
??????? 0.05η = ?????? 0.9α = ??? 5000????????????
?????? 4???(Node)??????? 80?????????? 80????
????? 6???????????????????????????????
?(Error)????????????????????? 5.5 ?? 5.7 ??????
?????????????????????????????????????
????????????????????????????????????
?? 
  - 49 -
0 5 10 15 20 25 30 35 40
-300
-250
-200
-150
-100
-50
0
50
Trial Number
dy
 (m
m
)
real
predict
 
圖 5.7 打擊位置( dy )的預測結果 
 
5.3 座標關係 
為了能使機器人能順利完成打擊，必須先了解其座標關係，如圖 5.8所示。在圖
5.8中， 45HhitA 是經由攝影機觀測並透過前一節所述的預測方法得到其打擊位置，而其
方位和座標系 45HO 對齊， hitassignA 是用以使座標系 hitO 之方位對齊座標系 RAbO ，方便拍
面姿態的計算，因此 hitassignA 為一固定值，而 assigndpA 則用以描述所需之拍面姿態，分別表
示為(5.10)及(5.11)式， 
0 1 0 0
0 0 1 0
1 0 0 0
0 0 0 1
hit
assignA
−⎡ ⎤⎢ ⎥⎢ ⎥= −⎢ ⎥⎢ ⎥⎣ ⎦
 (5.10) 
 
  - 51 -
 
圖 5.8 座標系關係圖 
 
5.4 拍面方位之決定 
為了順利求得手臂腕部和手臂肩部的相對關係 6RAbRAA ，必須先決定出 hitassignA ，其表
示為(5.15)式。在圖 5.9 中，我們定義拍擊點於球拍的正中心，在先前所述之球軌跡
估測，已獲得球在拍擊點時的進入速度 ,i ballv
v 及所要的球拍擊後的的離開的速度
,o ballv
v ，接著對 ,i ballv
v 及 ,o ballv
v 以(5.18)式進行正規化計算， 
0
0 1
hit
hit assign
assign
R
A
⎡ ⎤= ⎢ ⎥⎣ ⎦  
x x x
hit
assign y y y
z z z
n a o
R n a o
n a o
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
 (5.15) 
  - 53 -
 
圖 5.10 連桿座標系 
 
5.5.1 機械手臂之逆向運動解 
本計畫利用[1]所提出之第七個拘束自由度，如圖 5.11所示。以肩部三軸交點到
腕部三軸交點的虛擬連線為旋轉軸，肘部和此虛擬連線的垂直線為半徑，則此垂直線
和手臂第一軸之間所構成的夾角，我們定義為肘角(Elbow Angle)。利用 Modified 
Denavit-Hartenberg 表示法，可以求得手臂末端相對於基底座標系的位置和方位，並
用單一的轉換矩陣來描述，如(5.20)式之表示，通常我們都給定一末端的轉換矩陣，
以求得各關節的角度，定義一所要的末端的轉換矩陣如(5.21)式， 
0 1 2 3 4 5
6 0 1 2 3 4 5 6
RAb RAb RA RA RA RA RA RA
RA RA RA RA RA RA RA RAA A A A A A A A=  (5.20) 
6
0 0 0 1
x x x x
y y y yRAb
RA
z z z z
n b t p
n b t p
A
n b t p
⎡ ⎤⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
 (5.21) 
  - 55 -
ˆ ˆˆ cos sin
T
x y zelbow e e e
mlen z hlen x hlen yψ ψ
 =  
= ⋅ − ⋅ ⋅ − ⋅ ⋅
 (5.30) 
????????????????(5.31)?(5.32)?(5.33)??????? 0θ ?
1θ ? 2θ ??????? 0θ ? 1θ ? 2θ ??(5.34)?(5.35)???????????? 36Aˆ  
1
0 tan x
z
e
e
θ −  = −     (5.31) 
1
1 sin
ye
U
θ −  = −     (5.32) 
1 0 0
2
3
( ) cos ( )sincos
sin
x zpx e pz e
F
θ θθ
θ
−
 
− − − −
=  
−   (5.33) 
0 01 12 23bF A A A Aψ =  (5.34) 
1
36
ˆ [ ]A F Tψ
−
=  (5.35) 
????????????????(5.36)?(5.37)?(5.38)???? 4θ ? 5θ ? 6θ ? 
1 36
4
36
ˆ [1,3]tan ˆ [3,3]
A
A
θ −  = −    (5.36) 
1 36
6
36
ˆ [2,2]tan ˆ [2,1]
A
A
θ −  = −    (5.37) 
1
5 36
ˆsin [2,3]Aθ −  =    (5.38) 
??????????????????????????????????
?????????????????????????????????????
?????????????????(??)??????? 
?(5.21)???????????????? WP ?????? wˆ??(5.22)??
????? 3θ ????????(5.25)??(5.26)???? hlen?mlen??????
?????????????????????? 35 ?????????????
? A?? B???????????????? 
  - 57 -
????????????????????/??????????????
??????????????????????????????0 ~ 360°???
??????????(Limit)????????????????????????
?[1]???? 
 
5.5.2 ?????????[4] 
??????????????????????????????????
?????????????????????????????????????
???????????????????????????????????
(Quintic Polynomials)????????????? 
2 3 4 5
0 1 2 3 4 5
2 3 4
1 2 3 4 5
2 3 4
2 3 4 5
( )
( ) 3 4
( ) 6 12
p t a a t a t a t a t a t
v t a a t a t a t a t
a t a t a t a t a t
= + + + +
= + + +
= + +
＋
＋5
＋20
 (5.45) 
???????????????(jerk)????????????????
?????????????????????????????????????
ip ??? iv ???? ia ?????? fp ??? iv ???? ia ??????????
??????????? 
5 5
(( ) 6( )) 12( )
2
f f i f f i f i
f
t a a t v v p p
a
t
− − − + −
=  (5.46) 
4 4
(16 14 (3 2 ) ) 30( )
2
f i f i f f f i
f
t v v a a t p p
a
t
+ + − + −
=  (5.47) 
3 3
( 3 ) 8 12 ) 20( )
2
f f i f f i f i
f
t a a t v v p p
a
t
− − − + −
=  (5.48) 
2 2
iaa =         1 ia v=       0 ia p=  (5.49) 
?????????????????????????????????
?????????????????????????????????????
?????????????????????????????????(??)?
  - 59 -
 
圖 5.13 打擊運動循環 
依此規畫，可使機械手臂以一平滑軌跡完成一次打擊運動循環，圖 5.14顯示其
軌跡在三方向的變化，其中以紅虛線分隔，用以表示其運動階段。 
  - 61 -
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
-2.2
-2
-1.8
-1.6
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.5
-0.4
-0.3
-0.2
x-dir (m)
z-dir (m)
y
-
d
i
r
 
(
m
)
center of paddle
ball
 
? 5.15 ?????????(10???) 
-2.2-2-1.8-1.6-1.4-1.2-1-0.8-0.6-0.4
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
z-dir (m)
x-
di
r 
(m
)
center of paddle
ball
 
? 5.16 ? x-z?????????????(? 10???) 
 
  - 63 -
0 2 4 6 8 10 12 14
-1.2
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
time step (30 ms)
x-
di
r 
(m
/s
)
0 2 4 6 8 10 12 14
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
time step (30 ms)
y-
di
r 
(m
/s
)
0 2 4 6 8 10 12 14
-1.5
-1
-0.5
0
0.5
1
time step (30 ms)
z-
di
r 
(m
/s
)
-1
-0.5
0
-1
-0.5
0
0.5
0
0.5
1
x-dir (m/s)
z-dir (m/s)
y-
di
r 
(m
/s
)
 
圖 5.19 球拍之移動速度變化(10次打擊) 
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
-2.2
-2
-1.8
-1.6
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.5
-0.4
-0.3
-0.2
x-dir (m)
z-dir (m)
center of paddle
ball
 
圖 5.20 球及球拍之移動軌跡(11次打擊) 
  - 65 -
0 5 10 15
0.1
0.15
0.2
0.25
0.3
time step (30 ms)
x-
di
r 
(m
)
0 5 10 15
-0.46
-0.45
-0.44
-0.43
-0.42
-0.41
time step (30 ms)
y-
di
r 
(m
)
0 5 10 15
-0.5
-0.48
-0.46
-0.44
-0.42
-0.4
-0.38
time step (30 ms)
z-
di
r 
(m
)
0.15
0.2
0.25-0.48
-0.46
-0.44
-0.42
-0.4
-0.44
-0.42
x-dir (m)z-dir (m)
y-
di
r 
(m
)
 
圖 5.23 球拍之移動位置變化(11次打擊) 
0 2 4 6 8 10 12 14
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
time step (30 ms)
x-
di
r 
(m
/s
)
0 2 4 6 8 10 12 14
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
time step (30 ms)
y-
di
r 
(m
/s
)
0 2 4 6 8 10 12 14
-1.5
-1
-0.5
0
0.5
1
time step (30 ms)
z-
di
r 
(m
/s
)
-0.8-0.6-0.4-0.20
-1
-0.5
0
0.5
-0.2
0
0.2
0.4
0.6
0.8
x-dir (m/s)
z-dir (m/s)
y-
di
r 
(m
/s
)
 
圖 5.24 球拍之移動速度變化(11次打擊) 
 
  - 67 -
第六章 結論 
6.1 結果與討論 
本計畫改良先前本實驗室所建構之人形機器人，在雙眼機械頭加入四個攝影機
用以模仿人類視覺，並於頸部機構上加入四個自由度，使眼部能藉由頸部之彎曲及伸
展，來觀察靠近身體的物體。另外，將原本固定於輪式移動平台之身軀，改為一擁有
三自由度的腰部結構，使機器人能側身擊球，或撿拾起在輪式移動平台周圍的物體，
而增加機器人身軀之靈巧度。因此，新的具四十自由度之人形機器人，其上半身擁有
一個具三自由度的腰部的軀體、一個七自由度的雙眼機械頭、各七自由度之左右機械
手臂，以及各七自由度的左右機械手，而在下半身則為一具二自由度之輪式移動平
台。在機器人的控制上，我們發展以 DSP 晶片為核心的嵌入式控制系統，以取代原
PC-based控制系統，可將機器人的感測與控制，分散在多個處理器中進行計算，以增
加機器人控制的即時性，並得到最佳的控制性能。 
在打乒乓球的應用上，我們採用了映射的方法來預測球的軌跡，但是我們了解
此映射的關係是一組高度非線性的函數，因此我們透過類神經網路來學習此一關係，
可於離線時先完成初期訓練後，也應用於線上時繼續學習此關係，減少預測上的誤
差。由於所建構的人形機器人之手臂是具有類似人類手臂且為贅餘自由度的結構，所
以我們在進行拍擊時，可以以人的動作為基礎來進行規劃，在目前的研究中，並未深
入探討此一規劃，僅以數值上最佳化來完成機械手臂的控制，在未來的研究上，我們
將以此做深入探討。 
 
6.2計畫成果自評 
本計畫已完成打乒乓球人形機器人之建構，其主要是由輪移動平台、具腰部之
身軀、雙眼機械頭、雙機械手臂與雙機械手所組成；並完成嵌入式 DSP-based控制系
統之開發。在視覺伺服控制上，利用 SOIM類神經網路來學習影像資訊與雙眼機械頭
控制命令間的關係，配合灰色預測對於空間目標物進行視覺追蹤；並透過柯漢寧類神
  - 69 - 
參考文獻 
[1]. R. L. Andersson, “Computing the Feasible Configurations of a 7-DOF Arm Subject 
to Joint Limits,” IEEE Transactions on Robotics and Automation, Vol. 9, No. 2, pp. 
186-195, April. 1993. 
[2]. R. L. Andersson, “Understanding and Applying a Robot Ping-Pong Player's Expert 
Controller ”, Proceedings of IEEE International Conference on Robotics and 
Automation, pp. 1284-1289, 1989. 
[3]. R. L. Andersson, “Aggressive Trajectory Generator for a Robot Ping-Pong Player,” 
IEEE International Conference on Robotics and Automation, Philadelphia, PA, 
pp.188-193, April 1988. 
[4]. R. L. Andersson, A Robot Ping-Pong Player: Experiment in Real-Time Intelligent 
Control, The MIT Press, Cambridge, MA, 1988. 
[5]. H. Asada and J-J E. Slotine, Robot Analysis and Control, Wiley-Interscience, 1986. 
[6]. R. A. Brooks, C. Brazeal, M. Marjanovic, B. Scassellati and M. Williamson, “The 
Cog Project: Building a Humanoid Robot,” in C. Nehaniv (ed.), in Computation for 
Metaphors, Analogy, and Agents, Lecture Notes in Artificial Intelligence 1562, 
Springer-Verlag, pp. 52-87, 1999. 
[7]. R. A. Brooks and L. A. Stein, Building Brains for Bodies, MIT AI Lab Memo #1439, 
Aug. 1993. 
[8]. K. Hirai, M. Hirose, Y. Haikawa and T. Takenaka, “The Development of Honda 
Humanoid Robot P2 and P3,” Proceedings of 30th ISR, Tokyo, Japan, Oct. 1999. 
[9]. Q. Huang, Y. Nakamura and T. Inamura, “Humanoids Walk with Feedforward 
Dynamic Pattern and Feedback Sensory Reflection,” Proceedings of IEEE 
International Conference on Robotics & Automation, pp. 4220-4225, May 2001. 
表 Y04 1
???????????????????????????? 
                                                            95? 06? 08?
????? 
 
??? ????
??? 
 
????  ?????? 
??? 
     ?? 
?? 
     ?? 
 
? 94 ? 11 ? 29?? 94 
? 12? 01 ???????
????
????
 
NSC 94-2212-E-006-022 
?? 
?? 
 (??) ?????(2005?)????????? 
 (??)  36th International Symposium on Robotics (ISR 2005) 
?? 
?? 
?? 
 (??) ??????????????????????????? 
 (??) Behavior-Based Pose Control of Mobile Robots with an Uncalibrated 
Eye-in-Hand Vision System 
???????? 
??????????????(2005 International Symposium on Robotics; ISR
2005)??????????(IFR)????????(JARA)?????? 2005 ?
11? 29?? 12? 1??????? Keidanren Kaikan????????????
????????????????????????????????????
????????????? 
????????????????????????????(11 ? 29 ?)
?????????????(Plenary Speech)????????????????
???????????????:????????????????????
??????????????????????????????????
?????????????????????????????????????
???????????(11? 30??12? 1?)??????????????
??????(Special Invited Speech)????????????????(Mr. Joe 
Engelberger)???????????????? 1956 ????????????
????????????-Unimation Inc.????? 1961??????????
??????—Unimate?????????????????????????
???????????? 1975?????;????????????????
????????????????????????????????????
???????????????????????????????
?Behavior-Based Pose Control of Mobile Robots with an Uncalibrated Eye-in-Hand 
Vision System???????????Planning & Navigation II?????????
?????????????????? 
 
 1
Behavior-Based Pose Control of Mobile Robots 
with an Uncalibrated Eye-in-Hand Vision System 
 
T. I. James Tsay 
Department of Mechanical Engineering 
Nation Cheng Kung University 
Tainan, Taiwan 701, R.O.C. 
tijtsay@mail.ncku.edu.tw 
Ying-Feng Lai 
Department of Mechanical Engineering 
Nation Cheng Kung University 
Tainan, Taiwan 701, R.O.C. 
ufan0032@ms5.hinet.net 
 
Abstract 
 
 A mobile robot, which comprises a mobile base, a robot manipulator and a vision system, is a flexible material 
transfer system suitable for production lines with diverse products in small quantities. During pick-and-place operations 
between a predefined station and the mobile robot, positioning errors of the mobile base and the non-horizontality of 
ground inevitably cause position and orientation errors of the mobile base relative to the station. Therefore, this study 
employs an uncalibrated eye-in-hand vision system for controlling the manipulator to pick up a workpiece on the station. 
A vision-guided control strategy with a behavior-based look-and-move structure is proposed. This strategy is based on six 
predefined image features. In the designed neural fuzzy controllers, each image feature is taken to generate intuitively one 
DOF motion command relative to the camera coordinate using fuzzy rules, which define a specific visual behavior. These 
behaviors are then combined and executed in turns to perform grasping tasks. The experimental results reveal that the 
proposed vision-guided control strategy ensures that the mobile robot can perform pick-and-place operations on a 
non-planar ground without any special illumination. 
 
1. INTRODUCTION 
 The use of material transfer units constructed from 
AGVs and robot manipulators is becoming popular in 
production lines with small production volumes and 
periods, which must be quickly adapted to changes in the 
factory layout. This new material transfer unit is also 
called a mobile robot. A mobile robot is sufficiently 
mobile that it can very flexibly perform tasks in the 
production line. A guidance control system drives a mobile 
base with a robot manipulator mounted on it to a 
predefined station so the robot manipulator can pick up a 
workpiece in the station. However, the guidance control 
system inevitably causes positioning errors of the mobile 
base. These errors may lead to the failure of the following 
pick-and-place operation. During the pick-and-place 
operation, visual information is used to yield an intuitive 
solution to this problem. Therefore, a CCD camera is 
generally mounted on the end-effector of the robot 
manipulator to compensate for these errors using a visual 
servoing scheme. This eye-in-hand configuration has 
attracted considerable interest in the field of vision-based 
manipulator control. 
Industry has addressed problems of grasping without 
knowledge of the precise location of the target, using 
visual information to locate the target and control the 
manipulator. In the field of the Cartesian control vision 
systems, methods fall into two categories - position-based 
and image-based visual servo control systems [3][8]. The 
control law is obtained by measuring the features of the 
target extracted from each acquired image. 
Position-based control aims to eliminate the errors 
determined by the pose of the target with respect to the 
camera; the features extracted from the image planes are 
used to estimate the pose in space. The control law then 
sends the command to the joint-level controllers to drive 
the servomechanism. This control architecture is the 
so-called hierarchical control and is referred to as the 
dynamic position-based look-and-move control structure. 
The control architecture is referred to as the position-based 
visual servo (PBVS) control structure if the 
servomechanism is directly controlled by the control law 
mentioned above rather than by the joint-level controller. 
The errors to the control law in image-based control, are 
directly governed by the extracted features of the image 
planes. The control architecture is hierarchical and referred 
to as the dynamic image-based look-and-move control 
structure if the errors are then sent to the joint-level 
controller to drive the servomechanism. The visual servo 
controller eliminates the joint-level controller and directly 
controls the servomechanism. Such control architecture is 
termed an image-based visual servo (IBVS) control 
structure. 
Beyond the conventional visual servo control methods 
referred to above, behavior-based methods for visual servo 
control have already been developed in the literature 
[1][2][4][10][11]. A behavior-based system has been 
proposed to perform grasping tasks in an unstructured 
environment, in cases in which the position of the targets is 
not already known [10][11]. The controller maps input 
from the image space to the control values defined in the 
camera space. The control values are then transformed to 
joint controls by a Jacobain transformation, revealing that 
either a hand-eye calibration process has been 
implemented or the hand-eye relationship is known 
beforehand. 
This paper adopts an uncalibrated eye-in-hand vision 
system to provide visual information to a manipulator 
mounted on a mobile base. The vision system compensates 
for the uncertainties in location associated with a mobile 
base or the object. This work focuses on developing a 
 3
to the reference image at the target location are 1
rF , 2
rF , 
3
rF , 4
rF , 5
rF  and 6
rF . 
 
3.2. Motion Planning based on Behavior Design 
In this work, fuzzy rules are used to enable the 
controllers to map image features in the image space onto 
motion commands in the camera space. These are then 
transformed to the commands in relation to the 
end-effector frame, which eventually control the 
manipulator. The final control values are relative motion 
commands sent to the position controller of the 
manipulator. The primary motivation for this process is 
that, after the notion of the camera frame has been 
introduced, the control rules for implementing basic 
vision-based motions such as “Center” or “Zoom” can be 
very easily written. No analytical model of the system is 
required. 
1) Approach and Surround 
The complete manipulation task involved in 
implementing a human-like visual sevoing method is first 
divided into two complex behaviors - Approach and 
Surround - and one basic operation, Catch. The Approach 
behavior is the translational motion of the camera toward 
the workpiece, which is further divided into two basic 
behaviors - Center and Zoom. The Surround behavior is 
the orientational motion of the camera to keep the 
workpiece in the gripper, and is further divided into three 
basic behaviors - Yaw, Pitch and Roll. The Catch is a non 
vision-based operation. Only when the end-effector has 
reached the target location is the Catch activated and 
moves the end-effector 10 cm forward. The gripper then 
closes to grasp the workpiece. Fig. 3 displays the 
hierarchical composition of the behaviors, which are 
defined as follows. 
Center is based on the first two image features, 1F , 
and 2F . This behavior translates the camera along the 
C X  and CY  axes of the camera frame to keep the center 
of gravity of the quadrangular image at the desired pixel in 
the image plane;  
Zoom is based on 3F ; it moves the camera along the 
CZ  axis of the camera frame to keep the size of the object 
as a predefined value. 
Yaw is based on 4F ; it rotates the camera about 
C X  
to keep the ratio of the lengths of the two short sides equal 
to 4rF . 
Pitch is based on 5F ; it rotates the camera about 
CY  
to keep the ratio of the lengths of the two long sides equal 
to 5
rF . 
Roll is based on 6F ; it rotates the camera about 
CZ  
so that the principal angle equals that in the reference 
image, in which the gripper’s two fingers are arranged 
parallel to the short sides of target. 
Vision-based behaviors are defined from the 
perspective of an eye-in-hand camera, so movements are 
performed relative to the camera frame. Fig. 4 presents 
these behaviors. 
2)Neural Fuzzy Controller 
The main shortcoming of traditional fuzzy controllers 
is that they are unable to learn. The best control law or 
membership functions can be determined by experience. 
However, the manipulation tasks are non-linear and 
coupled. None set of membership functions is good for the 
entire work environment. With respect to learning capacity 
of the artificial neural network, back-propagation 
architecture is the most popular and effective for solving 
complex and ill-defined problems. Therefore, six simple 
neural fuzzy controllers (NFCs), employing 
back-propagation [5][7] are designed herein. One image 
feature is input to each controller, which changes one 
D.O.F. of the camera motion as the output. The 
back-propagation algorithm is used only to adjust the 
consequents of fuzzy rules for each neural fuzzy controller 
at each iteration during the manipulation. Restated, the 
camera is guided intuitively according to the image 
features on the image plane. For example, if the area of the 
quadrangular image is smaller than that in the reference 
image, the camera appears to be far from the workpiece, 
and so the camera is moved forward. Otherwise, the 
camera is moved backward. The other five D.O.F. of 
motion of the camera are controlled in the same manner. 
Fuzzy singleton rules are adopted to simplify the 
neural fuzzy controllers; they are defined as follows; 
j
iR : if iFδ  is jiA , then *C iXδ  is jiw  (1) 
where input variable iFδ  is an image feature error; output 
variable C iXδ ∗  denotes a relative motion command in the 
camera frame; jiA  are linguistic terms of the precondition 
part with membership functions ( )j
i
iA
Fµ δ , and jiw  
represent the real numbers of the consequent part, i = 1, 
2,…,6 and j = 1, 2,…,7. That is, each i = 1, 2,…, 6, can 
be regarded as a neural fuzzy controller with seven rules to 
control one D.O.F. of motion relative to the camera frame. 
Table 1 illustrates the fuzzy rules for each neural fuzzy 
controller. Herein, a simplified defuzzifier is used. The 
final output C iXδ  of the neural fuzzy system is 
calculated by,         
7
1
j
i
C j
i iA
j
X wδ µ
=
= ∑  (2) 
The parameter learning of NFC with fuzzy singleton 
rules can be the tuning of the real numbers jiw  and the 
input Gaussian membership functions ( )j
i
iA
Fµ δ , 
including the mean-points and the standard deviations [6]. 
In this investigation, the mean-points and the standard 
deviations of the input membership functions are fixed to 
simplify parameter tuning. Only real numbers jiw are 
tuned on-line. Accordingly, the error function to be 
minimized is defined by 
 5
However, gripper’s fingers are still not in the field of 
vision. An auto-focus function is exploited to maintain the 
sharpness of the workpiece image. The mobile base is 
stopped and fixed next to the workstation, which is roughly 
parallel to the surface of the workstation to verify the 
proposed control strategy. As presented in Fig. 10, the 
workpiece is placed in six different positions, which are 
separated by 15 cm, to simulate the possible position and 
orientation errors that arise in the application stage. In each 
position, the workpiece is pointed in three directions. It is 
placed at Pos2 and Pos5 in 0° and -45° directions, and is 
tilted by 3° and 6° to the station surface to simulate 
non-flat ground in the application stage. Before the 
manipulation is performed, one reference image is 
captured by applying the teach-by-showing method. The 
end-effector is initially driven to the target location, as 
shown in Fig. 2, using a teaching box. Then, the 
corresponding reference image features are extracted. 
The parameters of the above experimental setup to 
evaluate the positioning performance of the eye-in-hand 
manipulator are set as follows. In the approaching stage, 
the Approach and Roll behaviors are activated 
concurrently and are iteratively performed, guiding the 
end-effector from the top location toward the workpiece, 
based on the specified reference features 1
rF , 2
rF , 3
rF  
and 6
rF . Given the uncalibrated CCD camera and the 
imprecisely known hand-eye configuration, 1 2( , )
r rF F  
does not necessarily correspond to the center pixel of the 
image plane, and 6
rF  does not always equal zero degree. 
All of these three feature values depend on the captured 
reference image. However, the reference image feature 
3
rF , which presents the distance between the camera and 
the workpiece, is set to zero at all times. As the number of 
execution steps grows, the errors of the image features 
drop. This stage continues until the errors in the image 
features 1Fδ , 2Fδ , 3Fδ  and 6Fδ  are below 1 6ε = , 
2 6ε = , 3 0.1ε =  and 6 2ε = , respectively. In neural 
fuzzy controllers, the corresponding energy values 1E , 
2E , 3E  and 6E  are less than 18, 18, 0.005 and 2, 
respectively. In the fine positioning stage, the composition 
of the Yaw and Pitch behaviors and the composition of the 
Approach and Roll behaviors are iteratively activated in 
turns according to one set of image feature data, 4
rF  and 
5
rF , and the other set of image feature data, 1
rF , 2
rF , 3
rF  
and 6
rF , respectively. Those two feature values, 4
rF  and 
5
rF , depend on the captured reference image. This stage 
never terminates unless the errors in the image features 
1Fδ , 2Fδ , 3Fδ , 4Fδ , 5Fδ  and 6Fδ  are less than the 
specified values '1 2ε = , '2 2ε = , '3 0.01ε = , '4 0.01ε = , 
'
5 0.01ε =  and '6 0ε = . The corresponding energy values 
1 6~E E  are below 2, 2, 0.00005, 0.00005, 0.00005 and 0, 
respectively. Then, the Catch operation is executed. The 
Catch operation moves the end-effector 10 cm forward and 
closes the gripper to pick up the workpiece. 
The final location of the end-effector is recorded in 
the task manipulation. The coordinate transformation 
matrix between the final and desired locations of the 
end-effector can be written as follows. 
1( )
0 0 0 1
eFinal eFinal
eFinal eFinal eDesired eDesired eDesired
eDesired o o
R t
T T T −
⎡ ⎤= = ⎢ ⎥⎣ ⎦
 
0 0 0 1
x x x
y y yeFinal
eDesired
z z z
n t b X
n t b Y
T
n t b Z
⎡ ⎤⎢ ⎥⎢ ⎥⇒ = ⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
 (9) 
where eFinal
oT  corresponds to the final location of the 
end-effector with respect to the world frame, and eDesired
oT  
corresponds to the desired location for the end-effector 
with respect to the world frame. In each test, (10) gives the 
position error and (11) specifies the orientation error as the 
angle of rotation between the two coordinate frames about 
the principle axis. 
2 2 2
positionError X Y Z= + +  (10)           
2 2 2
1
( ) ( ) ( )
tan
1
z y x z y x
orientation
x y z
t b b n n t
Error
n t b
−
⎛ ⎞− + − + −⎜ ⎟= ⎜ ⎟+ + −⎝ ⎠
 (11) 
Table 2 presents the resulting positioning errors. In all 
of the tests, the final position error is less than 2.9 mm, and 
the final orientation error is less than 1.5o. 
 
5. CONCLUSION 
 Mobile manipulators commonly need a guidance 
control system to navigate the mobile base and a method 
for performing the pick-and-place operations. The 
guidance control system and the non-planar ground 
inevitably cause the position and orientation errors of the 
mobile base. Therefore, this study proposes a 
behavior-based control strategy that employs an 
uncalibrated eye-in-hand vision system to control the 
end-effector of the manipulator to approach and grasp the 
target workpiece. All the designed behaviors are defined 
from the perspective of the camera and are mediated 
through fuzzy rules with a look-and-move control structure. 
The presented neural fuzzy controllers map image features 
in image space to relative motion commands in the camera 
frame. These motion commands are then transformed to 
the end-effector frame by the proposed rough motion 
transformation. 
This work differs from the references [10][11] as 
follows. (1) A back-propagation algorithm is used to 
reduce image feature errors through the adjustment of the 
singletons of the consequent parts in the fuzzy singleton 
rules. (2) Two additional image features, the ratios of the 
lengths of the two pairs of opposite sides on the 
quadrangular image, are extracted to guide the rotation of 
the camera when the workpiece has a tilt angle. (3) The 
control values defined in the camera frame generated from 
the controller in [10] are transformed to joint controls by a 
Jacobain transformation, revealing that either a hand-eye 
calibration process has been implemented or the hand-eye 
relationship is known beforehand. However, this study 
 7
)( ZC
)( XC
)( YC
Zoom
Center
Roll
Yaw
Pitch
CCD
Camera
Object
 
Fig. 4  Motion associated with defined behaviors 
 
 
E X
EY
EZ
CCD
Camera
Fingers
XC
YC
ZC
E X
EY
EZ
XC
YC
ZC
dz
dx
dk
 
Fig. 5  End-effector and camera frame 
 
 
EY
YC
ZC
ZE
dx
4X
Cδ
'
3X
Cδ
'
2X
Cδ
XE
XC
 
Fig. 6  Unexpected camera displacement caused by a rotation 
about E X  
 
 
Fig. 7  Behavior-based look-and-move control structure 
Grab Image
Y
N
Catch
Start
i iFδ ε<
6,3,2,1=i
Approaching
Stage
'
i iFδ ε<
6~1=i
Approach+Roll
Fine Positioning
Stage
'
4 4Fδ ε<
'
5 5Fδ ε<
'
i iFδ ε<
6~1=i
'
i iFδ ε<
6,3,2,1=i
Approach+Roll
Grab Image
Grab Image
Yaw+Pitch
N
N
Y
Y
Y
N
N
Y
 
Fig. 8  Control strategy flowchart for manipulation 
 
 
 
  Calculate 1 2 3 6, , , , ,0 0C C C CX X X Xδ δ δ δ⎡ ⎤⎢ ⎥⎣ ⎦
1 2 3 6, , ,F F F Fδ δ δ δ
Rough Motion Transformation
      Send 1 2 3 6, , , , ,0 0E E E EX X X Xδ δ δ δ⎡ ⎤⎢ ⎥⎣ ⎦
  Tune Fuzzy Singleton
Approach+Roll  
 
 
      Calculate 4 5, , , , ,0 0 0 0C CX Xδ δ⎡ ⎤⎣ ⎦
4 5,F Fδ δ
Rough Motion Transformation
   Send 1 2 3 4 5, , , , ,0E E E E EX X X X Xδ δ δ δ δ⎡ ⎤⎢ ⎥⎣ ⎦
  Tune Fuzzy Singleton
Yaw+Pitch  
Fig. 9  Flowcharts for the combined behaviors in the control 
strategy 
