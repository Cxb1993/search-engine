 1
目錄 
 
一、 中文摘要               2 
二、 英文摘要               3 
三、 計畫緣由與目的             4 
四、 結果與討論              5 
五、 計畫成果自評              15 
六、 參考文獻               15 
七、 附錄（已發表論文）            16 
 3
二、英文摘要 
 
The current state-of-the-art approach to the automatic speech recognition (ASR) is based on 
the data-driven concept, which learns speech by constructing acoustic and language models 
exclusively from corpora. Unfortunately, even the best ASR system nowadays gives much 
higher error rates in some rather simple tasks, compared to the speech recognition of human 
beings. Though some specifically-suitable applications do exist, ASR performance is still far 
away from user's expectation. Until more recently, many researchers suggest that we should 
go back to bring in the linguistic and phonological knowledge, which has been largely 
ignored during this decade. The next generation ASR should follow a paradigm of integrating 
both knowledge-based and data-driven techniques. To make this paradigm a reality, a shared 
platform for system design and assessment must be set up for bringing in more researchers 
together to this area. This proposal attempts to set up a shared platform for ASR research in 
Taiwan. Several universities and research organizations will join together to carry out this 
collaborative project so as to promote the fundamental research of the next generation ASR. 
The major research items include (1) the detection of speech attributes and events, (2) the 
integration of speech events and relevant knowledge, (3) the verification of events and 
evidence, (4) the design of knowledgebases, speech corpora, speech and language models, 
and software tools, and (5) the shared platform for joint design and assessment. Under this 
collaborative mode, the knowledgebases, corpora, models, and software tools generated will 
be opened to public so as to bring in more researchers to the ASR area. This gives a decisive 
chance to promote the research and development of speech technology in Taiwan. 
As an indispensable part of the whole collaborative project, this subproject focuses on: 
(1) establishment of speech corpora for benchmark test, (2) analysis of tags in the speech 
corpora, (3) study on cues for speech/non-speech detection, (4) study on segmental features 
for ASR, (5) study on prosodic features for ASR, (6) study on cues for language identification, 
and (7) development and evaluation of tools and models for ASR. 
 
Keywords: speech recognition, knowledgebase, speech corpora, benchmark, shared platform, 
speech attributes, speech events 
 
 
四、結果與討論 
 
本計畫執行期限為94年8月1日至97年7月31日，主要工作項目包括：(1)開發自動音素分
段(automatic phoneme segmentation)技術，以加速語料庫標記的進度；(2)進行國語語料庫
的音素自動標記及人工修訂；(3)利用支向機(support vector machine, SVM)分類器，探討
各種語音特徵線索；(4)探討各種語音特徵系統(phonological feature system)及利用類神經
網路—多層感知器(multi-layer perceptron, MLP)建構語音事件偵測器；(5)利用條件隨機
域(conditional random fields, CRF)建構基於偵測之自動語音辨識系統(detection-based 
automatic speech recognition system)。 
 
A.HMM自動音素分段 
 
目前最廣為使用的自動音素分段方式是利用以最大相似度(maximum likelihood, ML)法
則訓練得到的音素 HMM 模型，透過維特比(Viterbi)演算法，在語音信號上根據人工標
記的音素序列(phoneme sequence)進行強迫校準(forced alignment)，由於 ML 訓練法則與
希望達成的音素邊界誤差最小的目標並不一致，傳統的 HMM 模型切音無法提供可靠的
精確度。有鑒於此，我們提出最小邊界誤差(minimum boundary error, MBE)訓練法來取
代傳統的 ML 訓練方式。 
假設有 R 句的訓練語料的特徵向量序列，即 { }ROOO ,..,1= 。所有語句期望邊界誤差
期望值之總和則定義為： 
∑∑
= ∈
=
R
r
S
r
c
r
i
rr
iMBE rr
i
SSEROSPF
1
),()|(
Φ
，     （1） 
其中 又稱作目標函式(objective function)，是訓練過程要最小化的函式。 是一條
邊界序列，而 則為所有可能邊界序列所形成的集合； 為 在給定 時的
事後機率； 則定義為 與實際邊界 的誤差，即序列中每個音素邊界誤差之
和。利用貝氏定理(Bayes' rule)，並假設 為均勻分佈，則目標函式可改寫為： 
MBEF
r
iS
rΦ )| rri OSP( riS rO
),( rc
r
i SSER
r
iS
r
cS
)( riSP
∑∑ ∑= ∈ ∈ Λ
Λ=
R
r
r
c
r
iS
S
r
k
r
r
i
r
MBE SSERSOp
SOpF rr
i
rr
k
1
),(
)|(
)|(
Φ
Φ
α
α
，    （2） 
其中 為利用 HMM 模型所求得的相似度，)|( rir SOpΛ Λ為模型參數，α 是一個介於 0~1
的控制常數，用來縮減邊界序列機率的動態範圍(dynamic range)，防止分母項被機率最
高的邊界序列唯一決定。由於 中包含相當多的序列，為了增加實作的可行性，我們
只保留一些較高機率的序列，使用較小的集合來近似 。此外，為了減少重覆資訊的
儲存，本計畫使用音素網格圖(phoneme lattice)來實作，並將這些邊界序列所成的集合記
作 。由於直接最小化目標函式無法求得閉解(closed-form solution)，故我們使用 EBW 
(extended Baum-Welch)演算法來進行最佳化。最後，模型中高斯混合(Gaussian mixture) m
的平均值向量(mean vector)
rΦ
rΦ
r
LatΦ
mμ 及共變異矩陣(covariance matrix) mΣ 估測分別為： 
m
MBE
m
mm
MBE
m
m D
DO
+
+= γ
μθμ )( ，      （3） 
[ ] T
mm
m
MBE
m
T
mmmm
MBE
m
m D
DO μμγ
μμθ −+
+Σ+=Σ )(
2
，    （4） 
其中 為一個高斯混合層次的控制常數，用來確保共變異矩陣為正定(positive definite)
矩陣；
mD
mμ 與 mΣ 則為前一次估測的參數；統計值 、 及 分別為： )(OMBEmθ )( 2OMBEmθ MBEmγ
 5
其中 表示 所包含的音素個數， 為 中第 個音素， 為 中第 n 個音素，
為此二音素的實際邊界誤差。同樣，為了增加實作的可行性，我們使用 來
近似 ，可得到 
N O nq S n
j
nq jS
),( jnn qqer LatΦ
Φ
),()|(minarg
),()|(minarg
1
1
j
nnj
S
N
nS
j
nnj
N
nSS
MBE
qqerOSP
qqerOSPS
j
j
∑∑
∑∑
∈=
=∈
=
=
Lat
Lat
Φ
Φ 。    （15） 
我們定義切集(cut) 為音素網格圖上所有第 個音素段落(phoneme arc)所成的集合，則
式(15)可再化簡成： 
nC n
{ } ),()|(minarg ,|1 ,, mnnjSqSq
N
nS
MBE qqerOSPS
jmnjnmn
∑∑∑
∈∈∈=
=
LatΦC
，  （16） 
其中 為 中第m 個音素段落。由於 可使用前向-後向演算法求
得，計作
mnq , nC { } )|(,| OSP jSqS jmnj∑ ∈∈ LatΦ
mnq ,
ρ 。擁有最小邊界誤差的邊界序列可進一步由動態規劃(dynamic programming)
演算法來搜尋，即 
),(minarg ,
1 ,,
mnnq
q
N
nS
MBE qqerS
mn
nmn
ρ∑∑
∈=
=
C
。    （17） 
然而，最小邊界誤差校準雖然可以找到更佳的邊界序列，但整個過程的複雜度提高
不少，主要原因在於最小邊界誤差校準為一種後處理的搜尋方法。第一階段需先產生音
素網格圖，包含大量的候選邊界序列，於第二階段時供最小邊界誤差校準搜尋，故執行
時間複雜度較高。 
由於 HMM 對時間持續所產生的機率分佈並不合乎實際統計所得的分佈，在過去，
有許多時間持續模型(duration model)被提出來解決此問題，如 hidden semi-Markov model 
(HSMM)、expanded state HMM (ESHMM)及後處理時間持續模型(post-processor duration 
model)等。後處理時間持續模型需於第一階段產生大量的候選序列，供第二階段結合時
間持續模型機率來找尋。故本計畫嘗試結合後處理時間持續模型及最小邊界誤差校準來
克服 HMM 的不足。時間持續模型機率是以音素段落為單位，故音素段落的相似度將修
正為： 
)()()(~ τβ qdqpqp ⋅= ，      （18） 
其中 為音素段落， 為原來的相似度，q )(qp )(~ qp 為修正之後的相似度， )(τqd 則表示 持
續時間為
q
τ 時的機率。 β 則是用來平衡兩項的動態範圍。本計畫使用非參數式
(nonparametric)的時間持續機率分佈，此分佈由訓練語料統計而來。 
本計畫以 TIMIT 英文語料作為實驗平台，用來測試所提出方法的成效。實驗語料中
分別包含了約 4,500 句訓練及 1,600 句測試語料。聲學模型則採用 50 個由左至右
(left-to-right)前後文無關 (context-independent) 的 HMM 模型，每個模型用來描述一個音
素。在特徵向量方面，則是採用 39 維經過變異數正規化 (cepstral variance normalization) 
的梅爾倒頻譜係數(MFCC)，其中音框寬度為 20ms，音框位移為 5ms。 
我們使用 HTK Toolkit 中的 HRest 工具進行 10 次的 ML 訓練，以得到 HMM 初始模
型。圖一說明進一步進行 10 次最小誤差訓練測得之音框錯誤率，藍、綠曲線分別代表
在訓練及測試語料上進行自動切音的實驗結果，紅曲線則是訓練時目標函數的傳回值
(所有語句期望邊界誤差期望值之平均)。結果顯示，隨著訓練次數漸增，不論在訓練語
料或是測試語料上測得的音框錯誤率也隨之下降。 
表一則包含所有組合的實驗結果：平均邊界誤差(毫秒)及在不同容忍誤差下邊界的
準確率(%)。表中D.M.表示於校準時結合使用時間持續模型，第九列(改進幅度)則是比較
ML10+MBE10/ MBE+D.M.與ML20/ ML所得到的結果。比較第二列及第三列可發現，多增加
10 次ML訓練，帶來的效果其實有限，這意謂著ML對HMM而言幾已達到過度訓練
 7
 9
distance)及頻譜特徵轉變率(spectral feature transition rate)，組成一個 92 維度的特徵向量。 
理想上我們可以針對每一種音素轉換(phone transition)訓練一個支向機分類器。但並
非每一種音素轉換的訓練資料皆充足，為了在有限的訓練資料和複雜的模型中取得平
衡，我們利用群聚法將信號特性相近的音素邊界集合在一起，並集合群聚中的轉換共同
訓練一個音素轉換相關的(phone-transition-dependent)支向機分類器。我們實作了兩種群
聚方法：K-均數(K-means)群聚法和決策樹(decision tree)群聚法。 
K-均數音素轉換群聚法描述如下：對每一種音素轉換，取出所有屬於該種音素轉換
之人工標記邊界對應的特徵向量，並計算其平均值。我們將音素轉換分為四大類：響音
(sonorant)接非響音、響音接響音、非響音接非響音和非響音接響音，並分別對屬於這四
大類的平均值向量做 K-均數分群。資料不夠的音素轉換不納入分群，而是待分群結束
後，歸入距離其平均值向量最近的群聚中心(cluster center)所屬的群聚。 
K-均數音素轉換群聚法的缺點是待測資料的音素轉換若未曾在訓練資料中出現，則
不知道該歸屬於哪一類，而決策樹群聚法則可避免這個問題。我們建立了以下形式的問
題集「轉換邊界左邊的音素是否屬於集合 X，而右邊的音素是否屬於集合 Y？」我們根
據語音學的知識定義了大小不等的 397 個集合，大者如響音、母音集合，小者則只包含
單一種音素。 
實驗結果如表二所示，HMM*為表一中HMM自動音素分段的最佳結果。支向機在
HMM自動音素分段的初始邊界左右各 5 毫秒範圍內尋找最佳的邊界。SVMKM是採K-均
數分群的結果，而SVMDT則是決策樹分群的結果。我們發現兩者皆可降低音素分段的誤
差，SVMKM略勝於SVMDT。 
 
不同容忍誤差下邊界的準確率(%) 訓練方式 平均邊界誤差 (毫秒) ≦5ms ≦10ms ≦20ms ≦30ms 
HMM* 7.14 59.58 81.57 93.73 97.17 
HMM*+SVMKM 6.83 62.07 83.70 94.12 97.28 
HMM*+SVMDT 6.75 62.47 84.00 94.33 97.35 
表二、TIMIT 語料下平均邊界誤差(ms)及在不同容忍誤差下邊界的準確率(%)。 
 
 
C. 國語語料音素自動標記及人工修訂 
 
MATBN (Mandarin Across Taiwan Broadcast News) 電視新聞語料為本實驗室耗時三年
與公共電視合作錄製完成，共收錄 198 天、每天一個小時的公視晚間新聞。所有的
MATBN 語料都含有正確轉譯文句以及音樂、背景雜訊、停頓、語助詞、呼吸、強調語
氣、反覆、不適當的發音等標記，這些轉譯文句與 SGML 標記均使用 DGA&LDC 的轉
寫器(Transcriber)來完成。由於 MATBN 語料含豐富標記資訊，使得本語料的應用價值
大為提高。為配合本整合型計畫的需求，本子計畫從中選出約五小時的語料，進一步進
行人工音素邊界的標記，建立之標準語料庫將作為總計畫及各子計畫之測試基準。 
為了節省人工標記的成本，本計畫利用 HMM 自動音素分段與 SVM 音素邊界調整
來提供初始音素標記，作為後續人工校正與驗證的基礎。本計畫將五小時的語料分成數
個五分鐘的子語料集，對第一個子語料集，我們使用非監督式訓練來估測模型參數，產
生初始音素邊界供人工校正，並於完成後作為監督式訓練的語料，用來產生下一子語料
集的初始音素邊界。為了避免語料過少引起的過度訓練(over-training)問題，我們也將未
標記的語料加入訓練語料中，用來提供更可靠的模型參數估測。當一子語料集經過人工
修正之後，將納入下一子語料集的監督式訓練語料，以提高下一子語集的音素邊界標記
正確率，如此反覆的進行，直到所有的子語料集都被修正完成。如此，每一階段具人工
頻譜中 100Hz 至 400Hz 的能量總合。 
我們對六大類語音分別訓練一個一對多(one against the rest)支向機分類器。對每個
分類器而言，首先搜集訓練語料中屬於該類特性的音框特徵向量當正例，其他類的音框
特徵向量則當反例。我們採用縮減支向機(reduced SVM)演算法來訓練支向機。對測試語
料中的每一個音框，辨認結果為六個分類器中輸出值最大的那一類。表四和表五分別表
示未加入聲學特徵參數和加入聲學特徵參數的實驗結果。表格中的數字代表該列語音特
性的音框被辨認成該欄語音特性的比例。 
 
 
 
approxi
-mant nasal stop fricative  silence vowel
 83.5 1.2 0.9 5.7 5.6 silence 3.5 
 
 
 
 
 
 
 
vowel 0.9 79.6 11.93 3.9 2.55 1.25 
approxi
-mant 0.9 33.3 53.3 8.0 3.3 1.1 
nasal 3.8 8.4 6.8 77.2 2.6 1.1 
stop 9.3 9.7 7.2 4.0 57.3 12.4 
fricative 6.1 2.4 
表四、未加入聲學特徵參數時的六大類語音混淆矩陣。 
1.5 2.9 11.6 75.5 
 silence vowel approxi-mant nasal
 
 
 
 Stop fricative 
silence 85.7 1.1 0.6 4.2  5.4 3.0 
 
 
 
 
 
 
 
vowel 0.9 12.8 3.5 2.0 0.8 80.0 
approxi
-mant 0.9 29.7 59.7 6.1 2.9 0.6 
nasal 3.6 7.6 5.1 80.3 2.6 0.8 
stop 8.5 8.6 6.2 3.3 62.8 10.6 
fricative 5.2 2.7 1.2 2.9 10.9 77.2 
表五、加入聲學特徵參數後的六大類語音混淆矩陣。 
 
 
E. 語音特徵系統探討及語音事件偵測 
 
在語言學領域中，研究學者曾提出了許多不同的語音特徵系統，用來分析人類語音的特
性，若能適當將這些語言學知識應用於語音辨識系統，應該可以某種程度提升辨識系統
的效能。基於此一想法，我們參考了 King 與 Taylor 在 2000 年發表的期刊論文中所列舉
的三套語音特徵系統，做為我們語音事件偵測的標的。三套語音特徵系統簡介如下： 
 
1、英語聲學模式特徵(Sound Pattern of English (SPE) feature)系統—此系統由 Chomesky
與 Halle 在 1968 年所提出，基於語音的發聲原理訂定一組 13 維的二元特徵(binary 
features)，透過此 13 維語音特徵的組合，即可表示所有英語中發音音素。此 13 維特
徵外加一靜音(silence)特徵列舉於表六。 
2、複合聲學特徵(Multi-Valued (MV) phonological feature)系統—係整理語言學中廣泛使
用的六種聲學特徵分類，如：發聲方式(manner)、發聲部位(place)…等成為一套特徵
系統。任一語言之任一語音音素均可在此六種聲學分類上分別對應一特徵值。此六
種聲學特徵分類類別列舉於表七。 
 11
 13
型，用以觀察不同特徵系統對自動語音辨識正確率的影響。在條件隨機域模型的設定
上，我們使用二元值特徵(binary-valued feature)，即僅以各語音事件是否發生做為模型之
輸入；在訓練上，我們採用兩種不同的方法：1、使用由訓練語料提供之正確語音事件
發生之音框標記做為訓練資料(oracle train, OT)，2、使用前述多層感知器(MLP)直接對訓
練語料進行偵測，所產生帶有若干偵測錯誤的語音事件發生音框標記做為訓練資料
(detection-based train, DT)。 
 在測試時，條件隨機域模型均使用多層感知器所偵測之語音事件音框標記做為輸
入，而以音素序列做為辨識結果輸出。實驗語料同樣使用 TIMIT 英文語料庫，表九分別
顯示三種語音特徵系統搭配不同訓練方式的基於偵測之自動語音辨識結果，其中準確率
%(accuracy) = 正確率%(correction) – 插入率%(insertion rate)。表九同時亦列出了傳統基
於 HMM 之自動語音辨識的結果做為比較。我們發現，使用正確語音特徵值訓練之條件
隨機域模型已能達到和傳統 HMM 系統匹敵的正確率，然而，三系統卻都因嚴重的插入
率使得準確率不佳；相對的，使用前端事件偵測器產生之語音特徵值做為訓練資料的條
件隨機域模型，由於訓練與辨識條件匹配，因此大符改善了插入率的問題，從而提升辨
識準確率。從表九我們還可發現，三個語音特徵系統相比之下，SPE 與 GP 系統相較於
MV 系統，在辨識效果上，有較好的表現。 
 
 正確率 (%) 準確率 (%) 
基於 HMM 之自動語音辨識 69.02 63.45 
SPE 66.19 29.68 
MV 59.24 30.33 
使用正確語音特徵值訓練之基於偵測
之自動語音辨識 (OT) 
GP 69.03 31.38 
SPE 56.56 55.27 
MV 51.84 50.68 
使用前端事件偵測器產生之語音特徵
值訓練之基於偵測之自動語音辨識 
(DT) GP 55.74 54.53 
表九、基於偵測之自動語音辨識之自由音素辨識(free phone decoding)結果比較。 
 
 為進一步比較三種語音特徵系統對語音辨識所造成之影響，我們分別對使用此三種
語音特徵系統作為正確語音特徵值訓練與測試的辨識結果，進行音素間混淆度分析。期
望能藉此觀察出是否會因使用不同特徵系統，而使語音辨識在不同音素間處理能力有所
不同，表十列出我們的觀察結果。我們發現，以混淆度來說，使用 MV 特徵系統最可能
造成後端整合後的音素辨識發生錯誤，相較之下，GP 特徵系統之音素間的混淆情形最
輕微。另一項有趣的發現是此三系統前五名最頻繁的混淆音素配對彼此不同，意味我們
可以預期此三語音特徵系統在辨識結果上，將能有互補效果。 
 
語音特徵
系統 
混淆音素
配對數 
前 5 名混淆音素配對與辨識結果
中混淆出現次數 
SPE 38 (iy,dh):1809 (z,aw):1236 (p,ey):956 (m,en):939 (f,v):911 
MV 59 (iy,ih):995 (s,sh):395 (er,ah):394 (ey,iy):371 (ae,ah):315 
GP 14 (el,sil):163 (uh,ah):126 (w,uw):64 (y,ih):39 (ah,sil):6 
表十、使用不同語音特徵系統進行音素辨識，所產生之可能混淆音素配對比較。 
 
 基於此項發現，我們使用條件隨機域(CRF)模型再對先前建立之辨識系統進行系統
整合，實驗結果列於表十一。我們發現結果的確印證我們稍早的假設：透過整合三特徵
系統(使用正確語音特徵值訓練之基於偵測之自動語音辨識)的辨識結果，我們成功將辨
識準確率大幅提升至 60.65%，若更進一步再將此三系統與傳統基於 HMM 之自動語音
辨識系統進行整合，辨識率又可再提升至 64.31%，已略微超越傳統基於 HMM 之自動
 15
五、計畫成果自評 
 
本三年期計畫已獲得若干研究成果，在自動音素分段方面，我們提出一套基於最小
邊界誤差(minimum boundary error, MBE)的自動音素分段解決方案，包括基於 MBE 的
HMM 模型訓練及基於 MBE 的音素強迫校準(forced alignment)，我們並整合時間持續模
型於強迫校準及進一步利用支向機進行邊界微調。這部份研究成果已經發表四篇會議論
文，並獲邀收錄於東方語言語料庫專門會議(Oriental-COCOSDA)十週年慶專書，期刊論
文則整理撰寫中。我們也利用提出的方法進行國語語料音素自動標記，目前已經完成近
20 分鐘語料的人工修訂，這項工作仍持續進行中，實驗證明隨著人工修訂語料的增加，
可以逐步提高模型的精確度，降低自動標記的誤差，減低人工修訂的成本。完成人工修
訂的語料已交由其他子計畫測試中。在語音事件偵測方面我們先進行靜音、母音、半母
音、磨擦音、鼻音和塞音六大類語音事件偵測的初步實驗，然後進行三種語音特徵系統
(Sound Pattern of English (SPE) feature, multi-valued (MV) feature, and Government 
Phonology (GP) feature)應用於基於偵測之自動語音辨識的可行性評估，實驗結果顯示
SPE 及 GP 兩種系統表現較佳。在本計畫的最後階段我們並完成自動語音辨識系統的整
合，初步的結果顯示基於偵測之自動語音辨識與傳統基於 HMM 的自動語音辨識效能相
當，如果我們用條件隨機域(CRF)將兩個語音辨識系統整合，辨識率可以再提升。 
 
六、參考文獻 
 
[1]  J. W. Kuo and H. M. Wang, "Minimum Boundary Error Training for Automatic Phonetic 
Segmentation," Interspeech2006-ICSLP. 
[2]  J. W. Kuo and H. M. Wang, "A Minimum Boundary Error Framework for Automatic 
Phonetic Segmentation," Int. Symposium on Chinese Spoken Language Processing 
(ISCSLP2006). 
[3]  H. Y. Lo and H. M. Wang, "Phonetic Boundary Refinement Using Support Vector 
Machine," ICASSP2007. 
[4]  J. W. Kuo, H. Y. Lo, and H. M. Wang, "Improved HMM/SVM Methods for Automatic 
Phoneme Segmentation," Interspeech2007-EUROSPEECH. 
[5]  H. M. Wang, J. W. Kuo, and H. Y. Lo, "Improved HMM/SVM Methods for Automatic 
Phoneme Segmentation," to appear in Resources and Standards of Spoken Language 
Systems – Advances in Oriental Spoken Language Processing, co-edited by S. Itahashi 
and C.-Y. Tseng. 
[6]  I. F. Chen and H. M. Wang, "An Investigation of Phonological Feature Systems Used In 
Detection-Based ASR," submitted to Int. Symposium on Chinese Spoken Language 
Processing (ISCSLP2008). 
distributed, and the likelihood  of alignment  is 
governed by the acoustic model parameter set Λ . Therefore, 
Eq.(1) can be rewritten as: 
)|( ir SOP iS
 ,)(
)α|
)|
1
∑ ∑ ∑ (
(=
= ∈ ∈ Λ
ΛR
r
iS
S kr
ir
MBE SER
SOP
SOPF
ri
rk
Φ
Φ
α
 (2) 
where α  is a scaling factor that prevents the denominator 
 being dominated by only a few 
alignments. If 
∑ ∈ Λ (rkS kr SOPΦ )|
α  is set to zero, all the hypotheses are equally 
weighted. Accordingly, the optimal parameter set can be 
estimated by minimizing the objective function defined in 
Eq.(2), i.e.,  
*Λ
.)(
)|
)|
minarg
1
* ∑∑ ∑= ∈ ∈ Λ
Λ
Λ (
(=Λ R
r
iS
S kr
ir SER
SOP
SOP
ri
rk
Φ
Φ
α
α
    (3) 
The boundary error  of the hypothesized alignment 
 can be calculated as the sum of the boundary errors of the 
individual phones in , i.e., 
)( iSER
iS
iS
  (4) ,)()( ∑= ∈ iSqi qerSER
where q is a phone involved in ;  is a phone boundary 
error function defined as, 
iS )(⋅er
 ),(5.0)( qqqq eessqer ′−+′−×=  (5) 
where  and  are, respectively, the hypothesized start 
time and end time of phone q; and  and correspond to 
the manually labeled start time and end time, respectively. 
Since  contains a huge number of hypothesized phonetic 
alignments, it is impractical to sum the boundary errors 
directly without first pruning some of the alignments. For 
efficiency, it is suggested that a reduced hypothesis space, 
such as an N-best list [5] or a lattice (or graph) [6], should be 
used. However, an N-best list often contains too much 
redundant information, e.g., two hypothesized alignments can 
be very similar. In contrast, as illustrated in Figure 1, a 
phonetic lattice is more effective because it only stores 
alternative phone arcs on different segments of time marks 
and can easily generate a large number of distinct 
hypothesized phone alignments. Although it cannot be 
guaranteed that all the phonetic alignments generated from a 
phonetic lattice will have higher probabilities than those not 
presented, we believe that the approximation will not affect 
the segmentation performance significantly.  
qs qe
qs′ qe′
rΦ
2.1 Objective function optimization and update 
formulae 
Eq.(3) is a complex problem to solve because there is no 
closed-form solution. Even so, some iterative techniques, such 
as the Expectation Maximization (EM) algorithm, can be 
applied to solve it. Since the EM algorithm maximizes the 
objective function, we reverse the sign of our objective 
function and re-formulate the optimization problem as, 
.)(
)|
)|
maxarg
1
* ∑∑ ∑= ∈ ∈ Λ
Λ
Λ (
(−=Λ R
r
iS
S kr
ir SER
SOP
SOP
ri
rk
Φ
Φ
α
α
   (6) 
However, the EM algorithm can not be applied directly 
because the objective function comprises rational functions 
[7]. The extended EM algorithm, which utilizes a weak-sense 
auxiliary function [8] and has been applied in the minimum 
phone error (MPE) discriminative training approach [9] for 
ASR, can be adapted to solve Eq.(6). The re-estimation 
formulae for the mean vector mµ  and the diagonal covariance 
matrix mΣ  of a given Gaussian mixture m thus derived can be 
expressed, respectively, as: 
 ,)(
m
MBE
m
mm
MBE
m
m
D
DO
+
+= γ
µθµ  (7) 
and 
 [ ] .)( 2 Tmm
m
MBE
m
T
mmmm
MBE
m
m
D
DO µµγ
µµθ −+
+Σ+=Σ  (8) 
In Eqs. (7) and (8),  is a per-mixture level control constant 
that ensures all the variance updates are positive; 
mD
mµ  and mΣ
(r
 
are the current mean vector and covariance matrix, 
respectively; and θ ,  , and are, 
respectively,  statistics defined as: 
)(OMBEm )(
2OMBEmθ MBEmγ
  (9) ),()()( totO r
r
qm
MBEr
q
et
stq
r
MBE
m
q
q
lat
r
γγθ ∑∑∑=
=
=∈Φ
  (10) ,)()()()( 2 Trrrqm
MBEr
q
et
stq
r
MBE
m tototO
q
q
lat
r
γγθ ∑∑∑=
=
=∈Φ
and 
  (11) ).(trqm
MBEr
q
et
stq
r
MBE
m
q
q
lat
r
γγγ ∑∑∑
=
=∈
=
Φ
In Eqs. (9), (10), and (11),  is the occupation 
probability for mixture m on q, o  is the observation vector 
at time t, and Φ  represents the lattice for sentence O . 
 is computed by 
)(trqmγ
)t
lat
r r
MBEr
qγ
 ( ),rqravgrqMBErq ηηγγ −=  (12) 
where  is the occupation probability of phone arc q, also 
referred to as its posterior probability; η  is the weighted 
average boundary error of all the hypothesized alignments in 
the lattice; and  is the weighted average boundary error of 
r
qγ
r
avg
r
qη
 
Figure 1: An illustration of the phonetic lattice for the speech 
utterance “where were they?”. The lattice can be generated 
by performing a beam search using some pruning techniques. 
w 
w
eh
eh w
w 
ih 
ih 
sil
w
eh 
eh
w 
eh 
eh
ih 
w
dh
ih 
sil w
w 
w 
w 
ih 
dh
dh
dh
dh 
ey
ey
ey
ih 
ih 
ih 
The line with diamonds and the line with rectangles represent 
the FER results of the training (inside test) and testing sets, 
respectively. We observe that the ML-trained acoustic models 
yield FER of 10.31% and 11.77%, respectively, for the 
training and testing sets. In contrast, with 10 iterations, the 
MBE-trained acoustic models yield FER of 6.88% and 9.25%, 
respectively. The MBE discriminative training approach 
achieves a relative FER reduction of 33.27% on the training 
set and 21.41% on the testing set. The results clearly show that 
the MBE discriminative training approach performs very well 
and can enhance the performance of the acoustic models 
initially trained by using the ML criterion.  
Table 1 shows the percentage of phone boundaries 
correctly placed within different tolerances with respect to 
their associated manually-labeled phone boundaries. The 
experiment was conducted on the testing set. We observe that 
the MBE-trained models with 10 iterations (ML10+MBE10) 
outperform their seed models, i.e., the ML-trained models with 
10 iterations (ML10), and the ML-trained models with 20 
iterations (ML20). We also observe that the I-smoothing 
technique can only slightly improve the performance. The last 
row of Table 1 shows the absolute improvements of the best 
results (ML10 + MBE10+ I-smoothing) compared to the results 
of ML20. It is clear that the MBE training is particularly 
effective in correcting boundary errors in the proximity of 
manually labeled positions. In [3], Brugnara et al. presented an 
excellent HMM-based phonetic segmentation system, which 
achieved an accuracy of 88.7% (under MSM configuration) 
within a tolerance of 20ms. In our experiments, the baseline 
HMM-based system yields an accuracy of 88.97% (ML20), 
while the MBE training improves the accuracy to 92.11%.  
4. Conclusions and future work 
We have explored the use of the minimum boundary error 
(MBE) criterion in the discriminative training of acoustic 
models for automatic phonetic segmentation. The underlying 
characteristics of MBE training have been investigated, and its 
superiority over conventional ML training has been verified by 
experiments. Naturally, the more accurate phonetic 
segmentation obtained by the MBE-trained models is very 
useful for subsequent manual verification or further boundary 
refinement using other techniques. The MBE training method 
is not difficult to implement, in particular some discriminative 
training tools, such as MPE, have been included in HTK.  
Table 1: The percentage of phone boundaries correctly placed 
within different tolerances with respect to their associated 
manually labeled phone boundaries. 
 
%Correct marks (error < tolerance)
 
Mean 
Boundary 
Distance  <5ms <10ms <15ms <20ms
ML10 9.83 ms 46.69 71.10 83.14 88.94
ML20 9.78 ms 46.95 71.23 83.11 88.97
ML10 + MBE10 7.83 ms 58.35 79.73 88.14 92.09
ML10 + MBE10  
+ I-smoothing 7.82 ms 58.48 79.75 88.16 92.11
absolute 
improvement 1.96 ms 11.53 8.52  5.05 3.14 
In addition to applying the MBE criterion to the training 
of acoustic models, we have applied it to discriminative 
feature training. The preliminary experiment results indicate 
that feature-based MBE training is more effective than model-
based MBE training. The segmentation accuracy could be 
improved by integrating the feature-based and model-based 
MBE training procedures. Moreover, a new decoding 
algorithm based on the minimum boundary error criterion is 
also under development. It is hoped that phone boundaries can 
be located more accurately by running a second pass search 
using the minimum boundary error criterion on the lattice 
generated by a first pass conventional search. In our current 
implementation, the phone boundary error function, defined in 
Eq.(5), is calculated in the time frame unit for efficiency. 
However, more accurate segmentation may be achieved by 
calculating boundary errors in actual time sample marks. 
5. Acknowledgements 
This work was funded by the National Science Council of the 
Republic of China under Grant: NSC94-2213-E-001-021. 
6. References 
[1] F. Malfrere and T. Dutiot, “High-quality speech synthesis 
for phonetic speech segmentation,” in Proc. Eurospeech’97, 
pp.2631-2634. 
[2] J. van Santen and R. Sproat, “High accuracy automatic 
segmentation,” in Proc. Eurospeech’99, pp.2809-2812. 
[3] F. Brugnara, D. Falavigna, and M. Omologo, “Automatic 
segmentation and labeling of speech based on Hidden 
Markov Models,” Speech Communication, Vol. 12, Issue. 4, 
pp.357-370, 1993. 
[4] D. Torre Toledano, M. A. Rodriguez Crespo, and J. G. 
Escalada Sardina, “Try to mimic human segmentation of 
speech using HMM and fuzzy logic post-correction rules,” 
in Proc. Third ESCA/COCOSDA International Workshop 
on Speech Synthesis, 1998, pp.1263-1266. 
[5] R. Schwartz and Y.-L. Chow, “The N-best algorithms: an 
efficient and exact procedure for finding the N most likely 
sentence hypotheses,” in Proc. ICASSP’90. 
[6] S. Ortmanns, H. Ney, and X. Aubert, “A word graph 
algorithm for large vocabulary continuous speech 
recognition,” Computer Speech and Language, Vol. 11, 
pp.43-72, 1997. 
[7] P. Gopalakrishnan, D. Kanevsky, A. Nádas, and D. 
Nahamoo, “An inequality for rational functions with 
applications to some statistical estimation problems,” IEEE 
Trans. Information Theory, Vol. 37, pp.107-113, 1991. 
[8] D. Povey, Discriminative Training for Large Vocabulary 
Speech Recognition. Ph.D. Dissertation, Peterhouse, 
University of Cambridge, July 2004. 
[9] D. Povey and P. C. Woodland, “Minimum phone error and 
I-smoothing for improved discriminative training,” in Proc. 
ICASSP’02. 
[10] L. Lamel, R. Kasel, and S. Seneff, “Speech database 
development: design and analysis of the acoustic-phonetic 
corpus,” in Proc. DARPA Speech Recognition Workshop, 
1986, pp.100-109. 
 
initial phonetic segmentation for subsequent manual segmentation and verification, 
e.g., dynamic time warping (DTW) [1], methods that utilize specific features and 
algorithms [2], HMM-based Viterbi forced alignment [3], and two-stage approaches 
[4]. 
The most popular method of automatic phonetic segmentation is to adapt an 
HMM-based phonetic recognizer to align a phonetic transcription with a speech 
utterance. Empirically, phone boundaries obtained in this way should contain few 
serious errors, since HMMs generally capture the acoustic properties of phones; 
however, small errors are inevitable because HMMs are not sensitive enough to detect 
changes between adjacent phones [4]. To improve the discriminability of HMMs for 
automatic phonetic segmentation, we proposed using a discriminative criterion, called 
the minimum boundary error (MBE), for model training in our previous work [5]. In 
this paper, the MBE criterion is extended to the segmentation stage, i.e., we propose 
an MBE forced alignment to replace the conventional maximum likelihood (ML) 
forced alignment. The superiority of the MBE framework over the conventional ML 
framework for automatic phonetic segmentation is verified by experiments conducted 
on the TIMIT acoustic-phonetic continuous speech corpus. 
The remainder of this paper is organized as follows. Section 2 reviews the 
methodology of the MBE discriminative training approach. In Section 3, we present 
the proposed MBE segmentation approach and discuss its relation to the minimum 
Bayes risk (MBR) criterion. The experiment results are detailed in Section 4. Finally, 
in Section 5, we present our conclusions and suggest some future research directions. 
2   Minimum Boundary Error Training 
Let { }ROO ,..,1=O  be a set of training observation sequences. The objective function 
for MBE training can then be defined as:  
∑ ∑=
= ∈
R
r
S
r
c
r
i
rr
iMBE rri SSEROSPF 1
),()|(Φ , (1) 
where  is a set of possible phonetic alignments for the training observation 
utterance ;  is one of the hypothesized alignments in ;  is the 
posterior probability of alignment , given the training observation sequence ; 
and  denotes the “boundary error” of  compared with the manually 
labeled phonetic alignment . For each training observation sequence ,  
gives the weighted average boundary error of all hypothesized alignments. For 
simplicity, we assume the prior probability of alignment  is uniformly distributed, 
and the likelihood  of alignment  is governed by the acoustic model 
parameter set . Therefore, Eq.(1) can be rewritten as: 
rΦ
rO riS
rΦ )| rri OSP(
r
iS
rO
),( rc
r
i SSER
r
iS
r
cS rO MBEF
r
iS
)|( ri
r SOp riS
Λ
time, respectively. Since  contains a large number of hypothesized phonetic 
alignments, it is impractical to sum the boundary errors directly without first pruning 
some of the alignments. For efficiency, it is suggested that a reduced hypothesis space, 
such as an N-best list [6] or a lattice (or graph) [7], should be used. However, an N-
best list often contains too much redundant information, e.g., two hypothesized 
alignments can be very similar. In contrast, as illustrated in Fig. 1, a phonetic lattice is 
more effective because it only stores alternative phone arcs on different segments of 
time marks and can easily generate a large number of distinct hypothesized phone 
alignments. Although it cannot be guaranteed that the phonetic alignments generated 
from a phonetic lattice will have higher probabilities than those not presented, we 
believe that the approximation will not affect the segmentation performance 
significantly. In this paper, we let  denote the set of possible phonetic 
alignments in the lattice for the training observation utterance . 
rΦ
r
LatΦ
rO
2.1   Objective Function Optimization and Update Formulae  
Eq.(3) is a complex problem to solve, because there is no closed-form solution. In this 
paper, we adopt the Expectation Maximization (EM) algorithm to solve it. Since the 
EM algorithm maximizes the objective function, we reverse the sign of the objective 
function defined in Eq. (3) and re-formulate the optimization problem as, 
∑∑ ∑ (
(−=Λ
= ∈ ∈ Λ
Λ
Λ
R
r
r
c
r
iS
S
r
k
r
r
i
r
SSER
SOp
SOp
rr
i
rr
k
1
* ),(
)|
)|maxarg Φ
Φ
α
α
. (6) 
However, the EM algorithm can not be applied directly, because the objective 
function comprises rational functions [8]. The extended EM algorithm, which utilizes 
a weak-sense auxiliary function [9] and has been applied in the minimum phone error 
(MPE) discriminative training approach [10] for ASR, can be adapted to solve Eq.(6). 
The re-estimation formulae for the mean vector mμ  and the diagonal covariance 
matrix  of a given Gaussian mixture m thus derived can be expressed, 
respectively, as: 
mΣ
m
MBE
m
mm
MBE
m
m D
DO
+
+= γ
μθμ )( , (7) 
and 
[ ] T
mm
m
MBE
m
T
mmmm
MBE
m
m D
DO μμγ
μμθ −+
+Σ+=Σ )(
2
. (8) 
In Eqs. (7) and (8),  is a per-mixture level control constant that ensures all the 
variance updates are positive; 
mD
mμ  and mΣ  are the current mean vector and 
∑
∑=
∈ Λ
∈ Λ
r
k
r
i
S kr
S iirr
avg SOp
SERSOp
Lat
Lat
Φ
Φ
α
α
η
)|(
)()|( , (14) 
and 
∑
∑=
∈∈ Λ
∈∈ Λ
k
r
k
i
r
i
SqS kr
SqS iirr
q SOp
SERSOp
,
,
)|(
)()|(
Lat
Lat
Φ
Φ
α
α
η , (15) 
respectively, where Λ  is the current set of parameters. The above three quantities 
can be calculated efficiently by applying dynamic programming to the lattice. 
2.2   I-smoothing Update  
To improve the generality of MBE training, the I-smoothing technique [10] is 
employed to provide better parameter estimates. This technique can be regarded as 
interpolating the MBE and ML auxiliary functions according to the amount of data 
available for each Gaussian mixture. The updates for the mean vector mμ  and the 
diagonal covariance matrix  thus become: mΣ
mm
MBE
m
ML
mML
m
m
mm
MBE
m
m D
ODO
τγ
θγ
τμθ
μ ++
++
=
)()(
, (16) 
and 
[ ]
T
mm
mm
MBE
m
ML
mML
m
mT
mmmm
MBE
m
m
D
OD
μμτγ
θγ
τμμθ
−++
++Σ+
=Σ
)( 2
, (17) 
respectively, where mτ  is also a per-mixture level control constant; and , 
, and  are computed by 
ML
mγ
)(OMLmθ )( 2OMLmθ
)(
11
t
MLr
m
T
t
R
r
ML
m
r γγ ∑∑=
==
, (18) 
)()()(
11
totO r
MLr
m
T
t
R
r
ML
m
r γθ ∑∑=
==
, (19) 
and 
)|(maxarg
))|(1(minarg
)|(minarg
)|(),(minarg
,
*
SOP
OSP
OSP
OSPSSLS
S
S
j
SSSS
jj
SS
jj
j
=
−=
∑=
∑=
≠∈
∈
Φ
Φ
. (24) 
It is clear from Eq. (23) that the zero-one loss function assigns no loss when , 
but assigns a uniform loss of one to the alignments 
jSS =
jSS ≠  no matter how different 
they are from . Thus, such a loss function causes all incorrectly hypothesized 
alignments to be regarded as having the same segmentation risk, which is obviously 
inconsistent with our preference for alignments with fewer errors in an automatic 
segmentation task.  
jS
In our approach, the loss function is replaced by the boundary error function, 
defined in Eq.(4), to match the goal of minimizing the boundary error. Consequently, 
the MBR forced alignment approach becomes the MBE forced alignment approach, 
defined as: 
)|(),(minarg
)|(),(minarg
1
*
OSPqqer
OSPSSERS
j
j
nn
N
nSS
jj
SS
j
j
∑∑=
∑=
=∈
∈
Φ
Φ
, (25) 
where N is the number of phones in utterance O ; and  and  are the n-th 
phone in the alignments  and , respectively.  
nq
j
nq
S jS
To simplify the implementation, we restrict the hypothesized space  to , 
the set of alignments constructed from the phone lattice shown in Fig. 1, which can be 
generated by a conventional beam search. Accordingly, Eq. (25) can be re-formulated 
as: 
Φ LatΦ
),()|(minarg
),()|(minarg
1
1
*
j
nnj
S
N
nS
j
nnj
N
nSS
qqerOSP
qqerOSPS
j
j
∑∑=
∑∑=
∈=
=∈
Lat
Lat
Φ
Φ
. (26) 
Let the cut  be the set of phone arcs of the n-th phone in the utterance. For 
example, in Fig. 1, there are four phone arcs for the second phone, “w”, in  and 
six phone arcs for the third phone, “eh”, in . From the figure, it is obvious that 
nC
2C
3C
6.50
7.50
8.50
9.50
10.50
11.50
12.50
13.50
0
(ML)
1 2 3 4 5 6 7 8 9 10 iter
Frame Err
(%)
TrainSet TestSet TrainSet cri
 
Fig. 2. The phonetic segmentation results (FER) for the models trained according to 
ML and MBE criteria, respectively. 
4.2   Experiment Results 
The acoustic models were first trained on the training utterances according to human-
labeled phonetic transcriptions and boundaries by the Baum-Welch algorithm using 
the ML criterion. Then, the MBE discriminative training approach was applied to 
further manipulate the models. The scaling factor α  in Eq.(2) was empirically set to 
0.1 and the I-smoothing control constant mτ  in Eqs.(16) and (17) was set to 20 for 
all mixtures. The results are shown in Fig. 2. In the figure, the line with triangles 
indicates the expected FER (frame error rate) calculated at each iteration of the 
training process. Clearly, the descending trend satisfies the training criterion. The line 
with diamonds and the line with rectangles represent the FER results of the training 
(inside test) and test sets, respectively. We observe that the ML-trained acoustic 
models (at the 0th iteration) yield an FER of 10.31% and 11.77% for the training set 
and test set respectively. In contrast, after 10 iterations, the MBE-trained acoustic 
models yield an FER of 6.88% and 9.25%, respectively. The MBE discriminative 
training approach achieves a relative FER reduction of 33.27% on the training set and 
21.41% on the test set. The results clearly demonstrate that the MBE discriminative 
training approach performs very well and can enhance the performance of the 
acoustic models initially trained by the ML criterion. 
Table 1 shows the percentage of phone boundaries correctly placed within different 
tolerances with respect to their associated manually-labeled phone boundaries. The 
experiment was conducted on the test set. From rows 2 and 3 of Table 1, we observe 
that the MBE-trained models significantly outperform the ML-trained models. Clearly, 
the MBE training is particularly effective in correcting boundary errors in the 
proximity of manually labeled positions. Comparing the results in rows 2 and 4, we 
also observe that MBE segmentation outperforms ML segmentation, though the 
MBE criterion in duration model training, but there was no significant improvement 
found in our preliminary work. However, the issue warrants further study. On the 
other hand, well-labeled phonetic training corpora are very scarce. Therefore, the 
unsupervised MBE training approach is also under investigated. Moreover, in our 
current implementation, the phone boundary error function, defined in Eq.(5), is 
calculated in the time frame unit for efficiency. However, more accurate segmentation 
may be achieved by calculating boundary errors in actual time sample marks. In 
addition, we are applying the MBE training and segmentation framework to facilitate 
the phonetic labeling of a subset of speech utterances in MATBN (Mandarin across 
Taiwan − Broadcast News) database [12]. 
Acknowledgment. This work was funded by the National Science Council, Taiwan, 
under Grant: NSC94-2213-E-001-021. 
References 
1. Malfrere, F., Dutiot, T.: High-quality speech synthesis for phonetic speech segmentation. 
Proc. Fifth Eurospeech (1997) 2631-2634 
2. van Santen, J., Sproat, R.: High accuracy automatic segmentation. Proc. Sixth Eurospeech 
(1999) 2809-2812 
3. Brugnara, F., Falavigna, D., Omologo, M.: Automatic segmentation and labeling of speech 
based on Hidden Markov Models. Speech Communication, Vol. 12, Issue. 4 (1993) 357-
370 
4. Torre Toledano, D., Rodriguez Crespo, M. A., Escalada Sardina, J. G.: Try to mimic human 
segmentation of speech using HMM and fuzzy logic post-correction rules. Proc. Third 
ESCA/COCOSDA International Workshop on Speech Synthesis (1998) 1263-1266 
5. Kuo, J.-W., Wang, H.-M.: Minimum Boundary Error Training for Automatic Phonetic 
Segmentation. Proc. Interspeech – ICSLP (2006) 
6. Schwartz, R., Chow, Y.-L.: The N-best algorithms: an efficient and exact procedure for 
finding the N most likely sentence hypotheses. Proc. ICASSP, Vol. 1(1990) 81-84 
7. Ortmanns, S., Ney, H., Aubert, X.: A word graph algorithm for large vocabulary continuous 
speech recognition. Computer Speech and Language, Vol. 11 (1997) 43-72 
8. Gopalakrishnan, P., Kanevsky, D., Nádas, A., Nahamoo, D.: An inequality for rational 
functions with applications to some statistical estimation problems. IEEE Trans. 
Information Theory, Vol. 37 (1991) 107-113 
9. Povey, D.: Discriminative Training for Large Vocabulary Speech Recognition. Ph.D. 
Dissertation, Peterhouse, University of Cambridge, July 2004 
10. Povey, D., Woodland, P. C.: Minimum phone error and I-smoothing for improved 
discriminative training. Proc. ICASSP, Vol. 1 (2002) 105-108 
11. Lamel, L., Kasel, R., Seneff, S.: Speech database development: design and analysis of the 
acoustic-phonetic corpus. Proc. DARPA Speech Recognition Workshop (1986) 100-109 
12. Wang, H.-M., Chen, B., Kuo, J.-W., Cheng, S.-S.: MATBN: A Mandarin Chinese 
Broadcast News Corpus. International Journal of Computational Linguistics & Chinese 
Language Processing, Vol. 10, No. 2, June (2005) 219-236 
 
class membership of each data point. The main goal of train-
ing is to find a classifier that can correctly predict the class
label of an unseen data point. This can be achieved by con-
structing a nonlinear separating surface which is implicitly
defined by a kernel function. In conventional SVM [4], the
nonlinear kernel matrix K(A,A′) ∈ Rm×m (where m is the
size of the training data set) on large data sets will lead to
some computational difficulties [5]. The RSVM [5], which
uses a very small random subset of size m¯ of the original
m data points, where m¯ << m, can avoid these difficulties.
We denote this random subset by A¯, which is used to gen-
erate a much smaller rectangular matrix K(A, A¯′) ∈ Rm×m¯
and to replace the huge and fully dense square kernel matrix
K(A,A′) used in conventional SVM to cut the problem size,
computational time and memory usage as well as to simplify
the characterization of nonlinear separating surface. We now
briefly describe the reduced support vector machine formu-
lation, which is derived from the generalized support vector
machine (GSVM) [6] and smooth support vector machine [7].
The RSVM solves the following unconstrained minimization
problem for an arbitrary rectangular kernelK(A, A¯′):
min
(u¯,γ)∈Rm¯+1
ν
2
‖p(e−D(K(A, A¯′)D¯u¯− eγ), α)‖22
+
1
2
(u¯′u¯+ γ2), (1)
where the function p(x, α) is a very accurate smooth approx-
imation to (x)+ [7], which is applied to each component of
the vector e −D(K(A, A¯′)D¯u¯ − eγ) and is defined compo-
nentwise by
p(x, α) = x+
1
α
log(1 + e−αx), α > 0. (2)
The function p(x, α) converges to (x)+ as α goes to infin-
ity. The positive tuning parameter ν here controls the trade-
off between the classification error and the suppression of
(u¯, γ).The diagonal matrix D¯ ∈ Rm¯×m¯ with ones or minus
ones along its diagonal to specify the membership of each
point in the reduced set. A solution of this minimization pro-
gram for u¯ and γ leads to the nonlinear separating surface
K(x′, A¯′)D¯u¯ = γ. (3)
Problem (1) retains the strong convexity and differentia-
bility properties in the Rm¯+1 space of (u¯, γ) for any arbi-
trary rectangular kernel. Hence we can apply the Newton-
Armijo Algorithm [7] directly to solve (1) and the existence
and uniqueness of the optimal solution of the minimization
problem (1) are also guaranteed. In a nutshell, the RSVM can
be split into two parts. First, it selects a small random subset{
K(·, A¯′1),K(·, A¯′2), · · · ,K(·, A¯′m¯)
}
from the full-data ba-
sis set. The full-data set is inefficient with possibly heavy
overlaps in function representation, but the conventional SVM
has been using it. Secondly, the RSVM determines the best
coefficients of the selected kernel functions by solving the un-
constrained minimization problem (1) using the entire data set
so that the surface will fit the whole data well.
3. PHONETIC BOUNDARY REFINEMENT USING
SVM
The proposed SVM-based phonetic boundary refinement pro-
ceeds as follows. For each initial boundary detected by the
HMM-based segmentation, several hypothesized boundaries
around it are identified first; then each of which is exam-
ined by a phone-transition-dependent SVM classifier; and fi-
nally the most likely boundary is selected to replace the ini-
tial boundary. The SVM classifiers for detecting boundaries
of various phone transitions are trained in advance based on
multiple discriminative features in addition to MFCCs.
3.1. Useful features
In the HMM-based segmentation, each frame of the speech
data is represented by a 39 dimensional MFCC-based feature
vectors comprised of 12 MFCCs and log energy, plus their
delta and delta-delta coefficients. In the refinement stage,
each frame is represented by a 45 dimensional feature vec-
tor consisting of the above 39 MFCC-based coefficients, plus
zero crossing rate, bisector frequency [8], burst degree [8],
spectral entropy, general weighted entropy [9], and subband
energy.
For each hypothesized boundary, the feature vectors of
the left and right frames next to it, together with the sym-
metrical Kullback-Leibler distance (SKLD) and the spectral
feature transition rate (SFTR) between the two feature vec-
tors, are concatenated to form a 92 dimensional augmented
vector. The augmented vectors are used as features to cluster
the phone transitions and as the input vectors to SVM.
3.2. Phone transition clustering
Ideally, we can train a SVM classifier for each kind of phone
transition. However, this is generally not feasible because the
training data is always limited and some specific phone tran-
sitions might have the sparse data problem. In practical im-
plementations, we need to partition the phone transitions into
clusters according to their acoustic characteristics, such that
the training data can be shared and the phone transitions with
little training data can be covered by the SVM classifiers of
categories to which they belong.
The partition can be determined based on either prior knowl-
edge [10] or statistical learning [1]. In this paper, we use a
data-driven clustering approach as follows:
1. For each specific phone transition case, we gather all
augmented vectors associated with the human-labelled
phone boundaries, and compute the mean vector.
Table 1. The percentage of phone boundaries correctly placed within different tolerances with respect to their associated
human-labelled phone boundaries.
Mean Boundary Accuracy%
Methods Distance (ms) < 5ms < 10ms < 20ms < 30ms < 40ms
HMMML 9.73 46.85 71.53 89.17 94.62 97.16
HMMML+MBE 7.79 58.73 80.15 92.09 95.93 97.89
HMMML-SVM 7.82 58.18 81.19 92.47 96.05 97.78
HMMML+MBE-SVM 7.73 58.25 81.23 92.46 96.08 97.95
4.2. Experiment results
Table 1 shows the percentage of phone boundaries correctly
placed within different tolerances with respect to their as-
sociated human-labeled phone boundaries. The second row
represents the results of the ML-trained HMM forced align-
ment, and the third row comes from the MBE-trained HMM
forced alignment. The fourth row is the performance of the
SVM-based refinement based on the initial boundaries given
by the ML-trained HMM forced alignment, and the fifth row
is the performance of the SVM-based refinement based on
the initial boundaries given by the MBE-trained HMM forced
alignment. We observe that the proposed HMMML-SVM ap-
proach performs as well as the discriminative HMMMBE-
based segmentation. However, the SVM-based refinement
can only slightly improve the segmentation accuracy given
the initial boundaries provided by the HMMMBE-based seg-
mentation. It seems that the refinement system doesn’t benefit
much from a more accurate initial alignment. The best accu-
racies achieved are 81.23% within a tolerance of 10 ms and
92.47% within a tolerance of 20 ms. The mean boundary dis-
tance is 7.73 ms.
5. CONCLUSIONS
SVM has been successfully applied in many applications, but
it is less widely applied in speech processing research. In
this paper, we have presented a SVM-based boundary refine-
ment approach to improve the HMM-based forced alignment
for automatic phonetic segmentation. The preliminary exper-
iment results on the TIMIT corpus show that the proposed
HMM-SVM approach performs as well as the improved HMM-
based segmentation, which used a minimum boundary error
(MBE) criterion for discriminative HMM training. Although
the current SVM-based refinement system seems not able to
benefit from a more accurate initial alignment given by the
HMMMBE forced alignment in our experiments, a more ac-
curate segmentation is expectable if a more comprehensive
investigation into the acoustic features and the characteristics
of various phone transitions can be carried out to improve the
phone-transition-dependent SVM classifiers.
6. ACKNOWLEDGMENTS
This work was supported in part by the National Science Coun-
cil, Taiwan, under Grant: NSC95-2221-E-001-035.
7. REFERENCES
[1] K. S. Lee, “MLP-based phone boundary refining for a
tts database,” IEEE Trans. on Speech and Audio Pro-
cessing, vol. 14, pp. 981–989, 2006.
[2] J. Keshet, S. Shalev-Shwartz, Y. Singer, and D. Chazan,
“Phoneme alignment based on discriminative learning,”
in Proc. Interspeech, 2005.
[3] J.-W. Kuo and H.-M. Wang, “Minimum boundary error
training for automatic phonetic segmentation,” in Proc.
Interspeech, 2006.
[4] V. N. Vapnik, The Nature of Statistical Learning Theory,
Springer, New York, 1995.
[5] Y.-J. Lee and O. L. Mangasarian, “RSVM: Reduced
support vector machines,” in Proc. SDM, 2001.
[6] O. L. Mangasarian, “Generalized support vector ma-
chines,” in Advances in Large Margin Classifiers, 2000.
[7] Y.-J. Lee and O. L. Mangasarian, “SSVM: A smooth
support vector machine,” Computational Optimization
and Applications, vol. 20, pp. 5–22, 2001.
[8] C.-Y. Lin, J.-S. Roger Jang, and K.-T. Chen, “Automatic
segmentation and labeling for mandarin chinese speech
corpora for concatenation-based TTS,” Computational
Linguistics and Chinese Language Processing, vol. 10,
no. 2, pp. 145–166, 2005.
[9] J.-L. Shen, J.-W. Hung, and L.-S. Lee, “Robust entropy-
based endpoint detection for speech recognition in noisy
environments,” in Proc. ICSLP, 1998.
[10] E.-Y. Park, S.-H Kim, and J.-H Chung, “Automatic
speech synthesis unit generation with MLP based post-
processor against auto-segmented phoneme errors,” in
Proc. IJCNN, 1999.
Figure 1: An illustration of the phoneme lattice for the speech
utterance “Where were they?”
segmentation task. The latter can be considered as an action,
αS(O), taken to identify a certain alignment, S, from all the
phoneme alignments of a given utterance O. Let the func-
tion L(S, Sc) be the loss incurred when the action αS(O) is
taken, given that the true alignment is Sc. During the classi-
fication stage, we do not know the true alignment in advance,
i.e., any arbitrary alignment Sj could be true. The MBR clas-
sifier is designed to select the action whose conditional risk,
R(αS |O) = ∑Sj∈Φ L(S, Sj)P (Sj |O), is minimal, i.e., the
best alignment based on the MBR criterion is found by:
S∗ = argmin
S
∑
Sj∈Φ
L(S, Sj)P (Sj |O). (3)
By replacing the loss function in Eq. (3) with the boundary
error function defined in Sec 2.1, the best alignment based on
the MBE criterion is found by:
S∗ = argmin
S
∑
Sj∈Φ
ER(S, Sj)P (Sj |O)
= argmin
S
∑
Sj∈Φ
N∑
n=1
er(qn, q
j
n)P (Sj |O), (4)
where N is the number of phonemes in utterance O; and qn
and qjn are the n-th phonemes in the alignments S and Sj , re-
spectively. To simplify the implementation, we restrict the hy-
pothesized space Φ to the set of alignments constructed from
the phoneme lattice, which can be generated by a conventional
beam search.
Let the cut Cn be the set of phoneme arcs of the n-th
phoneme in the utterance. For example, in Fig. 1, there are
four phoneme arcs for the second phoneme “w” in C2 and six
phoneme arcs for the third phoneme “eh” in C3. From the fig-
ure, it is obvious that each hypothesized alignment will pass a
single phoneme arc in each cut Cn, n = 1, 2, . . . , N . Based
on this observation, Eq. (4) can be rewritten as:
S∗ = argmin
S
N∑
n=1
∑
qn,m∈Cn
ρqn,mer(qn, qn,m), (5)
where qn,m is the m-th phoneme arc in Cn, and ρqn,m =∑
{Sj∈Φ|qn,m∈Sj} P (Sj |O) is equivalent to the probability of
qn,m given the utterance O, which can be calculated easily by
applying a forward-backward algorithm to the phoneme lattice.
In this way, MBE forced alignment can be performed efficiently
on the phoneme lattice via a Viterbi search.
2.3. Applying the phoneme duration model to segmentation
Duration information plays an important role in discriminat-
ing between certain words in various languages. For exam-
ple, in English, it is not easy to distinguish between “ship” and
“sheep” without using duration information. However, HMM-
based systems, in which the probability of the duration of a state
decreases exponentially over time, are known to be deficient in
modeling the duration of phonemes. Many duration modeling
techniques, such as the hidden semi-Markov model (HSMM)
[8], the expanded state HMM (ESHMM) [9], and the post-
processor duration model [10], have been proposed to model the
duration more accurately. Compared to HSMM and ESHMM,
the post-processor duration model used in re-scoring a list of
likely hypotheses is more suitable for integration into the MBE
segmentation process.
In the implementation, the phoneme duration probability is
integrated into the output likelihood of a specific phoneme arc
q on the lattice as follows:
pˆ(q) = p(q) · d(τq)β , (6)
where d(τq) is the duration probability of q, and β is a scal-
ing factor that controls the duration model’s impact. We use
a nonparametric probability mass function for duration mod-
eling, which makes no prior assumption about the parametric
form of the distribution, and is more computationally efficient
than parametric approaches. Phoneme durations from the train-
ing data are used to compute the histograms with a bin width of
5 ms.
3. Boundary refinement using SVM
As noted in [6], SVM is useful for refining the initial phoneme
boundaries detected by HMM-based segmentation. For each
initial boundary, several hypothesized boundaries around it are
identified, and each one is examined by a phoneme-transition-
dependent SVM classifier; then, the initial boundary is replaced
by the most likely boundary.
3.1. Phoneme transition clustering
Ideally, we should be able to train an SVM classifier for each
type of phoneme transition. However, this is not feasible be-
cause the training data is always limited. Maintaining a balance
between the available training data and the model’s complex-
ity is critical to the training process. Furthermore, since many
phoneme transitions have similar acoustic characteristics, we
can partition them into clusters so that the training data can be
shared and the phoneme transitions with little training data can
be covered by the SVM classifiers of the categories they belong
to. We implement phoneme transition clustering in two ways:
by K-means clustering and by decision-tree-based clustering.
K-means-based phoneme transition clustering is performed
as follows. For each type of phoneme transition, we gather all
the feature vectors associated with the human-labeled phoneme
boundaries and compute the mean vector. For each one of
the four phoneme transition classes, namely sonorant to non-
sonorant, sonorant to sonorant, non-sonorant to non-sonorant,
and non-sonorant to sonorant, we apply the K-means algorithm
to cluster the phoneme transitions according to their mean vec-
tors. Note that only phoneme transitions with enough instances
are considered in this step. Finally, we assign the phoneme tran-
sitions ignored during clustering (due to sparse instances) to the
nearest clusters according to the Euclidean distances between
their mean vectors and the cluster centers.
The drawback of K-means clustering is that it can not
cover phoneme transitions that do not occur in the training
data. In contrast, decision-tree-based clustering can generalize
to unseen phoneme transitions and take advantage of linguistic
Table 2: Results of HMM/SVM-based automatic phoneme seg-
mentation evaluated on the TIMIT database.
Mean %Correct marks
Methods boundary (error < tolerance)
distance <5ms <10ms <20ms
HMM* 7.14ms 59.58 81.57 93.74
HMM*+SVMKM 6.75ms 62.47 84.00 94.33
HMM*+SVMDT 6.83ms 62.07 83.70 94.12
Table 3: Results of automatic phoneme segmentation evaluated
on the MATBN database.
Mean %Correct marks
Training / Segmentation boundary (error < tolerance)
distance <5ms <10ms <20ms
Unsup.ML/ML 20.29 ms 16.80 29.68 58.65
Unsup.ML/MBE 18.62 ms 18.21 34.10 62.88
ML/ML 13.06 ms 27.67 50.50 83.70
ML/MBE 11.73 ms 30.28 58.25 87.22
ML+MBE/ML 11.99 ms 35.11 59.56 85.01
ML+MBE/MBE 10.91 ms 37.83 63.78 87.53
ML+MBE/MBEpdm 10.29 ms 40.24 66.30 88.43
HMM*+SVMKM 9.29 ms 49.02 71.36 89.11
4.2. Evaluation on the MATBN database
The MATBN Mandarin Chinese corpus contains 198 hours
of broadcast news from the Public Television Service Foun-
dation (Taiwan). The data includes orthographic transcripts
and SGML tagging for annotating acoustic conditions, back-
ground conditions, story boundaries, speaker turn boundaries,
and acoustic events, such as hesitations and repetitions. We se-
lect approximately five hours of speech data from the corpus
for further phoneme annotation. To reduce costs, we employ
HMM-based segmentation and SVM-based refinement to ob-
tain the initial phoneme segmentation for subsequent manual
segmentation and verification. To do this, we divide the speech
data into subsets, each containing five minutes of speech. First,
we perform unsupervised ML training and forced alignment on
the complete set to generate the initial segmentation. When the
first subset has been manually verified, it is used for supervised
training of the HMMs and SVMs. To prevent over-fitting in
HMM training, the remaining unverified data is re-segmented
and used to smooth the HMM parameters. Then, based on the
new HMMs, we apply forced alignment to the remaining sub-
sets to generate more accurate phoneme boundaries. The above
training and segmentation process is repeated until all the sub-
sets have been manually verified. In this way, the accuracy of
automatic segmentation can be improved stage by stage, and the
overall cost of manual segmentation can be reduced.
Now that the first subset has been processed completely,
we evaluate the efficacy of the proposed semi-automatic
phoneme segmentation process by applying four minutes of
the human-verified speech for supervised training and the
remaining one minute for testing. In total, 34 context-
independent phoneme HMMs and 14 SVMKM classifiers are
used. From Table 3, we observe that the proposed HMM-
based segmentation (ML+MBE/MBEpdm, HMM*) signifi-
cantly outperforms the conventional HMM-based segmentation
(Unsup.ML/ML). The mean boundary distance achieved is
10.29 ms. By using SVMKM for boundary refinement, the
mean boundary distance can be further reduced from 10.29 ms
to 9.29 ms. We believe that the segmentation accuracy could
be improved even further if more subsets are manually verified,
i.e., the cost of labeling one subset could be progressively re-
duced.
5. Conclusions
We have presented several improved HMM/SVM methods for
a two-stage phoneme segmentation framework that imitates
the human phoneme segmentation process. In the first stage,
HMM-based forced alignment is performed according to the
minimum boundary error (MBE) criterion, based on MBE-
trained HMMs and explicit phoneme duration models. In the
second stage, phoneme-transition-dependent SVM classifiers
are used to refine the phoneme segmentation derived by the
HMM-based forced alignment step. The efficacy of the pro-
posed framework has been validated on the TIMIT database.
We have also applied the framework in a semi-automatic pro-
cess to facilitate manual labeling of the MATBN Mandarin Chi-
nese database. The preliminary evaluation results are rather
promising. The annotation work is ongoing and the results will
be made available at a future time.
6. Acknowledgements
This work was funded by the National Science Council, Taiwan,
under Grant: NSC95-2221-E-001-035.
7. References
[1] F. Malfrere and T. Dutiot, “High-quality speech synthesis
for phonetic speech segmentation,” in Proc. Eurospeech
1997, pp. 2631–2634.
[2] J. van Santen and R. Sproat, “High accuracy automatic
segmentation,” in Proc. Eurospeech 1999, pp. 2809–2812.
[3] F. Brugnara, D. Falavigna, and M. Omologo, “Auto-
matic segmentation and labeling of speech based on hid-
den markov models,” Speech Communication, vol. 12, no.
4, pp. 357–370, 1993.
[4] J.-W. Kuo and H.-M. Wang, “A minimum boundary error
framework for automatic phonetic segmentation,” in Proc.
ISCSLP 2006, LNAI 4274 Springer, pp. 399–409.
[5] D. Torre Toledano, M. A. Rodriguez Crespo, and J. G. Es-
calada Sardina, “Try to mimic human segmentation of
speech using HMM and fuzzy logic post-correction rules,”
in Proc. the 3th ESCA/COCOSDA International Workshop
on Speech Synthesis, pp. 1263–1266.
[6] H.-Y. Lo and H.-M. Wang, “Phonetic boundary refine-
ment using support vector machine,” in Proc. ICASSP,
2007.
[7] H.-M. Wang, B. Chen, J.-W. Kuo, and S.-S. Cheng,
“MATBN: A Mandarin Chinese broadcast news corpus,”
International Journal of Computational Linguistics and
Chinese Language Processing, vol. 10, no. 2, pp. 219–
236, 2005.
[8] M. J. Russell and R. K. Moore, “Explicit modelling of
state occupancy in hidden markov models for automatic
speech recognition,” in Proc. ICASSP 1988, pp. 5–8.
[9] M. J. Russell and A. E. Cook, “Experimental evaluation
of duration modelling techniques for automatic speech
recognition,” in Proc. ICASSP 1987, pp. 2376–2379.
[10] J. Pylkko¨nen and M. Kurimo, “Duration modeling tech-
niques for continuous speech recognition,” in Proc. Inter-
speech 2004 - ICSLP.
[11] Y.-J. Lee and O. L. Mangasarian, “SSVM: A smooth sup-
port vector machine,” Computational Optimization and
Applications, vol. 20, pp. 5–22, 2001.
