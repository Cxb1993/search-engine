1 
行政院國家科學委員會專題研究計畫成果報告 
本體論為基之研發專利文件自動摘要方法論研究 
1. A Ontology Based Approach for Automatic Patent Document Summarization 
計 畫 編 號：NSC 95－2221－E －007－233－MY2 
執 行 期 限：95 年 8 月 1 日至 97 年 7 月 31 日 
主 持 人：張瑞芬   清華大學 工業工程與工程管理學系 
共同主持人： 
計畫參與人員：林彥伯、吳俊逸、王裕濱、黃雅莉、吳昌儒、倪煒均、蕭維承、 陳孟鈺 
執行機構及單位名稱 
一、中文摘要 
近年來專利文件的日益劇增，導致工
程師、知識工作者以及管理者無法有效且
快速地檢閱所有相關專利文件。專利文件
中包含了許多技術及法律上的專業詞彙，
更增加了專利文件閱讀的困難性。基於以
上的想法，本計畫提出一個以本體論為基
的智慧型專利文件自動摘要方法論。在本
研究中，我們提出一個以本體論為基的智
慧型專利文件自動摘要方法論，並整合了
關鍵詞彙辨識技術和重要資訊密度。本研
究針對關鍵辭彙自動擷取與辭彙關連性方
法論之發展，進行關鍵辭彙辨識技術之特
有文件自動摘要方法論之研究，實際建置
雛形系統並評估驗證其摘要方法論。在評
估本摘要方法論方面，我們得到平均
24.54%的摘要壓縮率以及利用關鍵字解
析得到 78.71%的重要資訊保留率。另一方
面，我們利用文件分類，得到了 93.69%的
分類準確率，這足以說明由此研究系統製
作出之文件摘要，其重要資訊與特徵都與
原始文件相似。因此本研究利用此應用文
字探勘技術與自動摘要技術所建構之文件
摘要平台，幫助領域專家更有效率的組織
知識、閱讀文件。 
關鍵字：本體論、摘要系統、專利文件、
關鍵詞彙擷取、文字探勘 
Abstract 
In recent years, owing to the increase 
dramatically in the number of patent 
documents, engineers can not read all 
related patent documents comprehensively. 
In addition, there are unique technical and 
legal vocabularies in the context of patent 
documents, and makes it more difficult to 
fully understand patent claims. According to 
the concerns described above, an automated 
ontology-based patent document 
summarization methodology integrates 
key-phrase recognition techniques and 
stacked significant information densities are 
proposed. The methodologies for key-phrase 
recognition, phrase relevance, and specific 
document summarization using 
domain-specific rules are proposed. In this 
research, we take actions to analyze, design, 
develop summarization platform, and using 
patents for testing documents. Furthermore, 
93.69% accuracy is measured in 
classification. To sum up, the attributes of 
summaries generated by the proposed 
platform are very similar to the attributes of 
 3
term weight of phrase  in document 
the number of term  that occurs in document 
( ) the average number of each term 
                   that occurs in document 
0.2
the aver
jk
jk
k
w j k
tf j k
avg TF
k
slope
pivot
=
=
=
=
= age number of each term 
             that occurs in document set
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
 
Further, the most commonly used term 
weighting method is to combine TF weight 
and IDF weight (Jones, 1972), called “term 
frequency - inverse document frequency” 
(TF-IDF) (Aizawa, 2003). The weight 
scheme (Salton and Buckley, 1988) 
expressed as: 
jk jk jw tf idf= ×  
where wjk is the phrase weight of phrase j in 
document k, tfjk is the number of phrase j 
that occurs in document k, and idfj, inverse 
document frequency of phrase j, is showed 
as follow: 
( )2log ,j jidf n df=   
where n is the total number of documents in 
document set, and dfj is the number of 
documents containing the phrase j in 
document set. 
Moreover, another weighting approach 
called entropy weighting scheme (Dumais, 
1991). The entropy weighting scheme on 
each term is addressed as follows: 
jk jk jw L G= ×  
the weight of term  in document 
the local weight of term  in document  
the global weight of term  in document set
jk
jk
j
w j k
L j k
G j
⎧ =⎪ =⎨⎪ =⎩
 
The Ljk and Gj are given by: 
( ) ( )
( )
1 log  , 0
0   , 0
jk jk
jk
jk
tf tf
L
tf
⎧ + >⎪= ⎨ =⎪⎩
 
the number of term  that occurs 
 
         in document 
jktf j
k
=⎧⎨⎩
 
1
1 log
log
n jk jk
j
j j
j
tf tf
F F
G
n
=+
=
∑
 
the number of term  that occurs 
         in document 
the total number of documents 
      in document set
the total number of term  in 
       the entire document set
jk
j
tf j
k
n
F j
=⎧⎪⎪⎪ =⎪⎨⎪⎪ =⎪⎪⎩
 
Another related research in phrase 
weighting is genetic-based methodology 
(Horng, and Yeh, 2000). They used genetic 
algorithms to measure the weight of each 
word or phrase in order to select document 
feature in document content; however, it 
needs a large number of calculation. 
 
2.3. Phrase Relevance 
Phrase relevance is often measured by 
using co-occurrence and location-based 
approaches. Co-occurrence based approach 
is to calculate how many times that two 
phrases occur in the same documents or 
sentences. Location-based approach is to 
identify the position of a phrase in a given 
document in order to calculate how many 
distances between two phrases. If the result 
of calculation is high, one can conclude that 
the two phrases have high phrase relevance. 
In addition, phrase relevance is usually used 
to synthesize two or more phrases into one 
key concept to enhance accuracy of keyword 
extraction for query and retrieved. Therefore, 
Hou and Chan (2003) proposed an algorithm 
using statistical correlation in automatic 
keyword analysis based on the keyword 
frequencies and keyword locations to extract 
document keywords Matsuo and Ishizuka 
(2003) presented a new keyword extraction 
algorithm without using a corpus. They 
 5
 
Figure 2. The concept of automatic summarization process (Chen, 2000) 
 
2.6. Previous Summarization Methods 
In this subsection, several 
summarization researches are described. 
Those researches utilize Text Mining 
technique, data analysis, statistical 
approaches, and even others heuristic 
methods. 
Keyword-based Methods 
The earliest approach of automatic 
summarization is mostly based on keywords, 
which are provided by manually document 
author(s) or domain experts. Furthermore, 
sentences are scored by sum of TF 
(Edmundson, 1968; Kupiec et al., 1995) or 
TF-IDF (Teufel and Moens, 1997) of 
keywords. After generating scores, n top 
score sentences are selected for 
consolidating a summary. However, the 
weakness of this method is that keywords 
must be provided manually (not 
automatically). 
Heading-keyword-oriented Methods 
Similar to keyword-based methods, some 
researchers consider headings in addition to 
keywords. Lorch et al. (2001) carry out 
three kinds of experiments about effects of 
headings on text summarization. He 
employs college students to summarize a 
multiple topic text in three different 
circumstances: gives those students (1) 
headings that introduce every new subtopic, 
(2) headings that introduce half of the new 
subtopics, and (3) no headings. In his 
experiments, topics of texts are more 
probable to be involved in their summaries 
while there are headings in the text. 
The discoveries illustrate that readers are 
strongly dependent on headings to support 
them understanding the topic structure of a 
text. In the view of deriving from Lorch et al. 
(2001) experiments, the phrases appearing in 
an article title or the paragraph headings (or 
subtitles) are assigned more weights in 
weights scheme. Furthermore, Lam-Adesina 
and Jones (2001) present the sentence score 
formula based on title keywords is shown as 
follows: 
( ) TTSSetence Score SS
TTT
=  
the total number of title terms found in a sentence
the total number of term in a document title
TTS
TTT
=⎧⎨ =⎩
 
Such as the keyword-based method, n 
top scoring sentences are extracted as 
summary. Nevertheless, the writings must 
have a title, headings, and subjects to 
support this type of key-phrase selection. 
Besides keywords and headings, researchers 
also combine many other features or 
methods to raise quality of summary. 
Corpus-oriented Method 
 7
of sentence in document content. 
Consequently, summaries generated by this 
approach can provide definite information of 
user’s interests, such as the interested 
themes. However, this is a human computer 
interactive (HCI) approach, which requires 
substantial users’ input for document 
summarization. 
Discourse-based Approaches 
Barzilay (1997) develops a method 
using lexical chains. Lexical chains are a 
collection of words having the same 
meanings in an article, and each lexical 
chain stands for a concept which is 
described in an article. The method requires 
that you first extract all nouns from the 
document. Afterwards, use a tool, WordNet 
(1990), to identify the meaning of each word, 
and put the words which have the same 
meanings together to form lexical chains. 
The disadvantage is that if the identification 
of a certain word is incorrect, the lexical 
chain is incorrect, and fails to express 
original concepts in the documents. 
Besides, well-defined domain ontology 
can effectively provide the basis for 
knowledge representation to support 
knowledge sharing and reusing. Wu and Liu 
(2002) presented an ontology-based article 
summarization system. First, map every 
word in the document to a domain ontology 
tree, and record frequencies of words map to 
the nodes. The number of sub-nodes sum up 
to the upper-node and the top node 
represents the concept and topic in the 
document. Then, determine the article topics 
according to statistical results of top nodes 
and use frequencies of corresponding top 
nodes as the weights. Finally, compute 
scores for each paragraph, as follows: 
nni owowowP +++= .......2211  
Pi : The scores of paragraph i 
wi: The weights of ith 
oi: The scores of topic i 
Give the paragraph priority according 
to scores, and select paragraphs of a fixed 
proportion as a document summarization. 
 
2.7. Text Mining Applications in 
Intellectual Property Documents 
Trappey et al. (2004) and Hsu et al. 
(2004) implement a multi-channel legal 
knowledge service center named Legal 
Knowledge Management Platform (LKM), 
which integrates knowledge management, 
contact center, and CRM data mining 
techniques. On the platform, unstructured 
IP/legal documents are uploaded, their 
key-phrases are extracted, documents are 
categorized, meta-data are analyzed, 
document version is controlled by naming 
scheme, and access authorized are processed 
automatically. Moreover, during key-phrase 
recognition process, the correlation of 
technical key-phrase can be discovered, and 
the system can automatically cluster 
knowledge documents. As the result of 
being supported by multi-channel legal 
knowledge service center, knowledge 
providers and knowledge demander can 
share knowledge effectively and efficiently. 
For enhancing LKM’s strategic IP analysis, 
Hsu, Trappey, et al. (2004) further addressed 
the technology of knowledge document 
clustering analysis for enterprise R&D 
strategic planning. In addition, Hsu (2006) 
proposed the process of patent knowledge 
extraction and methodologies of patent 
 9
original, and the Retention Ratio, which is 
how much information is retained. 
Compression Ratio can be stated as 
follow: 
,lenth of SummaryCR
lenth of Full Text
=  
and Retention Ratio formula is shown as 
follows: 
information in SummaryRR
information in Full Text
=  
 
3. Key-Phrase Recognition 
Key-phrase recognition is the first 
phase in our system methodology. In our 
proposed methodology, system would output 
key-phrases automatically without the 
document author or a domain expert’s 
assistance. The procedure of key-phrase 
recognition is detailed as follows. 
1) Import Ontology: A well-defined 
domain ontology constructed by experts 
would be imported in the database. 
2) Word Segmentation: After an IP 
document is uploaded, the document content 
is retrieved into our system and the text is 
separated into a list of words. 
3) Stopping: Following segmentation, 
we remove unmeaning words, which are 
called stop-words, in the word list. 
4) Morphology Diagnosing: According 
to character of key-phrase, we reserve both 
nouns and verbs to key-phrase recognition 
because nouns or verbs are sometimes 
important to illustrate the content of the 
document (Matsuo and Ishizuka, 2004). 
Therefore, we check the remnant word list 
with a lexical dictionary to identify their 
morphology and just reserve verbs and 
nouns to put on our key-phrase recognition 
method. 
5) Stemming: Following morphology 
diagnosing, Porter Stemming Algorithm 
(Porter, 1980) is used to restore the tense 
and plurality of words to a possible root 
word. 
6) Phrase Weight Calculation: For one part, 
we use TF-IDF method (term frequency – 
inverse document frequency) weight scheme 
(Salton and Buckley, 1988) to measure 
phrase information weight, because TF-IDF 
is easier and more effective than other 
weight scheme (Dumais, 1991). 
In this step, we calculate the phrase 
weight of distinct words that is reserved 
from previous process and order the distinct 
words based on their TF-IDF weight. For the 
other part, we utilize domain ontology 
defined by experts in advance as keywords 
and key-phrases, and an extraction algorithm 
is adopted to compare with full text of the 
patent document. At the same time, the 
frequency of mapping words will be 
calculated. Moreover, synonym which is 
implemented based on the key words and 
phrases (nodes) of ontology tree will also be 
picked up. Afterwards, the frequency of 
synonym appearing in the content of the 
patent document will be fed back to the node 
having the same meaning with the synonym. 
7) Phrase Synthesis: After weight 
calculation, we try to find multi-gram 
phrases based on single keywords which are 
extracted by TF-IDF method and synthesize 
phrase weight if the meaning of phrases is 
same. At the same time, if keywords or 
key-phrases extracted by ontology method 
are exactly the same, we will remove the 
words in common. 
 11
where RevDB(KPi, KPj) is the 
relevance value of key-phrase i and 
key-phrase j in whole document 
database, Revk(KPi, KPj) is the 
relevance value of key-phrase i and 
key-phrase j in document k, and n is the 
number of document that had 
calculated relevance value of 
key-phrase i and key-phrase j. 
 
4. Summarization Methodology 
This research uses text mining 
technique to develop an automated 
ontology-based patent document 
summarization system based on key-phrase 
recognition and significant information 
density. Therefore, people can read patent 
documents efficiently and acquire 
knowledge rapidly in generating quick 
summaries of lengthy documents. The 
ontology of Power Hand Tool (PHT) is 
imported in Figure 3 and the system 
methodology consists of two phases is 
shown in Figure 4. Phase 1 is the document 
key-phrase recognition. In Phase 1, the main 
task is to identify a good set of key-phrases. 
In addition, outputs of this phase will 
provide the second phase some relevant 
information for summary generation. Phase 
2 is to generate summary automatically. The 
significant information density is measured 
to identify the substantial information 
paragraphs in the target document for the 
synthesis of summary. 
 
Figure 3. The ontology of Power Hand Tool 
 
 13
Table 2. Paragraph similarity matrix 
 P1 P2 P3 … Pn 
P1 1 Sim(p1 , p2) Sim(p1 , p3) … Sim(p1 , pn)
p2 Sim(p2 , p1) 1 Sim(p1 , p2) … Sim(p1 , pn)
p3 Sim(p3 , p1) Sim(p2 , p2) 1 … Sim(p2 , pn)
…
 
…
 
…
 
…
 … 
…
 
pn Sim(pn , p1) Sim(pn , p2) Sim(pn , p3) … 1 
Pj = jth paragraph in a document. 
z Cluster paragraphs of a document 
based on paragraph similarity matrix in 
3 steps. 
i. Give a target value k of the cluster 
number (between two and the 
number of paragraphs in the 
document) and apply K-Mean 
Algorithm (Han and Kamber, 2000) 
to paragraph clustering. 
ii. Use RMSSTD (Root Mean Square 
Standard Deviation, inter-class 
similarity) and RS (R-Squared, 
intra-class similarity) (Sharma, 1996) 
to evaluate results of clustering with 
target value of the cluster number 
(maximizing the intra-class 
similarity while minimizing the 
inter-class similarity) and find the 
optimal k (Hsu et al., 2004; Wu, 
2004). 
iii. Export the result of clustering with 
optimal k. 
2) Significant Information Density: 
First, the significant information in 
document content is identified. In this 
methodology, we outline six pieces of 
significant information: (1) key-phrases, (2) 
title phrases, (3) phrases that has high 
relevancy with key-phrases, (4) topic 
sentences, (5) domain-specific phrases and 
(6) indicator phrases. Thus, we can say that 
there are key-phrase layer (KPL), title 
phrase layer (TPL), key-phrase relevance 
phrase layer (RPL), topic sentence layer 
(TSL), domain-specific phrase layer (DPL) 
and indicator phrase layer (IPL) in a 
document in addition to general phrase layer 
(Figure 6). 
 
Figure 6. Concept of significant information 
layers 
 
In order to calculate significant 
information density, one must quantify 
significant information, namely significant 
information mass (SIM) for each layer. 
Therefore, after concepts clustering, SIMs of 
six layers for each paragraph are calculated 
in every concept cluster. The SIM of each 
layer is described as follows. 
z Key-Phrase Layer (KPL): the 
significant information mass of 
key-phrase layer in paragraph m of the 
document, which denoted (SIM of 
KPL)m, is shown as follows: 
( ) ( ) ,im i ijmSIM of KPL = kpf idf ipf× ×∑  
where kpfim is the number of 
key-phrase i occurred in paragraph m 
of the document, idfi is the inverse 
document frequency of key-phrase i, 
ipfij is the inverse paragraph frequency 
of key-phrase i in cluster j, and ipfij can 
be calculated by following equation: 
 15
each paragraph of every concept cluster. 
Then, we extract paragraphs that have the 
highest significant information density in 
concept cluster into candidate summary set. 
We define Significant Information Density 
( ρ ) of paragraph is: the significant 
information mass which per phrase owns in 
a paragraph and its formula is shown as 
follows: 
( ) ( )
( ) ( )
( )
Significant Information Density( )
[     
    
  ] total number of phrases
in paragraph .
m m
m m
m
= SIM of KPL SIM of TPL
SIM of RPL SIM of DPL
SIM of IPL
m
ρ
+
+ +
+ ∕
 
Therefore, the higher density of the 
significant information a paragraph owns, 
the higher intensive degree of the significant 
information a paragraph has. In other word, 
the higher the amount of significant 
information appears in a paragraph, the 
higher intensive degree of the significant 
information a paragraph has, too. Therefore, 
the paragraph is really important in the 
document. Finally, we pick the paragraphs 
that own the highest significant information 
density in every concept cluster into 
candidate summary set. If the paragraph that 
we pick owns topic sentence of the 
document, we will choose a paragraph 
owning second high density in the same 
cluster instead of the paragraph which has 
the highest information density. 
 
3) Domain-Specific Rule: Besides 
significant information density, if we can 
mine special patterns of some 
domain-specific IP documents, we are able 
to build some domain-specific rules to 
increase quality of summarization system. 
Using patent documents as example, we find 
that patents are always divided into two 
parts. One is description of patent, and the 
content of description is addressed for patent 
technology aspect method. Another is claims 
of patent, and the content of claims is to 
declare the boundaries of right in law that 
this patent owns in inventing. Therefore, 
when we summarize patent document, we 
separate the two part of patent, generate 
summary of patent description applying 
key-phrase recognition and significant 
information density, and create summary of 
patent claim using domain-specific rules. 
We discover that claim could be subdivided 
into independent claim and dependent claim. 
Furthermore, Dependent claim attaches to 
independent claim. It is to say that 
independent claim is the most important part 
in whole claim; therefore, we propose 
following method to find out independent 
claim of a patent as the summary in claim 
part: 
z Divide patent claim into m section in 
accordance with assign numbers, where 
m is the total item of claim in patent. 
z Look for claim content i that have not 
been confirmed yet. If there is no claim 
i confirmed yet, we would end this 
method. 
z Search string of whole claim section. 
Supposing not having pattern of 
[“claim”+ Arabic numeric], it is 
independent claim, which we want to 
get. 
z Return to step 2. 
4) Summary Generating with 
Summary Template: In the end, we generate 
summary using key-phrases recognized in 
 17
Conference on Computational Statistics 
(Compstat 2002), Berlin, Germany, 
August 24 - 28, pp. 395-400. 
(6) Borgelt, C., 2003, “Efficient 
implementations of Apriori and Eclat,” 
Proceedings, the IEEE ICDM Workshop 
on Frequent Itemset Mining 
Implementations, Melbourne, Florida, 
November 19. 
(7) Buckley, C., Singhal, A., and Mitra, M., 
1995, “New retrieval approaches using 
SMART: TREC 4,” Proceedings, The 
4th Text REtrieval Conference, 
Gaithersburg, Maryland, November 1-3, 
pp. 25-48. 
(8) Chen, Y. J. (Advisor: Dr. Chang, J. S.), 
2000, Scalable Summarization for 
Chinese Text, MS Thesis, Department of 
Computer Science, National Tsing Hua 
University, Hsinchu, Taiwan. 
(9) Cheng, K. S, Young, G. H., and Wong, K. 
F., 1999, “A study on word-based and 
integral-bit Chinese text compression 
algorithms,” Journal of the American 
Society for Information Science, Vol. 
50(3), pp. 218-228. 
(10) Chien, L. F., Huang, T. I., and Chien, M. 
C., 1997, “Pat-tree-based keyword 
extraction for Chinese information 
retrieval,” Proceedings, The 20th Annual 
International ACM SIGIR’97 
Conference on Research and 
Development in Information Retrieval, 
Philadelphia, Pennsylvania, July 27-31, 
pp. 50-59. 
(11) Decker, S., and Melnik, S., 2000, “The 
Semantic Web: The Roles of XML and 
RDF,” Journal of IEEE Internet 
Computing, Vol. 4, No. 5, pp.63-74. 
(12) Dumais, S. T., 1991, “Improving the 
retrieval of information from external 
sources,” Behavior Research Methods, 
Instruments, and Computers, Vol. 23(2), 
pp. 229-236. 
(13) Edmundson, H. P., 1968, “New 
methods in automatic extraction,” 
Journal of the ACM, Vol. 16(2), pp. 
264-285. 
(14) Frank, E., Paynter, G. W., Witten, I. H., 
Gutwin, C., and Nevill-Manning, C. G., 
1999, “Domain-specific keyphrase 
extraction,” Proceedings, The Sixteenth 
International Joint Conference on 
Artificial Intelligence (IJCAI-99), 
Stockholm, Sweden, July 31 - August 6, 
pp. 668-673. 
(15) Han, J. and Kamber, M., 2000, Data 
Mining: Concepts and Techniques, 
Morgan Kaufmann Publishers, San 
Francisco, California. 
(16) Hassel, M., 2004, Evaluation of 
Automatic Text Summarization - A 
Practical Implementation, Licentiate 
Thesis, Department of Numerical 
Analysis and Computer Science, Royal 
Institute of Technology, Stockholm, 
Sweden. 
(17) Horng, J. T., and Yeh, C. C., 2000, 
“Applying genetic algorithms to query 
optimization in document retrieval,” 
Information Processing and 
Management, Vol.36, pp. 737-759. 
(18) Hou, J. L., and Chan, C. A., 2003, “A 
document content extraction model 
using keyword correlation analysis,” 
International Journal of Electronic 
Business Management, Vol. 1(1), pp. 
54-62. 
 19
approach to mechanized encoding and 
searching of literary information,” IBM 
Journal of Research and Development, 
Vol. 1(4), pp. 309-317. 
(32) Mani, I., and Maybury, M., 1999, 
“Intoduction,” Mani, I. and Maubury, M. 
(Ed.), Advances in Automatic Text 
Summarization, MIT Press, Cambridge, 
Massachusets, pp. ix-xv. 
(33) Matsuo, Y., and Ishizuka, M., 2004, 
“Keyword extraction from a single 
document using word co-occurrence 
statistical information,” International 
Journal on Artificial Intelligence Tools, 
Vol. 13(1), pp. 157-169. 
(34) Miller, G. A., 1990, “Wordnet: An 
Online Lexical Database,” Proceedings 
of the 14th IJCAI Workshop on Data 
Engineering for Inductive Learning, Vol. 
3, No. 4, pp. 235-312. 
(35) Motik, B., and Glavinic, V., 2000, 
“Enabling Agent Architecture through an 
RDF Query and Inference Engine,” 
Proceedings of the 10th Conference on 
Mediterranean Electro-technical. 
(36) Nomoto, T., Matsumoto Y., 2003, “The 
diversity-based approach to 
open-domain text summarization,” 
Information Processing and 
Management, Vol. 39, pp. 363-389. 
(37) Noy, N. F., Sintek, M., and Decker, S., 
2001, “Creating Semantic Web Contents 
with Protégé-2000,” Journal of IEEE 
Intell. Syst., Vol. 16, No. 2, pp. 60–71. 
(38) Peng, F. and Schuurmans, D., 2001, 
“Self-supervised Chinese word 
segmentation,” Proceedings, The Fourth 
International Symposium on Intelligent 
Data Analysis (IDA-2001), Lisbon, 
Portugal, September 13-15, pp. 238-247. 
(39) Porter, M. F., 1980, “An algorithm for 
suffix stripping,” Program, Vol. 14(3), 
pp. 130-137. 
(40) Resource Description Framework 
(RDF) Model and Syntax Specification 
W3C Recommendation, 1999, 
http://www.w3.org/RDF/. 
(41) Salton, G., and Buckley, C., 1988, 
“Term-weighting approaches in 
automatic text retrieval,” Information 
Processing and Management, Vol. 24(5), 
pp. 513-523. 
(42) Selamat, A., and Omatu, S., 2004, 
“Web page feature selection and 
classification using neural networks,” 
Information Sciences, Vol. 158, pp. 
69-88. 
(43) Sharma, S. C., 1996, Applied 
Multivariate Techniques, John Wiley & 
Sons, Hoboken, New York. 
(44) Staaba, S., and Erdmann, M., 2000, 
“An Extensible Approach for Modeling 
Ontologies in RDF(S),” Proceedings of 
ECDL Workshop on the Semantic Web, 
pp. 11-22. 
(45) Studer, R., Benjamins, V. R., and 
Fensel, D., 1998, “Knowledge 
Engineering: Principles and Methods,” 
Journal of Data and Knowledge 
Engineering, Vol.25, pp. 161-197 
(46) SUMMAC, 1998, 
“TIPSTER/SUMMAC summarization 
evaluation,” Firmin Hand, T. and 
Sundheim, B. (Ed.), Proceedings, The 
TIPSTER Text Phase III Workshop, 
October, Washington, D.C. 
(47) Sun, M. C. (Advisor: Prof. Hou, J. L.), 
2003, Heuristic Model and Technology 
