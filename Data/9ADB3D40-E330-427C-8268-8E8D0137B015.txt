音特徵向量或是鑑別分析並不保證此一特性，因而有學者
提出以最大相似度線性轉換(Maximum Likelihood Linear 
Transformation, MLLT)，嘗試讓轉換過後的共變異矩陣的
值集中在對角線上，在不影響聲學模型相似度估測的條件
下，儘量滿足對角化共變異矩陣的要求。因此，目前在大
詞彙連續語音辨識的語音特徵擷取上常見到線性鑑別分析
與最大相似度線性轉換結合(LDA-MLLT) [9]或是異質性線
性鑑別分析與最大相似度線性轉換結合(HLDA-MLLT) [10, 
11]等的一些作法。本計畫主旨在於探討各種資料導向線性
特徵轉換和各種結合時域-頻域資訊方法於中文大詞彙連續
語音辨識之應用，並結合其他強健性技術，研究噪音對於
上述各種資料導向線性特徵轉換技術的影響。我們以中文
廣播新聞為實驗語料，初步地以外場採訪記者 (Field 
Reporter)語料部分作為語音辨識實驗題材。 
本計畫結案報告內容接下來的安排如下：在第二節我
們將介紹各種資料導向線性轉換的技術，包含了主成份分
析、線性鑑別分析、異質性線性鑑別分析、最大相似度線
性轉換；第三節介紹大詞彙連續語音辨識系統；第四節將
描述實驗結果；第五節為結論與未來展望。 
2. 資料導向線性特徵轉換技術 
近幾年來資料導向(Data-Driven)線性特徵轉換在語音特徵
擷取的研究上佔有相當重要的地位。因為資料導向線性特
徵轉換可以藉由語音訓練資料的統計資訊來自動地找出語
音特徵空間中重要的基底向量，使得經轉換後的語音特徵
能保有重要的成份或具有較高的鑑別力，且可以進一步去
除多餘的維度；由於基底向量是根據訓練資料而來，所以
找出的基底向量將較能代表語音訊號的特徵。以下將針對
實驗組的資料導向線性特徵轉換方法做介紹。 
2.1 主成份分析 
主成份分析(Principal Component Analysis, PCA)在圖樣辨
識中為很常見的技術。它的精神在於將維度間為相關
(Correlated)的一群特徵向量用較少維度來表示，且使得維
度間變成彼此無關(Uncorrelated)，同時能保有特徵向量的
變異量(Variation) [12]，所以主成份分析可以找出特徵主
要成份所在的基底向量。如圖 2 所示，投影在第一主成份
基底向量的資料擁有最大的變異量，投影在第二主成份基
底擁有次大的變異量，且基底向量間各自為單位正交
(Orthonormal)。主成份分析的作法為使用所有訓練資料[ ]NxxxxX ,,,, 321 L= ，N為資料的總數，每個 ix 為 n維向
量，來統計訓練資料的整體共變異矩陣(Covariance Matrix) 
T ： 
( )( )TN
i
ii XxXxN
T ∑ −−=
=1
1    (1) 
其中 X 為整體的平均向量，所以T 為 nn× 維矩陣。 
∑=
=
N
i
ixN
X
1
1     (2) 
對 T 求特徵向量分解(Eigenvectors Decomposition)，以特
徵值(Eigenvalues)最大的前 P 個特徵向量(Eigenvectors)當
成基底矩陣(亦稱轉換矩陣) pθ 的行向量，最後所有資料
可 以 利 用 求 得 的 轉 換 矩 陣 投 影 到 新 特 徵 空 間[ ]NyyyyY ,,,, 321 L= ： 
Nixy i
T
pi ,,2,1, L==θ    (3) 
故本計畫結案報告以下所提及不同的資料導向線性鑑別分
析法的差別只在於轉換矩陣的求取不同，最終都是藉由式
(3)求得新的資料向量表示方式。 
2.2 線性鑑別分析 
線性鑑別分析(Linear Discriminant Analysis, LDA) [4]與主
成份分析在作法上相似，都是求取一個轉換矩陣再藉此作
線性轉換與特徵降維。但線性鑑別分析的目的在於使得轉
換後特徵之間可以保有最大的分類鑑別資訊。如圖 3 所
示，資料群投影到第一基底後比投影到第二基底後可以有
較大的鑑別力。線性鑑別分析則有幾個假設：第一，假設
投影後並不是所有的維度都包含鑑別資訊，所有鑑別資訊
都包含在前 p 維子空間，而後(n-p)維子空間即可省略；第
二，假設每個類別都是高斯分佈；第三，所有類別分佈的
變異量都相同。線性鑑別分析的作法必須使用到訓練資料
的類別資訊來統計各類別的分佈，所以算是一種監督式學
習(Supervised Learning)。當資料以向量方式呈現時，希望
類別間共變異矩陣 B  (Between-class Covariance Matrix)轉
換後的行列式值越大越好，且類別內共變異矩陣 W  
(Within-class Covariance Matrix)轉換後的行列式值越小越
好。也就是要求取一個基底矩陣 pθ 使得經線性轉換過後
訓練資料的兩行列式(Determinint)比值最大： 
p
T
p
p
T
p
W
B
p θθ
θθθ θmaxarg
ˆ =
   (4) 
在此，假設資料共分成 J個類別， jN 為屬於第 j個類別的
訓練語料個數， N 為所有訓練語料的個數， jW 為第 j 個
語音輸入
特徵
擷取
圖樣
辨識
聲學
模型
辨識
結果
特徵
擷取
訓練
語料
 
圖 1 基於圖樣辨識的語音辨識系統。 
資料群
X軸
Y軸
第一主成份基底
第二主成份基底
 
圖 2 主成份分析示意圖。 
{ }( ) ( )
( ) ( )
( ) θπ
μθμθ
θ
 log  2  log
2
1      
2
1   
 log, log
1
)(
1
1
)( )()(
1
N
xx
xpxL
N
i
ig
n
N
i
ig igi
TT
igi
T
N
i
ii
+∑ ⎟⎠
⎞⎜⎝
⎛ ∑−
∑ ∑ −−−=
∑=
=
=
−
=
 (13) 
其中 N 為訓練語料的資料向量總數。首先固定θ ，對式
(13)各別轉換後平均向量 jμ 和共變異矩陣∑ j 做微分，可
求得 jμ 和 ∑ j 。 pjμ 、 0μ 、 ∑ pj 與 ∑ − pn 分別表示如
下： 
j
T
p
p
j Xθμ =     (14) 
XT pn−=θμ0     (15) 
pj
T
p
p
j W θθ=∑     (16) 
pn
T
pn
pn T −−− =∑ θθ    (17) 
經由化簡並去掉與θ 無關的變數後，最大化類別相似度的
線性轉換矩陣 θˆ 可表示成： 
⎪⎭
⎪⎬⎫∑−
⎩⎨
⎧ −=
=
−−
J
j
pj
T
p
j
pnj
T
pn
W
N
WNN
1
log 
2
                     
log 
2
log max arg
θθ
θθθθ θ
)
 (18) 
由式(18)可以看出轉換後的共變異矩陣並不是對角化的，
可以在式子推導的過程中假設轉換後的共變異矩陣僅對角
線有值，稱為對角化異質性線性鑑別分析(Diagonal HLDA, 
DHLDA)，式子為：  
( )
( ) ⎪⎪⎭
⎪⎪⎬
⎫
⎪⎪⎩
⎪⎪⎨
⎧
∑−
−
=
=
−−
J
j
pj
T
p
j
pn
T
pn
WDiag
N
TDiagNN
1
log
2
log
2
log
maxargˆ
θθ
θθθ
θ θ
(19) 
期間θ 的求取並沒有固定解(Close-form Solution)，只能利
用數值方法的最佳化技術來求解，對θ 微分然後迭代更新
θ 的值，直到收斂。 
(b) 固定解方法(Close-form-solution Method) 
由於上述的數值方法沒有固定解，造成需要非常多次的迭
代來更新θ 值。每作一次實驗，都需花費相當多的時間來
求取異質性線性鑑別分析的基底矩陣，所以我們亦參考英
國劍橋大學 Gales 教授在 1999 年提出的方法對異質性線性
鑑別分析求解[14, 15]。這個方法會產生固定解，並且保證
整體相似度會隨著每次迭代更新θ 而增加，但只適用在對
角化異質性線性鑑別分析的求解。 
{ }( ) ( )
( )( ) ( ) ( )( )
( ) ( )( ) θπ
θθ
θ
log2log
2
1
2
1
log;log
1
1
1
1
N
XxXx
xPxL
ig
nN
i
igi
T
ig
TN
i
igi
N
i
ii
+∑∑−
∑∑ −−=
∑=
=
−−=
=
 (20) 
其中 θ 為 nn× 維矩陣，每一行(Column)為一基底向量，
θ 可分解成 TiTi ca 。 ia 為θ 的第 i 行，為 1×n 維向量。 ic
為θ 第 i行的餘因子(Cofactors)，為 n×1 維向量。由於假設
轉 換 後 類 別 共 變 異 矩 陣 為 對 角 化 ， 如 此 可 以 把
( )( ) ( ) ( )( )∑− −− igiTigTigi XxXx θθ 1 用一行一行(row by row)的方式
表示，則式(20)可以代換成如下： 
{ }( )
( )( )( )
( )( ) ( ) ( )( )
( )TiTi
N
i
ig
nN
i ig
diag
igT
k
i
caN
ixa
xL
k
log
2log
2
1
2
1
;log
11 2
2
+
∑ ∑−∑∑−=
==
π
σ
θ
(21) 
其中 ( )( ) ( )igiig Xxix −= ， ( )ig 為 ix 對應的類別， ( )( )2igdiagkσ為經線性轉換後共變異矩陣對角線上第 k個元素。把式(21)
與 a無關的變數集中成 K，並做代數轉換： 
{ }( )
( ) ( ) KcaNaGa
xL
T
i
T
i
k
k
kT
k
i
++∑−= log
2
1
;log θ
 (22) 
其中 
j
J
j j
diag
jk W
N
G
k
∑=
=1 2σ
   (23) 
式(22)對 ia 做微分並使其等於零，最後可求得： 
T
i
i
i
T
i
i
i
cGc
NcGa 1
1
−
−=
   (24) 
由於假設只有前 p 維子空間帶有分類資訊，所以 iG 和( )2jdiagkσ 可以拆成如下。 
( ) ( )
( ) ( )⎪⎪⎪⎩
⎪⎪
⎪
⎨
⎧
∑ >
∑ ≤
=
=
=
J
j j
diag
j
J
j
j
j
diag
j
k
pkT
N
pkW
N
G
k
k
1 2
1 2
σ
σ
  (25) 
( ) ( )( )⎪⎩⎪⎨
⎧
>
≤=
pjTaa
pjaWa
k
T
k
kj
T
kj
diag k
2σ
   (26) 
2.4 最大相似度線性轉換 
最 大 相 似 度 線 性 轉 換 (Maximum Likelihood Linear 
Transformation, MLLT) [7, 16]與上述資料導向線性特徵轉
換的作法是一樣的，都是求取一個轉換基底矩陣來轉換特
徵空間。但在用途跟精神有所不同，首先，最大相似度線
性轉換並不執行降維動作，而是保留訓練資料特徵的所有
維度。其次，它的精神在於使轉換後類別的共變異矩陣對
角化，同時希望分別使用對角化或全秩(Full)共變異矩陣所
計算出的特徵向量的整體類別相似度差要最小；也就是說
雖然經線性轉換後類別的共變異矩陣只保留對角線的值，
但希望整體類別相似度與保留共變異矩陣所有元素是相同
的。如此可以用來解決上述主成份分析、線性鑑別分析和
異質性線性鑑別分析的重大缺陷，就是經線性轉換後類別
的共變異矩陣不是對角化的現象，而造成後端隱藏式馬可
 
4. 實驗 
實驗語料與資料導向線性特徵轉換技術，以及這些技術與
最大相似度線性轉換結合的相關實驗結果，將詳細敘述如
下。 
4.1 實驗語料 
本計畫主要使用的語料庫為 MATBN 電視新聞語料[24]，
為中央研究院資訊所口語小組耗時三年與公共電視台合作
錄製完成。其中包含 2001 年的新聞 30 小時、2002 年 146
小時及 2003 年 24 小時。本計畫初步地選擇採訪記者語料
作為實驗語材，其中包含 25.5 小時的訓練集(5,774 句)，供
聲學模型訓練之用；1.5 小時的評估集(292 句)，供辨識評
估之用，其中男女語料各半。另外，為了探討不同語音特
徵參數對於語音辨識的強健性，本計畫對公視新聞的辨識
語料加入 Aurora 2.0 不同訊噪比(SNR)下的各種噪音[25]。
Aurora 2.0 是 由 歐 洲 電 信 標 準 協 會 (European 
Telecommunications Standards Institute, ETSI)所發行的語
料，提供了八種不同噪音，有地下鐵 (Subway)、人聲
(Babble) 、 汽 車 (Car) 、 展 覽 會 館 (Exhibition) 、 餐 廳
(Restaurant)、街道 (Street)、機場 (Airport)、火車站(Train 
Station)。本計畫使用 Aurora 2.0 提供的不同噪音搭配不同
程度的訊噪比，分別是 -5dB 、 0dB 、 5dB 、 10dB 、
15dB20dB。強健性實驗的方法為：使用 Aurora 2.0 內的不
同噪音搭配不同的 SNR 比例加入我們的測試語料中，使
用原始訓練語料所訓練出的模型來辨識加入噪音的測試語
料，藉此來驗證不同線性特徵轉換與強健性技術所求得語
音特徵值的強健性。 
4.2 資料導向線性特徵轉換技術的應用 
本計畫把資料導向線性特徵轉換應用在頻域-時域特徵擷
取，希望藉由資料導向線性特徵轉換來同時擷取頻域上與
時域上重要或具鑑別力的特徵向量。如圖 6 所示，首先由
資料向量 ix 本身加上前後各取 k 個資料向量形成超級資料
向量 iz  (Supervector)，此處的 k 為 2。超級資料向量 iz 經
由基底矩陣θ 線性轉換後可得新資料向量 iy 。其中的θ 就
是由資料導向線性特徵轉換求得。本計畫中 ix 為 18 組梅
爾濾波器(Mel Filter Banks)的輸出，前後各取 4 個資料向
量，所以 iz 為 162 維向量，最後新資料向量 iy 為 39 維。 
4.3 中文自由音節辨識 
本小節主要在探討各種資料導向線性特徵轉換效能的比
較，主要有三個觀察。首先從表 1 我們可看到 DHLDA-I 
(數值方法求解)與 DHLDA-II (固定解方法求解)的效能在伯
仲之間，這是因為兩個方法都是從同一個目標函式所推導
而成，自然地兩者會有相似的辨識率。不過數值方法由於
迭代次數太多，要找到跟固定解方法一樣好的轉換矩陣是
有困難的。其次，如下頁表 1 可以看出，LDA 的辨識率比
DHLDA-I 和 DHLDA-II 都差，本計畫推測這是因為線性鑑
別分析應用在頻域-時域特徵擷取時，擷取出的特徵向量並
不滿足類別共變異矩陣為對角化，造成隱藏式馬可夫模型
參數估測失真，進而影響辨識率。所以一但線性鑑別分析
結合最大相似度線性轉換後，辨識率就與 DHLDA-I 和
DHLDA-II 相差不遠，如表 2 所示。但這並不代表異質性
的線性鑑別分析沒有功效，觀察表 1 可以發現 HLDA 的辨
識率是最低的，但在表 2 所示 HLDA+MLLT 的辨識率與
HLDA 相比，大幅度的減少 15.71%相  對錯誤率。
HLDA+MLLT 的辨識率與 LDA+MLLT 的辨識率比較，減
少 2.65%相對錯誤率。而 HLDA+MLLT 的辨識率與
DHLDA-II 的辨識率相比，也減少 1.98%相對錯誤率。 
再來討論音節強健性實驗，比較主成份分析與線性鑑
別分析，從表 3 可以看出在 20dB 至 5dB 的訊噪比下，
LDA+MLLT+CN 的辨識率都比 PCA+MLLT+CN 來的高；
只有在 0dB 與-5dB 的訊噪比下會相反。比較 LDA 與
DHLDA-II 的辨識率，從表 3 可以看出在 20dB 至 5dB 的
訊噪比下，LDA+MLLT+CN 的辨識率都比 DHLDA-II+CN
來的高；只有在 0dB 與-5dB 的訊噪比下會相反，其中-5dB
時，DHLDA-II+CN 的辨識率表現十分出色，也因為如此
使 得 DHLDA-II+CN 在 所 有 噪 音 下 表 現 都 比
LDA+MLLT+CN 好。此外特別的是，在乾淨環境下，
特徵向量 
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
超級特徵向量
5 4 3 2 1 1 2 3 4 5 6
 
                             
i i i i i i i i i i i ix x x x x x x x x x x x− − − − − + + + + + +
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦
LL LL
                    
轉換矩陣 θ
iz
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
  ix
1ix −
2ix −
1ix +
2ix +
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
 iy
T
i iy zθ=
新特徵向量
 
圖 6 頻域-時域特徵擷取示意圖。 
表 1 各種資料導向線性特徵轉換與強健性技術結合，數據
皆為音節錯誤率。(+:表示加上倒頻譜平均消去法處理
(CMS)或加上倒頻譜正規化法處理(CN)) 
Method Baseline +CMS +CN 
MFCC 44.97 41.68 41.06 
PLP 46.46 42.50 41.82 
PCA 45.40 39.82 38.89 
LDA 43.17 38.80 38.30 
HLDA 47.10 40.08 39.22 
DHLDA-I 40.90 37.41 36.80 
DHLDA-II 40.50 37.05 36.45 
 
表 2 各種資料導向線性特徵轉換與最大相似度線性轉換及
強健性技術結合，數據皆為音節錯誤率。 
Method Baseline +CMS +CN 
MFCC 44.67 40.67 40.10 
PLP 46.92 42.36 41.94 
PCA 41.98 37.53 37.03 
LDA 40.78 37.06 36.47 
HLDA 39.70 36.57 36.12 
 
