ii
Abstract
Software security is largely dependent on the code quality and the quality can only be
assured by a high coverage testing tool. Testing has become the bottleneck of software
development process due to the cost of testing process even attributing to 50% of total cost.
The trend of contemporary software development process is toward the use of auxiliary tools
to reduce the resource usage of testing process. However, most testing tools require users to
manually produce testing drivers which have much to do with testing quality. It is due to the
lack of automatic input generation that ensures the existence of the execution path from inputs
to the specified program state with rendered testing condition. Nevertheless, it is our intuitive
way to prove the testing efficiency and testing quality. In this paper, we analyze program
semantics based on program logic reasoning and try to generate testing cases to cover all
possible paths. We design and implement an automatic testing tool, called ALERT, which can
analyze logic constraints to represent semantics of program executions, and reason testing
inputs of remaining paths. We perform coverage test by Gcov [1] to evaluate the testing
efficiency of ALERT and reach 90% of coverage rate. With our ALERT tool, we can
systematically generate input data to investigate potential software vulnerability and analyze
the corresponding program behaviors.
iv
4.2. Symbolic Evaluator ......................................................................................... - 22 -
4.2.1. Paired Symbols .................................................................................... - 22 -
4.2.2. Dependency ......................................................................................... - 25 -
4.3. Logic Resolver................................................................................................. - 25 -
4.3.1. Variable................................................................................................ - 25 -
4.3.2. Freezing Expression ............................................................................ - 25 -
4.3.3. Domain Range ..................................................................................... - 26 -
4.4. Implementation Difficulties............................................................................. - 26 -
4.4.1. Generation of User-defined Type Interface ......................................... - 26 -
4.4.2. State-sensitive Variable Renaming ...................................................... - 27 -
5. Experimental Results................................................................................................... - 28 -
5.1. Instrumentation and Generation Overhead...................................................... - 28 -
5.2. Expand and Explore......................................................................................... - 28 -
5.3. Similar Path Discovery.................................................................................... - 28 -
6. Conclusion................................................................................................................... - 30 -
6.1 Self Evaluation……………………………………………………………….- 30
6.2. Future Work ..................................................................................................... - 30 -
6.2.1. Feasible Input Generation.................................................................... - 31 -
6.2.2. Delta Debugging.................................................................................. - 31 -
References ........................................................................................................................... - 31 -
vi
List of Tables
Table 1, Manual vs. automated testing .................................................................................. - 1 -
Table 2, Comparison of former data recreation methods ...................................................... - 6 -
Table 3, Comparison of model checking methods ................................................................ - 7 -
- 2 -
Test execution 466 23 95%
Test result analyses 117 58 50%
Defect tracking 117 23 80%
Report creation 96 16 83%
Total hours 1090 277 75%
It encouraged us to develop ALERT, an automatic testing tool that automates these test
steps including test case development, test execution and test result analyses. In order to
generate test cases, ALERT must handle the logic information about program source and
static semantics. In addition to generating test cases, the creation of test interface is required
to automate test execution. To demonstrate all system functionalities in a direct way, we have
to test every component one by one, such a method named unit testing. Finally, due to the
automation of the whole test process, we need to generate test case for the remained paths
when performing test result analyses, imagining by figure 2.
Figure 2, Unit testing with inputs generation
1.2. Test Strategies
Traditionally, test strategies method divided into two classes, static and dynamic analyses,
based on the source code and the runtime execution, respectively. Both have been maturely
developed in modern times. Static analyses examine program source code or binary image
without invoking the programs. It is recommended to apply static analyses to program
- 4 -
branches. In order to generate feasible inputs according to program semantics, we base on
program logic to transform programs into special models for of possible test case resolution.
1.4. Objective
Our objective is to extract logic models from execution instances and create test cases
that cover as many as possible control branches by checking logic models, as figure 3 shows.
We focus on C language, which is one of the most popular imperative languages and present
the logic models with rule-based language. Clearly, there is some gap between imperative
language and rule-based language, and it is impossible to transform imperative programs into
equivalent rule-based models. However, we discover that it is feasible to make a one-way
transformation from run-time instances of imperative programs into rule-based ones.
Figure 3, Unit test with model extraction
We perform the test process by reasoning on the program behaviors, similar to manually
trace-back of program logic, interleaving with symbolic evaluation and concrete evaluations.
Two goals will be fulfilled: logic model extraction and run-time predicates resolution.
1.4.1. Logic constraints extraction
We transform a program running instance view to a list of rules which have the ability to
represent the dynamic states upon codes. We are going to build an executable logic model that
can be evaluated exactly like the original program running. There is still one difference: an
executable model doesn't surround by real program environment. To fix this, the incomplete
predicates of logic models would be resolved by symbolic evaluation of runtime traces.
- 6 -
2. Related Work
The implementation of program logic resolution spans many topics. Recently researches
focus on causes-transaction [2]. This method inspects into a program execution state, e.g.
breakpoints, tries to find some clues between each state changing, and extracts a cause effect
chain from the blamed site to the faulty site. We are not the first to consider logic
representations. Former research like PREFix[3], infers for logic conflicts by treating program
expressions as logic constrains and program functions as logic sub-models. We will also
introduce other implementations like model extraction[4-7], dynamic instrumentation for
tagged memory operations[8], and conditional toggling test method with full path-coverage
guarantee[9].
2.1. Data Reconstruction for Cause Effect Chains
Let us denote the memory structure occupied by a program as MStruct(t), where t stands
for time. We can conclude that: Zeler’s method [2] automatically creates memory graph
MGraph1 and MGraph2 from MStruct(t1) and MStruct(t2) respectively under the debugger.
Demskey[10] try to reconstruct data structure representation from MStruct(t) with
user-specified constrains and debugging information. Purify[8] searches the whole MStruct(t)
for potential dereference. In more general, we propose a logic representation from source code
and resolve logic predicates with MStruct(t). Table1 shows advantages and disadvantages of
our work and others.
Table 2, Comparison of former data recreation methods
Zeller[2] Brian[10] Purify[8] ALERT
Advantages
Automatic Memory
Graph Extraction
and Differentiation
Semi-Automatic
Data Structure
Construction
Memory Tag
Annotation
Automatic Logic
Model Extraction
and Resolution
- 8 -
Analysis
Type
Static and
Dynamic Analysis Static Analysis Static Analysis
Static Analysis and
Dynamic
Resolution
2.3. Techniques of Fault Localizers
There are many fault localization techniques used in debugging tools. Since 1960s these
techniques have changed and improved [11]. Our research involves two major conceptual
techniques: feasible path generation and logic symbol evaluation. Path generation[12] is an
automatic generation of program paths that lead to user-specified outputs by toggling each
control branch instances, and uses ESC[13] to check feasibility of toggled paths. Symbolic
evaluation [9] is an automatic dependency extraction for unit testing that guarantees full paths
coverage. We use path generation and symbolic evaluation to find associates of inputs and
conditional control variables, and generate reasonable input to cover all program paths.
2.4. Symbolic Evaluation and Concrete Evaluation
Another tool named DART [9] combines symbolic evaluation with concrete values. It
provides an automatic unit testing data and test interface generation for a specified C function,
and it can ensure to expand as more program control branches as the variety of testing inputs
by evaluation the symbolic expressions of programs and modification of testing data. DART
guarantees all the bugs reported have corresponding testing inputs to reveal. It is a very
attractive contribution for automatic testing. It also deploys lpsolve [14] to resolve linear
symbolic constraints.
CUTE [15], a successive implementation of DART, improves structural testing data
generation and pointer arithmetic. CUTE creates logic map for each pointer argument, which
mapping to its concrete value of some memory location. During execution, logic map updates
all pointer variables with symbolic representations.
- 10 -
During function execution, the control guards make control thread to branch two paths, one
ensures the sum of variable a, b equals 5 and another not. ALERT now can make assurance
that if the control thread takes the guard of (a + b == 5) and (a == 3) the value of variable at
that moment must be 2. But there is still one thing to took into consideration, the dependency
instance (a --) modify the value of variable a, that causes the logic engine misunderstand all
following conditional constraints. To modify constraints according to dependencies, we
develop an evaluating method over constraint symbols.
Figure 5, ALERT constraints example
As figure 6 shows, ALERT traces every conditional and dependency expressions and
output as constraint symbols. These constraint symbols are used to forward the modification
of dependencies. During runtime interaction, the execution instance outputs a unique set of
constraints according to the conditional and dependency expression it reaches. We analyze
these constraints to trace back to the initial state of the execution parameters of programs.
- 12 -
Figure 7, ALERT traces simulation
The following figures 8, 9, 10, 11 illustrate how ALERT records program execution and
handles program outputs. In brief, ALERT has three work stations: instrumenter, symbolic
evaluator, and logic resolver. Each station produce input for next the station. Logic resolver
generates input cases for next running iteration. The first input case is randomly generated or
all zero. Until ALERT can’tresolve inputs to satisfy the constraints after modification
anymore, the testing process is terminated.
- 14 -
Figure 10, Symbolic evaluate
Figure 11, Logic evaluate
The ALERT analysis structure is different from DART and CUTE. We perform a
transformation from symbol domain to logic domain which enables us to solve bounded-value
constraints. The idea of automatic generation of testing cases and interface is alike DART,
which performs symbolic evaluation and concrete evaluation on every program expression,
dynamic replacement of variables with symbols, maintenances of branch constraints for full
- 16 -
Figure 12, ALERT pseudo-code
Figure 12 is the algorithm of resolving inputs from symbols and constraints.
Conceptually, ALERT does the same things as DART and CUTE. That is to automate testing
process according to program execution, but we do trace the logic in the programs, not the
actual values in the memory.
//F is the function to be tested.
Alert(F) :
//S is the set of Symbols.
//I is the input set.
//V is the set of Variables
S <= find-symbols(F).
I <= random();
V <= empty.
do
//R is the rules generated while execution.
// Logic-execution throw Exceptions.
R <= Logic-execution(F,I).
For-each Rule in R do
R <= R - Rule.
If Rule = Assignment-Rule
Symbolic-evaluation(R,S).
Trigger-Rules(R,Rule).
If Rule is Constraints
If Contradiction(Rule, R-Rule)
throw Exceptions.
P <= find-association(Rule).
Q <= Associate-Rules(Rule, P).
R <= R∪{Q}.
Logic-evaluation(S, V).
I <= map-inputs(V).
- 18 -
stil A. If after modification B, another operation modifies A to ‘A + 1’; then the 
representations of both B and A become ‘A + 1’. Initialy, we have symbols which represent 
some variables, constraints that restrict the symbols to some fixed domain, and transaction
that refer to a sequence of operations.
Figure 13, Flow chart of symbolic evaluation
We transfer an execution states into rule-based description. By substitutions between
variables and symbols, we found that we can illustrate the whole running path that is
composed by every single step of program executions by the accumulation of those steps. For
every execution step can be represented by symbolic compositions, we can evaluate the
symbols to reduce to the minimal constraints. Figure 13 shows how to replace symbols with
representations.
Briefly speaking, the basic concept of ALERT symbolic evaluation is mapping of
symbols. Initially, ALERT enforces every variable to map to itself. Every variable is unique
before the execution. After any modification expression, ALERT traces the original symbol of
the modified variable and combines old symbol with new obtained one. ALERT maintains
symbol maps and composite symbol maps used to one-by-one mapping and many-by-one
mapping, respectively. Once a symbol is modified to another symbol expression, ALERT
- 20 -
Figure 14, Flow chart of logic resolution
Previous section points out that the transformation from imperative language programs
to rule-based programs is possible if the volatile state is revertible. For this reason, we deploy
symbolic evaluation before logic evaluation. Symbolic evaluation performs dependency
forwarding on symbolic modification, which enables us to trace back the volatile state of
imperative language programs.
The main purpose of logic evaluation is to find a feasible input data according to given
symbolic constraints. Like figure 14, the evaluation process begins with empty logic variables
as input data, initially with bounded range constraints to specify valid domain. For each
symbolic constraint, we refine both right-hand-side and left-hand-side to minimal unit,
eliminate duplicated constraints, and annotate possible boundary to every logic variables. A
series of associated constraints is linked with variables. The associated constraints of that
variable are triggered, when the boundary of some variable narrows down. It causes other
variables to get more precise boundaries. However, if the unification results are in conflict
with the constraints of some variables, the inference process restores the changed bindings
and traces back to the nearest alternatives to choose another branch.
- 22 -
Both ways influence the same thing in source code. In the option one, the operation of
instrumentation includes not only the controlling log but also the evaluation of the condition
statement. It is caled ‘cut point’. But in the option two, the operation is reduced to only 
controlling log of the branch choices. It is a tradeoff between time and space. For performance
consideration, we insert many log operations in each branch choice instead of one log and
evaluation operation in the cut point.
4.1.2. Condition Sensitive
We break composite condition into independent ones. Because different combination of
these conditions leads to different program states even they are in the same basic blocks. With
the support of different program states, we can resolve the execution logic and generate
different path.
4.1.3. Function driven
The smallest unit of program is a function. Based on this, we can analyze each path in
the specified function. Each invocation of a function has its unique logic. Parameters of
function have different logic meaning when they are passed. ALERT deals with function
invocation in the simplest way. With short tags labeling the parameters and local variables we
can distinguish variables under different scopes. And by aliasing all passed parameter and
return value, we can preserve the consistency of current program state while function
invocation.
4.2. Symbolic Evaluator
We discard the concrete values of program executions. We use the composted symbols to
explain how the program transfers information by variables. The following are design feature
of the symbolic evaluation.
4.2.1. Paired Symbols
ALERT requires two aspects of view of symbols to perform symbolic evaluation:
- 24 -
Define a replacement of an appearance A over a propagation X, denoted as Ra(A,
X), is that for every symbol C in A, if C equals X as propagation symbols, we
substitute C with As[X].
Ex:
a=3, b=4, c=foo(b, c);
foo(int x, int y){
return sqrt(x*x + y*y);
}//in function foo, Ra (As [foo.x,foo.y],[b,c]), As [foo.x]=As [a] and As [foo.y]=As [b]
Define a replacement of a propagation P over a appearance X, denoted as Rp(P,
X), is that for every symbol C in P, if C equals X as appearance symbols , we substitute
C with Ps[X].
Ex:
pi = 3.14159, r =10, area=r*r*pi;
//after Rp (Ps [area], r),Ps [area]=10*10*Ps [pi]
Define a symbol replacement of an appearance symbol x over symbolic domain S,
denotes as Ra (S, x), said S’ = Ra (S, x) and S’ = { {a | a in Ps and x in Ps } , { Ra ( p, Ps
[x] | p in Ps } }.
Define a symbol replacement of a propagation symbol x over symbolic domain S,
denotes as Rp (S, x), said S’ = Rp (S, x) and S’ = { {a | a in As and x in As } , { Rp ( p, As
[x] | p in Ps } }.
- 26 -
engine evaluates its freezing expressions with the binding value. If the result is true, the whole
process proceeds and triggers other variable binding, otherwise the engine would trace back to
previous decision site and make alternative decision. The logic resolving routine terminates
successfully if all variables are bound and their freezing expressions are satisfied.
4.3.3. Domain Range
Domains are used to set the boundary of variables. For variables to have storage type and
specified storage bits, we can set up a signed integer X with 2 bytes length like “X 
indomain(-32768.32767)”. For more advanced application, we can enumerate customized
types such as “Color indomain(red, blue, green)”, and it can be extended to display higher 
order of abstraction of symbolic evaluation.
4.4. Implementation Difficulties
The main problems we encountered during ALERT implementation are to generate
testing interface and eliminate program state-transaction into stateless logic rules. The former
problem appears in the instrument stage. There are usually various user-defined types
associated with testing function. Because we have to generate different testing drivers for
different user-defined type automatically, we maintain a stack for registration of structural
data-types, a vector for pointers and a list for array type.
The later problem comes from symbolic evaluating stage. It is the key idea for logic
evaluating. Because logic engine doesn’t care the order of rules, but program logic does, we 
make some modification on symbol variables to fix the transaction. We ensure that through
symbolic evaluating ALERT can preserve the states and reversibility of program logic.
4.4.1. Generation of User-defined Type Interface
To generate testing interface for automatic execution, we have to create fixed functions
that dedicates to input parameters of user specified types. In the beginning, we try to examine
the length of specified type and flip the bits to initialize. It can reflect the resolved values, but
without field information. The field information tells about the number of sub-elements and
- 28 -
5. Experimental Results
We evaluate our work with zlib [17], a well-known compressing library. We apply
ALERT on zlib and perform coverage evaluation by conjunction of gcc and gcov [1]. Gcov is
a coverage analysis tool that provides coverage statistics include how often each line of code
executes, what lines of code are actually executed, and how much computing time each
section of code uses.
5.1. Instrumentation and Generation Overhead
The space overhead of ALERT instrumentation is one line-of-code for a significant
branch. We insert one line of log trace code into the beginning of a branch block. This results
in that our space overhead is in proportion to the conditional statements in programs. The
execution overhead of instrumentation is an additional record operation for each conditional
evaluating, so one running instance may have extra operations in proportion to the condition
statements along the paths.
The testing cases generation overhead is one logic variable for an effective function
parameter, but if parameters contain sub-field, logic engine may require more than one logic
variable to resolve symbols. We generate one logic script for the next running instance, so the
total testing script generation of ALERT application is as many as the found possible paths.
5.2. Expand and Explore
ALERT is worthy to automate the testing process because the ability of alternative path
discovery. The discovery algorithm records constraints of previous running instance in
database and produces constraints that differ from those in database by negating some of the
constraints. In the running instance, the program itself may reach new branch that was not
visited before in previous instance. The trace log may contain new constraint entry for
negation. ALERT expand the program branches and explore the newly expended node at the
same time.
5.3. Similar Path Discovery
- 30 -
6. Conclusion
Testing is a time consuming process and hard to automate. Its main purpose is to make
sure every functionality works. The simplest suggestion is to “show it wrong or right”. If we 
do the testing manually, we could lose in the labyrinthine program execution states. To
automate the testing processes, it is essentially important to direct the testing program. We
divide ALERT into three stages: instrumenter, symbolic evaluator, and logic resolver. Each
stage will generate inputs for next stage. A control top module exchanges messages between
every stage. It is a better architecture for testing duty.
Program states consist of thousands of variables and their combinations are
unpredictable. We know it is impossible to enumerate program states through the values of
variables. So we try an alternative way to enumerate program logic. It is much feasible that a
program behavioral representation can be considered as the combination of its logic
constraints. There are many related work that take advantage of program logic. ALERT is the
first application for dynamic analysis.
We discover that program logic is reversible under some circumstances. The reversible
states enable ALERT to solve constraints and produce feasible inputs that lead program to
perform a different execution path. By recording constraints of each running instance, we can
enforce the program to perform all of its possible paths.
6.1. Self Evaluation
In this project, we have implemented a random testing system, called ALERT. It can
automatically generate input data to analyze the program behavior and investigate the
potential software vulnerabilities. This tool has been applied for a vulnerability finding system
and the results reveal several new integer overflow vulnerabilities. It will be published in ICS
2006.
6.2. Future Work
ALERT is just a beginning. For the different testing strategy, such as integration testing,
- 32 -
[1] "gcov: a Test Coverage Program http://gcc.gnu.org/onlinedocs/gcc-3.0/gcc_8.html,"
[2] H. Cleve and A. Zeller, "Locating causes of program failures," in ICSE '05: Proceedings
of the 27th International Conference on Software Engineering, 2005, pp. 342-351.
[3] W. R. Bush, J. D. Pincus and D. J. Sielaff, "A static analyzer for finding dynamic
programming errors," SPE, vol. 30, pp. 775-802, 2000.
[4] J. Whaley, M. C. Martin and M. S. Lam, "Automatic extraction of object-oriented
component interfaces," in ISSTA '02: Proceedings of the 2002 ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2002, pp. 218-228.
[5] D. Jackson, "Aspect: detecting bugs with abstract dependences," ACM Trans. Softw. Eng.
Methodol., vol. 4, pp. 109-145, 1995.
[6] D. Engler, D. Y. Chen, S. Hallem, A. Chou and B. Chelf, "Bugs as deviant behavior: A
general approach to inferring errors in systems code," in SOSP '01: Proceedings of the
Eighteenth ACM Symposium on Operating Systems Principles, 2001, pp. 57-72.
[7] S. Hallem, B. Chelf, Y. Xie and D. Engler, "A system and language for building
system-specific, static analyses," in PLDI '02: Proceedings of the ACM SIGPLAN 2002
Conference on Programming Language Design and Implementation, 2002, pp. 69-82.
[8] H. R. and J. B., "Purify: Fast Detection of. Memory Leaks and Access Errors." Winter
USENIX. Conference, 1992.
[9] P. Godefroid, N. Klarlund and K. Sen, "DART: Directed automated random testing," in
PLDI '05: Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language
Design and Implementation, 2005, pp. 213-223.
[10] B. Demsky and M. Rinard, "Data structure repair using goal-directed reasoning," in ICSE
'05: Proceedings of the 27th International Conference on Software Engineering, 2005, pp.
176-185.
[11] H. Agrawal and E. H. Spafford, "Bibliography on debugging and backtracking,"
