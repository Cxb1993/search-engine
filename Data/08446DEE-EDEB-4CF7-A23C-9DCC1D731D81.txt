2中文摘要
車牌辨識系統的研究迄今已超過十年，而且全世界不論學界、業界都
在做，然而到現在為止，在實際應用上仍有許多限制，否則就是辨識效果
太差，現實環境下的辨識率和實驗室環境下的辨識率有極大的差異，顯示
目前所提出的各種方法其外部效度極低。吾人有幸參加幾個縣警局的「贓
車自動辨識系統」採購案的評審工作，發現國內外號稱辨識率達九成以上
的車牌辨識系統，在天候良好光線充足的白天進行實際路測，其車牌正確
辨識率皆不到七成，甚至低到三成、五成，更遺憾的是這些結果都是固定
攝影機、而且幾乎是固定背景的條件下所做的實測，若屬於條件完全不設
限的情況下，其結果想必更差。反觀人類對車牌的定位、車號的辨認完全
不是問題，快而準。因而啟發吾人思索一個有效並接近人類辨識車牌的方
法與模式尚未誕生，是以提出本計畫擬就車牌辨識這個問題研發出具實用
價值且有效的解決方案。
本計畫的研究範疇在於對影像背景不設限〈任意方位、任意位置、任
意大小的車牌，天候變化、背景變化、複雜背景、多張車牌等〉，其可接
受的環境就像人類可接受的環境，亦即要能夠在複雜的背景中，不論是靜
態或動態影像，均能快速正確找到車牌，並能克服各種車牌不容易辨識干
擾，真正能有實用的價值。至於吾人考慮的車牌主要在於自小客車的車
牌，研發目標訂在實際路測達九成以上的車牌正確辨識率，並具有真正的
應用價值〈例如贓車查緝、違規取締、停車收費自動化等〉。車牌辨識系
統由三部分所構成：影像擷取、車牌定位與車號辨認，三者息息相關。影
像擷取與攝影機及影像擷取卡等硬體設備有關，車牌定位與車號辨認則是
本研究的重心。對於車牌定位我們提出了靜態影像、動態影像及整合定位
法，整合定位法其車牌日、夜間成功偵測比率平均達 94.57%，而針對深色
車的車牌偵測更高達 100%，深具實用價值。對於車號辨認我們提出了不
切割字元的辨識法與相關的一些技術例如拉普拉斯二值化法將彩色影像
轉成 bitmap、筆劃表示法、A*演算法等。我們在各種實際環境中驗證所提
的方法不受光線亮度、光線角度、環境陰影等環境變化的影響。實驗結果
顯示所提出的方法可達到 98%車牌辨識率。系統整合後的實際路測平均達
92.6%的車牌正確辨識率，具實用價值。本研究結果將於 96 年暑假使用在
靜宜大學主顧樓（資訊學院）地下停車場之門禁管制以控制汽車之進出。
關鍵詞：車牌辨識系統、影像擷取、車牌定位、車號辨認、人工智慧、車
牌正確辨識率、A*演算法、拉普拉斯二值化法
Abstract
Vehicle license plate (V.L.P.) recognition system is an interesting but
complicated research topic. The problem has gained more and more attention not
4PART 1: 車牌定位
第一章 緒論
1.1 研究動機
由於國內車輛快速成長，所產生的交通問題也愈來愈多，例如收費停
車、停車場管理、高速公路收費、超速、肇事逃逸、闖紅燈和汽車竊盜等
情形，傳統以人力從事交通運輸的控管和警方查緝車輛的方式會顯得非常
沒有效率，而改善這種現象的方法就是利用車牌號碼的唯一特性，並以先
進的科技設備和聰明的演算方法對車子車牌進行定位和辨識來追蹤管制。
智慧型的車牌辨識系統可以改善目前的做法以提升執行效率。在違規
車輛的取締上〈例如：超速、闖紅燈等〉，其最終目的在於舉發，執法者
舉發的先決條件在於舉證照片與車牌號碼，目前的舉證工作中之車牌號碼
判讀皆由人工來做，相當辛苦而費時，由於罰單的延遲寄達導致嚇阻效果
有限。若能有效應用智慧型車牌辨識系統，則可望從取像、辨識、開單一
氣呵成，不但降低成本提高效率，同時由於罰單的快速寄達導致對違規者
產生嚇阻效果。此外，在一些交通運輸管理方面，車牌辨識系統亦可扮演
重要的角色，例如無人看管的收費停車場、高速公路收費站、收費橋樑等，
若再配合銀行自動扣款的動作，則車輛在行經上述地點時不必停車繳費以
增加行車效率。雖然有許多利用微波或電子的方式達到自動收費或其他交
通管理與控制的目的，但是這些方式 都不若車牌自然，何況還須在車輛
上加裝一些額外的電子設備增加經費負擔，又有隱私權的顧慮，是以推廣
不易終究會被淘汰，只有車牌永遠不會退流行，只要車牌存在的一天，智
慧型車牌辨識系統就應及早達到實用階段以有效率解決前述的交通管理
與控制的問題。
近年來資訊與通訊等技術快速發展，使得傳統運輸與交通控管觀念得
以藉由這些新科技而有所突破，現今硬體設備技術已達一定水準，能夠以
高焦距、高解析度攝影車子影像，並以數位訊號以有線或無線方式輸出，
6在現今車輛使用頻繁的情形下，產生了相當多的交通問題，例如停
車、超速、肇事逃逸、闖紅燈以及汽車竊盜等情形，而要解決上述問題的
最快速有效方法是利用車牌號碼的唯一特性加以辨識並追蹤管理。車牌辨
識在許多交通控管應用中扮演著重要的角色，例如幫助警察在巡邏時，能
夠利用警車上的數位相機或攝影機拍攝，即時辨識車牌號碼，可以迅速的
與資料庫比對，確認該車是否有被登錄報失或是贓車，也可在巡邏時，直
接拍攝違規者的車號，辨識車牌，填寫罰單寄到違規者家中。這樣的車牌
辨識系統提供了更迅速的處理，節省人力、時間。
但目前所使用的車牌辨識系統都是應用在控制良好的環境之下，且
大多是針對靜態目標和單一的車輛進行辨識，例如收費停車場、路邊違規
車輛和贓車查緝，而又受限於各種環境天候因素和車牌方向、變形和污損
程度，辨識率不是很高。如果要克服這些困難，除了要使用解析度更高的
硬體設備外，所用的影像處理技術也要非常複雜，並花費大量計算時間和
記憶空間，反而因此沒有實用價值。
所以我們提出的方法是要能夠在複雜的背景中，不論是靜態或動態
影像，能克服各種干擾，快速正確找到車牌，真正能有實用的價值。(A)
在靜態影像中，我們提出了快速車牌尋找方法，減化了影像處理計算；依
車牌區域的明亮程度動態調整門檻值(threshold)，並利用模糊方法和加權方
法對車牌位置做最後的確認，以提高定位正確率。(B)在動態影像中，我們
對於利用 Video Camera 擷取影像，並如何針對連續兩個畫面(frame)所提
供的顏色訊息(information)做車牌定位的判斷，提出了模糊的概念和方法，
再依車牌的一些特性確認車牌位置。
最後我們認為車牌定位系統還有很多可以研究改良的空間，尤其是
動態影像比靜態影像包含更多的訊息，應該有更好的方法，而且較少有人
研究探討，而我們在本研究增加了動態影像的車牌定位功能和原理實驗，
使系統更符實際的應用情形，這對車牌辨認而言有重要的貢獻。
1.4 相關研究和修正對策
8灰階矩量保持法；車輛牌照顏色分底色與號碼色兩色，依車輛類別不同有
多種組合，但其共同點是兩色有明顯灰階差，二元化後此特性應被保存下
來，以此特性計算在一定距離內黑白灰階變化量，找尋車牌位置。馬西聰
[4]利用數位影像中各圖素間灰階度差值的變化特性來找尋車牌的位置。吳
孟聰[1]利用數位影像處理中的技巧找出影像中的邊緣；因為車牌字碼的灰
階度和車牌背景的灰階度有明顯的差異，所以字碼邊緣很容易被偵測出
來。再經過橫向掃瞄之後，留下有足夠灰階度變化的列，再經由垂直及水
平的投影直方圖，找出投影量較大的區間為我們可能的車牌區域。賴幼仙
[9]係對任意角度取像的車牌進行車號辨認的研究，嘗試解除車號辨認須由
正面取像的限制，文中除了提出利用車牌四個角點共面的特性將車牌由任
意角度取像所得之影像回復為正面取像所得影像的理論以利辨認外，並提
出車牌尋邊、角點計算、影像回復座標轉換計算、比對的方法。
義大利的 E.D.Di Claudio 等人[15]以空間頻域分析的原理來找到車牌
的位置，方法為利用快速且強韌的 1-D 離散傅利葉轉換(DFT, Discrete
Fourier Transform)來找出車牌和車牌字元的位置。韓國的 B.T. Chun, Y.S.
Soh 等人[12]使用一階水平差分空間頻域分析法，先將輸入影像經由線取
樣(Line sampling)，再使用 Sobel 運算子來強化影像，以其值的一階水平差
分是否大於某一數值來當作確認是否有較為激烈灰階變化，再做群聚
(Merging)，相互兩組突起之間如果長度小於可接受的車牌寬，則可群聚在
一起。經過以上的步驟，就可以選出車牌的候選區。再由候選區中，進行
上下兩方面的尋找，如果相當於車牌字元高的區域內其複雜度夠強，則認
定是實際的車牌區域。英國的 J. Barroso 等人[18]使用一階水平差分空間頻
域分析及配合垂直投影來決定車牌的位置。德國的 C. Busch, R. Domer 等
人[13]將輸入的影像經過水平 Sobel 運算子來加強邊緣，而邊緣點形成
線，確認這些線的長度是否介於一定的長度之間，由已知的車牌大小來找
鄰近區域的線來確認車牌的上下界；以同樣的方法可以找到車牌的左右
界。再經由找尋車牌中相連續的白色區域以做為車牌字元的上下左右界。
荷蘭的 H. A. Hegt 等人[16]先利用樣本比對來尋找車牌的四個明顯的
10
達成的目標，我們的方法就是對上述的缺點改正，並導入人類搜尋物件的
模式方法。
而我們特別針對上述缺點提出方法使得本研究成果真正能達到實用
價值，其分析如下：
對於複雜背景找不到車牌：我們利用影像色調(H)、飽合度(S)、強度
(I)的特性，並配合邊緣偵測(Edge Detection)，再加上車牌顏色做模糊處理，
使車牌不受複雜背景干擾。
對於外在環境光線照度無法掌控：我們利用影像的 Histogram 統計資
料，建立起可調式 (adaptive) 門檻值 (threshold) 提高本系統強韌性
(robustness)。
對於花費大量計算時間和記憶空間：我們統計車牌位置出現的機
率，先針對最有可能區域做簡單判斷，這可大量減少不必要的時間浪費。
大多系統只能對靜態照片進行辨識：我們特別針對動態影像兩個畫面
(frame)之間的差異進行色彩模糊運算，得出車牌位置的可能區域，進而快
速找到車牌位置。
1.5 報告組織架構
第一章對研究的動機、研究方向與目的、主要貢獻以及相關研究做
說明和探討，第二章對車牌定位系統架構及流程做完整敘述，第三章是針
對靜態影像車牌定位方法做詳細描述，我們先敘述如何利用人類直覺的方
法加上簡單的影像處理，初步探索車牌的可能位置，接著再說明如何利用
模糊整合的方法精確的定位出車牌位置。第四章是針對動態影像車牌定位
方法做詳細描述，先敘述如何利用顏色模糊的方法找出車牌的候選區域，
接著再描述如何初步探索車牌的可能位置和精確的定位出車牌位置。所有
的實驗結果則在第五章實驗結果與分析中探討，我們將整個實驗的結果依
天候、車子的顏色表列出來，並檢討各種數據優劣的可能原因。最後，第
六章為結論。
12
中拍攝和靜止中拍攝的兩種情形。拍攝的車子顏色亦分為淺色和深色。
拍攝的車輛區分為淺色車子和深色車子，車輛顏色深淺的詳細分類
如表 2-2 所示。
表 2-2 深淺色車輛分類一覽表
種類 車子顏色
淺色 白色、銀色、棕色、灰色
深色 紅色、黑色、綠色、藍色、紫色、黃色
2.2 發展環境
我們的系統分為兩大部分：硬體設備和車牌定位軟體。硬體除了個
人電腦之外還有其他週邊設備，如數位相機、數位攝影機、USB 2.0 介面，
主要是負責影像的取得；而軟體發展的語言為 Boland C++ Builder 6.0，並
利用 VCL(Visual Component Library)和 Windows API 輔助開發所使用的人
機圖形介面，主要是處理影像的擷取和進行車牌的定位，而車牌定位主系
統又區分為靜態車牌定位系統和動態車牌定位系統。
表 2-3 系統軟硬一覽表
硬體 軟體
個人電腦使用的電腦等級為 intel
PIII-733
記體體 128MB
數位攝影機為 Sony DCR-PC115
數位相機為 FUJIFILM F401
作業系統為 Windows 2000 Professional
靜態影像擷取系統
動態影像擷取系統
靜態車牌定位系統
動態車牌定位系統
發展程式的語言為 Boland C++ Builder 6.0
我們的車牌定位系統主要是要在車牌定位的程式上，所以在開始定
位前要先取得包含車牌的靜態和動態影像。取得車牌影像的方式是利用數
位相機和數位攝影機依實驗設計，在白天、黃昏和晚上等不同的時段，和
室內、室外等不同地點，對淺色和深色的車輛進行拍攝。數位相機所取得
的影像由儲存裝置(compact flash)以圖檔(BMP)的方式下載至個人電腦
中，再由程式中開啟檔案的方式就可執行車牌定位。
攝影機所取得的影像亦由儲存裝置(tape)以動態影像檔(AVI)的方式
14
圖 2-2 系統架構
2.3.1 靜態影像子系統
靜態影像子系統主要分為探索定位和精確定位二大部分，如圖 2-3
所示。
是
車牌定位主系統
靜態影像子系統 動態影像子系統
靜態影像?
輸入影像
輸出車牌
否
16
模糊整合先將連續兩個 frame 中和車牌顏色類似的區域定位出來並
記錄到陣列之中。
探索定位先將影像分割區塊，接下來的工作包括判斷包含車牌的候
選區塊和找出下一個最有可能包含車牌的區塊，其目的是要快速、正確的
找出車牌的大概位置。精確定位包括調整定位和依車牌的大小、長寬比率
判斷得出車牌正確位置。
圖 2-4 動態影像子系統
數位攝影機攝
擷取第一個 Frame
擷取下一個 Frame
將二個 Frame 做模糊整合
探索定位
定位輸出車牌
輸入影像
失敗
成功
精確定位
依大小、長寬比
率判斷車牌?
18
2%，故將區塊的高定為原影像高的八分之一，寬定為原影像寬的六分之
一，使得如此大小的區塊，符合車牌邊緣像素在影像中的比率。
接著再假設如果區塊中的邊緣像素大於影像像素 0.5%，則可以分辨
出此區塊是否可能包含車牌，原因是車牌最少要有 1/4 在區塊中才值得作
細部檢查，而 2%的車牌邊緣像素的 1/4 為 0.5%，以上假設根據實驗的結
果，使用如此分割大小的區塊，的確有最好的效果。
圖 3-1 靜態影像探索定位檢查順序示意圖
定義好區塊大小後，首先做初步檢查，先檢查影像中間的區塊，然
後依順時針或逆時針螺旋式往外檢查，因為愈靠近影像中間，車牌出現的
可能性愈高，檢查的方法是利用[-1,0,0,1]遮罩取區塊中每個像素 RGB 的 G
值運算，如果每個像素的結果大於門檻值(1)，則將結果 1 存入陣列 E，否
則存入 0，如式 3-1。另外經過這樣的運算後，也已經做完區塊的邊緣偵測
(edge detection)了。
1 if G(i)1,
E(i) =
0 otherwise, (3-1)
20
圖 3-4 車牌區域有陰影，門檻值較其他區域小
所以我們依每個區塊的 G 值標準差之不同，動態的調整每個區塊的
門檻值為兩倍的標準差，如式 3-2。
1 = 2 * SD
SD= N
N
Gij
Gij 
2
2 )(
(3-2)
接著計算結果陣列 E 中 1 的元素數目，如果達到一定臨界值(2)，代
表有強烈的灰階變化的像素數目很密集，則此區塊就疑似包含有車牌，需
進一步細部檢查，確認是否包含車牌，如果沒有密集的灰階變化，則跳至
下一個區塊做初步檢查。
2 值的大小為所有像素數目的 0.5%，原因是車牌最少要有 1/4 在區
塊中才值得作細部檢查，而 2%的車牌邊緣像素的 1/4 為 0.5%，如式 3-3。
2 = n * 0.5% (3-3)
n 為的影像像素個素。如果時間超出預定時間，則停止檢查，以達
到即時(real time)的目的，預定時間可以實際工作性質做調整，例如用在停
車場做車輛識別，因無時間壓力，故時間可以設定較久，而如應用在警方
偵察則相反，這是因為對車輛攝影作為辨識，通常會有多張影像，車輛會
22
3.2 調整定位
因為車牌再怎麼扭曲變形和傾斜，上下邊緣還是很明顯，如圖 3-6、
3-7，我們就利用這個特性來調整定位出車牌位置的上下邊界。
圖 3-6 傾斜車牌影像
圖 3-7 傾斜車牌邊緣偵測後結果
如圖 3-8，細部檢查是將疑似有車牌的區塊的上界往上提至上面區塊
的一半，再逐列往下檢查灰階變化數，因為最差的狀況是此區塊只包含到
1/4 的車牌，同樣方法將下界往下移至下面區塊的一半，再逐列檢查，檢
查的方法就直接利用陣列 E 的內容，但超出區塊的部分因尚未做遮罩處
理，所以要先做邊緣偵測後再加入陣列 E，當該列元素為 1 的數目超過某
個數量則停止，最後將上、下界之間的全部影像取出作處理，這是因為車
牌為長條形，上下邊緣非常明顯，故將疑似車牌的區塊上下擴張逐漸縮小
檢查車牌上下邊界，就很容易就能正確定位，但如果找不到上下界，則排
除此區塊包含有車牌的可能性，因為雖然此區塊的灰階變化數目很多，但
24
(A)原圖
(B)經色彩模糊的結果
圖 3-9 靜態影像色彩模糊
最後我們將陣列 A 與陣列 E 結合，結合的方法是依陣列的重要性來
給予不同的權重，如果陣列中元素 1 的數目愈多，則表示受外在干擾較大，
其重要性較低，因此我們定義陣列 A 的重要性權重為式 3-5，陣列 E 的重
要性權重為式 3-6。
w1 = nE / (nA + nE) (3-5)
w2 = na / (nA + nE) (3-6)
其中 nA為陣列 A 中元素為 1 的個數，nE 為陣列 E 中元素為 1 的個數，
而且有式 3-7 的關係存在。
w1 + w2 = 1 (3-7)
26
第四章 動態影像車牌定位方法
我們由數位攝影機所擷取的車牌影像，其中是由每隔 1/30 秒的畫面
(frame)所組成，我們先要利用程式將此影像檔案(AVI 檔)將每個畫面取出，
接著每個畫面和靜態影像一樣不僅包含車牌還有車體本身及週遭背景環
境，所以我們利用連續的兩個畫面中的影像差異不大的特性得知車牌位置
並不會移動太多，再模糊比較連續的兩個畫面將和車牌顏色相同的區域留
下，如此已排除大部份非車牌區域，再利用處理靜態影像的方法，先由影
像中央開始，大略掃描車牌的可能位置，再仔細分辨是否為車牌。
同樣先進行探索定位，但因為之前模糊比較連續的兩個畫面將和車
牌顏色相同的區域留下的步驟後，結果已經二值化，所以我們不用計算像
素的飽和度，而且因為顏色的干擾已經去除，所以也不用動態的(adaptive)
調整門檻值(threshold)。
如果確定可能包含車牌的區塊，接著進行調整定位和精確定位，同
樣的由於先前步驟處理的結果也已經是二值化，所以只要判斷調整定位後
的區域大小和比率是否符合車牌的特徵就可精確定位出車牌的位置了。
4.1 前後畫面模糊整合
在車牌辨識系統中使用攝影機拍攝動態影像的方式主要可分為兩
種：固定式和移動式。無論是那一種方式，如果我們 1/30 秒取一個畫面
(frame)，連續取兩個 frame 出來分析，其中的影像變化應該不會太大，車
牌位置也不會有太大的改變，然後我們去比較前後兩個畫面中同一個位置
像素的 RGB 值中之 G 值，如果都很接近白色(255)，則留下來否則去除，
因為車牌為白底黑字，所以車牌的區域一定會留下，而影像中的非白色區
域即被清除。接著再以靜態影像的方法來處理，由於影像中大部分區域已
被清除，速度會很快，這是利用動態的特性來增加定位成功的機率。
首先我們取出所拍攝的動態影像中相隔 1/30 秒的兩個畫面(frame)，
將第一個畫面中每一個像素的 RGB 值中的 G 值存入陣列 F，再將第二個
畫面中的每一個像素的 RGB 值中的 G 值存入陣列 L。
28
4.2 探索定位
使用和靜態影像相同的方法將陣列 R 的分割成大小相等的區塊，每
個區塊的高為原陣列高的八分之一，寬為原陣列寬的六分之一，首先做初
步檢查，先檢查陣列中間的區塊，然後依順時針或逆時針螺旋式往外檢
查，檢查的方法是計算陣列 R 中值為 1 的個數，如果個數大於門檻值(T1)，
則判定為疑似包含車牌的區塊，需進一步進行精確定位，如圖 4-2。
圖 4-2 動態影像探索定位檢查順序示意圖
4.3 精確定位
精確細部檢查是將疑似有車牌的區塊的上界往上提至上面區塊的一
半，再逐列往下檢查灰階變化數，因為最差的狀況是此區塊只包含到 1/4
的車牌，同樣方法將下界往下移至下面區塊的一半，再逐列檢查，檢查的
方法就直接利用陣列 R 的內容，最後將上、下界之間的全部影像取出作處
理，這是因為車牌為長條形，上下邊緣非常明顯，故將疑似車牌的區塊上
下擴張逐漸縮小檢查車牌上下邊界，很容易就能正確定位，但如果找不到
上下界，則排除此區塊包含有車牌的可能性。
30
二. 將原始的彩色影像，用 Sobel 邊緣偵測取得邊緣值。(圖 4-5)
圖 4-5. Sobel 影像
三. 將 Sobel 邊緣偵測取得的邊緣圖，進行物體區塊分割。(圖 4-6、圖 4-7)
物體切割方式為，若邊緣值圖出現連續 3 個 Pixels 的邊緣值大於門檻
值時，取原始彩色影像相同位置為中心(P22)與周圍的點進行比對（表
4-1），若符合下列(1)、(2)、(3)、(4)式，則將 Sobel 邊緣值影像完成後
之 S[22]進行切割。經過這個處理方式可以取的較佳之區塊。
if (P22 - P21) + (P22 - P23) > (P22 - P12) + (P22 - P32) then
DestImage[P22] = DestImage[P21]………..(1)
if (P22–P12) + (P22–P32) > (P22–P21) + (P22–P23) then
DestImage[P22] = DestImage[P12]………..(2)
if (S22 > 0) and (S32 = 0) and (S23 = 0) then
if (S42 = 0) and (S13 = 0) then
if ((P21 > P22) and (P34 > P33)) or ((P21 < P22) and (P34 < P33)) then
DestImage[P22] := 0
else
if (S31 = 0) and (S24 = 0) then
32
圖 4-7. 物體邊緣切割處理後
四. 使用 Mingwu Ren 等提出的 Tracing boundarycontours in a binary image
方法，進行物體外部輪廓與內部輪廓追蹤，這個方法的好處是追蹤快
速，每一個 Image 由左至右、由上到下走，每一個 Pixel 最多只走一次，
走完即可計算出所有物體的輪廓。(圖 4-8、圖 4-9、圖 4-10)
圖 4-8. 外部輪廓追蹤
34
不容易判別。
策略二：針對淺色車，但車牌影像因拍攝距離造成文字區塊不明顯(圖
4-11)，而使輪廓非常密集，無法判斷文字區塊者(圖 4-12)，則
由左至右、由上到下判斷內部輪廓集中者，其集中程度達到車
牌的長寬比，則列為候選區。(圖 4-13)
策略三：針對淺色車且文字區塊輪廓較清晰者，則由左至右、由上到下
判斷所有內、外部輪廓的區快取出，依據區塊大小相似者且呈
水平排列者標示為候選區，目前判斷方式是以一般自用車由 6
個英文字與數字組成為主。(圖 4-14、4-15、4-16)
圖 4-11. 車牌太小時
36
圖 4-14. 車牌文字區塊夠大時
圖 4-15. 車牌文字夠大時，產生的輪廓追蹤圖
38
接著我們再使用下列三個策略進行車牌區塊確認。
策略一： 當使用車牌邊緣取得候選區時，我們會計算該候選區之車牌邊
緣水平於垂直線分佈是否達到一定程度，再比對該區域之內部
輪廓與外部輪廓分佈是否與車牌文字分佈特徵相似。若是，則
標示出正確的車牌大小。（圖 4-19）
策略二： 因為車牌文字的輪廓太過密集，因此只比對候選區之內部輪廓
分佈的長寬比是否符合車牌的的長寬比(圖 4-12、圖 4-20)。
策略三： 依據候選區的文字區塊內部輪廓與外部輪廓分布是否與車牌文
字排列分佈相似，決定是否為車牌。（圖 4-21）
此外，我們的方法也能夠同時追蹤一個以上的車牌。（圖 4-22）
圖 4-19. 使用策略一完成車牌定位
40
圖 4-22. 能夠同時追蹤一個以上的車牌
42
圖 5-3 白天拍攝的靜態淺色車輛
圖 5-4 晚上拍攝的靜態淺色車輛
圖 5-5 室內拍攝的靜態深色輛
動態影像測試的影像則分為白天、傍晚拍攝，拍攝地點只有在室外，
而且分別以淺色車子和深色車子來實驗，其原因是我們沒有使用輔助光
源，所以攝影機無法在晚上和室內使用，如圖 5-6 所示。
44
表 5-3 成果統計在晚上時以車身顏色分類
淺色車身 深色車身 總數
車牌數 25 25 50
車牌定位成功數 25 25 50
車牌定位成功比率(%) 100 100 100
平均花費時間(ms) 844 812 828
表 5-4 成果統計以時間分類（為上列各表格之總和）
白天 傍晚 晚上 總數
車牌數 50 50 50 150
車牌定位成功數 47 48 50 145
車牌定位成功比率(%) 94 96 100 97
平均花費時間(ms) 1412 1340 828 1193
表 5-5 則是靜態影像在室內拍攝的實驗結果，定位成功的比率在 95%
以上，而花費的時間低於 1.6 秒。
表 5-5 成果統計在室內時以車身顏色分類
淺色車身 深色車身 總數
車牌數 25 25 50
車牌定位成功數 24 24 48
車牌定位成功比率(%) 96 96 96
平均花費時間(ms) 1550 1490 1520
由靜態影像實驗數據的結果，我們發現定位成功的比率都很理想，
可見我們動態調整門檻值有發揮出一定的作用，另外所花費的時間也都在
可接受的範圍之內，具有實用價值。
在花費時間方面我們觀察到和做探索定位區塊時並疑似包含有車牌
而進一步作調整定位的次數有倍數的關係，因為在我們方法中，探索定位
的次序是由影像中間以螺旋方向往外，如果所攝影的影像車牌都集中在中
間，則做探索定位的次數就很少，所花費的時間也就少。
另外天候愈暗，定位成功的比率會更高，所花費的時間也更短，原
因是天候愈暗背景愈不明顯，車牌在閃光燈的照射下相對比較突出，在探
46
使用閃光燈，除了晚上無法使用之外，在清晨或黃昏時所攝影的影像品質
較差，而造成在使用我們的方法時，前後兩張畫面中車牌區域由於昏暗而
無法比較出來，產生定位失敗的結果。
另外在花費時間方面，大部分時間是花在前兩張畫面的比較，由於
是對所有像素做處理，所以比靜態影像花的時間更多，但之後因為大部分
非車牌已被去除，且結果又只有 0、1 的二元值，所以利用靜態影像的方
法來定位車牌時只要花費極少時間。
5.3 整合定位法
我們實做了 4.4 節所提出的整合定位法，經過使用數位攝影機並架設
於自用汽車上，行進速度介於 0~70 公里之間，針對實際道路行使狀況進
行錄影測試，所取得的測試結果如表 5-9。每一個 Frame 完成定位時間介
於 240~350 毫秒之間，平均為 300 毫秒。車牌日、夜間成功偵測比率平均
達 94.57%，而針對深色車的車牌偵測更高達 100%。
表 5-9 整合定位法的實驗數據
白天 夜晚
深色車 淺色車 深色車 淺色車
正確 錯誤 正確 錯誤 正確 錯誤 正確 錯誤
車牌數 65 0 47 3 28 0 17 6
正確率(依日夜/
車體顏色)
100% 94.00% 100% 73.91%
正確率(依日夜) 97.39% 90.00%
白天淺色車車牌正確率只有 94%，主要原因是物體切割部分受到車牌
上的螺絲干擾，導致判斷物體大小及分佈時無法很準確。夜間車牌偵測平
均只有 73.91%的正確率，原因有二：
1. 硬體因素：因為使用的攝影機在夜間無輔助燈光的條件下，鏡頭對焦變
的遲鈍，導致影像極為模糊不清。
2. 外部光源因素：一般車子懸掛車牌處接會有輔助燈光，然而也因輔助燈
光集中投射在某區域，反而影響 Sobel 邊緣值的效果，也連帶影響到後
48
第六章 結論
一般偵察車牌的方式可分為三種：使用相機拍攝車輛、使用固定式
攝影機拍攝車輛、使用移動式攝影機拍攝車輛。在我們的研究中，則將上
述的情形區分為靜態影像和動態影像。而研究的車子類型中，只針對台灣
佔大部分的自用車車牌，即白底黑字的六個數字或字母的車牌來作實驗。
在靜態影像中，如要快速找出車牌位置就不能使用太複雜的影像處
技術並且不能掃描所有影像中的像素（pixel），所以我們利用經驗法則和
簡單的影像處理計算灰階變化（Low Computation）來搜尋，先試著大略掃
描車牌最有可能出現的區域，如果發現疑似包含有車牌（candidate），則再
做細步掃描加以確認定位，如果沒有可疑車牌，則可排除此區域，再找其
他車牌可能出現的區域，這和人類找尋車牌的模式類似，先找出車牌的大
略位置，再仔細分辨是否為車牌。
另外要正確找出車牌位置而不受陰影或其他因素干擾，在細步掃描
時先將可疑車牌區域將 R.G.B.模式轉換 Y.I.Q..模式，利用模糊的方法，排
除色彩部分，留下非色彩部分，這樣就不會受陰影干擾，而將車牌區域也
排除，接著在非色彩部分計算出所有像素明度（intensity）值的變異數值
（variance），如果變異數值較小，則在計算灰階變化時所使用的檢查門檻
（threshold）也要較小，相反的，如果變異數值較大，則在計算灰階變化
時所使用的檢查門檻值（threshold）也要較大，這是能依影像當時的環境
條件來動態調整（adaptive）門檻值，定位正確的比率才能提高。
在動態影像中，我們可以比較動態影像中前後兩個 Frame 影像，將
白色的像素留下，所以會留下車牌的部分，接著再同樣利用上述的方法處
理。在動態影像中，我們的方法無法處理白色車子，而且在夜間由於照明
設備的問題而無法使用，所以改進動態影像無法在晚上使用和在天候不良
時定位成功率差的缺點這些是未來可以思考改進的方向。整合定位法其車
牌日、夜間成功偵測比率平均達 94.57%，而針對深色車的車牌偵測更高達
100%，深具實用價值。
50
Proceedings of the IEEE 48th Annual Vehicular Technology Conference,
1998, pp. 1790-1794.
[15] E.D. DiClaudio, G. Lucarelli, G. Orlandi, and R. Parisi，"Car Plate
Recognition by Neural Networks and Image Processing," Proceedings of
the 1998 IEEE International Symposium on Circuits and Systems, Vol. 3,
1998, pp. 195-198.
[16] H. A. Hegt, R.J Haye, and N.A Khan，“A High Performance License Plate
Recognition System,”Proceedings of the 1998 IEEE International
Conference on Systems, Man, and Cybernetics, Vol. 5, 1998, pp.
4357-4362.
[17] Intelligent Transportation Systems(ITS), http://www.iot.gov.tw/its/, 1999.
[18] J. Barroso, J. Bulas-Cruz, E.L. Dagless, A. Rafael，"Number Plate Reading
Using Computer Vision," Proceedings of the 1997 IEEE International
Symposium on Industrial Electronics, Vol.3, 1997, pp. 761-766.
[19] K. Chmnongthai, T. Sirithinaphong，”Extraction of Car License Plate 
Using Motor Vehicle Regulation and Character Patern Recognition,” 
Proceedings of the 1998 IEEE Asia-Pacific Conference on Circuits and
Systems, 1998, pp. 559-562.
[20] L. S. Chen, C. Y. Fang, K. E. Chang, S. W. Chen, and S. Cherng,
“Automatic inspection of IC labels,” Chinese CVGIP 1998, Wu-Lyne,
Taipei, Taiwan, 1998, pp. 136-143.
[21] M.H. TerBrugge, J.H. Stevens, J.A.G. Nijhusis, L. Spaanenburg, “License 
plate recognition using DTCNNs,” 1998 Fifth IEEE International 
Workshop on Cellular Neural Networks and Their Applications
Proceedings, 1998, pp. 212-217.
[22] R.C. Gonzalez, R.E. Woods , Digital Image Processing ，
Addison-Wesley，New York，1992.
52
服這些問題，必然能促使車輛管理邁向自動化，使得車輛的管理成本降
低，對全國的交通秩序及治安也會有更顯著的貢獻。
在車牌辨識領域中，我們一般把車牌辨識問題可粗略劃分成「車牌定
位」及「車牌文字辨識」兩個較大的步驟，我們選擇其中的「車牌文字辨
識」當作我們研究的起始點，我們嘗試發展一些方法來克服其中的問題，
特別是希望能適應移動式的環境，我們希望研究結果除能直接貢獻於「車
牌文字辨識」階段外，也希望藉由「車牌文字辨識」的經驗，能更了解克
服各種干擾的技巧，這些技巧或許能對「車牌定位」有些幫助或提示作用。
1.2 研究方向與目的
在本研究中我們假設車牌的區域已由前段的某種車牌定位系統完成，
在我們得到車牌影像後，對車牌影像辨識其中的車牌號碼。我們將在各種
實際環境中驗證本研究所提的方法，對於光線亮度、光線角度、環境陰影
等我們任由其發生，目的是希望這個方法能在移動式的環境中使用。
目前有關車牌文字辨識的研究大部分都使用了切割技術，把車牌中的
文字切成一個個的字元[5][8][14][19]，切割後之文字逐字辨識。此法之優
點是，每個區塊代表一個字，針對單一區塊進行特徵比對，可簡化問題並
改善執行效能；但缺點是，切割的方法是以比辨識步驟更簡單的原理來預
先處理文字，因此常較無法抵禦干擾因子，致使切割錯誤而造成辨識率下
降。我們知道，人類在看文字時，即使筆劃有些相連或缺了某一邊，在大
部分情形下，人類仍然可以知道正確的文字內容。
而以切割技術而言，其背後的涵義是代表字與字間的筆劃能分得開，
或至少能在某種條件下被視為有分開，並且在辨識的演算法也假設切割所
得的文字，基本上是幾乎完整的。因此，我們有一個概念，若一排文字能
夠不要切割，則可能可以減少切割錯誤對辨識率的影響；另外，若我們能
對文字在某一邊缺損的情況，仍極有可能可以辨識成功，則除了可以抵禦
污損外，也可以降低對「車牌定位」的依賴，也就是說，對於「車牌定位」
系統有定位到，但是過大或過小甚至略偏某一邊而影響到邊界文字時，我
54
文字辨識的基本流程如圖 1-1[1]，圖中所示「前處理」包含位置、大
小的正規化、二值化、文字缺損的修補、雜訊的消除，以及細線化等等。
「文字分離」則在於從正規化的影像中抽取文字區域；「後處理」若有必
要，則可做如文脈處理等，使前一段處理後若有多個候選答案，能有機會
從中選出正確解答。
在圖 1-1 的「特徵抽取」及「辨識」步驟上，已有許多方法被提出，
例如樣板比對法(template matching)[1]、筆劃(stroke)解析法[1]、輪廓特徵
抽取法[1]、輪廓構造解析法[1]、線構造解析法[1]、線圖形化法[1]與幾何
特徵法等[1]。
1.4.1 樣板比對法
樣板比對法[1]預先準備好每個文字的模板(template)，將輸入的未知文
字與文字模板一個一個地重疊，比較兩張圖案在相同位置上畫素之異同。
相似性最高的文字模板即為答案。應用於中文字，衍生出了階層式樣板比
對法，實際運用是使用以兩層為主，第一層將原影像切成幾個 8*8 大小的
4-bits 灰階區塊，對每個小區塊做模糊處理，整張影像經與模板比對後，
可辨識出文字之大分類；然後第二層使用 40*40 大小的二值化區塊，藉以
細分同類字。實際的商業產品如蒙恬科技的產品[2]就使用到樣板比對法。
1.4.2 筆劃解析法
筆劃解析法[1]先定義幾種標準的筆劃，例如上水平線、中水平線、下
水平線、左上方短垂直線、左下方短垂直線、右上方短垂直線、右下方短
影像 前處理 文字分離
特徵抽取 辨識 後處理 文字
圖1-1 文字辨識基本流程
56
[1]可以處理字母、數字、片假名等。此法基本上是由 4 個階段所構成：(1)
大小正規化，(2)細線化，(3)鏈碼(chain code)化，及(4)辨識。為了進行鏈
碼化，首先定義 0~7 的編碼分別代表八個方向的向量，0 代表向下，依順
時針到編碼一直到 7 為止。所謂鏈碼化就是把這些從向量轉成編碼的一連
串向量碼聯繫起來，除去一些微小的變動後，得到平均化及整形後的結果。
此法當文字有斷裂、破損或雜訊干擾時，就需要額外的遷移表了，處
理上也會因此更複雜。
1.4.7 幾何特徵法
幾何特徵法[1]又有兩種，分別為文字幾何學及相位幾何學。構成文字
形狀的要素是端點、交點、直線性(曲線性) 、角、凹凸、孔、迴圈及及它
們的位相連接性等。而相位幾何是由中心點向四周不同方向散射來取特
徵，例如端點、分歧點、屈折點、孤立點和迴圈中心等。
在相位幾何中有所謂場(field)效果法，它首先取文字的凹凸構造，因為
即使文字變形，其凹凸構造是不變的。以白色底為場，取其中一點向 8 個
方向伸出方向線，根據方向線與文字線接觸的情形，求出這一點的封閉率
(被黑線包圍的比例)，然後繼續求其他點的封閉率。若沿文字的輪廓線走，
從封閉率的大小就可判斷該點附近的凹凸狀況。將連續的凹凸連在一起，
就可形成筆劃。幾何特徵法在其他如指紋辨識領域等領域也有人使用[3]。
幾何特徵法在實際應用時，對於未知圖形與標準圖形間之是否相似，
是以其特徵量的變動是否在門檻值內來判斷的。幾何特徵法的缺點是當文
字有中斷時，效果會較差。
1.5 近幾年車牌辨識相關文獻
1.5.1 類神經網路法
類神經網路具有非線性、抗雜訊及具適應性等優點，因此近幾年使用
58
李奇霖[12]採用倒傳遞演算法，不需二值化處理，並首先採用由左到
右的掃描車牌，因此不需切割字元﹐其車牌辨識率為 92.4%。
魏銪志的研究[8]是利用 Connected Components 找出元件，並用 Hough
Transform 及一些經驗法則來過濾元件，取出來的字元再做正規化。最後使
用SimNet進行辨識，對於640*480的影像其辨識率91.77%，對於1280*1024
的影像其辨識率為 88.58%。
1.5.2 樣板比對法
H. A. Hegt 等人的研究[16]使用樣板比對法，並以 Hotelling 轉換來輔
助，由於 Hotelling 是以主成分來進行描述，避免了將影像各像素的重要性
均視為同等的缺陷。Hotelling 轉換可以將字元的重要訊息保留並加以突
顯，使得每個字的特徵有所不同。這篇研究的辨識錯誤率為 0.4%。
1.5.3 統計法
統計法最主要是使用機率密度將圖形分割成幾種樣式(pattern)，例如在
莊志鴻的研究[6]使用 AWMMS(Adaptive Weighted Mask Matching
System)，一邊辨識一邊調整出最理想的樣板，最後直接進行樣板比對，其
辨識率約 93%。
1.5.4 其他
如吳孟聰採動量法[7]，此法的特點是其所得的字元特徵對於旋轉、縮
放、平移等的改變沒有明顯變化，得到特徵量後接著計算幾何距離，距離
最小者即為辨識出的字元，有些字型很相似的字如「1」與「I」或「8」與
「B」，必須再切塊，個別計算其動量，其辨識率則未提及。
1.6 報告組織架構
我們在第一章闡述本研究之動機及目的，也提到了我們期望這個研究
60
第二章 車牌文字辨識系統之架構
2.1 發展環境
本研究的發展環境非常單純，如列表 2-1 所示。在本研究中所開發的
「車牌文字辨識系統」即是利用 Delphi 7 所開發的系統，Delphi 是一套以
Object Pascal 語言為主的視窗程式語言，由於它結構嚴謹，具備良好的型
態檢查及物件導向功能，使得我們在程式實作上相當順利。至於作業系
統，也並沒有任何理由需使用 Advanced Server 等級的 WINDOWS 作業系
統，唯一的理由是因為本研究的機器上恰好使用這個等級的作業系統。
表 2-1 發展環境
硬體環境 軟體環境
AMD Athlon 800 個人電腦一部
記憶體容量 256MB
Windows 2000 Advanced Server 作業系統
Sony Cyber-shot 數位相機 Delphi 7 程式開發工具
2.2 環境限制
原則上，本研究的車牌影像在環境要求方面並無限制，我們蒐集的車
輛影像，地點方面不論在停車場或馬路上、在晴天或陰天、不管面光或背
光，甚至環境陰影交雜等場所均有；而在拍攝時間的安排則晝夜均有。不
過我們仍然在拍攝車輛時有幾種情形不予拍攝：
(1) 文字影像過於模糊，例如對焦模糊、車牌太小等，以至於連人眼都很
難以分辨之車牌。
(2) 拍攝角度上的限制，圖 2-1 說明了此項限制，(a)由上往下拍攝的角度
不超過 70 度，超過這個角度時，車號之筆劃容易產生糾結；(b)由側
往中央拍攝的角度不超過 40 度，超過這個角度時，字體將會嚴重扭
62
裁切後之車牌影像區塊，我們會先經二值化來降低資訊複雜度，然後
進行歪斜車牌較校正及大小正規化，最後我們在辨識階段會先讀入預先設
計好的 35 個字元1的「標準」影像資料。
1 由於數字「0」與英文字「O」在車牌上的字型完全一樣，因此我們視它
們為同一個字，如此除符合實際視覺外，在車牌的應用上也不是問題，因
為我們可以另外加入一些經驗法則，來判斷應該是「0」或「O」。另外，
若與車籍資料庫連線比對，也可排除此一問題。
圖 2-2 系統架構
裁切後之車牌影像區塊
歪斜車牌校正
車牌二值化
車牌大小正規化
車牌文字辨識
35 個標準字影像
輸出車牌號碼
影像表示法轉換 影像表示法轉換
64
第三章 車牌影像處理
3.1 車牌影像二值化
二值化(Binarization)的目的是為了降低資訊量及其圖像的複雜度，使
得原先灰階或彩色色彩變成黑白兩色階。雖然在文字辨識方面，二值化並
非必要的步驟，但二值化帶來的好處是資訊的簡化，有利於簡化辨識演算
法的設計，其直接的影響是運算更快且較容易實施。
再就二值化所帶來的問題，它雖然使簡化了辨識的工作量，但也損失
了一些資訊，不好的二值化方法，會使的影像變得非常糟而無法辨識，試
想一個原本完整的文字因為二值化而支離破碎，那麼對後面的辨識而言就
變成是禍非福了。因此如何設計一個良好的二值化法，使得它能降低資料
量，但又盡量保留足供辨識的資訊，對文字辨識領域而言是相當重要的。
圖 3-1 舉了一個二值化的例子，其中圖 3-1(a)為 256 灰階影像的灰階分
布圖，圖 3-1(b)為找出一個門檻值(threshold)後，利用它把圖 3-1(a)的灰階
值分成兩群，兩群之代表值分別為灰階 0 及 255，我們看到灰階分布圖祇
剩黑白兩色而已。由於大多數車牌辨識研究中的二值化方法都是使用 Otsu
或從 Otsu 延伸出來，因此在們在介紹本研究的二值化方法前，我們先簡單
地介紹 Otsu[18]。
66
如果令 C0 代表灰階 1~k 的群集，C1 代表灰階 k+1~L 的群集，w0 及 w1
個別代表 C0 及 C1 的出現機率，μ0 及μ1 個別代表 C0 及 C1 的平均值，則
)(1
)(
)(
)(
)(1
)(
1 0
1
1 0
0
1
1
1
0
kw
k
w
ip
kw
k
w
ip
kwpw
kwpw
T
L
ki
i
k
i
i
L
ki
i
k
i
i















而各群集的變異數及變異數總合就為
)()()(
)(
)(
2
11
2
00
2
1 1
2
1
2
1
1 0
2
0
2
0
kwkwk
w
p
i
w
p
i
w
L
ki
i
k
i
i










Otsu 認為當我們能找到一個 k 值使得變異數總合為最小，這個 k 就是
最佳的門檻值。
因此一個定義二值化門檻值的問題，變成是求取最佳化的問題。利用
此法所做的二值化，其效果較固定門檻值者佳。
3.1.2 拉普拉斯二值化法
許多研究都是以 Otsu 為基礎，但從他們的研究中發現了仍有不少因二
值化失敗而造成文字支離破碎情形，因此他們對於環境光線、陰影、髒污
等有較多的限制[7][8][9][10]。他們的研究亦直接指出 Otsu 雖然使用者眾，
但仍不足應付影像本身的色差或環境陰影；但是這種限制，對於車牌辨識
而言，太不切實際了，如圖 3-2 或甚至更嚴重的陰影干擾相當常見，我們
無法請使用者這個不行，那個也不能。
68
1/3*(R+G+B)的公式使得 R、G、B 具有相同的比重，我們在計算時可以不
理會常數項 1/3，運算上只需使用整數運算，並且少了三次乘法運算。
「拉普拉斯二值化法」先以拉普拉斯運算子計算每一點的第二動量，
對一個連續的影像，我們令 f 為影像在(x,y)點的亮度函數，則拉普拉斯運
算子[11]之公式如下
2
2
2
2
2
y
f
x
f
f



 (3-1-1)
對於一個數位影像，如表 3-1，對於像素(x,y)其一階偏微如下
)1,(),(),(
),(
),1(),(),(
),(








yxfyxfyxf
y
yxf
yxfyxfyxf
x
yxf
y
x
一個像素(x,y)的二階偏微如下
圖 3-3 將影像分成8等份
表3-1數位影像各相對位置亮度
f(x-1,y-1) f(x,y-1) f(x+1,y-1)
f(x-1,y) f(x,y) f(x+1,y)
f(x-1,y+1) f(x,y+1) f(x+1,y+1)
70
果，表 3-2 假設一個影像區塊為 7*7，原影像區塊右下方為亮度較暗區，
而左上方為相對較亮區，從表格 3-2(f)我們觀察到，在兩個區域的交界處
會產生兩排絕對值較大的拉普拉斯值(其絕對值大於門檻值θ)，其中小於
-θ那一排代表了高亮度區的邊界，大於θ那一排代表了低亮度區的邊界，
也就是說這兩排像素的亮度值分別就是二值化時白與黑的代表。我們定義
Lw及 Lb 個別為白色區及黑色區的平均亮度，這兩個平均亮度就個別代表
了文字與背景的亮度，並令 Li 為影像區塊中第 i 個像素的拉普拉斯值，Yi
為影像區塊中第 i 個像素亮度值，Sw 及 Sb 分別為 Li<-θ及 Li>θ的像素集
合，則 Yw及 Yb 的計算式如下






b
w
Si
i
b
b
Si
i
w
w
YL
YL
S
S
||||
1
,
||||
1
(3-1-4)
然後我們拿一個影像區塊的所有像素一一與 Lw 及 Lb 比較，其亮度接
近 Lw 者設為白點，亮度接近 Lb 者設為黑點，表 3-2(g)中的例子說明了最
後結果。我們以圖 3-4 展示一個二值化後的車牌影像，它的原圖為圖 3-3，
我們在系統中使用的θ值為 20。
3.2 歪斜車牌校正
由於本研究目的是針對移動式的拍攝裝置，因此車牌角度並不固定，
我們需先將車牌矯正以利後段辨識。對於歪斜車牌之校正，我們先使用連
通元件(Connected Components)來偵測各個元件，並使用一些經驗法則來濾
圖 3-4 二值化結果
72
式很簡單，只要把 ConnectedMap(x,y) 設為該元件的 ID，並把(x,y)
加入 ConnectedComponents 中該元件的鏈結串列中即可。
(4) 最後我們從 ConnectedComponents 可直接得到連通元件。
我們以圖 3-7 展示歪斜車牌被校正的過程，其中圖(a)為二值化後之歪
斜車牌影像，圖(b)為我們使用上述連通元件演算法所偵測出的連通元件，
為了便於理解，我們特別塗上不同顏色並以虛線框來區隔不同的連通元
件，圖(c)則在所有連通元件中濾除一些較不可能為文字的元件，我們使用
下列的經驗法則來過濾：
表3-3 八相連檢測
(x -1,y -1) (x ,y -1) (x +1,y -1)
(x -1,y ) (x ,y ) (x +1,y )
(x -1,y +1) (x ,y +1) (x +1,y +1)
元件ID 元件ID … 元件ID
元件ID 元件ID … 元件ID
… … … …
元件ID 元件ID 元件ID
(a)變數 ConnectedMap 的結構
元件ID3
…
元件ID0
元件ID1
元件IDn 像素資訊 像素資訊 …
像素資訊 像素資訊 …
像素資訊 像素資訊 …
像素資訊 像素資訊 …
(b)變數 ConnectedComponents 的結構
圖 3-6 連通元件演算法變數示意
74
經過上述條件過濾掉較不可能為文字的元件後，我們從合於條件者
中，找出最左邊與最右邊的元件，然後計算出這兩個元件的重心，再利用
這兩個重心計算其連接起來的直線公式，並進而計算此一直線與水平線所
形成的水平夾角α，圖 3-7(d)中的灰色三角形說明了這個概念。
得到水平夾角α後，我們使用圖形學上所最頻繁使用的齊次轉換矩陣
來校正歪斜之影像。矯正歪斜影像時，我們需兩種轉換，一個是旋轉
(rotation)，然後是平移(translation)。
旋轉運算的齊次座標型式如下
  
標點為像素在新座標系之座與
標點為像素在原座標系之座與此處
yx
yx
yxyx












100
0cossin
0sincos
11 

而平移運算的齊次座標型式如下
  












1
010
001
11
yx
yxyx
因此整個校正的矩陣型式運算為
  





















1
010
001
100
0cossin
0sincos
11
yx
yxyx 

(3-2-1)
76
由於我們有興趣的部分為圖 3-9(a)中標示虛線的區塊，因此我們只想
擷取該區塊並做正規化。在擷取的過程，本研究所開發的系統會自動分析
車牌影像投影在垂直軸上的輪廓(contour)分布，如圖 3-10。我們實驗發現
對於圖(a)，當我們由中央往上下兩端走持續計算出投影出來的輪廓分布
時，同一條水平線上黑點的比率小於 16.5 或大於 80.5 時，可以正確地判
斷為文字區塊的上下界。
投影於垂直軸的輪廓分布
0 10 20 30 40 50 60 70 80 90
0
11
22
33
44
55
66
77
88
99
110
121
132
143
154
Y
沿X累計之黑點百分比
由於車牌文字排列是水平走向，因此所有文字共同投影到垂直軸上
圖3-10 計算文字區上下界
圖 3-9 車牌正規化範例
78
第四章 車牌辨識
4.1 影像空間域之轉換
像素(pixel)是表示影像時最小的基本單位，影像處理領域上大多數的
技術都是在這樣的空間域上發展出來的，例如先前提到的二值化、連通元
件標示、影像旋轉平移等都是。但是以像素方式表達之影像就「辨識」而
言卻太過低階[17]，不夠接近人類在看一個字的方式，人類在識別一個物
體時，首先是基於腦中所記錄的相似物體，而非依據像素方式，因此一個
像素一個像素的比對效果並不能達到人的要求。此外像素的記錄方式存在
過多的冗贅資訊，因此我們先將影像掃描一遍，將像素之表達方式轉換成
另一種資料量更少且更具意義的表達法。
我們將轉換後的基本單位稱為筆劃(strokes)，觀念與前面文獻探討中所
提的筆劃解析法有點類似，但實際細節有所不同。
我們採取從上而下、由左至右的順序掃描車牌影像，即 plane sweep 的
方式，每掃完一條垂直軸的所有像素，我們便記錄在這條軸的黑點形成多
少線段，並記錄各個線段的位置及長度。在整個掃完後，我們針對記錄下
的黑色線段，從左到右，只要通過以下規則的過濾即可合併成同一個筆
劃，無法通過過濾條件者視為新筆劃：
1. 若目前垂直軸上的線段數與左邊相鄰垂直軸上的線段數不相等，
則視左邊為筆劃結束，目前的垂直軸為新筆劃開始。
2. 若目前垂直軸上的至少有一線段不與左邊相鄰垂直軸上的任何線
斷相連，亦視左邊為筆劃結束，目前的垂直軸為新筆劃開始。
3. 若目前垂直軸上的至少有一線段與其左邊相連之線段長度差超過
7 個像素，亦視左邊為筆劃結束，目前的垂直軸為新筆劃開始。
不同的內容及品質的影像產生的筆劃區塊數就有所不同，我們使用圖
4-1 及圖 4-2 來說明上述步驟執行的情形。圖 4-1 車牌文字影像經過上述規
則轉換成筆劃，將產生 S1~S23 共 23「塊」筆劃，如圖 4-2 之所示。我們
80
段的黑色線段，每個線段左右寬度為 1 個像素，集結這些左右相鄰且同樣
是 2 段的黑色線段，就形成了 S1 筆劃區塊；同理，S2 的最左邊像素由於
被垂直線掃過時產生 3 段的黑色線段，因而與 S1 的最右邊線段數有所不
同，因而結束了 S1 的右邊界，開始了新的筆劃 S2。
另一個例子同樣在圖 4-2 中，S4 與 S5 之黑色線段數皆為 1，且 S5 的
最左邊與 S4 的最右邊並未有不相連的線段，因此通過了上述規則 1 與 2
的檢查，但由於他們的線段長度差異超過 7 個像素，因此仍然被拆成兩個
不同的筆劃區塊。
但是經過上述的轉換後所得的筆劃區塊，有可能因原車牌影像雜訊較
多而得到比預期的筆劃更零碎，我們曾嘗試給一些經驗法則來合併過於支
離破碎的筆劃，但效果非常不理想，因為雜訊太難預估了。最後，我們令
這些筆劃可與相鄰的筆劃自由任意合併成新筆劃，而何者是最好的組合，
我們交給辨識端來評估。
經過上述之轉換程序後，一個以像素為主的車牌影像變成以一串由左
至右順序的 S1、S2 、S3，…，Sn 筆劃。在後面的辨識過程，我們對筆劃
進行辨識，因此對於一個受測車牌影像，資料量由 5,600(140*40)變成
c*n+3，此處 n 為文字字數，c 為平均每個字的筆劃區塊數，加 3 是因為車
牌中間有個「-」字號，另外假設兩旁可能有邊框黑線。我國車牌文字有 6
個字，而每個字的筆劃區塊數不太一樣，例如「1」有兩塊、「2」三塊、「3」
三塊、「4」四塊、「A」三塊、「B」三塊、「C」兩塊，整體而言，最多者
為 4 塊，平均則為 3 塊，當雜點多時，可成長到 8 塊之多。即使 c=8，也
可把需要運算的資料量由 5,600 降低為 c*n+3=51。
4.2 準備標準字
為了讓系統知道每個字的標準筆劃，我們也依照上一節所定規則，挑
選「適當」的車牌文字當作該文字的標準筆劃，所謂適當的車牌文字，並
不是指像剛印刷的新書一樣完美，基本上它只需要筆劃清楚端正即可，我
們甚至希望在筆劃的內部出現些微雜訊，因為適度雜訊的出現，反而使標
82
V W
X Y
Z
有了這份標準字的筆劃表格，每個受測車牌上的文字就有了比較對照
的基礎，受測文字基本上是在這樣的表個上尋找最相似的字。
4.3 受測車牌文字與標準字間之差異度評估
受測的車牌由於干擾多，有相當大的比例都沒有辦法如上一節的 35
個標準字一樣有簡潔的筆劃，例如表 4-1 中的標準字「3」有 3 個筆劃區塊，
但是圖 4-3 中的「3」卻因出現雜訊而有 8 個筆劃區塊，也就是說受測車牌
文字的筆劃區塊數有時會比標準字的還多。
我們希望對於過於瑣碎的筆劃區塊，有機會能把相鄰的筆劃併成與
標準字近似的區塊，至於該不該併，及由哪幾個併，我們使用成本(cost)
的觀念來做標準，若合併後的成本比合併前的成本更低，就採取合併的方
式；反之則不合併。用來評估成本的函數，就稱為評估函數(estimation
function)。
評估函數設計得宜，就能正確估算哪種組合才是最好的組合，以圖 4-4
中的文字「3」而言，它由於雜訊而產生 8 個筆劃區塊，我們另參照表 4-1
所列標準字筆劃表知道，「3」的標準字有 3 個筆劃區塊，在良好的評估函
圖 4-4 過於零碎之筆劃區塊
84
begin
if (S 或 S'的寬度 >θ2) then
Cost := Cost + 18*UnoverlappedSet 的元素個數
else Cost := Cost + 9*UnoverlappedSet 的元素個數
end
else
begin
for 在 S 中的每個次筆劃 T do
begin
T' := T 對應到 S'的相同 Y 軸位置的次筆劃
if (T 與 T'的 top 或 bottom 差值 >θ3) then
Cost := Cost + 9
else if (T 與 T'的長度差>θ4) then
Cost := Cost + 3
Cost := Cost + T 與 T'以 3*3 遮罩後的差異數
Cost := Cost + T 與 T'的 4 角差異數
end
end
end
Result := Cost
由於在評估函數中我們將每個次筆劃區塊(見 4.3.1 節之說明)劃分成
3*3 個區域，因此每個次筆劃就有 9 個區域，我們定義每個區域錯誤的成
本為 1，全錯則成本為 9。在 UnoverlappedSet 的元素由於與標準字的次筆
劃不相疊，因此視為 9 個區域全錯，以 9*UnoverlappedSet 的元素個數來
表示其成本，但有時候受測的筆劃之大小與標準字筆劃大小差異太大(差異
超過θ2)，我們實驗發現給它更大單位成本(18)效果會更好，因此當受測的
筆劃之大小與標準字筆劃大小差超過θ2 時，成本改以 18*UnoverlappedSet
的元素個數來估算。
對於相疊的次筆劃，若受測次筆劃與標準字次筆劃間之 top 或 bottom
86
中每個區塊黑色區的比例。當 T 中一個格子的黑色區域比例與 T'中對應之
格子的黑色區比例相減，其差值超過某一門檻值，及視為 T 與 T'在該區塊
有所不同。我們累加這些超過門檻的格子數成為 T 之遮罩比對成本，當 T
與 T'不同的格子愈多，其 Cost 就愈高。
4.3.3 次筆劃的四角成本
對於相疊的次筆劃，我們針對它的四個角落計算它的黑色區域比例，
如圖 4-8。當 T 中某個斜角的黑色區域比例與 T'中對應斜角之黑色區比例
相減，其差值超過某一門檻值，及視為 T 與 T'在該斜角有明顯不同。「8」
對「B」與「O」對「D」之間之所以大部分能分辨，特別依賴四角成本之
估算。
圖 4-7 次筆劃劃分成 3*3 個小格子
圖 4-8 次筆劃之四個角落
88
begin
if 在 OPEN 中存在一個 OLD 節點與 SUCCESSOR 相同 then
begin
捨棄 SUCCESSOR
if 目前路徑比原先 OLD 路徑成本更低 then
begin
把 OLD 的 parent link 重設成 BESTNODE
更新 g(OLD)及 f(OLD)
end
把 OLD 加到 BESTNODE 的子節點串列中
end
else if 在 CLOSED 中存在一個 OLD 節點與 SUCCESSOR 相同 then
begin
捨棄 SUCCESSOR
if 目前路徑比原先 OLD 路徑成本更低 then
begin
把 OLD 的 parent link 重設成 BESTNODE
傳遞更新 g(OLD)及 f(OLD)與 OLD 的子節點的 g 及 f
end
把 OLD 加到 BESTNODE 的子節點串列中
end
else
begin
把 SUCCESSOR 放到 OPEN 中
把 SUCCESSOR 的 parent link 設成 BEST
把 SUCCESSOR 加到 BESTNODE 的子節點串列中
f(SUCCESORR) := g(BESTNODE) +
從 BESTNODE 到 SUCCESSOR 的成本
end
end
end
90
在本研究中，對於 h 的估算就是由上一節的評估函數估算出來，圖 4-9
展示 A*演算法在我們系統中比對一個受測字的過程。我們令每一個節點
是由 Sdi 及 Sdj'共同組成，Sdi 及 Sdj'分別代表受測及標準字的一個筆劃，其
中 d 為節點在樹狀結構中的深度，換句話說，Sd'就是標準字的由左至右第
d 個筆劃區塊，而 Sd 就是測試字由左至右第第 d 個筆劃區塊。Sdi 代表受測
字在第 d 個筆劃區塊的所有排列組合中的第 i 個情形，Sdj'代表第 j 個標準
字的第 d 個筆劃區塊，此處 j=1, 2, 3,…, 35，代表文字「1」~「9」及「A」
~「Z」共 35 個標準字的序號。
在圖 4-9 中的第一層，A*會列舉所有配對一一送入評估函數來計算其
cost，其計算量為 n*35(n 為受測字的合併後的第一個筆劃區塊的所有排列
組合數)；在第二層以下，我們不需再計算所有的可能情形。為了方便討論，
假設在第二層以下皆很幸運走對了，則其計算量為 n-1，第三層為 n-2，…，
所以第二層以下之總計算量為(n-1)!。
由於實際上有可能會在較頂層時可能會有走錯的情形，假設走錯 e
次，而每次走錯的計算量即使高估也只有(n-1)!，因此使用 A*來求解最糟
可在(n*35)+(e+1)((n-1)!)，若 n=4，e=10(實際上走錯的次數常遠低於此)，
則計算量銳減為 206。
因此從演算法上的追蹤模擬我們看到了效能上的優勢，在下一章實驗
階果，我們開發的系統也在實務面支持了這樣的結論。
92
式顯示辨識過程所掃描出來的筆劃。此外為了便於追蹤問題，系統在上半
部顯示與辨識結果有關的筆劃資訊及各字元所花的成本，藉由這個訊息
窗，我們可以很快速的檢討辨識錯誤的原因，以協助修正系統。
圖 5-3 在交談模式辨識一個車牌
圖 5-2 在交談模式選擇車牌影像
94
(5) 以是否髒污來分，有髒污者 14 張，無明顯髒污者 186 張。髒污
之車牌影像如圖 5-9。
(6) 以是否有外物干擾(例如車號被電線遮到、螺釘釘在車號上)來
分，有外物干擾者 5 張，無外物干擾者 195 張。有外物干擾者如
圖 5-10。
(7) 以拍攝俯角來分，俯角大者 9 張，俯角小者(平地上手持正常拍
攝)者 191 帳。俯角較大之車牌影像如圖 5-11。
圖 5-5 夜間拍攝之車牌
圖 5-6 歪斜之車牌
圖 5-7 有裝飾框之車牌
96
且安裝的作業系統為 Windows 2000 Advanced Server，測試這 200 個車牌，
平均每個車牌二值化的時間為 1.66ms。
圖 5-12 各種情況下的二值化結果
98
分出兩者，不過其間的 cost 差異相較於其它字元就沒那麼明顯。圖 5-13「B」
其實有兩個解答，亦即「8」與「B」，因為在本例中，這兩個標準字與圖
中的「B」之間有同樣大小的 cost，由本系統並未與車籍資料庫連線，因
此本系統從「8」與「B」間隨意挑出一個字做為答案。
對圖 5-13(c)，系統將「I」誤判成「1」。當我們進一步分析，發現原圖
是歪斜的，雖然這樣的歪斜度遠在我們所限制的車牌歪斜度以內，大部分
的車牌均可正確被辨識出來，但若遇到歪斜的「I」與「1」，車牌歪斜所照
成的傷害就較大了，這是因為「I」與「1」的上下蓋子，向左右凸出只有
大約兩個像素，當車牌歪斜時，其間的差異就更小。本例也如同圖 5-13(b)
之「B」與「8」的情況一樣，圖 5-13(c)中的「I」也有兩個答案，即「I」
與「1」，因為它們的 cost 一樣大，因此本系統從「I」與「1」間隨意挑出
一個字做為答案。
最後一個是圖 5-13(d)，圖中的「N」被當作雜質而丟棄，在分析後，
我們發現原圖影像中的「N」與左邊界是有分開的，但是「拉普拉斯二值
化法」二值化錯誤，以致於它跟邊界黏在一起而被評估函數誤認為是車牌
邊緣的雜質。這個例子也是 200 個車牌影像中，唯一在二值化階段表現不
理想的案例。有許多直覺上在光線、陰影、車牌品質等條件比本例更差的
車牌影像，「拉普拉斯二值化法」都處理得不錯，但卻失敗在這樣的案例
上。
進一步分析圖 5-13(d)的例子，我們發現在原圖影像中的「N」與左邊
界間較亮的區域，其亮度值大約在 130 上下，而其所屬影像區塊之 Yw及
Yb 為分別 196 及 106，因為 130 與 106 之差距較小，所以被二值化成黑色。
為了釐清問題，我們刻意將整張影像的拉普拉斯值替代成視覺化的顏色，
拉普拉斯值大於 20 的塗成黑色，小於-20 者塗成白色，其餘塗成灰色，如
圖 5-14 所示，「N」與左邊界間並未出現大於 20(黑色)之拉普拉斯值，但
出現許多小於-20(白色)之拉普拉斯值，因此該區域是為白色區。換句話
說，拉普拉斯二值相當可靠，但對於其後所算出的 Yw及 Yb 卻無法充分代
表整個影像區塊的白與黑，顯然在求算 Yw及 Yb 的部分仍需進一步改進。
100
第六章 結論與未來方向
本研究中提出了一個適合文字影像處理的拉普拉斯二值化法，它有著
不錯的效果與效能。我們也將 bitmap 影像轉換成筆劃的表示法以降低資料
量，這樣的轉換對辨識而言，也比起 bitmap 更接近人類識別影像的方式。
而本研究的重點「不需切割的車牌文字辨識系統」，採取由左至右的掃描
方式，成功地辨識了 98%車牌，而字元辨識率為 99.66%。
我們也運用了在人工智慧(A.I.)領域常用的 A*演算法，大幅縮小原來
解空間的搜尋時間，A*演算法除了能搜尋最短的路徑來解決問題外，對於
解題規則的改良最主要是依賴於評估函數。評估函數的修改相對成本較為
低廉，因此不管是軟體上的改良或是實施於韌體或硬體成本都不高，至於
A*演算法本身是一個相當簡潔的演算法，它一旦實作完成後，沒有變動的
需要，因此 A*演算法的成本更為便宜。
雖然本研究成果未能達到 100%的理想，但累積了成功案例與失敗案
例之經驗，有助於未來更深入研究。在上一章圖 5-13(a)、(b)與(c)中的失
敗情形，在特定的應用上，我們可與資料庫即時連線，而使得相似字的「猜」
錯誤比例更低；但就影像辨識的技術層面而言，我們希望能在電腦視覺辨
識上進一步提高辨識率，而不需要太過依賴資料庫的連接，這也是為何本
研究並未與資料庫連接的主要考量。
就圖 5-13(d)而言，我們希望二值化正確性是 100%，而非 99.5%，因
此「拉普拉斯二值化法」仍有一些改善的空間，深入觀察光線對車牌的影
響，可能可以對車牌的明暗、對比等特性作分類及歸納，找出更合適的 Yw
及 Yb，提高二值化的正確性。
在評估函數部分，未來若持續改善評估函數中的規則，使它能善用更
合適的文字特徵，則可再提高文字間的區別力；另外一方面我們在系統中
所用的參數全為人工調校，我們有興趣使用統計模式或機器學習(Machine
Learning)理論來調整這些參數值，使得參數達到最佳化。
102
參考文獻
[1] 林仲芬，影像辨認技術，二版，全華科技，台北，民國八十四年。
[2] 蒙恬科技，「核心技術」，
http://www.penpower.com.tw/profile-4/world3/w-6.htm，民國九十二年。
[3] 高斯電腦，「指紋機介紹」，
http://www.gausscom.com.tw/Newweb/latelyNews.htm，民國九十三年。
[4] 李添貴，「車牌辨識系統」，機械工業雜誌，196-205 頁，民國七十九
年五月。
[5] 張簡子，「以小腦模型在 FPGA 上作車牌辨識」，國立台灣師範大學工
業教育所，碩士論文，民國七十九年。
[6] 莊志鴻，「以 AWMMS 法做車牌字元辨識」，國立交通大學電機與控
制工程學系，碩士論文，民國八十八年。
[7] 吳孟聰，「車輛牌照自動辨識系統」，私立淡江大學資訊工程學系，碩
士論文，民國八十七年。
[8] 魏銪志，「動態多標的車牌辨識系統之研究」，私立元智大學資訊研究
所，碩士論文，民國八十九年。
[9] 李仁軍，「中英文件混何影像的全自動分類系統」，國防大學國防管理
學院，碩士論文，民國九十二年。
[10] 張宜良，「自動即時車牌辨識系統之研究」，私立中華大學資訊工程學
系，碩士論文，民國九十年。
[11] 蕭進松，「數位影像處理」，全華科技，台北，民國八十八年。
[12] 李奇霖，「車輛牌照自動辨識系統-使用類神經網路」，私立淡江大學
資訊工程學系，碩士論文，民國八十八年。
[13] Donald Hearn, M. Pauline Baker, Computer Graphics, Prentice-Hall
International, London, 1986.
[14] Eun Ryung Lee, et al.,“Automatic Recognition of a Car License Plate
Using Color Image Processing,”Proc. of IEEE International Conference
on Image Processing, vol. 2, pp. 301-305, 1994.
[15] George F Luger, Artificial Intelligence, fourth edition, Addison Wesley,
104
PART3 系統整合實作與測試
1. Introduction
Vehicle license plate (V.L.P.) recognition system is an interesting but
complicated research topic. The problem has gained more and more attention not
only in the academic but also in the industrial research for the past ten years.
Although many experiment results show that their rates of correct recognition are
high (over 90%), the results for the real world test (or road test) of those systems are
not significant (less than 70%). In other words, the previous methods in the past ten
years cannot fit the requirement of real applications. The major reason lies in that
the environments of real world are complicated however the previous approaches
only work on many constraints or in restricted environments. On the other hand,
the human being can recognize the location of V.L.P. and the number in the license
efficiently and effectively. It means that an approach or model to simulate the
spirits of human being for recognition of V.L.P. is not developed yet.
Basically, a V.L.P. recognition system consists of three components, namely,
image capture, localization of vehicle plates, recognition of license number. The
most interesting yet challenge part lies in localization of vehicle plates. The previous
methods are divided into two major categories, namely, edge-based and color (gray
level)-based methods. The first approach is based on observation that characters
embossed on license plates contrast with their background in gray or binary level [1,
2, 11, 12]. An image is converted to a binary image. Then, a connected component
analysis is used to search the connected objects [1, 2]. Heuristic rules are employed
to filter those objects that cannot be license plates. The second approach utilizes
the fact that license plates in images often show unique and homogeneous color
(gray level) [3, 4, 9, 10]. The method scans the gray-level image row by row [3, 4].
The difference of gray level between two rows is computed and compared with a
threshold. If the difference is greater than the threshold, a candidate block is found.
Then, the candidate block is converted to binary image. Based upon the ratio of
length and width of a license plate, the parts of candidate block that do not belong to
plate are deleted. Finally, a license plate is localized. Both approaches are time
consumed because of two-pass processing. Most importantly, the environment of
above approaches is restricted. In other words, the results of plate localization are
very bad when the image has shadow or the image is dark. Due to their
time-consuming, both approaches cannot be applied to real applications.
In this research, based upon algorithm engineering, we develop and implement
several effective yet efficient algorithms to localize license plates and recognize
characters in the plate under unrestricted environment. The previous methods stress
106
much computation time.
1-D array E is maintained to keep the evaluation value as follows.
1 if G(i)1,E(i) =
0 otherwise,
where1is an adaptive value according to the variance of G values in the block. Our
statistics shows that 32% of pixels whose G values differ in two standard deviations
(SD). If the standard deviation in the block is larger, the picture is brighter.
Therefore, the threshold 1 should be larger to filter the ambiguous edges. On the
contrast, if the standard deviation in the block is smaller, the picture is darker or
more shadowed. As a result, the threshold1 should be smaller. Actually, in our case,
the threshold1 is equal to 2*SD.
Then, we count the number of “1”in array E. If the number of “1”in E is
greater then the threshold2, the block is a candidate block since it is dense for edge
pixels.2 is equal to n *0.5%, where n is the number of elements in E.
Those candidate blocks from the first step are detected further to check if the
candidate really contains a license plate. The checked area of a candidate block is
larger than the area of the block. The half areas of the upper block and lower block of
the candidate are included to check. The checked area is examined row by row to
find the upper and lower edges of a license plate by using the contents in array E. If
there is no such edge in the checked area, the candidate block is given up because it
is impossible to exist a rectangle in the candidate block. Otherwise, the checked area
is called the rectangle area. In Taiwan area, we only consider the license plates with
white background and black characters. Therefore, the pixels in the rectangle area
should be white or black if the area contains plates. A fuzzy method is used to
determine whether a pixel is white or black [7, 8]. According to the YIQ mode, we
have the RGB value of white is close to 0, the RGB value of black is close to 255,
and the RGB values for those grey-level pixels are almost the same. So, we maintain
a 1-D array A whose values are determined by the following equation:
1 if (max{|Ri-Gi|,|Gi-Bi|,|Bi-Ri|}<ε) and (max{(255-Ri),(255-Gi),(255-Bi)}
<εor
A(i) = max{Ri,Gi,Bi}<ε),
0 otherwise,
whereεis a small positive real. Ri, Gi, and Bi are the R, G, and B values of pixel i,
respectively. If A(i) = 1, it means that the pixel i is black or white.
Finally, a weighted combination is used to combine arrays A and E to get array
M. The content of M is computed by the following equation:
M(i) = (w1 * A(i) +w2 * E(i))*255,
where w1 = nE / (nA + nE) and w2 = na / (nA + nE). nA and nE are the number of“1”in
the array A and E, respectively. Note that w1 + w2 = 1. M(i) is the grey level of pixel i.
108
Given a pixel, as shown in table 3-1, we have the following 1 order differential:
)1,(),(),(
),(
),1(),(),(
),(








yxfyxfyxf
y
yxf
yxfyxfyxf
x
yxf
y
x
2-order differential functions for a pixel (x, y) are as follows:
),()1,(
),(
),(),1(
),(
2
2
2
2
yxfyxf
y
yxf
yxfyxf
x
yxf
yy
xx








By simple computation, we have the following Laplace’s operators for pixels:
),()1,(),(),1(
),(),(
),( 2
2
2
2
2
yxfyxfyxfyxf
y
yxf
x
yxf
yxf
yyxx 






表3-1數位影像各相對位置亮度
f(x-1,y-1) f(x,y-1) f(x+1,y-1)
f(x-1,y) f(x,y) f(x+1,y)
f(x-1,y+1) f(x,y+1) f(x+1,y+1)
110






b
w
Si
i
b
b
Si
i
w
w
YY
YY
S
S
||||
1
,
||||
1
Finally, we compare each pixel in the image block with Lw and Lb. If its
illumination is close to Lw, it is set to white pixel. Otherwise, if its illumination is
close to Lb, it is set to black pixel. In our experiment, threshold θis set to 20.
B. Algorithms for computing evaluation function
Suppose that a character is composed of strokes S1, S2,…, and SN. The
corresponding cost for each stroke is C1, C2,…, and CN, respectively. Then, the total
cost for each character is CT= C1+C2+…+CN. The following algorithm is used to
compute the evaluation function of S comparing with some standard stroke S’.
function CostEval(S, S') : Integer
if ((width of S–width of S') > θ1) then
Cost := ∞
else
begin
Cost := 0
UnoverlappedSet := set of strokes so that S and S' are not overlapped in Y axis
OverlappedSetOfS := set of strokes belonging to S so that S and S' are overlapped
in Y axis
OverlappedSetOfS' := set of strokes belonging to S’so that S and S' are overlapped
in Y axis
NoiseQty := abs(|OverlappedSetOfS| - |OverlappedSetOfS'|)
Cost := Cost + NoiseQty
if (|UnoverlappedSet|> 0) then
begin
if (width of S or S' >θ2) then
Cost := Cost + 18* |UnoverlappedSet|
else Cost := Cost + 9* |UnoverlappedSet|
end
else
112
for each SUCCESSOR of BESTNODE do
begin
if there exists an OLD in OPEN that is the same as SUCCESSOR then
begin
give up SUCCESSOR
if the cost of current path is less than that of OLD path then
begin
change parent link of OLD to BESTNODE
update g(OLD) and f(OLD)
end
put OLD into the node list of BESTNODE
end
else if there exists an OLD in CLOSED that is the same as SUCCESSOR
then
begin
give up SUCCESSOR
if the cost of current path is less than that of OLD path then
begin
put OLD into the node list of BESTNODE
propagate new g(OLD), f(OLD) and all g and f for subnodes of OLD
end
put OLD into the node list of BESTNODE
end
else
begin
put SUCCESSOR into OPEN
set the parent link of SUCCESSOR to BEST
put SUCCESSOR into the node list of BESTNODE
f(SUCCESORR) := g(BESTNODE) + the cost from BESTNODE to
SUCCESSOR
end
end
end
Fig. 4-1 shows an example that a testing character is compared with standard
characters (0–9 and A–Z) based on A* algorithm.
114
5. Results and Summary
In this research, based upon algorithm engineering, we have developed and
implemented several effective yet efficient algorithms to localize license plates and
recognize characters in the plate under unrestricted environment. We also integrated
implement the proposed methods in Part 1 and Part 2. The integrated VLP
recognition system is tested and with the following experiment results:
day night
dark-colored
vehicles
light-colored
vehicles
dark-colored
vehicles
light-colored
vehicles
correct wrong correct wrong correct wrong correct wrong
No. of
pictures
60 0 37 3 58 2 31 9
Success ratio 100% 92.5% 96.7% 77.5%
Success ratio 97% 89%
In average, the proposed VLP recognition system has 93% recognition rate, no
matter it is in day or at night, or the vehicle is dark-color or light color. However, the
system can reach 98.3% recognition rate for dark-colored vehicles, no matter it is in
day or at night. On the other hands, the system can only reach 85% recognition rate
for light-colored vehicles, no matter it is in day or at night
As shown in Part 1, the average success plate localization ratio for day and night
is 94.57%. Furthermore, the average success plate localization ratio for dark-colored
vehicles is 100%, no matter it is in day or at night.
As shown in Part 2, for character recognition problem, 200 license plates are
tested and 196 plates are recognized correctly. The success rate is 98%. However, if
we investigate four incorrectly recognized plates, only one and the same character is
incorrect for each plate. In other words, our approaches can reach 99.96% character
recognition rate.
References
[1] Lin, S.P., "Extraction for Plate Characters," Master thesis, Department of
Electrical and Control Engineering, National Chiao Tung University, 1999.
[2] Chou, J.N.," Vehicle license plate recognition system," Master thesis,
Department of Information Engineering, National Sun Yat-sen University, 1995.
[3] B.T. Chun, Y.S. Soh, and H.S. Yoon ， "Design of Real Time Vehicle
Identification System", Proceedings of IEEE International Conference on Image
Processing, Volume 3, 1994, pp. 2147-2152.
