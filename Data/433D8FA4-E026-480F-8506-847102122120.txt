 2
contains hundreds of thousands of lines of 
source code.  
Techniques to detect and correct software 
flaws include human code reviews, testing, 
and formal verification. Human code 
reviews are time-consuming and expensive 
but can find problems that are difficult to 
find automatically. However, even 
extraordinarily cautious people are likely to 
overlook some software defects. Code 
reviews can not find and remove all defects 
in our software. The best practices for 
software testing employ a toolset to both 
manage and monitor a large suite of tests 
that are intended to exercise software in 
real-world situations. Management tools 
include test generators (mostly available for 
Web applications) and test 
case-management tools. Monitoring 
solutions include code coverage tools, which 
determine what percentage of the code is 
exercised by a test suite, and dynamic 
analysis tools that instrument the code to 
pinpoint the causes of test failures (e.g., 
Purify, Insure++, BoundsChecker [2-3]).  
 While these tools enhance the efficiency of 
software testing, they do not address the 
fundamental challenge that arises from the 
path explosion property.  The path 
explosion property states that the number of 
executable paths through a program scales 
exponentially with the number of lines of 
code. Even 100 percent code coverage were 
achieved during testing, it might still amount 
to a small coverage of the executable paths 
through the code [4].  
For most large commercial software, it is 
infeasible to account for all of the scenarios 
through which the software will run, which 
would generally take the lifetime of the 
universe to manually execute and test every 
feasible path. Moreover, testing is typically 
ineffective for finding security 
vulnerabilities. Software attackers attempt to 
exploit weaknesses that system designers 
did not consider, and standard testing is 
unlikely to uncover such weaknesses. It is 
impossible to completely automate tests for 
such software features as user interaction 
and business logic. However, there are large 
classes of critical defects that can be 
detected by formal verification than 
traditional compilers and testing can catch. 
Tools for source code verification have been 
developing recently: 
Bandera [5] provides the automatic 
extraction of finite-state models from Java 
source code. The finite-state models can be 
further used to generate a Java program 
model in the input languages of several 
existing formal verification tools: SPIN [6],   
SMV [7]. The Bandera uses slicing to 
compresses paths in the program by 
removing control points, variables, and data 
structures that are irrelevant for checking a 
given property.   
Java PathFinder (JPF) [8-9] works at the 
bytecode level, hence all of Java can be 
model checked as well as errors that can 
only be found on the bytecode level can be 
uncovered. JPF uses symmetry reductions 
and partial-order reduction to reduce 
state-explosion.  
The Compaq Extended Static Checker for 
Java (ESC/Java) [10] is a programming tool 
for finding errors in Java programs.  
ESC/Java detects common programming 
errors that ordinarily are not detected until 
run time. By using an underlying automatic 
theorem proving, ESC/Java tries to detect 
certain kinds of errors only, and not prove 
the program's correctness, for example, null 
dereference errors, array bounds errors, type 
cast errors, and race conditions. 
SLAM [11] and BLAST [12] are closely 
related tools, whose technical flavor is 
similar to MAGIC [13]. SLAM is primarily 
optimized to analyze device drivers, and is 
going to be included in the Windows 
development cycle. SLAM uses symbolic 
algorithms. BLAST is an on-the-fly 
reachability analysis tool for C programs. 
BLAST is based on the lazy abstraction to 
integrate and optimize the three phases of 
the abstract-check-refine loop. Lazy 
abstraction continuously builds and refines a 
single abstract model on demand, driven by 
the model checker, so that different parts of 
the model may exhibit different degrees of 
precision, namely just enough to verify the 
 4
program, so it takes into account all 
relationships between functions. The 
analysis’s scope also determines the amount 
of context the tool considers. More contexts 
are better when it comes to reducing false 
positives, but it can lead to a huge amount of 
computation to perform. Therefore, an 
advanced state space reduction should be 
and have been used for this kind of problem. 
- State-space reduction for alleviating the 
state-space explosion problem:  
In our research, we have invented new state 
space reduction techniques to reduce the 
state explosion problem. The research 
continues our previous work and design new 
theories especial for the security properties 
of real-life sophisticated large software 
systems. Such systems could be 
multithreaded, concurrent, and/or 
distributed. 
- User-friendly toolset for non-professional 
users: Good analysis tools must be easy to 
use, even for non-professional people. The 
analysis results must be understandable to 
normal developers who might not know 
much about security. Our toolset gives a 
counterexample which help users identify 
the cause of software defect. In addition, our 
system gives correction suggestion if the 
solution is possible to be generated by 
machine. Such an approach can educate 
users about good programming practice. 
 
 
Figure 1 gives an overall architecture of our 
formal verification toolset. Figures 2 and 3 
give an example of software verification 
using our toolset. 
 
 
Fig. 2 The main menu of our toolset 
 
Fig. 3 A verification example of our toolset 
 
The key idea behind minimization 
algorithms is to generate equivalent state 
spaces for analysis. After defining an 
equivalence, we can classify models into 
different groups so that models belonging to 
an identical group can not be distinguished 
using the semantics of equivalence. We 
develop another concept “synonym”. 
Synonym is also defined w.r.t. a specific 
function. An equivalence E1 is said to be a 
synonym for an equivalence E2 w.r.t. 
function F iff function F can guarantee an 
E2 equivalent output model when we 
replace the input models of F with E1 
equivalent models. Synonym is valuable to 
the innovation of weaker and more effective 
equivalences for compositional verification. 
The concept of synonym allows us to 
develop a new equivalence model “IOTODL 
equivalence” for efficiently analyzing 
