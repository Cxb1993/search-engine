 2
行政院國家科學委員會專題研究計畫期末報告 
多焦點問句之自動分析技術研究 
A Study on Analysis of Multi-Focus Questions 
計畫編號：NSC 98-2221-E-019 -041- 
執行期限：98 年 08 月 01 日至 99 年 07 月 31 日 
主持人：林川傑 國立台灣海洋大學資訊工程學系 
計畫參與人員： 郭育旻、趙品銜、徐榮彬、劉奕緯、楊宗豫、
陳彥亨、鮑建威 
 
一、中文摘要 
 
自動問答系統為新一代的資訊搜尋系
統，目的是將答案抽取出來直接呈現給使用
者，減少尋找資訊所花的時間。除了網際網
路資訊搜尋外，自動問答技術也可以應用在
電子圖書館或是電子百科全書上，可以提供
使用者在浩瀚的資訊中快速搜尋解答。 
在自動問答技術的研究過程中，具有多
焦點的問句型態在世界上尚未被注視，對此
問句型態的可能句型或分析方法也仍未有研
究提及。本計畫希望能對於多焦點問句做個
全面性地研究。包含蒐集問句、觀察並提出
多焦點問句的分析準則，研究自動偵測問句
中焦點所在以及其間關係的技術，和自動分
解原問句為多個單焦點子問句的技術。 
由於原先規劃為兩年的計畫，而本次計
畫補助期間只有一年，因此本成果報告僅有
原先規劃在第一年之工作─雙焦點問句分析
的執行成果。 
 
關鍵詞：多焦點問句、問句分析、問句分解、
自動問答 
 
Abstract 
 
Question answering (QA) can be 
considered as a new generation information 
retrieval (IR) technique.  It tries to find and 
reply the answer itself to the user so that it can 
save lots of time.  Besides the applications on 
the Internet, QA can also be applied to digital 
libraries or digitized encyclopedias.  Users 
can easily find answers in the huge amount of 
high quality data. 
During our QA research, we found a new 
question type: multi-focus questions.  There 
are still few studies on multi-focus questions, 
not to mention the theory to analyze such a 
kind of questions.  This project plans to make 
a complete study about multi-focus questions, 
including collecting natural questions, 
observing and proposing a theory to analyze 
multi-focus questions, studying techniques to 
automatically detect question foci and 
determine the relationships among the foci, as 
well as the techniques to decompose original 
questions into several single-focus sub-
questions automatically. 
The original plan was scheduled in two 
years, but only the first year was supported.  
Therefore, this report only contains the results 
of the works originally scheduled in the first 
year, i.e. the analysis of two-focus questions. 
 
Keywords: multi-focus question, question 
analysis, question decomposition, question 
answering 
 
二、緣由與目的 
 
自動問答系統 (question answering, QA) 
為新一代的資訊搜尋系統。使用者以自然語
言來查詢 (亦即以日常生活所用文句來發問
或描述他的資訊需求)，系統則會將尋找到的
答案呈現給使用者。 
在自動問答研究中，問句分析模組是一
個自動問答系統中相當重要的一環 (Duan et 
al., 2008; Moschitti et al., 2007; Krishnan et al., 
2005; Zhang and Lee, 2003)。根據輸入問句所
需求的答案類型，自動問答系統可根據不同
的策略來尋找答案。比方說，如果問句是問
「誰是美國第一屆總統？」那麼答案應該是
 4
斷詞，再找出關鍵字確定獨立成詞的問句
們。由於奇摩知識+題庫龐大，各個關鍵詞最
多只會蒐集 1000 個問句。 
 
3.2 蒐集雙焦點問句集合 
在問答主題網站上出現的問句，除了真
正要問的問題外，常因要與其他使用者對話
而出現無關問題的綴語。在 95 年國科會計畫
「基於網際網路之社群中問題搜尋之研究」
(計畫編號 NSC 95-2221-E-019-036-) 就曾針
對這類社群中提問問句裡會出現的廢話字串
做過研究。今年便利用此研究成果先將新蒐
集問句中的廢話部份刪去，減少其所帶來的
雜訊。 
接著以人工標記方式來決定問句的焦點
個數、焦點位置以及焦點間的關係。雖然出
現了雙焦點關鍵詞，問句卻不一定是雙焦點
問句，像是： 
電腦跑的很慢和 C碟使用空間有關嗎? 
這裡的「和」是關聯連接詞而不是對等連接
詞，本問句也只問一件事，並非雙焦點問句。
下表列出了各個關鍵詞所蒐集到的問句個
數。 
 
關鍵詞 句數 焦點數=2 焦點數>2
和 1000 196 2
與 1000 178 29
跟 1000 103 5
以及 842 356 36
每 1000 79 6
各 1000 410 45
都 1000 27 4
總計 6842 1349 127
 
由表格中數據可發現，雙焦點問句雖非大
宗，卻也佔有無法忽視的比例。在 6842 被標
記的問句中，有 1349 句是雙焦點問句，佔
19.72%。多於兩個焦點的多焦點問句也有
127 句，佔了約 2%，表示未來仍需要再對這
類問句性質做更多的研究。 
當標記者找到一句雙焦點問句後，要把
兩個焦點以及其關係標記出來。焦點間關係
我們定了四種，分別是 equal (焦點地位相
同，相當於兩個問句合併成一問句 )、
entity-attribute (問各實體的某屬性值 )、
entity-relationship (問兩種相關實體的對應關
係) 以及 thematic (兩焦點出現在語法結構不
同位置，關係較複雜)。舉例來說： 
中國各朝代1的首都2分別在哪? 
標記者會標出焦點為「朝代」和「首都」，
這是「朝代」與「城市」這兩種實體間的對
應關係，所以標為 entity-relationship 關係。
底下是四種焦點關係的例句： 
equal: 
宗教的功用1與意義2為何? 
entity-attribute: 
每一個星座1的個性2? 
entity-relationship: 
中國各朝代1的首都2分別在哪? 
thematic: 
日本每個季節1適合去遊玩的地點2 
 
下表列出了各關鍵詞所蒐集到的各種焦點關
係問句個數。可以看到 entity-relationship 類
佔最少，其餘諸類的比例差不多。 
 
關係 和 與 跟 以及 每 各 都
總
計 
equal 79 53 15 116 3 58 4 328
enty-attr 42 22 23 77 55 236 12 467
enty-rel 5 6 0 47 1 32 0 91
thematic 70 97 65 116 20 84 11 463
 
四、雙焦點問句句型分析實驗 
4.1 字面句型比對 
 
第一個實驗是直接以字面字串來蒐集句
型，並藉以習得抽取焦點與判斷類型的方式。 
先將所有標記有焦點資訊的問句裡面的
兩個焦點所在字串取代為 PF (primary focus
主焦點) 以及 SF (secondary focus 次焦點)。 
接著找出每個問句中兩個焦點以及關鍵
字所在位置，三者中再決定出現在最左及最
右者為誰。若出現在最左或最右位置的詞是
關鍵字，它可成為該位置之邊界；否則需要
往左或往右再找一個詞做為邊界，若左右為
句首句尾，則以##代表之。最後抽出左右範
圍之間的整段文字成為判斷焦點位置的句
型。例如這個問句「世界各國 1 的全名2?」，
會學到「各PF的SF ?」這樣的句型。 
 6
thematic 類沒有幫助。數據如下表。 
 
 eql e-a e-r thm total 
句型數 8 5 0 0 13
Precision 100.0 100.0 0 0 100.0
Recall 66.04 24.39 0 0 31.25
F-measure 79.55 39.22 0 0 47.62
 
底下是幾個句型的範例： 
(NP_PF) 以及 (NP_SF)  equal 
(NP_PF) 和 (NP_SF)  equal 
各 (NP_PF) DE (NP_SF) entity-attribute 
每 Nf (Na_PF) DE (Na_SF) 
  entity-attribute 
 
有鑑於引入過多標籤所造成的低頻現象，我
們改為僅參考焦點所在的詞組邊界資訊，而
不考慮是何種詞組。因此改以 P 來標示所有
焦點所在的詞組，改以 LEAF 來標示位於葉
節點的焦點，而不考慮詞性。 
此策略多習得了幾個句型，可是 entity- 
attribute 類句型的精確率反而變差。數據請見
下表。 
 
 eql e-a e-r thm total
句型數 10 7 1 0 18
Precision 100.0 57.14 100.0 0 90.00
Recall 83.02 19.51 18.18 0 37.50
F-measure 90.72 29.09 30.77 0 52.94
 
底下是幾個句型的範例： 
(P_PF) 以及 (P_SF)  equal 
(P_PF) 和 (P_SF)  equal 
各 (P_PF) DE (P_SF)  entity-attribute 
每 Nf (LEAF_PF) DE (LEAF_SF) 
  entity-attribute 
 
再次通盤檢討樹狀結構策略不夠成功的原
因，發現最主要失誤皆出現在語法剖析的錯
誤上。錯的語法樹會導致抽取句型的不正
確，也就不易累積並習得正確句型。由於問
句數量不少，人工檢查並修正語法結構需要
不少人力和時間，而且需要具語言學專業知
識的人員才能勝任，因此期盼未來能有正確
率更高的語法剖析器來幫助實驗的進行。 
 
 
五、計畫成果自評 
 
本計畫蒐集了多於 6000 句的自然語言
問句，並由人工標計蒐集得 1349 個雙焦點問
句來進行分析。計畫中並提出了以字面、詞
性以及語法結構資訊來自動學習焦點分析句
型的方法，其中詞性資訊句型可得 28 種句
型，precision 和 recall 分別為 92.86%及
44.98%。 
整體而言，研究內容已完成大部份計畫
所列的工作項目。多焦點問句分析原本就預
期是較難的工作，本年度所蒐集到的實驗資
料將可做為未來繼續研究的資源，非常可
貴。參與實驗的研究人員亦能從中獲得不少
經驗，頗有收穫。 
 
六、參考文獻 
Giampiccolo, Danilo, Anselmo Peñas, Christelle Ayache, 
Dan Cristea, Pamela Forner, Valentin Jijkoun, Petya 
Osenova, Paulo Rocha, Bogdan Sacaleanu, and 
Richard Sutcliffe (2007) “Overview of the CLEF 
2007 multilingual question answering track,” 
Working Notes for the CLEF 2007 Workshop, pp. 
200-236. 
Duan, Huizhong, Yunbo Cao, Chin-Yew Lin, and Yong 
Yu (2008) “Searching Questions by Identifying 
Question Topic and Question Focus,” Proceedings 
of ACL-08: HLT, pp. 156–164. 
Moschitti, Alessandro, Silvia Quarteroni, Roberto Basili, 
and Suresh Manandhar (2007) “Exploiting Syntactic 
and Shallow Semantic Kernels for Question/Answer 
Classification,” Proceedings of ACL 2007, pp. 
776–783. 
Krishnan, Vijay, Sujatha Das, and Soumen Chakrabarti 
(2005) “Enhanced Answer Type Inference from 
Questions using Sequential Models,” Proceedings of 
HLT/EMNLP 2005, pp. 315–322. 
Zhang, Dell and Wee Sun Lee (2003) “Question 
Classification using Support Vector Machines,” 
Proceedings of SIGIR 2003, pp. 26-32. 
Mitamura, Teruko, Eric Nyberg, Hideki Shima, Tsuneaki 
Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, 
Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and 
Noriko Kando (2008) “Overview of the NTCIR-7 
ACLIA Tasks: Advanced Cross-Lingual Information 
Access,” Proceedings of NTCIR-7. 
Dang, Hoa Trang, Jimmy Lin, and Diane Kelly (2006) 
“Overview of the TREC question answering track,” 
Proceedings of TREC 2006, pp. 99-116. 
Description of the NTOU Complex QA System 
Chuan-Jie Lin    Yu-Min Kuo 
Department of Computer Science and Engineering 
National Taiwan Ocean University 
{cjlin, corn.guo}@ntou.edu.tw 
 
ABSTRACT We participated in NTCIR-8 CCLQA Task for Chinese-to-Chinese 
(C-C) and English-to-Chinese (E-C) tasks.  This is our first time 
developing a QA system for complex questions, although we 
already have some experiences on handling BIOGRAPHY, 
DEFINITION, and WHY questions. 
This paper provides the description of our complex QA system, 
the NTOU XQA System participated in NTCIR-8 CCLQA Task.  
This QA system can answer several types of factoid questions and 
5 types of complex questions defined in NTCIR-8 CCLQA Task.  
Different strategies are designed for finding answers to different 
types of questions.  Named entity recognition, distance scores of 
question keywords, answer information patterns, and search 
results from the Web are techniques integrated in these strategies.  
The best F-measure score achieved by our system is 19.88% in 
monolingual task and 13.62% in cross-lingual task.  But 
unfortunately the official evaluation is incorrect. 
This paper is structured as follows.  Section 2 gives a description 
of the architecture of our complex QA system, NTOU XQA 
System.  Section 3 gives details of methods to do answer type 
classification.  Section 4 introduces the procedures to find 
answers from the Internet for factoid and WHY questions.  
Section 5 expresses how we extract final answers from the 
ACLIA2 Corpus.  Section 6 presents the strategies used in our 
formal runs and their evaluation results.  Section 7 concludes this 
paper. Category and Subject Descriptions 
H3.4 [Information Storage and Retrieval] Systems and Software - 
Question-answering (fact retrieval) systems 
General Terms 
Performance, Experimentation 
Keywords 
Question answering, complex questions 
 
1. INTRODUCTION 
2. SYSTEM DESCRIPTION 
The architecture of the NTOU XQA System follows a typical QA 
procedure.  This QA system is designed to find answers from the 
web pages, not from a static document collection. 
Figure 1 shows the architecture of the NTOU XQA System.  
Given an input question, the system first decides its answer type 
(including factoid and complex question types) and extracts the 
focus and keywords of the question.  A search engine is used to 
retrieve relevant web pages from the Internet.  Possible answers 
are mined in the nuggets or the full text of retrieved web pages.  
Different types of questions will be answered by different answer 
finding modules.  More details will be described in the 
subsequent sections. 
Cross-lingual QA has been one of the tracks in NTCIR evaluations 
since 2005.  CLQA tasks in NTCIR-5 and NTCIR-6 [[1]] dealt 
with factoid questions, including PERSON, LOCATION, TIME, 
ORGANIZATION, NUMBER, etc.  We participated in NTCIR-5 
CLQA task and built our first cross-lingual QA system [[2]]. To participate in NTCIR-8 CCLQA Task, our system has to be 
adapted in order to find answers from a static document collection.  
For BIOGRAPHY, EVENT, and RELATIONSHIP questions, their 
answer-finding strategies are applied directly to the relevant 
documents retrieved from the ACLIA2 Corpus.  For 
DEFINITION, WHY, and factoid questions, possible answers 
found from the Internet are matched in the relevant documents 
retrieved from the ACLIA2 Corpus. 
CCLQA task in NTCIR-7 [[3]] dealt with complex questions, 
including BIOGRAPHY, DEFINITION, RELATIONSHIP, and 
LIST/EVENT questions.  Many research results were presented 
in this workshop [4][5][6][7][8]. 
This time, WHY questions are newly added in the formal test set 
which ask for causes or explanations.  Girju et al. have examined 
verbs which denoting causal relations [9] and沈天佐 et al. [10] 
tested the correctness of different causal patterns in English. Cross-lingual Experiments 
Because we have not developed our own cross-lingual handling 
methods yet, to perform cross-lingual QA from English to Chinese, 
we submitted all the English question sentences to the Google 
Translate webpage, and then used the Chinese translation results 
as the question inputs as in monolingual question answering. 
 
 
3. ANSWER TYPE CLASSIFICATION 
There are factoid and complex questions in the NTCIR-8 CCLQA 
question set.  The methods to guess types of factoid questions 
and types of complex questions are different in our system. 
3.3 Classification Voting 
To merge the guesses from these different strategies, a voting 
mechanism is designed to make the final decision.  A factoid type 
has the highest preference.  A complex type is decided by votes 
from different strategies where BIO > DEF > EVT > REL.  The 
type “OTHER” does not count as a vote.  Note that for a 
WHY-question, it gets no votes from the clue-mining classification 
methods and can only be detected by Lin’s rules.  If a question 
gets no vote at all, it will be classified according to the highest 
clue-coverage ratio. 
3.4 Keyword Extraction 
The answer type classification rules also consist of patterns to 
extract question keywords.  When the answer type of a question 
is determined, its keywords can be prepared at the same time. 
For a complex question, its keywords are extracted as follows: 
remove all the clue terms related to its answer type and collect the 
remaining substrings as keywords. 
Again, we use ACLIA2-CT-0012 as an example to demonstrate 
how to use clue terms (in words) to extract answer keywords.  
The matched clue terms in the question sentence are removed 
(blackened in the example) and the remaining segments 
(underlined) are keywords. 
Word segmented: 
睡眠 與 壽命 長短 的 關係 為何 
After clue term removed: 
睡眠 與 壽命 長短 的 關係 為何 
4. ANSWERING FROM THE WEB 
As we described in Section 2, for DEFINITION, WHY, and 
factoid questions, possible answers are first mined from the 
Internet, and then matched in the relevant documents.  The 
answers to the BIOGRAPHY, RELATIONSHIP, and EVENT 
questions are searched directly from the relevant documents 
retrieved from the ACLIA2 Corpus. 
The online answer mining methods are described in the following 
subsections. 
4.1 Factoid Questions 
The methods of finding answers to factoid questions are exactly 
the ones used in Lin’s work.  The following paragraph is a brief 
description of the procedures of finding answers to a factoid 
question. 
Question keywords are combined as one query and submitted to an 
online search engine.  Answer candidates are identified in the top 
N nuggets by the help of a named entity recognition system.  
Each candidate is scored according to the number of nuggets 
where it occurs and the distance scores related to the question 
keywords surrounding it in the nuggets.  Finally, at most top 10 
answers are proposed according to their scores. 
The response for a factoid question by our QA system is the exact 
string of the answer itself together with a set of snippets providing 
evidences for verification.  The evidence snippets are the snippets 
in the search results which support the same answer. 
4.2 DEFINITION and WHY Questions 
The methods of finding definition and causal answers from the 
Internet also follow Lin’s work, which are described here. 
DEFINITION 
There are some common patterns which express definitions in a 
written text, such as “XXX就是” (XXX is a ...) or “XXX的定義是” 
(the definition of XXX is ...).  Some patterns are invented to 
construct queries to the search engine.  Snippets matching the 
given patterns are considered as carrying definitions.  For 
example, given a question “類固醇是什麼 ” (“What is the 
so-called steroid”), the following queries are submitted to the 
search engine: 
“類固醇就是” (“steroid is”) 
“類固醇的定義是” (“the definition of steroid is”) 
“類固醇的意思是” (“the meaning of steroid is”) 
Note that the queries are given as phrases so they should not be 
separated in the retrieved snippets.  For a snippet containing the 
query, the substring directly following the query is extracted as a 
possible definition.  The bordered text in the following example1 
is one possible definition. 
類固醇就是治療氣喘抗發炎藥物中最有效的藥物 
(Steroid is the most effective anti-inflammatory medicine 
used to treat asthma) 
WHY 
Some cause-effect clue words, such as “因為” (because) and “因
此” (therefore), are collected in advance.  Each clue word is 
combined with all the questions keywords and submitted to the 
search engine.  The original web pages in the result lists are 
downloaded and further analyzed according to its rhetorical 
structure to find out the causal parts.  The causal parts are 
returned as the exact answer strings and the whole sentences as 
their evidence snippets. 
Take the question “禽流感為什麼會威脅人類” (“Why is the bird 
flu a threat to humans”) as an example.  A query “禽流感 威脅 
人類  因為” (“the bird flu” “threat” “humans” “because”) is 
constructed for the cause-effect clue word “因為” (because) and 
submitted to the search engine.  All three question keywords 
(marked as bold and italic in the example) appear in the effect 
passage, so the causal part of the text (the bordered text) is 
extracted as a possible reason2. 
流感病毒愈像禽流感病毒，其對人類的威脅也愈大， 
因為人類的免疫系統過去從未暴露在這些病毒的環境下 
(The greater similarity that a flu virus is to the bird flu virus 
means a bigger threat to humans, because the humans have 
never been in an environment with this kind of virus.) 
5. ANSWER MATCHING 
5.1 Relevant Document Retrieval 
In the architecture of our QA system, there should have been an IR 
module retrieving relevant documents from the ACLIA2 Corpus.  
We do have our own IR system.  However, we failed to complete 
the indexing of the corpus in time, so we decided to adopt the 
                                                                
1 Selected from the web page: 
http://www.tainantb.gov.tw/?aid=302&page_name=detail&iid=75 
2 Selected from the web page: 
http://www.dajiyuan.com/b5/4/2/7/n462106.htm 
at least one location name.  If a newly selected sentence contains 
the same temporal expression or location name as the ones in the 
previous selected sentences, it will be discarded. 
RELATIONSHIP 
As described in Section 3.4, two keyword substrings of a 
relationship question will be extracted.  They are regarded as the 
two targets in the relationship question.  Sentences containing the 
two targets are possible answers.  The distance between the two 
targets in a sentence is the main ranking key, i.e. the nearer that the 
two targets are close to each other, the higher this sentence will be 
ranked. 
6. EXPERIMENTAL RESULTS 
6.1 Answer Type Classification 
Table 3 and Table 4 list the answer type classification performance 
for monolingual QA.  Table 3 lists the numbers of questions 
being classified in the gold standard and by our system in every 
answer types, where rows represent the numbers in the gold 
standard and columns represent the numbers classified by our 
system.  Table 4 lists the recall, precision, and F1-score for each 
answer type, respectively.  The meanings of the symbols used in 
the tables are as follows: 
B Biography L Location 
D Definition T Date 
E Event O Organization 
R Relationship Q Quantity 
W Why J Object 
P Person Atype Answer type 
As we can see in Table 4, the overall accuracy (i.e. the recall in 
total) of the classification is 70%.  For DATE and WHY types, 
both recall and precision are higher than 90%.  Our system failed 
to detect any organization question, which means that the 
classification rules need to be re-written. 
Table 3. Answer type classification (MLQA) 
Gld\Sys B D E R W P L T Q J Total
B 7   2  1     10
D  9        1 10
E 2 3 4 7   1  1 2 20
R 1   19       20
W  1  1 18      20
P      5     5
L    1   3   1 5
T        5   5
O    4  1     5
Total 10 13 4 34 18 7 4 5 1 4 100
Table 4. Answer type evaluation (MLQA) 
Atype Recall Precision F1 
B 70.00 70.00 70.00
D 90.00 69.23 78.26
E 20.00 100.00 33.33
R 95.00 55.88 70.37
W 90.00 100.00 94.74
P 100.00 71.43 83.33
L 60.00 75.00 66.67
T 100.00 100.00 100.00
O 0.00 0.00 0.00
Total 70.00 73.68 71.79
However, we have different opinions on two questions in the 
ACLIA Test Set.  They are ACLIA2-CT-0056 “Please list the 
movies in which Zhao Wei participated” and ACLIA2-CT-0057 
“Please list the New Year films made by Feng Xiaogang”.  They 
are classified as EVENT questions in the ACLIA Test Set.  But to 
us, their answers are movie titles thus more like ARTIFACT 
(factoid, classified as OBJECT in our system) questions. 
Table 5 and Table 6 list the answer type classification performance 
for CLQA.  Not surprisingly, the overall accuracy (recall in total) 
drops to 50%. 
Table 5. Answer type classification (CLQA) 
Gld\Sys B D E R W P L T Q J Total
B 4 1    5     10
D  10         10
E 4 3 5 3   1   4 20
R  8 1 6   1   4 20
W  3  2 14    1  20
P      5     5
L    2   3    5
T  1  1    3   5
O    3  1    1 5
Total 8 26 6 17 14 11 5 3 1 9 100
Table 6. Answer type evaluation (CLQA) 
Atype Recall Precision F1 
B 40.00 50.00 44.44
D 100.00 38.46 55.56
E 25.00 83.33 38.46
R 30.00 35.29 32.43
W 70.00 100.00 82.35
P 100.00 45.45 62.50
L 60.00 60.00 60.00
T 60.00 100.00 75.00
O 0.00 0.00 0.00
Total 50.00 55.56 52.63
6.2 Run Description 
We only used the QUESTION fields in the test collection to 
produce formal runs.  English questions were first translated by 
Google Translate System and then processed in the same way as in 
monolingual QA. 
We submitted three monolingual runs and three cross-lingual runs 
according to the same three strategies described as follows. 
Run 1 
For each factoid question, only the best sentence containing the 
top-1 answer was proposed.  If more than sentences contained the 
top-1 answer, they were ranked by the following preference: Sans > 
Sqkw > Sevd.  If no exact answer was provided by the NTOU XQA 
System, we followed the strategy of Run 3 to prepare answers. 
For BIOGRAPHY, DEFINITION, EVENT and RELATIONSHIP 
questions, top 30 sentences were proposed by the methods 
described in Section 5.3.  When comparing temporal expressions 
in finding EVENT answers, we did not perform temporal 
resolution, i.e. comparing was performed on the surface strings. 
If only a few sentences were found in the first place, a back-off 
model was performed which selected sentences with a length 
between 10 to 30 characters containing question keywords in the 
order of the rankings of the relevant documents.  There is no 
[5] Shima, H., Lao, N., Nyberg E., and Mitamura T. 2008. 
Complex Cross-lingual Question Answering as a Sequential 
Classification and Multi-Document Summarization Task. In 
Proceedings of NTCIR-7, 33-40. 
[6] Mori, T., Okubo T., and Ishioroshi M. 2008. A QA System 
that Can Answer Any Class of Japanese Non-Factoid 
Questions and its Application to CCLQA EN-JA Task: 
Yokohama National University at NTCIR-7 ACLIA CCLQA 
EN-JA. In Proceedings of NTCIR-7, 41-48. 
[7] Ren, H., Ji, D., He, Y., Teng C., and Wan J. 2008. 
Multi-Strategy Question Answering System for NTCIR-7 
C-C Task. In Proceedings of NTCIR-7, 49-53. 
[8] Lee, YH., Lee, CW., Sung, CL., Tzou, MT., Wang, CC., Liu, 
SH., Shih, CW., Yang RY., and Hsu, WL. 2008. Complex 
Question Answering with ASQA at NTCIR 7 ACLIA. In 
Proceedings of NTCIR-7, 70-76. 
[9] Girju, R. and Moldovan, D. 2002. Text Mining for Causal 
Relations. In Proceeding of FLAIRS Conference 2002, 
360-364. 
[10] 沈天佐, 林川傑, 陳信希. 2003. 以網際網路內容為基礎之
問答系統 “Why” 問題之研究. In Proceedings of 
ROCLING-15, 211-229, written in Chinese. 
[11] Lin, CJ. 2004. A Study on Chinese Open-Domain Question 
Answering System. Ph.D. dissertation, National Taiwan 
University. 
 
國科會補助計畫衍生研發成果推廣資料表
日期 2010年11月01日
國科會補助計畫
研發成果名稱
發明人
(創作人)
技術說明
技術移轉可行性及
預期效益
技術/產品應用範圍
產業別
計畫名稱:
計畫主持人:
計畫編號: 學門領域:
(中文)
(英文)
成果歸屬機構
(中文)
(英文)
多焦點問句之自動分析技術研究
林川傑
98 -2221-E -019 -041 - 自然語言處理與語音處理
雙焦點問句分析句型集
Patterns of Two-Focus Question Analysis
國立臺灣海洋大學 林川傑
這是一套用以分析雙焦點問句之焦點位置以及焦點關係的句型。自動問答系統
為新一代的資訊搜尋系統，目的是將答案抽取出來直接呈現給使用者，減少尋
找資訊所花的時間。除了網際網路資訊搜尋外，自動問答技術也可以應用在電
子圖書館或是電子百科全書上，可以提供使用者在浩瀚的資訊中快速搜尋解答
。
在自動問答技術的研究過程中，具有多焦點的問句型態在世界上尚未被注視，
對此問句型態的可能句型或分析方法也仍未有研究提及。本計畫希望能對於多
焦點問句做個全面性地研究。包含蒐集問句、觀察並提出多焦點問句的分析準
則，研究自動偵測問句中焦點所在以及其間關係的技術，和自動分解原問句為
多個單焦點子問句的技術。
This is a set of patterns to analyze two-focus questions, determine
the positions of two foci, and the relation of the two foci. Question
answering (QA) can be considered as a new generation information
retrieval (IR) technique.  It tries to find and reply the answer
itself to the user so that it can save lots of time.
During our QA research, we found a new question type: multi-focus
questions.  There are still few studies on multi-focus questions, not
to mention the theory to analyze such a kind of questions.  This
project plans to make a complete study about multi-focus questions,
including collecting natural questions, observing and proposing a
theory to analyze multi-focus questions, studying techniques to
automatically detect question foci and determine the relationships
among the foci, as well as the techniques to decompose original
questions into several single-focus subquestions automatically.
研究發展服務業
資訊工程
研究持續進行中，期盼使效能更接近可移轉程度
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
98年度專題研究計畫研究成果彙整表 
計畫主持人：林川傑 計畫編號：98-2221-E-019-041- 
計畫名稱：多焦點問句之自動分析技術研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 70% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 70% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
