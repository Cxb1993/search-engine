 2
十八世紀工業革命以後，近代化的社會急劇
地取代了傳統的封建式自給自足社會，十九
世紀中葉開始出現了照相機以及留聲機，為
影音典藏帶來了突破性的發展。但直到二十
世紀初的電影工業及最早期的調幅廣播，對
於影像或音訊只能擇一，只能聽不能看或是
只能看不能聽。二零年代之後，有聲電影出
現；二戰末期電視機開始實用化，徹底開啟
了影音視訊高速發展的年代。人們對於音視
訊的需求由黑白影像與單聲道演進為現今的
彩色影像與立體聲，不久的將來，互動式自
由視角的 3D 音視訊播放系統即將實現，將
我們帶入一個立體與 3D 的世界中。在某些
應用中，例如舞台表演，體操教學，甚至球
類運動，賽道競速等等，人們期待的是一個
全方位、任意視角且身歷其境的影音效果。
在此一趨勢之下，3D AV 的研究逐漸發展。
國外 MPEG(Moving Pictures and associated 
Expert Group)委員會 自 2001 年開始已成立
了相關小組進行 3D AV 方面的討論，其間並
漸漸有成果發表。國內雖有幾位教授曾進行
過立體視訊壓縮或影像合成的相關研究，然
卻都屬點狀式，而少有從 3D 音視訊擷取，
壓縮到最後多媒體呈現的通盤整合研發。 
    本整合型計畫為研究 3D 音視訊的內容
製作、處理與互動式呈現。未來經由本系統
可以更自由地、更身歷其境地觀賞一個舞台
表演的影音效果；也可以從各種角度更仔細
地學習肢體表演。本整合型計畫共有三個主
要研究重點，一為負責內容製作時的 3D 音
視訊擷取，二為多視域音視訊整合資料表示
法，三是資料壓縮解壓縮及自由視角音視訊
合成/播放等的研究。藉由此項計畫之整合，
發展出應用於未來 3D TV 系統中之 3D AV
相關擷取製作，編碼儲存以及播放呈現等重
要核心技術之完整解決方案，進而增強我多
媒體 IT 產業未來國際市場競爭力。 
  
 
三、研究報告與方法 
3.1 簡介 
子計畫一的研究目標主要是發展 3D 
audio 在任一視角下之相對音場效果的轉
換與呈現，並進而探討其空間感效果加強
之技術，並以此為基礎，產生使用者所要
求任意觀看視角的 3D 音訊。 
子計畫二研究內容主要在針對表演平
台的環境進行有效的影像資料擷取，以不
同拍攝角度架設攝影機，以期在最少攝影
機拍攝的條件下，取得完整的多視域視訊
資料。為了簡化隨後壓縮處理中所需的計
算 (資料減量) 及預先計算視角內差與立
體播放時所需的視差與 3D 模型等資料，如
何在影像擷取後進行多視域影像整合並建
構一個有效而簡潔的表示法亦是本子計畫
的研究重點。 
子計畫三針對多視域視訊整合表示法
的相關資料進行壓縮處理以減少所需之儲
存空間，並依據所取得的多視域影像特性
來設計其特有的壓縮方式，以及預先計算
視角內差與立體播放時所需的視差或 3D
模型等資料。 
基於必須由多視域影像中建立出任意
視角影像，以達到自由視角的播放需求，
在子計畫四將因應使用者所要求的觀視角
度，利用解碼後資訊 (如影像視差、3D 模
型及影像內物體相對於空間的資訊等) 以
產生使用者所需視角的影像，並且將建立
一套可與使用者互動的介面，以利使用者
可以自由選取任意視角來觀看立體視訊。 
3.2 各計畫間之關連性 
雖然影音訊號在處理上其截然不同的
方式使得在計畫規劃上將音、視訊兩者分
列於不同子計畫中執行；而在視訊部分，
又因處理程序與研究領域皆不同劃分為三
個子計畫，但各子計畫間彼此間有著密不
可分的關係。關係圖如圖(1)所示。我們可
由下列說明瞭解各子計畫彼此間的關係： 
(1) 子計畫一與其他子計畫間相互關係： 
從音訊處理角度而言，首先在音視訊的
錄製擷取上，我們將配合子計畫二來探討
3D 音視訊進行數位內容製作時，麥克風相
對於攝影機之數量、擺放位置以及音訊
channel 之多寡，進行研究與實驗。而在音
訊儲存方面，我們將探討多通道音訊之壓
縮方式，利用通道間的相關性做最佳壓
縮，並配合子計畫三發展出音視訊的系統
多工與同步方式。接著在互動式播放呈現
方面，子計畫四將根據使用者回饋之視角
資訊利用視角內差方式實現自由空間場中
 4
0 2 4 6 8 10 12 14 16 18
x 104
-0.4
-0.2
0
0.2
0.4
0 2 4 6 8 10 12 14 16 18
x 104
-0.2
0
0.2
0.4
0.6
每個麥克風都有一個響應，而此響應為一個
FIR的濾波器。 
　　　
　　　
　　　
　　　
　　　
　　　
　
　
　
　
　
　
1s
qs
1x
px
1y
qy
11h
q1h
1qh
qqh
11w
p1w
1qw
pqw  
圖(3) 盲敝訊號分離演算法架構圖 
 
此演算法是希望藉由遞迴的方式來不
斷更新濾波器的係數，最後可以得到一組最
佳解以成功分離出各個訊號。但演算法主要
是針對麥克風數量必須為音源數的兩倍才
可行，但實際運用上，音源數與麥克風的數
目不可能是呈固定關係，故研究的目的主要
是希望使用少數的麥克風即可定位出多音
源。作法為在水平面上放置兩個麥克風 x1
與 x2，同樣地在垂直面上也放置兩個麥克風
x3與 x4，如圖(4)所示。再以 Shannon＇s 消
息理論中的通式為基礎，我們可以依據音源
訊號中非高斯成分(nongaussianity)、非白化
成 分 (nonwhiteness) 與 非 穩 定 成 分
(nonstationary)等性質來經由價值函數（cost 
function）的運算以取得四個音源相對於水
平面以及垂直面上麥克風之間的響應，有了
這些響應方能計算音源相對於麥克風的延
遲時間。延遲時間的意義是音源傳遞到兩麥
克風的時間差，而當音源以直接路徑(direct 
path)傳遞到麥克風時即為最大響應值，因此
我們才會取個別響應的最大值來做運算。 
 
 
 
 
 
 
 
圖(4) 四麥克風之擺設示意圖 
 
另外，我們針對音源訊號做了音訊分類
的處理，主要將其分成靜音(silent)、純音樂
(music)、純語音(speech，人類語音，本實
驗採用一男聲與一女聲)等三類。由於分析
的 訊 號 種 類 不 同 ， 取 樣 頻 率 皆 採 用
44.1kHz。表(1)展示了實驗中音源代號、位
置以及分類。圖(5)為 S1~S4 播放的音源
(source)波形，圖(6)為麥克風錄到的波形。  
 
表(1) 音源分類、位置及其代碼說明 
音源代號 音源位置(cm) 類型說明 
S1 (140,-80, 40) Male 
(男聲) 
S2 (230,-25, 15) Music 
(小提琴獨奏) 
S3 (150, 0 ,-85) Female 
(女聲) 
S4 (170, 90,-55) Music 
(鋼琴獨奏) 
 
 
 
 
 
 
 
 
 
圖(5) S1~S4 播放的音源波形 
 
 
 
 
 
 
 
 
 
 
 
圖(6) 麥克風所錄到的波形 
本實驗主要使用四個麥克風來定位四
個音源，其中音框的長度大小設定為512，
濾波器的長度設定為256，時間延遲參數設
x1 x2
x3
x4
(0,-157.5,0) (0,157.5,0)
(0,0,120)
(0,0,-120)
 6
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
在物體的三維重建方面，我們採用 Cheung 
對視覺殼所提出的束邊線(bounding edge)
表示方式：在考慮一組剪影的資訊下，透過
相機中心沿著影像中之物體剪影投射至三
維空間以形成一組射線，將每一射線上所有
的點投射到其他各個影像平面上形成一條
線段，然後將和剪影重疊的部分再投影到三
維空間中，而在三維空間中共同重疊的線段
就稱為約束邊線，如圖(10)所示。 
             
 
 
 
 
 
 
 
 
 
在多攝影機影像擷取系統的實作上，本
子計劃採用  8 架類比攝影機 (Watec 
221s)，搭配三張 RTV-24 類比攝影機影像
擷取卡，以圖(11)的系統流程與擺設方式進
行攝影機之校正，取得各架攝影機之內部參
數與相對位置。接著再對各擷取的影像進行
前景與背景分離，最後再以視覺外殼方法重
建出物體的三維模型。 
實驗結果如圖(12)至(17)所示，圖(12)
為輸入影片的部份影像，圖(13)為前景與背
景分割後的物體剪影結果。圖(14)與(15)為
多視角剪影重建技術建立三維模型之輸入
及輸出，此一方式其優點是重建速度快、不
受物體本身的紋理資訊所影響，缺點則是無
法處理物體凹陷的區域且需要很多架的相
機才可以重建出比較細節的物體外形。圖
(14)虛擬的兔子是由八架虛擬攝影機所拍攝
的影像，圖(15)則是由此視覺殼演算法所得
之完整三維模型結果的呈現。圖(16)為實際
實驗的場地與相機架設圖，以及部份測試玩
偶的影像，圖(17)為實際的三維模型重建結
果。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
線性判別分析法之投影 
原始影像以RB投影
原始影像以 BG 投影 
    圖(9) 影像平面之投影 
圖(10) 可視邊線表示法的剖面圖
圖(11) 影像擷取系統流程圖與相機設置校 
正圖 
原始影像以RG投影
原始影像以 RGB 投影 
 8
3.3.3 子計畫三、多視域視訊整合表示法之資
料壓縮技術。 
為了整合多視域視訊的表示法，必須對
多視域視訊的影像擷取與表示法建構進行
研究；主要在針對表演平台的環境進行有效
的影像資料擷取，以不同拍攝角度架設攝影
機，以期在最少攝影機拍攝的條件下，取得
完整的多視域視訊資料。在子計畫三中將針
對上述多視域視訊整合表示法的相關資料
進行壓縮處理以減少所需之儲存空間。 
本子計畫利用 JVT 組織正在發展中
H.264-based 的 JMVM (Joint Multi-view 
Video Model) 參考軟體作為平台，來架構立
體視訊的壓縮/解壓縮系統。利用 H.264 比
MPEG-2/4 擁有更好的壓縮率以及解壓縮後
高影像品質的特性，架構出適用於立體視訊
的壓縮/解壓縮系統。我們的方法除了具備傳
統視訊編碼在時間上去除冗餘性，更利用左
右眼影像間的視差 (disparity) 對應關係，來
找到更相像的預測影像，以增進壓縮效果。 
左眼影像就如同傳統視訊般的編碼，而
右眼影像在編碼時除了時間上的參考，也參
考左眼影像同一時刻的影像  (即視差關
係)，以增進編碼效率與影像重建品質。立
體視訊相對於單眼視訊的特點在於視差資
訊的輔助，它們的利用可以增進壓縮效率；
而本計畫開發了一種 H.264 立體視訊編碼
時的模式決策 (mode decision) 加速法則，
其模式包括傳統的可變區塊大小及相對於
立體視訊的運動/視差模式，該方法利用一類
神經網路分類器來進行模式選擇，改善傳統
方法需要對每一模式進行檢測而花費較多
時間的缺失。 
本計畫所開發雙眼視訊編碼方法中，
左右眼視訊均以 GOP (group of pictures)為
單位進行編碼。左右眼每一個 GOP 架構
中除了第一張影像為 I frame 外，其餘皆
以階層式 (hierarchical) B frame 當作參考
關係，而右眼視訊中的階層式 B frame 除
了前後時間上的參考外，也參考左眼的重
建影像 (利用視差預測)。由於右眼影像在
進行 mode decision (包括可變區塊大小及 
motion/disparity 模式選擇) 時其 mode 組
合數相當多，將導致整個最佳化過程相對
於左眼影像處理時緩慢。本計畫利用類神
經網路  (neural network) 分類器來輔助
H.264 在階層式 B frame 時的模式選擇。我
們在編碼時對原始影像做前處理：利用相
鄰兩張影像相減，依據差值的大小來分離
前景與背景，並擷取影像邊緣特徵。編碼
時將這些邊緣特徵及前/背景資訊當作類神
經網路的輸入，類神經網路將判斷出最適
合的參考方式  (motion/disparity) 及可變
的區塊大小，藉此減少所需嘗試的模式數
目，而達到編碼加速的目的。由於本計畫
採用適當的影像特徵，再藉由類神經網路
在訓練時將誤差最小化的能力，使得編碼
時模式選擇的分類精確度高，實驗顯示，
利用類神經網路進行模式選擇可以節省不
少時間，並且影像編碼品質損失也相當小。 
    本計畫類神經網路所作的模式決策加
速有兩種層級，一種是巨區塊層級 
(macroblock level，16 x 16) ，另一種是子
區塊層級 (sub-block level，8 x 8)。第一級
的類神經網路分類器輸出有六種：skip_16 
x 16、16 x 16、16 x 8、8 x 16、8 x 8、Intra，
從六種模式中選擇其中分類出機會較高的
三種候選模式來做模式決策。第二級的類
神經網路分類器的輸出則有五種：skip_8 x 
8、8 x 8、8 x 4、4 x 8、4 x 4，第二級應用
的時機是：當第一級分類器分類後輸出三
種的候選模式，其中之一包含了 8 x 8 模
式，此時再針對巨區塊中的四個 8 x 8 子區
塊各別作第二級的類神經網路分類。 
    表(3)顯示了各個右眼視訊測試影像經
過類神經網路 off line訓練後得到的權重係
數組合，再利用該係數組合來讓類神經網
路分類器輔助模式決策加速，與未加速前 
(full mode decision) 的實驗結果比較。實驗
環境的電腦設備為：Intel Core 2 Duo 2.33 
GHz, 2.34 GHz、1.49 GB ram。測試影像分
別有：ballroom (640 x 480)、race (640 x 
480)、exit (640 x 480)、puppy (720 x 480)、
soccer (720 x 480)。從表(3)中發現：PSNR
約減少 0.008 ~ 0.07 dB、Bit rate 約增加
2.63% ~ 4.69%、平均 B frame 編碼加速約
76.85% ~ 79.05% 的程度。其中對於測試影
像本身若有明顯的 camera motion 的話 (如
race 和 soccer)，其 PSNR 降得比其他測試
影像多，而加速時間比其他測試影像稍
少，大約 1 ~ 2%，估計若再對於前/背景資
 10
Anchor Image Reference Image 
Virtual Image 
Anchor Image Reference Image 
   actual Image 
加了對應點的精確性。搜尋對應點的方式如
圖(20)，類似 Motion estimation 的搜尋方式，
但 Block size 為一條同於影像寬的線條。紅色
框框為搜尋範圍，紅線為搜尋到的最佳對應
位置。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖(20) Disparity estimation scheme 
     
找到對應關係後，我們根據需要合成之影像
的位置來內差出虛擬影像。以合成中間影像
為例，若對應的關係如圖(21)所示，則合成影
像的位置為兩個對應線的中間位置，即
Virtual image 中虛線的位置。合成影像的資
訊則使用 Anchor image 的資訊來取代，如圖
(21)所示。 
 
 
 
 
 
 
 
 
 
 
 
 
圖(21) Synthesis scheme 
最後，我們再利用攝影機的內部參數 A
及外部參數 R、T 將合成的虛擬影像根據我
們的需求轉換成圖(19)的 Is影像。利用這樣
的方式，我們可以得到任意兩個攝影機之間
的虛擬影像，擴大視角範圍。 
最重要的實驗結果部份，我們使用以下
一系列圖片說明鄰近攝影機之間畫面合成
結果。圖(22)和(23)為原始未經過校正的兩
個鄰近攝影機的影像，圖(24)和(25)為經過
Prewarp 後的校正影像，圖(26)和(27)為兩個
鄰近攝影機之間所合成的虛擬影像。圖(26)
和 (27)的 0.3 和 0.5 是指 Camera_0 和
Camera_1 之間的距離中，權重為 0.3 和 0.5
的位置攝影機所預期看到的虛擬影像。我們
利用有限的攝影機所拍攝的影像，配合影像
合成的技術來擴展視角範圍。讓原本只能看
到固定視角的影像或影片，推廣至達到任意
自由視角的播放。 
 
 
圖(22) Camera_0_Image 
 
 
圖(23) Camera_1_Image 
 
