非監督式中文寫作自動評閱系統之研究 
An Unsupervised Chinese Automated Essay Scoring System 
計畫編號： 
執行期限：97年 8月 1日至 98年 7月 31日 
主持人：李嘉晃   國立交通大學資訊工程 
 
一、中文摘要 
    寫作自動評閱技術在語文教育領域中是
十分重要的輔助工具，然而目前的自動評閱
系統均需要一定數量的同主題文章做為訓練
資料，因而在使用上有其限制性。本計畫提
出一個基於文章相似度的中文寫作自動評分
系統，此系統不需要同主題文章的人工評分
資料，僅需一定數量的測試資料，便可藉文
章間的相似度及其表面特徵資訊自動進行評
分。另外，本計畫亦提出一個演算法可以偵
測不合題旨的文章，以避免評分的錯誤。實
驗結果允許一級分誤差的正確率可達 95%，
完全命中的精確率可達 52%；可以做為人工
評分的參考依據。 
 
二、英文摘要 (Abstract) 
    Automated essay scoring is an 
important research in education domain, 
but the cost of training is a problem. In 
this study, we describe a Chinese 
automated essay scoring system based on 
similarity between test essays. This 
system could grade essays without graded 
training data. We also describe an 
algorithm detecting off-topic essays. 
The adjacent rate in the experiment is 
about 95%, and the exact rate is about 
52%. 
 
三、計畫緣由與目的 
    寫作能力是語文教育中相當重要的一環，
然而人工評閱所耗費的人力及時間成本十分
可觀，文章自動評閱（Automated Essay 
Scoring, AES）是利用人工智慧技術讓電腦
可以模仿人工批改作文的技術，自動評閱技
術可以大幅降低評閱作文時的成本，因此在
教育測驗、課程教學及心理計量等領域中可
說是相當重要的工具。  
    自動評閱技術在英文寫作方面有著長久
的發展，並已經實際應用在大型入學測驗及
寫作教學上，如美國教育測驗服務社
（Educational Testing Service, ETS）所
主辦的TOEFL、GMAT均使用自動評閱系統做為
閱卷時的輔助工具；其開發的寫作練習軟體
Criterion可以讓學生練習特定題目的寫作
並自動進行評閱。在中文作文的領域上，也
已經有數個系統被提出（［7］［8］［9］［
10］）。  
    然而，目前的中文自動評分系統皆需要
150~300篇以上經人工批閱評定分數的同主
題文章做為訓練資料，才能正確地進行自動
評分；當我們希望系統評閱一個新的題目時，
便需要蒐集一定數量該題目的訓練文章，由
專業人員評閱這些訓練文章的成績後，才可
建立起評分模型，之後電腦才能自動評閱此
主題的文章。其訓練過程不僅耗費人力與時
間成本，而且使用者必需有蒐集及評閱訓練
資料的能力，因此在使用上有很大的限制
性。  
    本研究之目的在建立一個不需要訓練資
料的中文自動評分系統。此系統不需要人工
評閱過的訓練資料，僅需要一定數量的同主
題文章，便可藉由文章的間接特徵資訊與文
章之間的相似度自動評閱測試文章之成績，
否合乎主題的機制，因此當不合題意的文章
的表面特徵夠好時，便有出現評分錯誤的可
能性，使得系統容易被有心人士破解。 
 
四、想法與討論 
    中文作文的評分標準，主要分為立意取
材、結構組織、遣詞造句及錯別字與格式等
項目。本系統僅探討文章取材方面的優劣程
度，並未深入文章語意方面之表現。  
    第一節將描述整個系統的架構與流程，
其後的四個小節將詳細介紹各個模組的內
容。  
(一) 系統架構  
 
    當大量的測試資料進入系統時，系統首
先對於每篇測試文章進行中文斷詞處理，將
文章切割為詞語的串列；接著根據各篇文章
的間接特徵給予一初期分數；以此間接特徵
評分之結果為初始狀態，本系統使用一個投
票演算法不斷修正評分結果，直到評分結果
收斂到穩定狀態為止；在此階段的評分結束
後，系統根據其結果建立一個相關詞集以偵
測離題文章；最後由測試文章在投票演算法
的評分結果，以及歷史資料的成績分配情形
決定文章的六級分成績。  
 
(二) 中文斷詞處理  
    中文斷詞是將連續的中文字轉換為詞的
組合，詞為一個或多個中文字的組合，是句
法和語意上的最小單位，斷詞在自然語言處
理上是相當重要的工作。相對於歐美語系中，
詞與詞之間有明確的分隔（空白及標點），
中文斷詞處理是困難得多的工作，目前並沒
有任何演算法可以達到百分之百的斷詞正確
率。  
本系統採用長詞優先法，即斷出剩餘字串中
最長的有意義字串，如「下課十分鐘」一句，
可斷成「[下][課][十][分][鐘]」、「[下
課][十][分鐘]」等不同的詞組，而長詞優先
法斷詞將將先斷出[下課]，然後再由剩餘的
「十分鐘」字串中斷出[十分]，最終結果為
「[下課][十分][鐘]」。  
    長詞優先法的正確率在已知的演算法中
雖然並非是最高的，但效果已相當不錯，且
本系統僅計算文章間共同出現的詞語，並不
探討文章和詞語的語意，因此對斷詞錯誤的
容忍度更高。  
    以前述「十分鐘」為例，雖然長詞優先
法會造成斷詞上的錯誤，而使得文意上產生
了不通順的狀況。但由共用詞語數的角度來
看，若是兩篇文章均寫到「十分鐘」一句，
則錯誤的「[十分][鐘]」和正確的「[十][分
鐘]」得出的結果均為兩個共用詞語，並不會
影響評分結果。  
 
(三) 間接特徵評分  
    本系統的基本假設為與高分文章越相似
的文章，越可能是高分文章，反之則越可能
是低分文章。但在系統起始時，測試文章並
沒有任何附加資訊，因此我們無從得知測試
文章的好壞。為了建立投票演算法的初始狀
態，我們需要給定每一篇測試文章一個初期
分數；又因為欠缺測試文章的直接特徵資訊，
只能根據其間接特徵資訊來給定此初期分
數。  
    本系統選用「相異詞語數」這項表面特
徵做為初期評分的依據，即文章所使用的詞
語種類數；例如「我最喜歡跟我的朋友玩」，
經斷詞後為「[我][最][喜
歡][跟][我][的][朋友][玩]」共八個詞，其
中[我]出現兩次，相異詞語數為七。若一篇
    S
c,1 
= Sim
a,c
*Z
a,c,0 
+ Sim
b,c
*Z
b,c,0 
+ Sim
d,c
*Z
d,c,0  
              
= 2*(-0.79)+3*(-0.79)+6*(1.57)  
       = 5.50  
    S
d,1 
= Sim
a,d
*Z
a,d,0 
+ Sim
b,d
*Z
b,d,0 
+ Sim
c,d
*Z
c,d,0  
              
= 3*(-0.47)+4*(-0.47)+6*(0.94)  
       = 2.36  
重複以上動作，最後系統將收斂於 S
a
=-0.70，
S
b
=0.59，S
c
=4.47，S
d
=3.80。 
 
●相似度計算  
    在本研究中採用兩文章的共用詞語數做
為文章間的相似度，若參考文章與目標文章
用到越多相同的詞語，則認為兩篇文章的相
似度越高；反之，則認為越不相似。 
 
    表一為不同級分的文章之間共用詞語數
的平均值，同一篇文章與本身的共用詞語數
不計。觀察樣本集中所有文章之間的共用詞
語數，普遍來說，大多數的文章與高分文章
的共用詞語數均較低分文章為高，主要原因
是高分文章使用的詞語數量較低分文章多，
因此詞語出現的機率亦較高；然而成績越高
的文章，共用詞語 數隨成績上升的幅度越大，
而低分文章對不同級分文章的共用詞語數差
距相對不明顯。  
    將表一中每欄的值減去該欄平均後，可
以得到圖四的結果。由圖可見一二級分文章
的校正後共用詞語數隨成績遞減，代表這些
文章與高分文章使用到相同詞語的機會相對
較低；三四級分文章呈現中間高兩端低的鐘
型，代表與同樣為三四級分的文章用到相同
詞語較高，而與高低分文章用到相同詞語的
機率較低；五六級分文章的校正後共用詞語
數隨文章成績上升，代表這些文章與高分文
章使用到相同詞語的機會較高。 
 
圖三.各級分文章間平均校正後共用詞語數 
 
    在單一詞語之外，我們亦嘗試加入共用
Bi-word雙詞組的資訊，以「 [下課][十
分][鐘]」與「[十分][鐘][的][下課]」為 例，
兩者的共用詞語為[下課]、[十分]、[鐘]，
共三個，僅使用詞語的相似度即為3，而共用
Bi-word數為"[十分][鐘]"一個，故在加入
Bi-word的系統中，相似度為4。  
    標點符號不視為詞語，但句首與句尾可
以視為 Bi-word的組成分子。如「 [下
課][了][！][同學][們][馬上][衝][出][教
室][。]」與「[鐘聲][一][響][，][同
學][又][走][回][教室][。]」兩句，便有"<
句首>[同學]"及"[教室]<句尾>"兩個共用
Bi-word。  
    加入 Bi-word詞組之資訊可以提供系統
些許訓練文章的文法資訊，避免詞袋式評分
無視詞語間關聯性及詞語出現位置的問題。
實驗結果顯示加入 Bi-word資訊可以提高系
統對高分文章的鑑別力。 
●討論 
    為了分析使用詞語對文章的影響，以採
用共用詞語數為相似度的系統來說，我們可
以將公式改寫為： 
 
 
 
圖四.左右對稱分佈之樣本 
 
圖五.左偏分佈之樣本(低分文章比例降低) 
    以上圖為例，在第一 Z-Score門檻值設為
-1.73的情況下，左右對稱的樣本有5.3%的文
章會被評為一級分，而在低分文章比例下降
之樣本中，同樣的門檻值僅會使得3.7%的文
章被評為一級分。若是採用名次做為評分標
準，則不管訓練資料的分布如何，均會使分
數最低的5.3%的文章評為一級分。  
    採用Z-Score為基準的評分結果與實際
分佈較為接近，這就是我們採用Z-Score而非
名次比例做為六級分評鑑的標準的原因。  
 
(六) 離題文章偵測  
    有一些寫作技巧優秀卻不合題意的文章，
這些文章的間接特徵往往十分優異，而直接
特徵並不符合題目的要求；當這類文章夾雜
在測試文章中進入自動評分系統時，便有可
能造成系統的誤判。造成使用者可以靠著這
個弱點來欺騙系統，以取得不適當的高分，
因此我們需要一個機制來偵測這些文章，以
增強系統的完備性。  
    寫作所使用的詞語可分為「主題」和「修
辭」兩部份，這類離題文章在主題相關的詞
語與其餘測試文章普遍不同，故在本系統中
不致被評為高分，但在修辭方面卻經常與高
分文章用到相同的詞語，因此仍可得到一定
的分數，有可能被評為中等文章；而文章的
修辭技巧為高等文章與中等文章的主要差異，
故不可能將修辭部份的評分由系統中剔除。  
    在有訓練資料的系統，如CMU［5］和Allan
等人［1］在新主題偵測（New event detection）
上所使用的方法，通常是利用訓練資料的直
接特徵資訊來處理離題文章的問題，當測試
文章與訓練資料有顯著差異時便可認定其為
離題文章。我們使用相同的概念來處理無訓
練資料的離題文章偵測，我們認為若一篇文
章使用的詞語與高分文章有顯著的差異，則
此篇文章應為離題文章。此演算法必須設定
三個門檻值：詞頻門檻值x、比例門檻值y及
詞語門檻值n。  
    在所有文章經過投票演算法得到系統評
閱的分數後，計算詞語在分數高於平均值的
文章中出現的比例GDF，其值大於詞頻門檻值
x的詞語，我們認為是和題目相關的。根據實
驗的結果，詞頻門檻值x定在15%到25%之間較
為恰當。  
    然而有些常用詞會廣泛地出現在各種主
題的文章中（例：的、可以、因為…），雖
然其GDF高過詞頻門檻值，但並非與目前的特
定主題相關，為了刪去這些詞語，我們由中
研院平衡語料庫［6］中選取了876篇無特定
主題的文章，計算所有詞語在該語料庫出現
的文章頻率CDF。  
    一個詞語要被認定是相關詞語，其GDF
除了要高過必須詞頻門檻值x外，也要大於其
CDF的y倍，才代表和題目有特殊的相關性。
經實驗觀察結果，比例門檻值y定於4~8之間
較為適當。  
    得到相關詞語集後，便可計算每篇文章
出現的相關詞語數，若相關詞語數量未超過
詞語門檻值 n，代表此文章並未完整描寫題
目的意涵，則認定此篇文章為不合題意的文
章；反之，則認為是合乎題意的文章。 
 
(七) 實驗 
[一]投票系統 
●實驗資料 
    本實驗使用的資料為台灣都會區及非都
會區的各三所學校國二學生所撰寫的作文，
題目為「下課十分鐘」，所有資料共有 689
●分群實驗 
    為了探討測試文章數量對評分效果的影
響，我們將 346篇測試文章均分為五群，各群
的文章數量與成績分佈均相近，再以不同大
小的環狀滑動窗選取輸入之測試資料範圍進
行多次實驗，計算不同測試文章數時的正確
率及精確率。（例：群數大小設定為2時，進
行5次實驗，測試資料分別為群1&群2、群2&
群3、群3&群4、群4&群5、群5&群1，其餘同
理。） 
●分群實驗結果與討論  
    分群測試之結果如下表： 
 
    詞語+Biword 系統的詳細實驗結果如下
圖，直條圖表示個別實驗的正確（ 精確）率，
折線圖表示在各群數下的平均正確（精確）
率： 
 
 
圖六.分群測試結果比較圖 （詞語+Biword） 
    由表五及圖六可見，本系統的效果受到
樣本數多寡的影響，對於加入Biword資訊的
系統而言，由於有資料稀疏的狀況，此情況
更為嚴重。  
    本系統在樣本數不足 210篇時，平均正
確率和平均精確率均不理想，且可能出現很
糟糕的結果，表現並不穩定，但當測試文章
數達到 280篇後，系統效果便趨於穩定，不
易出現很低的正確率或精確率。 
 
[二]離題文章偵測 
●實驗資料  
    我們蒐集了 20篇與「下課十分鐘」無關
之文章，內容包括中學國文課文、新聞、小
說、散文作品、學術論文、以及流行歌歌詞
等，各文章的相 異詞語數在143到261之間。
將此20篇離題文章加入原本的346篇測試文
章中做為離題偵測實驗的測試資料。 
●實驗流程  
    在投票系統方面，先將366篇測試文章輸
入投票系統，得到系統評閱的分數，區分出
高分文章群後，再由離題偵測系統根據文章
使用的詞語及三個可調整的門檻值判斷是否
為不合題意之文章。  
    做為對照組的四個系統不需要其他測試
文章，因此直接將 20篇離題文章做為測試資
料輸入系統。 
 
●效果比較 
 
    在資料輸入離題偵測系統之前，先觀察
投票系統評閱離題文章的結果，由於此演算
法基於文章相似度的特性，20篇離題文章在
投票系統結束時多被評為一到三級分；在僅
使用詞語的系統中，被評為四級分以上者僅
有兩篇，在加入 Biword的系統中更是一篇也
沒有。與其他系統有 70%到 90%被評為四級分
以上的結果相比，顯然較為優秀。 
●評鑑方式  
Melbourne, Australia, 1998.  
  ［6］ 中研院平衡語料庫3.1版  
  ［7］ 林信宏，「基於貝氏機器學習法之
中文自動作文評分系統」，國立交通大學，
碩士論文，2006。  
  ［8］ 張佑銘，「中文自動作文修辭評分
系統設計」，國 立交通大學，碩士論文，2005。  
  ［9］ 粘志鵬，「基於支援向量機之中文
自動作文評分系統」，國立交通大學，碩士
論文，2006。  
  ［10］ 蔡沛言，「自動建構中文作文評分
系統：產生、篩選與評估」，國 立交通大學，
碩士論文，2005 
[11] Chien-Liang Liu and Chia-Hoang Lee. 
Simplify Multi-valued Decision Trees, The 3rd 
International Symposium on Intelligence 
Computation and Applications (ISICA 2008), 
LNCS 5370, 581–590 
[12] Chang, T. H., Lee, C. H., Tsai, P. Y. & 
Tam, H. P. (2008). Automated essay scoring 
using set of literary sememes. Paper presented 
at the IEEE International Conference on 
Natural Language Processing and Knowledge 
Engineering, 403-407, Beijing, China. 
[13] Chang, T. H., Tam, H. P., Lee, C. H. & 
Sung, Y. T. (2008). Automatic Concept Map 
Constructing Using Topic-specific Training 
Corpus. Paper presented at APERA 2008, 
Singapore. 
[14] Chang, T. H., Lee, C. H., Tsai, P. Y. & 
Tam, H. P. (2009). Automated essay scoring 
using set of literary sememes. accepted by 
Information: An International 
InterdisciplinaryJournal,12(2).
The 3th International Symposium on Intelligence 
Computation and Applications (ISICA 2008) 
 
研討會 
出國報告書 
 
 
 
 
 
 
 
 
 
姓名：劉建良 
會議舉辦地點：大陸湖北省中國地質大學 
會議日期：December 19-21, 2008 
 
 
Simplify Multivalued Decision Trees
Chien-Liang Liu and Chia-Hoang Lee
Department of Computer Science, National Chiao Tung University 1001 University
Road, Hsinchu, Taiwan 300, ROC,
clliu@mail.nctu.edu.tw
Abstract. Decision tree is one of the popular machine learning algo-
rithms and it has been applied on many classification application areas.
In many applications, the number of attribute values may be over hun-
dreds and that will be difficult to analyze the result. The purpose of
this paper will focus on the construction of categorical decision trees. A
binary splitting decision tree algorithm is proposed to simplify the clas-
sification outcomes. It adopts the complement operation to simplify the
split of interior nodes and it is suitable to apply on the decision trees
where the number of outcomes is numerous. In addition, meta-attribute
could be applied on some applications where the number of outcomes
is numerous and the meta-attribute is meaningful. The benefit of meta-
attribute representation is that it could transfer the original attribute
into higher level concept and that could reduce the number of outcomes.
1 Introduction
Over the last few decades, machine learning has become a very active and impor-
tant research area nowadays. Many machine learning algorithms are proposed
and have proven their capabilities on intelligent systems. Decision tree is one
of the popular machine learning algorithms and it has been applied on many
classification application areas. In essence, decision tree is simple to understand
and the inference result is easy for humans to interpret. Besides, decision tree is
robust and it performs well with large data set.
Decision tree is a supervised learning model and the input to the classifier
are attribute-value pairs tagged with class labels. Each interior node corresponds
to a test on the variable and an arc to a child represents a possible value of
that variable. A leaf represents the classification of records. When the classifier
finishes the training phrase, unknown records could be classified.
In general, decision tree construction includes tree-building phase and tree-
pruning phase. In tree-building phase, the training data is recursively partitioned
until all the records in a partition have the same class. In each partition, a new
attribute should be selected as the splitting criterion and new sub-partitions
are collected according to the splitting criterion. The same process is applied on
these sub-partitions recursively. Appropriate attributes have to be selected as the
root nodes in each partition and there are several approaches to the selection of
attributes. ID3 [1] adopts information entropy [2] and information gain on the
3In practice, greedy algorithms based on local search are used to yield reason-
able trees in reasonable time. In the followings, two decision tree construction
mechanisms are described.
2.1 ID3
The goal of classification decision trees is to construct a classifier from the given
training data and predict the class label of unknown data. The classifier classifies
the training data based on the homogenous level of the data. The more similar
they are, the most likely they will be grouped on the same class. Thus, the
attribute which could classify the data into homogeneous group will be preferred
when constructing decision trees. ID3 adopts information gain to evaluate the
level of homogeneous among the data. The one with the highest information
gain is selected as the interior node. In theory, the information gain is based on
entropy which could be used to measure the uncertainty of a random variable.
The entropy H(X) of a random variable X is defined by:
Entropy(X) = −
∑
x∈X
P (x) logP (x)
In decision tree, supporse that there are Nm instances reaching node m, and
N im belongs to class Ci. The entropy of node m will be: (p
i
m is the probability
of Ci)
pim =
N im
Nm
Im = −
∑
i=1
pim log p
i
m
In ID3, the information gain is based on the impurity after the splitting.
Assume that the number of instances taking branch j is Nmj and N
i
mj belong
to Ci. The impurity after the splitting will become:
pimj =
N imj
Nmj
I
′
m = −
l∑
j=1
Nmj
Nm
k∑
i=1
pimj log p
i
mj =
l∑
j=1
Nmj
Nm
Imj
5Table 1. Play Tennis Example Data
Day Outlook Temperature Humidity Wind Play Tennis
D1 Sunny Hot High Weak No
D2 Sunny Hot High Strong No
D3 Overcast Hot High Weak Yes
D4 Rain Mild High Weak Yes
D5 Rain Cool Normal Weak Yes
D6 Rain Cool Normal Strong No
D7 Overcast Cool Normal Strong Yes
D8 Sunny Mild High Weak No
D9 Sunny Cool Normal Weak Yes
D10 Rain Mild Normal Weak Yes
D11 Sunny Mild Normal Strong Yes
D12 Overcast Mild High Strong Yes
D13 Overcast Hot Normal Weak Yes
D14 Rain Mild High Strong No
This represents the potential information generated by dividing Nm into
k subsets, whereas the information gain measures the information relevant to
classification that arises from the same division. Then, gain ratio expresses the
proportion of information generated by the split that is useful and it is defined
as follows:
GainRatio(m) = gain(m)/SplitInfo(m)
The attribute with the maximum gain ratio is selected as the interior node.
According to Quinlan’s experiment [3], the gain ratio criterion is robust and
typically gives a consistently better choice of test than the gain criterion [1].
3 Multivalued Decision Tree
As described above, ID3 prefers the attributes with many outcomes and C4.5
overcomes this problem by using normalization of the information gain. Although
the normalization could reduce the bias of attributes with many outcomes, it is
still possible to choose the attribute with many outcomes as the interior node
in some circumstances. The number of rules generated by the decision tree is
associative to the number of selected attribute values. When the attributes with
many outcomes are selected as the internal node, the number of rules will be-
come numerous. However, the attribute with many outcomes is very common
in real applications. In the following, two multi-value attribute scenarios will be
described.
7Fig. 2. Decision tree splitting node with 10 outcomes
the user profile database to find out the user behavior patterns, address infor-
mation may be included in the analysis. In practice, city is one of the address
attributes and the number of cities may be over hundreds. Instead of adopt-
ing city attribute directly, meta-attribute concept could be applied to reduce
the number of attribute values. The grouping of the attribute value could make
the attribute value become more general concept and that may provide more
meaningful information as well. In this paper, an algorithm which adopts meta-
attribute to solve multi-values issues is proposed.
4 Binary Categorical Splitting
In this paper, a binary categorical splitting heuristic approach is proposed to re-
duce the number of subsets when the application contains attributes that consist
of many outcomes. The approach proposed in this paper splits the original set
into two subsets. One of the subsets is based on one of the attribute values and
the other is based on its complement. The gain ratio is adopted to represent the
homogenous level. For the node containing attribute with N values, there will
have N pairs. The highest gain ratio among these pairs will be selected. For ex-
ample, in the tree structure of the Fig. 2, there are ten paris (A1, NOT A1), (A2,
NOT A2), ..., and (A10 , NOT A10) and their corresponding subsets are (S1,
S−S1), (S2, S−S2), ..., and (S10, S−S10 ) respectively. Fig. 3 and Fig. 4 show
the result when the splits are applied on A1 and A10 respectively. The gain ra-
9Fig. 4. Decision Tree Splitting on A10 and Not A1p
Algorithm 1 Decision Tree Generation Algorithm
procedure GenerateT ree(X)
1: if NodeEntropy(X < θ) then
2: Create leaf labeled by majority class in X
3: return
4: end if
5: i← SplitAttribute(X)
6: for all branches of Xi do
7: Find Xi falling in branch
8: GenerateT ree(Xi)
9: end for
11
tion, every categorical attribute values could be dispatched to specific group by
considering the range group of their corresponding digital number.
Fig. 5. Mapping between the attribute values and the digital numbers
6 Experiment
In this section, a ”sunburned” example is used to demonstrate the use of comple-
ment attribute values. Table 2 shows that, there are name, hair, height, weight,
lotion attributes and a class label (sunburned or none) for each record. Fig. 7
shows the result of decision tree using C4.5 algorithm and the number of rules is
4. In the other hand, the complement operation is applied and Fig. 8 shows the
final result. Compare the final rules generated by these two models, two rules
in Fig. 7 are replaced by one complement rule in Fig. 8. The original rules de-
scribe the color of hair in a specific way (red and blonde) while the complement
one describes the color in a set way (not brown). Instead of specific meaning,
set representation provides more general concept and that may help the users
discover new concept from the complement operation. Furthermore, the “Spli-
tAttribute” procedure described in Algorithm 2 could be modified to incorporate
with other decision tree algorithms. Binary categorical split is applied only when
some criterion meets and the others adopt original splitting algorithms.
In many countries, the number of countries is over hundreds. In Taiwan,
there are 15 cities and there will have 15 branches if city attribute is selected
as a splitting criterion. In general, Taiwan cities could be categorized by their
geographical location. Based on the geographical location, the attribute name
could be transformed into corresponding digital number as shown in Table 3.
13
Table 2. Sunburned Example Data
ID Name Hair Height Weight Lotion Result
1 Sarah Blonde Average Light No Sunburned
2 Dana Blonde Tall Average Yes None
3 Alex Brown Short Average Yes None
4 Annie Blonde Short Average No Sunburned
5 Emily Red Average Heavy No Sunburned
6 Pete Brown Tall Heavy No None
7 John Brown Average Heavy No None
8 Katie Blonde Short Light Yes None
Table 3. Taiwan City
List
City Name ID Location
Taipei 1 North
Yilan 2 North
Taoyuan 3 North
HsinChu 4 North
Miaoli 5 North
Taichung 6 Central
ChangHua 7 Central
Nantou 8 Central
Yunlin 9 South
Chiayi 10 South
Tainan 11 South
Kaohsiung 12 South
PingTung 13 South
Taitung 14 East
Hualien 15 East
Table 4. Taiwan City Ordering Table
Location ID Range
North ID ≤ 5
Central 6 ≤ ID ≤ 8
South 9 ≤ ID ≤ 13
East 14 ≤ ID ≤ 15
Table 4 shows that the location of the city could be identified from the range of
“ID” value. For example, if the “ID” is less than 6, it could be transformed into
“North”. Fig. 9 shows the reduced decision tree where the number of branches
becomes 4.
7 Conclusion
In this paper, two algorithms are proposed to reduce the number of outcomes
when the number of attribute values is numerous. In the first binary categorical
splitting algorithm, node splitting is based on attribute value and its complement
value. Gain ratio is used to compute the homogenous level and the highest
pair will be selected as the splitting criterion. Instead of a specific attribute
value splitting approach, it performs the splitting in set way. In addition to
the simplification of the tree structure, it may help users discover unknown
concept from complement operations. Moreover, complement operation could
15
also be incorporated with existing decision tree splitting algorithms to reduce
the complexity of outcome at specific node.
As for meta-attribute grouping, some attributes could be grouped into higher
level concept practically. In this paper, the categorical attributes are transformed
into digital number and the order of these digital numbers are based on their
meta-attribute concepts. Thus, the range of the digital number could be used to
represent the meta-attribute. The benefit of this approach is that it could provide
more conceptual representation and it could reduce the number of outcomes
dramatically.
References
1. Quinlan, J.R.: Induction of decision trees. Machine Learning 1(1) (3 1986) 81–106
2. SHANNON, C.E.: A mathematical theory of communication. Bell System Tech-
nical Journal 27 (1948) 379–423, 623–656
3. Quinlan, J.R., ed.: C4.5: Programs for Machine Learning. Morgan Kaufmann
Publishers Inc. (1993)
4. Breiman, L., Friedman, J., Stone, C.J., Olshen, R.: CLASSIFICATION AND
REGRESSION TREES. Chapman & Hall/CRC (1984)
5. Mehta, M., Agrawal, R., Rissanen, J.: Sliq: A fast scalable classifier for data mining.
In: Proc. of the Fifth International Conference on Extending Database Technology.
(1996) 18–32
6. Shafer, J.C., Agrawal, R., Mehta, M.: Sprint: A scalable parallel classifier for data
mining. In: Proc. 22nd Int. Conf. Very Large Databases, VLDB. (1996) 544–555
7. Fayyad, U.M., Irani, K.B.: Multi-interval discretization of continuous-valued at-
tributes for classification learning. In: Proc. of the 13th International Joint Con-
ference on Arti cial Intelligence. (1993) 1022–1029
8. Mehta, M., Rissanen, J., Agrawal, R.: Mdl-based decision tree pruning. In: Pro-
ceedings of the First International Conference on Knowledge Discovery and Data
Mining (KDD’95). (1995) 216–221
9. Quinlan, J.R., Rivest, R.L.: Inferring decision trees using the minimum description
length principle. Information and Computation 80(3) (1989) 227–248
10. Wallace, C.S., Patrick, J.D.: Coding decision trees. Machine Learning 11(1) (1993)
7–22
11. Quinlan, J.R.: Simplifying decision trees. International Journal of Man-Machine
Studies 27(3) (1987) 221–234
12. Garofalakis, M., Hyun, D., Rastogi, R., Shim, K.: Building decision trees with
constraints. Data Mining and Knowledge Discovery 7 (4 2003) 2
