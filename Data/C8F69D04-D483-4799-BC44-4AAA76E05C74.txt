2 
 
影像分析於三維手指重構與運動量測 
 
研究成果報告: 
 
本計畫為兩年期之研究計畫，主要內容以發展”影像分析於三維手指重構與運動量測”之
技術與實用系統為主。本計畫於每一年度發展出不同階段之突破性技術，並輔以開發實際應
用系統，以達到臨床應用之目的。 
 
本研究可分為以下兩個階段； 
 
第一階段以單指運動分析系統為主，題目為 ”以模型為基礎之電腦手指連續運動影像分
析系統”。第二階段延續第一階段之成果，發展出全手指之運動分析系統，題目為” 使用多
台相機的三維手指影像運動分析系統”。所得電腦視覺量測系統已經部份臨床實驗測試，並
將再改善後作為臨床診斷輔助工具。其部分成果發表於國內與國外研討會上，共計三篇。進
一步之成果也經過整理之後，投稿國際期刊審查中。以下簡介兩階段之研究內容： 
 
Part I、 以 模 型 為 基 礎 之 電 腦 手 指 連 續 運 動 影 像 分 析 系 統  
     
三維手部動作的分析與量測對手部疾病之診斷與治療能提供很大的幫助。在臨床上，醫
師以專業經驗指定手部動作與進行靜態的手指角度量測；至於手部動態運動分析的研究，目
前通常利用受試者在指定動作下之螢光攝影影像進行參數分析，而缺乏整體動態的三維模型
以提供參考比對。本研究之主要目標是以研究如何建立患者手指部位之三維模型以及量測其
連續運動；經由低成本攝影機之連續動態影像，利用電腦視覺技術建立三維虛擬手指模型，
發展一套整合之三維手指動作分析與量測系統，取代昂貴的運動分析設備與提供可靠的手部
動態資訊以協助外科醫師對患者病灶區域進行臨床診斷、治療規劃與術後評估。 
 
    我們利用電腦視覺的三維資訊重構技術與相機校準技術來計算標記點在空間中之三維
資訊，然後依據手部運動學模型去計算相關指間角度值與手指長度值之參數，同時利用基本
的立體幾何模型建構虛擬手掌三維動態模型。擁有上述之靜態與動態虛擬手掌模型後，我們
結合影像資訊與三維標記點位置，採用創新的 Marker-guided particle filter 演算法實現手部運
動的動態追蹤，並可以即時計算出所有的最佳手部參數值。為了提升模型的精確性，本研究
最後會將 MRI 所得到的三維手骨實際放入所建構之虛擬指塊模型中；如此，我們可以交替
驗證手傷患者在動作時之骨骼之動作狀況，量測出動態運動參數或評估相關復健情況，同時
對手指病變也可以用定性與定量方式做出客觀分析與診斷。 
 
由實驗顯示，本論文所提的系統在運算速度和精準度上都有不錯的結果，可對表皮標記
點資訊做補償，這是目前使用的運動分析系統無法做到的。而且本方法僅需市售的數位相
4 
 
做更多的分析和評估。 
 
 
 
下圖是整個系統流程的示意圖。其主要步驟如下： 
 
(1) 首先安裝硬體系統，並對兩台攝影機做相機校準，之後便由受測者開始手指動作，
由相機做取像。 
(2) 首先我們會先對 t 時間點的影像做前處理，取出影像上的特徵資訊(標記點中心位
置、手部前景、手指輪廓)，進而由多台相機之二維標記點中心重構出三維標記點
位置。並且由 3D 標記點位置以最佳化方式初始化模型參數。(此步驟需利用 mean 
shift 演算法追蹤二維標記點位置)。 
(3) 利用 marker-guide particle filter 估計模型參數：模型參數的預測是連續的，假設我
們已經有了前一個時間點 t-1 的最佳模型參數與其產生之可能模型參數(每一組參
數為一粒子: particle)；若欲預測目前時間點 t 的模型參數，我們會先利用 t 時間點
標記點所定之初始化模型參數與前一個時間點 t-1 的粒子產生新的可能模型參數(t
時間點的粒子)，後根據 t 時間點的影像資訊來計算出每個粒子對應到的權重，而
其 t 時間點的最佳模型參數便由這些粒子之權重和(weighted sum)來表示。 
(4) 如欲預測下一個時間點 t+1 的模型參數，只要因循上列 (2)(3)步驟就可以求得。 
 
 
 
6 
 
3D 模型可以定出手指模型之中心軸。經由這些訊息可以計算出真正由專家所定之手指運動
角度(Flexion angles)作為真實值(Ground truth)，與由標記點與 3D 模型所計算出之運動角
度，並以統計方式對誤差做評估與得到結論。圖 C 尤其展現 3D 模型之中心軸與投射到螢光
影像平面之結果。結果顯示利用 3D 手指模型的確可以更正確的估測出手指運動角度(比用標
記點計算所得更接近真實值)，詳細數據與討論於發表論文中。 
 
下圖為三維的手指模型參數與座標軸 
 
圖 A 
 
驗證軟體開發： 
 
本系統利用模型的圓柱體來表示出整個骨節的位置，因此圓柱體的中心線(Cylinder of motion 
analysis system，簡稱 CM:虛線箭頭所示)，和 BF(骨節中心線)是相互對應的。圓柱體中心線
(CM)的資料是由利用影像上資訊，對 SM 資料: 實線箭頭所示)做補償修正的結果。如果 CM
與 BF 之間的的誤差比 SM 與 BF 的誤差來得小，就能彰顯本系統的實用價值。誤差的分析，
將是本驗證實驗的重點 
 
8 
 
   
(a) 
 
  (b) 
圖 C:   標記點中心連線與圓柱中心線的投影 (a)拇指(ABD/ADD) (b)食指(F/E) 
 
 
統計結果如論文所述 (CVGIP 2006, EMBS 2007) 
 
本研究的貢獻： 
1. 設計符合人體運動學構造的虛擬三維手指模型： 
醫學工程論文指出，將兩個表皮標記點放置在一個骨節(bone segment)上足以表示一個
骨節的位向，而且可以反映出手指皮膚在動作時發生的皮膚滑動、拉長和縮短的現象。
為了提供臨床上有用的診斷訊息，因此我們捨棄了資訊工程領域用來辨識手形的手指
模型，考慮醫學工程的實手運動模型，設計出符合人類手部運動學的三維虛擬手指模
m3
m2
m1
C1
C2
C3
C3
C2
C1m1
m2
m3
10 
 
 
Part II、 使 用 多 台 相 機 的 三 維 手 指 影 像 運 動 分 析 系 統  
 
    依據醫學上的研究，手部傷害與手部的運動有很大的關聯。三維手部動作的分析與量測
對因傷害引起之手部功能性障礙之診斷與治療能提供很大的幫助。傳統在臨床上，醫師用 X
光影像來診斷手傷病患，但是在 X 光拍攝的時候只有二維的投影資訊，並且只有提供靜態
的資訊。近年來，雖然有即時的動態螢光攝影影像，但是其所提供的資訊仍然只有二維平面，
顯然對於提供整體手傷運動之評估與診療上略有不足。有鑒於此，本系統藉由電腦視覺技術
建立三維虛擬手指模型，發展一套整合之三維手指動作分析與量測系統，並利用改良後的機
率追蹤演算法，在計算成本與量測精確度的要求上提供不錯的解決方案。然而手部動作具有
高自由度及高度限制性的特性，因此在一般的追蹤系統中，常常會因為姿態與動作的不同，
而有遮蔽現象的產生。本研究藉由使用多台相機來降低遮蔽效應的現象藉以增加量測手指運
動參數的精確性與系統之實用性。 
 
   目前醫院引進的運動分析系統可以精確的追蹤標記點的三維資訊。然而標記點所能提供
的資訊有限，不能代表真實手指的自由度，指節與指節之間的交錯位移等資訊。我們的系統
以三維模型為基礎，整合標記點資訊來預測模型參數，所以必須同時追蹤標記點和模型參
數。因此，本研究的重點主要分為兩部份：第一個是預測標記點三維資訊，第二個是預測模
型參數。在預測標記點三維資訊部分，我們藉由視覺技術極點限制和空間上的幾何限制解決
了因為多台相機而產生的複雜對應性問題。同時我們改善了傳統的 Mean-Shift 追蹤演算法來
防止標記點在時間序列上產生過大的跳動現象。最後我們提出了一套自動選擇視野的重構方
法排除了不可靠的視野，並且經過實驗大部分重構誤差均較兩台相機為低。 
 
   在預測模型參數部份，我們改善了傳統的粒子濾波器演算法，藉由動態整合由標記點定
義的模型參數與前ㄧ時間點的最佳參數使得權重較大的粒子能夠有效的被取樣出來。為了增
加系統的可靠度，我們藉由標記點資訊改善了粒子濾波器的 Weighting Function，並加入了限
制項來濾除不合理的參數。本研究最後會將 MRI 所得到的三維手骨實際放入所建構之虛擬
指塊模型中，可以嘗試觀察手傷患者在動作時骨骼之間的運動狀況與旋轉角度等。  
 
    由實驗結果顯示，本研究不但在單指的食指彎曲動作有不錯的追蹤結果，多指的對指動
作與全指的握球動作等均有不錯的追蹤結果。藉由我們的系統可以量測到不同手指的運動參
數，例如骨節與骨節之間的旋轉角度、交錯位移，每個骨節的長度，每個骨節的座標軸，運
動軌跡，角速度與角加速度等資訊，同時藉由多台相機系統也大大降低了遮蔽效應的現象。    
本研究提供了一個動態分析手指運動的系統，利用此系統，醫生可以方便的記錄病患術前術
後的動態資訊，進而建立完整的手部動態病例。 
 
 
 
 
12 
 
 
自行開發系統與成果 
 
我們的系統在此階段以展現多指運動追蹤分析為主。以下為自行開發系統之使用介面介紹，
此多指系統目前速度雖未達到即時分析追蹤(每秒處理約 2 frames)，但可以提供五指運動的
分析。軟體由 Visual C++ 6.0 , OpenGL 語言開發，並展現全手指 3D 模型，主要用以分析手
部功能性運動時，手指之交互與合作情形。 如圖 D，介面中顯示我們所依據的多視角的雙
指運動影像(pinch motion)，並標出三維模型投影到每一影像平面之訊息。圖 E 為五指運動
(grasping motion)。圖 F 為置入 MRI 的手指骨後所得，用以了解真正的手指骨運動現象。圖
G 為三維資訊重構誤差之比較: 與前一個單指運動系統相比。圖 H 為食指與拇指協同運動
之軌跡。圖 I 展現手指運動時之瞬間旋轉軸(instantaneous rotational axis)的方向。這些資訊對
於手部功能的判斷非常重要，足以提供更契合臨床需求的訊息。相關數據與討論於發表論文
中。 
 
 
 
 
圖 D:  多指實驗圖 
 
14 
 
 
                 (c)                               (d) 
 
                 (e)                                (f) 
圖 G :  多台相機與兩台相機重構誤差對照圖 
 
¾ 運動軌跡分析 
 
(a) 指尖運動軌跡(finger tip motion trajectory)： 
    對指(Pinch)動作時，拇指的指尖(Finger Tip)以及食指的指尖(Finger Tip)之三維運動軌跡
變化。由圖可以看出在對指(Pinch)運動時，拇指和食指的指尖確實是逐漸靠近中，而在最後
到達距離最近的位置。 
 
 
16 
 
   由圖 I 可以看出在對指(Pinch)運動時，拇指 IP joint 的運動軌跡以及兩種軸向變化。 
 
統計與驗證結果如論文所述 (ICMM-16 2008) 
 
本研究的貢獻 
 
1. 建立多台相機系統： 
       手指的運動是具有高自由度、高度限制的特性，所以往往會因為手姿態 
   與動作的不同極容易產生遮蔽現象。我們使用四台相機來追蹤標記點與模型 
   參數，將大大的降低遮蔽現象。我們解決了多台相機所會遇到的問題： 
(a) 我們提出了一套自動選擇視野機制，能夠排除因為相機角度的不同而產生的追蹤不穩
定性以及遮蔽問題，自動選擇可靠視野來重構標記點三維資訊。 
(b) 我們藉由視覺技術極點限制與空間上的幾何限制提出了一個在多重視野間標記點的
對應策略。 
(c) 我們藉由影像資訊和視覺投影建立了不可靠視野的標記點的補償機制。 
 
2. 改善追蹤演算法： 
       我們的系統以三維模型為基礎，整合標記點資訊來預測模型參數，所以必須同時追 
   蹤標記點和模型參數。本論文同時改善了標記點的追蹤方法與模型參數的追蹤方法： 
(a) 我們以 Modified Mean-Shift 演算法取代傳統的 Mean-Shift 演算法來追蹤標記點，我們
增加了 Correlation 項與 Damping 項以減少標記點在時間序列上不穩定的跳動現象。 
(b) 在追蹤模型參數方面，我們改善了傳統的粒子濾波器演算法，藉由動態整合由標記點
定義的模型參數與前ㄧ時間點的最佳參數使得權重較大的粒子能夠有效的被取樣出
來。為了增加系統的可靠度，我們藉由標記點資訊改善了粒子濾波器的 Weighting 
Function，並加入了限制項來濾除不合理的參數。 
 
3. 提供更多可用的動態資訊： 
我們提供的是一個全指的動態分析手指運動系統，可以量測骨節和骨節之間的旋轉 
   關係，骨節和骨節之間的交錯分離情形，每個骨節的長度，每個骨節的座標軸，每根手 
   指的運動軌跡，角速度與角加速度等資訊，由此可以測得大部份臨床上所需之手部運動 
18 
 
Computer Vision & Graphic Image Processing Conference (CVGIP 2006) 
 
MODEL-BASED VIDEO ANALYSIS SYSTEM FOR  
ARTICULATED FINGER MOTION 
 
Yen-Ting Chen (陳彥廷), Chung-Wen Chang (張瓊文) ,  
Chia-Hsiang Wu (吳佳祥) , Yung-Nien Sun (孫永年)* 
 
Dept. of Computer Science and Information Engineering 
National Cheng Kung University, Tainan, Taiwan 701 
Email: ynsun@mail.ncku.edu.tw 
 
 
ABSTRACT 
 
Computer aided motion analysis has been utilized to 
provide objective kinematics assessment of the human 
hand for clinical diagnosis and rehabilitation. It can 
provide detailed information that clinicians, therapists, 
researchers and engineers use to determine medical, 
therapeutic or orthotic/prosthetic clinic regimens. This 
study focused on constructing a synchronous 
model-based video system for analyzing articulated 3D 
hand motion. Since the relative motion between skin 
and bones exists during finger movements, we integrate 
reliable image and shape information to adjust and 
rectify model parameters directly derived from markers 
placed over the skin surface. The validity of our system 
is assessed with the synchronous X-ray fluoroscopy 
analysis. Experimental results show that the proposed 
method can reduce computational cost, and achieve 
more accurate tracking results than the conventional 
expensive motion analysis system. Therefore, our 
system has great potential to be a low-cost alternative 
for finger disease diagnosis and hand kinematics 
evaluation. 
 
 
1. INTRODUCTION 
 
The kinematics of hand has received significant 
attention. It can achieve several functional tasks of 
adaptation, exploration, prehension, perception and 
manipulation. The functional uniqueness of the human 
hand was anatomically and clinically studied in the last 
decade. The repeated strain or excessive use of fingers 
in work or hobby activities results in the fundamental 
disorder of hand, such as skier’s thumb, mallet finger. 
The motion assessment of fingers based on visually 
measured kinematics is helpful to surgeon for clinical 
diagnosis and evaluation. 
To quantitatively assess the objective kinematics of 
the hand, various methods have been proposed. 
Goniometric measurements [1] are simple and direct 
methods for only several specific postures. When it 
comes to the functional finger postures, this method 
fails due to the lack of finger dynamics. Motion-based 
system becomes a popular tool to record continuous 
fingers movement from digitized markers placed over 
the joints at present. In contrast to being assessed by a 
static system, patients can concentrate on moving 
fingers freely without attempting to force a single finger 
resting on some static postures. In order to validate the 
feasibility of the proposed model-based video analysis 
system, fluoroscopy is used to faithfully assess the joint 
kinematics based on X-ray image sequences penetrating 
the underlying bony structure. Kuo [3-5] verified the 
feasibility using motion analysis system to evaluate the 
area and angular variation of maximal fingertip motion. 
However, due to technical limitations in his study, 
fluoroscopy and video motion studies could not be 
executed simultaneously. Although the subjects perform 
high repeatability in test–retest trials, it is still doubtful 
if the surface markers placed on the surface segments 
can precisely represent the true joint movements of 
underlying bones [2]. 
The proposed system is based on visual tracking 
techniques. Recently, it has been proposed to track 
articulated human body model from set of synchronized 
video cameras. Model-based approaches [11-[15][17] 
compare the maximal similarity between 3D model 
projection and its observation to recover motion states 
describing the human pose. Over the past decades, 
sequential Monte Carlo simulation methods, such as 
particle filtering [10], has been widely used to provide a 
robust Bayesian framework for motion capture in a 
non-Gaussian system. However, it requires a huge 
number of particles to sample the high dimensionality of 
configuration space; Other than that, it is easy to lost 
track when motion between two successive frames is 
huge. The other approaches is appearance-based, it 
estimates motion states directly from images by learning 
the mapping from an image feature space to the human 
body configuration space [9]. Since it is impractical that 
the number of particles grows exponentially in high 
dimensional state space, Chang [18] integrates both 
sequential motion transition information and 
pre-collected appearance prior information to refine the 
performance of standard particle filtering. It 
significantly reduces the required samples and shows 
promising tracking results comparing with pure 
appearance-based approaches and standard particle 
20 
 
where 1{ ,..., }tM m m= , tm  notates a known motion 
state constructed from marker positions at frame t. Like 
standard particle filtering, a set of particles with 
associated weights, i.e. (1) (1) ( ) ( ){{ , },...,{ , }}N Nt t t tx xπ π are 
used to represent ( | , )t tp x Z M . The weights are 
chosen according to the principle of importance 
sampling [7]. Particles are easier to be drawn from 
importance density ( | , )t tq X Z M  than from original 
probability density ( | , )t tp X Z M . 
 
( | , )
( | , )
t t
t
t t
p X Z M
q X Z M
π ∝  (2)
 
Good choice of importance density can reduce the 
degeneracy problem [10]. Since the best choice for 
( | , )t tq X Z M leads to integral computation of weights, 
in practice, ( | , )t tq X Z M is usually set 
to 1( | , )t tp x x M− . In section 2.6, we will discuss 
1( | , )t tp x x M−  from which the particles are drawn. 
 
2.3. Feature extraction 
 
In our implementation, three factors including hand 
edges, hand silhouettes, marker centers are taken into 
account to define the observation model ( | )t tp z x .  
Gradient based edge detection (ex. Laplacian 
operator) is applied to find strong edges of hand 
silhouettes. Gaussian mask is used to eliminate noise in 
original image before edge detection.  
Since chrominance channel is more robust and less 
sensitive under varying lighting condition, the image is 
transformed to YUV color space where skin color has a 
narrow range in it [8]. From the chrominance histogram, 
two peak values are observed. The hand foreground 
pixels are distributed around the second highest peak 
value while the marker foreground pixels are distributed 
around the highest peak value. Both silhouettes are 
segmented by using thresholding. Edge pixels are 
remapped between 0 and 1. Foreground pixels are set to 
1 and background to 0. The results are shown in Figure 
3.  
 
   
         (a)                         (b) 
  
   
         (c)                         (d)  
Figure 3: Feature extraction results. (a) Original image. (b) 
Edge pixel map (c) Hand silhouettes pixel map. (d) Marker 
foreground.   
 
2.4. Marker prior information construction 
 
Given the marker foreground image at the first frame, 
the centers of marker are initialized as the centers of the 
ellipse that best fits the contour of each white block. The 
centers are then tracked sequentially using mean-shift [6] 
algorithm.  
A popular camera calibration technique [19] is 
applied in our system. The 3D position of each tracked 
marker at each frame is computed accordingly by 
optical triangulation with the intrinsic and extrinsic 
parameters estimated from each camera. The marker 
prior information, denoted as tm , is then constructed 
from those 3D marker positions (Figure 4(a)). tm  has 
the same form as the model configuration vector, 
describes a known motion state ((Figure 4(b)) from 
surface markers. Although the motion state derived from 
surface markers doesn’t accurately represent the 
movements of bony segments, it still provides a prior 
knowledge to initialize the model state.  
 
    
          (a)                             (b)
  
Figure 4: Marker prior information. (a). 3D position of tracked 
markers. Three most left markers are placed on the palm, 
while the other markers are placed over the joints (b). 3D 
finger model rendered using Initial configuration vector 
directly derived from the markers. 
 
 
 
22 
 
( | )t tp x m , then the N particles 
(1) ( ){ ,..., }Nt tx x at 
each frame are  the union of 1( )(1){ ,..., }Kt tg g and 
2( )(1){ ,..., }Kt th h , i.e. 
1 2( ) ( )(1) ( ) (1) (1){ ,..., } { ,..., } { ,..., }K KNt t t t t tx x g g h h= ∪ , 
where 1 2 1 2,   ,   (1 )K K N K s N K s N+ = = = −i i . 
The weights (1) ( ){ ,..., }Nt tπ π of the particles are then 
computed according to the observation model given in 
section 2.5. The model configuration vector of current 
frame t is estimated by: 
 
( ) ( )
1
N
i i
t t t
i
x xπ
=
=∑  (9)
 
 
3. EXPEREIMENTAL RESULTS 
 
The validity of our system is verified with 
fluoroscopy analysis as seen in Figure 6. The common 
functional action of fingers, known as pitch, involves 
the action of index finger flexion (finger bending in the 
direction of the palm) and the action of thumb. One 
subject performed pitch and pure thumb motion 
(abduction/adduction) separately at the same say. The 
tracking results in the first experiment are shown in 
Figure 7. Three data sets for statistic analysis are 
defined from fluoroscope images, surface markers, and 
estimated hand model as illustrated in Figure 8. BF 
serves as the ground truth representing the motion state 
of bony segments. SM is the marker positions of tracked 
markers. Since BF is defined on the plane perpendicular 
to the fluoroscopy system, both SM and CM should be 
transformed to two-dimensional data by being projected 
on the same plane. The orientation of this plane is 
determined in advance before the experiments. Each 
data set consists of two different joint projection angles 
of the associated finger in sequentially time-series. 
 
 
   
(a)                             (b)  
Figure 6: Hardware configuration in validation scenarios. (a) 
The fluoroscopy system. (b) Two synchronized video cameras 
settled below the fluoroscopy system capture the actions of 
fingers performed by the patient. 
 
   
(a)                        (b) 
Figure 7: Tracking results in the first experiments (pitch). (a) 
The two input images captured with model’s projection 
overlaid. (b) 3D finger models estimated. 
 
 
   
(a)                             (b)
  
    
(c)                             (d)
  
Figure 8: Description of three validation data sets. SM consists 
of the included angles between the blue lines which is the 
projection of the surface markers (denoted as solid white 
arrows). CM consists of the included angles between the red 
lines which are the projection of the center line of the 
cylindrical sections. BF, bony landmarks defined on the 
fluoroscopy images, consists of the included angles between 
the green lines. (b) and (c) are the fluoroscope images of index 
finger and thumb. (a) and (c) are the models and their 
projection on (b) and (d). (b) and (d) are the fluoroscope 
images in 1st experiment and the 2nd experiment respectively. 
 
The general-used statistical analysis is then applied 
to evaluate the accuracy and consistency. The measured 
angular difference and correlation coefficients in ten 
successive frames between the two systems (BF-SM and 
BF-CM) are summarized in Table I and Table II. Table I 
shows the average difference paired MCP and IP joint 
angles of thumb (2nd experiments), followed by the 
average difference of the paired PIP and DIP joints 
angles of the index finger(1st experiments). Table II 
shows the correlation coefficient between surface 
24 
 
 
 
6. ACKNOWLEDGEMENTS 
 
The authors would like thank the National Science 
Council for the support under contract NSC 
95-2221-E-006 -239 -MY2. We are also indebted to 
Professor LC Kuo, FC Su, and IM Jo National 
Cheng-Kung University, for valuable discussions and 
consultations in providing clinical evaluation and 
validation during the course of this study. 
 
 
7. REFERENCES 
 
[1] Malaviya, G. N. and Husain, S. “Evaluation of methods of 
claw finger correction using the finger dynamography 
technique”. J. Hand Surg., 1993, 18B, 635–638. 
 
[2] Li-Chieh Kuo, William P. Cooney III, Mineo Oyama, 
Kenton R. Kaufman, Fong-Chin Su, Kai-Nan An , “Feasibility 
of using surface markers for assessing motion of the thumb 
trapeziometacarpal joint,” Clinical Biomechanics , 
18(6):558-63 ,2003. 
[3]  L.-C. Kuo, F.-C. Su, H.-Y. Chiu, and C.-Y. Yu, 
“Feasibility of using a video-based motion analysis system for 
measuring thumb kinematics,” Journal of Biomechanics, vol. 
35, iss. 11, pp. 1499-1506, November 2002. 
 
[4] F.-C. Su, L.-C. Kuo, H.-Y. Chiu, and H.-Y. Hsu, “The 
validity of using a video-based motion analysis system for 
measuring maximal area of fingertip motion and angular 
variation,” Proceedings of the Institution of Mechanical 
Engineers. Part H, Journal of Engineering in Medicine, vol. 
216, no. 4, pp. 257-263, July 2002. 
 
[5] F.-C. Su, L.-C. Kuo, H.-Y. Chiu, and M.-J. Chen-Sea, 
“Video-computer quantitative evaluation of thumb function 
using workspace of the thumb,” Journal of Biomechanics, vol. 
36, iss. 7, pp. 937-942, July 2003. 
 
[6] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-Based 
Object Tracking,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 25, no. 5, pp. 564-575, May 2003. 
 
[7] A. Doucet, S. Godsill, and C. Andrieu, “On sequential 
Monte Carlo sampling methods for Bayesian filtering,” Statist. 
Comput., Vol. 10, No. 3, pp. 197-208, 2000. 
 
[8] R.L. Hsu, M. Abdel-Mottaleb, and A.K. Jain, “Face 
Detection in Color Images,” in Proc. Int’l Conf. Image 
Processing, pp. 1046-1049, 2001. 
 
[9] V. Athitsos and S. Sclaroff, “Estimating 3D Hand Pose 
from a Cluttered Image,” Proc. IEEE Conf. Computer Vision 
and Pattern Recognition, pp. 432-439, Vol. 2, 2003. 
 
[10] Arulampalam, M.S.  Maskell, S.  Gordon, N.  Clapp, 
T. ,“A tutorial on particle filters for online 
nonlinear/non-Gaussian Bayesian tracking”, Signal Processing, 
IEEE Transactions , vol. 50, iss.2, pp. 174-188, Feb 2002 
 
[11] C.-S. Chua, H. Guan, and Y.-K. Ho, “Model-based 3D  
hand posture estimation from a single 2D image,” Image and 
Vision Computing, vol. 20, iss. 3, pp. 191-202, March 2002. 
 
[12] S.U. Lee and I. Cohen. , “3D hand reconstruction from a 
monocular view,” In Proc. of the 7th Int’l Conf. On Pattern 
Recognition (ICPR’04), Cambridge, United Kingdom, August 
2004. 
 
[13] Y. Wu, J. Lin, and T.S. Huang, “Capturing natural hand 
articulation,” in Proc. of IEEE Int'l Conf. on Computer Vision, 
2001, Vol II, pp. 426-432. 
 
[14] J. Lin, Y. Wu, and T.S. Huang, “Capturing human hand 
motion in image sequences,” in Proc. of IEEE Workshop on 
Motion and Video Computing, 2002, pp. 99-104. 
 
[15] Bray, M., Koller-Meier, E., and Van Gool, L., “Smart 
particle filtering for 3D hand tracking”, . Int. Conf. on 
Automatic Face and Gesture Recognition, 2004, pp. 675–680 
 
[16] Z. Zhang, “A flexible new technique for camera 
calibration,” IEEE Trans. Pattern Anal. Machine Intell., vol. 
22, no. 11, pp.1330–1334, Nov. 2000. 
 
[17] J. Deutscher, A. Blake, and I. Reid, “Articulated Body 
Motion Capture by Annealed Particle Filtering”, Proc. Conf. 
Computer Vision and Pattern Recognition, pp. 126±133, June 
2000. 
 
[18] Wen-Yan Chang, Chu-Song Chen, Yi-Ping Hung  , 
“Appearance-guided particle filtering for articulated hand 
tracking,” IEEE Proc., Computer Vision and Pattern 
Recognition, , June 2005. 
 
[19] Z. Zhang, “A flexible new technique for camera 
calibration,” IEEE Trans. Pattern Anal. Machine Intell., vol. 
22, no. 11, pp.1330–1334, Nov. 2000. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 26
along the cross product of the x and y unit vectors. Then 
we utilize three rotation and three translation parameters 
in each coordinate system to provide full freedom in 
join motion. Since we concentrated only on observing 
the fingers motion rather than hand motion, six 
additional parameters were used to represent the 
translation and rotation of hand. Therefore, there are 
totally 24 degrees of freedom represented by a state 
vector 1 24{ ,... }tx d d=  to comprise single finger model. 
A. Motion state estimation by observable states  
The model-based hand tracking can be regarded as 
the hidden states estimation of the proposed 
regularization model based on the real observed states. 
Firstly, we model the real finger using the observable 
states zt (image features), hidden states xt (real motion 
states), and pivot observable states yt (markers), as in 
Equations (1)-(2). And the pivot states are retrieved by 
image processing from the observable states zt.  
 In concept, based on the bio-mechanism knowledge 
and geometrical analysis, the real finger can be regarded 
as a 3D object with underlying bony structure. The 
above equations show that the observable states c an be 
estimated from an estimation function of unknown 
hidden motion states xt with additional noise Nt. The 
observable states zt can be measured by using 
multiple-view cameras with perspective projection 
function P(.). The Φ(xt) is the representative model of 
finger so that the hidden states can be estimated to 
represent the real finger states. In Eq.(2), the symbol yt 
stands for the 3D marker positions that are reconstructed 
by processing the information zt using the reconstruction 
method K(. ) and is used as the pivot features.  
Note that, xtM should be initially estimated based only 
on the pivot states yt, and then based on both the 
pre-states and the pivot states from the second iteration 
on, as listed in (3). The S(.) denotes the sampling 
function applied on the 3D model and is assumed to be 
bounded as in (4). Therefore, the problem can be 
simplified as to minimize the error between the model 
appearance ztM and the observable state zt, 
i.e. ˆ argmin ( ( ( )))
M
t
M
t t t
x
x P S x zΜΜ≡ Φ − . Then we can 
make the hidden states approach to the real model states 
based on the observation similarity in an iterative way 
by using 
arg min arg min ( ( ( )))
M M
t t
M
t t t t
x x
x x P S x zΜΜ− ≅ Φ − . 
B.  Building probability observation model  
In tracking the finger motion, the stochastic optimal 
filtering has been proposed to estimate the states at the 
next frame. The major work in the stochastic 
optimization problem is the selection of observation 
probability model. Therefore, we focus on designing a 
proper observation model p(Zt | Xt ) to integrate various 
likelihood functions from the observed images that 
include the edge (Fe(.)), silhouette (Fr (.)) and marker 
positions (Fm(.)) as shown in Fig. 1 [9-11]. Based on the 
estimated hidden states, the synthesized projection 
image of the 3D model is expected to fall inside the 
hand silhouettes of real hand. Then, the first factor 
relating the 3D model to hand foreground measurement 
is expressed as the following cost function of sum of 
squared errors.  
 
Where Fr(zrM(uj , vj)) is the pixel value of sampled point 
on the synthesized model surface which corresponds to 
the location (uj , vj) on the real image. Here, we simply 
examine whether the model sampled points map on the 
hand region of the corresponding real image, and get the 
voting ratio Er  of correct mapping.  
Secondarily, the strong edges of hand silhouettes are 
used to indicate how the borders of estimated finger 
model coincide with its corresponding finger boundary 
in real image. It is formulated by the following cost 
function: 
 
2
1
1 ( ( ( , )) ( ( , )))Me e t j j e t j j
j
E F z u v F z u v
L =
= −∑L  (6) 
Where W and L indicate totally numbers of projected 
pixels on hand region and edge respectively. The third 
factor of likelihood measurement corresponds to marker 
positions. Since the long axis of the model cylinder 
should be parallel to the lines joining the markers of the 
two ends of the same phalange segment, we utilize the 
sine function to investigate the extent of parallelization. 
Therefore, the third factor pE is given by: 
(1) (2)( , ), 1,2,3j jj m m jangle F F A jθ = =  (7) 
2
1
(sin )p j
j
E θ
=
=∑3  (8) 
Where θj is the angle between the line (1) (2)j jm mF F  
joining the two 3D markers of bone segment j and the 
long axis of the corresponding cylindrical section Aj, 
j=1, 2, 3.These three afore-mentioned factors are then 
combined in a cascaded Gaussian kernel function to 
produce the probability observation model with 
weighting values as in Eq. (9). 
,
( ( ) )
( )
t t t
t t k t
z P x N
y K z N
= Φ +
= +
 (10) 
(2) 
1( , )
( ( ( )))
t t t
t t t
x y x
Z P S x N
Μ Μ
−
Μ Μ Μ
Μ
= Θ
= Φ +
 (3) 
(4) 
2
1
1 ( ( ( , )) ( ( , )))
W
M
r r t j j r t j j
j
E F z u v F z u v
W =
= −∑  (5) 
 28
(a) (b)
Fig. 2: Hardware configuration in validation scenarios. (a) The 
fluoroscopy system. (b) Two synchronized video cameras settled 
below the fluoroscopy system capture the actions of fingers performed 
by the patient. 
(a) (b) 
(c) (d) 
Fig. 3 Description of three validation data sets. SM consists of the 
included angles between the blue lines which is the projection of the 
surface markers (denoted as solid white arrows). CM consists of the 
included angles between the red lines which are the projection of the 
center line of the cylindrical sections. BF, bony landmarks defined on 
the fluoroscopy images, consists of the included angles between the 
green lines. (b) and (c) are the fluoroscope images of index finger and 
thumb. The (a) and (c) are the models and their projection on (b) and 
(d). The (b) and (d) are fluoroscope images in 1st experiment and the 
2nd experiment respectively. 
  
  
Fig.4   Tracking results using stochastic filters with proposed 
observation model. 
 
Table   I   Average difference of joint angles measured from thumb 
(ABD/ADD) and index (F/E) experiments. 
MCP joint (deg) IP joint (deg) 
BF-CM 1.7 1.4 
Thumb 
(ABD/ADD) 
BF-SM 2.0 2.7 
PIP joint (deg) DIP joint 
(deg) 
BF-CM 1.0 1.3 
Index 
(F/E) 
BF-SM 1.3 1.6 
Table   II   Correlation coefficient of joint angles measured from 
thumb (ABD/ADD) and index (F/E) experiments. 
Thumb MCP joint (deg) IP joint (deg) 
BF-CM 0.877 0.777 (ABD/ADD)
BF-SM 0.924 0.597 
PIP joint (deg) DIP joint 
(deg) 
BF-CM 0.999 0.983 
Index 
(F/E) 
BF-SM 0.997 0.967 
REFERENCES 
[20] Malaviya, G. N. and Husain, S. “Evaluation of methods of claw 
finger correction using the finger dynamography technique”. J. 
Hand Surg., , 18B, pp. 635–638, 1993 . 
 [ 21] Chiu HY, Su FC. The motion analysis system and the maximal 
area of fingertip motion. A preliminary report. J. Hand Surgery  
Oct;21(5), pp. 604-608. 1996  
[22] Edmund YS Chao, Kai-Nan An, William P Cooney III and  
Ronald L Linsceid,  Biomechanics of the hand, , JBW prjnters & 
Binder Pte.Ltd., 1989.  
[23] Li-Chieh Kuo, William P. Cooney III, Mineo Oyama, Kenton R. 
Kaufman, Fong-Chin Su, Kai-Nan An , “Feasibility of using 
surface markers for assessing motion of the thumb 
trapeziometacarpal joint,” Clinical Biomechanics , 18(6) , 
pp.:558-63, 2003. 
 [24] F.-C. Su, L.-C. Kuo, H.-Y. Chiu, and H.-Y. Hsu, “The validity 
of using a video-based motion analysis system for measuring 
maximal area of fingertip motion and angular variation,” 
Proceedings of the Institution of Mechanical Engineers. Part H, 
Journal of Engineering in Medicine, vol. 216, no. 4, pp. 257-263, 
July 2002..  
[25] F.-C. Su, L.-C. Kuo, H.-Y. Chiu, and M.-J. Chen-Sea, 
“Video-computer quantitative evaluation of thumb function using 
workspace of the thumb,” Journal of Biomechanics, vol. 36, iss. 
7,. pp. 937-942, July 2003.  
[26] C.-S. Chua, H. Guan, and Y.-K. Ho, “Model-based 3D hand 
posture estimation from a single 2D image,” Image and Vision 
Computing, vol. 20, iss. 3, , pp. 191-202 , March 2002.  
[27] S.U. Lee and I. Cohen. , “3D hand reconstruction from a 
monocular view,” In Proc. of the 7th Int’l Conf. On Pattern 
Recognition (ICPR’04), Cambridge, United Kingdom, August 
2004.  
[28] Arulampalam, M.S.  Maskell, S.  Gordon, N.  Clapp, T. ,“A 
tutorial on particle filters for online nonlinear/non-Gaussian 
Bayesian tracking”, Signal Processing, IEEE Transactions , vol. 
50, iss.2, pp. 174-188, Feb 2002,  
[29] V. Athitsos and S. Sclaroff, “Estimating 3D Hand Pose from a 
Cluttered Image,” Proc. IEEE Conf. Computer Vision and Pattern 
Recognition, pp. 432-439. Vol. 2,2003 . 
 [30] R.L. Hsu, M. Abdel-Mottaleb, and A.K. Jain, “Face Detection in 
Color Images,” in Proc. Int’l Conf. Image Processing, , 2001, pp. 
1046-1049. 
[31] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-Based Object 
Tracking,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 25, no. 5, pp. 564-575, May 2003. 
 
 
 
 
 
 
 
 
 
 
 
16th INTERNATIONAL CONFERENCE ON MECHANICS IN MEDICINE AND BIOLOGY 
Pittsburgh, PA, USA, 23rd-25th July 2008 
 30
in the course of tracking. In addition, we performed the 
semi-spherical grasp with the all fingers motion. From 
the inital pose (i.e. full extension) to the grasp, the 
finger model fits the real fingers very well as shown in 
Fig. 3. In addition, single finger flexion/extension and 
pinch motion have also been successfully investiagted. 
In summary, the proposed system provides a conveient 
and effective tool for clinical evaluation, such as the 
cooperation information of joint angles as illustrated in 
Fig.4. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1: The flowchart of the proposed system 
 
   
   
Fig. 2: Top: results of classical mean-shift algorithm. Bottom: 
results of illumination-robust mean-shift algorithm. 
 
(a) (b) 
Fig.3: The proposed 3D model and its projection on four 
views for the motion of semi-spherical grasping. (a) The initial 
model. (b) The grasping motion of model. 
 
Fig. 4: Joint angles in three axes ( x axis:  
pronation/supination, y axis: adduction/abduction, z axis: 
flexion/extension) of MCP and IP joints of thumb during pinch 
with index finger. 
 
Conclusions 
 
The complex occlusion problem of fingers motion is 
a quite challenging task due to the complexity in 
processing the articulated finger motion with a high 
degree-of-freedom. Although partial occlusions occur in 
some frames, the proposed strategy successfully 
conquers these difficulties to correctly reconstruct 3D 
markers from multiple views. And the 
illumination-robust mean-shift algorithm is designed to 
handle the occurrence of abruptly markers jumping in 
time sequence. In the experiments, the marker positions 
tracked by the proposed are more reliable than by those 
by the conventional method. The resulting model 
parameters provide useful kinematics information of 
fingers. They are valuable to the diagnoses and 
treatments of hand diseases. 
 
 
References 
 
[1] C. W. CHANG, L. C. KUO, Y.T. CHENG, F. C. SU, 
I.M.JOU, AND Y. N. SUN, (2007) ‘Reliable 
Model-based Kinematics Analysis System for 
Articulated Fingers’ IEEE Conf. on Biomedical 
Enginerringing (EMBS), Lyon. 2007, pp. 1003-18. 
[2] L.-C. KUO, F.-C. SU, H.-Y. CHIU, AND C.-Y. YU, 
(2002) ‘Feasibility of using a video-based motion 
analysis system for measuring thumb kinematics’, 
Journal of Biomechanics,  35 (11), pp. 1499-1506. 
 [3] M BRAY, E KOLLER-MEIER, L VAN GOOL, (2004)’ 
Smart particle filtering for 3D hand tracking’ 
Automatic Face and Gesture Recognition, sixth 
international conference,  pp. 675- 680.  
 [4] GREGORY S. RASH!, P.P. BELLIAPPA, MARK P. 
WACHOWIAK, NAVEEN N. SOMIA, AMIT 
GUPTA,(1999),’A demonstration of the validity of a 
3-D video motion analysis method for measuring 
finger flexion and extension’, Journal of 
Biomechanics 32, pp. 1337-1341. 
Illumination-robust 
mean-shift algorithm to 
get 2D marker position 
 Particle filter 
Set up the 
correspondence 
and reconstruction 
markers  
Generate particles 
Based on prior 
1( | , )t tp x x m−  
Color Feature extraction 
Observation model building and 
evaluation likelihood, get 
( ) , 1 ~it i nπ =  
Weighting particles and get estimated 
state 
Pre-frame state   
1ˆtx −  
ˆtx  
 
 
Multiple-view 
finger images  
