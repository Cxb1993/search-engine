 2
these methods based on spatial domain have more 
computation in than temporal domain with block-
calculation so that we have to find more efficient ways 
to compute segmentation in video sequence. 
For temporal domain, Bescos and Menendez [4] 
have proposed a method which focuses on a module 
that detects the number of people that crosses an 
entrance to a big store, so that the approach is oriented 
to be hardware implemented in order to allow for low 
cost real time operation. In Ref [5],the authors provided 
a background subtraction algorithm that uses multiple 
competing Hidden- Markov-Models(HMM) over small 
neighborhoods to maintain a locally valid background 
model in all situations. The use the DCT coefficients of 
JPEG encoded images directly to minimize 
computation and to use local information in a 
principled way. Region level processing is reduced to 
the minimum so that the extracted information that 
goes to higher level processing is unbiased. 
      All above approaches use the intensity, color 
information of the pixels or temporal domain data, and 
are susceptible to sudden lighting illumination changes. 
Recently, efforts have been made to incorporate other 
illumination robust features for scene modeling. The 
intensity and texture information can be integrated for 
change detection, with the texture based decision taken 
over a small neighborhood [6]. The color and gradient 
information van be fused and robust results have been 
reported [7]. However the computation of these 
algorithms is often too intensive for real time 
implementation. 
 
 
3. THE PROPOSED METHOD 
 
First of the all, each frame is divided into 8x8 
block according Ref [12]. Two features, fDC and fAC 
of each block, are determined from the 2D-DCT 
coefficients and each of them is modeled as 
independent Gaussian distribution with different mean 
μand variance  σ2. These means μ and variances σ2 are 
estimated initially from a first few frames, possibly with 
some moving object, and then they are updated 
depending on the scenery evolution. 
Segmentation of a new frame is obtained initially by 
calculating the thresholds of these feature values ( fDC 
and fAC ) which contain illumination change detection 
and texture difference detection, then refined the result 
using size filtering, merging blocks and filtering. Based 
on the final segmentation of the current frame, the 
parameters of means μ and variances σ2 are updated 
according to the segmentation result of the next frame. 
With 2D-DCT domain, block based features 
incorporate local neighborhood information such as 
illumination change and texture difference between 
neighbor blocks, so the algorithm using these features 
is lees sensitive to shadow, reflection of mirror-like 
wall, lighting illumination change, noise, small 
vibration of background movement and small scenery 
changes than those using pixel value. Furthermore, the 
calculation of block base is much lower than that of 
pixel base, so the algorithm of DCT based feature is 
faster to become real time. 
 
 
3.1. The features of dct domain 
We compute two features fDC and fAC which are 
derived from the 2D-DCT domain coefficients of each 
8x8 block within each frame. 
 
 
 
 
 
 
Fig 2. Definition of fDC and fAC 
 
fDC=DCT(0,0)………………………………………...Eq(3) 
fAC=åå
= =
+
3
0
3
0
22 )),(()(
i j
jiDCTabsji …….Eq(4) 
Where DCT(i,j) is the (i,j)th coefficient of 2D-DCT 
domain. 
The DC coefficient fDC is simply the DC 
coefficient without any frequency component, and 
according to the Eq(2), fDC is the average gray level 
intensity of the block and sensitive to large illumination 
changes. It is computationally easier to implement and 
more efficient to regard the 2D-DCT as a set of basis 
functions which given a known input array size (8 x 8) 
can be pre-computed and stored. This involves simply 
computing values for a convolution mask (8 x8 window) 
that get applied (average values x pixel the window 
overlap with image apply window across all 
rows/columns of block).   
The AC coefficient fAC is a weighted sum of the 
lower frequency AC component without any DC 
component. Much of the signal energy lies at low 
frequencies; these appearing in the upper left corner of 
the 2D-DCT and the high frequency coefficients are not 
used because they are more sensitive to noise and they 
relate to fine detail that are more susceptible to small 
illumination changes so that the AC coefficient fAC 
has a great stress on texture differences between 
neighbor blocks. 
In Eq(4), the weight )( 22 ji +  de-emphasizes the 
very low frequency components, which often 
correspond to smooth transition of intensity such as 
shadow, reflection of mirror-like wall and small 
vibration of background movement. The emphasis of 
middle frequency can help fAC concentrate on texture 
difference between blocks and avoid shadows being 
taken as part of foreground object. 
 
PDF created with pdfFactory Pro trial version www.pdffactory.com
 4
3.6. Segmentation 
After determining these four parameters: DCm , 
ACm , DC
2s  and AC2s  for all blocks of a frame, 
initial segmentation can be accomplished by computing 
the thresholds and classifying a block as foreground or 
background. The block at t-th frame is label as 
foreground if it satisfied both Eq(15) and Eq(16). 
Where TDC and TAC are thresholds and ABS means 
absolute value. 
 
DCDCtDC TtfABS >- ))(( ,m    ………………Eq(15) 
and 
ACACtAC TtfABS >- ))(( ,m     ………………Eq(16) 
 Eq(15) is the detection of great illumination 
change between the current block and the last block. 
There is a large difference in the DC value of the 
intensity between a moving foreground block and the 
background block, since fDC(t) means the average 
value of block in gray level intensity and DCt ,m has a 
portion of last block’s fDC(t-1). WE call those blocks 
satisfying Eq(15) “DC foreground blocks”. 
 
The use of Eq(16) is because the presence of the 
edge of the foreground objects and the different texture 
between the foreground and the background will lead to 
a large difference between the AC values of the current 
block and those of the background model block. The 
texture difference with small illumination change may 
occurs in shadow, reflection of mirror-like wall and 
small vibration of background movement, since 
ACt ,m also has a portion of last block’s fAC(t-1) if the 
last block is labeled as background. WE call those 
blocks satisfying Eq(16) “AC foreground blocks”. 
The use of these two threshold operation is 
equivalent to use both intensity and texture information, 
and will likely produce more vigorous and reliable 
segmentation results. The block at t-th frame is label as 
foreground if it is both DC block and AC block and we 
will move to next section to calculation the threshold of 
DC and AC value. 
 
3.7. Threshold calculation 
To handle the effect of illumination and texture 
change, we use for the threshold a value derived from 
both the sample mean and the sample standard 
deviation. Eq(17) is the threshold of DC value to detect 
a large illumination change and Eq(18) is the threshold 
of AC value to determine small illumination variation 
with texture difference between blocks. 
DCtDCDCtDCDCT ,, sgmd ´+´=   …………  Eq(17) 
ACtACACtACACT ,, sgmd ´+´=    …………  Eq(18) 
    Where
DCd
, ACd , DCg  and ACg  are parameters, whose 
value depends on how much lighting change is 
expected to be handled here. The values of d and 
g are sensitive to the speed of illumination change and 
velocity of object movement.  
5.15.0 << d  and 38.0 ££ g  
The above equation is only the estimation values of 
d and g . When training is not allowed or when re-
training for variance is taken at the time of severe 
environment change, i.e. when segmentation should be 
done without an estimate of the variance, we simple 
raise the value of d and g . 
 
3.6. Learning parameters 
As mention at section 3.5, both these tow equation 
below:  
)(.).1( ,,1,, tfDCDCDCtDCDCt mm amam +-= - Eq(11) 
)(.).1( ,,1,, tf ACACACtACACt mm amam +-= -  Eq(12) 
Those above equations have ma  which is called 
learning parameter. There is a tradeoff between 
stability and adaptation speed when choosing a learning 
parameter. When the scene is slowly varying, a small 
ma  is preferred as it avoids the impact of outliers. 
When there is a change of either the camera or the 
background environment, a large ma  is preferred in 
order to quickly arrive at a new background model. For 
the parameter ma  that controls mean value adaptation, 
we set a range [ minmax ,aa ] for it and use the scheme 
as below: 
 
update
block
block
fast
background
foreground
max
min
0
a
aa m =    …………Eq(19) 
    0 1ma£ £  
Apparently, when there is a foreground block at (t-
1)th frame, we won’t update the DC and AC 
parameters of the block at t-th frame, so ma  is zero 
when foreground block is applied. If there is a 
background block at (t-1)th frame, we will assign ma  
as mina  whose value depends on the overall situation 
of the video sequence. When fast update occurs, we 
assign ma  as maxa  which has grater value than mina . 
Fast update occurs when there is a large 
illumination change or hasty moving foreground object 
is applied in the video sequence. The blocks which are 
in the position at different frames change so quickly 
PDF created with pdfFactory Pro trial version www.pdffactory.com
 6
large moving object with slow motion and tardy 
illumination change. 
 
 
 
 
 
 
Fig 5. Size filtering 
We use 8 neighbor connected component to refine 
the result of the segmentation. If a small background 
region is surrounded by foreground blocks, we simply 
fill those background blocks to become foreground 
blocks. 
 
4. EXPERIMENT RESULT  
 
There are three typical experiments applied in 
DCT method in foreground segmentation:  
   1. A man walked with highly reflective wall in the 
background and persisting global illumination 
change.  
   2. Two persons cross paths at the entrance of a store. 
   3. A man walked with cluttered background of trees 
and swaying branches on a windy day. 
     These three representative works are different types 
from one another. Each video sample has particular 
circumstance for foreground segmentation on DCT 
domain and the source of every video sample comes 
from the video database online or is made by myself. 
We’ll discuss each of them at the sections below. 
 
4.1. Demo 1 
The first sequence is a feature of shadow in highly 
reflective wall in the background. A man walked with 
highly reflective wall in the background and persisting 
global illumination change. The mirror like 
background shows a considerable number of moving 
shadow from objects at a distance from the wall. In 
addition, the background contains a large area looking 
much like the skin tone in intensity image. The method 
of segmentation in DCT domain result in very few false 
negatives and almost no false positives. Several sample 
frames are shown in the sets of Fig 6. 
 
 
 
 
 
 
 
 
Fig 6-1. Frame 1st, background of DEMO 1. The right 
region is input video sample and the left region 
is the result. 
 
 
 
 
 
 
 
 
              Shadow                    Object 
Fig 6-2. Frame 20th. 
 
 
 
 
 
 
 
                                                      shadow 
Fig 6-3. Frame 27th. 
 
 
 
 
 
 
 
 
Fig 6-4. Frame 38th. 
As can be seen from the result, the foreground 
segmentation is almost perfect with the background 
moving shadow and the lighting condition kept 
changing while the foreground object kept moving 
forward. If the thresholds are raised higher, there will 
be more false negatives; if they are lower, a lot of false 
positives. While the simultaneous modeling of block 
based features leads to satisfactory result, with all the 
parameters kept unchanged, without any adaptation. 
 
However, there are still several drawbacks of this 
DCT method. In Fig 6-4, it is clear to observe that the 
result has rough edge in the segmentation because 
calculation is based on 8x8-pixels block. The DCT 
method has more rugged edge than traditional pixel 
method and the solution may be reducing the block size 
to be 6x6 or 4x4. 
 
4.2. Demo 2 
The second video sequence which comes from the 
video database online at“http://groups.inf.ed.ac.uk/ 
vision/CAVIAR/CAVIARDATA1/” presented that two 
persons cross paths at the entrance of a store. The main 
idea of this sequence is to observe the adaptation speed 
of learning parameter update  
 
 
 
 
 
 
PDF created with pdfFactory Pro trial version www.pdffactory.com
 8
 
5. CONCLUSION AND FUTURE WORK 
 
The method of segmentation in DCT domain for 
video sequence presents a simple way to separate the 
moving foreground object from solid background. It 
uses two features derived from the DCT domain and 
has low computation because the calculation is based 
on 8x8 pixels block so that it can be computed almost 
real time. A single Gaussian distribution is used to 
model the evolution of the two features in each block. 
An updating process is designed to handle effectively 
the changing background. The method is also able to 
successfully handle the problems of gradual global and 
local illumination change, moved background objects 
and small vibration background movement. 
   The drawbacks of DCT method are undeniable. 
According to the result, it has rougher edge than pixel 
method due to the block based computation. This may 
be overcome by using 4x4 or 6x6 pixels-block to get 
smoother margin. There is another disadvantage to be 
discovered that when the color of the foreground 
moving object is very similar to that of background, it 
is hard to segment the foreground form the video 
sequence. 
This method may be used on the security monitor 
or robot vision. As in DEMO 2, this video sequence 
actually come form the security monitor record video 
sequence of a department store. It also has utility on 
robot vision in order to extract the movement of 
foreground object. 
 
6. REFERENCES 
 
[1]Jiashu Zhang, Like Zhang and Heng Ming 
Tai,”Efficient Video Object Segmentation using 
Adaptive Background Registration and Edge Based 
Change Detection Techniques” Volume 2,  27-30 
June 2004 Page(s):1467 - 1470 Vol.2 IEEE Trans  
[2]C. Kim and J Hwang,”Video object extraction for 
object oriented applications”, Journal of VLSI 
signal processing, vol29,pp.7-21, 2001 
[3]C Ridder, O Munkelt et al, “Adaptive Background 
estimation and foreground detection using kalman 
filtering”, Pro of Intl Conf On Recent Adbances in 
Mechatroincs(ICRAM),pp193-199,1995. 
[4]Jesus Bescos, Jose M Menendez and Narciso 
Garcia”DCT based segmentation applied to a 
scalble zenithal people counter” IEEE 2003 
[5]Mathieu Lamarre, James J Clark, ”Background 
subtraction using competing models in the block 
DCT domain” Volume 1,  11-15 Aug. 2002 
Page(s):299 - 302 vol.1 Digital Object Identifier 
10.1109/ICPR.2002.1044695 IEEE CNF 
[6]L,Li and M Lenug, ”Integrating intensity and 
texture differences for robust change 
detection”IEEE Trans. On image processing, Vol 
11,No2, pp105-112 2002 
[7]O. Jabed, K Shafique and M Shah, “A Hierarchial 
Approach to robust background subtraction using 
color and gradient information”, Proc. Workshop 
on Motion and Video Computing, 2002, pp22-27 
[8]Jie Wei,”Image segmentation using situational DCT 
descriptors”, Volume 1,  7-10 Oct. 2001 
Page(s):738 - 741 vol.1 Digital Object Identifier 
10.1109/ICIP.2001.959151 IEEE CNF 
[9]Rahul Shankar Mathur, Punit Chandra and Dr. 
Sumana Gupta, “A low bit rate video coder using 
content based automatic segmentation and shape 
adaptive DCT” Volume 2,  19-22 Jan. 2000 
Page(s):119 - 122 vol.1 IEEE CNF 
[10] Salkmann Ji, Hyun Wook Park,”Region based 
Video segmentation using DCT coefficients” 
Volume 2,  24-28 Oct. 1999 Page(s):150 - 154 vol.2 
Digital Object Identifier 10.1109/ICIP.1999.822873 
IEEE Trans 1999 
[11] Salkmann Ji, Hyun Wook Park,”Moving-object 
Segmentation with adaptive sprite for DCT- based 
video coder” Volume 3,  7-10 Oct. 2001 
Page(s):566 - 569 vol.3 Digital Object Identifier 
10.1109/ICIP.2001.958177 IEEE Trans 2001 
[12] Juhua Zhu, Stuart C. Schwartz, Bede Liu, ”A 
transform Domain Approach  to Real-Time 
Foreground Segmentation in Video Sequence” 
Volume 2,  March 18-23, 2005 Page(s):685 - 688 
IEEE Trans 2005. 
PDF created with pdfFactory Pro trial version www.pdffactory.com
報告內容: 
一、 出席 2006 國際影像處理會議 
二、 參加會議經過及與會心得 
三、 大會的重點及發展趨勢 
四、 攜回資料名稱 
五、 結論 
 
出席國際會議報告 
『 2006 國際影像處理會議』 (2006 IEEE Int’l Conference on Image 
Processing)假美國亞特蘭大市盛大舉行，筆者應邀出席該會，發表論文一篇，
茲將參加會議的心得分述於下： 
一、本會共有一仟伍佰多位來自世界各地五十多個國家的學者、專家與會，
約有 1200篇論文在大會發表，內容著重在訊號處理、數位影像處理，
影像壓縮與視訊、及多媒體等是個內容非常豐富及具學術權威地位的重
要會議。 
二、此次大會的主辦單位為美國國際電機電子工程師學會、訊號處理分會協
辦(IEEE Signal Processing Society)，其目的想藉此會交換各國最新「影
像處理」科技，並促進學術交流。 
三、亞特蘭大的會議地點為城市旅館國際會議中心設備優良，交通方便，是
個理想的會議場所。 
四、兩個會議的頭一天均有短期課程班( Tutorials )，讓與會者有再接受新科
技之機會。大會的學術論文發表方式分成 12 個場地同時進行，另外有
個較大的場地以海報方式舉行，並有書展及影像系統及產品展示，海報
方式能與原作者親自相互討論及交換心得，效果十分良好。 
PDF created with pdfFactory Pro trial version www.pdffactory.com
