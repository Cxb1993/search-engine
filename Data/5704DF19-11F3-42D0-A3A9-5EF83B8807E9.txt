review most of the speech recognition algorithms used 
in emotion detection, and then choose those with 
relatively low computational complexity for further 
implementing on portable devices. Second, we would 
like to revise the facial emotion detection 
algorithms for a real-time processing. Evaluating and 
modifying the machine learning algorithms such as 
FCMAC (Fuzzy Cerebellar Model Articulation 
Controller) and SVM (Support Vector Machine) for 
audiovisual feature fusion will be the most important 
process. To verify the performance of overall system, 
the proposal model is also evaluated based on the 
consistency between audiovisual features and real 
emotions. 
 
英文關鍵詞： Affective analysis, Emotion detection, Feature 
fusion, Audiovisual signal, Speech recognition, and 
Facial expression recognition. 
 
 2
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 ■申請中 □無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
 
與此計畫相關送審中期刊論文 2 篇，已接受研討會論文 4 篇(詳見國科會資料庫
或報告附錄)，其中 2 篇正改寫成期刊論文。 
附件二 
 4
行政院國家科學委員會專題研究計畫成果報告 
音視訊號融合之情緒偵測系統 
Emotion Detection System Based on Audiovisual Signal Fusion 
 
計畫編號：NSC 99-2221-E-011-013- 
執行期限：99 年 8 月1 日  至  100 年 10 月31 日 
主持人：林敬舜   國立台灣科技大學 
 
中文摘要 
在人機互動上，對於情緒智能的需求在近年來日益殷切。為了適切地對使用者做出反
應，電腦就需具備有使用者情緒狀態的感知能力。就機器對人類情緒感知來說，透過影像追
蹤人臉表情可得到最多的情緒資訊。另一方面，隱含在言語中的聲音特徵亦提供了另一個獲
取情緒狀態的管道。研究發現，同時使用聲音與影像資訊可提升對負面情緒的鑑別。目前情
緒偵測自動化碰到的困難之一就是現今並無標準資料庫可供情緒偵測演算法的評估。人臉表
情的資料庫無法自然的反應出參與者實際的情緒狀態就是其中一個問題。有鑑於此，在這個
提案裡我們計畫完成三件任務。第一、就目前用來做情緒偵測的語音辨識演算法逐一檢視，
然後選擇計算複雜度相對較小的模式以方便日後應用於可攜式裝備。第二、同時對各種人臉
情緒偵測的演算法做評估，使其達到即時處理的效能。最後，對既有的FCMAC (Fuzzy 
Cerebellar Model Articulation Controller)與SVM (Support Vector Machine)等演算法做評估，針
對採用的音視訊特徵做修正將是最主要的研究方向。為了驗證系統的效能，採用的音視訊特
徵與實際情緒的一致性將會是最後情緒偵測模型的評估重點。 
 
關鍵詞：情緒分析、情緒偵測、特徵融合、音視訊號、語音辨識、人臉辨識。 
 
Abstract 
There is a growing demand for emotional intelligence in the human-computer interaction. To 
appropriately react to a user, the computer would need to have some perception of the emotional 
states of the user. In general, the most explicit information for machine perception of emotions is 
through facial expressions in video. On the other hand, emotional clues buried in speech also 
provide computer an alternative to recognize user‘s situation. Researchers have reported that 
improved recognition in negative emotions can be achieved when speech features are used along 
with visual features. One of the challenges in evaluating automatic emotion detection is that there 
are currently no international databases based on authentic emotions. The current existing facial 
expression databases contain facial expressions that are not naturally linked to the emotional states 
of the participant. The major tasks of this project are threefold: First, we plan to review most of the 
 6
-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
valence
ar
o
u
sa
l
 
 
happiness
fear
disgust
sadness
boredom
neutral
anger
 
圖 1.1  Arousal-Valence平面示意圖 
 
2. 文獻回顧 
在本計畫中，音視訊融合系統將以音頻與視頻訊號分別探討，本章舉出過去幾個重要的
特徵擷取方法，將情緒套用於這些特徵當中，並探討訊號分析辨識模型，以及回顧訊號融合
之研究。 
 
2.1 音頻訊號情緒偵測 
對於音訊情緒辨識，目前大多為語言獨立(Language Independent)的研究，即不論及說話
的內容，只針對語音特徵做辨識，好處是不須建立各種語言的訓練模型，但相對的無法利用
說話的內容對情緒做進一步的分析。對於語言獨立的音訊情緒辨識，我們並未有各國語音學
或語言學的分類結構模型可参考，較可供参考的唯有二維的 Arousal-Valence 情緒平面。而
Y. H. Yang 等人將音樂情緒辨識為音樂在二維情緒平面上的迴歸問題來預測音樂的 Arousal 
和 Valence值[11, 12]。在語音訊號上，亦可用類似的方法，判定每個的語句在二維情緒平面
上所屬的點，以觀測語者的情緒變化。 
 
在音訊情緒辨識的研究中提出許多的特徵向量表示方式，如梅爾頻率倒頻譜係數
(Mel-Frequency Cepstral Coefficients, MFCC)、音高輪廓(Pitch Contour)、能量輪廓(Energy 
Contour)、與頻譜平坦量測(Spectral Flatness Measure) [13]等。因特徵種類繁多，進而產生了
組合爆炸的問題，不適當的特徵向量組合不僅徒增計算複雜度，更可能降低不同類別間的鑑
別度，因此，特徵選取成為首要的問題。而一些特徵選取法中的啟發式選取演算法可供我們
 8
 
表情在人臉上的定義，可以五官的變化及形狀來做判斷基準，因此如何讓系統正確地擷
取到人臉特徵成為首要關鍵，而主動外觀模型(Active Appearance Models, AAM) [21, 22]為目
前比較常見獲取人臉特徵點的演算法。AAM 是基於 ASM (Active Shape Model) [23]改進而來
的演算法，此方法是由點框出人臉及特徵的模型，使用 AAM 得出的結果雖然不錯，但仍存
在不少缺點，例如 AAM 需要謹慎完善地訓練出一個模型，才能有效的定位，且判斷物件不
能與原訓練模型差異太大，否則結果會跟預期有極大的落差，當 AAM 在進行形狀擬合時出
現誤判，就很難導向正確結果，這個缺失可歸咎於其較差的自調整性。 
 
特徵點也有不使用類似 AAM 模型演算法來擷取，而是先做切割[24]，將眉毛、眼睛和
嘴巴分隔出來，再依照五官的各自條件去做運算，例如可用 Gabor Filter [25]或 Corner 
Detection [26, 27]取出特徵點，但因為眼睛與嘴巴的外觀弧線具特殊性與相似性，基於形狀
特性的擷取這兩種方法均有些類似。透過特徵點的擷取方法最大優點就是判別率與自由度
高，因為可以根據各個五官特徵以及自己的需求來完成針對性的演算法，以達成特徵點的擷
取，但其所衍伸出來的問題在於需要額外可靠的切割方法，以確保切割出來的部位能符合所
需求的格式。 
 
影像的情緒分類，一般是根據特徵點的資訊進行分類判斷，目前較多使用的分類方法
有：AdaBoost [28, 29]、Support Vector Machine (SVM) [30]以及 Neural Network (NN) [31]。這
些演算法皆是需經過資料訓練的過程，其共通特色為，用來訓練成模組或參數調整的原始資
料，會明顯影響分類的結果，所以若是用來做訓練的資料不夠具有代表性，分類結果就會與
實際情況產生落差。其中 NN 的內部參數無法保證能調整到最佳狀況，運算速度偏慢。
AdaBoost則屬於小巧輕便的分類器，但較容易受到雜訊干擾而影響結果，且極度依賴分類器
的選擇。SVM 適用性較廣，調整性佳，在各方面表現都相當優秀，但主要缺點在於訓練時
間過長，要運用在即時系統上，需事先準備好訓練過的模組，但這也是大部分分類器的共同
缺點。 
 
2.3 音視訊號融合 
    由融合的運作時機來分，訊號的融合基本上可大致分為早期融合、建模融合以及後期融
合，分述如下： 
 
2.3.1 Early Fusion (Frame-based Signal Fusion) 
    融合的運作在較早的特徵處理階段，對於音訊和視訊分別抽取出的特徵向量進行融合，
融合音視訊後的特徵向量再送至辨識器進行辨識。不同時間尺度和度量程度的音視訊特徵融
合增加了特徵向量的維度並影響到辨識結果。 
 
 
 10
音視訊融合系統的語音部份，選用 EMO-DB構成訓練語料庫，此語料庫包含七種情緒，
分別為 Happiness、Fear、Neutral、Disgust、Boredom、Sadness、Anger，分別由 10位 25-35
歲不同性別的男女生以不同情緒朗誦 10 個語句。另外我們以 Webcam 以及麥克風錄製語音
資料庫，由 10位同學以 5 種情緒朗誦 5 個語句，用以訓練建立比對資料庫。音頻訊號的辨
識，則使用英國劍橋大學工程系(Cambridge University Engineering Department, CUED)所開發
的軟體 HTK (HMM Toolkit) [34]來訓練與測試語音資料。在視頻訊號的辨識部分，採用支援
向量機(Support Vector Machine, SVM)以 C. C. Chang 等人開發的 LIBSVM (A Library for 
Support Vector Machines)來進行影像資料分類 [35]。 
 
3.2 音頻訊號情緒偵測方法 
音訊的特徵擷取，使用 HTK內建的 MFCC函式轉換，根據人耳聽覺特性，在對數頻譜
(Log Spectra)上取出的 12維特徵係數，加上一維的能量大小(Power Spectra)，再以此 13 個係
數做一次、二次導數運算，得到 39 維的特徵向量(HTK 的表示為 MFCC_E_D_A，代表
MFCC+Energy+Delta+Acceleration)。文獻指出，音高特徵(Pitch)為情緒的重要指標，但由於
HTK 內建的特徵並未包含音高特徵，因此需使用額外的工具如 OpenEAR 來擷取音高特徵
(Pitch, Delta Pitch, and Acceleration Pitch)與加上原本由 HTK 取得的特徵構成 42維的特徵向
量。 
 
我們使用 HTK 工具，針對語料庫搭配前述的特徵向量擷取方式建立關於聲音情緒的隱
藏式馬可夫模型(Hidden Markov Model, HMM)。在模型的訓練方面，我們將每ㄧ段的聲音起
伏如 Attack(擊發)-Decay(衰減)-Sustain(延續)-Release(消逝)等當成一個情緒音節(Emotional 
Syllable)。這七個情緒分別使用各包含六個狀態的 HMM 建模，如圖 3.2 所示，每個狀態的
Mixture Gaussian包含十個使用 Full Covariance Variance 的 Gaussian 分佈。另外對於 Silence
和 Short-Pause 則分別建立三個狀態和一個狀態的 HMM 模型以降低音節與音節之間特徵向
量的影響。 
 
 
圖 3.2 HMM 模型狀態圖 
 
在辨識未知語句時，由於不知句中會包含多少音節，所以使用句中可能包含一個或更多個情
緒音節的簡單文法代入，形成如圖 3.3 的音訊情緒辨識網路。 
 12
而我們在處理過程中添加了物件過濾條件，濾除掉不需要的物件，例如非五官之背景物件和
頭髮等，只擷取感興趣之物件並加以分類收集，再給予物件特定的名稱，以及儲存物件的大
小與位置參數。概述其搜尋分法，首先以雙眼作為一個重要的搜尋目標，此搜尋條件是利用
雙眼左右對稱及大小相稱的特性來做依據，接下來藉由眼睛位置來找尋代表嘴巴物件，擷取
後之物件位置參數 a~p，如圖 3.5 之標示，詳細物件位置參數定義請參考附錄 1。 
 
    經過濾後可得到眼睛、眉間與嘴巴之物件，使用邊緣偵測(Edge Detection)搜尋這些物件
的邊緣，針對這些特徵物件進行特徵點擷取。圖 3.6 為以邊緣偵測取得物件特徵之示意圖，
以嘴巴來說，我們擷取上下唇的端點與中心點，因其向量斜率可決定嘴型弧度；眼睛部份是
以特徵點計算長寬大小來決定眼睛的開合情形；眉間的紋路則可以表達眉頭的變化，根據此
三項資訊來記錄臉部的情緒。 
 
a
b
c
d
e
h
f
g
 
(a) 
i
j
k
l
 
(b) 
m
n
o
p
 
(c) 
圖 3.5 以連通法獨立物件  (a)左右眼(b)眉間距(c)嘴巴 
 
Mup(1) Mup(N)
Mdown(1) Mdown(N)
Mup(1) Mup(N)
Mdown(1) Mdown(N)
Mup(   N/2   )
Mdown(   N/2   )
 
(a) 
Eup
E down
Eleft
Eright
 (b) 
 
g1
Gw
Gh
 
 (c) 
圖 3.6 物件邊緣偵測特徵點擷取  (a)嘴巴特徵(b)眼睛特徵(c)眉間特徵 
 14
中將只有一個最大輸出存在。這些歸屬函數於刺激區塊的中心產生單一峰值，且相對應的輸
出也會隨著輸入往刺激區塊的邊緣移動而遞減。 
 
舉例來說，狀態變數 x 靠近刺激區域左邊界時，數列 13 12 11µ µ µ> > ， 23 22 21µ µ µ> > ，
33 32 31µ µ µ> > 會明顯地呈現相對關係。在極端的情況下，當每一個狀態變數 x都落在其所對
應的刺激區塊中心時，超空間中就會在每一個 Subset彼此的交會處產生一個模糊單點 1iµ = 。  
 
 
(a) 
 
(b) 
 
(c) 
圖 3.7 當 C=3 時不同 Subsets 的輸入歸屬函數 (a) Subset1 (b) Subset2 (c) Subset3 
 16
4.1音頻訊號情緒偵測結果 
    在語音辨識中，大多使用 HMM 模型來描述語句的狀態，本研究透過 HTK 系統來訓練
辨識語音資料，並選用德文的情緒語音資料庫 Emo-DB作為實驗測試語料庫，以其結果當作
參考資料，來比對我們自行錄製的五種情緒中文語音資料庫。本研究的情緒語音的辨識，首
先建立語音資料庫，標示每個音檔所屬情緒，透過 HTK 的內建函式，取出 MFCC 特徵係數。
將輸入的語音資料切分成訓練組與測試組，透過 HMM 分別對兩組資料作辨識，以產出訓練
測試之辨識資料，HTK整體工作流程如圖 4.1所示，並分別以三種不同訊號切割方式分類辨
識，以比較其差異與優缺點。 
 
 
4.1.1 GMM分類辨識 
在語音和語者辨識中，高斯混合模型 GMM (Gaussian Mixture Model)為最常使用的機率
分佈模型，在不考慮語音內容的情況下，將每個語句當成一種情緒，以 GMM 來訓練分類各
種語音的情緒，統計各種情緒相互間的機率分布，表一為使用 Emo-DB 語料庫的實驗結果，
而表二則是由自行錄製的中文語音資料庫以同樣的模型所得到的實驗結果。 
 
表一 Emo-DB 語料庫之情緒語音 GMM 辨識混淆矩陣 
Confusion Matrix 
 Fear Disgust Happiness Boredom Neutral Sadness Anger Accuracy(%) 
Fear 30 5 0 1 5 3 7 58.8 
Disgust 4 38 6 1 2 0 0 74.5 
Happiness 2 4 30 0 0 0 15 58.8 
Boredom 3 4 1 30 3 9 1 58.8 
Neutral 3 1 0 13 33 1 0 64.7 
Sadness 2 4 0 3 3 39 0 76.5 
Anger 0 1 4 0 0 0 46 90.2 
Overall Accuracy: 68.91% 
 
 
圖 4.1 HTK 語音辨識流程方塊圖 
 18
  
表三  Emo-DB 語料庫經情緒標記後之分類混淆矩陣 
Confusion Matrix 
 Fear Disgust Happiness Boredom Neutral Sadness Anger Accuracy(%) 
Fear 14 0 0 0 1 1 2 77.8 
Disgust 1 18 0 0 0 0 0 94.7 
Happiness 0 0 15 0 0 0 3 83.3 
Boredom 0 3 0 19 0 4 0 73.1 
Neutral 1 1 0 1 14 0 0 82.4 
Sadness 0 1 0 1 1 13 0 81.3 
Anger 1 0 0 0 0 0 32 97 
Overall Accuracy: 85.03% 
 
 
表四 中文語音資料庫經情緒標記後之分類混淆矩陣 
Confusion Matrix 
 Neutral Happiness Sadness Anger Surprise Accuracy(%) 
Neutral 27 0 0 0 3 90 
Happiness 0 21 0 1 8 70 
Sadness 3 0 25 0 0 89.3 
Anger 0 0 0 29 1 96 
Surprise 2 6 0 1 21 70 
Overall Accuracy : 83.11% 
 
 
    此結果中雖還有些誤判的情形，但其情況已大幅改善，由於語句區段被切割辨識，所以
特徵資料較明確，其辨識率也較粗糙的 GMM 模型高。同樣的，由於語料庫的資料量不同，
因此兩種資料庫的辨識情況略有落差。 
 
 
圖 4.2 以 Emotion Syllable 切割的音檔 
 20
待 SVM訓練完畢後，藉由得到的模型對測試檔進行分類與預測，並計算出其識別率以作為
情緒影像偵測結果的判別，整體工作流程如圖 4.6所示。 
 
 
圖 4.5 資料庫影像範例 
 
 
由圖 1.1 的情緒線索(Emotion Curves)二維 Arousal- Valence空間平面圖中，得知中性情
緒 Neutral落於平面之中心點，為突顯各情緒之特徵，我們以 Neutral 為基準參考情緒，在錄
得的影像資訊中，除了 Neutral 之外的另外四個情緒，取其各特徵的特徵值平均，分別與
Neutral 之特徵做比較，如圖 4.7所示，其中橫軸的 ( ) , 1, 2,...,7i iα = 代表嘴型特徵，由於人臉
的情緒表現大多以嘴型的變化來判定，可參照圖 2.1 之影像，在 4.7 圖中前段表示嘴型特徵
值相較於 Neutral 有較大的變化，而 ( ) , 8,9,...,12i iα = 表示眼睛部分特徵，因眼睛的變化幅度
較不明顯，以至於 Neutral 特徵與其他情緒特徵之相吻性相對較高。由此結果可推測，以這
些特徵點作為表情辨識的依據，極有可能帶來誤判的情形，本研究利用 SVM 學習機器對這
些向量作訓練分析，以取得影像資料對於各情緒的辨識率，統計各情緒相互判定的辨識情
形，將其結果及整體辨識率列於表五中。 
 
 
圖 4.6 SVM 影像辨識流程方塊圖 
 22
Articulation Controller)具有快速的聯想記憶學習功能，在此我們使用其來學習音視訊在 A-V
平面上的相對規則，以此作為融合聲音和影像的核心。 
 
我們自行錄製的影音檔，共有五種情緒，分別為 Neutral、Happiness、Anger、Sadness、
Surprise，由前兩小節的偵測結果，可得到聲音(Sound)與影像(Image)資訊，並各自計算取其
情緒 Arousal 與 Valence量值，分別表示為 ( )SA t , ( )IA t , ( )SV t , ( )IV t ，並且取其時間軸上的
變化 ( )SdA t , ( )IdA t , ( )SdV t , ( )IdV t ，將這 8 項資料輸入到 FCMAC 取得 5 項輸出 ( )neuE t , 
( )hapE t , ( )angE t , ( )sadE t , ( )surE t ，其系統方塊如圖 4.8所示。由於從影像取得的結果 ( )I IA t , 
( )I IV t 與聲音取得的結果 ( )S SA t , ( )S SV t ，其時間取樣間隔不同，若要融合兩種訊號，則需把
時間單位統一，而影像部分的取樣時間為 1/10秒，聲音部分則為 1/100秒，因此最直接的方
法為取每 10筆聲音資料的平均，如圖 4.9所示，將聲音的取樣時間調整成 1/10秒。 
 
 
圖 4.8 FCMAC 音視訊融合系統方塊圖 
 
 
(a) 
 
(b) 
圖 4.9 音視訊號時間單位同步 (a)聲音原始向量(b)資料平均 
 
    取得聲音與影像同步資料後，為了使其有時間軸上連續性關聯，各自計算出這些數值與
前一時間之差異，其計算如下： 
( ) ( )( ) S SS
V t t V tdV t
t
+ ∆ −
=
∆
                                      (4.2a) 
( ) ( )( ) S SS
A t t A tdA t
t
+ ∆ −
=
∆
                                      (4.2b) 
( ) ( )( ) I II
V t t V tdV t
t
+ ∆ −
=
∆
                                      (4.2c) 
( ) ( )( ) I II
A t t A tdA t
t
+ ∆ −
=
∆
                                      (4.2d) 
 
 24
( ) ( )tt
IS dVA µµ ,...,
( ) ( )ttT
IS dVAi µµ ,...,=
3,2,1=i
( ) ( )2121332121 TTTTTTTTTTG −+−+−+=
8,...,2,1 iiiG
( ) jiiiW ,8,...,2,1
jD
5,...,2,1=j
 
圖 4.12 八維 FCAMC 模型訓練方塊圖 
 
最後我們以輸出誤差來觀察訓練成效，由圖 4.13 可觀察出 FCMAC 整體結果誤差趨於收斂，
訓練資料量越多則越準確。 
 
0 1 2 3 4 5 6 7
-25
-20
-15
-10
-5
0
5
10
15
20
25
Training cycle
Er
ro
r
 
圖 4.13 FCAMC 之訓練誤差 
 
將每種情緒相對的 Arousal 與 Valence 量值輸入於 FCMAC 模組訓練測試，其辨識結果
於表六所示。此結果為訊號融合後每種情緒之辨識混淆矩陣，對映圖 1.1 之情緒二維
Arousal-Valence 空間平面，所取的情緒種類多分布於上平面，而下平面只取 Sadness 一種情
緒，因此其誤判為其他情緒機會較小，另一方面，由於 Neutral 位於平面中心，會是每種情
緒均有機會被誤判的情況，故其辨識率較其他情緒來得小。 
 
 26
雖然以目前結果來看，音視訊號融合系統的情緒辨識率較單一訊號高，但由於融合系統
需取音頻和視頻訊號個別的特徵來做分析運算，以至於 FCMAC 需以八維度的函數來計算，
對於融合系統來說，這樣的資料量相當龐大，於訓練階段需要長時間的計算，因此，未來的
研究可透過心理學或生理學的文獻探討，引入更精確的情緒模型來簡化這樣的運算模式，以
利更多資料帶入此系統時，能更有效率的得到最佳的結果。 
 
6. 參考文獻 
[1]   J. Ruiz-del-Solar and P. Navarrete, "Eigenspace-based face recognition: a comparative 
study of different approaches," IEEE Systems, Man, and Cybernetics Society, vol. 35, no. 3, 
pp. 315-325, Aug. 2005. 
[2]   S. L. Phung, A. Bouzerdoum, and D. Chai, "A novel skin color model in YCbCr color 
space and its application to human face detection," International Conference on Image 
Processing, vol. 1, pp. I-289 - I-292, Oct 2002. 
[3]   H. Wu, Q. Chen, and M. Yachida, "Face detection from color images using a fuzzy pattern 
matching method," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 21, no. 6, pp. 
557-563, June 1999. 
[4]   Z. Hammal and A. Caplier, "Eyes and eyebrows parametric models for automatic 
segmentation," 6th IEEE Southwest Symposium on Image Analysis and Interpretation, pp. 
138-141, Mar. 2004.  
[5]   J. B. Gómez-Mendoza, F. Prieto, and T. Redarce, "Automatic lip-contour extraction and 
mouth-structure segmentation in images," Computing in Science and Engineering, vol. 13, pp. 
22-30, May-Jun. 2011. 
[6]   C. E. Osgood, J.G. Suci, and P. H. Tannenbaum, The Measurement of Meaning, The 
University of Illinois Press, Urbana, 1957. 
[7]   J. Pittermann, A. Pittermann, and W. Minker, Handling Emotions in Human-Computer 
Dialogues, Springer, Germany, Jan. 2010. 
[8]   A. Hanjalic and L. Q. Xu, "User-oriented affective video content analysis," IEEE 
Workshop on Content-Based Access of Image and Video Libraries, pp. 50-57, Dec. 2001. 
[9]   R. W. Picard, Affective Computing, Cambridge, MIT Press, 1997. 
[10] M. Pantic and L. J. M. Rothkrantz, "Toward an affect-sensitive multimodal 
human-computer interaction," IEEE Proc., vol. 9, no. 9, pp. 1370-1390, Sep. 2003. 
[11] Y. H. Yang, Y. C. Lin, Y. F. Su, and H. H. Chen, "A regression approach to music emotion 
recognition," IEEE Trans. on Audio, Speech and Language Processing, pp. 448-457, 2008. 
[12] Y. H. Yang and H. H. Chen, "Ranking-based emotion recognition for music organization 
and retrieval," Trans. on Audio, Speech and Language Processing, vol. 19, no. 4, May 2011. 
[13] E. H. Kim, K. H. Hyun, S. H. Kim, and Y. K. Kwak, "Improved emotion recognition with 
a novel speaker-independent feature," IEEE/ASME Trans. on Mechatronics, vol. 14, pp. 
1083-4435, 2009 
[14] A. W. Whitney, "A direct method of nonparametric measurement selection," IEEE Trans. 
on Computers, vol. 20, no. 9, pp. 1100-1103, 1971. 
[15] P. Pudil, F. J. Ferri, and J. Kittler, "Floating search methods for feature selection with 
nonmonotonic criterion functions," IEEE Trans. on Computer Vision and Image Processing, 
vol. 2, pp. 279-283, Oct. 1994. 
[16] L. R. Rabiner, "A tutorial on hidden Markov models and selected applications in speech 
recognition," Proc. of the IEEE, vol. 77, pp. 257-286, Feb. 1989. 
 28
[36] C. C. Jou, "A fuzzy cerebellar model articulation controller," IEEE Int. Conf. on Fuzzy 
Systems, pp. 1171-1178, Mar. 1992. 
[37] S. H. Lane, D. A. Handelman, and J. J. Gelfand, "Theory and development of higher-order 
CMAC Neural Networks," IEEE Control System, pp. 23-30, Apr. 1992. 
[38] M. Brown and C. J. Harris, "A Perspective and critique of adaptive neurofuzzy systems 
used for modelling and control applications," Int. J. of Neural Systems, vol. 6, no. 2, pp. 
197-220, 1995. 
 
附錄 1. 物件位置參數定義 
 
 
 
 
 
 
 
 
 
 
 
 
 
附錄 2. 已發表期刊論文與研討會論文 
已投稿期刊論文 2 篇： 
[1] C. S. Lin, "Adaptive spectrogram morphing for synthesizing multichannel recordings," 
submitted to Journal of the Acoustical Society of America. 
[2] C. S. Lin and J. S. Wang, "Fast ensemble empirical mode decomposition for speech-like 
signal based on non-white noise addition," submitted to IEEE Signal Processing Letters. 
 
研討會論文 4 篇： 
[1] C. S. Lin, S. J. Hong, D. L. Shih, and Z. C. Cheng, "Instantaneously warping interval 
computation using low level intrinsic mode functions,"  IEEE International Conference on 
Granular Computing, pp. 393-398, Kaohsiung, Taiwan, Nov. 2011. (EI) 
[3] C. S. Lin, J. S. Wang, and Z. C. Cheng, "Fast ensemble empirical mode decomposition for 
speech-like signal analysis using shaped noise addition," 4th International Conference on 
Interaction Sciences: IT, Human and Digital Content, pp. 112-117, Busan, Korea, Aug. 2011. 
(EI) 
[4] Z. C. Cheng, C. S. Lin, and Y. H. Chen, "PAT-tree based DTW for fast music retrieval," 23rd 
Symposium of Acoustical Society of the Republic of China, pp. 105-115, Taichung, Taiwan, 
Nov. 2010. 
[5] C. S. Lin and Y. C. Chao, "Spatial audio analysis based on perceptually empirical mode 
decomposition," 40th AES Conference on Spatial Audio: Sense the Sound of Space, Tokyo, 
Japan, Oct. 2010. 
左眼Y軸座標位置
左眼X軸大小(寬度)
左眼Y軸座標位置
左眼Y軸大小(高度)
a
c
b
d
左眼X軸座標位置
左眼X軸大小(寬度)
左眼Y軸座標位置
左眼Y軸大小(高度)
e
h
f
g
嘴巴的X軸座大小 (寬度)
嘴巴的Y軸座標
嘴巴的X軸座標
嘴巴的Y軸座標大小(高度)
m
o
n
p
相對於雙眼，眉間的寬度
相對於雙眼，眉間的Y軸座標
相對於雙眼，眉間的X軸座標
相對於雙眼，眉間的高度
i
k
j
l
 
 2 
在 Keynote Speech 方面大會邀請了三位來分別來義大利與地主國的專家，由
Prof. Angelo Farina講述 History and current state of the war between the 
two main approaches to surround sound: discrete speaker feeds versus 
hierarchical matrix，Prof. Yoiti Suzuki 講授 Auditory displays and 
microphone arrays for active listening，以及身兼作曲家以及教授的 Dr. 
Mikako Mizuno傳達關於當代音樂的空間概念。 
 
 
二、與會心得 
抵達東京後閱讀大會所提供之論文集並準備報告，大會主要語言為英語，其
次為日語。排定的議程在第二天上午，主持人由Dr. Kazuho Ono擔綱，參與者多
半是音訊分析與雜訊濾除的專家，而我們提出的論文Spatial audio analysis 
based on perceptually empirical mode decomposition乃利用人耳的感知模
型，配合經驗模態分解法對空間聲響做分析；藉由最後的Q and A得到了不少學
者的指點與建議，這樣的討論除了激發出許多新的研究想法，從中也了解到自己
在研究上的盲點，在此除了感到獲益良多之外，也特別要感謝國科會在旅費上的
部分補助才得以成行。 
 
另外，於活動中認識了來不少來自日本的教授與學生，相談之下得知由於日
本的音響工業發達，相關企業由於希望得到最新的技術，多半會與大學合作，並
以較低的價格提供大學相關研究設備，加上日本的音響系統價格原本就比台灣的
售價要來得低，其整體的研發成本就低了許多，也因此間接鼓勵了學者與學生們
從事音響學相關的研究工作。而獨自來自歐美的博士生也不少，這也使得學生們
得以激發學習動機與擴展國際視野。 
 
 
三、考察參觀活動 
此次大會於第一天的活動中於NHK Science & Technology Research 
Laboratories安排了22.2聲道的demo，讓與會者有機會感受到更逼真的音場重
建，雖說此一技術已於2005年第一次發表，但這次的demo其實不只包含了硬體與
軟體，其播放內容其實才是真正的關鍵(現有的商業規格軟體內容播放只支援到
7.1聲道，而硬體的擴大機官方的說法則是支援到9.1聲道)，相對應的Video畫面
則是以日本的本土文化為主，而素材的選擇重點在於其相關自然景觀與人文活動
所收錄下來的聲音是否能在短時間內將聲音的特質清楚的表達出來，所以低頻的
鼓音、多而小音量的火山熔岩爆發音、清脆的玻璃撞擊聲與一般很難察覺的微風
吹拂聲，都以22.2聲道為前提被收錄、處理與配合現場空間再生，當然，NHK高
畫質的巨型螢幕投影機所播放出來的畫面亦為整個來自四面八方的聲音加分不
少，期待這樣的商品能儘早商業化並引入家庭，使得身歷聲不再是進電影院才會
 4 
 
六、其他 
與會照片： 
1. NHK Science & Technology Research Laboratories 音樂會會場： 
 
 
2. 大會所安排之日本古樂表演節目： 
 
SPATIAL AUDIO ANALYSIS BASED ON PERCEPTUALLY EMPIRICAL
MODE DECOMPOSITION
CHINGSHUN LIN AND YUNGCHENG CHAO
Department of Electronic Engineering
National Taiwan University of Science and Technology
43, Section 4, Keelung Rd., Taipei, Taiwan
mentortw@gmail.com AND M9702112@mail.ntust.edu.tw
The goal of immersion system is to create the illusion of proximity for people in different areas. To achieve this, it is
essential to pick-up and regenerate all the crucially visual and aural information that is perceptible by human senses.
One of the important decisions in any audio processing system is to choose appropriate features. The criteria may count
on how exactly the signals are represented, and how easily the following process can be performed. In addition, it is
more challenging if we would like to simulate the acoustical characteristics around a specific instrument based on the
universal recording. The totally distinct spectral characteristics generated by the target instrument should dominate the
recording, whereas the subtly surrounding audio clues must be preserved as well. In this paper, we analyze such spatial
audio and demonstrate how it can be systematically computed by the perception-based empirical mode decomposition.
As an example of our method, analysis for reference microphone signal and its counterpart that simulates what a spot
microphone would pick up near a percussive instrument is presented.
INTRODUCTION
Vividly spatial recreation of video significantly depends
on the accurate presentation of audio. On the other hand,
visual observation also plays an important role in human
localization and could overwhelm the aural impression
when any inconsistency happens between both percep-
tions. The creation of seamlessly aural environment re-
quires not only the reproduction of sound characteris-
tics, but also the reproduction of sound spatiality. How-
ever, audio signal processing, especially for the spatial
enhancement, is neglected compared to the bloom of im-
age and video processing. Virtual microphone is one of
the immersive audio processing topics that can be used
to create multichannel recordings from mono or stereo
materials [1]. This technique is able to transform one mi-
crophone signal into another while preserving the correct
acoustical characteristics of the recording venue. For ex-
ample, in the 5.1 audio system the extra four channels can
be extracted from the two-channel CD format, and then
be integrated in order to provide the six-channel DVD au-
dio. Another application of virtual microphone is to use
this technique to remix the old recordings into multichan-
nel audio even though the recording venue does not exist
anymore. However, it is challenging if we would like to
imitate the acoustical characteristics around a specific in-
strument for the same rendering, because such recordings
have extremely different spectral characteristics compared
to those of reference signals. These signals would be in-
dispensable in the multichannel rendering because they
would allow a much richer rendering of a complex mu-
sical performance with proper instrument placement and
imaging. The purpose of this work is to analyze the signal
that a microphone would pick up if it were present near
the instrument during the same classical music recording
by using the relative spectral transform-perceptual linear
prediction (RASTA-PLP) [2] and 2D empirical mode de-
composition [3]. The organization of this paper is as fol-
lows. In the next section, we formulate signals in the
time-frequency representation because it not only simul-
taneously provides temporal and frequency information,
but also remains free from tracking sinusoids and their
phases [4]. In Section 2, we explicate the principle of
empirical mode decomposition which is used for spectral
feature analysis. Experimental results are then presented
in Section 3 to illustrate the performance of proposed sys-
tem. Finally, conclusion and future research are provided
in Section 4. The flowchart outlining such a spatial au-
dio representation for 2D perceptually empirical mode
decomposition is illustrated in Fig. 1.
1. SPATIAL AUDIO REPRESENTATION
In this work, the feature representation and indexing are
used to pre-process the signals collected from recordings
in a concert hall, and then the spectral features may be
used for analyzing close-up microphone signals from other
recordings. To perform the spectral feature analysis, it
is first necessary to define a number of reference signal
pairs. As shown in Fig. 2, one of the pair-wise refer-
ence signals is recorded with one of the two microphones
used in the ORTF (Office de Radiodiffusion Te´le´vision
AES 40th International Conference, Tokyo, Japan, 2010 October 8-10 1
Time/10ms Frames
Fr
e
qu
e
n
cy
/B
a
rk
RASTA-PLP spectral features of microphone near conductor
100 200 300 400 500 600 700 800 900
5
10
15
20
25
Time/10ms Frames
Fr
e
qu
e
n
cy
/B
a
rk
RASTA-PLP spectral features of microphone near tympani
100 200 300 400 500 600 700 800 900
5
10
15
20
25
-10
-5
0
5
Figure 3: Segmented RASTA-PLP spectral features ex-
tracted from different locations of a concert hall.
tion, and cardiac arrhythmias measurement [8]. The com-
ponents result from EMD, called intrinsic mode function
(IMF), is characterized by two properties:
1. The number of extrema and the number of zero
crossings should differ by no more than one.
2. The local average defined by the average of the
maximum andminimum envelopes is zero, i.e., both
envelopes are locally symmetric around the enve-
lope mean.
Based on these rules, signal may be decomposed into a
number of IMFs. Considering a real and stable sequence
s(t), which may be divided into fine-scale details and a
residue. The IMFs of the signal s(t) are found by iterat-
ing the following sifting processes:
1. Initialize r0(t)← s(t), and i← 1.
2. Find the i-th IMF.
(a) Initialize gi,j(t) ← ri(t), and the number of
sifts j ← 0.
(b) Find the local maxima and minima of gi,j(t).
(c) Estimate the maximum envelope ui,j(t) of gi,j(t)
by passing a cubic spline through the local
maxima. Similarly, find the minimum enve-
lope li,j(t) with the local minima.
(d) Compute an approximation to the local aver-
age: mi,j(t)← 0.5(ui,j(t) + li,j(t)).
(e) Extract the detail gi,j+1(t)← gi,j(t)−mi,j(t),
and let j ← j + 1.
Spectrogram of microphone near conductor
Time/10ms Frames
Fr
e
qu
e
n
cy
0 100 200 300 400 500 600 700 800 900
0
0.5
1
1.5
2
x 104
Spectrogram of microphone near tympani
Time/10ms Frames
Fr
e
qu
e
n
cy
0 100 200 300 400 500 600 700 800 900
0
0.5
1
1.5
2
x 104
-140
-120
-100
-80
-60
-40
-20
0
Figure 4: Segmented spectrograms extracted from differ-
ent locations of a concert hall.
(f) Check whether the stopping criterion defined
as the standard deviation from two consec-
utive results in the sifting process is smaller
than a given value ϵ [8].
SD(t) =
T∑
t=0
[gi,j+1(t)− gi,j(t)]2
g2i,j(t)
(5)
where T denotes the length of original signal.
If SD(t) ≥ ϵ, return to step b). If not, let the
i-th IMF xi(t)← gi,j+1(t).
3. Update ri+1(t)← ri(t)− xi(t).
4. Repeat step 2) with i ← i + 1 until the residue
ri+1(t) has at most one extremum or a constant re-
mained.
The goal of sifting is to repeatedly subtract the large-scale
features of the signal from the fine-scale ones. Finally,
the signal s(t) is represented as a sum of IMFs and the
residue:
s(t) =
n∑
i=1
xi(t) + rn(t) (6)
The sifting processes remove the low frequency informa-
tion in each step of EMD until the highest frequency com-
ponent remains. Theoretically, adding all the IMFs to-
gether with the residue can reconstruct the signal without
signal distortion. Cubic spline interpolation is commonly
used to approximate the upper and lower envelopes in the
EMD; however, it usually fails to model boundaries, es-
pecially those with abrupt changes. Although one of the
approaches to circumvent this problem is to use a longer
AES 40th International Conference, Tokyo, Japan, 2010 October 8-10 3
Spectral features near conductor
200 400 600 800
5
10
15
20
25
Spectral features near tympani
200 400 600 800
5
10
15
20
25
Fr
e
qu
e
n
cy
/B
a
rk
200 400 600 800
5
10
15
20
25
Fr
e
qu
e
n
cy
/B
a
rk
200 400 600 800
5
10
15
20
25
200 400 600 800
5
10
15
20
25
200 400 600 800
5
10
15
20
25
Time/10ms Frames
200 400 600 800
5
10
15
20
25
Time/10ms Frames
200 400 600 800
5
10
15
20
25
-50
0
50
Figure 6: (Followed by Fig. 5) From top to bottom: The fifth to the eighth 2D-IMFs that correspond to the spectral
features.
1. Find the maxima and minima of the spectral feature
S(t, f) by the morphological reconstruction.
2. Estimate the maximum envelope ui,j(t, f) andmin-
imum envelope li,j(t, f) by connecting local max-
ima and local minima with RBFs, respectively.
3. Compute an approximation to the local average:
mi,j(t, f)← 0.5(ui,j(t, f) + li,j(t, f)).
4. Extract the detail Si,j+1(t, f)← Si,j(t, f)−mi,j(t, f).
5. Check whether the stopping criterion predetermined
by a fixed number of iterations is satisfied. If so,
processing for one of the IMFs is finished, other-
wise repeat steps 2-4.
As defined in the one-dimensional EMD, the above pro-
cesses must be continued until there is no more 2D-IMF.
3. EXPERIMENTAL RESULTS
3.1. Differences between Perceptual EMD and Con-
ventional Spectral Analysis
In this experiment, the sounds recorded from a symphony
performance are adopted for spatial audio analysis. There
are two signals we used to verify this system, includ-
ing microphone signals recorded above the conductor and
around the tympani. As shown in Fig. 4, the segmented
spectrograms directly extracted from the reference and
desired signals distribute over the similar bands, and re-
sult in the poor discrimination without further processing.
Compared to the RASTA-PLP spectral features shown in
Fig. 3, the conventional time-dependent spectral analysis
is not necessarily efficient owing to the nonlinear percep-
tion of human hearing. On the other hand, the 2D-IMFs
also come from several repeated experiments, and one of
the results is illustrated in Fig. 5 - Fig. 7. From the first
IMF to the seventh IMF, we cannot observe any distin-
guished correlation between the reference spectral fea-
tures (the first column) and their corresponding parts (the
second column). This explains why it is not easy to syn-
thesize a recording of specific instrument via any other
reference signals. The IMFs of reference signal continue
varying until the eighth IMF, whereas there is no any re-
markable change along the time axis after the second IMF
of desired signal. It implies that the detail of desired spec-
tral characteristics may be extracted by only a few sifting
processes, and the remaining relatively small IMFs could
be reasonably ignored in the perception-based synthesis.
This phenomenon may be observed in detail by further
expressing the fifth to the eighth IMFs with the related
scale (see Fig. 7). Moreover, the reference spectral fea-
ture and its counterpart show the high degree of depen-
dence across frequency bands in the eighth 2D-IMFs and
the residues. Through this experiment, this method not
only shows the correlation of texture, but preserves the in-
tegrity of spectral feature based on its adaptive basis. Us-
AES 40th International Conference, Tokyo, Japan, 2010 October 8-10 5
Spectral features near conductor
100 200 300 400
5
10
15
Spectral features near tympani
100 200 300 400
5
10
15
Fr
e
qu
e
n
cy
/B
a
rk
Vertical
100 200 300 400
5
10
15
Fr
e
qu
e
n
cy
/B
a
rk
Vertical
100 200 300 400
5
10
15
Horizontal
100 200 300 400
5
10
15
Horizontal
100 200 300 400
5
10
15
Time/10ms Frames
Diagonal
100 200 300 400
5
10
15
Time/10ms Frames
Diagonal
100 200 300 400
5
10
15
-20
-10
0
10
Approximation Approximation
Figure 8: From top to bottom: The approximation, vertical, horizontal, and diagonal coefficient matrices of 2D-DWT that
correspond to the spectral features used in the 2D-EMD experiment.
Spectral features near conductor
100 200 300 400
5
10
15
Fr
e
qu
e
n
cy
/B
a
rk
Vertical
100 200 300 400
5
10
15
Horizontal
100 200 300 400
5
10
15
Time/10ms Frames
Diagonal
100 200 300 400
5
10
15
Spectral features near tympani
100 200 300 400
5
10
15
Fr
e
qu
e
n
cy
/B
a
rk
Vertical
100 200 300 400
5
10
15
Horizontal
100 200 300 400
5
10
15
Time/10ms Frames
Diagonal
100 200 300 400
5
10
15
Approximation Approximation
Figure 9: Data are identical with those shown in Fig. 8 but reformulated with the related scale for each wavelet coefficient
matrix.
AES 40th International Conference, Tokyo, Japan, 2010 October 8-10 7
ACCEPTANCE LETTER
Dear ChingShun Lin, 
Congratulations! It is my great pleasure to announce you that your paper, Fast Ensemble Empirical Mode 
Decomposition for Speech-Like Signal Analysis Using Shaped Noise Addition, is accepted by the ICIS2011: The 
4th International Conference on Interaction Sciences which will be held from August 16-18, 2011 in Busan, Republic of 
Korea.
This year, we have accepted a large number of various papers from more than 25 countries and it wasn't easy work for 
us to select the most innovative and well-written papers among them. 
With the assigned ID and PW, you can check or revise your personal and paper information from web system of the 
ICIS2011. http://www.aicit.org/ICIS 
Mission and Aims
ICIS will trigger off the scientific community to propose topics that should be tackled from research perspective and let 
the community explain how to best use their tools for practical and theoretical problems of Interaction Sciences. Many 
various contributions are foreseen from prospective authors. This includes use-cases of theoretical tools and methods to 
solve practical problems. Such contributions should be as usable as possible by practitioners in the related field. We also 
expect research results from practitioners that have identified a problem that could be solved by tools from network 
sciences. One of the missions of ICIS is to make the scientific community aware of the importance of the issues in 
Interaction Sciences and to suggest means by which the problem may be solved by the scientific community. The 
contributions should stimulate interaction between theoreticians and practitioners and also have high potential impact in 
either field. 
We aims to bring together and share brilliant and ideas, accumulated knowledge and unique experiences for mutual 
benefit of researchers and practitioners and explore possible directions for development of Multidisciplinary and 
Hybrid/Convergent research in the areas of Interaction Sciences. 
Once again, congratulations on your paper acceptance.
I look forward to seeing you at the conference soon. 
Prof. Franz I. S. Ko, Ph.D.
General Chair, ICIS2011.
Honorary Director General, IBC, Cambridge, UK.
Vice-President, The World Congress of Arts, Sciences and Communications, Cambridge, UK.  
    
Address: 707, Seokjang-dong, Gyeongju-si, Gyeongbuk, 780-741, Korea(Rep. of) 
Registration Number: 505-10-96301 
TEL: +82-70-7730-2833 
Input target signal s(t)
and threshold ε
             Set   
noisetsts +← )()(
Original EMD
k-th trial finished and 
obtain M-th IMFs and r (t)         M
1+← kk
0←k
Nk ≤Yes
No
∑
=
+=
N
k
kii trtxN
tx
1
)]( )([1)( σ
 Noise generator
noise  SSDfor    )(
noisebrown  for    2
noisepink  for     1
noise  for  white   0
tkρ
α
α
α
=
=
=
Figure 1. Flowchart of improved ensemble empirical mode decomposition
for speech-like signal analysis using added shaped noises.
sible solutions in the sifting processes for minimizing mode
mixing [8]. The final presentation of the EEMD is still an
energy-frequency-time distribution, designated as the Hilbert
spectrum. Unlike the Fourier and wavelet transforms, EEMD
has no a priori defined basis, and therefore this technology
is capable of processing nonlinear and nonstationary signals
successfully. However, EEMD usually takes a long time
to obtain the consistent intrinsic mode functions (IMFs),
especially for signals with abrupt changes. In this work, we
would like to show how the introduction of shaped noises
may accelerate the EEMD processing for speech-like signal
analysis.
The organization of this paper is as follows. In the next
section, we review the processes of the original EMD, and
then explicate the principle of ensemble empirical mode
decomposition. In Section III, properties of shaped noises
for EEMD are explored, including colored noises and signal-
spectrum-dependent noises. Experimental results are then
presented in Section IV to illustrate the performance of
proposed system. Finally, conclusion and future research
are provided in Section V. The flowchart outlining such a
fast ensemble empirical mode decomposition for speech-like
signal analysis using added shaped noises is illustrated in
Fig. 1.
II. ENSEMBLE EMPIRICAL MODE DECOMPOSITION
The ensemble empirical mode decomposition (EEMD) is
proposed to improve the decomposition results in the EMD
method. The procedures for both algorithms are described
as follows.
A. Original EMD
Empirical mode decomposition (EMD) is one of the
effective approaches for processing nonstationary signals.
The components result from EMD, called intrinsic mode
function (IMF), is characterized by two properties:
1) The number of extrema and the number of zero
crossings should differ by no more than one.
2) The local average defined by the average of the
maximum and minimum envelopes is zero, i.e., both
envelopes are locally symmetric around the envelope
mean.
Based on these rules, signal may be decomposed into a
number of IMFs. Considering a real and stable sequence
s(t), which may be divided into fine-scale details and a
residue. The IMFs of the signal s(t) are found by iterating
the following sifting processes:
1) Initialize r0(t)← s(t), and i← 1.
2) Find the i-th IMF.
a) Initialize gi,j(t)← ri(t), and the number of sifts
j ← 0.
b) Find the local maxima and minima of gi,j(t).
c) Estimate the maximum envelope ui,j(t) of gi,j(t)
by passing a cubic spline through the local max-
ima. Similarly, find the minimum envelope li,j(t)
with the local minima.
d) Compute an approximation to the local average:
mi,j(t)← 0.5(ui,j(t) + li,j(t)).
e) Extract the detail gi,j+1(t) ← gi,j(t) −mi,j(t),
and let j ← j + 1.
f) Check whether the stopping criterion defined
as the standard deviation from two consecutive
results in the sifting process is smaller than a
given value ² [5].
SD(t) =
T∑
t=0
[gi,j+1(t)− gi,j(t)]2
g2i,j(t)
(1)
where T denotes the length of original signal. If
SD(t) ≥ ², return to step b). If not, let the i-th
IMF xi(t)← gi,j+1(t).
3) Update ri+1(t)← ri(t)− xi(t).
4) Repeat step 2) with i← i+1 until the residue ri+1(t)
has at most one extremum or a constant remained.
The goal of sifting is to repeatedly subtract the large-scale
features of the signal from the fine-scale ones. Finally, the
0 100 200 300 400 500 600 700 800 900 1000
−10
0
10
20
30
40
50
60
70
80
90
Number of trails
Si
gn
al
 to
 e
rro
r r
at
io
 (d
B)
Signal to added noise ratio: 15
 
 
White noise
Pink noise
Brown noise
SSD noise
Figure 4. Convergence rates for speech-like signal decomposition using
different shaped noises (λ = 15).
3) Repeat step 1 and step 2 with different white noise
series.
4) Obtain the ensemble means of corresponding IMFs of
the decompositions as the final result.
The iteration is terminated when the number in the ensemble
approaches a given boundary N :
xi(t) =
1
N
N∑
k=1
[xi(t) + σrk(t)] (9)
where xi(t) + σrk(t) is the k-th trial of the i-th IMF in
the noise-added signal, σ is the standard deviation of the
added noise, and rk(t) is the residual after extracting the
first k IMF components [7], [8]. As the noises in each
trial are different in individual trials, the noises can almost
be removed by the ensemble mean of entire trials. The
ensemble number N should be as large as possible for a
reliable result.
III. SHAPED NOISES
The EMD is a dyadic filter bank for any white (or
fractional Gaussian) noise-only series [11]. Recent studies
of the statistical properties of white noise showed that the
EMD is an effective self-adaptive dyadic filter bank when
applied to the white noise [12]. In general, if the length of
the time series is T , we can get blog2(T )c − 1 IMFs. The
ensemble number N and the noise amplitude σ are the two
parameters that need to be assigned in the EEMD approach.
The rule suggested in [8] is:
σN =
σ√
N
(10)
where σN is the final standard deviation of error defined
as the difference between the targeted signal and the corre-
sponding IMFs. If noise amplitude σ is too small, it could
0 100 200 300 400 500 600 700 800 900 1000
−10
0
10
20
30
40
50
60
70
80
90
Number of trails
Si
gn
al
 to
 e
rro
r r
at
io
 (d
B)
Signal to added noise ratio: 20
 
 
White noise
Pink noise
Brown noise
SSD noise
Figure 5. Convergence rates for speech-like signal decomposition using
different shaped noises (λ = 20).
not introduce any disturbance for signal diversity in EEMD.
The rule of thumb for noise amplitude setting is about one
fifth standard deviation of the amplitude of targeted signal
[8]. In addition, by increasing the ensemble members, the
effect of the added white noise will always be reduced to
a negligibly small level. In general, an ensemble number
of a few hundreds will lead to a consistent result. As the
improved EEMD flowchart shown in Fig. 1, in which we
take the EMD procedure as a special case by setting N = 1
and σ = 0.
A. Colored noises
Power-law noise is defined as a signal with components
at all frequencies and its power spectral density per unit
of bandwidth is proportional to 1/fα[13]. For white noise,
the spectral density is flat over the whole frequency by
setting α = 0, whereas pink noise has α = 1. Compared
with pink noise, brown noise has an even stronger shift in
energy towards the lower spectrum (α = 2). If the signal is
mainly composed of high-frequency components, the noise
amplitude may be relatively small. On the other hand, it
may be increased if the signal is dominated by the low-
frequency components. Therefore, for the signal with the
energy spectrum decreases with the increasing frequency,
pink noise or brown noise would be the suitably added
noise in the EEMD application with the spectral similarity
in between.
B. Signal-spectrum-dependent noises (SSDN)
EEMD fully uses statistical characteristics of the white
noise to perturb the target signal in their true solution neigh-
borhood and then cancels out the white noise via ensemble
averaging. Obviously, a small amplitude noise leads to a
small error for decomposition. However, if the added noise
With the inclusion of shaped noises in the signal decom-
position, the sifting process was repeated until 10 IMFs were
determined. Compared with the result of traditional EEMD,
as shown from Fig. 4 to Fig. 7, the proposed shaped noise
addition can greatly enhance the calculation speed with the
same decomposition accuracy. If the 90dB signal to error
ratio is treated as the final criterion, almost all the approaches
can reach it before the iterations have been terminated.
However, the white noise addition takes a longer time for
convergence in most cases. Moreover, if we use the 45dB
as the halfway criterion, most of the shaped noises finish
the calculation before hitting the 450-th trial (see Fig. 6
and Fig. 7), especially for the brown noise addition which
generally obtains the goal at the beginning. In general, the
shaped noise addition outperforms the white noise method
in terms of computation for the EEMD of speech-like signal.
Within a certain range of noise amplitude, the decomposition
results of the EEMD have little sensitivity to the noise
amplitude as long as the added noise has moderate amplitude
and the ensemble has sufficient trials. However, the ensemble
number should rise with the elevation of the noise amplitude
so as to decrease the contribution of the added noise to the
IMFs. Therefore, the decomposed results of the EEMD are
verified to be unique and robust.
V. CONCLUSION AND FUTURE RESEARCH
EEMD has been proposed to avoid the mode mixing in
EMD; however, it also brings the heavy calculation for
a consistent result. Since the energy of most real-world
signals distributes over the lower frequency, imposing high-
frequency ensemble averaging wastes a number of com-
putations. Compared to the conventional EEMD that adds
white noise as the perturbation, using shaped noise instead
provides a high convergence rate for the speech-like signal.
This work not only facilitates the processes of EEMD, but
also preserves the capability of mode-mixing alleviation
without scarifying the decomposition precision. Experimen-
tal results also provide a guideline on noise setting so that
the convergence rate of EEMD for the speech-like signal
analysis can be significantly improved. Therefore, a further
improvement is expected by designing a more adaptive noise
without losing its randomness. Generalizing this approach
to cover a broader variety of signals will be other intriguing
direction for future study.
ACKNOWLEDGMENT
The work reported in this paper was supported in part by
grant from the National Science Council of Taiwan under
contract NSC99-2221-E-011-013. The authors also thank
the anonymous reviewers for their useful comments.
REFERENCES
[1] W. J. Pielemeier, G. H. Wakefield, and M. H. Simoni, ”Time-
frequency analysis of musical signals,” Proc. IEEE, vol. 84,
no. 9, pp. 1216-1230, Sep. 1996.
[2] L. Cohen, Time-Frequency Analysis, Prentice-Hall, Engle-
wood Cliffs, NJ,1995.
[3] W. J. Staszewski, ”Wavelet based compression and feature
selection for vibration analysis,” Journal of Sound and Vibra-
tion, vol.211, no.5, pp. 736-760, 2000.
[4] P. M. Bentley and J. T. E. McDonnell, ”Wavelet transforms:
An introduction,” J. Electronics & Engineering, pp. 175-186,
Aug. 1994.
[5] N. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H. Shih,
Q. Zheng, N. C. Yen, C. C. Tung, and H. H. Liu, ”The
empirical mode decomposition and the Hilbert spectrum
for nonlinear and non-stationary time series analysis,” Proc.
Royal Soc. of London Series A-Mathematical Physical and
Engineering Sciences, vol. 454, no. 1971, pp. 903-995, Mar.
1998.
[6] Z. Wu and N. E. Huang, ”A study of the characteristics of
white noise using the empirical mode decomposition method,”
Proc. Royal Soc. of London Series A-Mathematical Physical
and Engineering Sciences, vol. 460A, pp. 1597-1611, 2004.
[7] N. E. Huang, M. L. Wu, S. R. Long, S. S. Shen, W. D. Qu,
P. Gloersen, and K. L. Fan, ”A confidence limit for the
empirical mode decomposition and Hilbert spectral analysis,”
Proc. Royal Soc. of London Series A-Mathematical Physical
and Engineering Sciences, vol. 459A, pp. 2317-2345, 2003.
[8] Z. Wu and N. E. Huang, ”Ensemble empirical mode decom-
position: A noise-assisted data analysis method,” Advances in
Adaptive Data Analysis, vol. 1, no. 1, pp. 1-41, 2009.
[9] Z. K. Peng, P. W. Tse, and F. L. Chu, ”An improved
Hilbert Huang transform and its application in vibration signal
analysis,” Journal of Sound and Vibration, vol. 286, no. 1-2,
pp. 187-205, 2005.
[10] S. L. Hahn, Hilbert Transforms in Signal Processing, Artech
House, 1996.
[11] P. W. Shan and M. Li, ”An EMD based simulation of frac-
tional Gaussian noise,” International Journal of Mathematics
and Computers in Simulation, vol. 1, no. 4, pp. 312-316,
2007.
[12] P. Flandrin, G. Rilling, and P. Goncalves, ”Empirical mode
decomposition as a filter bank,” IEEE Signal Processing
Letters, vol. 11, no. 2, pp. 112-114, Feb. 2004.
[13] N. J. Kasdin, ”Discrete simulation of colored noise and
stochastic processes and 1/fα power law noise generation,”
Proc. IEEE, vol. 83, no. 5, pp. 802-827, May 1995.
99 年度專題研究計畫研究成果彙整表 
計畫主持人：林敬舜 計畫編號：99-2221-E-011-013- 
計畫名稱：音視訊號融合之情緒偵測系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0%  
研究報告/技術報告 0 0 0%  
研討會論文 1 0 100% 
篇 
發表於中華民國
音響協會。 
論文著作 
專書 0 0 0%   
申請中件數 1 0 100% 
NN-based 
audiovisual 
signal late 
fusion for 
affective 
detection. 申請
中。 
專利 
已獲得件數 0 0 0% 
件 
 
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 1 0 50% 參與計畫人力碩士生共計 1人。 
博士生 1 0 50% 參與計畫人力博士生共計 1人。 
博士後研究員 0 0 0%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
 
期刊論文 0 0 0%  
研究報告/技術報告 0 0 0%  
研討會論文 3 0 100% 
篇 
國際研討會論文 3
篇，分別發表於
40th AES 
Conference, 4th 
ICIS, and 
GRC2011. 
論文著作 
專書 0 0 0% 章/本  
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
國外 
技術移轉 
權利金 0 0 0% 千元  
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
與此計畫相關送審中期刊論文 2篇，已接受研討會論文 4篇(詳見國科會資料庫或報告附
錄)，其中 2篇正改寫成期刊論文。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
人類的情緒表現長久以來一直是心理學、生理學、神經學、認知科學、社會學等學者所研
究之課題，近年來由於電腦理論的快速發展，突破以往單一向度的資訊處理框架，轉變成
可以與使用者雙向互動，而原本只能靠人力判斷的情緒表現，逐漸被電腦所取代。本研究
計畫之目的，在於擷取音頻與視頻訊號中的情緒特徵，並融合兩者以建立更精確的情緒辨
識系統。 
 
而完成之工作項目除衍生出審核中期刊論文 2篇，研討會論文 4篇外，亦有 2篇期刊論文
正改寫中，並完成相關軟體實作。而參與的學生亦藉由各種不同的實驗與模擬，配合實際
測試，從圖樣辨識的角度切入相關技術的開發。透過開發程式所達成的人機互動，將可有
效做到即時的情緒偵測，幫助我們判斷情勢的好壞、環境的安全、更有助於商業上客戶關
係的管理。而其他的效益尚有，若將情緒辨識系統結合於各類監視系統，便可從影音資訊
中得知每個人的情緒，如應用於監獄中的監視系統將可避免發生躁動，而連結於居家防護
的情緒監控系統則可透過無線設備的警報訊息來即時通知家人情緒是否有劇烈變化。另
外，由於近年來智慧型手機越見普及，情緒辨識系統將會是除了語音辨識系統外的下一個
重要功能，使用者可輕鬆得知通話對方的情緒，不用透過長串的對話去臆測彼此的情緒狀
況，以便達到更高品質的溝通交流。而此情緒偵測系統亦有助於醫療人員從事如自閉症方
面的治療，讓病人的相關資訊不但可以透過醫生診斷得知，同時也讓這套系統提供醫病雙
