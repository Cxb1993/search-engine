2研究計畫成果報告中文摘要
關鍵詞: 前景背景分割、陰影移除、人物偵測、人物追蹤
遠距教學與視訊會議是愈來愈普及的資訊應用，這兩項應用的共通點在於必須克服網路頻寬的限
制，將影像序列傳遞至遠端的電腦作呈現。利用電腦視覺技術來偵測與追蹤場景中人物的動作與表
情，可以有效降低必須傳遞的資料量，以達成即時反應與互動的要求。遠距教學與視訊會議中演講者
以投影片做簡報時，由於光線投射的因素，很容易在牆壁或地上留下陰影，造成後續分析的困難。專
業的演講者經常在場景移動，並有豐富的肢體動作，也有可能多人輪流或合作一起講授，複雜的互動
過程中，很可能會有被遮蔽的情況發生(演講者互相遮蔽或被其它物件遮蔽)。本計劃針對這些問題，
以兩年的時間，研發關鍵的技術。第一年已設計並實作一個考慮顏色、材質、空間與時間關聯性的前
景背景分割與陰影移除演算法，第二年已開發一個具可適性之多視角人物追蹤的系統。我們開發出的
技術具有一般性與實用性，除了遠距教學與視訊會議以外，期望可以擴展到其它更多的應用。
4目錄
一、前言……………………………………………………………………………5
二、研究目的………………………………………………………………………5
三、文獻探討………………………………………………………………………7
四、研究方法………………………………………………………………………14
研究成果(一) 背景與前景分割演算法…………………………………………14
研究成果(二) 動態背景更新演算法……………………………………………16
研究成果(三) 基於亮度、鄰居、顏色與時間特徵之陰影偵測演算法………17
研究成果(四) 立體影像之前景切割與人物偵測………………………………22
研究成果(五) 人物多視角樣本之建構與更新…………………………………24
研究成果(六) 利用多視角樣本之人物辨識與追蹤……………………………25
五、結果與討論……………………………………………………………………29
六、結論與建議……………………………………………………………………44
七、參考文獻………………………………………………………………………45
八、計畫成果自評…………………………………………………………………51
附錄一、2007 出席國際學術會議報告………………………………………53
附錄二、2008 出席國際學術會議報告………………………………………54
6景與背景以偵測場景中的人物，區塊式的背景相減法(background subtraction)可以把前景人物所佔據的
區塊從背景中區隔出來，而背景影像須要不斷地動態更新方能確保有效的前景偵測。前景人物一旦在
場景中被偵測了出來，我們提出了一個陰影移除演算法，可以透過背景影像與前景影像上的顏色、亮
度、材質、空間上鄰居與時間關聯性等，來找出場景中出現的影子區域，並將其移除，可以使我們之
後的分析與處理更加準確。
人物特徵的偵測與追蹤在電腦視覺的領域上有很多的研究與探討，也是一門非常有趣的議題，例
如一個人物該用什麼樣模式的資料來記錄、如何利用樣本比對來判定人物的身份、如何應付追蹤比對
的時候所遭遇到的各種的問題(如比例變化、遮蔽、旋轉等)。在單一特徵擷取的人物追蹤系統中，由
於過多環境因素的影響，會導致偵測出來的結果通常不太理想；例如，膚色特徵在動態光源下不穩定、
運動特徵易受雜訊干擾、形狀特徵計算複雜度過高等，所以我們希望能發展一個架構使得人物追蹤產
出的結果能更可靠，以適應各種不同的環境，並有效的降低計算的成本。
圖四為本計劃第二年的系統架構圖，整個系統分成立體前景切割、多視角樣本建構、人物辨識與
追蹤等三大部份，首先，我們利用距離特徵所產生出來的三度空間資訊來對人物進行辨識及追蹤，藉
由兩個已校正並以光軸平行方式擺設的攝影機分別攝取左右兩個影像，將背景和暫時的前景區塊分別
做立體視覺計算，根據深度(depth)資訊來做進一步的立體前景切割(stereo foreground segmentation)，再
將切割出的前景區塊投影至前向圖(front map)，並做人物偵測以得到候選區塊。其次，我們建構人物
多視角樣本資料庫來紀錄該人物的每一個不同方向的全身顏色分佈，藉此辨別人物的全方位特徵相似
度，並解決追蹤失效或全部遮蔽之後回復的問題。在辨識與追蹤系統上，我們分析個別人物區塊在下
一張 frame 中所移動的位置，估測該人物的運動方向與速度，並且持續進行修正與追蹤。在遮蔽復原
系統上，我們針對個別追蹤的結果，製定一系列的程序利用多視角樣本相似度來判斷追蹤的人物是否
產生失效、或是否有遮蔽的狀況發生，如果發生了遮蔽的狀況，系統就會利用現有的資訊去做還原或
比對的工作。
我們過去曾經開發一個人物偵測與追蹤之多模組融合系統[73]，並曾經建構一個多人線上的虛擬
英語教室(VEC3D)[40]；在本計畫中，我們以過去的經驗為基礎，針對前景分離、陰影去除、視覺部
位切割與遮敝復原等關鍵的問題設計有效的演算法，建構可靠實用的系統，並期望可以擴展到遠距教
學與視訊會議及其它更多的應用。
8在即時的情況下，透過Support Vector Domain(SVD)分類的方式來達成即時的影子移除的效果。然而，
這個系統最大的缺點就是要先透過手工的方法，把影子的樣本點找出來給系統，在戶外的場景的情況
使用時，如清晨、中午與傍晚都要有不同的訓練樣本，也就是要事前要將各種的樣本都找齊，是相當
耗時費力的方法。
陰影移除已有許多方法被提出，Toth[65]首先將影像轉到LUV的空間中，透過mean shift演算法，
對各個主要顏色進行分類，之後根據前景背景亮度比率，分別判斷每一個像素是否為影子；若一個區
域有一半以上像素都被判斷成影子，則判定該區域為影子。然而這個方法無法將半影的區域清除乾
淨，需要透過型態過濾器(morphological filter)來將沒有完全移除的半影區域，作完整清除。此外，這
個方法沒有更有效的利用顏色的資訊，也沒有使用到邊的資訊來判斷該點是否屬於影子，因此仍有可
以改進的空間。
Stauder[62]使用亮度，靜止邊緣和遮蔽資訊，來進行陰影的偵測，由於他們並沒有將顏色的資訊
考慮進去，因此這個方法的在前景與背景都是不具有材質的單調平面時，會發生判斷的錯誤。
Fung[19]藉由觀查亮度、色度、梯度的變化，將多種資訊結合成一張分數的影像，透過這張影像
的分數高低，來決定那些區域可能是陰影，並對其進行移除。然而這樣的方法須要假設已知部份幾何
資訊，且會導致前景物的內部具有許多的空洞，因此這個方法仍只適合在交通道路情況下使用。
Funt[20]提出一種記錄各點與其鄰居的亮度比率的方法，由於這個比率不會因光線的變化而改
變，可以藉以進行物件辨識。這樣的方法給了我們一些構想來應用在影子移除的部份，因此，我們設
計一個結合顏色、亮度、材質、鄰居資訊的演算法，來偵測並移除在場景中移動的陰影區域。相關陰
影偵測演算法的比較如表一所示。
表一、 相關陰影偵測演算法比較
type Shading Edge neighbor
Toth [65] Color Yes NO Yes
Stauder [62] Gray Yes Yes Yes
Fung [19] Color NO Yes Yes
Ours Color Yes Yes Yes
10
將前景與背景區分開來(foreground/background segmentation)[9]，此法常見於視訊壓縮的應用中；第
四種方法則是透過時空高斯過濾器(spatio-temporal Gaussian filter)[48]，同時在時間與空間上做變動
之偵測。
(3) 距離特徵(depth) : 影像中每一個點的距離(或稱深度)的資訊亦可做為物件偵測[11,34]的線索。立體
求距(depth by stereo)的原理是利用兩個不同位置的攝影機所攝取的一對影像中，同一特徵點所在位
置差(disparity)，來還原此特徵點與攝影機的距離(Depth)，藉此獲得一個距離影像(Range Image)。
由於通常影像中之人物與背景物件有明顯的景深差異，因此距離特徵提供人物偵測非常重要的線
索。
(4) 形狀特徵(contour): 最早的人物偵測系統即是利用人物輪廓的特性，通常這些方法都會做一些基本
假設，例如固定的人物形狀與單調的背景等。之後，許多動態形狀模型也被用在人物偵測上，例
如 Snakes[35], Deformable templates[74], Point distribution models[8]等。
(5) 特徵佈置(feature layout): 利用人體對稱的特性或人臉五官之相對位置關係做人物之偵測[5]。通常
採取由下而上的方式，先找出較小區域之特徵，再利用相對位置之關係來將人物定位。雖然可以
利用臉部部位特徵樣板(如眼睛)來找出特定的人物的臉部區域，但是卻會受到比例變化或旋轉的強
烈影響。
(6) 統計的方法(Statistical methods): 此類方法不用經驗法則分析特徵，而是使用統計的方法，透過訓
練的過程直接在影像上以視窗掃描的方式做人物偵測，例如 Principle component analysis[60], Linear
discriminant analysis[4], Eigenface[66], Multilayer perception[33], Support vector machine[28]等。這類
方法必須事先做樣本收集、挑選跟訓練，前置作業耗時費力。
但是，從這些文獻中，我們發現大部分的方法都有其限制；例如使用膚色過濾的方法只有在人臉
影像是正面或側面的情況下才能有效偵測；使用動態追蹤的方法只有在人物移動時才能發揮功能；使
用立體深度影像的方法只能得到較為粗糙的線索，無法作精確的偵測。
12
以統計的方法計算。Particle Filter[51,53]也是屬於時間傳播法的一種。
本計劃我們先將前景人物從背景中擷取出來，以建構多視角樣本特徵對人物全身進行追蹤，再利
用每個區塊的相對位置不變性質可以對遮蔽狀況做分析處理。而對於一個人物離開場景後又重新出現
在影像之中的狀況，系統會將新進物件跟資料庫裡所存人物樣本資料進行比對，以判定新偵測到的人
物是屬於第一次進入場景的物件或是之前已進入過場景的人物。主要系統流程如圖三與圖四所示。
Background
Subtraction
Multimodal
Fusion
Input
Image
Sequence
Color Constancy
between pixels
Color Constancy
within pixel
Temporal
Consistency
圖三、背景前景分割與陰影偵測之系統流程圖
Shadow Detection
Background
Update
Foreground
Background
Foreground
after Shadow
Removal
Shadow Removal in
Dynamic Background
工作項目一
工作項目二
工作項目三
14
四、研究方法
本計劃以兩年的時間，針對視覺式偵測與追蹤關鍵性的問題，研發解決的方法。第一年已完成背
景前景分割與陰影偵測演算法之設計與實作，第二年已開發一個具可適性之多視角人物追蹤系統。
我們開發出的技術具有一般性與實用性，除了遠距教學與視訊會議以外，期望可以擴展到其它更多
的應用。
第一年的主要系統流程如圖三所示，首先透過背景減法(background subtraction)找出有改變的前景
區域，並動態選擇性地更新背景，接著再針對所取出的前景區域進行分析，在場景中有些靜態的影子
存在，我們把它視為背景的一部份；真正須要偵測並移除的是場景中會隨物件移動的影子。經過觀察
後發現，陰影的特徵包括相鄰像素的亮度比、單一像素的內部顏色資訊、與時間關聯性等三種，透過
分析並結合這三種資訊，我們可以有效的偵測出影子的區域。
第二年的系統架構如圖四所示，整個系統分成立體前景切割、多視角樣本建構、人物辨識與追蹤
等三大部份，首先，我們提出了一個前向圖(front map)的投影方式作為立體前景切割的依據，再分析
深度(depth)切割的結果來進行人物偵測。其次，我們建構人物多視角樣本資料庫來紀錄該人物的每一
個不同方向的全身顏色分佈，藉此辨別人物的全方位特徵相似度，並解決追蹤失效或全部遮蔽之後回
復的問題。在辨識與追蹤系統上，我們分析個別人物區塊在下一張 frame 中所移動的位置，估測該人
物的運動方向與速度，並且持續進行修正與追蹤。在遮蔽復原系統上，我們針對個別追蹤的結果，製
定一系列的程序利用多視角樣本相似度來判斷追蹤的人物是否產生失效、或是否有遮蔽的狀況發生，
如果發生了遮蔽的狀況，系統就會利用現有的資訊去做還原或比對的工作。本計劃研究成果有六項，
分別詳述如下:
研究成果(一)、背景與前景分割之演算法
一般以運動為根據之人物偵測系統可分為靜態攝影機(static camera)與動態攝影機(active camera)
兩種狀況。使用靜態攝影機的偵測系統通常將時間上相鄰之影像相減(temporal subtraction)，或是將影
像與事先準備好的背景影像相減(background subtraction)，即可將運動的物體過濾出來。使用動態攝影
機的偵測系統由於背景並不固定，因此做背景相減前應先做背景運動補償的處理，方能得到正確的結
16
義為背景，由此可以偵測出場景中前景人物的位置。這種區塊式的方法缺點是也會把一些背景也給包
含進去，但是這針對整個系統並不會有很大的影響。
到目前為止的實驗結果而言，程式可以快速且準確的擷取出前景物，只是背景的改變可能會導致
背景移除法所產生的影像有殘留的區塊發生，研究成果二將提出在特定的條件下動態更新背景影像的
方法。此外背景前景分割與後續追蹤很容易受到影子的干擾，所以研究成果三探討去除影子的演算
法。另外一個問題就是當所取的區塊大小過大時，雖然執行速度會加快，但是前景物分割的不夠精細，
所取出來的區塊除了前景物之外也會包含一些背景像素，這會使得後續更新顏色樣本模型的時候，把
我們所不要的部份給登記進去，未來將繼續研究這個問題。
研究成果(二)、動態背景更新之演算法
背景減法(background subtraction)須要準備一張準確的背景影像，但是由於光源的改變或背景物的
移動，背景影像可能隨時間逐漸改變。為了解決這個問題，背景影像可以取最近 n 張影像的平均，以
遞迴的方式動態修正:
),(
1
),(
1
),(
1
),( 1
1
0
1 yxIn
yxB
n
n
yxI
n
yxB kk
n
i
kk 
 



其中 Ik 為在 k 時間點攝影機所攫取到的畫面，而 Bk 為在 k 時間點更新後的背景影像。n 的大小可以調
整更新的速度，n 太大將導致背景影像無法立即反應變動，n 太小則背景影像容易受到雜訊干擾。由
於更新後的每一個像素顏色是由數個像素平均而來，也可能得到完全未曾在攫取影像出現的顏色。為
了避免上述缺點，有些系統採用中間數來取代平均值，如下式:
)],(),...,,(),,([),( 11 yxIyxIyxImediumyxB nkkkk 
在彩色影像中，兩個顏色只能計算距離，無法直接比較大小，因此，中間數的計算比較複雜，可
以將與其它數值總距離最小的 Ii(x,y)做為中間數，定義如下:


 
1),(
11 ),(),(min)],(),...,,(),,([
nkjk
ji
yxI
nkkk yxIyxIaugyxIyxIyxImedium
i
除了中間數與平均值之外，也有參數式的方法，透過逼進高斯函式的參數來記錄每一個像素曾經
出現的顏色。另外，也有統計式的方法，透過 histogram 的計算來產生背景影像[68]。這些方法的目的
都是希望能快速且有效地找到正確的背景顏色，並將雜訊的干擾降至最低。上述背景更新應該只針對
18











),(
),(
),(
),('
),(
),(ˆ
),(ˆ
),(
),('),(
ji
ji
ji
jiI
jiI
yxK
yxK
jiI
jiIyx
其中為中心點在(x,y)的一個固定大小的視窗， Kˆ 是在視窗內所有位置的平均前後亮度比值。透過
這個式子，當得到的值小於一個門檻值，就判斷(x,y)為影子所覆蓋的區域。然而這個方法在半影的區
域經常會造成誤判，尤其是當考慮視窗較大時，或在變化緩慢的半影區域會特別嚴重(如圖五所示)，
因此Toth在做完這個方法之後仍需透過morphological filter將半影的區域移除。
我們所提出的方法，主要的概念是透過像素與其鄰居會具有一個固定不變的比例關係，在光源改
變的情況下，仍會維持不變，因此透過這樣的特性，我們可以將被判定為前景物中的影子移除。將(x,y)
位置與(x+1,y)位置鄰近兩點的亮度相除，我們可以得到這兩點的反射率的關係，假設兩相鄰的點接收
到相同的能量值E(x,y)，我們可以由上述反射模型導出下面的式子:
),1(
),(
),1(),1(
),(),(
),1(
),(
yx
yx
yxyxE
yxyxE
yxI
yxI




 



),1(
),(
),1(),1(
),(),(
),1(
),(
yx
yx
yxyxE
yxyxE
yxI
yxI










假設(x,y)位在一個陰影的區域，則 ),(),( yxyx   且 ),1(),1( yxyx   ，代入前式可得
),1(
),(
),1(
),(
yxI
yxI
yxI
yxI



我們可以經由這個式子將整張影像的反射率關係(或亮度關係)儲存起來，成為一張鄰居亮度比的
影像。這張鄰居亮度比的影像，除非受到前景遮敝，否則不會因為光線的變化而受到干擾，都會保持
一定的常數值。鄰居亮度比的影像可以透過一個梯度運算子快速計算，所得到的影像就是一張對數空
間上的 edge map，只有邊緣才會具較高的強度，如下式












)1,(ln),(ln
)1,(
),(
ln),(
),1(ln),(ln
),1(
),(
ln),(
yxIyxI
yxI
yxI
yxd
yxIyxI
yxI
yxI
yxd
v
h
其中 dh(x,y)指的是在當前影像在(x,y)位置上的水平方向鄰居亮度比，dv(x,y)指的是在當前影像在
(x,y)位置上的垂直方向鄰居亮度比。同理假設 dh'(x,y)與 dv'(x,y)是背景影像在(x,y)位置上的水平與垂直
方向鄰居亮度比。當 d(x,y)與 d'(x,y)明顯的變化時，將可以判定該區域可能是被前景物遮，反之則為陰
20
訊，來彌補彼此的不足，使我們的判斷在絕大部份的情況下得到良好的結果，增加系統的穩定性，這
樣的特性可以使得在後端的人物偵測及各種分析都能做的更好。
根據觀察得知，當一個像素被陰影覆蓋時，只會造成該點亮度降低，顏色則不會有太大變化。因
此，除了考慮與相鄰像素的亮度關係之外，我們也可以考慮各像素的顏色變化(color constancy within
pixel)，做為陰影偵測的依據。每一個像素可以分成 R、G、B 三種不同的顏色頻道，假設強度分別為
Ir, Ig, Ib，我們可以將 RGB 空間正規化轉換到一個顏色與亮度分離的 r-g 空間，轉換過程可以寫成下面
的式子:
( , )
( , ) ln
( , ) ( , ) ( , )
( , )
( , ) ln
( , ) ( , ) ( , )
R
R G B
G
R G B
I x y
r x y
I x y I x y I x y
I x y
g x y
I x y I x y I x y
   
   
其中 r 與 g 值不會隨著光線的改變而受到影響，會維持一個大致固定的值。因此我們又可以透過
下面的式子來計算當前影像顏色(r,g)與背景影像顏色(r,g)的誤差:
),('),(),('),(),( yxgyxgyxryxryx 
此距離在影子所產生的區域，會是一個很小的值，因此我們可以藉此特性找出可能為影子的區域。
當一個像素連續幾個 frame 都被判定成陰影，在前景物件移動速度不會太快的前題下，則下一個
frame 該像素是陰影的機率自然比較高。我們可以利用這個時間上延續的特性做為陰影判定的線索，
假設 ),( yx 是利用此特性將(x,y)判定成影子的誤差，我們考慮前數個時間點的判定記錄，用遞迴的方
式來計算 ),( yx 。最後，把鄰居亮度比、顏色、與時間一致性等三項特徵都列入考慮後，總誤差函式
可以採用線性整合寫成下面式子:
),()1(),(),(),( yxyxyxyxT  
其中α與β分別是亮度比資訊與顏色資訊的權重，可以依照不同的情況來進行調整。當我們計算完影
像上各位置的總誤差 T(x,y)之後，我們可以取一個適合的門檻值來判定屬於陰影區域的像素。
22
研究成果(四)、立體影像之前景切割與人物偵測
首先，經由左右兩台攝影機可以分別取得左右兩個不同的影像序列，在計算三度空間座標之前，
要先進行前景的切割動作(Foreground Segmentation)。影像中的前景切割問題向來一直是令人感興趣的
研究領域，因為人物通常是前景物體，因此要如何從我們所擁有的影像中去將前景物體與背景物體作
一個良好的切割，其中最廣為人知的前景切割方法就是背景去除法(Background Subtraction)。背景去除
法的觀念相當簡單，主要是利用事先所取得的不含任何前景物體的背景影像去與需要做前景切割動作
的影像計算差異，他的好處在於相當的方便快速，可是相對的，由於事先取得的背景影像是固定的，
所以一旦在拍攝的過程中背景有所變化，或者因光影變化，或者受到外力改變，都會導致在做背景去
除的時候，受到不良的影響。因此，一些相關的研究文獻使用了動態的背景去除法(Dynamic Background
Subtraction)，除了擷取事先的背景影像之外，亦不斷的偵測那些屬於背景影像的部分，進而將這些確
定為背景影像的部分，對事先所擷取的背景影像做同區域的更新，經由不斷的對全影像執行這個偵測
及更新的動作以達到背景影像的可適應性。有鑑於此，我們設計一個建立在立體視覺之上的改良方法
來做背景去除。首先針對最初連續 10 個影像畫框(Image Frame)，擷取此 10 個影像畫框的平均值做為
背景影像的模型(Background Model)，然後對每一個影像序列中的影像做背景去除，如下式:
0 | ( , ) ( , ) |
( , )
( , )
m
s
if C x y B x y T
B x y
C x y otherwise


， －
，
假設 C(x,y)為影像序列中的影像，Bm(x,y)為我們所取出來的背景影像模型，Bs(x,y)則是初步的影像
前景切割結果。接下來我們利用立體視覺計算將影像像素轉換成三度空間的座標點群，假設 ( , )tp x y 為
一連續的影像序列中在第 t個時間點的影像，經由立體視覺計算之後， ( , ) ( , , )t tp x y P X Y Z ，可得其
相對應的三度空間座標 ( , , )tP X Y Z 及該點的顏色C( , , )x y z ：
C( , , ) = C( , ) ( , ) ( , , )X Y Z x y p x y P X Y Z ｔ ｔ
其中C( , )x y 表示該點在影像中的顏色，C( , , )X Y Z 表示該點轉換到三度空間之後的顏色，假設點在二
度影像空間的顏色與點在三度空間的顏色應該相同，我們利用立體視覺計算的方法將背景影像模型也
轉換成三度空間的座標點群，接著利用下式將前景做過濾：
( , ) ( , ) ( , , )
( , , )
0
s Bs BmB x y if Z Z p x y P X Y ZF X Y Z
otherwise
   

ｔ ｔ， －
，
24
在投影到前向圖座標之後，我們根據視野的範圍以為寬度設定直條的數量，並以 h 做為人的正
常高度，劃分出我們所認為人物可能出現的區域，接著再根據我們規劃出來的直條，計算出每個直條
坐落於其中的總點數，以及這些點的平均 Z 值，再以點的數量及其平均 Z 值作為人物偵測的判斷依據。
max
max
1 ( , ) *
( , )
0 ,
front front
front front
Z Z
if N x y a b
ZBin x y
otherwise
  

－
,
其中 Bin(xfront,yfront)表座標為(xfront,yfront)的直條是否為人，N(xfront,yfront)為座標(xfront,yfront)之直條中的總點
數，為了因應在不同深度(Z 值)的狀況，人物在前向圖之中所投影的點數也不盡相同，所以我們設置
了一個能夠隨著 Z 資訊不同，而隨之更動的門檻值 max
max
*
Z Z
a b
Z
 － 。
研究成果(五)、人物多視角樣本之建構與更新
在完成立體影像之人物偵測之後，可以得到數個人物候選區塊，我們使用多視角人物樣本(Multiple
View Template, 簡稱 MVT)，來代表每一個人物的模型，在每個人的 MVT 之中，可以存有多張不同視
角的影像，儲存的影像數量 i 可以視需求而定，第 m 個人的 MVT 的一般式定義如下式：
Qm,n={Im1, Im2,…, Imi,candn}
為了方便比較第 n 個人物候選區塊與第 m 個 MVT，我們除了原本 MVT 中的 i 張影像之外，另外
再加入第 n 個人物候選區塊的影像 candn。MVT 可以依照需求設定佇列中存有的影像數量，不過當
MVT 的總數或者 MVT 中的影像數量過多的話，勢必會增加儲存空間的需求，因此在我們將影像存入
MVT 之前，先將影像轉換成直方圖(Histogram)以利儲存。一般的直方圖都是藉由統計各個顏色點的總
點數來製成，其橫座標為顏色，縱座標為點數，定義如下式：
*
0 0
( ) 1 ( , , ) ( , , ) ( , , ( , ))
h s
t t
A front front front front
b a
G color if C X Y Z color P X Y Z P x y Z x y

 
    ，
其中 A 為 Qm,n 中的一張影像，我們藉由此式將前向圖中偵測到的人物作顏色上的統計，其中 h 為
前向圖長條高度，s 為人物所佔的長條數量，δ為長條寬度，color 可以視需求而為灰階或者色調值
(Hue)。經由此直方圖的顏色統計，我們可以得知整體的顏色分布。此種直方圖雖然能夠得知構成影像
的主要顏色，可是卻沒有辦法掌握這些顏色的分布狀況，換句話說，即使構成的主要顏色相同，但是
這些顏色經由排列組合之後，可能出現的情況卻不只有一種。因此，使用這種直方圖的作法極有可能
26
其中 ( )
ncand front
G y 為 candn 的一般直方圖， ( )i
m
frontI
G y 為 Imi 的一般直方圖， ( )
ncand front
H y 為 candn 的改良
直方圖， ( )i
m
frontI
H y 為 Imi 的改良直方圖。將 candn 與 Imi分別在一般直方圖與改良直方圖上的平方差和
的計算結果相加起來，作為 candn 與 Imi 兩張影像之間差異度的最後定義。
由於 MVT 中共有 i 張影像，所以 candn 與 Qm 一共會有 i 個一般直方圖平方差和跟改良直方圖平
方差和的相加結果，我們必須從此 i 個平方差和的相加結果中挑選出最小者，作為代表 Qm 中與 candn
最相近者；由於個人 MVT 共有 m 個，所以必須再從 m 個個人 MVT 之中挑選出距離最小者，也就是
最接近者，方法如下式：
min ( , ) min ( , )
i
n m n mSSD cand Q SSD cand I
min minarg min ( , ) min ( , )
( )
1
n m n mmm
n
SSD cand Q if SSD cand Q
ID cand
m otherwise


，
，
上式所得到的結果 ID(candn)便是 candn 的 ID，而是相似度的門檻值，如果在 m 個人物 MVT 中所找
到的最相近者與 candn 的距離過大，也就是不夠相近，則將此 candn 視為新出現的人物並給予 m+1 作
為其 ID。當一個人從場景中出現的時候，這個人可能是之前曾經出現過的人物，這時系統應將該人物
所建立的人物區塊模組資料，跟之前所儲存在人物資料庫中的 MVT 做比對，如果區塊比對結果正確
的話，則判定該人物為曾經出現過之人物；如果找不到比對結果正確的資料，則判定該人物為新出現
的人物，並將此新人物與其所有子區塊顏色分佈登錄進資料庫。
在找到 candn 的 ID 之後，我們必須考慮是否對 candn 所對應的個人 MVT 進行更新，因此我們將
candn 加入其所對應的個人 MVT 中，與原本 MVT 中的 i 張影像成為新的影像集合，以再次計算 candn
所對應的個人 MVT 中的各個影像相互之間的距離，藉此來判斷 MVT 是否該對 candn 來進行更新。距
離的計算方法是利用平方差和之和(Sum of Sum of Squared Difference, SSSD)，這邊需要注意的是這裡
所說的平方差和之和與之前的直方圖的平方差和相加結果，雖然計算方法類似，但是在實質上的意義
不同。距離的平方差和計算如下式:
,
2
0
( , ) ( ( ) ( ))
m n
front
h
Q J front K front
y
SSD J K H y H y

  －
其中 J 與 K 分別為 Qm,n中的一張影像，經由上式可以得到 Qm,n 中所有影像相互計算出來的平方差
和，由於在 Qm,n 中一共有 i+1 張影像，所以 J 對 K 會產生 i+1 個平方差和，我們將此 i+1 個平方差和
28
的左右型遮蔽意即人物受到背景物體的影響，而由左而右或由右而左的逐漸受到遮蔽而消失，這種類
型的遮蔽現象其實對於人物偵測來說並不會很明顯，當受到遮蔽的部份不夠大的時候，如果其轉換至
三度空間中的點數仍然足夠，則仍可偵測到人物，直到被遮蔽的部份過大時，轉換至三度空間的點數
不夠，則會被判斷成人物消失。左右型的遮蔽現象與人物出入場景的消失情況極為類似，相當難以區
分。所謂的上下型遮蔽指的是人物受到背景物體的影響，而有上半身或者下半身受到遮蔽現象而突然
消失的情況，這類遮蔽的特色在前向圖的長條中，三度空間中的點會集中投影在長條的上半部或者是
下半部；因此，我們對於此類型遮蔽現象的偵測方法便是對前向圖的人物偵測設置長條的半人門檻
值，當同一長條中，上半部或下半部超過門檻值而另一半卻沒有超過時，我們則判別為發生上下型的
遮蔽現象，並且停止個人 MVT 的更新動作。
相較於部分遮蔽的起因主要是受場景物體的影響，短期遮蔽則大部分發生在人物行走時相互交
錯，或者是暫時性離開場景的時候，人物短暫的消失一陣子再出現。為了能夠正確的掌握場景中各個
人物的狀態，以便偵測暫時性離開場景的情形，基本上我們對於影像序列中每個時間點的畫框都去進
行人物的偵測、建模、辨識與更新動作。我們將人跟人之間的互動所可能產生的短期遮蔽情況大致分
為遮蔽後交錯與遮蔽後返回兩種。這兩種情況最大的差別就在於解除遮蔽現象之後，若不經過識別的
動作，非常容易產生追蹤錯換(Track Switch)的錯誤；我們秉於不論是遮蔽後交錯或者是遮蔽後返回，
其在遮蔽現象發生之前人與人必定先靠近的原理，當兩個人因為過於靠近而被人物偵測判斷為一個人
的時候，此由兩人聯合而成的區域必定有著誇張性的肥大，於是我們對於發生誇張性肥大的狀況發出
遮蔽現象的警告，此時由於我們斷定有遮蔽現象發生，為了不讓個人 MVT 因此而做錯誤的更新，所
以我們對於所有消失的人物停止一切的更新動作。由於我們採取對於每個畫框皆進行偵測及辨識，所
以當兩個人由過於靠近的誇張性肥大分開成正常的兩人時，可以根據其先前所儲存的個人 MVT 來進
行人物識別，藉由這樣的處理，我們可以解決人物互動之後，所產生的短期遮蔽現象。
延續遮蔽現象的判斷可以說是遮蔽現象中最困難者，延續遮蔽後出現與第一次出現的人物，只有
使用外觀資訊的系統能加以判斷，而我們提出的個人 MVT 的優勢便在於只要曾經出現便會建立其
MVT，不論人物消失多久，都可以藉由個人 MVT 識別之。延續遮蔽與短期遮蔽頗為類似，唯一的不
同就是消失時間的長短，如何分辨物體是遭到長時間的遮蔽還是離開視野是值得繼續研究的問題。
30
來的結果，我們可以發現在影子外圍的半影區域發生大量的誤判。而在圖八(c) 是我們單純考慮材質
(between pixel)的結果，在半影的區域得到比較好的結果，但是還是有部分的半影被誤判為前景。而在
圖八(d) 是我們結合材質(between pixel)與顏色(within pixel)的結果，大部分的半影都被正確偵測出來，
但是人物身上還是有一些誤判造成的空洞。而在圖八(e)是我們結合材質、顏色與時間關聯性的結果，
可以看得出來偵測結果相當好，空洞的問題也修正了。圖八 (f)是各種方法 Receiver Operating
Characteristic (ROC)曲線的比較，橫軸為 Miss Detection Rate，縱軸為 False Alarm Rate。
32
(a) (b)
(c) (d)
(e) (f)
圖七、車輛偵測的例子 (a)原始影像 (b)Toth 方法的結果 (c)單純考慮 within pixel
的結果 (d)結合 within pixel 與 between pixel 的結果 (e)同時考慮 within pixel、
between pixel 與時間關聯性的結果 (f)各種方法 ROC 曲線的比較
34
第二年的實驗結果主要分成人物偵測、人物樣本建構、人物辨識的結果以及正確率等四個部份，
而其中人物辨識的結果和正確率兩個部份皆分別比較了使用一般的樣本比對與使用影像佇列的差
別。我們使用的立體攝影機為 STH-MDCS-VAR，深度影像之建構採用 SRI Small Vision System。人物
偵測的結果如圖九。個人影像佇列樣本建構的結果如圖十。人物辨識與追蹤，圖十一為無遮蔽的狀況，
圖十二為部份遮蔽的狀況。圖十三為短期遮蔽後交錯的狀況。圖十四為遮蔽後返回的狀況。圖十五為
延續遮蔽的狀況。圖十六與圖十七為三人以上複雜交錯的狀況。圖十八為亮度變化的狀況。
圖九、人物偵測的實驗結果
圖十、個人影像佇列樣本建構的實驗結果
36
圖十二、部份遮蔽的實驗結果
38
圖十四、短期遮蔽之遮蔽後返回實驗結果
40
圖十六、三人以上的實驗結果
42
圖十八、亮度變化實驗結果
正確率實驗的部份，我們先對實驗數據的種類做一些定義，偵測失誤(Miss Detection)之分母為所
有影像序列中所出現的人次總和，意即總共出現幾個必須被偵測為人物的狀況，分子即為理應被偵測
成為人物卻沒有偵測的發生數量。錯誤偵測(False Alarm) 之分母為所有影像序列中所被偵測的人次總
和，意即在整個影像序列中總共偵測了幾個人物，分子即為不該被偵測成為人物卻偵測成為人物的發
生數量，包含了追蹤分裂及識別錯誤所造成的假偵測。追蹤分裂(Track Split)之分母為所有影像序列中
所追蹤的人數總和，分子即為理應是同一個人物卻分裂成兩個或兩個以上人物的發生次數。追蹤錯換
(Track Switch)之分母同上述追蹤分裂之分母，分子為當人物成功偵測之後，卻發生因為識別錯誤而產
生的人物 ID 交換狀況。我們根據各種錯誤的種類去進行個人影像佇列中存放影像數量不同所造成影
響的實驗，共分成一張、二張以及四張等三種個人影像佇列。
如表二所示，我們可以看的出來發生的錯誤大部分都是發生在偵測失誤，其他假偵測、追蹤分裂
與追蹤錯換三種錯誤比較少見；而偵測失效之所以數量較多的原因，大部分是發生在人物剛進入或離
開場景，身處於可見視野的邊界地帶，由於左右兩台攝影機會有視差上的視野差距，因此當人物在其
中一台攝影機中出現時，在另一台攝影機中並不一定就會跟著馬上出現，因此造成了在立體視覺計算
44
六、結論與建議
根據第一年實驗結果，在地上有多重影子的狀況下的 Toth 的方法會在半影的區域產生明顯的 miss
detection；尤其在半影寬且考慮的鄰居較多的情況下，會使得誤差的值變大，而得到錯誤的結果；而
我們結合多種特徵的陰影偵測演算法，在半影的區域也不會有明顯的 miss detections 發生；即使在人
物的內部有大量與背景相似的單調材質的情況下，我們的方法也不會產生明顯的 false alram，可以得
到比較理想的結果。只有在具有 specular 的反光特性的區域(如門把)，因為不符合我們光線反射模型
所預估的變化，導致偵測失誤(miss detection)，是值得繼續改進的問題。
第二年我們完成了以人物偵測、人物定位以及人物追蹤為訴求的人物追蹤系統，實驗結果顯示利
用我們所提出的前向圖進行人物偵測的方法是可行的；除此之外，我們所提出的個人影像佇列的人物
建模方法，可以有效處理發生遮蔽現象之後所產生的誤判狀況。比較可惜的是對於遮蔽現象我們僅止
於能進行偵測，然後對於偵測結果再佐以個人影像佇列的辨識，而無法對遮蔽現象做出更進一步的分
割與識別，這一部分將是未來值得繼續進行的方向；另一個值得改進的地方是系統的執行時間，由於
個人影像佇列的利用必須進行相當複雜且龐大的運算，因此相較之下在時間上的考驗便處於弱勢，如
何改進系統的執行速度以達到即時運算，也是值得繼續思考的方向。
46
International Conference on Automatic Face and Gesture Recognition, 1998.
[14] N. Eveno, A. Caplier, P. Coulon, A new color transformation for lips segmentation, IEEE Workshop
Multimedia Signal Processing, pp. 3-8, 2001.
[15] N. Eveno, A. Caplier, P. Coulon, Accurate and Quasi-Automatic Lip Tracking, IEEE Transaction on
Circuits and Systems for Video Technology, vol. 14, no. 5, pp. 706-715, May. 2004.
[16] N. Eveno, A. Caplier, P. Coulon, Key Point Based Segmentation of Lips, IEEE International Conference
of Multimedia and Expo., vol. 2, pp. 125-128, 2002.
[17] P. Fieguth & D. Terzopoulos. Color-based tracking of heads and other mobile objects at video frame
rates, IEEE Conference on Computer Vision and Pattern Recognition, 1997.
[18] M. Fleck, D. Forsyth, & C. Bregler. Finding naked people. Proceedings of European Conference of
Computer Vision, pp. 593-602, 1996.
[19] G. Fung, N. Yung, G. Pang, A. Lai. Towards detection of moving cast shadows for visual traffic
surveillance. IEEE International Conference on Systems, Man, and Cybernetics, pp. 2505-2510 Vol. 4,
Oct., 2001.
[20] B. Funt and G. Finlayson. Color Constant Color Indexing. IEEE Transactions on Pattern Analysis and
Machine Intelligence, Issue 5, pp. 522-529, May, 1995.
[21] T. Gandhi, M. T. Yang, R. Kasturi, O. Camps & L. Coraor. Detection of Obstacles in the Flight Path of
an Aircraft, IEEE Transactions on Aerospace and Electronic System, vol. 39, no. 1, pp. 176-191, January,
2003.
[22] M. Harville. Stereo person tracking with adaptive plan-view statistical templates, ECCV Workshop on
Statistical Methods in Video Processing, pp. 67-72,June, 2002.
[23] M. Harville, D. Li. Fast integrated person tracking and activity recognition with plan-view templates
from a single stereo camera, CVPR, 2004.
[24] M. Harville. Stereo person tracking with short and long term plan-view appearance models of shape and
color. IEEE Conference on Advanced Video and Signal Based Surveillance, 2005.
[25] R. Herbert, S. Thomas, B. Csaba, W. Martin, & B. Horst, Shape -based Detection of Humans for Video
Surveillance Applications, IEEE Conference on Computer Vision and Image Processing, 2003.
48
[41] A. Lipton, H. Fujiyoshi, & R. Patil. Moving target classification and tracking from real-time video.
IEEE workshop on application of computer vision, 1998.
[42] K. Lo, & M.T. Yang. Shadow Detection by Integrating Multiple Features, International Conference on
Pattern Recognition. August, HongKong, 2006.
[43] K. Lo, M.T. Yang, & R. Lin. Shadow Removal for Foreground Segmentation, Lecture Notes in
Computer Science, vol. 4319, pp. 342-352, December, 2006.
[44] E. Loutas, I. Pitas, & C. Nikou. Entropy-based metrics for the analysis of partial and total occlusion in
video object tracking, IEE Proceedings on Vision, Image and Signal Processing, 2004.
[45] J. Lu, & M. Liou. A simple and efficient search algorithm for block-matching motion estimation. IEEE
Transactions on Circuits and Systems for Video Technology, vol. 7, no. 2, pp. 429-433, 1997.
[46] J. Luettin, N. Tracker, and S. Beet, Active Shape Models for Visual Speech Feature Extraction,
University of Sheffield, Sheffield, U.K., Electronic System Group Rep. 95/44, 1995.
[47] M. Mason & Z. Duric. Using histograms to detect and track objects in color video, 30th Applied
Imagery Pattern Recognition Workshop, 2001.
[48] S. McKenna, S. Gong, & H. Liddell. Real-time tracking for an integrated face recognition system.
Second Workshop on Parallel Modeling of Neural Operators. 1995.
[49] E. Patterson, S. Gurbuz, Z.Tufekci & J. Gowdy, Moving-Talker, Speaker-Independent Feature Study and
Baseline Results Using the CUAVE Multimodal Speech Corpus, EURASIP Journal of Applied Signal
Processing, no. 11, pp. 1189-1201, Nov. 2002.
[50] S. Russel, P. Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall, pp. 111-113, 1995.
[51] Y. Satoh, T. Okatani & K. Deguchi. A color-based tracking by Kalman particle filter, IEEE International
Conference on Pattern Recognition, 2004.
[52] J. Sethian. Level set methods: evolving interfaces in geometry, fluid mechanics, computer vision and
material science. Cambridge University Press, 1996.
[53] C. Shen, M. Brooks & A. Hengel. Augmented particle filtering for efficient visual tracking, IEEE
International Conference on Image Processing, 2005.
[54] Y. Shih, & M.T. Yang. A Collaborative Virtual Environment for Situated Language Learning Using
50
[67] C. Wang, Y. Chang, & Y. Chen. Moving object extraction using mosaic technique and tracking with
active camera. Proceedings of CVGIP, 2001.
[68] J. Wang & S. Chen. Progressive background image generation, CVGIP, 2001.
[69] G. Welch & G. Bishop. An introduction to the Kalman filter. Technical report, TR 95-041, Department of
Computer Science, University of North Carolina at Chapel Hill, 1995.
[70] C. Wren, A. Azarbayejani, T. Darrell, & A. Pentland. Pfinder: real-time tracking of the human body.
IEEE Transactions on Pattern Analysis & Machine Intelligence, 1997.
[71] Z. Wu, P. Aleksic, A. Katsaggelos, Lip Tracking for MPEG-4 Facial Animation, 4th IEEE International
Conference of Multimodal Interfaces, pp. 293-298, 2002.
[72] M.T. Yang, K. Lo, C. Chiang, & W. Tai. Moving Cast Shadow Detection by Exploiting Multiple Cues,
IET Image Processing (former IEE Proceedings Vision, Image & Signal Processing), vol.2, no. 2, April,
2008.
[73] M. T. Yang, S. Wang & Y. Lin. A Multi-modal Fusion System for People Detection and Tracking,
International Journal of Imaging System and Technology, vol. 15, issue 2, pp. 131-142, 2005.
[74] A. Yuille, P. Hallinan, and D. Cohen, Feature extraction from faces using deformable templates,
International Journal Computer Vision, vol. 8, no. 2, pp. 99-111, 1992.
[75] L. Zhao, Dressed Human Modeling, Detection, and Parts Localization, The Robotics Institute Carnegie
Mellon University, July, 2001.
52
 達成預期目標情況
兩年來之預期目標已如期完成，可延伸之方向目前持續研究中。
 研究成果之學術或應用價值
本計畫所研發的偵測與追蹤技術，是許多視覺式感知應用的基礎，不久的將來，應可以在一般大
眾的日常生活應用中扮演相當重要的角色，例如無線人機介面、機器人控制、保全監視等。基於我們
過去在物件追蹤上的研究成果，以及在工作站網路方面的即時實作經驗，藉由此計畫建立新的技術與
理論。
 是否適合在學術期刊發表或申請專利
目前已發表四篇期刊論文(兩篇 SCI [43,72]、一篇 SSCI[54]、一篇其它[55])、三篇研討會論文(兩
篇國際研討會[42,56]、一篇國內研討會[57])、及四篇碩士論文。
 主要發現或其他有關價值等
由於人物偵測與追蹤是發展智慧型機器視覺系統的關鍵技術，本計畫之研究成果可作為相關應用
的基礎，對相關的軟硬體產業也將有很大的幫助。
54
附錄二
行政院國家科學委員會補助國內專家學者出席國際學術會議報告
報告人姓名 楊茂村
服務機構
及職稱
國立東華大學資工系副教授
會議時間
會議地點
June 29-July 2, 2008
San Antonia, USA
本會核定
補助文號
於 95-96 年度專題研究計劃中核定
95-2221-E-259-027-MY2
會議
名稱
(中文) 第 29 屆美國教育計算會議
(英文) The 29th National Educational Computing Conference
一、參加會議經過
第 29 屆 NECC 美國教育計算會議是每年舉辦一次之國際盛會，由 ISTE(International Society for
Technology in Education)國際科技教育協會主辦，是美國最大的學習科技會議。本次會議在美國
德州聖安東尼市舉行，估計有 17,600 人報名參加，與會人士來自 57 個國家與美國 50 個州，包
含了學校英語教師與學習科技研究人員。四天的會議中包含了 924 個平行場次(sessions)的論文
發表，每個 sessions 分別在幾個會議廳內同時進行，類型非為正式論文發表(paper presentation)、
圓桌論文討論(paper discussion)與海報發表(poster)等三種；在緊密的論文發表之外，另外穿插了
3 場 keynotes 與 115 場 workshops，議程結構完整且內容豐富。第一天完成報到後，參加了由 Dr.
Surowiecki 主講的 keynote speech。我們的論文在第二天早上發表，圓桌的設計讓與會者有更多
討論與互動的機會。之後兩天並參加了主題為先進教學技術、資料探勘在教學上之應用等數個
sessions，中場休息時間我有多次機會與許多與會者針對虛擬實境學習平台交換意見，會議在 7
月 2 日傍晚正式閉幕。
二、與會心得
本次會議投稿與報名人數非常踴躍，可見各國在電腦輔助教學領域之研究非常活躍。發表之論
文中，除了歐美等先進國家外，亞洲國家則以日本，台灣為主。在跟其它與會者的討論過程中，
瞭解到許多人在教學上其實有借助網路虛擬學習平台的想法與需求，他們也對台灣在E-learning
領域的廣泛參與印象深刻。
三、考察參觀活動
會議後短期參訪賓州州立大學，賓州州立大學是一所國際知名的綜合型大學，校園除了許多古
典的宏偉建築外，也有很多現代化的軟硬體設備，傳統與科技結合並存，可見美國政府對大學
教育之重視與用心。
四、建議
國內 E-learning 之研究在會議論文與相關期刊表現上並不遜色，建議國科會對相關的專題研究計
劃的補助可以相對成長。
五、攜回資料名稱及內容
2. NECC2008議程手冊 2.NECC2008論文集
六、其他
這次會議議程安排緊密豐富，議場佈置整潔明亮，處處可見主辦單位的嚴謹與細心。另一方面，
會議場地的軟硬體設備也相當完備，這些都可以作為以後我們舉辦大型國際會議之借鏡。
2Language Learning through Multimodal Communication in VEC3D
Ya-Chun Shih & Mau-Tsuen Yang
Department of English, National Hualien University of Education, Taiwan;
Department of CSIE, National Dong Hwa University, Taiwan
Abstract
The aim of this research is to achieve multimodal communication in
Collaborative Virtual Environments (CVEs) available for EFL learners’
communicative competence development via text and voice chat, lip motion and body
language. We assume that developing strategic competence should take precedence
over the other communicative competence components. Lately, the proposed CVE
with multimodal communication, named VEC3D, has supported the development of
EFL learner strategic competence, the ability to use verbal and nonverbal
communication strategies (CSs) to overcome communication breakdowns. A concept
model for the future description and analysis of communication forms and CSs is
proposed. Calling nonverbal CSs into interaction is essential for communication
success. Nonverbal communication forms are also examined in the virtual
communication context.
Keywords: Collaborative Virtual Environment, Multimodal Communication, EFL,
VEC3D
Introduction
In our current research we are exploring how 3D Virtual Reality technologies
might be used for the development of communicative competence in English in a
college setting. With the advent of 3D Virtual Reality together with on-line 3D chat,
computer vision and graphics technologies, our new generation 3D CVE named 3D
Virtual English Classroom (VEC3D) enhanced the user’s immersive experience via
real-time written and voice-to-voice communications. We are primarily involved in
pedagogical research and investigation surrounding the integration of these
technologies to support English language learning and teaching.
Our attempt to ease and naturalize human communication over the Internet is
4acquisition takes place as learners engage in activities of a social nature with
opportunities to practice language forms for variety of communicative purposes”
(TESOL, 1997, p. 7).
Furthermore, multimodal interaction: text messaging, voice communication, lip
contour extraction and gesture are assumed to efectively improve learners’
interaction and communication efficiency when communication occurs over the
Internet. The whole communication process consists of the interplay between two
modes: verbal and nonverbal communication. (Allen, 1999) Usually, communication
difficulties arise because people receive relatively limited aural or visual sensory input.
Speech is well-perceived through audio-visual integration and multimodal. (Massaro,
1998; Massaro & Stork, 1998; Chen & Rao, 1998) Both verbal and nonverbal CSs
help language learners compensate for communication breakdowns. (Canale, 1983;
Swain, 1984; Canale & Swain, 1980) With the emergence of advanced 3D VR,
tracking and animated technologies, we witnessed natural human communication and
sophisticated language use in 3D virtual multimodal environment.
Based on constructivist perspectives, our learners derived great benefit from 3D
exploring and chatting in a community. The development of communicative
competence is believed to be situated in interaction, communication and challenging
goal-based activities. The way of achieving successful second or foreign language
acquisition depends on whether a learner receives appropriate communicative input in
accordance with his or her level of linguistic competence. (Krashen, 1985) A fluent
native English speaking instructor joined in the project and helped students receive
communicative input, facilitating proper English usage via multimodal
communication channels.
Figure 1. Video Conferencing and Avatars to Complement the Lack of
Nonverbal Clues in the CVE
6Figure 2. An instructor is experimenting with the use of multiple communication
channels (slide, gesture, voice and text) offered by the multimodal CVE
Figure 3. Group discussion through Audio/Video conferencing
Multimodal Communication Tasks Embedded in the Virtual Environments
VEC3D is a foreign language learning and teaching platform that uses
web-based 3D interactive multi-user environment to build virtual connection between
a native English speaking instructor and students. The primary purpose of this project
is to attract students through offering tasks that enhance their communicative
competence. VEC3D project requires that students participate in both real-world and
virtual activities, such as navigation, on-line discussion, team work and constructing
virtual environment. Concurrently, we also constructed VEC3D in Activeworlds and
used Second Life as a gateway to teleport to many other virtual 3D online
communities, and meet other target language speakers around the world.
The VEC3D instructional design principles and theories are based on
Constructivist Learning Design (CLD), Collaborative Learning (CL), Communicative
Language Teaching (CLT), Goal-Based Scenarios (GBS) (Schank, 1992),
8Information-Gap Odd man out Avatar, Drawing
Drawing Drawing and Imagination do. Voice Whiteboard
Rescuing Miss SaigonGame/Simulation
Virtual Field Trips
do. Text /Voice Chat
Avatar
Cross-Cultural Greeting & Interview Jason do. Voice Chat, Video
Read your mind/lips
Distraction
What on earth is Jason talking
about?
Vocabulary Pantomime
Simon says
Action Game/
Body Language
Exercise & Explore
Verbal;
Nonverbal: physical
appearance,
kinesics-gestures,
body movement,
paralanguage, facial
expressions-lip,
mouth shape
Text/ Voice Chat
Avatar, Video
Pronunciation Minimal Pairs Verbal; Nonverbal:
facial expressions-
lip, mouth shape
Voice Avatar
Lip Motion
Lecture PPT presentation
Group Discussion What’s diference?
Verbal; Nonverbal:
physical appearance,
kinesics-gestures,
body movement
Text /Voice Chat
Avatar Video
Slides
Hide & SeekVirtual Field Trip
Exercise & Explore
do. Text /Voice Chat
AWEDU/
Second Life
Research Methods
Our ultimate project goal is to answer the main research question: How can
multimodal communication in the CVE context to improve the EFL learner’s
communicative competence and stimulate their interaction? Investigations of the
potential for applying 3D virtual reality to develop communicative competence have
been continuously conducted to this day. The results provide practical implications for
future application possibilities of gesture integration in English language learning.
Graduate and undergraduate students participated in both real-world and virtual
activities, such as navigation, on-line discussion and team work.
This study is primarily a qualitative investigation employing the ethnography of
communication (Saville-Troike, 1982; Hymes, 1972, 1992; Schiffrin, 1994) to
explore speech acts and communicative event systems within a virtual context. The
10
make recognition easy and reveal their identity in a small group setting, they preferred
to select an avatar that has some similarity to their own appearance such as hair style
and clothes. Many conversation topics relating to the avatar’s physical appearances
were found. The avatar’s physical appearance naturally and intuitively allowing
learners to address ideas about identity, personality and creativity was used to draw
attention to themselves.
Strategic competence includes non-linguistic strategies used to foster
communication. Savignon indicated, “A gesture may serve as a coping strategy by 
either filling in for a word or expression or sustaining rapport throughout a
momentary silence” (1983, p. 44).Body language is included among the
communication strategies used by the learners through miming. For instance, in the
early stages of CVE communication, the act of waving the hand and arm as a greeting
and attention-gaining device is used to replace the word, Hello. Unexpected
communication problems can frequently be solved using emoticons or text smileys to
convey meaning and emotions when writing and using body language while talking.
The learners employ nonverbals to promote communication. For example an avatar
named Kou uses a walking gesture to show how to reach the target location,
‘waterfall’. 
Wen: hello
Kou: hello…(She shouted)there’s a waterfall over there
Wen: how to get there?
Kou: follow me (avatar walking through and forward)
This study shows the acquisition of strategic competence, multimodal
communication process and interaction in the virtual learning community. The CVEs
provides the multiple communication channels for learners to overcome
communication breakdowns more easily. Data collected from the interviews and
participant observation illustrates the learners relied heavily on non-linguistic means,
such as mime, and manipulating virtual objects. The use of 3-D Virtual Reality can
play a crucial part in providing an optimal context for the foreign language learners to
acquire the strategic competence that ensures that the conversation remains ongoing.
Moreover, the communication gap between interlocutors is more successfully bridged
via multimodal VEC3D. Authentic context, synchronous text-based and voice-based
communication, language-based and nonverbal communication all play critical roles
in natural language acquisition and the acquisition of strategic competence.
12
VEC3D aims to reinforce authentic communication by offering a communicative
context in CVEs. Learners are expected to overcome communication barriers and
breakdowns, and develop communicative competence. Current analysis provides
implication for the follow-up design and forms a tentative concept model depicting
multimodal communication forms in CVEs. In addition to continuing to fulfill the
object, development of communicative competence, we are beginning to elaborate our
study more on the user socio-cultural experience in the forming a virtual community
of practice in the multimodal CVE setting.
Acknowledgment
This research is supported by the National Science Council, Taiwan.
Copyright Statement
Copyright to this paper archived on ISTE’s Web site remains with the authors
References
Allen, L.Q. (1999). Functions of nonverbal communication in teaching and learning a
foreign language. The French Review, 72, 469-480.
