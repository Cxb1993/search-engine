 2
目錄 
可供推廣之研發成果資料表…………….3 
報告內容………………………………….5 
參考文獻…………………………………12 
計畫成果自評……………………………13 
 4
英文：The proposed system relates to an interactive scoring system 
for learning a language, in which a means such as a web camera is used 
to capture the learners lip movements and then a score is given by making 
a comparison with images stored in the database. The images stored 
in the database are those previously recorded by a teacher. By means 
of the scoring system, the learner can identify and rectify pronunciation 
problems concerning the lips and tongue. The system also records sounds 
as well as images from the student. The proposed system processes this 
data with multimedia processing techniques. With regard to the interactive 
perspective, a user-friendly visual interface was constructed to 
help learners use the system. The learners can choose the words they 
want to practice by capturing their lip image sequences and speech. The 
lip region image sequences are extracted automatically as visual feature 
parameters. Combining the visual and voice parameters, the proposed 
system calculates the similarity between a learners and a teachers pronunciation. 
An evaluation score is suggested by the proposed system 
through the previous similarity computation. By this learning process, 
learners can see the corresponding lip movement of both themselves and 
a teacher, and correct their pronunciation accordingly. The learners can 
use the proposed system to practice their pronunciation as many times 
as they like, without troubling the human teacher, and thus they are 
able to take more control of improving their pronunciation. 
可利用之產業 
及 
可開發之產品 
語言學習機，視覺化電子字典，聽障者語言學習輔具 
技術特點 
本研究中發展一套評分機制：透過視覺化的嘴唇參數結合聲音
的辨識技術(bimodal speech recognition)來對發音者進行評分，透過
唇形變動狀態來輔助語言發音教學，並可有效的改善目前發音教學
的品質。 
推廣及運用的價值 
本研究期望能夠利用視覺化的方式來輔助使用者學習外語＂說＂
的部份，並呈現出使用者在發音過程中，偵測出唇形的變化以用來
跟教學者做評量比對，以顯示出發音過程中可能有錯誤的嘴唇位
置，並將使用者的聲音錄製下來，透過影像處理和音訊處理技術來
取得重要參數，以供後續用來做聲音及影像兩者之間的評分。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
Figure 1, a learner, teacher, database and scoring mechanism are involved in the proposed system. Prior 
to being used by the learner, the teacher can input words (hereinafter this will also include a letter or 
sentence) into the database as both text and mouth images, as captured by the web camera. These data 
will be stored in the database for comparison, and this procedure can be updated at any time. The words 
and lip images stored in the database are accessible, the learner can first elect a word for practicing, and a 
web camera provided with the system captures consecutive images of the learner’s mouth while saying the 
word. The appropriate area covering the learner’s mouth will be determined by moving a frame shown on 
the computer screen. The scoring mechanism is to determine a score or grade for the learner’s 
pronunciation by comparing the learners lip movements with those stored in the database. The accuracy of 
the learner’s pronunciation can be evaluated according to a difference process or a dynamic time warping 
process. The onscreen process is shown in Figure 2. Notice that the blue ellipse lines are the detected lip 
regions operated by the proposed system. The images will be further processed for comparison. The 
consecutive images in color mode are then presented on the screen for the user to determine an area 
covering the mouth by moving a selection frame. The frame can be scaled by inputting a width and a height 
or by directly dragging the frame. Figure 3 shows the corresponding images for both teachers and learners 
lip images after dynamic time warping (DTW) operation. The top line of images is for the teacher and the 
bottom line of corresponding images is for the learner. 
Start
New Sentence Select Sentence
Teacher Learner
Web Cam
Serial Image
Lip Auto Detection
Sound Capture
Lip Image Capture
Save Teacher’s Image/Sound Load Teacher’s Image/Sound
Teacher Learner
Show both Lips
Evaluation
End
Record
Yes
No
 
Fig. 1. System Framework 
 6
 8
speech evaluation [1], three types of speech characteristics, i.e., magnitude, pitch contour and 
Mel-Frequency Cepstral coefficients, are evaluated by dynamic time warping (DTW) and Hidden Markov 
Model (HMM). As a result, Mel-Frequency Cepstral coefficients show the highest relationship, pitch contour 
shows less, and magnitude shows the least. In Liang’s research about speech practicing [5] , the main 
activities should include syllables associated with Pin-Yin and accent, rhythm, students speech and 
recognition in pronunciation types. In the investigations about recognition of lip shapes [6] [3], [4] [9], [2], lip 
contours for different vowels pronounced by different people are statistically analyzed. According to the 
results of statistics, several recognizable parameters are selected to establish a classification tree for a 
single vowel. The modified one-dimension fast Hartley transform provides a structural analysis of lip 
contours, and the test results indicate that recognition ratios of single vowel with this classification tree are 
95% for trained users and 85% for those that are untrained. Taking another approach, a bimodal 
audio-visual system has been developed. Matthews [7] provides a recognition method for reading lips 
according to visual features. Three parameters representing consecutive lip contours are adapted and 
analyzed with Hidden Marko Model (HMM). Silsbee [10] provide other solutions in researching lip-reading 
with lip features, which, for example, are contour-based and image based. In the contour-based process, 
edge information, deformable templates or active contours are used to find the features which can be 
preserved after translation, rotation, scaling and different illumination. However, much useful information 
may be omitted in this method, for example, features of teeth and tongue. The image-based process 
includes principal component analysis, wavelet and fast Fourier transform (FFT), which describe the 
consecutive lip images with less information. The automatic speech recognizer (ASR) provides is able to 
distinguish end users speech. However, the ASR system is usually disrupted by ambient noises, and thus 
its accuracy is lower. Leung [4] provide another solution, in which an area covering the mouth is selected 
according to Elliptic Shape Function and FUZZY C-MEANS. They also apply this technique to determine 
the lip contours on consecutive RGB images by dividing the lips into several parts, then finding the lip 
features and recognizing with Hidden Markov Model. 
3 Methods 
The technological basis of the proposed system is based on the hybrid of image and signal processing. For 
a sequence of lip movement images, automatic lip detection is executed. At the same time, the users 
sound is recorded and the voice parameters are extracted. The Pronunciation Evaluation Mechanisms will 
be introduced in the last subsection. 
3.1 Automatic Lip Detection 
Figure 4 shows the framework of automatic lip detection. First, we need to detect the location of the human 
face by using YCbCr color segmentation technique. Then we use the RGB and HSV color segmentation to 
extract the rough part of the lip image. The noise removal method is implemented by morphology operation 
and median filter. Finally, the refined lip detection is obtained by the connected component. 
3.2 Voice Parameter Extraction 
Voice is a time-varying signal. In signal processing, under the short time stationary assumption, a frame is 
extracted by fixed sampling windows size. There are three common characteristic parameters used in 
voice processing: Linear Predictive Coding, Cepstrum Coefficient, Mel-scale Frequency Cepstral 
Coefficients [1,5] We adopt the Mel-scale Frequency Cepstral Coefficients as our voice parameter. Figure 
5 shows the procedure of voice parameter extraction. 
 
Fig. 6. The process of pronunciation evaluation 
4 Experimental Results 
After extracting lip image and voice parameters, real data experiments are implemented. The quantitative 
measurements of automatic lip detection and the pronunciation evaluation are performed. 
4.1 Automatic Lip Detection 
There were 15 people involved in testing the recognition rate of automatic lip detection and each person 
spoke 3 ˜4 sentences. We used a web camera to capture lip images, and each image size was 320x240. 
The sampling rate was about 20 frames per second. The experimental platform was Pentium 4 CPU 
2.4GHz, 256M RAM. Since every sentence had a different length, each person has about 90 120 frames. 
For each lip image, there are four parts of the lip to be evaluated, the top, bottom, left and right. Table 1 
shows the correct detection rate for automatic lip detection. The overall correct detection rate is about 92%.  
4.2 The Pronunciation Evaluation 
In our experimental environment, there were nine people using the proposed system. Every one speaks 
eleven different sentences. Each sentence was recorded three times. Therefore, there were a total of 297 
sample sentences. From all these sample sentences, the maximal error between learner and teacher is 
obtained and is set to be the lowest score. The evaluation result from the proposed system was divided into 
five groups, very good, good, normal, not good, and bad. 
Table 1. The correct detection rate for automatic lip detection 
Result 
Range 
Correct Error Total Percentage 
Lip(top) 1570   104 1674 93.78%
Lip(bottom) 1637 37 1674 97.78%
Lip(left) 1625 49 1674 97.07%
Lip(right) 1628 46 1674 97.25%
Total 1543 131 1674 92.17%
  
 10
 12
interface was constructed to help learners use the system. The learners can choose the words they want to 
practice by capturing their lip image sequences and speech. The lip region image sequences are extracted 
automatically as visual feature parameters. Combining the visual and voice parameters, the proposed 
system calculates the similarity between a learners and a teacher’s pronunciation. An evaluation score is 
suggested by the proposed system through the previous similarity computation. By this learning process, 
learners can see the corresponding lip movement of both themselves and a teacher, and correct their 
pronunciation accordingly. The learners can use the proposed system to practice their pronunciation as 
many times as they like, without troubling the human teacher, and thus they are able to take more control of 
improving their pronunciation. Future research includes finding an intelligent way to improve the 
correctness of the evaluation rate from the proposed system. Furthermore, by exporting the system to a 
handheld device and/or web-based application, the proposed system would be able to serve more learners, 
at any time, any place, and any where. 
References 
1. Deng, L., Yu, D., Li, X., Acero, A.: A Long-Contextual-Span Model of Resonance Dynamics for Speech Recognition: 
Parameter Learning and Recognizer Evaluation. In: IEEE workshop on Automatic Speech Recognition and 
Understanding, 384–389 (2005) 
2. Kaynak, M.N.: Analysis of lip geometric features for audio-visual speech recognition. IEEE Transactions Systems, 
Man and Cybernetics, Part A, 564–570 (2004) 3. Lee, K.D., Lee, M.J., Lee, S.Y.: Extraction of frame-difference 
features based on PCA and ICA for lip-reading. In: IEEE International Joint Conference on Neural Networks, pp. 232–
237 (2005) 
4. Leung, S.H., Wang, S.L., Lau, W.H.: Lip Image Segmentation Using Fuzzy Clustering Incorporating an Elliptic Shape 
Function. In: IEEE Transactions on Image Processing, pp. 51–62 (2004)  
5. Liang, W., Liu, J., Liu, R.: Automatic Spoken English Test for Chinese Learners. In: International Conference on 
Communications, Circuits and Systems, vol. 2, pp. 27–30 (2005) 
6. Littlefield, K., Hashemi-Sakhtsari, J.A.: Performance evaluation of an automatic speech recognizer incorporating a 
fast adaptive speech separation algorithm. In: IEEE International Conference on acoustic, speech and signal 
processings, vol. 6,pp. 6–10 (2003) 
7. Mattews, I., Cootes, T.F., Bangham, J.A., Cox, S., Harvey, R.: Extraction of visual features for lip reading. IEEE 
Trans on Pattern Analysis and Machine Intelligence 24(2), 198–213 (2002) 
8. Mok, L.L.: Visual speech features representation for automatic lip-reading. In: IEEE International Conference on 
Acoustics, Speech, and Signal Processing, pp.397–400 (2004) 
9. Ou, G.B., Li, X.: Speaker identification using speech and lip features. In: IEEE International Joint Conference on 
Neural Network 2565–2570 (2005) 
10. Silsbee, P.L., Bovik, A.C.: Computer lip reading for improved accuracy in automatic speech recognition, IEEE 
Trans. Speech Audio Processing, 4(2), 337–351 (1996). 
11. Tamburini, F.: A multimedia framework for second language teaching in self-access environments. Computers and 
Education 32, 137–149 (1999) 
12. Tsou, W., Wang, W., Tzeng, Y.: Applying a multimedia storytelling website in foreign language learning. Computers 
and Education 47, 17–28 (2006) 
 
