行政院國家科學委員會專題研究計畫成果報告 
模擬退火法應用在離散事件模擬最佳化之全域最佳解上 
Simulated Annealing for Global Discrete-Event Simulation Optimization 
計畫編號：NSC 95-2221-E-033-036 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
主持人：陳慧芬教授  中原大學工業工程系 
E-Mail：huifen@cycu.edu.tw 
計畫參與人員：黃彥登、鄭郁諺、郭維倫、陳玫均、柯佳甫 中原大學工業工程系 
 
中文摘要 
    本研究考慮離散型模擬最佳化問題，
即尋找離散型數學模式的最佳點，然而由
於系統的複雜性，目標函數值很難計算，
必須用模擬估計的。這種問題出現在系統
設計上，決策變數為欲設定的系統參數，
目標函數為系統成本，最佳化問題是尋找
使系統成本最低的參數值。離散模擬最佳
化問題常見於網路設計、供應鏈設計、排
程與存貨控制等問題上。 
    我們提出 RASA 方法論來求解離散模
擬最佳化問題。RASA 結合回溯近似法與
模擬退火法（啟發式演算法），它以回溯近
似法為架構，以模擬退火法來求解一系列
的樣本路徑問題，而樣本數是遞增的。 
    我們進行模擬實驗來評估此方法論的
效率，並與現有的四種方法論作比較。方
法論的績效指標是函數值的均方誤MSEf 
和收斂頻率CP（即模擬重複次數中收斂的
次數），MSEf 值愈小愈好，CP值愈大愈
好。模擬結果顯示方法論RASA的收斂速
度較其他四種方法快速，且方法論對參數
值設定都具有強健性。 
關鍵詞：離散型模擬最佳化、回溯近似法、
樣本路徑、模擬退火法。 
 
Abstract 
     We consider the discrete simulation 
optimization problem, finding the optimal 
point (in a discrete set) that minimizes the 
objective function value, where the function 
value is difficult to compute but can be esti-
mated by simulation experiments. Such 
problems occur in system design. The deci-
sion variables are system parameter values to 
be determined and the objective function 
value is the associate system cost.  The op-
timization problem is to find the system pa-
rameter values that minimize the system cost. 
Applications include the network-design, 
supply-chains design, scheduling, and in-
ventory-control problems.  
We propose a new algorithm RASA 
for solving the discrete simulation optimiza-
tion problem. RASA combines DRA (de-
pendent retrospective approximation) and 
the heuristic SA (simulated annealing) algo-
rithms.  RASA uses the framework of DRA 
and iteratively solves a sequence of sam-
ple-path approximation problems, with in-
creasing sample sizes, via SA. 
     Simulation experiments are run to 
evaluate the performance of RASA. The al-
gorithm performance measures are the mean 
square error MSEf in function value and 
convergent path (CP), the number of replica-
tions that converge. A smaller MSEf and a 
bigger CP are preferred. The empirical re-
sults show that RASA converges much faster 
than four existing algorithms. RASA is also 
robust to its algorithm parameters. 
Keywords: discrete simulation optimization, 
retrospective approximation, sample path, 
simulated annealing 
 
1 Introduction 
 
     The discrete simulation optimization 
 1
optimization for solving a buffer allocation 
problem. Haddock and Mittenthal (1992) 
propose an SA algorithm to optimize for a 
manufacturing system. Gelfand and Mitter 
(1989) propose an SA algorithm converging 
to the set of global optimal solutions pro-
vided that sequence of annealing tempera-
tures is chosen properly. Fox and Heine 
(1995) assume that the range of the objective 
function values is restricted to a finite set 
and show that their algorithm converges in 
probability if the sequence of annealing 
temperatures is chosen properly.  Alrefaei 
and Andradóttir (1999) propose the MSSA 
algorithm using a constant annealing tem-
perature, and show that MSSA converges 
almost surely to the set of global optimal so-
lutions. 
     Miller and Goldberg (1996) propose a 
GA algorithm and provide a domain inde-
pendent lower bound for the optimal sample 
size that was experimentally verified using 
the OneMax domain problem for noisy fit-
ness function. Philippe and Goldberg (1998) 
have revealed that maximizing the ending 
generation of the GAs was more successful 
in maximizing performance for the optimal 
sample size of the Onemax problem.  
     Gendreau et al. (1996) first use the TS 
algorithm to solve the stochastic vehicle 
routing problem, where customers are pre-
sent at locations with some probabilities and 
have random demands. Costa and Silver 
(1998) propose the noisy TS algorithm based 
on the sampling and statistical tests. The ob-
jective function is evaluated through sam-
pling and the search for good potential 
moves is governed by statistical tests.  
 
2.2. Random Search Algorithms 
 
     Pure random search procedures have 
been used for optimization problems as early 
as 1958 (Brooks 1958). These techniques are 
attractive due to their simplicity. The con-
vergence, however, is extremely slow when 
the parameter space is of high dimension. 
     Andradóttir (1996) develops a pure 
random search algorithm SRS over a large 
discrete set of input parameter values. SRS 
converges almost surely to the global opti-
mal point under certain conditions. 
     Yan and Mukai (1992) propose the 
stochastic ruler algorithm for discrete simu-
lation optimization. Alrefaei and Andradóttir 
(2001) propose a modified stochastic ruler 
method, called MSSR. They show that 
MSSR converges almost surely to the set of 
global optimal solutions. They also show 
that MSSR can solve discrete optimization 
problems of which the objective function 
value is estimated via either transient or 
steady-state simulation. 
     Gong et. al. (1992) propose the sto-
chastic comparison method for discrete 
simulation optimization. Alrefaei and An-
dradóttir (2001) present a modified stochas-
tic comparison method MSSC. MSSC con-
verges almost surely to the set of global op-
timal solutions, while the algorithm pro-
posed by Gong et. al. (1992) converges only 
in probability. 
 
3. Methods 
 
     We propose here a new algorithm, 
RASA for solving the discrete simulation 
optimization problem (P0). The algorithm 
combines DRA and a heuristic algorithm 
such as simulated annealing (SA) algorithm. 
It uses the framework of DRA, which itera-
tively solves a sequence of sample-path ap-
proximation problems (Equation 2) with in-
creasing sample sizes. Each sample-path ap-
proximation problem is solved by SA so that 
the global optimal solution can be found. For 
a set of random numbers )( 1 m, ..., ωωω= , 
the sample-path approximation problem of 
(P0) is 
            
,         
) ,(min      
dR
y
⊂Θ∈x
x ω
       (2) 
where myy i
m
i )/ ,() ,( 1 ωω xx ∑ == . Fixed ω , 
) ,( ωxy  is a deterministic function of x.  
RASA starts with a small sample to 
quickly move to the solution region. The 
sample size is then increased for the next 
sample path so that a good initial point will 
 3
2. Generate 
imi
, ..., ωωω 1=  dependently 
with 1-1  ..., , iωω . That is, the first mi-1 
observations in iω  are the same as the 
observations in 1-iω . 
3. Use simulated annealing to solve the 
sample-path optimization problem: 
3.1 Set k = 1 and xk = , where =x* 1-ˆ ix *0xˆ 0. 
3.1.1 Set x = xk. Choose a candi-
date zk = z∈N(xk) for all x, z Θ∈ . 
3.1.2 Generate mi independently 
observations )(y ),...,(y z1z imωω  of 
y(z) and mi independently observa-
tions ),...,(y 1x ω )(y x imω  of y(x). 
Compute ii
m
ii myy i )/ ,() ,( 1 ωω xx ∑= =  
and ii
m
ii myy i )/ ,() ,( 1 ωω zz ∑ == . 
3.1.3 Generate Uk ~ U(0, 1) and set 
             { }
⎩⎨
⎧ ≤=+ else  
, -exp  if  
1
k
kk
k
cU
x
z
x
where [ ] kii tyyc /) ,(-) ,( += ωω xz  
and for any a∈ R, a+ equals to a if a 
> 0, otherwise, a+ = 0. 
3.1.4 If k < K, update tk using the 
cooling schedule, let k ← k + 1, 
and go to Step 3.1.1. Otherwise, set 
= x*ˆ ix k and go to Step 4. 
4. If the stopping rule is satisfied, stop and 
return . Otherwise, let m*ˆ ix i+1 = c1mi, i ←
i + 1, and go to Step 1. 
 
    The other SA parameters include the 
initial solution x0, initial temperature t0 and 
number K of SA iterations. If a 
user-provided initial solution x0 is not avail-
able, x0 can be generated randomly from the 
feasible set . In Section 3.2, we consider 
an M/M/1 queuing system and empirically 
compare these combinations with three an-
nealing schedules and two neighborhood 
functions; both have a fixed size for every x 
considered. Section 3.2 also empirically 
studies the effects of the initial temperature 
t
Θ
0 and the number K of SA iterations to the 
RASA performance. 
 
3.2. The Robustness of RASA 
 
     Simulation experiments are run to in-
vestigate the efficiency and robustness of 
RASA. The purpose is to study the sensitiv-
ity of algorithm parameters of RASA to the 
convergence speed. We study here the effects 
of RASA algorithm parameters to its algo-
rithm performance using an M/M/1 queuing 
system described below. A robust algorithm 
should be insensitive to the choices of its 
algorithm parameters. Four RASA parame-
ters are considered: the initial temperature t0, 
the number K of SA iterations, the annealing 
schedule and the neighborhood structure 
N(x). Simulation results show that RASA is 
robust to choices of algorithm parameter 
values. 
 
Example: Consider an M/M/1 system with 
arrival rate 1 and service rate μ(x), where x 
is in Θ  = {1,..., 50} and the values of μ(x) 
are listed in Table 1. We are interesting in 
finding an optimal x in  so that the mean 
system time E[W(x)] per customer is mini-
mized. That is, solve the optimization prob-
lem: 
Θ
. )]([  )( min xWExf
x
=Θ∈  
For this special example, the mean 
system time in steady state is f(x)= [μ(x)-1]-1.    
The optimal value of x is therefore 28 (i.e., 
the optimal service rate is 2) and the optimal 
objective function value is 1. Figure 2 shows 
the plot of f(x). We can see from Figure 2 
that there are many local optimal points. In 
this experiment, the estimate y(x) for f(x) is 
set as the average system time for the first 5 
customers. Notice that y(x) is a biased esti-
mate, which is not unusual in practice. 
     To evaluate the algorithm performance, 
two measures are considered: the number of 
convergent paths (CP) and mean squared er-
ror (MSEf) in the optimal objective function 
value. The value of CP is the number of 
convergent paths out of the total number of 
replications in the testing experiment. Here 
we set the total number of replications as 
100. Hence, CP ranges from 0 to 100. A lar-
 5
fMSE  and CP . In case that the objective 
function is unimodal, N2(x) might perform 
better than N1(x).  
     We summarize here the setting of 
RASA algorithm parameters: (1) when the 
objective function is unimodal, we set the 
neighborhood N2(x), K2 = 1000, and three 
annealing schedules be applied except for 
geometric function. (2) When the objective 
function is with many local optimal, we set 
the neighborhood N1(x), K1 = 70, and three 
annealing schedules be applied. The other 
parameters are set to be m1 = 2 and c1 = 2. 
 
4. Empirical Results 
 
     Here we empirically compare RASA 
with four existing algorithms MSSA, SRS, 
MSSR, and MSSC, reviewed in Section 2. 
Simulation results show that RASA con-
verges faster and is more efficient than the 
other four algorithms. 
     The RASA parameter values are set as 
follows. The annealing schedule is set to be a 
constant temperature .1; the initial sample 
size is set to be m1 = 2 and the sample-size 
multiplier is set to be c1 = 2. The number of 
SA iterations is set as K1 = 70 when N1(x) is 
used and set as K2 = 1000 when N2(x) is 
used. 
     The parameter values for MSSA, 
MSSR, and MSSC are chosen optimally 
based on a preliminary experiment. (Notice 
that SRS has no algorithm parameter.) For 
MSSA, we study four constant temperatures 
t = .01, .1, .5, and 1; setting t = .1 results in 
the best convergence speed. Therefore, the 
temperature parameter for MSSA is set to be 
t = .1. For MSSR, there are two algorithm 
parametersθ, is a uniform distribution over 
[0, a] and M is considered. Three values (2, 
10, 100) of a and five values (2, 3, 4, 5, 6) of 
M are studied. The results show that the 
combination of a = 2 and M = 2 has the best 
convergence. Therefore, θ is set to be a 
uniform(0, 2) random variable and M is set 
to be 2 for MSSR. For MSSC, five values of 
the algorithm parameter M are studied, M = 
2, 3, 4, 5, 6, and M = 2 has the best conver-
gence speed. Hence, M is set to be 2 for 
MSSC. For MSSA, SRS, MSSR, and MSSC, 
the sample size for the function estimate y(x) 
is set to be 5 (customers). 
     Figure 5 shows the simulation results 
for RASA, MSSA, SRS, MSSR, and MSSC 
algorithms. From Figure 5, we see that 
RASA converges much faster than the other 
four algorithms. The computation effort for 
RASA to reach CP  = 100 is around N  = 
6000. For the other four algorithms, the CP  
values is less than 100 even when N  = 105.  
Comparing the results in Figure 4 to the re-
sults in Figures 5, we can see that RASA 
usually outperforms the other four algo-
rithms even when the parameters of RASA 
are not chosen optimally. Only when RASA 
uses a bad neighborhood structure with a bad 
temperature schedule, the convergence speed 
of RASA is worse but not extremely bad. 
 
5. Conclusions 
 
We list our conclusions as follows:  
(1) We propose a new algorithm, called 
RASA, for the discrete simulation opti-
mization problem, using a combination 
of DRA and SA (simulated annealing). A 
primitive version of RASA is shown in 
this proposal. In our simulation results, 
RASA outperforms the four existing al-
gorithms. With these promising results, 
we plan to provide a more sophisticated 
version in the future so that it converges 
faster and is more robust. 
(2) We discuss parameter settings for RASA. 
A robust algorithm should be insensitive 
to choices of its algorithm parameter 
values. We will study the effect of algo-
rithm parameters to the convergence 
speed. The purpose is to provide guide-
lines for setting algorithm parameters. 
For example, in RASA, the value of K 
(number of SA iterations) should de-
pends on the sample size mi for the ith 
DRA iteration. When mi is large, the es-
timate )(⋅y  is close to the real objective 
function value f(.) and hence, K should 
be large. 
(3) When the objective function is unimodal, 
N2(x) may perform better than N1(x).  
 7
discrete stochastic optim ization
Infinite
or 
finite but many
finite
or 
finite but only a few
sam ple 
path
1 . sim ulated    
    annealing
2 . tabu search
3 . genetic 
    algorithm
4 . nested partition
random 
search
heuristic statistic 
selection
branch 
and
bound
1 . pure random
    search
2 . stochastic 
    ruler
3 . stochastic 
    com parison
sample average 
approximation
1 . ranking and 
    selection
2 . multiple 
    comparisons    
    w ith the best
 
Figure1: An overview of discrete simulation optimization techniques 
 
 
Table 1: The value of the service rate )(xμ  for all x Θ∈  
(10) ..., (1), μμ  1.65 1.6 1.5 1.6 1.7 1.75 1.65 1.6 1.55 1.5 
(20) ..., (11), μμ  1.47 1.45 1.5 1.55 1.6 1.65 1.6 1.55 1.5 1.47
(30) ..., (21), μμ  1.45 1.5 1.55 1.6 1.65 1.7 1.75 2.0 1.7 1.6 
(40) ..., (31), μμ  1.55 1.5 1.47 1.5 1.6 1.65 1.7 1.75 1.65 1.6 
(50) ..., (41), μμ  1.55 1.5 1.47 1.5 1.6 1.65 1.7 1.6 1.5 1.45
 
 
 
Figure 2: The objective function values of M/M/1 queuing system for all x  Θ∈
 9
metric, and logarithmic) and two neighborhood structures (N1 and N2) for M/M/1 example 
 
Figure 5: Plots of fMSE  and CP  curves for RASA, MSSA, SRS, MSSR, and MSSC al-
gorithms for two neighborhood structures N1 and N2  
 
 
References 
 
1. Alrefaei, M. H., and S. Andradóttir 
(1999). A Simulation Annealing Algo-
rithm with Constant Temperature for 
Discrete Stochastic Optimization. Man-
agement Science 45, 748-764. 
2. Alrefaei, M. H., and S. Andradóttir 
(2001). A Modification of the Stochastic 
Ruler Method for Discrete Stochastic 
Optimization. European Journal of Op-
erational Research 133, 160-182. 
3. Andradóttir, S. (1996). A Global Search 
Method for Discrete Stochastic Optimi-
zation. SIAM journal on Optimization 6, 
513-530. 
4. Brooks, S. H. (1958). Discussion of 
random methods for locating surface 
maxima. Operations Research 6, 
244-251 
5. Bulgak, A.A. and J.L. Sanders (1988). 
Integrating a Modified Simulated An-
nealing Algorithm with the Simulation 
of a Manufacturing System to Optimiza-
tion Buffer Sizes in Automatic Assem-
bly Systems. In Proceedings of the 1988 
Winter Simulation Conference, 684-690. 
6. Cheh, K. M., Goldberg, J. B. and Askin, 
R. G. (1991). A Note on the Effect of 
Neighborhood Structure in Simulated 
Annealing. Computers and Operations 
Research 18, 537-547. 
7. Chen, H. (1994). Stochastic Root Find-
ing in System Design. Ph.D. Dissertation, 
School of Industrial Engineering, Pur-
due University, West Lafayette, Indiana, 
USA 
8. Chen, H. and B.W. Schmeiser (2001). 
Stochastic Root Finding via Retrospec-
tive Approximation Algorithms. IIE 
Transactions 33, 259-275. 
9. Costa, D., and E. A. Silver (1998). Tabu 
Search when Noisy is Present: An Illus-
tration in the Context of Cause and Ef-
fect Analysis. Journal of Heuristics 4, 
5--23. 
 11
