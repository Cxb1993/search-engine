which can make efficient use of the long-span lexical 
cues inherent in the word history in a recursive 
fashion. In this research project, we not only 
investigate to leverage extra information relevant to 
the word history for RNNLM, but also devise a dynamic 
model estimation method to obtain an utterance-
specific RNNLM. We experimentally observe that our 
proposed methods can show promise and perform well 
when compared to the existing LM methods on a 
Mandarin LVCSR task. Our research results on language 
modeling and its related applications had also been 
published at several major prestigious international 
journals and conferences, such as IEEE Transactions 
on Audio, Speech and Language Processing, Information 
Processing & Management, Multimedia Tools and 
Applications, ICASSP, ASRU, Interspeech, and ICME, 
among others. Especially, our other LM-related papers 
also won the best paper awards of ROCLING 2011, 2012 
and 2013. From here on down, we will focus 
exclusively on our development of novel NNLM 
techniques and their corresponding results. 
英文關鍵詞： language modeling, speech recognition, neural 
networks, semantic cues, relevance information 
 
傳達的主題或概念。因此，目前在國際間，以語音為基礎的對
話系統、機器翻譯、多媒體搜尋與摘要的研究課題已成為矚目
的焦點，有許多令人鼓舞的研究成果陸續被發表出來；在這其
中，自動語音辨識(Automatic Speech Recognition, ASR)技術是
開啟上述研究大門的關鍵金鑰 [1, 2]。 
一般的大詞彙連續語音辨識(Large Vocabulary Continuous 
Speech Recognition, LVCSR)系統包括聲學特徵擷取(Acoustic 
Feature Extraction)、聲學模型 (Acoustic Model)、語言模型
(Language Model)與語言解碼(Linguistic Decoding)等四個主要
部分，如圖 4 所示。我們使用文字語料以訓練出語言模型，來
代表語言使用的規律、預測語句的機率；使用語音語料訓練出
聲學模型，來建立語音訊號之聲學特徵與語句中的詞彙或音素
間之對應關係。而語音辨識便在給定一段語音訊號的情況下，
透過聲學特徵擷取，根據已由聲音語料訓練而成的聲學模型、
以及文字語料訓練而成的語言模型，對聲學特徵向量進行語言
解碼，產生對應的可能語句(或詞序列)做為輸出結果。 
本研究目的在於發展新穎的語言模型，希望藉由語言模型
能捕捉語言的規律性，輔助解決語音辨識時聲學模型混淆問題。
N 連語言模型是較常見的語言模型之一，它易於產生且容易使
用的特性引發許多學者研究與使用。但 N 連語言模型有資料
稀疏與缺乏長距離語意與詞彙資訊等問題，因此有不同類型的
語言模型被發展出並期望解決這些問題[3]。 
例如，Kuhn 與 Mori 在 1990 年提出了快取模型(Cache 
Model)來輔助Ｎ連語言模型[4]，其概念是：如果我們講了一
些詞，則一段時間內這些詞再次出現的機率將會提高。例如，
基於語音辨識已經產生出的歷史詞序列來動態地估測單連詞快
取模型(Word Unigram Cache Model)或更高階快取模型，並與
傳統Ｎ連語言模型作線性結合；在辨識過程中某些詞或詞的組
合若經常出現，則會快取模型將給予較高的語言模型機率。再
者，Brown 等人在 1992 年提出了 N 連類別模型(class-based N-
gram model)[5]；N 連類別模型是為了改善Ｎ連語言模型所面
臨的資料稀疏問題，透過對於詞的分類，將語言模型所要預測
標的數目從詞典中不同詞的數目降至詞所屬於不同詞類別的數
目，可以大量地降低所需的語言模型參數。有關於詞的自動分
類，通常可以透過使用諸如貪婪演算法(Greedy Algorithm)最大
化交互資訊(Maximum Mutual Information, MMI)而達成。 
在 1996 年 Rosenfeld 提出將最大熵值(Maximum Entropy, 
ME)法準則應用於語言模型[6]。熵值可以用於描述隨機變數之
機率分佈特性：當所觀察的隨機變數之分佈愈平均時有愈大的
熵值，反之亦然。最大熵值語言模型以對數線性(Log-linear)結
合不同語言詞彙特徵(例如某一詞的出現與否、或不同詞的共
同出現與否等)來做為語言模型預測依據；在模型訓練時則是
在滿足某些已知特徵統計資訊限制(例如訓練語料中詞的出現
頻率、或不同詞的共同出現頻率)下，希望語言模型機率分佈
能夠越平均(亦即其熵值愈大愈好)，可透過改善迭代調整法
(Improved Iterative Scaling, IIS) 或 是 廣 義 迭 代 調 整 法
(Generalized Iterative Scaling, GIS)來達成。最大熵值語言模型
的最大優點之一便是能夠整合不同的自然語言詞彙特徵，例如
有學者提出使用觸發對(trigger pair)資訊於最大熵值語言模型，
而有所謂的觸發對語言模型的發表[7]。 
略詞模型(Skipping Model)在 1993 年被提出[8]。傳統Ｎ連
語言模型需要以緊連的前 個詞為條件下，預測候選詞的發生
機率，當 N 愈大，訓練語料出現的 N 連詞組合愈少，不易獲
得正確的機率估測。略詞模型可視為是長距離 N 連詞(Long-
distance N-gram)模型，即在使用Ｎ連語言模型時，允許略過數
個詞位置(或距離)、使用在歷史詞序列中更早出現的詞來在做
為預測目前詞的發生機率之條件。另外，在 1997 年聚合式馬
可夫模型(Aggregate Markov Model, AMM)與混合階層馬可夫
模型(Mixed-order Markov Model)被提出[9]。聚合式馬可夫模
型亦可視為是一種二連類別(word class)語言模型，有別於 N
連類別模型需要將詞一對一對應到某詞類別，聚合式馬可夫模
型允許每個詞有不同的機率會屬於不同的類別。透過這種柔性
(soft)分類，聚合式馬可夫模型比 N 連類別模型更有彈性。混
合階層馬可夫模型可視是略詞二連模型的延伸，除了使用略詞
二連模型捕捉不同(長)距離的詞與詞關係之外，也同時估測不
同略詞距離的影響。 
在 1997 年，結構化語言模型(Structured Language Model)
被提出[10]。結構化語言模型主要是使用語句結構資訊。首先
針對歷史詞序進行剖析(parsing)，得到詞剖析字首(word-parse 
prefix) ，每個詞剖析字首頂點代表那個段落的主導詞
(headword)。根據主導詞與歷史詞，可以進一步估測目前詞出
現的機率。 
潛藏語意分析(Latent Semantic Analysis, LSA)最早是 1990
年左右在資訊檢索(Information Retrieval, IR)領域被提出，在
1998 年首度被應用於語言模型上[11]。潛藏語意分析主要是對
詞與文件建立潛藏的語意關係，並使用線性代數方法中的奇異
值分解(singular value decomposition, SVD)將詞與文件(或歷史
詞序列)共同在潛藏語意空間以向量形式表示，藉此可以計算
彼此之間相關性，並轉換成語言模型預測機率。由於線性代數
方法不易調整參數，所以在 1999 年時有學者提出了機率式潛
藏語意分析(probabilistic latent semantic analysis, PLSA)用於資
訊檢索，後來也用於語音辨識的語言模型使用上[12]。與潛藏
語意分析相比，機率式潛藏語意分析使用較有彈性的機率架構
建立模型。此外，由於在潛藏語意分析中，奇異值分解後的矩
陣將會包含負數，造成對於語意的詮釋上不甚明確；所以有學
者以使用非負矩陣分解 (Non-Negative Matrix Factorization, 
NMF)方式來產生詞與文件(或歷史詞序列)共同在潛藏語意空
間以向量形式表示，並應用於語言模型研究上[13]。 
而在 2003 年，分散式馬可夫模型 (Distributed Markov 
Model, DMM)被提出[14]，它主要是改進聚合式馬可夫模型
(AMM)只能使用二連詞的限制，透過將參數設定成二元值
(Binary)向量，簡化語言模型所需的參數量，進而延伸可參考
到的歷史詞序列長度。另一種觸發對語言模型在 2004 年被提
出[15]：有別於[7]使用最大熵值法整合資訊，這種觸發對語言
模型先收集出現在同一窗內的詞配對(Word Pairs)統計值，然
後直接估測觸發對的條件機率。再透過一些選擇觸發對的機制，
如詞頻數與反文件頻數(Term Frequency and Inverse Document 
Frequency)保留部分觸發對，最後使用插補法(interpolation)與
N 連語言模型結合。2005 年，原本於在資訊檢索領域被提出
的潛藏狄利克雷分配(Latent Dirichlet Allocation, LDA)[16]被推
廣到語音辨識的語言模型使用上[17]。潛藏狄利克雷分配可視
為是機率式潛藏語意分析(PLSA)的延伸。潛藏狄利克雷分配
使用了額外的超參數(Hyper-parameters)來當作模型參數的事前
機率，並且使用貝氏估測(Bayes Estimation)等來求取模型參數。
換句話說，機率式潛藏語意分析可視為是使用最大事後機率估
測潛藏狄利克雷分配的模型參數，並且其模型參數的事前機率
為平坦(Flat)的一種特例。 
IBM 學者 Afify 等人於 2007 年提出了高斯混合語言模型
(Gaussian mixture language model, GMLM)[18]。高斯混合語言
模型與類神經網路模型有相似的概念，都是使用連續空間建立
)()( kk netgty       (6) 
其中，wkj是第k個隱藏層節點對第j個輸入層節點的鏈結權重值，
θk為第k個隱藏層節點的偏權值，netk(t)為第k個隱藏層節點淨
輸入值，yk(t)為第k個輸出層節點。為了使輸出層各節點的值
總和為 1，最後的 g(netk)為軟化最大值活化函數 (Softmax 
Activation Function)，也是轉移函數(Transfer Function)的一種。
如下式來表示： 
 k x
x
e
exg )(      (7) 
三、遞迴式類神經網路語言 
有別於傳統類神經網路，遞迴式的類神經網路更能帶來好的訓
練能力，一般常見的是於 1990 年由 Elman 所發展的艾爾曼網
路(Elman Networks)[21]。其概念是將隱藏層的輸出當作下一
次時間點隱藏層的輸入，而根據不同的需求也有許多不同的網
路形成，如喬丹網路(Jordan Networks)[22]是將輸出層的輸出
再傳遞給下一時間點的隱藏層、雙向遞迴式類神經網路(Bi-
directional RNN)利用歷史資訊和未來資訊來做預測，使用的是
兩個遞迴式類神經網路來做結合及階層遞迴式類神經網路
(Hierarchical RNN)等。本研究則是以艾爾曼網路來進行探討。
輸入層沒有具備運算能力，表示方式是以詞為單位由訓練資料
來給予，依照輸入的資料型態需先做前處理；隱藏層則是用來
處理輸入單元傳遞進來的資料；輸出層用來表現網路的輸出結
果。遞迴式類神經網路結構是把輸入層加大，且將上一時間點
的隱藏層利用暫存複製起來，若以時間方式來階層展開的話，
將會更清楚看出其遞迴的概念，如圖二所示。輸入層、隱藏層
和輸出層設計也與之前一樣，但是在前一時間點和目前時間點
的隱藏層間會多增加一個權重 U。由於遞迴式類神經網路具有
時序處理(Temporal Processing)的能力，而一般來評估此類型
的 網 路 常 會 注 意 它 們 的 穩 定 性 (Stability) 、 可 控 性
(Controllability)及可觀察性(Observability)。穩定性關心的有侷
限性在時間上的網路輸出和反應網路輸出的微小變化，例如網
路的輸入或權重。可控性在意的是「是否能夠控制的動態行
為」，如果在有限的步驟中，一個初始狀態是可控制至任何期
望的狀態，則此遞迴式網路可被稱為具有可控性的。可觀察性
關注的是「是否可觀察出控制應用的結果」，如果網路的狀態
可以確定從一組有限的輸入或輸出測量，則稱做此網路有可觀
察性。但 Bengio 等學者[23]發現，利用梯度下降法(Gradient 
Descent Method)於遞迴式類神經網路中，對於學習長距離的資
訊是十分困難的。而要獲得長距離資訊必須要具有學習任意時
間內的資訊，且擁有抵抗其它資訊干擾的能力。但因為隨著時
間變化，距離較遠的資訊會被每一次時間點的輸入資訊所干擾，
反而降低了遞迴式結構的好處。因此下一節將討論如何將遞迴
式類神經網路語言模型做進一步的改進。 
四、遞迴式類神經網路語言模型之改進 
1. 結合關聯資訊於遞迴式類神經網路語言模型 
傳統統計式 N 連語言模型是容易使用且目前常見的方法之一，
但此模型仍有缺乏長距離資訊與資料稀疏之問題。而使用類神
經網路語言模型能有效解決資料稀疏之問題，可惜在長距離資
 
 
圖三、語句關聯資訊概念圖。 
 
 
圖四、詞關聯資訊概念圖。 
 
 
 
 
圖二、遞迴式類神經網路架構。 
 
w1 w3 w2S1
1
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
0
0
0
1
1
0
1
0
1
1
0
1
0
1
1wR 3wR 2wR
1w 3w 2w
w1 w3 w5R1
w1 w3 w2S1
1
0
0
0
0
1
0
1
0
1
0
1
0
0
0
0
0
0
1
0
1
1
1
0
0
1
0
1
0
1
1wR 3wR 2wR
1w 3w 2w
w1 w3 w41wR
w1 w2 w33wR
w1 w3 w52wR
前一次時間點
之隱藏層
權重 V 權重 W
複製
權重 U
輸入層 隱藏層 輸出層
…
…
…
…
…
…
…
…
22
),cos(
pk
pk
pk
vu
vu
RNNLMU
           (10) 
其中 Uk 表示第 k 句測試語句，RNNLMp 為使用第 p 群訓練語
句訓練出的遞迴式類神經網路語言模型，uk為測試語句第 k 句
中 M 條最佳辨識結果第一名之單連詞向量。以下則介紹三種
選取方式： 
(1) 選取相似度最大權重法：此方法只選取和測試語句最相似
的訓練語句群，也就是所謂相似度最大的。可用下式來表
示： 
),cos(maxarg pk
p
U RNNLMURNNLM k           (11) 
其中，
kURNNLM 代表所挑選出來的遞迴式類神經網路語言模型，因此此公式則表示挑選 P 群中餘弦值最大之遞迴
式類神經網路語言模型。 
(2) 相似度線性組合法：此方法之組合係數是經由計算測試語
句與 P 群訓練語句間的相似度來求得，因此如果某測試語
句和某群相似度較大則表示該群較符合測試語句之特性，
也就是該群會有較大之貢獻。而測試語句與各訓練語句群
的組合係數 pk, 為： 



P
c
pk
pk
pk
RNNLMU
RNNLMU
1
,
),cos(
),cos(            (12) 
接著將各群之模型分數用此係數線性組合以獲得新的語言
模型分數： 
pk RNNLM
P
p
pkRNNLM ScoreScore  1 ,             (13) 
而
kRNNLMScore 為測試語句經過線性結合之遞迴式類神經網路語言模型分數，
pRNNLMScore 為第 p 群之遞迴式類神經網路語言模型分數。 
(3)相似度均勻組合法：該方法類似相似度線性組合法，將原
本組合係數調整成均勻(Uniform)組合係數，因此各群的貢
獻度將會相同，則各群均勻組合係數 pk, 為： 
Ppk
1
,            (14) 
其中 P 為分群個數。所以最後結合完的分數可以用下式來
表示： 
pk RNNLM
P
p
pkRNNLM ScoreScore  1 ,            (15) 
在最後，我們還會將算好的分數結合背景語言模型，以此來得
到更好的結果，並將結合完的詞序列分數來做排序。 
五、實驗結果與討論 
1. 實驗語料 
本研究使用之實驗語料是來自於公視新聞(Mandarin Across 
Taiwan-Broadcast News, MATBN)[25]。公視新聞語料是 2001
年至 2003 年間由中研院資訊所口語小組(SLG)與公共電視
(PTS)合作錄製，共計 197 個小時之語音資訊與其內容標記，
選自公視新聞 2001 年至 2002 年外場採訪記者，分別為：訓練
集語料 30,600 句(約 23 小時)、測試集語料 1,997 句(約 1.5 小
時)及發展集語料 1,998 句(約 1.5 小時)。如表一所示。 
聲學模型訓練語料為公視新聞 2001 至 2002 年外場採訪
記者語料，共 30,632 句(約 23 小時)，其中包含了實驗訓練語
料 30,600 句。另外背景 N 連(三連)語言模型使用的訓練語料是
來自 2001 至 2002 年中央通訊社(Central News Agency, CNA)的
文字新聞語料，內含有約一億五千萬個中文字，經由斷詞之後
約有八千萬個詞。此語言模型是使用 SRI Language Modeling 
Toolkit (SRILM)[26]訓練而得，採用 Katz Back-off 平滑化方法
[27]。 
 發展集語言 複雜度 
測試集語言 
複雜度 
發展集語料字 
正確率(%) 
測試集語料字 
正確率(%) 
絕對 
提昇率(%) 
相對 
提昇率(%) 
背景三連語言
模型(BG) 450.93 459.06 84.73 83.61 - - 
RNN 607.07 623.50 82.31 82.41 -1.2 -7.32 
RNN+BG 232.31 236.97 85.67 85.17 1.56 9.52 
Oracle - - 93.22 92.66 - - 
表二、遞迴式類神經網路語言模型之基礎實驗結果。 
 
找其相鄰詞作為關聯資訊，出現相同的相鄰詞則會累加出現
次數，本研究則是取左右距離為 3。 
另一方面，表四為使用詞關聯資訊於遞迴式類神經網路
語言模型的實驗結果，由於部分詞的詞關聯資訊相當多，包
含了關聯詞和非關聯的詞，因此，我們試著去調整詞關聯資
訊的使用程度，其中詞關聯資訊的長度是根據發展集中最好
的結果來設定。語言複雜度方面，詞出現次數、正規化值及
出現該詞則設為 1 否則為 0 也都有進步；而字正確率方面，詞
出現次數、正規化值及出現該詞則設為 1 否則為 0 均有提昇，
絕對提昇率分別為 0.19%、0.17%及 0.23%，相對提昇率則有
1.26%、1.16%及 1.52%的進步。比較三種表示法的辨識率，
可看到出現該詞設為 1 否則為 0 的辨識結果較好，因為此方法
對於每個關聯詞的關聯度較公平，大家皆設定為 1。而詞出現
的次數和正規化值，因為每個關聯詞之間的歧異度較高，尤
其是詞出現的次數，次數的差距很大，導致有些關聯詞的貢
獻被埋沒。 
值得一提的上述結果實驗結果亦顯示出，使用出現該詞
設為 1 且調整詞關聯資訊的長度較好；於是我們進一步去觀
察使用此表示法在不同詞關聯資訊長度上的比較。圖六則是
其辨識結果，可以看出使用過多的資訊反而會導致效果減弱。 
雖然從實驗結果中得知，使用詞關聯資訊的確提升了辨
識率，但我們仍希望可以突破目前的瓶頸。於是我們發現到，
雖然詞關聯資訊解決了語句關聯資訊的問題，但是仍有類似語
句關聯資訊的缺點存在，其缺點則是語料中的每個詞所對應到
的詞關聯資訊仍然一樣，造成在訓練中重複使用同樣的詞關聯
資訊。此作法的詞關聯資訊是比較屬於全域(Global)的，也就
是是針對所有訓練語料中，獲得詞與詞的關聯度。而在訓練遞
迴式類神經網路語言模型中，我們也需要區域性的資訊，因為
相同的詞在不同的句子可能代表著不同的意思，所以我們希望
藉由區域性的資訊來獲得上下文或句子中的資訊，使得在預測
下一個詞時能夠符合句子中的意思。因此，我們希望詞的關聯
資訊必須是會變動的或是動態的，如此一來才能包含全域性的
資訊和區域性的資訊。為此，本研究提出了動態詞關聯資訊來
做更進一步的改進，此部分是先將所有歷史詞的關聯資訊做結
合，其結合依據遠近來給予權重。因此，歷史詞與目前的詞距
離越遠，則該歷史詞的關聯資訊貢獻越小；反之，歷史詞與目
前的詞距離越近，則該歷史詞的關聯資訊越大。表五則是使用
動態詞關聯資訊的實驗結果，結果顯示使用動態詞關聯資訊效
果與一般的詞關聯資訊較差一點，辨識率大約為 85.34%左右；
而其語言複雜度表現則較詞關聯資訊好。探究其原因，應為關
聯資訊中常包含關聯與非關聯的資訊，因此我們難以準確知道
越近距離的詞關聯資訊有較相關的資訊，造成使用的關聯資訊
關聯資訊表示 
方式 
發展集語言 
複雜度 
測試集語言 
複雜度 
發展集語料字 
正確率(%) 
測試集語料字正
確率(%) 
絕對 
提昇率(%) 
相對 
提昇率(%)
RNN 基礎辨識
率 232.31 236.97 85.67 85.17 - - 
詞出現的次數 229.72 234.35 85.84 85.26 0.09 0.58 
正規化值 231.42 236.19 85.83 85.34 0.17 1.14 
出現該詞則設為
1 否則為 0 229.98 234.62 85.86 85.34 0.17 1.16 
表五、結合動態詞關聯資訊之實驗結果。 
關聯資訊表示 
方式 
發展集語言 
複雜度 
測試集語言 
複雜度 
發展集語料字 
正確率(%) 
測試集語料字正
確率(%) 
絕對 
提昇率(%) 
相對 
提昇率(%)
RNN 基礎辨識
率 232.31 236.97 85.67 85.17 - - 
詞出現的次數 229.72 234.35 85.84 85.26 0.09 0.58 
正規化值 231.42 236.19 85.83 85.34 0.17 1.14 
出現該詞則設為
1 否則為 0 229.98 234.62 85.86 85.34 0.17 1.16 
表六、發展集語料字正確率之兩群辨識結果。 
 字正確率(%) 絕對提昇率(%) 相對提昇率(%) 
選取相似度最大權重法 85.41 0.24 1.60 
相似度線性組合法 85.24 0.07 0.46 
相似度均勻組合法 85.29 0.12 0.82 
表七、測試語料字正確率之兩群辨識結果。 
確率的下降。而實驗中也可看出，結合完各群結果後的辨識率
仍不好，需要加入遞迴式類神經網路背景語言模型來輔助，以
得到更好的辨識率。 
六、結論與未來展望 
1. 實驗語料 
本研究針對遞迴式類神經網路語言模型做了更進一步的改善，
期望使用關聯資訊和動態調整語言模型來輔助機率的估測。從
實驗結果中可以看出使用關聯資訊的確能帶來幫助，但是效果
仍不夠明顯，其原因應為輸入層或前一時間點的資訊被關聯資
訊所干擾，導致成效有限。而實驗中也發現到減少部分關聯資
訊能提升辨識率，因此關聯資訊或其他資訊的表示法在未來研
究上也是值得注意的部分。另一部分，本研究藉由將訓練語料
分群並訓練各群的遞迴式類神經網路語言模型，期望藉由動態
的調整語言模型來達到更好的辨識率。此部分實驗結果也顯示
分兩群時，使用相似度線性組合法有較佳的成效。但分成四群
時，由於各群中的訓練語料不足，因此無法訓練出學習能力較
佳的遞迴式類神經網路語言模型。 
關於未來研究方向，除了持續改進遞迴式類神經網路語
言模型的效能外，我們將發展鑑別式語言模型(Discriminative 
Language Model)以及關聯語言模型(Relevance Language Model)。
此外，我們也會把所發展出的新穎語言模型應用至語音辨識相
關領域，諸如語音文件摘要與檢索。 
七、參考資料 
[1] S. Furui, L. Deng, M. Gales, H. Ney and K. Tokuda, 
“Fundamental technologies in modern speech recognition,” 
IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 16–17, 
2012 
[2] D. O'Shaughnessy, L. Deng and H. Li, “Speech information 
processing: Theory and applications,” Proceedings of the IEEE, 
vol. 101, no. 5, pp. 1034–1037, 2013. 
[3] J. R. Bellegarda, “Statistical language model adaptation: review 
and perspectives,” Speech Communication, vol. 42, no.1, pp. 
93-108, 2004. 
[4] R. Kuhn and R. De Mori, “A cache-based natural language 
model for speech reproduction. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 12, no. 6, pp. 570–583, 
1990. 
[5] Peter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, 
Jennifer C. Lai, and Robert L. Mercer. “Class-based N-gram 
models of natural language,” Computational Linguistics, vol. 
18, no. 4, pp. 467–479, 1992. 
[6] R. Rosenfeld, “A maximum entropy approach to adaptive 
statistical language modeling,” Computer Speech and 
Language, vol. 10, pp. 187-228, 1996. 
[7] R. Lau, R. Rosenfeld and S. Roukos, “Trigger-based language 
models: a maximum entropy approach,” in Proc. IEEE 
International Conference on Acoustics, Speech, and Signal 
Processing, vol. II, pp. 45–48, 1993. 
[8] X. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F. Lee, and 
R. Rosenfeld, “The SPHINX-II speech recognition system: An 
overview,” Computer Speech and Language, vol. 2. pp. 137–
148, 1993. 
[9] L. Saul and F. Pereira, “Aggregate and mixed-order Markov 
models for statistical language processing,” in Proc. the 2nd 
Conference on Empirical Methods in Natural Language 
Processing , pp. 81–89, 1997. 
[10] C. Chelba, “A structured language model," in Proc. the 35th 
Annual Meeting of the Association for Computational 
Linguistics and Eighth Conference of the European Chapter of 
the Association for Computational Linguistics, pp. 498–500, 
1997. 
[11] J. R. Bellegarda, “A multispan language modeling framework 
for large vocabulary speech recognition,“ IEEE Transactions 
on Acoustic, Speech and Signal Processing, vol. 6, no. 5, pp. 
456–467, 1998. 
[12] D. Gildea and T. Hoffmann, “Topic-based language models 
using EM,” in Proc. the European Conference on Speech 
Communication and Technology, pp. 2167–2170, 1999. 
[13] M. Novak and R. Mammone, “Use of non-negative matrix 
factorization for language model. adaptation in a lecture 
transcription task”, in Proc. the 26th IEEE International 
Conference on Acoustics, Speech, and Signal Processing, pp. 
541–544, 2001. 
[14] J. Blitzer, A. Globerson, and F. Pereira, “Distributed latent 
variable models of lexical co-occurrences,” in Proc. the 10th 
International Workshop on Artificial Intelligence and Statistics, 
2005. 
[15] C. Troncoso, T. Kawahara, H. Yamamoto and G. Kikui, 
“Trigger-based language model construction by combining 
different corpora,” IEICE Technical Report, SP2004-100, 
pp.25–30, 2004. 
[16] D.M. Blei, A.Y. Ng, and M.I. Jordan, “Latent Dirichlet 
allocation,” Journal of Machine Learning Research, vol. 3, pp. 
993–1022, 2003. 
[17] Y. C. Tam and T. Schultz, “Dynamic language model 
adaptation using variational bayes inference,” in Proc. the 6th 
Annual Conference of the International Speech Communication 
Association, pp. 5–8, 2005. 
[18] M. Afify, O. Siohan, R. Sarikaya, “Gaussian mixture language 
models for speech recognition,” in Proc. the 32th IEEE 
International Conference on Acoustics, Speech, and Signal 
Processing, vol. IV, pp. 29–32, 2007. 
[19] Y. Bengio, P. Frasconi, and P. Simard, “The problem of 
learning long-term dependencies in recurrent networks,” in 
Proc. IEEE International Conference on Neural Networks, vol. 
3, pp. 1183–1188, 1993. 
[20] Y. Bengio, R. Ducharme, P. Vincent, C. Jauvin, J. K, T. 
Hofmann, T. Poggio, and J. Shawetaylor, “A neural 
probabilistic language model,” Journal of Machine Learning 
Research, 2003. 
[21] J. L. Elman, “Finding structure in time,” Cognitive Science, 
vol. 14, no. 2, pp. 179–211, 1990. 
[22] M. L. Jordan, “Attractor dynamics and parallelism in a 
connectionist sequential machine,” in Proc. the eighth annual 
conference of the cognitive science society, pp.531–546, 1986 
[23] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term 
dependencies with gradient descent is difficult,” IEEE 
Transaction on Neural Networks, Vol. 5, No. 2, pp. 157-166, 
1994. 
[24] J. B. MacQueen, “Some methods for classification and 
analysis of multivariate observations,” in Proc. 5th Berkeley 
Symposium on Mathematical Statistics and Probability, pp. 
281–297. 
[25] H.-M. Wang, B. Chen, J.-W. Kuo and S.-S. Cheng, “MATBN: 
A Mandarin Chinese broadcast news corpus,” International 
Signal Processing (ICASSP 2012), Kyoto, Japan, March 25 - 
30, 2012. (EI) 
[11] Yu-Chen Kao, Berlin Chen, “Leveraging distributional 
characteristics of modulation spectra for robust speech 
recognition,” the 11th International Conference on 
Information Sciences, Signal Processing and their 
Applications (ISSPA 2012), Montreal (Quebec), Canada, July 
3-5, 2012. (EI) 
[12] Yueng-Tien Lo, Shih-Hsiang Lin, Berlin Chen, "Constructing 
effective ranking models for speech summarization." the 37th 
IEEE International Conference on Acoustics, Speech, and 
Signal Processing (ICASSP 2012), Kyoto, Japan, March 25 - 
30, 2012. (EI) 
[13] Berlin Chen, Pei-Ning Chen, Kuan-Yu Chen, "Query 
modeling for spoken document retrieval," IEEE workshop on 
Automatic Speech Recognition and Understanding (ASRU 
2011), pp. 389-394, Hawaii, USA, December 11-15, 2011. 
(EI) 
[14] Ea-Ee Jan, Niyu Ge, Shih-Hsiang Lin and Berlin Chen, "An 
effective and robust framework for transliteration 
exploration," the 5th International Joint Conference on 
Natural Language Processing (IJCNLP 2011), pp. 1332-1340, 
Chiang Mai, Thailand, November 8-13, 2011. 
[15] Wen-Yi Chu, Jeih-weih Hung and Berlin Chen, "Modulation 
spectrum factorization for robust speech recognition," 2011 
APSIPA Annual Summit and Conference (APSIPA ASC 
2011), Xian, China, October 18-21, 2011. (EI) 
[16] Pei-Ning Chen, Kuan-Yu Chen, Berlin Chen, "Leveraging 
relevance cues for improved spoken document retrieval," the 
12th Annual Conference of the International Speech 
Communication Association (Interspeech 2011), pp. 929-932, 
Florence, Italy, August 28-31, 2011. (EI) 
[17] Berlin Chen, Jia-Wen Liu, "Discriminative language 
modeling for speech recognition with relevance information," 
IEEE International Conference on Multimedia & Expo 
(ICME 2011), Barcelona, Spain, July 11-15, 2011. (EI) 
[18] Kuan-Yu Chen, Berlin Chen, "Relevance language modeling 
for speech recognition," the 36th IEEE International 
Conference on Acoustics, Speech, and Signal Processing 
(ICASSP 2011), pp. 5568-5571, Prague, Czech Republic, 
May 22-27, 2011. (EI) 
[19] Shih-Hsiang Lin, Ea-Ee Jan, Berlin Chen, "Handling verbose 
queries for spoken document retrieval," the 36th IEEE 
International Conference on Acoustics, Speech, and Signal 
Processing (ICASSP 2011), pp. 5552-5555, Prague, Czech 
Republic, May 22-27, 2011. (EI) 
[20] Kuan-Yu Chen, Berlin Chen, "A study of topic modeling 
techniques for spoken document retrieval," 2010 APSIPA 
Annual Summit and Conference (APSIPA ASC 2010), pp. 
237-242, Biopolis, Singapore, December 14-17, 2010.  
[21] Ea-Ee Jan, Shih-Hsiang Lin, Berlin Chen, "Transliteration 
retrieval model for cross lingual information retrieval," the 
6th Asia Information Retrieval Societies Conference (AIRS 
2010), pp. 183-192, Taipei, Taiwan, December 1-3, 2010.  
[22] Shih-Hsiang Lin, Berlin Chen, Ea-Ee Jan, "Improving the 
informativeness of verbose queries using summarization 
techniques for spoken document retrieval," the 7th 
International Symposium on Chinese Spoken Language 
Processing (ISCSLP 2010), pp. 75-79, Tainan, Taiwan, 
November 29 - December 3, 2010. (EI) 
[23] Shih-Hsiang Lin, Yao-Ming Yeh, Berlin Chen, “Extractive 
speech summarization － From the view of decision theory,” 
the 11th Annual Conference of the International Speech 
Communication Association (Interspeech 2010), pp. 1684-
1687, Makuhari, Japan, September 26-30, 2010. 
[24] Hung-Shin Lee, Hsin-Min Wang, Berlin Chen, “A 
discriminative and heteroscedastic linear feature 
transformation for multiclass classification,” the 20th 
International Conference on Pattern Recognition (ICPR 2010), 
Istanbul, Turkey, August 23-26, 2010. (EI) 
  
國內會議論文 
[1] Yu-Chen Kao, Berlin Chen, “Improved Modulation Spectrum 
Histogram Equalization for Robust Speech Recognition,” 
ROCLING XXV: Conference on Computational Linguistics 
and Speech Processing (ROCLING 2013), October 4-5, 2013. 
(in Chinese)  
[2] Shih-Hung Liu, Kuan-Yu Chen, Hsin-Min Wang, Wen-Lian 
Hsu, Berlin Chen, “Improved Sentence Modeling Techniques 
for Extractive Speech Summarization,” ROCLING XXV: 
Conference on Computational Linguistics and Speech 
Processing (ROCLING 2013), October 4-5, 2013. (Best Paper 
Award) (in Chinese)  
[3] 陳憶文、陳俊諭、陳冠宇、陳柏琳, “探究多種虛擬關聯文
件篩選方法於語音文件檢索之研究,”「第十七屆人工智慧
與應用研討會」, November 16-18, 2012. (Excellent Domestic 
Paper Award) 
[4] Bang-Xuan Huang, Hank Hao, Kuan-Yu Chen, Berlin Chen, 
"Recurrent Neural Network-based Language Modeling with 
Relevance Information," ROCLING XXIV: Conference on 
Computational Linguistics and Speech Processing (ROCLING 
2012), September 21-22, 2012. (in Chinese) (Best Paper Award) 
[5] Hsin-Ju Hsieh, Jeih-weih Hung, Berlin Chen, “Improved 
histogram equalization methods for robust speech recognition,” 
ROCLING XXIV: Conference on Computational Linguistics 
and Speech Processing (ROCLING 2012), September 21-22, 
2012. (in Chinese)  
[6] 羅永典、林士翔、陳柏琳, “基於最小化貝氏風險之鑑別式
語音文件摘要模型,”「第十六屆人工智慧與應用研討會」, 
November 11-13, 2011. 
[7] Min-Hsuan Lai, Bang-Xuan Huang, Kuan-Yu Chen and Berlin 
Chen, “Empirical comparisons of various discriminative 
language models for speech recognition,” ROCLING XXIII: 
Conference on Computational Linguistics and Speech 
Processing (ROCLING 2011), September 8-9, 2011. (in 
Chinese) (Best Paper Award)  
[8] Wen-Yi Chu, Yu-Chen Kao, Kuan-Yu Chen, Berlin Chen and 
Jeih-Weih Hung, “Probabilistic modulation spectrum 
factorization for robust speech recognition,” ROCLING XXIII: 
Conference on Computational Linguistics and Speech 
Processing (ROCLING 2011), September 8-9, 2011. (in 
Chinese) 
[9] 劉 家 妏 、 朱 紋 儀 、 陳 珮 寧 、 陳 柏 琳 ,”Exploiting 
discriminative language models for speech recognition,”「第
十五屆人工智慧與應用研討會」, November 18-20, 2010.  
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
 
報告人姓名 
 
 
陳柏琳 
 
服務機構
及職稱 
 
國立台灣師範大學 
資訊工程系教授 
     時間 
會議 地點 
民國 102年 5月 26至 31日
Vancouver, Canada 
本會核定
補助文號
NSC 99-2221-E-003-017-MY3 
會議 
名稱 
 (中文) 2013年國際聲學、語音及訊號處理會議 
(英文) The 38th International Conference on Acoustics, Speech and Signal 
Processing (ICASSP 2013） 
發表 
論文 
題目 
(一) 
(中文) 語音文件檢索使用有效虛擬相關回饋 
(英文) Effective Pseudo-Relevance Feedback for Spoken Document Retrieval 
發表 
論文 
題目 
(二) 
(中文) 語音文件檢索使用加權式矩陣分解 
(英文) Weighted Matrix Factorization for Spoken Document Retrieval 
 
報告內容應包括下列各項： 
 
一、參加會議經過 
計畫主持人與所指導的研究生陳憶文同學於民國 102年 5月 26至 31日在加拿大溫
哥華參加 2013年國際聲學、語音及訊號處理會議（International Conference on Acoustics, 
Speech and Signal Processing, ICASSP 2013），並發表兩篇論文。 ICASSP 為信號處理領
域每年舉行之國際性代表會議，與會者多為國際上相關研究機構的頂尖科技人士，論文
審查甚為嚴謹，會議涵蓋的主題涵蓋語音、影像、多媒體、訊號處理技術之範疇。申請
人於 5月 27日從台北出發，在 6月 2日返國。 
 
ICASSP 為每年舉行之國際性重要會議，會議所收錄的論文相當多樣性，每種研究
主題都可以看到與學習到許多近期被提出的新穎技術。而且，會議中學術界與產業界對
於當前的語音暨語言處理科技發展，都作了非常坦誠、頗具建設性的知識和經驗的交流
討論與溝通。尤其在 Keynote Speech的演講與發問過程當中，因為被邀之演講者皆為聲
學、語音及訊號處領域之資深學者及業界人士，從他們的言談中可窺見此領域之未來研
究與發展趨勢。本次會議發表第一場 Plenary Speech的國際學者是加拿大多倫多大學的
教授 Geoffrey E. Hinton，Hinton的研究成果相當傑出，演講內容有關他近年來所發展的
深層類神經網路(Deep Neural Networks)技術。此技術已經成功運用至語音辨識領域，獲
得普遍性的成功並蔚為風潮，效能超越傳統隱藏式馬可夫模型(Hidden Markov Model)。
計畫主持人的兩篇論文同時發表在 5 月 31 日(星期五) Spoken Term Detection and 
Spoken Document Retrieval Session。在會場中我們也遇到來自台大的李琳山教授、中研
院的王新民博士、陳信宏教授、北科大的廖元甫教授等。我們於 6月 1日搭機返回台北，
此次的出國開會與論文發表任務算是圓滿達成。 
EFFECTIVE PSEUDO-RELEVANCE FEEDBACK FOR SPOKEN DOCUMENT RETRIEVAL 
 
Yi-Wen Chen*, Kuan-Yu Chen, Hsin-Min Wang, Berlin Chen* 
 
*National Taiwan Normal University, Taipei, Taiwan 
Institute of Information Science, Academia Sinica, Taipei, Taiwan 
E-mail: *{699470462, berlin}@ntnu.edu.tw, {kychen, whm}@iis.sinica.edu.tw 
 
 
ABSTRACT 
With the exponential proliferation of multimedia associated with 
spoken documents, research on spoken document retrieval (SDR) 
has emerged and attracted much attention in the past two decades. 
Apart from much effort devoted to developing robust indexing and 
modeling techniques for representing spoken documents, a recent 
line of thought targets at the improvement of query modeling for 
better reflecting the user’s information need. Pseudo-relevance 
feedback is by far the most commonly-used paradigm for query 
reformulation, which assumes that a small amount of top-ranked 
feedback documents obtained from the initial round of retrieval are 
relevant and can be utilized for this purpose. Nevertheless, simply 
taking all of the top-ranked feedback documents obtained from the 
initial retrieval for query modeling (reformulation) does not always 
work well, especially when the top-ranked documents contain 
much redundant or non-relevant information. In the view of this, 
we explore in this paper an interesting problem of how to 
effectively glean useful cues from the top-ranked documents so as 
to achieve more accurate query modeling. To do this, different 
kinds of information cues are considered and integrated into the 
process of feedback document selection so as to improve query 
effectiveness. Experiments conducted on the TDT (Topic 
Detection and Tracking) task show the advantages of our retrieval 
methods for SDR. 
Index Terms—Spoken document retrieval, pseudo-relevance 
feedback, query modeling, Kullback-Leibler (KL)-divergence 
1. INTRODUCTION 
In the recent past, spoken document retrieval (SDR) has received a 
growing amount of interest and activity in the speech processing 
community. This is due in large part to the advances in automatic 
speech recognition (ASR) and the ever-increasing volumes of 
multimedia associated with spoken documents made available to 
the public [1, 2, 3]. Unlike research on spoken term detection (STD) 
[3] that usually embraces the goal of extracting probable spoken 
terms or phrases inherent in a spoken document that could match 
the query words or phrases literally, research on SDR revolves 
more around the notion of relevance of a spoken document in 
response to a query [4].  
There are at least two fundamental problems facing SDR. On 
one hand, the imperfect speech recognition transcript carries wrong 
information and thus would deviate somewhat from representing 
the true theme of a spoken document. On the other hand, a query is 
often only a vague expression of an underlying information need, 
and there probably would be word usage mismatch between a 
query and a spoken document even if they are topically related to 
each other. A large body of SDR work has been placed on the 
exploration of robust indexing or modeling techniques to represent 
spoken documents [3, 5, 6, 7, 8], but very limited research has been 
conducted to look at the other side of the coin, namely, the 
improvement of query formulation for better reflecting the 
underlying information need of a user [9]. As for the latter problem, 
pseudo-relevance feedback [4] is by far the most commonly-used 
paradigm, which assumes that a small amount of top-ranked 
documents obtained from the initial round of retrieval are relevant 
and can be utilized for query reformulation. Subsequently, the 
system performs a second round of retrieval with the enhanced 
query representation to search for more relevant documents.  
We had recently introduced a new perspective on query 
modeling [9], saying that it can be approached with pseudo-
relevance feedback and the language modeling (LM) retrieval 
approach [10] leveraging the notion of relevance [11], which 
seems to show preliminary promise for query reformulation. The 
success of such query modeling depends largely on the assumption 
that the set of top-ranked feedback documents obtained from the 
initial round of retrieval are relevant and can be used to estimate a 
more accurate query model. Nevertheless, simply taking all of the 
top-ranked feedback documents obtained from the initial round of 
retrieval does not always work well for query modeling (or 
reformulation), especially when the top-ranked documents contain 
much redundant or non-relevant information.  
With the above background, in this paper we turn our attention 
to a more challenging problem of how to additionally glean useful 
cues from the top-ranked feedback documents to achieve more 
accurate query modeling. Towards this end, several kinds of 
information cues are considered and integrated to select 
representative feedback documents for better retrieval performance. 
The rest of this paper is organized as follows. We briefly review 
the basic mathematical formulations of the LM-based retrieval 
models for SDR, as well as the idea of pseudo-relevance feedback, 
in Section 2. In Section 3, we describe and explain several cues we 
explore to select representative feedback documents during 
pseudo-relevance feedback. After that, the experimental settings 
and a series of retrieval experiments are presented in Sections 4 
and 5, respectively. Finally, Section 6 concludes the paper with 
directions for future research. 
8535978-1-4799-0356-6/13/$31.00 ©2013 IEEE ICASSP 2013
     
   ,                                       
, ,1 maxarg
PTop
*
DMDM
DQMDQMD
DensityDiversity
NRRel
D





DD  (3) 
where pD  is the set of already selected feedback documents;  DQMRel , ,  DQMNR , ,  DMDiversity  and  DMDensity  are 
measures of relevance, non-relevance, diversity and density for 
each document D  in TopD , respectively;  ,   and   are 
weighting coefficients. The selection process illustrated in (3) will 
be executed iteratively until PD  contains a pre-defined number of 
feedback documents. It is worth mentioning that the method 
described in (3) bears a close resemblance in spirit to the maximal 
marginal relevance (MMR) ranking algorithm [16, 17] which was 
originally proposed for extractive document summarization. Note 
also that  DQMRel ,  is just the similarity (query-likelihood) 
measure of the ULM retrieval model depicted in (1). In the 
following, we will describe how to model the other information 
cues we explore for a given candidate feedback document. 
3.1. Non-Relevance Measure 
For a given query , we can estimate a non-relevance model  QNRwP |  of it based on the low-ranked documents obtained from 
the initial round of retrieval, and the non-relevance measure of a 
candidate feedback document  is thus defined by 
   .DNRKLDM QNR      (4) 
The additional incorporation of  DM NR  for feedback document 
selection will prefer those documents that have only a small 
probability distance to the original query model but also a larger 
probability distance to the non-relevance model. Since the number 
of relevant documents with respect to a given query is usually very 
small compared to that of non-relevant ones in practice, we may 
assume that the entire spoken document collection (more 
specifically, the background language model  BGwP | ) could 
offer an alternative estimate of the non-relevance model. 
3.2. Diversity Measure 
Recently, diversification of retrieval results has gained popularity 
in the text IR community, since it can be used to complement the 
conventional document ranking criteria which only consider 
relevance information and often suffer from returning too many 
redundant documents. By analogy, in the context of pseudo-
relevance feedback, if we use the top-ranked documents that 
contain too much redundant information to estimate the query 
model, then the second round of retrieval is prone to return too 
many “redundant” documents to the user. In order to diversify the 
selected feedback documents for better query reformulation, we 
compute the diversity measure of a candidate feedback document 
with respect to the set  of already selected feedback documents, 
which is expressed as follows: 
      .||||
2
1min
P
jj
D
Diversity DDKLDDKLDM
j

D
 (5) 
3.3. Density Measure 
Intuitively, the structural information among the top-ranked 
documents can be taken into account as well during feedback 
document selection. For this idea to work, we can compute the 
average negative, symmetric probability distance between a 
document  and all the other documents hD  in , which is 
expressed as follows: 
      ,||||
1
1
TopTop
 



DD
D
hhDensity
h
h
DDKLDDKLDM
DD
 (6) 
where TopD  is the number of documents in . A document 
 having a higher value of  DMDensity  is deemed to be closer to 
the other documents in  and thus to be more representative 
(and less likely to be an outlier). 
4. EXPERIMENTAL SETUP 
4.1. Spoken Document Collection 
We used the Topic Detection and Tracking collection (TDT-2) [9, 
18] for this work. The Mandarin news stories from Voice of 
America news broadcasts were used as the spoken documents. All 
news stories were exhaustively tagged with event-based topic 
labels, which served as the relevance judgments for performance 
evaluation. The average word error rate (WER) of the spoken 
documents is about 35% [19]. The retrieval results, assuming that 
manual transcripts for the spoken documents to be retrieved 
(denoted TD, text documents, in the tables below) are known, are 
also shown for reference, compared to the results when only the 
erroneous transcripts by speech recognition are available (denoted 
SD, spoken documents, in the tables below). The retrieval results 
are expressed in terms of non-interpolated mean average precision 
(mAP) following the TREC evaluation [4]. Table 1 shows some 
basic statistics about the TDT-2 collection. In order to evaluate the 
performance of the various feedback document selection methods 
studied in this paper, the number of the top-ranked documents 
obtained from the first round of retrieval is set to 25 (i.e., 
=25) and the target number of selected feedback document is set to 
5 (i.e., PD =5). Albeit that, it is known that the way to 
systemically determine the values of the free parameters that the 
feedback document selection methods, as well as the retrieval 
models, incorporate is still an open issue and needs further 
investigation and proper experimentation. 
4.2. Query Modeling 
In this paper, we employ RM and SMM for query reformulation in 
concert with the various feedback document selection methods 
studied in this paper. The refined query model based on RM [11] is 
formulated by 
Q
D
PD
D TopD
TopD
D
TopD
TopD
Table 1. Statistics for TDT-2 Collection. 
# Spoken documents 2,265 stories 46.03 hours of audio 
# Distinct test queries 16 Xinhua text stories (Topics 20001∼20096) 
 Min. Max Med. Mean
Document length 
(in characters) 23 4841 153 287 
Length of query 
(in characters) 8 27 13 14 
# Relevant documents 
per test query 2 95 13 29 
 
8537
8. REFERENCES 
[1] L. S. Lee and B. Chen, “Spoken document understanding and 
organization,” IEEE Signal Processing Magazine, 22(5), 42–
60, 2005. 
[2] M. Ostendorf, “Speech technology and information access,” 
IEEE Signal Processing Magazine, 25(3), 150–152, 2008.  
[3] C. Chelba, T. J. Hazen and M. Saraclar, “Retrieval and 
browsing of spoken content,” IEEE Signal Processing 
Magazine, 25(3), 39–49, 2008. 
[4] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information 
Retrieval: The Concepts and Technology behind Search, 
ACM Press, 2011. 
[5] V. T. Turunen and M. Kurimo, “Indexing confusion networks 
for morph-based spoken document retrieval,” in Proc. ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, 631–638, 2007. 
[6] S. Parlak and M. Saraçlar, “Performance analysis and 
improvement of Turkish broadcast news retrieval,” IEEE 
Transactions on Audio, Speech and Language Processing, 
20(3), 731–743, 2012. 
[7] B. Chen, “Word topic models for spoken document retrieval 
and transcription,” ACM Transactions on Asian Language 
Information Processing, 8(1), 2:1–2:27, 2009. 
[8] T. K. Chia, K. C. Sim, H. Li, and H. T. Ng, “Statistical lattice-
based spoken document retrieval,” ACM Transactions on 
Information Systems, 28 (1), 2:1–2:30, 2010. 
[9] B. Chen, K.-Y. Chen, P.-N. Chen and Y.-W. Chen, “Spoken 
document retrieval with unsupervised query modeling 
techniques,” IEEE Transactions on Audio, Speech and 
Language Processing, 20(9), 2602-2612, 2012. 
[10] C. X. Zhai, “Statistical language models for information 
retrieval: A critical review,” Foundations and Trends in 
Information Retrieval, 2 (3), 137–213, 2008. 
[11] V. Lavrenko and W. B. Croft, “Relevance-based language 
models,” in Proc. ACM SIGIR Conference on Research and 
Development in Information Retrieval, 120–127, 2001. 
[12] S. Kullback and R. A. Leibler, “On information and 
sufficiency,” The Annals of Mathematical Statistics, 22(1), 
79–86, 1951. 
[13] C. Zhai and J. Lafferty, “Model-based feedback in the 
language modeling approach to information retrieval,” in 
Proc. ACM SIGIR Conference on Information and knowledge 
management, 403–410, 2001. 
[14] X. Shen and C. Zhai, “Active feedback in ad hoc information 
retrieval,” in Proc. ACM SIGIR Conference on Research and 
Development in Information Retrieval, 55–66, 2005. 
[15] 24. Z. Xu, R. Akella and Y. Zhang, “Incorporating diversity 
and density in active learning for relevance feedback,” in 
Proc. European conference on IR research, 245–257, 2007. 
[16] J. Carbonell and J. Goldstein, “The use of MMR, diversity-
based reranking for reordering documents and producing 
summaries,” in Proc. ACM SIGIR Conference on Research 
and Development in Information Retrieval, 335–336, 1998. 
[17] B. Chen and S.-H. Lin, “A risk-aware modeling framework 
for speech summarization,” IEEE Transactions on Audio, 
Speech and Language Processing, 20(1), 199–210, 2012. 
[18] LDC, “Project topic detection and tracking,” Linguistic Data 
Consortium, 2000. 
[19] H. Meng, B. Chen, S. Khudanpur, G. A. Levow, W. K. Lo, D. 
Oard, P. Schone, K. Tang, H. M. Wang, and J. Wang, 
“Mandarin–English information (MEI): investigating 
translingual speech retrieval,” Computer Speech and 
Language, 18(2), 163–179, 2004. 
[20] A. P. Dempster, N. M. Laird and D. B. Rubin, “Maximum 
likelihood from incomplete data via the EM algorithm,” 
Journal of Royal Statistical Society B, 39(1), 1–38, 1977. 
[21] D. M. Blei, A. Y. Ng and M. I. Jordan, “Latent Dirichlet 
allocation,” Journal of Machine Learning Research, 3, 993–
1022, 2003 
[22] B. Chen and K.-Y. Chen, “Leveraging relevance cues for 
language modeling in speech recognition,” Information 
Processing & Management, 49(4), pp. 807-816, 2013.  
[23] B. Chen, H.-C. Chang and K.-Y. Chen, “Sentence modeling 
for extractive speech summarization,” in Proc. IEEE 
International Conference on Multimedia & Expo, 2013. 
 
 
8539
One major difference between PLSA (and other variants of 
PTM) and LSA (and other variants of NPTM) is that the former 
only takes the words occurring in a document into account, but the 
latter explicitly models all the words in the vocabulary (including 
both words occurring and non-occurring in a document) [17, 18]. 
A document usually contains only a few distinct words, i.e., most 
words in the vocabulary do not occur in a document. Although 
LSA has the advantage of modeling both the occurring and non-
occurring words in a given document, treating all the words with 
equal importance could be a serious disadvantage. In order not to 
overemphasize the non-occurring words in the original LSA model, 
we leverage a weighted matrix factorization (WMF) framework to 
properly modulate the impact of the occurring and non-occurring 
words on the semantic analysis. We also exploit multi-levels of 
index features, including word- and syllable-level units, in concert 
with the proposed WMF framework. The results of SDR 
experiments on the TDT-2 (Topic Detection and Tracking) 
collection demonstrate the superior performance of the 
instantiations of the WMF framework over several existing 
methods, including LSA. It is worth noting that the WMF 
framework can be applied to general IR tasks as well. 
The remainder of this paper is organized as follows. We briefly 
review the mathematical formulations of the topic models for SDR 
in Section 2. In Section 3, we detail our proposed WMF framework. 
Then, the experimental settings and results are presented in 
Sections 4 and 5, respectively. Finally, Section 6 gives our 
conclusion and future work. 
2. RELATED WORK 
2.1. Probabilistic Topic Models (PTM) 
Instead of matching a query and a document in a literal index term 
space, the relevance between a pair of query and document can be 
estimated on the grounds of a set of latent topics. For this idea to 
work, each document d  is taken as a document topic model dM , 
consisting of a set of K  shared latent topics  Kk TTT ,,,,1   
associated with the document-specific weights  dkTP M , where 
each topic 
kT  in turn offers a unigram distribution  ki TwP  for 
observing an arbitrary word of the language [12, 14, 19, 20]. For 
example, in the PLSA model, the probability of a word 
iw  
generated by a document d  is expressed by: 
     .Μ Μ
1PLSA
 
 K
k dkkidi
TPTwPwP    (1) 
A document is believed to be more relevant to the query if the 
query words appear frequently in the topics on which the document 
has higher weights.  
On the other hand, LDA, having a formula analogous to PLSA 
for document modeling, is thought of as a natural extension to 
PLSA, and has enjoyed much empirical success for various text IR 
tasks. LDA differs from PLSA mainly in the inference of model 
parameters: PLSA assumes that the model parameters are fixed and 
unknown; while LDA places additional a priori constraints on the 
model parameters, i.e., thinking of them as random variables that 
follow some Dirichlet distributions [19]. Since LDA has a more 
complex form for model optimization, which is hardly to be solved 
by exact inference, several approximate inference algorithms, such 
as the variational Bayes approximation [13, 14] and the Gibbs 
sampling algorithm [21], have been proposed to facilitate the 
estimation of the parameters of LDA according to different training 
strategies. 
2.2. Non-Probabilistic Topic Models (NPTM) 
Unlike probabilistic topic modeling, LSA [4, 6, 16] is an 
alternative way to describe the “word-document” co-occurrence 
characteristics. It assumes that there is an implicit semantic 
structure between words and documents, and the semantic structure 
can be explored by performing SVD on a pre-defined word-by-
document matrix. When given N  documents, which consist of M  
distinct words, we have an NM   matrix A . Each element 
ijA
 of 
A  is the frequency of word (or term) iw  in document jd
. To 
eliminate some noisy words (e.g., function words), the inverse 
document frequency (IDF) can be used to weight the term 
frequency (TF) count, leading to the well-known TF-IDF [7]. 
Subsequently, SVD decomposes A  into three sub-matrices: 
,
~
NM
T
NKKKKMNM   AVUA                  
 (2) 
where  NMK ,min ; U  and V  are orthonormal matrices, i.e., 
IVVUU  TT ; and   is a diagonal matrix. Each word is 
uniquely associated with a row vector of matrix U , and each 
document is uniquely associated with a column vector of matrix 
T
V . In the retrieval phase, a query is viewed as a new document, 
and its K-dimensional vector representation is computed by a 
“fold-in” process as follows, 
.~ 1 Uqq T                    
 (3) 
The relevance degree between a pair of query and document is 
estimated by the cosine similarity measure between the query and 
the document representations (vectors). 
2.3. Some Extensions of NPTM 
LSA captures most of the important associations between words 
and documents, and removes the noise or variability in word usage, 
which often plague conventional IR models. Intuitively, since the 
number of dimensions is much smaller than the number of 
vocabulary words, the words occurring in similar documents will 
be represented in the nearby vicinity in the K-dimensional space 
even if they have never co-occurred in the same document. 
Consequently, LSA has been shown to achieve pretty good 
performance in IR, and much effort has been paid to improve LSA 
from different aspects. 
Semantic context inference (SCI) [3] is a specially designed 
model for concept mapping and context expansion of spoken 
documents in SDR. The major difference between SCI and LSA is 
that SCI takes the word-word associations into account, while LSA 
considers the word-document co-occurrence relationships. SCI 
builds a semantic relation matrix to reflect the word-word 
associations, and performs SVD on the matrix to remove the noisy 
factors and capture the most important associations. In addition, a 
few LSA-based language models [4, 5, 22] attempt to construct a 
matrix to render the word-ordering information. Regularized latent 
8531
where Q  is the number of test queries, iN  is the total number of 
documents that are relevant to the i-th test query, and 
i,jr
 is the 
position (rank) of the j-th document that is relevant to the i-th 
query, counting down from the top of the ranked list.  
5. EXPERIMENTAL RESULTS 
First, our proposed WMF model is compared with two instances of 
non-probabilistic topic models, namely LSA and SCI. The results 
when using different types of queries (i.e., long or short queries) 
and different kinds of index features (i.e., word- or subword-level 
index features) are shown in Tables 2 and 3. The number of latent 
topics of all the topic models is set to 128, the regularization 
parameters (
X  and Y ) are set to 1, and the frequency count of 
words is weighted by using the standard IDF method. From the 
results, at first glance, it seems that the high word error rate (WER) 
for the spoken document collection (about 35%) does not lead to 
catastrophic failures probably due to the reason that recognition 
errors are overshadowed by a large number of spoken words 
correctly recognized in the documents. The experimental results 
seem to reveal that the proposed WMF framework outperforms 
both LSA and SCI in most cases. It is worth noting that, when 
using word-level indexing features for SDR, WMF yields 
significant improvements over LSA and SCI. We have also found 
that WMF achieved better performance when   in Eq. (5) was set 
around 0.08. The results confirm our idea that the words non-
occurring in a document should not be considered as important as 
the occurring ones, although the former might provide additional 
information. 
From Tables 2 and 3, it can also be observed that the 
conventional term matching strategy (denoted by VSM) can also 
achieve a certain level of performance. Although merely matching 
terms in the original query and document may not always capture 
the semantic intent of a query, term matching still provides an 
important clue for retrieval, which is complementary to the concept 
matching. To use both literal and concept information, we 
concatenate the VSM and the WMF features to construct a new 
index vector for both queries and documents (cf. Eqs. (9) and (10) 
in Section 3.1), and the retrieval results are also shown in Tables 2 
and 3 (denoted by “Hybrid”). As expected, the hybrid method 
outperforms VSM and WMF in all cases. In fact, it performs better 
than all the other models in all cases. 
Finally, Figure 1 reports the retrieval results for short queries 
with respect to the number of latent topics when using word-level 
index features. It is known that the way to systemically determine 
the optimal number of latent topics for topic models is still an open 
issue and needs further investigation. As can be seen from Figure 1, 
the performance of most non-probabilistic topic models is apt to be 
improved as the topic number increases, except that LSA seems to 
saturate when the topic number is larger than 32. Due to space 
limitations, we only report on the results for one setting, but similar 
tendencies are observed for other settings (e.g., different types of 
queries and indexing mechanisms). 
6. CONCLUSION & FUTURE WORK 
This paper has proposed a weighted matrix factorization 
framework for spoken document retrieval, which suggests a 
promising way to improve the latent semantic analysis model by 
directly modulating the impact of words occurring and non-
occurring in documents on the document (or query) representations. 
The utility of the proposed framework has been validated by 
extensive comparisons with several existing information retrieval 
models. Our future work includes the development of supervised 
training, incorporation of some prior knowledge, and extension to 
probabilistic topic models. 
 
Figure 1. Retrieval results (in MAP) for short queries with word-level 
index features with respect to the number of latent topics. 
0.00
0.10
0.20
0.30
0.40
0.50
8 16 32 64 128
LSA SCI
WMF Hybrid
Table 1. Statistics of the TDT-2 collection. 
 TDT-2 (1998, 02~06) 
# Spoken documents 
2,265 stories, 
46.03 hours of audio 
# Distinct test queries 
16 Xinhua text stories 
(Topics 20001~20096) 
 Min. Max. Med. Mean 
Doc. length 
(in characters) 
23 4,841 153 287.1 
Length of test short query 
(in characters) 
8 27 13 14 
Length of test long query 
(in characters) 
183 2,623 329 532.9 
# Relevant documents  
per test query 
2 95 13 29.3 
Table 2. Retrieval results (in MAP) with word-level index features for 
short and long queries. 
 VSM LSA SCI WMF Hybrid 
short 0.273 0.379 0.270 0.438 0.448 
long 0.484 0.512 0.413 0.561 0.561 
Table 3. Retrieval results (in MAP) with subword-level index features 
for short and long queries. 
 VSM LSA SCI WMF Hybrid 
short 0.257 0.330 0.270 0.328 0.338 
long 0.499 0.466 0.349 0.475 0.513 
 
 
 
(Number of Topics) 
(MAP) 
8533
國科會補助計畫衍生研發成果推廣資料表
日期:2013/11/10
國科會補助計畫
計畫名稱: 語言模型在大詞彙連續語音辨識之進一步研究
計畫主持人: 陳柏琳
計畫編號: 99-2221-E-003-017-MY3 學門領域: 自然語言與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
與 IBM 研究員進行跨國研究合作，並將相關成果發表在 AIRS 2010、ISCSLP 
2010、ICASSP 2011、IJCNLP 2011、ASRU 2013。同時並派學生於 2010 赴美國
IBM 研究中心擔任 Intern。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
