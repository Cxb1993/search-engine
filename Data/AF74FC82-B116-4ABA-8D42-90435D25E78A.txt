 2
移動，含有 motion 的畫面在時間軸上所帶來
資訊常與真正的分鏡畫面含有相似的時間
特性，故本研究在分鏡變換偵測上便以去除
畫面中移動物件的資訊做為有效的改進方
法，且將本研究所提出之方法流程圖分成三
個小節於下概述之。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖 1：分鏡變換偵測架構流程圖。 
3.1.1 特徵分析(Features Analysis) 
在本研究中，視訊資料的分鏡偵測流程
啟始是由視訊資料，也就是一部彩色影片
(Video)做為測試的輸入端，影片是經由一張
張連續且獨立的畫面(Frame)組成，為了得到
影片最基礎單位的資訊，必須將這一張張畫
面先行切割出來，得到一連串的視訊資料，
這些資料同時也代表著影片的最原始資
訊，故稱之為原始畫面 (Original frames: 
Ot(X,Y))。原始資訊的特徵資料 ft(X,Y)是經
由彩色模型轉換(Color Model Transform)取
出原始畫面裡每個像素點(X,Y)的灰階亮度
值，所以 ft(X,Y)所代表的正是影片在空間軸
上的資訊。 
此時若 ft(X,Y)在時間 t 與 t+1 有分鏡變換的
發生，反應到時間軸上將會得到一個為數大
量的差異值，然而差異值的大量產生卻未必
全是由分鏡變換而來，導致誤判的最大誘因
在於移動物體(Moving object)對連續畫面同
樣能造成一股大量差異值，為了解決移動物
體對分鏡變換偵測所帶來的不良影響，在計
算差異量之前，若可以得到移動物體在空間
及時間上的訊息，將可大為提升在分鏡偵測
上的良好表現，所以在本研究中便利用空間
加時間的快速小波轉換(STWT)，同時可以得
到連續畫面之間的差異量，更可幫助得到移
動物體所帶來的資訊，這部份將在接下來的
3.2 節中詳加解釋。 
3.1.2 移 動 影 像 的 建 立 與 特 徵 統 計
(Establishing for Moving Image and 
Effects of Correlation) 
從流程圖中可見 ft(X,Y)經由 2+1D 快速
小波轉換可得到各個次頻帶(subband)的小
波係數，至此分為兩邊進行，右線將次頻帶
中所有屬於時間軸高頻的四個次頻帶
(HHH、HHL、HLH、HLL)運用各種特徵分
析方法加以合併(Combine)之後得到的即為
移動物體的資料畫面(Moving Image :MI)；左
線的 HLLt(x,y)就是時間軸上的差異，接下來
的步驟就是結合 HLLt(x,y)與 MIt(x,y)，並統
計在每一個時間 t 的差異量(Di(t))為實驗的
特徵值移動影像的建立與特徵統計方法將
在本研究的第 3.3 節中做充份的說明。 
3.1.3 視訊資料分鏡切割方法 
將上述每個時間點 t 所得到的特徵值 Di(t)
輸入貝氏辨認器，所得之輸出即可將與 t 相
對應時間點上的原始畫面分為分鏡變換畫
面(Shot change frames)以及非分鏡變換畫面
(No shot change frames)兩大類，以達成分鏡
偵測的結果。對於分鏡切割的方法將詳述於
本文第 3.4 節。 
3.2 空間與時間軸快速小波轉換(STWT) 
執行分鏡變換偵測最重要的步驟就是特
徵的選取，要如何選擇一個適當且符合需求
的特徵將是影響偵測結果最關鍵的楔子，在
本章中將從 1D 與 2D 快速小波轉換公式進
而推演至本研究之應用原理。且由彩色影像
轉換至彩色模型開始，經由空間與時間上之
2+1D 快速小波轉換處理來探討利用小波轉
Establishing for MI and Effects of Correlation
Video Segmentation Algorithm 
Features Analysis 
No shot 
change frames 
Video 
Segment 
STWT 
Combine 
Estimate for feature 
Bayes classifier 
Shot change detection Shot  
change frames 
Di(t) 
MIt(x,y) : 
 Moving Image 
HHHt(x,y),HHLt(x,y) 
HLHt(x,y),HLLt(x,y) 
HLLt(x,y) 
Original frames : Ot(X,Y) 
ft(X,Y) 
Color Model Transform 
 4
 
(a)     (b) 
圖 3：Cut shot change 在第 j 個 stage 時的
Recall rate 和 Precision rate。(a) Recall rate， 
(b) Precision rate。 
 
(a)     (b) 
圖 4：Gradual shot change 在第 j 個 stage 時
的Recall rate和Precision rate。(a) Recall rate， 
(b) Precision rate。 
由圖可知，不同的小波轉換方程式，因
為參考的畫面張數有所不同，張數小的在位
置上會比較精準，但是會找到比較局部性的
變化，對 cut 來說會比較好；而張數大的在
位置上得到結果可能會有偏移，但對 gradual
的收斂結果較好，所以 Haar 與 B-spline 兩
種小波基底在 Cut 分鏡變換的偵測上對
precision rate 與 recall rate 有著極優益的表
現，而 Optimal edge 在 Gradual 分鏡偵測的
表現上享有前兩者所沒有的優勢。[5]實驗是
以多階相乘為主軸，而本方法只取 cut 與
gradual 降一階為實驗結果；另一個基底
Daubechies 4 在[6]偵測移動物體的研究結
果上也是占有其獨領風騷的地位，所以本實
驗選用了四個小波基底使訓練資料的特徵
維度為一組四個特徵，分別是由 Haar、
B-spline、Optimal edge 與 Daubechies 4 四個
小波基底得到的四個 )(xDi 值。 
3.3 移動影像的建立與特徵統計 
接下來就 STWT 所得到的結果，進行特
徵的擷取、運算及達成最後的特徵統計結
果。方法是由 STWT 的三個時間軸高頻
yxWH , 、 yxWV , 、 yxWD , 先行合成，利用所
得之合成畫面中分辨屬於移動物體的像素
建立移動影像(Moving Image ,MI)，最後藉由
MI 統計 HLL 頻帶的差異量，完成分鏡偵測
的特徵值。 
3.3.1 移動影像(Moving Image)的建立 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖 5：建立移動影像之流程圖。 
建立移動影像(MI)的流程圖如上圖 5 所以，
接下來將分成四個小點詳述每一個步驟的
的目與方法。 
3.3.2 合成 
首先，四個時間軸的高頻頻帶都保有移
動物體的資訊，而為了得到每一個時間 t 的
移動影像，必須將這四個頻帶的高頻資訊用
相加的方式做結合成為時間軸的高頻結合
影像(H Combine Image)，： 
∑∑
= =
+++
=
Ms
x
Ns
y
tttt
t
yxWDyxWVyxWHyxWS
yxHCI
0 0
),(),(),(),(
),( (9) 
),( yxWSt 、 ),( yxWHt 、 ),( yxWVt 、
),( yxWDt 分別為 LL、HL、LH 及 HH 頻帶上
(x,y)資訊於時間 t 時的高頻訊號，下圖為圖 6
為範例原圖與於時間 t 所得到的 HCI： 
 
 
 
 
 
Combine 
Estimate 
Thresholding 
Morphology 
Median Filter 
HHH、HHL、HLH 
Moving Image 
 6
 
 
 
 
圖 9：Morphology 的結果。 
 
 
 
 
 
 
 
 
 
 
圖 10：MI 與原始畫面相對應圖。 
圖 9 是將中值濾波器得到的影像做
Morphology 的結果，Morphology 可以隨著不
同次數及不同組合的擴張與收縮達到最佳
的效果，在本文中是選用兩組一次擴張、一
次收縮的方式得到最後的結果，且將圖 9 的
影像反相後得到的新影像稱為移動影像
Moving Image(MI)，MI 是一張由零與壹組成
的二值化影像，且移動物體的像素點就是
MI 畫面中像素值為 0 的點。圖 10 是將原始
畫面與 MI 畫面做交集所得到的結果，也清
楚的顯示了應用 STWT 所得到的移動影像與
原始畫面之間的關係。 
步驟至此已經完成了移動影像的建立，也已
經達到了欲以先行偵測移動物體之所在，使
接下來的特徵統計做更有力、更有效的估
算。 
3.3.6 統計特徵 
得到移動影像之後，接下來就是特徵的
統計。連續畫面之間有分鏡變換發生時，畫
面像素之間在時間上會有符合反應的不連
續性，也就是兩張畫面之間所存有的差異，
所以只要將連續畫面 ft(x,y)與 ft+1(x,y)的差異
值統計起來，就能找到分鏡變換所在的畫
面。時間上的差異量，可以利用 STWT 得到
的 HLL 來代表，並以上節得到的 MI 資訊做
為條件，將 HLL 上的像素值相加得到適用於
分鏡變換偵測的特徵值 Di(t)，如公式 11 所
示： 
∑∑
= = −⋅
⋅=
Ms
x
Ns
y no
t
MINsMs
yxMIyxWStDi
0 0 )(
),(),()(   (11) 
∑∑
= =
=
Ms
x
Ns
y
no yxHCIMI
0 0
),(     (12) 
,: sizeSubbandNsMs×∈ℜ  
),( yxWSt 是 HLL 上的像素值，Mino 是
移動影像 MI 中像素值為 0 的個數。至此為
止，小波轉換的工作已經告一段落，從 STWT
得到的 Di(t)也將成為分鏡偵測裡最重要的
輸入特徵，接下來就是將畫面於時間 t 得到
特徵值 Di(t)做分鏡變換的偵測。 
小波轉換與一般的時、空間轉換函式不同點
在於小波轉換擁有許多不同的轉換基底，每
種轉換基底都有其獨有或適用的特性而各
司其職，使特徵的選取及演算法的寫作簡單
而又具有效性，這也是本文之所以採行小波
轉換為特徵的最佳解釋。 
3.4 視訊資料分割 
從上一章得到的特徵值 Di(t)存在著畫面
t 與 t+1 之間的相關性，在分鏡變換偵測的原
理上有其合理的代表性。所以接下來就是從
特徵值 Di(t)下手，目的是為了要將視訊資料
裡的所有畫面分成兩大類：分鏡變換畫面與
非分鏡變換畫面。首先將使用一般的臨界值
法對畫面進行分類的探討，另一方面，本文
將使用貝氏辨認器(Bayes classifier)做為分
鏡變換偵測的分類法。 
四、成果與討論 
本章節將對以貝氏辨認器為分鏡偵測的
方法進行實驗。接下來先介紹實驗資料以及
 8
圖 11 是四種小波基底兩兩之間的組合
分佈，Xc、Xg 與 Xn 分別表示畫面 X 是屬
於 Cut、Gradual 與一般畫面[D1,D2,D3,D4]
則是畫面 X 在 Haar、B-spline、Optimal edge
與 Daubechies 4 四個小波基底得到的四個
)(xDi ，其中圖 11(a)(b)(c)都可以得到良好的
區域分佈，但圖 11(a)(c)沒有包含對 Gradual
有用的 D3 資訊，圖 11(b)則欠缺位置準確度
的 D4 資訊，其他的圖 11(d)(e)(f)的分佈有相
互的干擾，所以也不是一個最理想的分佈。 
 
(a)      (b) 
 
(c)      (d) 
 
(e)      (f) 
圖 11：Cut、Gradual 與一般畫面的 )(xDi 在四種小波基底兩兩之間的組合分佈。
(a)Xi[D1,D2] (b) Xi[D1,D3] (c) Xi[D1,D4] (d) 
Xi[D2,D3] (e) Xi[D2,D4] (f) Xi[D3,D4]。 
下圖 12 是四種小波基底三者之間的組
合 分 佈 ， 從 圖 14(a)(b)(c) 觀 察 得 知 圖
12(d)(e)(f)分別加入 D1 的資訊後都使分佈趨
於良性發展，由此可知，當圖 12(d)再加上
一個維度放置 D1 資訊後，可使特徵分佈達
到我們理想上想要的清晰分區的目的。 
 
 
 
(a)      (b) 
 
(c)      (d) 
圖 12：Cut、Gradual 與一般畫面的 )(xDi 在四種小波基底三者之間的組合分佈。 (a) 
Xi[D1,D2,D3] (b) Xi[D1,D2,D4] (c) 
Xi[D1,D3,D4] (d) Xi[D2,D3,D4]。 
由上可知，同時使用此四種小波轉換基
底當做特徵輸入不僅能囊括 Cut 與 Gradual
分鏡變換時需要的特性且能得到清楚的分
區，彌補了只考慮單個或資訊不足的特徵。 
下圖 13 與圖 14 是應用本方法與 Nagasaka 
et al.[1]所提出的χ2 測試法代入測試影像後
得到的結果，其中的臨界值選取是利用[2]
的 Thresholding 法自動求得，事前機率值 P(n)
由 0.01 開始，以每次增加 0.01 的方式加至 1
得到最佳的 precision 和 recall rate 做為實驗
的結果，其中(P(c)=1- P(n))及(P(g)=1- P(n))。 
 
(a)      (b) 
圖 13: Cut shot change detect 結果。(a) 
Precision rate， (b) Recall rate。 
 
(a)      (b) 
圖 14: Gradual shot change detect 結果。(a) 
Precision rate， (b) Recall rate。 
 10
scale multiplication in wavelet domain,” 
Pattern Recognition Letters, Vol. 23, No. 
14, pp. 1771-1784, Dec. 2002.  
[4] S. G. Chang, B. Yu, M. Vetterli, “Adaptive 
wavelet thresholding for image denoising 
and compression,” IEEE Transactions on 
Image Processing, Vol. 9, No. 9, Sep. 
2000. 
[5] Y. K. Wang and P. H. Chen, “Shot change 
detection using temporal wavelet 
transform,” Proceedings of IPPR 
Conference on CVGIP’ 2004, Hwa Leing, 
Taiwan, R. O. C., pp. 71, 2004. 
[6] Y. K. Wang and S. H. Chen, “A robust 
vehicle detection approach,” Proceedings 
of IPPR Conference on CVGIP’ 2004, 
Hwa Leing, Taiwan, R. O. C., pp. 46, 
2004. 
六、計畫成果自評 
本研究利用 STWT 同時在空間與時間上
獲取用來降低物體移動的資訊與供給 Bayes 
classifier 做為輸入的特徵，更善用了 Bayes 
classifier 在分鏡偵測應用上的辨識能力，成
功的加強了大量的物體和攝影機移動對偵
測結果的影響，且經由實驗證實其結果的不
俗。 
從實驗的錯誤情形看來，本方法在計算
差異量的方法上仍有進步的空間，未來可以
進一步考慮空間上位置的資訊來彌補區塊
的變化量以改善計畫差異量時的缺失，而閃
光燈的影響則有許多前進已提出相當有效
的方法來解決了；本實驗的兩個參數 min1
與 min2 是以經驗法則定立，未來可以試著
用數值來解析 min1 與 min2 的最佳參數解。
另外，實驗上花費較多時間的是 STWT 的部
份，其中又以 thresholding 占據頗大的運算
時間，由於本論文並不以即時(Real time)偵
測為主，所以並不考慮時間消耗的問題，未
來若需達到即時偵測的目的則可權衡
thresholding 的方法以大幅度降低運算時間。 
本研究成果之內容與原計畫之預期進行內
容相符，預期目標皆達成，研究成果適合在
學術期刊發表。 
大會於每日上下午安排有 1個主題演講，讓與會者能瞭解近來相關領域的最新研究現
況，聆聽這些演講以及其他的論文報告，皆獲益良多。最後，會議在 8/24 日下午結束，也
結束此次的學術探訪之旅。 
 
二、與會心得 
 
這次為第 18 屆 ICPR 研討會，投稿論文約 2000 篇，錄取約 1000 篇，透過 50％的錄取
率以獲得高品質的論文。參加此極富盛名之國際研討會，與國外一流學者請教最新研究現
況，可學習不少知識。此次共約有 1000 篇論文發表。該會議的特色，在於圖形識別於影像、
視訊等各種訊號之辨識與應用。參與這樣的大型會議，可以擴展研究創意，多瞭解相關的
研究成果，據以研思如何在日後繼續產生更好的研究成果。這是參與本次會議最重要的收
穫。此次會議看到台灣有人獲得 IAPR Fellow，深感與有榮焉。 
參與本次這種大型國際會議，可以提升研究品質、促成跨領域研究的創意，是相當好
的會議，建議將來能多加支持國內學者參與此型會議。 
 
 
)(~)()(),,(5 tyxtyx ψϕψψ =   (6) 
)(~)()(),,(6 tyxtyx ϕψψψ =   (7) 
)(~)()(),,(7 tyxtyx ψψψψ =   (8) 
ϕ(x, y, t) in Equation (1) means low frequency on each dimension. 
Equations (2) ~ (8) display different choices on each dimension. 
The following discrete convolution operations used to compute 
the approximation and detail coefficients resolution narrate the 
computation for a given image sequence V [7]. 
]~[ hhhVa ∗∗∗=     (9) 
]~[1 ghhVd ∗∗∗=   (10) 
]~[2 hghVd ∗∗∗=   (11) 
]~[3 gghVd ∗∗∗=   (12) 
]~[4 hhgVd ∗∗∗=   (13) 
]~[5 ghgVd ∗∗∗=   (14) 
]~[6 hggVd ∗∗∗=   (15) 
]~[7 gggVd ∗∗∗=   (16) 
The discrete convolution operator *, which acts on the 
approximation coefficient and generates one three-dimensional 
approximation coefficient and seven detail coefficients, convolves 
signal on x, y and z axes in order. Equations (17) ~ (20) explain 
the relationships between two pairs (h(n), g(n)) and ( h~ (n), g~ (n)), 
where )(nh = h(-n), )(~ nh =  )(
~ nh  in Equations (9) ~ (16). 
dxnxxnh )()
2
()( −≡ ∫∞∞− ϕϕ  (17) 
dtnttnh )(~)
2
(~)(~ −≡ ∫∞∞− ϕϕ  (18) 
)1()1()( 1 nhng n −−= −   (19) 
)1(~)1()(~ 1 nhng n −−= −   (20) 
Horizontal, vertical and diagonal edges corresponding to the 
signals of high frequency in text areas are left on certain sub-
bands produced by this three-dimensional spatio-temporal wavelet 
transform on image sequences. h  represents for low frequency, 
and g  for high frequency. For example, d4 in Equation (13) is a 
detail coefficient and also called a sub-band of high frequency on 
x axis and low frequency on y and t axes in turn, and other sub-
bands contain different mixtures of low and high frequencies. Due 
to low frequency both in x and y direction, a and d1 are discarded 
and the other six sub-bands are adopted for further processes. Low 
and high frequencies are also expressed as L and H. Fig. 2 gives 
the whole idea for the description of spatio-temporal wavelet 
transform in this kind of expression. 
We combine d2, d4 and d6 for static texts but d3, d5 and d7 
for dynamic texts. A salience map results from each combination. 
A pixel on the edge of text might have high intensity value, and it 
remains high value after this combination. However, a pixel of the 
background region won’t react the same owing to having high 
intensity value only on a certain direction. Such distinguishing 
step is considered some kind of enhancement to tell the text and 
background apart more. The method of combination can be 
obtained as follows: 
Fig. 1. The chart explains the process of the three-dimensional 
wavelet transform. 
 
 
 
,        (21) 
.        (22) 
 
By calculating the absolute value of every point on certain 
sub-bands and combining them, it signifies that enhancement is 
accomplished. FS is the salience map composed by three sub-
bands in Equations (11), (13) and (15) for static texts, and FM is 
the salience map with respect to the sub-bands in Equations (12), 
(14) and (16) for dynamic texts. Appropriate features are selected 
and extracted from the salience maps for next step – classification. 
Example salience maps for static and dynamic texts are shown in 
Fig. 3. 
 
    
(a) (b) 
Fig. 2. Salience maps for static and dynamic texts. (a) An input 
frame with static texts. (b) The salience map created from (a). 
 
3. FEATURE EXTRACTION 
 
Features are used to describe distinctive properties and particular 
characteristics for a certain class. Objects in the same class should 
have similar features. For example, there are often rich of 
horizontal, vertical and diagonal edges within text areas at the 
same time, and contrast is also higher. Although some complex 
backgrounds might act similarly, most of the backgrounds have 
smoother edges and only edges of one or two directions exist in 
background areas. The strength, contrast and direction of edges 
that we see provide us criteria to tell texts from backgrounds.  
We use a window ( 8 × 8 ) sliding from left to right, top to 
down with four pixels overlapped, with which more precise 
boundary of texts could be extracted. Ten features, one from mean 
),(),(),(),( 642 yxyxyxyx dddF S ++=
),(),(),(),( 753 yxyxyxyx dddF M ++=
0-7695-2521-0/06/$20.00 (c) 2006 IEEE
