 II
ABSTRACT 
 
 
Intrusion detection is the process of monitoring the events occurring in a 
computer system or network and analyzing them for signs of intrusions. There 
have been a lot of researches done to invent an ideal intrusion detection system 
(IDS) that is a system which can detect both known attacks and new attacks. 
Support vector machines (SVM) has been known as a promising methods for 
classification accuracy and its generalization ability. In this research, we design an 
SVM-based intrusion detection system which combines a hierarchical clustering 
algorithm, feature selection process and SVM classification techniques. The 
hierarchical clustering will provide SVM with a high quality training instances 
from the original training set. The feature selection process will eliminate 
unimportant features from the training set so that the SVM model can be used to 
classify the network traffic data accurately. Our experiments which are use KDD 
Cup 1999 data set show that our method can achieve high accuracy classification 
rate. 
Key word: intrusion detection system, Support vector machines, hierarchical 
clustering
  IV
CHAPTER V Conclusion ........................................................................................... 36 
REFERENCES............................................................................................................ 38 
 
 
 LIST OF TABLES 
 
 
Table 4.1.  Features Description of KDD Cup 1999 Data Set .............  28 
Table 4.2.  System Performance ..........................................................  30 
Table 4.3.  Number of Instances in Training Set .................................  31 
Table 4.4.  Important Features .............................................................  31 
Table 4.5.  System Performance Using All Features and Selected  
 Features ..............................................................................  32 
Table 4.6.  Performance Comparison...................................................  33 
Table 4.7.  System Performance on New Attack .................................  34 
Table 4.8.  Normal Connection and Snmpgetattack Similarity............  35 
 
 
 
 
 VI
 
 
Network Intrusion Detection System (NIDS) have become an important 
component in network security infrastructures. NIDS enables the network 
administrator to detect the policy violations. These policy violations range from 
external attackers trying to gain access to the system to insiders abusing their 
access. Detecting such violation is a necessary step before taking action such as 
blocking, reporting them (to their ISP or supervisor), or taking legal action against 
them. Detecting policy violations also allows network administrator to identify 
areas where their defenses need improvement, system vulnerability, or a user that 
needs further attention because of their behavior. 
I.1. Overview of Network Intrusion Detection System 
A network intrusion detection system is an intrusion detection system that 
tries to detect malicious activity such as denial of service attacks, port scans or 
even attempts to crack into computers by monitoring network traffic. The NIDS 
does this by reading all the incoming packets and trying to find suspicious 
patterns. If, for example, a large number of TCP connection requests to a very 
large number of different ports are observed, one could assume that there is 
someone committing a "port scan" at some of the computers in the network. It 
also, mostly, tries to detect incoming shell codes in the same manner that an 
ordinary intrusion detection systems does. 
A NIDS is not limited to inspecting incoming network traffic only. Often 
valuable information about an ongoing intrusion can be learned from outgoing or 
local traffic as well. Some attacks might even be staged from the inside of the 
monitored network or network segment, and are therefore not regarded as 
incoming traffic at all. Often, network intrusion detection systems are not a 
standalone system, but usually work with other systems as well. They can for 
example update some firewalls' blacklist with the IP addresses of computers used 
by (suspected) crackers. According to [9], intrusions are classified as six types.  
1. Attempted break-ins, which are detected by typical behavior profiles or 
violations of security constraints.  
2. Masquerade attacks, which are detected by atypical behavior profiles or 
violations of security constraints.  
 2
 
 
that it can be differentiated by their different model functions and representation, 
preferences criterion, and algorithms [1]. The main model we build through data 
mining process is to classify a packet from the network as normal, or malicious, or 
as a particular type of attack. 
Common representations of data mining techniques are clusters, rules, 
decision trees, linear and non-linear functions, instance-based examples and 
probability models [1]. SVM is just another technique that, like clustering, relies 
on mapping the network connections to a hyper-plane. SVM attempts to separate 
data into multiple classes (or two in the basic) through the usage of hyper-plane. A 
special property of SVM is that they simultaneously minimize the empirical 
classification error and maximize the geometric margin; hence they are also 
known as maximum margin classifiers. After we know all these techniques, our 
hope is to find the unseen “pattern” based on previously undetected intrusion in 
order to develop the new detection system.  
Several others research have shown that SVM is a promising method for 
data classification. The good result of SVMs in classifying data set is related to 
their solid mathematical foundations, with several properties [2]: 
• Margin maximization: The classification boundary functions of SVMs 
maximize the margin, which in machine learning theory, corresponds to 
maximizing the generalization performance.  
• Systematic nonlinear classification via kernel tricks: SVMs can handle 
nonlinear classification using kernel tricks efficiently, which implicitly 
transforms the input space into a high dimensional feature space. 
Although SVMs show good result for data classification, this success also 
comes with several drawbacks. SVM is not favorably used in large scale data 
mining because of their training complexity. The training complexity of the SVM 
is very dependent on the number of data in the training set. The larger the number 
of data in the training set, the higher the training complexity of the SVM.  It is a 
fact that many data mining applications involve huge number of data records 
(could be millions or even billions).  Based on this two facts, when we want use 
SVMs in such a huge training set, we have to tuned the system so that it could 
reduce the number of data record (but not as simple as cut the number of data or 
 4
 
 
training the SVM using all the centroid of the clusters as the training set. In this 
way, not only we can reduce the time needed to train the SVMs, but we can also 
get a better prediction because we “provide” SVMs with a “good” data records. 
 
I.3 Related Works 
There have been a lot of researches in Intrusion Detection System area 
using different method to classify a packet as an attack packet or legitimate packet. 
In our research we use KDD Cup 1999 data set as our benchmark data set hence 
we also want to compare our method with other method which is also use the 
same data set. 
A decision trees is one of method used in IDS. This method has been 
shown a good result in several researches. In KDD Cup 1999, the top three 
winner’s approaches are based on this method with some variation. Bagged 
boosting [15], which is use C5 decision trees, is the method used by the KDD’99 
winner. The runner up entry also use variant of decision trees, Kernel Miner [16] 
which is a data-mining tool based on building the optimal decision forest. 
There are also several IDS that based on fuzzy set theory. One of them is 
the research done by Witcha et al with their method Fuzzy Rough C-means 
(FRCM) [17]. FRCM integrates the advantage of fuzzy set theory and rough set 
theory that the improved algorithm to network intrusion detection. Another fuzzy 
approach which is use neuro-fuzzy classifiers [18] also has been used to analyze 
KDD Cup 1999 data set. In [18] the combination of neuro-fuzzy network, fuzzy 
inference approach, and genetic algorithms is used. 
Neural Network has also been used in several researches. In [19] the 
combination of Radial Basis Function (RBF) Neural Network and Multi Layer 
Perceptron (MLP) Neural Network used to classify an attack. In other research, 
Sabhnani and Serpen analyzed the performance of comprehensive set of pattern 
recognition and machine learning algorithms. Their research [20], which is 
outperform KDD Cup 1999 winner’s system, use different classifier for each 
specific attack category of KDD Cup 1999 data set. Another research applies 
 6
 
 
CHAPTER II 
HIERARCHICAL CLUSTERING 
AND SUPPORT VECTOR MACHINES 
 
 
This chapter will give some the detail discussion about the hierarchical 
clustering algorithm that we use in our system and also short discussion about the 
Support Vector Machine (SVM). The purpose of this chapter is to give some 
background theory explanation about what is SVM, how SVM solve classification 
problem and also how hierarchical clustering can be used to support SVM training. 
 
II.1 Hierarchical Clustering 
There are two approaches in data clustering techniques; they are 
probability-based approaches and distance-based approaches. The probability-
based approaches usually make the assumption that the probability distributions 
on separate attributes are statistically independent to each other. In reality, this 
assumption is not true because somehow the correlations between these attributes 
are exists and important. The probability representations make the updating and 
storing the clusters very expensive, moreover when the attributes have a large 
number of values because the complexities are also dependent on the number 
possible of values in each attribute. The other approaches, distance-based 
approaches, use the assumption that all data points are given in advance and can 
be scanned frequently. This method ignores the fact that not all data points in the 
data set are equally important for the clustering purposes. The methods that based 
on this approach are global or semi-global methods at the granularity of data 
points. It means for each cluster they inspect all data points or currently existing 
clusters, neglecting the distance. This approach cannot reach good quality 
clustering with only single scan of the data set, hence it’s time complexity is high. 
The hierarchical clustering algorithm we present and will be applied in our 
system was originally studied by T. Zhang et al. [3] and is called BIRCH 
 8
 
 
II.1.1 Clustering Feature 
 
First, we have to define the basic concepts. Given N d-dimensional data 
points in a cluster: {xi} where i = 1, 2, …, N, The centroid C and radius R of the 
clusters are defined as: 
 
  (1) 
 
 
     (2) 
 
 
where R is the average distance from all the members points to the centroid. 
The concept of the clustering feature (CF) tree is at the core of the 
BIRCH’s incremental clustering which is not expensive in computation. A 
Clustering Feature is a triplet which summarizes the information that CF tree 
maintains for a cluster. 
 
Definition 1 (Clustering Feature [3]) 
Given N d-dimensional data points in a cluster: {xi}, where i = 1, 2, …, N, 
the Clustering Feature (CF) vector of the cluster is define as triple: CF = (N, LS, 
SS) where N is the number of data points in the cluster, LS is the linear sum of the 
data points, i.e., , and SS is the square sum of the N data points, i.e., 
 
∑ =Ni iX1
2
1∑ =Ni iX
 
Theorem 1 (CF Additivity Theorem [3]) 
Assume that CF1 = (n1, LS1, SS1) and CF2 = (n2, LS2, SS2) are the CF 
vectors of two disjoint clusters. Then the CF Vector of the cluster that is formed 
by merging the two disjoint clusters is: 
 CF1 + CF2 = (n1 + n2, LS1 + LS2, SS1 + SS2) (3) 
 
For example, suppose there are three points (2, 3), (4, 5), (5, 6) in a cluster C1, 
then the clustering feature C1 is: 
N
x
C i i
N∑== 1
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −= ∑ =
N
Cx
R
N
i i1
2||||
 10
 
 
CF1 CF2 ---- CFB
CF11 ---- CF1B CFB1 ---- CFBB- - - -
CF111 ---- CF11B CFBB1 ---- CFBBB- - - -
 
Figure 2.1 
CF Tree with height h = 3 
 
A non-leaf node represents a cluster made up of all subclusters represented 
by its entries. Different from a non-leaf node, a leaf node contains only at most L 
entries, each of the form [CFi] where i = 1, 2, ..., L. A leaf node does not have 
pointer to other node. The tree is hierarchical in the following sense: the CF at any 
particular node contains information for all data points in that node’s sub-tree. A 
leaf node also represents a cluster made up of all sub-clusters represented by its 
entries, but all entries in a leaf node must satisfy a threshold requirement, with 
respect to a threshold T: the radius has to be less than T.  
The size of the tree is depends on parameter T. The larger T is, the smaller 
the tree is. Another threshold is the branching factor B which determine the 
memory page size such that a leaf or a non-leaf node fits in a page. Based on the 
explanation above now we know that CF tree is a compact representation of the 
data set because each entry in a leaf node is not a single data point but it is a 
cluster which absorbs many data points within radius of T or less. 
A CF tree is built up dynamically as new data objects are inserted. The 
insertion process is similar to that of a B+-tree use to guide a new insertion into 
the correct position for the sorting purposes. The insertion process is as follow. 
 
1. Identifying the appropriate leaf 
To identify the appropriate leaf to put the new entry, starting from the root 
it traverses the CF tree down (recursively) to the leaf level by choosing the 
child node whose centroid is closest to the new data object at each level. 
We will see the description of this process on Figure 2.2. Given CF tree 
 12
 
 
To explain how the leaf node does the split we can see from figure 2.3 (c). 
In this example we have a leaf node with three entries (figure 2.3 (c) upper 
part) and we want to add new entry CF4 there but we have to add new 
entry because the entry (lets say entry CF3) cannot absorb CF4 without 
violating T threshold. First we compute the distance between each entry in 
the leaf node (CF1 - CF2, CF1 – CF3, and CF2 – CF3) and choose the 
farthest pair. For example if the farthest pair is CF1 and CF3 then we split 
these two entries into two different node and redistribute the remaining 
entries (CF2 and CF4) based on their closeness to the current centroid of 
the entries. After finish with redistribution we compute a parent node with 
two entries CFA and CFB. 
 
 
CF1 CF2
CF1 CF2
CF1 CF2
CF1 CF2 CF3
CF1 CF2 CF3
CFA CFB
CF1 CF2 CF3 CF4
(a) (b) (c)
 
 
Figure 2.3 
Modifying the Leaf 
 
3. Modifying the path to the leaf 
After inserting new entry into a leaf, we must update the CF information 
for each non-leaf entry along the path to the root. In the absence of a node 
split, this process simply involves adding the CF vector to reflect the 
addition of new entry. If the previous step of modifying a leaf entry caused 
a node split, check the parent node for satisfaction of the branching factor 
constraint. If the parent node violates the B threshold as well, split it and 
recursively traverse back up to the root while performing the same checks. 
 14
 
 
Given a training set of instance-label pairs {(x, y)}, where y is the label 
of x, the optimal class boundary function g(x) is the one that gives the best 
generalization performance (performance on unseen sample rather than the 
training data). Classification performance on the training data is not regarded as a 
good evaluation measure for hypothesis, because the hypothesis is usually over 
fitted for that particular set of data through training. 
 
 
 
 
 
 
 
 
 
large margin
support vectorssmall margin
 
Figure 2.4 
Class Boundary and Margin 
 
SVM is a supervised learning method. SVM performs classification by 
constructing an N-dimensional hyperplane that optimally separates the data into 
different categories. On the basic classification SVM classify the data into two 
categories. SVM works by maximizing the generalization performance by 
maximizing the margin. The margin in SVM denotes the distance from the class 
boundary to the closest data points in the feature space.  
In SVM, the problem of computing a margin-maximizing boundary 
function is specified by the following quadratic programming (QP) problem: 
 minimize: )(2
1)(
1 1 1
j
l
i
l
i
l
j
ijijii xxkyyW ∑ ∑∑
= = =
+−= αααα  
 subject to: ,0: Ci i ≤≤∀ α  and  ∑
=
=
l
i
ii y
1
0α
 16
 
 
 18
computing an SVM boundary function is equal as finding data points whose αi’s 
is not zero. 
As mentioned earlier in Chapter I, although SVM is a promising method 
for data classification, SVM also has its problem. The main problem with SVM is 
that the training complexity is very dependent on the size of the data set. It is 
known to be at least quadratic to the number of data points. In our experiment, 
there will be more than four million data points involved, hence if we use SVM 
directly with all those data points, SVM will not be able to finish the training 
process or maybe it takes forever to finish the training. 
There have been many techniques developed to train SVM efficiently 
because the training time of the standard Quadratic Programming (QP) algorithm 
grows too fast. Since we know that the main problem in SVM is the size of data 
set, hence we can say that the most effective way to speed up the SVM training is 
to divide the original QP problem into smaller pieces. In our experiment 
techniques, the very large data set is condensed into statistical summaries of data 
groups. 
 
 4. Train four SVM separately, each SVM represent one type attack, using the 
centroid of all entries in leaf level of each tree. 
5. Combine four SVM models to build an intrusion classifier. 
 
In building an intrusion detection system, there are four types of attack that 
should be considered: Denial of Service (DoS) attack, User to Root (U2R) attack, 
Remote to User (R2L) attack, and Probing attack. Each type of attack has its own 
characteristic; hence it is very difficult to build a system that able to detect these 
four types of attack with only one model. It is easier to detect four types of attack 
using four models rather than using one model, hence we need to train four 
models independently to detect four types of attack. 
 
III.1 Data Transformation and Scaling 
In general data set, it is very common to have a non-numerical attribute 
or categorical attribute such as gender attribute, color attribute, etc. This kind of 
attribute must be transformed before applying in SVM. SVM requires that each 
data instance is represented as a vector of real numbers. Hence, for every non-
numerical attributes, we first have to convert them into numerical data. 
There are two ways to convert categorical attributes. The first method is 
by encoding using asymmetric binary variable for each possible value. For an 
attribute with a given value, the binary variable representing that state is set to 1, 
while the remaining binary variables are set to 0. For example, to encode attribute 
protocol_type with three possible values tcp, udp, and icmp, three new attributes 
are created for each possible value. For a record having protocol type tcp, the tcp 
attribute is set to 1 while the other two are set to 0.  
The second method is simply replacing the value of the categorical 
attributes with numeric value that represents each possible value. For 
protocol_type attributes, we simply change the value tcp with 0, udp with 1, and 
icmp with 2. This method is simpler because we do not create new attributes. 
According to [10], using the first method is more stable. But since in our 
data set there is a categorical attributes with large number values, this approach is 
 20
 cluster. We will discuss more about these parameters setting later in experiment 
part in Chapter IV. 
 
CF1
CF3
CFB
CF2
LS
N
SS
F1
F3
F41
F2
...
.
.
.
 
 
Figure 3.1 
Leaf Node of CF Tree for KDD Cup 1999 Data Set 
 
In KDD Cup 1999 data set, there are 41 features in each record. For our 
CF tree construction step, we do not apply feature selection hence we still use 41 
features for each record to build the CF tree. As mentioned earlier on previous 
chapter, each non leaf node in CF tree contains at most B entries of the form (CFi, 
childi) and each leaf node contains only the CF information without pointer to 
other node. On our implementation we represent a node (leaf node or non leaf 
node) as an object which contains an array of CF, actual size of the node (actual 
number of entry on that node), and pointer to child node (for non leaf node only). 
As an illustration of a leaf node, please refer to figure 3.1.  
Figure 3.1 contains three parts. The left part is the leaf node which 
contains at most B entry of CF. A single CF entry (middle part of figure 3.1) 
keeps information about the number of data points (N), the linear sum (LS) of the 
data points, and square sum (SS) of the data points. Basically the structure of LS 
and SS (right part of figure 3.1) are the same, in which for KDD Cup 1999 data 
set it contains the sum and square sum value of each features. 
After we complete building the CF tree for four types of attack and 
normal traffic, we can build our training set for SVM from those trees. The 
training set is collected from the entry of each tree at leaf level. The centroid of a 
single cluster at leaf level is considered as one entry in the training set. Consider a 
simple CF tree on Figure 3.2. The CF tree in Figure 3.2 has five leaf nodes (the 
 22
 is compared to the original classifier (based on all features) to decide the 
importance of the deleted feature. The methodology is summarized as follows:  
1. compose the training set and the testing set; 
for each feature do the following 
2. delete the feature from the (training and testing) data; 
3. use the resultant data set to train the classifier; 
4. analyze the performance of the classifier using the test set, in terms of 
the selected performance criteria; 
5. decide the importance of the feature according to the rules; 
 
III.3.2 Performance metrics 
To decide the importance of a feature from the data set, we decide two 
main performance criteria: overall accuracy and false positive rate. Each feature 
will be ranked as “important” or “insignificant” according to the following simple 
rules that are applied to the result of performance comparison of the original 41-
feature SVM: 
1. If accuracy decreases and false positive decreases, then the feature is 
important 
2. If accuracy decreases and false positive increases, then the feature is 
important 
3. If accuracy unchanges and false positive increase then the feature is 
important 
4. If accuracy unchanges and false positive decreases then the feature is 
insignificant 
5. If accuracy increases and false positive increases then the feature is 
important 
6. If accuracy increases and false positive decrease then the feature is 
insignificant 
7. If accuracy unchanges and false positive unchanges then the feature is 
insignificant 
 
 24
 CHAPTER IV 
EXPERIMENTAL RESULT 
 
 
Using the methods described in the previous chapter, we have done 
some experiments. The objective of these experiments is to evaluate the system 
accuracy then we can find out the completed accuracy calculation of the system. 
The rest of this chapter will describe the data set we use in our experiment, 
methodology of our experiment, experimental result, performance comparison 
with other research, and also evaluation of our method.  
 
IV.1. KDD Cup 1999 Data Set  
KDD Cup 1999 data [12] is the data set used for The Third International 
Knowledge Discovery and Data Mining Tools Competition, which was held in 
conjunction with KDD-99 The Fifth International Conference on Knowledge 
Discovery and Data Mining. This database contains a standard set of data to be 
audited, which includes a wide variety of intrusions simulated in a military 
network environment. 
The KDD Cup 1999 data set is originally created by The 1998 DARPA 
Intrusion Detection Evaluation Program which was prepared and managed by 
MIT Lincoln Labs. In this program, Lincoln Labs set up an environment to 
acquire nine weeks of raw TCP dump data for a local-area network (LAN) 
simulating a typical U.S. Air Force LAN.  They operated the LAN as if it were a 
true Air Force environment, but peppered it with multiple attacks. The objective 
was to survey and evaluate research in intrusion detection.  A standard set of data 
to be audited, which includes a wide variety of intrusions simulated in a military 
network environment, was provided.  
This data set provides us with train data and test data. The test data in 
this data set is not from the same probability distribution as the training data, and 
it includes specific attack types not in the training data which makes the task more 
difficult and realistic. Some intrusion experts believe that most novel attacks are 
 26
  
Table 4.1 
Features Description of KDD Cup 1999 Data Set 
 
No. Feature Name Description Type 
1 duration  length of the connection (second) continuous
2 protocol_type  type of protocol, e.g. tcp, udp, etc.  discrete 
3 service  network service on the destination, e.g., http, telnet, etc.  discrete 
4 flag  normal or error status of the connection  discrete  
5 src_bytes  number of data bytes from source to destination  continuous
6 dst_bytes  number of data bytes from destination to source  continuous
7 land  1 if connection is from/to the same host/port; 0 otherwise  discrete 
8 wrong_fragment  number of ``wrong'' fragments  continuous
9 urgent  number of urgent packets  continuous
10 hot  number of ``hot'' indicators continuous
11 num_failed_logins  number of failed login attempts  continuous
12 logged_in  1 if successfully logged in; 0 otherwise  discrete 
13 num_compromised  number of compromised condition continuous
14 root_shell  1 if root shell is obtained; 0 otherwise discrete 
15 su_attempted  1 if ``su root'' command attempted; 0 otherwise  discrete 
16 num_root  number of ``root'' accesses  continuous
17 num_file_creations  number of file creation operations  continuous
18 num_shells  number of shell prompts  continuous
19 num_access_files  number of operations on access control files  continuous
20 num_outbound_cmds number of outbound commands in an ftp session  continuous
21 is_host_login  1 if the login belongs to the ``hot'' list; 0 otherwise  discrete 
22 is_guest_login  1 if the login is a ``guest''login; 0 otherwise  discrete 
23 count  
number of connections to the same 
host as the current connection in 
the past two seconds  
continuous
24 srv_count  
number of connections to the same 
service as the current connection 
in the past two seconds  
continuous
25 serror_rate  % of connections that have ``SYN'' errors  continuous
 28
 use the weighting parameters for the training. For other SVM parameter selection 
we decide to use cross validation since the number of instance in the training set is 
not too large.  
 
IV.3. Experimental Result 
Using data set described in the previous section, we perform several 
experiments and analyze the classification result of the system. The best 
performance of our system is 95.71% accuracy with only 0.73% false positive. 
This result achieved with CF tree parameters: the threshold T = 0.2 and branching 
factor B = 100 and the feature selection process is applied. These parameters value 
were set based on the experiment. The number of instances is adjusted so that it is 
not too small (we will lose of information) or too large (the time needed for SVM 
to finish its training is too long).  Moreover we also want to maintain the balance 
between the number of instances for normal traffic and each type of attack traffic. 
The detail of the experiment result can be seen on table 4.2. 
 
Table 4.2 
System Performance 
 
Type of Traffic Correctly Detected Miss Detected Accuracy (%)
Normal 60,166 427 99.29 
Denial of Service 228,769 1,084 99.53 
Probe 4,064 102 97.55 
U2R 45 183 19.73 
R2L 4,664 11,525 28.81 
Overall 297,708 13,321 95.72 
 
The system suffers the most from both U2R and R2L attacks. This 
condition occurs because the number of occurrences of those two attacks is too 
small. Since the number of instances is too small, SVM does not have enough 
training data to finish its training. The composition of data of the training set 
based on their attack type for the original training set and the training set produced 
by CF tree with different parameters can be seen on table 4.3. 
 30
 SVM which use only the important features. The false positive value of the 
system which use 41 features is higher (0.91%) compared with system after 
feature selection process (0.73%). From this comparison we conclude that feature 
selection process which we applied in our system is working. 
 
Table 4.5 
System Performance Using All Features and Selected Features 
 
Traffic Type Using All Features 
After Feature 
Selection 
DoS  99.52 99.53 
Probe 89.05 97.55 
U2R 10.09 19.73 
R2L 24.26 28.81 
Normal 99.14 99.29 
Overall  97.31 97.72 
 
Although feature selection does not dramatically increase the system 
performance for all attack type, but at least the detection accuracy for Probe attack 
can be highly increased from 89% to more than 97%. We also notice that False 
Positive rate is decrease from 0.85% to 0.7%. Based on this result we can say that 
our Feature Selection is working. 
 
IV.4. Comparison with Other Intrusion Detection System 
There have been a lot of researches done in Intrusion Detection System 
that try to build a general intrusion detector that able to detect four types of 
network attacks. In this part we want to compare our system performance with 
other method. To make a comparison with other method we choose researches 
that use the same data set which also use KDD Cup 1999 data set.  
Compared to the KDD Cup 1999 winner’s system and several other 
researches, our system shows higher accuracy detection for several attacks type. 
Although our system shows higher false positive rate compared to the KDD’99 
winner’s system, but the difference is not so big and still can be tolerate.  
 32
 on the data set or our system’s fault? This section we will try to explain why this 
problem occurs. 
As we know that KDD Cup 1999 data set is very interesting data set 
because there are a lot of new type of attack instances which never appear in the 
training set. There are 18, 729 records of various new attacks appear only on the 
test data. This condition makes it harder to achieve higher classification accuracy. 
 
Table 4.7 
System Performance on New Attack 
 
Attack Type Attack Name # Occurrences on Test Data 
# Occurrences 
Detected 
apache2 794 536 
mailbomb 5,000 4,459 DoS 
processtable 759 578 
mscan 1,053 981 Probing 
saint 736 724 
httptunnel 158 15 
ps 16 3 
sqlattack 2 2 
U2R 
xterm 13 5 
sendmail 17 2 
named 17 3 
snmpgetattack 7,741 0 
snmpguess 2,406 1 
xlock 9 2 
xsnoop 4 0 
R2L 
worm 2 0 
Total  18, 729 7, 311  (39.04%) 
 
From table 4.7, our IDS detection rate for new attack is only 39.04%. 
The worst detection is on new R2L attacks. None of snmpgetattack, xsnoop, and 
xlock attack is detected and only one instance of snmpguess is detected as attack. 
It shows that our system cannot distinct a new R2L instances and normal 
instances. How can it be very difficult to distinct? As we can see in table 4.8 [14], 
 34
 CHAPTER V 
CONCLUSION 
 
 
This research proposes improvement for Intrusion Detection System 
using SVM. In this research combination between hierarchical clustering, SVM, 
and feature selection is used. Previously, a lot of research has been done in IDS by 
using SVM because SVM already well known for its generalization performance. 
The problem is the standard SVM cannot handle such huge data sets due to their 
high complexity computation. In this research we apply SVM after we apply 
clustering technique and feature selection to the original data set. 
From our experimental result we can conclude that the methods 
described in this research have a good result and effectively work, although for 
some minor part of the data set our method fails. Overall, our system can reach 
95.71% detection accuracy on KDD Cup 1999 Data Set with very low false 
positive rate (only 0.7%). Our experiment is performed in the whole KDD Cup 
1999 data set without selection. The method described in this research also can 
detect some of new attacks which only appear in the test data. 
However, our method is still suffers in identifying new U2R and R2L 
attacks due to the fact that those attack traffic pattern is very similar to the normal 
connection. This problem has been discussed in the previous chapter. At this point 
we are still trying to develop the improvement. There are at least two ideas for 
future our work for Intrusion Detection System: 
1. Since the main problem with new R2L attacks are the same pattern with 
normal connections, perhaps we can try to create new feature (instead of 
reduce it, although we still need to reduce unimportant features) which are 
combination of two or more available features so that using these feature we 
can distinct the R2L and normal connections. 
2. As we discussed in previous chapter that basically we can change the 
hierarchical clustering (in this case BIRCH algorithm) with any other 
clustering algorithm as long as it can cluster the original data into a “good” 
 36
 REFERENCES 
 
 
 
[1] Fayyad, U. M., G. Piatetsky-Saphiro, and P. Smyth. The KDD process for 
extracting useful knowledge from volumes of data. Communications of the 
ACM 39(11), 27-34, November 1996. 
[2] H. Yu, J. Yang, J. Han, and X. Li, Classifying Large Data Sets Using 
SVM with Hierarchical Clusters, In Proc 2003 Int. Conf. on Knowledge 
Discovery in Databases (KDD’03). 
[3] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: an efficient data 
clustering method for very large databases, In Proc. ACM SIGMOD Int. 
Conf. Management of Data (SIGMOD’96), pp. 103–114, 1996. 
[4] S. Guha, R. Rastogi, and K. Shim. Cure: An efficient clustering algorithm 
for large databases. In Proc. 1998 ACM-SIGMOD Int. Conf. Management 
of Data (SIGMOD’98), pages 73–84, Seattle, WA, June 1998. 
[5] S. Guha, R. Rastogi, and K. Shim. ROCK: A robust clustering algorithm 
for categorical attributes. In Proc. 1999 Int. Conf. Data Engineering 
(ICDE’99), pages 512–521, Sydney, Australia, March 1999. 
[6] G. Karypis, E.-H. Han, and V. Kumar. CHAMELEON: A hierarchical 
clustering algorithm using dynamic modeling. COMPUTER, 32:68–75, 
1999. 
[7] Vapnik, V. The Nature of Statistical Learning Theory. New York, NY: 
Springer-Verlag, 1995. 
[8] Boser, B. E., I. Guyon, and V. Vapnik. A training algorithm for optimal 
margin classifiers. In Proceedings of the Fifth Annual Workshop on 
Computational Learning Theory, pp. 144-152. ACM Press, 1992. 
[9] A. Sundaram, “An introduction to intrusion detection”. ACM Cross Roads, 
vol. 2, no. 4, Apr. 1996. 
[10] Hsu, C.-W., Chang, C.-C., and C.-J. Lin. A Practical Guide to Support 
Vector Classification, 2007. 
[11] http://www.csie.ntu.edu.tw/~cjlin/libsvm 
 38
出席國際會議報告
會議名稱：The 13th IEEE Pacific Rim International Symposium
on Dependable Computing (PRDC'07)
會議時間： Dec. 17~19, 2007
報告單位： 台灣科技大學
報告人： 洪西進 教授
