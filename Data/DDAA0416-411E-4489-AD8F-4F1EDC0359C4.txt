 2
一個前景物件影像 F、一個背景影像 B 和一
個 alpha matte 的組合，於是就有了式子(1)
的 matting 數學定義。 
( )BFI αα −+= 1  (1) 
在影像物件的分割上，因為物件的邊緣
部分有光線的折射的關係，所以造成影像邊
緣的像素同時接受到前景與背景的光線，故
使得像素因為光線的混合，之間存在了一個
比例(α 值)；因此，一般的影像分割方法是
無法處理邊緣像素受到光線混合影響的問
題，直到 matting 技術的提出後才克服了此
一問題。 
對於給定一張影像而必需在前景 F、背
景 B 及α 值均未知的條件下去求解所有的
像素點，這是一個 under-constrained 問題，
因此常用的解決方法是利用人工給定額外
資訊的方式來求解 alpha matte，例如利用
trimap 來標示出已知的前景、已知的背景及
未知區域 (unknown regions)；或是利用
scribbles 來標示出前景區域及背景區域。 
近十年來陸續一些新的影像去背法被
提出，大大提升了影像去背法的闗注[2-7]。
這些方法大致上可粗分成三大類[2]，分別為
植 基 於 取 樣 的 方 法 (Sampling-based 
approaches) 、 植 基 於 增 殖 的 方 法
(Propagation-based approaches)和利用額外
資訊的方法(Matting with extra information)。 
影像去背法採用 trimaps 來標示出前景
區域、背景區域及未知區域，雖然可以獲得
很好的 alpha matte；但是如何畫出很好或很
精準的 trimaps 對使用者而言是一大挑戰，
再者對於有些很複雜的前景要得到其很好
的 trimaps 是不大可能的。因此，採用
scribbles來標示出大略的前景區域及背景區
域是一較可行的方式。但是無論是影像去背
法採用 trimaps 或 scribbles 來標示出前景區
域及背景區域，都不能達到自動化影像去背
的目的。Spectral matting [6]是一創新的方
法，該方法可以達到自動化影像去背的目
的，然而其所獲得的去背結果卻是準確率不
高。所以，本計畫提出了一改良式的 Spectral 
matting 法，該方法除可達到自動化影像去
背外，亦大大提升了 Spectral matting 法求算
透明圖層遮罩的準確度與執行時間。 
 
三、研究方法 
在 Spectral matting 方法中，假設影像中
每一像素是由 K 個影像圖層所組合形成
的，如式子(2)所示。而式子中 kα 即為影像
的 matting component ， 這 些 matting 
components 是非負值的且對每一像素點其
總和為1。且matting components是由matting 
Laplacian [5]最小特徵向量(eigenvectors)所
推導獲得的。 
∑
=
=
K
k
k
i
k
ii FI
1
α  (2) 
假設 b 為 K 維的二元向量，代表著前景
區域 components；也就是若 kα 為前景區域
component，則 1=kb 。所以完整的 alpha 
matte 便可由式子(3)求得。 
∑=
k
kkb αα  (3) 
Spectral matting 方法是先對影像中所
有 的 像 素 進 行 分 群 ， 然 後 找 到 影 像
components，再針對每個 component 利用
matting Laplacian 估算出其個別的 matting 
component ， 最 後 再 由 這 些 matting 
components 找出最佳組合(matte cost)以獲
得影像去背所需的 alpha matte。 
至於 matte cost 的求法如下；對於每一
種 matting components 的組合，其 matting 
components 和 matting Laplacian 的
correlations 可先計算出來並儲存在一 KK ×
矩陣Φ中，而矩陣Φ中的元素定義如式子
(4)。式子(5)即為 matte cost，其中 b 是標示
所選的 matting components 的 K 維二元向
量。 
lTk Llk αα )(),( =Φ  (4) 
bbJ TΦ=)(α  (5) 
在本計畫所提出的方法中，為了提升
Spectral matting 法求算透明圖層遮罩的準
確度，因此提出了植基於樣板的元件分類法
去獲得可靠的前景先件和背景元件。首先我
們先選取原始影像中邊緣區域的取樣點加
以分析 RGB 成份，然後再利用式子(6)來決
定應該採用 RGB 那一個色相(hue)的色板作
為背景色板。 
然後再分別求算各個元件的平均色
相，並利用式子(7)和(8)來求算候選前景/背
景元件。 
 4
 
(a) 
 
(b) 
圖 6. Wolf 影像去背模擬結果。 
 
 
(a) 
 
(b) 
圖 7. Boy 影像去背模擬結果。 
 
 
(a) 
 
(b) 
圖 8. Fox 影像去背模擬結果。 
 
另外，在時間計算成本上，本計畫所提
出的方法亦有效地降低了 Spectral matting
法的計算成本，如表 1 所示。 
 
表 1. 時間計算成本效能評比。 
 Spectral 
matting 
Modified 
spectral 
matting 
Reduction rate of 
computational 
cost 
Woma
n 
18.39 s 18.13 s 1.41 % 
Wolf 59.60 s 52.54s 11.85 % 
Boy 8.43 s 6.24 s 25.92 % 
Fox 8.43 s 5.43 s 35.54 % 
 
五、計畫成果自評 
本研究均依原計畫內容執行並達成預
期目標，研究成果具有高度的學術及應用價
值，本研究的成果目前已發表了兩篇論文在
重要的國際研討會中，詳細資料請參考附錄
論文[8, 9]；計畫結果亦已整理出一篇期刊論
文並已投稿至 SCI 國際期刊 (Journal of 
Visual Communcation &  Image 
Representation)。 
 
參考文獻 
[1] T. Porter and T. Duff , “Compositing 
digital images”, Proceedings of 
SIGGRAPH, pp. 253-259, 1984. 
[2] J. Wang, and M.F. “Cohen, Image and 
video matting: A survey, Foundations and 
Trends in Computer Graphics and Vision”, 
Vol. 3, No. 2, pp. 1-78, 2007. 
[3] J. Wang, and M.F. Cohen, “Optimized 
color sampling for robust matting”, in: 
Proceedings of IEEE Computer Vision and 
Pattern Recognition, pp. 1-8, 2007. 
[4] Y. Guan, W. Chen, X. Liang, Z. Ding, and 
Q. Peng, “Easy matting: A stroke based 
approach for continuous image matting”, 
Computer Graphics Forum, Vol. 25, No. 3, 
pp. 567-576, 2008. 
[5] A. Levin, D. Lischinski, and Y. Weiss, “A 
closed-form solution to natural image 
matting”, IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 
30, No. 2, pp. 228-242, 2008. 
[6] A. Levin, A. Rav-Acha, and D. Lischinski, 
“Spectral matting”, IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
Vol. 30, No. 10, pp. 1699-1712, 2008. 
[7] J. Sun, Y. Li, S.B. Kang, and H.Y. Shum, 
“Flash matting”, in: Proceedings of ACM 
SIGGRAPH, Vol. 25, No. 3, pp. 772-778, 
2006. 
[8] W.C. Hu, D.Y. Huang, C.Y. Yang, J.J. Jhu, 
and C.P. Lin, “Automatic and accurate 
image matting”, in: Proceedings of the 2nd 
International Conference on 
Computational Collective 
Intelligence--Technology and Applications, 
Vol. 3, pp. 11-20, 2010. 
[9] W.C. Hu, D.Y. Huang, C.Y. Yang, and J.F. 
Hsu, “Automatic video object 
segmentation with opacity estimate”, in: 
Proceedings of the 4th International 
Conference on Genetic and Evolutionary 
Computing, pp. 683-686, 2010. 
 
附錄 
(請見下頁) 
12 W.-C. Hu et al. 
Although image matting has been studied for more than two decades, image 
matting has received increasing attention in the last decade and many methods have 
been proposed for image matting [2], especially in the last five years, such as robust 
matting [3], easy matting [4], closed-form matting [5], and flash matting [6]. 
Generally, image matting can be roughly classified into three types [2, 3]: sampling-
based methods, propagation-based methods, and matting with extra information. 
Sampling-based methods assume that an unknown pixel can be estimated as the 
foreground and background colors based on examining nearby pixels that have been 
specified by the user as foreground or background. Next, these color samples are used 
to directly estimate the alpha value. Propagation-based methods assume that 
foreground and background colors are locally smooth. Next, the foreground and 
background colors are systematically eliminated from the optimization process to 
obtain the alpha matte. Matting with extra information is designed to provide 
additional information or constraints to matting algorithms in order to obtain better 
results on natural images. 
In contrast to matting with extra information, sampling-based methods and 
propagation-based methods are suitable for video matting. However, almost 
sampling-based methods and propagation-based methods are the supervised matting 
(non-automatic matting). Therefore, video matting based on these supervised methods 
is a time-consuming and troublesome process for the user. Unsupervised (automatic) 
image matting is thus an important and interesting issue. It is a big challenging task 
for unsupervised image matting to solve the alpha matte from the highly under-
constrained problem without any user input. 
The unsupervised image matting (spectral matting) was first proposed by Levin et 
al. [7], and spectral matting is the only unsupervised method in the current proposed 
image matting algorithms. Spectral matting extends the ideas of spectral segmentation 
[8, 9]. In spectral matting, a set of fundamental fuzzy matting components are 
automatically extracted from an input image based on analyzing the smallest 
eigenvectors of a suitably defined Laplacian matrix (matting Laplacian [5]). The 
smallest eigenvectors of the matting Laplacian span the individual matting 
components of the image, thus recovering the matting components of the image is 
equivalent to finding a linear transformation of the eigenvectors. These matting 
components are then combined to form the complete alpha matte. However, using 
spectral matting without user guides, the accuracy is usually low. Therefore, it is a 
challenging task for unsupervised image matting to obtain an accurate alpha matte. 
In this paper, the modified spectral matting is proposed to obtain automatic and 
accurate alpha matte. The proposed method extends the idea of spectral matting. In 
the modified spectral matting, the palette-based component classification is proposed 
to find the components of foreground, background and unknown regions. Next, the 
corresponding matting components of foreground components, background 
components, and components of unknown region are obtained via a linear 
transformation of the smallest eigenvectors of the matting Laplacian matrix. Finally, 
only matting components of foreground and unknown regions are combined to form 
the complete alpha matte based on minimizing the matte cost. Therefore, the accuracy 
of obtained alpha matte is greatly increased. Experimental results show that the 
proposed method can obtain the high-quality alpha matte for natural images without 
any user input. 
14 W.-C. Hu et al. 
Fig. 1 is the result of distinct component detection using spectral segmentation 
with the k-means algorithm, where Fig. 1(a) is the input image, Fig. 1(b) is the 
clustering result, and Fig. 1(c) is the result of distinct components. 
    
(a) (b) 
 
(c) 
Fig. 1. Distinct component detection. (a) Input image; (b) clustering result; (c) distinct 
components. 
In order to greatly increase the accuracy of alpha matte, the palette-based 
component classification is proposed to obtain the reliable foreground and 
background components. The palette-based component classification is described as 
follows. First, the boundary pixels (with a width of one pixel) of the input image are 
extracted to analysis components of RGB. If the R component is the maximum, then 
suppose the background is the red palette. Green and blue palettes are with the same 
assumption. Next, the RGB color space of distinct component is transformed to the 
HSV color space with Eq. (5), where ),,max( BGRMax = , ),,min( BGRMin = , and 
HSV color space is shown in Fig. 2. 
⎪⎩
⎪⎨
⎧
=−−+
=−−+
=−−
=
MaxBMinMaxGR
MaxGMinMaxRB
MaxRMinMaxBG
H
  if  ,)(60*)(240
 if  ,)(60*)(180
 if  ,)(60*)(
                       
(5) 
16 W.-C. Hu et al. 
 
Fig. 3. Component classification 
kCTk mEE ~~=α  (8) 
⎩⎨
⎧
∉
∈
=
Ci
Ci
m
C
i
 ,    0
 ,    1
 (9) 
Compute matting components by minimizing an energy function defined as Eq. (10) 
subject to ∑ =k ki 1α . This goal is to find a set of K linear combination vectors ky . 
The above energy function is minimized optimally using Newton’s method, where γ  
is chosen to be 9.0  for a robust measure. Fig. 4 is the matting components of the 
resulting components. 
kkk
i
ki
k
i yE
~
    where,1
 ,
=−+∑ ααα γγ  (10) 
 
Fig. 4. Matting components of the resulting components 
Finally, only matting components of the foreground and unknown region are 
combined to form the complete alpha matte based on minimizing the matte cost, as 
18 W.-C. Hu et al. 
        
(a)                                     (b) 
 
(c) 
 
(d) 
Fig. 6. Image matting of a face image. (a) Original image; (b) clustering result; (c) alpha matte 
detection using the spectral matting; (d) alpha matte detection using the proposed method. 
      
(a)                           (b) 
Fig. 7. Image matting of a wolf image. (a) Original image; (b) clustering result; (c) alpha matte 
detection using the spectral matting; (d) alpha matte detection using the proposed method. 
20 W.-C. Hu et al. 
4   Conclusion 
The modified spectral matting was proposed to obtain automatic and accurate image 
matting. In the proposed method, the palette-based component classification is 
proposed to obtain the reliable foreground and background components. Using the 
proposed palette-based component classification, the distinct components are further 
classified as the components of foreground, background and unknown regions. Next, 
the corresponding matting components of foreground components, background 
components, and components of unknown region are obtained via a linear 
transformation of the smallest eigenvectors of the matting Laplacian matrix. Finally, 
only matting components of foreground and unknown regions are combined to form 
the complete alpha matte based on minimizing the matte cost. Therefore, the accuracy 
of the alpha matte is greatly increased. Experimental results show that the proposed 
method can obtain the high-quality alpha matte for natural images without any user 
input, and the proposed method has better performance than the spectral matting. 
 
Acknowledgments. This paper has been supported by the National Science Council, 
Taiwan, under grant no. NSC99-2221-E-346-007. 
References 
1. Porter, T., Duff, T.: Compositing Digital Images. In: Proceedings of ACM SIGGRAPH, 
vol. 18(3), pp. 253–259 (1984) 
2. Wang, J., Cohen, M.F.: Image and Video Matting: A Survey. Foundations and Trends in 
Computer Graphics and Vision 3(2), 1–78 (2007) 
3. Wang, J., Cohen, M.F.: Optimized Color Sampling for Robust Matting. In: Proceedings of 
IEEE Computer Vision and Pattern Recognition, pp. 1–8 (2007) 
4. Guan, Y., Chen, W., Liang, X., Ding, Z., Peng, Q.: Easy Matting: A Stroke based Approach 
for Continuous Image Matting. Computer Graphics Forum 25(3), 567–576 (2008) 
5. Levin, A., Lischinski, D., Weiss, Y.: A Closed-Form Solution to Natural Image Matting. 
IEEE Transactions on Pattern Analysis and Machine Intelligence 30(2), 1–15 (2008) 
6. Sun, J., Li, Y., Kang, S.B., Shum, H.Y.: Flash Matting. In: Proceedings of ACM 
SIGGRAPH, vol. 25(3), pp. 772–778 (2006) 
7. Levin, A., Rav-Acha, A., Lischinski, D.: Spectral Matting. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 30(10), 1–14 (2008) 
8. Weiss, Y.: Segmentation using Eigenvectors: A Unifying View. In: Proceedings of 
International Conference on Computer Vision, pp. 975–982 (1999) 
9. Yu, S.X., Shi, J.: Multiclass Spectral Clustering. In: Proceedings of International 
Conference on Computer Vision, pp. 313–319 (2003) 
10. Ng, A., Jordan, M., Weiss, Y.: On Spectral Clustering: Analysis and an Algorithm. In: 
Proceedings of Advances in Neural Information Processing System (2001) 
for complex images or color-like foreground and background. 
The mistake image segmentation makes the errors of the 
alpha mattes and the foreground objects. 
In this paper, we propose the video object segmentation 
with opacity estimate to obtain good alpha mattes and the 
foreground objects. In the proposed method, the trimap is 
automatically generated from the result obtained using video 
object segmentation based on motion detection without 
background construction [5]. Next, closed-form matting [11] 
is used with the automatically generated trimaps to yield the 
alpha mattes and the foreground objects. Experimental 
results show that the proposed method has good performance 
in video object segmentation with opacity estimate. 
The rest of this paper is organized as follows. The 
proposed video object segmentation with opacity estimate is 
described in Section II. Section III presents experimental 
results and their evaluations. Finally, the conclusion is given 
in Section IV. 
II. EXTRACTING FOREGROUND OBJECT WITH ALPHA 
MATTE 
In this session, we describe two main components of the 
proposed video object segmentation with opacity estimate. 
Section II-A gives a review of video object segmentation 
based on motion detection without background construction. 
Section II-B describes the automatic generation of the trimap 
and closed-form matting. Closed-form matting is used with 
the automatically generated trimaps to yield the alpha mattes 
and the foreground objects. 
A. Video Object Segmentation based on Motion Detection 
without Background Construction 
The block diagram of the video object segmentation 
based on motion detection without background construction 
[5] is illustrated in Fig. 1.  
 
 
Figure 1. Block diagram of the video object segmentation based on motion 
detection without background construction [5]. 
The motion information of the video object is first 
detected based on the angle-module rule [6] using (1)-(4). In 
the motion detection, the image ),( yxI  and the image 
),( yxB in the angle-module rule are replaced by the thk  
frame and the th)1( −k  frame. Furthermore, 1),( =yxFt  
implies that this pixel is the motion pixel. 
⎪⎩
⎪⎨⎧ >>=
                                             else0
)(or  )( if1),( 0mod0 hx,yΔx,yΘyxF
t
t
t
ω  (1) 
)(absmod tt
tΔ BΙ −=  (2) 
ε+⋅
⋅
≈
−
tt
tt
tΘ BI
BI1cos  (3) 
b
t
b
t
g
t
g
t
r
t
r
ttt BIBIBIBI ∗⋅+∗⋅+∗⋅=⋅  (4) 
The Sobel operator is used to approximate the magnitude 
of the gradient. In gradient-variation detection, the 
magnitudes of the gradients of the thk  frame and th)1( −k  
frame are evaluated. White pixels satisfy (5), where 
),( yxGk  and ),(1 yxGk−  are the magnitudes of the gradients 
of point ),( yx  in the thk  frame and th)1( −k  frame, 
respectively. 
Gkk TyxGyxG ≥− − ),(),( 1  (5) 
The coarse moving object mask is obtained using 
EMC III ∪= , where the operator ∪  is the Boolean operator 
“OR”; CI , MI , and GI  are the images of coarse moving 
object mask, motion detection, and gradient-variation 
detection, respectively. Next, the still regions in the moving 
object are not detected using motion detection; still regions 
thus appear in the coarse moving object mask as holes. A 
compensation scheme is used to detect still regions in a 
moving object.  
 
 
                  (a)                                      (b)                                    (c) 
 
                  (d)                                      (e)                                    (f) 
   
                                       (g)                                      (h) 
Figure 2. Result obtained using the video object segmentation: (a) the thk  
frame, (b) motion detection, (c) gradient-variation detection, (d) coarse 
moving object mask, (e) compensation for still regions, (f) fine moving 
object mask, (g) final moving object mask, (h) object segmentation. 
684
was evaluated. Fig. 5 (a) shows the image composition with 
a new background and the results obtained using video 
object segmentation based on motion detection without 
background construction [5]. Fig. 5 (b) shows the image 
composition with a new background and the results obtained 
using the proposed method. Image composition with results 
obtained using the proposed method can obtain more 
realistic image composition. Therefore, the proposed method 
is very useful for video editing. 
 
     
(a) 
     
(b) 
Figure 4. Comparison of video object segmentation with opacity estimate: 
(a) results obtained using method suggested by Gupta et al. [9], (d) results 
obtained using the proposed method. 
     
(a) 
     
(b) 
Figure 5. Comparison of image composition: (a) with results obtained 
using the video object segmentation based on motion detection without 
background construction [5], (b) with results obtained using the proposed 
method. 
IV. CONCLUSION 
In this paper, video object segmentation with opacity 
estimate is proposed. In the proposed method, the trimap is 
automatically generated from the result obtained using video 
object segmentation based on motion detection without 
background construction. Closed-form matting is then used 
with the automatically generated trimaps to yield the alpha 
mattes and the foreground objects. In contrast to the methods 
of video object segmentation using binary segmentation, the 
proposed method can obtain good object segmentation for 
objects with translucency to keep the important features of 
the foreground objects. 
Experimental results show that the proposed video object 
segmentation outperforms the method suggested by Gupta et 
al. Furthermore, image composition with results obtained 
using the proposed method can obtain more realistic image 
composition. Therefore, the proposed video object 
segmentation is very useful for the interesting applications of 
video editing. 
ACKNOWLEDGMENT 
This paper has been supported by the National Science 
Council, Taiwan, under grant no. NSC99-2221-E-346-007.  
REFERENCES 
[1] C. R. Huang, H. P. Lee, and  C. S. Chen, “Shot Change Detection via 
Local Keypoint Matching,” IEEE Transactions on Multimedia, vol. 
10, no. 6, pp. 1097-1108, 2008. 
[2] B. Sugandi, H. Kim, J. K. Tan and S. Ishikawa, “Real Time Tracking 
and Identification of Moving Persons by using a Camera in Outdoor 
Environment,” International Journal of Innovative Computing, 
Information and Control, vol.5, no.5, pp.1179-1188, 2009. 
[3] S. Zhu and Y. Liu, “Automatic Video Segmentation for Advanced 
Story Retrieval,” International Journal of Computer Sciences and 
Engineering Systems, vol.2, no.3, pp.157-166, 2008. 
[4] G. Zhang, and W. Zhu, “Automatic Video Object Segmentation by 
Integrating Object Registration and Background Constructing 
Technology,” Proceedings of 2006 International Conference on 
Communications, Circuits and Systems, pp. 437-441, Guilin, China, 
2006. 
[5] W. C. Hu, “Real-time On-line Video Object Segmentation based on 
Motion Detection without Background Construction,” International 
Journal of Innovative Computing, Information and Control, vol.7, 
no.4, 2011. 
[6] E. J. Carmona, J. Martínez-Cantos, and J. Mira, “A New Video 
Segmentation Method of Moving Objects based on Blob-level 
Knowledge,” Pattern Recognition Letters, vol. 29, pp. 272-285, 2008. 
[7] T. Y. Chen, T. H. Chen, D. J. Wang and Y. C. Chiou, “Real-time 
Video Object Segmentation Algorithm based on Change Detection 
and Background Updating,” International Journal of Innovative 
Computing, Information and Control, vol.5, no.7, pp.1797-1810, 
2009. 
[8] T. Porter and T. Duff, “Compositing Digital Images,” Proceedings of 
ACM SIGGRAPH, vol. 18, no. 3, pp. 253-259, 1984. 
[9] A. Gupta, S. Mangal, P. Nagori, A. Jain, and V. Khandelwal, “Video 
Matting by Automatic Scribbling using Quadra Directional filling of 
Segmented Frames,” Porceedings of 2nd IEEE International 
Conference on Computer Science and Information Technology, pp. 
336-340, Beijing, China, 2009. 
[10] J. Shi and J. Malik, “Normalized Cuts and Image Segmentation,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 
22, no. 8, pp. 888-905, 2000. 
[11] A. Levin, D. Lischinski, and Y. Weiss, “A Closed-Form Solution to 
Natural Image Matting,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 30, no. 2, pp. 1-15, 2008. 
 
686
 2
David Powers, Australia)。考量個人研究領域與興趣，故只參與了 Wen-Lian 
Hsu 教授及 David Powers 教授的演講場次，兩位教授深入淺出的講題及其
研究方法，以及會中多位與會學者的提問，著實讓我受益良多。 
本人所發表的論文則是被安排在 12/14 日(第二天)下午 14:00~15:40 進
行的 Session B05: Intelligent Video Processing 的第 1 篇論文及第 2 篇論文發
表，本人為該場次的主持人。在該場次中共有 8 篇論文發表，會中討論相
當熱烈，發表過程順利，也同時和與會學者交換研究心得與相關問題討論，
是一個成功的論文發表會。 
 
二、 與會心得 
此次國際研討會是在大陸深圳舉行，除了大陸及台灣的學者外，亦有
不少日本、澳洲、美國、挪威…等地的學者與會。除個人論文發表場次外，
亦參與了其他場次的論文發表，會中除了與論文作者有交談、互換研究心
得。本次出席會議的台灣學者(含研究生)都能如期出席發表論文口頭報告，
是一次成功的學術交流。透過會後休息時間，和與會的學者進行有關學術
研究的深度討論，此次研討會亦有多位國內學者參加，如高雄應用大學潘
正祥教授、中央大學蔡宗漢教授、朝陽科大李金鳳教授、大葉大學黃登淵
教授…等等；參與此次研討會對於個人來說，將會對於個人在未來的研究
上助益良多。 
 The Fourth International Conference on  
Genetic and Evolutionary Computing 
 
December 13-15, 2010, Shenzhen, China 
http://bit.kuas.edu.tw/~icgec10 
 
Dear Prof./Dr./Ms./Mr. Wu-Chih Hu, 
Thank you for your submission to the Fourth International Conference on Genetic and Evolutionary 
Computing (ICGEC-2010), to be held on December 13-15, 2010, in Shenzhen, China. We are pleased 
to inform you that your paper 
ID No.: ICGEC-2010-IS23-01 
Title: Automatic Video Object Segmentation with Opacity Estimate 
Author(s): Wu-Chih Hu, Deng-Yuan Huang, Ching-Yu Yang, and Jung-Fu Hsu 
has been accepted for presentation in ICGEC-2010. Your paper will be published in the conference 
proceeding with the Conference Publishing Services of the IEEE Computer Society. Please do take the 
comments and suggestions of the reviewers into account in the revision to further improve the quality 
of your paper. Please refer to http://bit.kuas.edu.tw/~icgec10 for further information regarding the 
conference registration and to the online Author Guide at http://bit.kuas.edu.tw/~icgec10 for detailed 
procedures in the preparation of your camera-ready copy and copyright release form. Both deadlines 
are September 10, 2010. 
We are looking forward to meeting you in Shenzhen. Further information on ICGEC-2010 can be 
obtained from the conference web sites: http://bit.kuas.edu.tw/~icgec10 
 
Sincerely Yours, 
 
 
Jeng-Shyang Pan, Conference Chair 
Shenzhen Graduate School, Harbin Institute of Technology, China 
 
Xia Li, Program Committee Chair 
Shenzhen University, China 
for complex images or color-like foreground and background. 
The mistake image segmentation makes the errors of the 
alpha mattes and the foreground objects. 
In this paper, we propose the video object segmentation 
with opacity estimate to obtain good alpha mattes and the 
foreground objects. In the proposed method, the trimap is 
automatically generated from the result obtained using video 
object segmentation based on motion detection without 
background construction [5]. Next, closed-form matting [11] 
is used with the automatically generated trimaps to yield the 
alpha mattes and the foreground objects. Experimental 
results show that the proposed method has good performance 
in video object segmentation with opacity estimate. 
The rest of this paper is organized as follows. The 
proposed video object segmentation with opacity estimate is 
described in Section II. Section III presents experimental 
results and their evaluations. Finally, the conclusion is given 
in Section IV. 
II. EXTRACTING FOREGROUND OBJECT WITH ALPHA 
MATTE 
In this session, we describe two main components of the 
proposed video object segmentation with opacity estimate. 
Section II-A gives a review of video object segmentation 
based on motion detection without background construction. 
Section II-B describes the automatic generation of the trimap 
and closed-form matting. Closed-form matting is used with 
the automatically generated trimaps to yield the alpha mattes 
and the foreground objects. 
A. Video Object Segmentation based on Motion Detection 
without Background Construction 
The block diagram of the video object segmentation 
based on motion detection without background construction 
[5] is illustrated in Fig. 1.  
 
 
Figure 1. Block diagram of the video object segmentation based on motion 
detection without background construction [5]. 
The motion information of the video object is first 
detected based on the angle-module rule [6] using (1)-(4). In 
the motion detection, the image ),( yxI  and the image 
),( yxB in the angle-module rule are replaced by the thk  
frame and the th)1( −k  frame. Furthermore, 1),( =yxFt  
implies that this pixel is the motion pixel. 
⎪⎩
⎪⎨⎧ >>=
                                             else0
)(or  )( if1),( 0mod0 hx,yΔx,yΘyxF
t
t
t
ω  (1) 
)(absmod tt
tΔ BΙ −=  (2) 
ε+⋅
⋅
≈
−
tt
tt
tΘ BI
BI1cos  (3) 
b
t
b
t
g
t
g
t
r
t
r
ttt BIBIBIBI ∗⋅+∗⋅+∗⋅=⋅  (4) 
The Sobel operator is used to approximate the magnitude 
of the gradient. In gradient-variation detection, the 
magnitudes of the gradients of the thk  frame and th)1( −k  
frame are evaluated. White pixels satisfy (5), where 
),( yxGk  and ),(1 yxGk−  are the magnitudes of the gradients 
of point ),( yx  in the thk  frame and th)1( −k  frame, 
respectively. 
Gkk TyxGyxG ≥− − ),(),( 1  (5) 
The coarse moving object mask is obtained using 
EMC III ∪= , where the operator ∪  is the Boolean operator 
“OR”; CI , MI , and GI  are the images of coarse moving 
object mask, motion detection, and gradient-variation 
detection, respectively. Next, the still regions in the moving 
object are not detected using motion detection; still regions 
thus appear in the coarse moving object mask as holes. A 
compensation scheme is used to detect still regions in a 
moving object.  
 
 
                  (a)                                      (b)                                    (c) 
 
                  (d)                                      (e)                                    (f) 
   
                                       (g)                                      (h) 
Figure 2. Result obtained using the video object segmentation: (a) the thk  
frame, (b) motion detection, (c) gradient-variation detection, (d) coarse 
moving object mask, (e) compensation for still regions, (f) fine moving 
object mask, (g) final moving object mask, (h) object segmentation. 
684
was evaluated. Fig. 5 (a) shows the image composition with 
a new background and the results obtained using video 
object segmentation based on motion detection without 
background construction [5]. Fig. 5 (b) shows the image 
composition with a new background and the results obtained 
using the proposed method. Image composition with results 
obtained using the proposed method can obtain more 
realistic image composition. Therefore, the proposed method 
is very useful for video editing. 
 
     
(a) 
     
(b) 
Figure 4. Comparison of video object segmentation with opacity estimate: 
(a) results obtained using method suggested by Gupta et al. [9], (d) results 
obtained using the proposed method. 
     
(a) 
     
(b) 
Figure 5. Comparison of image composition: (a) with results obtained 
using the video object segmentation based on motion detection without 
background construction [5], (b) with results obtained using the proposed 
method. 
IV. CONCLUSION 
In this paper, video object segmentation with opacity 
estimate is proposed. In the proposed method, the trimap is 
automatically generated from the result obtained using video 
object segmentation based on motion detection without 
background construction. Closed-form matting is then used 
with the automatically generated trimaps to yield the alpha 
mattes and the foreground objects. In contrast to the methods 
of video object segmentation using binary segmentation, the 
proposed method can obtain good object segmentation for 
objects with translucency to keep the important features of 
the foreground objects. 
Experimental results show that the proposed video object 
segmentation outperforms the method suggested by Gupta et 
al. Furthermore, image composition with results obtained 
using the proposed method can obtain more realistic image 
composition. Therefore, the proposed video object 
segmentation is very useful for the interesting applications of 
video editing. 
ACKNOWLEDGMENT 
This paper has been supported by the National Science 
Council, Taiwan, under grant no. NSC99-2221-E-346-007.  
REFERENCES 
[1] C. R. Huang, H. P. Lee, and  C. S. Chen, “Shot Change Detection via 
Local Keypoint Matching,” IEEE Transactions on Multimedia, vol. 
10, no. 6, pp. 1097-1108, 2008. 
[2] B. Sugandi, H. Kim, J. K. Tan and S. Ishikawa, “Real Time Tracking 
and Identification of Moving Persons by using a Camera in Outdoor 
Environment,” International Journal of Innovative Computing, 
Information and Control, vol.5, no.5, pp.1179-1188, 2009. 
[3] S. Zhu and Y. Liu, “Automatic Video Segmentation for Advanced 
Story Retrieval,” International Journal of Computer Sciences and 
Engineering Systems, vol.2, no.3, pp.157-166, 2008. 
[4] G. Zhang, and W. Zhu, “Automatic Video Object Segmentation by 
Integrating Object Registration and Background Constructing 
Technology,” Proceedings of 2006 International Conference on 
Communications, Circuits and Systems, pp. 437-441, Guilin, China, 
2006. 
[5] W. C. Hu, “Real-time On-line Video Object Segmentation based on 
Motion Detection without Background Construction,” International 
Journal of Innovative Computing, Information and Control, vol.7, 
no.4, 2011. 
[6] E. J. Carmona, J. Martínez-Cantos, and J. Mira, “A New Video 
Segmentation Method of Moving Objects based on Blob-level 
Knowledge,” Pattern Recognition Letters, vol. 29, pp. 272-285, 2008. 
[7] T. Y. Chen, T. H. Chen, D. J. Wang and Y. C. Chiou, “Real-time 
Video Object Segmentation Algorithm based on Change Detection 
and Background Updating,” International Journal of Innovative 
Computing, Information and Control, vol.5, no.7, pp.1797-1810, 
2009. 
[8] T. Porter and T. Duff, “Compositing Digital Images,” Proceedings of 
ACM SIGGRAPH, vol. 18, no. 3, pp. 253-259, 1984. 
[9] A. Gupta, S. Mangal, P. Nagori, A. Jain, and V. Khandelwal, “Video 
Matting by Automatic Scribbling using Quadra Directional filling of 
Segmented Frames,” Porceedings of 2nd IEEE International 
Conference on Computer Science and Information Technology, pp. 
336-340, Beijing, China, 2009. 
[10] J. Shi and J. Malik, “Normalized Cuts and Image Segmentation,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 
22, no. 8, pp. 888-905, 2000. 
[11] A. Levin, D. Lischinski, and Y. Weiss, “A Closed-Form Solution to 
Natural Image Matting,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 30, no. 2, pp. 1-15, 2008. 
 
686
99 年度專題研究計畫研究成果彙整表 
計畫主持人：胡武誌 計畫編號：99-2221-E-346-007- 
計畫名稱：自動化且具可靠性之影像去背及視訊去背的研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
