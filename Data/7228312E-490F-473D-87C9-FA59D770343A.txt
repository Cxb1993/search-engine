指導式神經網路最佳學習機制研製--探討網路對訊號具有非凹性
誤差曲線(面)之最佳學習(I) 
 
The Development of Optimal Learning Mechanism for Supervised 
Neural Network ---A Study of Optimal Learning for The Signals 
with Non-Convex Error Curve (Surface) 
 
計畫編號：NSC 95－2221－E－214－038 
執行期間：95/08/01 ~ 96/07/31 
計畫主持人：黃瑞初 教授 
計畫參與人員： 翁彬烜、陳俊任、黃致堅 
執行單位：義守大學電機工程系 
 
中文摘要 
本研究主要目的在於探討指導式類神經網路最
佳學習機制之研製，試圖解決類神經網路應用時
常發生之陷入局部極小值的問題。吾等知道多層
感知器被廣泛應用於非線性系統之模擬與應
用，然其所使用的學習法則－倒傳遞(BP)演算法
卻仍有其缺點，為改善及強化此種學習法則之效
率與穩定性，我們在感知器的輸出層中引入線性
迴歸的輔助修正方法，並藉由兩個範例問題比較
傳統BP學習法則及所提出之方法之差異，結果傳
統BP學習法則容易使網路落入局部極小值中，相
反的我們提出之新方法除可加快網路學習速度
並可得到較佳之學習效果。 
 
關鍵詞：多層感知器、倒傳遞、線性迴歸 
 
Abstract 
 
In this project, the optimal leaning mechanism for 
supervised neural network was investigated and 
studied. We tried to solve the problem of local 
minimum usually happened in the real applications 
of neural network. As we know, the multi-layer 
perceptrons （MLP） has been widely applied in the 
modeling and application of the nonlinear systems. 
However, the commonly used method, 
back-propagation (BP) learning algorithm, still has 
its unavoidable shortcomings. In order to improve 
the efficiency and stability of BP learning algorithm, 
a new learning method combining with the linear 
regression （ LR ） technique is presented. To 
demonstrate the superiority of the method we 
developed, two examples were simulated. The 
conventional BP learning method was also 
performed as the comparison with the new method 
proposed. From the results shown, the conventional 
BP method easily makes neural model plunge into 
the local minima. On the contrary, the new method 
we proposed not only has a fast leaning, but also 
has a better learning efficiency. 
 
Keywords：multi-layer perceptrons, 
back-propagation, linear regression 
 
1. Introduction 
 
The powerful learning and adaptive capability of 
neural network (NN) has been applied into many 
applications, such as management, decision-making, 
system identification, control systems, image 
processing and so on [1-4]. Back-propagation (BP) 
learning rule is the most popular learning algorithm 
used in ANN techniques [1]. It is an iterative 
gradient method to define the parameters of a 
multi-level network. However, the slow convergent 
speed and easy plunging into the local minimum 
are two main shortcomings of BP learning 
algorithm.  
 
Generally, the learning rate (α) and momentum (ξ) 
are two parameters associated with the back 
propagation learning that are selected as constants 
in the most of applications. An improper setting for 
either of these two values may either slow down the 
learning process or may even prevent the 
convergence of the network. Usually, the network 
may fall into a local minimum if these parameters 
are not optimized. Selection of these parameters, 
and investigation of the relationship of these 
parameters, together with the convergence speed 
and the accuracy of ANN, has been the center of 
numerous academic and research publications 
[5-12]. In general, no single algorithm could be 
concluded to be the best learning method for all the 
Step 6: Present another input and go back to step 2. 
All the training inputs are presented 
cyclically until the weights stabilize.  
 
2.2 Linear Multi-Regression (LMR) method 
 
Generally, the linear multi-regression is a method 
which can obtain the optimal solution, if the data 
base is linear and good for estimation. The LMR 
method could be briefly described as follows [9]. 
 
Usually, a prediction linear model is expressed as  
nn xbxbxbby ˆˆˆˆˆ 22110 ++++= L   （2.10）            
In this expression, yˆ  represents a predicted value 
for the model based on specified values of the 
predictor variables nxxx ..., , , 21 . The estimated 
regression coefficients are nbbbb ˆ ..., ,ˆ ,ˆ ,ˆ 210 . 
Suppose the input data is denoted as  
),,,,( 21 niniii yxxx L , mi ,,1 L=                           
Then, the multiple linear regression model  
niniii xbxbxbby ++++= L22110  , 
mi ,,1 L=   （2.11） 
can be written in the matrix form as  
βXY =   （2.12） 
where, X ,Y , and β  










=
nmmm
n
n
n
xxx
xxx
xxx
xxx
X
L
MMMM
L
K
L
21
32313
22212
12111
1
1
1
1
 , 










=
my
y
y
y
Y
M
3
2
1
                  
T
nbbbb ],,,,[ 210 L=β  
 
Based on the least-squares estimation, the following 
equation can be obtained.  
YXXX TT =β)(
  （2.13） 
Therefore, the least squares estimator can be 
expressed as 
)()(ˆ 1 YXXX TT −=β   （2.14） 
Actually, the combination of LMR and BP learning 
ways for the neural network training has been 
studied and reported by several articles [10-12]. 
However, the best performance is only shown for a 
two-layer neural network. In other words, the 
optimal weights can be reached only for the neural 
model with input and output layers. In article [12], 
the LMR method was used in a multi-layer neural 
network. However, from the neural performance 
shown, only the learning speed can be improved. 
But, the accuracy of neural performance is not 
guaranteed to be better than the performance of 
neural model using the conventional BP learning 
method only. To improve the performance of LMR 
method in neural network training, we present a 
new method which combines LMR and BP 
techniques for neural model training. The detailed 
description of the method we proposed is reported 
in the following section. 
 
3. Hybrid Learning Method with LMR 
and BP Techniques 
 
To improve the learning efficiency of neural 
network, a hybrid learning method with LMR and 
BP techniques is proposed. In this method, the 
LMR and BP algorithms are alternately used in the 
process while the neural model is training. For 
instance, we firstly train the neural network by BP 
learning method and keep monitoring the training 
result at the same time. When the situation of slow 
convergence or small divergence of the network’s 
weight is happened on the training process, we stop 
the training process temporally. At this moment, the 
weight increments ( ]2[W∆ ) between hidden layer 
and output layer will be solved by using the LMR 
method. However, unlike the way used in the article 
[12], the optimal computed increments, ]2[W∆ , 
will not be added into the present weights directly. 
All computed increments by LMR method are 
multiplied by a small real number and then be 
added into the present weights. That means we only 
specify one direction for the weights adjustment. 
Then, the weights between input layer and hidden 
layer will continuously be trained by using BP 
method. The detailed training algorithm is 
described and summarized as follows. 
 
The hybrid learning method with LMR and BP 
techniques: 
 
Step 1: Initialize all weights to the small random 
values. 
Step 2: Train neural model till the network has the 
slow convergence or small divergence.  
Step 3: Fix the weights between input layer and 
hidden layer, then use the LMR method to 
solve the optimal increments for the 
weights between hidden layer and output 
layer. 
Step 4: Multiply the increments by a small real 
number and adjust the weights by  
]2[]2[]2[ )()1( WkWkW ∆⋅+=+ λ .(3-1) 
Step 5: Fix the weights between hidden layer and 
output layer, then retrain the weights 
between input layer and hidden layer by 
BP method till the weights are convergent. 
Step 6: Go back Step 2 and keep training the neural 
network until the error of testing data is 
convergent. 
 
4. Simulation 
Selection.” World scientific publishing Co. 
Pte. Ltd. 1998. 
[10] Brijesh Verma, “Fast learning of Multilayer 
Perceptrons,” IEEE Transactions on Neural 
Network, vol. 8, no. 6, pp 1314-1320, 1997.  
[11] Verma B.K., Mulawka J.J. “Training of the 
multiplayer perceptron using direct solution 
methods,” in Proc. 12th IASTED int. Conf. 
Appl. Inform., Annecy, France, pp. 18-24, 
1994. 
[12] Xu Chunhui, Xu Xiangdong, “Research of 
new learning method of feedforward neural 
network,” Journal of TSINGHUA university, 
（science and technology）pp.301-307,1999 
 
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.1
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.2
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.3
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.4
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.5
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.6
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.7
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.8
AB
E
BP HLRBP
0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
1 11 21 31 41 51 61 71 81 91 101
Time*200  learning rate 0.9
AB
E
BP HLRBP
Figure 2: The error curves performed by BP and proposed methods 
with 9 different learning rates for example 1. 
 
出席國際會議報告 
 
 
 
 
 
 
 
報告人: 黃瑞初 教授 
 
義守大學 電機工程學系 
 
 
 
 
 
中華民國九十五年十一月二十六日 
 
 
 
參與本次會議，由相關的論文發表中，無論是在研究的應用價值與對
問題的探討深度，個人深深以為應多鼓勵國內各大專以上之教師與研
究生參加此類型國際會議並發表論文，藉由吸取相關知識外，並可將
國內的研究情形與水準介紹予他國學者了解，經由與世界各國學者的
交談，適時的將國內概況介紹於他們認識。 
 
三、 建議: 
建議國內各大學與研究機構應多舉辦類似此國際性質之學術會
議，以提昇本國學術在國際間的地位。為吸引國外學者專家參加，應
可試著與國際性組織聯絡，如 IEEE、IEE、IASTED等，爭取年度性
之會議於台灣舉行，擴大外國學者到台灣之機會，以便其了解台灣，
並可將台灣高等教育的現況介紹于其認識。 
 
四、 攜回資料名稱及內容: 
本次會議發表論文之論文光碟乙片。 
 
 
 
 
 
 
 
 
  
 
  
 
  
 
