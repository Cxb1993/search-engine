2 
 
Fig. 1.2  Multiple reference frames versus single reference frame. 
 
Fig. 1.3  Multiple reference frame motion estimation. 
 
1.3 Variable Block-size Motion Estimation 
The VBS motion estimation [4] allows macroblocks to be divided into variable-sized 
sub-blocks according to their motion conditions and object shapes. H.264 supports 7 types of 
block-size mode which are shown in Figs. 1.4 and 1.5. A macroblock can be divided into one of 
the four block-size modes shown in Fig. 1.4. If the 8×8 mode is chosen, the divided macroblocks 
can be further divided according to Fig. 1.5. The variable-sized blocks divide a frame according 
to the actual shape of moving objects, and the performance of motion estimation and 
compensation can be increased. After block dividing, there may be one, two, four, eight, or 
sixteen motion vectors in a macroblock. The VBS increase the performance of motion estimation 
and reduces the residual values need to be compensated in complex blocks. 
 
Fig. 1.4  MB-level block-size modes: 16×16, 8×16, 16×8, 8×8 
 
Fig. 1.5  SubMB-level block-size modes: 8×8, 4×8, 8×4, 4×4 
4 
z Center-biased Frame Selection Algorithm 
A search pattern is provided to find a matched block in all reference frames systematically. 
Fig. 2.2 shows a simple search pattern. Nine locations in each reference frame are searched to 
decide the reference frame. Then exhaustive search is performed only in the decided reference 
frame. Ting et al. [9] have proposed a method based on center-biased frame selection. These 
kinds of algorithm have a chance of missing the best motion vector if the video motion is fast. 
 
Fig. 2.2  A simple search pattern. 
 
z Reference Frame Feature-based Methods 
While doing motion estimation with multiple reference frames, there is 80% probability 
finding the best matched block in the nearest reference frame. According to this, reference frame 
feature-based methods always search the nearest reference frame first. The information of the 
searched motion vector is used to decide whether the rest of reference frame are searched or not. 
Chen et al. [10] proposed a method with four major rules. The major disadvantage of these 
methods is a full search in the nearest reference frame is always performed even the best matched 
block is not in the nearest frame. 
An efficient multiple reference frame selection algorithm is proposed in this project. The 
proposed method decide the reference frame used of a block fast. A pyramid structure fast search 
method is developed to find the global motion vector of a frame. By comparing the motion vector 
of blocks to the global motion vector, we can find blocks belong to the background. For blocks 
belonging to the background, the search in the rest reference frames may be omitted. 
 
2.3 Block Size Selection for VBS Motion Estimation 
As described in Section 1.2, H.264 can divide a macroblock into several sub-blocks. There 
are 259 possible combinations of block divisions. H.264 JM uses exhaustive search to select the 
best block division. There are many fast block size selection methods that being proposed. Yin et 
al. [12] proposed a method to reduce the number of possible combinations by the relationship of 
error surface and block size. Ahmad et al. [13] developed a technique using the division of 
neighboring coded blocks as a prediction since the block characteristics of neighboring blocks 
may similar. All of the above-mentioned methods only focus on still and homogeneous area. 
However, if the video without these two features, the efficiency of existing methods will be worse 
and constricted. For examples, for those videos with camera-motion, most MBs on background 
area are moving and the exhaustive search is conducted to get their proper block modes. However, 
for each of them, we can usually find a very similar MB in the previous frame. 
In this project, we propose a fast block-size mode decision algorithm while keeping a similar 
6 
 
Table 3.1  The formulations of predicted samples. 
mode formulation block constraints 
0 pred[x,y]=p[x,-1], with x,y=0,1,2,3 A is available 
1 pred[x,y]=p[-1,y], with x,y=0,1,2,3 B is available 
pred[x,y]=(p[0,-1]+p[1,-1]+p[2,-1]+p[3,-1]+p[-1,0]+ +p[-1,1] +p[-1,2] +p[-1,3])/8, 
with x,y=0,1,2,3 A and B are available
pred[x,y]=(p[0,-1]+p[1,-1]+p[2,-1]+p[3,-1])/4, with x,y=0,1,2,3 A is available and B is unavailable 
pred[x,y]=(p[-1,0]+p[-1,1]+p[-1,2]+p[-1,3])/4, with x,y=0,1,2,3 B is available and A is unavailable 
2 
pred[x,y]=128, with x,y=0,1,2,3 A and B are unavailable
pred[x,y]=(p[6,1]+3*p[7,-1])/4, with x=3 and y=3 
3 pred[x,y]=(p[x+y,-1]+2*p[x+y+1,-1]+p[x+y+2,-1])/4, with x is not equal to 3 or y 
is not equal to 3 
A and D are available
pred[x,y]=(p[x-y-2,-1]+2*p[x-y-1,-1]+p[x-y,-1])/4, with x is greater than y 
pred[x,y]=(p[-1,y-x-2]+2*p[-1,y-x-1]+p[-1,y-x])/4, with x is less than y 4 
pred[x,y]=(p[0,-1]+2*p[-1,-1]+p[-1,0])/4, with x is equal to y 
A, B and E are available
pred[x,y]=(p[x-(y>>1)-1,-1]+p[x-(y>>1),-1])/2, with zVR equal 
to 0,2,4, or 6 
pred[x,y]=(p[x-(y>>1)-2,-1]+2*p[x-(y>>1)-1,-1] 
+p[x-(y>>1),-1])/4, with zVR equal to 1,3, or 5 
pred[x,y]=(p[-1,0]+2*p[-1,-1]+p[0,-1])/4, with zVR equal to -1 
5 Let zVR be set equal to 2*x-y 
pred[x,y]=(p[-1,y-1]+2*p[-1,y-2]+p[-1,y-3])/4, with zVR equal 
to -2 or -3 
A, B and E are available
pred[x,y]=(p[-1,y-(x>>1)-1]+p[-1,y-(x>>1)])/2, with zHD equal 
to 0,2,4, or 6 
pred[x,y]=(p[-1,y-(x>>1)-2]+2*p[-1,y-(x>>1)-1] 
+p[-1,y-(x>>1)])/4, with zHD equal to 1,3, or 5 
pred[x,y]=(p[-1,0]+2*p[-1,-1]+p[0,-1])/4, with zHD equal to -1 
6 Let zHD be set equal to 2*y-x 
pred[x,y]=(p[x-1,-1]+2*p[x-2,-1]+p[x-3,-1])/4, with zVR equal 
to -2 or -3 
A, B and E are available
pred[x,y]=(p[x+(y>>1),-1]+p[x+(y>>1)+1,-1])/2, with y is equal to 0 or 2 
7 pred[x,y]=(p[x+(y>>1),-1]+2*p[x+(y>>1)+1,-1]+ 
p[x+(y>>1)+2,-1])/4, with y is equal to 1 or 3 
A and D are available
pred[x,y]=(p[-1,y+(x>>1)]+p[-1,y+(x>>1)+1])/2, with zHU is 
equal to 0,2, or 4 
pred[x,y]=(p[-1,y+(x>>1)]+2*p[-1,y+(x>>1)+1] 
+p[-1,y+(x>>1)+2])/4, with zHU is equal to 1 or 3 
pred[x,y]=(p[-1,2]+3*p[-1,3])/4, with zHU is equal to 5 
8 Let zHU be set equal to x+2*y 
pred[x,y]=p[-1,3], with zHU is greater than 5 
B is available 
 
In each mode, we select six pixel pairs. And the directional difference corresponding to 
mode m is defined as DD(m), 
 ( ) ∑ −=
),(
)()(
βα
βα ggmDD , (3.1) 
where (α,β) is the selected pair in mode m in Table 3.2, and g(α) and g(β) are the corresponding 
pixel values in the original block. 
We keep three modes with smallest directional difference and the DC mode as candidate 
modes. For each candidate mode m, a down-sampled prediction error is estimated to further 
reduce the computation time: 
 ∑
∈
−=
'
)()()(
H
mfgmDS
α
αα , (3.2) 
8 
modes and prediction errors of all 4×4 blocks in the MB will be found. Then the block type 
prediction described in Section 3.2 will be adopted to decide whether the MB uses 16×16 intra 
prediction or not. If 16×16 prediction is not performed, the 4×4 block type coding result will be 
the final block type mode for this MB. Otherwise, the 16×16 intra prediction the same as the 
RDO full search scheme in JM software is performed. Then “intra block type decision” in JM is 
applied to select from 4×4 and 16×16 block type according to the prediction error. 
 
(a) 
 
(b) 
 
(c) 
Fig. 3.2  Procedures of intra prediction according to the QP values.   
(a) QP value is extremely large. (b) QP is extremely small. (c) Others. 
 
The component “proposed 4×4 intra prediction” in Figs. 3.2(b) and (c) is shown in Fig. 3.3. 
For each 4×4 image block, a boundary test is applied, and an exhaustive search on intra 
prediction modes denoted as “full prediction” is adopted if the block is on the boundary of the 
frame. For non-boundary blocks, MPM prediction is applied to get an initial prediction mode. 
Then a “Good Enough Test” is used to test the performance of MPM-predicted mode by 
 ))(,(),( AIPMAPEmCPE c <  and ))(,(),( BIPMBPEmCPE c < , (3.3) 
where A, B, and C are the blocks described before. IPM(A) and IPM(B) are the intra prediction 
modes of reconstructed blocks A and B respectively. mC represents the mode we predicted by our 
prediction method. We will consider mC is a good mode for block C if the prediction error is 
smaller than the prediction error of block A and B. 
If the MPM prediction is good, the predicted mode is used directly. Otherwise the proposed 
FIFM prediction is employed. The mode predicted by FIFM is also tested by the “Good Enough 
Test”. The result of FIFM is used if the test considered as a good mode. Otherwise, the “full 
prediction” is employed to find a good mode by exhaustive search. 
10 
In Table 3.3, we can see that the proposed method provides about 28.288% time saving, 
PSNR loss about 0.0564 dB, and bit rate rising about 0.939%. The Pan’s method provides about 
25.272% time saving, 0.0637 dB PSNR loss, and 1.427% bit rate rising. These results show that 
our method is superior to Pan’s. We also plot the RD cost of the “Foreman” and “Mobile” 
sequences in Fig. 3.4. The proposed process has a much-closed curve with the original JM8.4 
scheme. This means that we could pay few bit-rates and loss a little quality to gain lots of time 
saving. 
     
(a)                                 (b) 
Fig. 3.4  The RD curves of the sequences.  (a) Foreman; (b) Mobile. 
 
4. Proposed Fast Multiple Reference Frame Selection for H.264/AVC 
In this section, we propose an efficient approach to ME in the multiple reference frames 
selection. From our observation, we find the fact that for each block on the background, we can 
usually find a good motion vector in the nearest reference frame. However, for a block containing 
a moving object, a good match may not appear in the nearest reference frame. Based on the fact, 
we will develop a method to quickly decide the reference frame used for each block. 
 
4.1 Observation and Analysis 
Three facts are found in our observation. First, most part of a background is keep unchanged 
for at least several seconds. Second, a background will be still when a camera is fixed, and some 
backgrounds will be moving smoothly due to the camera motion. Third, a background usually 
occupies a large space in the video sequence. We also found two phenomena. One is that those 
blocks belonging to the background usually have the same motion vector as the global motion 
vector. The other is that the blocks belonging to the background usually refer to the nearest 
reference frame. The global motion vector will dominate the frame motion and many blocks 
belonging to the background only need to refer to the nearest reference frame. 
 
4.2 Proposed Global Motion Vector Finding Algorithm 
We provided an efficient algorithm to obtain the global motion vector quickly. The algorithm 
utilizes the concept of multi-resolution. A pyramid structure is used. The current frame and the 
nearest reference frame are both down-sampled into five levels as the structure shown in Fig. 4.1, 
where M and N are the width and height of a frame, respectively. 
12 
int erTH  are thresholds which depends on the quantization step size. And interSATD is the sum of 
absolute transform difference (SATD) of the encoding block with the inter motion estimation. 
First, the algorithm takes a block with a certain inter-mode as input data. Second, the motion 
vector of the block is obtained by applying ME on the nearest reference frame. Third, if the 
difference between the global motion vector and the obtained motion vector in the above step is 
small enough, we do not search the rest reference frames. However, due to the zooming motion in 
the video sequences we may get a wrong global motion vector. To solve this problem, we restrict 
the SATD to refine our algorithm. That is the restriction about the global motion vector is 
satisfied, and the SATD is small enough, we determine not to search the rest reference frames. 
 
Fig. 4.2  The flowchart of our proposed method. 
 
4.4 Results and Discussions 
Nine video sequences are used as test sequences. The results of the proposed method versus 
the exhaustive search in H.264 JM are shown in Table 4.1. We can find that the maximum PSNR 
drop is 0.13dB, and the average PSNR drop is 0.05dB. The ME time reduction which is 
dependent on video sequences is 15%-72%. However, the average bit-rate only increases 1%. The 
proposed methods provide the trade-off among the encoding time, the quality, and the bit-rate of 
the encoded image sequences. 
“Miss detection” means the selected reference frame of a block using the proposed method 
is different from that of using the exhaustive method. A comparison of miss detection rates using 
the proposed method and those of the Huang et al.’s method are listed in Table 4.2. The proposed 
method have lower miss detection rate. Note that the video sequence “Mobile and Calendar” has 
higher miss detection rate than other video sequences this is due to that the video sequence has 
zooming effect and the complicated texture. The proposed method applied to this kind of video 
sequences have higher miss detection rate, since we only consider translation motion. 
 
No 
Yes 
Input block with a 
certain inter-mode 
Find MV in the nearest 
reference frame 
interSATD<THinter and 
Diff(MVblock)< THgmv ? 
Find the best MV in the 
rest reference frames 
Using the nearest frame as the 
reference frame 
Find the global motion 
vector of the current 
frame referred to the 
nearest reference frame 
14 
5.1 Still Area Detection 
When the camera is not moving in a video, consecutive frames of a video sequence usually 
have a common background area. MB located in background area can be well estimated by the 
MB located in the same place of the previous frame. Mean of absolute differences (MAD) 
between two MBs are used to extract the MBs in a still area and their block-size mode are set to 
{16×16} or {16×16, 16×8, 8×16} according to various conditions. 
MAD between the current encoding MB and the co-located MB in the nearest reference 
frame is calculated by 
 ∑
==
−−=
NM
ji
n
ji
n
ji SSMN
TMAD
,
0,0
1
,, ||
1 , (5.1) 
where (i, j) means the location of pixels in the current MB, M and N denote the block size in 
X-axis and Y-axis, n j,iS  and 
1−n
j,iS  denote the intensity value of pixel (i, j) of original and the 
previous frame, respectively. We also evaluate the MAD between each 8×8 block k of the current 
encoding MB and the corresponding block k of the co-located MB in the nearest reference frame 
and denoted as SMAD(k). 
If the TMAD and all SMAD(k) are lower than a preset threshold Tstill, the 16×16 block-size 
mode is considered as the best mode for this MB. TMAD is less than Tstill, ad at least one SMAD(k) 
is larger than Tstill, this means that some sub-blocks in the MB may not in still area. Therefore, we 
may split the MB into smaller blocks. Here we only consider 16×16, 16×8 and 8×16 modes as 
candidate modes, and the mode with the minimum cost is selected from these candidates. 
 
5.2 Residual Image Judgment 
Background may be moving due to camera motion, and some objects in the video sequence 
may move steadily. This kind of area appearing in a frame is usually appearing in a contiguous 
area of its neighboring frames. Thus, if a MB is located in this kind of area, we can find a very 
similar MB in the reference frame. To find this kind of MBs, a residual image is created. 
For each MB in the current frame, the exhaustive search using 16×16 block-size is applied to 
find the best matching MB in the reference frame. The difference between the current MB and its 
best matching MB is calculated to forms a residual MB. All these residual MBs are combined to 
form a residual image, As Fig. 5.1(c) shows. 
   
(a)                       (b)                       (c) 
Fig. 5.1  VBS Selection.  (a) The current frame. (b) The reference frame.  
(c) Residual image between (a) and (b). 
 
A bi-level thresholding is applied to the residual image to get a binary image F’. That is, 
16 
each of them, we apply motion vectors relation analysis again to determine the most suitable 
mode. We also classify theses situations into following four cases: 
Case 1: deciding the best mode for a 8×8 block to be 8×8 mode, if 
  SMV(0) = SMV(1) = SMV(2) = SMV(3). 
Case 2: deciding the best mode for a 8×8 block to be 8×4 mode, if 
  SMV(0) = SMV(1) and SMV(2) = SMV(3) and SMV(0) ≠ SMV(2). 
Case 3: deciding the best mode for a 8×8 block to be 4×8 mode, if 
  SMV(0) = SMV(2) and SMV(1) = SMV(3) and SMV(0) ≠ SMV(1). 
Case 4: otherwise. 
Examining all block-size modes {8×8, 8×4, 4×8 and 4×4} and selecting the best one. 
where SMV(i) is the motion vector of 4x4 block i and i indicates the index of 4×4 block shown in 
Fig. 5.2. 
 
5.5 The Overall Algorithm 
First, we apply still area detection to decide if a MB is located in still area. If the MB 
satisfied the rules, best mode of this MB is set to 16×16 or one of 16×16, 16×8 or 8×16 according 
to its conditions, and the search for the others modes are skipped. Otherwise, 16×16 block-size 
mode ME is applied to create the residual block. If the number of white points in a residual block 
is less than a threshold, we set the best mode to 16×16 and skip the other modes. Otherwise, we 
split the MB into four 8×8 blocks. Then 8×8 block-size mode ME is performed. The cases 
described in Section 5.3 are checked for each MB. If one of the first three cases is satisfied, the 
mode is decided. Otherwise, enter to the final step. For each 8×8 blocks, we split 8×8 block into 
four 4×4 blocks and do ME using 4×4 mode to get motion vectors of 4×4 blocks. Then, we check 
four cases in Section 5.4, and the best mode is decided for the 8×8 block. The mode obtained 
through above process is denoted as P8×8. 
After the mode decision of each 8×8 block, we sum the prediction error of the four 8×8 
blocks. Then, we do ME using 16×8 and 8×16 modes, and select the mode with minimum 
prediction error from the 16×16, 16×8, 8×16 and P8×8 modes as the best mode for the MB. 
 
5.6 Results and Discussions 
The proposed algorithms for inter-frame mode decision are implemented and simulated. 
Three factors: time-saving rate, PSNR and bit-rate are used to compare. The simulation is based 
on the JM 8.2. In the following tables and figures, “Method 1” means that we only use the still 
area detection described in Section 5.1 to reduce candidate modes for MBs. If the modes can not 
be decided, exhaustive search is applied. “Method 1+2” means the residual image judgment 
described in Section 5.2 is also applied. Method described in Sections 5.3 and 5.4 is also applied 
in the “Method 1+2+3”. 
Comparison of speed and bit-rate with Yu’s method and Wu’s method are shown in Figs. 5.3 
and 5.4, respectively. Yu proposed a fast approach for inter mode decision based on the 
homogeneous area detection. They classified the block-size modes into three categories, {16×16}, 
{16×16, 16×8, 8×16} and {all possible modes} corresponding to different level of homogeneity. 
The speed-up-rate of encoding time lied in between 17%-32% for each sequence; the increasing 
rate of bit-rate is about 3.15% in average. Comparing to our algorithm shown in Fig. 5.3, we have 
18 
參考文獻 
 
[1] “Draft ITU-T recommendation and final draft international standard of joint video 
specification ITU-T Rec. H.264/ISO/IEC 14496-10 AVC,” in Joint Video Team (JVT) of 
ISO/IEC MPEG and ITU-T VCEG, JVTG050, 2003. 
[2] T. Wiegand, Gary J. Sullivan, Gisle Bjontegaard, Ajay Luthra, “Overview of the H.264/AVC 
video coding standard,” IEEE Trans. Circuits Syst. Video Technol., vol. 13, pp. 560–570, July 
2003. 
[3] T. Wiegand, X. Zhang, and B. Girod, “Long-term memory motion-compensated prediction,” 
IEEE Trans. Circuits Syst. Video Technol., vol. 9, no. 1, pp. 70-84, Feb. 1999. 
[4] M. Wien, “Variable block-size transforms for H.264/AVC,” IEEE Trans. Circuits Syst. Video 
Technol., vol. 13, pp. 604–613, July 2003. 
[5] F. Pan, X. Lin, S. Rahardja, K. P. Lim, and Z. G. Li, “A Directional Field Based Fast Intra 
Mode Decision Algorithm For H.264 Video Coding,” IEEE International Conference on 
Multimedia and Expo, Vol. 2, pp. 1147-1150, June 2004. 
[6] F. Pan, X. Lin, et. al., “Fast Mode Decision for Intra Prediction,” ISO/IEC JTC1/SC29/WG11 
and ITU-T SG16 Q.6, JVT 7th Meeting, Pattaya ІІ, Thailand, March 2003. 
[7] B. Meng, O. C. Au, C. W. Wong, and H. K. Lam, “Efficient Intra-Prediction Algorithm in 
H.264,” IEEE International Conference on Image Processing, Vol. 3, pp. 837-840, Sept. 2003. 
[8] C. Kim, H. H. Shih, and C. C. Jay Kuo, “Feature-Based Intra-Prediction Mode Decision for 
H.264,” IEEE International Conference on Image Processing, Vol. 2, pp. 769-772, Oct. 2004. 
[9] C. W. Ting,  L. M. Po, and C. H. Cheung,  “Center-biased frame selection algorithms for 
fast multi-frame motion estimation in H.264,” Proceedings of the 2003 International 
Conference on Neural Networks and Signal Processing, Vol. 2,  pp. 1258- 1261, Dec. 2003 . 
[10] Y. W. Huang, B.Y. Hsieh, T. C. Wang, S. Y. Chen, S.T. Ma, C. F. Shen, and L.G. Chen. 
“Analysis and reduction of reference frames for motion estimation in MPEG-4 
AVC/JVT/H.264”, IEEE Multimedia and Expo, 2003 (ICME ’03), vol. 2, pp. 6-9, Jul. 2003. 
[11] A. Chang, O.C. Au, and Y. M. Yeung, ”A novel approach to fast multi-frame selection for 
H.264 video coding”, IEEE Circuits and Systems 2003. (ISCAS ’03), vol. 2, pp. 25-28, May 
2003. 
[12] P. Yin, H.-Y.C. Tourapis, A.M. Tourapis, and J. Boyce, “Fast mode. decision and motion 
estimation for JVT/H.264,” in Proceedings of International Conference on Image Processing, 
pp. 853-856, Sep. 2003. 
[13] A. Ahmad, N. Khan, S. Masud, M. A. Maud, “Efficient block size selection in H.264 video 
coding standard”, Electronics Letters , vol. 40 , no. 1 , pp.19 – 21, Jan. 2004. 
20 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：97 年 8 月 30 日 
國科會補助計畫 
計畫名稱：架構於視訊壓縮標準 H.264/AVC 下，有關運動估計與
補償之研究 
計畫主持人：陳玲慧 教授 
計畫編號：NSC 96－2221－E－009－201－ 學門領域：影像處理
技術/創作名稱 快速 Intra frame 預測模式選擇方法 
發明人/創作人 陳玲慧 
中文：我們對於畫面內模式選擇提供了一套有效率的演算法。我們
使用一種名為「快速過濾畫面內模式方法」，將一些模式成為候選
模式，並且針對候選模式來做選擇。同時我們也使用一些空間上的
資訊，使畫面內預測的演算法提早結束，達到加快編碼時間的效果。
 
技術說明 英文：We presented an efficient algorithm for the intra mode decision 
which has nine prediction modes for a 4x4 block coding, and four 
prediction modes for a 16x16 block coding. A Fast Intra-mode Filtering 
Method (FIFM) is provided to quickly find out the candidate modes, 
and the spatial coherence is utilized to achieve some earlier 
termination. 
可利用之產業 
及 
可開發之產品 
H.264 編碼軟體、硬體，硬體編碼晶片製作。 
技術特點 
「快速過濾畫面內模式方法」，將一些模式成為候選模式，並且針
對候選模式來做選擇。 
推廣及運用的價值 
本技術可加快 H.264 編碼時，畫面內預測模式的計算時間。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
22 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：97 年 8 月 30 日 
國科會補助計畫 
計畫名稱：架構於視訊壓縮標準 H.264/AVC 下，有關運動估計與
補償之研究 
計畫主持人：陳玲慧 教授 
計畫編號：NSC 96－2221－E－009－201－ 學門領域：影像處理
技術/創作名稱 快速可變動方塊大小運動估計之模塊選擇方法 
發明人/創作人 陳玲慧 
中文：我們提出一個快速演算法，對每一個 macroblock 快速找出
已找出合適的模塊選擇。該方法利用靜態區域、剩餘模塊影像、運
動向量的關係等，來減少可能的模塊組合數目，以降低搜尋時間。
 
技術說明 英文：We proposed a fast method to find a suitable block size for each 
macro-block in inter prediction. The proposed method uses some 
information, such as still area, residue image after motion search, and 
the relation of motion vectors, to reduce the number of possible 
block-size modes in the variable block-size mode decision. 
可利用之產業 
及 
可開發之產品 
H.264 編碼軟體、硬體，硬體編碼晶片製作。 
技術特點 
利用靜態區域、剩餘模塊影像、運動向量的關係等，來減少可能的
模塊組合數目，以降低搜尋時間。 
推廣及運用的價值 
本技術可減少 H.264 編碼時，進行多可變動方塊大小運動估計之模
塊選擇所需的時間。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
 
二、與會心得 
 
此次出國主要是參加於昆明舉辦的機器學習與控制理論國際會議(ICMLC)，ICMLC 是機器學
習與控制理論的主要國際會議。已為EI Compendex收錄。每年有來自全世界的研究人員聚集
一起，相互交流。此次本人共發表了兩篇有關資訊隱藏的論文, 一篇是利用power point 檔中
之動態特效做資料隱藏；另一篇則是利用迷宮做資料隱藏，由本人指導的兩位博士生分別上
台以英文發表論文，兩位博士生都有不錯的表現。也和一些國外學者做些討論。台灣也有數
十位學者參加。 
 
另外此次研討會涵蓋領域很廣。擴展了我們的視野，也認識了不少同領域及不同領域之學者。
在研討會的晚宴中和貴州大學教授同桌，談到大陸的急速發展，產生很多問題，如倫理觀念
漸失，對台灣的民主、有制序、整潔及服務頗為欣賞。整體而言，参加此次研討會，不管在
學術方面或與國際學者交流都有不少的收穫。 
 
 
  
attention of audience on the emphasized object. 
 
 
 text object 
 line object 
image object 
 
Figure 1. Three different types of objects appearing on a 
slide. 
 
 
Figure 2. The presentation without animation effect. 
 
 
time 
2 sec 3 sec 
time 
0 sec 1 sec 
 
Figure 3. The presentation with “Bold Reveal” animation 
effect on the text object “Steganography & Watermarking”. 
 
The Microsoft PowerPoint animation effects can be 
grouped into four categories: Entrance, Emphasis, Exit and 
Motion Paths. Entrance effects can be set to objects so that 
they enter with animations during Slide Show. Emphasis 
effects animate the objects on the spot. Exit effects allow 
objects to leave the Slide Show with animations. Motion 
Paths allow objects to move around the Slide Show. Each 
effect contains variables such as start (On click, With 
previous, After previous), delay, speed, repeat and trigger. 
This makes animations more flexible and interactive. 
After analyzing the animation effects, we find that the 
perceptive results of “Emphasis” and “Motion Paths” 
animation categories are similar somewhere. So we 
re-group the PowerPoint animation effects into three 
categories: Entrance, Emphasis/Motion Paths, and Exit. 
Two selected effects for each category are shown in Figure 
4 to Figure 9. 
 
 
 
Figure 4. The “Box” effect of “Entrance” animation 
category. 
 
 
 
Figure 5. The “Fly In” effect of “Entrance” animation 
category. 
 
 
 
Figure 6. The “Color Wave” effect of “Emphasis/Motion 
Paths” animation category. 
 
 
 
Figure 7. The “Transparency” effect of “Emphasis/Motion 
Paths” animation category. 
 
  
is just appearing, we now enforce to add an “Exit” 
animation, this will make the slide presentation unusual. 
According to the antecedent discussion, we suggest two 
principles of designing an animation codebook as follows: 
1. Re-grouping all animation effects into three types 
of animation effects: “Entrance”, “Emphasis/Motion Paths”, 
and “Exit” animation categories. 
2. Every character in secret message should be 
represented by at least one animation effect in each 
category. 
3.2. Embedding Process 
Before introducing the embedding process, we will 
make two general assumptions and three embedding rules 
at first, these will be followed by the proposed embedding 
process. 
Assumption 1: The transmitter and receiver have the 
same animation codebook. 
Assumption 2: The embedding animation effects 
applied on objects must obey the human perception. 
Rule 1: If the object used for embedding message is 
not in a slide, only effects of the “Entrance” animation 
category can be applied. 
Rule 2: If the object used for embedding message has 
been in a slide or has embedded message through an 
animation effect of “Entrance” animation category, only 
effects of the “Emphasis/Motion Paths” or “Exit” animation 
categories can be chosen to embed message.  
Rule 3: If the object used for embedding message has 
embedded message through the animation effect of 
“Emphasis/Motion Paths” animation category, only effects 
of the “Exit” animation category can be used to embed 
message. 
In the embedding process, we need three materials to 
create the embedded PowerPoint file. The first is the secret 
message M being transmitted. The second is the Animation 
Codebook (AC), which will record the corresponding 
between characters used in secret message and the 
animation effects. The last is the cover media, the 
PowerPoint file, which will be used to embed secret 
message. In the following, we will describe the embedding 
steps in detail. 
1. Find out the set of objects, O, which could be text 
boxes, directed lines, and images, in all slides. 
2. Generate a random binary sequence, R, with the 
number of 1 being the same as the length of secret message. 
3. Process each object in O based on the binary 
sequence R. 
4. For each object processed and the corresponding bit 
on R, Ri: 
(1) If Ri = 0, either an animation effect not in AC is 
applied or no any animation effect is applied. 
(2) If Ri = 1, the animation effect in AC corresponding 
to the secret character is applied. 
5. Repeat step 4 until all characters in secret message 
are embedded. 
Note that each object can be applied at most three 
animation effects, one of which belongs to the “Entrance” 
animation category, one belongs to “Emphasis/Motion 
Paths” animation category, and the other belongs to “Exit” 
animation category. This means that each object can embed 
at most three characters. 
3.3. Extracting Process 
Similar to the embedding process, we need two 
materials to extract our secret message in the extracting 
process. One is the embedded PowerPoint file and the other 
is the same Animation Codebook, AC, used in the 
embedding process. The steps of the extracting process are 
described as follows. 
1. Extract animation effects, applied in objects from 
the embedded PowerPoint file sequentially. 
2. For each extracted animation effect, check if it is 
in AC. If yes, convert it to the corresponding character 
based on AC. Otherwise, skip. 
3. Merge all extracted characters to form the secret 
message. 
4. Experiments 
Note that one of the most important works in the 
proposed method is to design an adaptive animation 
codebook, which will be used to embed and extract secret 
message. Here, we will give an example of codebook 
design. A simple codebook with 37 different 
characters/symbols is created and shown in Table 2. Every 
character/symbol can be represented by three different 
kinds of animation effects (Entrance, Emphasis or Motion 
Paths, or Exit animation categories); this makes information 
hiding securer and more natural. All of the animation 
effects in the codebook can be easily extracted by human 
vision or the customer animation function in Microsoft 
PowerPoint 2003. 
 
