(5) Identify Skeleton Motion  Jump to New Video 
Segment (or Next Frame)
TP Queue 
Initial Background Synchronization 
(2) Background 
Variation Update
Get Image Frames 
Reconstruct Background 
for Large Variation
Get Track Points (TPs) (3) Get TP Property 
Predict TP position 
Yes 
No 
Match TP? 
Search TP in a 
movement boundary
New TP Array 
(1) Start 
Yes
No Match TP? 
Update TP Property
Median Filter Erosion and Dilation
Count Miss Detection 
and Lost TPs 
(4) Map TPs to Skeleton  
Restore 3D TPs 
Using 2 Cameras 
Project Skeleton to 2D Add Skeleton to Scenario Video 
Figure 2: Algorithm of Video Tracking and Interaction
(a) (b)
(c) (d)
Figure 3: Tracking Process (a) Original Video 
(b) Track Points (c) Track Points Identification 
(d) Add Skeleton to Scenario Video 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2.2. Background and track points separation 
Since light sources to track points are quite difficult 
to stay stable, it is necessary to update background 
dynamically in order to obtain a better separation result of 
background and track points. Assuming that BBk(x, y) and 
Bk+1B (x, y) are values of point (x, y) on the kth and the k+1th 
frames, respectively. And, Ik(x, y) is the value of point (x, 
y) on the coming kth frame. We use 
 
 
 
 
[ ]
[ ]
⎪⎪⎩
⎪⎪⎨
⎧
∉
=−+
∈
=−+
=+
pointobject   ),( if      
01.0  ,  ),(),(),(
pointobject   ),( if      
1  ,  ),(),(),(
),(1
yxI
yxIyxByxB
yxI
yxIyxByxB
yxB
k
kkk
k
kkk
k ββ
αα
 
 
This strategy works if small variation of light source 
occurs on background. However, if the variation is too 
large, the background should be reconstructed. After that, 
we use median filter to exclude isolated points. Also, we 
use two morphology operators (erosion and dilation) to 
perform the close operation. The close operation is able to 
eliminate small blocks. Figure 3 shows the original video 
in 3a and the track points in 3b after the second step. 
 
2.3. Track points identification 
The most challenge issue of multiple object tracking 
is to identify each and all objects. We maintain a track 
point array to store properties (location and color 
information) of all track points. In step (3), we use seed 
filling algorithm on the original video for basic color 
segmentation. Boundaries of track points are roughly 
computed, to obtain the center of each track point. Each 
record in the track point array contains a location of the 
center and the HSI color information. To precisely track 
skeleton motions. Each of the track points, L_Wrist, 
R_Wrist, L_Foot, and R_Foot, is recorded in the track 
point array with a motion vector, in addition to TP 
coordinate and color information. A motion vector can be 
one of the eight directions (i.e., East, West, South, North, 
Southeast, Southwest, Northeast, and Northwest) and the 
directions are extended to include z-movements. The 
motion vectors of the 4 track points are analyzed by an 
interactive controller. With special designed ‘hot spot’ in 
the interactive video, the hot spot is able to tell which of 
the 4 track points is engaged with a certain motion vector. 
Thus, a video hyper jump can be performed. Figure 1 
shows our system with an interactive video made by real 
person. Note that, each section of the video has a small 
movement of the avatar to response to a specific trigger 
from a track point and its motion vector. Our preliminary 
system contains only hard coded video segments. An 
authoring tool is underdevelopment to enable other 
scenario video clips. 
 
3. EXPERIMENT RESULTS AND ANALYSIS 
We evaluate the experiment system from two 
perspectives: the precision rate of tracking and the 
performance of the system. High tracking precision avoids 
misunderstanding of motion hence the system reacts as the 
player expected. Human motion tracking is affected by 
three important issues: changing light sources, speed of 
motion, and track point occlusion. We tested 12 video 
clips to analyze rate of lost track points and rate of miss 
detection. They are summarized in table 1.   
Table 1: Rates of Tract Point Detection 
 
Video 2 and 3 have a relatively unstable light source 
and higher occlusion. The miss detection rates are 7.2 and 
6.7 percents. Video 7 and 11 have a stable light source. 
However, track points move faster. The miss detection 
rates are 3.0 and 4.3 percents. Video 8 and 9 have very 
low rates since light source is stable. Intuitively speaking, 
miss detection rates get higher due to changing light 
source the most. Occlusion and motion speed are 
secondary factors. Our average miss detection rate is 
2.43%, which is tolerable.  
The performance of our system is not as we have 
expected but is reasonable. We use 320 by 240 as the 
resolution of the two video cameras. The resolution of 
scenario video is also 320 by 240. We try to lower the 
resolution of the two cameras. But, precision of tracking 
is decreased. Step three of the algorithm (i.e., track point 
identification) is computational expensive, especially in 
searching miss matched track points. In step four (i.e, 
skeleton reconstruction on background video), restoration 
of 3D information takes time. However, we use a 
simplified approach to use the front camera for skeleton 
mapping. But, the side camera is used only in the 
identification of skeleton motion. Thus, project from 3D 
to 2D is simplified to speed up computation. The system 
is running on a Pentium 4 2.8GHz computer with 1.5 GB 
RAM. Preliminary experience shows that, player is able to 
interact with the video with limited motions to control 
video reactions.  
 
4. CONCLUSIONS 
It is interesting to combine video tracking technique 
with interactive video to develop a system for video-based 
games. Our approach involves five steps of schemes to 
realize the system on a Window based environment. Our 
experience shows that the approach is feasible on ordinary 
personal computers, instead of using special hardware 
devices such as Play Station 2. Our future work includes 
the design and implementation of an authoring system for 
the scenario video and motion identification. We hope 
software on PCs will allow players to design their own 
video games using PCs and ordinary cameras. 
 
 REFERENCES 
[1] Eric R. Bachmann, Robert B. McGhee, Xiaoping Yun, and 
Michael J. Zyda “Inertial and magnetic posture tracking for 
inserting humans into networked virtual environments,” 
Proceedings of the ACM symposium on Virtual reality software 
and technology, November 2001.  
[2] D.E. Difranco, T.J. Cham, and J.M. Rehg. “Recovery of 3-D 
figure motion from 2-D correspondences,” in CVPR, 2001. 
[3] Z. Luo, Y. Zhuang, F. Liu, and Y. Pan, “Incomplete Motion 
Feature Tracking Algorithm in Video Sequences,” in 
Proceedings of the IEEE 2002 International Conference on 
Image Processing, New York, USA, September 22-25, 2002. 
[4] L. Sigal, S. Bhatia, S. Roth, M.J. Black, and M. Isard. 
“Tracking loose-limbed people,” in CVPR, 2004. 
[5] C. Sminchisescu and B. Triggs. “Kinematic jump processes 
for monocular 3D human tracking,” In CVPR, 2003. 
[6] KangKang Yin, and Dinesh K. Pai “Intuitive interfaces for 
animation: FootSee: an interactive animation system,” in 
Proceedings of the 2003 ACM SIGGRAPH/Eurographics 
Symposium on Computer animation, July 2003. 
Video No of 
Frames 
Lost TP 
Rate 
Miss Detection 
Rate 
1 600 0.8 % 1.1 % 
2 403 4.5 % 7.2 % 
3 350 3.9 % 6.7 % 
4 545 1.2 % 1.7 % 
5 570 1.2 % 0.2 % 
6 330 1.3 % 1.0 % 
7 420 2.7 % 3.0 % 
8 391 0.3 % 0.1 % 
9 818 0.3 % 0.8 % 
10 514 0.36 % 1.3 % 
11 467 3.7 % 4.3 % 
12 687 1.0 % 1.8 % 
Average  1.77 % 2.43 % 
Hyper-Interactive Video Browsing by a Remote 
Controller and Hand Gestures 
Hui-Huang Hsu, Timothy K. Shih, Han-Bin Chang,  
Yi-Chun Liao, and Chia-Tong Tang 
Multimedia Information Network Lab 
Department of Computer Science and Information Engineering 
Tamkang University, Taiwan, ROC 
E-mail: {hhsu, tshih}@cs.tku.edu.tw 
Abstract. Interactive video browsing tools are designed for e-learning applica-
tions on future interactive TVs. The integrated system includes an authoring 
tool that produces multi-paths videos and a playback tool that uses video track-
ing technology and a remote controller. The playback tool enables multi-modal 
interaction between the user and a multi-story video clip. Three types of hyper-
interactive controls are incorporated, which include a reference link of a video 
object to show supplementary information on the Web, a hyper link to enable 
hyper-video jumps, and a choice link for online answers to pre-designed ques-
tions. The underlying video is coded using the standard MPEG technology, 
with navigation information hidden in the user-defined data of MPEG. Thus, 
the default sequence of a hypervideo can also be presented using an ordinary 
video player. 
1   Introduction 
Interactive TV aims at giving full interactivity between the TV programs and the 
viewers. The viewers will be able to select needed video in a simple and easy way. 
The interface design and related issues has been under massive research and discus-
sions in recent years [1, 2, 3]. On the other hand, hypervideo and multi-story video 
are also an important video technology under development [4, 5, 6, 7]. Video not only 
can be played in a linear sequence, but also can have multiple choices at certain 
points in the video. The story of a movie can progress in different ways and have 
different endings chosen by the viewer. An object in the video can be annotated with 
extra information in the form of a text file, an image, another clip of video, or a Web 
page. As long as the viewer triggers the reference link associated with it, the informa-
tion will reveal. It would be very useful to integrate the hypervideo technology into 
interactive TVs to further enhance the interactivity.  
How the reference link in a video can be triggered is an interesting issue. It is 
quite natural to do it by clicking a mouse, just like what is usually done on the World 
Wide Web. However, a mouse seems not well suited in the living room. People are 
 
Fig. 2. The authoring tool of interactive-hyper video 
The basic element of hypervideo is a video clip. A hypervideo is composed of 
several video clips. In our digraph-structured hypervideo, every node represents a 
video clip marked by the producer. The root node is the starting point of the hyper-
video and is marked with a red square in the authoring tool. The red line between two 
nodes represents a video hyperlink between the two video clips. The audience can 
activate the hyperlink in a specified temporal-spatial domain to jump from one video 
clip to another. Nodes can have more than one branch. So the user can decide which 
video clip is the target via the multi-modal interaction. One of the hyperlinks of each 
node is set as the default link. If the audience does not make any choice, the video 
playing will proceed to the video clip with the default link. 
Figure 3 shows the result of a hypervideo structure. The producer can change the 
linearly played video sequence into a digraph-structured hypervideo. The audience 
can select the video clip he/she wants to watch by triggering a reference link. If they 
do not like the selected clip, they can go back to the parent video clip to choose an-
other video hyperlink. The producer can also put some extra information to describe 
certain objects in the video clip. By the authoring tool, the producer can add text 
descriptions, existing image files, webpage files or URLs on the Internet to give more 
information to the audience.  
 
 
 
Fig. 3. A linear sequence versus a directed graph-like structure 
The video produced by the hypervideo authoring tool can also be played in other 
video players. Other players will play the default sequence of the digraph structure. 
The annotation added by the authoring tool will be ignored. 
4   Multi-Modal Interaction 
Mouse clicking is natural to computer users on the WWW environment. Hypervideo 
is an extension of the idea of hypertext in Web pages to video. But here we look for 
other ways of interactions for future interactive TV applications. Under the scenario, 
people might not be used to mouse clicking. Thus, two other modes of interactions 
are introduced: 1. interaction with a remote controller, 2. interaction with camera-
captured hand gestures. 
4.1   Interaction with a remote controller 
The first recommended device is a remote controller for TV. Most people are used to 
pressing buttons of a remote controller. However, current design of a TV remote 
controller needs to be enhanced to incorporate certain functions of the hypervideo 
player. A few buttons are redefined for such a purpose. The workflow of the remote 
control is shown in Figure 5. In order to simulate the TV on a computer screen, an IR 
(infrared rays) receiver is added to the COM port to receive the IR signal from the 
remote controller. The IO fetch component receives data from the COM port and 
translates the data for the hypervideo player. Figure 6 are the IR receiver and the 
redefined remote controller. 
 
 
Fig. 5. The workflow of the remote control for the hypervideo player 
 
1. Compute the difference of lightness in each pixel between the current frame and 
the former frame. 
2. If the result is greater than the threshold, the value is set to 255. Otherwise, the 
value is set to 0.  
This can be expressed by the following equation. 
  

 >−=
Otherwise   0
  |),(),(|if    1
),(
TyxByxI
yxP jjj
With an adequate threshold, we can get results of separating the foreground object 
from a still background.  
 
Next we need to reduce the noise shown with the foreground object. The median 
filter and the closing operation are used. Here, the two techniques are briefly intro-
duced. 
Median filter: All the pixels values (in gray-level) in an N*N mask are arranged in a 
sequential order (from the smallest to the largest), then the middle 
value is selected from the ordered set to replace the value of the cen-
tral pixel. 
Closing operation: The closing operation performs the dilation followed by the ero-
sion operation. Usually, it is used to fill in small holes or gaps 
and connect object’s fragments. 
By using these techniques, we can remove the redundant noise and get the hand area 
more precisely. Figure 8 shows the hand area after removing noise.  
 
 
noise   object      
         (a)                                        (b) 
Fig. 8. Result of noise reduction (a) before and (b) after the noise reduction 
Illumination change and background change are the two factors for getting an in-
accurate background image. Without an accurate background, the foreground object 
cannot be separated. So, a dynamic background update mechanism is necessary. The 
major component of background adaptation is to calculate the difference between the 
current frame and background image. If the difference exceeds a threshold, the back-
ground image must be updated. 
In this system, we use the number of on-off times of each hand object to deter-
mine if it is folding or unfolding. The procedure of determining on-off times is as 
follow: 
1. Scan the source image horizontally from the top to the bottom with an interval of 
two pixels until whole image is scanned. 
event of mouse clicking. So if there is a hyperlink in the area, it will be activated by 
the action. 
5. Conclusions 
In this research we integrated the hypervideo structure into the interactive TV frame-
work. With the developed multi-modal interaction tools, the viewer can sit back and 
relax on a couch to learn an e-learning application with hypervideo content and enjoy 
the interactivity easily. The interactivity of the interactive TV is enhanced with the 
hypervideo. 
 
References 
 
1. Bing J., Dubreuil J., Espanol J., Julia L., Lee M., Loyer M., Serghine M., 
“MiTV: rethinking interactive TV,” in Proceedings Seventh International Con-
ference on Virtual Systems and Multimedia Oct. 25-27, 2001  
2. Liang-Jie Zhang, Jen-Yao Chung, Lurng-Kuo Liu, Lipscomb J.S., Qun Zhou, 
“An integrated live interactive content insertion system for digital TV com-
merce,” in Proceedings Fourth International Symposium on Multimedia Soft-
ware Engineering, Dec. 11-13, 2002. 
3. Cesar P., Vierinen J., Vuorimaa P., “Open graphical framework for interactive 
TV,” in Proceedings Fifth International Symposium on Multimedia Software 
Engineering, Dec. 10-12, 2003. 
4. Chang, H.-B., H.-H. Hsu, Y.-C. Liao, T. K. Shih, and C.-T Tang, “An Object-
Based HyperVideo Authoring System,” in CD-ROM Proceedings of the Int’l 
Conf. on Multimedia Expo, June 28-30, 2004. 
5. Yoshiaki Hada, Hiroaki Ogata, and Yoneo Yano, “XML-based Video Annota-
tion system for Language Learning Environment,” in Proceedings of the Sec-
ond International Conference on Web Information Systems Engineering, Vol. 1, 
Dec. 3-6, 2001. 
6. Correia, P.L. and Pereira, F, “Objective evaluation of video segmentation qual-
ity,” in  IEEE Transactions on Image Processing, Vol. 12, Issue 2, Feb. 2003. 
7. Nitin Sawhney, David Balcom, and Ian Smith, “Authoring and navigating video 
in space and time,” in IEEE Multimedia, Volume 4, Issue 4, Oct.-Dec. 1997. 
8. Jim Taylor, DVD Demystified (2nd edition), McGraw-Hill Professional, Dec. 
2000. 
9. Oka, K., Sato, Y., and Koike, H., “Real-time tracking of multiple fingertips and 
gesture recognition for augmented desk interface systems,” in Proceedings of 
the Fifth IEEE International Conference on Automatic Face and Gesture Rec-
ognition, May 20-21, 2002 
10. Yang Liu and Yunde Jia, “A robust hand tracking and gesture recognition 
method for wearablevisual interfaces and its applications,” in Proceedings of 
the Third International Conference on Image and Graphics, Dec. 18-20, 2004. 
