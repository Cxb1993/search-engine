 2
 
中文摘要： 
平台式系統晶片設計(platform-base SoC design)已成為現今集電腦、通訊與多媒體
功能於一身之嵌入式系統設計主流。基於對支援多媒體行動通訊之要求與日俱增，可
攜式系統之耗電問題儼然已成為最迫切需解決的問題之一，而大部分多媒體與通訊方
面的應用程式(如影像的壓縮與解壓縮，語音辨識等等之應用程式)，皆需大量記憶體，
因此，能夠顧慮耗電量的記憶體架構設計(memory hierarchy design)，對省電的嵌入式
系統設計是一個重要的關鍵。在這三年的計劃中，我們每年分別針對記憶體階層中的
各主要階層：快取記憶體(cache)、主記憶體(main memory)，與次要儲存設備(secondary 
storage)，發展適合用於多媒體與通訊行動裝置之創新節能(energy-efficient)機制。 
在第一年的計劃中，我們針對以快閃記憶體為次要儲存設備之嵌入式系統，設計
一省電之虛擬記憶體系統(energy-efficient virtual memory system)，以達到省電及延長快
閃記憶體的壽命之目的。在第二年的計劃中，我們則是針對快取記憶體，發展一套適
用於多功即時環境(multitask real-time system)之快取記憶體漏電管理機制(leakage 
control mechanism)，這套機制可在滿足應用程式對即時之要求下，同時有效減少快取
記憶體之漏電量。在第三年的計畫中，我們則針對動態記憶體實作之主記憶體(DRAM- 
based main memory)，提出一低功率機制，此機制同時考量主記憶體的效能、功率與溫
度之間的交互作用，使主記憶體能在符合效能的需求下，達到省電與降低溫度的目的。 
 
 
 
 
 
關鍵字詞：平台式系統晶片設計、記憶體架構設計、節能設計、快取記憶體、主記憶
體、快閃記憶體、次要儲存設備
目錄 
 
中文摘要： ................................................................................ 2 
英文摘要： ................................................................................ 3 
一、 研究計畫之背景與目的： ........................................... 5 
二、 研究方法與成果： ....................................................... 6 
三、 參考文獻： ................................................................. 27 
四、 計畫成果自評： ......................................................... 29 
 
 6
HotCache, subpaging and duplication-aware garbage collection, to reduce writes to flash 
memory. The details of the proposed mechanism and the experimental results are discussed 
in Section I First Year: Energy-Aware Flash Memory Management in Virtual Memory 
System. 
Cache – Timing-Aware Cache Leakage Control (TACLC): To reduce cache leakage 
for a real-time system, we developed a time-aware cache leakage control mechanism. Most 
of the multimedia and communication applications running on a mobile device have 
real-time requirements. However, previously proposed cache leakage control mechanisms 
[3][4] all introduce performance un-predictability thereby not suitable for hard real-time 
systems. Therefore, we proposed TACLC, which can effectively reduce cache leakage 
without violating timing constraints. Moreover, we also looked into the issue of integrating 
TACLC with existing combined CPU DVS (Dynamic Voltage Scaling) and shutdown 
techniques for hard real-time systems. The details of the proposed TACLC mechanism and 
the experimental results are discussed in Section IISecond Year: Cache Leakage Control 
Mechanism for Hard Real-Time Systems. 
Main Memory – Joint Performance, Power and Thermal Management 
Framework: Due to the interplay among performance, power and thermal, a technique 
optimized for only one factor often causes uncontrollable degradation at other design 
metrics. To tackle this problem, in this project, we proposed a joint performance, power 
and thermal management framework (PPT) that orchestrates task execution and page 
allocation to achieve desirable tradeoff among these factors. To adapt to system loading, the 
PTT framework adjusts the grouping granularity dynamically to meet the user-specified 
performance requirement while minimizing power consumption. The details of the 
proposed PTT framework are discussed in Section III Third Year: Joint 
Performance/Power/Thermal Management of DRAM Memory System on Multi-Core 
Architecture.  
 
二、 研究方法與成果： 
In this section, we discuss the low-power techniques we proposed in each year of the 
project. 
I. First Year: Energy-Aware Flash Memory Management in Virtual 
Memory System 
Recently, flash memory becomes a popular storage alternative for many portable 
devices with the continuing improvements on its capacity, reliability and much 
lower power consumption than mechanical hard drives. The characteristics of flash 
 8
dirty blocks in a victim page assuming a 512B block, and 4K virtual memory page. 
Therefore, writing a full victim page to flash memory is not energy efficient. 
Subpaging divides a virtual memory page into a set of subpages. We divide a 
page in the granularity of flash page size. Each subpage is associated with a dirty 
bit. On a page fault, only dirty subpages are written into flash memory. Park et 
al. [6] proposed a new replacement policy CFLRU (Clean First Least Recently 
Used) to reduce writes to flash memory by keeping dirty pages in memory as long 
as possible. Although this method reduces the energy consumption of flash 
memory effectively, it could incur more page faults. In contrast, the subpaging 
technique reduces writes to flash memory without increasing page faults. 
 
ii. HotCache: 
To reduce writes to flash memory, we propose to keep frequent writes in an 
SRAM, which is referred to as HotCache in this project. HotCache is organized 
as a fully-associative cache with the HotCache block size equal to the page size of 
flash memory. Tree new policies are investigated for the HotCache management: 
Time-Frequency (TF): 
In the TF policy, every write request is cached in HotCache. The replacement is 
based on the weight function: timestamp × write_counts. The HotCache block with 
the smallest weight is selected as the victim when a replacement occurs. This 
policy considers both the time and frequency factors. The advantage of this TF 
policy over the traditional LRU is that it prevents a hot page from being replaced 
by a recently accessed cold page2. 
Time-Frequency-Locality (TFL):  
TF policy does not guarantee the flash pages of a virtual memory page to be 
allocated in the same block since they may not be replaced out in sequence. To 
avoid destroying intra-page locality, we enhance the TF policy by forcing all 
the pages of the same virtual memory page to be replaced in sequence. For 
more implementation detail, please refer to [2]. The enhanced TF policy is called 
TF-locality (TFL). 
Two-Level LRU(2L): 
Different from the TF and TFL policies, the two-level LRU policy observes a page 
for a period of time to determine whether this page should be allocated in HotCache 
or not. Similar to hot/cold data separation policy proposed by Chang et al. [7], we 
use the first-level LRU list to record the pages considered as hot data, and the 
                                                 
2 Hot (cold) pages are those frequently (rarely) accessed. 
 10
 
Figure 2 Duplication-Aware Garbage Collection 
 
2. Experimental Results: 
To evaluate the proposed schemes, we adopt a trace-driven simulation. Our 
simulator contains a main memory paging system and flash storage with an SRAM. 
We use Valgrind [8] on an x86-linux machine to collect memory traces. The 
applications we tested in this study are: kword, a word processor; mozilla, a web 
browser; kspread, a spreadsheet application; openoffice, a popular office suite 
similar to Microsoft office; gqview, a image viewer. We also create 
multi-programming workloads by running juk, an MP3 jukebox program, with 
applications described above. The OS adopts the round-robin scheduling policy. We 
assume the physical memory allocated to user programs is 16 MB, and the main 
memory page size is 4 KB. The virtual memory is managed by the LRU policy. For 
flash memory, we assume a 16 KB block with 512 B page size or a 128 KB block 
with 2 KB page size. We adopt the cost-benefit policy [7] as our garbage collection 
policy.  
Figure 3 shows the combined effect of HotCache, Subpaging, and DA-GC. In this 
set of results, we assume 1 MB HotCache managed in the TFL policy, and 512 KB 
flash pages. We also compare our scheme with the work CFLRU [10], which 
proposed a new virtual memory replacement policy to reduce writes to flash 
memory. Since CFLRU could incur more page faults, we plot the number of page 
faults of CFLRU normalized to the baseline architecture in Figure 3. The 
experimental results show that the energy reduction of adopting a 1 MB HotCache, 
 12
high performance and energy overhead to save register values, restore cache 
contents, etc. For example, the Transmeta processor implemented in 70nm 
technology incurs 483μJ energy overhead and 2.01ms performance overhead, which 
is 1.005 million cycles with 500MHz processor frequency [13][14]. When available 
slack is not enough for shutting down the processor, we can further slow down the 
CPU speed to reduce dynamic energy [17]. Current processors only provide a set of 
discrete voltage/frequency levels. For example, Intel XScale [18] provides processor 
speeds of 150Mhz, 600Mhz and 800Mhz. Adjusting the processor speed to the next 
level could incur significant performance degradation. Therefore, by using existing 
combined CPU DVS (Dynamic Voltage Scaling) and shutdown techniques only, a 
hard real-time system is very likely to have unutilized slack. This slack could be 
further utilized to reduce leakage if architectural-level shutdown techniques are 
adopted.   
Unfortunately, existing architectural-level shutdown techniques often introduce 
unpredictable performance degradation [3][4] thereby not suitable for hard real-time 
systems. In the second year of the project, we proposed a Timing-Aware Cache 
Leakage Control (TACLC) mechanism, which is the first cache leakage control 
mechanism designed for hard real-time systems. The proposed TACLC mechanism 
achieves leakage savings with hard real-time guarantee through exploiting system 
slack to tolerate delay caused by accessing low-leakage cache lines. TACLC allows 
the joint use of low-leakage cache techniques, Drowsy Cache [3] or Gated-Vdd 
[10], and exploits task-level information to manage cache lines of idle and active 
tasks differently. In the following subsections, we discuss the proposed TACLC 
mechanism and how it cooperates with existing low-power technique designed for 
hard real-time systems, and more details can be found in [19]. 
 
1. Timing-Aware Cache Leakage Control (TACLC) 
The objective of TACLC is to maximize cache leakage reduction provided that the 
timing constraint is not violated. To achieve this, TACLC utilizes available slack to 
put cache lines into low-leakage modes. The proposed TACLC scheme is composed 
of off-line and on-line phases. Through off-line analysis, TACLC obtains 
information to estimate the worst case wake-up delays in a drowsy window (Worst 
Case Active Set Analysis). At run time, we dynamically determine the drowsy 
window size based on the available slack. TACLC manages cache lines allocated to 
active and idle tasks separately. For cache lines allocated to an active task, due to 
short idle period between accesses, only the Drowsy Cache technique is applied. For 
 14
 
Figure 4 Overview of the WCAS Drowsy Window Resizing Method 
 
PAS-based Drowsy Window Resizing:  
The Predictive Active Set (PAS) method is a history-based method that assumes 
the active set of the new window is the same as that of the previous one. The 
PAS method is also the new drowsy window resizing technique developed in the 
third year of the project. With the prediction method, we may under-estimate the 
active set size of the new window. Therefore, to prevent deadline violation, if 
available slack is not enough to tolerate the worst case wake-up overheads, the PAS 
method will not start a new drowsy window; that is, all the cache lines of the current 
task remains active. In the PAS method, in addition to context switches or a change 
of the active set size, the drowsy window resizing is invoked when the predicted 
active size of the next window is different from the previous one. In the extreme 
case, we may need to perform drowsy window resizing every window. If we 
calculate the drowsy window size based the method used in the WCAS method, two 
divide operations are required. With potentially high window resizing frequency, 
this method may degrade energy reduction since caches are not put into the drowsy 
mode during divide operations which could be up to 32 cycles [20]. Therefore, to 
facilitate efficient window size calculation, we propose a simple approximation 
method to efficiently derive the window size. According to the inequality discussed 
in the WCAS method, we observe that the ratio of two active set sizes is the same as 
the ratio of two drowsy window sizes when the value of remaining worst case 
executing time and available slack are fixed. Therefore, the drowsy window size of a 
 16
and leakage reduction achieved by Gated-Vdd and Drowsy Caches. We choose the 
low-leakage mode with the most leakage reduction while meeting the timing 
constraint as the leakage mode of an idle task. For more details, please refer to [19] 
or the mid-term report of the second year of the project.  
 
2. How to Combine TACLC with Existing CPU DVS & Shutdown Techniques 
In this section, we discuss how to integrate the proposed TACLC with the existing 
combined CPU DVS and shutdown techniques for a hard real-time system, 
Simulate-Scheduling Procrastination (SS-Procrastination) [21], which shows the 
most energy savings in literature. In SS-Procrastination, dynamic energy 
consumption is first minimized by executing real-time tasks/jobs at speeds no lower 
than the critical speed, which is the speed that minimizes the execution energy of a 
cycle. The speed of each task is determined by applying existing DVS algorithms, 
e.g., [22] and [23]. Then, the procrastination scheduling technique is adopted at 
run-time to aggregate scattered idle periods into a longer idle interval to turn the 
processor off when there is no job for execution. In SS-Procrastination, simulated 
scheduling that is based on virtual EDF schedule is performed periodically to decide 
the schedule of jobs executing in an interval and when to turn off the processor. 
However, the processor may have some idle periods that are not long enough for 
turning on/off the processor. In such a case, SS-Procrastination further reduces the 
energy consumption by executing jobs at lower speeds, maybe even lower than the 
critical speeds.  
As described earlier, turning on/off processors or reducing CPU speed to the next 
level both incur significant performance overheads. So, unutilized slack could be 
used by the proposed TACLC scheme. Figure 6 shows how to combine TACLC 
with SS-Procrastination. The first half of Figure 6 shows the major steps in 
SS-Procrastination. After performing simulated scheduling, we know how much 
available slack is. If available slack is long enough for shutting down CPU or 
slowing down to the next level of voltage/frequency, jobs will execute without 
applying the TACLC scheme. Otherwise, we will utilize this slack to put cache lines 
into the low-leakage mode. Each idle period is first distributed to jobs that have 
deadlines after the idle period. After slack distribution, the TACLC method is then 
invoked. Simulated scheduling is performed again at the end of the current interval 
or when the arrival time of an idle task’s next instance is beyond the range covered 
by this interval. Recall that in TACLC, we put the cache lines of an idle task into the 
low-leakage mode if the slack allocated to this task’s next instance is long enough to 
 18
 
Figure 7 Evaluation of Leakage Reduction achieved by TACLC and Drowsy+Simple with Small Task Set 
 
Figure 8 Evaluation of Leakage Reduction achieved by TACLC and Drowsy+Simple with Medium Task Set 
  
 
Table 3 Comparison of Leakage Savings achieved by TACLC with Drowsy Caches only and TACLC 
with Drowsy Caches and Gated-Vdd 
 
Figure 7 and Figure 8 shows the leakage reduction achieved by TACLC and 
Drowsy+Simple with small and medium task set, respectively. When available slack 
is 3% and 6%, where Drowsy+Simple has tasks missing their deadlines for both the 
small and medium task set, in order to satisfy the timing constraint, TACLC 
achieves less energy savings than Drowsy+Simple. The experimental results show 
that TACLC with PAS performs much better than WCAS due to more accurate 
estimation of the active set size. Table 3 shows the effect of turning off cache lines 
 20
Only one group is active at each scheduling interval. The memory modules of 
non-active groups could be turned into the low-power mode to save energy. 
Alternating active groups periodically allows memory modules to cool down in 
their idle periods as shown in Figure 10(b). The grouping granularity (i.e., number 
of memory modules in each group) affects both the performance and power. Larger 
grouping granularity exposes more memory access parallelism at the cost of losing 
opportunity to turn off memory modules.  
 
Figure 9: Partitioning T threads and M memory modules into N groups  
 
 
Figure 10: Group scheduling in round-robin fashion 
 
There are three main design decisions in the PPT framework. They are group 
switch interval, thread assignment policy and grouping granularity. To achieve 
power saving and prevent memory modules from overheating, the group switch 
interval needs to be longer than the break-even time of DRAM low-power mode, 
and short enough to prevent overheating. (With the current DDR2 technology, it 
typically takes tens of seconds to reach the temperature threshold [27], and its 
break-even time is roughly several microseconds). In conventional operating 
systems, the context switch interval is about 1 millisecond. Therefore, the group 
switching interval could be equal to the context switch interval. The thread 
 22
pages of T(A) could be in M(B), and vise versa. However, due to the temporal 
locality of data accesses, after a period of time, the accessing pattern will gradually 
conform to the new grouping granularity.  
We define a variable called response time emergency level to trigger group merging 
and splitting. For available number of groups, 1 to 2n, based on the user-specified 
response threshold, we define n+1 response time emergency levels, L0 to Ln with n 
response time thresholds. Adaptive grouping may merge or split the groups 
according to the current average response time emergency level. In the highest 
emergency level, the number of group is set to 1. The emergency levels are defined 
as following:  
groups 2 :c*n - T > t  c*1)-(n - T :Ln
groups 4 :2c - T > t  c - T :L2
groups 2 :c - T > t  T :L1
group 1 :T > t :L0
n≥
≥
≥
 
Eq.(1) 
, where t is the average response time, T is the pre-defined response time threshold, 
and c is the interval width as a constant. Adaptive grouping mechanism merges or 
splits the groups to 2k group in state Lk. For example, when the average response 
time reaches T - c, the groups are merged into 2 groups. On the other hand, when 
the average response time drops to less than T - c, the groups are split to 4 groups. 
 
2. Experimental Setup 
To evaluate the proposed PPT framework, we adopt a trace-driven simulation 
environment to perform our experiments. The proposed trace-driven simulation 
infrastructure is as shown in Figure 11. The traces are gathered by running Valgrind 
[28] on AMD Opteron 2.4GHz dual-core PC. We also incorporate DRAMsim [29] 
to accurately model performance and power consumption of DDR2-SDRAMs. For 
temperature evaluation, we use HotSpot 3.0 [30]. The workloads we used to 
evaluate the proposed PPT framework are 13 applications selected from the 
SPEC2000 benchmark suit [31]. The selected applications all have the 
characteristics of high memory bandwidth requirement and large memory footprint.  
 
 24
average response time with 9.2W average power consumption. The 8-group setting 
reduces 45.9% power consumption of 1-group setting.  
 
Figure 12: Average Response Time and Power Consumption of Static Grouping Method with 1, 2, 
4 and 8 Groups 
 
To show the effect of the proposed adaptive grouping mechanism, we set the 
response time requirement as 140ns and define 4 emergency levels per 5ns (T = 
140ns, c = 5ns and n = 4 in Eq.(1)). Figure 13 shows the comparison of response 
time and power consumption with adaptive grouping mechanism and static 
grouping mechanism with 4/8-group settings. We can observe that the adaptive 
grouping mechanism successfully maintain the average response time lower than or 
equal to 140ns. During the period that the 8-group setting violates the response time 
requirement, the power consumption of adaptive grouping is higher than that of 
4-group setting but lower than that of 8-group setting as shown in Figure 13.  
 
Figure 13: Average Response Time and Power Consumption of Adaptive Grouping and Static 
Grouping Method with 4/8 Groups 
 
We next compare the proposed PPT scheme with three baseline page allocation 
polices, conventional, performance-driven and power-driven page allocation, in 
power, performance and thermal aspects. In this set of experiments, the 
configuration of PPT with adaptive grouping is the same as the one used for 
 26
Figure 16 shows the peak temperature of all ranks in the memory system. We 
observe that PPT achieves the lowest peak temperature. The power-driven method 
results in significantly higher peak temperature than PPT (up to 14.4∘C). PPT is 
better than the power-driven page allocation because PPT changes the hot rank 
periodically to prevent overheating. Based on the thermal-aware group switch 
interval, PPT can prevent memory overheating without sacrificing the power saving. 
Moreover, PPT is also better than the performance-driven and conventional page 
allocation because PPT reduces power consumption and power density, so that the 
peak temperature decreases. 
 
 
Figure 16: The peak temperature of three baseline mechanism and PPT 
 
From the experimental results, we observe that PPT provides good tradeoff between 
average response time, power and peak temperature. PPT is the best mechanisms 
for joint performance, power, and thermal management. To understand the tradeoff 
between all mechanisms, we provide the average response time, normalized power 
consumption and peak temperature of all mechanisms in Table 4. 
 
Table 4 Average Response Time, Normalized Power and Peak Temperature 
 
4. Conclusion and Publication:  
In the third year of the project, we proposed a joint performance, power and thermal 
(PPT) management mechanism to orchestrate task execution and page allocation 
among tasks to achieve desirable tradeoffs between performance, power and 
temperature of main memory subsystem. We also built a trace-driven simulation 
infrastructure for evaluating the mechanism on multi-core processor with 
DDR2-SDRAM memory. Our results show that PPT can prevent overheating 
 28
embedded systems. In Proceedings of the 42nd annual conference on Design automation, 2005.  
[14] R. Jejurikar, C. Pereira, and R. Gupta. Leakage aware dynamic voltage scaling for real-time embedded 
systems. In Proc. the 41st Design Automation Conference (DAC ’04), 2004.  
[15] P. Juang, K. Skadron, M. Martonosi, Z. Hu, D. W. Clark, P. W. Diodato, and S. Kaxiras. Implementing 
branch-predictor decay using quasi-static memory cells. ACM Transactions on Architecture and Code 
Optimization (TACO), Vol.1. 
[16] N. S. Kim, T. Austin, D. Blaauw, T. Mudge, K. Flautner, J. S. Hu, M. J. Irwin, M. Kandemir, and V. 
Narayanan. Leakage current: Moore’s law meets static power. IEEE Computer, 36.  
[17] J.-J. Chen and T.-W. Kuo. Procrastination determination for periodic real-time tasks in leakage-aware 
dynamic voltage scaling systems. In Proceedings of the 2007 IEEE/ACM international conference on 
Computer-aided design (ICCAD ’07), 2007.  
[18] Intel. Corp. Intel xscale core developer manual. In 
http://download.intel.com/design/intelxscale/27347302.pdf. 
[19] J.-W. Chi, Y.-J. Chen, and C.-L. Yang, “Cache Leakage Control Mechanism for Hard Real-Time 
Systems”, in Proceedings of International Conference on Compilers, Architecture, and Synthesis for 
Embedded Systems (CASES '07), Saizburg, Austria, September, 2007. 
[20] Cortex-R4F. http://www.arm.com/pdfs/cortex-r4f.  
[21] J.-J. Chen and T.-W. Kuo. Procrastination determination for periodic real-time tasks in leakage-aware 
dynamic voltage scaling systems. In Proceedings of the 2007 IEEE/ACM international conference on 
Computer-aided design (ICCAD ’07), 2007  
[22] H. Aydin, R. Melhem, D. Moss´e, and P. M. Alvarez. Determining optimal processor speeds for 
periodic real-time tasks with different power characteristics. In Proceedings of the 13th Euromicro 
Conference on Real-Time Systems, 2001.  
[23] F. Yao, A. Demers, and S. Shenker. A scheduling model for reduced cpu energy. In Proceedings of the 
36th IEEE Symposium Foundations of Computer Science, 1995  
[24] Y. Zhang, D. Parikh, K. Sankaranarayanan, K. Skadron, and M. Stan. Hotleakage: A 
temperature-aware model of subthreshold and gate leakage for architects  
[25] Snu real-time benchmarks. In http://archi.snu.ac.kr/realtime/benchmark/index.html.  
[26] D. Brooks, V. Tiwari, and M. Martonosi. Wattch: A framework for architectural-level power analysis 
and optimizations. In Proceedings of the 27th annual international symposium on Computer 
architecture (ISCA’ 00), 2000. 
[27] J. Lin, H. Zheng, Z. Zhu, H. David, and Z. Zhang. Thermal modeling and management of dram 
memory system. The 34th International Symposium on Computer Architecture, 2007. 
[28] C. Armour-Brown, K. Fitzhardinge, T. Hughes, N. Nethercote, P. Mackerras, D. Mueller, J. Seward, R. 
Walsh, and J. Weidendorfer. Valgrind. http://valgrind.org/, 2000-2007.  
[29] D.. Wang, B. Ganesh, , and B. Jacob. The university of maryland memory-system simulator. HTTP: 
http://www.ece.umd.edu/dramsim/, 2006.C. Armour-Brown, K. Fitzhardinge, T. Hughes, N. Nethercote, 
P. Mackerras, D. Mueller, J. Seward, R. Walsh, and J. Weidendorfer. Valgrind. In http://valgrind.org, 
2000~2007 
[30] K. Skadron, M. R. Stan, W. Huang, S. Velusamy, K. Sankaranarayanan, and D. Tarjan. 
Temperature-aware microarchitecture. Proceedings of the 30th International Symposium on Computer 
Architecture, pages 2-13, 2003. 
[31] S. P. E. Corporation. Spec2000. http://www.spec.org, 2000.  
出席國際學術會議報告 
 
報 告 人 
姓 名 楊佳玲 
服 務 機 構
及 職 稱
台 灣 大 學 資 工 系
副教授 
會 議 時 間 
地 點 
2008/6/8 – 6/13  
會議名稱 45rd Design Automation Conference 
發表論文題目 A Progressive-ILP Based Routing Algorithm for Cross-reference 
Biochips  
 
一、參加會議經過 
 
 Design Automation Conference為當今最著名之國際設計自動化研討會。 本
人很榮幸發表論文於45rd  DAC，故於六月赴美國參與 DAC。 
 
此次會議，涵蓋在 design automation 之主要課題，從 physical design 到 
system-level design，共有 54 sessions。本人於 Session 16 : Emerging 
Nano/Biotechnologoes 發表論文 “A Progressive-ILP Based Routing Algorithm for 
Cross-reference Biochip”，此篇論文之發表於會中廣獲好評。 本人除參與論文發表外，
並和與會研究人員進行意見交流，獲益良多。 
 
二、與會心得 
本人於此次會議中穫益甚多，尤其在3D IC design and ESL Design methodology 兩
領域。在 3D IC 設計方面，vertical stacking 可以縮減在 nano-technology 下 wire 
delay 造成之 performance hit ，但 thermal 的問題須被小心處理。在 ESL design 
methodology 領域 ，MPSOC (Multi-processor SOC) 是一個挑戰，如何在此平台上發
展軟體， 是個困難的問題 ， interconnection and memory system optimization 也
非常重要 。 
 
A Progressive-ILP Based Routing Algorithm for
Cross-Referencing Biochips ∗
Ping-Hung Yuh1, Sachin Sapatnekar2, Chia-Lin Yang1, Yao-Wen Chang3
1Department of Computer Science and Information Engineering, National Taiwan University
2Department of Electrical and Computer Engineering, University of Minnesota
3Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University
{r91089, yangc}@csie.ntu.edu.tw; sachin@umn.edu; ywchang@cc.ee.ntu.edu.tw
ABSTRACT
Due to recent advances in microﬂuidics technology, digital
microﬂuidic biochips and their associated CAD problems
have gained much attention, most of which has been de-
voted to direct-addressing biochips. In this paper, we solve
the droplet routing problem under the more scalable cross-
referencing biochip paradigm, which uses row/column ad-
dressing scheme to activate electrodes. We propose the first
droplet routing algorithm that directly solves the problem of
routing in cross-referencing biochips. The main challenge of
this type of biochips is the electrode interference which pre-
vents simultaneous movement of multiple droplets. We ﬁrst
present a basic integer linear programming (ILP) formula-
tion to optimally solve the droplet routing problem. Due to
its complexity, we also propose a progressive ILP scheme to
determine the locations of droplets at each time step. Exper-
imental results demonstrate the eﬃciency and eﬀectiveness
of our progressive ILP scheme on a set of practical bioassays.
Categories and Subject Descriptors
B.7.2 [Integrated Circuits]: Design Aids
General Terms
Algorithm, Performance, Design
Keywords
Microﬂuidics, biochip, routing, progressive-ILP
1. INTRODUCTION
Recently, there have been many signiﬁcant advances in mi-
croﬂuidic technologies [7]. Microﬂuidic biochips show numerous
advantages over conventional assay methods, including portabil-
ity and sample/reagent volume reduction, and oﬀer a platform for
developing clinical and diagnostic applications, such as health-
care of infants and point-of-care diagnostics of disease. This
breadth of applicability implies that microﬂuidic biochips are in-
creasingly used in laboratory procedures in molecular biology.
Lately, a new type of biochips, which are based on digital-
izing continuous liquid ﬂow into discrete liquid particles, called
∗
This work was partially supported by the National Science Coun-
cil of Taiwan under Grant No’s. NSC 96-2221-E-002-250, NSC 96-
2752-E-002-008-PAE, NSC 96-2628-E-002-248-MY3, NSC 96-2628-E-
002-249-MY3, NSC 96-2221-E-002-245, and by the Excellent Research
Projects of National Taiwan University, 96R0062-AE00-07.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
DAC 2008, June 8–13, 2008, Anaheim, California, USA.
Copyright 2008 ACM 978-1-60558-115-6/08/0006 ...$5.00.
droplets, have been proposed [5]. These type of biochips are more
suitable for large-scale and scalable systems due to their recon-
ﬁgurability. Each droplet can be independently controlled by the
electrohydrodynamic forces generated by electrodes. In this way,
droplets movement can be controlled by a system clock. Due to
this parallel with digital electronic systems, this type of biochips
is referred to as “digital microﬂuidic biochips.”
Typically, a digital microﬂuidic biochip has a 2D microﬂuidic
array, which consists of a set of electrodes for fundamental op-
erations, such as droplet mixing and transportation. In the sim-
plest and most common droplet control scheme, each electrode
is directly addressed and controlled by a dedicated control pin,
which allows each electrode to be individually activated. In this
paper, we refer to these types of digital microﬂuidic biochips as
direct-addressing biochips. While this architecture provides very
ﬂexibility for droplet movement, it suﬀers from the major draw-
back that the number of control pins rapidly increases as the sys-
tem complexity (i.e., the size of the array) increases. The large
number of control pins aﬀects product cost and the control wire
routing problem complicates the design process, and therefore,
this architecture is only applicable to small-scale biochips [9].
To overcome these limitations, recently a new digital microﬂu-
idic biochip architecture has been proposed [3]. This architecture
uses a row/column addressing scheme, where a set of electrodes
in one row/column is connected to a control pin. Therefore, the
number of control pins is greatly reduced: this number is now
proportional to the perimeter of the chip rather than the area of
the chip. We refer to this type of biochip architecture as cross-
referencing biochips. However, this architecture also introduces a
new set of limitations. Since an electrode can potentially control
the movement of all droplets in a row/column at the same time,
this architecture incurs higher droplet movement complexity than
that of direct-addressing biochips. Moreover, the manipulation
of more than two droplets causes electrode interference among
droplets, which prevents multiple droplets to move at the same
time. This performance limitation is a major drawback to high-
performance applications, such as large-scale protein analysis.
In this paper, we tackle the problem of droplet routing in cross-
referencing biochips. The main challenge of this routing problem
is to ensure the correctness of droplet movement; the ﬂuidic prop-
erty which avoids unexpected mixing among droplets needs to be
satisﬁed, and electrode interference patterns that prevent mul-
tiple droplets from moving at the same time must be avoided.
The goal of droplet routing is to minimize the maximum droplet
transportation time, and has several motivations. First, this min-
imization is critical to real-time applications, such as monitoring
environmental toxins. Second, the minimized droplet transporta-
tion time leads to shorter time a sample spent on a biochip, which
is desirable to maintain the bioassay execution integrity.
1.1 Previous Work
Droplet routing is a critical step in biochip design automation.
Previous routing approaches mainly focus on direct-addressing
biochips [4, 6, 10]. Recently, the problem of manipulating droplets
on a cross-referencing biochip has attracted some attention, but to
our knowledge, all existing methods begin with a direct-addressing
routing result, and transform it to a cross-referencing routing re-
Droplet
Deactivated cell Activated cell
Cell (x, y)
Cell (x+1, y)
(a) (b)
Figure 3: Modeling of electrode constraint when a
droplet moves (a) or stays at its original location (b).
trode constraint, which will result in longer droplet transportation
times. For example, in Figure 2, we can stall droplet 3 and move
the other two, and this will avoid electrode interference. The
manner in which electrode constraints are handled is a critical
routing consideration in cross-referencing biochips, and is impor-
tant in minimizing the droplet transportation time.
2.3 Problem Formulation
In this paper, we focus on the droplet transportation problem.
As stated in [6, 10], the droplet transportation problem can be
represented in a 3D space, where the third dimension corresponds
to time. At each time step, the problem reduces to working with
the droplet movement problem in a 2D plane. We focus the prob-
lem formulation on one 2D plane, noting that similar constraints
may be added for every other 2D planes. When routing droplets
on one 2D plane, we consider the modules (and the surrounding
segregation cells) that are active as obstacles.
Besides the electrode constraint, we also need to satisfy the
static and dynamic ﬂuidic constraints for correct droplet move-
ment [6]. The static ﬂuidic constraint states that the minimum
spacing between two droplets is one cell. Note that once electrode
constraint is satisﬁed, the dynamic ﬂuidic constraint is automat-
ically satisﬁed, since the neighboring cells of the destination cell
and the cell where a droplet is originally at will not be activated.
Therefore, we do not need to explicitly handle the dynamic ﬂu-
idic constraint on cross-referencing biochips. Besides the above
constraint, for practical bioassays, we must be able to handle 3-
pin nets to represent the fact that two droplets may have to be
merged during their transportation for eﬃcient mix operations [6].
Therefore, the droplet routing problem for each 2D plane can be
formulated as follows:
Input: A netlist of m nets N = {n1, n2, . . . , nm}, where each
net ni is a 2-pin net (one droplet) or a 3-pin net (two droplets),
and the locations of pins and obstacles.
Objective: Route all droplets from their source pins to their
target pins while minimizing the maximum time to route all droplets.
Constraint: Both fluidic and electrode constraints must be
satisfied.
3. ILP FORMULATION FOR DROPLET
ROUTING
In this section, we present the basic ILP formulation for one
2D plane. We show how the ILP optimizes droplet routing and
scheduling and voltage assignment on the electrodes, with the
consideration of 3-pin nets.
3.1 Basic ILP Formulation
One of the major challenges in the formulation of this problem
is in modeling the electrode constraint within an ILP. Figure 3
illustrates the electrode constraint. When a droplet moves from
cell (x, y) to cell (x+1, y) at time t+1, as shown in Figure 3 (a),
all neighboring cells of (x, y) and (x + 1, y) must be deactivated,
except for cell (x+1, y), which must be activated. Note that in the
case that a droplet stays at its original location, cell (x, y) does
not necessarily need to be activated and all the neighboring cells
of (x, y) must be deactivated, as demonstrated in [3]. Figure 3
(b) shows this case. Therefore, the electrode constraint can be
modeled by the following three rules:
1. If a droplet is at cell (x, y) at time t, then the four diagonally
adjacent cells cannot be activated at time t + 1.
2. If a droplet moves to cell (x′, y′) at time t+1, then the eight
neighboring cells of (x′, y′) cannot be activated at t + 1.
Table 1: Notations used in our ILP formulation.
Wˆ/Hˆ biochip width/height
T maximum droplet transportation time
N ′ set of 3-pin nets
D set of droplets
dij j-th droplet of net ni; j = {1, 2}
(si,jx , s
i,j
y ) location of the source of d
i
j
(tˆix, tˆ
i
y) location of the sink of net ni
(xi,jt , y
i,j
t ) location of droplet d
i
j at time t
C set of available cells
E(x, y) set of cell (x, y) and its four adjacent cells
E′(x, y) set of cell (x, y)’s four adjacent cells
Eˆ(x, y) set of available neighboring cells of (x, y)
E′x(x, y)/E
′
y(x, y) set of available cells adjacent to (x, y)
with the same x- (y-) coordinate
Dˆ(x, y) set of available diagonal cells of (x, y)
Tl maximum droplet transportation time
pij(x, y, t) a 0-1 variable to represent that droplet d
i
j
locates at (x, y) at time t
Lcx(t) a 0-1 variable represents that column x
is set to voltage low at time t
Hcx(t) a 0-1 variable represents that column x
is set to voltage high at time t
Lry(t) a 0-1 variable represents that row y
is set to voltage low at time t
Hry(t) a 0-1 variable represents that row y
is set to voltage high at time t
ax,yt a 0-1 variable represents that (x, y)
is activated at time t
mit a 0-1 variable represents that d
i
0 and d
i
1
are merged at time t
3. If a droplet is at cell (x, y) at time t, at most one cell can
be activated among cells (x, y) and its four adjacent cells.
Note that both rule #1 and #2 state that the cells diagonally
adjacent to cell (x, y) cannot be activated at time t + 1. This re-
dundancy reduces the size of the basic ILP formulation; otherwise,
we would need extra variables to represent the moving direction
of each variable pij(x, y, t) and extra constraints to determine the
values of these extra variables.
In the following sections, we introduce the objective function
and constraints of our basic ILP formulation. The notations used
in our ILP formulation are shown in Table 1.
3.1.1 Objective Function
The goal is to minimize the latest time a droplet reaches its
sink. Therefore, the objective function is deﬁned by the following
equation:
Minimize : Tl. (1)
3.1.2 Constraints
There are total eight constraints in our basic ILP formulation.
1. Objective function computation: If a droplet reaches its
sink at time t + 1, then the time it reaches its sink can be
computed as t + 1 times the diﬀerence of the two variables
pij(tˆ
i
x, tˆ
i
y , t + 1) and p
i
j(tˆ
i
x, tˆ
i
y , t). Therefore, the objective
function can be computed by the following constraint:
(t + 1)(p
i
j(tˆ
i
x, tˆ
i
y, t + 1)− pij(tˆix, tˆiy, t)) ≤ Tl, ∀dij ∈ D, 0 ≤ t < T. (2)
2. Source and sink requirements: We assume that at time
zero, all droplets are at their source locations. All droplets
must reach their sinks. Once a droplet reaches its sink, it
remains there. Therefore, the above requirements can be
represented by the following constraints:
p
i
j(s
i,j
x , s
i,j
y , 0) = 1, ∀dij ∈ D (3)
T−1∑
t=0
p
i
j(tˆ
i
x, tˆ
i
y, t) ≥ 1, ∀dij ∈ D (4)
p
i
j(tˆ
i
x, tˆ
i
y, t)− pij(tˆix, tˆiy, t + 1) ≤ 0, ∀dij ∈ D, 0 ≤ t < T. (5)
ing cells and their adjacent cells, the progressive ILP does need
not to consider the eﬀects of activated cells outside the 5 × 5 re-
gion around a droplet. To satisfy the static ﬂuidic constraint, we
observe that after a droplet moves, two droplets are adjacent to
each other only if a cell with more than one droplets nearby is ac-
tivated. Therefore, we do not activate this cell in our progressive
ILP formulation. Finally, each droplet dij can either move to one
of its four adjacent cells, or stay at its original location. That is,
the possible position of a droplet can be represented as a ”cross”
shown in Figure 5.
Therefore, we only need variables ax,yt+1 to represent the cell
(xi,jt , y
i,j
t ) and its four adjacent cells. Other cells either must be
deactivated (within the 5× 5 region) or have no eﬀect on droplet
movement (outside the 5 × 5 region). In this way, unnecessary
variables can be eliminated. We now present the progressive ILP
formulation in the following sections. The notations used in the
progressive ILP formulation are also listed in Table 1.
4.2.1 Objective Function
The goal of the progressive ILP formulation is to ﬁnd the min-
cost droplet movement. Therefore, the objective function can be
represented as the following equation:
Minimize
∑
di
j
∈D
∑
(x,y)∈E(xi,jit ,y
i,j
t )
wij(x, y)p
i
j(x, y, t + 1),(19)
where wij(x, y) is the cost when a droplet d
i
j moves to cell (x, y).
4.2.2 Constraints
There are ﬁve constraints in the progressive ILP formulation.
1. Droplet movement constraint: A droplet can move to one
of its four adjacent cells if and only if this cell is activated.
Therefore, this constraint can be expressed by the following
constraint:
p
i
j(x, y, t + 1)↔ ax,yt+1, ∀dij ∈ D, ∀(x, y) ∈ E′(xi,jt , yi,jt ). (20)
Note that since only the four adjacent cells are associated
with cell activation variables, the cell activation and the
droplet movement must be modeled as a ”if-and-only-if” re-
lation, unlike the ”if” relation in the basic ILP formulation.
Moreover, a cell stays at its original location if and only if
all its four adjacent cells are deactivated. Therefore, we can
use the following constraints to represent this situation:
p
i
j(x
i,j
t , y
i,j
t , t + 1) +
∑
(x,y)∈E′(xi,jt ,y
i,j
t )
a
x,y
t+1 ≥ 1, ∀dij ∈ D (21)
4p
i
j(x
i,j
t , y
i,j
t , t + 1) +
∑
(x,y)∈E′(xi,jt ,y
i,j
t )
a
x,y
t+1 ≤ 4, ∀dij ∈ D. (22)
2. Electrode constraint: Rule #1, which states that the diag-
onal cells cannot be activated, can be represented by the
following constraints:
L
c
x(t + 1) + H
r
y(t + 1) ≤ 1, ∀(x, y) ∈ Dˆ(xi,jt , yi,jt ) (23)
H
c
x(t + 1) + L
r
y(t + 1) ≤ 1, ∀(x, y) ∈ Dˆ(xi,jt , yi,jt ). (24)
The following two constraints, similar to constraint (14),
are used for rule #2:
if pij(x, y, t + 1) = 1→
Lc
x′ (t + 1) + H
r
y′ (t + 1) ≤ 1 or Hcx′ (t + 1) + Lry′ (t + 1) ≤ 1
∀dij ∈ D, ∀(x, y) ∈ E′x(xi,jt , yi,jt ), ∀(x′, y′) ∈ E′y(x, y) (25)
if pij(x, y, t + 1) = 1→
Lc
x′ (t + 1) + H
r
y′ (t + 1) ≤ 1 or Hcx′ (t + 1) + Lry′ (t + 1) ≤ 1
∀dij ∈ D, ∀(x, y) ∈ E′y(xi,jt , yi,jt ), ∀(x′, y′) ∈ E′x(x, y). (26)
As shown in Figure 3, if dij moves to cell (x + 1, y), then
cells (x + 2, y), (x + 2, y − 1), and (x + 2, y + 1) cannot
be activated. A similar condition holds for cell (x − 1, y).
Constraint (25) is used to represent these two cases, and
constraint (26) is used to represent the case when dij moves
to cell (xi,jt , y
i,j
t +1) or (x
i,j
t , y
i,j
t −1). Finally, for rule #3,
we impose the following constraint:
∑
(x′,y′)∈E(xi,jt ,y
i,j
t )
a
x′,y′
t+1 ≤ 1, ∀dij ∈ D. (27)
3. Static fluidic constraint: Since the locations of the droplets
are known, we can model the static ﬂuidic constraint in
a simpler way. The ﬂuidic constraint is violated only if a
cell with more than one droplet nearby is activated. There-
fore, instead of using constraint (7), we represent the ﬂuidic
constraint as follows:
if more than one droplet are around cell (x, y)
a
x,y
t+1 = 0, ∀(x, y) ∈ E′(xi,jt , yi,jt ), ∀dij ∈ D. (28)
4. Sink requirement: Once a droplet reaches its sink, it must
stay there. The sink requirement can be modeled by the
following constraint:
p
i
j(x
i,j
t , y
i,j
t , t)− pij(xi,jt+1, yi,jt+1, t + 1) ≤ 0, ∀dij ∈ D, (29)
which means that if dij reaches its sink at time t, then it
must stay there at time t + 1.
5. Voltage assignment and cell activation: This constraint is
the same as that in the basic ILP formulation.
4.3 Droplet Movement Cost
A key issue in the progressive ILP routing scheme is the deter-
mination of the droplet movement cost. Since our goal is to min-
imize the maximum droplet transportation time, our objective is
to move a droplet toward its sink at each time step. Furthermore,
we need to avoid congestion among droplets; otherwise, we may
either have to detour or stall some droplets for certain time, to
satisfy both the ﬂuidic and electrode constraints. Therefore, the
droplet transportation time is potentially increased. In this sub-
section, we detail how to determine the droplet movement cost.
The droplet movement cost wij(x, y) when a droplet d
i
j moves
to cell (x, y) consists of routing and congestion costs and is deﬁned
as follows:
w
i
j(x, y) = β r
i
j(x, y, tˆ
i
x, tˆ
i
y) + γ g
i
j(x, y, tˆ
i
x, tˆ
i
y), (30)
where rij(x, y, tˆ
i
x, tˆ
i
y) is the routing cost and g
i
j(x, y, tˆ
i
x, tˆ
i
y) is the
congestion cost. β and γ are user-speciﬁed constants. In this
paper, we empirically set β = 15 and γ = 0.1. The routing
cost is the real distance computed by the maze routing algorithm
from cell (x, y) to the sink of dij over the area of a biochips (for
normalization). We consider all other droplets except dij as 3× 3
obstacles when performing the maze routing algorithm due to the
static ﬂuidic constraint.
The goal of the congestion cost component is to reduce the
probability of the violations of the electrode and ﬂuidic con-
straints. In this paper, we use the concept of the idle interval
presented in [10] for congestion cost computation. The idle inter-
val Iij(x, y) = [T
m(x, y, dij), T
M (x, y, dij)] represents all possible
times a droplet dij will be at cell (x, y) with a time limitation
on the latest time when dij must reach its sink. Here, T
m(dij)
represents the earliest time when dij reaches (x, y) from its source
and TM (dij) represents the latest time when d
i
j can stay at (x, y)
without violating the time limitation. If we set Tl as the time
limitation, the congestion cost of a cell can be measured as the
length of the idle interval of dij on cell (x, y) over the summation
of the length of the idle intervals of other droplets on cell (x, y).
Instead of using the source location of a droplet, as in [10], we use
the possible destination cell (x, y) to compute the idle interval.
The values of Tm(dij) and T
m(dij) for a subproblem at time t can
be computed by the following two equations:
T
m
(d
i
j) = t + d
m
(x, y, tˆ
x
t , tˆ
y
t ) (31)
T
M
(d
i
j) = Tl − dm(x, y, tˆxt , tˆyt ), (32)
where dm(x, y, x′, y′) is the Manhattan distance between cells
(x, y) and (x′, y′). The congestion information is updated at every
time step for the latest congestion information.
To obtain more accurate congestion information, it is not suf-
ﬁcient to only consider the congestion information of the possible
destination cell (x, y). In this paper, we consider all cells’ conges-
tion information within the bounding box deﬁned by cell (x, y)
