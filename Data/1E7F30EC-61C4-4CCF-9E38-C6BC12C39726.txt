much harder.  
In the first year of the project, we 
establish impossibility results for a natural 
type of hardness amplification, called 
black-box hardness amplification. In fact, 
almost all hardness amplification results 
known so far are done in a black-box way, so 
it is nice to know their limitation. We show 
that to amplify hardness substantially in a 
black-box way, the amplification procedure 
cannot be realized by a circuit of small depth 
and moderate size. 
In the second year, we show that such a 
black-box hardness amplification must be 
inherently non-uniform, even without any 
restriction on the complexity of the 
amplification procedure, in the following 
sense. To guarantee the hardness of the 
resulting function, even against uniform 
machines, one has to start with a function 
which is hard against non-uniform 
algorithms with a long advice. Moreover, we 
derive similar lower bounds for any 
black-box construction of a pseudorandom 
generator (PRG) from a hard function.  
In the final year, we study some 
analogous problems in the cryptographic 
setting. We show that it is basically 
impossible to construct a strongly one-way 
function (OWF) from a OWF with 
worst-case hardness in a black-box way, 
unless the amplification procedure itself 
already embeds such a strongly one-way. 
Moreover, we show that any black-box 
construction of a cryptographic PRG from a 
strongly OWF which is realized by a 
constant-depth circuit of a moderate size can 
only have a small stretch. 
 
Keywords: randomness, hardness, black-box 
hardness amplification, constant 
depth circuits, list decodable code. 
 
二、前言與研究目的 
 
Randomness has become a valuable 
resource in computation, and understanding 
the power of randomness in computation has 
become one of the central research topics in 
theoretical computer science. There are 
several important computational problems 
for which the most efficient (or the only 
efficient) algorithms known so far are 
randomized. On the other hand, more and 
more randomized algorithms have later been 
successfully converted into deterministic 
ones. Is randomness really needed? A major 
open question is the BPP versus P question, 
asking whether or not all randomized 
polynomial-time algorithms can be converted 
into deterministic polynomial-time ones. One 
standard approach to derandomizing BPP is 
to construct the so-called pseudorandom 
generators (PRG), which can stretch a short 
random seed into a long pseudorandom 
string that looks random to circuits of 
polynomial size. So far, all known 
constructions of PRG are based on 
assumptions of the nature that certain 
functions are hard to compute.  
The idea of converting hardness into 
pseudorandomness first appeared in the work 
of Blum & Micali [2] and Yao [21], who 
showed how to obtain a PRG from a 
one-way function. Then Nisan and 
Wigderson [12] showed that a PRG can be 
constructed from a Boolean function which 
is hard in average case, and this initiated a 
series of works. To get a stronger result, one 
would like to relax the hardness assumption, 
and a series of research [12, 1, 7] then 
developed transformations that can convert a 
function into a harder one. Finally, 
Impagliazzo and Wigderson [9] were able to 
convert a function in E=TIME(2O(n)) that is 
hard in worst case into one, also in E, that is 
correctly on α fraction of the input. Again, 
Dec only uses A as an oracle and does not 
depend on the internal structure of A. We call 
Amp the encoding procedure and Dec the 
decoding procedure. In fact, almost all 
previous hardness amplification results 
(except [10, 18]) are done in a black-box 
way, so it is nice to establish impossibility 
results for such type of hardness 
amplification. 
Finally, we study some analogous 
problems in a cryptographic setting. In 
particular, we study the task of hardness 
amplification for one-way functions (OWF), 
and the task of constructing cryptographic 
PRGs from one-way functions. OWF and 
PRG are two of the most basic primitives in 
cryptography, which can be used to build a 
wide range of cryptographic protocols. It is 
known that given a strongly one-way 
function, for which any polynomial-time 
algorithm can only invert on 1/poly(n) 
fraction of the inputs, one can construct a 
PRG that fools all polynomial-time 
distinguishers. It is also known that one can 
construct a strongly one-way function from a 
weakly one-way function, for which any 
polynomial-time algorithm must fail to invert 
on 1/poly(n) fraction of the inputs. Of course 
one would like to build a PRG under a 
weaker condition. A natural question is: can 
we obtain a weakly one-way function from a 
one-way function with only worst-case 
hardness, namely, a one-way function for 
which any polynomial-time algorithm must 
fail to invert on at least one input. No such 
hardness amplification has been found and 
this question has remained open. Again, we 
would like to show that this is indeed 
impossible if it is done in a black-box way. 
 
三、文獻探討 
 
Viola [20] gave the first lower bound on 
the complexity required for black-box 
hardness amplification of Boolean functions. 
He showed that to transform a worst-case 
hard function f into a mildly hard function f’, 
both against circuits of size 2o(n), the 
encoding procedure Amp can not possibly 
belong to the complexity class ATIME(O(1), 
2o(n))). This rules out the possibility of doing 
such hardness amplification in PH, which 
explains why previous such procedures all 
require a high computational complexity. He 
also showed a similar lower bound for 
black-box construction of PRG from a 
worst-case hard function.  
Trevisan and Vadhan [18] observed that a 
black-box hardness amplification from 
worst-case hardness corresponds to an 
error-correcting code with some 
list-decoding property. Then results from 
coding theory can be used to show that for 
such amplification from worst-case hardness 
to hardness (1-ε)/2, the decoding function 
Dec must need Ω(log(1-ε)) bits of advice in 
order to compute f. This explains why almost 
all previous hardness amplification results 
were done in a non-uniform setting, except 
[10,18] which did not work in a black-box 
way. 
There were also impossibility results on 
weaker types of hardness amplification, from 
worst-case hardness to average-case hardness. 
Bogdanov and Trevisan [22] considered 
hardness amplification for functions in NP in 
which the black-box requirement on the 
encoding function is dropped. They showed 
that the decoding function cannot be 
computed non-adaptively in polynomial time 
unless PH collapses. Viola, in another recent 
paper [23], considered hardness 
amplification in which the black-box 
requirement on the decoding function is 
dropped. He showed that if the encoding 
Circuits of a small size, or circuits of a small 
depth and a moderate size can be shown to be 
insensitive to noise on their input. Thus, they 
cannot be used to compute such a code and 
the corresponding hardness amplification. 
In the second year, we show that even 
without any complexity constraint on the 
amplification procedure Amp, there is still an 
inherent non-uniformity if we want to amplify 
hardness beyond a certain range. For this, we 
show that given a corrupted codeword with a 
high fraction (1-ε)/2 (for a small ε) of errors, 
one may need a long list of candidate 
messages in order to have one of them within 
a small relative distance (1-δ)/2 (for a large δ) 
to the original message. To show this, we 
would like to find a set of messages such that 
some ball of relative radius (1-ε)/2 in the 
codeword space contains many of their 
corresponding codewords, but any ball of 
relative radius (1-δ)/2 in the message space 
contains only a small number of messages 
from that set. We choose these messages 
randomly and show that they have some 
chance of satisfying the condition above when 
(1-ε)/2 is larger than (1-δ)/2 to some extent. 
Another goal of the second year is to prove 
lower bounds for black-box constructions of 
PRG from hard functions. For this, we 
discover that there is also a connection 
between the error-reduction codes we just 
considered (for hardness amplification) and 
such PRG constructions. This new connection 
may have interest of its own. Then the results 
we obtain for such codes immediately yield 
results for such PRG constructions. 
 In the final year, we derive lower bounds 
in the cryptographic setting for the task of 
hardness amplification of OWFs and the task 
of constructing PRGs from OWFs. First, 
assume that an AC0 circuit can amplify 
hardness beyond a certain bound. It is known 
that a random function f is likely to be 
one-way. As shown in [23], it is still likely to 
be so even with a random restriction ρ applied 
to its output bits, as long as ρ gives each 
output bit the symbol * (leave the bit free) at a 
rate above some threshold. On the other hand, 
AC0 circuits are likely to become biased after 
applying a random restriction on its input. As 
the rate of * decreases, the effect a random f 
on Ampfρ(x’) becomes smaller, for any input 
x’. If the rate of * is small enough, the 
functions Ampfρ(x’) for most f become close 
to each other (agreeing with each other on 
most inputs). As a result, they are close to 
some fixed function (depending on ρ) which 
can then be used as an oracle to invert fρ. This 
would lead to a contradiction, and we could 
conclude that such hardness amplification 
cannot be realized by AC0 circuits. However, 
there is an obstacle in front us. In order to 
guarantee that the functions Ampfρ(x’) for 
most f are close to each other, we need the 
random restriction to give * in a very low rate. 
Had we applied a conventional random 
restriction, say from [3, 5] (as was done in 
[23]), we would end up having too few free 
bits left in f(x) for almost every x, and 
consequently fρ would not be one-way for 
most f. To overcome this problem, we would 
like the *'s to appear in a somewhat clustered 
fashion: for any x, either f(x) has no * at all, 
or it has a sufficient number of *'s. This 
motivates us to consider a new kind of random 
restriction, and we show that it also makes the 
output bits of AC0 circuits highly biased. 
 
五、結果與討論 
 
In the first year, we prove that if the 
encoding procedure Amp for such a hardness 
amplification is computed by a circuit of 
depth d and size 2o(k
1/d), the decoding 
procedure Dec must need an advice of length 
2Ω(n). As a result, with Amp∈PH when k = 
show that any such a construction realized by 
a constant-depth 2n
o(1)
-size circuit can only 
have a sublinear stretch (with m’-n’ = o(n’)). 
 
六、計畫成果自評 
 
 We are glad to report that the results we 
have obtained match closely to what we 
expected in our proposal. The results we 
derived in the first two years will appear in 
our paper: “On the Complexity of Hardness 
Amplification,” which will soon appear in 
IEEE Transactions on Information Theory, 
October 2008. The result we obtain in the 
final year is still in preparation, which will 
soon be submitted to a suitable journal. 
 The power of randomness in computation 
is a very basic and very important question in 
computer science. This research topic has 
attracted many top researchers in the world 
and has produced many important results. 
Basic research has played an important role in 
advancing the frontier of human knowledge. 
Unfortunately, basic research in Taiwan has 
not received the recognition that they deserve. 
We hope that by carrying out such research 
here, more people in Taiwan can start to 
realize the importance and beauty of basic 
research. 
 
七、參考文獻 
 
[1] Laszlo Babai, Lance Fortnow, Noam 
Nisan, and Avi Wigderson. BPP has 
subexponential time simulations unless 
EXPTIME has publishable proofs. 
Computational Complexity, 3(4), pages 
307--318, 1993. 
[2] Manuel Blum and Silvio Micali. How to 
generate cryptographically strong sequences 
of pseudo random bits. In Proceedings of the 
23rd Annual IEEE Symposium on 
Foundations of Computer Science, pages 
112--117, 1982. 
[3] Merrick L. Furst, James B. Saxe, and 
Michael Sipser. Parity, circuits, and the 
polynomial-time hierarchy. Mathematical 
Systems Theory, 17(1), pages 13--27, 1984. 
[4] Oded Goldreich, Noam Nisan, and Avi 
Wigderson. On Yao's XOR lemma. Technical 
Report TR95-050, Electronic Colloquium on 
Computational Complexity, 1995. 
[5] Johan Hastad. Computational limitations 
for small depth circuits. PhD thesis, MIT 
Press, 1986. 
[6] Alexander Healy, Salil P. Vadhan, and 
Emanuele Viola. Using nondeterminism to 
amplify hardness. In Proceedings of the 36th 
ACM Symposium on Theory of Computing, 
pages 192--201, 2004. 
[7] Russel Impagliazzo. Hard-core 
distributions for somewhat hard problems. In 
Proceedings of the 36th Annual IEEE 
Symposium on Foundations of Computer 
Science, pages 538--545, 1995. 
[8] Russell Impagliazzo, Ronen Shaltiel, and 
Avi Wigderson. Extractors and 
pseudo-random generators with optimal seed 
length. In Proceedings of the 32nd ACM 
Symposium on Theory of Computing, pages 
1--10, 2000. 
[9] Russel Impagliazzo and Avi Wigderson. 
P=BPP if E requires exponential circuits: 
derandomizing the XOR lemma. In 
Proceedings of the 29th ACM Symposium on 
Theory of Computing, pages 220--229, 1997. 
[10] Russel Impagliazzo and Avi Wigderson. 
Randomness vs. time: de-randomization under 
a uniform assumption. In Proceedings of the 
39th Annual IEEE Symposium on Foundations 
of Computer Science, pages 734--743, 1998. 
[11] Henry Lin, Luca Trevisan, and Hoeteck 
Wee. On hardness ampli¯cation of one-way 
functions. In Proceedings of the 2nd Theory of 
出席國際學術會議心得報告 
                                                             
計畫編號 96-2221-E-001-005- 
計畫名稱 計算中難度與亂度的關係研究(3/3) 
出國人員姓名 
服務機關及職稱 
呂及人 
中央研究院副研究員 
會議時間地點 96 年 12 月 16 日至 12 月 22 日  大陸杭州市 
會議名稱 The Fourth International Congress of Chinese Mathematicians (ICCM 2007) 
發表論文題目 On the Complexity of Hardness Amplification 
 
一、參加會議經過 
 The Fourth International Congress of Chinese Mathematicians (ICCM 2007)會議為三年一度
的會議，前三屆分別於北京、台北、香港舉行，為華人界最重要的數學會議。參與會議的學
者不只來自亞洲，更有不少來自世界各地的頂尖學者。會議的規模非常大，共分為 11 個
track，本屆首次增加資訊領域中 Theoretical Computer Science 的 Track。會議中的演講
者都是受邀請的，這與典型資訊領域的會議性質不同。演講大致分為兩類，第一類為特別性
質的一個小時演講，包括 Morningside Public Lectures、New World Lectures、Taikang Lectures、
Mui's Lecture、Plenary Lectures、International Distinguished Lectures，演講者多是數學界各個
領域中，具有舉足輕重的學者，其中包括資訊界最高榮譽「Turing Award」得主姚期智教授，
與哈佛大學著名的 Leslie Valiant 教授（Turing Award 的熱門候選人）。姚教授給了兩個演講，
分別是關於隨機式證明與密碼學方面的問題，Valiant 教授的演講，則是探討以資訊科學中機
器學習的概念，來研究生物演化的問題，兩位大師的演講，都有不少發人深省的內容，對我
有不少的啟發。第二類的演講則為一般性的 45 分鐘演講，我擔任的演講則屬於此類。我的演
講題目為“On the Complexity of Hardness Amplification＂，探討計算理論中一個重要概
念 Hardness Amplification 的複雜度問題，這些問題在瞭解隨機計算的能力上，扮演了關鍵
的角色。幾位與會者對我們的結果表示興趣，與我有不少的討論，讓我受益匪淺。 
與我屬於同 Track（Theoretical Computer Science）的，還有其他四位演講者，他們
是：Jin-Yi Cai（University of Wisconsin-Madison）、Xiaotie Deng（City University of Hong 
Kong）、Yaoyun Shi（University of Michigan）、與 Shanghua Teng（Boston University）。他們在
國際上都是頗負盛名的學者，研究領域包括：複雜度理論、賽局理論、量子計算、與演算法
設計。我很榮幸能與他們一同與會，聆聽他們的演講，並與他們進行討論。 
 
二、與會心得 
參加本次會議，除了學習到計算理論研究目前的核心問題及其最新的進展，亦有機會認
識相關領域的傑出學者，並與他們討論，對許多問題有更深入的瞭解。同時也吸收了相關數
學領域的一些知識，擴展了我的眼界，對我未來的研究相信會有不小的幫助。 
 
三、考察參觀活動(無是項活動者省略) 
