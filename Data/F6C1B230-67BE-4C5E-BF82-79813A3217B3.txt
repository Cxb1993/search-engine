 
1 
 
行政院國家科學委員會專題研究計畫成果報告 
一種兼具強韌及高容量數位音訊浮水印之多場域架構 
An annexable multi-domain framework for robust and high-capacity digital audio watermarking 
 
計畫編號：NSC 98-2221-E-197 -019 
執行期間：98 年 08 月 01 日至 99 年 10 月 31 日 
主持人： 胡懷祖  國立宜蘭大學 電子工程學系教授 
Email： hthu@niu.edu.tw 
 
 
中文摘要： 
本次專題計畫所設計的系統裡，同步碼與浮水
印資訊是以音框時序劃分，倒頻譜域與小波域內的
浮水印則是以頻率區隔，同步碼採量化索引調變的
技術置入時域信號，緊隨其後再嵌入浮水印位元序
列，其中部分係以均值操縱法修改倒頻譜域係數，
部分以量化索引調變的方式改變中低頻區的小波
包裹係數，對於音訊之影響程度經轉換為頻域上的
信噪比，可推算出不為人耳察覺的最大嵌入強度，
然最具關鍵的技術還是在於如何再從已嵌入浮水
印的音訊尋回先前的量化步階，以符合盲水印的要
求。 
此一技術的檢測項目有量化、取樣、低通濾
波、MP3 壓縮、雜訊、抖動、位移、裁切等常見之
數位處理攻擊，結果顯示所研發的浮水印技術具有
足夠的強健性且人耳難以察覺。所建構的系統中，
除採用擾亂技術將浮水印打亂，藉此增強安全性。
另也納入能夠檢測、校正錯誤的 BCH 碼，提高浮
水印還原後的正確性。以上研究成果經整理後，均
將以期刊或研討會論文呈現。 
 
關鍵詞：複場域音訊浮水印、心理聲響模型、聽覺遮蔽、
同步碼、小波包裹轉換、倒頻譜 
 
Abstract: 
In this project the proposed watermarking 
technique locates information bits and synchronous 
codes in separate time slots.  While the placement of 
synchronous codes is an application of quantization 
index modulation in the time domain, the embedding 
of the watermark is through either wavelet or cepstral 
approach manifested in different frequency regions.  
This study employs the statistic mean manipulation to 
adjust the cepstral coefficients, in addition to the use 
of quantization index modulation to alter the wavelet 
packet coefficients that span from low to middle 
frequencies.  The influence in spectra due to the 
watermark is reckoned as signal-to-noise in decibel, 
which renders an estimator of the ideal embedding 
strength.  Nevertheless, the most critical technique is 
to retrieve the quantization step previously adopted to 
embed the watermark bit without referring 
information other than the watermarked audio signal. 
The performance analysis takes into 
consideration of commonly-encountered attacks such 
as requantization, resampling, lowpass filtering, MP3 
compression, noise corruption, jitter, shifting, 
cropping.  The experimental results revealed that the 
proposed technique not only possesses sufficient 
robustness but also is unperceivable by human ears.  
As for the system integration, apart from the 
incorporation of a scrambler to enhance to security, 
the intended system reinforces the capability of 
correcting errors by employing the BCH code 
technique.  All the details will be organized and 
presented as journal/conference papers in the near 
future. 
 
Keywords: Multi-domain audio watermarking, 
psychoacoustic model, auditory masking, 
synchronous code, wavelet package transform, 
cepstrum 
 
一、計畫緣由與目的： 
近年來資訊科技的大幅躍進，讓數位資料（包
括聲音、視訊、影像或者文字檔等）的傳播、複製
及修改都變的非常容易，所衍生的智財侵權問題卻
也層出不窮。所幸數位浮水印(Digital Watermark)技
術的出現提供了問題解決的可能途徑。以音訊信號
為例，浮水印的嵌入可以是時域[1][2][3][12][13]、頻
域[5][6][7][11]、倒頻譜[4] [9][10] [11]、餘弦[14]、小
波[8][15][16]等不同途徑與方法。甚或不同場域的共
構模式，皆不時有精闢探討。技術發展至今均已趨
於完整，即便如此，個別技術仍難免有其侷限之處。
對我們而言，如何將技術改良與整合，令其在產業
的實用價值得以提昇，會是此次研究計畫的主要企
圖。而我們的研究主軸有： 
 
一、 找出浮水印（意指一切所欲隱匿的資訊）在
音訊信號的各種場域中的有效嵌入模式。 
二、 異質場域浮水印技術之共構模式。 
三、 如何利用聽覺特性，強化浮水印的藏匿與抵
禦攻擊的能力。 
 
二、研究方法： 
音訊浮水印之技術研發從三方面切入，其一是
 
3 
 
其中 *kr 與 *k 分別是嵌入浮水印後重新計算各次頻
帶小波係數之均值及量化步階， 'mW 為萃取出的浮水
印資訊。 
本處所討論的DWPT浮水印技術經過實地測
試，從初步的實驗結果來看，在聽覺遮罩的有效防
護下，藏匿音訊之中的浮水印已非人耳所能感知，
其對於重新量化、重新取樣、低通濾波、SNR=20dB 
白雜訊摻入、64kbps MP3壓縮都有令人滿意的準確
度，惟對於兼具剪裁與插補作為的抖動(jitter)卻只能
抵擋輕微程度的攻擊。從中透露的訊息是，要想讓
上述方法具有實用價值，尚須輔以同步碼之類的技
術，確保音訊信號在偏移校準能萃取出正確的浮水
印。 
上述過程有個非常弔詭的迷思，亦即浮水印強
度是由音訊信號估算而得，而浮水印強度換算得到
的量化步階(quantization step)則是QIM中極為關鍵
的參數，想要正確從音訊中解出浮水印，量化步階
必須正確無誤，亦即是 *k k   。事實上，要想讓音
訊信號在嵌入之前與之後所算出的量化步階一致，
最起碼必須讓前述之幾何、算術均值比保持等比關
係。最直接有效的作法是 
 
 
 
 
1/1 22
0
12 2
0
1/1
2
2
0
1 2
2
0
ˆ ( )ˆ ( )
1ˆ ˆ( ) ( )
( ) ( )
.
1 ( )( )
kk
k
kk
k
ll
g i
l
a
ik
ll
gi
l
a
ik
Z iPM Z i
PM Z i Z i
l
Z i PM Z i
PM Z iZ i
l








   
    




(14) 
 
之後解析出來的量化步階也就應該維持原有大小。 
 
(II) 倒頻譜域之浮水印 (cepstral 
watermarking) 
倒頻譜的表述方式早被作為語音分析及辨識
之用，時域信號 ( )s n 經傅立葉轉換、複數模數、對
數及反傅立葉轉換等過程轉成實數倒頻譜係數
( )c n 。而改過之後的倒頻譜係數 ˆ( )c n 可以式(15)之
逆向程序轉成音訊信號 ˆ( )s n ，完成個別浮水印位元
的嵌入動作。 
 
   ( )ˆ ˆ( ) IFFT exp FFT ( ) j S ns n c n e   ,  (15) 
 
在[11]的研究裡，位元資料的嵌入是經由統計
均值的操縱(statistic mean manipulation, SMM)達
成，實際的步驟有二，其一為去除均值偏差，其二
為依據應嵌入位元值調整 ( )c n 的統計均值： 
 
( ) ( ) 0    &    
ˆ( )
( ) ( ) 0    &    
C
C
c n if c n n I
c n
c n if c n n I
         
 (16) 
 
此處的 CI  代表索引項， 則強迫為 4 / M ，而 M
則是 CR 中使 ( ) 0c n  的個數。所導致的最終效果便
是重新取出的倒頻譜係數  ˆ( )c n 在浮水印位元 
0mW  的情況下， 其總和亦為0，若 1mW   則總
和應為4。 
為擴充同一音框的嵌入容量，我們試著提出另
一 個 替 代 方 案 ， 設 若 AI  與  BI 分 別 為
 65, ,544   1504, , 2043 與  545, ,1503 的
兩組區段，各自的係數個數為 AN (=960) 與 
BN (=959)。設若 AI 、 BI  與 A BI I 內的係數均
值分別以 AM 、 BM  與 A BM  表之，則 AM  與 BM
二者之間的差異可用作二元數值的表徵，其具體作
法如下： 
 
(1) 當 1mW   且 A BM M T  ， 則倒頻譜係數
修改成 
 
/ 2( ) for
ˆ( )
/ 2( ) for
B
A A B A
A B
A
B A B B
A B
N Tc i M M i I
N Nc i
N Tc i M M i I
N N
           


(17) 
 
其中 T  是選作區隔的閥值，在此可選 0.002。  
(2) 若 是 恰 巧 相 反 ， 亦 即  0mW   且 
B AM M T  ，則改為 
 
/ 2( ) for
ˆ( )
/ 2( ) for
B
A A B A
A B
A
B A B B
A B
N Tc i M M i I
N Nc i
N Tc i M M i I
N N
           


 
        (18) 
 
須知 Eq. (17) and (18) 兩式並未牽動整體係
數的統計均值，亦即， 1 1
0 0
ˆ( ) ( )N N
i i
c i c i    。既然
倒頻譜係數的均值沒變，原先以傳統SMM手法所嵌
入的位元自然不受影響。意謂著同一音框可同時以
式(16)以及式(17)~(18)可分別嵌入不同位元，浮水印
之酬載因而得以倍增。 
Table 1: Correct watermark detection rates (in 
percentage) under various attacks. 
 
SMM  The proposed 
scheme 
  
1Wm   0Wm   1Wm  0Wm 
0. None 100.00 100.00 100.00 100.00 
A. Resampling 100.00 100.00 100.00 100.00 
B. Requantisation 100.00 100.00 100.00 99.99 
C. Noise corruption 100.00 100.00 99.90 99.48 
D. Lowpass filtering 100.00 100.00 100.00 100.00 
E. Jittering 100.00 100.00 99.99 99.96 
F. MPEG compression 100.00 98.50 100.00 99.98 
G. Time shifting 96.02 61.85 92.12 87.85 
 
 
5 
 
是採音框劃分方式，先將同步碼置於指定的音框，
再於其後緊鄰之音框依序植入浮水印位元。 
 
 
Embedding strength controlled 
by psychoacoustic analysis 
Frame 
categorization 
Time-domain synchronous code 
embedding 
Signal with synchronous 
code embedded 
Watermarked signal 
Original audio signal 
1[ ]w k
2[ ]w k
psychoacoustic 
analysis 
Arnold 
Transform 
BCH encoding 
Bit stream 
convertor 
Cepstral 
Transform 
Inverse cepstral 
Transform 
Cepstral domain 
data embedding 
DWP 
decomposition 
Subband 
Selection 
DWP coefficient 
modification 
Inverse DWP 
Transform 
 
Fig. 2.  The proposed audio watermark 
embedding system. 
 
 
三、計畫成果： 
原計畫是以兩年為期，第一年著重學理推導及
模擬驗證，主要在分析各場域最合適之浮水印嵌入
方式及其對原始音訊的影響程度，並確認時序、頻
率、小波、倒頻譜等多場域的共構關係，以及驗證
及檢查各場域之浮水印是否存在連動或彼此相互干
擾之現象。最後針對各式不同的音訊資料進行特徵
比較與實驗模擬分析。第二年則定位為系統整合階
段，除了整合「同步碼校對」、「聽覺遮蔽」及「多
場域浮水印嵌入」技術，另外再細調系統架構，強
化浮水印的組合形式，提升其對於外在攻擊的強韌
度。末了希望加入軟體開發之規劃，期能以 C++語
言程式撰寫使用者介面，製作實用的應用程式。 
 
惟因國科會僅核定一年期經費補助，計畫的執
行亦隨之調整為主題區隔導向，亦即先鎖定特定項
目（如同步碼、倒頻域浮水印、小波浮水印等）進
行個別探索，再嘗試將各自獨立的技術組裝為完整
的架構。過程中所需的測試平台現已粗具雛形，爾
後若有模擬驗證的需求，可在同樣的框架下設定檢
測條件，據以確認研發之浮水印技術是否具有低失
真、高容量、難察覺以及惡意攻擊的耐受度。而最
終的系統則是以模組化方式組合產生，選用的浮水
印採二元圖像，經擾亂與 BCH 碼處理後轉為一維位
元串列，併同步碼植入音框當中。 
運用以上策略，本次專題研究計畫已順利執行
完成，個別項目研發成果非惟可以獨立的技術單元
來發表，亦可整合為學術論文，這部分工作正著手
進行中。 
 
四、計畫成果自評： 
成果符合原先規劃的進程，部分項目甚或略有超
前。具體細節參見所附之自評表。 
 
五、參考文獻： 
[1] Boneyk, L., Tewfik, A. H., and Hanmdy, K. N., 
“Digital Watermarks For Audio Signals,” in Proc. 
IEEE Int. Conf. on Multimedia Computing and 
System, pp.473-480, 1996.   
[2] Bassia, P., Pitas, I., and Nikolaidis, N., “Robust 
Audio Watermarking in the Time Domain,” IEEE 
Trans. on multimedia, 3 (2), pp. 232-241, 2001.  
[3] Gruhl, D., Lu A., and Bender, W., “Echo Hiding,” 
Lecture Notes in Computer Science Information 
Hiding, 1174, pp. 295-315, 1996.   
[4] Liu, S. C., and Lin, S. D., “BCH code-based 
robust audio watermarking in cepstrum domain,” 
Journal of Information Science and Engineering, 
22 (3), pp. 535-543, 2006.   
[5] Wu, C.-P., Su, P.-C. and Kuo., C.-C. J., “Robust 
audio watermarking for copyright protection,” 
SPIE 44th Annual Meeting, pp. 387-397, 1999.   
[6] Johnston, J. and Brandenburg, K., “Wideband 
coding perceptual considerations for speech and 
music,” in Advances in Speech Signal Processing. 
(S. Furui and M. Sondhi eds), New York, Dekker, 
1992.   
[7] Swanson, M. D., Zhu, B., Tewfik, A. H., Boney, 
L., “Robust audio watermarking using perceptual 
masking,” Signal Processing, 66, pp. 337-355, 
1998. 
[8] Srinivasan, P. and Jamieson, L. H., “High-quality 
audio compression using an adaptive wavlet 
packet decomposition and psychoacoustic 
modeling,” IEEE Trans. on Signal Processing, 46 
(4), pp. 1085-1093, 1998. 
[9] Lee, S. K., and Ho, Y. S., “Digital audio 
watermarking in the cepstrum domain,” IEEE 
Trans. on Consumer Electronics, 46 (3), pp. 
744-750, 2000.   
[10] Hsieh, C.-T. and Tsou P.-Y., “Blind cepstrum 
domain audio watermarking based on time-energy 
features,” IEEE Int. Conf. on Digital Signal 
Processing, pp. 705-708, 2002.  
[11] Li, X. and Yu, H. H., “Transparent and robust 
audio data hiding in cepstrum domain,” Proc. Int. 
Conf. on Multimedia and Expo, pp. 397-400, 
2000.   
[12] Lemma, A. N., Aprea, J, Oomen, W., van de 
Kerkhof, L., “A temporal domain audio 
watermarking technique,” IEEE Trans. on Signal 
Processing, 51 (4), pp. 1088-1097, 2003.   
[13] Li, W., Xue, X., and Lu, P., “Localized audio 
watermarking technique robust against time-scale 
modification,” IEEE Trans. on Multimedia, 5(1), 
pp. 60-69, 2006. 
[14] Yeo, I. K. and Kim, H. J., “Modified patchwork 
algorithm: a novel audio watermarking scheme,” 
IEEE Trans. on Speech and Audio processing, 
11(4), pp. 381-386, 2003.   
[15] Cui, L. S., Wang, X. and Sun, T. F., “The 
application of wavelet analysis and audio 
compression technology in digital audio 
watermarking,” in Proc. IEEE Int. Conf. on 
Neural Networks and Signal Processing, pp. 
1533-1537, 2003.   
[16] Wang, R., Xu, D., Chen, J., and Du, C., “Digital 
audio watermarking algorithm based on linear 
 
7 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價 
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適 
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
█ 達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 █撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價 
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以 
500 字為限） 
 
在數位多媒體資料裡頭加入浮水印應是解決智財爭端的最佳手段。為增進
浮水印在真偽鑑別、保密通信、電子身份認證等方面之應用價值，本計畫所研
發的技術是屬於不必仰賴原始音訊的盲水印，尤其是充分利用聽覺特性將浮水
印藏入各個指定場域，藉此達成隱匿、容量、強健等多重要求。 
隨著本次計畫的執行，已有一篇國內研討會論文發表、研究成果尚可整理
成另一篇國際會議論文及至少一篇 SCI/EI 學術期刊論文。由於從已嵌入浮水印
之音訊信號反相推算量化步階的觀念具有相當之新穎性，未來視技術實用性與
市場潛力，可考慮另外申請發明專利。 
 
 
 
 
 
 
 
9 
 
推廣及運用的價值 
數位浮水印技術的研究涉及資訊學、密碼學、網路通訊、訊號處理、
型態識別等多種學科，其應用前景早已受到學界、業界和軍方的廣
泛關注。 
浮水印是現今解決多媒體智財爭端的最佳手段，本項技術可增進浮
水印在真偽鑑別、保密通信、電子身份認證等方面之應用價值。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
 
 
二、與會心得 
整體來說，本次會議雖然投稿者眾多，然實際參加會議的人數卻不如預期般踴躍，在
經費拮据的情況下只得臨時換至武漢大學學術交流中心。而主辦單位也許是經驗不足，事
前聯繫工作並未確實，致使我必須多花時間奔波於住處與開會地點。 
 
此次會議大陸高校參加者眾多，似有意鍛鍊學生在國際會議場合以英文發表研究心
得，這些初生之犢的臨場經驗或嫌稚嫩，但假以時日必當有所成長，為往後之學術生涯開
啟更廣闊的視野。見到這種發展情勢，我們實應多鼓勵自己的學生大膽走入國際學術社群。
 
三、考察參觀活動(無是項活動者省略) 
無 
 
四、建議 
政府不僅應鼓勵學術界人士多參與國際研討會，也應有計畫地培訓推動國際學術事務
的人才。類似活動如能在國內舉辦，亦應獎助學生多多參與，唯有及早打開眼界，才能獲
取國際競爭的先機。 
 
五、攜回資料名稱及內容 
ICIMA 2010 Conference Proceeding （本次發表之論文位在 pp. 282-287） 
 
六、其他 
無 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABLE I. HNM PARAMETERS 
Model parameters Number Unvoiced Voiced 
Frame length: 1>[ I ,( ,( 
LSF parameter: I, 18 ,( ,( 
Power level: ,,2 5 ,( 
Fundamental frequency: fo 1 ,( 
Maximum voiced frequency : I>;, I ,( 
Magnitude harmonics: ak LFm I j,J - ,( 
Phase hannonics: 'A L/<"ljoJ - ,( 
Hannonic variation: Yk 13 - ,( 
III. SPECTRAL ENVELOPE CONVERSION TECHNIQUES 
To perform HNM-based voice conversion , one of the 
conversion functions is dedicated to adjusting the source 
spectral parameters. This study adopts 18th order LSF 
parameters to represent the spectral envelope of a speech 
signal sampled at 16 kHz. Let x = [It ,I; ,. .. ,1t8 ] ' and 
y = [Ii ,Ii,.· ·,1i8 ] denotes the source and target LSF 
vectors. The four techniques under investigation are coding 
mapping (eM) [11] , Gaussian mixture model (GMM) [10], 
piecewise deviation linear transformation (POL T) [15] , and 
Hidden Markov model (HMM) with weighted deviation 
linear transformation (WOLT) [16]. 
A. Code book mapping (CM) 
The codebook mapping technique assumes a one-to-one 
correspondence between the code book entries of the source 
speaker and the target speaker. The conversion between x 's 
and y 's is carried out using 
y = 7;(x) = Cik); (I) 
where k is chosen from a Q -entry code book using 
k = a�gmin{
(x-c�,)/ (x-C�i)}' i E {1 ,2 , . .. ,Q}. (2) 
In Eq. (1) and (2) ,  c�i) and ci,) denote the ith entries in 
the source and target code books , respectively. 
B. Gaussian mixture model (GMM) 
By modeling the probability density function of the 
conjoint vector z = [ XT , yT r as the sum of Q multivariate 
Gaussian components , the GMM conversion can be 
implemented as 
o 
y = T2 (x) = L 
h,(x ) [ .. ii) + l:i,� (l:� r1 (x -"�i)) J, (3) 1=1 
where ";1) = [ ("�i) / , ( .. ii) / J represents the mean vector of 
the ith Gaussian component , and l:;;) is the covariance 
matrix of the following form: 
l:;;) =E[(X-,,;,»)(x-";i)rJ=[��} 
l:
� ] . (4) 
(I) l:(i) 
The weighting factor h,(x) is computed by 
h 
( ) = 
a,N(x;"�i),l:��) i x 0 ' 
La, N (x; .. �,l' l:�) 
1=1 
(5) 
where N (x; "�i)' l:�;» denotes a multivariate normal 
distribution with mean "�I) and variance l:�;) , and a, 
denotes the apriori probability of the ith Gaussian component. 
C. Piecewise deviation linear transformation (PDLT) 
The POL T can be viewed as a simplified variant of the 
GMM. Instead of assigning a weight to each multivariate 
Gaussian component , the POL T merely picks the most likely 
entry to carry out the transformation using 
y = 7; (x) = "ik) + V(k) (x -"�k» ; (6) 
where V(k) is the multiplicative matrix obtained by least 
square optimization. The selection of k follows Eq. (2). As 
revealed by Eq. (3), (5), and (6) , the POLT requires much 
less computation than the GMM and HMM. 
D. Hidden Markov model (HMM) 
The HMM is an efficient way to exploit the temporal 
correlation across consecutive frames. To examine the 
efficacy of this approach , this study also examines the 
HMM-WOL T developed in [16). The conversion function is 
formulated as 
Q 
Y(m) = � (x(m») = L p( S, (m) I X(m) ) [ .. ri) + V(i) (x(m) -"�i)) ] 
i=l 
(7) 
where X(m) denotes the feature vector sequence observed 
up the m thframe , i.e. X={x(I), ... ,x(m)}. The term SI(m) 
signifies the ith Markov state at frame m , and Q 
corresponds to the number of states. According to Bayes' 
rule , the conditional probability P (S, (m) I X( m») is obtained 
by 
P(SI(m) I X(m») = op(s,
(m),X(m») 
(8) 
L p( S;(m), X(m») 
;�l 
The joint probability P (S, (m), X( m») can be computed 
in a recursive manner as in [17]. The derivation of "�i)' .. ii) 
and V(i) follows the procedure in [16). 
283 
and Td(c5x) matches c5y. In many practical situations , the 
magnitude of Td (c5x) is often less than c5y , making the 
mean term Tm (x) even more predominant. Due to the 
statistical averaging of Tm (x) , the converted spectral 
envelope generally exhibits a broader formant structure , 
which is an unfavorable outcome. 
In [19], the variances of source vectors are preserved to 
avoid over-smoothing of the converted spectral envelope. A 
similar effect can be achieved by rescaling the offset term by 
, r � var (W» ) [ IIc5xll 1 Td(c5x) = 18 
II II 
Td(c5x) (11) 
Ivar (tiX» ) 
Td(c5x) 
k�1 
where the first term on the right hand side of Eq. (11) reflects 
the rudimentary differences in variance for parameters 
extracted from the source and target objects. Adding the new 
term Td (c5 x) to Tm (x) sharpens some of the formants , but 
broadens others. Further refinement is necessary if the 
resulting spectral envelope needs to have distinct formant 
characteristics. 
Let c5t?') denote the ith element of Td (c5x), and let T,CV) 
correspond to the ith element of Tm (x). The refinement 
procedure should abate c5tiYl if the addition of c5tiYl to T,(Yl 
tends to widen the nearby formant bandwidth. For the LSF 
representation , a noticeable formant occurs if neighboring 
LSFs are very close and the formant frequency lies in the 
vicinity of these LSFs. Hence , this paper proposes using the 
LSFs in Tm (x) to estimate the inclination of formant 
formation. 
(12) 
where VI signifies the relative position of T,(Y) in a line 
segment from T,�f) to T,H) . If Vi is close to 0, any shift 
toward a smaller value will yield a sharper formant. In 
contrast , moving tiY) (i.e. T,(Y) + c5i,(Y» ) to a larger value is 
not preferable since this may weaken the current formant. 
This suggests that c5tiY) should be decreased. Likewise , 
when Vi approaches 1, a shift toward a larger value causes a 
b andwidth reduction. Hence , it is better to trim the c5tiY) 
before adding it to the mean term. Figure 2 illustrates the 
process of either enhancing or flattening the formants. The 
adaptive process can be implemented using a sigmoid 
function: 
c5t(Y) . = 1 c5t(Y) (13) 1, mod�fied 
1 
pxsign(O.5-vi ) lo.5-vi I I 
+ e 
where p is set at 25 to obtain the desired effect. 
Moving direction 
for formant sharpening 
• 
,-------, � �()) �!{) � ,----, 
� - . .  � J/,( Y) J/(Y) 
......- �I 
Moving direction for 
fonnant flattening 
• 
Figure 2. Illustration of how to adjust the offset c51,�i to enhance 
formants. 
V I. HARMONIC VARIATIONS ( HV) 
Although the LSF is an efficient representation of the 
spectral envelope , it is still unable to reveal the details of 
harmonics. Using the harmonics as input variables appears 
impractical since this will complicate the conversion 
function. To exploit the merit of the HNM within a 
manageable extent , this study keeps the HV information in 
the first 13 critical bands given in Table IV. This strategy is 
similar to the one adopted by the M ELP vocoder , which 
encodes the first ten pitch h armonic magnitudes [20]. 
The spectral variation in each critical band is 
characterized by the power ratio of the harmonics to the LP 
spectrum. 
112 
Yi = (14) 
_
1 r /�g /2 df 
Ai Ai_I ,-1 AU) 
where Ai is the upper bound of the ith critical band , 
I ! IAU)I denotes the magnitude response of the synthesis 
filter , and g is the associated gain. On the other hand , a(k) 
represents the kth magnitude of the pitch harmonics and NI 
is the number of pitch harmonics within the band. The 
denominator in Eq. (14) provides the average magnitude 
spectral density , while the numerator derives a similar figure 
using harmonics. When no pitch harmonic exists in a 
specific band , the corresponding HV can be simply derived 
by interpolation. 
TABLE IY. THE UPPER BOUNDS OF THE 13 CRITICAL BANDS. 
Number I 2 3 4 5 6 
Critical band 0 100 200 300 400 510 630 boundary A.i (Hz) 
Band Number 7 8 9 10 11 12 13 
Critical band 770 920 1080 1270 1480 1720 2000 boundary A.i (Hz) 
285 
In the XAB test, listeners were asked if the converted 
speech "X" sounds like the utterance delivered either by the 
source speaker "A" or by the target speaker "B". The 
listening stimuli are those previously used for the MOS test. 
As also seen in Table V, most of the converted speech is 
regarded as the target speaker's utterance. The correct rate 
for the fourth kind stimuli reaches 100%. However, these 
preliminary experimental results are insufficient to make any 
conclusion about how well the conversion system actually 
performs. Some disputes arise when the source and target 
speakers are of different genders. The listener may also use 
pitch as a cue to discriminate one type of speech from 
another. Apparently, more experiments should be conducted 
for the conversion between voices uttered by the same 
gender. 
IX. CONCLUDING REMARKS 
This study presents an efficient approach called HMM­
WDL T to convert spectral parameters. Compared with the 
CM, GMM, and PDL T approaches, the HMM-WDL T 
demonstrates superior ability in terms of average spectral 
distortion. The proposed voice conversion system employs 
two functions to convert the HNM parameters, and provides 
several new features such as sharpening the converted 
formants, estimating the frame duration and tracking the 
harmonic variation below 2 kHz. The combination of all 
these features leads to quality improvement and allows easy 
identification of the speaker's individuality. 
Listening tests results indicate that the proposed system 
is capable of converting one speaker's voice to another's 
with satisfactory quality. However, further investigation is 
needed to justifY the effectiveness of the proposed system in 
converting voices of the same gender. 
ACKNOWLEDGMENT 
This research was supported by the National Science 
Council, ROC, under Grants NSC96-2221-E-197-020-MY2 
& NSC98-2221-E-197-019. 
REFERENCES 
[I] E. P. Neuburg, "Dynamic frequency warping, the dual of dynamic 
time warping", 1. Acoust Soc. Am. Volume 81, Issue SI, pp. S94-
S94, 1987. 
[2] H. Valbret, E. Moulines, and J. P Tubach, "Voice transformation 
using PSOLA techniques," Speech Comm, Vol. II, pp. 175-187, 
1992. 
[3] M. Narendranath, A. Hema, S. Rajendran, and B. Yegnanarayana, 
"Transformation of formants for voice conversion using artificial 
neural networks," Speech Comm, Vol. 16, No. 2, pp. 207-216. 
[4] H. Mizuno and M. Abe, "Voice conversion algorithm based on 
piecewise linear conversion rules of formant frequency and spectral 
tilt," Speech Commun., vol. 16, no. 2, pp. 153-164, 1995. 
[5] D Rentzos, S. Vaseghi, and Q. Van, "Parametric formant modelling 
and transformation in voice conversion" International Journal of 
Speech Technology, vol. 8, pp. 227-245, 2005 
[6] E. K. Kim, S. Lee, and Y. H. Oh, "Hidden Markov model based voice 
conversion using dynamic characteristics of speaker," Proc. 
EUROSPEECH, vol. 5, Rhodes, Greece, 1997. 
[7] L. M. Arslan, "Speaker transformation algorithm using segmental", 
codebooks (STASC), Speech Communication, 28, pp. 211-226, 1999. 
[8] M. Savic and 1. H. Nam, "Voice personality transformation," Digital 
Signal Process., vol. 4, pp. 107-110, 199J. 
[9] K. S. Lee, "Statistical Approach for Voice Personality 
Transformation," IEEE Trans. on audio, speech, and language 
processing, vol. 15, no. 2, 2007. 
[10] Y. Stylianou, O. Cappe, and E. Moulines, "Continuous Probabilistic 
Transform for Voice Conversion," IEEE Trans. on Speech and Audio 
Processing, Vol. 6, No. 2, pp. 131-142, 1998. 
[II] M. Abe, S. Nakamura, K. Shikano, and H. Kuwabara, "Voice 
conversion through vector quantization," Proc. ICASSP, pp. 655-658, 
1988. 
[12] Z. Vue, X. Zou, Y. Jia, and H. Wang, "Voice conversion using 
HMM combined with GMM", in Congress on Image and Signal 
Processing, vol. 5, pp. 366-370, 2008. 
[13] Y. Stylianou, "Applying the Harmonic plus Noise model in 
concatenative speech synthesis", IEEE Trans. Speech and Audio 
Processing, Vol. 9, No. I, pp.21-29, 2001. 
[14] Y. Stylianou, "Removing linear phase mismatches in concatenative 
speech synthesis", IEEE Trans. Speech and Audio Processing, Vol. 9, 
No. 3, pp. 232-239, 2001. 
[15] H. T. Hu and Y. Chu "Narrowband-to-wideband expansion of 
telephony speech using piecewise deviation linear transformation", 
International Journal of Electrical Engineering, Vol. 17, No.1, pp. 7-
17, 2010. 
[16] H. T. Hu and C. Yu, "Combining HMM and Weighted Deviation 
Linear Transformation for Highband Speech Parameter Estimation", 
IEICE Transactions on Information and Systems, Vol. E92. D , No. 7, 
pp.1488-1490, 2009. 
[17] P. Jax and P. Vary "On artificial bandwidth extension of telephone 
speech," Signal Processing, 83, 1707-1719, 2003. 
[18] L. Rabiner and B. H. Juang, Fundamentals of speech recognition, 
Prentice-Hall, 1993. 
[19] Y. Chen, M. Chu, E. Chang, J Liu, and R. Liu, "Voice Conversion 
with Smoothed GMM and MAP Adaptation", Proc. EUROSPEECH, 
pp. 2413-2416, 2003. 
[20] A. McCree, K. Truong, E. B. George, T. P Barnwell, and V 
Viswanathan, "A 2.4 kbit/s MELP coder candidate for the new US 
Federal Standard," in ICASSP, No. I, pp. 200-203, 1996. 
287 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/27
國科會補助計畫
計畫名稱: 一種兼具強韌及高容量數位音訊浮水印之多場域架構
計畫主持人: 胡懷祖
計畫編號: 98-2221-E-197-019- 學門領域: 自然語言處理與語音處理 
研發成果名稱
(中文) 依憑心理聲響模型之自調適量化索引調變技術
(英文) An adaptive quantization index modulation technique emerging from the psychoacoustic 
model
成果歸屬機構
國立宜蘭大學 發明人
(創作人)
胡懷祖
技術說明
(中文) 本技術係依以心理聲響模型將小波包裹係數切分為25個臨界頻帶，並根據各頻帶
內的頻譜結構分析其對於浮水印的遮蔽效果，從而推算適合用於量化索引調變之
步階大小，使浮水印嵌入過程所造成的失真均控制在人耳難以察覺的範圍。然而
最具關鍵的部分當屬如何讓音訊信號在嵌入浮水印之前與之後所算出的量化步階
一致，其後方能解出正確的浮水印資訊。 
 
在數位多媒體資料裡頭加入浮水印應是解決智財爭端的最佳手段。為增進浮水印
在真偽鑑別、保密通信、電子身份認證等方面之應用價值，所研發的技術是屬於
不必仰賴原始音訊的盲水印，尤其是充分利用聽覺特性將浮水印藏入各個指定場
域，藉此達成隱匿、容量、強健等多重要求。
(英文) The developed technique divides the wavelet package coefficients into 25 critical bands 
according to the psychoacoustic model. For each band, the auditory masking level for 
watermarking is estimated based on the spectral characteristics, leading to the derivation 
of an adequate step for quantization index modulation. Hence the distortion caused by 
watermarking can be controlled within an imperceptible range of human ears. 
Nevertheless, the most crucial part of this technique is to make the extracted quantization 
steps from the watermarked signals to be consistent with that used in the embedding 
processing. Only a perfect acquisition of the quantization step can guarantee the retrieval 
of correct watermark information. 
產業別 研究發展服務業；其他專業、科學及技術服務業
技術/產品應用範圍
常見的數位浮水印的應用類型有廣播監控(Broadcast Monitoring)、版權確認(Owner 
Identification)、認證(Authentication)、指紋鑑別(Fingerprinting 
discrimination) 、複製管制(Copy Control)、秘密通訊(Covert Communication)等幾種
技術移轉可行性及
預期效益
因應智財保護之需要之附加技術，適合數位多媒體資料。
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
件數 0 0 100% 件  技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重要
國際合作、研究成果
國際影響力及其他
協助產業技術發展
之具體效益事項
等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
