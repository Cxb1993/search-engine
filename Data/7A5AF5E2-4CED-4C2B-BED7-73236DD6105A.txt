 2
應用於格式適合化系統之協同與分散式的快取機制 
A Coordination and Distribution Cache Mechanism for Transcoding 
Systems 
計畫編號: NSC 94-2213-E-110-040 
執行期間: 94 年 8 月 1 日 至 95 年 7 月 31 日 
主持人：李宗南 教授  國立中山大學資訊工程系 
 
一、中文摘要 
 
這個計畫將設計一個創新的快取機制針
對在一個協同和分散式的快取系統中的客
戶端裝置與格式適合化快取伺服器。在這
個系統中，客戶端可以彼此分享快取的內
容。由於這樣的特性，所使用的快取演算
法不僅需要去考慮儲存相同物件的多個版
本在快取伺服器之中的效益，也需要分別
去計算儲存物件的某些版本在快取伺服
器，客戶端，或同時在二者之中的效益。
我們將提出一個效益計算函式去評估不同
的快取狀態所獲得的效益。 效能的分析包
含節省的延遲時間和位元組命中率的評
估。這個計畫也將建構一個雛形系統，這
個系統將應用這個快取機制到 WWW 服務
和內容適合化系統，去證明這個計畫的可
行性 
關鍵字：分散式快取，格式適合化快取伺
服器，效益計算函式 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Abstract: 
 
This project proposes a new cache 
mechanism for clients and transcoding 
proxies in a coordination and distribution 
cache environment, from which clients can 
share cached contents each other. Due to the 
characteristics of such environment, the 
cache algorithm not only needs to consider 
the aggregate profit from caching multiple 
versions in the proxy, but also the 
combination profit for caching some versions 
in clients or both. We will propose an 
extended generalized profit function to 
estimate the benefit from reserving each 
version of objects in proxies, clients or both. 
Performance analysis includes the estimation 
of the delay saving ratio and byte hit ratio. 
We also will construct a prototype using the 
proposed algorithm for the WWW services 
and content adaptation systems to verify the 
feasibility of this project. 
Keywords: Distribution cache, Transcoding 
proxies, Generalized profit function. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Accepted to publish in IEEE Transactions on Multimedia 2
 
the full, original version and serves requests to lower 
versions with transcoding. However, the transcoded objects 
are not cached. TVO (Transcoded Version Only) always 
caches the transcoded objects, and if a request does not 
yield an exact hit, the full version is fetched from the origin 
to generate a transcoded version. Shen et al. [13] developed 
a transcoding-enabled caching (TeC) system that performs 
transcoding and caching for efficient rich media delivery to 
heterogeneous network users. However, the TeC strategy 
fails to make optimal cache replacement decisions and uses 
only the least recently used (LRU) as the replacement 
policy.  
Hence, an appropriate video profit function must be 
formulated to estimate the aggregate profit obtained by 
caching multiple versions of the same video object, and an 
innovative cache replacement algorithm must be designed 
for video streaming transcoding systems. The main 
contributions of this paper are as follows. 1) The impact of 
the transcoding relationships among the cached versions of 
the same video object during startup delay are estimated. 2) 
The fact that the byte hit saving obtained by caching a new 
and detailed version is associated not only with the 
demands on the current version but also with the demand 
for on other versions of the videos. 3) New cache 
replacement algorithms and a generalized video object 
profit function are designed to meet the demand for a 
streaming media transcoding proxy system. 
  The rest of this paper is organized as follows. Section 2 
introduces the proposed system environment. Section 3 
presents the cache replacement algorithms for video 
transcoding systems. Section 4 presents a performance 
analysis of the competing cache algorithms, and Section 5 
draws conclusions.  
 
2. SYSTEM ENVIRONMENT 
  Figure 1 depicts the components of the transcoding proxy. 
The proxy acts as an RTP/RTSP client that receives the 
streamed content from the content server. The request 
procedures are described as follows: 1) RTP/RTSP client 
module in a variety of devices build connections with the 
RTP/RTSP server module of the transcoding proxy. 2) The 
server module of the transcoding proxy checks whether the 
cache system has the requested stream. 3) If it is cache hit 
(cache system has a match version), the cache system 
transmits the requested stream to output buffer for 
transmission. 4) If it is transcoding hit (cache system has a 
more detailed version), the cache system transmits the more 
detailed stream to transcoder for transcoding. 5) If it is 
cache miss, the cache system forwards the request to the 
RTP/RTSP client module of the proxy. 6) Then, the client 
module builds connections with RTP/RTSP server module 
of the server to get the requested stream into the input 
buffer. 7) If the obtained version of the input buffer 
matches with the requested version, forwarding the stream 
to the cache system for caching consideration. 8) If the 
obtained version of the input buffer is more detailed than 
the requested version, forwarding the stream to the 
transcoder for transcoding and then to the cache system for 
caching consideration. Finally, the RTP/RTSP server 
module of the transcoding proxy that communicates with 
the RTP/RTSP client module that is built into the devices to 
deliver multimedia content. 
  The speed of the transcoding process is measured by the 
transcoding bit rate, which is the number of bits generated 
by the transcoder per second. When the transcoding bit rate 
exceeds the playback bit rate, then the transcoding delay 
from the transcoding process is the transcoding time of the 
first several segments of the requested video. Afterwards, 
enough transcoded segments will be delivered to the end 
users, so the subsequent transcoding process runs in 
parallel with the delivery and the display of the content. 
Therefore the transcoding delay can be calculated from the 
first several segments rather than all of the segments in the 
following discussion. In video transcoding, a higher bit-rate 
version x can be transcoded into a lower bit-rate version y. 
Version x is called the transcodable version of y, and 
version y is called the transcoded version of x.  
 
  
Fig. 1. System architecture of the transcoding-enabled caching proxy.
 2
Accepted to publish in IEEE Transactions on Multimedia 4
 
accesses of version v of object n. The average duration of 
access of version v of object n is denoted by Dn,v , which is 
is given by .  represents the bit rate for 
version v. The mean fetching delay for first several frames 
of version v of video object n from the remote server to the 
transcoding proxy is denoted by d
vn,vn, N/ T vR
n,v. The fetching delay is 
defined as the interval between the sending of the request 
and the receiving of sufficient data to play the film. The 
cached size of version v of object n is Sn,v. The 
corresponding extended weighted transcoding graph of 
object n is Gn and the cost of transcoding object n from 
version v to version  is the weight on the edge (v, ) in 
E[G
v' v'
n], which is denoted by wn(v, ). The transcoding 
length from version v to version  is L
v'
v' n (v, ). Table 1 
lists symbols used in this paper. They are used in estimating 
the profit from caching version v of object n. 
v'
Table 1 
List of symbols 
 
First, the profit from caching a single version of an 
object in the video transcoding proxy, when no other 
versions are cached, is considered. For client users, an 
optimal cache replacement algorithm is expected to 
minimize the response time. In other words, it is designed 
to maximize bandwidth saving and reduce network traffic 
by caching an optimal set of segments of video objects. 
Thus, the individual profit from caching the Dn,v-length 
segments for version v of object n is calculated from the 
delay saving and bandwidth saving as follows.  
Definition 1. VPF(Dn,v, on,v) is a function used to calculate 
the individual profit for caching Dn,v -length segments of 
on,v , when no other version of object n is cached.  
 .
))',()',(()',(
),(
,
][)',(
1','',
,,
vvn
GEvv
nnvnvvn
vnvn
RD
vvwvvwdRvvLr
oDVPF
n
×
−+×××
=
∑
∈
  (1) 
If the server can provide all versions of each video object, 
then the profit function is reformulated as follows. 
 .
))',(()',(
),(
,
][)',(
','',
,,
vvn
GEvv
nvnvvn
vnvn
RD
vvwdRvvLr
oDVPF
n
×
−×××
=
∑
∈
           (2) 
Notably, every (v, ) v' ∈  E[Gn] represents a transcoding 
from version v to version The expression v'.
')',( vRvvL ×  specifies the consumed bandwidth, and the 
expression )',( 1, 1 vvwd nvn +  represents the delay if 
 is not cached and  is requested. It requires the 
bandwidth to transfer the cache-miss data and the delay to 
fetch the first several segments of the original version of 
the video object n from the server; the delay also includes 
the latency of transcoding from the original version to 
version . Notably, if the server can directly provide all 
versions of a video object, then the fetch delay is  
and the transcoding delay  is unnecessary. 
Thus, equation (1) can be reformulated as (2). Restated, if 
 is cached and  is requested, then  
denotes the delay for transcoding version v to version . 
Thus, the delay saving gained by caching  and 
requesting  is  in 
(1) and 
vno , ',vno
v'
',vnd
)',( 1 vvwn
vno , ',vno )',( vvwn
v'
vno ,
',vno )',()',( 1, 1 vvwvvwd nnvn −+
)',(', vvwd nvn −  in (2).  
However, the transcoding proxy may cache multiple 
versions of an object simultaneously if it can gain more 
aggregate profit from caching them together. The 
transcoding relationship and the costs among different 
versions of an object determine how much aggregate profit 
can be obtained. As mentioned above, given a weighted 
transcoding graph, the MATC procedure can be used to find 
the subgraph of a graph that represents the transcoding 
relationship for a web object. When objects are too large to 
be cached completely, a partial cache should be employed. 
Therefore, the average access duration of the cached object 
is added to the graph to yield a subgraph  of 
the transcoding relationship that maximizes the delay 
saving and minimizes the network traffic when 
simultaneously caching multiple versions of a partial video 
object. 
n
'
n GG ⊆
The proposed Minimal Partial Transcoding Cost (MPTC) 
procedure illustrated in Fig. 5, is used to find the subgraph. 
Suppose that Gn and wn are given by the graph in Fig. 5(a) 
and the cached versions are 1 and 2. In Fig. 5(b), vertex 1 
and vertex 2 are gray, and an average access duration is 
assigned to each version. Assume Edge_sub is the set of 
resulting edges of the subgraph to be returned. In this 
example, edges (1, 1) and (2, 2) are the only edges that 
point from the gray vertices to themselves, so edge (1, 1) is 
added to the Edge_sub set, as shown in Fig. 5(c). Also edge 
(2, 2) is added to the Edge_sub set, as shown in Fig.5(d). 
This fact indicates that when version 1 or 2 is requested, the 
proxy can respond to them without transcoding. In Fig. 5(e), 
for vertex 3, edge (2, 3) is added to Edge_sub set, because 
w(2,3) < w(1,3). Version 2 only provides video clips of 
length 45(min) to version 3, so version 1 needs to provide 
45-55 minute clips to version 3. Thus edge (1,3) with 
L(1,3)=10 is added to the Edge_sub set, as shown in Fig. 
5(f). For vertex 4, edge (2,4) in the Edge_sub set is added 
because w(2,4) < w(1,4), as shown in Fig. 5(g). For 
example, version 2 provides video clips only of length 
45(min) to version 4, so version 1 must provide 45-55 
 4
Accepted to publish in IEEE Transactions on Multimedia 6
 
 
 
4. PERFORMANCE ANALYSIS 
This section evaluates the performance of the proposed 
cache replacement algorithms using an event-driven 
simulation. 
4.1 Simulation Model  
  The system environment of the transcoding proxy for 
simulation is described in Section 2. A repository of 500 
CBR video clips, whose lengths are uniformly distributed 
(5-120 minutes) is considered. For the client model [18], 
five classes of mobile devices are distributed as a device 
vector, < 15%, 20%, 30%, 20%, 15% >. Five classes of 
mobile clients are involved, so without loss of generality, 
the bit rate of the five versions of each media object are 
assumed to be 512, 256, 128, 64 and 32 kbps. The 
transcoding delay for the first sufficient segments of 
version i is determined to be 200ms/i. Restated, the 
transcoding process for the higher bit-rate version is more 
costly than that for the lower bit-rate version. (For version 
1, it is 200ms, which exceeds 100ms for version 2.) The 
popularity of the video objects follows a Zipf distribution 
with a skew factor α of 0.47 [19]. The simulation lasts 400 
hours with 100000 accesses that follow a random Poisson 
distribution. Consequently, the average access inter-arrival 
time is 14.4s. The default cache capacity is assumed to be 
0.2*(∑ ). The delays for fetching 
the first several segments of video objects from the server 
are exponentially distributed (μ = 1.5 seconds). As shown 
elsewhere [20], the correlation between the size of the web 
object and the fetching delay is relatively low. Although the 
sizes of video objects vary substantially with bit rate, the 
fetching delay is considered for only the first several 
segments of the video objects. Thus, no correlation is 
assumed to exist between the fetching delay and the 
different bit-rate versions of the objects. In the simulation 
environment, the content origin server provides multiple 
bit-rate versions of each video object, and equation (4) is 
used as the estimation function. The performance is 
evaluated by performing two kinds of simulations, which 
separately use complete viewing traces and partial viewing 
traces. The complete viewing trace indicates that users 
always request the complete video sequence, and the partial 
viewing trace indicates that users only request a part of the 
video sequence. The ratio of the access duration to the total 
duration of a video sequence in a partial viewing 
environment is exponentially distributed (μ = 0.5).  
sizeobject  rate-bit 128kbps
 
4.2 Experimental Results 
Two performance metrics - byte hit ratio and delay 
saving ratio are employed. The byte hit ratio is defined as 
the number of bytes that are delivered to the client directly 
from the proxy divided by the total number of bytes 
requested by the client. It is the major metric for evaluating 
the reduction of the network traffic to the server. The delay 
saving ratio is defined as the ratio of the total saved delay 
associated with cache hits to the total delay without caching. 
The proposed algorithms are respectively referred to as 
VPFcomplete, VPFcomplete-ver1, VPFpartial, and VPFpartial-ver1, based 
on their cache policy. VPFcomplete always caches the 
complete video object with more profits. VPFcomplete-ver1 
uses a policy that is similar to that of VPFcomplete, but it 
caches only one version of a certain object. VPFpartial caches 
the mean access duration of the video object with more 
profits. VPFpartial-ver1 uses a policy similar to that of VPFpartial, 
but it caches only one version for a certain object.  
Since the TeC system [13] has analyzed extensive 
caching conditions for transcoding proxy, including 
caching one version or multiple versions, caching lower 
bit-rate transcoded version or higher bit-rate transcodable 
version, the proposed algorithms are compared with the 
TeC system. The TeC system consists of three caching 
algorithms. Both TEC-11 and TEC-12 cache at most one 
version of a video object into the proxy at any time. When a 
transcode hit occurs, TEC-11 refreshes the access record of 
the already cached object (higher bit-rate) without caching 
the newly transcoded version. However, TEC-12 may evict 
the transcodable version (higher bit-rate) from the cache 
and store the newly transcoded version. However, TEC-2 
may cache multiple versions of the same video. The TeC 
system uses only LRU as the cache replacement algorithm 
in combination with their proposed caching strategies.  
Two simulations, separately using complete viewing 
traces and partial viewing traces, are conducted. Notably, 
with the complete viewing traces, VPFpartial determines the 
cache set that is the same as that determined by VPFcomplete. 
Therefore, the simulation results for complete viewing 
traces are denoted only by VPFcomplete and VPFcomplete-ver1 in 
the following figures. 
4.2.1 Impact of Cache Capacity  
  In the first experiment, the variation of the performance 
of the proposed cache replacement algorithms with cache 
capacity is investigated. Figs. 6 and 7 plot the results of the 
evaluations of the complete and partial viewing traces, 
respectively. As shown in Fig. 6, the proposed algorithms 
outperform the others by 14-17% in delay saving ratio, and 
by 5-7% in byte hit ratio, because the TEC systems based 
on LRU algorithms ignore the size and the popularity of the 
objects as well as the delay. Notably, VPFcomplete-ver1 
outperforms VPFcomplete in delay-saving ratio, while 
VPFcomplete outperforms in byte hit ratio because 
VPFcomplete-ver1 caches a variety of objects to reduce the fetch 
delay, but VPFcomplete specifically considers the versions or 
objects with the most profits based on the profit function to 
be cached to increase the byte hit ratio. 
  As shown in Fig. 7, VPFpartial outperforms VPFcomplete by 
19-29% in delay saving ratio, and 2-5% in byte hit ratio, 
because VPFpartial only caches the mean access clips of an 
object rather than the complete object, to avoid caching 
segments with little access. This selectivity allows it to 
cache a variety of objects to increase delay saving and byte 
hit ratios. VPFpartial-ver1 outperforms VPFpartial in delay saving 
ratio, but VPFpartial outperforms in byte hit ratio, because 
VPFpartial-ver1 caches segments of a variety of objects to 
reduce the fetch delay, but VPFpartial specifically considers 
segments of the versions or objects with the most profits, 
based on the profit function, to increase the byte hit ratio. 
However, VPF partial-ver1 has a similar byte hit ratio to 
VPFcomplete. 
4.2.2 Impact of Object Popularity 
  This experiment is conducted to investigate the 
performance of the proposed and the competing caching 
algorithms, by adopting different values of α in the 
Zipf-like distribution. The α parameter determines the 
access pattern of the system. A larger value indicates that 
the accesses are biased to a certain set of popular objects. If 
 6
Accepted to publish in IEEE Transactions on Multimedia 8
 
 
(a)                                                            (b) 
Fig. 9. Evaluation on partial viewing traces: (a) Delay Saving Ratio, and (b) Byte Hit Ratio under various values of Zipf parameter.
the objects were assumed to follow a normal distribution 
with mean m, which stands for the dominant version. The 
access probability of a version x is 
       ,
2
1)(
22 2/)( σ
σπ
mxexp −−=        (5)                            
  Figures 10 and 11 show that the VPF algorithms use the 
cache space more efficiently, and have higher delay saving 
and byte hit ratios, regardless of popularity. As σ increases, 
accesses are more heterogeneous; and each version is more 
evenly accessed. Consequently, the caching performances 
of all of the algorithms drop. In Fig. 10(b), when σ is small, 
the byte hit ratio of VPF
where m and σ2 represent the mean and the variance, 
respectively, of the accesses of the versions. When σ is 
small, one version (m) tends to be preferentially accessed. 
As σ is large, the accesses are evenly distributed among the 
different versions. The access model explores the effect of 
the concentration of accesses to versions on caching 
performance. In this experiment, m=2 was set (such that the 
128kbps version was dominant when σ was small) and m=4 
was set (such that the 32kbps version was dominant version 
when σ was small). 
complete is close to that of 
VPFcomplete-ver1 because VPFcomplete-ver1 specifically caches 
only the most frequently accessed version. However, as σ 
increases, VPFcomplete increasingly outperforms 
VPFcomplete-ver1. A similar phenomenon occurs with the 
partial viewing traces. In Fig. 11(a), the VPF algorithms 
perform worse when σ=0.2 than when σ=0.4, because when 
σ=0.2, most of the accesses are to the 128kpbs version. 
Thus, the VPF algorithms cache the 128kpbs version of 
object. However, when σ=0.4, the VPF algorithms may 
cache other simple versions, such as 32kbps and 64kbps, to 
reduce the fetch delay. 
 
 
(a)                                                            (b) 
Fig. 10. Evaluation on complete viewing traces: (a) Delay Saving Ratio, and (b) Byte Hit Ratio under various values of σ (m=2). 
  
(a)                                                             (b) 
Fig. 11. Evaluation on partial viewing traces: (a) Delay Saving Ratio, and (b) Byte Hit Ratio under various values of σ (m=4).
 8
