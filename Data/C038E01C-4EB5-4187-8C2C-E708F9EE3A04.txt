I 
 
中文摘要 
 
在本計畫中提出一套結合空間域與頻率域之數位半色調影像浮
水印演算法。我們利用 dither halftoning 之半色調影像轉換
方法，同時利用多個臨界值矩陣的選擇，做為浮水印嵌入之依據。
我們亦引用了 Binary Pseudo-wavelet Transform，將浮水印成
功的嵌入在半色調影像之頻率域中。為了使已嵌入浮水印之半色
調影像更為強健，且影像品質更高，在計畫中，我們引用了粒子
群最佳化演算法求得臨界值矩陣之最佳化。同時，為了能有效且
充分的表現數位半色調影像之品質及失真程度，文中我們亦引用
粒子群最佳化演算法尋求一符合人眼特性且能充分表現半色調
區塊影像蘊含之灰階值的平滑濾波器，藉以衡量半色調數位影像
之品質及失真程度。 
數位灰階影像轉換為數位半色調影像的過程中嵌入浮水印，應避
免直接從空間域上大幅修改其係數值，影響半色調影像品質與視
覺效果；為了提高浮水印系統嵌入容量與增加強健性，我們在具
有空間域浮水印之半色調影像中，利用小波轉換找出紋理變化大
以及灰階值極大與極小之區域，當作頻率域浮水印嵌入之特定區
塊。之後利用更改特定區塊之高頻係數值，嵌入頻率域浮水印。
我們期待實本文所提出之數位浮水印系統可抵抗旋轉、縮放、黑
色塗鴉、剪裁、胡椒鹽雜訊以及列印與掃瞄攻擊，充分代表其具
有非常好的強健性。 
 
 
關鍵詞: 浮水印，半色調影像，粒子群最佳化演算法，二元類小
波轉換 
  
1 
 
利用粒子群最佳化演算法於混合型數位半色調影像浮水印系統 
The hybrid digital halftone image watermarking system using particle 
swarm optimization
Abstract 
 
we propose a PSO based multi-purpose watermarking scheme for halftone image by 
dithering method. One of the optimized dispersed-dot dither cells achieving the 
robustness and transparency would be selected as the embedding patterns. The 
embedding positions of the robust watermarks determined by the human visual 
characteristics would increase the quality of the watermarked halftone image measured 
by the modified PSNR.  
We propose the improved watermarking scheme that the support vector machine 
(SVM) would be adopted as the classification method in the extraction method. The SVM 
used to watermark extraction system could enhance the robustness of the watermarking 
scheme. We would also discuss the resynchronization system for the print-and-scan 
attacks. The automatic procedure of resynchronization for the print-and-scan attacks is 
proposed.  
 
1. Introduction 
 
Image watermarking provides a solution for protecting the copyright of digital contents. 
In recent years, several watermarking methods for halftone images have been proposed. 
Digital halftone images are widely used in the printing of books, magazines, newspapers 
and computer printers [1-2]. It is a crucial step in printing or displaying images on limited 
3 
 
easily generate the other illegal watermarked images with these two cells that the system 
lacks of security.  
Here, we propose the improved watermarking scheme that the support vector machine 
(SVM) would be adopted as the classification method in the extraction method. The SVM 
used to watermark extraction system could enhance the robustness of the watermarking 
scheme. The automatic procedure of resynchronization for the print-and-scan attacks is 
proposed.   
SVM is a novel machine learning method introduced by Vapnik[11] in the 1990s and 
has been successfully applied to numerous classification and pattern recognition 
problems such as image recognition [12-13] and text categorization [14]. SVM is based 
on minimizing the structural misclassification risk using the Vapnik-Chervonenkis (VC) 
domain. A significant advantage of the SVMs is that their performance and complexity 
are independent of the dimensionality of the input domain and the performance of SVMs 
is superior to that of traditional learning models indicated by recent literatures [12-13]. It 
has been successfully applied in various applications including not only pattern 
recognition, classification, but also digital watermarking. In [14], Tsai proposed a 
watermark detection algorithm in spatial domain using SVM and in [15], Li also 
proposed the watermark embedding scheme based on SVM for copyright protection and 
authentication. 
In section 2, we will describe the watermarking scheme in dithered halftone images. 
And the proposed improved watermarking system in halftone images is shown in section 
5 
 
ECC coding and random permuting, denoted as W′R୭ୠ. dither cell of each sub-block will 
be saved as nDS . nDS  is dither cell selection in the nth sub-block. The procedures of robust 
watermarks embedding system are described as follows:  
1. The host image I is segmented into sub-images of size N ൈ N pixels.  
2. For the nth sub-image I୬ , the cell C଴  is adopted to be the dither cell when the 
corresponding robust watermark W′R୭ୠ୬  is “0”. Cell Cଵ  will be selected when the 
corresponding robust watermark W′R୭ୠ୬  is “1”. 
3. Halftone the sub-image by (1) with the dither cell T, C଴ or Cଵ, selected by step 2. And 
get the halftone sub-image H୬. 
In our simulation, the robust watermarks are embedded in all points of the image. 
However, the watermarking scheme described above is not robust to scaling attack. In 
order to enhance the robustness of the watermarking system, the other robust watermark 
is essential to embed in the watermarked image. Here, the hybrid watermarking system is 
proposed that another watermark is embedded based on the frequency domain upon the 
former watermarks based on spatial domain that the frequency domain based watermarks 
should be independent to the spatial domain based watermarks described above. In the 
first step, the sub-image ܪ௡ is transformed into the frequency domain by the BPWT and 
denoted as ܪ஻ௐ௡ . For each block, the watermarks should be embedded into the high 
frequency BPWT coefficient of the watermarked image with robust watermarks. The 
equation is shown as follows:  
ܪ஻ௐ௡ ሺ63ሻ ൌ ቊ0, if ܹ
′R୭ୠ
௡ ൌ 0 
1, if ܹ ′R୭ୠ௡ ൌ 1
                                                                                (2) 
7 
 
For each sub-image, after the BPWT, the frequency domain based watermarks are 
extracted from the highest frequency component of BPWT coefficients. 
5. For the nth sub-image HഥH୷୬  , after the BPWT, extract the highest frequency components 
denoted as HഥH୷BW୬ ሺ63ሻ. 
6. Extract watermark Wഥ ୤୰ୣ୯୬  by the following equation. 
Wഥ୤୰ୣ୯୬ ൌ ቊ
1, if HഥH୷BW୬ ሺ63ሻ ൌ 1 
0,   if HഥH୷BW୬ ሺ63ሻ ൌ 0  
7. De-permute and extract the frequency domain based watermark by ECC decoding. 
 
Fig.1 The proposed improved watermark embedding systems during halftoning. 
 
 
 
Fig.2 The proposed watermark extracting systems. 
9 
 
ܪഥொ஺ே௡ ൌ ∑ ∑ ݂ሺݑ, ݒሻ௨,௩אோ ܪഥ௡ ሺ݅ ൅ ݑ, ݆ ൅ ݒሻ                                                          (11) 
ܦ݂݂݅௡ ൌ ∑ ∑ หܪ஼బ௡ ሺݔ, ݕሻ െ ܪഥ௡ ሺݔ, ݕሻหே௬ୀଵே௫ୀଵ                                                              (12) 
ܪഥொ஺ே௡  is the reconstructed gray-level image in the nth sub-images; f is the visual lowpass 
filter generated by PSO. ܦ݂݂݅௡ is the distance between the test nth sub-image and the 
reconstructed nth reference sub-image. 
If the distance equals to zero, it means that the halftone test sub-image is generated by 
the dither cell C0 in the embedding stage, and the corresponding watermark could be 
detected. Note that only one dither cell is needed in the detecting procedure different 
from other proposed method discussed in section 2. One simple watermark extracting 
method to achieve the goal is using the thresholding method to classify the sub-images 
into two groups according to the distances and detect the corresponding watermark. The 
equation of the thresholding based simple watermark extracting system is described as 
follows.  
ഥܹ୭௡ ൌ ൜0,   ܦ݂݂݅
௡ ൏ ߙ
1,   ݋ݐ݄݁ݎݓ݅ݏ݁                                                                                              (13) 
where α is the threshold value. The halftone region which the distance less than threshold 
would be selected as the dither cell for halftoning and extract corresponding watermark 
bit. However, it is difficult to find an appropriate value for the threshold and is also 
sensitive to noise and malicious attacks.  
Here, a machine learning method for the classification is proposed and the proposed 
system is robust to the malicious attacks and noises. In the proposed method, the SVM is 
adopted to be the classification machine and classifies the sub-images into two groups 
11 
 
4. Experimental Results 
 
Table 1  DR of the extracted watermark with noise attack tested by various reference watermark lengths. 
 64 bits 128 bits256 bits384 bits
Salt and pepper 30% 0.7891 0.9701 0.9992 0.9984
Salt and pepper 20% 0.7923 0.9844 1 1 
Salt and pepper 10% 0.9672 1 1 1 
 
 
Table 2 Visual quality simulated by different method. 
Watermarking 
Method Guo [16] Hagit [8] Proposed
PPSNR (dB) 28.0400 26.7517 27.8962
Quality (bits) 1783 4096 4096 
 
 
Table 3 DR of the extracted watermark under tamper-ing attack. 
Watermarking 
Method 
SVM SVM+ECC Guo 
PPSNR (dB) 0.9867 1 0.9958
 
 
Table 3 DR of the extracted watermark attacked by boundary cropping. 
 SVM SVM+ECC Guo 
1x1 1 1 1 
5x5 0.9701 0.9984 1 
10x10 0.9482 0.9922 0.9262 
20x20 0.9161 0.9844 0.8372 
30x30 0.9091 0.9672 0.7543 
 
 
Table 5 DR of the extracted watermark under noise (salt and pepper) attacks. 
 SVM SVM+ECC Guo 
Salt and pepper 30% 0.9977 0.9992 0.9939
Salt and pepper 20% 1 1 0.9989
Salt and pepper 10% 1 1 1 
 
 
Table 6 DR of the extracted watermark under print-and-scan attacks. 
Printing at 150 dpi 
Scanning 2400 1200 600 300 150 
DR 0.984 0.965 0.859 0.848 0.575 
Printing at 300 dpi 
Scanning 2400 1200 600 300 150 
DR 0.958 0.938 0.794 0.512 0.475
Printing at 600 dpi 
Scanning 2400 1200 600 300 150 
DR 0.698 0.715 0.611 0.524 0.469
 
13 
 
different areas respectively. Table 4 tabulates the DR of each extracted watermark from 
the dropped watermarked image. Table 5 shows the decoding rates of the proposed 
watermarking scheme and others. In the robustness simulation described above, it is 
obvious that the proposed watermark has the higher robustness than the Guo’s method 
under malicious attacks. It is worth to note that the error correcting code (ECC) at the end 
of the embedding stage could enhance the robustness of watermarking system. 
In the section, we discuss the robustness of the proposed watermarking scheme for the 
print-and-scan attack. HP LaserJet 1015 printer and an EPSON Perfection V10 scanner 
are utilized in the simulations. Table 6 tabulates the decoding the rate of the watermarked 
images under the print-and-scan attacks. The embedded images are printed at 150, 300, 
600 dpi, and scanned at 150, 300, 600, 1200, 2400 dpi. Experimental results show that 
the proposed method yields good tolerance for print-and-scan attacks. It has the higher 
decoding rate when the image scanning at higher dpi and printing at lower dpi. 
 
 
Figure 3 Tampering attack. 
 
 
15 
 
[3] M.S. Fu and O.C. Au, “A robust public watermark for halftone images,” in Proc. 
Circuits and Systems, 2002, pp. 639 -642.  
[4] M.S. Fu and O.C. Au, “A multi-bit robust watermark for halftone images,” in Proc. 
Multimedia and Expo, 2003, pp. 213-216. 
[5] M.S. Fu and O.C. Au, “A novel self-conjugate halftone image watermarking 
technique,” in Proc. Circuits and System, 2003, pp. 790 –793. 
[6] M.S. Fu and O.C. Au, “A novel method to embed watermark in different halftone 
images: data hiding by conjugate error diffusion (DHCED),” in Proc. Multimedia and 
Expo, 2003, pp. 609 –612 
[7] S.C. Pei, J.M. Guo and H. Lee, “Novel robust watermarking technique in dithering 
halftone images,” IEEE Letter on Signal Processing, 2005, pp. 333-336. 
[8] H.Z. Hel-Or, “Copyright labeling of printed images,” in Proc. Image 
Processing ,2000, pp. 702–705. 
[9] C.T. Hsieh, Y.K. Wu, and K.M. Hung, “Hybrid watermarking scheme for halftone 
images,” Int. Journal of Advanced Science and Technology, 2008, pp. 9 –20. 
[10] C.T. Hsieh, Y.K. Wu, and W.Z. Chung, “Digital watermarking system for halftone 
images based on particle swarm optimization,” in IEEE Proc. Ubi-Media Computing, 
pp.201-206, 2008. 
[11] V. Vapnik, “The nature of statistical learning theory,” Springer, New York, 1995. 
[12] X.Y. Wang, P.P. Niu, and W. Qi, “A new adaptive digital audio watermarking based 
on support vector machine,” Journal of Network and Computer Applications, vol.31, 
pp. 725-749, 2007.  
 
 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期： 年 月 日 
國科會補助計畫 
計畫名稱：利用粒子群最佳化演算法於混合型數位半色調影像浮水
印系統 
計畫主持人：謝景棠         
計畫編號：NSC 97-2221-E-032-025  學門領域：資訊安全 
技術/創作名稱 利用粒子群最佳化演算法於混合型數位半色調影像浮水印系統 
發明人/創作人 謝景棠 
技術說明 
中文: 在本計畫中提出一套結合空間域與頻率域之數位半色調影
像浮水印演算法。我們利用dither halftoning 之半色調影像轉換
方法，同時利用多個臨界值矩陣的選擇，做為浮水印嵌入之依據。
我們亦引用了Binary Pseudo-wavelet Transform，將浮水印成功
的嵌入在半色調影像之頻率域中。為了使已嵌入浮水印之半色調影
像更為強健，且影像品質更高，在計畫中，我們引用了粒子群最佳
化演算法求得臨界值矩陣之最佳化。同時，為了能有效且充分的表
現數位半色調影像之品質及失真程度，文中我們亦引用粒子群最佳
化演算法尋求一符合人眼特性且能充分表現半色調區塊影像蘊含
之灰階值的平滑濾波器，藉以衡量半色調數位影像之品質及失真程
度。 
數位灰階影像轉換為數位半色調影像的過程中嵌入浮水印，應避免
直接從空間域上大幅修改其係數值，影響半色調影像品質與視覺效
果；為了提高浮水印系統嵌入容量與增加強健性，我們在具有空間
域浮水印之半色調影像中，利用小波轉換找出紋理變化大以及灰階
值極大與極小之區域，當作頻率域浮水印嵌入之特定區塊。之後利
用更改特定區塊之高頻係數值，嵌入頻率域浮水印。我們期待實本
文所提出之數位浮水印系統可抵抗旋轉、縮放、黑色塗鴉、剪裁、
胡椒鹽雜訊以及列印與掃瞄攻擊，充分代表其具有非常好的強健
性。 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                         98 年    7 月    31 日 
  報    告    人 
  姓          名  
謝景棠 
 服 務 機 關 
 及   職  稱 
淡江大學電機工程學系 
教授 
           時  間  
  會   議 
           地  點   
2009年6月22日至2009
年 6 月 24 日 
美國拉斯維加斯
Imperial Palace Hotel
 本 會 核 定 
            
 補 助 文 號 
NSC  97-2221-E-032-025- 
                    
會  議  名   稱 
  (中文) 第 18 屆軟體工程與資料工程國際研討會 
  (英文)18th International Conference on Software Engineering and Data 
Engineering (SEDE-2009) 
                    
 發 表 論 文 題 目 
 (1)A Lip Segmentation Using New Threshold Method Towards An 
Automatic Speech Recognition System. 
(2) Facial Image Aging Synthesis Based on the Log-Gabor Wavelet.
(3)PSO accelerated 3D face angle searching system for face
recognition. 
 
一、參加會議經過   
    此次 ISCA 協會所舉辦之第 18 屆軟體工程與資料工程國際研討會，於 2009 年 6 月 22
日至 6月 24 日在美國拉斯維加斯之 Imperial Palace Hotel 舉行，與會者包括國內外相
關領域的學者，會議中並收錄共 51 篇論文，三天的議程，供各國與會人員發表論文，並
參與討論。 
二、與會心得 
數位多媒體資訊被廣泛地運用於我們的日常生活當中，人們可以輕易地在未經擁有者
同意的情況下，恣意地複製與傳播。數位浮水印技術正是一個解決此問題的好方法，實行
數位多媒體版權的認證與智慧財產權的保護。數位浮水印技術中，根據嵌入浮水印的目的
不同，可分為強健型與易碎型浮水印系統，每種系統都有其一定的限制與考量。在數位影
像的應用中，半色調影像相較於灰階影像僅需使用極少的記憶體儲存空間，不僅利於網路
上的傳輸，亦能產生近似灰階的效果，遂其廣泛地利用在網路與列印之相關應用中。 
本次的與會除了參與有關半色調浮水印相關議題的討論外，亦發表了三篇與影像處理
應用有關的論文。參加會議的過程中，有學者發表新一代影像技術，讓我獲益良多; 也有
學者發表機器學習在其他領域的應用，讓我有新的啟發與思維。  
  
distribution using color information to minimize a fuzzy 
entropy measure. Lip segmentation is then performed 
using this distribution. In order to enhance the 
performance of FCM, different modifications of fuzzy 
clustering methods have been proposed. Leung et al. [9] 
has introduced a new dissimilarity measure that 
integrates the color dissimilarity and the spatial distance 
in terms of an elliptic shape function. Because of the 
presence of this elliptic shape function, their method can 
differentiate the pixels having similar color information 
but located in different regions. However, from 
experiment results, it is found that these approaches 
cannot perform satisfactorily, especially when dealing 
with images of weak color contrast. 
In this paper, we propose a new lip contour 
segmentation method that works by choosing dynamic 
threshold from frame to frame. It is performed in two 
stages. To avoid the disturbance of complex background, 
first we use a face detecting method to extract the 
human face as well as the lip ROI, discussed in Section 
II. Then we extract the lip contour by a dynamic 
threshold method. In Section III, we’ve chosen the 
features of the point length of lip contour [7], here we 
found the feature points in lip ROI. Some experiment 
results have been shown in Section IV, and Section V 
gives the conclusion and future work. 
 
Figure 1: Face detection using AdaBoost. 
 
Figure 2: Human face geometric properties. 
2. LIP CONTOUR SEGMENTATION 
Since the methods of lip contour extraction have been 
widely discussed, however, there still exist some problems 
such as complex background and lighting conditions. Based 
on these problems, we propose a new lip contour 
segmentation method that using the dynamic threshold 
method. In this work, it has been performed in two stages. 
First, a face detecting method has been applied to locate the 
lip ROI. Then, a dynamic threshold method has been used 
to extract the lip contour. 
2.1. Lip ROI Location 
To avoid the disturbance of background noise, we 
extracted the human face before the lip contour 
segmentation process. Here, a face detecting method has 
been used to extract the human face [10]. The AdaBoost is a 
powerful method which used in face detecting. In our 
system, we only concerned about the front face. The human 
front face contains most of the lip features that could 
support an automatic speech recognition system. Since the 
detected face region shown in Figure 1 contained less 
background, we chose the lip ROI by the geometric 
properties of human face [11] shown in Figure 2. We can 
separate the face image into several parts, by the distance of 
each facial feature we could extract the lip region. The lip 
ROI location result is shown in Figure 3. 
 
Figure 3: The lip ROI extraction by AdaBoost and human 
face geometric properties. 
2.2. Lip Contour Segmentation 
Since the region we concerned about is in the lip ROI, 
there’s no background noise, just the lip and skin pixels. We 
could use the color based method to extract the lip contour 
272
  
(c)                  (d) 
Figure 6: Histogram-based lip feature points extracting 
method. (a): the lip corner points. (b): y-projection of lip 
image. (c): the top and bottom points. (d): x-projection 
of lip image. 
 
(a) Frame 1      (b) Frame 5      (c) Frame 8 
 
 (d) Frame 11     (e) Frame 15     (f) Frame 17 
 
 (g) Frame 22     (h) Frame 26     (i) Frame 30 
Figure 7: Feature points extracting results of some 
frames of the speech “zero” in Chinese. 
 
6. Acknowledgements 
This work was supported by the National Science 
Council under grant number NSC  97-2221-E-032-025-. 
 
REFERENCES 
[1] H. McGurk and J. MacDonald, “Hearing Lips and 
Seeing Voices,” Nature, vol. 264, pp. 746-748, 1976. 
[2] B. Le Goff, T. Guiard-Marigny and C. Benoit. “Read 
my Lips… and my Jaws! How Intelligible are the 
Components of a Speaker’s Face”, Proc. of European 
Conf. on Speech Communication and Technology, pp. 
291-294, 1995. 
[3] D.G. Stock and M.E. Hennecke, “Speechreading by 
Humans and Machines”, NATO ASI Series F, Vol. 150, 
Springer Verlag, 1996. 
[4] X. Zhang and R.M. Mersereau, “Lip Feature 
Extraction Towards an Automatic Speechreading 
System”, Proc. of IEEE Int. Conf. on Image Processing, 
pp. 226-229, 2000. 
[5] R. Ma, Y. Wu, W. Hu, J. Wang, T. Wang, Y. Zhang 
and H Lu. “Lip Localization on Multi-view Faces in 
Video”, Proc. of IEEE Int. Conf. on Multimedia and Expo, 
pp. 1395-1398, 2007. 
[6]M. Kass, A. Witkin, and D. Terzopulos, “Snakes: Active 
Contour Models”, Int. Journal of Computer Vision, Vol. 1, 
pp. 321-331, 1987. 
[7] L. G. Silveira, J. Facon, and D. L. Borges, “Visual 
Speech Recognition: a Solution from Feature Extraction to 
Words Classification," Proc. of Int. Conf. on Computer 
Graphics and Image Processing, pp. 399-405, 2003. 
[8] V. Pahor and S. Carrato, “A Fuzzy Approach to Mouth 
Corner Detection”, Proc. of Int. Conf. on Image Processing, 
pp. 667-671, 1999.  
[9] S. Leung, S. Wang and W. Lau, “Lip Image 
Segmentation Using Fuzzy Clustering Incorporating an 
Elliptic Shape Function”, IEEE Trans. on Image Processing, 
Vol. 13, pp. 51-62, 2004. 
[10] P. Viola and M. Jones, “Robust Real-Time Face 
Detection”, Inte. Journal of Computer Vision, pp. 137-154,  
2004. 
[11] A. Colmenarez, B. Frey and T.S. Huang, “Detection 
and Tracking of Faces and Facial Features,” Proc. of IEEE 
Int. Conf. on Image Processing, Vol. 1, pp. 657-661, 1999. 
[12] H. Jun and Z. Hua “A Real Time Lip Detection 
Method in Lipreading” Proc. of Conf. on Chinese Control 
Conference, pp. 516-520, 2007. 
274
 
(a) 
 
 
(b) 
Figure 2. Extraction of face image 
 
2. Normalization of Facial Feature’s Position 
 
Before starting our experiment, we have to conduct 
a simple pre-process.  First of all, we normalize each 
image, and that will make us conduct the remainder 
experiment effectively. 
The purpose of our experiment is that how to 
extract the age texture of each part on the face, therefore 
we have to normalize the facial part, such as eyes, nose, 
and mouth to the same position, and that will make us 
conduct the remainder experiment effectively. 
In 2001, Paul Viola and Michael Jones proposed a 
real-time object detection by AdaBoost algorithm [6] 
which is the approach of lowest feature classification 
error and can be used for face detection.  Because 
AdaBoost algorithm can be employed to classify the 
face feature, so we can use it to extract the face position 
on the whole image by automatic. 
In our system, we employ the property of AdaBoost 
algorithm to extract the face position on the whole 
image and normalize the facial part with each subjects’ 
images.   
An example of face detection by AdaBoost 
algorithm is shown in Fig. 2, (a) is a process of this 
approach, and (b) is the results of normalization. 
 
3. Principal Component Analysis 
 
In order to find the most appropriate image from 
our database, this paper we adopt the approach of 
principal component analysis (PCA) [7].  In general, 
the PCA is usually employed to reduce the amount of 
data.  However, there are few studies have attempted to 
use PCA for pattern recognition by finding minimal 
distance.  In this paper, we use the properties of 
recognition of PCA, and then compare the weight to find 
the most similar target age image from our database.   
The main process of PCA can be represented by the 
following steps: 
 
1) Find the average luminance of face image Xഥ. 
Xഥ  ൌ  
1
M
  ෍  X୧
M
୧ୀଵ
 
2) Subtract the mean face A. 
A ൌ  X୧  െ Xഥ 
3) Compute the covariance matrix C. 
C ൌ  
1
M
 ෍  A AT
M
୬ୀଵ
 
4) Compute the eigenvalues and eigenvectors of 
covariance matrix. 
5) Keep the first K eigenvalues, and then calculate 
the weight of image by projecting mean 
subtracted images on the eigenvectors. 
6) According to steps (1) ~ (5), we calculate the 
weight of subject, and then compare it with the 
weights of our database images. 
 
We determine one target age image from our 
database which has the minimal distance with subject 
image by comparing the each weight through PCA.   
 
4. Extraction of Age Texture 
 
The key point of any pattern recognition system is 
that how to extract the texture feature effectively.  At 
present, the Gabor wavelet is the main approach for 
texture feature extraction, because Gabor wavelet offers 
an excellent simultaneous localization of spatial and 
frequency information.  Hence the Gabor wavelet is 
usually applied for facial expression classification in 
many researches [8] [9].  
A Gabor wavelet g(x,y) is defined by the following 
equation： 
 
      g(x,y)=s(x,y)w(x,y)        (1) 
 
Where s(x,y) is a complex sinusoidal known as the 
carrier, and w(x,y) is a 2-D Gaussian-shaped function 
known as the envelope. 
However, there are two major limitations while 
using Gabor wavelet.  The first limitation is the 
maximum bandwidth of Gabor wavelet is limited to 
approximately one octave.  The second limitation is 
that Gabor wavelet would cost lots of time when 
searching broad spectral information with maximal 
282
Fig. 5 shows an example of aging synthesis, the 
female subject is 48 years old, the male subject is 24 
years old and the target age are both 88 years old.  Fig. 
5 (a) (d) are the female and male subject’s original 
image separately, (b) (e) are results overlay the first four 
components of decomposition map from target age 
image, and (c) (f) are results overlay the first eight 
components of decomposition map from target age 
image.   
According to the result of Fig5, it can be generated 
well that Fig. 5(c) (f) is older than Fig. 5(b) (e) and Fig. 
5(a) (d) with different decomposition map.  It is 
apparent that much more decomposition maps 
components of target age image covering on the 
subject’s faces image, and much older results that come 
out in our eyes.  
 
6. Conclusion 
 
We present an approach to extract the texture of age, 
and then accomplish the facial images synthesis by 
manipulating different age texture.  The experimental 
results show that the synthesis of facial images can be 
generated well by using our proposed approach.  If we 
have enough information, such as the texture of each age, 
and then we can achieve the facial image synthesis with 
target age more effectively.  
So far, our proposed method is not only successful 
through the gray-level images, but also we extend our 
system to be adaptable with color images.  However, 
there are still a few disadvantages in our approach, such 
as the unnaturally smooth skin cause by replacing 
different subjects’ image.  The next step, we hope that 
we can improve these problems in the future. 
 
7. Acknowledgements 
This work was supported by the National Science 
Council under grant number NSC  
97-2221-E-032-025-. 
 
References 
 
[1] M. Nakagawa, T. Munetsugu, Y. Kado, F.Maehara 
and K. Chihara. “The Facial Aging Simulation 
Based on the Skeletal Model”, IEICE(A), Japan, 
Vol. J80-A, pp. 1312-1315, 1997. 
[2] Mukaida. S and Ando. H. “Extraction and 
manipulation of wrinkles and spots for facial image 
synthesis”, Proc. IEEE, pp. 749-754, 2004.  
[3] D. A. Rowland, D. I. Perrett. “Manipulating Facial 
Appearance through Shape and Color”, Computer 
Graphics and Applications IEEE Computer 
Gfaphics and Applications, Vol.15, pp.70-76, 1995. 
[4] D. M. Burt, D. I. Perrett. “Perception of age in adult 
Caucasian male faces: computer graphic 
manipulation of shape and colour information”, 
Proc.R.Soc.Lond, pp. 137-143, 1995. 
[5] C. Choi. “Age Change for Predicting Future Faces”, 
Proc. IEEE, pp. 1603-1608, 1999. 
[6] P. Viola, M. Jones. "Robust Real-time Object 
Detection,” Second International Workshop on 
Statistical and Computational Theories of Vision, 
Vancouver, Canada, 2001 
[7] Khan, M. A. U, Khan, M. K, Khan, M. A, Ibrahim, 
M. T, Ahmed, M. K and Baig J. A. “Improved PCA 
based Face Recognition using Directional Filter 
Bank”, Proc. IEEE 2005 International Conference 
on Emerging Technologies, pp. 118-124, 2004. 
[8] Rose. N. “Facial Expression Classification using 
Gabor and Log-Gabor Filters”, Automatic Face and 
Gesture Recognition IEEE, pp. 346-350, 2006. 
[9] Lajevardi. S. M and Lech. M. “Facial Expression 
Recognition Using Neural Networks and 
Log-Gabor Filters”, Computing: Techniques and 
Applications IEEE, pp. 77-83, 2008. 
[10] Field, D. “Relations between the Statistics of 
Natural Images and the Response Properties of 
Cortical Cells.” Journal of Optical Society of 
American A, Vol. 4, pp. 2379-2393, 1987. 
284
 
3. The concept of Particle Swarm Optimization 
(PSO) 
In this section, we will briefly introduce what is 
Particle Swarm Optimization, “PSO” for short. 
PSO was proposed by J. Kennedy and R. C. Eber-
hart in 1995. It basically works like a swarm of animals 
searching food in a great area. For example, there are 
100 sheeps in a wide grassland. How do they find an 
area with the most grass? Because sheep can communi-
cate with each others, so they do know which sheep 
found a better area in this group, and move forward to 
that sheep’s position gradually.  This is the principle 
how PSO works. 
The common equation of PSO is shown as follows: 
ܸ݅݀ ൌ ܹ כ ܸ݅݀ ＋ܥ1 כ ݎܽ݊݀ሺݔሻ כ ሺܾܲ݁ݏݐ െ  ܺ݅݀ሻ ൅ ܥ2 כ
            ݎܽ݊݀ሺݔሻ כ ሺܩܾ݁ݏݐ െ ܺ݅݀ሻ                                             (1-a)       
ܺ݅݀ ൌ ܺ݅݀ ൅ ܸ݅݀                                                                        (1-b) 
Vid and Xid means velocity and position of each 
particle in every iteration. Pbest is the optimal solution 
of each particle during the movement. And Gbest is the 
optimal solution of all particles during the iterations. W 
is inertia weight, it means keeping every particle active 
in movement. C1 and C2 are acceleration constants. C1 
is the cognitive ability, it urges each particle to focus its 
movement on the best solution which it passed before. 
C2 is the social ability, it urges each particle to focus its 
movement on the solution of Gbest. 
 In other words, a higher value of C1 will urge 
each particle to move slowly and cautiously; it will 
spend more time to converge, but won’t fall into a local 
solution easily. Contrary to C1, a higher value of C2 will 
urge each particle to move quickly and boldly; it will 
spend less time to converge, but usually fall into a local 
solution. So it’s a important task to define these para-
meters. 
Figure 2 shows the concept of PSO, the bird with 
orange color is the one at the present time. And the bird 
with a yellow circle is the one in the past. And the bird 
with a purple circle is the Gbest. The arrow with orange 
color point at the direction that the bird at the present 
time will move to. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Diagram of the concept for PSO 
 
4. Face angles matching process accelerated by 
PSO 
First, it is generally acknowledged that the differ-
ence between facial images of different subjects in the 
same pose is less than that between facial images of the 
same subject in different poses. So we define fitness 
function as follows : 
ܧ݊݁ݎ݃ݕ ൌ ∑  |ܫ݅݊݌ݑݐሺݔ, ݕሻ െ ܫ݌ܽݎݐ݈݅ܿ݁ሺݔ, ݕሻ|ଶሻ௫௬ |            (2) 
ܨ݅ݐ݊݁ݏݏ ൌ ܯ݅݊ሺܧ݊݁ݎ݃ݕሻ                                                                            (3) 
The optimization process accelerated by PSO is shown 
as follows step by step: 
 
Step1) Divide z axis into 180 parts, each part contains a 
x-y plane. 
Step2) Set 25 particles uniformly in each x-y plane. 
Step3) Input test Data. 
Step4) Compute the Energy, and derive the particle with 
the minimum Energy. 
Step5) After Step3, we can find out which x-y plane is 
the most appropriate plane that we are looking 
for. Then set 25 particles uniformly in this plane. 
Each particle have an angles set and one image at 
the same time. 
Step6) Initiate parameters W, C1 and C2 as 0.6, 0.2 and 
0.4. 
Step7) Set the parameter Iteration equals to 0. 
Step8) Pbest = Xid(iteration), Xid(iteration) means the 
position of each particle in each iteration. 
Step9) Computing the Energy between test data and 
image from each particle as follows: 
ܧ݊݁ݎ݃ݕ ൌ ∑  |ܫ݅݊݌ݑݐሺݔ, ݕሻ െ ܫ݌ܽݎݐ݈݅ܿ݁ሺݔ, ݕሻ|ଶ௫௬        
Step10)Find out the Pbest for each particle. 
  
 
 
Step11)Find out the Gbest for all the particles. 
 
 
 
Step12)Updating the velocity of each particle as follows:  
 ܸ݅݀ ൌ ܹ כ ܸ݅݀ ൅ ܥ1 כ ݎܽ݊݀ሺݔሻ כ ሺܾܲ݁ݏݐ െ  ܺ݅݀ሻ ൅
ܥ2 כ ݎܽ݊݀ሺݔሻ כ ሺܩܾ݁ݏݐ െ ܺ݅݀ሻ 
 ܺ݅݀ ൌ ܺ݅݀ ൅ ܸ݅݀ 
Step13)After each iteration, we set a new parameter 
Count1 to count the continuity of Gbest between 
iterations. 
 
 
 
Step14)If Count1 equals to 10, reset the positions of 25 
particles around the Gbest uniformly and set a 
new parameter Count2 = Count2 + 1. Then, reset 
Count1 = 0 and go to Step 15. 
 If Count1 less than 10, jump to step 16. 
 
Step15)If Count2 equals to 2, terminate the program.  
 Otherwise, go to step 16. 
Step16)Update the parameter Iteration, return to step9. 
( ) ,    ( )  ( )
               ,                          
Xid iteration Energy Iteration Energy Pbest
Pbest
Pbest else
<⎧= ⎨⎩
( ) ,    ( )  ( )
 ,                          
Xid iteration Energy Iteration Energy Gbest
Gbest
Gbest else
<⎧= ⎨⎩
1  1 , ( ) ( 1)
1
1 = 0  ,                          
Count Gbest iteration Gbest iteration
Count
Count else
+ = −⎧= ⎨⎩
296
