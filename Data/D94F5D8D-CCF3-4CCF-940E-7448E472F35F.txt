 2
fault tolerance mechanism proposed in the literature, we develop a new framework for 
XCS, which is called XCS with Fault Tolerance (XCS/FT), in this research project. 
XCS/FT introduces the concept of the fault tolerance mechanism into XCS in order to 
increase the readability and to find more useful knowledge of the given data. For this 
research project, we will propose the design of XCS/FT and implement XCS/FT in the 
first year. Also in the first year, we will conduct experiments to understand the behavior 
and to observe the characteristics of XCS/FT. In the second year, we will conduct 
theoretical studies of XCS/FT according to the five previously identified evolutionary 
pressures: (1) set pressure; (2) mutation pressure; (3) deletion pressure; (4) subsumption 
pressure; (5) fitness pressure. Based on the expected outcomes of this project, we 
believe that we can make significant contributions to both practitioners who use XCS to 
tackle the problems as well as theorists who try to understand XCS in theory. 
Keywords: XCS, learning classifier system, fault tolerance, evolutionary pressures 
 
 
Background, Motivation, and Purposes: 
Data mining (DM) [1], also called Knowledge-Discovery in Database (KDD) or 
Knowledge and Data Mining, is the process of automatically searching large volumes of 
data for patterns, such as association rules and classification rules. It is a fairly recent 
topic in computer science and utilizes many traditional computational techniques from 
statistics, information retrieval, machine learning, and pattern recognition. Classification 
is a branch of data mining and a kind of the supervised method. In the supervised 
learning, the initial step is to assemble a subset of samples, called the training or 
learning set, which are diagnosed by an external supervisor. Then, we use the training 
set to build the prediction model. Finally, we use the model to predict the class level of 
the unclassified data. There are many existing, useful algorithms employed by industries, 
such as C4.5 [2], ID3 [3], Artificial Neural Network (ANN), Learning Classifier System 
(LCS) [4], and XCS [5]. 
The aforementioned algorithms offer high classification accuracy, but they might 
not reveal the appropriate relationship among data or the desired knowledge of data. We 
divide these algorithms into two categories. The algorithms belonging to the first 
category, such as ANN, cannot provide human readable rules. Otherwise, we classify 
them into the other category. LCS belongs to the latter. Since the introduction of LCS, 
there have been a lot of investigations on the architecture and performance of LCS. 
Recently, XCS has become one of the main representatives of LCS. It is the first 
learning classifier system where accurate and maximal generalizations are reported. Its 
success is due to two main changes made to the LCS architecture: the fitness based on 
accuracy and a niche genetic algorithm (GA). In XCS, the accuracy-based fitness results 
in an evolutionary pressure toward higher accuracy and thus higher specificity starting 
from the over-general classifiers. Meanwhile, a continuous generalization pressure 
assures that a maximally accurate, maximally general problem solution evolves [6]. 
During the rule discovery, XCS applies GA on the action set rather than the match set 
and/or the whole population. Therefore, it can discover more meaningful rules [7]. 
 4
z Prove XCS guided by fault tolerance works better for the real-world like data 
set. 
 Display the results of XCS for the artificial real-world like data. 
 Display the results of XCS guided by fault tolerance for the artificial 
real-world like data. 
 Compare the two sets of experimental results. 
 Check if fault tolerance is useful for XCS in the artificial real-world like 
data. 
z Integrate fault tolerance into XCS to offer a new framework of XCS (i.e., 
XCS/FT) 
 Propose XCS/FT. 
 Discuss XCS/FT in theory. 
 Understand the difference between XCS/FT and XCS. 
 Show the detail implementation of XCS/FT. 
 Apply XCS/FT to the artificial real-world like data. 
 Check if XCS/FT works for the artificial real-world like data. 
 Prove that XCS/FT can provide knowledge and high prediction accuracy. 
In addition to the listed working items, we also finished a draft of a journal paper, 
entitled “Introducing Fault Tolerance to XCS,” which is included in the appendix and 
will be submitted to an appropriate journal. 
 
 
Self-Evaluation: 
In this project, we finished the expected working items as well as a draft of a 
journal paper. Therefore, the goals of this project are achieved. It is unfortunate that we 
did not receive the project grant for the following years. As a consequence, we cannot 
continue our work on this topic. If in the near future the funding for the project can be 
appropriately secured, the follow-up working items can then be handled. 
 
 
References: 
[1] J. Han and M. Kamber, Data Mining Concepts and Techniques, 2000. 
[2] J. R. Quinlan, C4.5: Programs for Machine Learning, 1993. 
[3] J. R. Quinlan, "Induction to decision trees," Machine Learning, vol. 1, pp. 
81-106, 1986. 
[4] J. H. Holland, "Adaptation in Natural and Artificial Systems," Progress in 
Introducing Fault Tolerance to XCS
Ying-ping Chen ypchen@nclab.tw
Department of Computer Science, National Chiao Tung University, Hsinchu, TAIWAN
Hong-Wei Chen hwchen@nclab.tw
Department of Computer Science, National Chiao Tung University, Hsinchu, TAIWAN
Abstract
In this paper, we introduce fault tolerance to XCS and propose a new XCS framework
called XCS with Fault Tolerance (XCS/FT). As an important branch of learning classi-
fier systems, XCS has been proven capable of evolving maximally accurate, maximally
general problem solutions. However, in practice, it oftentimes generates a lot of rules,
which lower the readability of the evolved classification model, and thus, people may
not be able to get the desired knowledge or useful information out of the model. In-
spired by the fault tolerance mechanism proposed in field of data mining, we devise
a new XCS framework by integrating the concept and mechanism of fault tolerance
into XCS in order to reduce the number of classification rules and therefore to im-
prove the readability of the generated prediction model. The workflow and operations
of the XCS/FT framework are described in detail. A series of N -multiplexer experi-
ments, including 6-bit, 11-bit, 20-bit, and 37-bit multiplexers, are conducted to exam-
ine whether XCS/FT can accomplish its goal of design. According to the experimental
results, XCS/FT can offer the same level of prediction accuracy on the test problems
as XCS can, while the prediction model evolved by XCS/FT consists of significantly
fewer classification rules.
Keywords
XCS, XCS/FT, Learning classifier systems, Classificationmodels, Population size, Fault
tolerance, Data mining.
1 Introduction
Since the introduction of learning classification systems (LCS) by Holland (1975), there
have been a lot of studies and investigations on the architecture and performance of
LCS. In recent years, XCS (Wilson, 1995) has become one of the main representatives
of LCS. XCS evolves rules (i.e., classifiers) through which the system gradually im-
proves its ability to obtain the environmental reward. XCS mines the environment for
the prediction pattern, which is expressed in the form of classifiers. The repeatedly re-
fined prediction pattern allows the XCS system to make better and better decisions for
action. According to the operations, XCS is potentially applicable to data mining prob-
lems because it is capable of evolving maximally accurate, maximally general rules in
many environments (Kovacs, 1996). Reports regarding applying XCS to data mining
problems can be found in the literature (Wilson, 2000b,a; Stone and Bull, 2003).
Pei et al. (2001) indicated that real-world data tend to be diverse and dirty. For the
real-world application, frequent pattern mining (Han and Kamber, 2005) often results
in a large number of frequent item sets and rules, which reduce not only the efficiency
but also the effectiveness of data mining since the users have to go through a large
c©2008 by the Massachusetts Institute of Technology Evolutionary Computation x(x): xxx-xxx
Introducing Fault Tolerance to XCS
Event Action
Detectors Effectors
Population  [P]
Match Set  [M] Prediction Array
Action Set  [A]
Genetic 
Algorithm
 
Figure 1: Framework of XCS.
3. Payoff prediction: The payoff prediction p estimates the average payoff after execut-
ing the action in response to the environment event.
4. Prediction error: The prediction error e estimates the average error of the payoff
prediction.
5. Fitness: The fitness F reflects the scaled average relative accuracy of the classifier.
2.2 Performance Component
The performance component presents the overall XCS framework, shown in Figure 1.
The population of XCS starts with randomly generated classifiers, potentially useful
“seed” classifiers, or no classifiers. When an event occurs, a match set [M ] is generated
from those classifiers which match the event in the whole population [P ]. The system
prediction is then computed for each action. The system prediction for each action
is placed in the prediction array for action selection. If the predition array does not
include all actions, random classifiers are generated through the covering operation.
The system selects an action from the prediction array and forms an action set [A].
Finally, the chosen action is executed, and an environmental reward may be returned.
2.3 Reinforcement Component
The reinforcement component consists of updating the parameters of classifiers in the
action set [A] to achieve higher accuracy and to complete mappings of the problem
space. The definition of variables and the procedure for updating the parameters are
shown in Figure 2. With experimental results, Butz et al. (2001) indicated if the pre-
diction update (line 11) in comes before the error update (line 10), the prediction of a
classifier in its very first update immediately predicts the correct payoff, and conse-
quently the prediction error is set to zero. This operation can lead to faster learning
in simple problems but may result in misleading for more complex problems. A more
conservative strategy which puts the error update first seems to work better on hard
problems (Butz and Wilson, 2000). More information on the update of XCS parameters
can be found in the literature (Butz et al., 2001; Horn et al., 1994).
Evolutionary Computation Volume x, Number x 3
Introducing Fault Tolerance to XCS
2.6 Covering and Subsumption Deletion
XCS uses the genetic algorithm (GA) to generate new classifiers, and in addition, cov-
ering is another method to introduce new classifiers into the population. When an
environment event occurs, the match set is determined accordingly. If the match set
does not contain all possible actions defined for the environment, the system will gen-
erate new classifiers through the covering operator to cover all possible actions. When
a classifier is created through covering, its condition is made to match the current sys-
tem input (i.e., the environment event), and it is given an action chosen at random.
Each attribute in the condition is mutated to don’t care (#) with a probability. Finally,
the system puts the newly generated classifier into the population.
Subsumption deletion (Wilson, 1998) is a way to improve the generalization ca-
pability of XCS. There are two kinds of subsumption: GA-subsumption and action-
subsumption. Using the GA-subsumption deletion, when new classifiers are gener-
ated, they are compared to their parent classifiers. If the parent classifiers with the
the experience parameter, exp, exceeding a certain threshold are more general than the
new classifiers, the new classifiers are subsumed by the parents. At the same time, the
numerosity of the subsuming classifier is incremented. Otherwise, the system puts the
new classifiers into the population. Using the action-subsumption deletion, for each
classifier R in the action set [A], if there exists a classifier Q more general than R, Q
subsumes R, and the numerosity of Q is incremented.
3 Fault Tolerance
Because of its versatility and capability, XCS is a promising methodology for handling
classification in data mining. However, in real-world data, noise and irregularities al-
ways exist and bring difficulties and challenges to the adopted classification model. In
order to provide a remedy for this problem, fault tolerance is proposed in the realm
of data mining such that the rules extracted from the given data can be more useful
and may offer meaningful implications. Hence, if one would like to tackle data mining
tasks with XCS, certain mechanisms are required to resolve similar situations. In this
section, we will channel the concept and mechanism of fault tolerance into XCS.
3.1 Fault Tolerance in Data Mining
Data mining (Han and Kamber, 2005), also called knowledge and data mining or
knowledge-discovery in database, is the process of automatically searching large vol-
umes of data for patterns, such as association and classification rules. However, there
usually exist some non-trivial frequent patterns and rules with both high support and
confidence in the database because real-world data tend to be diverse and dirty. In
other words, for commonly used data mining techniques, the extracted rules could
have a high prediction accuracy but may cover only a very small subset of cases. Pei
et al. (2001) showed an example of the real-world data as Example 1.
Example 1: For students’ performance in courses, one may find rules like R1 and R2.
good(x, Y ): Student x gets a good grade in course Y , where Y is one of the courses:
• AI: Artificial intelligence;
• Algo: Algorithm;
• DBMS: Database management system;
Evolutionary Computation Volume x, Number x 5
Introducing Fault Tolerance to XCS
c1 c2 c3 c4 c5 class
Event1 v1 v2 v3 v4 v5 a2
Event2 N-v1 v2 v3 v4 v5 a2
Event3 v1 N-v2 v3 v4 v5 a2
Event4 v1 v2 N-v3 v4 v5 a2
Event5 v1 v2 v3 N-v4 v5 a2
Event6 v1 v2 v3 v4 N-v5 a2
Table 2: Example of fault tolerance. N-x stands for all possible values except for x.
3.3 Fault Tolerance in XCS Viewpoint
In this section, we show how fault tolerance proposed in data mining can be integrated
into XCS at the concept level such that the rules similar to R2 described in Example 1
can be expressed by XCS classifiers. Considering the condition part with five attributes
of an XCS rule, C: {c1, c2, c3, c4, c5} and the action part with 2 attributes, A = {a1, a2}.
We may have a rule R3 as in the following example.
Example 3: XCS rule for the data set shown in Table 2.
R3: (c1 = v1) ∧ (c2 = v2) ∧ (c3 = v3) ∧ (c4 = v4) ∧ (c5 = v5) → (class = a2) [with fault
tolerance: zero or one mismatched condition attribute] 2
We conceptually integrate R3 with the fault tolerance mechanism. Firstly, R3 can
match event 1 because all specific attributes are matched, as a classical XCS rule. Al-
though events 2 to 6 have attributes different from the condition of R3, they still can
be matched by R3 because the fault tolerance mechanism requires to match only part
of the condition. Thus, R3 can match all the six events in Table 2. XCS rules equipped
with certain fault tolerance mechanisms may result in covering more data items.
4 XCS/FT
Based on the concept of fault tolerance, in this section, we propose a new framework of
XCS, called XCS with Fault Tolerance (XCS/FT). We will discuss the major components
of XCS/FT, including the representation, the fault tolerance match operator, the fault
tolerance increase operator, and the framework of XCS/FT.
4.1 Representation
In order to enable XCS classifiers to deal with the noisy real-world data, we modify the
representation of XCS rules to make them capable of tolerating “faults”, which stands
for the mismatched condition attributes. For this purpose, we add a parameter, called
fault tolerance (FT), into the classifier representation as
< Classifier >::= < Condition >:< Action >:< FT >:
< Payoff prediction >:< Payoff error >:
< Fitness >
FT indicates how many condition attributes to be ignored when a rule matches an
event. Rules with FT will have a better “fault capacity” than the standard XCS rules
and will result in fewer classifiers when the covering operation is triggered. For exam-
ple, if there are six dirty conditions as shown in Table 3, the covering operation will
Evolutionary Computation Volume x, Number x 7
Introducing Fault Tolerance to XCS
1: // cl: Classifier.
2: // cl.C: The condition part of cl.
3: // cl.FT : The maximum number of mismatched attributes allowed for cl.
4: // e: Event of environment.
5: // ErrorCount: The counter for mismatched attributes between cl.C and e.
6:
7: procedure FAULT TOLERANCE MATCH(cl, e)
8: ErrorCounter ← 0;
9: for each attribute x ∈ cl.C do
10: if x is not # then
11: if xmismatches e then
12: ErrorCounter ← ErrorCounter + 1;
13: end if
14: if ErrorCounter > cl.FT then
15: return false;
16: end if
17: end if
18: end for
19: return true;
20: end procedure
Figure 3: Fault Tolerance Match.
1: // cl: Classifier.
2: // cl.fit: The fitness of classifier.
3: // cl.FT : The maximum number of mismatched attributes allowed for cl.
4: // FTC: The maximum number of times to apply FTA during rule discovery.
5: // avgF it: The average fitness of the population.
6:
7: procedure FAULT TOLERANCE ADJUSTMENT(cl)
8: counter ← 0;
9: Compute the avgF it of the current population;
10: while counter < FTC do
11: Select a classifier cl at random
12: if (cl.fit < avgF it) then
13: cl.FT ← cl.FT + 1;
14: end if
15: counter ← counter + 1;
16: end while
17: end procedure
Figure 4: Fault Tolerance Adjustment.
4.4 Framework of XCS/FT
In this paper, we concentrate on the classification problem for data mining. We model
the condition as attributes and the action as class in a rule. With this interpretation,
we make XCS a classification system. The overall framework of XCS/FT is shown
in Figure 5. Different from the original XCS framework, which is shown in Figure 1
Evolutionary Computation Volume x, Number x 9
Introducing Fault Tolerance to XCS
5 Experimental Results
In previous sections, we have briefly reviewed the original XCS framework, introduced
the concept of fault tolerance, and described in detail the proposed XCS/FT. In this
sections, in order to verify the effect of fault tolerance in XCS, we employ both XCS and
XCS/FT to tackle the boolean multiplexer function and compare the performance, system
error, and population size. Boolean multiplexer functions are defined for binary strings
of length ` = k + 2k. The function value is determined by treating the first k bits as an
address that indexes into the remaining 2k bits, and the value of the indexed bit, either 0
or 1, is the function value. The performance is the fraction of the last 50 exploit trials that
were correct. The system error is the absolute difference between the system prediction
for the chosen action and the actual external payoff, divided by the total payoff range
(1000) and the average over the last 50 exploit trials. The population size is the number
of macroclassifiers.
In the experiments, for the XCS part, we employ the XCS system publicly available
on the Internet Butz (2000). For the proposed XCS/FT, wemodify the XCS system to in-
clude the mechanisms described in section 4 and establish the XCS/FT framework for
testing. Both XCS and XCS/FT are used to handle the boolean multiplexer of four dif-
ferent sizes, including 6 bits, 11 bits, 20 bits, and 37 bits. Each experiment is conducted
for 200 independent runs, and the statistics averaged over the 200 runs are reported.
The experimental results are presented in the following sections.
5.1 6-Multiplexer
Figure 6 shows the experimental results for the booleanmultiplexer of 6 bits. As we can
observe, XCS gets approximately 100% performance in 4000 expolit trails, and XCS/FT
gets approximately 100% performance in 10000 expolit trails. For the system error, XCS
gets approximately 0% system error in 3000 expolit trails, and XCS/FT gets approx-
imately 0% system error in 10000 expolit trails. Finally, XCS evolves the population
with 28.11 classifiers, and XCS/FT evolves the population with 27.79 classifiers.
Based on the experimental results for 6-Multiplexer, we can find that XCS and
XCS/FT can achieve the same performance, system error, and population size when the
exploit trails is appropriate. Furthermore, we can know that XCS converges faster than
XCS/FT does. Overall, XCS performs slightly better than XCS/FT for 6-Multiplexer.
5.2 11-Multiplexer
Figure 7 shows the experimental results for the booleanmultiplexer of 11 bits. From the
results, XCS gets approximately 100% performance in 7000 expolit trails, and XCS/FT
gets approximately 100% performance in 23000 expolit trails. As for the system error,
XCS gets approximately 0% system error in 6000 expolit trails, and XCS/FT gets ap-
proximately 0% system error in 22000 expolit trails. For the size of the resulting rule
set, XCS evolves the population with 80.59 classifiers, and XCS/FT evolves the popu-
lation with 66.67 classifiers.
As the experimental results we obtained for 6-Multiplexer, similar outcomes are
also for 11-Multiplexer. However, the effect of introducing fault tolerance into XCS
starts to appear. In this experiment, XCS/FT on average saves 17.27% of the population
size over the 200 runs.
5.3 20-Multiplexer
Figure 8 demonstrates the experimental results for the boolean multiplexer of 20 bits.
XCS gets approximately 100% performance in 20000 expolit trails, and XCS/FT gets
Evolutionary Computation Volume x, Number x 11
Introducing Fault Tolerance to XCS
approximately 100% performance in 60000 expolit trails. For the system error, XCS
gets approximately 0% system error in 25000 expolit trails, and XCS/FT gets approx-
imately 0% system error in 50000 expolit trails. For the population size, XCS evolves
the population with 256.98 classifiers, and XCS/FT evolves the population with 208.25
classifiers. As we can see, for a larger problem, the effect of fault tolerance is more sig-
nificant. In this experiment, XCS/FT on average saves 18.96% of the population size
over the 200 runs.
5.4 37-Multiplexer
Finally, Figure 9 presents the experimental results for the boolean multiplexer of 37
bits. In this experiment, XCS gets approximately 100% performance in 175000 expolit
trails, and XCS/FT gets approximately 100% performance in 250000 expolit trails. For
the system error, XCS gets approximately 0% system error in 150000 expolit trails, and
XCS/FT gets approximately 0% system error in 250000 expolit trails. For the popula-
tion size, XCS evolves the population with 789.23 classifiers, and XCS/FT evolves the
population with 620.83 classifiers. For 37-Multiplexer, XCS/FT further saves 21.34% of
the population size.
6 Discussion
The experimental results regarding the number of the evolved rule set are summarized
in Table 4. We can observe that XCS and XCS/FT can evolve the final rule set with the
similar rule numbers for simple problems, such as 6-Multiplexer. However, when the
problem becomes more complicated, XCS/FT starts to provide the prediction model
of significantly fewer rules. Hence, the ability of XCS/FT to provide models of fewer
rules is empirically verified.
From the experimental results, we can also find that during the evolutionary pro-
cess, XCS/FT has higher system error at the early stage than XCS does, and when the
system converges, the XCS/FT system error also drops down to 0%. The fault toler-
ance mechanism makes XCS/FT converges more slowly. Furthermore, according to
the changes in the population size, the fault tolerance mechanism can help XCS/FT to
fully utilize the system resources because the population size of XCS/FT reaches the
predefined maximum of the population size.
On the other hand, the boolean multiplexer problem is not the most appropriate
testing problem to demonstrate the capability of XCS/FT, because there exits no “dirty”
data item. Under such a condition, XCS/FT can still provide classification models of
the same quality but a smaller size. Therefore, although it has to been further examined
and carefully verified, we expect that XCS/FT can perform well on the real-world data
containing noisy data items.
Overall, integrating the fault tolerance mechanism into XCS can be considered as
adding an extra degree of freedom for the classifiers to express the encountered data
items (i.e., environment events). Although fault tolerance is not totally “orthogonal” to
don’t care in a manner of speaking, by representing a classifier as a row in a table, we
can think of don’t care as the verticalmatching relaxation (for values of attributes) and
fault tolerance as the horizontalmatching relaxation (for the number of attributes).
As a result of this interpretation, none of the theoretical results for XCS, such as that
XCS can evolvesmaximally accurate, maximally general rule sets, has been overthrown
by the experimental results for XCS/FT because the fundamental representation as well
as the expressive capability of the classifier are changed. On the contrary, the ability
of the XCS framework to evolve maximally accurate, maximally general rule sets is
Evolutionary Computation Volume x, Number x 13
Introducing Fault Tolerance to XCS
6-Multiplexer 11-Multiplexer 20-Multiplexer 37-Multiplexer
XCS 28.11 80.59 256.98 789.23
XCS/FT 27.79 (1.14%) 66.67 (17.27%) 208.25 (18.96%) 620.83 (21.34%)
Table 4: The average sizes of the final rule sets for different multiplexers with XCS and
XCS/FT. x% is ((XCS-XCS/FT)/XCS)× 100%.
actually verified once more by the results of this study, since the XCS framework can
indeed utilize the new equipment (rules with the fault capacity) to achieve the same
performance (maximally accurate) with fewer rules (maximally general for the new
framework).
7 Summary and Conclusions
In this paper, we first briefly reviewed XCS, followed by the introduction of the concept
of fault tolerance. After channeling fault tolerance into XCS, we proposed the mech-
anisms of fault tolerance for XCS and described in detail the integrated framework of
XCS/FT. Finally, we implemented XCS/FT by modifying an existing XCS system and
conducted a series of boolean multiplexers as well as fault-tolerant data sets for both
XCS and XCS/FT. The experimental results confirmed that fault tolerance permitted
the XCS classifier to be more expressive and that XCS/FT is capable of handling fault-
tolerant data mining.
With the fault tolerance mechanism, XCS/FT can be applied to data mining and
can explain a data set with the least rules. People may get the desired knowledge or
information from the evolved classification model. Therefore, XCS/FT may be proven
a useful technique for data mining applications, such as commerce, finance, or biology.
As for XCS/FT itself, interesting research topics and directions, including theoreti-
cal understanding and algorithmic improvement, are waiting to be explored. Research
along this line should be continuously pursued and conducted in order to develop
classification systems that are not only feasible in theory but also viable in practice to
further advance all the related domains and disciplines.
References
Butz, M. V. (2000). Java implementation of XCS. ftp://ftp-illigal.ge.uiuc.edu/pub/src/XCSJava/
XCSJava1.0.tar.Z.
Butz, M. V., Kovacs, T., Lanzi, P. L., and Wilson, S. W. (2001). How XCS evolves accurate classi-
fiers. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 927–934.
Butz, M. V. andWilson, S. W. (2000). An algorithmic description of XCS. Lecture Notes in Computer
Science, 1996:253–272.
Han, J. and Kamber, M. (2005). Data Mining: Concepts and Techniques. Morgan Kaufmann. ISBN:
1558609016.
Holland, J. H. (1975). Adaptation in natural and artificial systems. University of Michigan Press.
ISBN: 0-2625-8111-6.
Horn, J., Goldberg, D. E., and Deb, K. (1994). Implicit niching in a learning classifier system:
Nature’s way. Evolutionary Computation, 2(1):37–66.
Kovacs, T. (1996). Evolving optimal populations with XCS classifier systems. Master’s thesis,
University of Birmingham. Technical report No. CSRP-96-17.
Evolutionary Computation Volume x, Number x 15
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-009-196 
計畫名稱 於 XCS 分類系統中導入容錯機制: 實務影響與理論分析 
出國人員姓名 
服務機關及職稱 
陳穎平 
國立交通大學資訊工程系助理教授 
會議時間地點 97 年 7 月 12 日至 7 月 16 日於 Atlanta, Georgia, USA 
會議名稱 ACM SIGEVO Genetic and Evolutionary Computation Conference 
發表論文題目 On the Effectiveness of Distributions Estimated by Probabilistic Model Building
 
 
一、參加會議經過 
 
今年的基因暨演化計算學術會議 (ACM SIGEVO Genetic and Evolutionary 
Computation Conference) 於 7 月 12 日至 7 月 16 日在美國喬治亞州亞特蘭大市 
(Atlanta, GA, USA) 的 Renaissance Hotel 舉行。 
整個會議的議程包括兩場精彩的 Keynote Speaker 講演、十數場的 Tutorials、
Workshops、數項演化計算方法成果競賽、以及論文之口頭報告和海報展。其內容之豐富、
主題之精彩，實為基因暨演化計算界最高水準之國際學術會議。 
本屆會議之論文接受比例為百分之四十二左右，為歷屆最低。雖然本會議組織章程
中有規定，接受比例不得超過百分之五十，但除上屆及上上屆為百分之四十六之外，過
去皆接近百分之五十。本屆接受比例較前兩屆再下降四個百分點，足顯演化計算領域現
正蓬勃發展，受到各界之重視。 
 
二、與會心得 
 
吾人於到場聽取各研究相關論文之口頭報告，增進對演化計算各領域，獲益良多。
由於此會議之審稿方式為 double blind (即 author 與 reviewer 皆不知對方)，不但嚴格於
絕大多數之學術會議，事實上更甚於一般之 single blind 期刊審稿。因此，如原先所預
期之，大會所選出的論文品質都相當優良，口頭報告者亦或海報展示者之缺席率皆非常
低，各研究人員也都盡心盡力，做出最好的表現。 
吾人所屬之實驗室今年在此會議，發表有兩篇口頭報告論文，其中一篇並被提名角
逐最佳論文獎。雖最終與該獎項失之交臂，但在與國際學者交流之過程中，獲得豐碩之
經驗與討論。 
由於本實驗室兩篇論文的研究主題，在演化計算領域中，分屬不同的子領域，因此，
不但在各論文之子領域中，與國際級之相關學者切磋最先進的研究議題，同時本實驗室
研究方向的多樣性亦吸引許多研究同儕之目光，收穫豐盛。 
 
On the Effectiveness of Distributions Estimated by
Probabilistic Model Building
Chung-Yao Chuang
Department of Computer Science
National Chiao Tung University
HsinChu City 300, Taiwan
cychuang@nclab.tw
Ying-ping Chen
Department of Computer Science
National Chiao Tung University
HsinChu City 300, Taiwan
ypchen@nclab.tw
ABSTRACT
Estimation of distribution algorithms (EDAs) are a class
of evolutionary algorithms that capture the likely structure
of promising solutions by explicitly building a probabilis-
tic model and utilize the built model to guide the further
search. It is presumed that EDAs can detect the structure of
the problem by recognizing the regularities of the promising
solutions. However, in certain situations, EDAs are unable
to discover the entire structure of the problem because the
set of promising solutions on which the model is built con-
tains insuﬃcient information regrading some parts of the
problem and renders EDAs incapable of processing those
parts accurately. In this work, we ﬁrstly propose a general
concept that the estimated probabilistic models should be
inspected to reveal the eﬀective search directions. Based
on that concept, we design a practical approach which uti-
lizes a reserved set of solutions to examine the built model
for the fragments that may be inconsistent with the actual
problem structure. Furthermore, we provide an implemen-
tation of the designed approach on the extended compact
genetic algorithm (ECGA) and conduct numerical experi-
ments. The experimental results indicate that the proposed
method can signiﬁcantly assist ECGA to handle problems
comprising building blocks of disparate scalings.
Categories and Subject Descriptors
I.2.8 [Artiﬁcial Intelligence]: Problem Solving, Control
Methods, and Search—Heuristic methods; I.2.6 [Artiﬁcial
Intelligence]: Learning
General Terms
Algorithms, Design, Veriﬁcation
Keywords
Sensible linkage, eﬀective distribution, model pruning, esti-
mation of distribution algorithms, EDAs, extended compact
genetic algorithm, ECGA, evolutionary computation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
GECCO’08, July 12–16, 2008, Atlanta, Georgia, USA.
Copyright 2008 ACM 978-1-60558-130-9/08/07...$5.00. 
1. INTRODUCTION
Genetic algorithms (GAs) are search techniques based on
the paradigm of natural evolution, in which, species of crea-
tures tend to adapt to their living environments by muta-
tion and inheritance of useful traits. GAs mimic this mech-
anism by introducing artiﬁcial selections and operators to
discover and recombine partial solutions. By properly grow-
ing and mixing promising partial solutions, which are often
referred to as building blocks (BBs), GAs are capable of
solving many problems eﬃciently. The ability of implicitly
processing a large number of partial solutions has been rec-
ognized as an important source of GA’s power. According
to the Schema theorem [13], short, low-order, and highly ﬁt
substrings increase their share to be combined, and also as
stated in the building block hypothesis [6], GAs implicitly
decompose a problem into subproblems by processing build-
ing blocks. This decompositional bias is a good strategy for
tackling many real-world problems, because many real-world
problems can be reliably solved by combining the pieces of
promising solutions in the form of problem decomposition.
However, proper growth and mixing of building blocks
are not always achieved. GA in its simplest form employing
ﬁxed representations and problem-independent recombina-
tion operators often breaks the promising partial solutions
while performing crossovers. This can lead to the vanishing
of crucial building blocks and thus the convergence to local
optima. In order to overcome this building block disruption
problem, various techniques have been proposed. In this
study, we focus on one line of such eﬀorts which are often
called the estimation of distribution algorithms (EDAs) [17].
These methods construct probabilistic models of promising
solutions and utilize the built models to generate new so-
lutions. Early EDAs assume no interaction between vari-
ables [1, 12]. Subsequent studies start from capturing pair-
wise interactions [4, 2, 19] to modeling multivariate inter-
actions [11, 18, 5, 16]. With the reasoning of dependencies
among variables by building probabilistic models, these ap-
proaches can capture the structure of the problem and thus
avoid the disruption of identiﬁed partial solutions.
Another topic concerning this study is the impact of dis-
parate scalings among diﬀerent building blocks to the behav-
ior and performance of GAs. For real-world applications, it
is often the case that some parts of the problem are more im-
portant and contribute more to the ﬁtness evaluation than
other parts. This situation can pose two types of diﬃculties.
Firstly, because the processing in the population is statisti-
cal in nature, the disparate scalings can cause inaccurate
processing of less salient building blocks [8, 10]. The second
391
Generation Marginal Product Model Eﬀective Partial Model
1 [1 2 3 4] [6 11 14] [5 8 12] [7 9 13] [10 15 16] [1 2 3 4]
2 ///[1] ///[2] ////[3] ///[4] [5 6 7 8] [9 12 13] [10 15 16] [11] [14] [5 6 7 8]
3 ///[1] ///[2] ////[3] ///[4]////[5] ////[6] ///[7]////[8] [9 10 11 12] [13 15 16] [14] [9 10 11 12]
4 ///[1] ///[2] ////[3] ///[4]////[5] ////[6] ///[7]////[8] ////[9] /////[10] /////[11] /////[12] [13 14 15 16] [13 14 15 16]
Table 1: Marginal product models built by ECGA in solving an exponentially scaled problem. The variables
are denoted by their index numbers. Each group of variables represents a marginal model in which a marginal
distribution resides. The variables with converged alleles are crossed out.
then we can sample only the corresponding marginal distri-
butions which are, in this case, eﬀective. That is, in the
ﬁrst generation, for each solution string, we re-sample only
s1s2s3s4 according to the marginal distribution and keep the
alleles of s5s6 · · · s16 unchanged. In the second generation,
we re-sample only s5s6s7s8 according to the marginal distri-
bution and keep s9s10 · · · s16 unchanged (note that s1s2s3s4
are already converged). In this way, we do not have to resort
to increasing population sizes to deal with the problems that
are caused by the disparate BB scalings.
The aforementioned thoughts leave us one complication:
the identiﬁcation of eﬀective distributions. However, direct
identiﬁcation of eﬀective distributions might not be an easy
task if not impossible. Hence, it may be wise to adopt a com-
plementary approach—to identify those distributions that
are not likely to be eﬀective. If there is a way to identify
the ineﬀective distributions, we can bypass them and sample
only the rest distributions, thus, to approximate the result
of knowing eﬀective distributions. Our basic idea is that
if we split the entire population into two sub-populations
and use only one sub-population for building probabilistic
model, we can utilize the other sub-population to collect the
statistics for possible indications of ineﬀectiveness of partial
distributions in the probabilistic model built on the ﬁrst sub-
population. That is, with certain appropriate design, we can
prune the likely ineﬀective portions of the model.
In the next section, our implementation of the above idea
on ECGA will be detailed. More speciﬁcally, a judging crite-
rion will be proposed to detect the likely ineﬀective marginal
distributions of a given marginal product model.
3. ECGA WITH MODEL PRUNING
This section starts by brieﬂy reviewing ECGA. Based on
the idea of detecting the inconsistency of statistics gathered
from two sub-populations, a mechanism is devised to identify
the possibly ineﬀective parts of a probabilistic model. Fi-
nally, an optimization algorithm incorporating the proposed
technique is described in detail.
3.1 Extended Compact Genetic Algorithm
ECGA [11] uses a product of marginal distributions on a
partition of the variables. This kind of probability distri-
bution belongs to a class of probabilistic models known as
marginal product models (MPMs). In this kind of model,
subsets of variables can be modeled jointly, and each subset
is considered independent of other subsets. In ECGA, both
the structure and the parameters of the model are searched
and optimized using a greedy approach to ﬁt the statistics
of the selected set of promising solutions. The measure of
a good MPM is quantiﬁed based on the minimum descrip-
tion length (MDL) principle, which assumes that given all
things are equal, simpler distributions are better than com-
plex ones. The MDL principle thus penalizes both inaccu-
rate and complex models, thereby, leading to a near-optimal
distribution. Speciﬁcally, the search measure is the com-
plexity of the MPM which is quantiﬁed as the sum of model
complexity, Cm, and compressed population complexity, Cp.
The model complexity, Cm, quantiﬁes the model represen-
tation in terms of the number of bits required to store all the
marginal distributions. Suppose that the given problem is
of length  with binary encoding, and the variables are par-
titioned into m subsets with each of size ki, i = 1 . . .m, such
that  =
Pm
i=1 ki. The marginal distribution corresponding
to the ith variable subset requires 2ki − 1 frequency counts
to be completely speciﬁed. Taking into account that each
frequency count is of length log2(n + 1) bits, where n is the
population size, Cm can be deﬁned as
Cm = log2(n + 1)
mX
i=1
“
2ki − 1
”
.
The compressed population complexity, Cp, quantiﬁes the
suitability of the model in terms of the number of bits re-
quired to store the entire selected population (the set of
promising solutions picked by selection) with an ideal com-
pression scheme applied. The compression scheme is based
on the partition of the variables. Each subset of the variables
speciﬁes an independent “compression block” on which the
corresponding partial solutions are optimally compressed.
Theoretically, the optimal compression method encodes a
message of probability pi using − log2 pi bits. Thus, taking
into account all possible messages, the expected length of
a compressed message is
P
i−pi log2 pi bits, which is opti-
mal. In the information theory [3], the quantity − log2 pi is
called the information of that message and
P
i−pi log2 pi is
called the entropy of the corresponding distribution. With
the knowledge, Cp can be derived as
Cp = n
mX
i=1
2kiX
j=1
−pij log2 pij ,
where pij is the frequency of the jth possible partial solution
to the ith variable subset observed in selected population.
Note that in the calculation of Cp, it is assumed that the
jth possible partial solution to the ith variable subset is en-
coded using − log2 pij bits. This assumption is fundamental
to our technique to identify the likely ineﬀective marginal
distributions. More precisely, the information of the partial
solutions, − log2 pij , is a good indicator of inconsistency of
statistics gathered from two sub-populations.
3.2 Model Pruning
The proposed technique to identify the possibly ineﬀec-
tive parts of an MPM is based on the notion that ECGA
uses the compression performance to quantify the suitabil-
ity of a probabilistic model to the given set of solutions. The
degree of compression is a representative metric to the ﬁt-
ness of modeling, because all good compression methods are
based on capturing and utilizing the relationships among
393
Algorithm 1 ECGA with Model Pruning
Initialize a population P with n solutions of length .
while the stopping criteria are not met do
Evaluate the solutions in P .
Divide P into S and T at random.
S′ ← Apply t-wise tournament selection on S.
T ′ ← Apply t-wise tournament selection on T .
M ← Conduct greedy MPM search on S′.
M ′ ← Prune M based on the inconsistency with T ′.
for each remaining marginal distribution D in M ′ do
for each string s = s1s2 · · · s in P do
Change the values in s by sampling D.
end for
end for
end while
The empirical results on exponentially scaled problems
are shown in Figure 1. The minimum population sizes re-
quired by the proposed method are lower than that needed
by the original ECGA. Furthermore, with an appropriate se-
lection pressure, the population size needed by the proposed
method grows in a relatively slow rate. The same situation
is observed in the function evaluations that the proposed
method works remarkably well when t = 16.
Figure 2 shows the results on power-law scaled problems.
The results on required population sizes are similar to the
previous set of experiments. The proposed method still uses
fewer function evaluations, but the diﬀerences are reduced.
The empirical results on uniformly scaled problems are
presented in Figure 3. As expected, the proposed method
requires larger population sizes than that needed by the orig-
inal ECGA. The function evaluations used by the proposed
method are about twice as many as that spent by the origi-
nal ECGA under the same selection pressure.
It is noted that a common phenomenon appears in all
of the above three sets of experiments that the proposed
method needs more generations before convergence than the
original ECGA under the same selection pressure. In the
next section, we will further explore this phenomenon using
sets of experiments that augment the population sizes.
4.2 Time-Space Interactions
This section describes sets of experiments that reveal the
behavior of the proposed method when the population size
is adjusted and presents the results to illustrate the interac-
tive eﬀect between population sizes and generations for the
proposed method. In these experiments, the 60-bit problems
(m = 15) are adopted as test functions and the population
sizes are augmented proportional to the minimum popula-
tion sizes estimated in the previous sets of experiments.
As presented in Figure 4, only slight decreases in gen-
erations are achieved by increasing population sizes on the
exponentially scaled 60-bit problem. Among others, the pro-
posed method with tournament size 16 delivered the most
reduction. With no prominent reductions in the generations
and the increasing population sizes, the function evaluations
grow up as expected in all four settings.
Figure 5 shows the results on the power-law scaled 60-bit
problem. In this case, prominent reductions in generations
are observed in the proposed method. However, despite the
presence of these reductions, the function evaluations still
grow up with the increasing population size.
40 45 50 55 60 65 70 75 80
0
500
1000
1500
2000
2500
Problem Sizes (Bits)
Po
pu
la
tio
n 
Si
ze
s
 
 
Original ECGA, t=8
Original ECGA, t=16
ECGA w/ MP, t=8
ECGA w/ MP, t=16
(a) Population Sizes
40 45 50 55 60 65 70 75 80
10
15
20
25
30
35
Problem Sizes (Bits)
G
en
er
at
io
ns
 
 
Original ECGA, t=8
Original ECGA, t=16
ECGA w/ MP, t=8
ECGA w/ MP, t=16
(b) Generations
40 45 50 55 60 65 70 75 80
0
1
2
3
4
5
6
7
8
x 104
Problem Sizes (Bits)
Fu
nc
tio
n 
Ev
al
ua
tio
ns
 
 
Original ECGA, t=8
Original ECGA, t=16
ECGA w/ MP, t=8
ECGA w/ MP, t=16
(c) Function Evaluations
Figure 1: Empirical results of the proposed method
compared to the original ECGA on exponentially
scaled problems. Two tournament sizes t = 8 and
t = 16 are adopted to observe the behavior under
diﬀerent selection pressures.
395
The most signiﬁcant decrease in generations is observed
on the uniformly scaled 60-bit problem as shown in Figure 6.
With tournament size 16, the proposed method reduced up
to 8 generations needed to converge when twice larger popu-
lation size is used. It somehow keeps the number of function
evaluations from climbing up with the population size.
5. DISCUSSION
The proposed method improves the original ECGA on
problems where disparate scalings exist among diﬀerent BBs.
As illustrated in Figure 1(c) and Figure 2(c), prominent re-
ductions in ﬁtness evaluations are achieved. Moreover, in the
uniformly scaled problems where the linkage are completely
sensible, it seems that the proposed method uses just nearly
twice as many function evaluations as the original ECGA.
An extraordinary behavior of the proposed method can
be observed that when a conﬁned population size is given,
it tends to perform a time-space trading using more gener-
ations to overcome the problem. The most notable case is
on uniformly scaled problems shown in Figure 6 that the
proposed method with an appropriate selection pressure re-
duces the generations aggressively when a larger population
is available and thus keeps the function evaluations from
rising up. This phenomenon may be worth further investi-
gations in the hope of discovering a way to relieve the burden
of setting appropriate population sizes.
6. SUMMARY AND CONCLUSIONS
This paper started at reviewing previous studies on EDAs
and scaling diﬃculties. It illustrated how scaling diﬃculties
shadows EDAs’ ability in recognizing BBs. A notion called
linkage sensibility was described, and the term sensible link-
age was proposed to refer to those problem structures that
can be extracted by inspecting only the set of selected so-
lutions. Based on the concept, we deﬁned the eﬀectiveness
of distributions estimated by probabilistic model building
and proposed a general approach to achieve a more eﬀec-
tive modeling. Finally, an implementation of the proposed
approach on ECGA was described and examined on several
test functions with diﬀerent scaling diﬃculties.
In this study, we focused on scaling diﬃculties and their
inﬂuences on EDAs’ ability in recognizing BBs. However, at
a higher level, our attempt was trying to resolve an impor-
tant issue which was rarely addressed: what if the informa-
tion contained in the given population is inevitably insuﬃ-
cient? The approach to solve this problem was proposed and
successfully implemented for ECGA. It may be adopted and
carried over to other EDAs such that more ﬂexible, friendly,
and robust EDAs may be developed.
Acknowledgments
The work was partially sponsored by the National Science
Council of Taiwan under grants NSC-96-2221-E-009-196 and
NSC-96-2627-B-009-001 as well as by the Aiming for the
Top University and Elite Research Center Development Plan
(ATU Plan), National Chiao Tung University and Ministry
of Education, Taiwan. The authors are grateful to the Na-
tional Center for High-performance Computing for computer
time and facilities.
7. REFERENCES
[1] S. Baluja. Population-based incremental learning: A
method for integrating genetic search based function
optimization and competitive learning. Technical
report, Pittsburgh, PA, USA, 1994.
[2] S. Baluja and S. Davies. Using optimal
dependency-trees for combinational optimization. In
Proceedings of the 4th ICML, pages 30–38, 1997.
[3] T. M. Cover and J. A. Thomas. Elements of
information theory. Wiley-Interscience, 1991.
[4] J. de Bonet, C. Isbell, and P. Viola. MIMIC: Finding
optima by estimating probability densities. Advances
in Neural Information Processing Systems, 9:424–430.
[5] R. Etxeberria and P. Larran˜aga. Global optimization
using bayesian networks. In Proceedings of the 2nd
Symp. on Artiﬁcial Intelligence, pages 332–339, 1999.
[6] D. E. Goldberg. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-Wesley,
Boston, MA, USA, 1989.
[7] D. E. Goldberg. The Design of Innovation: Lessons
from and for Competent Genetic Algorithms. Kluwer
Academic Publishers, Norwell, MA, USA, 2002.
[8] D. E. Goldberg, K. Deb, and J. H. Clark. Genetic
algorithms, noise, and the sizing of populations.
Complex Systems, 6(4):333–362, 1992.
[9] D. E. Goldberg, K. Deb, and B. Korb. Messy genetic
algorithms revisited: Studies in mixed size and scale.
Complex Systems, 4(4):415–444, 1990.
[10] D. E. Goldberg and M. Rudnick. Genetic algorithms
and the variance of ﬁtness. Complex Systems,
5(3):265–278, 1991.
[11] G. Harik. Linkage learning via probabilistic modeling
in the ECGA. Technical Report 99010, Illinois Genetic
Algorithms Laboratory, UIUC, IL, USA, 1999.
[12] G. R. Harik, F. G. Lobo, and D. E. Goldberg. The
compact genetic algorithm. IEEE Transactions on
Evolutionary Computation, 3(4):287, 1999.
[13] J. H. Holland. Adaptation in natural and artiﬁcial
systems. MIT Press, Cambridge, MA, USA, 1992.
[14] F. G. Lobo, D. E. Goldberg, and M. Pelikan. Time
complexity of genetic algorithms on exponentially
scaled problems. In Proceedings of GECCO-2000,
pages 151–158, 2000.
[15] T. M. Mitchell. Machine Learning. McGraw-Hill
Higher Education, 1997.
[16] H. Mu¨hlenbein and R. Ho¨ns. The estimation of
distributions and the minimum relative entropy
principle. Evolutionary Computation, 13(1):1–27, 2005.
[17] H. Mu¨hlenbein and G. Paaß. From recombination of
genes to the estimation of distributions I. binary
parameters. In Proceedings of PPSN IV, 1996.
[18] M. Pelikan, D. E. Goldberg, and E. Cantu´-Paz. BOA:
The Bayesian optimization algorithm. In Proceedings
of GECCO-99, pages 525–532, 1999.
[19] M. Pelikan and H. Mu¨hlenbein. The bivariate
marginal distribution algorithm. In Advances in Soft
Computing, pages 521–535, 1999.
[20] D. Thierens, D. E. Goldberg, and Aˆ. G. Pereira.
Domino convergence, drift and the temporal salience
structure of problems. In Proceedings of ICEC ’98,
pages 535–540, 1998.
397
