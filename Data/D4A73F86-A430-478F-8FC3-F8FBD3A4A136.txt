 2
time, and these temporal variables are denoted as 
{F1,F2, …, Fe}. Thus, each instance consists of e 
temporal sequences which represent recorded value 
series of e temporal variables respectively.  
 
instance 1
instance 2
instance x
Time
Variable
Fevn…Fev2Fev1Fe
Cm
……………
F2vn…F2v2F2v1F2
F1 F1vn…F1v1F1v1
classt=n…t=2t=1
D
 
Figure 1. A multivariate temporal dataset. 
 
We defined a temporal sequence of Fe as T= 
{Fev1, Fev2,…, Fevt,…, Fevn}. The value Fevt 
represents the value of Fe at time point t. Each 
recorded value of temporal variables may be 
numerical or categorical. In general case, we assume 
each temporal variable keeps the same sample rate, 
that is, each time interval between t to (t+1) is the 
same length. As temporal variable assumption, the 
dataset D can also be denoted as e}k1|{DD k ≤≤= . 
Dk is a temporal dataset, which consists of the 
temporal sequences of the kth temporal variables of all 
instances in D. Figure 1 shows the dataset that follows 
our descriptions. In a practical case, these temporal 
variables may be different measurements for one 
object need to be described, such as many bio-signals 
for a patient, daily climate measurements for the 
atmosphere of a city. 
In this work, we proposed a new method named 
Progressive Temporal Class Rule Miner 
(PTCR-Miner), to achieve multivariate temporal data 
classification. PTCR-Miner is a rule-based classifier. 
All its rules are generated from the features of each 
temporal variable and the relations between the 
features, which associate with the classes of data. 
In a multivariate temporal dataset, each data is 
composed by sequences of different temporal variables. 
In order to reduce the dimension and complexity of 
data, symbolization of data value is required in data 
preprocessing. In this paper, we assume each temporal 
variable keeps its own class related features. These 
features and the relations between the feature set of 
different variables would be key points for 
classification. The amplitude of each temporal variable 
is determined by the nature of the variable and it 
should not be affected by other variables theoretically. 
Meanwhile, the amplitude of a temporal variable is 
usually not directly related to data classification. So, 
we separate variable data firstly before symbolization. 
As regards the discretization methods, users can 
choose suitable one for different temporal variable or 
the ones suggested by experts, such as PAA and SAX 
[8]. 
Confidence of rules in a rule-based classifier is a 
very important attribute for accurate classification. For 
multiple class data classification, most features 
support more than one class and the importance of a 
feature can not be evaluated only with its confidence 
values easily. Thus, we take confidence concept and 
classification related factors into consideration and 
integrate them into a “purification” concept. The 
purpose of purification is to extract really useful 
features and to make the rules generated by the 
features simpler and stronger for classification. The 
major definition of purification is to seek less class 
support and higher class confidence of the generated 
rules. For example, a generated classification rule R 
supports a class set {c1, c3, c6}, that means that the 
data predicted by R are possible to be classified as one 
of the class set. Given another rule R’ which is 
extended from R has a class set {c1, c3}, that is the 
descriptions of R’ is totally cover the descriptions of R. 
So, the confidence values of c1 and c3 of R’ must be 
higher than those of R. We say that this extension obey 
our purification concept. R’ is simpler and stronger 
than R for classification. 
For convenience of algorithm description, few 
symbols and words should be defined. In our method, 
we expect to discover classifiable rules from the 
dataset D for classification. We define each discovered 
rule as a Progressive Temporal Class Rule (PTCR), 
which consists of one temporal sequence and a 
supported class set. The temporal sequence is defined 
as Progressive Temporal Sequence (PTS) or 
Multivariate Progressive Temporal Sequence (M-PTS). 
A PTS is formed by a sequence 
}ba, andrn rbra1 b,a| v..., ,v,v..., ,v,v{ rnrbrar2r1 Ζ∈≤≤≤≤
 of single temporal variable. One M-PTS is composed 
by time ordered PTSs of different temporal variables 
and we defined it as {PTSs1, PTSs2, …, PTSsa, 
PTSsb,  …, PTSsn 
}ba, andsn sbsa1 b,a| Ζ∈≤≤≤≤ . Besides, the 
supported class set is the subset of the class set of 
dataset D, which are highly supported by PTS or 
M-PTS. Therefore, a PTCR can be denoted as 
 
PTS: {a sub-class set of dataset D}  or 
M-PTS: {a sub-class set of dataset D} 
 
We integrate the key points mentioned above to 
propose a Progressive Temporal Class Rule Miner 
(PTCR-Miner) algorithm on multivariate temporal 
data. Through the algorithm class rules of a 
multivariate temporal dataset can be discovered and a 
classification mechanism can be built by the class rules 
indirectly. Rule generation part is designed as an 
apriori-like architecture following the purification 
concept. 
 
Algorithm of PTCR-Miner 
Figure 2 is the algorithm of PTCR-Miner. The 
algorithm needs three input parameters, which are a 
multivariate temporal dataset, minimum support and 
minimum confidence. The output is the discovered 
Progressive Temporal Class Rule set of the input 
dataset. As descriptions of the algorithm, the whole 
input dataset are separated into several sub-dataset 
 4
In Figure 4(a), a temporal series data has nine 
instances and two parameters are given. Firstly, a 
candidate tree is built to evaluate all elements. In the 
candidate tree, all support and confidence values under 
the minimum parameter setting are marked in dark 
gray color. In this case, building tree for frequent 
elements discovery is to make following steps easier to 
be understood. In general, user can perform any 
method to extract the elements which can pass 
minimum support and confidence threshold. Through 
essential item discovery as shown in line 1 of the 
algorithm in Figure 3, the items pass the minimum 
parameter settings are extracted and built in a tree, like 
Large 1 in Figure 4. 
In Figure 4(b), the extension of a tree is shown 
in detail. The tree of Candidate 2 is the extension of 
the tree of Large 1 of Figure 4(a). The depth of the 
tree is extended from 1 to 2. For node ‘C’ in Large 1, 
the elements of itself and its sibling nodes are {A, B, C, 
D}, so these elements could be added as its child 
nodes for the extension. Nevertheless, according to the 
class set of the “C” node, only C1 and C2 pass the 
minimum confidence check. Therefore, only {A, B, C} 
can really be extended nodes of the node “C”. The 
reason is that the class set of node “D” contains only 
C3 and the intersection of the class sets of node “C” 
and node “D” is null. That means the path from root to 
“D” is focus on class set {C3}and it is unreasonable to 
generate any essential rules which can imply an class 
set strongly by joining two nodes without the same 
class focus. Even the element “A” is added as a new 
child node of the node “C”, its class C3 is not be 
considered in the new candidate tree. Thus, we mark 
the classes cannot be count again in gray color in 
candidate trees of Figure 4. 
Figure 4(c) shows the last PTS mining step of 
the dataset, and all discovered PTSs are list in the right 
gray box. In candidate 4, all length-4 candidate PTSs 
are generated with the tree architecture, which are 
{AACC, ACBB, DADD}. However, all of them do 
not pass the threshold of minimum support, so the 
PTS-mining is stopped in Large 4. No node exists and 
no candidate PTS can be generated. 
Figure 4 is a progressive temporal sequence 
mining example on single temporal variable data, 
which explains the work flow of PTS-mining in detail. 
PTS-mining is performed twice in PTCR-Miner. The 
PTSs of each temporal variable are extracted in first 
mining. Based on the output of first mining the 
advanced information would be discovered as stronger 
and more confident classification rules in second one. 
 
M-PTS Mining 
After all PTSs of each variable sub-dataset are 
prepared, the advanced relations between the PTSs of 
different temporal variables could be generated. All 
discovered PTSs represent important features of their 
own temporal variables. However, there must be 
useful class information hidden in relations between 
different temporal variables. Therefore, we translate 
the whole original multivariate temporal dataset into a 
new PTS sequence dataset. In the new dataset, each 
data consists of the PTSs it contains and all PTSs are 
ordered by their happen time. This translation makes 
the progressive temporal sequence mining can be 
performed directly on the dataset. New progressive 
temporal sequences consist of feature sequences 
instead of value sequences and they represent 
classifiable relations between different temporal 
variables. We name these relations as Multivariate 
Progressive Temporal Sequences (M-PTS). 
After executing PTS-Mining processes twice, all 
class- related features of the multivariate temporal 
dataset, the PTS and M-PTS, are extracted. Each 
feature consists of a sequence of values or PTSs and a 
class set they support. Following the purification 
concept, so the confidence values of the class set of 
these features are very significant. The higher 
confidence of a class Cy in a feature F represents that 
the data contains F has higher probability to belong to 
the class Cy. Therefore, we set directly the confidence 
values of class set of features as scores for temporal 
data classification. We name these features with the set 
score “progressive temporal class rules” (PTCR).  
Classification with PTCR 
In following segments, we explain how to classify a 
class unknown multivariate temporal data in detail 
with PTCR classifier. Figure 5 shows the detail 
algorithm of classification with the discovered PTCRs. 
For a class unknown multivariate temporal data X, we 
must check which PTCRs it contains. The PTCRs 
consist of PTSs and M-PTSs, so all PTSs and M-PTSs 
must be checked. The contained PTSs can be 
discovered by tracing a reconstructed tree structure of 
all PTSs, instead of brute-force matching, and M-PTSs 
can also be check in the same approach. Subsequently, 
the sum of the score for each class can be accumulated 
by the score of all matched PTSs and M-PTSs. The 
classification result is the class with the highest score. 
 
 
ID Temporal Series Class
T1 A,C,A,E,C C1
T2 A,D,A,C,C C1
T3 B,A,A,C,B C1
T4 A,A,C,B,A C2
T5 C,A,E,B,D C2
T6 C,A,B,C,B C2
T7 E,D,A,B,D C3
T8 D,B,A,C,D C3
T9 B,B,D,A,D C3C1 C2
3/6 2/6
C2
3/5 L1:
{A(C1,C2,C3), B(C2,C3), C(C1,C2), 
D(C3)}
L2:
{AA(C1),AB(C2),AC(C1,C2),AD(C
3),BA(C3),BD(C3),CA(C2),CB(C2),
CC(C1),DA(C3),DD(C3)}
L3:
{AAC(C1),ACB(C2), DAD(C3)}
L4:
{}
Root
C1 C2 C3
3/9 3/9 3/9
A
C2 C3
3/7 3/7
B
C1 C2
3/7 3/7
C
C3
3/5
D
C1
3/4
A B C
C3
3/5
D
C3
2/4
A
C3
3/4
D
C2
3/4
A
C2
2/3
B
C1
2/3
C
C3
3/4
A
C3
3/4
D
C1
3/4
C
C2
2/3
B {A,B,C},{BA,AA,AC,AB,CB},{AAC,ACB}
C1:3/9+3/7+3/4+3/6+3/4+2/3 = 3.428571
C2:3/9+3/7+3/7+2/6+3/5+2/3 = 2.790476
C3:3/9+3/7+2/4 = 1.261905
C2
2/3
D
 
Figure 6. An example of classification with PTCR set. 
ss_index]result_claclass_set[return    8
]e[class_scor argmaxss_indexresult_claset    7
e.confidencr.class[c]e[c]class_score[c]class_scor               6
rc classeach For            5
r) contains(x  if       4
PTCR_setr ruleeach For    3
0 as  valuesall and ]class_set)e[size_of(class_scormatrix  aset    2
lass_setPTCR_set.cclass_setset    1
x)(PTCR_set,classifier-PTCR Procedure
ss)data(x.clainput  ofresult tion classifica a :Output
)instance(x  temporaltemultivariaunknown -class a            
et),set(PTCR_s rule class  temporaleprogressiv:Input
classifier-PTCR Algorithm
y
y
=
+=
∈
∈
=
Figure 5. The algorithm of classification with PTCR classifier. 
 6
average accuracy. The accuracy of our method grows 
up with the decreasing of minimum support. That 
means that lower threshold makes our classifier more 
accurate. Under smaller value of minimum support 
cause more possible sequences are considered and 
more useful rule are discovered. Therefore, we can 
know the relationship between accuracy of 
classification results and minimum support value is 
reverse. Figure 9(b) shows the execution time of both 
methods. Thus, the execution time of Sum-1NN is 
almost a constant value for comparison with our 
method in Figure 9(b). Execution time of our method 
is increasing slightly with decreasing of minimum 
support but all its values are smaller than ones of 
Sum-1NN. 
0
50
100
150
200
250
300
350
0.04 0.05 0.06 0.07 0.08 0.09 0.1
minimum support
ex
ec
u
ti
o
n
 t
im
e(
s)
PTCR-Miner
Sum-1NN
60
65
70
75
80
85
90
95
100
0.04 0.05 0.06 0.07 0.08 0.09 0.1
minimum support
ac
cu
ra
cy
(%
)
PTCR-Miner
Sum-1NN
 
Figure 9. (a) Accuracy vs. minimum support, (b) 
Execution time vs. minimum support. 
Figure 10 shows accuracy results with varying 
minimum confidence setting of our method. In Figure 
10(a), our method reaches highest accuracy result at 
minimum confidence=0.2. Basically, the lower 
confidence setting makes higher accuracy of our 
classification result but accuracy goes down when 
minimum support is set too small. For execution time 
of our method, its value grows up with reduction of 
minimum confidence. However, all execution time of 
our method are less than that of 1-NN. 
60
65
70
75
80
85
90
95
0.1 0.2 0.3 0.4 0.5
minimum confidence
ac
cu
ra
cy
(%
)
PTCR-Miner
Sum-1NN
0
50
100
150
200
250
300
350
0.1 0.2 0.3 0.4 0.5
minimum confidence
ex
ec
u
ti
o
n
 t
im
e(
s)
PTCR-Miner
Sum-1NN
 
Figure 10. (a) Accuracy vs. minimum confidence, (b) 
Execution time vs. minimum confidence. 
 
The number of relation type vs. accuracy 
Figure 11 shows accuracy results with varying 
relation_type_num setting of data simulator. The 
parameter relation_type_num represents the quantity 
of relation types and the relation is the classification 
information between different temporal variables. 
75
80
85
90
95
100
5 10 15 20 25 30
the number of relation types
ac
cu
ra
cy
(%
)
PTCR-Miner Sum-1NN
0
50
100
150
200
250
300
350
5 10 15 20 25 30
the number of relation types
ex
ec
u
ti
o
n
 t
im
e(
s)
PTCR-Miner
Sum-1NN
 
Figure 11. (a) Accuracy vs. parameter 
relation_type_num, (b) Execution time vs. parameter 
relation_type_num. 
In Figure 11(a), the accuracy trends of both 
methods are almost the same. The accuracy of 
PTCR-Miner is always higher than the accuracy of 
Sum-1NN. The accuracy results of both methods go 
down with increasing relation_type_num. For a 
simulated dataset, that more relation types increase the 
number of M-PTSs. However, fixed PTS setting lets 
all generated M-PTSs too easy to keep similar PTSs. 
Thus, the dataset becomes not conducive to be 
classified. In Figure 11(b), the frequency of each 
relation type becomes lower with the increasing of 
relation types. It causes that PTCR-Miner discovers 
fewer rules under the same mining threshold setting 
and costs less time on rule mining. Nevertheless, the 
kNN-based method is not affected for any information 
hidden in data on execution time. 
 
Useful variable number vs. accuracy 
Figure 12(a) shows the accuracy results of both 
methods with varying useful_var_num. This parameter 
represents the number of the temporal variables really 
related to class label in the simulated dataset. We set 
variable_number as 6 in this experiment. According 
the results of Figure 12(a), PTCR-Miner outperformed 
Sum-1NN in all partial related conditions and the 
highest accuracy results of both methods are at the 
fully related condition. The accuracy trends of both 
methods go down with the value of useful_var_num. 
The trend of Sum-1NN almost decreases linearly. 
PTCR-Miner is not so sensitive to these changes, and 
its trend has a lower result at useful_var_num=2. The 
reason is that the general kNN-based methods assume 
all temporal variables are related to class. However, 
PTCR-Miner is designed to discover class related 
information so it would be affected very slightly by the 
redundant temporal variable. Figure 12(b) shows the 
execution time trends of both methods grow down 
with decreasing the redundant temporal variables. That 
means that redundant information causes more 
execution time. In whole results, PTCR-Miner still 
outperforms than Sum-1NN on execution time. 
40
50
60
70
80
90
100
2 3 4 5 6
The number of useful temporal variable
ac
cu
ra
cy
 (
%
)
PTCR-Miner
Sum-1NN
0
2000
4000
6000
8000
10000
2 3 4 5 6
The number of useful temporal variable
ex
ec
u
ti
o
n
 t
im
e(
s)
PTCR-Miner
Sum-1NN
 
Figure 12. (a) Accuracy vs. parameter useful_variable, 
(b) Execution time vs. parameter usefule_variable. 
 
3.2. The Applications on Asthma Care 
For the application of multivariate temporal data 
classification, we explore to predict asthma attacks 
automatically by mining asthma physiological 
symptoms and environmental factors together, which 
constitute multivariate temporal data. Thus, we used 
data mining methods to discover features of asthma 
attacks and construct a classification mechanism for 
those features in order to establish an alarm system 
and to aid healthcare practitioners. For most patients 
with chronic diseases who need to be monitored, 
hospitals usually record their daily disease related data. 
 8
preprocessing elements, these datasets are integrated 
and transformed into a specific dataset with medical 
and environmental knowledge for mining. 
Subsequently, two alternative methods were designed 
for the data mining segment. Finally, the disease 
related diagnosis rules would be discovered or the 
prediction model would be built. These results could 
aid healthcare practitioners in disease diagnosis and 
construct an advanced predictive alarm mechanism. 
In order to reduce problem complexity, and to 
increase the medical significance and readability of 
outputs, we symbolized these numerical values with 
the meaning and information of each value on its 
related studies. For example, the concentration of 
environmental factor CO is a numerical attribute, but 
we can symbolize its value into different levels of air 
pollution. This discretization method of numerical 
value is more convenient to be conducted and makes 
the result of any analysis more comprehensive. 
Resultantly, we reviewed all related studies of each 
data and chose appropriate policies for data 
preprocessing, such as common level definitions for 
some air pollutants. In our proposed mechanism if 
there was no suitable expertise of one factor data with 
numerical, some common methods of data mining 
could be applied directly, such as piecewise aggregate 
approximation(PAA). 
In addition, feature selection can be considered 
according to input datasets. In our proposed 
mechanism, the purpose of feature extraction is mainly 
to derive characteristics from multivariate time series 
datasets. All extracted features could be meaningful 
and applied directly on the following classification 
mining. However, feature selection also can be 
considered to be performed after feature extraction 
under some conditions, such as a huge quantity of 
input dataset or space limitation for the final classifier. 
We suggest choosing the feature selection methods 
which do not destroy or rebuild features, such as 
Relief [13], Information Gain (IG) [11], or Mutual 
Information (MI) [4]. 
As shown in Figure 15, we adopted a two-phase 
concept to perform the integrated data mining. The 
concept consists of a feature extraction phase and a 
rule mining phase. After preprocessing, there could 
still be differences in the format of data. We extracted 
all disease related features in the feature extraction 
phase. Then, all the features could be rearranged in 
some specified format. Finally, the rules or the model 
for disease attack prediction can be constructed by the 
features in the last part of the mechanism. Based on 
the proposed architecture, we presented two policies 
for asthma attack prediction. The first policy is named 
Pattern Based Decision Tree (PBDT), which integrates 
sequential pattern mining and decision tree mining; the 
second is named Pattern Based Class-Association Rule 
(PBCAR), which merges sequential pattern mining 
and classification-by-associations mining. 
 
PBDT: Pattern Based Decision Tree 
We designed a tree-based method, named the PBDT, 
for asthma high-risk prediction, which follows the 
two-phase design and the architecture shown in Figure 
15. After data preprocessing, the concept of sequential 
pattern mining is adopted to retrieve the representative 
sequences of these time series data as static features in 
the first phase. Secondly, the decision tree is 
assembled with these extracted features. The pseudo 
code of the PBDT algorithm is shown in Figure 16 and 
its implementation on all asthma related dataset are 
described in the following sub-sections. 
 
 
Phase 1 of PBDT: Feature Extraction 
Most prepared related data are time series 
datasets, such as asthma symptom dataset and 
environmental dataset. These types of datasets are 
more difficult to comprehend for users and have to be 
processed by mining methods. Thus, the PBDT 
segmented all these data into two datasets. The first 
dataset consists of all segments of time series datasets 
with the risk of asthma attacks. The other comprises 
all segments of time series datasets, but without the 
risk of asthma attacks. There two datasets are 
represented as Sa and Sn in the proposed algorithm in 
Figure 16. Each segment of both datasets contains 
time series data for asthma symptom and 
environmental ones. It is expected that there could be 
obvious differences in their characteristics between 
these two datasets. Therefore, after all segments in 
both datasets are processed, all sequential patterns as 
features were extracted from both datasets respectively. 
The lines 5-7 of the PBDT algorithm in Figure 16 are 
written for this extraction. 
In the implementation of this phase of the PBDT, 
a parameter λ is defined as observation interval and it 
represents the number of the days for observation. The 
c
Algorithm PBDT
Input: A dataset consists of considered factors (D = {d | c>0}),
          Observation interval ( ),
          Minimum support (min_sup)
Output: a disease classifier (PBDT_classifier)
Proced
λ
p
a p
ure PBDT ( D, w, min_sup)
Phase1:  //Feature Extraction
1    let D  be the dataset which is rearranged data D with patient id.
2    t  = ExtractDiseaseAttacks(D ); //To collect time points of disease att
a a
n a
a a
n n
a n
acks
3    S  = HighRiskSegmentsExtraction(D, t , );
4    S  = NormalSegmentsExtraction( (D-S ), );
5    F  = FeatureExtraction(S );
6    F  = FeatureExtraction(S );
7    F = F  + F ;
Phase 2:   //The tabl
λ
λ
p p
p
p
e of features F is prepared in phase 1.
8    set feature table B =  
9    for each patient data d  D
10       set a vector v, and v.class=d .class
11       for each feature f  F
12           if d  conta
φ
∈
∈
f
f
in f
13               v  = 1;
14           else
15               v  = 0;
16       end for
17       B = B v
18   end for
19   PBDT_classifier = DecisionTreeBuilding(B);
U
 
Figure 16. PBDT algorithm 
 10
prepared respectively for each environmental factor. 
There are numerous environmental factors, such as 
daily averaged temperature, and the daily 
concentrations of carbon dioxide, so we can get 
high-risk and normal datasets for each environmental 
factor. Figure 20 shows a high-risk segment of a daily 
averaged temperature dataset is extracted by matching 
the time interval of a high-risk segment of the asthma 
dataset. The value sequence {28.4, 28.4, 27.8, 28, 25.3} 
which represents the temperature situations can be 
accessed before this patient suffer from the asthma 
attack. 
 
 
 
According to the design of Phase1, the PBDT 
performs sequential pattern mining to discover 
features of each environmental factor on its high-risk 
and normal segment sets. Thus, high frequent 
sub-sequences of high-risk segments and normal 
segments for each environmental factor can be 
extracted and have the following format: 
 
Pattern Pe from high-risk segments in 
Environmental factor M :{ state i, state j} 
Pattern Ne from normal segments in 
Environmental factor M :{ state x, state y, state z}. 
 
All features of each asthma related dataset were 
prepared, which contain patient clinical data, asthma 
symptom dataset, weather factor dataset, and air 
pollution dataset. These features are integrated as an 
indexed table for the second phase of the PBDT. Note 
that the two important data tables must be generated. 
The first table contained the time intervals of high-risk 
segments and normal segments. All high-risk and 
normal segments of patients must be identified by 
these time intervals. Table 2 presents a sketch of this 
table. Another table listed all features extracted with 
sequential pattern mining respectively from high-risk 
and normal segments of each sub-datasets. The sketch 
of this table is shown in Table 3. 
 
 
 
 
Phase 2 of PBDT: Rule Mining 
In this phase, a disease classifier was built with 
all the extracted information from the last phase. After 
phase 1 of the PBDT, the all asthma related high-risk 
and normal frequent features were prepared. Firstly, 
the high-risk and normal segments of each factor data 
were transformed into the attribute table with their 
own frequency features. This processing was 
represented from lines 8 to 18 in the PBDT algorithm 
and makes traditional decision tree mining performing 
on the datasets having time series data. Subsequently, 
as the last line of the PBDT algorithm in Figure 16, 
decision tree mining is performed to get a 
classification model. Its tree-based model can predict 
asthma attacks and the features related to asthma 
attacks can be informed from its matching paths. 
In practice, the process consists of the following 
two steps: 
In the first step, a classification module is built 
with decision tree mining. The tables prepared in 
phase 1 of the PBDT must be integrated and 
transformed into an attribute table. Each segment 
contains many sequence data of different asthma 
related factors and it is processed as a transaction in 
the attribute table. All features of all asthma related 
factors are attributes. The table keeps all connections 
between transactions and attributes. So that all 
categories, and types of features and transactions can 
be accessed from this table. Table 4 presents the 
sketch of the attribute table. The grid of segment p and 
factor pattern q is 1 when all data in the time interval 
of segment p contain the pattern q. Otherwise, it 
should be 0. The grids of high-risk attribute follow the 
high-risk values of segments. 
After the attribute table is established, all 
relations between features, high-risk segments and 
normal segments are labeled. Therefore, decision tree 
mining can be performed on this table with setting 
“High risk?” attribute as target to generate a decision 
tree. The decision tree is a binary tree and like the 
tree-like structure shown in Figure 21. Each tree node 
(i.e. leaf) of the tree represents one feature, and two 
sub-trees of a tree node are under the conditions with 
the feature and without the feature, respectively. All 
paths in the tree are oriented to produce one result, so 
a prediction can be made if an unknown segment is 
high-risk by tracing the tree with its feature set. Figure 
21 shows a sketch of the tree with more details. 
g …
…
y
X
rgygDaily record
Asthma attack
Asthma allergic data: risk light grade
Jan. 7Jan. 6Jan. 5Jan. 4Jan. 3Jan. 2Jan. 1Date Time
28.4
(L7)
…
27
(L6)
25.3
(L5)
28
(L7)
27.8
(L6)
28.4
(L7)
Daily record
Environmental factors: daily average temperature
 
Figure 20. Extraction of a high-risk segment from 
Table 3. All features of all asthma related factors. 
Pattern Pan
…
Pattern Pa1
Pattern Pemr
Pattern Pe1q
…
Allergic
(1~n)
Asthma 
Symptom data
…Factor m
(1~r)
……
Pattern Pe11
The features of high risk segments 
and normal segments
Factor 1
(1~q)
Environmental 
data 
FactorsDataset
Table 2. The time intervals of all segments. 
N
…
N
Y
…
Y
High risk?
patient e: d(j+h)1, d(j+h)2, …, d(j+h)λSegment (j+h)
……
patient 1: d
(j+1)1
, d
(j+1)2
, …, d
(j+1)λSegment (j+1)
patient e: d
k1
, d
k2
, …, d
kλSegment k
……
patient 1: d11, d12, …, d1λ
Time interval
Segment 1
 12
Two tables of phase 1 should be integrated into a 
transaction table in this step. Table 5 shows the result, 
in which the features and class label of each segment 
are listed clearly. In this table, each segment is a 
transaction, and each feature is an item of the segment. 
In order to fill the table, all features of all factors must 
be checked if they are contained for each segment. 
Each data segment consists of data sequences of 
different factors. Each data sequence may contain 
several features of its factor. Subsequently, the table 
collects all contained features of all factors as the item 
set for each segment. After collection of item sets of 
all transactions, the high-risk attribute of each segment 
is appended to this transaction table. 
 
In this step, the classifiable frequent item sets 
are extracted. Therefore, association rule mining was 
adopted here to extract frequent item sets firstly. After 
frequent item sets are collected, the confidence of item 
sets for each class are counted. Finally, the classifiable 
frequent item sets are prepared in which each one is 
frequent and keeps confidences of each class. The 
CBA method [15] can predict the class of a novel 
transaction with these item sets. However, the 
classification policy of the CBA method is too simple 
to have a good accuracy, we then further modify the 
classification method in next step. 
The purpose of this step is to build a scoring 
mechanism. Scoring methods are widely applied in 
rule-based classification methods. Basically, the CBA 
classifies a new data by the matched class-association 
rule which have the highest confidence of the result 
class. Though the method can produce the result 
rapidly, the considered factors are not enough. In [16], 
Liu et al. enhanced the CBA and proposed a scoring 
method, which performs classification by considering 
all rules. Following their concept, each rule keeps 
different scores for different classes. For each rule, the 
score of a class is defined as the value which is the 
confidence of that class multiplied by the support. 
Thus, a new case is classified with the cumulated 
scores of all classes from matched rules in the 
PBCAR. 
The scoring method of the PBCAR takes more 
factors into consideration, but there are still some 
irregularities. Most datasets for classification have 
imbalanced distribution of the target attribute, 
especially disease prediction datasets. The asthma 
dataset also has this problem, that is, there are fewer 
high-risk (positive) cases than normal (negative) ones. 
The skew datasets make a huge difference between the 
quantity of positive features and negative features. 
That promotes the results of the scoring method to be 
more negative, and reduces the recall of positive 
simultaneously. Therefore, we referred the work in [16] 
to define a parameter k as balance parameter to fix this 
imbalance in the two-class dataset. Figure 24 shows 
the definition of modified score function in the 
PBCAR. All normal scores of rules are needed to be 
divided by k, so that the final cumulated scores of two 
classes are more appropriate for classification. The 
value of the parameter k generally needs to be 
formulated by experts or set by experimental 
evaluations. In regard of this, we did experiments to 
evaluate its value and discuss its effects, which are 
discussed and explained in detail section 4. 
 
After the four steps of building classifiers, the 
generated rule set appears as is represented in Figure 
25. According to the patient’s daily record, the 
PBCAR gathers all data sequences of related factor 
datasets in λ days. The data sequences are transformed 
into a feature set like a transaction in Table 4 by 
matching the ones in Table 2. Then, the PBCAR 
checks the rule set of Figure 25 and marks all matched 
rules. Finally, the PBCAR classifier accumulates the 
scores of normal and high-risk ones of these matched 
rules and gives the patient the result keeping higher 
score as a prediction result for the coming tomorrow. 
 
 
If the second rule in Figure 25 is marked, it 
represents the existence of asthma related data and 
high temperature difference, a high PM10 value of air 
pollutant data indicates that the patient had a fever in 
the past λ days. So according to these features, if this 
rule keeps the highest confidence of all marked rules, 
the CBA predicts that the patient will suffer from an 
asthma attack at (λ+1)th day. However, our PBCAR 
method takes the scores of all marked rules into 
consideration, Figure 25 just offers a score 0.45*0.3/2 
= 0.0675 for normal and a score of 0.45*0.7 = 0.315 
for high-risk, respectively. 
These classification steps are shown as a flow 
chart in Figure 26. There are two differences in 
classification method from the PBDT. 1) Input data 
must be transformed into transaction format instead of 
attribute format. 2) More rules are matched. Those 
{Photophobia, Apply controller} sup: 0.51, conf: [ N: 0.2, A:0.8 ]
{High temperature difference, PM10 is high, Fever} sup: 0.45, conf: [ N: 0.3, A:0.7 ]
{No treatment advised, No air pollution} sup: 0.33, conf: [ N: 0.68, A:0.32 ]
…
{CO2 is high, PM10 is high} sup: 0.31, conf: [ N: 0.35, A:0.65 ]
and balance parameter k = 2  
Figure 25. Class-Association Rule of the proposed 
PBCAR for classification. 
The score of each matched rule of both classes (Positive/Negative)
- Positive Score = support * confidence
- Negative Score = support * confidence / k
- k is a constant value to adjust the imbalance scores.
Classification
If (Positive score > negative score) result = positive
else result = negative
return result  
Figure 24. Scoring function of class-association rules of the 
PBCAR. 
Table 5. A transaction based data table. 
N
…
N
Y
…
Y
High risk?
{X3, P1},{X5, P3},…,{Xm, P5}Segment (j+k)
……
{X6, P2},{X1, P3},…,{Xm-1, P3}Segment (j+1)
{X2, P3},{X3, P1},…,{Xm-2, P1}Segment j
……
{X1, P1},{X1, P3},…,{Xm, P4}Segment 1
Features {Factor , Pattern}
 
 14
 
According to the design of our proposed 
methods, we must symbolize all numerical values of 
the dataset to simplify the following operations. 
 
Table 6 shows the relationship between patient 
respiratory flow and the risk level of asthma attacks. In 
the asthma dataset, there are two important indicators: 
(1) peak expiatory flow rate (PEFR) and (2) asthma 
medication dosage instructions. The PEFR is the value 
measured from a patient’s breath every morning using 
a peak expiratory flow meter. According to the 
patient’s physiological conditions and the proper 
normal respiratory flow rate of his/her physiological 
conditions, patients’ circumstances can be classified 
into the following three levels of red, yellow and green 
lights in the proposed mechanism. The light level and 
patients’ allergic symptoms indicate which medication 
instructions should be adopted. Therefore, we 
symbolized a patient’s physiological conditions by the 
values of these two indicators. Combinations of their 
possible values are represented by six categorical 
symbols and we took them to symbolize patients’ 
asthma statuses in our experiments of this work. 
In environmental dataset, the values of its data 
are numeric. It is necessary to symbolize them to 
increase the efficiency of classification model building 
and the readability of prediction results. Therefore, we 
refer the deputy index values of each air pollutant, 
which are defined by proficient practitioners. The 
symbolized values simplify the complexity of data and 
become effortlessly comprehensible. Table 7 shows a 
Table 7. PEFR value vs. its corresponding medication 
instructions and lights. 
PEFR Value (Peak 
Expiratory Flow 
Rate) 
Medication instruction Light 
The value is between 
80%~100% of 
personal optimal 
value or normal 
estimates. 
Variability : < 20%
Meaning: the state of 
asthma is under 
control. 
Do not need medication Green 
The value is between 
60%~80% of personal 
optimal value or 
normal estimates. 
Variability: 
20%~30% 
Meaning: Asthma 
Symptom: 
the nigh 
attack, 
activity 
reduction, 
cough, 
stridor, chest 
tightness. 
To use short-acting inhaled 
expansion of tracheal twice each 
20 minutes. If there is no 
improvement of respiratory flow 
rate after using 20~60 minutes, 
please go for medical treatment as 
soon as possible. 
Yellow 
The value is between 
60%~80% of personal 
optimal value or 
normal estimates. 
Variability: >30% 
Meaning: The state 
of asthma would 
affect patient’s 
normal routines and 
activities. 
To use short-acting inhaled 
expansion of tracheal 
immediately. If the respiratory of 
flow is still lower than 60% after 
using, please go for medical 
treatment as soon as possible. 
Red 
 
Table 6. Attribute list of our dataset. 
Sub-dataset Category Attributes Count
asthma 
allergic 
dataset 
Patients’ 
diagnostic 
record 
ssn, d_ssn, age, reference, 
date, 5 
Asthma 
symptoms 
day_record, day_pefr, 
night_record, night_pefr, 
fever, 
night_symptom, 
day_symptom, 
nose_symptom1, 
nose_symptom2, 
nose_symptom3, 
nose_symptom4, 
eye_symptom, skin_symptom, 
grade, 
asthma_medicine_instructions
, 
a_rhinitis, a_conjunctivitis, 
a_dermatitis, 
18 
environmenta
l dataset 
Air 
pollutants 
Date, PSI, indicators of the 
quality of air pollutants, 
daily concentration of 
pollutants reports, 
SO2, NO2, O3, CO, PM10, 
NO, NitroOxy, 
NHHC, HydraCarbon, 
HydraCarbon2, Temperature, 
PH, 
16 
Weather 
data  
Relative Humidity (%), 
Temperature ( ),℃  
Absolute Maximum 
Temperature ( ),℃  
Absolute Minimum 
Temperature ( ),℃  
4 
 
 16
discovered rules of the PBCAR are significant for 
asthma attack prediction. The last parameter is the 
training rate of a dataset. In the general data mining 
case, we set 70% of the whole dataset as the training 
constituent and 30% as the testing one. Our 
experimental evaluations also proved that this training 
rate can produce the best result on the asthma dataset. 
 
Comparisons of PBDT and PBCAR 
The whole asthma related dataset consists of weather, 
air pollutant and asthma allergic datasets. In this 
sub-section, we evaluate the recall and accuracy of 
both proposed methods by applying different types of 
datasets and endeavor to demonstrate each factor 
dataset considered are useful for asthma attack 
prediction. In other words, we performed both 
methods on the weather dataset, the air pollutant 
dataset, the asthma allergic dataset and the asthma 
related dataset, which integrates all single factor 
datasets. Figure 30 shows the accuracy and the recall 
of each dataset respectively. Overall, the results of the 
integrated dataset are better than the results of other 
single factor datasets. Although the recall of the PBDT 
is especially different, we inferred that the greedy 
architecture of the decision tree makes the result easier 
to be confused and unstable by inputting more 
information. Therefore, we discussed the two major 
problems in these experimental results in the following. 
Firstly, we compared the classifiers of both proposed 
methods in details to ascertain what makes the 
experimental results different from ideal inference. 
Secondly, we discussed why the results of integrated 
dataset are not better than those of asthma allergic 
dataset using the PBDT method. 
 
In Figure 30, the PBCAR shows 86.89% of 
accuracy and 84.12% of recall, and the PBDT shows 
87.52% and 85.59 of them, respectively. As we can 
observe, the accuracy and the recall of the PBDT are 
slightly better than those of the PBCAR. This result is 
contrary to what we expected. In general, the 
rule-based classifier, like the CBA, can take more 
information into consideration than the 
decision-tree-based classifier and perform better on 
doing classification. This concept is also described 
clearly in [29]. Tree architecture and greedy algorithm 
makes decision tree- based classifier lose some 
information. In the simulated experiments, we found 
that the number of rules resulted by the PBCAR are 
really more than the ones for the PBDT. Hence this 
condition may be caused by the applied dataset. Figure 
31 shows a classifier of the PBDT, which is in tree 
architecture. The tree shown in Figure 31 is skewed 
meaning some features keep very high weights for 
classification and that additional information would 
cause more classification errors. 
 
 
 
Analysis of trees generated by PBDT  
By studying the results of the PBDT presented in 
Figure 31, we observe two major conditions. Firstly, 
the accuracy of two environmental datasets is larger 
than 60%, but the recalls for high-risk are very low. 
This situation indicates both environmental datasets 
are not particularly related to asthma attacks but can 
still provide some features about asthma attack 
prediction. Secondly, the result of asthma allergic 
dataset is better than the result of the integrated one. In 
a generalized case, the classifier considers more 
related information and can perform better, but this 
condition shows some varying results. We checked the 
detail of the classifier to find reasons for this 
occurrence. Figure 32 shows the classifier of the 
asthma allergic dataset and the classifier of the 
integrated one. We endeavored to compare the 
difference between both classifiers. There are just few 
differences in their sub-trees and they are located near 
the leaves. According to this evidence, we can infer 
that environmental factors contain some information 
for asthma attack prediction, but their effect is too 
insignificant to make great improvement in 
performance. With regard to recall, it is because the 
●Inhaled maintenance medicine
and bronchodilator
●Inhaled maintenance medicine
and oral maintenance day and night
◎ Inhaled maintenance medicine
and bronchodilator
Humidity: pattern (up, down)
Difference in temperature: 3°,2°, 5°
●
◎ Take emergency oral medicine four times 
a day immediately, until no wheeze.
Difference in temperature: 3°,1°
Difference in temperature: 1°,8°
Difference in temperature: 8°,3°
○ no need medicine, ○ inhaled maintenance
medicine or oral maintenance medicine day
and night, ○ inhaled maintenance medicine and
oral maintenance medicine day and night
○ inhaled maintenance medicine or
oral maintenance medicine day and
night, ○ inhaled maintenance medicine and
oral maintenance medicine day and night
●inhaled maintenance medicine or oral
maintenance medicine day and night
Highest temperature: down
●no need medicine
H
H
H
H
H
H
H
H
H
H
H
H
H
N
N
N  
Figure 31. Conducted rules of the PBD. 
Accuracy of Outside Testing
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Air Pollution Weather Asthma
Allergic
Integrated
PBCAR
PBDT
Recall of Outside Testing
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Air Pollution Weather Asthma
Allergic
Integrated
PBCAR
PBDT
 
Figure 30. Experimental results of both proposed methods 
on different datasets. 
 18
Science Publishers. pp. 49–61. 
[15] Kudenko, D., & Hirsh, H. (1998). Feature generation 
for sequence categorization. In Proceedings of the 
15th national/10th conference on artificial 
intelligence/innovative applications of artificial 
intelligence (AAAI/IAAI’ 98) (pp. 733– 738). 
Menlo Park, California, USA. 
[16] Lendasse, A., Verleysen, M., de Bodt, E., Cottrell, M., 
& Grgoire, Ph. (1998). Forecasting time-series by 
Kohonen classification. In Proceedings of the 
European symposium on artificial neural networks 
(ESANN’98) (pp. 221–226). D Facto Brussels. 
[17] Xi, X., Keogh, E., Shelton, C., Wei, L., & 
Ratanamahatana, Chotirat Ann (2006). Fast time 
series classification using numerosity reduction. In 
Proceedings of the 23rd international conference on 
machine learning (ICML’06), New York, USA. 
[18] Liu, B., Ma, Y., Wong, C. K., & Yu, P. S. (2003). 
Scoring the data using association rules. Applied 
Intelligence, 18(2), 119–135. 
[19] Lesh, N., Zaki, M. J., & Ogihara, M. (1999). Mining 
features for sequence classification. In Proceedings of 
the 5th ACM SIGKDD international conference on 
knowledge discovery and data mining (pp. 342–246). 
San Diego, California, USA. 
[20] Frank, E., Hall, M. A., Holmes, Geoffrey, Kirkby, 
Richard, Pfahringer, Bernhard, & Ian, H. (2005). 
Weka –  a machine learning workbench for data 
mining. The data mining and knowledge discovery 
handbook. Springer. pp. 1305–1314. 
[21] PhysioNet (2001). Challenge. 
<http://www.physionet.org/challenge/2001>. 
[22] W. H. Au and K.C.C. Chan, “Mining fuzzy rules for 
time series classification,” The 2004 IEEE 
International Conference on Fuzzy Systems, Vol. 1, 
pp. 239-244, 2004. 
[23] R. Agrawal and R. Srikant, “Fast algorithm for 
mining association rules,” The International 
Conference on Very Large Databases, pp. 487-499, 
1994. 
[24] R. Bellman, "On the approximation of curves by line 
segments using dynamic programming," 
Communications of the ACM, Vol. 4, No. 6, pp. 284, 
1961. 
[25] F. L. Chung, T. C. Fu, R. Luk and V. Ng, 
“Evolutionary time series segmentation for stock data 
mining,” The IEEE International Conference on Data 
Mining, pp. 83 - 90, 2002. 
[26] F. L. Chung, T. C. Fu, V. Ng and R. W. P. Luk, “An 
evolutionary approach to pattern-based time series 
segmentation,” IEEE Transactions on Evolutionary 
Computation, Vol. 8, No. 5, pp. 471- 489, 2004. 
[27] K. Chan, A. W. Fu, “Efficient Time Series Matching 
by Wavelets,” The IEEE International Conference on 
Data Engineering, pp 126-133, 1999. 
[28] C. H. Chen, T. P. Hong and Vincent S. Tseng, 
"Mining fuzzy frequent trends from time series", 
Accepted and to appear in Expert Systems with 
Applications. 
[29] G. Das, K. Lin, H. Mannila, G. Renganathan and P. 
Smyth, ”Rule discovery from time series,” The 
Fourth International Conference on Knowledge 
Discovery and Data Mining, pp. 16-22, 1998. 
[30] T. C. Fu, F. L. Chung, V. Ng and R. Luk, 
“Evolutionary segmentation of financial time series 
into subsequences,” The Congress on Evolutionary 
Computation, Vol. 1, pp. 426 - 430, 2001. 
[31] C. Faloutsos, M. Ranganathan, Y. Manolopoulos, 
“Fast subsequence matching in time-series 
databases,” In proceedings of the ACM SIGMOD 
International Conference on Management of Data, 
May 24-27, 1994, Minneapolis, MN. pp 419-429. 
[32] P. Geurts, “Pattern extraction for time series 
classification,” In proceedings of the 5th European 
Conference on Principles of Data Mining and 
Knowledge Discovery, Sep 3-7, 2001, Freiburg, 
Germany. pp. 115-127. 
[33] V. Guralnik and J. Srivastava, "Event detection from 
time series data," Proceedings of the fifth ACM 
SIGKDD international conference on Knowledge 
discovery and data mining, pp. 33-42, 1999. 
[34] P. Geurts, “Pattern extraction for time series 
classification,” The Fifth European Conference on 
Principles of Data Mining and Knowledge Discovery, 
pp. 115-127, 2001. 
[35] S. Hettich and S. D. Bay, The UCI KDD Archive, 
Department of Information and Computer Science, 
University of California, Irvine, CA, 1999. 
[36] T. P. Hong, C. S. Kuo and S. C. Chi, "Mining 
association rules from quantitative data", Intelligent 
Data Analysis, Vol. 3, No. 5, pp. 363-376, 1999. 
[37] J. Himberg, K. Korpiaho, H. Mannila,J. Tikanmaki 
and H.T.T. Toivonen, "Time series segmentation for 
context recognition in mobile devices," The IEEE 
International Conference on Data Mining, pp. 
203-210, 2001. 
[38] K. Kalpakis, D. Gada, V. Puttagunta, ”Distance 
measures for effective clustering of ARIMA 
time-series,” In proceedings of the 2001 IEEE 
International Conference on Data Mining, San Jose, 
CA, Nov 29-Dec 2, 2001. pp 273-280. 
[39] E. Keogh, , K. Chakrabarti, , M. Pazzani, S. Mehrotra, 
“Locally adaptive dimensionality reduction for 
indexing large time series databases,” In proceedings 
of ACM SIGMOD Conference on Management of 
Data, Santa Barbara, CA, May 21-24, 2001. pp 
151-162. 
[40] E. Keogh, S. Kasetty, “On the need for time series 
data mining benchmarks: a survey and empirical 
demonstration,” In proceedings of the 8th ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, July 23 - 26, 2002. 
Edmonton, Alberta, Canada. pp 102-111. 
[41] E. Keogh, M. Pazzani, “An enhanced representation 
of time series which allows fast and accurate 
classification, clustering and relevance feedback,” In 
proceedings of the 4th Int'l Conference on 
Knowledge Discovery and Data Mining, New York, 
NY, Aug 27-31, 1998. pp 239-241. 
[42] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A 
Symbolic Representation of Time Series, with 
Implications for Streaming Algorithms. In 
proceedings of the 8th ACM SIGMOD Workshop on 
Research Issues in Data Mining and Knowledge 
Discovery, San Diego, CA. June 13, 2003. 
[43] J. Lin, M. Vlachos, E. Keogh and D. Gunopulos, 
“Iterative incremental clustering of time series,” The 
IX Conference on Extending Database Technology, 
pp. 106-122, 2004. 
[44] Vincent S. Tseng, C. H. Chen, C. H. Chen and T. P. 
Hong, “Segmentation of time series by the clustering 
and genetic algorithms,” The Workshop on 
Foundation of Data Mining and Novel Techniques in 
High Dimensional Structural and Unstructured Data 
in The Fifth IEEE International Conference on Data 
Mining, pp. 443-447, 2006. 
 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          99 年 4 月 14 日 
報告人姓名  曾新穆 
 
服務機構 
及職稱 
國立成功大學 
資訊工程系 
教授 
     時間 
會議 
     地點 
99 年 3 月 22 日至 3月 26 日 
Sierre, Switzerland 
本會核定 
補助文號 
 
NSC 96-2221-E-006-143-MY3 
會議 
名稱 
 (中文) 第 25屆 ACM 應用計算會議 
 (英文) The 25th ACM Symposium on Applied Computing 
發表 
論文 
題目 
A Novel Music Recommender by Discovering Preferable Perceptual-Patterns 
from Music Pieces 
 
報告內容應包括下列各項： 
一、參加會議經過 
 
 
二、與會心得 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
四、建議 
 
 
五、攜回資料名稱及內容 
 
 
六、其他 
 
 
 
 
 
 
 3
 攜回之資料有一、會議論文集(電子檔型式)，二、應用計算國際會議及學會活動資訊，
將可供國內學者參考。 
96 年度專題研究計畫研究成果彙整表 
計畫主持人：曾新穆 計畫編號：96-2221-E-006-143-MY3 
計畫名稱：高效率時序資料探勘技術之研發及應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
