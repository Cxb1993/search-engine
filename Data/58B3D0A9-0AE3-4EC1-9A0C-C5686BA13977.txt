image matching algorithm in Section 2. Then, the pro-
posed image retrieval algorithm is described in Section 3.
The performance evaluation of our approach is reported
in Section 4. Finally, some concluding remark is given in
Section 5.
2. The SIFT algorithm
Lowe proposed a SIFT (scale invariant fea-
tures transform) method to extract and describe key-
points which are robustly invariant to common image
transforms[5]. These distinctive image features are ap-
propriate to applied to image matching rapidly and ac-
curately in mass image database. The SIFT algorithm
consists of four major steps:
1. Scale space extrema detection
The basic idea is to find image regions that are
salient not only spatially but also across different
scales. Candidate keypoints are initially detected
from local extrema in Difference of Gaussian (DOG)
filtered images in scale space. Each point in the
three dimensional DOG scale space is compared
with its eight spatial neighbors at the same scale,
and with its nine neighbors at adjacent higher and
lower scales. If a point is a local maximum or mini-
mum, it is selected as a candidate keypoint.
2. Keypoint localization
In order to achieve sub-pixel accuracy, the keypoint
position is slightly correctly by a quadratic inter-
polation. Meanwhile, to improve the stability and
reduce the sensitivity to noise, those keypoints that
have low contrast and are poorly localized along an
edge will be rejected.
3. Orientation assignment
To determine the keypoint orientation, a gradient
orientation histogram is computed in the neighbor-
hood of the keypoint. The contribution of each
neighboring pixel is weighted by the gradient magni-
tude and a Gaussian window with variance σ that is
1.5 times the scale of the keypoint. Peaks in the his-
togram correspond to dominant orientations. Addi-
tional keypoints are generated for keypoints with
multiple local dominant peaks whose magnitude is
within 80% of the main.
4. Keypoint descriptor
A feature descriptor is then constructed from an im-
age region (16×16 pixels) centered at each keypoint.
The feature descriptor consists of histograms of gra-
dient orientation computed from 4× 4 sub-regions.
The keypoint orientation is used to aligned the gra-
dient orientation to make this descriptor rotation in-
variant. As the gradient orientations are quantized
into 8 bins, the final feature vector has dimension
128 (4× 4× 8).
After the SIFT feature vectors are extracted, key-
points of two images are matched. Euclidean distance be-
tween two feature vectors (keypoint descriptors) is calcu-
lated. In the classic method, K-D tree algorithm is used
to search each keypoint’s nearest 2 neighbors in the sec-
ond image[15]. If the ratio of distance between the closest
pair and distance between the second closest pair is less
than a threshold, the closest pair is accepted as match-
ing pair. The number of matching keypoints is used as
a similarity measure of two images. In some work[9], ge-
ometry verification method called RANSAC[16] is used
to eliminate the spurious matches.
3. The proposed algorithm
The keypoints can capture the gross structure of ob-
ject shape, but they can not describe the minute struc-
ture of shape. On the other hand, edges can capture lo-
cal details of shape but are difficult to put together in a
global context. Therefore, in the proposed algorithm, the
keypoint and edge informations are integrated to com-
plement each other. The degree of matching between
two keypoints is determined by the edge information in
the neighborhood of each keypoint. Two keypoints are
matched if in the neighborhood of each keypoint
• The edge orientation histograms are similar.
• The spatial distribution of edges are similar.
Thus, the proposed algorithm is made up of five main
steps:
1. Keypoint detection
2. Edge detection
3. Edge orientation histogram comparison
4. Edge based template matching
5. Similarity measure computation
The location and orientation of keypoints are calculated
by step 1-3 of the SIFT algorithm. The edge points are
detected by the Canny edge detector[17]. Step 3-5 of the
proposed algorithm are described in the following sub-
sections.
3.1 Edge orientation histogram comparison
In the neighborhood ( 5×5 pixels) of each keypoint,
the edges orientation are recorded by a histogram. The
orientation is quantized into 8 bins. The histogram of
edge orientation is invariant to translations of an image.
The location of an object of an image have no effect on
the edge orientation. However, the histogram is not in-
variant to rotation. To achieve rotation invariance, the
2
Table 1: Performance comparison in term of preci-
sion
Query image SIFT algorithm Our algorithm
car 0.2 0.7
motorcycle 0.3 0.6
bicycle 0.4 0.6
shoe 0.5 0.7
glove 0.5 0.8
images that are similar to the query image, a robust im-
age retrieval algorithm should extract accurate shape in-
formation for discrimination. As the SIFT descriptor is
based on the image statistics of all the pixels in the win-
dow around the keypoint, this may be redundant and
even more sensitive to noise. Our algorithm uses only
the edge information in the same window to capture the
shape of object more accurately. Our approach is invari-
ant to translation, rotation and partial occlusion. Exper-
imental results show that the proposed approach works
reasonably well in retrieving the relevant images. Com-
pared with the well known SIFT algorithm[5], our ap-
proach is promising.
References
[1] C. Harris and M. Stephens. A combined corner and
edge detector. In Proceedings of Alvey Vision Con-
ference, pages 147–151, 1988.
[2] R. Horaud, T. Skordas, and F. Veillon. Finding ge-
ometric and relations structures in an image. In
Proceedings of European Conference on Computer
Vision, pages 374–384, 1990.
[3] W. Forstner. A framework for low level feature ex-
traction. In Proceedings of European Conference on
Computer Vision, pages 383–394, 1994.
[4] K. Mikolajczyk and C. Schmid. Scale and affine in-
variant interest point detectors. International Jour-
nal of Computer Vision, 60(1):63–86, 2004.
[5] D.G. Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Com-
puter Vision, 60(2):91–110, 2004.
[6] M. Heikkila, M. Pietkainen, and C. Schmid. De-
scription of interest regions with local binary pat-
tern. Pattern Recognition, 42(3):425–436, 2009.
[7] K. Mikolajczyk and C. Schmid. A performance
evaluation of local descriptors. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
27(10):1615–1630, 2005.
[8] X. Hu, Y. Tang, and Z. Zhang. Video object match-
ing based on SIFT algorithm. In Proceedings of In-
ternational Conference on Neural Networks and Sig-
nal Processing, pages 412–413, June 2008.
[9] W. Xu et al. Application of image SIFT features to
the context of CBIR. In Proceedings of International
Conference on Computer Science and Software En-
gineering, pages 552–555, December 2008.
[10] H. Huan et al. Butterfly image retrieval based on
SIFT feature analysis. In Proceedings of SPIE Con-
ference on Image Processing and Photonics for Agri-
cultural Engineering, 2009.
[11] L. Zhi et al. Medical image retrieval using SIFT
feature. In Proceedings of International Congress
on Image and Signal Processing, pages 1–4, October
2009.
[12] X. Liu, Z. Shao, and J. Liu. Ontology-based im-
age retrieval with SIFT features. In Proceedings of
International Conference on Pervasive Computing,
Signal Processing and Applications, pages 464–467,
September 2010.
[13] M.S. Sunder and A. Ross. Iris image retrieval
based on macro-features. In Proceedings of Inter-
national Conference on Pattern Recognition, pages
1318–1321, August 2010.
[14] T. Pavlidis. An evaluation of the SIFT.
http://www.theopavlidis.com/technology/CBIR
/PaperE/AnSIFT1.htm, 2008.
[15] B. Jeff and D.G. Lowe. Shape indexing us-
ing approximate nearest-neighbor search in high-
dimensional spaces. In Proceedings of IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1000–1006, June 1997.
[16] M.A. Fischler and R.C. Bolles. Random sample con-
sensus: a paradigm for model fitting with applica-
tions to image analysis and automated cartography.
Communication of ACM, 24(6):381–395, 1981.
[17] J. Canny. A computational approach to edge detec-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 8(6):679–698, November 1986.
4
 6
    
   
  
( a ) 
 
     
   
   
( b ) 
 
Figure 3: Image retrieval results by (a) SIFT algorithm (b) our algorithm. 
 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                             100 年    8 月     
報告人姓名 王棣雲 服務機構 
及職稱 
輔仁大學資訊工程研究所碩士生 
     時間 
會議 
     地點 
2011 年 8 月 17 日至 19 日 
新加坡 
本會核定 
補助文號 
專題研究計畫 
NSC99-2221-E-030-017 
會議 
名稱 
 (中文) 第 8屆國際電腦繪圖影像與視覺化會議 
 (英文) 8th International Conference on Computer Graphics, Imaging and Visualization
發表 
論文 
題目 
 (中文) 偵測電影中的暴力現象 
 (英文)  Violence Detection in Movies 
前言: 
為了將 98 年度國科會研究計畫”偵測電影中的暴力場景”的研究成果發表，在網路上的徵求論文通告
中，我們發現在新加坡的第 8 屆國際電腦繪圖影像與視覺化會議有相關論文主題。於是便撰文投稿。此
會議是由國際著名學術組織 IEEE 協辦，以審稿嚴謹而吸引各國學者紛紛投稿。由於計畫主持人陳良華教
授有其他要務不克參加,於是由也是論文作者之一的王棣雲前往發表論文. 
一、 參加會議經過 
我於 8月 16 日早上搭乘中華航空客機由台北直飛法新加坡。在旅館休息一晚之後，便到會議所在地-南
洋理工學院的會議廳，辦理報到手續，領取名牌及會議論文集。在開幕典禮中，大會主席首先致歡迎辭，
並介紹整個會議的流程。接下來就是 keynote talk 及 invited talk，由大會邀請各個領域的專家就最新
的研究主題做概念性的介紹，內容均十分精闢。在接下來的論文發表中，我選擇了下面幾項主題(場次)
聆聽演講: Image & Video Analysis, Multimedia 和 Visualization。在各場次之間，則有 30 分鐘的休
息時間，由大會提供飲料及點心使與會者可以盡情交誼及交換意見。我的論文則被安排在 Imaging 這個
場次發表，聽眾雖不多卻是有備而來，所問的問題均十分深入。整個大會的高潮則發生在 17 日的晚會上，
主辦單位在學校餐廳以新加坡美食招待大家，席間並穿插歌舞表演;大家一邊品嘗美食，一邊聊天，氣氛
十分愉快。最後值得一提的是，在這次會議中，我遇見數位大師級人物，包括 Ebad Banissi 及 Paul Bourke
等。這些人對台灣去的同行皆十分友善，對我們所提的問題也耐心及詳細的回答，使人有不虛此行的感
覺。 
 
二、與會心得 
從這次會議可以大略看到當今影像與視覺化技術的發展趨勢--朝向更生活化，更應用化的課題發展。此
外，我亦觀察到很多熱門的研究主題，國內甚少有人觸及。平心而論，雖然台灣發表的論文數量不少，
但質方面仍稍稍遜於歐美各國。就自我而言，經由和一些高手的討論，我發現到了自己研究上的盲點，
謝謝他們的指導，使我受益良多。 
 
三、建議 
雖然國際上的學術會議種類很多且良莠不齊，但知名度高及具有權威性的會議就是固定那幾個。因此希
望今後有關單位能夠多鼓勵及補助國內的專家學者參加此類高水準的學術會議，因為從會議中我們不但
可以學到不少東西，更可以展現自己的研究成果以提昇國家的學術形象與水準。 
 
四、攜回資料的名稱及內容 
The 8th International Conference on Computer Graphics, Imaging and Visualization 論文集 (電子檔)。 
 
 
 
 
Violence Detection in Movies
Liang-Hua Chen, Hsi-Wen Hsu Chih-Wen Su
and Li-Yun Wang
Department of Computer Science Departmnet of Information and
and Information Engineering Computer Engineering
Fu Jen University, Taipei, Taiwan Chung Yuan University, Chung-Li, Taiwan
Abstract
As violence in movies has harmful influence on chil-
dren, in this paper, we propose an algorithm to detect vio-
lent scene in movies. Under our definition of violence, the
task of violent scene detection is decomposed into action
scene detection and bloody frame detection. While pre-
vious approaches addressed on shot level of video struc-
ture only, our approach works on more semantic-complete
scene structure of video. The input video (digital movie)
is first segmented into several scenes. Based on the film-
making characteristics of action scene, some features of
the scene are extracted to feed into the support vector ma-
chine for classification. Finally, the face, blood and motion
information are integrated to determine whether the action
scene has violent content. Experimental results show that
the proposed approach works reasonably well in detecting
most of the violent scenes. Compared with related work,
our approach is computationally simple yet effective.
1 Introduction
The advances in low cost mass storage devices, higher
transmission rates and improved compression techniques,
have led to the widespread use and availability of digital
video. Nowadays, everyone can download movies easily
using home computer. However, violence in movies has
harmful influence on children. It was reported that children
who liked to watch violent TV programs when they were
8 years old were more likely to behave aggressively at age
18[1]. To prevent children from watching violent movies,
the automatic detection of inappropriate violence in movies
is of substantial importance. For content provider, the vi-
olence detection technique can be used to assist in movie-
rating; for end user, it can block the violent content in client
terminal devices. On the other hand, violent scenes attract
attention and make viewers curious. They are usually the
highlights of a movie. Therefore, violence detection would
also be useful for movie skimming.
In this paper, we propose an empirically motivated ap-
proach for violence detection in movies. The task of vio-
lent scene detection is decomposed into action scene detec-
tion and bloody frame detection. Our approach is based on
the integration of visual characteristics and temporal dy-
namics information of video. The rest of this paper is or-
ganized as follows. In the next section, we review some re-
lated works and give the motivation for our approach. An
action scene detection algorithm is presented in Section 3.
In Section 4, we describe how to integrate several visual
features to detect violent content. The performance eval-
uation of our approach is reported in Section 5. Finally,
some concluding remarks are given in Section 6.
2 Background and Motivation
Relatively few approaches have been proposed to the
problem of violent scene detection in video. The main rea-
son is that the definition of violence is ambiguous. It is dif-
ficult to describe this high-level concept using mathemati-
cal formulation precisely. Each related work addressed the
problem by its own definition of violence. Depending on
the type of video features, current techniques for violence
detection can be broadly classified into three categories.
The first one is based on visual cue. Using motion trajec-
tory information and orientation information of a person’s
limbs, Datta et al. addressed the problem of detecting hu-
man violence in video such as fist fighting and kicking[2].
Their approach relies on the extraction of silhouette of each
person from the image. Thus it works well only in pres-
ence of two persons. Mecocci and Micheli proposed to
use maximum warping energy as criterion to detect violent
acts among more people in crowded environments[3]. But,
it is still difficult to differentiate fighting from basketball
playing using this approach. It is also noted that both ap-
proaches ([2, 3]) use video data from surveillance cameras
and are not suitable for movies which have large camera
movement. The second category is the audio based ap-
proach. Giannakopoulos et al. used eight audio features,
both from the time and frequency domain, as input to a bi-
nary classifier which decides the video content with respect
to violence[4]. Then, they extended thir work to multi-
2011 Eighth International Conference Computer Graphics, Imaging and Visualization
978-0-7695-4484-7/11 $26.00 © 2011 IEEE
DOI 10.1109/CGIV.2011.14
119
∙ Fast edits are frequently used to build a sense of ki-
netic action and speed.
∙ Two action scenes with high film rhythm may not be
juxtaposed together.
Using these guideline, the director and editor control the
pace of a movie to grasp the attention of the viewers. Thus,
most of the action scenes consist of a consecutive sequence
of short shots with high motion activity. This type of video
sequences will provide a lot of rapidly changing visual in-
formation displayed on screen to excite the viewers. Based
on these action scene characteristics, the following features
are extracted.
(1) Average Motion Intensity:
Motion is a visual feature which is essential to capture tem-
poral variation of video. It also reveals the correlations be-
tween frame sequences within a video scene. To character-
ize the degree of motion within a scene, the average motion
intensity is computed based on the motion vectors encoded
in the MPEG-1 video stream[16]. In MPEG video, each
frame is partitioned into blocks of size 16×16 pixels called
macro blocks (MBs). MPEG defines motion vector as the
displacement from the Target (current frame) MB to the
Prediction (reference frame) MB. In MPEG format, there
are three types of frames: I, P and B frames. I frames are
skipped because they are intra-coded and no motion infor-
mation is available. P frames have forward motion predic-
tion and B frames have both forward and backward motion
prediction. In our system, only the forward motion vectors
encoded in P frames are extracted. For a given P frame, the
motion intensity matrix is defined as
𝑀(𝑖, 𝑗) =
√
𝑢2𝑖,𝑗 + 𝑣
2
𝑖,𝑗
where (𝑢𝑖,𝑗 , 𝑣𝑖,𝑗) is the motion vector associated with
(𝑖, 𝑗)th macroblock. Assuming there are 𝑚 × 𝑛 mac-
roblocks in the frame, then the average motion intensity
of the frame is
𝑀 =
1
𝑚𝑛
𝑚∑
𝑖=1
𝑛∑
𝑗=1
𝑀(𝑖, 𝑗)
Then, the average value of 𝑀 over all frames within a
scene is obtained. Finally, this value is normalized to be
in the interval [0, 1].
(2) Camera Motion Ratio:
If a frame has less than 10% motion vectors to be zero,
then this frame has camera motion. Assuming a scene 𝑆
consists of 𝑚 frames, and 𝑘 frames have camera motion.
The camera motion ratio is defined as
𝐶 =
𝑘
𝑚
(3) Average Shot Length:
Assuming a scene 𝑆 consists of 𝑛 shots, and their corre-
sponding shot length is 𝐿𝑖, 𝑖 = 1, ⋅ ⋅ ⋅ , 𝑛. The average shot
length of 𝑆 is defined as
𝐿 =
1
𝑛
𝑛∑
𝑖=1
𝐿𝑖
Likewise, 𝐿 is normalized to be in the interval [0, 1].
(4) Shot Cut Frequency:
Assuming a scene 𝑆 consists of 𝑛 shots, then the shot cut
frequency of 𝑆 is defined as
𝐹 =
1
𝑛
Thus, a 4-dimensional feature vector for classification
is constructed.
4 Bloody Frame Detection
For each detected action scene, we further check
whether it contains the bloody frame. As scene is com-
posed of several shots, key frames can be extracted from
each shot. Key frame is the frame which can represent the
salient content of the shot. Currently we choose the mid-
dle frame of each shot as key frame. Then, we determine
whether one of these key frames has bloody content. We
first identify the existence of human and blood in the key
frame.
Face detection is the natural and convenient way to de-
termine whether human appear in the frame image. We
adopt the excellent face detection algorithm proposed by
Viola and Jones[17]. This algorithm is capable of process-
ing images extremely rapidly while achieving high detec-
tion rates. Figure 1 shows some face detection results.
There are some sophisticated techniques to detect spe-
cific color object such as flame and skin. However, for
the blood pixel detection task, the easiest method is to de-
fine blood-color cluster decision boundaries for RGB color
space. Ranges of threshold values for each color space
component are defined and the image pixel with values
that fall within these predefined ranges is detected as blood
pixel. According to our observation, there are two types of
blood color: bright red and dark red. The ranges of RGB
color space are defined as (1) 170 > 𝑅 > 80 & 𝐺 < 5
& 𝐵 < 5 (2) 200 > 𝑅 > 120 & 𝐺 < 90 & 𝐵 < 90 &
𝑎𝑏𝑠(𝐺 − 𝐵) < 8, respectively. Figure 2 shows the blood
detection result.
Then, a connected components analysis is performed
to group these detected blood pixels into several poten-
tial blood regions. A size filter is used to eliminate some
potential blood regions which are too small or too large.
Small region may result from noise and large region may
121
Table 1: Accuracy measures for four test movies.
Movie Number of Correct Missed False
ID No. Violent Scenes Detection Detection Detection
(1) 17 14 3 0
(2) 10 8 2 0
(3) 18 17 1 0
(4) 12 10 2 0
Table 2: Performance comparison for violent scene detection.
Movie Our Approach Lin’s Approach
ID No. Recall Precision Recall Precision
(1) 82.35% 100% 82.35% 82.35%
(2) 80.00% 100% 70.00% 87.50%
(3) 94.44% 100% 88.89% 80.00%
(4) 83.33% 100% 75.00% 81.82%
[8] J. Lin and W. Wang. Weakly-supervised violence
detection in movies with audio and video based co-
training. In Proceedings of the 10th Pacific Rim Con-
ference on Multimedia, pages 930–935, Bangkok,
Thailand, December 2009.
[9] T. Giannakopoulos et al. Audio-visual fusion for de-
tecting violent scenes in videos. In Proceedings of
the 6th Hellenic Conference on Artificial Intelligence,
pages 91–100, Athens, Greece, May 2010.
[10] L.-H. Chen, Y.-C. Lai, and H.-Y. liao. Movie scene
segmentation using background information. Pattern
Recognition, 41(3):1056–1065, March 2008.
[11] L.-H. Chen, C.-W. Su, C.-F. Weng, and H.-Y. Liao.
SVM based action scene detection. In Poster Pro-
ceedings of the 6th International Conference on
Machine Learning and Data Mining, pages 45–50,
Leipzig, Germany, July 2009.
[12] T. Evgenious, M. Pontil, and T. Poggio. A unified
framework for regularization networks and support
vector machines. A.I. Memo 1654, MIT, Cambridge,
MA, 1999.
[13] V.N. Vapnik. Statistical Learning Theory. Wiley,
New York, 1998.
[14] C.W. Hsu, C.C. Chang, and C.J. Lin. A practical
guide to support vector classification. Technical re-
port, Department of Computer Science, National Tai-
wan University, Taipei,Taiwan, 2003.
[15] D. Arijon. Grammar of the Film Language. Silman-
James Press, Los Angels, 1991.
[16] D.L. Gall. MPEG: A video compression standard for
multimedia applications. Communication of ACM,
34(4):46–58, April 1991.
[17] P. Viola and M.J. Jones. Robust real-time face de-
tection. International Journal of Computer Vision,
57(2):137–154, May 2004.
123
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/07
國科會補助計畫
計畫名稱: 一個整合式的物件檢索方法
計畫主持人: 陳良華
計畫編號: 99-2221-E-030-017- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
