 2
 
符號區間資料含有離異點的建模技術之研究 
 
A study on the modeling techniques for symbolic interval data with outliers 
計畫編號：NSC 97－2221－E －197 －014－ 
執行期限：97 年 8 月 1 日 至 98 年 7 月 31 日 
主持人：莊鎮嘉          職稱：副教授 
電子信箱：ccchuang@niu.edu.tw 
執行機構：宜蘭大學電機系 
 
一、中文摘要 
本計畫分為一年期計劃。在此計劃中，我們
對文獻上有關符號區間資料的回歸模型加以研究
分析。然而，在許多應用上，我們所取的資料也
許會包含離異點。目前，有關符號區間資料的回
歸模型都沒有探討離異點的影響。在此同時，我
們主要是探討離異點對符號區間資料的回歸模型
的影響與分析。對於符號區間資料的離異點，在
此計劃中，我將定義出為兩種不同型態的離異點
－位置偏移的離異點與符號區間資料之極端點的
偏移。根據這兩種離異點的組態，我將利用強健
式學習法則或與強建式回歸或其他強健式統計理
論來分析其效能，並推導相關演算法和提出一種
可行性之分析。最後也將探討引入至 TSK 模糊
系統之可能性。 
 
關鍵詞：符號區間資料、離異點資料、強健式線
性回歸。 
 
英文摘要 
For the scientific and engineering 
applications, the obtained training data are always 
subject to outliers. The intuitive definition of an 
outlier is “an observation which deviates so much 
from other observations as to arouse suspicions 
that it was generated by a different mechanism.” 
However, outliers may occur due to various 
reasons, such as erroneous measurements or noisy 
data from the tail of noise distribution functions. 
Moreover, the outlier’s effects are not considered 
in the modeling approaches for the symbolic 
interval data. In the project, some of regression 
approaches for symbolic interval data in the 
literature are discussed and analyzed. Additionally, 
the outlier’s effects in the modeling approaches for 
the symbolic interval data are discussed and 
analyzed. In this project, the definition of outliers 
in the symbolic interval data is re-defined. One is 
the location of symbolic interval data are deviated 
from its desired location. This situation is similar 
to the definition of outliers in the neural networks 
community and introduced as following section. 
Another is the extreme values (i.e. minimum 
values and maximum values) of symbolic interval 
data are disturbed. This situation is also introduced 
as following section. In order to overcome the 
outliers in the symbolic interval data, some of 
robust methods in the neural networks community 
and in the robust statistical theory are reviewed. 
Then, a novel method is proposed to overcome the 
outliers in the symbolic interval data. Finally, the 
above (proposed) approach is applied to the TSK 
fuzzy system. 
 
Keyword: Symbolic Interval-Values Data, Outliers, 
Robust Linear Regression. 
 
二、緣由與目的 
In the real word applications, the symbolic 
interval data (i.e. interval input-output data) are 
often existed [1, 2]. An example is the 
 3
For the scientific and engineering 
applications, the obtained training data are 
always subject to outliers. The intuitive 
definition of an outlier is “an observation 
which deviates so much from other 
observations as to arouse suspicions that it 
was generated by a different mechanism.” 
However, outliers may occur due to various 
reasons, such as erroneous measurements or 
noisy data from the tail of noise distribution 
functions. When the outliers are exists, there 
still exist some problems in the traditional 
neural networks approaches. Hence, the robust 
learning approaches are proposed to overcome 
the problems of traditional neural network 
approaches while facing with outliers [25-31]. 
As a pioneer in pursuit of robust learning for 
neural networks, Chen and Jain [25] adopted 
the theory of the M-estimator and took the 
general shape of Hampel’s hyperbolic tangent 
(tanh) estimator in the cost function to degrade 
the effects of outliers. The basic idea of 
M-estimators is to replace the quadratic error 
term ( 2L  norm) in the cost function by loss 
functions [32-34] such that the effects of those 
outliers may be degraded. However, this 
approach suffers from the problem of not 
capable of discriminating outliers from data 
[31]. In fact, this problem also occurs for 
nonlinear regression approaches in the 
statistics theory [36] and is referred to as the 
initialization problem. Chuang, et al. [34] 
proposed the Annealing Robust 
Back-Propagation (ARBP) learning algorithm 
that adopts the annealing concept into robust 
learning algorithms to deal with this problem. 
However, the outlier’s effects are not 
considered in the above analysis method for the 
symbolic interval data. It is note that the outliers 
in the symbolic interval data can be divided into 
two categories. One is the location of interval 
output that departure from the majority of the 
symbolic interval data. Figure 1 shows the outliers 
in the symbolic interval data. This situation is 
same to the outlier definition in the neural 
networks community. Another is the extreme 
values (i.e. minimum value or maximum value) of 
symbolic interval data are deviated their desire 
location. Figure 2 shows this situation. In this 
project, the situations of two categories are 
considered and analyzed in this project. Moreover, 
the robust linear regression models with outliers 
for the symbolic interval data is proposed 
 
 
Figure 1: The outliers in the symbolic interval 
data are shown. 
 
 
Figure 2: The extreme values of symbolic interval 
data are deviated their desire location (i.e. 
outliers). 
 
本計畫的主要目的是 
 
在文獻上，各領域(分群、分類、線性回歸…
等)學者對於符號區間資料提出許多不同的做
法。然而，這些做法都沒有對離異點加以探討、
分析與解決。所以在本計畫中，我將探討與分
析含有離異點對於符號區間資料之線性回歸的
影響。在此同時，我將定義”符號區間資料之離
異點”，如具有位移特性之離異點與符號區間資
料之大小偏移。根據上述離異點的定義，我們
-0.5 0 0.5 1 1.5 2
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
interval(x)
in
te
rv
al
(y
)
-0.5 0 0.5 1 1.5 2
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
interval(x)
in
te
rv
al
(y
)
 5
 
( )   ˆxyˆ TLL β= and ( ) βˆxyˆ TUU = ,        (5) 
 
where ( ) ( )TLx 1  p,...,a= and ( ) ( )TUx 1  p,...,b= . 
 
B. MinMax method 
 
The MinMax method (MinMax) suggests 
estimating the lower and upper bounds of the 
intervals by using different vectors of parameters. 
This is equivalent to supposing independence 
between the values of lower and upper bounds of 
the intervals. 
In this method, let us consider pXXX ,...,, 21  
related to Y according to the linear regression 
relationship: 
 
....
,...
110
110
Uiip
U
pi
UU
Ui
Liip
L
pi
LL
Li
aay
aay
εβββ
εβββ
++++=
++++=
         (6) 
 
Based on Eqs. (6), the sum of the squares of 
deviations in the MinMax method is defined by  
 
( ) ( )
( ) ( )
2 2
2
1 1
2 2
0 0
1 1
... ...
n n
Li Ui
i i
n n
L L U U
Li p ip Ui p ip
i i
S
y a y b
ε ε
β β β β
= =
= =
= +
= − − + − −
∑ ∑
∑ ∑
,                                     (7) 
 
which represents the sum of the lower bound 
square error plus the sum of upper bound square 
error, considering independent vectors of 
parameters to predict the bounds of the interval. 
It is possible to find the values of 
L
p
LL βββ ˆ,...,ˆ,ˆ 10  and UpUU βββ ˆ,...,ˆ,ˆ 10  that 
differentiating Eq. (7) with respect to the 
parameters and setting the results equal to zero. 
Thus, after some algebra, the normal equations are 
denoted by 
 
( )
( )
( )
( ) ,ˆˆˆ
,ˆˆˆ
,ˆˆˆ
,ˆˆˆ
,ˆˆˆ
,ˆˆˆ
11 1
2
1
1
0
1
1
1 1 1
1
2
110
11 1
10
11 1
2
1
1
0
1
11 1
1
2
1
1
10
111
1
1
1
1
1
10
ip
n
i
U
i
n
i
n
i
ip
U
pipi
U
n
i
ip
U
n
i
i
U
i
n
i
n
i
n
i
iip
U
pip
U
i
U
n
i
U
i
n
i
n
i
ip
U
pi
UU
ip
n
i
L
i
n
i
n
i
ip
L
pipi
L
n
i
ip
L
i
n
i
L
i
n
i
n
i
iip
L
pi
L
n
i
i
L
n
i
L
i
n
i
ip
L
p
n
i
i
LL
bybβbbβbβ
    
bybbβbβbβ
ybβbβnβ 
ayaβaaβaβ 
       
      
ayaaβaβaβ
yaβaβnβ
∑∑ ∑∑
∑∑ ∑ ∑
∑∑ ∑
∑∑ ∑∑
∑∑ ∑∑
∑∑∑
== ==
== = =
== =
== ==
== ==
===
=+++
=+++
=+++
=+++
=+++
=+++
"
#
"
"
"
#
"
"
                                     (8) 
 
The least square estimates of Lp
LL βββ ˆ,...,ˆ,ˆ 10  and 
U
p
UU βββ ˆ,...,ˆ,ˆ 10 , which minimize the Eq. (7), are 
the solution to the system of 2(p+1) normal 
equations that can be rewritten in matrix notation 
as  
 ( ) ( ) bAˆˆˆˆˆˆˆ 11010 -TUpUULpLL β,,β,β,β,,β,ββ == "" ,
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
=
∑∑
∑∑
∑
∑∑∑
∑∑∑
∑∑
==
==
=
===
===
n
i
ipip
n
i
ipi
n
i
ipi
n
i
i
n
i
ip
n
i
ipip
n
i
ipi
n
i
ip
n
i
ipi
n
i
ii
n
i
i
i ipi i
bbbb
bbb
bn
aaaaa
aaaaa
aan
11
1
1
1
1
1
1
11
1
1
1
1
1
11
1
1
1
000
000
000
00
00
00
A    
""
#######
""
""
""
#######
""
""
                                     (10) 
and 
1 1 1 1
b ( )
n n n n
L L U U T
i i ip i i ip
i i i i
y ,..., y a , y ,..., y b
= = = =
= ∑ ∑ ∑ ∑  
 
Given a new example e that described by 
),x(z y=  where ( )pxxx ,...,,x 21=  with 
 7
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
=
∑∑
∑∑
∑
∑∑∑
∑∑∑
∑∑
==
==
=
===
===
==
n
i
r
ip
r
ip
n
i
r
ip
r
i
n
i
r
ip
r
i
n
i
r
i
n
i
r
ip
n
i
c
ip
c
ip
n
i
c
ip
c
i
n
i
c
ip
n
i
c
ip
c
i
n
i
c
i
c
i
n
i
c
i
n
i
c
ip
n
i
c
i
xxxx
xxx
xn
xxxxx
xxxxx
xxn
A
11
1
1
1
1
1
1
11
1
1
1
1
1
11
1
1
11
1
0.00
000
000
00.
00
00.
……
#######
……
……
……
#######
……
……
and 
T
1 1 1 1
b ( ,..., , ,..., )
n n n n
c c c r r r
i i ip i i ip
i i i i
y y x y y x
= = = =
= ∑ ∑ ∑ ∑ .   (16) 
 
Then, given a new example e, described by ( )x,yz = , ( )cc ,yxw =  and ( )rr ,yxr = , where 
),...,,( 21 pxxxx =  with ],[ jjj bax = , 
),...,,( 21
c
p
ccc xxxx =  with 2/)( jjcj bax +=  and 
),...,,( 21
r
p
rrr xxxx =  with 2/)( jjrj bax −= . The 
value ],[ UL yyy =  of Y is predicted from the 
predicted values cyˆ  of cY  and ryˆ  of rY , as 
follows: 
 
rc
U
rc
L yyy  and   y-yy ˆˆˆˆˆˆ +== ,           (17) 
 
where cTcc β)x(y ˆ~ˆ = , rTrr β)x(y ˆ~ˆ = , 
11
c T p(x ) ( ,...,x )= , 11r T r(x ) ( ,...,x )= , 
0
ˆ ˆ ˆc c c T
pβ (β ,...,β )=  and 0 0ˆ ˆ ˆr r r Tβ ( β ,...,β )= . 
 
In order to overcome the outlier’s effects in the 
symbolic interval-values data, some of robust 
concepts and robust approaches are introduced.  
 
D. Robust Concepts in the Neural Networks 
 
Most function approximation problems are 
concerned with predictive learning from data. The 
goal is to approximate the dependencies of a given 
set of training data. However, for any real-world 
applications, to obtain training data sets are always 
subject to the existence of noise or outliers. When 
noise becomes large or outliers exist, traditional 
modeling approaches may try to fit those improper 
data in the training process and thus, the learned 
systems are corrupted. In other words, if the 
outliers are existed in the training data and the 
training process lasts long enough; the obtained 
systems may have overfitting phenomena. Various 
robust learning algorithms have been proposed to 
overcome this problem in the neural networks 
community. Basically, those robust learning 
algorithms made use of the so-called M-estimators 
in the training process. The basic idea of 
M-estimators is to replace the squared error term 
( 2L  norm) in the cost function by the loss 
functions so that the effects of outliers may be 
degraded. In the literature, the tanh-estimator is 
used as the robust loss function and is defined as: 
 
[ ] ( )( )( )( )
( )( )[ ]⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
<−+
≤<⎥⎥⎦
⎤
⎢⎢⎣
⎡
−
−+
<≤
=
, if  coshln
2
1 
, if    
cosh
coshln
2
1
,0 if
2
1
2
2
12
2
2
2
12
2
ebabc
c
ca
bea
ebc
abc
c
ca
ae                   e                   
eσ
(18) 
 
where a  and b  are time-dependent cutoff points, 
and 1c  and 2c  are constants that selected as 1.73 
and 0.93, respectively. The differential function of 
Eq. (18) is obtained as 
 
[ ] ( )[ ] ( )
⎪⎩
⎪⎨
⎧
<
≤<−
<≤
=
. if                                0        
, if           tanh
,0 if                                               
21
eb
beaesignebcc
aee
eϕ   (19) 
 
When outliers exist, they have great impact on the 
approximated results. Such an impact can be 
understood through the analysis of Eq. (19). In the 
literature, some of robust loss functions are often 
used.  
Another robust loss function is applied to 
modify the errors and is defined as: 
 
⎪⎪⎩
⎪⎪⎨
⎧
≤≤−
−
≤
=
− otherwise,  10
,~ˆ/~ if  ~~
ˆ/~
,~ˆ/ if          1
)(
4
21
12
2
1
csec
cc
sec
cse
ev ,   (20) 
 
where sˆ  is a robust estimate that defined as 
 
 9
where X is the model matrix with Tix  as its ith 
row, and }{W )1()1( −− = tit wdiag  is the current 
weight matrix. 
 
Step 2 and Step 3 are repeated until the estimated 
coefficients converge. 
 
Simulation Results 
 
To illustrate the effectiveness of the proposed 
robust approaches, examples with two cases of 
outliers in the symbolic interval-values data and 
one real data set are considered in this study. The 
performance measures are obtained from the 
observed values ],[ UiLii yyy =  and their 
corresponding predicted values ]ˆ,ˆ[ˆ UiLii yyy =  for 
i=1,2,…,n. Two performance indexes are used. 
One is the performance assessment of these 
regression method is defined as [24] 
 
( )
n
yy
RMSE
n
i
LiLi
L
∑
=
−
= 1
2ˆ
,           (27a) 
( )
n
yy
RMSE
n
i
UiUi
U
∑
=
−
= 1
2ˆ
.           (27b) 
 
Another, the Hausdorff distance is used to measure 
the distance between two interval data for the 
symbolic interval data. Then, this distance is also 
regarded as the errors between two interval data. 
Hence, the RMSE for the interval data can be 
rewritten as  
 
n
yyDM
RMSE
n
i
ii∑
== 1
2
Interval
)ˆ,(
,         (28) 
 
where 2)ˆ,( ii yyDM  is a Hausdorff distance 
between iy  and iyˆ  (i.e. ( )UiUiLiLiii yy,yy)Y,DM(Y ˆˆmaxˆ −−= ). 
In the following examples, the generating 
method of the symbolic interval-values data is 
stated as follows. The simple linear regression 
model is considered as 
 
285.0 += xy , ]2,0[Ux∈ ,              (29) 
 
where ]2,0[U  is represented as the uniformly 
distributed in the interval [0, 2]. The number of the 
training data generated is 201. In order to build 
interval data sets, each point ( )21 , zz  of the 
training data set is considered as the ‘seed’ of a 
rectangle. Each rectangle is therefore a vector of 
two intervals defined as [ ] [ ]( )2/,2/,2/,2/ 22221111 γγγγ +−+− zzzz . The 
parameters 1γ  and 2γ  are the width and the 
height of the rectangle, respectively. They are 
drawn randomly within a given range of values. 
Based on the above procedure, the number of the 
testing symbolic interval-values data generated is 
401. 
In example 1, the symbolic interval-values 
data with location-outliers are considered. The 
parameters 1γ  and 2γ  are drawn randomly form 
the interval [1, 2]. In this case, some of the outputs 
in the interval-values data are artificially moved to 
other locations. Those data are called the artificial 
outliers in neural networks [31]. The symbolic 
interval-values data with the location-outliers are 
shown in Figure 1. In example 2, the symbolic 
interval-values data with extreme-outliers are 
considered. The parameters 1γ  and 2γ  are 
drawn randomly form the interval [0, 5]. Figure 2 
shows the symbolic interval-values data with 
extreme-outliers. It is noted that the outlier 
situations in these two examples are different. 
In example 3, a real data set is considered. 
The exchange rates of JPY/USD and NTD/USD 
for each week are collected from Jan. 2003 to Dec. 
2007. Totally, 260 interval-values data are 
considered as the training data set. The minimum 
values and the maximum values of the exchange 
rates in each week for JPY/USD and NTD/USD 
are considered as the input and output of the 
training data set, respectively. Figure 3 shows that 
the interval-values data for JPY/USD versus 
NTD/USD. Similarly, the exchange rates of 
JPY/USD and NTD/USD for each week are 
collected from Jan. 2008 to Mar. 2008. The 
minimum values and the maximum values of the 
exchange rates in each week for JPY/USD and 
NTD/USD are considered as the input and output 
 11
 
 
四、結論 
  
In this study, the outliers existing in symbolic 
interval data are considered. Two kinds of outliers, 
the location-outliers and the extreme-outliers of the 
symbolic interval data problems are defined in this 
paper. In order to degrade the effects caused by 
outliers in the symbolic interval-values data 
problem, the robust modeling techniques are 
developed. These robust modeling techniques are 
extended from the Centre and MinMax method, 
the MinMax method and the Centre and Range 
method to the robust version of the linear 
regression methods while facing with outliers. 
From the simulation, the non-robust versions of 
linear regression methods are easily affected by 
location outliers and extreme outliers existing in 
symbolic interval-values data. It can be found that 
the proposed methods are superior to the 
non-robust versions of the linear regression 
methods and the proposed methods have nice 
robust effects while facing with no matter location 
outliers or extreme outliers of symbolic 
interval-values data. 
 
五、參考文獻 
 
[1]  G. Xiang, S. A. Starks, V. Kreinovich, and L. 
Longpre, New Algorithms for Statistical 
Analysis of Interval Data, Workshop on 
State-of-the-Art in Scientific Computing, 
NASA PACES, El Paso, TX, USA, 2004, pp. 
1-15. 
[2]  Chen-Chia Chaung, “Extended Support 
Vector Interval Regression Networks for 
Interval Input-Output Data,” Information 
Sciences, vol. 178, no. 3, pp. 871-891, 2008. 
[3]  Billard, L. and Diday, E., “Regression 
analysis for interval-valued data. In: Data 
Analysis, Classification and Related 
Methods,’ Proceedings of the Seventh 
Conference of the International Federation 
of Classification Societies (IFCS’00), pp. 
369–374, Belgium, Springer, 2000. 
[4]  Bertrand, P. and Goupil, F., “Descriptive 
statistic for symbolic data. In: Bock, H.-H., 
Diday, E. (Eds.), Analysis of Symbolic 
Data,” pp. 106–124, Springer, Heidelberg, 
2000. 
[5]  Billard, L., Diday, E., “From the statistics of 
data to the statistics of knowledge: symbolic 
data analysis,” J. Amer. Statist. Assoc., vol. 
98, no. 462, pp. 470–487, 2003. 
[6]  De Carvalho, F.A.T., “Histograms in 
symbolic data analysis,” Ann. Oper. Res., vol. 
55, pp. 229–322, 1995. 
[7]  Cazes, P., Chouakria, A., Diday, E. and  
Schektman, S., “Extension de l’analyse en 
composantes principales des donnes de type 
intervalle,” Rev. Statist. Aplique XLV, vol. 3, 
pp. 5–24, 1997. 
[8]  Lauro, N.C. and Palumbo, F., “Principal 
component analysis of interval data: a 
symbolic data analysis approach,” Comput. 
Statist., vol. 15, no. 1, pp. 73–87, 2000. 
[9]  Palumbo, F. and Verde, R., 
“Non-symmetrical factorial discriminant 
analysis for symbolic objects,” Appl. 
Stochastic Models Business Indust. vol.15, 
no. 4, pp. 419–427, 2000. 
[10]  Lauro, N.C., Verde, R. and Palumbo, F., 
Factorial discriminant analysis on symbolic 
objects. In: Bock, H.-H., Diday, E. (Eds.), 
Analysis of Symbolic Data. Springer, 
Heidelberg, pp. 212–233, 2000. 
[11]  Ichino, M.,Yaguchi, H. and Diday, E., A 
fuzzy symbolic pattern classifier. In: Diday, 
E. et al. (Eds.), Ordinal and Symbolic Data 
Analysis, Springer, Berlin, pp. 92–102, 1996. 
[12]  Rasson, J.P. and Lissoir, S., Symbolic kernel 
discriminant analysis. In: Bock, H.-H., Diday, 
E. (Eds.), Anal. Symbolic Data, Springer, 
Heidelberg, pp. 240–244, 2000. 
[13]  Perinel, E. and Lechevallier,Y., Symbolic 
Discriminant Rules. In: Bock, H.-H., Diday, 
E. (Eds.), Analysis of Symbolic Data, Springer, 
Heidelberg, pp. 244–265, 2000. 
[14]  Gowda, K.C. and Diday, E., “Symbolic 
clustering using a new dissimilarity 
measure,” Pattern Recognition, vol. 24, no. 6, 
 13
 
 SICE Annual Conference 2008 (SICE2008)： 
International Conference on 
Instrumentation, Control and Information 
Technology 
 
 
報告人: 莊鎮嘉 
 
國立宜蘭大學電機工程系 
宜蘭市神農路 1 段 1 號 
電話:0952783266 
Email: ccchuang@niu.edu.tw 
 
本次由 SICE 所舉行的儀表、控制與資訊技術會議於九十七年八月二
十到二十三日在日本東京盛大舉行，本次會議整合資訊技術、網路系統、
控制系統、神經網路計算、模糊系統計算、量測系統及相關儀表、控制與
資訊技術之開發與應用，本次會議除了相關議題的專題演講之外，尚有一
些相關論文發表－模糊控制系統、網路控制系統、智慧型控制系統及一些
實際自動化系統與機器人成果的展示。 
 
有關專題演講之內容有：邀請 H. Kitano 演講  “Biological 
robustness” ，M. Hallikainen 演講 “Microwave radiometry for romote 
sensing of the earth surfaces”，J. Baillieul 演講 “The psychology of 
human robot interaction＂， H. Asama 演講 “Mobiligence: emergence of 
adaptive motor function through interaction among the body, brain and 
environment＂， T. Iwasaki 演講 “Feedback control for oscillation by 
central pattern generator＂，  M.J. Tahk 演講  “Hybrid methods for 
numerical optimization problems＂， K. Hakuta 演講 “Manipulating and 
controlling atoms and photons using optical nanofibers”等相關儀表、控制
36、 非線性系統。 
37、 GPS 和應用。 
38、 網路感測系統。 
 
此外本人也在這次會議中，發表一篇有關含有區域資料之模糊聚集法則的
研究論文。論文(如附件)名稱如下： 
 
Fuzzy C-Means Clustering Algorithm with Unknown Number of Clusters for 
Symbolic Interval Data 
 
 
此次本人很榮幸的獲得行政院國家科學委員會的補助(計畫編號 NSC 
96-2221-E-197-021)，來參加由 SICE 所舉行的會議。會中除了學習有關上
列之模糊理論、控制與資訊技術與在工業應用與開發，更能增加在神經網
路、模糊系統與智慧型控制系統之論文研究能量的提昇，也期望能藉由此
次的會議的參與可在下年度的行政院國家科學委員會的計畫中提出一個
更新的研究計畫。最後，在此深深的感謝行政院國家科學委員會對此次行
程的補助。 
algorithm is used to partition a set of symbolic objects 
into classes so as to minimize the sum of the description 
potentials of the classes. In [17], a dynamic clustering 
algorithm for the symbolic data is proposed. In [18], 
authors has proposed several clustering algorithms for 
the symbolic data described by interval variables, based 
on a clustering criterion and has thereby generalized 
similar approaches in the classical data analysis. In [19], 
authors proposed a dynamic clustering algorithm for the 
interval data where the class representatives are defined 
by an optimality criterion based on a modified 
Hausdorff distance. In [20], authors proposed 
partitioning clustering methods for the interval data 
based on the city-block distances, also considering the 
adaptive distances. In [21], an adequacy criterion based 
on the adaptive Hausdorff distance is introduced into the 
partitioning clustering algorithm for the interval-values 
data. Recently, FCM clustering algorithm is extended to 
deal with the symbolic interval-values data [22]. This 
approach is called as IFCM clustering algorithm. 
Moreover, this algorithm is superior to the previous 
results. 
However, the reasonable initialization and the 
number of clusters are hardly determined in the IFCM 
clustering algorithm. To overcome the above problems, 
the concepts of competitive agglomeration clustering 
algorithm [23] is incorporated into FCM clustering 
algorithm for symbolic interval-values data. The 
proposed approach is called as FCMwUNC clustering 
algorithm. Due to the competitive agglomeration 
clustering algorithm possess the advantages of the 
hierarchical clustering algorithm and the partitional 
clustering algorithm, IFCMwUNC clustering algorithm 
can be fast converges in a few iterations regardless of 
the initial number of clusters. Moreover, it is also 
converges to the same optimal partition regardless of its 
initialization. Experiments results show the merits and 
usefulness of IFCMwUNC clustering algorithm for the 
symbolic interval-values data. 
The organization of the rest of the paper is as follows. 
In Section 2, an IFCM clustering algorithm is briefly 
introduced. In Section 3, IFCMwUNC clustering 
algorithm is proposed and discussed. The simulation 
results are shown in the Section 4. Finally, the 
conclusions are summarized in the Section 5. 
 
2. INTERVAL FUZZY C-MEANS (IFCM) 
CLUSTERING ALGORITHM [22] 
This algorithm is an extension of the 
standard fuzzy c-means clustering algorithm 
that furnishes a fuzzy partition and a prototype 
for each cluster by optimizing an adequacy 
criterion based on a suitable squared Euclidean 
distance between the vectors of interval-values 
data. An IFCM clustering algorithm is stated 
as follows.  
Let { }nkX k ,...,1|x == G  be a set of n vectors in an 
n-dimensional feature space with coordinate axis labels 
( )pj xxx ,...,,...,1 . Each pattern k is represented as vector 
of intervals ( )pkjkkk xxx ,...,,...,x 1=G  where ],[ jkjkjk bax =  
with j
k
j
k ba ≤ . Let ( )ci gggG GGG ,...,,...,1=  represent a 
c-tuple of prototypes each of which characterizes one of 
the c clusters. The prototype 
ig
G  can be also 
represented as a vector of intervals ( )pijii ggg ,...,,...,1  
where ],[ jijijig βα=  with jkjk βα ≤ . An IFCM 
clustering algorithm minimizes the following objective 
function: 
( ) ( ) ( )
( ) ( ) ( )∑ ∑ ∑
∑ ∑
= = =
= =
⎥⎦
⎤⎢⎣
⎡
−+−=
=
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
c
i
n
k
ikik
bau
guXUGW
1 1 1
222
1 1
21
,    
,x,,
βα
φ GG     (1) 
subject to  
1
1
=∑
=
c
i
iku , for k=1,…,n.                (2) 
In (1), φ  is the square of Euclidean distance measuring 
the dissimilarity between the vectors of the 
interval-values data, 
iku  is the membership degree of 
pattern k in ith cluster, ][ ikuU =  is a nc×  matrix 
called a constrained fuzzy c-partition matrix. 
To minimize the objective function in (1) with respect 
to U , the Lagrange multipliers method is applied and 
obtained as 
( ) ( ) ( ) ( )[ ]
.1
,,
1 1
1 1 1
2221
∑ ∑
∑∑ ∑
= =
= = =
⎟⎠
⎞⎜⎝
⎛
−−
−+−=
n
k
c
i
ikk
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
u
bauXUGJ
λ
βα   (3) 
Then, G is fixed and solve 
( ) ( )[ ] 02
1
22
1
=−−+−=
∂
∂ ∑
=
t
p
j
j
s
j
t
j
s
j
tst
st
bau
u
J λβα ,  
for s=1,…,c and t=1,…,n , (4) 
to obtain an updating equation for the memberships 
stu . 
Thus, equation (4) can be rewritten as 
( ) ( )[ ]∑
=
−+−
= p
j
j
s
j
t
j
s
j
t
t
st
ba
u
1
22
2 βα
λ , for s=1,…,c and 
t=1,…,n. (5) 
Substituting (5) into (2), tλ  is obtained as 
( ) ( )[ ]∑ ∑
= =
⎥⎦
⎤⎢⎣
⎡
−+−
=
c
i
p
j
j
i
j
t
j
i
j
t
t
ba
1 1
22
21
1
βα
λ
. (6) 
According to (5) and (6), the updating equation for 
the memberships 
iku  is 
( ) ( )[ ]
( ) ( )[ ]
1
1
1
22
1
22
−
=
=
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−+−
−+−
= ∑ ∑
∑c
h
p
j
j
h
j
k
j
h
j
k
p
j
j
i
j
k
j
i
j
k
ik
ba
ba
u
βα
βα , for i=1,…,c, 
k=1,…,n. (7) 
For the updating equation of G (i.e. j
iα  and jiβ ), 
equation (3) is minimized with respect to G. Then, we 
updating equation of j
iα  and jiβ  are obtained as 
( )
( )∑
∑
=
=
= n
k
ik
n
k
j
kik
j
i
u
au
1
2
1
2
α
 and ( )
( )∑
∑
=
=
= n
k
ik
n
k
j
kik
j
i
u
bu
1
2
1
2
β . (21) 
The choice of η  in (10) is important matter since it 
reflects the importance of the second term relative to the 
first term. If η  is too small, the second term will be 
neglected and the number of clusters will not be reduced. 
If η  is too large, the first term will be neglected, and 
all points will be lumped into just one cluster. The value 
of η  should be chosen such that both terms are of the 
same order of magnitude. As the dimensionality of 
feature space increases, the first term becomes larger 
since more components contribute to the value of the 
distance. Thus, to make the algorithm independent of 
the distance measure, η  should be proportional to the 
ratio of the two terms. In this study, η  is chosen as 
( )
( ) ( ) ( )[ ]
,)(
1
2
1
1 1 1
222
∑ ∑
∑∑ ∑
= =
= = =
⎥⎦
⎤⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−+−
=
c
i
n
k
ik
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
u
bau
itritr
βα
τη
 (22) 
where itr is an iteration index. In general,  η  is not 
confined to [0,1]. The best choice for τ  is the 
exponential decay that defined as [24]  
( ),exp)( 0 ζττ itritr −=  (23) 
where 
0τ  is the initial value and ζ  is the time 
constant. 
 
4. SIMULATION 
To show the usefulness of the IFCMwUNC 
clustering algorithm, four interval-values data sets are 
considered in this study. In following examples, a 
cluster 
ig
G  was discarded if its cardinality 
iN  is less 
than 4.5. Besides, in (26), the initial value 
0τ  and the 
time constant ζ  are set as 5 and 10, respectively. The 
initial partitions are randomly assigned.  
Two data sets (Data set 1 and Data set 2) of the 350 
points are constructed. Those data sets are also used in 
[20-22]. In each data set, the 350 points are drawn from 
three bi-variate normal distributions 
),...,;,...,( 2211 ppN σσμμ of independent components. The 
parameters 
jμ  and 2jσ  are represented as the mean 
and variance of jth variables, respectively. There are 
three clusters of unequal sizes and shapes. The 
situations of the well-separated clusters for the Data set 
1 are considered. The data points of each cluster in this 
data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points): 
281 =μ , 222 =μ , 10021 =σ  and 922 =σ . 
(b) Cluster 2 (150 points): 
601 =μ , 302 =μ , 921 =σ  and 14422 =σ . 
(c) Cluster 3 (50 points):  
451 =μ , 382 =μ , 921 =σ  and 922 =σ . 
The situations of the overlapping clusters for the Data 
set 2 are also considered. The data points of each cluster 
in this data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points):  
451 =μ , 222 =μ , 10021 =σ  and 922 =σ . 
(b) Cluster 2 (150 points): 
601 =μ , 302 =μ , 921 =σ  and 14422 =σ . 
(c) Cluster 3 (50 points):  
521 =μ , 382 =μ , 921 =σ  and 922 =σ . 
Other data sets, the Data set 3 and the Data set 4, 
contain 520 data points. There are six clusters of 
unequal sizes and shapes. Similarly, the situations of 
well-separated clusters for the Data set 3 are considered. 
The data points of each cluster in this data set were 
drawn according to the following parameters: 
(a) Cluster 1 (150 points): 
151 =μ , 122 =μ , 921 =σ  and 14422 =σ . 
(b) Cluster 2 (50 points):  
401 =μ , 102 =μ , 421 =σ  and 422 =σ . 
(c) Cluster 3 (60 points):  
601 =μ , 252 =μ , 2521 =σ  and 3622 =σ . 
(d) Cluster 4 (100 points):  
401 =μ , 402 =μ , 921 =σ  and 922 =σ . 
(e) Cluster 5 (80 points):  
201 =μ , 602 =μ , 3621 =σ  and 922 =σ . 
(f) Cluster 6 (80 points):  
551 =μ , 552 =μ , 3621 =σ  and 1622 =σ . 
The situations of the overlapping clusters for the Data 
set 4 are also considered. The data points of each cluster 
in this data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points): 
311 =μ , 122 =μ , 921 =σ  and 14422 =σ . 
(b) Cluster 2 (50 points):  
401 =μ , 102 =μ , 421 =σ  and 422 =σ . 
(c) Cluster 3 (60 points):  
501 =μ , 252 =μ , 2521 =σ  and 3622 =σ . 
(d) Cluster 4 (100 points):  
401 =μ , 402 =μ , 921 =σ  and 922 =σ . 
(e) Cluster 5 (80 points):  
301 =μ , 482 =μ , 3621 =σ  and 922 =σ . 
(f) Cluster 6 (80 points):  
551 =μ , 472 =μ , 3621 =σ  and 1622 =σ . 
In order to build the interval data sets from the Data 
set 1 to the Data set 4, each point ( )21 , zz  of these data 
sets is considered as the ‘seed’ of a rectangle. Each 
rectangle is therefore a vector of two intervals defined 
as [ ] [ ]( )2/,2/,2/,2/ 22221111 γγγγ +−+− zzzz . The 
parameters 
1γ  and 2γ  are the width and the height of 
the rectangle. They are drawn randomly within a given 
range of values. Figures 1, 2, 3 and 4 show the 
interval-values data sets, ID1~ID4, that built from the 
Data set 1 to the Data set 4 as 
1γ  and 2γ  are drawn 
during the 25th Annual Conference of the 
Gesellschaft für Klassifikation e.V. University of 
Munich, March 13, 2001 
[18] H. –H. Bock, “Clustering algorithms and Kohonen 
maps for symbolic data,” Journal of the Japanese 
Society of Computational Statistics, vol. 15, pp. 
1–13, 2002. 
[19] M. Chavent and Y. Lechevallier, Dynamical 
clustering algorithm of interval data: Optimization 
of an adequacy criterion based on Hausdorff 
distance. In: Sokolowski, A., Bock, H.-H. (Eds.), 
Classification, Clustering and Data Analysis, 
Springer-Verlag, Heidelberg, pp. 53–59, 2002. 
[20] R. M. C. R.Souza and F. A. T. De Carvalho, 
“Clustering of interval data based on city-block 
distances,” Pattern Recognition Letter, vol. 25, no. 
3, pp. 353– 365, 2004. 
[21] F. A. T. De Carvalho, R. M. C. R.Souza, M. 
Chavent and Y. Lechevallier, “Adaptive Hausdorff 
distances and dynamic clustering of symbolic 
data,” Pattern Recognition Letter, vol. 27, no. 3, 
pp. 167–179, 2006. 
[22] F. A. T. De Carvalho, “Fuzzy c-means clustering 
methods for symbolic interval data,” Pattern 
Recognition Letter, vol. 28, no. 6, pp. 423–437, 
2007. 
[23] H. Frigui and R. Krishnapuram, ”Clustering by 
Competitive Agglomeration,” Pattern Recognition , 
vol. 30, no. 7, pp. 1109–1119, 1997. 
-10 0 10 20 30 40 50 60 70 80
-10
0
10
20
30
40
50
60
70
z1
z
2
 
Figure 1: The interval-values data set of the Data set 1 
(ID1) is shown. 
20 30 40 50 60 70 80
0
10
20
30
40
50
60
z1
z2
 
Figure 2: The interval-values data set of the Data set 2 
(ID2) is shown. 
0 10 20 30 40 50 60 70 80
-20
-10
0
10
20
30
40
50
60
70
z1
z2
 
Figure 3: The interval-values data set of the Data set 3 
(ID3) is shown. 
 
10 20 30 40 50 60 70 80
-20
-10
0
10
20
30
40
50
60
z2
z
1
 
Figure 4: The interval-values data set of the Data set 4 
(ID4) is shown. 
 
0 1 2 3 4 5 6 7 8 9 10
0
2
4
6
8
10
12
14
16
18
Number of iterations
Nu
m
be
r 
of
 
cl
u
st
er
s
 
Figure 5: The reduction of the number of clusters for the 
ID2 in process is shown for the IFCMwUNC clustering 
algorithm with the different 
maxc  values. 
0 1 2 3 4 5 6 7 8 9 10
0
5
10
15
20
25
30
Number of iterations
Nu
m
be
r o
f c
lu
st
e
rs
random1
random2
random3
 
Figure 6: The reduction of the number of clusters for the 
ID4 in process is shown using the IFCMwUNC 
clustering algorithm with the different values of initial 
prototypes and 20max =c . 
 
 SICE Annual Conference 2008 (SICE2008)： 
International Conference on 
Instrumentation, Control and Information 
Technology 
 
 
報告人: 莊鎮嘉 
 
國立宜蘭大學電機工程系 
宜蘭市神農路 1 段 1 號 
電話:0952783266 
Email: ccchuang@niu.edu.tw 
 
本次由 SICE 所舉行的儀表、控制與資訊技術會議於九十七年八月二
十到二十三日在日本東京盛大舉行，本次會議整合資訊技術、網路系統、
控制系統、神經網路計算、模糊系統計算、量測系統及相關儀表、控制與
資訊技術之開發與應用，本次會議除了相關議題的專題演講之外，尚有一
些相關論文發表－模糊控制系統、網路控制系統、智慧型控制系統及一些
實際自動化系統與機器人成果的展示。 
 
有關專題演講之內容有：邀請 H. Kitano 演講  “Biological 
robustness” ，M. Hallikainen 演講 “Microwave radiometry for romote 
sensing of the earth surfaces”，J. Baillieul 演講 “The psychology of 
human robot interaction＂， H. Asama 演講 “Mobiligence: emergence of 
adaptive motor function through interaction among the body, brain and 
environment＂， T. Iwasaki 演講 “Feedback control for oscillation by 
central pattern generator＂，  M.J. Tahk 演講  “Hybrid methods for 
numerical optimization problems＂， K. Hakuta 演講 “Manipulating and 
controlling atoms and photons using optical nanofibers”等相關儀表、控制
36、 非線性系統。 
37、 GPS 和應用。 
38、 網路感測系統。 
 
此外本人也在這次會議中，發表一篇有關含有區域資料之模糊聚集法則的
研究論文。論文(如附件)名稱如下： 
 
Fuzzy C-Means Clustering Algorithm with Unknown Number of Clusters for 
Symbolic Interval Data 
 
 
此次本人很榮幸的獲得行政院國家科學委員會的補助(計畫編號 NSC 
96-2221-E-197-021)，來參加由 SICE 所舉行的會議。會中除了學習有關上
列之模糊理論、控制與資訊技術與在工業應用與開發，更能增加在神經網
路、模糊系統與智慧型控制系統之論文研究能量的提昇，也期望能藉由此
次的會議的參與可在下年度的行政院國家科學委員會的計畫中提出一個
更新的研究計畫。最後，在此深深的感謝行政院國家科學委員會對此次行
程的補助。 
algorithm is used to partition a set of symbolic objects 
into classes so as to minimize the sum of the description 
potentials of the classes. In [17], a dynamic clustering 
algorithm for the symbolic data is proposed. In [18], 
authors has proposed several clustering algorithms for 
the symbolic data described by interval variables, based 
on a clustering criterion and has thereby generalized 
similar approaches in the classical data analysis. In [19], 
authors proposed a dynamic clustering algorithm for the 
interval data where the class representatives are defined 
by an optimality criterion based on a modified 
Hausdorff distance. In [20], authors proposed 
partitioning clustering methods for the interval data 
based on the city-block distances, also considering the 
adaptive distances. In [21], an adequacy criterion based 
on the adaptive Hausdorff distance is introduced into the 
partitioning clustering algorithm for the interval-values 
data. Recently, FCM clustering algorithm is extended to 
deal with the symbolic interval-values data [22]. This 
approach is called as IFCM clustering algorithm. 
Moreover, this algorithm is superior to the previous 
results. 
However, the reasonable initialization and the 
number of clusters are hardly determined in the IFCM 
clustering algorithm. To overcome the above problems, 
the concepts of competitive agglomeration clustering 
algorithm [23] is incorporated into FCM clustering 
algorithm for symbolic interval-values data. The 
proposed approach is called as FCMwUNC clustering 
algorithm. Due to the competitive agglomeration 
clustering algorithm possess the advantages of the 
hierarchical clustering algorithm and the partitional 
clustering algorithm, IFCMwUNC clustering algorithm 
can be fast converges in a few iterations regardless of 
the initial number of clusters. Moreover, it is also 
converges to the same optimal partition regardless of its 
initialization. Experiments results show the merits and 
usefulness of IFCMwUNC clustering algorithm for the 
symbolic interval-values data. 
The organization of the rest of the paper is as follows. 
In Section 2, an IFCM clustering algorithm is briefly 
introduced. In Section 3, IFCMwUNC clustering 
algorithm is proposed and discussed. The simulation 
results are shown in the Section 4. Finally, the 
conclusions are summarized in the Section 5. 
 
2. INTERVAL FUZZY C-MEANS (IFCM) 
CLUSTERING ALGORITHM [22] 
This algorithm is an extension of the 
standard fuzzy c-means clustering algorithm 
that furnishes a fuzzy partition and a prototype 
for each cluster by optimizing an adequacy 
criterion based on a suitable squared Euclidean 
distance between the vectors of interval-values 
data. An IFCM clustering algorithm is stated 
as follows.  
Let { }nkX k ,...,1|x == G  be a set of n vectors in an 
n-dimensional feature space with coordinate axis labels 
( )pj xxx ,...,,...,1 . Each pattern k is represented as vector 
of intervals ( )pkjkkk xxx ,...,,...,x 1=G  where ],[ jkjkjk bax =  
with j
k
j
k ba ≤ . Let ( )ci gggG GGG ,...,,...,1=  represent a 
c-tuple of prototypes each of which characterizes one of 
the c clusters. The prototype 
ig
G  can be also 
represented as a vector of intervals ( )pijii ggg ,...,,...,1  
where ],[ jijijig βα=  with jkjk βα ≤ . An IFCM 
clustering algorithm minimizes the following objective 
function: 
( ) ( ) ( )
( ) ( ) ( )∑ ∑ ∑
∑ ∑
= = =
= =
⎥⎦
⎤⎢⎣
⎡
−+−=
=
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
c
i
n
k
ikik
bau
guXUGW
1 1 1
222
1 1
21
,    
,x,,
βα
φ GG     (1) 
subject to  
1
1
=∑
=
c
i
iku , for k=1,…,n.                (2) 
In (1), φ  is the square of Euclidean distance measuring 
the dissimilarity between the vectors of the 
interval-values data, 
iku  is the membership degree of 
pattern k in ith cluster, ][ ikuU =  is a nc×  matrix 
called a constrained fuzzy c-partition matrix. 
To minimize the objective function in (1) with respect 
to U , the Lagrange multipliers method is applied and 
obtained as 
( ) ( ) ( ) ( )[ ]
.1
,,
1 1
1 1 1
2221
∑ ∑
∑∑ ∑
= =
= = =
⎟⎠
⎞⎜⎝
⎛
−−
−+−=
n
k
c
i
ikk
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
u
bauXUGJ
λ
βα   (3) 
Then, G is fixed and solve 
( ) ( )[ ] 02
1
22
1
=−−+−=
∂
∂ ∑
=
t
p
j
j
s
j
t
j
s
j
tst
st
bau
u
J λβα ,  
for s=1,…,c and t=1,…,n , (4) 
to obtain an updating equation for the memberships 
stu . 
Thus, equation (4) can be rewritten as 
( ) ( )[ ]∑
=
−+−
= p
j
j
s
j
t
j
s
j
t
t
st
ba
u
1
22
2 βα
λ , for s=1,…,c and 
t=1,…,n. (5) 
Substituting (5) into (2), tλ  is obtained as 
( ) ( )[ ]∑ ∑
= =
⎥⎦
⎤⎢⎣
⎡
−+−
=
c
i
p
j
j
i
j
t
j
i
j
t
t
ba
1 1
22
21
1
βα
λ
. (6) 
According to (5) and (6), the updating equation for 
the memberships 
iku  is 
( ) ( )[ ]
( ) ( )[ ]
1
1
1
22
1
22
−
=
=
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−+−
−+−
= ∑ ∑
∑c
h
p
j
j
h
j
k
j
h
j
k
p
j
j
i
j
k
j
i
j
k
ik
ba
ba
u
βα
βα , for i=1,…,c, 
k=1,…,n. (7) 
For the updating equation of G (i.e. j
iα  and jiβ ), 
equation (3) is minimized with respect to G. Then, we 
updating equation of j
iα  and jiβ  are obtained as 
( )
( )∑
∑
=
=
= n
k
ik
n
k
j
kik
j
i
u
au
1
2
1
2
α
 and ( )
( )∑
∑
=
=
= n
k
ik
n
k
j
kik
j
i
u
bu
1
2
1
2
β . (21) 
The choice of η  in (10) is important matter since it 
reflects the importance of the second term relative to the 
first term. If η  is too small, the second term will be 
neglected and the number of clusters will not be reduced. 
If η  is too large, the first term will be neglected, and 
all points will be lumped into just one cluster. The value 
of η  should be chosen such that both terms are of the 
same order of magnitude. As the dimensionality of 
feature space increases, the first term becomes larger 
since more components contribute to the value of the 
distance. Thus, to make the algorithm independent of 
the distance measure, η  should be proportional to the 
ratio of the two terms. In this study, η  is chosen as 
( )
( ) ( ) ( )[ ]
,)(
1
2
1
1 1 1
222
∑ ∑
∑∑ ∑
= =
= = =
⎥⎦
⎤⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−+−
=
c
i
n
k
ik
c
i
n
k
p
j
j
i
j
k
j
i
j
kik
u
bau
itritr
βα
τη
 (22) 
where itr is an iteration index. In general,  η  is not 
confined to [0,1]. The best choice for τ  is the 
exponential decay that defined as [24]  
( ),exp)( 0 ζττ itritr −=  (23) 
where 
0τ  is the initial value and ζ  is the time 
constant. 
 
4. SIMULATION 
To show the usefulness of the IFCMwUNC 
clustering algorithm, four interval-values data sets are 
considered in this study. In following examples, a 
cluster 
ig
G  was discarded if its cardinality 
iN  is less 
than 4.5. Besides, in (26), the initial value 
0τ  and the 
time constant ζ  are set as 5 and 10, respectively. The 
initial partitions are randomly assigned.  
Two data sets (Data set 1 and Data set 2) of the 350 
points are constructed. Those data sets are also used in 
[20-22]. In each data set, the 350 points are drawn from 
three bi-variate normal distributions 
),...,;,...,( 2211 ppN σσμμ of independent components. The 
parameters 
jμ  and 2jσ  are represented as the mean 
and variance of jth variables, respectively. There are 
three clusters of unequal sizes and shapes. The 
situations of the well-separated clusters for the Data set 
1 are considered. The data points of each cluster in this 
data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points): 
281 =μ , 222 =μ , 10021 =σ  and 922 =σ . 
(b) Cluster 2 (150 points): 
601 =μ , 302 =μ , 921 =σ  and 14422 =σ . 
(c) Cluster 3 (50 points):  
451 =μ , 382 =μ , 921 =σ  and 922 =σ . 
The situations of the overlapping clusters for the Data 
set 2 are also considered. The data points of each cluster 
in this data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points):  
451 =μ , 222 =μ , 10021 =σ  and 922 =σ . 
(b) Cluster 2 (150 points): 
601 =μ , 302 =μ , 921 =σ  and 14422 =σ . 
(c) Cluster 3 (50 points):  
521 =μ , 382 =μ , 921 =σ  and 922 =σ . 
Other data sets, the Data set 3 and the Data set 4, 
contain 520 data points. There are six clusters of 
unequal sizes and shapes. Similarly, the situations of 
well-separated clusters for the Data set 3 are considered. 
The data points of each cluster in this data set were 
drawn according to the following parameters: 
(a) Cluster 1 (150 points): 
151 =μ , 122 =μ , 921 =σ  and 14422 =σ . 
(b) Cluster 2 (50 points):  
401 =μ , 102 =μ , 421 =σ  and 422 =σ . 
(c) Cluster 3 (60 points):  
601 =μ , 252 =μ , 2521 =σ  and 3622 =σ . 
(d) Cluster 4 (100 points):  
401 =μ , 402 =μ , 921 =σ  and 922 =σ . 
(e) Cluster 5 (80 points):  
201 =μ , 602 =μ , 3621 =σ  and 922 =σ . 
(f) Cluster 6 (80 points):  
551 =μ , 552 =μ , 3621 =σ  and 1622 =σ . 
The situations of the overlapping clusters for the Data 
set 4 are also considered. The data points of each cluster 
in this data set were drawn according to the following 
parameters: 
(a) Cluster 1 (150 points): 
311 =μ , 122 =μ , 921 =σ  and 14422 =σ . 
(b) Cluster 2 (50 points):  
401 =μ , 102 =μ , 421 =σ  and 422 =σ . 
(c) Cluster 3 (60 points):  
501 =μ , 252 =μ , 2521 =σ  and 3622 =σ . 
(d) Cluster 4 (100 points):  
401 =μ , 402 =μ , 921 =σ  and 922 =σ . 
(e) Cluster 5 (80 points):  
301 =μ , 482 =μ , 3621 =σ  and 922 =σ . 
(f) Cluster 6 (80 points):  
551 =μ , 472 =μ , 3621 =σ  and 1622 =σ . 
In order to build the interval data sets from the Data 
set 1 to the Data set 4, each point ( )21 , zz  of these data 
sets is considered as the ‘seed’ of a rectangle. Each 
rectangle is therefore a vector of two intervals defined 
as [ ] [ ]( )2/,2/,2/,2/ 22221111 γγγγ +−+− zzzz . The 
parameters 
1γ  and 2γ  are the width and the height of 
the rectangle. They are drawn randomly within a given 
range of values. Figures 1, 2, 3 and 4 show the 
interval-values data sets, ID1~ID4, that built from the 
Data set 1 to the Data set 4 as 
1γ  and 2γ  are drawn 
