 2 
1. 前言與 研究目的 
Only a few years ago, most microprocessor vendors were still striving to uphold Moore’s Law – each 
new chip generation would have a faster clock rate, more transistors, deeper pipelines, wider issues to 
exploit ILP (Instruction Level Parallelism), and with larger on-chip caches. However, system designers 
who were using that approach have long been facing increasing difficulty in dealing with power/thermal 
issues, speed gap between processors and memories, and difficulties in exploiting more ILP in application 
programs. These challenges have been dubbed by some pundits as power wall, memory wall and ILP wall. 
Almost all of the general-purpose microprocessor vendors have been forced to adopt 
multi-core/many-core approach in their technology roadmap today to overcome some of these difficulties. 
Many embedded processors have also followed the similar trend. This new kind of platforms has brought 
with it many new challenges and also many opportunities for system designers and application 
programmers. It has a profound impact on the entire computer industries. The most important fallout of 
this change is that application programs can no longer take the free ride of ever increasing clock rate to 
improve their performance. It will require them to exploit various forms of parallelism that could include 
ILP, DLP (data-level parallelism, e.g. SSE on IA32, Altivec on IBM Power6, Neon on ARM), TLP 
(thread-level parallelism), MLP (memory-level parallelism), to name a few, to achieve high performance. 
It becomes extremely difficult for programmers to manage so many forms of parallelism in their 
application programs. Usually they need to rely on more powerful static compilers to extract such 
parallelisms for them. However, the increasingly complex dynamic environment on a 
multi-core/many-core processor makes such a task ever more challenging for a static compiler. It becomes 
very appealing if some kind of dynamic runtime optimizer could make up for such a short fall for static 
compilers. Such a dynamic optimizer could re-optimize the statically generated binary code and re-adapt 
its execution according to the unexpected changes of dynamic runtime environment. Such a dynamic 
optimizer not only could improve single-application performance (we call it latency computing because it 
could speed up single-application performance), but also improve system throughput (we call it 
throughput computing). Even if it cannot improve either of them, it could still re-adapt and re-optimize 
applications for better power/thermal management. Obviously, the ideal outcome is to achieve optimizing 
single-application performance, improving system throughput, and minimizing power/thermal effect all at 
the same time. 
Another challenge for multi-core/many-core processors is to effectively and efficiently managing 
their large amount of system resources at a finer-grained level. Existing runtime resource management 
primarily rests on OS, hypervisor or middleware. They often incur too large an overhead for many forms 
of parallelism mentioned above. Again, a dynamic optimizer will become very useful and desirable if it 
could take advantage of many existing low-overhead hardware performance counters and power/thermal 
sensors to identify system bottlenecks or power over usage, detect repeating program execution phases to 
amortize program re-optimization and re-adaptation overhead, and rapidly patch and launch re-optimized 
code for better performance, lower power usage and bottleneck elimination. However, such dynamic 
optimizers are receiving only minimal attention and support in static compilers, architectures, or operating 
systems today. 
As memory wall could only become worse on multi-core/many-core processors due to competition 
for the shared last-level on-chip cache and off-chip bandwidth from many concurrent threads more 
 4 
speed up these applications on multi-core systems. However, the potential degradation in energy efficiency 
associated is an important factor that hinders the deployment of this technique. For multi-core systems that 
integrate same-ISA heterogeneous cores, it is possible to judiciously allocate speculative threads to achieve 
both performance and energy efficiency improvement over sequential execution. During this period of time, 
we examine a multi-core system with multiple same-ISA heterogeneous cores, some of which supporting 
simultaneous multithreading. In this environment, we propose a thread allocator that dynamically determines 
how speculative threads are allocated. For each segment of execution, the following decisions are made to 
maximize energy efficiency: (i) whether speculative parallel threads should be deployed to a single core with 
SMT support or to multiple cores each supporting a single thread of execution; (ii) whether the threads should 
utilize more powerful cores with a high issue width or a less powerful core with low issue width; (iii) whether 
the L1 cache should be fully activated or partially activated. Our allocator can migrate the threads and/or 
re-size the L1 caches based on these decisions. We have also incorporated throttling mechanisms to suppress 
thread management operations when the benefit cannot justify the high overhead associated with thread 
migration and/or cache re-sizing. By evaluating speculatively parallelized benchmarks from SPEC CPU 2006 
and 2000, we found that TLS execution, on a heterogeneous multi-core system with dynamic thread 
management support, is 44% more energy efficient, in terms of ED2P, than the unmodified sequential 
program executing on an unmodified superscalar. This corresponds to a 38% performance improvement with 
6% increase in energy consumption. The most energy-efficient homogeneous system, for executing 
speculatively parallelized benchmarks, is a SMT processor with a 64K L1 cache. The proposed heterogeneous 
system is 13% more energy efficient than the homogeneous system 
 
This work has been accepted by the internationally renowned IEEE PACT (Parallel Architecture and 
Compilation techniques) conference, and will be presented in September in Vienna. 
 
Another progress we have made is in the area of seeking dynamic optimization opportunities in data 
mining applications. Explosive growth in the availability of various kinds of data in both commercial and 
scientific domains have resulted in an unprecedented need to develop novel data-driven, knowledge discovery 
techniques. Data mining is one such data-centric application. It consists of methods to discover interesting, 
nontrivial, and useful patterns hidden within massive amounts of data. Researchers from both academia and 
industry have recognized that the challenges of data mining applications will help shape the future of 
multi-core processor and parallelizing compiler designs. However, relatively little has been done to 
understand the performance characteristics of these applications on modern multi-core processors. 
 
The exponential growth of on-chip resources make it critical to exploit parallelism at all granularities for 
improving the performance of data mining applications. In this paper, we examine the instruction-level, 
memory-level and thread-level parallelism available in data mining applications. We observe that (i) data 
mining applications have a slightly different instruction mix from SPEC integer applications, and this 
difference can potentially lead to different ILP extraction; ii) although many data mining applications 
suffer from data cache miss penalty, similar to SPEC integer applicationis, different techniques must be 
developed to enable effective prefetching due to the existance of complex and irregular data structures, such 
as hash tables; (iii) although data mining applications have large amount of thread-level parallelism, efficient 
 Report2010 1 
 
 
出國報告（出國類別： □ A 類、考察訪問 
□ B 類、出國短期研究 
X  C 類國際會議）  
 
2010 International Conference on 
Parallel Processing Program Committee 
Meeting 
國際平行處理學議程會議 
 
              
服務機關：國立交通大學 
姓名職稱：徐慰中教授 
前往國家：美國 Minneapolis 
出國期間：5/05/2010-5/09/2010 
報告日期：5/10/2010 
 
撰 寫 人 審
核
人 
初 閱 複 閱 
   
 
 
  
 
 
 Report2010 3 
報告內容應包括下列各項： 
一、參加經過 
The ICPP (International Conference on Parallel Processing) provides a forum for 
engineers and scientists in academia, industry and government to present their latest 
research findings in any aspects of parallel and distributed computing. ICPP 2010 
will be held in San Diego from 9/13 to 9/16. This year will be the 39th annual conference 
of ICPP. I have been requested to serve for the program committee as the track chair 
for programming language, compilers and tools. Other than the compiler track, there 
are several tracks such as Architecture, Data intensive and I/O computing, Algorithms 
and applications, P2P and service-oriented architectures, Cluster, grid and cloud 
computing, Performance evaluation and simulation, Information retrieval and 
knowledge discovery, OS/Resource management and Wireless networks and pervasive 
computing. The program committee was held on 5/08/2010, in Minneapolis.  
 
This year’s program chair is director Yew from Institute of Information Science, 
Academia Sinica.The following track chairs attended the PC meeting:  
Pen Chung Yew, IIS, Academia Sinica 
Andreas Moshovos, Univ. of Toronto 
Wei Chung Hsu, NCTU, Taiwan 
John Douceur, Microsoft, 
Pavan Balaji, Argonne National Lab., 
David A. Bader, Georgia Inst. of Tech 
Qing Yang, Univ. of Rhode Island, 
Allen Malony, Univ. of Oregon 
 
For the compiler track, we selected 9 papers out of 30 for presentation at the 
conference in September. Many of them are multi-core related. There are also  
many submissions on GPGPU programming. The PC meeting started at 8AM, and ended near 
3PM. I got a chance to chat with Andreas from U. of Toronto, John Douceur from 
MicroSoft, and Pavan Balaji from Argonne lab after the PC meeting. 
 
二、心得（可含照片） 
 
Participating in the international conference PC meetings can help understanding 
the current trend of research, the process of paper selection, and different 
viewpoints from other PC members. We should encourage researchers in Taiwan to 
actively participate in international conference organizations. This could broaden 
the view of our researchers and increase the impact of our research in a timely manner. 
As a track chair for the compiler and programming language track, I have observed 
that a large number of submissions come from mainland China. This indicates that 
computer scientists in China have paid much attention to the internationally renowned 
conferences. Although only a few of them get into the program at this time, I believe 
they will quickly learn how to get paper accepted to these conferences.   
 
三、考察參觀活動(無是項活動者，或前已敘述者可省略此項) 
 
No such activities. 
 
無研發成果推廣資料 
