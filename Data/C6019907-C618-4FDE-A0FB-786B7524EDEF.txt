 2
relational structure constructed by all training 
instances will be investigated here.  By 
using this relational structure, the relations 
among all training instances can be 
determined.  In this manner, the importance 
or criticalness of each training instance will 
be identified (for example, instances that are 
rarely used in determining the class labels of 
other instances will be considered to be 
eliminated at first).  Meanwhile, instances 
that greatly affect the classification accuracy 
of a specific domain or noisy instances will 
also be marked.  Accordingly, various 
instance-pruning rules can be defined by 
considering the above different kinds of 
training instances.  As a result, a suitable 
and small training subset can be obtained for 
classifying new instances.  Here, the storage 
requirement and time complexity of the 
nearest neighbor classification rule will be 
reduced.  Also, the above pruned training 
subset is expected to quickly classify new 
instances.  Furthermore, the proposed 
instance pruning or selection method will be 
evaluated using various classification 
problems.  By using the proposed pruning 
method, the classification accuracy will be 
expected to be maintained or even increased.  
As a result, related research results have been 
published in prominent international journals 
and conferences, including [40-42].    
Keywords: Instance pruning, Instance 
selection, Pattern classification, Nearest 
neighbor classification rule, Relational 
structure 
 
二、緣由與目的 
 
在人工智慧(Artificial Intelligence)領
域中，已有許多不同的機器學習方法
(Machine Learning Methods)[12]陸續被提
出，以解決真實世界中的分類問題
(Real-world Classification Problems) 。其
中，最近鄰居分類法扮演著相當重要的角
色，它已被證實能提供良好的分類效能
[5-6]，同時目前也廣泛地運用於各種有關
樣本分類 (Pattern Classification) 的問題
[21]。它又被稱為以樣本為基礎的學習方法
(Instance-based Learning Methods)[1-2]或怠
惰 的 學 習 方 法 (Lazy Learning 
Methods)[20]。 
除 了 最 近 鄰 居 分 類 器 (Nearest 
Neighbor Classifier) [5-6]之外，以樣本為基
礎的機器學習方法有幾種變型，例如以相
似度為基礎的學習法則 (Similarity-based 
Learning)[31]，以記憶為基礎的學習法則
(Memory-based Learning)[32]與以案例為基
礎的推論法則(Case-based Reasoning)[34]
等等。 
Cover 及 Hart [6]等學者已提出一個重
要證明，在任何數目的類別 (Class)個數
下，最近鄰居分類法對於一分類問題的分
類 錯 誤 (Classification Error) 可 以 限 制
(Bound)在 R*及 2R*之間，其中 R*代表貝
式錯誤(Bayes’error)，亦即是最理想的分類
錯誤(Optimal error)。換言之，對於任一分
類問題而言，最近鄰居分類法可以提供基
本的效能保證。 
在上述最近鄰居分類法(或以樣本為
基礎的學習方法)中，儲存了一個稱為訓練
集(Training Set)的樣本集合，該集合是由許
多已知類別 (Class Label) 為何的樣本
(Instance or Example)所組成[17]。最近鄰居
分類法的優點主要是讓人容易瞭解及易於
實現(作)，同時也能產生良好的分類效果
( 即高分類正確率， high classification 
accuracy) [5-6]。另外，不同於類神經網路
及決策樹等分類學習機制，最近鄰居分類
法不需要事前的訓練，在此方法中，所謂
的學習則是一直延遲到真正作分類動作時
才開始，因此，如前所述，最近鄰居分類
法 有 怠 惰 的 學 習 方 法 (Lazy Learning 
Methods) [20]之稱。 
雖然最近鄰居分類法已被廣泛地應用
於真實世界中的分類問題，但它仍存在著
一個急待解決的問題，即計算量及儲存成
本(Computation and Storage Costs)龐大的
問題[17][33][37-39]。一般而言，最近鄰居
分類法針對一未知類別的樣本(又稱測試
樣本)進行分類時，需比對在訓練集合中所
有的訓練樣本(也就是說，測試樣本與所有
訓練樣本之間的相似度需一一決定)，換言
之，在進行最近鄰居分類計算時將會變得
無法忍受地緩慢(Unbearably Slow)[3]，亦即
是 分 類 計 算 的 反 應 速 度 過 慢 (Slow 
Response)。假設在訓練集合(Training Set)
 4
加入了數值正規化(Normalization)與遺漏
值(Missing Attribute Value)處理的機制。而
在 IB3 演算法中，則特別挑選有相對較高
分類正確率的訓練樣本，以形成樣本縮減
之後最終的訓練集合。 
Zhang 提出 TIBL，即以典型樣本為基
礎 (Typical Instance-based Learning 
Approach)[38]的樣本縮減方法，這個方法
主要保留在樣本空間(Instance Space)中區
域中心(Region Center)裡具代表性的訓練
樣本，換言之，這個方法將不考慮在樣本
空間(Instance Space)中處於邊界(Border)的
訓練樣本。而在最小前後一致集合 Minimal 
Consistent Set (MCS)方法中[14]，則是考慮
保留最近而類別不相同的鄰居 (Nearest 
Unlike Neighbor)，這個樣本縮減方法執行
後所產生之最後的訓練樣本集合 (Final 
Training Set)將是唯一(Unique)的。在實證
研究結果方面則說明了這個方法優於
Condensed Nearest Neighbor Rule 之樣本縮
減規則。在參考文獻[15]中，作者則提出了
一種結合最近鄰居選取(Nearest Neighbor 
Selection) 及 訓 練 樣 本 壓 縮 (Instance 
Condensing)的技術，以最小化(Minimize)
樣本縮減所需要的計算成本(Computational 
Cost)。 
針對不同的樣本特性，C-pruner[3]提出
兩個刪除訓練樣本的規則，除了過濾雜訊
樣本(Noisy Instances)之外，同時也考慮刪
除多餘但不是關鍵的訓練樣本。然而，在
此方法中，選取各個訓練樣本的次序(Order)
仍舊會影響最終的樣本縮減結果，此外，
該 作 法 是 啟 發 式 的 方 法 (Heuristic 
Method)，刪除訓練樣本的次序主要由亂數
來決定(Randomly Decided)[3]，顯而易見
地，以亂數決定選取訓練樣本的方式降低
了 C-pruner 的實用性。在針對多個資料集
(Datasets) 進行實驗研究比較結果後，
C-pruner 在 分 類 正 確 率 (Classification 
Accuracy)的表現上優於 IB3 和 DROP3[16]
等樣本縮減方法，而在資料保留率(Data 
Retention Rate)方面的表現則較兩者差。 
Wilson 與 Martinez 在[39]中提出稱為
RT3 的樣本縮減方法，在此方法中，作者
針對每一個訓練樣本 x，定義了一個數值
associate(x)，該數值代表以 x 為 k 個最近鄰
居之訓練樣本的樣本個數； RT3 以
associate(x)的大小來決定是否移除該訓練
樣本 x。此外，RT3 也運用了前述的 Edited 
Nearest Neighbor Rule 決策規則來過濾雜
訊樣本。在 RT3 的基礎下，作者進行更進
一步的延伸設計，包括 DROP1-DROP5[16]
等五種不同的樣本縮減演算法。上述 IB 系
列(IB2，IB3)的樣本縮減方法主要專注於選
取或過濾在樣本空間(Instance Space)中處
於中心點(Central Point)的訓練樣本，而
DROP 系列的樣本縮減方法則特別考量過
濾一些非關鍵的訓練樣本 (Non Critical 
Instances)[16]。 
一般而言，樣本縮減方法多半存在著
不可避免的取捨(Trade-off)，一邊是最後保
留在樣本空間中的訓練樣本個數，另一邊
則是經樣本縮減後執行機器學習分類方法
所 得 到 的 分 類 正 確 率 (Classification 
Accuracy)與效率(Efficiency)。當然，我們
通常希望所選取具代表性的訓練樣本越少
越好[17]。 
針對上述種種樣本縮減方法，大致可
以區分為兩類，一種是遞增的(Incremental)
樣 本 縮 減 方 法 ， 一 種 則 是 遞 減 的
(Decremental)樣本縮減方法。在遞增的樣本
縮減方法中，一開始所挑選的訓練樣本集
合是空(Empty)集合，在一一檢查各訓練樣
本後，滿足篩選條件(Selection Rules)的訓
練樣本則納入最後的訓練樣本集合中，不
符合條件者(如具雜訊的訓練樣本)則未被
納入。而在遞減的樣本縮減方法中，過程
正好相反，滿足篩選條件的訓練樣本保留
在訓練集合中，不符合條件者則予以刪
除。屬於遞增的樣本縮減方法，包括
Condensed Nearest Neighbor Rule[9]，IB2
與 IB3[17]等等。屬於遞減的樣本縮減方
法 ， 則 包 括 Edited Nearest Neighbor 
Rule[11]，Reduced 最近鄰居規則(Nearest 
Neighbor Rule)[10]，DROP1-DROP5[16] 等
等。無論是遞增的或是遞減的樣本縮減方
法，在設計上多半會面臨訓練樣本選擇次
序(The Order of Presentation of Training 
Instances)[3]的考量，也就是說，訓練集合
中的訓練樣本，哪些該優先進行樣本篩選
條件的檢查，這個問題將會影響樣本縮減
方法最後的結果，亦即是最後所得到的訓
等相似度函數。在這些相似度函數中，有
些主要適用於符號型態(Symbolic)的屬性
(Attribute)，有些則主要適用於數值型態
(Numeric)的屬性。在[19]中，我們已提出
一個有效的測定樣本間相似程度的關聯模
型(Relational Model)。在此關聯模型中，樣
本之間的關聯性或相近性是由修正後的灰
關聯分析方法 (Improved Grey Relational 
Analysis Method)[22-24]來決定，值得關注
的是，這一個新的關聯模型除了可處理符
號型態的屬性，也可以處理數值型態的屬
性。應用此灰關聯模型(即灰關聯結構)以作
為最近鄰居分類法測定樣本間相似度的函
數，將會得到比以 HOEM、Value Difference 
Metric、HVDM、IVDM 及 WVDM 等為相
似度函數，有更佳的分類正確率[19]。 
假設在最近鄰居分類法的樣本空間
(Instance space)中有 m+1 個樣本，分別為
{x0, x1, x2, …, xm}。其中 x0 為參考樣本
(Referential Observation)，而 x1, x2, …, xm 
等 則 為 比 較 樣 本 (Compared 
Observations)。每一個樣本 xe 共有 n 個屬
性(Attributes)並可以表示為 xe = (xe(1), 
xe(2), …, xe(n))。則兩樣本之間的灰關聯係
數(Grey Relational Coefficient，GRC)可求
得如下： 
( ) ,
max)()(
maxmin
)(),(
0
0 ∆+−
∆+∆= ζ
ζ
pxpx
pxpxGRC
i
i
   (1) 
其 中  )()(minminmin 0 kxkx jkj −=∆ ∀∀ , 
)()(maxmaxmax 0 kxkx jkj
−=∆
∀∀
, ζ∈[0,1] (一般而言, ζ
多半設為 0.5), i = j = 1, 2, …, m, and k = p = 
1, 2, …, n。 
在此，對於每一個樣本的單一屬性
(Attribute)p 而言，  代表了
 與 之間的相似程度(或關聯程
度，Similarity)。假如
( )(),(0 pxpxGRC i )
)(0 px )( pxi
( ))(),( 10 pxpxGRC 大於 
，則  與   之間的
Similarity 大 於  與  之間的
Similarity，反之亦然。基於此，樣本與樣
本之間的灰關聯程度 (Grey Relational 
Grade，GRG)可計算如下： 
( ))(),( 20 pxpxGRC )(0 px )(1 px
)(0 px )(2 px
( ) ∑
=
= n
1
00 ))(),((n
1
,
k
ii kxkxGRCxxGRG           (2) 
其中 i = 1, 2, …, m。 
在此，灰關聯分析(或灰關聯程度)的主
要特性如下： 
假如 ( )10 , xxGRG  大於 ，那麼樣本x
( 20 , xxGRG )
0 與 x1 之間的差異(Difference)將小
於x0 與 x2之間的差異，反之亦然。 
藉由以灰關聯分析作為測量樣本與樣
本之間的關聯程度之基準，我們很快地可
以獲知樣本與樣本之間的相近性或相似性
(有關符號型態屬性的處理近似於數值型
態屬性的處理，詳細的內容可見[19])。以
該相近性所建構出來的樣本空間(Instance 
Space)，我們將之定義為灰關聯結構(Grey 
Relational Structure)。本研究以此灰色關聯
結構為基礎，以測量樣本與樣本之間的相
似或關聯程度，在研究方法上有其創新
性，亦可避免傳統的最近鄰居分類法採用
Euclidean Distance Function [17]、Manhattan 
Distance Function [36]、HOEM [36]、Value 
Difference Metric [32]、HVDM [36]、IVDM 
[36]及 WVDM [36]等作為相似度函數可能
伴隨的缺點[19]，包括無法處理符號型態的
屬性或無法處理數值型態的屬性[32]。 
在[19]中，實驗研究結果已證明，應用
此處所提出的灰關聯模型(即灰關聯結構)
以作為最近鄰居分類法測定樣本間相似度
的函數，將會得到比以 HOEM、Value 
Difference Metric 、 HVDM 、 IVDM 及
WVDM 等為相似度函數，有更佳的分類正
確率。除此之外，此關聯模型也可以同時
處理符號型態與數值型態的屬性。同時，
上述灰關聯模型擁有下列特性及優點
[22-23]：(1)所有的相似度測量會被規範在
數值 0 和 1 之間，這是進行屬性正規化
(Attribute Normalization)的重要作法。(2)
該模型所獲得的關聯程度具有方向性和順
序性(Wholeness)。(3)關聯程度具非對稱性
(Asymmetric)。以上種種特性將有助於在最
近鄰居分類方法中，進行樣本間的相似度
測量及可獲得的分類效果[19]。 
我們之所以採用該關聯結構(或模型)
作為本研究中樣本間相似程度之測度方法
及整個研究架構基礎的原因，除了考量上
述種種灰關聯結構的特性和優點之外，以
此樣本間的關聯結構為基礎，將可以有效
地決定(或選取)在樣本空間(Instance Space)
中最具代表性或關鍵性的訓練樣本、以及
哪些是雜訊樣本或會嚴重影響分類正確率
的訓練樣本；進而具體歸納出幾種不同類
 6
 8
果。舉例而言，考量著名而具 150 個訓練
樣本的蝴蝶花分類問題(Iris Classification 
Problem)[25]，其中所有的訓練樣本經本研
究所提出的部分樣本縮減規則進行縮減
後，可將訓練樣本由 150 個縮減為僅剩 3
個，也就是說，可獲得最低的資料保留率
(Data Retention Rate)－三個類別(Iris-setosa, 
Iris-versicolor, Iris-Virginica)最後分別僅各
保留一個訓練樣本。然而，縮減訓練樣本
後所得的分類正確率卻沒有因訓練樣本縮
減而降低。本研究所採用的研究方法與模
型除了可以得到極佳的資料保留率(Data 
Retention Rate)之外，也可保有良好的分類
正確率(Classification Accuracy)與加速分類
速度，並降低所需的樣本儲存空間(Storage 
Requirement)。該方法亦能針對具大量樣本
(如超過 20000 筆訓練樣本 )的資料集
(Dataset)進行樣本分類動作及研究分析。相
信本研究所提的樣本縮減方法將有助於加
快最近鄰居分類法的分類速度，並減少其
所需要的樣本儲存空間。同時，我們預期
將此一方法推展至網際網路、線上(Online)
或即時(Real Time)的最近鄰居分類法相關
應用，特別值得注意的是在以本研究所提
方法進行訓練樣本縮減後所帶來的各種效
益(包括極佳的資料保留率、快速的分類速
度及良好的分類正確率)。 
藉由上述種種探討分析，本計畫所發
展的方法有下列多項優點(與文獻探討中
現有種種樣本縮減方法不同)： 
(1) 優於其他方法的是，本方法同時考量
在關聯結構(或樣本空間)中處於中心
的 訓 練 樣 本 (Central Training 
Instances) 或是關鍵的、具代表性
(Representative)且有助於提昇分類正
確率的訓練樣本 (Critical Training 
Instances) 、處於樣本空間中邊界
(Border)的訓練樣本、雜訊樣本(Noisy 
Instances)、以及容易產生分類錯誤的
訓練樣本。特別是挑選可產生較高分
類正確率的訓練樣本，以形成最終的
訓練集合。 
(2) 本方法採用的關聯結構(即樣本間的
相似度測量方法)在樣本分類的表現
上優於其他現有的相似度函數(詳細
的比較分析可見參考文獻[19])。 
(3) 進行樣本縮減後所產生的訓練集合
是唯一(Unique)的，也就是最小且具
前後一致性的樣本集合 (Minimal 
Consistent Subset)。 
(4) 本 方 法 可 同 時 適 用 於 符 號 型
(Symbolic)和數值型(Numeric)的樣本
屬性(Attributes)。 
(5) 不同於著名的 C-pruner[3]等方法，本
方法刪除訓練樣本的次序不會由亂
數來決定(Randomly Decided)，因此不
會影響本方法的實用性。 
    另外，我們亦將所提出的樣本縮減方
法與其他現有的著名縮減方法 (如前述
CNN、ENN、 IB1、 IB2、 IB3、RT3、
DROP1-DROP5、ICPL、C-pruner 等等)進
行比較分析，包括分類正確率及資料保留
率的比較分析，以驗證所提方法的分類正
確率與各項效能。各項實驗結果已相繼發
表於[40-42]。 
在此，由嚴謹的理論探討至完整的實
驗設計過程，本計畫亦遭遇了一些困難，
包括相關理論之實現、樣本縮減相關演算
法的實作與除錯、以及與其他樣本縮減方
法的比較等等課題。在有效地掌握整個專
案計畫的執行進度下，方得以一一解決這
些困難，順利完成本計畫。 
 
五、成果自評 
 
根據本研究計畫內容，依序執行已完
成進度如下： 
 
(1) 已完成蒐集相關文獻、確定研究方
向、擬定研究目的、方法與步驟。 
(2) 有關理論分析與文獻探討部份，已蒐
集有關最近鄰居分類方法(或以樣本
為基礎的學習方法)、訓練樣本縮減方
法、分類器設計、機器學習理論、灰
關聯結構及相似度函數等相關重要
文獻，作為本研究的理論基礎與樣本
縮減方法研究設計之參考。 
(3) 已完成研究設計最近鄰居分類法中
有關樣本縮減的機制，同時尋找不同
應用領域之分類問題的訓練樣本及
測試樣本。將本研究所得到的分類結
果與人工分類的結果及其他各種樣
 10
[17] D. W. Aha, D. Kibler, and M. K. Albert, 
“Instance-based learning algorithms,” Machine 
Learning, vol. 6, pp. 37-66, 1991. 
[18] P. Njiwoua and E. M. Nguifo, “Examplar-based 
prototype selection for a multi-strategy learning 
system,” in Proc.of 11th IEEE International 
Conference on Tools with Artificial 
Intelligence, pp. 37-40, 1999. 
[19] C. C. Huang and H. M. Lee, "An 
Instance-based Learning Approach based on 
Grey Relational Structure," Applied 
Intelligence. (accepted) 
[20] D. Aha, “Lazy Learning”, Artificial 
Intelligence Review, vol. 11, pp.1-5. 1997. 
[21] R. Duda, P. Hart, and D. Stork. Pattern 
Classification. WileyInterscience, 2001. 
[22] J. Deng, “The theory and method of 
socioeconomic grey systems,” Social Sciences 
in China, vol. 6, pp. 47-60, 1984. (in Chinese) 
[23] J. Deng, “Introduction to grey system theory,” 
The Journal of Grey System, vol. 1, pp. 1-24, 
1989. 
[24] J. Deng, “Grey information space,” The Journal 
of Grey System, vol. 1, pp. 103-117, 1989. 
[25] R. Fisher, “The use of multiple measurements 
in taxonomic problems,” Annals of Eugenics 
Part 2, vol. 7, pp. 179-188, 1936. 
[26] H. M. Lee, C. C. Huang and C. Y. Chao, 
"Association Thesaurus Construction for 
Interactive Query Expansion based on 
Association Rule Mining," IEE Proceedings of 
Software (submitted). 
[27] H. M. Lee, C. C. Huang and W. T. Hung, 
"Mining Navigation Behaviors for Term 
Suggestion of Search Engines," IEEE 
Transactions on Systems, Man, and Cybernetics 
Part C: Applications and Reviews (submitted). 
[28] H. M. Lee, C. C. Huang and Y. C. Chen, 
"Thesaurus Based on Grey Theory for 
Interactive Query Expansion," in Proc. of 
Seventh Conference on Artificial Intelligence 
and Applications (TAAI-2002), Taiwan, pp. 
533-538, 2002. 
[29] C. C. Huang and H. M. Lee, "A Grey-based 
Nearest Neighbor Approach for Missing 
Attribute Values Prediction," Applied 
Intelligence, vol. 20, no. 3, pp. 239-252, 2004. 
[30] C. L. Blake and C. J. Merz, UCI Repository of 
machine learning databases 
[http://www.ics.uci.edu/~mlearn/MLRepository
.html]. Irvine, CA: University of California, 
Department of Information and Computer 
Science, 1998. 
[31] D. Lowe, “Similarity metric learning for a 
variable kernel classifier,” Neural Computation, 
7(1): pp.72-85, 1995. 
[32] C. Stanfill and D. Waltz, “Towards 
memory-based reasoning,” Communications of 
the ACM, vol. 29, no. 12, pp. 1213-1228, 1986. 
[33] M.A. Maloof and R.S. Michalski, “Selecting 
Examples for Partial Memory Learning,” 
Machine Learning, vol. 41, no. 1, pp. 27-52, 
2000. 
[34] D. W. Aha, “Case-Based learning algorithms,” 
in Proc. of the Case-Based Reasoning 
Workshop, pp. 147-159, 1991. 
[35] I. Witten and E. Frank, Data mining - practical 
machine learning tools and techniques with 
java implementations, Morgan Kaufmann, San 
Francisco, CA, 2000. 
[36] D. R. Wilson and T. R. Martinez, “Improved 
heterogeneous distance functions,” Journal of 
Artificial Intelligence Research, vol. 6, pp. 1-34, 
1997. 
[37] D. Kibler and D. W. Aha, “Learning 
Representative Exemplars of Concepts: An 
Initial Case Study,” in Proc. of the Fourth 
International Workshop on Machine Learning, 
Irvine, CA: Morgan Kaufmann, pp. 24-30, 
1987. 
[38] J. Zhang, “Selecting typical instances in 
instance-based learning,” in Proc. of the 9th 
International Conference on Machine Learning, 
pp. 470-479. Morgan Kaufmann, 1992. 
[39] D. R. Wilson and T. R. Martinez, “Instance 
pruning techniques”, Proc. of the Fourteenth 
International Conference Machine Learning, 
San Francisco, CA, pp. 403-411, 1997. 
[40] C. C. Huang, "A Novel Grey-Based Reduced 
NN Classification Method," Pattern 
Recognition. (accepted) 
[41] H. M. Lee, C. C. Huang and W. T. Hung, 
"Mining Navigation Behaviors for Term 
Suggestion of Search Engines," Journal of 
Information Science and Engineering. 
(accepted) 
[42] H. M. Lee, C. C. Huang and C. Y. Chao, 
"Association Thesaurus Construction for 
Interactive Query Expansion based on 
Association Rule Mining," Journal of 
Information Science and Engineering. 
(accepted) 
 
 
 
 
1. Introduction 
Instance-based learning (IBL) [1,2,24,28], also known as memory-based learning [37] or 
case-based reasoning [3], has become increasingly popular for pattern recognition and 
machine learning over the past ten years.  An IBL system normally gathers a set of 
training instances, each having n attributes and an output label (or value).  To classify a 
new, unseen instance, some similarity functions (such as Euclidean distance [1]) are first 
adopted to determine the relationships (i.e., the ‘nearness’ or ‘similarity’) among the new 
instance and all training instances.  Accordingly, the output label of the new instance is 
assigned as that of its “nearest＂ training instance or instances.  Using the nearest 
neighbor rule [7,8,18,43], instance-based learning can yield excellent performance, 
compared with other pattern classification algorithms. 
Generally, because all training instances are stored before learning in the IBL system, the 
storage rises along with the number of training instances.  Moreover, in such a case, a new, 
unseen instance takes a long time to classify because all training instances have to be 
considered when determining the ‘nearness’ or ‘similarity’ among instances.  To solve 
these problems, various instance pruning [20,22,25,28,29,32,41] or nearest neighbor editing 
[5,6,19,21,26,33,34,39] techniques have been examined.  That is, only some training 
instances in the original training set are adopted to categorize new, unseen instances.  In 
[22], the Condensed Nearest Neighbor rule has been developed to choose representative 
instances from the original set of training instances for pattern classification.  In this 
algorithm, a set S of p instances corresponding to p classes in a specific domain is first 
randomly chosen from the original training set.  If a training instance i is misclassified by 
 12
which have the same distance from x as the kth nearest neighbor of x.  In [17], the 
sensitivity of the sample size of various edited nearest neighbor rules is studied. 
In [11] and [12] ([11] is an extension of [12]), an instance pruning approach with nearest 
neighbor editing and condensing has been proposed based on the combination of the 
proximity graph (PG) based editing algorithm and the minimal consistent set (MCS) 
condensing [9] method.  This approach is experimentally shown to yield good 
performance (including training set size reduction rate and classification accuracy).  Here, 
MCS condensing [9] is based on the concepts of Nearest Unlike Neighbor Subset [10].  
This subset is determined by a set of instances which are the nearest unlike neighbors of 
one or more given instance.  Similarly, another instance selection method for the nearest 
neighbor rule has been presented based on proximity graph (PG) in [35]. 
In [30], another instance editing and condensing techniques have been developed to 
improve the quality of training dataset in the context of Markov models.  Accordingly, the 
reduced dataset is used in the K-NN HMM system.  In [36], a comparative analysis of fast 
nearest neighbor search algorithms and data reduction algorithms has been investigated for 
efficient NN reduced classification.  Generally, fast nearest neighbor search algorithms 
reduce the number of computations.  However, they still have high storage requirements.  
Meanwhile, instance pruning methods, in many situations, yield higher classification 
accuracy than that of the fast nearest neighbor search algorithms. 
Moreover, an instance pruning method, namely supervised clustering editing [16], has 
been studied.  In this work, the original dataset will be replaced by a set of cluster 
prototypes, which is obtained based on a novel clustering approach - supervised clustering.  
This method is experimentally shown to perform better than the method presented in [40].  
 14
The rest of this paper is organized as follows.  The concept of grey relational structure 
is reviewed in Sections 2.  Section 3 proposes a novel reduced classification method based 
on the grey relational structure.  In Section 4, experiments performed on nine datasets are 
discussed.  Finally, Section 5 gives the conclusions. 
2. Grey Relational Structure 
A previous investigation from this author [24] presents an instance-based learning 
method based on grey relational structure.  The relational structure is based on grey 
relational analysis [13,14,15,31] and determines the relationships among instances by 
computing the grey relational coefficient (GRC) and the grey relational grade (GRG), to  
Assume that a set of m instances {v1, v2, v3, …, vm+1} is given, where v1 denotes the 
referential instance and v2, v3, …, vm+1 are the compared instances.  Each instance vi 
includes n attributes and is represented as vi = (vi(1), vi(2), …, vi(n)).  The grey relational 
coefficient can be written as, 
( ) ,
max)()(
maxmin
)(),(
1
1 ∆+−
∆+∆= ζ
ζ
pvpv
pvpvGRC
i
i  
(1) 
where )()(minminmin 1 kvkv jkj −=∆ ∀∀ , )()(maxmaxmax 1 kvkv jkj −=∆ ∀∀ , ζ ∈[0,1], i = j = 2, 3, …, 
m+1, and k = p = 1, 2, …, n. (In the initial version of grey relational analysis [13,14,15,31], 
the coefficient ζ  is used to determine the ratio between )()(minminmin 1 kvkv jkj −=∆ ∀∀  and 
)()(maxmaxmax 1 kvkv jkj
−=∆
∀∀ .  That is, for each attribute p of instance v1, the value of each 
 can be uniformly tuned.  Generally, ( )(),(1 pvpvGRC i ) 5.0=ζ [14], as in this work.  
 16
between v1 and v3; otherwise the difference between v1 and v2 is the larger.   
 
Based on the degree of GRG, the grey relational structure (GRS) from the viewpoint of 
instance v1 can be identified as follows: 
( ) )( yyyvGRS m1,,21,11m,1 K=  (3) 
where ( ) ( ) ( )yvGRGyvGRGyvGRG m1,21,11, 111 ≥≥≥ K , {v∈y r1 2, v3, v4, …, vm+1}, r=1, 2, …, m, 
and  if a b. yy b1
a
1 ≠ ≠
The grey relational structure (GRS) from the viewpoint of instances v2, v3, …, vm+1  can 
be similarly derived as follows: 
( ) )( y qy qyqvGRS q m,,2,1m, K=  (4) 
where q=2, 3, …, m+1, ( ) ( ) ( )yqvGRGyqvGRGyqvGRG qqq m,2,1, ≥≥≥ K , {v∈yrq 1, v2, v3, …, vm+1}, 
v≠y rq q, r=1, 2, …, m, and  if ay qyq
ba ≠ ≠ b. 
In Eq. (4), the GRG between observations vq and  exceeds those among vyq
1
q and other 
observations , , …, .  That is, the difference between vyq
2 yq
3 y q
m
q and  is the 
smallest. 
yq
1
Consequently, a graphical structure, namely h-level grey relational structure (h≦m), can 
be obtained as follows to describe the relationships between referential instance vq and 
every other instance, where the total number of nearest instances of referential instance vq 
 18
label of instance vq based on the nearest neighbor rule [7,8,18].  Additionally, instance 
, with the largest similarity, is the nearest neighbor of instance vyq
1
q in the one-level grey 
relational structure.  Hence, a new, unseen instance can be categorized based on its nearest 
instance in the one-level grey relational structure or its nearest instances in the h-level grey 
relational structure (The nearest neighbor rule [7,8,18]).  Obviously, only h inward edges 
connected to instance i in the above h-level grey relational structure are needed to classify a 
new, unseen instance i.    
3. A Grey-Based Reduced NN Classification Method 
This section presents a novel grey-based reduced NN classification method for pattern 
recognition.   
Firstly, in the h-level grey relational structure, an inward edge connected from instance 
vq to instance  (r=1, 2, …, h) can be defined as a correct inward edge if the class labels 
of instances v
yrq
q and its nearest instance  (r=1, 2, …, h) are the same; otherwise, the 
inward edge can be defined as a incorrect one if the class labels of instances v
yrq
q and its 
nearest instance
 
 (r=1, 2, …, h) are different. yrq
In the above h-level grey relational structure, an instance may not be linked via any 
inward edges from other instances.  Restated, this instance is seldom employed in 
determining the class labels of other instances, implying that it is probably a good choice 
for instance pruning in a learning system.  The inward edges of every training instance in 
 20
Let Tl,k denote the set of training instances having no less than l inward edges in the 
above k-level grey relational structure, where Tl,k ⊆ T, 1≦l≦w(T,k) and w(T,k) denotes the 
maximum number of inward edges of an instance in the k-level grey relational structure of 
T. (Clearly T0,k = T for each k; while || Ti,k || ≦ || Tj,k || for every i > j, where || Ti,k || 
represents the total number of training instances in Ti,k.)  The training set Tl,k used for 
learning indicates that the k nearest neighbors of an instance are considered to evidence the 
class label of that instance.  By considering different values of || Ti,k ||, the size of the 
training set utilized for reduced NN classification here can be varied.  Additionally, the 
number of nearest neighbors (i.e., k) employed to evidence the class label of an instance is 
also considered in the proposed instance pruning method. 
Let Rl,k = T- Tl,k.  Consider that Tl,k and Rl,k are adopted as training and test sets, 
respectively.  The classification accuracy ACCl,k corresponding to each Tl,k and Rl,k can 
then be obtained by using a classification approach.  (Restated, each Rl,k is adopted as a 
test set to validate the corresponding training set Tl,k).  Here, the value of ACCl,k is used to 
evaluate the corresponding classification ability of training set Tl,k.  In other words, a 
training set Tu,v with high or acceptable classification accuracy ACCu,v will be suitable for 
the classification tasks.  Obviously, by considering different values of || Ti,k ||, the size of 
the training set Ti,k and the corresponding classification accuracy ACCl,k, a reduced and 
small training set with high classification accuracy can be finally selected.  Consequently, 
this training set is adopted for the learning tasks.  Restated, a training set V is selected 
from all Ti,k for instance pruning if acceptable classification accuracy with a few training 
instances can be obtained.   
 22
 3.2 Algorithm 2: Grey-Based Reduced NN Classification Algorithm (By Considering 
the Classification Ability of Each Inward Edge) 
Similarly, another grey-based reduced NN classification algorithm with the above two 
main instance pruning ideas (i.e., by considering the classification ability of each training 
instance) is introduced.  In this algorithm, the training instances with ‘no’ or ‘few’ correct 
inward edges are excluded from the original training set for the learning tasks.  By 
contrast, in Algorithm 1, the training instances with ‘no’ or ‘few’ inward edges are excluded 
from the original training set for the learning tasks. 
Let Tl,k* denote the set of training instances having no less than l correct inward edges in 
the above k-level grey relational structure, where Tl,k* ⊆ T, 1≦l≦w*(T,k) and w*(T,k) 
denotes the maximum number of correct inward edges of an instance in the k-level grey 
relational structure of T. (Clearly T0, k*= T for each k; while || Ti, k* || ≦ || Tj, k* || for every i 
> j, where || Ti, k* || represents the total number of training instances in Ti, k*.)  The training 
set Tl, k* used for learning indicates that the k nearest neighbors of an instance are 
considered to evidence the class label of that instance.  By considering different values of 
|| Ti, k* ||, the size of the training set utilized for reduced NN classification here can be varied.  
Additionally, the number of nearest neighbors (i.e., k) employed to evidence the class label 
of an instance is also considered in the proposed instance pruning method. 
Let Rl, k* = T- Tl, k*.  Consider that Tl, k* and Rl, k* are adopted as training and test sets, 
respectively.  The classification accuracy ACCl, k* corresponding to each Tl, k* and Rl, k* can 
then be obtained by using a classification approach.  (Restated, each Rl, k* is adopted as a 
 24
instance-based learning approach presented in [24].  (Restated, each Rl, k* is 
adopted as a test set to validate the corresponding training set Tl, k*). 
Step2. Let CS * represent a set containing each training set Tl, k* with ACCl, k* greater than 
α  ,where α is set to the average classification accuracy calculated by applying 
T as the validating data set in ten-fold cross-validation [38]).  A training set V * is 
chosen with the smallest number of instances from CS * as the final training set to 
classify the new instance x1.  Restated, the above-mentioned instance-based 
learning approach [24] is adopted again with the above final training set V * for the 
pattern classification task. 
4. Experimental Results 
In this section, experiments performed on nine application domains (UCI Repository of 
machine learning databases [4]) are reported to demonstrate the performance of the above 
two proposed instance pruning algorithms.  These nine application domains include 
Ionoshpere, Iris, Glass, Shuttle, Vowel, Letter, New-Thyroid, Segmentation and Wine.  
Among these domains, Iris is the smallest classification problem with 150 instances and 
Shuttle is the largest classification problem with 43500 instances. 
In the experiments, ten-fold cross validation [38] was used and applied ten times for each 
application domain.  Restated, the entire data set of each application domain was divided 
equally into ten parts in each trial; each part was utilized once for testing and the remaining 
parts were utilized for training.  The above two proposed instance pruning algorithms 
were employed respectively to select a suitable subset from each original training set using 
 26
Experiments performed on nine application domains are reported to demonstrate the 
performance of the proposed reduced classification method.  The proposed reduced 
classification method clearly yields high performance with small training set, compared 
with other existing instance pruning techniques. 
Acknowledgment 
The authors would like to thank the National Science Council of the Republic of China, 
Taiwan for financially supporting this research under Contract No. NSC 
94-2213-E-022-006. 
References 
[1] D. W. Aha, D. Kibler, M. K. Albert, Instance-based learning algorithms, Machine 
Learning, 6 (1991) 37-66. 
[2] D. W. Aha, Tolerating noisy, irrelevant and novel attributes in instance-based learning 
algorithms, Int. J. Man-Machine Studies, 36 (1992) 267-287. 
[3] N. Arshadin, I. Jurisica, Data Mining for Case-Based Reasoning in High-Dimensional 
Biological Domains, IEEE Transactions on Knowledge and Data Engineering, 17 
(2005) 1127-1137. 
[4] C. L. Blake, C. J. Merz, UCI Repository of machine learning databases 
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of 
California, Department of Information and Computer Science, (1998). 
 28
[14] J. Deng, Introduction to grey system theory, The Journal of Grey System, 1 (1989) 
1-24. 
[15] J. Deng, Grey information space, The Journal of Grey System, 1 (1989) 103-117. 
[16] C. Eick, N. Zeidat, R. Vilalta, Using Representative-Based Clustering for Nearest 
Neighbor Dataset Editing, in Proc. of Fourth IEEE International Conference on Data 
Mining (ICDM), (2004) 375-378. 
[17] F. J. Ferri, J. V. Albert, E. Vidal, Considerations about sample-size sensitivity of a 
family of edited nearest-neighbor rules, IEEE Trans. on Systems, Man, and 
Cybernetics, Part B, 29 (1999) 667-672. 
[18] E. Fix, J. L. Hodges, Discriminatory analysis: nonparametric discrimination: 
consistency properties, Technical Report Project 21-49-004, Report Number 4, USAF 
School of Aviation Medicine, Randolph Field, Texas, (1951). 
[19] K. Fukunaga, J.M. Mantock, Nonparametric data reduction, IEEE Trans. on Pattern 
Analysis and Machine Intelligence, 6(1), (1984) 115-118. 
[20] G.W. Gates, The Reduced Nearest Neighbor Rule, IEEE Trans. Information Theory, 
18 (1972) 431-433. 
[21] K. C. Gowda, G. Krishna, “The condensed nearest neighbor rule using the concept of 
mutual nearest neighborhood,” IEEE Trans. on Information Theory, 24(4), (1979) 
488-490. 
[22] P.E. Hart, The Condenced Nearest Neighbor Rule, IEEE Trans. Information Theory, 
14 (1968) 515-516. 
[23] K. Hattori, M. Takahashi, M., A new edited k-nearest neighbor rule in the pattern 
classification problem, Pattern Recognition, 33, (2000) 521-528. 
 30
[34] G. L. Ritter, H. B. Woodruff, S. R. Lowry, T. L. Isenhour, An algorithm for a selective 
nearest neighbour decision rule”, IEEE Trans. on Information Theory, 21, (1975), 
665-669. 
[35] J. S. Sánchez, J. M. Sotoca, F. Pla, Efficient nearest neighbor classification with data 
reduction and fast search algorithms, in Proc. of IEEE International Conference on 
Systems, Man & Cybernetics, (2004) 4757-4762. 
[36] J. S. Sánchez, F. Pla, F. J. Ferri, Prototype selection for the nearest neighbour rule 
through proximity graphs, Pattern Recognition Letters, 18(6), (1997) 507-513. 
[37] C. Stanfill, D. Waltz, Towards memory-based reasoning, Communications of the 
ACM, 29 (1986) 1213-1228. 
[38] M. Stone, Cross-validatory choice and assessment of statistical predictions, Journal of 
the Royal Statistical Society B 36 (1974), 111-147. 
[39] I. Tomek, An experiment with the edited nearest neighbor rule, IEEE Trans. on 
Systems, Man, and Cybernetics, 6(6), (1976) 448-452. 
[40] D. L. Wilson, Asymptotic properties of nearest neighbor rules using edited data, IEEE 
Trans. on Systems, Man and Cybernetics, 2 (1972) 408-421. 
[41] D.R. Wilson, T.R. Martinez, Reduction Techniques for Instances-based Learning 
Algorithm, Machine Learning, 38 (2000) 257-286. 
[42]  J. Zhang, Selecting typical instances in instance-based learning, in Proc. of the 9th 
International Conference on Machine Learning, (1992) 470-479. 
[43] W. Zheng, L. Zhao, C. Zou, Locally nearest neighbor classifiers for pattern 
classification, Pattern Recognition, 37 (2004) 1307-1309. 
 
 32
 Fig. 1. 1-level grey relational structure of 18 instances.  Notably, four attributes of each 
instance are used to construct the 1-level grey relational structure. 
 
 34
Table 1. 18 instances randomly selected from the well-known Iris dataset [4] 
No. of Instance 
Sepal 
Length 
(SL) 
Sepal 
Width 
(SW) 
Petal 
Length 
(PL) 
Petal 
Width 
(PW) 
Class 
Label 
1 4.7 3.2 1.3 0.2 Setosa 
2 5.4 3.9 1.7 0.4 Setosa 
3 4.9 3.1 1.5 0.1 Setosa 
4 4.3 3.0 1.1 0.1 Setosa 
5 4.8 3.4 1.9 0.2 Setosa 
6 5.0 3.5 1.6 0.6 Setosa 
7 6.9 3.1 4.9 1.5 Versicolor 
8 4.9 2.4 3.3 1.0 Versicolor 
9 6.7 3.1 4.4 1.4 Versicolor 
10 6.1 2.8 4.0 1.3 Versicolor 
11 6.1 2.8 4.7 1.2 Versicolor 
12 5.5 2.4 3.8 1.1 Versicolor 
13 5.8 2.8 5.1 2.4 Virginica 
14 6.9 3.2 5.7 2.3 Virginica 
15 5.6 2.8 4.9 2.0 Virginica 
16 6.2 2.8 4.8 1.8 Virginica 
17 6.4 2.8 5.6 2.2 Virginica 
18 6.7 3.0 5.2 2.3 Virginica 
 
Table 2. Comparison with other existing instance pruning techniques 
Dataset RT3 ICPL Proposed Algorithm 1 
Proposed Algorithm 
2 
 Average accuracy 
Data 
retention 
rate 
Average 
accuracy
Data 
retention 
rate 
Average 
accuracy
Data 
retention 
rate 
Average 
accuracy 
Data 
retention 
rate 
Ionoshpere 0.869 0.068 0.878 0.048 0.890 0.065 0.892 0.067 
Iris 0.947 0.080 0.947 0.043 0.951 0.047 0.951 0.052 
Glass 0.672 0.211 0.677 0.154 0.682 0.149 0.695 0.158 
Shuttle 0.998 0.007 0.998 0.003 0.998 0.003 0.999 0.004 
Vowel 0.956 0.293 0.923 0.151 0.957 0.151 0.962 0.153 
Letter 0.696 0.282 0.692 0.179 0.811 0.178 0.825 0.183 
New-Thyroid 0.948 0.111 0.935 0.031 0.969 0.030 0.969 0.030 
Segmentation 0.949 0.091 0.937 0.040 0.962 0.063 0.965 0.071 
Wine 0.938 0.114 0.960 0.040 0.960 0.042 0.961 0.043 
 
 
 
 
 36
 38 
engine-related problems persist, such as short query term, word mismatch, and partial 
match [5,7,12].  To address these problems, query expansion (also called query 
suggestion or term suggestion) [11,12,15,17,20] is used to help users finding the 
information they require effortlessly. 
In general, search engines return result lists for query terms entered by users based 
on matching similarities [22,24,25] and ranking algorithms [1,2,3,4,6].  Traditional 
query expansion methods analyze the relationships among user query term and 
documents to automatically or interactively generate some suggested terms for users.  
Reformulating the initial query term of a user by adding suggested terms may enhance 
the precision and recall rates of the retrieved documents.  Nevertheless, the fact that 
suggested terms are generated from documents may create the following disadvantages 
[20]: 1) sensitivity to document quality, 2) ranking algorithm bias, 3) difficulty in 
comprehending suggested terms, and 4) poor capability to handle new associations of 
terms.  These disadvantages may influence query expansion performance. 
This work proposes a novel method of term suggestion by considering user 
navigation behavior.  When users click on certain documents, it is assumed that they are 
interested in these documents [18].  Different query terms are thus assumed to be 
related if users click on the same document after entering them [18].  Based on this 
concept, the navigation behaviors of users derived from the access logs in the search 
engines are mined to suggest query terms.  The suggested terms in the novel method are 
extracted from the access logs based on the popularities of terms and clicked hyperlinks.  
Since the access logs contain user information, such as navigation behavior, the 
candidate terms are user-oriented. 
Regarding the proposed term suggestion, the suggested terms are originally entered 
by users, and thus must be recognized and comprehended by users in advance, i.e., these 
suggested terms are implicitly verified by the users in advance.  Consequently, these 
suggested terms must be considered reasonable for other users to comprehend.  
Therefore, the quality of the suggested terms in the novel method is expected to be 
reliable.  Furthermore, the proposed method does not consider the relationships among 
query term and documents as conventional query expansions do.  Consequently, the 
novel method is not influenced by document quality.  Besides, log-based term 
extraction used herein is clearly adaptive.  The proposed term suggestion can thus 
 38 
 
 40 
Title Short Description Web Site Category 
AI Artificial Intelligence AI Artificial Intelligence AI 
Artificial Intelligence DVD 
for ratings reasons, go to 
filmratings.com, 
parentalguide.org and 
mpaa.org. © 2001 Warner 
Bros. ... 
Arts > Movies > Titles > A 
> A.I. 
AI on the Web AI on the Web. This page 
links to 874 pages around the 
web with information on 
Artificial Intelligence. ... 
CiteSeer: Google: Overview 
of AI. Reference. ... 
Computers > Artificial 
Intelligence 
CMU Artificial Intelligence 
Repository 
CMU Artificial Intelligence 
Repository. Notice - the 
official home of the comp.ai 
Artificial Intelligence FAQ is 
now at UCLA. ... UCLA AI 
FAQ. ... 
Computers > Artificial 
Intelligence 
 
Figure 1 gives an example of the concept employed herein for term suggestion.  In 
this figure, numerous users click on the same hyperlink corresponding to Web site “911 
Crime”, via two different query terms, “Bin Laden” and “911”, respectively.  The Web 
site “911 Crime” can then be assumed to contain information required by these users.  
Meanwhile, the required information in the Web site “911 Crime” is highly related to the 
query terms “Bin Laden” and “911”.  Assuming that query terms “Bin Laden” and 
“911” are highly related in this case is thus reasonable.  Therefore, if other users enter 
the query term “911”, then the term “Bin Laden” can also be suggested to them as an 
alternative search term.  Restated, highly related terms are grouped for term suggestion 
in this work. 
 
 
 
 
 
 
 
 
 40 
 
 42 
 
 
Users Access logs
Term Refiner
Online
Suggestion
Process
User
Interface
Log DB
Log Extractor
Preprocessing
Rules
Suggestion
Term DB
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 2.  System architecture of the proposed term suggestion. 
 
3.1 Log Extractor 
 
The Log Extractor extracts the original access logs and stores the extracted results 
in the Log DB.  For term suggestion, the Log Extractor only extracts the queries of 
users, along with their clicked hyperlinks (i.e., Web sites or pages), and other 
information such as date and IP.  Noises are filtered using the Log Preprocessing 
component of the Term Refiner.  The dataset used in this work is WWW server access 
logs (from Nov. 18 to Nov. 25, 2001) derived from the YAM portal site [25] (a portal 
site that collects numerous Chinese Web sites).  A database Log DB with about three 
million records is created by extracting the original access logs. 
 
3.2 Term Refiner 
 
The Term Refiner, as shown in Figure 3, contains two components: Log 
Preprocessing and Term Weighting.  The Log Preprocessing component processes the 
Log DB and builds the relations among query terms and hyperlinks (i.e., Web sites or 
 42 
 
 44 
information.  This work hypothesizes that removing single Chinese character 
searches will improve the quality of the suggested terms. 
c. Removal of infrequently-used terms.  In general, users often enter hot query terms 
(i.e., frequently-used query terms) to find the desired information.  To reduce the 
complexity in the proposed method, infrequently-used query terms are removed, 
that is, only hot query terms are retained.  This step uses a threshold α  to 
identify the hot query terms.  All terms with a number of queries below α  will 
be removed.  The value of α  is calculated as, 
 
n
STK
n
i
i∑
== 1α  (1) 
 
where STKi denotes the number of queries for term i, and n represents the number 
of query terms corresponding to a clicked hyperlink (i.e., Web site or page).  In 
other words, α is an average number, implies that, we do not need to predefine the 
value of α.  Corresponding to various clicked hyperlinks, a query term might have 
different thresholds.  In this manner, appropriate and user-oriented search terms 
for each clicked hyperlink can be derived.  If a Web site is unpopular, its 
corresponding query terms are rarely used and with fewer query times.  However, 
unpopular Web sites do not act as bad sites.  For example, some fewly used Web 
sites may contain specialized professional knowledge and thus attract little 
attention from most users.  Therefore, rarely-used web sites must also be 
considered here (i.e., calculating different thresholds for various query terms) to 
ensure that appropriate search terms are suggested to all users. 
 
Regarding the operation of the Online Suggestion Process, the record size of the 
Suggestion Term DB must be carefully controlled.  Therefore, according to the above 
rules, the Log Preprocessing component aims not only to filter noises but also control the 
record size.  Furthermore, these rules are adaptive and thus can enhance the 
performance of the Term Refiner. 
 
 
 44 
 
 46 
general, a query term i with Ii close to one means that users can quickly find their 
required information, i.e., web site or page with highest ranking, after entering 
term i.  This term is thus appropriate to be a suggested term.  Thus, the factor 
fitness can also be considered for term weighting.  Obvously, this point requires 
the assumption that the ranks provided by the web portals are reliable.  As 
mentioned earlier, clicked behaviors of users will be recorded in the WWW access 
logs.  In other words, the number of times that users have clicked on hyperlink j 
after entering query term i in all of the WWW access logs will be calculated. 
 
By considering the above two factors, i.e., normalized support and fitness, the 
normalized weight  of each query term i corresponding to hyperlink j is calculated 
by using the following formula: 
ijW
 
 
2
I
1NS
W i
ij
ij
+
=  (4) 
 
where NSij is the normalized support of query term i corresponding to clicked hyperlink j, 
and Ii is the fitness of query term i.  Consequently, each pair of terms and clicked 
hyperlinks, which is stored in the Suggestion Term DB, is associated with its own weight.  
The normalized weight  is then used in the Online Suggestion Process to rank 
suggested terms.  That is, query terms with high normalized-weight will be suggested to 
users with high ranking.  The operation of Log Preprocessing in Section 3.2.1 will be 
executed to handle new Web access logs.  Accordingly, Term weighting is performed 
to calculate the weight of each query term in access logs. 
ijW
 
 
 
 
 
 46 
 
 48 
5. EXPERIMENTAL RESULTS 
In this section, a search engine prototype is developed to show the results of the 
proposed term suggestion.  Accordingly, experimental results are given to demonstrate 
the performance of the proposal presented herein. 
 
5.1 Search Engine Prototype 
 
Document
DB
Term
Refiner
Query Agent
Log DB
Adaptive
Agent
Online
Suggestion
Process
Suggestion
Results Search Results
Users
Ranking
User Interface
Log
Extractor
Query term
Access logs
Preprocessing
Rules
Suggestion
Term DB
Query term
HitRank sorting
 
 
Fig. 4.  The proposed search engine prototype. 
 
Figure 4 illustrates the architecture of the proposed search engine prototype, which 
is built by using a Windows 2000 platform, with an IIS 5.0 WWW server and a SQL 
2000 database.  Meanwhile, the programming language used to build the system is PHP 
4.0.  Since the prototype is built for experimental use, this work only implements basic 
search engine functions (such as keyword matching), that is, users can only submit their 
query terms to the system.   
After a user entering a search term, the proposed prototype will provide two 
 48 
 
 50 
Query Term: CD-ROM 
Web Matches: 66   Page Number: 1 of 7      HitRank 
Suggested Query Terms: 
☉GISH (1) ☉AACOM (1) ☉ACER (0.57) ☉FUJITSU (0.44) ☉SCSI (0.38) ☉ELOHA (0.33) 
☉CD-Recorder (0.28) ☉ACER Technology (0.23) ☉MO (0.21)  
 
(a) 
Web Matches: 
 ACER Technology [1.42169] – Agent of High-performance desktop PCs, Notebooks, 
Tablet PC and Monitors. 
 Chung Sheng Computer Ltd. [1.19446] – Located in Kaohsiung, Retailer of computers 
and related products, including CPU, RAM, Motherboard, video accelerator, monitor, 
keyboard, CD-ROM, Speaker System, sound cards, modem, scanner, mouse, etc. 
 Advance Application Technology Inc. [1.12365] – R&D designer, manufacturer and 
exporter of digital products, including LCD monitor, DVD-Player and Portable 
DVD-Player. 
 wangman.boys.com.tw [1.04725] – introduces MP3 CD-ROM, Walkman and 
DVD-Player. 
 www.yuanyu.com.tw [0.99792] – Agent of digital products, including camera, CD/RW, 
microscope and karaoke player. 
 Yung Fu Electrical Appliances Corp., Ltd. [0.89794] – Manufacturer of DVD-Player, 
Amplifier, DVD Receiver, Speaker System, Slim DVD-Player and portable DVD. 
 www.f8.com.tw [0.85776] – Retailer of DVD-Player, Amplifier and Speaker System. 
 www.3city.com.tw [0.8203] – Product Online Order, including TV, telephone, digital 
camera and DVD-Player. 
 Storage Solutions Company [0.81789] – Manufacturer of Mobile Rack, RAID System 
and External Enclosure for CD-ROM, Located in Hisn-Tien, Taipei, Taiwan. 
 
First Page 1 2 3 4 5 6 7 Last Page 
 
Copyright 2001-2002 by NTUST, Fuzzy and Neural Networks Lab, YAM 
Any problem please contact Webmaster 
SINCE: 2001/7/1 
 (b) 
Fig. 5.  An example of result list of the proposed search engine prototype.  (a) Suggested Terms.  
(b) Result list with HitRank ranking. 
 
5.2 Precision and Recall Rates 
 
The dataset used herein for term suggestion is WWW server access logs (from Nov. 
18 to Nov. 25, 2001) derived from the YAM portal site [25] (a famous portal site that 
collects numerous Chinese Web sites).  Extracting the original access logs and storing 
the extracted results in the Log DB creates a database around three million records.  In 
this database, most query terms are in Chinese.   
To examine the relationships among query terms and clicked hyperlinks, ten 
hyperlinks are randomly selected from the database; meanwhile, corresponding to each 
 50 
 
 52 
To demonstrate the performance of the proposed term suggestion, the following two 
rates, namely recall rate and precision rate, were used. 
 
Number of Retrieval and Relevant Documents  
Precision =                     (5) 
Number of Total Retrieval Documents  
Number of Retrieval and Relevant Documents  
Recall =                                         (6) 
Number of Total Relevant Documents 
 
 
As stated earlier, after entering a query term, a user will be presented with a result 
list and suggested terms.  The proposed approach allows the users to choose one 
suggested term.  Once users use the suggested terms with the logic ‘AND’ or ‘OR’ 
operations to reformulate the initial query term, the reformulated query term will be sent 
to the query server again to return search results.  Consequently, various presented 
results corresponding to different query terms (i.e. Using query term A or using query 
terms A and B), will have different recall rates.  Thus, we can draw the curve of the 
precision rates corresponding to these various presented results with recall rates.  In the 
experiments, two kinds of curves can be obtained when the original query term is 
reformulated by adding the suggested terms of the proposal used herein or not.  
Experimental results, as shown in Figure 6, demonstrate that the average precision rates 
can be enhanced (13.67%) when the proposed term suggestion was applied.  That is, the 
proposal presented herein can help users quickly finding the information they require. 
 
 52 
 
 54 
Table 3.  A comparison of suggested terms in the proposed approach and OPENFIND 
Original 
query term 
Suggested terms of the proposed 
approach 
Suggested terms of OPENFIND 
化石 (Fossil) 礦物 (Mineral) 液化石油氣 (Liquefied Petroleum 
Gas)、高壓氣體 (High Pressure Gas)、
砂岩 (Sandstone)、儲槽 (Store)、頁岩 
(Shale)、恐龍化石 (Dinosaur Fossil)、
活化石 (Living Fossil)、化石燃料 
(Fossil Fuel) 
光碟機 
(CDROM) 
志旭 (GISH)、德巍 (AACOM)、宏碁 
(Acer)、富士通 (FUJITSU)、SCSI、
ELOHA、燒錄器 (CD Recorder)、宏碁
科技 (Acer Tech.)、MO、ACER 
音軌 (Soundtrack)、光碟片 (Compact 
Disc)、挑片 (Unreadable Disc)、抓音軌 
(Extract Soundtrack)、虛擬光碟 (Virtual 
CDROM)、光牒機 (CDROM)、磁碟機 
(Disk)、硬碟機 (Hard Disk)、軟碟機 
(Floppy Disk) 
液晶螢幕 
(LCD) 
LCD 閃光燈 (Flash Lamp)、快門 (Camera 
Shutter)、自動對焦 (Automatic focus)、
光圈 (Diaphragm)、記憶卡 (Memory 
Card) 
阿貴 
(A-Kuei) 
諾基亞下載城 (Download City of 
Nokia)、流氓兔 (Mashi Maro)、蕃薯藤
動畫 (YAM comic)、手機鈴聲 (Ringing 
of Cellular Phone)、FLASH、諾基亞 
(Nokia)、詌譙龍 (Gan Giau Long) 
阿貴網站 (Web Site of A-Kuei) 
911 911 事件 (911 Event)、驚爆 911、美國
911 (America 911)、保時捷 (Porsche)、
美國世貿中心 (America World Trade 
Center)、五角大廈 (Pentagon)、
ETTODAY、汽車圖片 (Car Picture)、
賓拉登 (Bin Laden)、跑車 (Sport Car)
N/A 
流星雨 
(Meteor 
Shower) 
台北市立天文科學教育館 (Taipei 
Astronomical Museum)、天文館 
(Planetarium)、台北市立天文館、台北
市立天文台、獅子座流星雨 (The Leo 
Meteor Shower)、天文台 
(observatory)、九大行星 (Nine 
Planets)、F4、天文科學教育館、月蝕 
(Lunar Eclipse) 
流星 (Meteor)、獅子座流星雨 (The Leo 
Meteor Shower)、慧星 (Comet)、流星
群 (Meteor Group)、獅子座流星 (The 
Leo Meteor)、火流星 (Bolide)、英仙座
流星雨 (Perseus Meteor Shower)、雙子
座流星雨 (Gemini Meteor Shower) 
 
 
 54 
 
 56 
can identify new associations of terms, and the old ones will be ranked in the bottom of 
the result list or even filtered out.  For example, the access logs used herein are 
collected from Nov. 18 to Nov. 25, 2001.  Regarding the query term “911”, the 
suggested terms highly related to the terrorist attack on Sep. 11, 2001 are ranked in the 
top of the result list.  The terms suggested by the proposed approach are thus clearly 
adaptive.  By contrast, OPENFIND doesn’t suggest any candidate terms regarding the 
original query term “911”. 
 
 
6. CONCLUSIONS 
This paper proposed a novel term suggestion method for users to quickly find the 
desired information.  A co-clicked behavior based term suggestion is presented to 
suggest search terms.  Analyzing the co-clicked behaviors of users in the access logs for 
term suggestion eliminates the need to perform textual analysis.  The proposed 
approach thus provides some positive characteristics that previous approaches neglected, 
including content independent, adaptability, and extensibility.  In addition, limitations 
of current search engines such as problems of word mismatch and partial match can also 
be overcome.  Here, a search engine prototype is also developed to show the results of 
term suggestion.  Experimental results demonstrate that the precision rates of 
information retrieval systems can be improved by using the suggested terms of the 
proposal presented herein.  The proposed approach also performs better than another 
famous Chinese term suggestion system, OPENFIND. 
 
 
REFERENCES 
1. S. Brin, L. Page, “The Anatomy of a Large-Scale Hypertextual Web Search Engine,” 
Computer Networks and ISDN Systems, Vol. 30, pp. 107-117, 1998. 
2. S. Brin, L. Page, “The PageRank Citation Ranking: Bringing Order to the Web,” 
 56 
 
 58 
Master Thesis, National Taiwan University of Science and Technology, 2002. 
15. M. Mitra, A. Singhal, C. Buckley, “Improving Automatic Query Expansion,” 
Proceedings of 21th Annual International ACM SIGIR Conference on Research and 
Development in Information Retrieval, pp. 206-214, August 1998. 
16. C. Silverstein, M. Henzinger, H. Marais, M. Moricz, Analysis of a Very Large 
AltaVista Query Log, SRC Technical Note 1998-014. 
17. J. Wei, S. Bressan, B. C. Ooi, “Mining Term Association Rules for Automatic 
Global Query Expansion: Methodology and Preliminary Results,” Proceedings of 1st 
International Conference on Web Information Systems Engineering, Vol. 1, pp. 
366-373, 2000. 
18. J. R. Wen, J. Y. Nie, H. J. Zhang, “Query Clustering Using User Logs,” ACM 
Transactions on Information Systems, Vol. 20(1), pp. 59-81, January 2002. 
19. B. Xu, W. Zhang, H. Yang, W. C. Chu, “A Rough Set Based Self-Adaptive Web 
Search Engine,” Computer Software and Applications Conference (COMPSAC 2001), 
pp. 377-382, 2001. 
20. J. Xu, W. B. Croft, “Query Expansion using Local and Global Document 
Analysis,” Proceedings of 19th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp. 4-11, 1996. 
21. C. C. Yang, J. Yen, H. C. Chen, “Intelligent Internet Searching Agent Based on 
Hybrid Simulated Annealing,” Decision Support Systems, Vol. 28, pp. 269-277, 2000. 
22. URL: http://www.google.com/, Google. 
23. URL: http://www.OPENFIND.com.tw/, OPENFIND. 
24. URL: http://www.teoma.com/, Teoma. 
25. URL: http://www.yam.com/, YAM. 
 
 58 
 
 60 
generally two to three [4][14].  This often leads to some problems.  First, the search 
engine may respond a lot of irrelevant results.  Furthermore, it is usually difficult for a 
user to describe his or her request precisely according to just a few query terms (words).  
To overcome these problems, various query expansion (or modification) techniques have 
been developed [2][8][7][11][16].  That is, the search engine returns more precise 
results to a user based on a refined word list, which is obtained by expanding (or 
modifying) the original query term or terms.   
In general, two kinds of methods are commonly used for query expansion: (a) 
Automatic query expansion [11]; (b) Interactive query expansion [2][8][7][16].  In 
automatic query expansion (AQE), a user’s query is expanded automatically according to 
some useful information, such as co-occurrence data, document classification, syntactic 
context, and so on [11].  In interactive query expansion (IQE), by contrast, a user has to 
select some additional search terms and then the original query is expanded based on 
these selected terms.  Another variety of IQE is relevance feedback [12].  In this 
method, a user has to identify relevant documents in an initial retrieved document set.  
Then a new query is created based on these relevant documents.   
Moreover, thesaurus has attracted great attention for query expansion (formulation) 
in recent years, such as synonym-based thesaurus [3][9] and similarity thesaurus 
[5][6][10][14][15].  To create a synonym-based thesaurus, a set of synonym terms 
should be identified from a dictionary of words.  However, term co-occurrence data are 
not considered within the synonym-based thesaurus.  As for similarity thesaurus [10], 
term similarity (term-to-term relationship, such as term co-occurrence) is determined and 
then the thesaurus for a term i is created from terms with high similarity to term i. 
To expand a query effectively, we believe that the query logs of users are useful.  
That is, the results of a query can be refined precisely when useful information is derived 
from the query logs containing selected web pages (or documents) of users.  When a 
user enters a query term, he or she will be presented with search results (for example, 
web pages or documents).  Accordingly, some of these web pages (or documents) are 
selected by the user.  Consequently, the entire information about the entered query term, 
such as selected web pages, user id, and query term, is recorded in the query logs.  For 
example, a query term ‘A’ can be expanded with term ‘B’ according to the high 
correlation between terms ‘A’ and ‘B’ in selected web pages of users.  Such useful 
 60 
 
 62 
generally stated as the following expression [1]: 
 
cYX →  (1) 
 
where I⊂X , I⊂Y , φ=∩YX , and c is a constant indicating the confidence 
of the rule. 
 
In marketing research, for example, the researchers analyze the past transaction 
records and then derive some useful information for making proper decisions.  An 
association rule mined from the transaction records may be described as follows. 
 
Rule r1: 88% of the people who buy dried milk also buy beer. 
 
The antecedent and the consequent of rule r1 are dried milk and beer respectively, 
and the confidence of rule r1 is 88%.  Clearly, with high confidence, rule r1 is 
somehow helpful for decision-making. 
Wur and Leu [17] proposed an effective Boolean algorithm, named Sparse-Matrix 
approach (BSM), for mining association rules in large databases.  In this approach, two 
tables, IT and TT, are created to generate frequent item sets [17].  Each row in ITk-1 
represents a frequent item set and each ‘1’ in ITk-1 represents an item in item set I.  Each 
‘1’ in TTk-1 represents a record that contains the corresponding item set in ITk-1.   
Accordingly, logic OR operation is employed on any two rows in ITk-1 to generate a 
k-item set; meanwhile, logic AND operation is employed on the corresponding rows in 
TTk-1 to generate TTk.  Then, using logic AND and XOR operations, interesting 
association rules are derived from the frequent item sets in all ITs and TTs.   
Consider a simple database with a set I of five items {A, B, C, D and E}, as shown 
in Table 1.  Each row in Table 1 represents a record; for example, row one represents a 
record (T100) that contains three items (A, D, and E). 
 
 
 62 
 
 64 
3. ARCHITECTURE OF ASSOCIATION THESAURUS GENERATION 
This section describes a framework, as shown in Figure 1, to generate association 
thesaurus for query expansion.  When a user enters a query term, he or she will be 
presented with search results (for example, web pages or documents).  Accordingly, 
some of these web pages (or documents) are selected by the user.  Consequently, the 
entire information about the entered query term, such as selected web pages, user id, and 
query term, is recorded in the query logs.  As mentioned earlier, these records will be 
used further for association thesaurus generation. 
 
Sets of 
Terms 
Term 
Selection  
Associatio
n 
Thesaurus 
Query 
Logs 
Term 
Correlation 
Mining 
 
 
 
 
 
 
Fig. 1.  Architecture of association thesaurus generation. 
 
In general, the above selected web pages (or documents) consist of many words 
(terms).  Thus, sets of terms, which are informative in selected web pages (or 
documents) and useful for the mining tasks, should be identified from selected web 
pages (or documents).  To accomplish this, a value, named Q, is introduced as follows. 
 
j
m
i
ij
j dfm
tf
Q ×=
∑
=1  (2) 
 
where Qj denotes the average term frequency and document frequency of term j, tfij is the 
frequency of occurrence of term j in selected page i, dfj is the frequency of occurrence of 
term j in all selected pages, and m is the total number of selected pages. Clearly, terms 
with high value of Q(greater than a threshold θ) are informative in selected pages.  In 
this work, these terms are collected as sets of terms and further used for term correlation 
 64 
 
 66 
4. QUERY AGENT WITH ASSOCIATION THESAURUS 
The proposed query expansion method with association thesaurus is implemented 
within the Query Agent of a course recommendation system, Coursebot [21].  Figure 2 
describes the architecture of the Query Agent.  To find the desired course pages, a user 
has to specify a query term via the Interface Agent.  Consequently, the Query 
Expansion module retrieves and ranks candidate expansion terms from the association 
thesaurus database by using a SQL (Structure Query Language) form (as shown in Table 
5); the user is then presented with these expansion terms.  Accordingly, the candidate 
expansion terms selected by the user are combined with the original query for query 
modification.  Finally, the reformulated query is applied again to generate structured 
course pages for the user. 
Query 
ExpansionSearch
user
candidate expansion terms
search result
Query Agent
reformulated queries
Course 
database Interface 
Agent
candidate expansion terms
original queries
structured course pages
Course 
Constructor
selected terms
Association 
ThesaurusQuery Logs 
Scheduler
Query 
Modification
query log
Fig. 2.  Architecture of the Query Agent. 
 
 66 
 
 68 
 
 minimum support = 4%
0
50
100
150
200
250
300
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
nu
mb
er 
of 
ass
oc
iat
ion
 ru
les
 
 
 
 
 
 
 
 
 
 
 
 threshold of term selection
 
Fig. 3.  Relationship between the threshold used for term selection and the number of association 
rules. 
 
Table 6 
A thesaurus with query term ‘learning’ 
Antecedent Consequence Confidence 
learning algorithm 82.78% 
learning network 66.68% 
learning neural 66.25% 
learning function 52.69% 
learning training 50.97% 
learning perception 49.58% 
learning supervised 49.20% 
learning vector 35.41% 
learning output 26.87% 
learning unsupervised 24.30% 
learning system 18.85% 
learning concept 10.24% 
learning backpropagation 8.75% 
 
The proposed approach for interactive query expansion was evaluated by using two 
ratios, precision ratio and recall ratio, which are defined as follows [13]. (Relevant 
documents are identified by an expert in “Neural Network”) 
 
 
 
 
 
 68 
 
 70 
 
0
10
20
30
40
50
60
70
80
90
100
0 50 100 150 200 250
number of query logs
pr
ec
si
on
 ra
tio
 a
nd
 re
ca
ll 
ra
tio
  (
%
)
precision
recall
 
Fig. 4.  Relationship between the number of query logs used for mining association rules and the 
two performance ratios of the proposed approach. 
6. CONCLUSIONS 
This paper proposed an interactive query expansion method with association 
thesaurus, which is mined from the selected web pages of users in the query logs.  The 
selected web pages of users in the query logs are transferred into sets of terms and then 
used for term correlation mining.  Then, various association thesauruses concerning 
different query terms are constructed from these term correlations.  Consequently, the 
proposed method combines the original query term specified by a user with the 
corresponding thesaurus to offer the user more precise results. 
The query expansion mechanism is implemented within the Query Agent of a 
course recommendation system, Coursebot.  The proposed approach for interactive 
query expansion was evaluated by using two ratios, precision ratio and recall ratio.  
Experimental results have shown that the performance of the course recommendation 
system is improved a lot when the proposed approach is applied. 
 
 
 70 
 
