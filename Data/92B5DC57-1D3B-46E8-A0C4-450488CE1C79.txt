commercial providers, the way to place dynamically 
requested workloads into available multi-core 
computing and stor-age resources, taking into 
consideration performance, energy consumption and 
heat dissipation issues, has become an important 
topic, which would lead to improvement of the overall 
cluster system performance and efficiency on the 
usage of available resources, such as hundreds or 
thousands of GPU and x86 cores in high end servers, 
storage and other devices. 
 
In this research, we target at the investigation of a 
dynamic energy-aware management framework on the 
execution of independent workloads (e.g., bag-of-
tasks) in hybrid CPU-GPU PARA- computing platforms, 
aiming at optimizing the execution of  workloads in 
appropriate computing resources concurrently whilst 
balancing the use of solely virtual or physical 
resources or hybridly selected resources, to achieve 
the best performance in executing application 
workloads and minimizing the energy associated with 
computation selected. Experimental results show that 
the proposed strategy can contribute to improving 
performance by introducing optimization techniques, 
such as workload consolidation and dynamic 
scheduling. We observed that workload consolidation 
can potentially improve performance, depending on 
characteristics of the workload. Also, the workload 
scheduling results present the importance of resource 
management by revealing the performance gap among 
different execution schedules for shared computing 
resources. 
 
英文關鍵詞： GPU, hybrid CPU-GPU environment, energy consumption 
 
行政院國家科學委員會專題研究計畫結案報告 
 
PARA-雲端環境中虛擬主機動態管理於效能及耗能之研究 
 
Abstract 
With the rapid increase in the number of high performance computing environments 
throughout all segments of our society, from academia, industry to commercial providers, the 
way to place dynamically requested workloads into available multi-core computing and stor-
age resources, taking into consideration performance, energy consumption and heat dissipa-
tion issues, has become an important topic, which would lead to improvement of the overall 
cluster system performance and efficiency on the usage of available resources, such as hun-
dreds or thousands of GPU and x86 cores in high end servers, storage and other devices. 
 
In this research, we target at the investigation of a dynamic energy-aware management 
framework on the execution of independent workloads (e.g., bag-of-tasks) in hybrid 
CPU-GPU PARA- computing platforms, aiming at optimizing the execution of  workloads in 
appropriate computing resources concurrently whilst balancing the use of solely virtual or 
physical resources or hybridly selected resources, to achieve the best performance in execut-
ing application workloads and minimizing the energy associated with computation selected. 
Experimental results show that the proposed strategy can contribute to improving perfor-
mance by introducing optimization techniques, such as workload consolidation and dynamic 
scheduling. We observed that workload consolidation can potentially improve performance, 
depending on characteristics of the workload. Also, the workload scheduling results present 
the importance of resource management by revealing the performance gap among different 
execution schedules for shared computing resources.  
 
Keywords: GPU, hybrid CPU-GPU environment, energy consumption 
 
 
1. Introduction 
Current chip fabrication technologies allow placing several million transistors in a chip, 
enabling more complex designs in each new generation of chips. To illustrate such advance, 
the GPU Kepler GK110 is designed with 7.1 billion transistors [1]. The clock speed of micro-
processors is no longer increasing with each new generation of chip fabrication, since there 
are a number of limitations that discourage the design of even more complex uniprocessors 
that include increasing in-chip heat, power consumption, diminishing instruction-level paral-
lelism gains, almost unchanged memory latency, inherent complexity of designing a single 
core with a large number of transistors, among others. Taking into consideration issues listed, 
the current strategy on chip manufacturing is to gang together many slower processor cores on 
a chip, as alternative to obtain more computational power. 
 servers in a high performance CPU-GPU PARA-Virtualization computing environment to-
ward energy-aware GPU and x86 computing, keeping resources utilization at a desired level. 
 
3. Research and Experiments 
 
3.1. Potential of Dynamic Workload Consolidation on GPU Computing Envi-
ronment 
To investigate various aspects on the design of CPU-GPU para-virtualized environment, 
we have conducted a set of experiments with GPU applications. On virtualized environment, 
multiple VM instances share available physical GPU devices managed by hypervisor. How-
ever, we run experiments without virtualized layer, since there is no currently available soft-
ware platform to construct CPU-GPU PARA-virtualized system. In fact, one of the main 
goals of the virtualization layer is to provide additional abstraction and flexibility to users. In 
that sense, we believe that the virtualized configuration does not affect to results reflecting 
nature of the system that want to show: performance characteristics on shared CPU-GPU re-
sources among different applications.  
As experimental setup, we used Intel i5-2400 Quad-core PC with 8GB main memory, 
NVIDIA GeForce GTX 550Ti GPU installed, running Linux. Application workloads are se-
lected among NVIDIA CUDA SDK; BlackScholes, scan, Interval, and radixSortThrust. The 
applications are carefully selected to heavily utilize system resources, reflecting characteris-
tics of CPU-GPU para-virtualized environment correctly. 
Figure 1 shows how parallel execution of each workload improves performance com-
pared to sequential execution. We run each application simultaneously in parallel up to five. 
The result is presented as relative performance, normalized to sequential execution. The im-
plication of the result is the potential of workload consolidation. For applications like Interval, 
the performance is significantly improved up to 2.5x, which means it is possible to scale per-
formance, and improvement is expected if consolidating these type of applications as many as 
possible. On the other hand, application like scan does not show any remarkable improvement. 
Moreover, the performance was dropped when parallelism reaches five due to resource con-
tention, since available CPU core is restricted to four in our configuration. This means work-
load like scan fully utilizes GPU resource; therefore there is no room left for consolidated 
execution. For these ‘heavy’ workloads, consolidation would not impact the performance se-
verely, which means the consolidation should be avoided to prevent performance degradation 
due to unnecessary overheads. 
 
 
Table 1. Impact on Application Pairs on Performance 
Case  Application Pair  Sequential 
Exec. Time(s) 
Parallel Exec. 
Time(s) 
Speedup 
A  <Interval, radixSortThrust>  1.51  1.06  1.42 
B  <BlackScholes, scan>  4.51  3.63  1.24 
C  <radixSortThrust, BlackScholes>  2.95  2.39  1.23 
D  <Interval, scan>  3.1  2.58  1.20 
E  <Interval, BlackScholes>  2.88  2.26  1.27 
F  <radixSortThrust, scan>  3.16  2.88  1.10 
 
3.3 Energy-Aware Task Consolidation Technique 
As partial result of this project (see subsection 6.3, #12), we presented a novel task con-
solidation technique based on virtual clusters for power optimization. Both CPU utilization 
and network transmission cost are taken into account when dealing with task migration. Con-
sidering machines with idle state, or even at a very low load, the power consumption is still 
noticeable. On the contrary, machines over loaded or stay on high utilization might also cause 
energy inefficient. Upon such observation, the main idea of the proposed technique is to de-
fine an ideal utilization level (also known as threshold) of virtual machines. When CPU utili-
zation of virtual machines is lower than the predefined threshold, we tend to balance workload 
within a cluster. If CPU utilization of virtual machines higher than the threshold, tasks will be 
migrated to other proper resources. To dispatch tasks to other machines or clusters, the extra 
power consumption due to task consolidation should be considered. 
If there are multiple resources within a data center available for receiving the tasks, the 
one with minimal energy cost, including transmission cost and execution cost, will be chosen 
to perform task consolidation. To evaluate the effectiveness of the propose method, our ex-
periments analyze the impact of different workload and cluster size to the performance of task 
consolidation. The experimental results show that our strategy can improve energy usage. 
 
4. Conclusions 
The goals of this proposed research involves dynamic energy-aware management on the 
execution of workloads in hybrid CPU-GPU PARA virtualization computing platforms, aim-
ing at optimizing the execution of workloads in appropriate computing resources concurrently 
whilst balancing the use of solely virtual or physical resources or hybridly selected resources, 
to achieve the best performance in executing application workloads and minimizing the ener-
gy associated with computation selected. 
Essentially, the proposed research involves the search of strategies that best satisfies 
constraints of a multi-core computer system. As from the experimental results presented in 
Section 3, we demonstrated how the optimized management strategy can impact performance 
in virtualized environment in which multiple virtual machines share physical devices. Work-
load consolidation increases resource utilization and improves performance by merging and 
executing workloads in parallel, considering the characteristics of workload. Dynamic sched-
 6. Results under this NSC Support 
6.1 Graduate Student supervision 
1. 研二 
洪肇鴻 
 
2. 研一 
古振浩 
吳承諭 
謝旻翰 
黃月紅 
 
3. 五年一貫 
賴嵐韻 
 
 
6.2 大學專題研究 
 賴嵐韻，”於 OpenCL 與 CUDA 語法的程式設計與效能評估”  (101 學年度) 
 
6.3  論文 
1. “On Migration and Consolidation of VMs in Hybrid CPU- GPU Environments”, The 1st 
International Conference on Intelligent Technologies and Engineering Systems 
(ICITES’2012), LNEE, Springer, 2012. (accepted) 
2. “IP Address Management in Virtualized Cloud Environments”, The 1st International 
Conference on Intelligent Technologies and Engineering Systems (ICITES’2012), LNEE, 
Springer, 2012. (accepted) 
3. “Designing Parallel Sparse Matrix Transposition Algorithm Using CSR for GPUs”, The 
1st International Conference on Intelligent Technologies and Engineering Systems 
(ICITES’2012), LNEE, Springer, 2012. (accepted) 
4. “Dynamic Power-Saving Model for Cloud Computing System”, IJHIT, IST’2012 The 
2012 International Conference on Information Science and Technology, China, 2012 
5. “XtratuM/PPC: A Hypervisor for Partitioned System on PowerPC processors”, Rui Zhou, 
Qingguo Zhou, Sheng Yong, Kuan-Ching Li, The Journal of Supercomputting, Springer 
(accepted and available at OnlineFirst) 
6. “A Secure Distributed File System Based on Revised Blakley’s Secret Sharing Scheme”, 
Su Chen, Yi Chen, Hai Jiang, Laurence T. Yang, Kuan-Ching Li, in Trustcom’2012  
IEEE 11th International Conference on Trust, Security and Privacy in Computing and 
Communications, 2012.  
7. “在雲端虛擬化環境下自動化配置虛擬 IP”, 洪肇鴻, 古振浩, 李冠憬, 資訊教育與
科技應用研討會, 台中, 2012 
8. “Deploying Scalable and Secure Secret Sharing with GPU Many-Core Architecture”, Su 
Chen, Ling Bai, Yi Chen, Hai Jiang, Kuan-Ching Li, in IPDPS’2012 workshops 
 
[5]   Philippe Chanclou, Stéphane Gosselin, Juan Fernández Palacios, Victor López Alvarez 
and Evi Zouganeli, “Overview of the optical broadband access evolution: a joint arti-
cle by operators in the IST network of excellence e-Photon/One,” IEEE Communica-
tions Magazine, Vol. 44, No. 8, pp. 29-35, August 2006. 
[6]   Jayant Baliga, Kerry Hinton, Rodney S. Tucker, “Energy Consumption of the Internet,” 
Proceedings of Conference on Optical Internet and Australian Conference on Optical 
Fiber Technology, pp. 1-3, June 2007. 
[7]   Muhammad H Alizai, Georg Kunz, Olaf Landsiedel and Klaus Wehrle, “Promoting 
Power to a First Class Metric in Network Simulations,” Proceedings of the Workshop 
on Energy Aware Systems and Methods, February 2010. 
[8]   Young Choon Lee and Albert Y. Zomaya, “Energy efficient utilization of resources in 
cloud computing systems,” The Journal of Supercomputing, pp. 1-13, March 2010. 
International Journal of Hybrid Information Technology  
Vol. 5, No. 2, April, 2012 
 
 
182 
 
resources on a specific virtual machine.  Trieu C. Chieu et al. [9] proposed a useful 
architecture for dynamic scaling of the web application in a virtualized cloud computing 
environment.  Their architecture includes front-end load-balancer, virtual cluster monitor 
system and auto-provisioning system, and works efficiently for general web applications in 
virtual machines.  However, a user can rent many virtual machines to perform his applications 
which are not only web applications.  Meanwhile, for the security, virtual machines for a 
specific user can be regarded as a group, and a user cannot access the resources from other 
groups.   
In this paper, we will present a dynamic-scaling scenario with novel design of applications 
deployed in virtual machines in a virtual cluster.  The rest of the paper is organized as follows.  
Section 2 illustrates the virtual cluster architecture in a virtualized cloud computing 
environment.  Section 3 describes our auto-scaling algorithm.  Finally, section 5 concludes 
the paper. 
 
 
Figure 1. Architecture of the Auto-scaling in a Cloud 
 
2. Architecture Design 
We consider two scenarios about web service and parallel processing application in the 
Cloud Computing environment.  A web service should be available at anytime and should 
provide the fastest response time regardless of the number of users served.  Therefore, a 
Cloud service system should scale the service dynamically; extending and shrinking the 
number of web servers and web service components for large requirement and small 
requirement.  Cloud Computing also allows users to access supercomputer-level computing 
power by distributing tasks into large amount of computing nodes (virtual machines).  Users 
may not know the exact number of computing nodes should be utilized to efficiently perform 
their jobs.  Thus, the under-provisioning and over-provisioning happen often.  A Cloud 
computing system should scale the computing nodes according to the workload.  A scalable 
International Journal of Hybrid Information Technology  
Vol. 5, No. 2, April, 2012 
 
 
184 
 
virtual machine are over the threshold of use of physical resources in a virtual cluster.  
As shown in Figure 1, the auto-scaling algorithm is implemented in Auto-provisioning 
system, and Virtual cluster monitor system is used to control and trigger the scale-up and 
scale-down in Auto-provisioning system on the number of virtual machine instances based on 
the statistics of the scaling indicator.   
Installation of a software appliance on a virtual machine creates a virtual appliance. 
Virtual appliance image is a set of virtual appliances on a virtual machine. A virtual 
appliance image includes guest OS and applications, and it can eliminate the 
installation, configuration and maintenance costs associated with running complex 
stacks of software.  Therefore, to simplify provisioning process, a virtual machine 
appliance image template that includes the web application and its corresponding web 
server is stored in the image repository of the Cloud system.  The new virtual machines 
of web servers and web applications can be rapidly created and provisioned to the 
virtual cluster using the corresponding appliance image template.  
For web service application, network bandwidth and number of sessions are most 
important index for the service level agreement (SLA) and quality of service (QoS).  Figure 2 
illustrates our auto-scaling algorithm for web service application.  The algorithm first 
determines the current VMs with network bandwidth and active sessions above or below 
given threshold numbers, respectively.  If all VMs have network bandwidth and active 
sessions above the given upper threshold, a new VM will be provisioned, started, and then 
added to the front-end load-balancer, respectively.  If there are VMs with network bandwidth 
and active sessions below a given lower threshold and with at least one VM that has no 
network traffic or active sessions, the idle VM will be removed from the front-end load-
balancer and be terminated from the system. 
For distributed computing tasks, physical computing resources such as the uses of CPU 
and memory are most important indices for evolution of workload of a virtual cluster.  Figure 
3 illustrates our auto-scaling algorithm for distributed computing tasks.  The algorithm first 
determines the current VMs with the use of physical resources above or below given 
threshold numbers.  If the uses of resources of all VMs are above the given upper threshold, a 
new VM will be created, provisioned, started, and then perform the same computing tasks in 
the virtual cluster.  If the uses of resources of some VMs are below a given lower threshold 
and with at least one VM that has no computing job, the idle VM will be terminated from the 
virtual cluster. 
 
5. Conclusion 
In this paper, we have presented two scaling scenarios to address the automatic scalability 
of web applications and distributed computing jobs in a virtual cluster on the virtualized 
Cloud Computing environment.  The proposed Cloud Computing architecture is constructed 
with a Front-end load balancer, a Virtual cluster monitor system and an Auto-provisioning 
system.  The Front-end load balancer is utilized to route and balance user requests to Cloud 
services deployed in a virtual cluster.  The Virtual cluster monitor system is used to collect 
the use of physical resources of each virtual machine in a virtual cluster.  The Auto-
provisioning system is utilized to dynamically provision the virtual machines based on the 
number of the active sessions or the use of the resources in a virtual cluster.  In addition, the 
idle virtual machines are destroyed, and then the resources are able to be released.   In the 
other hand, the energy cost can be reduced by remove the idle virtual machines.  Our work 
has demonstrated the proposed algorithm is capable of handling sudden load requirements, 
maintaining higher resource utilization and reducing energy cost.  The auto-scaling 
International Journal of Hybrid Information Technology  
Vol. 5, No. 2, April, 2012 
 
 
186 
 
 
point-to-multipoint optical access solutions.  They presented 
the view of the evolution towards broadband optical access 
networks and pointed out some aspects for simulating network 
development for network environment in the future.  A 
network-based model [3] was presented to estimates the power 
consumption of core, metro and access networks which were 
three main parts of a standard Internet Service Provider's 
network.  Baliga et al. [2] presented power consumption for 
network to analyze the use of optical and wireless access 
networks, e.g. passive optical network (PON), fiber to the node 
(FTTN), point-to-point (PtP) optical systems and worldwide 
interoperability for microwave access (WiMAX).  In the results, 
PON and PtP were the best solutions in terms of energy 
consumption.  Tucker et al. [19] proposed a model to estimate 
the energy consumption for IP networks.  The hybrid fiber-to-
the-node was not recommended due to more energy 
consumption.  Lange and Gladisch [9] compared the energy 
consumption of FTTH network based on passive optical 
networks (PON), active optical network (AON) and point-to-
point (PtP) network.  The results showed that FTTH networks 
based on PON had better performance in terms of energy 
consumption.  Vasić and Kostić [20] tried to reduce the energy 
consumption by increasing the percentage of links over the 
Internet to sleep state.  They proposed Energy-Aware Traffic 
engineering (EATe) to improve the traditional works which did 
not consider energy measuring.  EATe successfully moved 
links to sleep state and handled changes in traffic load while 
keeping the same traffic rates.  Aliza et al. [1] listed the 
components of hardware and software which affect the energy 
consumption.  In the list, CPU was the most important 
component. 
Lien et al. [12] collected the data of power consumption 
and CPU utilization, and concluded the relation in a model.  
They designed a virtual instrumentation software module to 
measure the power consumption of the streaming media server 
in real time.  Using their design, users can estimate energy 
consumption accurately and easily without additional hardware.  
A Scalable Multiple Server (SMS) architecture [11] design for 
resource management in a server center was presented for 
better performance and power consumption.  Linear and 
exponential power models were proposed for estimating power 
consumption of different states.  The SMS architecture 
improved 16.9% energy consumption in the experiment results.  
To achieve maximum power capacity for a warehouse-size 
computing system in practice is difficult since power 
consumption varies mainly with the computing activity.  Fan et 
al. [6] studied power usage of thousands of servers (cluster 
level) and even found a noticeable power gap in well-tuned 
applications.  They argued that both peak performance and 
activity range should be considered for power efficiency.  
Rivoire et al. [15] compared five high-level full-system power 
models over a laptop to a server and concluded that models 
based on OS utilization and CPU performance was accurate.  
Meisner et al. [13] proposed a two-state energy-conservation 
approach, PowerNap, to simply the complex power-
performance states of systems.  Demonstrating with a power 
provisioning approach, Redundant Array for Inexpensive Load 
Sharing (RAILS), they improved power consumption by 74%.  
Beri et al. [4] studied energy-saving research for the 
management of integrated systems.  They identified several 
impacts that might be occurred while implementing energy-
saving strategies for cloud computing environments.  Power 
consumption of physical machine can be measured in modern 
server hardware.  However, power of VM cannot be measured 
directly by hardware.  Kansal et al. [8] proposed Joulemeter to 
solve this problem.  There were several coefficients in their 
formula changing from time to time.  Thus these coefficients 
had to be adjusted according the threshold they defined. 
Nathuji et al. [14] proposed VirtualPower approach for 
power management to support virtual machines (VMs) and the 
virtualized resources.  The experimental evaluations showed 
34% improvement on power consumption.  Torres et al. [18] 
proposed a consolidation strategy for a data center by jointing 
memory compression and request discrimination techniques.  
They evaluated the proposed strategy with a representative 
workload scenario and a real workload.  Srikantaiah et al. [17] 
studied the inter-relationships between performance 
degradation, energy consumption, CPU utilization and Disk 
utilization.  They transformed the consolidation problem into a 
bin packing problem and found appropriate server with better 
performance and lower power consumption for each request.  
Song et al. [16] proposed a utility analytic model for Internet-
oriented servers to provide upper bound of physical server 
consolidation for QoS and estimate the power and utility.  The 
experiments showed improvements on power and CPU 
resource utilization without performance degradation.  Lee and 
Zomaya [10] proposed two energy-conscious task 
consolidation heuristics, ECTC and MaxUtil, to reduce energy 
consumption without performance degradation for cloud 
environment with homogeneous resource in terms of 
computing capability and capacity. 
III. PRELIMINARY 
To simplify the presentation of the proposed technique, we 
first define and clarify research models, notations and 
terminologies that will use in this paper.  To this end, an 
abstracted cloud computing model is given in Figure 1.  A 
cloud system consists of several clusters, each provides limited 
number of virtual machines.  Without lost of generality, virtual 
machine is the basic unit to execute tasks.  The percentage of 
CPU utilization is used to judge whether a virtual machine has 
enough resource to work for a desired task.  Network 
bandwidth among clusters is assumed reliable and ranging 
from 100Mb/s to 1Gb/s at different links, showing the 
conditions in practice.  Because the available resource of each 
cluster varies dynamically, a cluster has its own strategy to 
consolidate tasks for minimizing energy consumption.  Namely, 
a cluster could ask resource support from other clusters and 
consolidate tasks to proper resources.  Each cluster is with a 
job queue that has information of all tasks, such as task ID (tj), 
arrival time of task tj (Ta,j), CPU processing time of task tj (Tp,j), 
data size of task tj (DSj).  Information of system status, such as 
CPU utilization of virtual machines could be also obtained 
from cloud monitoring systems. 
A well established energy consumption model [12] explains 
the relation between CPU utilization and energy consumption 
is not with linear increasing.  According to this study, the 
energy consumption of a VM can be divided into 6 levels, an 
116
principle.  As shown in Figure 4, after five tasks being assigned 
to all virtual machines, t5 could be assigned to both V0 and V1.  
Applying the best fit strategy, task t5 is assigned to V1 because 
the total CPU utilization is closest to 70%. 
 
Figure 3.  A list of tasks. 
To dispatch and execute task t6, because t6 will consumes 
50% CPU utilization, assigning it to any VM in VCA will break 
the 70% principle.  To conform the 70% principle, cluster A 
asks for resource support from other clusters, for example, VCB 
and VCC.  As shown in Figure 5, both VCB and VCC have 
available resources conforms the 70% principle, thus VCA 
needs to decide the destination cluster that reflects better 
energy usage.  To estimate the energy consumption in 
consolidating a task to other resources that located in different 
clusters, both CPU computation and network transmission 
should be considered.  Given a task tj to be migrated from VCP 
and consolidated with virtual machine Vi that resides in VCQ, 
the expected energy consumption can be formulated as 
following, 




jpja
ja
TT
Tt PQ
j
itji swattBW
DS
VECost
,,
,
/2)(, 
                    (4) 
where BWPQ and DSj are bandwidth from VCP to VCQ and 
size of task tj’s data set, respectively. 
According to (4), migrating task t6 to VCB and VCC will 
consume (3β+α)*10+ 210/250*2β and (2β+α)*10+ 210/500*2β, 
respectively.  It’s easy to see that VCC presents better energy 
consumption.  Thus task t6 will be consolidated to virtual 
machine in VCC.  Figure 5 shows the above scenario. 
 
 
Figure 4.  Assigning t5 in VCA. 
 
Figure 5.  VCA asks for resource support while assigning t6. 
 
Figure 6.  VCA assigns t6 to V0 without conforming to the 70% principle. 
The last example shows what if VCB and VCC do not have 
enough resources while VCA asks support for t6.  In Figure 6, 
CPU utilization of VMs in VCB and VCC are higher than or 
equal to 70%.  Thus VCA will not seek outside resources due to 
extra overheads might be incurred.  As a result, it assigns t6 to 
local resources even the VM could not conform the 70% 
principle.  In this example, V0 is chose. 
A high level description of the ETC algorithm is described 
as following. 
 
Algorithm ETC 
1. { 
2.       Principle_70 (VCA, tj)  
3.     // Validating the 70% principle for all tasks in job queue  
of virtual cluster VCA 
4.       If no resource in VCA conforms the 70% principle for  
executing task tj 
5.       { 
6.             Principle_70 (VCB, tj) 
7.             Principle_70 (VCC, tj) 
If more than one resource that conforms the 70%  
principle, dispatch task tj to VC that has better energy  
usage. 
8.        Otherwise, consolidate task tj to VM in VCA that with 
118
observation, test cases with extremely high work loading, 4000 
tasks were further conducted.  Such cases are denoted as 
extremely high loading (EHL) as shown in Table II.  Figure 10 
shows the results of these cases.  Extremely high loading forces 
VCA to ask resource support form VCB and VCC.  Overall 
speaking, the ETC technique can provide up to 17% 
improvement. 
 
 
Figure 9.  The results while VCB and VCC have high loading. 
 
Figure 10.  The results while VCA has extremely high loading. 
TABLE II.  CASES OF VCA WITH EXTREMELY HIGH LOADING. 
 VCB and VCC 
LL ML HL 
MR 
VC
A 
 
EHL 
LR(5) (E,5,L) (E,5,M) (E,5,H) 
MR(10) (E,10,L) (E,10,M) (E,10,H) 
HR(15) (E,15,L) (E,15,M) (E,15,H) 
VI. CONCLUSIONS 
  The maximization of the profit is a high priority in cloud 
computing systems, in this regard the minimization of energy 
consumption plays a crucial role.  Many literature show that 
energy consumption and resource utilization in clouds are 
highly coupled, and task consolidation is an effective technique 
to increase resource utilization and in turn reduces energy 
consumption.  In this paper, an energy-aware task 
consolidation (ETC) technique is presented to minimize energy 
consumption.  Considering the architecture most cloud systems 
conforms, a 70% principle is proposed to manage task 
consolidation among virtual clusters.  Idle state of virtual 
machines and network transmission are assumed to be 7 times 
and 2 times of basic energy consumption unit in this study.  
Note that these values could be adjusted in the ETC method to 
be adaptive in different cloud systems.  The simulation results 
show that ETC can significantly reduce power consumption in 
managing task consolidation for cloud systems.  Up to 17% 
improvement as compare to a recent work in [10] that aims to 
maximize resource utilization can be obtained. 
REFERENCES 
[1] Muhammad H Alizai, Georg Kunz, Olaf Landsiedel and Klaus Wehrle, 
“Promoting Power to a First Class Metric in Network Simulations,” 
Proceedings of the Workshop on Energy Aware Systems and Methods, 
February 2010. 
[2] Jayant Baliga, Robert Ayre, Wayne V. Sorin, Kerry Hinton, Rodney S. 
Tucker, “Energy Consumption in Access Networks,” Proceedings of 
Optical Fiber communication/National Fiber Optic Engineers, pp. 1-3, 
February 2008.  
[3] Jayant Baliga, Kerry Hinton, Rodney S. Tucker, “Energy Consumption 
of the Internet,” Proceedings of Conference on Optical Internet and 
Australian Conference on Optical Fiber Technology, pp. 1-3, June 2007.  
[4] Andreas Beri, Erol Gelenbe, Marco Di Girolamo, Giovanni Giuliani, 
Hermann De Meer, Minh Quan Dang and Kostas Pentikousis, “Energy-
Efficient Cloud Computing,” The Computer Journal, Vol. 53, No. 7, pp. 
1045-1051, 2010.  
[5] Philippe Chanclou, Stéphane Gosselin, Juan Fernández Palacios, Victor 
López Alvarez and Evi Zouganeli, “Overview of the optical broadband 
access evolution: a joint article by operators in the IST network of 
excellence e-Photon/One,” IEEE Communications Magazine, Vol. 44, 
No. 8, pp. 29-35, August 2006.  
[6] Xiaobo Fan, Wolf-Dietrich Weber and Luiz Andre Barroso, “Power 
Provisioning for a Warehouse-sized Computer,” Proceedings of the 34th 
annual international symposium on Computer architecture, pp. 13-23, 
2007.  
[7] Chamara Gunaratne, Ken Christensen and Bruce Nordman, “Managing 
energy consumption costs in desktop pcs and lan switches with proxying 
split tcp connections, and scaling of link speed,” International Journal 
of Network Management, Vol. 15, No. 5, September 2005. 
[8] Aman Kansal, Feng Zhao, Jie Liu, Nupur Kothari and Arka A. 
Bhattacharya, “Virtual Machine Power Metering and Provisioning,” 
Proceedings of the 1st ACM symposium on Cloud computing, pp. 39-40, 
2010. 
[9] Christoph Lange and Andreas Gladisch, “On the energy consumption of 
FTTH access networks,” Proceedings of Optical Fiber Communication, 
pp. 1-3, March 2009. 
[10] Young Choon Lee and Albert Y. Zomaya, “Energy efficient utilization 
of resources in cloud computing systems,” The Journal of 
Supercomputing, Online First, pp. 1-13, March 2010. 
[11] Chia-Hung Lien, Ying-Wen Bai, Ming-Bo Lin, Chia-Yi Chang and 
Ming-Yuan Tsai, “Web Server Power Estimation, Modeling and 
Management,” Proceedings of 14th IEEE International Conference on 
Networks, Vol. 2, pp. 1-6, September 2006. 
[12] Chia-Hung Lien, Ming Fong Liu, Ying-Wen Bai, Chi Hsiung Lin and 
Ming-Bo Lin, “Measurement by the Software Design for the Power 
Consumption of Streaming Media Servers,” Proceedings of the IEEE 
Instrumentation and Measurement Technology Conference, pp. 1597-
1602, April 2006. 
[13] David Meisner, Brian T. Gold and Thomas F. Wenisch, “PowerNap: 
Eliminating Server Idle Power,” Proceeding of the 14th international 
conference on Architectural support for programming languages and 
operating systems, Vol. 44, No. 3, pp. 205-216, March 2009. 
[14] Ripal Nathuji and Karsten Schwan, “VirtualPower: Coordinated Power 
Management in Virtualized Enterprise Systems,” Proceedings of twenty-
120
On Migration and Consolidation of VMs
in Hybrid CPU-GPU Environments
Kuan-Ching Li1, Keunsoo Kim2, Won W. Ro2, Tien-Hsiung Weng1,
Che-Lun Hung3, Chen-Hao Ku1, Albert Cohen4, and Jean-Luc Gaudiot5
1 Dept. Computer Science & Information Engr., Providence University, Taiwan
2 School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea
3 Dept. Computer Science & Communication Engr., Providence University, Taiwan
4 INRIA Saclay Parc Club Orsay Universite´, ZAC des vignes, France
5 Dept. of Electrical Engr. and Computer Science,
University of California Irvine, USA
Abstract. In this research, we target at the investigation of a dynamic
energy-aware management framework on the execution of independent
workloads (e.g., bag-of-tasks) in hybrid CPU-GPU PARA-computing
platforms, aiming at optimizing the execution of workloads in appropri-
ate computing resources concurrently whilst balancing the use of solely
virtual or physical resources or hybridly selected resources, to achieve the
best performance in executing application workloads and minimizing the
energy associated with computation selected. Experimental results show
that the proposed strategy can contribute to improving performance by
introducing optimization techniques, such as workload consolidation and
dynamic scheduling. We observed that workload consolidation can po-
tentially improve performance, depending on characteristics of the work-
load. Also, the workload scheduling results present the importance of
resource management by revealing the performance gap among different
execution schedules for shared computing resources.
Keywords: Virtualization, Workload Consolidation, GPGPU, PARA-
Computing Platforms
1 Introduction
With the rapid increase in the number of high performance computing envi-
ronments, the way to place dynamically requested application workloads into
available multi-core computing and storage resources has become an important
topic. In that sense, GPGPU, short for general-purpose computation on graphics
processing units, has already demonstrated its ability to accelerate the execu-
tion of scientific applications. These accelerated applications benefit from the
GPUs massively parallel, multi-threaded multiprocessor design, combined with
an appropriate data-parallel mapping of an application to it.
With the increasing number of this category of compute-intensive environ-
ments, we have observed there exist computing platforms composed by some
On Migration and Consolidation of VMs in Hybrid CPU-GPU Environments 3
Resource provisioning [9] in compute clouds often requires an estimate of the
capacity needs of Virtual Machines (VMs). The estimated VM size is the basis
for allocating resources commensurate with demand. A number of researches on
how resources may be allocated to an application mix such that the service level
agreements (SLAs) of all applications are met can also be found.
3 Proposed Research
Scientific and engineering workloads written as general purpose computations on
GPU and x86 multicore processors is ever increasing, in efforts to obtain higher
speedups. The way to obtain these computation results taking into consideration
their performance, energy consumption, and energy efficiency is important issue.
That is, the exploration of the CPU and GPU design spaces - threads, blocks, and
kernel launches, using CUDA and scheduling in x86 cores and their subsequent
impact to produce better results. One final target of this proposed research may
integrate all items listed into a single design. The goals in this research seek at
some important issues, as listed below:
– Impact on the ability to execute computing workloads that demand excel
management of VMs and servers toward high throughput at the cost of low
power consumption and high device reliability;
– Management and monitoring task support for energy-efficient scheduling of
VMs on physical servers in a high performance CPU-GPU PARA-Virtualization
computing environment toward energy-aware GPU and x86 computing, keep-
ing resources utilization at a desired level;
– Search and extract maximum amount of performance from the CPU pro-
cessor cores which they are capable of delivering under inter- and intra-die
process variations scheduled by OS, with migration of workloads appropri-
ately decided by taking into consideration several of system factors.
4 Experimental Results
4.1 Potential of Dynamic Workload Consolidation on GPU
Computing Environment
To investigate various aspects of designing CPU-GPU para-virtualized environ-
ment, we have conducted motivational experiments with a set of GPU applica-
tions. On virtualized environment, multiple VM instances share available physi-
cal GPU devices managed by hypervisor. However, we run experiments without
virtualized layer in this paper. In fact, one of the main goals of the virtualization
layer is to provide additional abstraction and flexibility of shared resources to
users, thereby ensures transparent access as if the resource is dedicated to each
user. In that sense, we believe that the virtualized configuration does not affect
to reflect nature of the system that is intended to show in this paper.
As experimental setup, we used Intel i5-2400 Quad-core PC with 8GB main
memory, NVIDIA GeForce GTX 550 GPU installed, running Ubuntu Linux 12.04
On Migration and Consolidation of VMs in Hybrid CPU-GPU Environments 5
Table 1. Impact on Application Pairs on Performance
Case Application Pair Sequential
Exec. Time(s)
Parallel
Exec. Time(s)
Speedup
A (Interval, radixSortThrust) 1.51 1.06 1.42
B (BlackScholes, scan) 4.51 3.63 1.24
C (radixSortThrust, BlackScholes) 2.95 2.39 1.23
D (Interval, scan) 3.1 2.58 1.20
E (Interval, BlackScholes) 2.88 2.26 1.27
F (radixSortThrust, scan) 3.16 2.88 1.10
Given that we have to map four different applications into two available
GPU space, there can exist six pair of applications. Due to runtime behavior of
applications, co-execution of each application shows different performance char-
acteristics. We run experiments for all possible cases, which result is presented in
Table 1. Case A showed 1.42x performance improvement when the application
is run in parallel, which is the greatest value, while the minimum value is 1.1x
for case F.
The result implies that an effective scheduler can optimize execution per-
formance. In detail, the optimal scheduling process can be described as follows.
We find three valid combinations of application scheduling pairs which complete
all four applications, which consisted of Case A+B, Case C+D, and case E+F,
respectively. For each schedule, each pair executes application in parallel while
the pairs are serialized due to resource constraint. Total execution time for each
case can be calculated from Table 1, which is 4.69s, 4.97s, and 5.14s, respectively.
In conclusion, we could observe that performance difference between worst and
best schedules, which are 4.69s and 5.14s, reaches up to 9.6%.
5 Conclusions
The goals of this proposed research involves dynamic energy-aware management
on the execution of independent workloads in hybrid CPU-GPU PARA Virtual-
ization computing platforms, aiming at optimizing the execution of workloads in
appropriate computing resources concurrently whilst balancing the use of solely
virtual or physical resources or hybridly selected resources, to achieve the best
performance in executing application workloads and minimizing the energy as-
sociated with computation selected.
Essentially, the proposed research involves the search of strategies that best
satisfies constraints of a multi-core computer system. We demonstrated how the
optimized management strategy can impact performance in virtualized environ-
ment in which multiple virtual machines share physical devices. Workload con-
solidation increases resource utilization and improves performance by merging
and executing workloads in parallel, considering the characteristics of workload.
Dynamic scheduling improves performance based on resource constraint and co-
execution characteristics of workloads. We prospect the further investigation of
IP Address Management in Virtualized 
Cloud Environments 
Kuan-Ching Li1    Chen-Hao Ku1   Ching-Hsien Hsu2    Kuan-Chou Lai3   
Meng-Yen Hsieh1   Tien-Hsiung Weng1   Hai Jiang4 
1Dept. of Computer Science and Information Engineering, Providence University, Taiwan 
2Dept. of Computer Science and Information Engineering, Chung Hua University, Taiwan 
3Dept. Computer and Information Science, National Taichung University, Taiwan 
4Dept. Computer Science, Arkansas State University, USA 
Abstract. The ability to deploy resources and services in cloud computing 
utilizing virtualization technologies in a timely and cost-effective manner is 
important, bringing significant economies and business performance, yet 
accelerating the pace of innovation. Services in Cloud platforms are scheduled 
and executed in optimized and on-demand resources, taking into consideration 
flexibility and scalability. Due to existing limitation on the quantity of allocated 
IPv4 IP addresses, a number of alternatives have been discussed. In this paper, 
we propose a novel yet low overhead method for cloud service providers based 
on the concept of Infrastructure as a Service (IaaS) framework and network 
virtualization. The idea behind the design is to respond quickly to the 
infrastructure needs for the creation of virtual machines (VMs) that meet 
capacity requirements of those cloud services as requested by users. It improves 
the utilization of providers' resources within this infrastructure as heterogeneous 
hardware resources are aggregated to existing platform on demand. Testing on 
the prototype built demonstrates its effectiveness and performance. 
Keywords: Cloud computing, Virtual Machine, IP address, Management.  
1   Introduction 
Most current Cloud computing systems are composed of large numbers of 
relatively inexpensive computers, interconnected by standard routers and huge 
number of disk drives. Virtualization is a fundamental technique in cloud computing 
that dynamically assigns and reassigns physical and virtual resources according to the 
services’ needs on demand. Network virtualization, as one of key enabling 
technologies for cloud has attracted extensive attention [1]. With this inclusion, it is 
able to take cloud services to increase the overall QoS by delivering optimized 
resources, on-demand utilization, flexibility and scalability [5].  
As ways to decrease troubleshooting in such cloud environments, there exist a 
number of techniques for deploying and managing computing systems with minimal 
human involvement. Moreover, with the impressive increasing number of services, 
the number of VMs (virtual machines) increases in the same pace. With the increasing 
Bottlenecks that hinder deployment, reduce performance, or result in downtime add 
cost to every operation. As a result, organizations can only be as agile as their IP 
infrastructure can support. For example, managing thousands of IP addresses by hand 
creates bottlenecks during provisioning and troubleshooting as well as increases the 
possibility of service outages caused by human error [2]. 
3 Proposed Mechanism 
In order to serve several users simultaneously in a model that dynamically assigns 
and reassigns virtual resources according to these consumers’ needs by temporal 
demand, schemes have been designed as building blocks in the proposed method, 
including resource description and virtual network request mechanisms. The proposed 
method is able to respond quickly to the infrastructure needs for those cloud services 
through a real-time based graphical interface. It improves the utilization of providers' 
resources that allows the trading of IP network resources between infrastructure and 
services, an important and complementary innovation within the cloud landscape. 
3.1 Environment Settings 
In order to operate in a stable computing environment, we have made use of one 
1U server containing two network interface cards (NICs) serving as a router. One of 
these NICs interconnects with outside internet, while the second card contains the 
functionality of virtual switch. In the proposed environment, we have used Ubuntu 
Linux 10.04 LTS server as underlying system, enabling also the NAT and DHCP 
services in this environment. In addition, KVM and QEMU have also been installed, 
in order to produce a reliable virtualization environment. The proposed system with 
implementation of a graphical interface is implemented, to provide users easy 
alternative to specify virtual resources they plan to have in virtual machine being 
designed, as depicted in Fig.1, as the type of processor and number of cores, amount 
of memory, amount of storage and finally, the operating system. The 1U server used 
in our experiments is IBM system x3550 M2, and its configuration includes 16 cores 
(2CPUs/4 cores HT), 16 GB DDR3 Memory and 2TB network storage. 
 
 
Fig. 1. Graphical user interface coupled with the proposed method. 
cleanups such a VM’s firewall rules, including also will unmark this virtual IP 
in the IP configuration file, as being available virtual IP address. 
3.3 Network Architecture 
As a cloud computing environment is constructed and some VMs have been 
created and in use, through directives to Linux operating systems’ IPtable to configure 
physical machine and VM packet transfer rule, the following connections are possible 
for any two VMs in the system, whether they are in the same network range or not, or 
a connection of VM to web.  
- Case 1: Inside same network range, through bridge, such a connection is 
feasible, 
- Case 2: VMs with virtual IPs in different network ranges, through bridge and 
routing directed by servers turn connection feasible, 
- Case 3: through bridge and routing path directed, VM can be connected to 
Internet, as depicted in Fig. 2. 
In a design which contains more than two physical servers, it is needed to 
configure static routing table or alternatively, to install exiting routing software as 
Quagga [X] to provide such a dynamic routing service. 
 
 
 
Fig. 2. Cloud environment routing schemes. 
 
4 Conclusions 
With the number of cloud computing environments being added each day, in 
parallel with the utilization of virtualization technologies to increase the usage of 
resources, the demand for IPs available is increasing, surpassing the total amount of 
Designing Parallel Sparse Matrix Transposition 
Algorithm Using CSR for GPUs  
Tien-Hsiung Weng1   Hoa Pham1   Hai Jiang2   Kuan-Ching Li1 
 
1
Dept. of Computer Science and Information Engr., Providence University, Taiwan 
2
Dept of Computer Science, Arkansas State University, USA 
Abstract. In this paper, we propose a parallel algorithm for sparse matrix 
transposition using CSR format to run on many-core GPUs, utilizing the 
tremendous computational power and memory bandwidth of the GPU offered 
by parallel programming in CUDA.  Our code is run on a quad-core Intel 
Xeon64 CPU E5507 platform and a NVIDIA GPU GTX 470 card. We measure 
the performance of our algorithm running with input ranging from smaller to 
larger matrices and our experimental results show that preliminary results is 
scaling well up to 512 threads and promising for bigger matrices.  
Keywords: CUDA, Sparse Matrix Transposition, Parallel Programming.  
1   Introduction 
Matrix transposition is a basic operation in linear algebra and this basic operation can 
be found in the software package such as MATLAB and LINPACK. The most recent 
works on parallel sparse matrix-transpose-vector multiplication using CSB has been 
proposed by Buluç et.al [2]; it is implemented using Cilk++ [3] in recursive style with 
no actual matrix transposition is required. However, it needs more effort to port it for 
GPU. Krishnamoorthy et. al [4] proposed efficient parallel algorithm for out-of-core 
matrix transposition, where a large dense matrix is stored in disk and when main 
memory has not enough memory to hold the whole matrix, the transposition is done 
by reading part of matrix at any time, and its goal is to minimize the number of I/O 
operations. Mateescu et.al [5] develops a dense matrix transpose to run on POWER7 
machine by utilizing its cache model and prefetching technique. Stathis et.al [6] 
proposes the parallel transposition of matrix using special hardware mechanism on 
vector architecture.  Gustavson et.al [7] proposed two fast serial algorithms for 
sparse matrix multiplication and permuted transposition using CSR, but they were not 
parallelized. 
Among sparse formats, CSR (Compressed Sparse Row) and CSC (Compressed 
Sparse Column) are the simplest and easy to understand and maintain.  It consists of 
three arrays, namely val, idx, and ptr as shown in Fig. 1.  Let A be a sparse matrix 
whose order is r by c and NNZ is number of nonzeros of A. The value of nonzero 
elements are stored in a double-precision array val of length NNZ. The column indices 
for each nonzero element are stored in the integer array idx of length NNZ.  The 
is a kernel entry point, and a function call transp<<< dimGrid, dimBlock >>> is 
used to launch from host in parallel across blocks of threads in device kernel. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 3.  CUDA kernel function for sparse matrix transposition code 
01 __global__ void transp(int *AT.idx, ……) {  
02 tid = blockIdx.x * blockDim.x + threadIdx.x; 
03 while (tid < NNZ) {      
04    temp = tex1Dfetch(tex_AT.prt,A.idx[tid])+  
tex1Dfetch(tex_off,tid);  
05    AT.idx[temp]=tex1Dfetch(tex_idxtemp, tid); 
06    AT.val[temp]=tex1Dfetch(tex_A.val,tid); 
07    tid += THREADS; 
08 } // endwhile   
09} 
 
Fig. 2.  Main program for sparse matrix transposition on CUDA 
01 Declare texture reference 
02 Define the grid size and the number of threads   
03 for i=1 to NNZ-1 do l 
04     off[i]= count[A.idx[i]]++; 
05 enddo 
06 AT.ptr[0]=0; 
07 for i=1 to c do //row pointer for AT 
08   AT.ptr[i]= AT.ptr[i-1] + count[i-1]; 
09 enddo 
10 h = 0; 
11 for k=0 to r-1 do 
12   for i= AT.ptr[k]to AT.ptr[k+1] do 
13      idxtemp[h++] = k; 
14   enddo 
15 enddo 
16 move arrays to device memory and bind as texture 
17 Allocate AT.idx, AT.val, yT in device memory and 
bind them as texture 
18 transp<<<dimGrid,dimBlock>>>(dev_AT.idx,); 
19 cudaThreadSynchronize(); 
 T0 T1 T2 T3 T0 T1 T2 T3 T0 T1 T2 T3 
| | | | | | | | | | | | 
0 1 2 3 4 5 6 7 8 9   
4 6 8 3 2 9 1 7 0 5   
Figure 4.  Fine-grain thread mapping of our parallel transpose  
.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table I 
Total execution time of parallel sparse matrix transposition on GPU using 2 and 512 
threads 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table I shows the name, size, number of nonzeros, and shaped of the matrices as well 
as detail of the execution time using 2 and 512 threads.  The matrices are listed from 
smaller size of nonzero values to larger.  Most matrices are square matrices, except 
Rucci1. The 3rd and 4th column of this table shows the total execution time of sparse 
matrix transposition in milliseconds (ms) using 2 and 512 respectively. The execution 
time includes the sequential part and data transfer from CPU to GPU device memory 
that we have discussed earlier how we measure the time.  In the case of Serena 
matrix, it is a 1.4Mx1.4M matrix with 64.13M of nonzero values, which is the biggest 
in eleven matrices, our parallel matrix transposition takes 8,412 ms using 2 threads 
and it reduces to 313 ms using 512 threads. 
 
name size NNZ 2 Threads 512 Threads 
Asic_320k 321Kx321K 
 
1,931K 683 29 
Sme3Dc 42K×42K 
 
3,148K 801 30 
Parabolic_fem 525Kx525K 
 
3,674K 560 26 
Rucci1 1,977K×109K 7,791K 1994 85 
Torso 116K×116K 8,516K 2161 78 
Kkk_power 2.06M×2.06
M 
12.77
M 
2118 95 
Rajat31 4.69M×4.69
M 
20.31
M 
5320 214 
Ldoor 952K×952K 42.49
M 
6083 225 
Bone010 986K×986K 47.85
M 
9242 337 
af_shell10 1.5M*1.5M 52.26
M 
6915 268 
Serena               1.4M×1.4M 64.13
M 
8412 313 
Fig. 5.  The performance of parallel sparse matrix transposition on GPU 
Parallel Sparse Matrix Transposition on GPU
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
2 4 8 16 32 64 128 256 512
Number of Threads
Ex
ec
ut
io
n 
Ti
m
e 
in
 m
s
Bone010
Serena
Af_shell10
Rajat31
Ldoor
Kkt_power
Rucci1
Torso1
Sme3Dc
ASIC_320k
Parabolic_fem
Deploying Scalable and Secure Secret Sharing with
GPU Many-Core Architecture
Su Chen, Ling Bai, Yi Chen, Hai Jiang
Department of Computer Science
Arkansas State University
{su.chen, ling.bai1, yi.chen}@smail.astate.edu, hjiang@cs.astate.edu
Kuan-Ching Li
Dept. of Computer Science and Information Engr.
Providence University, Taiwan
kuancli@pu.edu.tw
Abstract—Secret sharing is an excellent alternative to the
traditional cryptographic algorithms due to its unkeyed encryp-
tion/decryption and fault tolerance features. Key management
hassle faced in most encryption strategies is removed from
users and the loss of a certain number of data copies can
be tolerated. However, secret sharing schemes have to deal
with two contradictory design goals: security and performance.
Without keys’ involvement, large security margin is expected for
the illusion of being computationally secure. In the meantime,
such design will degrade the performance of “encrypting” and
“decrypting” secrets. Thus, secret sharing is mainly for small data
such as keys and passwords. In order to apply secret sharing to
large data sets, this paper redesigned the original schemes to
balance the security and performance. With sufﬁcient security
margin, Graphics Processing Unit (GPU) is adopted to provide
the performance satisfaction. The proposed secret sharing scheme
with GPU acceleration is a practical choice for large volume
data security. It is particularly good for long-term storage for
its unkeyed encryption and fault tolerance. Performance analysis
and experimental results have demonstrated the effectiveness and
efﬁciency of the proposed scheme.
I. INTRODUCTION
The goal of a secure system is to prevent data from being
lost, which concerns high conﬁdentiality and availability of
data. Conﬁdentiality means the data will become meaningless
if it is discovered by unauthorized people, which can be
protected by cryptographical algorithms. Availability means
that data risks from physical or logical loss. Physical loss
means the data or encrypted data can be erased or tampered.
Logical loss means that the key to the encrypted data can
be lost or forgot, which leads to the meaninglessness of
the existing encrypted data. Traditionally, physical loss is
alleviated by making duplications and logical loss is controlled
by key management.
Secret sharing can achieve data conﬁdentiality and availabil-
ity without keys’ involvement. Then, the logical loss of data
can be eliminated. In a sense, it is a safer approach. All secret
sharing schemes convert original secret/data/information into a
group of different shares distributed across among S people. If
T out of these S shares are collected, the original secret can be
recovered. However, it is impossible or extremely difﬁcult to
do so with less than T shares. The number of T is the threshold
of the secret sharing scheme. With few shares, attackers can try
all the possibilities using brute-force approach. Unfortunately
they are unable to identify which one is the actual secret, since
bigger threshold T imposes larger security margins.
In recent years, secure storage in Grid and Cloud computing
systems becomes more important since data are saved across
multiple organizations or in Cloud computing providers. Infor-
mation disclosure protection and fault tolerance consideration
are still the major concerns. Commonly used data encryption
with key-based cryptography algorithms faces several prob-
lems. First, key-based encryption risks key loss for long-
term storage. Second, key management implies extra burdens
to users. Third and last, redundancy for fault tolerance can
only be achieved through simple copy duplication. Therefore,
scalable unkeyed secret sharing is on demand.
In the design of practical secret sharing schemes, as the
bigger threshold enlarges the security margin, it also increases
the complexity. Since the algorithms of our secret sharing
2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops
978-0-7695-4676-6/12 $26.00 © 2012 IEEE
DOI 10.1109/IPDPSW.2012.173
1354
2012 IEEE 26th International Parallel and Distributed Proc ssing Symposiu  Workshops & PhD Forum
60
 Share S-1 
Share T 
Share T-1 
Share 1 
Share 0 
... 
... 
Data: 
a0 
a1 
... 
... 
... 
aT-1 
r0,0 r0,1 r0,2  ... r0,T-1 
r1,0 r1,2 r1,2 ... r1,T-1
 
... ... 
...  ... 
...   ... 
rT-1,0 rT-1,1 rT-1,2 ... rT-1,T-1
 
rT,0 rT,1 rT,2 ... rT,T-1
 
... ... 
...  ... 
...   ... 
rS-1,0 rS-1,1 rS-1,2 ... rS-1,T-1
 
a0 a1... aT-1  
y0 
y1 
... 
... 
... 
yT-1 
yT 
... 
... 
... 
yS-1  
h   
r0,0 r0,1 r0,2  ... r0,T-1 
r1,0 r1,2 r1,2 ... r1,T-1
 
... ... 
...  ... 
...   ... 
rT-1,0 rT-1,1 rT-1,2 ... rT-1,T-1
 
rT,0 rT,1 rT,2 ... rT,T-1
 
... ... 
...  ... 
...   ... 
rS-1,0 rS-1,1 rS-1,2 ... rS-1,T-1
 
Figure 2. Blakley’s Secret Sharing Scheme
in vector A. Any T rows in matrix R and their corresponding
values yi help determine all planes passing through point D
in T dimensional space. The right-hand side shows S shares.
Vector A is retrieved in data restoration phase to recover the
raw data.
C. Shamir’s Secret Sharing
 
Share S-1 
Share T 
Share T-1 
Share 1 
Share 0 
... 
... 
Data: 
a0 
a1 
... 
... 
... 
aT-1 
a0 a1... aT-1  
y0 
y1 
... 
... 
... 
yT-1 
yT 
... 
... 
... 
yS-1  
h   
1 x0 x0
2
  ... x0
T-1
 
1 x1 x1
2
 ... x1
T-1 
... ... 
...  ... 
...   ... 
1 xT-1 xT-1
2
 ... xT-1
T-1 
1 xT xT
2
 ... xT
T-1 
... ... 
...  ... 
...   ... 
1 xS-1 xS-1
2
 ... xS-1
T-1 
x0 
x1 
... 
... 
... 
xT-1 
xT 
... 
... 
... 
xS-1  
x0 
x1 
... 
... 
... 
xT-1 
xT 
... 
... 
... 
xS-1  
Figure 3. Shamir’s Secret Sharing Scheme
Shamir’s scheme is a special version of Blakley’s one.
When a point in T dimensional space is given, all associ-
ated hyper-planes are represented by normal vectors whose
elements are powers of the same numbers as shown in Fig.3.
Although some element combinations in Blakley’s scheme
will be lost, there still are inﬁnite possibilities to generate
shares in real domain. The right-hand side still represents S
shares which is much smaller than that geenrated by Blakley’s
scheme. In this case, Shamir’s scheme is more space-efﬁcient
than Blakley’s scheme since its share storage complexity is
O(T ) for normal vectors, whereas the one in Blakley’s scheme
is O(T 2).
D. Security vs. Performance
Secret sharing schemes always intend to balance two contra-
dictory design goals: security and performance. Improvement
of one factor might sacriﬁce the other one. Classic secret
sharing contains the following restrictions:
• Shamir’s scheme suggested that only the ﬁrst coefﬁcient
can be used for secret/data and other coefﬁcients should
be random numbers for security.
• Numbers involved in share generation and data restora-
tion might be large or troublesome, especially when
division is required in data restoration phase. Both large
and ﬂoating point numbers could cause accuracy and
performance problems.
• Normally, the bigger the threshold T is, the higher
security level will be. However, large threshold values
will introduce extra overheads in share generation and
data restoration.
• If data is large than threshold T , it should be truncated
into blocks with the size of T for multiple share genera-
tion and data restoration operations.
III. SCALABLE AND SECURE SECRET SHARING
The proposed scheme considers both security and perfor-
mance for practical use in Cloud computing systems. Perfor-
mance is emphasized without over-sacriﬁcing security issues.
A. Design Issues
Major restrictions in classic secret sharing schemes are
considered and eliminated.
1) Coefﬁcient Utilization: Placing secret/data only in the
ﬁrst coefﬁcient is inefﬁcient for large data sets. With threshold
T , data can be put into all T coefﬁcients [7]. If continuous
data items exhibit randomness characteristics, security will not
be sacriﬁced. Otherwise, a preprocessing randomization ﬁlter
can be applied to ensure security margin.
2) Data Representation Scalability: Finite ﬁelds such as
GF (28) can be employed to represent data in coefﬁcients
[8]. Both large and ﬂoating point number problems will be
resolved due to modular arithmetics and multiplicative inverse
135662
All data blocks form the matrix A and the coefﬁcient
matrix R is ﬁxed. Then the operation for share generation
is R × A = Y . The goal of data restoration is to recover
the matrix A. Matrix Multiplication (MM) is the most time-
consuming portion. For one data block, the computation
complexity in share generation is O(ST ), whereas the one
for data restoration is O(T 2).
Due to ﬁnite ﬁeld GF (28), the maximum number of shares
that can be generated with Shamir’s scheme is 255. In Fig.3,
the value of xi varies from 0 to 255. Then, S ≤ 255 and by
inference, T ≤ 255. This makes Shamir’s scheme even more
vulnerable for brute-force attacks since matrix X in Fig.3 can
be easily guessed out with only 256. For Blakley’s scheme,
the number of shares can reach O(255 ∗ T 2) which is much
larger. The only overhead is the extra storage, i.e., an S × T
matrix needs to be stored in shares.
Matrix R is generated stochastically. However, when S is
greater than T+1, there is no guarantee that all possible T out
of S rows in the randomly generated S×T matrix are linearly
independent. For example, later shares might repeat previous
ones. If such dependency happens, data cannot be restored
correctly and more shares are expected. However, when T
is large, the possibility for this kind of error to happen is
extremely small. The ﬁnal concatenated matrix R|Y in Fig.5
is the result for storage. Each row in this matrix can be treated
as one share. For security purposes, different share distribution
strategies can be used.
C. Operations for One Data Block
For each data block, operations in share generation and
data restoration are listed in Fig.6. The orders for these two
sequences are exactly opposite. In share generation phase,
random sequence addision helps bring in randomness and
avoid reoccurence situation, i.e., identical data blocks gen-
erates same shares. The random number sequences on both
ends should be the same for correctness. Then “Add Padding”
operation can set correct internal padding size. The following
“Multiply Matrices” can conduct R × A = Y operation.
Finally, matrix R|Y is constructed and partitioned for S shares.
In data restoration phase, T distinguished shares can recover
the original data as shown in Fig.6.
Since the share generation and data restoration phases share
the same modules with the same time complexity, only the
  
Message 
Add Random Seq 
Add Padding 
Matrix R 
Multiply Matrices 
Concatenate R 
Divide into Rows 
S Shares 
Known Random  
Sequence 
T Shares 
Message 
Add Random Seq 
Delete Padding 
Multiply Matrices 
Separate R 
Combine Rows 
Matrix R
-1
 
(a) Share generation (b) Data restoration 
Figure 6. Operations for One Data Block
share generation part is considered in later discussion.
IV. GPU CONFIGURATION FOR ACCELERATION
In secret sharing, data blocks (partitioned from raw data) as
well as operations in share generation and data restoration are
ideal data parallel cases. Graphics Processing Units (GPUs)
are perfect devices for acceleration and high performance.
However, GPU computing is different from the one for CPUs.
The awareness of hardware properties can beneﬁt performance
directly. Also, some software parameters should be set prop-
erly.
The aforementioned matrix multiplication is different from
the traditional one, which conducts the multiplication of two
numbers in ALUs and registers. The new design is based on
table checking to reduce the computation overhead signiﬁ-
cantly. However, it introduces 64 KB additional space to save
the multiplication table which leads to more frequent memory
accesses and further decreases the cache hit ratio. Therefore,
this new design needs to be tuned and evaluated thoroughly
for better performance.
A. NVIDIA Fermi GPU and CUDA
Compared to its former architectures, NVIDIA proposes
Fermi [5] in 2010 with several new features, such as the intro-
duction of conﬁgurable cache and more streaming processors
(core) in a streaming multiprocessor (SM). Many of the new
135864
and L1 cache takes 16 KB. This proportion can be reversed
to 1:3 by calling the function “cudaFuncSetCacheConﬁg()”
before any GPU-related code.
CUDA also provide an option of avoiding L1 cache or not
and it should be conﬁgured in the compilation phase. In order
to avoid hitting L1 cache for source code “program.cu”, the
corresponding compilation command should be “nvcc –arch
sm_13 –Xptxas –dlcm=cg program.cu”.
With these two types of conﬁguration, there can be four
different settings for the cache management on GPU. Secret
sharing performance on different L1 cache conﬁgurations are
shown in Fig.8.
 
4 8 16 32 64 128 256
S
c
a
le
d
 T
im
in
g
 G
r
a
p
h
Threshold T
Avoid L1, SM:L1=3:1 Avoid L1, SM:L1=1:3
Hit L1, SM:L1=3:1 Hit L1, SM:L1=1:3
Figure 8. Secret Sharing Performance with Variant Cache Conﬁgurations
When threshold T ≤ 128, GPU code prefers larger L1 cache
since the cache hit ratio is high. So when T is small (T ≤ 128),
L1 cache is encouraged to be used and set larger. In Fig.8,
when T ≤ 128, it is clear that the fourth setting “Hit L1,
SM:L1=1:3” is the best choice.
However, in the case of T = 256, cache hit ratio drops
signiﬁcantly. In this case, it may be better for GPU to skip the
step of searching data in cache, which is unnecessary in this
situation. In Fig.8, we can see that the ﬁrst and the second
settings, corrsponding to avoid L1 cache, becomes the top 2
choices. Through observation we can see the second setting is
better than the ﬁrst setting when T = 256. But the reason to
the superiority of second setting is unknown.
D. Other Optimization Considerations
To achieve better performance, some other features of GPU
are also considered, including the uses of asynchronous copy,
constant memory, shared memory and register reduction.
1) Asynchronous Copy: For large data, it is always better
to cut it into smaller pieces to be executed on devices.
CUDA driver supports asynchronous operations for both data
transfer and computations. This means that computations on
CPU, computations on GPU and the data transfers between
them can overlap with each other. Later Fig.14 and Fig.15
will demonstrate the potential overlapping in both CPU and
GPU implementations. Asynchronous copy can hide some
data transfer time here. Also, when GPU is performing its
computation, CPU can load new batch of data or do some
preprocessing work such as adding random sequences and
padding.
2) Constant Memory: NVIDIA Tesla-C2050 has 64 KB
constant memory, which is said to be as fast as L1 cache.
Coincidentally, GF (28) requires 256 × 256 Bytes = 64 KB
to save all possible multiplication calculation in a table for
better performance under ﬁnite ﬁeld GF (28). This could be a
perfect design. However, the introduction of constant memory
increases the execution time by about 4 times for secret
sharing. Therefore, such optimization is given up and the
multiplication table is placed in the global memory instead.
3) Shared Memory: Shared memory is another possible
way for performance gains. However, data sets in secret shar-
ing might be too big for shared memory. Then shared memory
is traded for bigger cache in some cases as aforementioned.
4) Register Usage Reduction: GPU code can be very sen-
sitive to the number of registers used by a single thread. Since
there are hundreds or thousands GPU threads, the register
ﬁles could be very large. The register ﬁle size of NVIDIA
Tesla-C2050 is 8, 196. This means that, if 1, 024 threads are
generated for concurrent execution, 8 registers can be assigned
to each thread. If the program has more than 8 scalar variables,
some of them will be mapped onto the local memory and drag
the performance down. The scalable secret sharing code has
been tuned so that registers are sufﬁcient for those variables
to achieve better performance.
V. PERFORMANCE ANALYSES AND EXPERIMENTAL
RESULTS
Sequential CPU and GPU codes are tested on a server with
two Intel Xeon E5504 Quad-Core CPUs (2.00 GHz, 4 MB
cache) and two NVIDIA Tesla 20-Series C2050 GPUs (448
13606
 0
5
10
15
20
25
0 50 100 150 200 250
S
p
e
e
d
-u
p
Threshold T
1MB 2MB
4MB 8MB
16MB
Figure 13. Speed-up of Total Execution (GPU vs. CPU)
cases, the computation time, including MM computation and
others, always occupies the largest proportion. The difference
is that GPUs help reducing much of the MM computation
time. So for GPU program, other computations, including
format transformation and padding processing, becomes the
major bottleneck when T is relatively smaller. However, as
T becomes larger, MM computation turns into the dominant
factor.
 
4 8 16 32 64 128 256
T
im
e
 P
r
o
p
o
r
t
io
n
Threshold T
I/O Other Computation MM Computation
Figure 14. Overhead Breakdowns of CPU Program
VI. RELATED WORKS
Long-term non-encryption secure data storage receives
much attention recently. POTSHARD [9], which is based on
secret splitting and RAID technologies, is a reliable system for
long-term storage. HASS [10] is a highly available, scalable
and secure distributed data storage system integrated with
identity-based encryption (IBE) and objected-based storage
devices (OSD). Shamir’s secret sharing scheme and internal
 
4 8 16 32 64 128 256
T
im
e
 P
r
o
p
o
r
t
io
n
Threshold T
Data Transfer I/O Other Computation MM Computation
Figure 15. Overhead Breakdowns of GPU Program
padding [7] were integrated to a ﬁle system for more conve-
nient ﬁle operations.
In 1979, Shamir [6] and Blakley [2] proposed two different
secret sharing schemes respectively. Their schemes are highly
secure but impossible to be used for common data storage
because of the scalability issus. In his paper, Shamir suggested
using modulo p to form a ﬁeld while maintaining ﬂoating
point operations to achieve security level. In 1983, Chinese
Remainder Theorem was introduced by Asmuth, Bloom [1]
and Mignottes [3] to develop new secret sharing schemes.
Their schemes got rid of ﬂoating point completely and im-
proved the performance to some extent. However it was still
hard for them to provide efﬁcient data storage. To achieve
better performance and apply secret sharing schemes to data
storage, some researchers trade unconditionally security while
maintaining enough security level. In Shen and Jiang’s work
[7], GF (28) is used in Shamir’s scheme for table checking.
Also, they expand the range to all bits for secret hiding.
These designs improved performance to a large extent however
over-sacraﬁced the security. The proposed scheme adopted
Blakley’s approach to avoid the serious security hole, added
a new module to prevent the potential attack on highly
structured message and optimized the whole processes of share
generation and data restoration. Compared to Shen and Jiang’s
work, this new scheme for data management is faster, more
secure, more ﬂexible and more scalable.
Fermi architecture [5] and its corresponding programming
toolkit [4] were released by NVIDIA in 2010. The linear
algebra libraries has been regularly updated. In 2011, a latest
implementation of matrix multiplication on NVIDIA GPUs
13628
A Secure Distributed File System Based on Revised
Blakley’s Secret Sharing Scheme
Su Chen1, Yi Chen1, Hai Jiang1, Laurence T. Yang2, Kuan-Ching Li3
1Department of Computer Science, Arkansas State University, USA
2Department of Computer Science, St. Francis Xavier University, Canada
3Department of Computer Science and Information Engr., Providence University, Taiwan
Abstract—To support cloud storage effectively, a Distributed
File System (DFS) should be well-rounded with excellent features
in multiple major aspects and without signiﬁcant drawbacks. The
main design goals of a DFS in Clouds include security, reliability
and scalability. Traditionally, cryptography, data duplication and
powerful machines are common approaches to support DFS.
However, the success of such a DFS will depend on cumber-
some key management, large storage and costly infrastructure,
respectively. This paper intends to revise Blakley’s secret sharing
and apply it to a DFS for both security and reliability without
sacriﬁcing the scalability in performance too much. A DFS is
deployed with GPU (Graphics Processing Unit) as an acceleration
option to tackle with scalability issue further. Experimental
results have demonstrated the effectiveness of the new DFS.
I. INTRODUCTION
As cloud computing becomes pervasive in recent years,
issues related to data such as security and reliability become
paramount. Although cloud storage services such as Dropbox
provide great convenience to the public, the conﬁdentiality
of uploaded data cannot be guaranteed. In order to avoid the
exposure of some personal data such as credit card and social
security numbers, users have to avoid sending sensitive data
ﬁles to cloud providers. Because of such security concern, the
growth of public clouds has slowed down. In the meantime,
as more and more nodes are added into clouds, the crush
of hardware becomes a regular case. Therefore, a robust and
efﬁcient design for data recovery from system crush becomes
indispensable. In clouds, a secure and reliable Distributed File
System (DFS) is on demand.
Traditionally, cryptography is used to solve conﬁdentiality
issue. However, this key-based data encryption/decryption
approach incurs another issue: key management, which causes
inconvenient cloud management, especially for long-term stor-
age. If keys are lost or misplaced, original data will be unre-
coverable. Unkeyed security schemes become more attractive.
Secret sharing was proposed by Shamir and Blakley in 70’s
separately, which can hide secret information in a number of
different shares and recover the original secret by collecting
enough shares without any key involvement.
In the mean time, redundancy is a popular strategy to
achieve data reliability. Data items are replicated and dis-
tributed across many machines. Losing some of them will not
prevent from retrieving the original ones. A certain number
of machine failures can be tolerated. Coincidentally, secret
sharing can be applied for redundancy as well, so shares
can replace the identical data copies. Then, both security and
reliability issues can be resolved together.
However, the original secret sharing was designed to handle
small data items, such as secrets and encryption keys. For large
data sets, the corresponding computational overhead will be
signiﬁcant. When secret sharing is conﬁgured to handle big
data, its security level will drop as well. There is a tradeoff
between security and data processing speed. Therefore, it is
improper to apply the original secret sharing to data storage
directly.
This paper intends to propose a secret-sharing based dis-
tributed ﬁle system, which will support cloud storage with
unkeyed security, acceptable processing speed, reasonable
storage cost, and sufﬁcient reliability. The design purpose is to
determine a balancing point so that all aforementioned features
are reasonably supported without degrading any of them. This
paper makes the following contributions:
• Differences between Shamir’s and Blakley’s secret shar-
2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications
978-0-7695-4745-9/12 $26.00 © 2012 IEEE
DOI 10.1109/TrustCom.2012.56
310
scheme is still widely used although it is no longer safe under
Galois ﬁelds. Comparisons between Shamir’s and Blakley’s
schemes are given below in terms of security, ﬂexibility, space
complexity and time complexity. All the following discussion
is based on GF(28).
1) Flexibility: First of all, compared to rational number
ﬁeld, GF(28) is a much more vulnerable to brute force attack.
Under rational number ﬁeld, Shamir’s scheme could give
inﬁnite number of different coefﬁcient vectors; but under
GF(28), only 256 choices exist. This means it is impossible
for a user to split his secret into more than 256 pieces. On
the other hand, Blakley’s scheme has about 28T choices for
its coefﬁcient vectors, which is big enough for share number
S when T is not too small.
2) Security: Although secret sharing is treated as an “un-
keyed” approach, it actually concatenates the “key” matrix,
which is the aforementioned coefﬁcient matrix, to the “cipher-
text.” So, the coefﬁcient matrix should be generated in a very
secure way. Clearly, Shamir’s 256 possibilities for coefﬁcient
vector is not so secure and could be easily guessed out.
Combined with insufﬁcient shares obtained by adversaries, an
insecure coefﬁcient matrix secret will lead to a faster discovery
of the secret.
3) Time Complexity: Matrix multiplication is the major
operation for secret sharing. Although these two schemes
use different ways to generate the coefﬁcient matrix, later
operations are almost the same. Thus their time complexity
is in the same level.
4) Space Complexity: Original secret sharing schemes re-
quires different coefﬁcient matrices across multiple processing
blocks to achieve extremely high level of security. However,
this space-inefﬁcient design has to be abandoned for less
cost and better performance. Right now, the coefﬁcient matrix
does not have to vary among different blocks. However,
such simpliﬁcation erases the space complexity advantage of
Shamir’s scheme. As the data size increases, more and more
blocks share the same coefﬁcient matrix and make the spacial
cost negligible.
Overall, Blakley’s scheme is more ﬂexible, secure and
scalable for massive data processing.
III. REVISED SECRET SHARING FOR DFS
Blakley’s secret sharing scheme is selected and customized
to implement our distributed ﬁle system. Large data will
be split and random numbers will be added to improve the
security level in generated shares, which can be used to extract
the original data as well.
  
Message 
Add Padding 
Insert & XOR RBs 
Matrix R 
Multiply Matrices 
Concatenate R 
Divide into S Rows 
S Shares T Shares 
Message 
Delete Padding 
XOR & Delete RBs 
Multiply Matrices 
Separate R 
Combine T Rows 
Matrix C = R
-1
 
(a) Share generation (b) Data restoration 
Figure 2. Secret Sharing Flowcharts
A. Large Data Handling
In a ﬁle system, especially in DFS, there are many large ﬁles
that cannot be loaded into memory at once. In this case, they
have to be cut into smaller trunks to be processed. In Google
ﬁle system [4] and Hadoop DFS [6], size for one trunk is 64
MB. In our DFS, the trunk size is not about the original data,
but for the shares. For example, if we need the share size to
be 64 MB, we need to calculate the actual data trunk size
based on the threshold T and the inner structure of data. The
actual trunk size in our DFS will be given in the beginning
of Section V. Then, secret sharing can be applied to these
trunks. Both share generation and data restoration consist of
six phases as shown in Fig. 2.
B. Share Generation
The six phases in share generation include: Padding Ad-
dition, Internal Random Bytes (IRB) Insertion and XOR,
Random Matrix Generation, Matrix Multiplication, Matrix
Concatenation, and Share Division. Fig. 3 gives a detailed
example of share generation from a small trunk of data.
312
C. Data Restoration
The six phases of data restoration are: Share Combination,
Matrix Extraction, Gauss Elimination, Matrix Multiplication,
IRB XOR and Deletion, and Padding Deletion, as shown in
Fig. 4.
• Share Combination: Enough shares should be collected
and combined together to form the matrix as shown in
Fig. 4. At least T shares should be gathered so that the
dimension of matrix R can be T × T .
• Matrix Extraction: Once T shares are collected, a partial
matrix RT×T can be built for later use.
• Gauss Elimination: According to the principles of
RS×T ’s generation, the extracted matrix RT×T should
be invertible. With Gauss elimination, its inverse matrix
CT×T = (RT×T )−1 can be calculated for matrix multi-
plication operation.
• Matrix Multiplication: This stage is similar to the one
in share generation. The process can be expressed as
CT×T × YT×n = BT×n.
• IRB XOR and Deletion: With matrix B, an inverse
procedure can be applied to recover the data step by step.
In this phase, the last byte for each block needs to XOR
with the ﬁrst T − 1 bytes. Then the last byte in each
block, except the one in the last block, will be discarded.
• Padding Deletion: The last byte of the last block indicates
the padding size. With it, all padding is removed to
recover the original data trunk accurately.
IV. DESIGN ANALYSIS OF REVISED SECRET SHARING
To apply secret sharing to DFS, it has to exhibit sufﬁ-
cient security, scalability and reliability. The revised Blakley’s
scheme needs to balance them well without degrading any of
them too much.
A. Security
Originally, secret sharing was designed to be uncondition-
ally secure. This means the number of attempts in brute-force
attack can be signiﬁcantly large due to the use of ﬂoating
point numbers. However, with GF(28), only computational
security can be achieved. The security margin shrinks as the
number of obtained shares increases. When T − 1 shares are
obtained by an adversary, the plaintext can be easily broken
by trying less than 28 times since these shares can form T −1
linear equations with T unknown values and GF(28) limits the
unknown values to a only 256-element set. One meaningful
guess might indicate a successful attack if the data is a readable
message, not in binary format. Such attack can be applied to
every block.
On the other hand, the random matrix R is much harder
to break since it has larger search space than the plaintext.
However, the security of the whole scheme is determined by
the weakest point, i.e., the plaintext.
To maintain a large enough security margin, T cannot be
treated as the secure threshold alone. Assuming 264 attempts
from a brute-force attack is considered secure based on current
computing technologies, the relationship between the number
of obtained shares and their corresponding security levels are
listed in Fig. 5. To design secure distributed ﬁle systems,
the revised secret sharing scheme should meet the following
requirements:
• T > 8
• Every untrusted share holder can only be assigned k
shares, where k  (T − 8).
Share Number Search Space Security Level 
S 1 None 
... ... ... 
T 1 None 
T-1 2
8
 Vulnerable 
... ... ... 
T-7 2
56
 Vulnerable 
T-8 2
64
 Computationally Secure 
... ... ... 
T-k 2
8k
 Secure 
... ... ... 
0 2
T
 Unconditionally Secure 
 
Figure 5. Security Levels with Various Numbers of Collected Shares
For example, if the implemented DFS is based on (T, S) =
(9, 20) secret sharing and none of share holders can be
totally trusted, only one share can be assigned to each share
holder. Also, by inference, at least 20 share holders should
be employed. In reality, splitting a ﬁle into 20 shares will
incur heavy communication overheard later on. So it is better
to set T larger instead. In current major ﬁle systems [4],
[6], normally three ﬁle copies exist in the system for data
redundancy. To meet a similar design goal, a possible option
314
V. DISTRIBUTED FILE SYSTEM DESIGN
To meet the requests from cloud storage, a secure distributed
ﬁle system is developed based on the revised Blakley’s secret
sharing. The design goals include security, scalability and
reliability.
A. System Layout
A secret sharing based DFS contains an index node, some
compute nodes and data nodes as shown in Fig. 6. A traditional
DFS does not need special devices for the computational work.
However, the secret sharing’s involvement turns the computa-
tion work, instead of I/O operations, into the bottleneck. GPU
and NVIDIA’s CUDA help accelerate the computing process.
[3]
The index node manages the status of compute and data
nodes by positively requesting their status periodically and
passively receiving status reports from them. In a client’s point
of view, the index node acts as the receptionist . Once it assigns
proper compute nodes for share generation and data restoration
as well as proper data nodes for share storage, it will phase out.
The client will communicate with compute nodes directly and
the latter ones work with data nodes internally. In the current
system, each client ﬁle will need one compute node and four
data nodes to maintain sufﬁcient security and reliability levels.
B. File Operations
POSIX-like ﬁle operation APIs have been developed so that
this secret sharing based DFS can be accessed easily in clouds.
1) s2fs_open(): Open operation informs DFS to prepare
for possible read or write operations. First, the client sends a
request to the index node with information of ﬁle name and
open mode. Then index node chooses an available compute
node and forwards the ﬁle name to it along with the chosen
data node information. Finally, index node responds the client
with the connection information of the compute node.
2) s2fs_lseek(): The seek operation requires more efforts
to implement. Given an offset in the original data, the offset
in shares needs to be calculated. Also, since one byte in the
share is corresponding to multiple bytes in original data, two
more inner offsets are needed to recover the data accurately.
In the current version of our DFS, the seek operation only
supports read mode. The compute node will inform the client
if the seeking process is successful or the offset is out of the
range of the associated ﬁle.
3) s2fs_read(): After open operation and a possible seek
operation, a client is able to read the ﬁle. First, the client sends
a read request to the compute node with a data length. Then,
the compute node calculates a read length on shares based on
the offset calculated in seek operation. After that, the compute
node contacts the relevant data nodes. Next, the data nodes
read the relevant data sections and portions of matrix R from
share ﬁles, and send them to the compute node. Then, the
compute node recovers data from the received shares, cuts the
possible unwanted head and tail bytes, and sends the data back
to the client. In the end, the compute node tells index node
that the reading work is done.
4) s2fs_write(): To clients, write operation is accom-
plished much faster than read because an “early return”
strategy is applied. This asynchronous write design can prevent
the client from being blocked for further operations. First, the
client sends write request to the compute node with data length
and data itself. Then, the compute node receives the data and
replies client with a message of successful writing so that the
client can do something else. Next, the compute node begins
transforming the data into shares and sending them to relevant
data nodes, which save shares into data chunks of a ﬁxed
size. Finally, data nodes will inform the compute node if the
operation is successful or not.
5) s2fs_close(): Close operation is not absolutely neces-
sary since share ﬁles on data nodes are opened and closed for
all operations. However, it helps the compute node release the
thread generated for the client earlier.
C. Experimental Results
The performance of secret sharing based distributed ﬁle
system is evaluated on the server side. The communication
time between the client and the compute node is not counted.
Each timing in Fig. 7 is an average value of ﬁve independent
tests.
In Fig. 7, two test groups are developed for different security
levels. With T = 16, the security level is similar to the one in
64-bit DES and with T = 32 it is like a 128-bit DES. For each
test group, short and long operations are conducted for both
read and write. Also, the corresponding total sizes of the shares
stored on data nodes are given. It is clear that GPU approach
316
Exploiting Dynamic Distributed Load Balance by 
Neighbor-Matching on P2P Grids
1Po-Jung Huang, 1You-Fu Yu, 1Kuan-Chou Lai, 2Ching-Hsien Hsu, and 3Kuan-Ching Li
1Department of Computer and Information Science 
National Taichung University, 
Taichung, Taiwan 
kclai@mail.ntcu.edu.tw
2Department of Computer Science and Information Engineering 
Chung Hua University 
Hsinchu, Taiwan 
chh@chu.edu.tw
3Department of Computer Science and Information Engineering 
Providence University 
Taichung, Taiwan 
kuancli@pu.edu.tw
Abstract—Recently, more and more researches and applications 
exploit grid computing systems to deal with high performance 
computing. However, the mass data transmissions across 
different grid sites affect overall computing performance. 
Therefore, grid systems start to integrate with the P2P 
technology to support the high performance distributed 
computing. The new distributed computing system is named the 
P2P Grid computing system. Although the P2P Grid computing 
system combines the advantages of the grid computing system 
and the P2P technology, some issues are still needed to be solved. 
For example, the highly variable resource usage and the 
heterogeneity of resources could intensely affect the P2P Grid 
system performance. In this case, the computing performance 
depends on the resource management policy.  Therefore, this 
study proposes a distributed dynamic load balance policy to 
manage resources more effectively and to further improve the 
resource utilization. The prototype is implemented on the sites of 
the Taiwan UniGrid, and the P2P grid sites exchange 
information by JXTA advertisements. Experimental results show 
that the proposed algorithm could efficiently distribute the 
workload for execution; that is, it not only can minimize the job 
execution time, but also maximize the resource utilization. 
Keywords-- P2P; Grid; Distributed; Dynamic; Load balance policy 
I. INTRODUCTION
Grid computing is one of emerging high performance 
platforms. Such a system combines distributed resources (e.g., 
the computer, the database, the storage device, etc.) by a 
network to form a virtual supercomputing system. These 
geographic distributed resources may belong to different virtual 
organizations (VOs), the grid computing system exploits 
middleware to exchange information and coordinate resources 
across VOs. Therefore, grid systems support distributed 
dynamic computing resources of heterogeneous virtual 
organizations. Peer-to-Peer (P2P) computing is another 
emerging distributed platform. In general, P2P computing is 
used in the resource-sharing system (e.g., the disk, the 
bandwidth, the file, etc.). Such a P2P system is composed of 
peers which are not only the resources’ suppliers but also the 
resources’ consumers in the P2P system. Therefore, P2P 
systems support a flexible resource sharing model. In order to 
enhance the resource management and improve the overall 
resource usage, grid computing systems usually integrate with 
the P2P architecture to support high-performance distributed 
computing. This is named the “P2P grid computing system.” 
In order to achieve high performance in P2P grid computing, 
the load balance policy is one of the key issues in the effective 
resource management. The load balance policy promotes the 
resource management to distribute the workload from high-
loading computing sites to low-loading computing sites for 
maximizing the resource utilization and minimizing the job 
execution time. 
Load balance policies must consider the following problems 
[7]: how to collect the information from sites in the distributed 
system, how to manage the resource for maximizing the usage 
of the resource, how to minimize the job execution time and 
maximize the throughput, and how to handle the dynamic 
resource at any time? 
In general, load balance policies could be categorized into 
the static/dynamic ones or the centralized/decentralized ones. 
The static policy is easy to be implemented; but, it usually 
couldn’t obtain the optimal performance. The dynamic policy 
makes decisions of resource allocations at runtime to obtain the 
better performance. But the dynamic approach needs to collect 
the dynamic information to make the optimal decision. The 
centralized policy adopts one computing node as the resource 
manager which collects the system information and makes the 
load balance decisions. But, since the centralized policy needs 
one node as the manager, the efficiency of the node is the 
bottleneck in distributed systems. The decentralized policy 
allows every site to make load balance decisions. However, it 
is very expensive to obtain and maintain the dynamic system 
information. 
The rest of this paper is structured as follows. Related 
works are discussed in Section II, followed by our load balance 
policy discussed in Section III. The experimental results are 
2011 IEEE Asia -Pacific Services Computing Conference
978-0-7695-4624-7/11 $26.00 © 2011 IEEE
DOI 10.1109/APSCC.2011.52
131
information from the neighbor sites by using the P2P approach. 
In this phase, some parameters are evaluated, e.g., the average 
waiting time (TAW), the weight priority (WR) and the site 
resource usage (SRU), etc.. These parameters are used to select 
neighbors and to define the site status in NLB. 
In the job-transferring phase, if there is at least one idle job, 
the load balance policy is triggered. The NLB approach 
transfers jobs to the neighboring site with the minimal job 
turnaround time (TNLB).
Before introducing the neighbor-selecting phase, some 
terms are pre-defined, i.e., the TRJ, TIJ, TJR, TAW, WR, RU, SRU,
TRS and NI.
Let TRJ and TIJ respectively be the execution time of 
running jobs and the idle jobs. We define the total job 
responding time (TJR) in Eq.(1).  
    JR RJ IJT T T             (1) 
Let TAW be the estimated waiting time in the site, and NoC
is the number of CPU in the site. We define the average 
waiting time (TAW) in Eq.(2). 
JR
AW
T
T
NoC
                      (2)
Let WR be the weighted priority of the site, and Max_TAW
be the maximal TAW in the neighboring candidates and the local 
sites. We define the weight priority (WR) in Eq.(3). 
_
AW
AW
T
WR
Max T
                        (3) 
Let RUk(i) denote the resource usage of resource k in the 
site i, where k = 1, …, m, and the m is the number of types of 
resources in the P2P grid system. Let Wk(i) be defined as the 
weight of resource k in the site i. And we define the site 
resource usage (SRU) as resource usage in the site, then 
1 1( ) ( ) * ( )  ......  ( ) * ( )k kSRU i   W i RU i W i RU i   (4)
Let neighbor-reselecting time (TRS) denote the time interval 
of re-selecting neighbor sites. We define the neighbor index 
(NI) in Eq.(5) for selecting neighboring sites. Let NI(i) be the 
neighbor index. Then, 
( ) 1 -  * ( )NI i   WR SRU i                                   (5) 
According to the NI and WR in Table 1, we define four 
cases for sites, as shown in the following: 
In case 1, the resource usage in the site is low, and the site 
can execute jobs from other sites quickly.  
In case 2, the resource usage in the site is high, but the site 
can execute jobs from other sites quickly.  
In case 3, the resource usage in the site is low, but the site 
cannot execute jobs from other sites quickly.  
In case 4, the resource usage in the site is high, and the site 
cannot execute jobs from other sites quickly. 
Let Avg_WR be the average of all WR in neighbor 
candidates and the local site; and, let Avg_NI be all average of 
NI in neighbor candidates. Table 1 shows different 
combinations of the site’s cases. 
Table 1. Different combinations of the site’s cases. 
State of WR State of NI Case of the site
WR̰ Avg_WR NI˚ Avg_NI 1
WR̰ Avg_WR NḬ Avg_NI 2
WR > Avg_WR NI˚ Avg_NI 3
WR > Avg_WR NḬ Avg_NI 4
Algorithm NLB 
Input : LocalSite LS, Job j, Site_List SL, total number of sites N, time 
interval TRS
Output : The optimal execution site of job j
Begin 
Main () { 
LS’s neighbors = Find_Neighbors(LS, N)
time_last = time_now
while ( the number of jobs in LS > 0 ) { 
if ( time_now – time_last) > TRS { 
LS’s neighbors = Find_Neighbors( LS, N ) 
time_last = time_now
}
Find out the first idle job j in the local job queue 
Calculate the TRJ, TIJ, TJR, TJ, TT, TAW, TNLB of neighbor sites 
if the TNLB of job j in LS > the TNLB of job j in some LS’s neighbor 
sites { 
if TAW - TT > Migration Threshold { 
Find out the neighbor with the minimal TNLB to be candidate 
Migrate job j to the candidate site 
}
}
else { 
Calculate the TRJ, TIJ, TJR, TJ, TT, TAW, TNLB of non-neighbor sites 
if TAW - TT > Migration Threshold { 
if the TNLB of job j in LS > the TNLB of job j in some non-
neighbors { 
Find out the site with the minimal TNLB to be candidate 
Migrate job j to the candidate site 
}
}
}
}
Return the neighbor site with the minimal TNLB
}
Find_Neighbors( LS, N ){ 
Get the minimal Nr, s.t. Nr6 > N.
  Find at most 2* Nr site to the neighbor candidate queue 
  Defines the case 1 - 4 of the sites according to NI and WR
  while ( number of neighbor sites < Nr) { 
Random select the site with the minimal case as the neighbor site 
Deletes the site from neighbor candidate queue 
  } 
}
End
133
Figure 1. Execution time (f77split) 
Figure 2. Execution time (fd_predator_prey) 
Figure 3. Execution time (fd1d_heat_explicit) 
Figure 4. Execution time (linpack_bench) 
Figure 5. Execution time (satisfiability) 
Figure 6. Number of information exchange (f77split) 
135
Figure 13. Average CPU utilization in 50 jobs (fd1d_heat_explicit) 
Figure 14. Average CPU utilization in 50 jobs (linpack_bench) 
Figure 15. Average CPU utilization in 50 jobs (satisfiability) 
Figure 16. Average CPU utilization in 100 jobs (f77split) 
Figure 17. Average CPU utilization in 100 jobs (fd_predator_prey) 
Figure 18. Average CPU utilization in 100 jobs (fd1d_heat_explicit) 
137
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
 
                                                             2012 年 5 月 28 日 
報告人姓名  李冠憬 
 
服務機構
及職稱 
靜宜大學 資訊工程學系 
教授 
     時間 
會議 
     地點 
2012 年 7 月 21 日- 25 日 
中國、上海 
本會核定
補助文號
 
NSC 100-2221-E-126 -006 -  
會議 
名稱 
PDSEC’2012/IPDPS’2012 
The 13th IEEE International Workshop on Parallel and Distributed Scientific 
and Engineering Computing, in conjunction with IPDPS’2012 IEEE 26th 
International Parallel & Distributed Processing Symposium 
發表 
論文 
題目 
Deploying Scalable and Secure Secret Sharing with GPU Many-Core 
Architecture 
表 Y04 
四、攜回資料名稱及內容 
 
1 copy of CD Proceedings: IPDPS’2012 Main conference and Workshops, published by IEEE 
Computer Society and CPS. All publications are also available through IEEE Xplore digital 
library. 
 
 
五、其他 
 
 
出席國際學術會議報告 
出席人員：洪哲倫 
靜宜大學資訊傳播工程學系  助理教授 
會議名稱：資訊科學與技術國際會議 
一、參加會議經過 
資訊科學與技術國際會議 IST 2012 於 2012 年 4 月 28 日至 30 日在 Shanghai, China
舉辦。 
本人於 4 月 29 日上午 Technical Session 12，發表兩篇論文 Dynamic Power-Saving 
Model for Cloud Computing System 與 Efficient Load Balancing Algorithm for Cloud 
Computing Network。本人出席了會議 session (ISA, IST, AFIT) 及 keynote speech，以期瞭
解目前世界上各個研究單位在雲端運算、資料探勘與影像處理等最新的研究方向以及成
果。 
二、與會心得 
資訊科學與技術國際會議的規模年年增長，雲端運算領域為會議其中一項重要的議
題。這顯示出了雲端運算與高效能運算、平行運算、以及網路服務研究的重要性，目前
世界各地學者在這些方面的研究仍然持續地在創新以及進展中。 
由於電腦硬體不斷的快速發展，以及新興高速網路架構的出現，雲端運算是目前很
熱門的研究主題，而相關的研究應用更是不勝枚舉。因此各種雲端運算的研究，在今年
的研討會中出現了相當多的相關主題。其中包含了多核心雲端平台運算、圖形處理器運
算、分散式處理及演算法、叢集系統、格網系統及互連網路演算法等。其中值得注意的
是，隨著雲端運算的快速發展，對於高效能雲端系統、雲端大型資料運算、以及雲端排
程的相關研究與應用也益發蓬勃。 
在雲端運算及相關軟硬體技術日益進步之際，也連帶著引發出許多新的研究議題，
如圖形處理器運用於建立高效能雲端運算平台，運算資料的分配以及區域化等等，由此
可見，雲端運算用於平行處理、高效能運算和網路應用在未來的數年之內還是有許多研
Deploying Scalable and Secure Secret Sharing with
GPU Many-Core Architecture
Su Chen, Ling Bai, Yi Chen, Hai Jiang
Department of Computer Science
Arkansas State University
{su.chen, ling.bai1, yi.chen}@smail.astate.edu, hjiang@cs.astate.edu
Kuan-Ching Li
Dept. of Computer Science and Information Engr.
Providence University, Taiwan
kuancli@pu.edu.tw
Abstract—Secret sharing is an excellent alternative to the
traditional cryptographic algorithms due to its unkeyed encryp-
tion/decryption and fault tolerance features. Key management
hassle faced in most encryption strategies is removed from
users and the loss of a certain number of data copies can
be tolerated. However, secret sharing schemes have to deal
with two contradictory design goals: security and performance.
Without keys’ involvement, large security margin is expected for
the illusion of being computationally secure. In the meantime,
such design will degrade the performance of “encrypting” and
“decrypting” secrets. Thus, secret sharing is mainly for small data
such as keys and passwords. In order to apply secret sharing to
large data sets, this paper redesigned the original schemes to
balance the security and performance. With sufﬁcient security
margin, Graphics Processing Unit (GPU) is adopted to provide
the performance satisfaction. The proposed secret sharing scheme
with GPU acceleration is a practical choice for large volume
data security. It is particularly good for long-term storage for
its unkeyed encryption and fault tolerance. Performance analysis
and experimental results have demonstrated the effectiveness and
efﬁciency of the proposed scheme.
I. INTRODUCTION
The goal of a secure system is to prevent data from being
lost, which concerns high conﬁdentiality and availability of
data. Conﬁdentiality means the data will become meaningless
if it is discovered by unauthorized people, which can be
protected by cryptographical algorithms. Availability means
that data risks from physical or logical loss. Physical loss
means the data or encrypted data can be erased or tampered.
Logical loss means that the key to the encrypted data can
be lost or forgot, which leads to the meaninglessness of
the existing encrypted data. Traditionally, physical loss is
alleviated by making duplications and logical loss is controlled
by key management.
Secret sharing can achieve data conﬁdentiality and availabil-
ity without keys’ involvement. Then, the logical loss of data
can be eliminated. In a sense, it is a safer approach. All secret
sharing schemes convert original secret/data/information into a
group of different shares distributed across among S people. If
T out of these S shares are collected, the original secret can be
recovered. However, it is impossible or extremely difﬁcult to
do so with less than T shares. The number of T is the threshold
of the secret sharing scheme. With few shares, attackers can try
all the possibilities using brute-force approach. Unfortunately
they are unable to identify which one is the actual secret, since
bigger threshold T imposes larger security margins.
In recent years, secure storage in Grid and Cloud computing
systems becomes more important since data are saved across
multiple organizations or in Cloud computing providers. Infor-
mation disclosure protection and fault tolerance consideration
are still the major concerns. Commonly used data encryption
with key-based cryptography algorithms faces several prob-
lems. First, key-based encryption risks key loss for long-
term storage. Second, key management implies extra burdens
to users. Third and last, redundancy for fault tolerance can
only be achieved through simple copy duplication. Therefore,
scalable unkeyed secret sharing is on demand.
In the design of practical secret sharing schemes, as the
bigger threshold enlarges the security margin, it also increases
the complexity. Since the algorithms of our secret sharing
2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops
978-0-7695-4676-6/12 $26.00 © 2012 IEEE
DOI 10.1109/IPDPSW.2012.173
1354
2012 IEEE 26th International Parallel and Distributed Proc ssing Symposiu  Workshops & PhD Forum
60
 Share S-1 
Share T 
Share T-1 
Share 1 
Share 0 
... 
... 
Data: 
a0 
a1 
... 
... 
... 
aT-1 
r0,0 r0,1 r0,2  ... r0,T-1 
r1,0 r1,2 r1,2 ... r1,T-1
 
... ... 
...  ... 
...   ... 
rT-1,0 rT-1,1 rT-1,2 ... rT-1,T-1
 
rT,0 rT,1 rT,2 ... rT,T-1
 
... ... 
...  ... 
...   ... 
rS-1,0 rS-1,1 rS-1,2 ... rS-1,T-1
 
a0 a1... aT-1  
y0 
y1 
... 
... 
... 
yT-1 
yT 
... 
... 
... 
yS-1  
h   
r0,0 r0,1 r0,2  ... r0,T-1 
r1,0 r1,2 r1,2 ... r1,T-1
 
... ... 
...  ... 
...   ... 
rT-1,0 rT-1,1 rT-1,2 ... rT-1,T-1
 
rT,0 rT,1 rT,2 ... rT,T-1
 
... ... 
...  ... 
...   ... 
rS-1,0 rS-1,1 rS-1,2 ... rS-1,T-1
 
Figure 2. Blakley’s Secret Sharing Scheme
in vector A. Any T rows in matrix R and their corresponding
values yi help determine all planes passing through point D
in T dimensional space. The right-hand side shows S shares.
Vector A is retrieved in data restoration phase to recover the
raw data.
C. Shamir’s Secret Sharing
 
Share S-1 
Share T 
Share T-1 
Share 1 
Share 0 
... 
... 
Data: 
a0 
a1 
... 
... 
... 
aT-1 
a0 a1... aT-1  
y0 
y1 
... 
... 
... 
yT-1 
yT 
... 
... 
... 
yS-1  
h   
1 x0 x0
2
  ... x0
T-1
 
1 x1 x1
2
 ... x1
T-1 
... ... 
...  ... 
...   ... 
1 xT-1 xT-1
2
 ... xT-1
T-1 
1 xT xT
2
 ... xT
T-1 
... ... 
...  ... 
...   ... 
1 xS-1 xS-1
2
 ... xS-1
T-1 
x0 
x1 
... 
... 
... 
xT-1 
xT 
... 
... 
... 
xS-1  
x0 
x1 
... 
... 
... 
xT-1 
xT 
... 
... 
... 
xS-1  
Figure 3. Shamir’s Secret Sharing Scheme
Shamir’s scheme is a special version of Blakley’s one.
When a point in T dimensional space is given, all associ-
ated hyper-planes are represented by normal vectors whose
elements are powers of the same numbers as shown in Fig.3.
Although some element combinations in Blakley’s scheme
will be lost, there still are inﬁnite possibilities to generate
shares in real domain. The right-hand side still represents S
shares which is much smaller than that geenrated by Blakley’s
scheme. In this case, Shamir’s scheme is more space-efﬁcient
than Blakley’s scheme since its share storage complexity is
O(T ) for normal vectors, whereas the one in Blakley’s scheme
is O(T 2).
D. Security vs. Performance
Secret sharing schemes always intend to balance two contra-
dictory design goals: security and performance. Improvement
of one factor might sacriﬁce the other one. Classic secret
sharing contains the following restrictions:
• Shamir’s scheme suggested that only the ﬁrst coefﬁcient
can be used for secret/data and other coefﬁcients should
be random numbers for security.
• Numbers involved in share generation and data restora-
tion might be large or troublesome, especially when
division is required in data restoration phase. Both large
and ﬂoating point numbers could cause accuracy and
performance problems.
• Normally, the bigger the threshold T is, the higher
security level will be. However, large threshold values
will introduce extra overheads in share generation and
data restoration.
• If data is large than threshold T , it should be truncated
into blocks with the size of T for multiple share genera-
tion and data restoration operations.
III. SCALABLE AND SECURE SECRET SHARING
The proposed scheme considers both security and perfor-
mance for practical use in Cloud computing systems. Perfor-
mance is emphasized without over-sacriﬁcing security issues.
A. Design Issues
Major restrictions in classic secret sharing schemes are
considered and eliminated.
1) Coefﬁcient Utilization: Placing secret/data only in the
ﬁrst coefﬁcient is inefﬁcient for large data sets. With threshold
T , data can be put into all T coefﬁcients [7]. If continuous
data items exhibit randomness characteristics, security will not
be sacriﬁced. Otherwise, a preprocessing randomization ﬁlter
can be applied to ensure security margin.
2) Data Representation Scalability: Finite ﬁelds such as
GF (28) can be employed to represent data in coefﬁcients
[8]. Both large and ﬂoating point number problems will be
resolved due to modular arithmetics and multiplicative inverse
135662
All data blocks form the matrix A and the coefﬁcient
matrix R is ﬁxed. Then the operation for share generation
is R × A = Y . The goal of data restoration is to recover
the matrix A. Matrix Multiplication (MM) is the most time-
consuming portion. For one data block, the computation
complexity in share generation is O(ST ), whereas the one
for data restoration is O(T 2).
Due to ﬁnite ﬁeld GF (28), the maximum number of shares
that can be generated with Shamir’s scheme is 255. In Fig.3,
the value of xi varies from 0 to 255. Then, S ≤ 255 and by
inference, T ≤ 255. This makes Shamir’s scheme even more
vulnerable for brute-force attacks since matrix X in Fig.3 can
be easily guessed out with only 256. For Blakley’s scheme,
the number of shares can reach O(255 ∗ T 2) which is much
larger. The only overhead is the extra storage, i.e., an S × T
matrix needs to be stored in shares.
Matrix R is generated stochastically. However, when S is
greater than T+1, there is no guarantee that all possible T out
of S rows in the randomly generated S×T matrix are linearly
independent. For example, later shares might repeat previous
ones. If such dependency happens, data cannot be restored
correctly and more shares are expected. However, when T
is large, the possibility for this kind of error to happen is
extremely small. The ﬁnal concatenated matrix R|Y in Fig.5
is the result for storage. Each row in this matrix can be treated
as one share. For security purposes, different share distribution
strategies can be used.
C. Operations for One Data Block
For each data block, operations in share generation and
data restoration are listed in Fig.6. The orders for these two
sequences are exactly opposite. In share generation phase,
random sequence addision helps bring in randomness and
avoid reoccurence situation, i.e., identical data blocks gen-
erates same shares. The random number sequences on both
ends should be the same for correctness. Then “Add Padding”
operation can set correct internal padding size. The following
“Multiply Matrices” can conduct R × A = Y operation.
Finally, matrix R|Y is constructed and partitioned for S shares.
In data restoration phase, T distinguished shares can recover
the original data as shown in Fig.6.
Since the share generation and data restoration phases share
the same modules with the same time complexity, only the
  
Message 
Add Random Seq 
Add Padding 
Matrix R 
Multiply Matrices 
Concatenate R 
Divide into Rows 
S Shares 
Known Random  
Sequence 
T Shares 
Message 
Add Random Seq 
Delete Padding 
Multiply Matrices 
Separate R 
Combine Rows 
Matrix R
-1
 
(a) Share generation (b) Data restoration 
Figure 6. Operations for One Data Block
share generation part is considered in later discussion.
IV. GPU CONFIGURATION FOR ACCELERATION
In secret sharing, data blocks (partitioned from raw data) as
well as operations in share generation and data restoration are
ideal data parallel cases. Graphics Processing Units (GPUs)
are perfect devices for acceleration and high performance.
However, GPU computing is different from the one for CPUs.
The awareness of hardware properties can beneﬁt performance
directly. Also, some software parameters should be set prop-
erly.
The aforementioned matrix multiplication is different from
the traditional one, which conducts the multiplication of two
numbers in ALUs and registers. The new design is based on
table checking to reduce the computation overhead signiﬁ-
cantly. However, it introduces 64 KB additional space to save
the multiplication table which leads to more frequent memory
accesses and further decreases the cache hit ratio. Therefore,
this new design needs to be tuned and evaluated thoroughly
for better performance.
A. NVIDIA Fermi GPU and CUDA
Compared to its former architectures, NVIDIA proposes
Fermi [5] in 2010 with several new features, such as the intro-
duction of conﬁgurable cache and more streaming processors
(core) in a streaming multiprocessor (SM). Many of the new
135864
and L1 cache takes 16 KB. This proportion can be reversed
to 1:3 by calling the function “cudaFuncSetCacheConﬁg()”
before any GPU-related code.
CUDA also provide an option of avoiding L1 cache or not
and it should be conﬁgured in the compilation phase. In order
to avoid hitting L1 cache for source code “program.cu”, the
corresponding compilation command should be “nvcc –arch
sm_13 –Xptxas –dlcm=cg program.cu”.
With these two types of conﬁguration, there can be four
different settings for the cache management on GPU. Secret
sharing performance on different L1 cache conﬁgurations are
shown in Fig.8.
 
4 8 16 32 64 128 256
S
c
a
le
d
 T
im
in
g
 G
r
a
p
h
Threshold T
Avoid L1, SM:L1=3:1 Avoid L1, SM:L1=1:3
Hit L1, SM:L1=3:1 Hit L1, SM:L1=1:3
Figure 8. Secret Sharing Performance with Variant Cache Conﬁgurations
When threshold T ≤ 128, GPU code prefers larger L1 cache
since the cache hit ratio is high. So when T is small (T ≤ 128),
L1 cache is encouraged to be used and set larger. In Fig.8,
when T ≤ 128, it is clear that the fourth setting “Hit L1,
SM:L1=1:3” is the best choice.
However, in the case of T = 256, cache hit ratio drops
signiﬁcantly. In this case, it may be better for GPU to skip the
step of searching data in cache, which is unnecessary in this
situation. In Fig.8, we can see that the ﬁrst and the second
settings, corrsponding to avoid L1 cache, becomes the top 2
choices. Through observation we can see the second setting is
better than the ﬁrst setting when T = 256. But the reason to
the superiority of second setting is unknown.
D. Other Optimization Considerations
To achieve better performance, some other features of GPU
are also considered, including the uses of asynchronous copy,
constant memory, shared memory and register reduction.
1) Asynchronous Copy: For large data, it is always better
to cut it into smaller pieces to be executed on devices.
CUDA driver supports asynchronous operations for both data
transfer and computations. This means that computations on
CPU, computations on GPU and the data transfers between
them can overlap with each other. Later Fig.14 and Fig.15
will demonstrate the potential overlapping in both CPU and
GPU implementations. Asynchronous copy can hide some
data transfer time here. Also, when GPU is performing its
computation, CPU can load new batch of data or do some
preprocessing work such as adding random sequences and
padding.
2) Constant Memory: NVIDIA Tesla-C2050 has 64 KB
constant memory, which is said to be as fast as L1 cache.
Coincidentally, GF (28) requires 256 × 256 Bytes = 64 KB
to save all possible multiplication calculation in a table for
better performance under ﬁnite ﬁeld GF (28). This could be a
perfect design. However, the introduction of constant memory
increases the execution time by about 4 times for secret
sharing. Therefore, such optimization is given up and the
multiplication table is placed in the global memory instead.
3) Shared Memory: Shared memory is another possible
way for performance gains. However, data sets in secret shar-
ing might be too big for shared memory. Then shared memory
is traded for bigger cache in some cases as aforementioned.
4) Register Usage Reduction: GPU code can be very sen-
sitive to the number of registers used by a single thread. Since
there are hundreds or thousands GPU threads, the register
ﬁles could be very large. The register ﬁle size of NVIDIA
Tesla-C2050 is 8, 196. This means that, if 1, 024 threads are
generated for concurrent execution, 8 registers can be assigned
to each thread. If the program has more than 8 scalar variables,
some of them will be mapped onto the local memory and drag
the performance down. The scalable secret sharing code has
been tuned so that registers are sufﬁcient for those variables
to achieve better performance.
V. PERFORMANCE ANALYSES AND EXPERIMENTAL
RESULTS
Sequential CPU and GPU codes are tested on a server with
two Intel Xeon E5504 Quad-Core CPUs (2.00 GHz, 4 MB
cache) and two NVIDIA Tesla 20-Series C2050 GPUs (448
13606
 0
5
10
15
20
25
0 50 100 150 200 250
S
p
e
e
d
-u
p
Threshold T
1MB 2MB
4MB 8MB
16MB
Figure 13. Speed-up of Total Execution (GPU vs. CPU)
cases, the computation time, including MM computation and
others, always occupies the largest proportion. The difference
is that GPUs help reducing much of the MM computation
time. So for GPU program, other computations, including
format transformation and padding processing, becomes the
major bottleneck when T is relatively smaller. However, as
T becomes larger, MM computation turns into the dominant
factor.
 
4 8 16 32 64 128 256
T
im
e
 P
r
o
p
o
r
t
io
n
Threshold T
I/O Other Computation MM Computation
Figure 14. Overhead Breakdowns of CPU Program
VI. RELATED WORKS
Long-term non-encryption secure data storage receives
much attention recently. POTSHARD [9], which is based on
secret splitting and RAID technologies, is a reliable system for
long-term storage. HASS [10] is a highly available, scalable
and secure distributed data storage system integrated with
identity-based encryption (IBE) and objected-based storage
devices (OSD). Shamir’s secret sharing scheme and internal
 
4 8 16 32 64 128 256
T
im
e
 P
r
o
p
o
r
t
io
n
Threshold T
Data Transfer I/O Other Computation MM Computation
Figure 15. Overhead Breakdowns of GPU Program
padding [7] were integrated to a ﬁle system for more conve-
nient ﬁle operations.
In 1979, Shamir [6] and Blakley [2] proposed two different
secret sharing schemes respectively. Their schemes are highly
secure but impossible to be used for common data storage
because of the scalability issus. In his paper, Shamir suggested
using modulo p to form a ﬁeld while maintaining ﬂoating
point operations to achieve security level. In 1983, Chinese
Remainder Theorem was introduced by Asmuth, Bloom [1]
and Mignottes [3] to develop new secret sharing schemes.
Their schemes got rid of ﬂoating point completely and im-
proved the performance to some extent. However it was still
hard for them to provide efﬁcient data storage. To achieve
better performance and apply secret sharing schemes to data
storage, some researchers trade unconditionally security while
maintaining enough security level. In Shen and Jiang’s work
[7], GF (28) is used in Shamir’s scheme for table checking.
Also, they expand the range to all bits for secret hiding.
These designs improved performance to a large extent however
over-sacraﬁced the security. The proposed scheme adopted
Blakley’s approach to avoid the serious security hole, added
a new module to prevent the potential attack on highly
structured message and optimized the whole processes of share
generation and data restoration. Compared to Shen and Jiang’s
work, this new scheme for data management is faster, more
secure, more ﬂexible and more scalable.
Fermi architecture [5] and its corresponding programming
toolkit [4] were released by NVIDIA in 2010. The linear
algebra libraries has been regularly updated. In 2011, a latest
implementation of matrix multiplication on NVIDIA GPUs
13628
International Journal of Hybrid Information Technology  
  Vol. 5, No. 2, April, 2012 
 
 
181 
 
Auto-Scaling Model for Cloud Computing System 
 
 
Che-Lun Hung
1*
, Yu-Chen Hu 
2
 and Kuan-Ching Li
3
 
1
 Dept. of Computer Science & Communication Engineering, Providence University 
2
 Dept. of Computer Science & Information Management, Providence University 
3
 Dept of Computer Science & Information Engineering, Providence University  
{clhung, ychu, kuancli}@pu.edu.tw 
*corresponding author 
Abstract 
Recently, Cloud Computing introduces some new concepts that entirely change the way 
applications are built and deployed.  Usually, Cloud systems rely on virtualization techniques 
to allocate computing resources on demand.  Thus, scalability is a critical issue to the success 
of enterprises involved in doing business on the cloud.  In this paper, we will describe the 
novel virtual cluster architecture for dynamic scaling of cloud applications in a virtualized 
Cloud Computing environment.  An auto-scaling algorithm for automated provisioning and 
balancing of virtual machine resources based on active application sessions will be 
introduced.  Also, the energy cost is considered in the proposed algorithm.  Our work has 
demonstrated the proposed algorithm is capable of handling sudden load requirements, 
maintaining higher resource utilization and reducing energy cost. 
 
Keywords: Cloud Computing, Auto-Scaling, Virtualization, Virtual Machine. 
 
1. Introduction 
Recently, Cloud Computing [1, 2, 3] as a new enterprise model has become popular to 
provide on-demand services to user as needed.  Cloud Computing is essentially powerful 
computing paradigm to deliver services over the network.  The model of Cloud Computing 
has been distinguished into infrastructure-as-a-service (IaaS), Platform-as-a-service (Paas), 
and software-as-a-service (SaaS) [4].   
In Cloud Computing environment, the virtualization technology [5, 6, 7] is the import role 
to provision the physical resources, such as processors, disk storage, and broadband network.  
Virtualization refers primarily to platform virtualization, or the abstraction of physical 
resources for users.  In the Could, these physical resources is regarded as a pool of resources, 
these resources thus can be allocated on demand.  Computing at the scale of the Cloud system 
allows users to access the enormous and elastic resources on-demand.  However, a user 
demand on resources can be various in different time, maintaining sufficient resources to 
meet peak resource requirements can be costly.  On the contrary, if user maintains only 
minimal computing resources, the resource is not sufficient to handle the peak requirements.  
Therefore, dynamic scalability is a critical key point to the success of Cloud Computing 
environment.  Dynamic resizing is a feature allows server to resize the virtual machine to fill 
the new requirement of resources [7].  When a virtual machine is under-provisioning or over-
provisioning, dynamic resizing can utilized to overcome these problem.  However, it is not 
suitable in the Cloud business model as Amazon EC2.  In Amazon EC2 [8], a virtual machine 
is as a basic unit, and user can rent a unit or more units rather than a unit with a special 
resources.  Dynamic resizing cannot overcome the load balancing since it only grows the 
International Journal of Hybrid Information Technology  
  Vol. 5, No. 2, April, 2012 
 
 
183 
 
architecture that effectively handles these two scenarios is illustrated in Figure 1. This 
architecture includes three main components: Front-end load balancer, Virtual cluster monitor 
system and Auto-provisioning system with an auto-scaling algorithm. Front-end load balancer 
can balance the requests between different virtual machines that perform the same application 
in a virtual cluster.  Virtual cluster monitor system collects resource usages of all VMs for 
each virtual cluster that are running on Cloud.  Auto-provisioning system can horizontally 
expand/shrink the number of VMs according to workload of a virtual cluster where VMs 
belong.  If the application running on the virtual cluster consumes most of the resources, auto-
scaling can create a new VM that perform the same application and front-end load balancer 
then balances the requests among VMs in the same virtual cluster.  
  
3. Load Balancing 
In our architecture, Front-end load balancer is utilized to balancing the web 
application load.  Apache HTTP Load-Balancer is utilized as Front-end load balancer to 
allow incoming HTTP request to be routed into web servers that perform the web 
application.  Since the Apache HTTP Load-Balancer configuration can be updated 
dynamically, this allows a Cloud system to automatically and dynamically add new web 
servers for a virtual cluster.  The additional web servers enable the virtual cluster to 
scale and thus provide better response time for incoming HTTP requests.   
 
Auto-Scaling Algorithm1 
Input:   n: number of Clusters 
 VC: an Virtual Cluster consists of VMs that run the same 
computational system 
 VMns: number of the active sessions in a virtual machine 
 SiMax: maximum sessions for a virtual machine of i-th Cluster 
 Supper_bound: session upper-threshold 
 Slow_bound: session low-threshold 
 Ebelow: a virtual machine set records virtual machines that 
exceed the session upper-threshold 
Output: Front Load-Balancer Set FLB 
 
1. For i = 1 to n 
2.    For each VM  VCi   
3.       If (VMns/SiMax  Supper_bound) then 
4.         e = e + 1 
5.       If (VMns/SiMax  Slow_bound) Then 
6.         b = b + 1 
7.         Record VM to Ebelow 
8.    If (e == | VCi|) then 
9.       Provision and start a new VM that runs the same system as VCi 
10.       Add new VM to FLB //Front Load-Balancer Set  
11.    If (b  2) then 
12.       For each VM in Ebelow 
13.          If (VMns == 0) then 
14.            Remove VM from FLB //Front Load-Balancer Set 
15.            Destroy VM 
16.    Empty Ebelow 
Figure 2. Auto-Scaling Algorithm for Web Applications 
 
4. Auto-Scaling Algorithm 
For web application, Virtual cluster monitor system can detect whether the number of 
active HTTP sessions are over the threshold in a virtual cluster.  For distributed 
computing task, Virtual cluster monitor system is able to detect whether the number of 
International Journal of Hybrid Information Technology  
  Vol. 5, No. 2, April, 2012 
 
 
185 
 
mechanism of the Cloud system is the essential element in enhancing the resource utilization, 
thus reducing the infrastructure and management costs. 
 
Auto-Scaling Algorithm2 
Input:   n: number of Clusters 
 VC: an Virtual Cluster consists of VMs that run the same 
computational system 
 VMR: The use of resources in a virtual machine 
 Rupper_bound: The upper-threshold of use of physical resources  
 Rlow_bound: The low-threshold of use of physical resources 
 Vbelow: a virtual machine set records virtual machines that  
 
1. For i = 1 to n 
2.    For each VM  VCi   
3.       If (VMR  Rupper_bound) then 
4.         e = e + 1 
5.       If (VMR  Rlow_bound) Then 
6.         b = b + 1 
7.         Record VM to Vbelow 
8.    If (e == | VCi|) then 
9.       Provision and start a new VM that runs the same computing 
tasks as VCi 
10.    If (b  2) then 
11.       For each VM in Vbelow 
12.          If (VM is idle) then 
13.            Destroy VM 
14.    Empty Vbelow 
Figure 3. Auto-Scaling Algorithm for Distributed Computing Tasks 
 
Acknowledgements  
 
This research was partially supported by the National Science Council under the Grants NSC-
100-2221-E-126-006 and NSC-99-2632-E-126-001-MY3. 
 
References 
 
[1] E. Knorr and G. Gruman, InfoWorld, (2010). 
[2] R. Buyya, Y. S. Chee and V. Srikumar, Dept. Computer Science and Software Engineering, 
University of Melbourne, Australia, (2008). 
[3] D. Chappel, David Chappell & Associates, (2008). 
[4] R. Prodan and S. Ostermann, Proceedings of the 10th IEEE/ACM international conference on Grid 
Computing, (2009) October 13-15; Banff, Canada. 
[5] VMware Inc., http://www.vmware.com/products/vi/esx/. 
[6] Xen, http//www.xen.org. 
[7] Kernel-based Virtual Machine (KVM), http://www.linux-kvm.org. 
[8] Amazon EC2, http://aws.amazon.com/ec2/. 
[9] T. C. Chieu, A. Mohindra, A. A. Karve and A. Segal, Proceedings of the IEEE international 
Conference on e-Business Engineering, (2009) October 21-23; Macau, China. 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/27
國科會補助計畫
計畫名稱: PARA-雲端環境中虛擬主機動態管理於效能及耗能之研究
計畫主持人: 李冠憬
計畫編號: 100-2221-E-126-006- 學門領域: 平行與分散處理
無研發成果推廣資料
等，請以文字敘
述填列。) 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
