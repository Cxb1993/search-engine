Query Expansion via Wikipedia Link for CLIR
Chih-Chuan Hsu, Yu-Te Li, You-Wei Chen and Shih-Hung Wu*
Department of Computer Science and Information Engineering
Chaoyang University of Technology
Taichung County 41349, TAIWAN (R.O.C)
*Contact author: shwu@cyut.edu.tw
摘要
我們研發了一種跨多語言資訊檢索技
術。利用 NTCIR-7 研討會公開資料評
估系統。我們主要測試處理英文查中
文、簡體中文、日文文件的跨語言資
訊檢索。我們主要利用維基百科的網
路資訊，整合傳統的檢索演算法，同
時使用查詢擴張解決未知詞問題。
Abstract
In this paper, we deal with that
English to traditional Chinese,
simplified Chinese and Japanese
cross-language information retrieval
(CLIR) in NTCIR-7, and purpose our
results on the T-run, D-run and DN-run
total nine subtasks. In this task, we
didn’t process about Question. In these 
runs, we adoptive use Dictionary-Based
Approach to translate query terms and
the Wikipedia to take advantage to
translate and query expansion (QE) in
order to solve Out-Of-Vocabulary (OOV)
term.
Keywords: CLIR, Wikipedia, OOV
terms
1. Introduction
IR is a much useful technique for
users to find information what satisfy
user need, but information may be in
different languages on the web. For
satisfied users’ information need, the 
CLIR system could be help users to get
these information in different languages.
Query translation in CLIR, two major
constraints are word sense
disambiguation (WSD) and Out Of
vocabulary (OOV) term. In the case of
WSD, the source language of query
terms might be translated to target
language incorrectly. Ballesteros and
Croft proposed the co-occurrence
statistics method [2], Mirna proposed a
term-sense disambiguation technique [6],
Federico and Rada Mihalcea proposed to
use the Wikipedia to solve the WSD
problem [5]. In addition, Ying, Phil and
Justin collected co-occurrence from the
retrieved web text by statistics method
[12, 13] to translation the Chinese OOV
terms.
In this paper, we focus on dealing
with OOV terms. In previous research,
Su et al. [9] adopt the online translation
website services as a fixed dictionary
and the Wikipedia as a live dictionary to
translate query terms. Their method
could be translate OOV term efficiently,
and we use his method in our system to
translate query term. However,
their method still had a lot of terms that
couldn’t find translations. For retrieved 
more relevance documents, the
algorithm OKAPI BM25 [7, 8] to help
query expansion to raise recall.
Furthermore, Lin et al. [4] purposed
using OKAPI BM25 for ranking the
expanded query and Wikipedia anchor
text as expanded query. In this paper, we
combined Su’s and Lin’s methods in our 
system.
The following sections are organized
as follows: Section 2, 3, 4 and 5 describe
the index methods, the translation
methods, the query expansion methods
and the retrieval methods respectively.
We show the experiment results in
section 6 and give the conclusions and
future work in section 7.
OKAPI BM25
The ranking method in our system is
the standard OKAPI BM25 algorithm [8]
[7]. The OKAPI BM25 formulas are as
follows. The similarity between a query
Q and a document Dn can be computed
by
 
 

QT
n qtfktfK
qtfktfk
wDQSim
))((
)1()1(
,
3
311
Where
)5.0/()5.0(
)5.0/()5.0(
log1


rRnNrn
rRr
w
))1((1 avdl
dl
bbkK 
N: Number of items (documents) in the
collection
n: Collection frequency: number of
items containing a specific term
R: Number of items known to be
relevant to a specific topic
r: Number of these containing the term
tf: Frequency of occurrence of the term
within a specific document
qtf: Frequency of occurrence of the term
within a specific query
dl: Document length (arbitrary units)
avdl: Average document length
ki,b: Constants used in various BM
functions
Wikipedia Query Expansion
In Wikipedia, every entry has links
to related entries or related other
relevance WebPages in other Website.
These anchor texts with hyperlink must
be useful related terms to this page.
Therefore, we treat these anchor texts as
candidates for second retrieval of query
expansion [4]. Note that these
candidates should compete the terms
from OKAPI.
5. Retrieval System
Our system flow is shown in Fig 1.
There are two parts in translating query
in our system. The first part, the query in
source language is translated into target
language using an online dictionary. The
system segments the query in target
language into terms. The other part is
that the system segments the query in
source language into terms and the
query terms are translated into target
language using Wikipedia. Finally, the
translated query terms are from the two
parts to combine and the IR system
retrieves documents in target language
based on these. The E-CT, E-CS and
E-JA process queries are all follow this
flow.
After getting the query of target
language, in order to QE we process
these query terms by N-gram and get the
relevance anchor texts from Wikipedia.
The system use the query of target
language to first retrieval of
pseudo-relevance feedback (PRF) [11]
to get the relevance documents of top N
and then use the Okapi BM25 to rank
these terms from Wikipedia and
relevance documents. Finally, we use the
top r to be the new terms of QE.
In the official runs, the default
OKAPI BM25 parameters are: k1=1.2,
k3=7, b=0.75, and the top 100
documents of first search are treated as
relevant documents. The feedback new
terms number=50.
EN-JA-T-01 0.2543 0.2528 0.4252
EN-JA-D-02 0.2294 0.2300 0.4124
EN-JA-DN-03 0.2568 0.2545 0.4366
Additional Run
We would like to know that if it is
helpful in translating query in EN-CT,
EN-CS, and EN-JA by our method.
Besides, we want to know the affection
in different numbers of QE term and
different proportion of QE term in Okapi
and Wikipedia. Our experiments are as
follows.
1. The first experiment is to test our
method with CT-CT, CS-CS, and JA-JA.
The performance will be compared with
CLIR and SLIR.
2. The second is to compare 10, 20, 30,
40, and 50 terms in query expansion.
3. Finally, we compare the different
proportion in QE term from Okapi and
Wikipedia. Proportion are 0% to 100%.
In Table 3, 4, and 5 is the
representative result of experiment in CT,
CS, and JA respectively.
We have a better performance in
translating query, especially in EN-CS.
The value of MAP in EN-CS and CS-CS
is very close and it’s shown in Table 4. 
Although the performance in EN-JA is
not bad, our MAP of EN-JA run was
much lower than the NTCIR-7 average.
The proportion in Wikipedia and
Okapi, although it is better to use the QE
terms at all from Okapi, it could be
helpful in several cases in Wikipedia
especially in CT-runs and DN-runs in
each language. Furthermore, the MAP in
using Okapi at all and Wikipedia at all
are almost the same, and it’s very 
interesting. Because they to use different
QE terms at al. It’s shown in Table 6.
We also discover the funny things,
and in many DN-runs, the MAP is better
than T-runs. The phenomenon is
especially in CS-runs, and it’s shown in 
Table 4.
Table 3. The performances of CT-runs; QE term=20; the different
proportion in QE term from Okapi and Wikipedia.
Okapi QE : Wikipedia QE
Run 100:0 90:10 80:20 70:30 60:40 50:50 40:60 30:70 20:80 10:90 0:100
EN-CT-T 0.2681 0.2770 0.2786 0.2776 0.2765 0.2806 0.2814 0.2840 0.2797 0.2739 0.2680
EN-CT-D 0.2635 0.2705 0.2746 0.2723 0.2695 0.2729 0.2734 0.2765 0.2661 0.2588 0.2561
EN-CT-DN 0.2484 0.2570 0.2616 0.2628 0.2619 0.2627 0.2639 0.2627 0.2563 0.2483 0.2416
CT-CT-T 0.4211 0.4318 0.4264 0.4185 0.4122 0.4123 0.4119 0.4160 0.4131 0.4061 0.4008
CT-CT-D 0.3922 0.4018 0.3980 0.3897 0.3824 0.3774 0.3779 0.3795 0.3766 0.3713 0.3650
CT-CT-DN 0.3920 0.4037 0.4043 0.4017 0.4027 0.4026 0.4059 0.4088 0.4061 0.4017 0.3955
precision in CLIR.
Future works
In our experiment, the MAP in
Wikipedia is unstable in query
expansion, and it could be the anchor
texts in CT, CS, and JA we treat didn’t 
segment to lead the TF-IDF to be the
zero of the true relevant terms. We will
try to segment with these anchor texts in
CT, CS, and JA and observe if it
improve the performance in system.
We only use the “LongQE” method 
[4] in our system, and we will use the
“ShortQE” in the future to compare the
performance in EN-CT, EN-CS, EN-JA,
CT-CT, CS-CS, and JA-JA in these two
method.
Our MAP of JA runs was much
lower than the NTCIR-7 average, and
we will move forward a single step in
Japanese.
Reference
[1] L. Ballesteros, and W.B. Croft,
“Dictionary-based Methods for
Cross-Lingual Information
Retrieval”, Proc. of International
Conference on Database and Expert
System Applications, 1996, pp
791-801.
[2] L. Ballesteros, and W.B. Croft,
“Resolving Ambiguity for 
Cross-Lingual Information
Retrieval”, Research and 
Development in Information
Retrieval, 1998, pp 64-71.
[3] M. Federico, and N. Bertoldi,
“Statistical Cross-Language
Information Retrieval Using N-Best
Query Translations”, Proc. of the 
25th annual international ACM
SIGIR conference on Research and
development in information
retrieval, Tampere, Finland, 2002,
pp 167-174.
[4] Tien-Chien Lin, Shih-Hung Wu,
“Query Expansion via Wikipedia
Link”, International Conference on 
Information Technology and
Industrial Application, 2008.
[5] Rada Mihalcea, "Using Wikipedia
for AutomaticWord Sense
Disambiguation," Proceedings of
NAACL HLT, pp. 196–203.
[6] A. Mirana, Using statistical term
similarity for sense disambiguation
in cross-language information
retrieval. Information Retrieval 2, 1,
2000, pp.67–68.
[7] Tetsuji Nakagawa, and Mihoko
Kitamura, “NTCIR-4 CLIR
Experiments at Okapi”, Proc. of the
Fourth NTCIR Workshop on
Research in Information Access
Technologies Information Retrieval,
Question Answering and
Summarization, April 2003 –June
2004.
[8] S. E. Robertson, S. Walker, S. Jones,
M. Hancock-Beaulieu and M.
Gatford, “Okapi at TREC-3”. In 
Proceedings of the Third Text
Retrieval Conference (TREC-3),
NIST, 1995.
[9] Chen-Yu Su, Tien-Chien Lin,
Shih-Hung Wu, “Using Wikipedia
to Translate OOV Terms on
MLIR”, Proceedings of NTCIR-6
Workshop Meeting, May 15-18,
2007, pp 109-115.
[10] Tetsuya Sakai, Noriko Kando,
Chuan-Jie Lin, Teruko Mitamura,
Donghong Ji, Kuang-Hua Chen,
Eric Nyberg, , “Overview of the 
NTCIR-7 ACLIA IR4QA Subtask”, 
In Proceedings of the 7th NTCIR
Workshop, 2008.
[11] Fan, Weiguo, Luo, Ming, Wang, Li,
Xi, Wensi and Fox, Edward A.
Tuning Before Feedback ：
Combining Ranking Discovery and
Blind Feedback for Robust
Retrieval. Proceedings of the 27th
annual international ACM SIGIR
conference on Research and
development in information
retrieval. 2004, pp 138-145.
[12] Ying Zhang, Phil Vines, and Justin
Zobel, “Chinese OOV Translation 
可供推廣之研發成果資料表
□ 可申請專利 □V 可技術移轉 日期：97 年 10 月 24 日
國科會補助計畫
計畫名稱：利用維基百科為語料庫之跨語言資訊檢索
計畫主持人：吳世弘
計畫編號：NSC 96-2221-E-324-046 學門領域：資訊
技術/創作名稱 Query Expansion via Wikipedia Link for CLIR
發明人/創作人 吳世弘
中文：我們研發了一種跨多語言資訊檢索技術。利用 NTCIR-7 研討
會公開資料評估系統。我們主要測試處理英文查中文、簡體中文、
日文文件的跨語言資訊檢索。我們主要利用維基百科的網路資訊，
整合傳統的檢索演算法，同時使用查詢擴張解決未知詞問題。
技術說明 英文：We deal with that English to traditional Chinese, simplified
Chinese and Japanese cross-language information retrieval (CLIR) in
NTCIR-7, and purpose our results on the T-run, D-run and DN-run
total nine subtasks. In this task, we didn’t process about Question. In 
these runs, we adoptive use Dictionary-Based Approach to translate
query terms and the Wikipedia to take advantage to translate and query
expansion (QE) in order to solve Out-Of-Vocabulary (OOV) term.
可利用之產業
及
可開發之產品
資訊檢索產業。
技術特點
可同時跨多國語言之資訊檢索技術。
整合網路資訊與傳統檢索演算法，同時使用查詢擴張解決未知詞問
題。
推廣及運用的價值
跨語言之資訊檢索。
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
※ 3.本表若不敷使用，請自行影印使用。
出席國際學術會議心得報告
計畫編號 NSC 96-2221-E-324-046
計畫名稱 利用維基百科為語料庫之跨語言資訊檢索
出國人員姓名
服務機關及職稱
吳世弘
會議時間地點 2008/7/13~2008/7/15, Las Vegas, USA
會議名稱 IEEE International Conference on Information Reuse & Integration (IRI)
發表論文題目 Curate a Transliteration Corpus from Transliteration/Translation Pairs
一、 前言
資訊重複利用與整合研討會 IEEE-IRI，是一個每年固定舉辦一次的學術性研討會，今
年在美國拉斯維加斯希爾頓飯店舉行。大會每年都邀請柏克萊大學(UC Berkeley)著名
教授扎德(Lotfi Zadeh)，模糊邏輯(Fuzzy Logic)創始人，發表主題演講，是最近這幾屆
固定的特色。大會主辦人盧賓博士(Stuart Rubin)對這個會議的經營相當用心，除了積
極的推廣會議，使得會議論文收錄於 EI index，也將與多本著名期刊例如 IEEE
Transaction SMC part A合作，刊登會議特刊。
二、 研討會主題與內容
本屆 IRI 會議，相關議題包括資料庫、搜尋最佳化、網站服務、生物應用、智慧型代
理人、軟體重複利用、生物醫學應用、風險管理與社交網路、資訊網智慧、問答系統、
資料處理、設計樣版、知識表達與重複利用、機器學習、資料探勘、自然語言處理、
本體論、工作流管理、元件基礎方法、資料整合、商業計算、多媒體、網路等議程。
大會邀請 Zadeh及 Adeli教授作兩場專題演講並且有一場 panel討論會，議題為「資訊
搜尋與檢索在刺激經濟的角色」。本屆 IRI研討會的論文投稿有 169 篇，來自 32個不
同國家，72篇錄取為一般論文，接受率 42.6%，另有 12篇為短論文，接受率 49.7%。
三、 研討過程與收獲
Curate a Transliteration Corpus from Transliteration/Translation Pairs
Shih-Hung Wu and Yu-Te Li
Department of Computer Science & Information Engineering
Chaoyang University of Technology, Taiwan
{shwu, s9627613}@cyut.edu.tw
Abstract
Transliteration of new named entity is important for
information retrieval that crosses two or multiple
language. Rule-based machine transliteration is not
satisfactory, since different information sources have
different standards for the transliteration. To build a
statistic machine transliteration module, researchers have
to curate a transliteration corpus for any given two
languages of interest. Since a large amount of
transliteration/translation pairs can be collected from the
Web, a large transliteration-training corpus can be
curated from these pairs. In this paper, we proposed a
bi-directional approach to classify
transliteration/translation pairs. Our approach combines
both forward transliteration and backward transliteration
to classify transliteration from translation. An experiment
on English and Chinese transliteration is conducted.
1. Introduction
Translation or transliteration of new named entities is
important for information retrieval that crosses two or
multiple languages. On the web, information of a person
or a company might exist in various languages and in
different transliterations. However, transliterations of new
named entities are not in a dictionary. This is called the
out-of-vocabulary (OOV) problem.
Previous researches used machine translation to handle
this problem in cross language information retrieval
(CLIR). There is no simple rule for translation, since
translation requires the meaning of the term. However,
transliteration can be implemented by using simple rules,
given the pronunciations of a term; a system can
transliterate it to another language.
Take the transliteration from English to Chinese, a
process might follow these steps. First, the system
segments the pronunciation of a term into a syllable or
phoneme string. Second, the system matches the syllable
or phoneme string to a Chinese syllable or phoneme string.
Thus, the system can output some suitable Chinese
candidates. The matching step can be implemented by
using a rule or statistic method.
Rule-based machine transliteration is not satisfactory,
since different information sources use different standards
for the transliteration. To build a statistic machine
transliteration module, researchers have to curate a
transliteration corpus first for any given two languages of
interest.
In previous study, a name originally in English
transliterated into Chinese is called forward E2C
transliteration [11]. Try to find the original Chinese name
in English from a transliteration is called backward C2E
transliteration .
Many researches on transliteration focus on statistic
machine transliteration, such as forward transliteration [2,
11, 15] and backward transliteration [3, 11, 12]. Another
transliteration related research topic is extraction
transliteration pairs [8, 9, 18]. There are several
approaches on transliteration modeling, such as using the
joint-source channel model to compute the English
phoneme and Chinese phoneme probability of similarity
[2, 11], using a phonetic similarity model to compute the
English and Chinese probability of a confusion matrix and
using the Expectation-maximization model (EM) as an
unsupervised learning algorithm [8]. Li proposed a joint
source-channel model using a noisy channel model with
DOM to calculate the probability between English and
Chinese transliterations. [11]. Lin and Chen propose edit
distance to compute phonemes in a scoring matrix, higher
score higher similarity [12]. Gou and Wang used the
English and Chinese syllable and statistic model to
compute the probability of similarity [3]. Lin et al. used
English syllable matching with Chinese syllable
extraction transliteration pairs on parallel corpus and
considered the phonetic insertion/deletion to extract
transliteration pairs on the web [13].
Many researches adopted extraction transliteration
pairs on parallel corpus. However, parallel corpora are
rare in the real world compared to Web. Kou proposed
recognition by validation. Kou’s system found an English
word in every sentence [8] and used k-neighborhood
where k is a constant by which English syllables are
entity in different languages. Our goal is to classify
whether the pair is a translation or a transliteration. We
define several possible ways to transliterate the Chinese
part in the pair and check if the transliteration matches the
English part. Then try another direction, translate the
English part in the pair and check if the transliteration
matches to the Chinese part.
The simplest way to do the English-to-Chinese (E2C)
transliteration is to use a mapping table to find a possible
Chinese transliteration. Table 1 shows the mapping table
that we adopted from the Wikipedia template 1.
Table 1 Mapping Table
b p d t g k v w f z ts s th sh ch th
布 普 德 特 格 克 夫
(弗
)
夫
(弗
)
夫
(弗
)
茲 茨 斯
(絲
)
日 什 奇 奇
a 安 巴 帕 達 塔 加 卡 瓦
(娃
)
瓦
(娃
)
法
(娃
)
扎 察 薩
(莎
)
扎 沙
(莎
)
賈 查
e 埃 貝 佩 德 泰 蓋 凱 韋 韋 費 澤 策 塞 熱 謝 傑 切
ei 埃 貝 佩 德 特 蓋 凱 韋 韋 費 澤 策 塞 熱 謝 傑 切
4 厄 伯 珀 德 特 格 克 弗 沃 弗 澤 策 瑟 熱 捨 哲 徹
i 伊 比 皮 迪 蒂 吉 基 維 威 菲 齊 齊 西 日 希 吉 奇
o 奧 博 波 多 托 戈 科 沃 沃 福 佐 措 索 若 肖 喬 喬
u 烏 布 普 杜 圖 古 庫 武 伍 富 祖 楚 蘇 茹 舒 朱 楚
ju 尤 比
尤
皮
尤
迪
尤
蒂
尤
久 丘 維
尤
威
尤
菲
尤
久 丘 休 休 久 丘
ai 艾 拜 派 代 泰 蓋 凱 韋 懷 法 宰 蔡 賽 夏 賈 柴
au 奧 鮑 保 道 陶 高 考 沃 沃 福 藻 曹 紹 紹 焦 喬
an 安 班 潘 丹 坦 甘 坎 萬 萬 凡 贊 燦 桑 尚 詹 錢
au 昂 邦 龐 當 唐 岡 康 旺 旺 方 藏 倉 桑 讓 尚 章 呂
en 恩 本 彭 登 滕 根 肯 文 文 芬 曾 岑 森 任 申 真 琴
in 因 賓 平 丁 廷 金 金 溫 溫 芬 津 欣 辛 欣 金 欽
in
g
英 賓 平 丁 廷 京 金 溫 溫 芬 京 青 辛 興 京 青
n 溫 本 蓬 敦 通 貢 昆 文 文 豐 尊 聰 孫 順 準 春
h m n l r j g k h dz
赫 姆 恩 爾 爾 伊 古 庫 胡 扎
a 哈 馬 納 拉 拉 亞 瓜 誇 華 澤
e 赫
/
黑
梅 內 萊 雷 耶 圭 奎 惠 澤
1 Wikipedia 英語譯音表
http://zh.wikipedia.org/wiki/Template:%E8%8B%B1%E8
%AA%9E%E8%AD%AF%E9%9F%B3%E8%A1%A8
b p d t g k v w f z ts s th sh ch th
ei 赫 梅 內 萊 雷 耶 圭 奎 惠 澤
h m n l r j g k h dz
i 希 米 尼(妮) 利(莉) 裡(麗) 伊 圭 奎 惠 佐
o 霍 莫 諾 洛 羅(蘿) 約 果 闊 霍 祖
u 胡 穆 努 盧 魯 尤 庫 久
ju 休 繆 紐 柳 留 宰
ai 海 邁 奈 萊 賴 耶 瓜伊 誇 懷 藻
au 豪 毛 瑙 勞 勞 堯 闊 贊
an 漢 曼 南 蘭 蘭 揚 關 寬 環 藏
au 杭 芒 南 朗 朗 揚 光 匡 黃 曾
en 亨 門 嫩 倫 倫 延 古恩 昆 津
in 欣 明 寧 林(琳) 林(琳) 因 古因 昆 京
ing 興 明 寧 林(琳) 林(琳) 英 古英 尊
n 洪 蒙 農 倫 倫 雲
The system can calculate the ratio of characters that
are in common between the mapping result and the
original Chinese part of the translation/transliteration pair.
We define a similarity score of the mapping as:
||/|2|OC)(E2C,ScoreECm OCOCCE  .
where E2C is the mapping result and OC is the Chinese
part of the original pair. For example, for the pair  “馬尼
拉/Manila,” while the mapping result of “Manila” is “馬
尼拉,” we get ScoreECm 3/3=1.0.
A further improvement of the naïve way is to take
the homophone into account. The system calculates the
ratio of homophonous characters in common. The
similarity score of homophones is formulated as:
||/|2|OC)(E2C,ScoreECp OCOCHPCE 
where the OCHP is the homophonous of the original
Chinese word. For example, for the pair “帝力/Dili,”
where the mapping result of “Dili”is “迪利,”we get
ScoreECp 2/2=1.0, since the pronunciation of ”帝力” and
“迪利” is the same.
Then we try another direction. The simplest way to do
the Chinese to English (C2E) transliteration is to apply a
pinyin system. Our system adopts three popular pinyin
systems: Hanyu Pinyin, Tongyong Pinyin and Wade-Giles.
For example, “台 中”could be transliterated as “Tai
Zhong”, "Tai Jhong", and ”Tai Chung”respectively. Our
system then does the matching of original English
syllables. The similarity score of syllables is defined as:
||/||OEs)(Ps,ScoreCEs OEsOEsPs
(CEs) TL2TL 0 %
T2T 549SubSyllable
(CEss) TL2TL 233
77.7
%
T2T 717α=0.8
TL2TL 109
82.1
%
T2T 692
Parameter
smoothing
α=0.75
TL2TL 57
74.5
%
T2T 5593 dimensions
feature TL2TL 78
63.4
%
T2T 609
EM
4 dimensions
feature TL2TL 61
65.7
%
Table4. System error rate
Type Methods Result
type
Num
ber
Error
rate
T2TL 0 0%Mapping table
(ECm) TL2T 235 96.3%
T2TL 30 3.9%
E2C
Homophonous
(ECp) TL2T 210 86.0%
T2TL 0 0%Syllables
(CEs) TL2T 244 100%
T2TL 213 27.9%
C2E
SubSyllable
(CEss) TL2T 11 4.5%
T2TL 45 5.9%α=0.8
TL2T 135 55.3%
T2TL 70 9.1%
Parameter
smoothing
α=0.75
TL2T 187 76.6%
T2TL 203 26.6%3 dimensions
feature TL2T 166 68.0%
T2TL 153 20.0%
EM
4 dimensions
feature TL2T 183 75.0%
5.1 Discussion
In E2C, using mapping table the system will not make
mistakes in T2TL, but the recognition of transliteration
was very low. In the homophonous experiment, the
recognition of transliteration is higher than using the
mapping table. In C2E, the syllable experiment resulting
in low classification accuracy may be due to two reasons.
First, the original names are in English. Therefore it is
hard to do back transliteration from Chinese to obtain the
original English. Second, authors adopt non-standard
Chinese pinyin methods. In the subsyllable experiment,
there is a high recognition of transliteration, but the
system errors in translation is also higher. In parameter
smoothing, the total accuracy is highest, however, system
errors in both translation and transliteration are not the
lowest. We also used the EM model and compared it with
the proposed smoothing method. In Table 3 and Table 4,
we also find that the EM did not perform very well in
terms of accuracy in our experiment.
5.2 Error analysis
Sometimes a named entity is translated by a
combination of translation and transliteration, for example:
“Taipei City/台北市”, where “Taipei” is a transliteration, 
but “City” is a translation. This kind of mixed up might
affect system accuracy, and it occurs not only in Chinese
to English but also in English to Chinese
translation/transliteration, where many English place
names are transliterated into Chinese and a translation,
such as ”市/city”, ”湖/lake”, ”省/province”is added. One
of our future works will attempt to solve this problem.
The other problem involves the translation and
transliteration of titles . For example, “清華大學/Tsing
Hua University” includes both translation and
transliteration.
We also find some inconsistencies of the styles of
various Wikipedia authors. For examples, the Chinese title
“明惠帝”is translated into “Jianwen Emperor”according
to his title, not his name. The other example is “Joseph 
Stalin,”which is translated into Chinese “約瑟夫·維薩里
奧諾維奇·史達林”, where the middle name was omitted
in English.
The performance of C2E back transliteration is worst.
Since the original name is transliterated into Chinese, it is
difficult to find original name using any of the three
pinyin methods. For example, "四川省/ Sichuan", the
back transliteration of the three pinyin systems gives "si
chuan", "sih chuan", and "ssu ch’uan", which are all quite
different from“Sichuan”.
6. Conclusion and future work
In this paper, we proposed a bi-directional approach for
transliteration/translation classification. This approach
combines forward and backward transliteration via the
parameter smoothing method. In our experiment, the
result is better than use only one-way approaches in our
experiments.
Although we conducted the experiment on Chinese and
English only, this bi-directional approach can be used on
many other languages, such as German, French, Italian,
and Spanish. Just replace the language mapping table and
grapheme-to-phoneme standards for the target language.
Thus, we can curate many transliteration corpora in
different languages effectively. We will also use Soundex
and double metaphone in our experiments. That will help
us higher precision of pronunciation in recognition of
transliteration.
In the future, we would like to automatically extraction
transliteration pairs on comparable corpora. Our data
source will be the short contents of Wikipedia that have
both English and Chinese versions. The short contents in
