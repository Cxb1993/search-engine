ii 
摘要 
 
在這個為期兩年的計劃中，我們提出了一些新穎的方法來設計模糊回歸與分類
的模型，其分別使用支持向量回歸(SVR)與支持向量機(SVM)來實現。其中部分的
研究成果已經發表於國際期刊論文[1]-[3]中。本報告介紹這些設計方法所使用的
兩個核心技術。報告介紹了兩個 Takagi-Sugeno(TS)的模糊模型，一個為基於模糊
分群與SVR的模糊類神經網路(FCFNN-SVR)而另一個為藉由SVM訓練的TS形式模糊
分類器(SVM-TFC)。本計劃藉由模擬驗證了 FCFNN-SVR 與 SVM-TFC 在處理回歸以及
分類問題的能力。特別的是，SVM-TFC 也被應用在基於視覺的即時的物體偵測系統
上。 
關鍵詞: 支持向量回歸、支持向量機、模糊回歸模型、模糊分類器、物體偵測。 
 1 
1. INTRODUCTION 
Fuzzy systems and support vector machines (SVMs) have been proposed to solve the 
regression estimation and classification problems in these years. When an SVM is applied to 
regression estimation problems, it is referred to as support vector regression (SVR). Fuzzy system is 
known to be powerful tool for modeling nonlinear functions. One general approach for fuzzy logic 
system design is fuzzy neural networks (FNNs). One major disadvantage of using FNNs is that 
network learning is based training error minimization which does not account for small test error. In 
contrast to FNNs, SVM learning formulation is based on the principle of structural risk 
minimization and attempts to minimize a bound on the generalization error [4, 5]. The SVM 
learning approach also exhibits desirable properties such as generalization and over-fitting 
prevention capabilities.   
1.1  SVR-trained Fuzzy Regression Models 
Many studies have used SVMs (SVRs) to deal with different classification (regression) 
estimation problems due to their high generalization ability [6]-[10]. One disadvantage of using 
SVR is that a large model size is obtained due to a large number of support vectors (SVs). For this 
reason, several studies on the combination of fuzzy set theory and SVR for regression problems 
have been proposed [11]-[15]. The fuzzy weighted SVR (FWSVR) in [11] partitions the training 
data into several subsets using fuzzy c-means and learns each different partitioned space locally 
using SVR. In studies [12,13,14], a fuzzy rule with a Gaussian membership function is regarded as 
a Gaussian kernel in an SVR. The number of rules is dependent on the number of SVs. Since the 
number of support vectors in an SVR is usually very large, the model size of a designed FS is 
equally large. The performance of a fuzzy system with first-order TS-type consequent, i.e., a linear 
combination of input variables plus a constant, has been shown to outperform that with zero-order 
TS-type consequent [16]. Based on this observation, the authors in [13] proposed TS-type fuzzy 
system-based SVR (TSFS-SVR) that combines the idea of TS-type fuzzy system and SVR. In 
TSFS-SVR, only one free consequent part parameter in each rule is tuned though TS-type fuzzy 
rules are used. This project proposes a Fuzzy Clustering-based FNN with Support Vector 
Regression (FCFNN-SVR) that tunes all of the consequent part parameters in TS-type rules using 
SVR.  
This project proposes a FCFNN-SVR for function approximation. The approach consists of 
two learning phases. The first phase uses one-pass clustering algorithm to automatically determine 
network structure, where the number of rules equals the number of clusters.  The second phase 
uses a linear SVR learning approach to tune the TS-type consequent parameters. This approach 
reduces overall network size because the size of FCFNN-SVR depends on the number of clusters 
instead of support vectors. The use of SVR for parameter learning endows the FCFNN-SVR with 
high generalization ability. This study conducts simulations of function approximation under 
 3 
1
kx
n
kx
y ′
1
kµ
2
kµ rkµx
K xK xK
b
2a
K
1a
K
ra
K
1R 2R rR…
…
………
1f rf
2f
Layer 1
Layer 2
Layer 3
Layer 4
Layer 5 Σ
 
Fig.1. The structure of the FCFNN-SVR. 
 
next layer, where n  is the number of input variables. 
Layer 2(Fuzzification layer): Each node in this layer represents a membership function. For 
input variable jx , the following Gaussian membership function is used 
2
2
( )
( ) exp ( )
j
ijj
ij
ij
x m
M x
σ
⎧ ⎫
−⎪ ⎪
= −⎨ ⎬⎪ ⎪⎩ ⎭
, 1,...,i r=  and 1, ...,j n=           (1) 
where ijm  and ijσ  denote the center and width of the Gaussian membership function, respectively, 
and r  is the number of rules. 
Layer 3 ( rule layer): The number of nodes in this layer equals the number of rules. Each node 
first performs a t-norm operation on inputs from Layer 2 using an algebraic product operation to 
obtain a spatial firing strength ( )i xµ K . Here, the firing strength ( )i xµ K  is determined by 
22
2 2
1 1
( )
( ) ( ) exp ( ) exp
jn n
iji j i
ij
j j ij ij
x m x m
x M xµ
σ σ
= =
⎧ ⎫⎧ ⎫
− −⎪ ⎪ ⎪ ⎪
= = − = −⎨ ⎬ ⎨ ⎬⎪ ⎪ ⎪ ⎪⎩ ⎭ ⎩ ⎭∏ ∑
K KK K             (2) 
Layer 4 (Consequent layer): Each rule in a FCFNN-SVR has a TSK-type consequent. For each 
node i  in this layer, the node output is a linear combination of network inputs, i.e., 
01
n j
i j ij
a x a
=
+∑ , where sija ′  are the consequent part parameters of rule i . Each node then 
multiplies this combination result by its corresponding temporal firing strength ( )i xµ K . The output 
 5 
k krm x=
K K
                               (7) 
and 
2
2
|| | |
k
k I
I
r
x m
σ β
σ
−
= ⋅
K K
                          (8) 
where 0β >  determines the overlap degree between two clusters. In this project, β  is set to 1.2. 
The cluster (rule) width in Eq. (8) is determined according to the geometrical relation between the 
input data kx  and its nearest cluster I . Equations (2) and (8) show that only one width is used for 
each rule. Using one width for each rule in a FCFNN-SVR helps reduce the overall model size. 
2.3  FCFNN-SVR Parameter Learning 
2.3.1 Basic SVR concepts 
This section briefly reviews the theory of SVR [4,5]. Suppose there is a labeled training set 
1 1 2 2{( , ), ( , ), , ( , )}N NS x y x y x y=
K K K"                     (9) 
where nkx ∈
K \  and ,  1, ,ky k N∈ =\ … . The main goal of ε -SVR regression is to find a function 
( )f xK  that has no more than ε  deviation from the actually obtained targets ky  for all the training 
data, and is as close as possible at the same time. First, consider the case of a linear function ( )y x′ K , 
taking the form 
( )= Ty x w x b′ +K K K                             (10) 
where nw ∈K \  is a high dimensional weight vector and the bias b∈\  is a scalar. The 
optimization problem is 
1M i n    
2
S u b j e c t  t o      ( )
                    
T
w
T
k k
T
k k
w w
y w x b
w x b y
ε
ε
− + ≤
+ − ≤ ⋅
K K K
K K
K K
                   (11) 
The convex optimization problem is feasible when a function ( )y x′ K  exists that approximates all 
pairs ( , )k kx y
K  with ε  precision in the above constraint. If the problem is infeasible or some errors 
are allowed, then the problem can be defined as  
1
ˆ, ,
1 ˆM i n    ( ) ,
2
ˆS u b j e c t  t o      ( ) ,
                    ,
ˆ                    , 0 , 1 , 2 , , .
k k
N
T
k k
k
T
k k k
T
k k k
k k
w
w w C
y w x b
w x b y
k N
ξ ξ ξ ξ
ε ξ
ε ξ
ξ ξ
=
+ +
− + ≤ +
+ − ≤ +
≥ =
∑K K K
K K
K K
"
                   (12) 
This formulation introduces two slack variables, ξˆ  and ξ : one for exceeding the target value by 
 7 
error bound (structural risk) minimization, and thus improves the FCFNN-SVR generalization 
ability. 
2.3.2 FCFNN-SVR parameter learning  
The objective of parameter learning is to optimally tune the free parameters ija of the constructed 
FCFNN-SVR. The following subsection introduces parameter learning for ija  using an  linear 
SVR algorithm.  
The FCFNN-SVR output function in (4) can be written as follows  
1 0 1 0
( ) ( ) ( )
r n r n
i j i j
ij ij
i j i j
y x a x b a x x bµ µ
= = = =
′ = ⋅ + = +∑ ∑ ∑∑K K              (20) 
where 0x  1. Equation (20) shows that the output y′  is a linear function of ( )i jx xµ K  with 
weights ija . Therefore, linear SVR can be employed to learn parameters ija . After structure 
learning, the number of rules r  and rule antecedent part parameters are determined.  
Each input data kx
K  at time step k  is transformed to the following vector  
1 0 1 2 0 2 0 ( 1)( ) [ , , , , , ,  , , , ]n n r r n r nkx x x x x x xφ µ µ µ µ µ µ += ∈
K K … … …… … \ .               (21) 
The vector φK  is fed as an input to a linear SVR, and the training data pairs in (9) are represented 
by 
1 1 2 2{( ( ), ), ( ( ), ), , ( ( ), )}N NS x y x y x yφ φ φ=
K K KK K K"              (22) 
According to (19), the optimal linear regression function is 
1
ˆ( ) ( ) ( ), ( )
N
k k k
k
y x x x bα α φ φ
=
′ = − < > +∑ K KK K K               (23) 
where kα  and ˆkα  are solved by the linear SVR in Subsection IV-A. Equation (23) can be 
represented as follows 
 9 
2 2
2 2
1 1
( ) || ||( ) ( ) exp{ [ ]} exp{ }
2 2
n n
j ij i
i ij j
j j i i
x m x mx M x
d d
µ
= =
−
−
= = − = −∏ ∑ K KK .            (28) 
The output from each rule is a crisp value )(xfi
K , where 
0
1 0
( )
n n
i i ij j ij j
j j
f x a a x a x
= =
= + =∑ ∑K                         (29) 
where 0 :x =1 . The final output is a combination of the output of each rule using the simple 
weighted sum for defuzzification. For the SVM-TFC consisting of r rules, the classifier output is 
1 1 0
( ) ( ) ( )
r r n
T
i i ij i j
i i j
y x f x a x b a x bµ µ φ
= = =
′ = = + = +∑ ∑∑ KK K K K                 (30) 
where 
          1 ( 1)10 1 0[ ]
r n
n r rna a a a a
× +
= ∈ℜK … …… …               (31)  
( 1) 1
1 0 1 0( ) [ ]
T r n
n r r nx x x x xφ µ µ µ µ + ×= ∈ℜ
K K … …… … .             (32) 
and b  is a bias term. For SVM-TFC learning, an SSC algorithm is proposed to determine the 
number of rules r , fuzzy set parameters ijm  and id  in (28). A linear SVM is used to determine 
the consequent parameters ija  in (29). Details of these two learning algorithms are introduced in 
the following two sections.  
3.2 Antecedent Part Learning By SSC Algorithm  
In the proposed SSC algorithm, a cluster in the input space corresponds to a rule in SVM-TFC. 
There are no rules in SVM-TFC initially. All rules (clusters) are generated via performing the SSC 
algorithm on the input data xK . In the SSC algorithm, the variance 2σ  of a cluster is used as the 
criterion to decide whether or not a cluster should be split into two. For existing r  clusters, the 
algorithm finds 
2
1
arg max i
i r
I σ
≤ ≤
=                            (33) 
where 2iσ  is the variance of cluster i . If 
2
Iσ  is larger than a pre-defined threshold thv  
( thv =0.13 in this project), then cluster I  is split into two clusters, resulting in a total of 1r +  
clusters. Denote the center of cluster I  as Im
K . Centers of the two new clusters are initially set to  
1I Im m=
K K
 and 1 2K Im m+ =
K K
,                         (34) 
where 1Im
K  and 2ImK  are the two input samples that have the top two minimum Euclidean 
 11 
1
( ) sign( )
       sign( , )
       sign( , )
T
N
k k k
k
k k k
k SV
f x w x b
y x x b
y x x b
α
α
=
∈
= +
= < > +
= < > +
∑
∑
K K K
K K
K K
                     (38) 
where kα  is a Lagrange multiplier and the training samples for which 0kα ≠  are called support 
vectors (SVs). 
In SVM-TFC, each original input training data xK  is mapped via (7) to a new feature space 
( )xφK K . That is, the original training data set in (11) can be represented as following  
    1 1 2 2{( ( ), ), ( ( ), ) ( ( ), )}N NS x y x y x yφ φ φ=
K K KK K K" , ( 1) 1r nφ + ×∈ℜK .       (39) 
Equation (30) shows that the SVM-TFC output is a linear function of ( )xφK K  with the consequent 
parameter vector aK  functioning as linear combination coefficients. Therefore, it is feasible to 
determine the consequent vector aK  using the linear SVM. The vector φK  is fed as input to a linear 
SVM and the data pairs are classified by the linear SVM in (38). According to (38), we have the 
following output function 
1
( ), ( )
N
k k k
k
y y x x bα φ φ
=
′ = < > +∑ K KK K ,                      (40) 
where kα  is solved by linear SVM. Equation (40) can be represented by 
( 1)
1 1
( 1)
=1 1
1 0 1
 ( ) [ ( ) ( )]
          = [ ( )] ( )
         = {[ ] }
r nN
k k m m k
k m
r n N
k k m k m
m k
r n N
k k i j i j
i j k
y x y x x b
y x x b
y x x b
α φ φ
α φ φ
α µ µ
+
= =
+
=
= = =
′ = +
+
+
∑ ∑
∑ ∑
∑∑ ∑
K K K
K K .                      (41) 
According to the equivalence between (28) and (41), we can easily solve consequent parameters 
ija s’ which are given as following:  
  
1
N N
ij k k i j k k i j
k k SV
a y x y xα µ α µ
= ∈
= =∑ ∑ , 1,...,i r=  and 0,...,j n= .            (42) 
 
4. SIMULATIONS 
4.1 Simulation results of FCFNN-SVR 
This section uses computer simulations to illustrate the performance and capabilities of the 
proposed FCFNN-SVR. The example considers the problem of function approximation. First, 
training data without noise was used to examine the FCFNN-SVR performance. Training data with  
 13 
Example 1 (Noise-free regression).  
The approximated function is defined as follows 
   [ ]
2
2
1 2 1 22 2
1 2
(5 )( , ) ,     , 0,10
3(5 ) (5 )
xf x x x x
x x
−
= ∈
− + −
             (43) 
A clean training set containing 200 samples was randomly generated from the input domain. The 
testing set had 200 points randomly generated by the same way. For FCFNN-SVR training, the 
parameters in the clustering algorithm were set at thµ =0.78 and β =0.7, and 41 clusters were 
generated. For the linear SVR for weight training, different insensitive values (ε =0.07 and 0.02) 
were studied to determine their influence on performance. Table 1 shows the optimal cost parameter, 
the training and test errors, and the total number of parameters in FCFNN-SVR for these different 
insensitive values. Results show that with the same FCFNN-SVR structure, a smaller ε  value 
generates smaller training and test errors. Figure 2 shows the FCFNN-SVR test results when 
ε =0.02.  
For comparison, a Gaussian kernel-based SVR was designed using the same set of training data. 
Different combinations of cost  parameter C  and Gaussian kernel width were tested. Table 1 
shows the best test performance, the resulting test error, and the number of SVs for different 
insensitive values. Results indicate that a smaller ε  value generates a smaller error at the cost of a 
greater number of support vectors and parameters. Table 1 shows that both the number of 
parameters and RMSE of FCFNN-SVR at different ε  values are smaller than the best test results 
of the Gaussian kernel-based SVR at ε =0.02.  
A fuzzy neural network, the self-constructing neural fuzzy inference network (SONFIN) [16] is 
also applied to the same problem. SONFIN also uses TS-type consequent in a fuzzy rule. 
Construction of SONFIN is based on neural learning whose objective is to minimize a training error. 
For clarity of comparison, Table 2 shows the performance of the FCFNN-SVR and SONFIN. For 
clean data, the test error of SONFIN is slightly smaller than the FCFNN-SVR at the cost of a larger 
number of parameters. 
Example 2 (Noisy regression).  
To compare robustness, a noisy training set was generated by adding white Gaussian noise with 
mean zero and standard deviation  0.05 to the output of the clean training set. The original clean 
test set was also used for testing. The FCFNN-SVR contains 46 clusters when thµ =0.79 and 
β =0.7. In contrast to the clean training problem, more clusters were used because learning this 
noisy training data set was more difficult. Figure 2 shows the FCFNN-SVR test result when 
ε =0.02. Table 3 shows the performance of FCFNN-SVR with ε =0.02 and 0.07.  
For comparison purposes, Table 3 also show the performance of a Gaussian kernel-based SVR 
with ε =0.02 and 0.07. As in Example 1, Table 3 shows that both the number of parameters and 
RMSE of FCFNN-SVR are smaller than those of the Gaussian kernel-based SVR at different ε  
values. Table 2 shows the performance of SONFIN for this noisy regression problem. The result  
 15 
Table 4. STATISTICS OF DATA SETS USED FOR EVALUATION. 
Data set Diabetes Sonar Wine Glass 
# attributes 8 60 13 9 
# classes 2 2 3 6 
# samples  768 208 178 214 
# training samples  90% 90% 90% 90% 
# test Samples  10% 10% 10% 10% 
 
Table 5. COMPARISONS OF AVERAGE TEST ERROR RATES FOR DIFFERENT 
CLASSIFIERS ON DIFFERENT DATA SETS. THE BEST RESULT IN EACH COLUMN IS 
INDICATED BY BOLDFACE. 
Classifiers Diabetes Sonar Wine Glass 
FC-GBML[23] - 24.06 6.90 40.38 
GA-NFC [24] - - 8.33 - 
eClass1 [25] 23.26 23.43 2.78 -- 
FC-HGBML [26] 24.17 23.70 4.94 34.63 
FC-SGERD [27] 26.92 24.80 3.81 36.62 
SV-L-inc [28] 29.58 14.93 -- -- 
SVM-TFC 24.52 11.79 1.44 34.16 
 
used as the test pattern. After exchanging the role of each subset, the same training and testing 
procedure was also performed nine times. The 10-CV was iterated five times by using a different  
random sequence. That is, the performance of a classifier was evaluated according to the average 
results of 50 training-testing subsets.  
4.2.2 SVM-TFC Performance 
The threshold thv  determines the number of rules in an SVM-TFC. A smaller value of thv  
generates a larger number of rules. The value of thv  for each data set was selected so that the 
number of generated rules was smaller than five. For parameter learning using the linear SVM, the 
cost parameter C  has to be determined in advance. This parameter was selected from the set { 02 , 
12 , …, 62 } and was determined according to the validation performance measured by training 70% 
of the training set and testing the other 30% of the training set in one CV. For simplicity, this project 
set C  to 42  for all data sets because it achieved good validation performance for different data 
sets. The linear SVM was implemented using the sequential minimal optimization (SMO) algorithm 
[20]. Table 5 shows the performance of the SVM-TFC.  
4.2.3 Comparisons With Different Classifiers 
 17 
(a) (b) (c) 
  
Fig. 5. (a) Test images. (b) Centers of object candidate regions. (c) Morphological opening 
operation results. 
 
 
 
Fig. 6. The searching process and final detection result of the real time system for detecting a can.  
 
used in a real-time object detection system using a Sony EVI-D70 camera having panning, tilting,  
5.1 Feature extraction 
The hue, saturation, and value (HSV) color space is a typical nonlinear transformation of the red, 
green, and blue (RGB) color space. This project uses the HS color space for color histogram 
computation to reduce the influence of lightness. A color histogram is obtained by counting the 
number of pixels that fall into each bin in the HS space. To represent the histogram information as 
accurately as possible, instead of a uniform partition, a non-uniform partition of the HS space is 
used. For example, suppose that there are nine pixels, as shown in Fig. 4. The non-uniform partition 
process is described as follows. For the non-uniform partition of the HS space, a horizontal partition 
is followed by a vertical partition in each partition process. To perform a new partition, the number 
of pixels in each bin is calculated. The bin with the maximum number of pixels is partitioned. In 
each partition process, first a horizontal partition that partitions the bin into two equally sized bins is 
performed. Then, the bin with the maximum number of pixels is found and vertically partitioned 
into two equally sized bins. One partition process ends after the vertical partition is completed. Thus, 
after q  partition processes, we have a total of 2 1q +  bins. Figure 4 shows the non-uniform 
partition process.  
To detect an object from a test image, a rectangle window of size 1 2ω ω×  pixels is defined, 
 19 
  This project next proposes a new fuzzy classifier, the SVM-TFC, which shows the advantage 
of good classification performance with a small number of fuzzy rules. For structure learning, the 
proposed VSSC algorithm automatically assigns proper fuzzy set positions and shapes in the input 
space. For parameter learning, the use of linear SVM helps in finding a set of consequent 
parameters with high generalization ability. Experimental results and comparisons with different 
classifiers verified the advantages given above.  
The proposed FCFNN-SVR and SVM-TFC aims at improving regression or test classification 
accuracy without consideration of model interpretability. A fuzzy classifier with high regression or 
classification accuracy and good interpretability in the constructed classification rules would be 
studied in the future.  
 
REFERENCES 
[1] C. F. Juang and C. D. Hsieh, “A locally recurrent fuzzy neural network with support vector 
regression for dynamic system modeling,” IEEE Trans. Fuzzy Systems, vol. 18, no. 2, pp. 
261-273, April 2010. 
[2] C. F. Juang, R. B. Huang, and W. Y. Cheng, “An interval type-2 fuzzy neural network with 
support vector regression for noisy regression problems,” IEEE Trans. Fuzzy Systems, vol. 18, 
no. 4, pp. 686-699, Aug. 2010. 
[3] C. F. Juang and G. C. Chen, “A TS fuzzy system learned through a support vector machine in 
principal component space for real-time object detection,” accepted to be published in IEEE 
Trans. Industrial Electronics, Dec. 2011. 
[4] V. Vapnik, The Nature of Statistical Learning Theory. New York : Springer - Verlag, 1995. 
[5] C. Cortes and V. Vapnik, “Support vector networks,” Machine Learning Journal, vol. 20, no. 3, 
pp. 1-25, 1995. 
[6] A. J. Smola and B. Scholkopf, “A tutorial on support vector regression,” Statistics and 
Computing, vol.14, no. 3, pp. 199-222, 2004. 
[7] D. H. Hong and C. Hwang, “Interval regression analysis using quadratic loss support vector 
machine,” IEEE Trans. Fuzzy Systems, vol. 13, no.2, pp. 229-237, April 2005. 
[8] D. Li, R. M. Mersereau, and S. Simske, “Blind image deconvolution through support vector 
regression,” IEEE Trans. Neural Networks, vol. 18, no. 3, pp. 931-935, May 2007. 
[9] X. Li, B. Hu, and R. Du, “Predicting the parts weight in plastic injection molding using least 
squares support vector regression,” IEEE Trans. on Systems, Man and Cybernetics, Part C: 
Applications and Reviews, vol. 38, no. 6, pp. 827-833, Nov. 2008. 
[10] G. Wang, D. Y. Yeung and F. H. Lochovsky, “A new solution path algorithm in support vector 
regression,” IEEE Trans. Neural Networks, vol. 19, no. 10, pp. 1753-1767, Oct. 2008. 
[11]  C. C. Chuang, “Fuzzy weighted support vector regression with a fuzzy partition,” IEEE 
Trans. Syst., Man, and Cyber., Part B-Cybernetics, vol. 37, no. 3, pp. 630-640, June 2007. 
 21 
[27] E. G. Mansoori, M. J. Zolghadri, and S. D. Katebi, “SGERD: A steady-state genetic 
algorithm for extracting fuzzy classification rules from data,” IEEE Trans. Fuzzy Systems, vol. 
16, no. 4, pp. 1061-1071, Aug. 2008. 
[28] S. Ruping, “Incremental learning with support vector machines,” Proc. IEEE Int. Conf. Data 
Mining, California, pp. 641-642, Nov.-Dec.2001.  
 2
論文題目為「以連續空間螞蟻群最佳化演算法設計遞迴控制器並用於動態系統控制」。同時
個人亦擔任此 session 的共同主席(session co-chair)。接下來有一場大會演講 Plenary 3: 
Prof. Dr.- Ing. habil. Edgar Koerner, Learning to behave in a natural environment 。晚上參加晚
宴，在船上舉辦，航行於博斯普魯斯海峽，並有肚皮舞與演奏表演。 
13 日: 於前一日一樣，白天為一般 session 進行。共有 9 個 oral session 平行舉辦且今日共四
個 session 場次，早上兩場下午兩場。參加者可自由挑選有興趣的領域。中午大會提供自
助午餐。會議並於今日結束。 
14 日: 早上搭乘新加坡航空由土耳其出發至台灣。 
15 日: 下午返回台灣，結束一次豐富的會議行程。 
 
二、與會心得 
     此次會議為國際電機電子工程師學會(IEEE)系統、人與人工頭腦(SMC)學會每年均會辦理的國際
研討會。個人在 2008,2009 均參加此會議。去年因 SMC 學會底下成立了「Intelligent learning in 
control」這一個學術委員會，個人受邀為創始會員。今年委員會主席籌劃了一個特別議程「Intelligent 
learning in control」，並邀請筆者擔任議程主席。此次會議主要目的為藉此機會幫忙提升此研究領
域的能見度，加強與相關學者的交流。另一個目的為發表個人的研究論文成果「以連續空間螞蟻群最
佳化演算法設計遞迴控制器並用於動態系統控制」與相關領域的學者分享，並討論相關研究主題未來
趨勢。 
  此次會議共接受了 733 篇論文，其中台灣共有 91 篇，顯見台灣學者在這領域已有相當程度的
影響力。此次與國內眾多學者共同組團與會，省去許多時間安排的瑣事，較能全心與會，會議期間遇
到相當多的台灣來的同儕。事實上，個人已出國參加會議多次，每次均自覺收穫頗豐。與第一次出國
參與會議相較，因有了多次的經驗，因此較懂得如何安排時間，以穫得最大的收益。本次計會議地點
位在土耳其，除了可來此希望相關領域的新資訊之外，亦多了解此歐亞交界處的民風信仰與制度。就
個人而言，參加這次會議，無論在學術或生活視野的擴展上，均覺得相當有收穫。 
    此次會議主題為 Intelligent Systems for a Safe and Secure World。隨著社會與科技的進步，
我們需要更複雜的大型系統，如電力、通訊與交通網路系統等，來提高生活品質。設計、控制並保證
 4
 
 6
 
 8
 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/14
國科會補助計畫
計畫名稱: 基於支持向量機之模糊回歸器與分類器及其應用
計畫主持人: 莊家峰
計畫編號: 98-2221-E-005-041-MY2 學門領域: 智慧型控制
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 計畫主持人莊家峰榮獲中興大學 99 年度「研究績優獎」(2011 年) 
2. 計畫主持人莊家峰榮獲中興大學 98 年度「研究績優獎」」(2010 年) 
3. 計畫主持人莊家峰榮獲中華民國系統學會第一屆「傑出青年獎」(2010 年)
4. 計畫主持人莊家峰指導碩士班學生榮獲 2010 年「中華民國模糊學會碩士論
文獎」第一名 (2010 年) 
5. 計畫主持人莊家峰於 2009-2011 期間有 4 篇 ISI 高引用論文(Highly cited 
paper). 
 
 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
