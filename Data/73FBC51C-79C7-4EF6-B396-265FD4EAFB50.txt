 
中 文 摘 要 ： 現代高性能處理器以亂序執行模型（out-of-order 
execution model）達到高指令平行度(instruction-level 
parallelism, ILP)，然而複雜的控制電路也限制了時脈速
度。主要的效能障礙包括大指令發出窗(instruction issue 
window)，二次方成長的資料相依檢查電路，以及隨著導線延
遲增加帶來的晶片佈局困難。我們提出瀑布執行模型
（waterfall execution model）來達到亂序執行同等的性
能，但以循序執行（in-order execution）控制方式來降低
電路複雜度。 
構想之瀑布執行模型串連一序列的單層資料驅動的執行階，
稱之為 waterfall stage，以達到亂序執行效能。階層數可
為任意。指令基本區塊(basic block)逐階下送。可執行指令
在本階執行，無法此刻本階執行的指令則送往下階。對比於
各自執行所有指令，前階因跳過需等待執行的指令而加速，
後階因前階執行了部份指令而加速。以這種簡單持續的前進
方式，瀑布模型不會被執行時間阻礙指令流的流動。理想狀
態下，以足夠的階層數，指令基本區塊可以在 critical 
path 所限制的時脈數內執行完畢。 
發展中的半導體技術提供了豐富的電路容量，鼓勵計算機架
構充分使用這些電晶體。然而導線延遲同時限制了電路的複
雜度。瀑布模型優美的設計在於規則的線路結構及極簡化的
流動控制。本計劃將以三年時間探討瀑布模型及設計問題，
包括瀑布模型的正規描述，參數或設計考量，實作問題，效
能計算及評估，軟體環境，及可能的延伸應用。我們也將開
發開放源碼的模擬器及 tool chain，以作為評估及發展平
台，並進以推動學界及業界的後續研究。此技術一旦證明可
行，將為同等效能的最簡單設計。其模組化的設計觀念兼可
應用於高效能或嵌入式處理器，也是多核心時代（many-core 
era）很有希望的處理器設計方向。 
中文關鍵詞： 處理器執行模型、控制電路複雜度、指令平行度、時脈頻率 
英 文 摘 要 ： Out-of-order execution model achieves high 
instruction-level parallelism but control complexity 
impedes performance. Major performance hurdles 
include large issue-window size, quadratic complexity 
growth of data dependency check, and increased wiring 
delays due to difficult floor-planning. We propose a 
waterfall execution processor model that achieves 
out-of-order performance but only with in-order 
control complexity. 
Waterfall execution model concatenates a series of 
 行政院國家科學委員會補助專題研究計畫 □ 成 果 報 告   □期中進度報告 
 
瀑布執行之處理器模型： 
以循序控制電路複雜度達到亂序執行效能 
 
計畫類別：□ 個別型計畫  □ 整合型計畫 
計畫編號：NSC97－2221－E－009－058－MY3 
執行期間：  97 年 8 月 1 日至 100 年 7 月 31 日 
 
計畫主持人：鍾崇斌 
共同主持人： 
計畫參與人員：李元化，黃勁霖，謝凱仲，黃清裕，陳泓翔，陳永志 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立交通大學資訊工程學系（所） 
 
中   華   民   國  100  年  10  月   31  日 
X 
X 
X 
X X 
accelerated because the waiting instructions are skipped, and latter stages are accelerated because 
some instructions have been executed in early stages. With simple, always-advance-one-stage 
instruction dispatching rule, no waterfall stall will ever occur. Ideally, with sufficient stage depth, 
a basic block can be executed in a number of cycles equal to its critical path depth. 
      Current semiconductor technology offers abundant circuit capacity, encouraging 
architectures that utilize more transistors. And the beauty of waterfall execution model lies in its 
extremely regular circuit structure, and very simplified and streamlined control. This three-year 
project will study waterfall model and related design issues. The study will include formal 
description of waterfall execution model, parameter and design considerations, implementation 
issues, performance evaluation and calculation, software environment, and possible extensions. 
We will develop open-source simulator and tool chain, which will serve as the evaluation and 
development platform and will promote the deployment and following research of the model. 
This model, if proved working, will be the simplest design providing the equivalent performance. 
The modularized design concept can apply to both high-performance processors and embedded 
processors, and is a very promising direction for processor design in many-core era. 
 
Keywords:  Processor Execution Model, Control Circuit Complexity, Instruction-Level 
Parallelism, Clock Rate 
 
3.1.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.3 Completed tasks and results . . . . . . . . . . . . . . . . . . . . . . . . . 11
4 Performance evaluation 14
4.1 Evaluation environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.3 Completed tasks and results . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2 Performance evaluation and application suggestions . . . . . . . . . . . . . . . . . 15
4.2.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2.3 Completed tasks and results . . . . . . . . . . . . . . . . . . . . . . . . . 16
5 Extended use of waterfall execution model 17
5.1 Waterfall execution model to accelerate memory accesses . . . . . . . . . . . . . . 17
5.1.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.1.2 Our approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5.1.3 Completed tasks and results . . . . . . . . . . . . . . . . . . . . . . . . . 18
5.1.4 Further discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1 Preface
Out-of-order execution model, as shown in Figure 1b, exploits ILP by executing instructions in
instruction window at the earliest possible moment. Although instructions can execute out of pro-
gram order, the model still has to maintain the data and control dependency specified in sequential
program by instruction scheduler, physical register file, and ROB. As dependency could exist be-
tween any pair of instructions in instruction window, the implementation complexity limits the
maximum achievable performance.
Researches that exploit ILP with less delay complexity lie in several directions. Low complex-
ity instruction scheduler has been studied in last decades [1] [2] [3] [4] [5] [6]. Recent researches
2
on an instruction stream. We also theoretically proved the correctness of this computation
concept.
• We completed major designs of waterfall execution model, the data structures, the operation
rules, and logic circuits.
• We built a simulator by implementing these structures and logic circuits on Simplescalar
[14] to evaluate the correctness and effectiveness of our design.
• We completed a conference paper on our early results [15].
• We completed the extended use of waterfall execution model: preserving buffer to tolerate
long D-cache miss latency. We also completed a master thesis titled as “Tolerating Long
Cache Miss Penalty by Extending Effective Instruction Window with Low Complexity”.
• We explored the performance enhancement of waterfall execution model for loop-level par-
allelism (usually existing in a program exhibiting data-level parallelism): loop pipelining.
In this (the third) year, we further work on these tasks:
• We worked on simulating the power consumption of the processor with preserving buffer.
• We quantitatively analyzed the area overhead of preserving buffer.
• We completed a conference paper on hiding L2 D-cache miss penalty [16].
In this report, we briefly describe the problems of the scheduled research topics in this year and
summarize completed tasks, results, and on-going tasks.
2 Fundamental study of waterfall execution model
2.1 Proposal of stream computation concept
2.1.1 Problem description
Out-of-order processors suffer from high delay complexity of instruction window that limits max-
imum achievable performance. The difficulty is inherited from the sequential execution model,
4
P P
Figure 4: Two processors cooperatively execute one instruction stream.
11
11
222
2
2
1
1
1
1
1
1
1
1 2 3 4 5 6 71 2 3 4 5 6 7 81 2 3 4 5 6 7 8 910 11 12 139
ld r5, [r2]
ld r6, [r3]
mul r5, r5, r1
add r6, r6, r5
st [r3], r6
add r2, r2, 4
add r3, r3, 4
add r4, r4, 1
ble l1, r4, r1
Figure 5: Two processors cooperatively execute one instruction stream.
The computation is complete if the last processor generates an equivalent instruction stream with
all instructions in value states.
Figure 5 illustrates the two-stage waterfall execution of the well-known subroutine “DAXPY”.
On the left is the assembly code of the major loop of DAXPY. The loop load one element from
each of the two arrays (which are X and Y), multiphy one element (X) by a constant (A), adds
the result with the other element (Y) and save the multiplication result back to one element (Y).
The first diagram shows, over the time axis, the execution of the loop in an in-order processor.
The execution latency of load is two, and that of multplication is three. The number of cycles
to complete one iteration is 13. A cheap method to speedup the execution is superscalar in-order
execution, which is illustrated in the second diagram. The number of cycles to complete one
iteration is 9.
In the third diagram, the loop is executed by two processing elements (PE). Instructions executd
in PE 1 is labeled with “1” and shaded in blue color, and so forth. The instructions not colored
6
a EXE
IQ
b
a EXE
IQ
R
b
a EXE
IQ
b
c
c
R
R
Figure 6: Organization of waterfall model.
2.2 Proposal of waterfall execution model
2.2.1 Problem description
Stream execution model is conceptual and not practical because it needs to look up the source
operands upward the instruction stream. We will realize this concept by waterfall execution model.
The new computation paradigm need to conquer following difficulties.
• How to maintain register status
• How to exchange value
• Precise exception
• Memory dependency handling
2.2.2 Our approach
Waterfall execution model realizes stream execution model by organizing the computation ele-
ments by stages, each has the following designs as depicted in Figure 6.
• R: Stage register file or register cache, which stores the most recently generated values. The
register file supplies the source operands of the coming instructions. It contains values which
could be generated locally, carried by instruction stream, propagated from the following
stages, or invalid value.
8
– register status table
• How to exchange value
– local register cache
– value state instructions
– backward link
• Precise exception
– in-order between stages
– inter-stage, maintained by traditional approaches
– intra-stage, flush by stage
3 Performance enhancement of waterfall execution model
3.1 Loop pipelining (static instruction deployment)
3.1.1 Problem description
Loops take major parts of execution time in a program, especially for stream processing programs.
In a system of processor array, we see a chance that programs can fully utilize the computation
resources if program flow is stable and repeated. By statically deploiting instructions on the pro-
cessor array, we can have throughput of one loop per cycle.
3.1.2 Our approach
We observed that the loop index is usuallt the only variable depending on previous loop. (Consider
the incremental operation of “for” loop.) The remaining variables depends on the loop index. If
the loop index manipulation can be deployed in the beginning stages of a processor array, and the
remaining operations can be formed in a pipelied way, we can achieve throughput up to one loop
per cycle. This can be achieved if a simple loop can be detected from an instruction stream, and
the computation can be deployed in the remaining stages.
10
in
c
r4
c
m
p
bg
t
ld m
u
l
st bld
r0 r1 r2
r4
r5
r4 r4 r4 r4 r4
r5
r3
r6r6
r4 r4
r5 r5
r4
r5
r7
r4 r4 r4
r7
Pipelined D-cache / memory / data path
re
g
fil
te
r 
da
ta
fif
o
ex
e
pi
pe
pa
c
ke
r
Figure 9: Loop pipeline implementation.
A legal loop for loop pipelining needs to meet two criterias: fixed program flow and forward
data movement. Figure 8 shows a loop and the corresponding assembly code. The loop can be
rotated to start with the “add” instruction and end with the “str” instruction, so the only inter-loop
dependency (which is “r4”) is manipulated at the begin of the loop. After the rotation, the loop
meets both criterias.
Figure 9 shows the implementation of the loop:
for i = 0 to N
c[i] = a[i] * b[i]
inc r4 // i
cmp r4, r0 // N
bgt l_exit
12
involved in the PE pipeline, which is the number of instructions in the loop.
4 Performance evaluation
4.1 Evaluation environment
4.1.1 Problem description
Since waterfall execution model is completely different from existing ILP approaches, we need a
new simulator to evaluate performance and facilitate verification.
4.1.2 Our approach
We will start our simulator by studying these items. We will also study existing simulators and
construct our simulator with minimum effort.
• Simulation considerations
– ISA
– tool chain and OS
– benchmark suites
– Modeling methodology: A. target architecture B. structure / algorithm / evaluation
parameters and method
• Simulator structures / inputs / outputs
• Machine definition
• Instruction decode and execution in simulator
• Configuration
• Information collection
14
00.5
1
1.5
2
2.5
3
3.5
4
1 2 4 8 16
Waterfall stage depth
IP
C
DW 1
DW 2
DW 4
Figure 11: IPC increases with number of waterfall stage, for dispatch width of one or four. (DW
denotes dispatch width.)
4.2.3 Completed tasks and results
Figure 11 shows the performance, in terms of IPC, of waterfall model under configuration of
different stage number and dispatch width. Here we assume a perfect branch predictor and no
cache miss penalty. Execution latency is two. The results are consistent with our prediction that
dispatch width is the upper bound of IPC. The IPC curve rises toward the performance limit as
stage depth increases. The one-dispatch curve reaches 90% of achievable IPC at stage depth of
one, the two-dispatch curve at stage depth of four, and the four-dispatch curve at stage depth of
16, which is consistent with the square-root law of ILP [17]. This figure verifies the performance
analysis and exhibits the machine parallelism of waterfall model.
Now we come closer to the performance of each benchmark program. Figure 12 shows the
performance of waterfall model, having dispatch width of four and stage number of four. Besides
waterfall model, four-issue sequential model, and four-issue out-of-order model are put into com-
parison. The size of out-of-order instruction window is set the same as the aggregated instruction
queue size of a four-dispatch four-stage waterfall model, which is 32. The waterfall model has IPC
of 2.3, which is comparable to IPC of 2.2 of out-of-order model. In most cases, out-of-order model
performs slightly better. Waterfall model outperforms out-of-order model for two programs, gap
16
ROB are replaced with checkpoint [19], and LSQ are replaced by hierarchical designs [19, 20, 21].
These will be discussed with more details in the next section.
5.1.2 Our approach
The difficulties of execute-ahead designs root on preserving instructions with data and bringing
them back into the execution core later. We propose to preserve the changing processor state incre-
mentally in a stream structure. The rationale is based on two observations. First, a processor does
not need to monitor the status of instructions depending on long cache miss, but only need to mon-
itor this long-latency operation. Second, these long-latency operations complete in order if the bus
system follows a strict bus ordering rule of in-order completion. The stream preserves long-waiting
instructions and manages them sequentially. Sequential management means that these waiting in-
structions are removed from the instruction window, preserved in the stream, and returned to the
processor all in program order.
5.1.3 Completed tasks and results
Figure 13: Microarchitecture of the proposed design. PB, RAC, RRF, and some renaming or
bookkeeping logic circuits are augmented to an OOO processor.
The design The idea is realized by preserving instructions with execution results in a first-in-
first-out preserving buffer (PB), as shown in Figure 13. Therefore, as the processor enters the run-
ahead state, instructions together with execution results in ROB are sequentially driven from the
ROB to the PB. PB is a simple FIFO structure with linear complexity and therefore can be scaled
18
parameters. These establish a foundation for the following exploration of parameters. Finally,
we evaluated how big the instruction window of a conventional design has to be to achieve the
performance achieved by this design. Our simulator was constructed on SimpleScalar 3.0 simulator
[14]. The performance was compared using a set of pre-compiled SPEC INT2000 and SPEC
FP2000 benchmark suites on Alpha ISA, using MinneSPEC inputs [22] to shorten simulation time.
Parameter OOO baseline Run-ahead execution ROB+PB
Width fetch:4, decode:4, issue:4, commit:4
Function units 4 integer ALUs (1-cycle), 2 integer multipliers (7-cycle),
4 FP ALUs (4-cycle), 2 FP multipliers (7-cycle)
RS size 128 128 128
ROB size 128 128 128
PB size - - unlimited
LSQ size 128 128 + perfect RAC 128 + perfect RAC
Branch prediction Gshare, 4K-entry prediction table
L1 data cache 32 KB, 4 way
L1 inst. cache 32 KB, 4 way
L1 latency 2 cycles
L2 Unified Cache 1 MB, 8 way
L2 latency 12 cycles
Memory latency 300 cycles
TLB 32-entry, 4 way associative, 4 KB page size, 32-cycle penalty
Table 1: Parameters of the designs under comparison.
(a) SPEC INT2000 (b) SPEC FP2000
Figure 15: Overall performance.
We first evaluate the achievable performance of the proposed design and compare it with the
baseline with or without run-ahead execution. The result is shown in Figure 15. Our design has
20
Figure 17: Area details of the indiviual components for the processors based on run-ahead execu-
tion, preserving buffer (PB), and defer queue(DQ).
and DQ is about the same. Figure shows the transistor counts of individual components.
5.1.4 Further discussion
Besides the performance advantage of preserving execution results, our design also reduces wrongly
predicted branches when execution resumes. The processor can employ the proposed design and
a smaller L2 D-cache for the same performance target. Old wisdom taught us that performance
comes from parallelism, speculation, and locality. To support the wide superscalar out-of-order
parallel execution, the front-end unit has to provide an accurate speculative instruction stream and
the instruction and data caches need to exploit all possible locality. In this work, we verified a
new approach that does not rely on the diminishing investment on cache or pre-fetchers but utilizes
early execution of future instructions. This approach is even more effective for the series memory
wall problem of future semiconductor technology or chip multi-core processor.
During the development of this design, we also observed several extension. First, the run-ahead
execution scope can be carried out by another execution context in a multi-thread processor. There-
fore, the processor can constantly exploit distant ILP, comparing with the occasional activation of
the scheme only when L2 cache misses loads. Second, we observed a possible way to reduce the
instructions store in the PB. The PB can preserve waiting instructions and those instructions carry
the values needed by the waiting instructions. Those instructions not needed by the main instruc-
tion scope can be omitted. However, such an approach need checkpoints for possible recovery due
22
national symposium on Computer architecture. Washington, DC, USA: IEEE Computer
Society, 2002, pp. 59–70.
[8] A. Cristal, D. Ortega, J. Llosa, and M. Valero, “Out-of-order commit processors,” in HPCA
’04: Proceedings of the 10th International Symposium on High Performance Computer Ar-
chitecture. Washington, DC, USA: IEEE Computer Society, 2004, p. 48.
[9] S. T. Srinivasan, R. Rajwar, H. Akkary, A. Gandhi, and M. Upton, “Continual flow pipelines,”
in ASPLOS-XI: Proceedings of the 11th international conference on Architectural support for
programming languages and operating systems. New York, NY, USA: ACM Press, 2004,
pp. 107–119.
[10] M. Perica`s, A. Cristal, R. Gonza´lez, D. Jime´nez, andM. Valero, “A decoupled kilo-instruction
processor,” in High-Performance Computer Architecture, 2006. The Twelfth International
Symposium on, 2006, pp. 53–64.
[11] O. Mutlu, J. Stark, C. Wilkerson, and Y. N. Patt, “Runahead execution: An alternative to very
large instruction windows for out-of-order processors,” in HPCA ’03: Proceedings of the 9th
International Symposium on High-Performance Computer Architecture. Washington, DC,
USA: IEEE Computer Society, 2003, p. 129.
[12] R. Barnes, S. Ryoo, and W. Hwu, “”flea-flicker” multipass pipelining: an alternative to the
high-power out-of-order offense,” Microarchitecture, 2005. MICRO-38. Proceedings. 38th
Annual IEEE/ACM International Symposium on, pp. 12 pp.–, 12-16 Nov. 2005.
[13] D. Burger, S. Keckler, K. McKinley, M. Dahlin, L. John, C. Lin, C. Moore, J. Burrill, R. Mc-
Donald, and W. Yoder, “Scaling to the end of silicon with edge architectures,” Computer,
vol. 37, no. 7, pp. 44–55, July 2004.
[14] D. Burger, T. M. Austin, and S. Bennett, “Evaluating future microprocessors:
The simplescalar tool set,” Tech. Rep. CS-TR-1996-1308, 1996. [Online]. Available:
citeseer.ist.psu.edu/burger96evaluating.html
[15] W. Y.-H. Li and C.-P. Chung, “Waterfall execution model for cooperative instruction level
parallelism,” in International PhD Student Workshop on SOC, 2008, 2008.
24
[23] M. Steinhaus, R. Kolla, J. L. Larriba-Pey, T. Ungerer, and M. Valero, “Transistor count and
chip-space estimation of simplescalar-based microprocessor models,” 2001.
26
國科會補助計畫衍生研發成果推廣資料表
日期:2011/12/21
國科會補助計畫
計畫名稱: 瀑布執行之處理器模型：以循序控制電路複雜度達到亂序執行效能
計畫主持人: 鍾崇斌
計畫編號: 97-2221-E-009-058-MY3 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
