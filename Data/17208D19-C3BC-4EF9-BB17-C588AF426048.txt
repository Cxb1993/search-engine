2012-2013 (Final Year) NSC Project Outcomes 
 
Project Code: NSC 99-2221-E-492 -005 -MY3  
PI: Prof. Matthew Smith, 
      Department of Mechanical Engineering, NCKU 
CO-PI: Prof. Jong-Shinn Wu, 
      Department of Mechanical Engineering, NCTU 
CO-PI: Mr. Fang-An Kuo, 
      National Center for High-performance Computing (NCHC) 
CO-PI: Mr. Chih-Wei Hsieh, 
      National Center for High-performance Computing (NCHC) 
 
 
Publication (SCI Papers, International) resulting from NSC project (3rd Year): 
 
[1] Y.-J. Lin, M.R. Smith, F.-A. Kuo, H.M. Cave, J.-C. Huang and J.-S. Wu, A True 
Directional Reconstruction to the Quiet Direct Simulation Method, Computer 
Physics Communications, 184[11], pp. 2378–2390, 2013. 
 
[2] C.-W. Lim, M.R. Smith, M.C. Jermy, J.-S. Wu and S.P. Krumdieck, The 
Direction Decoupled Quiet Direct Simulation Method for Rapid Simulation of 
Axisymmetric Inviscid Unsteady Flow in Pulsed Pressure Chemical Vapour 
Deposition, Computers and Fluids, 88, pp:14-27, 2013. 
 
[3] C.-C. Su, M.R. Smith, J.-S. Wu, C.-W. Hsieh, K.-C. Tseng and F.-A. Kuo, Very 
Large Scale Simulations on Multiple Graphics Processing Units (GPUs) for the 
Direct Simulation Monte Carlo Method, Journal of Computational Physics, 231, 
pp. 7932-7958, 2012. 
 
[4] K-M. Lin, C.-T. Hung, F.-N. Hwang, M.R. Smith, Y.-W. Yang and J.-S. Wu, 
Development of a Parallel Semi-Implicit two-dimensional Plasma Fluid Modeling 
Code using the Finite Volume Method, Computer Physics Communications, 
183[6], pp. 1225-1236, 2012. 
 
Conference Papers (International) resulting from NSC project (3rd Year) 
 
[5] M.R. Smith and Y.-C. Chen, Accelerated Composite Distribution Function Methods 
for Computational Fluid Dynamics using GPU, International Workshop on 
Computational Science and Engineering, October 14-17, 2013, National Taiwan 
University, Taipei, Taiwan. 
 
[6] M.R. Smith, Vector Accelerated Finite Volume Computations with Adaptive Grids 
using GPU and Advanced Vector Extensions, 2013 Conference on Computational 
Mathematics and Annual Meeting of TWSIAM, June 1-2, 2013, Providence University, 
Taichung, Taiwan. 
 
controlled volume of precursor solution at high supply pressure is injected into a 
continuously evacuated reactor volume via an ultrasonic atomizer or choked orifice. The 
injection of precursor mixture is carried out rapidly in the partially evacuated reactor 
volume, increasing reactor pressure to a maximum. The process is followed by a pump-
down phase when the reactor inlet valve is closed while the reactor volume is continuously 
evacuated by a vacuum pump to achieve a set minimum pressure before the next pulse 
cycle begins. The rapid injection of precursor solution leads to a high vapour concentration 
near the reactor inlet during the injection phase while the continuously evacuated reactor 
chamber causes the fluid density to reduce significantly with time after the end of the 
injection phase, and with the distance from the inlet. This pulsed process cycle causes a 
highly unsteady flow field with large density gradients throughout the reactor volume.   
 
In order to apply QDS to a high-resolution, axisymmetric simulation of a PPCVD reactor, 
we require fluxes in a conventional Finite Volume form – i.e. rather than choosing true 
directional fluxes, we require the direction decoupled fluxes. These can be found directly 
by applying Gauss-Hermite integration on the surface fluxes computed by the EFM 
technique. This permits the fluxes splitting to approximate the EFM flux expressions 
given in Equation (2.2) as: 
          





N
j
jjjs
N
j
jjjs
uv
qfwqHqfwqHdvvf
e
11
2
)(
2
2
2


 
where Hs(x) = 1 if x > 0, else Hs(x) = 0 while wj and qj are the weights and abscissas of the 
Gauss-Hermite parameters. The abscissas are the roots of the Hermite polynomials which 
can be defined by: 
 11 22)(   nnn nHqHqH   
where H-1=0 and H0=1. The weights can be determined from: 
 
  21
2
1 !2
in
n
j
qHn
n
w




  
The flux due to the gas molecules transporting from the left to the right of the surface can 
be determined by evaluating the integral given as: 
  

 
0
dvvfvF    
while the flux due to the gas molecules transporting from the right to the left of the surface 
is computed by evaluating the integral given as: 
  

 
0
dvvfvF    
Using the equilibrium molecular velocity distribution on either side of the surface, we can 
evaluate these equations by letting the thermal velocity v = v – u and taking the moments 
of the conserved macroscopic property η around f(v) as: 
 Figure: Control volume element for a single cell of a cylindrically axisymmetric geometry. 
 
In the second order scheme, the split fluxes at the cell interface are reconstructed in a 
manner similar to the conventional reconstruction method. Hence, by taking a Taylor series 
expansion of the split fluxes at the cell interface, a second order accurate expression for the 
net fluxes FR and FL are given as: 
 







































1
1
2
1
2
1
22
i
i
i
i
ii
R
dr
dFr
F
dr
dFr
FFFF   
 







































1
1
2
1
2
1
22
i
i
i
i
ii
L
dr
dFr
F
dr
dFr
FFFF   
where the flux gradients dF/dr are calculated from the finite difference approximations of 
F+ and F– using a slope limiter to maintain positivity. The gradients of the fluxes are 
determined using the MINMOD (Minimum Modulus) or the MC (Monotonized Central 
Difference) slope limiter. A variable time step scheme is used to maintain the maximum 
kinetic Courant–Friedrichs–Levy (CFL) number in the domain below a desired value 
(usually ≤1). It is important to note that this CFL restriction is to maintain physical realism 
and is not related to the numerical stability of the scheme.  For a two-dimensional or 
axisymmetric simulation, the CFL number is given by: 
 
   













r
tRTqu
,
x
tRTqu
maxCFL
i(max)jri(max)jx
  
where qj(max) is the maximum value of the particle abscissas (i.e. the value which gives the 
maximum particle thermal velocity). 
 
 
α 
r 
Δr 
Δx 
 
FR  
FL 
 
 
AR 
AL 
 Figure: Normalized density [top] and velocity [bottom] profiles taken from radial location 
0.1r (where r is the initial shock location) for a spherically imploding shock (MS = 5.0). 
The QDS solution (using 3 sample points, or particles, per pair of split fluxes) employs 800 
cells with second order spatial accuracy using a MINMOD limiter. Results are compared 
with numerical and analytical solutions from Ponchaunt et al. The gas is assumed ideal (γ 
= 7/5) and inviscid. 
 
The DD-QDS simulation was conducted with 800 cells and a second-order spatial accuracy 
using a MINMOD limiter to ensure positivity. Since the gas is ideal (γ = 1.4) and inviscid, 
this problem has no characteristic length. Hence, the variables ρ, u and time t shown in the 
figure above are normalized by the undisturbed density in region I (ρI), uref and tref, 
respectively, where uref and tref are the velocity and time at which the imploding shock 
passes the location at 0.1r. The regions I (undisturbed gas, prior to the arrival of the 
incoming shock), II (post incoming shock, pre-shock reflection), III (post shock reflection, 
pre-reflected shock) and IV (post reflected shock) are clearly shown and in good agreement 
with numerical and theoretical solutions provided by Ponchaunt et al..  
 
A more complex benchmark is the axisymmetric shock-bubble interaction problem, as 
shown below. Following this, we show the comparison of density contour between the 
simulation results of the DD-QDS solver and the EFM solver at the simulation time of 0.2 
s. As a consequence of interaction of a right-moving incident shock, the bubble region with 
lower density than the surroundings expands radially outward and deforms into a kidney-
shape vortex as a re-entrant jet forms at the upstream interface, as visualised by Haas & 
Sturtevant [J. Flu. Mech., 181, 1987] and also predicted by Picone & Boris [J. Flu. Mech, 
189, 1988.] and Bagabir et al. [Shock Waves, 11, 2001]. The comparison shows that the 
DD-QDS solver is able to capture similar shock-bubble interaction flow field features 
similar to the EFM. The backward-moving reflected shock and right-moving transmitted 
shock have almost exact match in position to that in the EFM’s result. 
 
 
 
 
Figure: Axisymmetric shock bubble interaction problem employed as part of the 
verification of the direction decoupled extension of the higher order QDS fluxes. A shock 
wave (left) is approaching a stationary bubble of lower density (higher temperature) 
surrounded by a similar gas at the same pressure.  
0.0 0.1 0.4 1.0
nozzle, are captured by the QDS solver. This demonstrates the capability of the QDS solver 
in modelling unsteady flow phenomena at low computational cost. 
 
 
 
Figure: Natural logarithms of density contours (left) and pressure contours (right) for the 
unsteady flow development of an under-expanded jet in the PP-CVD reactor at standard 
operating conditions during first 4 ms of the injection phase. 
 
Higher Order UEFM Development 
 
Previously, the Uniform Equilibrium Flux Method (UEFM) was derived using a volume-
to-volume flux computation for the sake of reducing the errors associated with direction 
decoupling [Smith, JCP, 2008]. However, before the UEFM method can be meaningfully 
extended to the unstructured Finite Volume Method simulation using GPU acceleration, a 
         
0.2 ms 
0.1 ms 
0.3 ms 
0.5 ms 
1.0 ms 
2.0 ms 
3.0 ms 
4.0 ms 
0.1 ms 
0.2 ms 
0.3 ms 
0.5 ms 
1.0 ms 
2.0 ms 
3.0 ms 
4.0 ms 
  vvwvwvwdv
a
vw
dv
a
vw
dv
a
vw
dvafvw N
av
av
av
av N
N
av
av
N
i
ii
N
N
  







 
...
2
...
22
21
2
2
1
1
1
2
2
11
1
 
which is essentially equivalent to the mass compatibility equation above. The final 
requirement regards the third moment, for which we essentially desire the thermal variance 
of the approximate distribution to be equal to the original Maxwell-Boltzmann equilibrium 
distribution function.  
      2
1
2
2
2
2
2
1
1
2
1
2
1
2
1
2
2
2
11
1
2
...
22
svwdv
a
wv
dv
a
wv
dv
a
wv
vwdvafwvdvafwvv
N
i
i
av
av
av
av N
N
av
av
N
i
i
N
i
ii
N
i
ii
N
N









  








 

 
            
which leads to the equations: 
    2
2
2
2
21
2
1
1
2
3
....
33
s
wawawa
dvafwvv NN
N
i
ii 

 
      
Hence, for the determination of thermal velocities a and weighting fractions w, there is 
essentially an infinite number which will satisfy the restrictions given by these equations. 
In the original UEFM, these fractions and thermal speeds were provided by the binomial 
distribution for large numbers of N. In the case where we employ two discrete step 
functions, however, we can make some assumptions about the relationship between the 
thermal velocities of each. Substitution of w2 = 1 – w1 into previous equations provides: 
 
  21
2
21
2
1 31 swawa                     
 
2/1
1
1
2
1
2
2
1
3









w
was
a     
Therefore, to prevent physically non-meaningful imaginary solutions of a2, the 
numerator of the second equation must remain positive. Otherwise, values of a and w can 
(more or less) be selected as desired. The values employed during this study are w = (1/6, 
5/6) and a = (3/2(3)1/2, 3/2) which are shown in the figure below. For comparison, the 
original equilibrium distribution function is also shown. Following the determination of 
suitable values of w and a, the approximate equilibrium split fluxes are easily determined. 
Due to the lack of bulk velocity in the velocity distribution, moments must be slightly 
modified from those shown previously and must be taken around thermal velocities c using 
the frame of reference of the moving gas, i.e.: 
 
  dcE
vc
vc
a
c
F
a
v
in























 

2
22
            dcE
vc
vc
a
c
F
v
a
in
























 

2
22
              
 
For the sake of completeness, we write the full form of these (one dimensional) fluxes 
below: 
 
complicated due to (i) the lack of structured memory access to the GPU device, and (ii) the 
differing number of interfaces requiring net flux calculation for each cell. The algorithm 
can basically be described by the following steps: 
 
1. The unstructured grid is computed. In this case, the GPU is employed to assist in 
the acceleration of this process. The details of this acceleration fall outside the 
scope of this (analytically-centered) paper. 
2. The problem size is confirmed and the memory required for both the CPU and 
GPU calculations is allocated. 
3. The grid – consisting of a one dimensional array of cell neighbours 
(d_neighbours), the number of neighbours for each cell (d_no_neighbours) and 
the key index for each cell (d_neighbour_key) is transferred, together with 
boundary condition information and initial conditions, to the device from their 
respective host copies. 
4. The transient time stepping begins – for each time step, there are several 
processes, each computed in parallel with their own CUDA kernel:  
a. Calc_Fluxes kernel: Computes the forward and reverse fluxes within 
each cell. Split fluxes in the x, y and z coordinate directions are computed. 
Here the solver exhibits embarrassingly parallel behavior.  
b. Update_State kernel: Each cell i cycles through its available neighbors j 
and calculates the gradients of split fluxes. Following this, or alternately 
if the solver is configured for first order, the net flux between cells i and 
j is computed. The array of conserved quantities in the cell is updated 
using the net flux, time step and the ratio of the area shared between the 
cells and the volume of cell i.  
c. Update_Primitives kernel: In the event where the user needs to know the 
value of primitive quantities in the cell (i.e. temperature, bulk velocity 
etc.), this kernel is called to do so. Again, due to the lack of inter-cell 
communications, this process is embarrassingly parallel. 
 
Hence, the time stepping procedure requires 3 steps (performed in serial), each of 
which is performed in parallel. This is essentially identical to the parallelization of the 
SHLL technique discussed in the original NSC project application except the data and 
memory access is not well structured.  
 
A three dimensional, compressible transient flow around an aerodynamic body was used 
to test the performance characteristics of the proposed scheme. In addition to the original 
EFM scheme, we also use the Quiet Direct Simulation (QDS), another approximation to 
the EFM method which replaces the continuous moments taken in UEFM with numerical 
integration using Gauss-Hermite Quadrature. The comparison of EFM and UEFM overall 
performance is shown in the table below. We see an average increase in performance of 9% 
over the EFM method when using a two bucket distribution as shown above. We can see 
that the UEFM solver – when employing two step functions to approximate the equilibrium 
– performs 9.19% faster overall. Considering that the only differences between the solvers 
was the flux computation kernel, which requires approximately 64.48% of the overall 
computational load (for a first order configuration), we find that there is an approximate 
CPU architectures which occurred in this time. With the introduction of the Sandy Bridge 
core came the extension of the x86 instruction set known as the Advanced Vector 
eXtensions (AVX), first proposed by Intel in 2008 as the logical successor to the Streaming 
SIMD Extensions (SSE) used on earlier CPUs. The basic premise of the AVX concept is 
the ability of an AVX enabled core to perform a single instruction across a set of data 
contained within the AVX registers. These registers – 256 bits in length – allow for SIMD 
style parallel computation upon eight floating point variables (or 4 double variables). 
Previous research into the use of AVX to support CFD computation have shown that a 
large fraction of the performance increase between AVX and older SSE-based CPUs is due 
to the use of the AVX extensions.  
With the introduction of the E5 series of Intel Xeon CPU towards the end of this NSC 
project, which contain up to 8 physical cores and are designed to be employed in dual CPU 
systems (allowing a total of 16 physical cores), it seems only natural that the AVX registers 
present in each core be used to their maximum potential for application to CFD research. 
In order to fairly compare the performance of a GPU enabled system containing a dual-
CPU configuration, or to know if the increase in performance of a hybrid CPU/GPU 
implementation is worth the cost of communication between the GPU and CPU, the 
optimal and complete performance of a dual CPU system must be evaluated. Hence, we 
believe it was required to apply the QDS method to AVX acceleration to better (i.e. fairer) 
estimate the relative increase in performance obtained by using GPU devices.  
 
The application of OpenMP and AVX for parallelization of the QDS algorithm is 
relatively straightforward. A one dimensional data structure containing N elements holding 
our problem variables is shown in Figure 1. Each OpenMP thread will manage (N/P) 
elements – where P is the number of threads – is further subdivided into groups of 8 floating 
point variables, giving N_AVX = (0.125N/P) groups in total. Looping over these elements 
using AVX for the computation of the conserved quantities U from primitive values P is 
also shown in Figure 1, together with its conventional (un-optimized) equivalent.  The key 
to high performance is the exclusive use (where possible) of AVX intrinsic functions and 
their associated variables within the computation kernel.  Each variable employed in 
AVX_STATE_P2U contains floating point variables representing 8 values from the 
physical flow solver. Uncoalesced memory access for the state and flux function (not 
shown) is not problematic due to the high locality of the solver. Hence, retrieving 
information from outside the AVX or OpenMP boundary is also not a concern. For the 
computation of the new state (following computation of fluxes), a hybrid approach is 
employed with loops over N/P elements (for each OpenMP) containing sub-loops for 
optimized AVX use. 
 
 
 
 
 
 
 
 
 
 
 
 
  /* Fork a team of threads giving them their own copies of variables */ 
  #pragma omp parallel private(…) shared (…) { 
  /* Obtain thread number */ 
  tid = omp_get_thread_num(); 
  // Set the index for access to different parts of memory 
  index = tid*N_SSE; 
  p0 = (__m256*) s_p0 + index; // Assign desired value (i.e. density) 
  … 
  Fp0 = (__m256*) s_Fp0 + index;  // Assign forward / backward fluxes 
  … 
  i = 0; 
  while (i < CYCLES) { 
 
   SSE_FLUX(…); 
   SSE_TIMESTEP(…);  
   SSE_STATE_U2P(…);  
   #pragma omp barrier 
   i++; 
  } 
}  /* All threads join master thread and disband */ 
 
Figure: Incomplete (semi pseudo code) of main function showing OpenMP 
implementation and the mapping of AVX registers.  
 771: c5 f0 14 c9            vunpcklps %xmm1,%xmm1,%xmm1  
 775: c5 f8 5a c9            vcvtps2pd %xmm1,%xmm1  
 779: c5 f3 59 ce            vmulsd %xmm6,%xmm1,%xmm1  
 77d: c5 d3 58 c9           vaddsd %xmm1,%xmm5,%xmm1  
 781: c5 db 59 c9           vmulsd %xmm1,%xmm4,%xmm1  
 785: c5 fb 12 c9            vmovddup %xmm1,%xmm1  
 789: c5 f9 5a c9            vcvtpd2ps %xmm1,%xmm1 
 
 a1f: c5 fc 5e 04 07        vdivps (%rdi,%rax,1),%ymm0,%ymm0  
 a24: c5 fc 29 04 02        vmovaps %ymm0,(%rdx,%rax,1)  
 a29: c4 c1 7c 28 04 04     vmovaps (%r12,%rax,1),%ymm0  
 a2f: c5 fc 5e 04 07        vdivps (%rdi,%rax,1),%ymm0,%ymm0  
 a34: c5 fc 29 04 01        vmovaps %ymm0,(%rcx,%rax,1)  
 a39: c5 fc 59 c0            vmulps %ymm0,%ymm0,%ymm0  
 a3d: c5 fc 28 0c 06        vmovaps (%rsi,%rax,1),%ymm1  
 a42: c5 f4 59 d9            vmulps %ymm1,%ymm1,%ymm3 
 
Figure: Incomplete listing of assembly code for [Top] the conventional state function 
using –O3 optimization, and [Bottom] the OpenMP/AVX state  function also using –O3 
optimization.  
 
The assembly code produced following compilation was produced using g++ (Version 
4.4.3, 64 bit build) and is shown in the Figure above. Each of the codes compiled were 
contained within the same file and compiled simultaneously using –O3 optimization. The 
computation was performed on a modest problem set (803 ~ 0.51million cells) for 1000 
time steps. The assembly code shown indicates that the g++ compiler has only employed 
SSE intrinsic functions in its optimization (note the xmm registers used). However, the g++ 
  
 
 
 
Figure: The real-time gridding software [left] and solver [right] as presented at Nvidia’s 
GTC in March 2013 at San Jose, USA as part of ongoing industrial collaboration with Acer 
and Velocite Tech. The solver is capable of returning the real-time drag coefficient for the 
imported geometry as required by our collaborators. 
 
 
 
 
, ,ghost
, ,ghost
    
  
    
image ghost
n image n
t image t
image ghost
V V
V V
T T
 
 


  
      
 
The figures below illustrate the sketch of cubic-spline IBM near a curved solid surface. 
The procedures of implementing the IBM are described in the following in turn. Firstly, 
we define a cubic-spline function for each segment between any two consecutive solid-
boundary points to best fit the solid boundary geometry through the interpolation of four 
surface geometry data points (two points each prior to and after the segment), as shown 
below. Secondly, we use the cubic-spline function to identify all the solid cells, fluid cells, 
boundary cells and ghost cells. Note in the IBM ghost cells are considered to be the 
“boundary solid cells” mathematically. Thirdly, we locate each image point corresponding 
to the cell center of each ghost cell for all solid boundary cells using the cubic-spline 
function wherever it is appropriate. Fourthly, we obtain the data at each image point 
through the data of fluid cells from previous time step around it. Finally, we obtain the data 
at each ghost cell through the enforcement of boundary conditions.  
 
 
 
Figure: Illustration of the relation between the ghost points and the image points. 
 
  
Figure: Instantaneous density contour resulting from the interaction of the moving shock 
wave with a speed of M=1.5 and a wedge in air. (43 equal spaced intervals from 0.26 to 
2.38) 
 
Additional Outcomes – PhD graduates 
 
The following PhD students have graduated based on work and results which have 
resulted directly from the outcomes of this project. These students successfully defended 
their thesis prior to the completion of this project in July 2013 and are currently involved 
in post-doctoral studies. 
 
Student: Lin YaJu – Thesis / Research topic: True Directional, General higher order 
extensions of the QDS Method. 
Student: Su, Cheng-Chin – Thesis / Research topic: Application of a hybrid GPU / MPI 
approach to the acceleration of the DSMC method. 
 
Unexpected Difficulties and Resolutions 
 
The final year was particularly challenging with regard to the completion of this project. 
There were several unforeseen challenges which were not disclosed in the initial NSC 
application which warrant some discussion: 
 
1. My status as a foreigner in Taiwan results in multiple administrative difficulties. 
The difference between my ID and a local ID results in the forced use of my 
passport number for most administrative procedures. As such, most applications 
are treated as being made by a non-local – this results in restrictions in access to 
NSC International Travel Report 
 
Project Code: NSC 99-2221-E-492 -005 -MY3  
 
Attendees: Prof. Matthew Smith 
        NSC Project PI, Department of Mechanical Engineering, NCKU. 
       Mr. Chen Yen-Chih 
       Post-graduate (Masters) student, Dep. ME, NCKU. 
       Mr. Liu Ji-Yeh 
       Post-graduate (Masters) student, Dep. ME, NCKU. 
 
Conference Title: 25th International Conference on Parallel Computational Fluid 
Dynamics (ParCFD) 
Conference Location/Date: Chang-Sha (Hunan Province), China, 20th May to 24th May, 
2013.  
 
Conference Summary:  
 
The series of international conference on Parallel CFD, otherwise known as ParCFD, 
represent the forefront of parallel computing applied to Computational Fluid Dynamics 
(CFD). Invited and accepted presentations and papers presented at ParCFD represent the 
cutting edge of both parallel computation and CFD technologies, making it an ideal 
location to present the findings of this work. The location for the conference cycles 
between (i) Asia, (ii) North America, and (iii) Europe on a 3 year period. This year, the 
conference was held in Asia – in Chang-Sha, making it an ideal opportunity to take 
several graduate students due to the lower travel and accommodation expenses. This year, 
over 200 papers were submitted to the conference with under 100 of these accepted for 
presentation and only 40 papers accepted for printing in proceedings. 
 
Website: http://aca.hnu.cn/parcfd2013/  ,   http://parcfd.org/ (General webpage) 
 
Details of Involvement at the Conference 
 
Research Papers and Presentation 
 
This year, we presented two papers and gave two talks as part of the ParCFD 
proceedings:  
 
M.R. Smith, Y.-C. Chen, J.-Y. Liu, A. Ferguson, and J.-S. Wu, Extension of the Uniform 
Equilibrium Flux Method (UEFM) to Second Order Accuracy and its Graphics 
Processing Unit Acceleration, 25th International Conference on Parallel Computational 
Fluid Dynamics, ChangSha, China, May 20-25, 2013. 
M.R. Smith, J.-Y., Liu, F.-A. Kuo, and J.-S. Wu, Hybrid OpenMP/AVX Acceleration of 
a Higher Order Quiet Direct Simulation Method for the Euler Equations, 25th 
International Conference on Parallel Computational Fluid Dynamics, ChangSha, China, 
May 20-25, 2013. 
Additional Honors and Invitations 
 
In addition to delivering two talks, the international advisory committee – specifically, 
the founder of the conference series (Dr. Akin Ecer) invited me to join the Panel of 
Experts for an open panel discussion which was held on the third day of the conference. 
The panel consisted of 5 members – one from each major continent, of which I was 
selected to represent the Asian continent.  
 
Outcomes of the Conference 
 
1)  Following conclusion of the meeting of the international advisory committee for the 
ParCFD series of conferences, the existing advisory committee formally invited me to 
join them as a fellow international advisee. 
 
2) Following the conclusion of the conference, the advisory committee, together with 
Computers and Fluids, elected me as a Guest Editor for the upcoming special edition 
(journal) of Computers and Fluids. 
 
 
=============== END OF REPORT =================== 
99年度專題研究計畫研究成果彙整表 
計畫主持人：李汶樺 計畫編號：99-2221-E-006-258-MY3 
計畫名稱：利用動力學理論及大型混合式 MPI/GPU 平行計算架構進行紊流流場模擬研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 200% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 2 150% 
Readjustment of 
funds were 
required to make 
this work. 
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 3 133%  
研究報告/技術報告 0 0 100%  
研討會論文 2 1 200% 
篇 
Presented two 
papers at 
Parallel CFD 
toward the end of 
the project. 
Papers were 
selected for 
following SCI 
publication 
(pending). 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
國外 
參與計畫人力 
（外國籍） 博士生 0 0 100% 
人次 
 
