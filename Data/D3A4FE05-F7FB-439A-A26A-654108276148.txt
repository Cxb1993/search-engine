第一年:嬰兒臉部表情辨識之研究 
AN INFANT FACIAL EXPRESSION RECOGNITION SYSTEM 
BASED ON MOMENT FEATURE EXTRACTION 
Abstract 
This paper presents a vision-based infant surveillance system utilizing infant facial 
expression recognition software. In this study, the video camera is set above the crib 
to capture the infant expression sequences, which are then sent to the surveillance 
system. The infant face region is segmented based on the skin colour information. 
Three types of moments, namely Hu, R, and Zernike are then calculated based on 
the information available from the infant face regions. Since each type of moment 
in turn contains several different moments, given a single fifteen-frame sequence, 
the correlation coefficients between two moments of the same type can form the 
attribute vector of facial expressions. Fifteen infant facial expression classes have 
been defined in this study. Three decision trees corresponding to each type of 
moment have been constructed in order to classify these facial expressions. The 
experimental results show that the proposed method is robust and efficient. The 
properties of the different types of  moments have also been analyzed and 
discussed.  
Keywords: Facial expression recognition, Decision tree, Moment, Correlation 
coefficient. 
1 INTRODUCTION 
Infants are too weak to protect themselves and lack disposing capacity, and 
therefore are more likely to sustain unintentional injuries especially when compared 
to children of other age groups. These incidents are very dangerous and can 
potentially lead to disabilities and in some cases even death. In  Taiwan‟s Taipei 
city, the top three causes of infant death are (1) newborns affected by maternal 
complications during pregnancy, (2) congenital anomalies, and (3) unintentional 
injuries, which in total account for 83% of all infant mortalities (Doi, 2006). 
Unintentional injuries are a major cause of infant deaths each year, a majority of 
which can be easily avoided. Some of the most common causes include dangerous 
objects surround the infant and unhealthy sleeping environments. Therefore, the 
promotion of safer homes and better sleeping environments is critical to reducing 
infant mortality caused by unintentional injuries. 
Vision-based surveillance systems, which take advantage of camera technology 
to improve safety, have been used for infant care (Doi, 2006). The main goal behind 
(a)                    (b) 
Figure 1: A video camera set above the crib. 
expressions. Three decision trees, which correspond to each different type of 
moment, are used to classify the infant facial expressions. 
Five infant facial expressions, including crying, gazing, laughing, yawning and 
vomiting have been classified in this study. Different positions of the infant head 
namely front, turn left and turn right have also been considered. Thus, a total of 
fifteen classes have been identified. 
3 INFANT FACE DETECTION 
Three color components from different color models have been used to detect 
infant skin colour. They are the S component from the HSI model, the Cb 
component from the YCrCb model and a modified U component from the LUX 
model. Given a pixel whose colour is represented by (r, g, b) in the RGB color 
model, its corresponding transfer functions in terms of the above components are: 
),,min(
)(
3
1 bgr
bgr
S

  (1) 
bgrCb 5.03313.01687.0   (2) 






otherwise.255
, and 5.1 if 256 0gr
g
r
r
g
U  (3) 
The ranges of the infant skin colour are defined as S = [5, 35], Cb = [110, 133] 
and U = [0, 252]. These ranges have been obtained from experimental results. 
Figure 3 (b) shows the skin color detection results of the input image in Figure 3 (a). 
Figure 3 (c) shows the result after noise reduction and image binarization. Here, a 
10x10 median filter has been used to reduce the noise and the largest connected 
component has been selected as the face region (Figure 3 (d)). 
4 FEATURE EXTRACTION 
In this section, we will briefly explain the different types of moments. Given an 
image I, let f represent an image function. For each pair of non-negative integers (p, 
q), the digital (p, q)
th
 moment of I is given by 



Iyx
qp
pq yxfyxIm
),(
),( )(  (4) 
Let 
00
10
0
m
m
x   and 
00
01
0
m
m
y  . Then the central (p, q)
th
 moments of I can be defined as  
),()()( )(
),(
00 yxfyyxxI
Iyx
qp
pq 

  (5) 
(a)         (b)       (c)        (d) 
Figure 3 Infant face detection. 
Z99 respectively. 
5 CORRELATION COEFFICIENTS 
Given a video sequence I = (I1, I2, …., In) which describes an infant facial 
expression, the system can calculate one type of moment for each particular frame. 
Suppose there are m moments, then the system can obtain m ordered sequences A
i
 = 
},...,,{
21 niIiIiI
AAA , i = 1, 2,…, m, where 
kiI
A  indicates the ith moment Ai of the frame 
Ik for k = 1, 2,…, n. Now the variances of the elements in each sequence A
i
 can be 
calculated by 





n
k
iiI AA
n
S
k
i
1
22 )(
1
1
A
 (12) 
where iA  is the mean of the elements in A
i
, and the covariance between A
i
 and A
j
 
is given by 





n
k
jjIiiI AAAA
n
S
kk
ji
1
)])([(
1
1
AA
 (13) 
Therefore, the correlation coefficients between A
i
 and A
j
 can be defined as 
ji
ji
ji
SS
S
r
AA
AA
AA
  (14) 
Moreover,
ijji
rr
AAAA
 , 1iir AA , for i, j = 1, 2,…, m. For example, since seven 
Hu moments have been defined, we can obtain a total of 21 beneficial correlation 
coefficients. Figure 4 shows a video sequence of an infant crying with fifteen 
frames. The twenty-one correlation coefficients between the seven ordering 
sequences are shown in Table 1. 
Table 1: The correlation coefficients between the seven Hu moment sequences. 
 
H
2
 H
3
 H
4
 H
5
 H
6
 H
7
 
H
1
 0.1222 0.2588 0.8795 -0.4564 -0.4431 -0.9140 
H
2
 -- -0.8272 -0.1537 0.6927 -0.1960 0.0573 
H
3
  -- 0.4458 -0.9237 0.2070 -0.3432 
H
4
   -- -0.6798 -0.2366 -0.9800 
H
5
    -- -0.1960 0.5663 
H
6
     -- 0.3218 
Similarly, we can calculate the correlation coefficients between every two R 
moments and every two Zernike moments. We believe that the correlation 
Figure 4: A video sequence of an infant crying. 
)(minarg)( 
,
**
SESr
ji
ji r
ji AAAA
  (17) 
It is to be noted that once a correlation coefficient has been selected, it cannot be 
selected again by its descendants. 
The algorithm to construct a binary classification tree is shown here: 
Algorithm: Decision tree construction 
Step 1: Initially, put all the training instances into root SR Regard SR as an internal 
decision node and input SR into a decision node queue. 
Step 2: Select an internal decision node S from the decision node queue. 
Calculate the entropy of node S using Eq. 15. If the entropy of node S is larger 
than a threshold Ts, proceed to Step 3, 
otherwise label node S as a leaf node and  proceed to Step 4. 
Step 3: Find the best correlation coefficient 
**
 jir AA
 to split the training instances 
in node S using Eqs. 16 and 17. Split the training instances in S into two nodes 
S1 and S2 using the correlation coefficients ** jir AA  and then subsequently add 
S1 and S2 into the decision node queue. . 
Step 4: If the queue is not empty, return to Step 2, otherwise stop the algorithm. 
7 EXPERIMENTAL RESULTS 
The input data for our system was acquired using a SONY TRV-900 video camera 
mounted above the crib and processed on a PC with an Intel
R
Core™ 21.86GHz 
CPU. The input video sequences recorded at a rate of 30 frames/second were 
down-sampled to a rate of six frames/second, which is the processing speed of our 
current system. In order to increase the processing rate, we further reduced the size 
(640 x 480 pixels) of each image to 320 x 240 pixels. 
Five infant facial expressions, including crying, dazing, laughing, yawning and 
vomiting have been classified in this study. Three different poses of the infant head, 
including front, left, and right (an example of an infant yawning as shown from the 
three positions is shown in Figure 4) have been considered and a total of fifteen 
classes have been identified. 
In the first experiment, the Hu moments and their correlation coefficients were 
calculated using Eqs. 7 and 14. A corresponding decision tree was constructed using 
the decision tree construction algorithm. Figure 7 shows the decision tree 
constructed using the correlation coefficients between the Hu moments as the split 
function. Node HS1  is the root, and the split function of 
HS1  is 054 HHr . Nodes 
HS2  and 
HS9  are the left and right branches of 
HS1  respectively. The left subtree 
Figure 6:The decision tree of the Hu moments. 
yes 
no 
054 HHr
HS1
HS2
HS9
Table 2: The experimental results. 
 (1) (2) (3) (4) (5) 
Hu  
Moments 
59 16+17 8 30 90% 
R  
Moments 
59 15+17 10 30 80% 
Zernike  
Moments 
59 19+20 7 30 87% 
PS. (1) Number of training sequences 
 (2) Number of nodes (internal node + leaf) 
 (3) Height of the decision tree 
 (4) Number of testing sequences 
 (5) Classification Rate 
Moreover, the Zernike moments and their correlation coefficients are calculated 
using Eqs. 9 and 14. The decision tree created based on the correlation coefficients 
of the Zernike moments includes nineteen internal nodes and twenty leaves, with a 
height of seven. 
Table 2 also shows the classification results of the same thirty testing sequences. 
We observe that the correlation coefficients of the moments are useful attributes for 
classifying infant facial expressions. Moreover, the classification tree created from 
the Hu moments has a smaller height and a fewer number of nodes but a higher 
classification rate.  
8 CONCLUSION 
This paper presented an infant facial expression recognition technique for a 
Figure 7: The left subtree of the decision tree depicted in Figure 
6. 
053 HHr
HS2
yes no 
076 HHr
HS3
061 HHr
HS6
yes no 
063 HHr
HS4
 
vomiting 
yes 
no 
063 HHr
HS5
 vomiting 
yes no 
 
laughing vomiting 
yes no 
051 HHr
HS7
 
yawning 
yes no 
031 HHr
HS8
 
crying 
yes no 
 
laughing crying 
第二年:嬰兒肢體動作分析之研究 
A Monitoring System Based on Behavior Analysis 
 
ABSTRACT 
This paper presents a vision-based infant-monitoring system that adopts an infant 
behavior analysis approach to reduce infant injuries. In our study, the video camera 
is set above the crib to capture infant sequences. The system first preprocesses the 
input sequence to filter out the noise and reduce the effects of lights and shadows. 
Then, the infant‟s head and limbs are detected from the input frames and compared 
with pre-defined posture maps to select the most similar map. A posture map 
describes the current infant posture; the selected posture map can be regarded as a 
node to be linked by the occurrence order to construct a dynamic behavior graph 
that describes infant behaviour captured over time. If an input posture map does not 
exist in the dynamic behavior graph, this means that an unexpected situation has 
occurred and the system would then alert the baby sitter to attend to the infant 
immediately. A weighted dynamic behavior graph adjustment algorithm is used to 
accomplish the behavior analysis. Since infants grow very quickly and their growth 
processes may differ, the dynamic behavior graph should be continuously updated 
to fit the current behavior of infants. The experimental results show that the 
proposed method is able to perform robustly in real-time. 
 
Keyword: Vision-based infant monitoring system, posture maps, dynamic behavior 
graphs, behavior analysis 
 
the infant (from head to foot), as shown in Figure 1. Although the video camera is 
set indoors and the light sources are fixed, some problems should still be considered 
and resolved in the vision-based monitoring system. For example, the motion of the 
baby sitter will cast moving shadows on the crib or even on the baby‟s face, and 
would affect the intensity values of the input images. In addition, since infants grow 
up quickly, the dynamic behavior graphs used to describe their behavior should be 
updated progressively. Infants also grow up at different rates and thus, each infant 
should have his/her own dynamic behavior graph. 
   
Figure 1: The input frames of the infants. 
2. System Flowchart 
The flowchart of the proposed infant monitoring system is shown in Figure 2. 
Initially, once a video sequence has been input into the system, a detection 
technique is applied to the input sequence to detect motions after the preprocessing 
stage. If no motion has been detected, then the system detects the skin color regions 
from the frames of the input sequence. If the system does not detect any skin color  
 
Figure 2: The flowchart of the infant monitoring system. 
largest foreground component as the infant. The result is shown as the upper image 
in Figure 3(c). Using the predefined skin color range, the system extracts the skin 
color regions inside the largest foreground component to avoid the noise introduced 
by the background. The skin color regions inside the largest foreground component 
are shown in Figure 3(d). Moreover, the morphological erosion and dilation is 
applied to Figure 3(d) to obtain more complete skin color regions as shown in 
Figure 3(e). 
 
(a)           (b)          (c)          (d)           (e) 
Figure 3: An example of the preprocessing stage. (a) The upper one is the 
background image, and the lower one is the input frame (b) The foreground image 
(c) The upper one shows the largest foreground component, and the lower one is the 
input frame (d) The skin color regions of the largest foreground component (e)The 
skin color regions after the application of the morphological technique. 
4. Infant Posture Mapping 
4.1 The Classes of Infant Posture Maps 
In this study, we comprehensively define 203 infant posture maps to describe 
various infant postures, including three main classes. The first class contains 31 
normal and complete posture maps, where all five skin color regions of the infant 
head and limbs have been identified. These skin color regions may be occluded by 
others. Some examples of these posture maps and their corresponding infant 
postures are shown in Figure 4. The yellow node label H in the map represents the 
relative position of the infant head region. The orange nodes having a dotted line 
circle with labels R and L indicate the relative positions of the left and right hands, 
respectively. Similarly, the green nodes with labels R and L indicate the relative 
positions of the left and right legs, respectively. The first map in Figure 4 is called 
the initial posture map since the initialization of the system will be successful when 
color regions are extracted first and these skin color regions can be identified as the 
infant‟s head and limbs initially. The regions initially identified will be continuously 
processed until the five skin color regions have been detected separately and 
correctly. Its corresponding posture map is the initial posture map shown in Figure 
4. Subsequently, the system tracks these regions and selects the next corresponding 
posture map from each frame. The flowchart of region tracking is shown in Figure 
7. 
 
Figure 7: The flowchart depicting region tracking. 
The center position and area of the region are defined as the features of the 
regions here. In Figure 7, once the features of a region Rt at time t are input into the 
system, its nearest region Rt-1 at time t-1 can be found. If these two regions overlap 
each other and their center positions and region sizes are close enough, then the 
feature values of region Rt-1 will be replaced by those of region Rt. If these two 
regions do not overlap or their center positions are not close enough, then the 
system retains the feature values of region Rt-1 in the database. Conversely, the 
feature values of region Rt are used to decide whether or not the regions occlude 
with each other.  
Figure 8 shows an example of region tracking. In Figure 8, the skin color 
regions extracted from the frames at time t-1 and time t, respectively, are shown in 
the first row, and they are overlapped and compared to trace the skin color regions. 
Here, we can observe that the regions obtained from the frame at time t-1 are shown 
distances are shown in Figure 10(b). The contour segment that has the maximum 
segment distance is defined as the position of the head. 
0
20
40
60
80
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73 76 79
Frame 400 Frame 405 Frame 410
 
(a)                      (b) 
Figure 10: An example of infant head position decision/identification. 
5. The Dynamic Behavior Graph 
The dynamic behavior graphs in the database are used to represent dangerous 
or safe behavior exhibited by the infant. We believe that each infant should have 
his/her own dynamic behavior graph because infants grow up at different rates and 
they also have different orders of motion behavior. The dynamic behavior graph 
contains nodes and weighted arcs. Each node corresponds to a posture map, and the 
arcs between the nodes indicate a path that the infant can change his/her posture. 
The weight associated with a link indicates transition probabilities, whereby the 
infant changes his/her posture from one posture to another. 
 
(a)        (b) 
Figure 11: An example of the dynamic behavior graph.  
Figure 11 shows a simple example of the dynamic behavior graph. This graph 
contains two nodes, whose corresponding maps are map 31 and map 3 respectively, 
and four links. The corresponding maps of the nodes are shown in Figure 11(a) and 
(b) respectively. In this example, we can observe the transition probability that the 
infant changes his/her posture from map 31 to map 3 is 0.125, and the transition 
probability that the infant stays at map 31 is 0.875. Similarly, the transition 
probability that the infant changes his/her posture from map 3 to map 31 is 0.167, 
and the transition probability that the infant would stay at map 31 is 0.833. This 
Seg. no. 
distance
w 
the infant head position location is used to correct the posture map mapping. 
 
 
Figure 12: The flowchart of infant behavior analysis. 
7. Experimental Results 
The input data for our system was acquired using a video camera mounted 
above the crib and processed on a PC with an IntelRCore™ 21.86GHz CPU. The 
input video sequences were recorded at a rate of 30 frames/second. 
 
map 31 map 28 map 31 map 28 map 31 map 31 
Figure 13: A two-month infant test. 
 
Figure 14: An example dynamic behavior graph. 
behavior graph of this four-month old infant. In this example, 12 maps including 
maps 31 and 28 are selected to construct the dynamic behavior graph. The transition 
probabilities are also shown in Figure 16. It can be observed that the dynamic 
behavior graph of a four-month old infant is more complex than that of a two-month 
old infant. If more time is taken to construct the dynamic behavior graph, we 
believe that the nodes and the transition probabilities may become more stable. 
However, once the infant grows up, the nodes and the transition probabilities of the 
dynamic behavior graph will be updated progressively. 
 
Figure 17: The dynamic behavior graph of a six-month old infant. 
Figure 17 shows the dynamic behavior graph of a six-month old infant. In this 
example, 16 maps are selected to construct the dynamic behavior graph. The 
transition probabilities are also shown in Figure 17. Notice that three nodes (maps 0, 
32, and 33), which belong to the second class of the infant posture maps, are 
marked using the pink color in this graph. If these nodes identify the infant‟s motion, 
it indicates that the infant may be outside the field of view of the camera. These 
situations are dangerous, thus the system will output the warning message to the 
baby sitter.  
One example depicting the infant moving out of the field of view of the 
camera is shown in Figure 18. The length of the input sequence is two minutes, and 
only 6 frames of the sequence and their corresponding maps are shown in Figure 18. 
In this example, the system will output the warning message once map 32 has been 
selected.  
of China, Taiwan for financially supporting this research under Contract No. 
NSC-98-2221-E-003-014-MY2 and NSC 98-2631-S-003-002. 
References 
Bobick, A. and Davis, J., The Recognition of Human Movement Using Temporal 
Templates, IEEE Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 23, pp.257 – 267, 2001. 
Juang, C. F., Chang, C. M., Wu, J. R., and Lee, D., Computer Vision-Based Human 
Body Segmentation and Posture Estimation, IEEE Transactions on 
Systems, Man and Cybernetics, Vol. 39, pp.119-133, 2009. 
Juang, C. F. and Chang, C. M., Human Body Posture Classification by a Neural 
Fuzzy Network and Home Care System Application, IEEE Transactions 
on Systems, Man and Cybernetics, Vol. 37, pp.984-994, 2007. 
Kirishima, T., Sato, K., and Chihara, K., Real-time Posture Recognition by 
Learning and Selective Control of Visual Interest Points, IEEE 
Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, 
pp.351 – 364, 2005. 
Kosta, G. and Benoit, M., Group Behavior Recognition for Posture Analysis, IEEE 
Transactions on Circuits and Systems for Video Technology, Vol. 18, 
pp.211-222, 2008. 
Li, X., Yan, S., Tao, D., and Xu, D., Gait Components and Their Application to 
Gender Recognition, IEEE Transactions on Systems, Man, and 
Cybernetics, Part C: Applications and Reviews, Vol. 38, pp.145-155, 
2007. 
 
 -2- 
一、 參加會議經過 
    禮拜五(5/14 日)深夜搭乘十一時五十五分長榮航空直飛法國巴黎戴高樂機場的飛
機，再搭乘法國 SNCF 的 TGV 火車約一個
半小時可抵逹昂熱市，即本國際研討會的舉
辦地點。 
    本研討會 VISIGRAPP2010 是一個聯合
研討會，包含下列四個子研討會、一個
workshop、以及一個一個 special session. 
 VISAPP-International Conference on 
Computer Vision Theory and Applications  
 IMAGAPP-International Conference on 
Imaging Theory and Applications  
 GRAPP-International Conference on 
Computer Graphics Theory and 
Applications 
 -4- 
    本論文主要探討以嬰兒表情為基礎的監控系統。由於嬰兒無法保護自己，若照顧
者有疏忽可能讓嬰兒處於危險中。因此我們希望研發以嬰兒表情為基礎的監控系統來
協助照顧者監控嬰兒，即使照顧者離開嬰兒身邊，也可防止意外的發生。 
    本研究將攝影機架設在嬰兒床上方以擷取嬰兒影像。此系統首先針對影像去除雜
訊及減少受到光源的影響。藉由膚色的資訊來做嬰兒臉部區塊的擷取。接著利用 Hu
動差、R 動差和 Z 動差去計算臉部區塊。由於每種動差包含許多不同動差，例如 Hu
動差有七個動差，因此給十五張影像去計算相同類別下臉部表情的特徵，並且藉此了
解動差間的關係。本研究將嬰兒表情分成十五個類別，分別是哭、笑、發呆…等，接
著再利用決策樹做分類。利用動差所計算出的相關係數所建構的三個決策樹來進行分
類分別是用來。實驗的結果顯示本研究所提出的方法可行，而且也針對不同種類的動
差進行分析及討論。 
    本篇論文的發表頗受歡迎，並引起與會者的討論，是一個大家感興趣的新的研究
主題。討論期間，更有其他研究者也想要開始進行相關的研究主題，並希望能分享我
們所收集的嬰兒表情資料庫。研討會結束後，我們在禮拜五下午搭 TGV 火車回巴黎
第二天一早從戴高樂機場搭乘長榮航空直飛台灣。 
二、與會心得 
    本次 VISIGRAPP2010 是聯合研討會在法國昂
熱市舉辦，昂熱市雖然是一個小城，但與巴黎只有
一個半小時的火車車程，且有 TGV 直達該市，交
通尚稱便利。昂熱市的巴士路線很多，一路巴士可
直達昂熱大學內的 ISTIA 會議地點，主辦單位還熱
心提供每人八張巴士票供與會者使用，非常貼心。
由於昂熱大學內用餐不便，故午餐皆在大學內部的
餐廰進行，這也給了與會者另一個互相交流的機會。 
 -6- 
Dear Dr. Chiung-Yao Fang, 
 
We are happy to inform you that the regular paper you have submitted to VISAPP, with 
number 22, entitled 'AN INFANT FACIAL EXPRESSION RECOGNITION SYSTEM 
BASED ON MOMENT FEATURE EXTRACTION', has been accepted for a 20 minutes 
oral presentation (Short Paper).  
  
All reviews performed by the program committee are now available at the PRIMORIS 
Author's Home (www.insticc.org/Primoris/). Please login and then click on Author’s 
home / Paper Reviews, to access the reviews. 
 
It is very important that you try to follow the suggestions indicated in the reviews during 
the preparation of the camera-ready manuscript.  
Furthermore, it is EXTREMELY important that you follow the camera-ready paper 
format and preparation guidelines for the proceedings, which are available at the VISAPP 
web site http://www.visapp.visigrapp.org//Paper_Templates.htm. 
 
Any non-conformance with the specified format may force the proceedings editing team 
to return the paper to you for re-formatting, and in case of repeated problems it may 
prevent the paper from being published altogether.  
 
Please note that the publication of any paper in the conference proceedings requires that 
at least one of the authors is registered until next February 10th, 2010. 
 
 
Should you have any question regarding the camera ready submission and registration, 
please don't hesitate to contact the secretariat. 
 
Best regards, 
Bruno  Encarnação 
VISAPP Secretariat 
   
 -8- 
system for infant safety surveillance. 
Many facial expression recognition methods 
have been proposed recently. However, most of 
them focus on recognizing facial expressions of 
adults. Compared to an adult, the exact pose and 
position of the infant head is difficult to accurately 
locate or estimate and therefore, very few infant 
facial expression recognition methods have been 
proposed to date. Pal et al. (Pal, 2006) used the 
position of the eyebrows, eyes, and mouth to 
estimate the individual motions in order to classify 
infant facial expressions. The various classes of 
facial expressions include anger, pain, sadness, 
hunger, and fear. The features they used are the 
local ones. However, we believe that global 
moments (Zhi, 2008) are more suitable for use in 
infant facial expression recognition systems. 
2 SYSTEM FLOWCHART 
The data input to the system consists of video 
sequences, which have been acquired by a video 
camera set above the crib as shown in Figure 1(a). 
An example image taken by the video camera is 
shown in Figure 1 (b). 
Figure 2 shows the flowchart of the infant 
facial expression recognition system. The system 
first pre-processes the input image to remove any 
noise and to reduce the effects of lights and 
shadows. The infant face region is then segmented 
based on the skin colour information and then the 
moment features are extracted from the face region. 
This study extracts three types of moments as 
features, including seven Hu moments, ten R 
moments, and eight Zernike moments.  
For each fifteen-frame sequence, the 
correlation coefficients between two moments 
(features) of the same type are calculated as the 
attribute of infant facial expressions. These 
coefficients aid in the proper classification of the 
facial expressions. Three decision trees, which 
correspond to each different type of moment, are 
used to classify the infant facial expressions. 
Five infant facial expressions, including crying, 
gazing, laughing, yawning and vomiting have been 
classified in this study. Different positions of the 
infant head namely front, turn left and turn right 
have also been considered. Thus, a total of fifteen 
classes have been identified. 
3 INFANT FACE DETECTION 
Three color components from different color 
models have been used to detect infant skin colour. 
They are the S component from the HSI model, the 
Cb component from the YCrCb model and a 
modified U component from the LUX model. 
Given a pixel whose colour is represented by (r, g, 
b) in the RGB color model, its corresponding 
transfer functions in terms of the above 
components are: 
),,min(
)(
3
1 bgr
bgr
S

  (1) 
bgrCb 5.03313.01687.0   (2) 






otherwise.255
, and 5.1 if 256 0gr
g
r
r
g
U  (3) 
The ranges of the infant skin colour are defined 
as S = [5, 35], Cb = [110, 133] and U = [0, 252]. 
These ranges have been obtained from 
experimental results. Figure 3 (b) shows the skin 
color detection results of the input image in Figure 
3 (a). Figure 3 (c) shows the result after noise 
reduction and image binarization. Here, a 10x10 
median filter has been used to reduce the noise and 
the largest connected component has been selected 
as the face region (Figure 3 (d)). 
4 FEATURE EXTRACTION 
In this section, we will briefly explain the different 
types of moments. Given an image I, let f represent 
an image function. For each pair of non-negative 
integers (p, q), the digital (p, q)
th
 moment of I is 
given by 



Iyx
qp
pq yxfyxIm
),(
),( )(  (4) 
Let 
00
10
0
m
m
x   and 
00
01
0
m
m
y  . Then the central (p, 
q)
th
 moments of I can be defined as  
),()()( )(
),(
00 yxfyyxxI
Iyx
qp
pq 

  (5) 
Hu (Hu, 1962) defined the normalized central 
moments of I to be  



00
pq
pq   where 1
2



qp
  (6) 
Infant Face Detection Feature Extraction
Classification
Feature Correlation
Calculation
Image Sequences
Figure 2: Flowchart of the proposed system. 
(a)         (b)       (c)        (d) 
Figure 3 Infant face detection. 
 -10- 
H
4
   -- -0.6798 -0.2366 -0.9800 
H
5
    -- -0.1960 0.5663 
H
6
     -- 0.3218 
Similarly, we can calculate the correlation 
coefficients between every two R moments and 
every two Zernike moments. We believe that the 
correlation coefficients describe the relationship 
between these moments, which vary depending on 
for different facial expressions. Therefore, these 
coefficients can provide important information, 
which can be in turn used to classify the different 
infant facial expressions.  
6 CLASSIFICATION TREES  
In this study, a decision tree [8], which implements 
the divide-and-conquer strategy, has been used to 
classify the infant facial expressions. A decision 
tree is a hierarchical model used for supervised 
learning and is composed of various internal 
decision nodes and terminal leaves. Each decision 
node implements a split function with discrete 
outcomes labeling the branches. The advantages of 
the decision tree are (1) it can perform a quick 
search of the class of the input features and (2) it 
can be easily understood and interpreted by mere 
observation. In this study, we have constructed 
three binary classification trees corresponding to 
the three different types of moments.  
Suppose K infant facial expressions are to be 
classified, namely, Ci where i = 1,…, K. Given a 
decision node S, let NS indicate the number of 
training instances reaching the node S and 
i
SN  
indicate the number of NS belonging to the class Ci. 
It is apparent that S
K
i
i
S NN  1 . The impurity 
measure applied in this study is an entropy 
function given by 



K
h S
h
S
S
h
S
N
N
N
N
SE
1
2log)(  (15) 
where 00log0  . The range of this entropy 
function is [0, 1]. If the entropy function is zero, 
then node S is pure. It means that all the training 
instances reaching node S belong to the same class. 
Otherwise, if the entropy is high, it means that the 
many training instances reaching node S belong to 
different classes and hence should be split further. 
The correlation coefficients jir AA
 (Eq. (14)) 
between two attributes A
i
 and A
j
 of a training 
instance can be used to split the training instances. 
If 0jir AA
, then the training instances can be 
assigned to one branch. Otherwise, the instances 
can be assigned to a second branch. Let the 
training instances in S be split into two subsets S1 
and S2 (where SSS  21 and  21 SS ) by 
the correlation coefficient jir AA
. Then the 
accuracy of the split can be measured by 
 
 

K
h
K
h S
h
S
S
h
S
S
h
S
S
h
S
r
N
N
N
N
N
N
N
N
SE
ji
1 1
22 ,loglog)(
2
2
2
2
1
1
1
1
AA
 (16) 
 
Finally, the best correlation coefficient selected by 
the system is  
)(minarg)( 
,
**
SESr
ji
ji r
ji AAAA
  (17) 
It is to be noted that once a correlation coefficient 
has been selected, it cannot be selected again by its 
descendants. 
The algorithm to construct a binary 
classification tree is shown here: 
Algorithm: Decision tree construction 
Step 1: Initially, put all the training instances into 
root SR Regard SR as an internal decision node 
and input SR into a decision node queue. 
Step 2: Select an internal decision node S from the 
decision node queue. Calculate the entropy of 
node S using Eq. 15. If the entropy of node S 
is larger than a threshold Ts, proceed to Step 
3, 
otherwise label node S as a leaf node and  
proceed to Step 4. 
Step 3: Find the best correlation coefficient 
**
 jir AA
 to split the training instances in node 
S using Eqs. 16 and 17. Split the training 
instances in S into two nodes S1 and S2 using 
the correlation coefficients 
**
 jir AA
 and then 
subsequently add S1 and S2 into the decision 
node queue. . 
Step 4: If the queue is not empty, return to Step 2, 
otherwise stop the algorithm. 
7 EXPERIMENTAL RESULTS 
The input data for our system was acquired using a 
SONY TRV-900 video camera mounted above the 
Figure 6:The decision tree of the Hu moments. 
yes no 
054 HHr
HS1
HS2
HS9
 (a) turn right    (b) front    (c) turn left 
Figure 5: Three head poses of infant yawning. 
 -12- 
and a fewer number of nodes but a higher 
classification rate.  
Table 2: The experimental results. 
 (1) (2) (3) (4) (5) 
Hu  
Moments 
59 16+17 8 30 90% 
R  
Moments 
59 15+17 10 30 80% 
Zernike  
Moments 
59 19+20 7 30 87% 
PS. (1) Number of training sequences 
 (2) Number of nodes (internal node + leaf) 
 (3) Height of the decision tree 
 (4) Number of testing sequences 
 (5) Classification Rate 
8 CONCLUSION 
This paper presented an infant facial expression 
recognition technique for a vision-based infant 
surveillance system. In order to obtain more 
reliable experimental results, we will be collecting 
more experimental sequences to construct a more 
complete infant facial expression database. Binary 
classification trees constructed in this study may be 
less tolerant of. If the correlation coefficients are 
close to zero, then the noise will greatly affect the 
results of the classification. The fuzzification of 
the decision tree may help solve this problem. The 
infant facial expression recognition system is only 
one part of the intelligent infant surveillance 
system. We hope that this recognition system will 
be embedded into the intelligent infant surveillance 
system in the near future. 
ACKNOWLEDGMENT 
The authors would like to thank the National 
Science Council of the Republic of China, Taiwan 
for financially supporting this research under 
Contract No. NSC 98-2221-E-003-014-MY2 and 
NSC 98-2631-S-003-002. 
REFERENCES 
Doi, M., Inoue, H., Aoki, Y., and Oshiro, O., 2006. 
Video surveillance system for elderly person living 
alone by person tracking and fall detection, IEEJ 
Transactions on Sensors and Micromachines, Vol. 
126, pp.457-463. 
Department of Health, Taipei City Government, 2007. 
http://www.health.gov.tw/. 
The State of Alaska, 2005. Unintentional infant injury in 
Alaska, Women’s, Children’s, and Family Health, 
Vol. 1, pp. 1-4, http://www.epi.hss.state.ak.us/ 
mchepi/pubs/facts/fs2005na_v1_18.pdf. 
Pal, P., Iyer, A. N., and Yantorno, R. E., 2006. Emotion 
detection from infant facial expressions and cries, 
IEEE International Conference on Acoustics, Speech 
and Signal Processing, Vol. 2, pp. 14-19. 
Zhi, R., and Ruan, Q., 2008. A comparative study on 
region-based moments for facial expression 
recognition, The Proceedings of Congress on Image 
and Signal Processing, Vol. 2, pp. 600-604. 
Hu, M. K., 1962. Visual pattern recognition by moment 
invariants, IRE Transactions on Information Theory, 
Vol. 8, pp. 179-187. 
Liu, J., Liu, Y., and Yan, C., 2008. Feature extraction 
technique based on the perceptive invariability, 
Proceedings of the Fifth International Conference 
on Fuzzy Systems and Knowledge Discovery, 
Shandong, China, pp. 551-554. 
Alpaydin, E., 2004. Introduction to Machine Learning, 
Chapter 9, MIT Press, Massachusetts, USA. 
 
 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：方瓊瑤 計畫編號：98-2221-E-003-014-MY2 
計畫名稱：視覺式嬰兒監控系統之研發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
