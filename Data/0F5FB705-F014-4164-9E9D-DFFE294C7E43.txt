 2
is sensitive to noise because the model depends on 
local brightness in the image [7]. Ardizzone et al [9] 
suggested a simplified multi-resolution model 
(MSTAR) based on STAR. Fablet and Bouthemy [10] 
defined the temporal texture as the motion magnitude 
estatimation using nonparametric causal Gibbs 
distribution fitted at the ML sense. 
The motion field derived by optical flow is one 
of the most popular techniques to investigate the 
temporal texture. Normal flow is derived from 
optical flow hypothesis. We denote the intensity of a 
given pixel (x,y) at time t as I(x,y,t). Suppose I(x,y,t) 
remains unchanged when it moves to (x+dx,y+dy) at 
time t+dt. Therefore, 
Using Taylor’s expansion, we obtain  
Combing Eq.(1) and Eq.(2) yields 
Eq.(3) can be rewritten as vector form: 
where (vx,vy) represents the velocity vector, 
( )I II
x y
∂ ∂∇ = +∂ ∂  is the spatial gradient vector of I(x,y,t). 
Accordingly, the normal flow can then be derived 
from Eq.(4) and expressed as Eq.(5) 
Because the normal flow considers only the 
motion magnitude, it requires much less computation. 
However, there are at least three disadvantages about 
normal flow as follows. 
1) The assumption of unchanged pixel intensity over 
dt is necessary. In fact, the lighting condition 
possibly changes because the object motion in a 
video sequence. The lighting change affects the 
accuracy of  normal flow calculation. In addition, the  
normal flow is very sensitive to noise. 
2) If any region has constant pixel intensity, i.e.  ||∇I || 
=0, then the normal flow could not be determined by 
Eq.(5).  
3) If the motion vector v is perpendicular to the 
gradient of image intensity ▽I, then I
t
∂
∂  will equal to 
zero from Eq.(5). As a result, this motion will not be 
correctly computed by normal flow calculation. 
Based on the above reasons, it can be concluded 
that normal flow is still restricted to describe 
temporal motion in video sequences. 
Autoregressive model (AR) is another widely 
used technique to describe temporal texture. STAR is 
a three-dimensional (spatio-temporal) extension of 
autoregressive models as Eq.(6), which is given by 
where pixel intensity ( , , )I x y t  is modeled as a linear 
combination of P neighbor pixel values, 
( , , )i i iI x x y y t t+ ∆ + ∆ + ∆ , plus a Gaussian noise 
process, ( , , )x y tη . The STAR model makes an 
assumption that ( , , )x y tη  is wide-sense stationary. 
Because many temporal textures approximately 
satisfy the condition, these parameters 1φ ,…, Pφ  can 
be used to describe and characterize different texture 
classes. Generally, minimizing the mean square 
prediction error is a common criteria to determine 
parameters. However, parameter estimation is very 
time-consuming due to iterative computations, it is 
difficult to carry out short-response retrieval systems. 
Moreover, the synthesis of rotational motion (e.g. 
spiraling water flow of a toilet) cannot be captured by 
the STAR model, because it violates the stationary 
assumption [5]. 
Fablet and Bouthemy et al [10] considered three 
different neighborhoods η1, η5 and 9η (Fig.1) and 
used Gibbs random field, derived from 
spatial-temporal intensity function, to analyze the  
local nonparametric motion distribution. The residual 
normal velocity at pixel p  is defined as: 
Since the normal flow is also very sensitive to 
noise as well, an appropriately weighted average and 
quantization are performed in [10] for reliability.  
Besides, a complex Maximum Likelihood (ML) 
estimation of Gibbsian potentials is required for the 
characterization of motion information in video shot. 
Most of these works recommended model 
parameters to describe temporal textures. However, 
to find these solutions require very complicated 
parameter estimations, and the solutions can not 
achieve desirable retrieval (recognition) performance. 
With no clear exposition of parameters of texture 
model or dense motion estimation inspires us to 
propose an interpretable (e.g. directional similarity, 
spatial and temporal smoothness) temporal feature 
extraction for video retrieval; the method does not 
require parametric estimation. Experimental results 
show that it is more efficient than the conventional 
methods. 
This paper is organized as follows. In Section 2, 
we will present our spatial-temporal texture feature 
( , , ) ( , , )I x y t I x dx y dy t dt= + + + , (1) 
dt
t
Idy
y
Idx
x
ItyxIdttdyydxxI ∂
∂+∂
∂+∂
∂+=+++ ),,(),,( (2) 
0=∂
∂+∂
∂+∂
∂ dt
t
Idy
y
Idx
x
I . (3) 
0=∂
∂+∂
∂+∂ t
Iv
y
Iv
x
I
yx  or 0=∂
∂+⋅∇
t
II v , (4) 
I
t
Ivn ∇∂
∂−= /)( .  (5) 
∑
=
+∆+∆+∆+⋅=
P
i
iiii tyxttyyxxItyxI
1
),,(),,(),,( ηφ (6) 
*
*
*
1 ( )( )
( )n
I pv p
tI p
− ∂= ∂∇
. 
(7) 
 4
The mean feature represents the average of the 
prediction residual, whereas the standard deviation 
and the Dirac features express the degree of 
spreading out of the motion distribution. The ASM 
feature quantifies the temporal coherence [13], and 
SM measures the spread of the distribution around the 
origin. 
 
 
2.3 Concatenation of Descriptor 
Considering a fixed scale s, the feature matrix M(s) is 
defined as 
where h(d,s) presents the histogram of the quantized 
level (l) in clique d with scale s. To handle the 
numerous cliques, the five statistic properties in Eq. 
(13-17) are assigned to be the probabilities in Eq. 
(18). Then, we have 
 
 
where column vector f (d,s)= [µ (d,s) σ (d,s) δ (d,s) ASM (d,s) 
SM (d,s)]T. Note that each entry of this vector preserves 
dominant properties in terms of the spatial and 
temporal information of the gray levels in texture 
images. 
To evaluate the dissimilarity between two video 
sequences, A and B, the distance between the two 
feature matrixes is given by  
The above procedures for temporal texture 
descriptor are summarized in Fig.5. First, we scale 
down a video into three resolutions. Then, we adopt 
MRF to define a neighborhood structure. The eight 
neighbors of the current pixel are known as 2nd order 
neighborhood system (NS). The predicted residuals 
are extracted from these fifteen cliques, and then be 
quantized. Finally, the feature matrix is obtained 
based on histograms of these residuals. 
 
 
3   Experimental Results (結果與討論) 
To evaluate the performance of the proposed method, 
we use a video database that contains 220 sequences. 
The database is categorized into nine classes that 
include bird, boil, cloud, crowd, fire, flag, grass, river 
and toilet. See Fig.6 for some examples. The retrieval 
accuracy is measured by ARR (Average Retrieval 
Rate) [14]. The ARR value represents the ratio of the 
similar videos retrieved by the system to the total 
number of similar videos. 
To analyze the effectiveness of scale size, 
retrieval results by different scales are listed in Table 
1, where M8×15×1 and M8×15×2 denote the one-scale (S1) 
matrix and the two-scale (S1+ S2) matrix, respectively. 
It is observed that more scales considered lead to 
limited efficiency, but pays the cost of twice retrieval 
time. In the following, we are going to examine 
different quantization levels. Table 2 lists three 
quantization levels: L=4, L=8, and L=16. Results 
indicate that M16×15×1 improves ARR gain 1.5% as 
compared to M4×15×1. However, if the performance 
and complexity are the two main considerations, 
M4×15×1 performs an adequate ARR value with the 
lowest feature number. 
Table 3 displays the ARR results of the two 
feature matrices M16×15×1 and M′5×15×1, which are 
defined in Eq. (18), (19) respectively. The 
performance of M′5×15×1 is slightly lower 2% than that 
of M16×15×1; however, the dimension of simplified 
statistical matrix is highly reduced to 69%. 
Table 4 shows the comparison results of the 
proposed method, F&B [10] and STAR [5]. To 
evaluate retrieval efficiency under the same matrix 
size, the set of cliques are modified. Some directions 
are merged together if their moving directions are 
similar. For example, the directions of D6 
(topÆbottom) and D7 (bottomÆtop) are quite near if 
we neglect their downward and upward          
directions; therefore, they are merged together. As a 
result, the set of original cliques, {D1, D2…, D15}, is 
modified to a smaller set, {D1, D2…, D5, D6∪D7, 
D8∪D9, D10∪D11, D12∪D13, D14, D15} (eleven 
cliques). Then we are able to construct a new feature 
matrix, M4×11×1. It is found that the retrieval 
performance has 45.4% improvement comparing 
with that of F&Bs.  
When working at limited band-widths, the 
numbers of feature vectors are quite large. We use 
similar way to decrease the vector number. We chose 
5 five statistics and two cliques (D5 and D15) as 
feature vectors. The two cliques are regarded as the 
151 2
151 2
151 2
( , )( , ) ( , )( )
( , )( , ) ( , )
( , )( , ) ( , )
15
(0) (0) (0)
( 1) ( 1) ( 1)
d sd s d ss
d sd s d s
d sd s d s
L
  
p p p
p L p L p L ×
⎡ ⎤= ⎣ ⎦
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥− − −⎣ ⎦
M h h hL
L
M M M
L
(18) 
151 2
151 2
151 2
( , )( , ) ( , )( )
( , )( , ) ( , )
( , )( , ) ( , )
5 15
d sd s d ss
d sd s d s
d sd s d s
  
SM SM SM
µ µ µ
×
⎡ ⎤′ = ⎣ ⎦
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
M f f fL
L
M M O M
L
,  (19) 
( ) ( )
( , ) ( , )
,
( , ) ( , )s sA B A B
s
d s d s
A B
s d
        dist dist=
= −
∑
∑
M M M M
h h
,  (20) 
( ) ( )
( , ) ( , )
,
( , ) ( , )s sA B A B
s
d s d s
A B
s d
or     dist dist′ ′ ′ ′=
= −
∑
∑
M M M M
f f
  (21) 
,
 6
 
1η
9η
5η＋
＋ ＋
 
Fig.1 Causal temporal neighborhoods of pixel p. 
 
 
Fig.2 Four spatial cliques of different directions. 
 
Fig.3 Nine temporal cliques of different moving directions. 
 
 
 
Fig.4 Three scale resolutions. 
D6 D7 D8
D10 D11 D12 D13
frame t-1
frame t
frame t+1
frame t-1
frame t
frame t+1
D9
D14
