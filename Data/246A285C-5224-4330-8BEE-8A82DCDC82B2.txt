2 
 
I. Introduction 
With the rapid development of multimedia 
technologies, digital videos are now ubiquitous on 
the Internet. The enormous growth in the amount of 
video data has led to the requirement for efficient 
and effective techniques of video indexing and 
retrieval. In particular, since digital videos can be 
easily duplicated, edited, and disseminated, video 
copying has become an increasingly serious 
problem. A video copy detection technique would 
thus be helpful for protecting and managing video 
content. For instance, with such a technique, 
content providers could track particular videos with 
respect to royalty payments and possible copyright 
infringements. Platform providers could remove 
identical copies uploaded by users or aggregate 
near-duplicate videos into groups to facilitate 
browsing. Fig. 1 shows the result of inputting the 
phrase "UFO Apollo 11" to YouTube. Several sets 
of identical or near-duplicate video clips are 
displayed on the search result page. If the 
near-duplicates of the same set are compacted into a 
single item, the search result page can display more 
diverse video clips for users. 
 
 
 
Fig. 1. The first fifteen search results retrieved by inputting 
the phrase "UFO Apollo 11" to YouTube. 
 
In this paper, we address the limitations posed by 
existing methods. We classify various video copies 
into four categories with respect to the spatial and 
temporal transformation. While current methods are 
limited to handling one or two categories, we 
propose a novel video content analysis scheme, 
called spatio-temporal matching, to tackle all the 
categories in a unified manner. The proposed 
matching scheme, which analyzes the content 
similarity of two videos compiled from the spatial 
and temporal aspects, can effectively handle a 
variety of video transformation. 
To avoid the limitation caused by the pre-built 
index structures, we employ a sliding window 
approach that would be more appropriate for 
monitoring broadcast streams. Without the index 
structures, however, the search time spent on a 
large-scale video sequence would be considerable. 
To make the search more efficient, the proposed 
method adopts a coarse-to-fine matching strategy. 
In the coarse stage, the sliding window scans the 
entire sequence to find potential candidate clips 
quickly. Each windowed sequence, as well as the 
given query clip, is represented by a novel feature 
called the min-hashing signature that is compact 
and robust. For example, in this study, a 
30-dimensional signature is sufficient to represent a 
sixty-frame sequence. In the fine stage, the 
above-mentioned spatio-temporal matching is used 
to filter out false positives from the candidate set. In 
addition, some speed-up techniques are applied in 
both stages with great effect; hence, overall, the 
proposed method is very fast. 
To evaluate the proposed method, we implement 
several baseline methods that use different feature 
descriptors (e.g., the ordinal measure and the SIFT 
descriptor), as well as different matching schemes. 
The results of extensive experiments show that, 
with the composed feature descriptor, the proposed 
method achieves excellent results, which 
demonstrate the widest coverage for various 
transformation types among all compared methods. 
The proposed method is also very efficient, as it 
only needs about 0.06 seconds to search for copies 
of a thirty-second query clip in a six-hour video 
sequence. 
4 
 
verifies the two sequences from spatial and 
temporal aspects. Then, the window moves forward 
to the next target frame. The whole process is 
repeated until the window reaches the end of the 
target sequence. 
We implement several methods with different 
feature descriptors and matching schemes for the 
performance comparison. The experiment results 
demonstrate that the proposed framework yields a 
relatively excellent and stable accuracy among 
these methods on various video transformation 
types. The coarse-to-fine framework is efficient, as 
the excellent result can be obtained with 
approximately 0.4 seconds to search for copies of a 
thirty-second query sequence in a six-hour video 
sequence using a PC with 1.8 GHZ CPU and 2GB 
RAM. The successful integration of the 
coarse-to-fine matching scheme ensures that the 
proposed framework is fast and robust. 
 
Fig. 2. An overview of the proposed framework. 
 
IV. Experiments 
We compiled a 6.1-hour video dataset from the 
Open Video Project for evaluations. The video clips 
were transformed into a uniform format, namely 
MPEG-1, 320×240 pixels, and 30 frames per 
second (fps), and then concatenated into a single 
sequence that served as the target sequence. 
From the target sequence, we randomly extracted 
31 sequences, each of thirty seconds duration. Each 
sequence was used as a source to derive six video 
copies, including compression 50%, noise addition 
10% (preserved frame region transformation), 
cropping 20%, zooming in 10% (discarded frame 
region transformation), Halve the video speed and 
double the video speed (changed frame number 
transformation). This yielded a total of 186 (31×6) 
video copies, which served as the queries. We then 
used each query to detect the corresponding 
subsequences in the target sequence. 
We implemented the following six methods to 
compare their performances: 
 
(1) The ordinal measure with spatio-temporal 
matching; 
(2) The SIFT descriptor with spatio-temporal 
matching; 
(3) The combination of the above two methods; 
(4) The ordinal measure without spatio-temporal 
matching; 
(5) The SIFT descriptor without spatio-temporal 
matching; 
(6) Hoad & Zobel's method [2]. 
 
Figs. 3(a)-(f) show the proposed methods 
demonstrate robust performances on (a) 
compression; (b) noise addition; (c) cropping; (d) 
zooming in; (e) slow motion; and (f) fast forward 
with different k = {10, 20, …, 100}. The above 
experiments demonstrate the proposed methods 
achieve excellent accuracy with very high recall 
and precision rates. In particular, Method (5), which 
integrates all the proposed techniques, yields a 
consistently robust performance for every type of 
video transformation. In addition, according to our 
experience, k = 30 that requires 30 milliseconds to 
search in a 6.1 hour video sequence is sufficient to 
obtain a satisfactory accuracy rate. 
 
1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期：100 年 1 月 20 日 
一、 參加會議經過 
2010 海峽兩岸信息管理發展與策略學術研討會(CSIM 2010)於中國香港的香港城市大學舉行。本
次大會主席為趙建良(香港城巿大學)與梁定澎(台灣國立中山大學)；大會執行主席為廖少毅(香港
城巿大學)與謝俊霖(香港城巿大學)。本人在此會議的Session 1b: IT in Organization發表一篇論文，
進行口頭報告。 
 
•  (2010/08/05)第一天上午由香港城市大學校長郭位開幕致詞，10：00 開始今天的論文發表，本
人參與 Session 1b: IT in Organizations，主持人是復旦大學的 Ling, Hong 教授。本人在此場次發表論
文：行動載具存取 OAIS 與 OAI-PMH 系統之研究。報告過程中得到許多專家學者分享的寶貴
的知識與建議。此場次共有 4 篇論文，主題皆是電腦科學在管理領域上的實務、技術、與應用，
得到許多豐富的經驗。下午則跟與會教授進行交流。 
 
•  (2010/08/06)第二天上午舉行兩場 sessions。本人參加了 Session 3b & 4e: Mobile Technologies，
聆聽相關報告。其中對於台灣中央警察大學發表的：智慧型手機數位證據鑑識處理程序之研究、
以及台灣國立政治大學發表的：以手機為基礎的帳號救援雲端服務，這兩篇論文印象特別深刻，
提出非常有創見的應用課題，並且導入目前最新的手機與網路技術來進行研究，與會人士都相當
肯定台灣方面的創意。 
計畫編號 NSC 98－2218－E－415－003－ 
計畫名稱 多媒體內容拷貝偵測技術之研究 
出國人員
姓名 邱志義 
服務機構
及職稱 國立嘉義大學資訊工程系 
會議時間 
 99 年 8 月 5 日
至 
 99 年 8 月 6 日 
會議地點 中國香港 
會議名稱 
(中文)海峽兩岸信息管理發展與策略學術研討會 
(英文)The Cross Straits Information Management Conference 
發表論文
題目 
(中文)行動載具存取 OAIS 與 OAI-PMH 系統之研究 
(英文)Mobile accessing on OAIS and OAI-PMH systems 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/19
國科會補助計畫
計畫名稱: 多媒體內容拷貝偵測技術之研究
計畫主持人: 邱志義
計畫編號: 98-2218-E-415-003- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
