研究摘要(500字以內)： 
 
影像診斷學中，醫療影像儲傳系統(Picture Archiving and Communication System, PACS)
是一種專門用來儲存、取得、傳送與展示醫療影像的電腦或網路系統。PACS的主要目的
在於將醫療系統中所有影像，以數位化的方式儲存，並經由網路傳輸至系統中，供使用者
遠端電腦螢幕閱讀影像並判讀。同時也可作為不同影像傳遞交換的工具，隨著軟體及運用
程式的進步，將來更可進一步協助醫師進行診斷、教學及醫學研究。而成功的 PACS不只
須要強大硬體，更依賴完善軟體功能及作業程序。透過日漸成熟的網格計算 (Grid 
Computing)技術，將散佈各地之虛擬組織的資源可以透過網格的概念來調派和集中。再者，
利用資料網格(Data Grid)的容錯特性與高可用性，因此可以滿足各種在醫療資訊應用方面
的計算與檔案儲存需求。 
為降低 PACS系統之擴建成本與設置第二 PACS系統為考量，本計畫為期 3年，主要
是研發 Smart Broker Centric與具有自適性副本管理元件於於協同配置(Co-allocation)資料
網格環境中。藉由導入 Open Source PACS解決方案，建置於特別設計的網格功能元件之
上，期能證實以網格技術來支援 PACS系統的可行性，除保有 PACS的優勢、實際提昇 PACS
影像副本交換效率與一個成本較低廉的 PACS導入方案。 
第一年度，計畫完成平台架構上所有系統元件的雛型設計開發，平台架構區分
Application、Smart Broker、Cyber Abstraction、Grid Middleware以及 Fabric等 5個階層，
其中較高層以使用者為中心，並以 Smart Broker作為本架構核心；底層專注於資源整合，
並以 Cyber Abstraction描述高低二階層之間如何連結。另外設計一個網格知識管理(GKM)
模型以支援各元件計算的參數資料輸入。其成果已在 APSCC08, IPCADS08 與 JNCA 發
表。 
第二年度，計畫完成整合各階層元件，使用 Globus Tookit與雲端計算(Cloud Computing)
結合 Cross-CA 技術，建構跨網格系統的交互驗證機制，提昇計算與儲存資源利用率。實
作 Co-Allocator元件，該元件能經由 Resource Management System (RMS)提供對應資源，
並透過 GKM的知識基礎，設計出具有認識網格環境現況的能力，即為自適性。在資料網
格環境中，資料集被重複製為複本且分送到多重的站台。由於資料集的檔案通常很大，如
何有效率的存取及傳輸成為重大的課題。因此先前有學者發展出協同配置的架構，使得同
時從多重站台平行下載資料變成可能，目前發展出數種協同配置法用來解決傳輸時本地端
與伺服端網路傳輸率不斷變動的問題。本研究中，我們採用 TCP 頻寬估計模型與突發模
式等新策略，藉此強化預測性遞迴調整的協同配置法，進一步提高大量資料集於資料網格
中的傳輸效能。我們的方法能有效地找出一群快速伺服器並分配較多的工作量提高其資源
利用率，動態計算出檔案切割量，有效減少各伺服器間的相互等待時間。藉由各項實驗證
明其傳輸的高效能，並具有網路自適應性與高度容錯性，有效因應不同的網格環境。使用
Open Source PACS系統為應用實例，進行實驗及測試雛型系統功能及相容性。其成果已在
Journal of Supercomputing與 CCPE發表。 
最後一年，我們根據應用實例的實驗結果，進一步改善各階層元件與雛型應用系統與
GKM學習樣本調較。並就我們的PACS系統實際使用情況與真實醫院PACS系統作整體效能
比較。我們基於雲端計算平台(Hadoop)設計了一個醫療影像檔案存取系統，稱為 MIFAS 
(Medical Image File Accessing System)，透過這個系統可以查詢及取得醫療影像。系統容錯
(System Fault Tolerance)架構設計，使前端應用的可靠度大為增加。HDFS (Hadoop 
Distributed File System)分散式檔案系統及複本配置機制的導入，使檔案儲存的穩定性得到
fragments. In this report, the TCP Bandwidth Estimation Model (TCPBEM) is used to evaluate 
dynamic link states by detecting TCP throughputs and packet lost rates between grid nodes. We 
integrated the model into ARAM, calling the result the anticipative recursively adjusting mechanism 
plus (ARAM+); it can be more reliable and reasonable than its predecessor. We also designed a 
Burst Mode (BM) that increases ARAM+ transfer rates. This approach not only adapts to the worst 
network links, but also speeds up overall performance. Taking Open Source PACS for examples, 
we plan to conduct experiments on functionality and compatibility of the prototype.  
In the last year, we will plan to refine the components and the application according to the 
experimental results. Also, the proposed system will be compared with a real-world PACS of 
hospitals in terms of overall performance. We designed a Medical Image File Accessing System 
(MIFAS) based on Cloud-based computing platform (HADOOP). We can be queried by the 
system and access to medical images. The design in system fault tolerance architecture that 
greatly increased the reliability of front-end applications. Hadoop distributed file system (HDFS) 
and replication service, so that the stability of stored files is protected, and the investment and 
management costs can be reduced. Download the strategy and the addition of the Co-allocation 
model middleware that making the medical imaging in the wide area network (WAN) 
transmission speed has been improved. In addition, we also provide easy operation and 
management of WEB based interface. Compared with traditional PACS, our system provided 
the best quality medical image file access services to improve the medical imaging issue 
between the different hospitals. 
 
Keywords: Grid Computing, Data Grid, Medical Grid, PACS, Co-allocation 
 
人才培育成果說明： 
 
研究初期工作為叢集計算系統的規劃與設計，參與人員能在叢集系統的架設及應用熟
悉其技巧。對於叢集伺服器管理能更有經驗。參與研究的人員能深切瞭解 PACS與格網的
功能與意義，應用軟體工程的理論到實際的開發過程。對於未來投入軟體產業有非常大的
幫助。 
對於參與本子計畫研究之人員將能對，健康服務格網，醫療資料格網，及 HL7 資料
管理這些重要的技術有更深刻的認識，包含其歷史背景、發展過程、時空環境、遭遇到的
問題與未來發展前景都能有通盤的學習與了解。 
對於參與本子計畫研究之人員將能對服務格網資源的擷取有更多的練習，包括服務格
網資訊監控系統技術應用、RRDTool技術應用、JRobin圖表繪製，同時為了將資源提供給
其他子計畫之模組來使用，如何把資料整合並有效利用也將會是學習的重點。包含其歷史
背景、系統發展過程、系統整合測試與技術應用、遭遇到的問題與未來發展前景都能有通
盤的學習與了解。 
參與人員可以學習到結合不同理論建構問題以及獨立思考能力，在實驗的過程，參與
者可學習到判斷研究成果的正確性以及回饋修正的能力。另外，也可以學習整套研究的方
法，團隊工作的精神，以及撰寫科技論文的經驗。也可以建立觀摩網站供有志從事高效能
醫療影像儲傳系統工作的朋友學習，規劃未來高效能醫療影像儲傳系統計畫的方向。 
由於本計畫為整合資料網格環境與醫療影像儲傳系統，對於參與本計畫研究之人員，
Cross-CA技術結合 TigerGrid及 UniGrid資源，在得到近 3百顆 CPU計算能力及超過百部
伺服主機的硬碟空間之後，以採用自由軟體的 PACS系統架構於其上，測試其相容性與效
能並藉以調整 Smart Broker之 Grid KM及Workflow Engine效能。 
 
技術特點說明： 
本計畫預計開發展一套可以運行在現行醫療院所環境中的以協同配置資料網格環境
中具適應性複本管理的高效能醫療影像儲傳系統，並應用在科學、教育、商業等領域。在
核心技術部份，我們應用各種 Grid Computing and Cloud Computing技術開發具輕量化，
高速傳輸，與容錯的格網系統。在軟體開發上，與開放原始碼社群整合，預期提供使用者
一個便利、安全的高效能醫療影像儲傳系統平台，提供管理者一個集中式的管理介面，與
提供開發者一個具高度擴充性及相容性的系統架構，使得未來管理的人力及時間成本大幅
降低。執行本計畫所得到的研究理論、工具開發、與實務經驗亦可作為相關領域學術研究
與教學的素材。 
學術研究上的預期貢獻：本項研究以創新的方法與高等軟體開發技術解決複雜且難以
設計的格網中介軟體。結合理論與應用，建構高效率、容易操作、管理、與維護的高效能
醫療影像儲傳系統。本項研究所提出方法與技術，預期對研究或發展相似的高效能醫療影
像儲傳系統，有一定的參考的價值。本計畫的研究成果，預期將在國際著名期刊與國際研
討會上發表。另外，透過國內研討會的交流，可以結合國內從事此方面研究的教授研發能
量。未來可以發展具有國際競爭力的系統。 
所開發出來的系統教育上的預期貢獻，對於有興趣學習分散式計算、Data Grid與 Grid 
Service 計算、叢集與格網計算的資訊相關科系的高年級學生與研究生，可以作為練習的
平台；同時對於推廣高效能運算教育具有很大的助益。對教師而言，也可以有實際而且容
易操作的高效能醫療影像儲傳系統作為課堂 Demo的素材。 
應用上的預期貢獻：本計劃研發新一代的醫療影像儲傳中介軟體系統，除了能夠透過
Data Grid 技術來達到資料的相容性與容錯度，高彈性的大量管理及部署，也能夠以更好
的系統效能，輕量化的系統結構實現中介資料索引搜尋，虛擬空間等功能。研發成功後預
期的影響，為提供產業、學術以及大眾網路一個可行的商業化高效能醫療影像儲傳系統平
台，並實際導入學術及醫療院所應用。 
本計畫若能承蒙貴會支持，透過計畫的功能規畫與研發，建立適合醫療院所之協同配
置資料網格環境中具適應性複本管理的高效能醫療影像儲傳系統平台、相關服務元件及計
算環境、PACS裝置技術，有助於未來我國面臨高齡化人口所需的健康照護基礎環境建設。
同時透過自由軟體技術及平台對國內相關產業能提供相關技術及支援，對於提昇我國資訊
產業在健康照護方面的競爭力，將有相當助益。 
 
 
 
 
 
 
 
 
計畫成果自評部份 
在學術期刊發表或申請專利 
專利(申請中) 
台灣申請案： 
「進階預測遞迴式調整協同配置法」，申請案號：098105346，申請日期為 2009/2/19。 
 
美國申請案：  
申請日：September 9, 2009 (主張台灣優先權申請日) 
申請案號：12/556,413 
專利名稱：ANTICIPATIVE RECURSIVELY-ADJUSTING CO-ALLOCATION MECHANISM 
 
Journal Papers 
[1] Chao-Tung Yang*, Chiu-Hsiung Chen, and Ming-Feng Yang, “Implementation of a 
Medical Image File Accessing System in Co-allocation Data Grids”, Future Generation 
Computer Systems, Article in Press, June 2010. (ISSN: 0167-739X, SCI JCR IF=2.229, 
1.476, EI) 
[2] Chao-Tung Yang*, Yao-Chun Chi, Ming-Feng Yang, and Ching-Hsieh Hsu, "An 
Anticipative Recursively-Adjusting Mechanism for Parallel File Transfer in Data 
Grids," Concurrency and Computation: Practice and Experience, 2010. (ISSN: 
1532-0626, SCI JCR IF=1.004, 1.791, EI) 
[3] Chao-Tung Yang*, I-Hsien Yang, and Chun-Hsiang Chen, "RACAM: Design and 
Implementation of a Recursively-Adjusting Co-Allocation Method with Efficient 
Replica Selection in Data Grids," Concurrency and Computation: Practice and 
Experience, 2010. (ISSN: 1532-0626, SCI JCR IF=1.791, EI) 
[4] Chao-Tung Yang*, Shih-Yu Wang, and William C. Chu, “A Dynamic Adjustment 
Strategy for Parallel File Transfer in Co-Allocation Data Grids,” Journal of 
Supercomputing, Springer Netherlands, 2010. (ISSN: 1573-0484, SCI JCR IF=0.615, EI) 
[5] Chao-Tung Yang*, Chun-Pin Fu, and Ching-Hsien Hsu, “File Replication Maintenance 
and Consistency Management Services in Data Grids,” Journal of Supercomputing, 
Springer Netherlands, 2010. (ISSN: 1573-0484, SCI JCR IF=0.615, EI) 
[6] Chao-Tung Yang*, Chih-Hao Lin, Ming-Feng Yang, and Wen-Chung Chiang, “A 
Heuristic QoS Measurement with Domain-based Network Information Model for Grid 
Computing Environments”, International Journal of Ad Hoc and Ubiquitous Computing 
(IJAHUC), Volume 5, Number, 4, pp. 235-241, 2010. (ISSN Online: 1743-8233 - ISSN 
Print: 1743-8225, SCI JCR IF=0.66, EI) 
[7] Chao-Tung Yang*, Ming-Feng Yang, and Wen-Chung Chiang, "Enhancement of 
anticipative recursively adjusting mechanism for redundant parallel file transfer in data 
grids," Journal of Network and Computer Applications, Elsevier B.V., Volume 32, Issue 
4, July 2009, Pages 834-845. (ISSN: 1084-8045, SCI JCR IF=0.660, 1.111, 1.000, EI) 
 
※ 備註：精簡報告係可供國科會立即公開之資料，並以四至十頁為原則，如
有圖片或照片請以附加檔案上傳，若涉及智財權、技術移轉案及專
利申請而需保密之資料，請勿揭露。 
Author's personal copy
Future Generation Computer Systems 26 (2010) 1127–1140
Contents lists available at ScienceDirect
Future Generation Computer Systems
journal homepage: www.elsevier.com/locate/fgcs
Implementation of a medical image file accessing system in co-allocation
data gridsI
Chao-Tung Yang a,∗, Chiu-Hsiung Chen b, Ming-Feng Yang a
a High-Performance Computing Laboratory, Department of Computer Science, Tunghai University, Taichung, 40704, Taiwan
b Tungs’ Taichung MetroHarbor Hospital, No. 699, Sec. 1, Jhongci Rd., Wuci Township, Taichung County 435, Taiwan
a r t i c l e i n f o
Article history:
Received 9 November 2009
Received in revised form
26 May 2010
Accepted 28 May 2010
Available online 8 June 2010
Keywords:
Medical images
Data grid
Grid computing
Co-allocation
File transferring
a b s t r a c t
There are two challenges of using the PACS (Picture Archiving and Communications System). First, PACS
are limited to certain bandwidths and locations. Second, the high cost of maintaining Web PACS and the
difficultmanagement ofWeb PACS servers. Besides, the quality of transporting images and the bandwidth
of accessing large files from different locations are difficult to guarantee. For instance, radiologists make
use of PACS information system for achieving high-speed accessing medical images. Physicians, on the
other hand, utilize web browsers to indirectly access the PACS information system via non-high-speed
network. The insufficient bandwidth may cause bottleneck under a host of querying and accessing. As
hospitals exchange large files such as medical images with each other via WANs, the bandwidth cannot
support the huge amount of file transportation. In this paper, we propose a PACS based on data grids, and
utilize MIFAS (Medical Image File Accessing System) to perform querying and retrieving medical images
from the co-allocation data grid. MIFAS is also suitable for data grid environments with a server node and
several client nodes. MIFAS can take advantage of the co-allocation modules to reduce the medical image
transfer time. Also, we provide experiments to show the performance of MIFAS. Furthermore, in order to
enhance the security, stability and reliability in the PACS, we also provide the user-friendly management
interface.
© 2010 Elsevier B.V. All rights reserved.
1. Introduction
Nowadays, 2D, 3D, and 4D medical imaging devices are
increasingly needed by hospitals. With the progress of medical
photograph, the resolution of medical images is raising. Therefore,
the scale of medical image files range from MB to GB. The size of
high-resolutionmedical images, such as 64/128-slice CT scans, 3.0T
MRI, and PET, often exceed one hundredMB ormore. However, the
speed of progress on many high-quality imaging devices and it re-
lated infrastructure are not match. Current Picture Archiving and
Communication Systems (PACS) [1–4] are unable to provide effi-
cient query response services. It is difficult to sustain huge queries
and file retrievals under limited bandwidth. Therefore, the quality
of communication in the Web PACS network would be restricted
by bandwidth and conventional access strategies about exchang-
ing and downloading a large amount of images. In order to enhance
the quality of medical treatment, the medical imaging needs to
I This work is supported in part by the National Science Council, Taiwan R.O.C.,
under grant nos. NSC 97-2622-E-029-003-CC2 and NSC 98-2622-E-029-001-CC2.∗ Corresponding author. Tel.: +886 4 23590415; fax: +886 4 23591567.
E-mail addresses: ctyang@thu.edu.tw (C.-T. Yang), t2884@ms.sltung.com.tw
(C.-H. Chen), orsonyang@gmail.com (M.-F. Yang).
associate with efficiency file transfer strategy to achieve high-
speed accessing.
In this paper, we present a new strategy for processing medical
image queries, which is based on the co-allocation [5–10] strategy
for data grid environments. A data grid is a system composed
of multiple servers that work together to manage information
and related operations – such as computations – in a distributed
environment. Our proposed system is called the Medical Image
File Accessing System (MIFAS) for co-allocation data grids. To solve
these problems, we propose the PACS based on the co-allocation
data grid environment. MIFAS helps us to transfer huge medical
images into the co-allocation data grid environment.We utilize the
Globus Toolkit 4.0.7 [11–13] to establish the data grid environment
for deploying co-allocation strategy and processing medical
images. MIFAS helps users to quickly retrieve medical images
from Medical Data Grid. The Cyber Agent Service and the Grid
Service GUI desk application are implemented to assist in query
and retrieve medical images. Also, MIFAS provides resume broken
transfer to deal with the unstable circumstance of network. It not
only enhances the overall quality of medical care system but also
supports multiple replicas of medical images for failover recovery.
This paper presents a strategy to improve the security, stability
and reliability of the PACS. Our strategies focus on integrating
the service of processing medical images and stimulating PACS
architecture into grid environments. The remainder of this paper is
0167-739X/$ – see front matter© 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.future.2010.05.013
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1129
nicely with changes in server speed performance. When a re-
quested block is received from a server, one of the unassigned
blocks is assigned to that server. The co-allocator repeats this
process until all blocks have been assigned. DCDA behaves well
even when server links are broken or idled. The DCDA scheme
is flawed: it consumes network bandwidth by repeatedly trans-
ferring the same blocks. This wastes resources and can easily
cause bandwidth traffic jams in the links between servers and
clients.
The ARAM [25] scheme adjusts the workloads on selected
replica servers and handles unpredictable variations in network
performance. Our algorithm uses the finish rates of the previously
assigned transfers to anticipate the bandwidth status for the next
selection, adjust workloads, and reduce file transfer times in grid
environments. Our approach is useful in grid environments with
the unstable network. It not only reduces the idle time thatwaiting
for slowest server, but also decreases the file transfer completion
times.
2.3. Medical images
Medical image processing technique refers to procedures that
use special equipment to capture images of various body organs for
clinical purposes (medical procedures intended to aid in diagnosis
or examine diseases) or medical science researches (including
normal anatomy and function).
This technique refers to one aspect of biological imaging, incorpo-
rating radiology, radiological sciences, endoscopy, thermography,
medical photography, and microscopy for human pathological in-
vestigations.
In clinical applications, medical imaging is also known as radi-
ology or ‘‘clinical imaging’’. Diagnostic radiography indicates the
technical aspects of medical imaging and focus on the acquisition
of medical images. Radiologic technologists and physicians are re-
sponsible for acquiring high-quality medical images of diagnostic
and performing radiological interpretations.
In the fields of Medicine, Medical Engineering, Medical Physics,
and Bioinformatics, Medical Imaging is usually defined as the tech-
nology for image creation, retrieval and storage. Researches on
medical image applications and interpretations are classified as ra-
diology or other relevant medical sub-disciplines, areas of medical
science and neuroscience, cardiology, psychology, etc. Many tech-
niques developed for medical imaging also have scientific and in-
dustrial applications.
Medical imaging can be seen as the solution of mathematical
inversion problems, all related analyses are inferred from observed
signals. In the case of ultrasonic devices, the probe produces
ultrasonic pressure waves and echoes inside the tissue to show the
body’s internal structure. A projection radiography probe produces
X-ray radiation which is absorbed at different rates by various
tissue types such as bone, muscle, and fat.
Modern medical imaging technology includes:
• X-ray: Radiographs,more commonly knownasX-rays, are often
used to determine the type and extent of a fracture as well as to
detect pathological changes in the lungs.
• Computed Tomography (CT): Digital geometry processing is
used to generate a 3D image of the inside of an object from a
large series of 2D X-ray images taken around a single axis of
rotation.
• Ultrasound: Medical ultrasonography uses high frequency
sound waves of between 2.0 and 10.0 MHz to produce 2D im-
ages, traditionally on a TV monitor.
• Magnetic Resonance Imaging (MRI): MR imaging uses a pow-
erful magnetic field, radio waves and a computer to produce
detailed pictures of organs, soft tissues, bone and virtually all
other internal body structures.
• Gamma camera: Gamma rays (denoted as γ ) are the form
of electromagnetic radiation or the light emission of frequen-
cies produced by sub-atomic particle interactions, such as elec-
tron–positron annihilation or radioactive decay.
• Positron Emission Tomography (PET): It is a nuclear medicine
imaging technique which produces a 3D image or maps of
functional processes in the body. The system detects pairs of
gamma rays emitted indirectly by a positron-emitting radionu-
clide (tracer),which is introduced into the bodyon abiologically
active molecule. Images of tracer concentration in 3D space
within the body are then reconstructed by computer analysis.
• Others: Fluoroscopy, Angiography, Microscopy, Photo Acoustic
Imaging, Thermography, Endoscopy, and etc.
ImageJ [26] is a public domain Java-based image processing
software developed by the National Institutes of Health that runs
on Windows, Mac OS, Mac OS X, Linux, Sharp PDA, and other plat-
forms. It can display, edit, analyze, process, save and print 8-bit,
16-bit, and, 32-bit images in TIFF, GIF, JPEG, BMP, DICOM, FITS
and raw image formats. It supports stacks and series of images
that share a single window and provides multithreading for time-
consuming operations. The accessing of image files could be per-
formed in parallel with other operations. ImageJ is a free open
source software that supports customupgrades, edits and plug-ins.
It has a built-in editor and Java compiler, and provides users with
any IDE to directly process images.
3. System design and implementation
3.1. System architecture
MIFAS was deployed in the co-allocation data grid with Globus
Toolkit 4.0.7. We aggregated desktop PCs and servers to establish a
data grid. The descriptive medical image information (metadata)
about logical data items is stored in the MIFAS Catalog Service.
The four-layer architecture of the data grid is shown in Fig. 2. The
yellow parts are development and implementation by Health-Box.
Health-Box (H-Box) is a set of integrated scripts developed by the
High Performance Lab at Tunghai University. It provides a quick
way to form grid environments and integrated grid framework
sets. H-Box integrates the grid middleware, Globus Toolkit, to
connect the nodes, then installs necessary software, including
MPICH [27], Ganglia [11], NWS [28], SYSSTAT [5], SRB [29], JDK [30],
ApacheAnt [31], XML-Parser [32], and xinetd [33]—all Open Source
Software. We provide a grid manager to download, distribute, and
modify it. Thus, we can use H-Box to quickly form prototyping of
medical imaging storage grid architectures.
3.2. System flow
3.2.1. System workflow
Our design for the co-allocation grid is shown in Fig. 3. AsWeb-
based Enquiries PACS, every client node access point uses the Cyber
Agent to enter the co-allocation data grid, andmanage queries and
image retrievals. Overall, the benefit of our method is to speed up
query accessing and image retrieving. It also provides the security
for queries and image retrievals in the data grid environment.
3.2.2. Simple cyber agent transformer workflow
Fig. 4 shows the simple Cyber Agent Transformer workflow and
transfer steps. Basically, physicians search and retrieve medical
images via the MIFAS co-allocation data grid. The steps of the
MIFAS co-allocation data grid workflow are described below. As
users want to access the data grid, they must first set up a User
Certificate, Private Key, Certificate Authority (CA) file, and a Proxy
File for retrieving the data grid authentication. After the access is
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1131
Fig. 4. Simple workflow for cyber agent transformer.
1.Authentication 
Setup
4.Authentication
Gate Keeper
2.Request 
Connect
3.Connect
5.Search 
Medical 
Image replica 5.1Search MIFASCatalog 
service
5.2Report back
6.Request 
Transfer
6.1Query MIFAS 
Catalog service Gate Keeper
MIFAS Co-allocation 
Data Grid
6.2Report back
6.3Dynamic 
Transfer
7.Request 
Transfer
MIFAS 
Co-allocation
D.C.D.A
History Based
Conservative 
Load Blancing
Aggressive 
Load Blancing
DAS
R.A.M
A.R.A.M
9.Start Transfer
10.Finish
Transfer
8.Select 
Node
Brute Force
6.Select Medical 
Image replica
Fig. 5. Cyber agent transaction flow.
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1133
Fig. 8. User tools.
Fig. 9. Selecting replica download from candidate node.
4.3. Experiment 1: compare 8 co-allocation schemes for downloading
different data sizes
In this experiment, we downloaded the files whose size is from
10 MB to 1 GB with the eight MIFAS Co-allocation algorithms. The
results show the best algorithm varied according to the file size
downloaded. Fig. 13 shows the download data for the eight transfer
modes. Fig. 14 shows no differences among the eight MIFAS Co-
allocation algorithms on downloading a 10 MB file. In this case, all
methods used to query or retrieve fileswould be completed around
the same time. Fig. 14 also shows that the ARAM algorithm was
the best method for 50 MB–1 GB files. This experiment showed
the best transmission method, which were used to perform other
experiments.
4.4. Experiment 2: compare query/retrieval times from local grid node
using ARAM and web PACS
In this experiment, we simulated a Web PACS in the local grid
node and used the best transfer method, ARAM, for comparison
tests. Physicians may need to search and retrieve the files listed in
Table 2 for diagnosis or to compare medical cases. These files are
usually X-ray images, CT scans, or series’ of CT scans. In order to
compare the difference in data retrieval performance between the
Web PACS and the Cyber Agent Transformer, we customized test-
bed A, as shown in Fig. 15. The Cyber Agent Transformer retrieved
images via parallel-download fromMedical Data Grid B (Data Flow
B, Fig. 15), whereas the Web PACS retrieved from the Web PACS
(Data Flow A in Fig. 15). The times for the MIFAS Co-allocation
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1135
Fig. 12. Ganglia page of TIGER.
Fig. 13. Cross-hospital PACS architecture.
4.6. Experiment 4: conventional medical image exchange vs. co-
allocation download
All the medical images that exchanged between hospitals are
produces from high-level imaging systems such as 64-slice CT and
3.0T MRI. There are various PACSs simulated to exchange the 64-
slice CT and 3.0T MRI images listed in Table 3. In order to compare
the difference in data retrieval performance between PACSs, test-
bed Cwas customized to transfer images from the THU PACS to the
HIT PACS, as shown in Fig. 22.
The Cyber Agent Transformer exchanged images in the conven-
tional way (The data flow A in the Fig. 22), as compared to parallel
downloading from the Medical Data Grid (The data flow B in the
Fig. 22).MIFAS Co-allocation andDICOM transfer results are shown
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1137
Fig. 19. Our test-bed B.
Fig. 20. Compare query/retrieval times for the first image from remote grid node
using ARAM and web PACS.
PACS for medical image queries, retrievals, and exchanges. Unfor-
tunately, network interruptions will delay or halt the Web PACS
and medical image exchanges. Network fault tolerance design is
important in improving the usability and reliability of the full grid
system. Among all co-allocation methods, only ARAM was able to
overcome network faults.
As shown in Figs. 25–27, we built an environment with replicas
at four grid sites. Each site was disconnected in a period of time
during file transfer. The ARAM scheme was designed to continue
file transfers from the previous point, and avoid having faster
sites waiting for slower sites. Therefore, the overall file transfer
performance keeps stable.
4.8. Experiment 6: image machine recovery
In the near, Tungs’ TaichungMetroHarbor Hospital experienced
a Web PACS breakdown that affected physicians in diagnosing
patients. It took us 6 months to reconstruct theWeb PACS without
affecting ongoing general operations and providing image data
to physicians. (We used only the off-peak times of Monday to
Friday from 22:00 pm to 8:00 am, Saturday afternoon, and Sunday.
Fig. 21. Compare the average transfer times from a remote grid node for ARAMand
web PACS.
Fig. 22. Our test-bed C.
Author's personal copy
C.-T. Yang et al. / Future Generation Computer Systems 26 (2010) 1127–1140 1139
Fig. 28. Compare machine replacement cost.
Fig. 29. Compare storage environment construction and medical image recovery
times.
way. Fig. 28 shows both are better than the Web PACS-related
software and hardware. And Fig. 29 shows both constructed and
recovered data faster than the Web PACS.
5. Conclusions
This paper proposed the MIFAS solution to reduce medical im-
ages’ transfer time, and integrated medical image process tech-
nique into co-allocation data grid environments.We also report on
implementing a Cyber Agent Service that enables users to use co-
allocation data grid. Currently, MIFAS offers four advantages.
First, compared with PACS, MIFAS can reduce co-allocation
transfer time from experimental results. Second, MIFAS
provided a fast, secure, stable, reliable system for obtaining med-
ical images. Three, co-allocation architecture enables parallel
downloading from a co-allocation data grid. It can also speed up
downloads and overcome network faults. Four, we provided easy
management, reduced expense, and increased the stability ofmed-
ical image system.
We reported on successfully moving medical images on the
MIFAS Co-allocation data grid. We proposed a means of integrat-
ing a medical image file accessing system with a co-allocation
data grid to improvemedical image query, retrieval, exchange, and
download speeds. Our experiments showed ARAM to be the best
among the eight co-allocation schemes. We found that parallel
downloading via File Transfer Protocols yields better performance
than single-point downloading. ARAMalso overcomes the problem
of broken network links. It completes transfer jobs by continuing
from the previous point.
In our managerial experiments we used H-Box to build a
medical image storage grid node on a desktop PC and tested image
recoverywith it.We found that the recovery rate and building new
grid nodes are both better than making a new Web PACS. Anyone
can use H-Box to establish an open source medical image storage
environment, and use grid architecture to increase medical image
storage stability without incurring the high cost of a Web PACS.
Furthermore, we enhanced the security with a data grid au-
thentication environment: the User Certificate, the Private Key, the
Certificate Authority (CA) File, and the Proxy File. In conclusion,
Medical Image File Accessing in a co-allocation data grid pro-
vides users with a reliable and secure environment for processing
queries and medical image retrievals efficiently.
References
[1] V. Breton, R. Medina, J. Montagnat, Datagrid, prototype of a biomedical grid,
Methods of Information in Medicine 42 (2) (2003) 143–147.
[2] G.V. Koutelakis, D.K. Lymperopoulos, Member, IEEE, a grid PACS architecture:
providing data-centric applications through a grid infrastructure, in: Proceed-
ings of the 29th Annual International Conference of the IEEE EMBS Cite Inter-
national, Lyon, France, ISBN: 978-1-4244-0787-3, August 23–26, 2007.
[3] N.E. King, B. Liu, Z. Zhou, J. Documet, H.K. Huang, in: Osman M. Ratib, Steven
C. Horii (Eds.), The Data Storage Grid: The Next Generation of Fault-Tolerant
Storage for Backup and Disaster Recovery of Clinical Images Medical Imaging
2005: PACS and Imaging Informatics, in: Proceedings of SPIE, vol. 5748, 2005,
pp. 208–217.
[4] Z. Zhou, S.S. Chao, J. Lee, B. Liu, J. Documet, H.K. Huang, A data grid for imaging-
based clinical trials, in: Steven C. Horii, Katherine P. Andriole (Eds.), Medical
Imaging 2007: PACS and Imaging Informatics, in: Proc. of SPIE, vol. 6516, 2007,
p. 65160U.
[5] SYSSTAT utilities home page. http://perso.wanadoo.fr/sebastien.godard/.
[6] Chao-Tung Yang, Chun-Pin Fu, Ching-Hsien Hsu, File replication maintenance
and consistency management services in data grids, Journal of Supercomput-
ing (ISSN: 1573-0484) (2010).
[7] S. Vazhkudai, Enabling the co-allocation of grid data transfers, in: Proceedings
of Fourth International Workshop on Grid Computing, 17 November 2003,
pp. 44–51.
[8] L. Yang, J. Schopf, I. Foster, Improving parallel data transfer times using
predicted variances in shared networks, in: Proceedings of the Fifth IEEE
International SymposiumonCluster Computing and theGrid, CCGrid ’05, 9–12
May 2005, pp. 734–742.
[9] Chao-Tung Yang, Yao-Chun Chi, Ming-Feng Yang, Ching-Hsieh Hsu, An
anticipative recursively-adjusting mechanism for parallel file transfer in data
grids, Concurrency and Computation: Practice and Experience (ISSN: 1532-
0626) (2010).
[10] C.T. Yang, I.H. Yang, K.C. Li, S.Y. Wang, Improvements on dynamic adjustment
mechanism in co-allocation data grid environments, The Journal of Supercom-
puting, Springer 40 (3) (2007) 269–280.
[11] Ganglia. http://ganglia.sourceforge.net.
[12] Open Grid Forum. http://www.ogf.org/.
[13] The Globus Alliance. http://www.globus.org/.
[14] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster, C. Kesselman,
S. Meder, V. Nefedova, D. Quesnel, S. Tuecke, Data management and transfer
in high-performance computational grid environments, Parallel Computing 28
(5) (2002) 749–771.
[15] B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster, C. Kesselman,
S. Meder, V. Nefedova, D. Quesnel, S. Tuecke, Secure, efficient data transport
and replica management for high-performance data-intensive computing,
in: Proc. of the Eighteenth IEEE Symposium on Mass Storage Systems and
Technologies, 2001, pp. 13–28.
[16] B. Allcock, S. Tuecke, I. Foster, A. Chervenak, C. Kesselman, Protocols and
services for distributed data-intensive science, in: ACAT2000 Proceedings,
2000, pp. 161–163.
[17] A. Chervenak, E. Deelman, I. Foster, L. Guy, W. Hoschek, A. Iamnitchi,
C. Kesselman, P. Kunszt, M. Ripeanu, B. Schwarz, H. Stockinger, K. Stockinger,
B. Tierney, Giggle: a framework for constructing scalable replica location
services, in: Proc. of SC 2002, Baltimore, MD, 2002.
[18] W. Hoschek, J. Jaen-Martinez, A. Samar, H. Stockinger, K. Stockinger, Data
management in an international data grid project, in: Proceedings of the First
IEEE/ACM International Workshop on Grid Computing-Grid 2000, Bangalore,
India, ISBN: 3-540-41403-7, December 2000, pp. 77–90.
[19] S. Vazhkudai, J. Schopf, I. Foster, Predicting the performance of wide area data
transfers, in: Proceedings of the 16th International Parallel and Distributed
Processing Symposium, IPDPS 2002, April 2002, pp. 34–43.
CONCURRENCY AND COMPUTATION: PRACTICE AND EXPERIENCE
Concurrency Computat.: Pract. Exper. (2010)
Published online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/cpe.1571
RACAM: design and
implementation of a recursively
adjusting co-allocation method
with efficient replica selection
in Data Grids
Chao-Tung Yang∗,†, I-Hsien Yang and Chun-Hsiang Chen
High-Performance Computing Laboratory, Department of Computer Science,
Tunghai University, Taichung 40704, Taiwan
SUMMARY
Data Grids enable the sharing, selection, and connection of a wide variety of geographically distributed
computational and storage resources for addressing large-scale data-intensive scientific application needs
in, for instance, high-energy physics, bioinformatics, and virtual astrophysical observatories. Data sets
are replicated in Data Grids and distributed among multiple sites. Unfortunately, data sets of interest
sometimes are significantly large in size, and may cause access efficiency overhead. A co-allocation archi-
tecture was developed in order to enable parallel downloading of data sets from multiple servers. Several
co-allocation strategies have been coupled and used to exploit download rate by specifying among various
client–server divides files into multiple blocks of equal sizes to link and address dynamic rate fluctuations.
However, one major obstacle, the idle time of faster servers having to wait for the slowest server to
deliver the final block, makes it important to reduce differences in finishing time among replica servers.
In this paper, we propose a dynamic co-allocation method, called Recursively Adjusting Co-Allocation
Method (RACAM), to improve the performance of parallel data file transfer. Our approach reduces the
idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide
an effective scheme for reducing the cost of reassembling data blocks. Copyright © 2010 John Wiley &
Sons, Ltd.
Received 22 August 2008; Revised 25 June 2009; Accepted 30 January 2010
KEY WORDS: Grid computing; Data Grid; replica selection; co-allocation; data transfer; Globus toolkit;
GridFTP; parallel transfer
∗Correspondence to: Chao-Tung Yang, High-Performance Computing Laboratory, Department of Computer Science, Tunghai
University, Taichung 40704, Taiwan.
†E-mail: ctyang@thu.edu.tw
Contract/grant sponsor: National Science Council; contract/grant numbers: NSC 97-2622-E-029-003-CC2, NSC 98-2622-
E-029-001-CC2, NSC 98-2218-E-007-005
Copyright q 2010 John Wiley & Sons, Ltd.
DESIGN AND IMPLEMENTATION OF AN RACAM
Then, a GridFTP client tool use the Java programming language and Java CoG kit [22]. We use CoG
API to submit the instructions for partial file transfer. The input parameters include the start offset
and the length of the file. Many applications require the transfer of portions rather than complete
files. GridFTP introduces new FTP commands to support transfers of subsets or regions of a file.
We used these features in the client tool. Therefore, our GridFTP client tool can start transfer with
a file offset and then read a certain number of bytes. Experimental results show that our approach
is superior to the previous methods and achieved the best overall performance. We also discuss
combination cost and provide an effective scheme for reducing it.
The remainder of this paper is organized as follows. Related background review and studies are
presented in Section 2. The co-allocation architecture and related work are introduced in Section 3.
In Section 4, an efficient replica selection service is proposed by us. Our research approaches are
outlined in Section 5, and experimental results and a performance evaluation of our scheme are
presented in Section 6. Section 7 presents our conclusions and future work.
2. BACKGROUND
Data Grids enable the sharing, selection, and connection of a wide variety of geographically
distributed computational and storage resources for solving large-scale data-intensive scientific
applications (e.g. high-energy physics, bioinformatics applications, and astrophysical virtual obser-
vatory) [1,2,12,23]. The term ‘Data Grid’ traditionally represents the network of distributed storage
resources, from archival systems to caches and databases, which are linked using a logical name
space to create global, persistent identifiers and provide uniform access mechanisms [3].
2.1. Replica management
Replica management involves creating or removing replicas at a Data Grid site [10]. A replica
manager typically maintains a replica catalog containing replica site addresses and the file instances
[10]. The replica management service is responsible for managing the replication of complete and
partial copies of data sets, defined as collections of files.
A Data Grid may contain multiple replica catalogs. For example, a community of researchers
interested in a particular research topic might maintain a replica catalog for a collection of data sets
of mutual interest. It is possible to create hierarchies of replica catalogs to impose a directory-like
structure on related logical collections. In addition, the replica manager can perform access control
on entire catalogs as well as on individual logical files.
The replica management service is just one component in a Data Grid environment that provides
support for high-performance, data-intensive applications. A replica or location is a subset of a
collection that is stored on a particular physical storage system. There may be multiple possibly
overlapping subsets of a collection stored on multiple storage systems in a Data Grid. These Grid
storage systems may use a variety of underlying storage technologies and data movement protocols,
which are independent of replica management.
2.2. Replica catalog
As mentioned above, the purpose of the replica catalog is to provide mappings between logical
names for files or collections and one or more copies of the objects on physical storage systems.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
2.4. Globus toolkit and GridFTP
In this work, we used the grid middleware Globus Toolkit [4,6,23–25] as the Data Grid infras-
tructure. The Globus Toolkit provides solutions for grid as security, resource management, data
management, and information services. One of its primary components is MDS [11,23,25,26],
which is designed to provide a mechanism for discovering and publishing resource status and
configuration information. It provides a uniform and flexible interface for data collected by lower-
level information providers in two modes: static (e.g. OS, CPU types, and system architectures)
and dynamic data (e.g. disk availability, memory availability, and system load). The Globus Toolkit
makes it easier to build a grid and its grid-based applications. Many organizations use the Globus
Toolkit to build computational grids or Data Grids to support their applications [27].
The Globus alliance proposed a common data transfer and access protocol called GridFTP
[12,23–25,28,29] that provides secure, efficient data movement in grid environments. This protocol,
which extends the standard FTP protocol, provides a superset of the features offered by the various
grid storage systems currently in use. GridFTP is a high-performance, secure, reliable data transfer
protocol optimized for high-bandwidth wide-area networks. Among its many features are security,
parallel streams, partial file transfers, third-party transfers, and reusable data channels. There is
another key technology from Globus project, called replica catalog [23], which is used to register
and manage complete and partial copies of data sets. The replica catalog contains the mapping
information from a logical file or collection to one or more physical files.
2.5. Java commodity kit
The Java CoG (Commodity of Grid) Kit [22] provides access to Grid services through Java
via higher-level framework. Components providing client and limited server side capabilities are
included. The Java CoG Kit provides a framework for utilizing the many Globus services as part of
the Globus toolkit. Many of the classes are provided as pure Java implementations. Thus, writing
client-side applets without installing the Globus toolkit is possible. The Java CoG Kit combines
Java technology with Grid Computing to develop advanced Grid Services and accessibility to basic
Globus resources. It allows easier and more rapid application development by encouraging collabo-
rative code reuse and avoiding duplication of effort among problem-solving environments, science
portals, Grid middleware, and collaborative pilots.
2.6. Network weather service
The network weather service (NWS) [30] is a generalized and distributed monitoring system for
producing short-term performance forecasts based on historical performance measurements. The
goal of the system is to dynamically characterize and forecast the performance deliverable at
the application level from a set of network and computational resources. It is composed of three
component processes:
• nws nameserver, a naming and discovery service used to manage an nws sensor and
nws memory system,
• nws memory, which provides persistent storage for the measurement data collected by the
NWS deployment,
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
The standard data transfer and access protocols most commonly used on the Internet, FTP, and
HTTP lack key features necessary for Grid applications. Many storage archive systems (HPSS,
DPSS, SRB, etc.) have implemented specialized interfaces to provide additional features, but only
the APIs are available, not the underlying protocols. Thus interoperability between such systems is
problematic [2]. In this research, we use GridFTP [12,23,25] to enable parallel data transfers. Its
partial file transfer ability allows files to be retrieved from data servers by specifying the start and
end offsets of file sections.
As data sets are replicated within Grid environments for reliability and performance, clients
require the abilities to discover existing data replicas, and create and register new replicas. A
replica location service (RLS) [3] provides a mechanism for discovering and registering existing
replicas. Several prediction metrics have been developed to help replica selection. For instance,
Vazhkudai et al. [33–35] used past data transfer histories including the estimate of current data
transfer throughputs.
In our previous work [18,32], we proposed a replica selection cost model and a replica selection
service to perform replica selection. In [9], the author proposes a co-allocation architecture for co-
allocating Grid data transfers across multiple connections by exploiting the partial copy feature of
GridFTP. It also provides Brute-Force, History-Base, and Dynamic Load Balancing for allocating
data block.
• Brute-Force Co-Allocation: Brute-Force Co-Allocation (Figure 2) works by dividing files
equally among ‘n’ available flows (locations). Thus, if the data to be fetched is of size, ‘S’
and there are ‘n’ locations to fetch it from, then this technique assigns to each flow a data
block of size, ‘S/n’. For example, if there are three sources, the target file will be divided
into three blocks equally. And each source provides one block for the client. With this tech-
nique, although all the available servers are utilized, bandwidth differences among the various
client–server links are not exploited.
• History-based Co-Allocation: The History-based Co-Allocation (Figure 3) scheme keeps block
sizes per flow proportional to transfer rates predicted by the previous results of file transfer
results. In history-based allocation scheme, the block size per flow is commensurate to its
predicted transfer rate, decided based on a previous history of GridFTP transfers. Thus, the
file-range distribution is based on the predicted merit of the flow. If these predictions are
not accurate enough, renegotiations of flow sizes might be necessary as slower links can get
assigned to larger portions of data, which could weight heavily on the eventual bandwidth
achieved. With the history-based approach, client divides the file into ‘n’ disjoint blocks,
corresponding to ‘n’ servers. Each server ‘i’, 1≤ i ≤n, has a predicted transfer rate of ‘Bi ’ to
the client. In theory then, the aggregate bandwidth ‘A’ achievable by the client for the entire
download is A=∑i=ni=1 Bi . For each server ‘i’, 1≤ i ≤n, and for the data to be fetched of size
‘S’, the block size per flow is Si = Bi/A×S.
• Conservative Load Balancing: One of their dynamic co-allocation is Conservative Load
Balancing (Figure 4). The Conservative Load Balancing dynamic co-allocation strategy divides
requested data sets into ‘k’ disjoint blocks of equal size. Available servers are assigned single
blocks to deliver in parallel. When a server finishes delivering a block, another is requested, and
so on, till the entire file is downloaded. The loadings on the co-allocated flows are automatically
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
Client
File Server 1
File Server 2
File Server 3
Figure 4. The process of Conservative Load Balancing.
The co-allocation strategies described above do not handle the shortcoming of faster servers
having to wait for the slowest server to deliver its final block. In most cases, this wastes much time
and decreases the overall performance. Thus, we propose an efficient approach called RACAM
based on a co-allocation architecture. It improves dynamic co-allocation and reduces waiting time,
thus improving overall transfer performance.
4. AN EFFICIENT REPLICA SELECTION SERVICE
We constructed a replica selection service to enable clients to select the better replica servers in
Data Grid environments.
4.1. Replica selection scenario
Our proposed replica selection model is illustrated in Figure 5, which shows how a client identifies
the best location for a desired replica transfer. The client first logins at a local site and executes
the Data Grid platform application, which checks to see if the files are available at the local site.
If they are present at the local site, the application accesses them immediately; otherwise, it passes
the logical file names to the replica catalog server, which returns a list of physical locations for all
registered copies. The application passes this list of replica locations to a replica selection server,
which identifies the storage system destination locations for all candidate data transfer operations.
The replica selection server sends the possible destination locations to the information server,
which provides performance measurements and predictions of the three system factors described
below. The replica selection server chooses better replica locations according to these estimates and
returns location information to the transfer application, which receives the replica through GridFTP.
When the application finishes, it returns the results to the user.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
[36] utilities. The Sysstat [36] utilities are a collection of performance monitoring tools for the
Linux OS. The Sysstat package incorporates the sar, mpstat, and iostat commands. The
sar command collects and reports system activity information, which can also be saved in a
system activity file for future inspection. The iostat command reports CPU statistics and
I/O statistics for tty devices and disks. The statistics reported by sar concern I/O transfer
rates, paging activity, process-related activities, interrupts, network activity, memory and swap
space utilization, CPU utilization, kernel activities, and tty statistics, among others.
4.3. Our replica selection cost model
The target function of a cost model for distributed and replicated data storage is the information
score from the information service. We listed some influencing factors for our cost model in the
preceding section. However, we must express these factors in mathematical notation for further
analysis. We assume that node i is the local site the user or application logs in, and node j possesses
the replica the user or application wants. The seven system parameters our replica selection cost
model considers are
• Scoreij: the score value represents how efficiently a user or application at node i can acquire
a replica from node j.
• P BWij : percentage of bandwidth available between node i and node j ; current bandwidth
divided by highest theoretical bandwidth.
• W BW : network bandwidth weight defined by the Data Grid administrator.
• PC PUj : percentage of CPU idle states of replica node j .
• W C PU : CPU load weight defined by the Data Grid administrator.
• P I/Oj : percentage of I/O idle states of replica node j .
• W I/O : I/O state weight defined by the Data Grid administrator.
We define the following general formula using these system factors.
Scoreij = PBWij ×W BW + PCPUj ×W CPU + P I/Oj ×W I/O (1)
The three influencing factors in this formula: W BW , W CPU , and W I/O . They describe CPU, I/O,
and network bandwidth weights, which can be determined by Data Grid organization administrators
according to the various attributes of the storage systems in Data Grid nodes since some storage
equipment does not affect CPU loading. After several experimental measurements, we determined
that network bandwidth is the most significant factor directly influencing data transfer times. When
we performed data transfers using the GridFTP protocol, we discovered that CPU and I/O statuses
slightly affect data transfer performance. Their respective values in our Data Grid environment are
80, 10, and 10% [21].
We implemented a replica selection cost model computer program using these factors and
executed the program on our Data Grid testbed. Because the program was developed using the Java
CoG Kit [22] and Java programming language, we can execute it on any computing platform using
JVM platform. Figure 6(a) shows costs calculated according to the three system factors (CPU idle
percentage, I/O idle, and bandwidth from other sites) to alpha1. Figure 6(b) shows the average
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
Client
Key
File Server 3
File Server 1
File Server 2
Key
Key
Figure 7. The client must authenticate itself to the GridFTP server.
Client
File Server 3
File A
File A
File A
File Server 2
File Server 1File A1
File A2
File A3
File A4
File A5
Figure 8. Transmission time.
File A1 File A2 File A3 File A4 File A5File A
Figure 9. Combination time.
5. DYNAMIC CO-ALLOCATION STRATEGY
Dynamic co-allocation, described above, is the most efficient approach to reducing the influence of
network variations between clients and servers. However, the idle time of faster servers awaiting
the slowest server to deliver the last block is still a major factor affecting overall efficiency, which
Conservative Load Balancing and Aggressive Load Balancing [9] cannot effectively avoid. The
approach proposed in the present paper, a dynamic allocation mechanism called RACAM, can
overcome this, and thus, improve data transfer performance.
5.1. Recursively adjusting co-allocation method
RACAM works by continuously adjusting each replica server’s workload to correspond to its real-
time bandwidth during file transfers. The goal is to make the expected finish time of all the servers
the same. As Figure 10 shows, when an appropriate file section is first selected, it is divided into
proper block sizes according to the respective server bandwidths. The co-allocator then assigns the
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
performance matrix. The co-allocation service gets this list of selected replica servers. Assuming that
n replica servers are selected, Si denotes server ‘i’ such that 1in. A connection for file
downloading is then built to each server. The recursively adjusting process is listed as follows.
A new section of a file to be allocated is first defined. The section size, ‘S E j ’, is
SE j =UnassignedFileSize × (0<<1) (2)
where S E j denotes the section j such that 1 jk, assuming we allocate k times for the download
process. And thus, there are k sections, whereas Tj denotes the time section j allocated. Unassigned-
FileSize is the portion of file A not yet distributed for downloading; initially, UnassignedFileSize
is equal to the total size of file A.  is the rate that determines how much of the section remains to
be assigned.
In the next step, S E j is divided into several blocks and assigned to ‘n’ servers. Each server has
a real-time transfer rate to the client of Bi , which is measured by the NWS [33]. The block size
per flow from S E j for each server ‘i’ at time Tj is
Si =
(
S E j +
n∑
i=1
UnFinishSizei
)
× Bi
/
n∑
i=1
Bi −UnFinishSizei (3)
where UnFinishSizei denotes the size of unfinished transfer blocks that is assigned in previous
rounds at server ‘i’. UnFinishSizei is equal to zero in first round. Ideally, depending on the real
time bandwidth at time Tj , every flow is expected to finish its workload in future.
This fulfills our requirement to minimize the time faster servers must wait for the slowest server
to finish. If, in some cases, network variations greatly degrade transfer rates, UnFinishSizei may
exceed (S E j +∑ni=1 UnFinishSizei )× Bi/∑ni=1 Bi , which is the total block size expected to be
transferred after Tj . In such cases, the co-allocator eliminates the servers in advance and assigns
S E j to other servers. After allocation, all channels continue transferring data blocks. When a faster
channel finishes its assigned data blocks, the co-allocator begins allocating an unassigned section
of file A again. The process of allocating data blocks to adjust expected flow finish time continues
until the entire file has been allocated.
5.2. Determining when to stop continuous adjustment
Our approach gets new sections from whole files by dividing unassigned file ranges in each round
of allocation. These unassigned portions of the file ranges become smaller after each allocation.
Since adjustment is continuous, it would run as an endless loop if not limited by a stop condition.
However, when is it appropriate to stop continuous adjustment? We provide two monitoring
criteria, LeastSize and ExpectFinishedTime, to enable users to define stop thresholds. When a
threshold is reached, the co-allocation server stops dividing the remainder of the file and assigns that
remainder as the final section. The LeastSize criterion specifies the smallest file we want to process,
and when the unassigned portion of UnassignedFileSize drops below the LeastSize specification,
division stops. ExpectFinishedTime criterion specifies the remaining time transfer is expected to
take. When the expected transfer time of the unassigned portion of a file drops below the time
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
Selected nodes
Data Grid
GateKeeper
GateKeeper
1. Create Proxy
2. Request connect
4. Search replica
5. Select replica
6. Request transfer
7. Transfer complete
6.3. Dynamic transfer
6.5. Start transfer 
6.4. Request 
transfer
6.1. Query information service
6.2. Report back
3. Authentication
2.1. Connect
4.1. Search replica 
information
4.2. Report back
Figure 12. The transaction flow of RACAM.
6. EXPERIMENTAL RESULTS AND ANALYSIS
In this section, we discuss the performance of our Recursively Adjusting Co-Allocation strategy.
We evaluate four co-allocation schemes: (1) Brute-Force (Brute), (2) History-based (History), (3)
Conservative Load Balancing (Conservative), and (4) RACAM (Recursive). We analyze the perfor-
mance of each scheme by comparing their transfer finish time, and the total idle time faster servers
spent waiting for the slowest server to finish delivering the last block. We also analyze the overall
performances in the various cases. In [17,18], we have conducted some simulations that show our
RACAM schemes can improve the performance on parallel file downloading. In this section, we
will perform these experimentations on a real data grid.
We performed wide-area data transfer experiments using our GridFTP GUI client tool. We
executed our co-allocation client tool on our testbed at THU, Taichung City, Taiwan, and fetched
files from four selected replica servers: one at Providence University (PU), one at Li-Zen High
School (LZ), one at Hsiuping Institute of Technology School (HIT), and one at DALI. All these
institutions are in Taiwan, and each is at least 10 km from THU. Figure 13 shows our Data Grid
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
Figure 14. Our GridFTP client tool.
GridFTP with Parallel Data Transfer
29
.
08 6
6.
89
14
0.
50
26
8.
46
28
.
52 6
2.
28
13
4.
51
25
3.
53
28
.
08 6
0.
34
13
0.
09
23
8.
59
28
.
89 5
8.
77
12
6.
24
23
2.
24
28
.
83 56
.
54
11
6.
58
22
8.
00
56
.
32
11
1.
17
22
0.
80
28
.
99
0.00
50.00
100.00
150.00
200.00
250.00
300.00
256 512 1024 2048
File Sizes (MB)
Fi
le
 
Tr
a
n
sf
e
r 
Ti
m
e
 
(se
c)
GridFTP with no Parallel
Data Transfer
GridFTP with 1 TCP
Stream
GridFTP with 2 TCP
Streams
GridFTP with 4 TCP
Streams
GridFTP with 8 TCP
Streams
GridFTP with 16 TCP
Streams
Figure 15. GridFTP with parallel data transfer.
The parallelism option is used by the source data note to control how many parallel data connec-
tions may be established to each destination data node. Figure 15 shows the performance of GridFTP
transferring 256, 512, 1024, and 2048 MB files with 1, 2, 4, 8, and 16 TCP streams from THU
site alpha02 to Li-Zen site lz04. According to the experiment result, we observed that parallel
data transfer technique showed better performance for larger file sizes. Parallel data transfer really
improves aggregate bandwidth, with the establishment of multiple data channels.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
0
10
20
30
40
50
60
70
80
90
100
500 1000 1500 2000
File Size (MB)
Co
m
bi
na
tio
n 
Ti
m
e 
(S
ec
)
Brute3 History3 Conservative3 Recursive3
Figure 17. Combination times for various methods; servers are at PU, DL, and HIT.
0
100
200
300
400
500
600
PU
1
D
L1
H
IT
1
BR
U3
H
IS
3
CO
N3
R
EC
3
PU
1
D
L1
H
IT
1
BR
U3
H
IS
3
CO
N3
R
EC
3
PU
1
D
L1
H
IT
1
BR
U3
H
IS
3
CO
N3
R
EC
3
PU
1
D
L1
H
IT
1
BR
U3
H
IS
3
CO
N3
R
EC
3
500 1000 1500 2000
File Size (MB)
Co
m
pl
et
io
n 
Ti
m
e 
(S
ec
)
Authentication Time Transmission Time Combination Time
Figure 18. Completion times for various methods; servers are at PU, DL, and HIT.
the other co-allocation strategies. Thus, we may infer that the main gains our technology offers are
lower transmission and combination times than other co-allocation strategies.
In the next experiment, we used the Recursively Adjusting Co-Allocation strategy with various
sets of replica servers and measured overall performances, where overall performance is
Total Performance=File size/Total Completion Time (5)
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
0
10
20
30
40
50
60
70
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
R
EC
3_
2
R
EC
4
10 50 100 500 1000 1500 2000
File Size (MB)
O
ve
ra
ll 
Pe
rfo
rm
a
n
ce
 (M
bit
s)
Authentication Time Transmission Time Combination Time
Figure 20. Detailed cost structure view for the case of REC3 2 and the case of REC4.
0
50
100
150
200
250
300
LZ
1
R
EC
2_
2
LZ
1
R
EC
2_
2
LZ
1
R
EC
2_
2
LZ
1
R
EC
2_
2
500 1000 1500 2000
File Size (MB)
Co
m
pl
et
io
n 
Ti
m
e 
(S
ec
)
Authentication Time Transmission Time Combination Time
Figure 21. A single fast server may perform better than co-allocation technology;
REC2 2 denotes servers are at LZ and TC.
transmission; however, the additional overhead of reassembling blocks decreased overall perfor-
mance. The results indicate that when replica servers in specific Data Grid environments have higher
transfer abilities than the highest point of co-allocation efficiency, using co-allocation technology
is not suitable.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
DESIGN AND IMPLEMENTATION OF AN RACAM
9. Vazhkudai S. Enabling the co-allocation of grid data transfers. Proceedings of Fourth International Workshop on Grid
Computing, Phoenix, AZ, U.S.A., 17 November 2003; 41–51.
10. Vazhkudai S, Tuecke S, Foster I. Replica selection in the globus data grid. Proceedings of the First International
Symposium on Cluster Computing and the Grid (CCGRID 2001), Brisbane, Australia, 15–18 May 2001; 106–113.
11. Zhang X, Freschl J, Schopf J. A performance study of monitoring and information services for distributed systems.
Proceedings of 12th IEEE International Symposium on High Performance Distributed Computing (HPDC-12 ’03), Seattle,
WA, 22–24 June 2003; 270–282.
12. Allcock B, Bester J, Bresnahan J, Chervenak A, Foster I, Kesselman C, Meder S, Nefedova V, Quesnel D, Tuecke S. Data
management and transfer in high-performance computational grid environments. Parallel Computing 2002; 28(5):749–771.
13. Chervenak A, Foster I, Kesselman C, Salisbury C, Tuecke S. The data grid: Towards an architecture for the distributed
management and analysis of large scientific datasets. Journal of Network and Computer Applications 2001; 23:187–200.
14. Koo SGM, Rosenberg C, Xu D. Analysis of parallel downloading for large file distribution. Proceedings of the Ninth
IEEE Workshop on Future Trends of Distributed Computing Systems (FTDCS ’03), San Juan, Puerto Rico, 28–30 May
2003; 128–137.
15. Rodriguez P, Biersack EW. Dynamic parallel access to replicated content in the internet. IEEE/ACM Transactions on
Networking 2002; 10(4):455–465.
16. Tian Y, Wu D, Ng KW. Analyzing multiple file downloading in BitTorrent. Proceedings of the 2006 International
Conference on Parallel Processing (ICPP’06), Columbus, OH, U.S.A., 14–18 August 2006; 297–306.
17. Funasaka J. An analysis on adaptive parallel downloading method. Proceedings of the 19th International Conference on
Advanced Information Networking and Applications (AINA’05), Taipei, Taiwan, vol. 1, 28–30 July 2005; 835–840.
18. Yang C-T, Yang I-H, Wang S-Y, Hsu C-H, Li K-C. A recursively-adjusting co-allocation scheme with a cyber-transformer
in data grids. Future Generation Computer Systems 2009; 25(7):695–703.
19. Yang C-T, Yang I-H, Li K-C, Wang S-Y. Improvements on dynamic adjustment mechanism in co-allocation data grid
environments. The Journal of Supercomputing 2007; 40(3):269–280.
20. Yang C-T, Yang I-H, Chen C-H, Wang S-Y. Implementation of a dynamic adjustment mechanism with efficient replica
selection in co-allocation data grid environments. Proceedings of the 21st Annual ACM Symposium on Applied Computing
(SAC 2006)—Distributed Systems and Grid Computing Track, Dijon, France, 23–27 April 2006; 797–804.
21. Yang C-T, Wang S-Y, Chu WC. Implementation of a dynamic adjustment strategy for parallel file transfer in co-allocation
data grids. Journal of Supercomputing 2009. DOI: 10.1007/s11227-009-0307-4.
22. von Laszewski G, Foster I, Gawor J, Lane P. Java CoG Kit, a Java commodity grid kit. Concurrency and Computation:
Practice and Experience 2001; 13(89):643–662.
23. The Globus Alliance. Available at: http://www.globus.org/ [5 March 2010].
24. Foster I, Kesselman C. Globus: A metacomputing infrastructure toolkit. Intlernational Journal of Supercomputer
Applications 1997; 11(2):115–128.
25. Global Grid Forum. Available at: http://www.ogf.org/ [5 March 2010].
26. Czajkowski K, Fitzgerald S, Foster I, Kesselman C. Grid information services for distributed resource sharing. Proceedings
of the 10th IEEE International Symposium on High-performance Distributed Computing (HPDC-10’01), San Francisco,
CA, U.S.A., 7–9 August 2001; 181–194.
27. The Globus Projects. Available at: http://www.globus.org/alliance/projects.php [5 March 2010].
28. Allcock W, Bresnahan J, Kettimuthu R, Link M, Dumitrescu C, Raicu I, Foster I. The globus striped GridFTP framework
and server. Proceedings of SC’05. ACM Press: New York, 2005. Available at: http://www.globus.org/alliance/publications/
papers/gridftp final.pdf [5 March 2010].
29. Globus Toolkit 4.2.1 GridFTP User’s Guide. Available at: http://www.globus.org/toolkit/docs/latest-stable/data/gridftp/
user/#gridftp-user-quickstart [5 March 2010].
30. Wolski R, Spring N, Hayes J. The network weather service: A distributed resource performance forecasting service for
metacomputing. Future Generation Computer Systems 1999; 15(5–6):757–768.
31. Czajkowski K, Foster I, Kesselman C. Resource co-allocation in computational grids. Proceedings of the Eighth IEEE
International Symposium on High Performance Distributed Computing (HPDC-8’99), Redondo Beach, CA, U.S.A., 3–6
August 1999.
32. Yang C-T, Chen C-H, Li K-C, Hsu C-H. Performance analysis of applying replica selection technology for data grid
environments. PaCT 2005 (Lecture Notes in Computer Science, vol. 3603). Springer: Berlin, September 2005; 278–287.
33. Vazhkudai S, Schopf J. Using regression techniques to predict large data transfers. International Journal of High
Performance Computing Applications (IJHPCA) 2003; 17:249–268.
34. Vazhkudai S, Schopf J. Predicting sporadic grid data transfers. Proceedings of 11th IEEE International Symposium on
High Performance Distributed Computing (HPDC-11 ‘02), Edinburgh, Scotland, 24–26 July 2002; 188–196.
35. Vazhkudai S, Schopf J, Foster I. Predicting the performance of wide area data transfers. Proceedings of the 16th
International Parallel and Distributed Processing Symposium (IPDPS 2002), Rhodes Island, Greece, 25–29 April 2002;
34–43.
36. SYSSTAT utilities home page. Available at: http://perso.wanadoo.fr/sebastien.godard/ [5 March 2010].
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
1. INTRODUCTION
Heretofore, Grid research has mainly focused on coordinating geographically distributed computing
resources to achieve high performance. Presently, requests to collaborate and share huge amounts
of widely distributed data are as important as sharing high-performance computing resources. Data
Grids gather distributed resources to solve large-size data set management problems, and enable
the selection, sharing, and connection of a wide variety of geographically distributed computational
and storage resources to deal with large-scale data-intensive application requests [1–8].
In order to manipulate huge amounts of data dependent on data file management systems to
replicate files, manage file transfers and access distributed data, Data Grid infrastructures integrate
data storage devices and data management services in Grid environments consisting of scattered
computing and storage resources, perhaps located in different countries/regions yet accessible by
users worldwide [9]. In Data Grid environments, large data sets are replicated and distributed to
multiple data sets/servers. Since these are significantly large in size, it usually brings the access
efficiency problems.
Replicating popular content in distributed data sets/servers is in widespread use [10–14].
Recently, large-scale, data-sharing scientific communities, such as [1,15], have come to use this
technology to replicate their large data sets over several sites. Downloading large data sets from
replica locations may result in varied performance rates because replica sites may have different
architectures, system loading, and network connectivity. Bandwidth quality is the most important
factor affecting internet transfers between clients and servers, and download speeds are bounded
by traffic congestion due to bandwidth limitations.
One method for improving download speeds uses replica selection techniques to determine the
best replica locations [12–14]. However, by downloading data sets from the single best server
often results in ordinary transfer rates, because bandwidth quality varies unpredictably due to the
shared nature of the Internet. Another method uses co-allocation [11] technology to download data.
Co-allocation architectures were developed to enable clients to download data from multiple loca-
tions by establishing multiple connections in parallel, thus improving performance as compared
with the single server case and alleviating the internet congestion problem [11].
In previous works [11,16], several co-allocation strategies were addressed. However, there are
still drawbacks in these approaches. As shown in [16,17], this may degrade network performance
by repeatedly transferring the same block. Hence, it is important to minimize the differences in
finishing time among different servers, and to prevent the same block from being transferred in
different links between servers and clients.
In this paper we propose the Anticipative Recursively Adjusting Mechanism (ARAM) , a dynamic
co-allocation scheme based on the co-allocation architecture for Grid environment file transfers. The
algorithm adjusts continuously the workload on each selected replica server according to the finish
rates for previously assigned transfers, and anticipates the bandwidth status for the next section. The
method reduces the total idle times spent waiting for the slowest server, simultaneously improving
file transfer performance. Our results show that this approach is superior to previous methods.
The remainder of this paper is organized as follows. Related background review and studies
are presented in Section 2, and the co-allocation architecture and related work are introduced in
Section 3. Our research approaches are outlined in Section 4, and experimental results and a perfor-
mance evaluation of our scheme are presented in Section 5. Section 6 concludes this research article.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
each containing a (possibly overlapping) subset of the files in the collection. Using multiple location
entries, users can easily register logical collections that span multiple physical storage systems.
Despite the benefits of registering and manipulating collections of files using logical collection
and location objects, users and applications may also want to characterize individual files. For this
purpose, the replica catalog includes optional entries that describe individual logical files. Logical
files are entities with globally unique names that may have one or more physical instances. The
catalog may optionally contain one logical file entry in the replica catalog for each logical file in a
collection.
2.2. Replica selection
Replica selection [19] is used to choose replicas from among the sites comprising a Data Grid. The
selection criteria depend on application characteristics. This mechanism enables Data Grid users
to easily manage replicas of data sets at their sites for better performance. Much effort has been
devoted to solving the replica selection problem. The replica selection process commonly consists
of three steps: data preparation, preprocessing, and prediction. When a user requests access to a
data set, the system determines an appropriate way to deliver the replica to the user. One issue
concerning replica selection, predicting transfer times, involves inspecting many characteristics and
is a complex piece of work.
In the situations where replicas are selected according to access times, Grid information services
can provide information about network performance and perhaps the ability to reserve network
bandwidth, whereas the metadata repository can provide information about file sizes. This enables
selectors to rank all of the existing replicas to determine which one will yield the fastest data access
time. Alternatively, selectors can consult the same information sources to determine whether there
is a storage system that would yield better performance if a replica was created on it. In deciding
where to create new copies, we consider these system factors: network bandwidth, CPU loading,
and I/O state, to calculate the number of replicas.
2.3. Globus toolkit and GridFTP
The Globus Project [19,20] provides software tools collectively called The Globus Toolkit that
makes it easier to build computational Grids and Grid-based applications. Many organizations
use the Globus Toolkit to build computational Grids to support their applications. The composi-
tion of the Globus Toolkit can be pictured as three pillars: Resource Management, Information
Services, and Data Management. Each pillar represents a primary component of the Globus
Toolkit and makes use of a common foundation of security. GRAM implements a resource
management protocol, MDS implements an information services protocol, and GridFTP imple-
ments a data transfer protocol. They all use the GSI security protocol at the connection layer
[20–22].
In Data Grid environments, access to distributed data is typically as important as access to
distributed computational resources. Distributed scientific and engineering applications require
transfers of large amounts of data between storage systems, and access to large amounts of data
generated by many geographically distributed applications and users for analysis and visualization,
among others. Unfortunately, the lack of standard protocols for transferring and accessing data in
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
Application
Information
Service
Co-allocator
Broker
GASSGASS GASS
DPSS HPSS UNIX FX
Data Access/Transport using GridFTP
Forecasts
Local Storage System
ClassAd / RSL
Queries
Information
Figure 1. The Data Grid co-allocation architecture.
multiple connections. Thus, it improves transfer performance and helps to alleviate Internet
congestion.
The architecture proposed in [11] consists of three main components: an information service, a
broker/co-allocator, and local storage systems. Figure 1 shows co-allocation of Grid Data trans-
fers, which is an extension of the basic template for resource management [32] provided by the
Globus Toolkit. Applications specify the characteristics of desired data and pass attribute descrip-
tions to a broker. The broker queries available resources and gets replica locations from the
Information Service [23] and Replica Management Service [12], then gets lists of physical file
locations.
In [11], the authors propose architecture for co-allocating Grid data transfers across multiple
connections by exploiting the partial copy feature of GridFTP. They supply strategies, such as Brute-
Force, History-based, and two Dynamic Load Balancing techniques, conservative and aggressive,
for allocating data blocks. Several co-allocation strategies presented in previous works are described
below.
• Multi-Source Data Transfer: Proposed in [3] provides an efficient data replication algorithm
for multi-source data transfers, whereby data replicas are assembled in parallel from multiple
distributed data sources.
• Tuned Conservative Scheduling Technique: In [8] is used to predict means and variations in
network performance that aid in making data selection decisions. This stochastic scheduling
technique adjusts the amounts of data fetched on links based on link performance and expected
variations in performance.
• Brute-Force Co-Allocation [11]: It divides file sizes equally among available flows; it does not
address bandwidth differences among various client–server links.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
to the anticipated bandwidth statuses. By continuously adjusting selected replica server workloads,
the ARAM scheme measures actual bandwidth performance during data file transfers and regulates
workloads by anticipating bandwidth statuses for subsequent transfers according to the finish rates
for previously assigned transfers.
The basic idea is to assign less work to selected replica servers on network links with
greater performance variability. Links with more bandwidth variation will have smaller
effective bandwidth, and the finish rates for previous assigned transfers will be smaller
as well.
The goal is to have the expected finished times of all servers be the same. Our approach performs
well, even when the links to selected replica servers are broken or idled. It also reduces the idle
time wasted waiting for the slowest server.
As appropriate file sections are selected, they are first divided into proper block sizes according
to the respective server bandwidths, previously assigned file sizes, and transfer finish rates.
Initially, the finish rate is set to 1. Next, the co-allocator assigns the blocks to selected replica
servers for transfer. At this moment, it is expected that the transfer finish times will be consistent
with E(t1).
However, since server bandwidths may fluctuate during segment deliveries, the actual completion
times may differ from the expected time E(t1) (solid lines in Figures 2 and 3). When the fastest
server finishes at time t1, the size of unfinished transfer blocks (italic blocks in Figures 2 and 3) is
measured to determine the finish rate.
Two outcomes are possible: the quickest server finish time t1 may be slower than or equal to the
expected time, E(t1), indicating that network link performance remained unchanged or declined
during the transfer. In this case, the difference in transferred size between the expected time and
actual completion time (italic block in Figure 2) is then calculated.
The other outcome is that the quickest server finish time t1 may be faster than the expected time,
E(t1), indicating an excessively pessimistic anticipation of network performance, or an improvement
in replica server network link performance during the transfer. The difference in transferred size
between the earliest and the expected time (italic block in Figure 3) is then measured. If the
anticipated network performance was excessively pessimistic for the previous transfer, it is adjusted
for the next section.
The next task is to assign proper block sizes to the servers along with respective bandwidths and
previous finish rates, enabling each server to finish its assigned workload by the expected time,
E(t2). These adjustments are repeated until the entire file transfer is finished. Figure 4 illustrates
the ARAM processing.
3.3. Parameters and evaluation model
Here we introduce the affect parameters, and define the measurable parameters and evaluation
models we used to measure the performance of the ARAM. We define the parameters of our strategy
as follows:
• A: User-requested file,
• n: Selected replica servers,
• : Rate determining how much of the section remains to be assigned,
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
Define new section to be allocated
SEj = (UnassignedFileSize + Total
UnfinishedFileSize)
Allocate blocks to each selected
replica server
according as Bandwidth,
PreviousFinishedRate
 
Monitor each download flow
The fastest flow finishs its
assigned data blocks
If
UnassignedFileSize + Total
UnfinishedFileSize > 0
End
Define  final section
SEj = UnassignedFileSize + Total 
UnfinishedFileSize
Measure the finished rate of
previous assigned
(0 ≤ PreviousFinishedRate ≤ 1)
Monitor every selection replica
server
IFUnassignedFileSize + Total
UnfinishedFileSize / Total
EnabledTransferBandwidth > 1
Start
False
True
True
False
True
False
≤1)(0<
IF ((UnassignedFileSize + Total
UnfinishedFileSize)    )          
EnabledTransferBandwidth > 1
/ Total
Figure 4. Anticipative Recursively Adjusting Mechanism Transaction Flow.
The ARAM process is as follows. A new section of a file to be allocated is first defined. The
section size
S E j = (UnassignedFileSize+TotalUnfinishedFileSize)∗, 0<≤1 (1)
where SEj denotes section j such that 1 jk, assuming that k time is allocated for
the download process and there are k sections, whereas Tj denotes the time allocated for
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
Table I. Selected replica server bandwidth statuses.
Time
Server 1 2 3 4 5 6
A 10 10 10 10 20 30 . . .
B 20 10 0 10 10 20
C 30 30 30 30 20 10
D 40 40 40 30 30 30
Table II. Round 1 execution results.
Time (seconds) 0 1 2 3 4 5 6 ...
A
20 10 0
B
Round 1
C
40 20 10
60 30 0
80 40 0
D
20
40
60
80
Transfer
Rate
1
0.75
1
1
of file A. The data block allocation process continues adjusting workflows until the entire file has
been allocated.
3.4. An example
Assume data set file A of 50 MB, replicated in selected replica servers a to d, is requested, the
server bandwidth statuses are as listed in Table I, and the  is 0.5. Initially, r0a, r0b, r0c, and r0d
are 1 and SE1 is 25 MB. The transfers during Round 1 listed in Table II finish at Second 2. Replica
server b had poor bandwidth during transfer; hence, its UnfinishedFileSize is 10M bits and its r1b
is 0.75.
During Round 2, shown in Table III, the link to replica server b was broken; hence, S2b is
0. Round 2 finished at Second 4 and since the bandwidths of the selected replica servers were
unchanged, Total UnfinishedFileSize is equal to zero. In Round 3, listed in Table IV, file A finished
downloading and the round stopped at Second 6. Statuses during the execution rounds are listed in
Table V.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
3.5. The algorithm
The ARAM algorithm is given as follows:
[Initialization]
Define new section to be allocated
[Allocation of blocks to the selected servers]
LOOP WHEN (The UnassignedFileSize and Total UnfinishedFileSize are greater than zero)
{
IF (The UnassignedFileSize and Total UnfinishedFileSize are greater than Total Enabled Transfer
Bandwidth) THEN
{
IF (The UnassignedFileSize and Total UnfinishedFileSize multiplied by  are greater than
Total Transfer Bandwidth) THEN { Define new section to be allocated }
}
ELSE
{
Define final section
SE j = UnassignedFileSize + TotalUnfinishedFileSize
}
Step 1. Define new section to be allocated SEj
Step 2. Monitor every selection replica server
Step 3. Allocate blocks to each selected replica server, according to the Bandwidth
of the selected replica server Bji, and the Previous Finished Rate r j−1 of the
selected replica server (Initial r0 =1)
Step 4. Monitor each download flow
WHEN (the fastest flow finishes its assigned data blocks) THEN
IF (the first finished time of RTji is earlier than the expected time ETji and the transferred size
TSji is greater than the expected size Sji) THEN
{r j i =1}
ELSE {Measure the finished rate of the previous assigned file size to be delivered,
0≤r j i ≤1}
}
END LOOP;
4. EXPERIMENTAL RESULTS AND ANALYSIS
To evaluate the performance of the proposed technique, we have implemented the following co-
allocation scheme along with the ARAM: Brute-Force (Brute), History-based (History), Conser-
vative Load Balancing (Conservative), DCDA. We analyze the performance of each scheme by
comparing transfer finish times, and the total idle times faster servers spent waiting for slower
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
0
100
200
300
400
500
500 1000 1500 2000
Id
le
 W
ai
tin
g 
Ti
m
es
 (s
ec
on
ds
)
File Size (MB)
Brute History Conservative DCDA ARAM
Figure 6. Idle waiting times for various methods.
0
10
20
30
40
50
60
70
80
90
0 500 1000 1500 2000
Server1 Server2 Server3 Server4
Ba
nd
w
id
th
 (M
bp
s)
Time (seconds)
Figure 7. Network variations between client and servers in simulation
Using files of various sizes, we studied the effects of changing  values on block numbers and total
completion times for the ARAM. Figures 11 and 12 show that as the  value increased, completion
times and block numbers decreased for all file sizes.
When the  value was near 1, completion time performances were better whether the network
was stable or unstable because great increases in block numbers may cause high co-allocation costs.
When the network is unstable, more block numbers may result in more fragments from unfinished
transfer blocks assigned in previous rounds, and more blocks may cause communication latency
overhead and management problems.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
0
5
10
15
20
25
30
35
40
45
0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00
a Value
Bl
oc
k 
N
um
be
r
500M 1000M 1500M 2000M 2500M
3000M 3500M 4000M 4500M 5000M
Figure 10. Block numbers and  value for various file sizes.
0
100
200
300
400
500
600
700
0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00
a Value
Co
m
pl
et
io
n 
Ti
m
e 
(S
ec
)
500M 1000M 1500M 2000M 2500M
3000M 3500M 4000M 4500M 5000M
Figure 11.  value and completion times for various file sizes when network links were broken or idled.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
S3
Hsiuping Institute 
of Technology , HIT
S4
S5
S2
S6
lz1
lz2
lz3
lz4
Da-Li Senior 
High School , DL
tc1
tc2
tc3
tc4
Tunghai 
University , THU
delta
 x
 4
alpha
 x
 4
beta
 x
 4
gamma
 x
 4 zeta
 x
 5
eta
 x
 8
75
 Mbps
37 M
bps
16 Mbp
s
National 
Taichung 
University , 
NTCU
host101
host102
host103host104
23
 Mbps
36
 
M
bp
s
40 
Mb
ps
48
 M
bps
Lizen High 
School, LZ
TungGrid 4
TungGrid3
TungGrid 2
TungGrid 1
3.6
 
Mp
bs
S1
Tungs ’ Taichung 
MetroHarbor 
Hospital
2.8 Mpbs
2.1
 
M
pb
s
81
 Mbps
Figure 13. Our Data Grid test-bed.
Table VIII lists all the experiments we performed and the sets of replica servers used. The
results in Figure 15 show that using co-allocation technologies yielded no improvement for
smaller file sizes such as 10 MB. They also show that in most cases, overall performance
increased as the number of co-allocated flows increased. We observed that for our test-bed and
our co-allocation technology, overall performance reached its highest value in the ARAM 2
case. However, in the ARAM3 case, when we added one flow to the set of replica servers,
the performance did not increase. On the contrary, it decreased. We can infer that the co-
allocation efficiency reached saturation in the ARAM2 2 case, and that additional flows caused
additional overhead and reduced overall performance because the DL file site had worse
network bandwidth, and ARAM2 1 choosing DL for file transfer led to the differences between
ARAM2 1 and ARAM2 2. This means that more download flows do not necessarily result
in higher performance. We must choose appropriate numbers of flows to achieve optimum
performance.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
0
50000
100000
150000
200000
250000
10 50 100 500 1000 1500 2000
C
o
m
pl
et
io
n 
 T
im
e (
ms
)
File Size (MB)
Brute History Conservative DCDA ARAM
Figure 14. Completion times for various methods; servers are at DL, LZ, and HIT.
Table VIII. The sets of replica servers for all cases.
Case Replica servers
DL1 DL
LZ1 LZ
HIT1 HIT
TUNG1 TUNG
ARAM2 1 DL, LZ
ARAM2 2 HIT, LZ
ARAM2 3 TUNG, LZ
ARAM3 1 DL, LZ, HIT
ARAM3 2 DL, LZ, TUNG
The experiment results shown that when some grid node was disconnected at period time, the file
can be completely transformed. The ARAM scheme was designed to transform file base on current
network status; it can avoid the situation where the faster site waits for the slower site. The overall
transform file performance was still to keep in stability.
4.4. Experimental: Compare performance using ARAM and Video On Demand (VoD)
Protocol
In this experiment, we will use the best transfer method ARAM to do other comparison tests. When
demander needs to search and retrieve video content for movie and TV program cases, they would
need the files listed in Table IX. These files are usually video for large data set, or a series of
streaming cases. In order to compare the difference in the data retrieval performance of data retrieval
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
Table IX. Query and Retrieve for X-Ray, CT and MRI.
Image Query and Retrieve Data size
Video 640*480 ∼32MB
Video 1024*768 ∼65MB
Series of mp4 ∼79MB
Demander
GbE
GbE
VoD Services Grid
VoD Services
GbE
Data Flow A
Data Flow B
Figure 17. Our test-bed.
The time consumed for Data Flow A and B is shown in Figure 18. The result shows that the
performance in print-to-print query and retrieval of the first video data through B for all sizes is
better than A. The average transfer time of flow B is better than A.
5. CONCLUSIONS
Using a parallel-transfer approach to download data from multiple data sets/servers can enhance
server performance and reduce transfer times. The co-allocation architecture provides a coordinated
agent for assigning data blocks. In previous works, we showed that a dynamic co-allocation scheme
led to performance improvements, but could not handle the idle time of faster servers having to
wait for the slowest server to finish its last block; other schemes cannot avoid the degraded network
performance resulting from repeatedly transferring the same block.
The ARAM scheme is proposed in this study to improve the file transfer performance of the
co-allocation architecture in [30]. Our approach continuously adjusts the workload on each selected
replica server during file transfers according to the previously assigned file transfer finish rates,
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
C.-T. YANG ET AL.
9. Hoschek W, Jaen-Martinez J, Samar A, Stockinger H, Stockinger K. Data management in an international data grid
project. Proceedings of the First IEEE/ACM International Workshop on Grid Computing—Grid 2000, Bangalore, India,
December 2000.
10. Stockinger H, Samar A, Allcock B, Foster I, Holtman K, Tierney B. File and object replication in data grids. Journal
of Cluster Computing 2002; 5(3):305–314.
11. Vazhkudai S. Enabling the co-allocation of grid data transfers. Proceedings of Fourth International Workshop on Grid
Computing, Phoenix, AZ, U.S.A., 17 November 2003; 44–51.
12. Vazhkudai S, Tuecke S, Foster I. Replica selection in the globus data grid. Proceedings of the First International
Symposium on Cluster Computing and the Grid (CCGRID 2001), Brisbane, Australia, 15–18 May 2001; 106–113.
Available at: http://www.informatik.uni-trier.de/∼ley/db/conf/ccgrid/ccgrid2001.html.
13. Ranganathan K, Foster IT. Identifying dynamic replication strategies for a high-performance data grid. GRID 2001,
Denver, CO, U.S.A., 12 November 2001; 75–86. Available at: http://www.gridcomputing.org/grid2001/.
14. Ranganathan K, Foster I. Computation scheduling and data replication algorithms for data grids. Grid Resource
Management: State of the Art and Future Trends, Nabrzyski J, Schopf J, Weglarz J (eds.). Kluwer: Dordrecht, 2003.
15. Chervenak A, Foster I, Kesselman C, Salisbury C, Tuecke S. The data grid: Towards an architecture for the distributed
management and analysis of large scientific datasets. Journal of Network and Computer Applications 2001; 23:187–200
(based on conference publication from Proceedings of NetStore Conference 1999).
16. Bhuvaneswaran RS, Katayama Y, Takahashi N. Dynamic co-allocation scheme for parallel data transfer in grid
environment. Proceedings of First International Conference on Semantics, Knowledge, and Grid (SKG2005), Beijing,
China, 28–29 November 2005; 17. Available at: http://kg.ict.ac.cn/SKG2005/SKG2005.htm.
17. Bhuvaneswaran RS, Katayama Y, Takahashi N. A framework for an integrated co-allocator for data grid in multi-sender
environment. IEICE Transactions on Communications 2007; E90-B(4):742–749.
18. Donno F, Gaido L, Ghiselli A, Prelz F, Sgaravatto M. DataGrid Prototype 1. TERENA Networking Conference. Available
at: http://tnc2002.terena.org/Papers/p5a2-ghiselli.pdf [5 February 2009].
19. The Globus Alliance. Available at: http://www.globus.org/ [5 February 2010].
20. Open Grid Forum. Available at: http://www.ggf.org/ [5 February 2010].
21. Foster I, Kesselman C. Globus: A metacomputing infrastructure toolkit. International Journal of Supercomputer
Applications 1997; 11(2):115–128.
22. IBM Red Books. Introduction to Grid Computing with Globus. IBM Press. Available at:
http://www.redbooks.ibm.com/abstracts/sg246895.html?Open, 1 October 2003 [5 February 2010].
23. Czajkowski K, Fitzgerald S, Foster I, Kesselman C. Grid information services for distributed resource sharing. Proceedings
of the 10th IEEE International Symposium on High-performance Distributed Computing (HPDC-10’01), San Francisco,
CA, 7–10 August 2001; 181–194.
24. Zhang X, Freschl J, Schopf J. A performance study of monitoring and information services for distributed systems.
Proceedings of 12th IEEE International Symposium on High Performance Distributed Computing (HPDC-12 ‘03), Seattle,
WA, U.S.A., August 2003; 270–282. Available at: http://www-csag.ucsd.edu/HPDC-12/.
25. Yang CT, Yang IH, Chen CH, Wang SY. Implementation of a dynamic adjustment mechanism with efficient replica
selection in co-allocation data grid environments. Proceedings of the 21st Annual ACM Symposium on Applied Computing
(SAC 2006)—Distributed Systems and Grid Computing (DSGC) Track, vol. 1, Dijon, France, 23–27 April 2006; 797–804.
26. Chervenak A, Deelman E, Foster I, Guy L, Hoschek W, Iamnitchi A, Kesselman C, Kunszt P, Ripeanu M. Giggle:
A framework for constructing scalable replica location services. Proceedings of the 2002 ACM/IEEE Conference on
Supercomputing, Baltimore, MD, U.S.A., 16–22 November 2002; 1–17. Available at: http://www.sc-2002.org/.
27. Vazhkudai S, Schopf J. Using regression techniques to predict large data transfers. International Journal of High
Performance Computing Applications (IJHPCA) 2003; 17:249–268.
28. Vazhkudai S, Schopf J. Predicting sporadic grid data transfers. Proceedings of 11th IEEE International Symposium on
High Performance Distributed Computing (HPDC-11 ‘02), Edinburgh, Scotland, July 2002; 188–196.
29. Vazhkudai S, Schopf J, Foster I. Predicting the performance of wide area data transfers. Proceedings of the 16th
International Parallel and Distributed Processing Symposium (IPDPS 2002), Fort Lauderdale, FL, April 2002; 34–43.
Available at: http://www.ipdps.org/ipdps2002/.
30. Yang CT, Yang IH, Li KC, Wang SY. Improvements on dynamic adjustment mechanism in co-allocation data grid
environments. Journal of Supercomputing 2007; 40(3):269–280.
31. Yang CT, Chen CH, Li KC, Hsu CH. Performance analysis of applying replica selection technology for data grid
environments. PaCT 2005 (Lecture Notes in Computer Science, vol. 3603). Springer: Berlin, September 2005; 278–287.
32. Czajkowski K, Foster I, Kesselman C. Resource co-allocation in computational grids. Proceedings of the Eighth IEEE
International Symposium on High Performance Distributed Computing (HPDC-8’99), Redondo Beach, CA, 3–6 August
1999. Available at: http://www.ece.arizona.edu/∼hpdc/hpdc8/.
33. Chen CH, Yang CT, Lai CL. Towards an efficient replica selection for data grid. Proceedings of the First Workshop on
Grid Technologies and Applications (WoGTA’04), Hsinchu, Taiwan, 7–8 December 2004; 89–94.
Copyright q 2010 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. (2010)
DOI: 10.1002/cpe
412 C.-T. Yang et al.
physics in Europe [3] have come to require more and more computing power to
generate results. Those simulation results in turn produce terabytes, even petabytes
of data. Only computer centers with many supercomputers and storage devices are
sufficient to handle these data. However, data grid technologies, developed to solve
these kinds of problems, offer an effective alternative means of utilizing large-
scale computing power and storage capacities to compute and store data. Grids
[11, 12, 20, 21, 27, 29–34] enable sharing of computing power and storage capacities
geographically distributed around the world such that they work together as tremen-
dous virtual computers [1, 2] on experiments and simulations. The Globus Toolkit
[13, 27] is open-source software for building data grid environments. It provides mid-
dleware for creating information infrastructures including resource management, data
management, communication, fault detection, security, and portability.
Data replication and consistency [5, 26] refer to the same data being stored in
distributed sites, and kept consistent when one or more copies are modified. A good
file maintenance and consistency strategy can reduce file access times and access
latencies, and increase download speeds, thus reducing overall computing times. And
if one storage site breaks, users can fetch desired data from another storage site, which
improves overall fault tolerance and contributes to making the entire grid environment
more stable and reliable. Another advantage of data grids is the ability grid users
have to download data in parallel from the better sites they choose or an application
chooses automatically after evaluating with a grid environment evaluation model. The
bandwidth utilization of those parallel links is the most important factor affecting
overall download speeds. Network environments vary, which means that replica sites
also vary in their ability to download data efficiently. Replica files should be kept
consistent and downloaded from storage sites nearest users to reduce download times
and ensure high performance.
This paper presents two services, the Dynamic Maintenance Service (DMS) for
maintaining files in data grid systems, and the One-way Replica Consistency Service
(ORCS) for keeping all copies of files consistent when one is modified. The DMS
automatically maintains data statuses such as access frequency, space available on
storage elements to which data will be replicated or migrated, and the network sta-
tuses of file source sites to other sites. The ORCS provides asynchronous data repli-
cation and replica consistency mechanisms that can reduce replica maintenance costs
and free up storage space for new data or temporary data produced by experiments
or simulations to avoid creating too many identical replicas. Users can easily find the
best replica sites for downloading desired data, thus increasing replica usage rates,
and improving storage device usage efficiency ratios over other strategies.
The contribution of this paper is to help make data grid environments more effi-
cient by using the DMS and ORCS algorithms. Using DMS adjusts data to locations
appropriate to the sites that request the data more often, thus reducing the times re-
quired by those sites to get needed data and improving performance. Using ORCS
improves accessing performance by keeping replica content consistent and improving
data grid storage device usage ratios. The DMS and ORCS algorithms also consider
storage element free space when storing new and temporary data produced while
computing. This decreases the probability of applications crashing or having to re-
submit jobs to other computing resources for processing. Our experimental results
414 C.-T. Yang et al.
[22, 23], Rashedur M. Rahman et al., proposed a static replica placement algorithm
for placing replicas in the best p candidate nodes to minimize the total response time
of each node using Lagrangian relaxation, which is a heuristic approach [10] to mea-
suring the response time of each client node to its nearest server node. The algorithm
is most likely the p-median problem. They also use user requests and network la-
tency as parameters in deciding when to maintain replicas dynamically. They use a
simulator called OptorSim [19], developed by the EU Data Grid project, to compare
their method, and called dynamic p-median, with static p-median and Best_client.
Static p-median replicates no files to other nodes in the data grid environment when
user requests or network latency change. Best_client replicates the desired data to
client nodes when request ratios for certain files in a node are very high. Simula-
tion results show average response times for the authors’ method are the lowest over
various network loadings and user requests. Although dynamic p-median is a good
method for dynamic replica maintenance, it can’t be used in real grid environments
since p-median is an NP-hard problem that consumes too much computing power
determining new replica locations.
In [20], Sang-Min Park et al. proposed a dynamic replica maintenance algorithm
called Bandwidth Hierarchy based Replication (BHR) that divides sites into many
regions putting sites close to one another in the same regions in the bandwidth hier-
archy. The BHR optimizer terminates replication if a replica duplicate already exists
in another site in the same region. In [4], Ruay-Shiung Chang and Jih-Sheng Chang
indicated that the BHR algorithm performs better than other strategies only when the
storage element capacity is small. We found the following problem in BHR: If a file
must be replicated in a region, BHR replicates it to one other site in the region. If an
attempt is made to replicate the same file to a third site in the same region, BHR will
see that there is already a duplicate file in the region and terminate. Thus, files will
have at most two copies in each region, which means only two links will be available
for parallel data downloading [1, 2, 30–34] in any one region. This limitation leads to
high time costs, thus reducing the effectiveness of an important grid computing fea-
ture and adversely affecting overall performance. Furthermore, the two-copy practice
will cause load imbalances on the sites where the copies are stored.
2.2 Replica consistency
Grid environment files modified by grid users raise the critical problem of maintain-
ing data consistency among the replicas distributed across various machines. Over the
past decade, considerable effort has been devoted to developing several consistency
models. These studies concentrated on trading off consistency for performance and
availability. The various consistency models developed include Strong, Weak, Con-
tinuous, Data-centric, Strict, Sequential, Eventual, Causal, FIFO, and Release. For
instance, the strong consistency approach keeps data consistent across all replicas
simultaneously, which requires many more resources and expensive protocols than
other consistency models. The converse of strong consistency is weak consistency,
which can tolerate inconsistencies for certain periods of time.
Many studies on replica consistency in data grid environments have been pub-
lished [2, 4, 7, 8, 14, 15]. Data grid environments need consistency services to syn-
chronize them when replicas are modified by grid users. The European Data Grid
416 C.-T. Yang et al.
Fig. 1 The software stack
diagram of each node
Fig. 2 The software stack
diagram of all sites, services,
and portals
• Bottom Layer: shows the software installed on each node in the grid environment.
The major components of the Bottom Layer are the Information Provider and Grid
Middleware. The Information Provider consists of the Ganglia [35] and Network
Weather Service (NWS) [18]. Ganglia gathers machine information such as num-
bers of processors and how many cores each has, the loading on each processor,
total memory size and free space, and disk usage. The NWS gathers inter-node
network bandwidths and each link’s latency. The Grid Middleware consists of the
Globus Toolkit [27], which is used to join nodes to the grid environment.
• Middle Layer: This layer is the Site, consisting of several nodes usually located
in the same place or connected to the same switch or hub. Nodes in the Site are
connected to one another via the Internet. Sites are usually built up as clusters, but
each node has a real IP; the Site’s first node is called the head node.
• Top Layer: This layer holds Applications, Services, the Monitoring Service, and
Records [17]. Services consist of the Anticipative-Recursively-Adjusting Mecha-
nism, Replica Selection Service, One-way Replica Consistency Service, and Dy-
namic Maintenance Service. Services operate on information gathered from the
Monitoring Service and Records. Records can provide machine and file informa-
tion prior to downloading files or adjusting file locations. The Monitoring Service
provides a web front-end page for users to observe variations during job process-
ing.
Relations among the components described above are shown in Fig. 3. The four
services mentioned above are classified as User-side and System-side. The User-
418 C.-T. Yang et al.
side, which allows users to monitor application operations as the applications serve
their needs, includes the Anticipative-Recursively-Adjusting Mechanism (ARAM)
and Replica Selection Service (RSS). The System-side includes the Dynamic Main-
tenance Service (DMS) and One-way Replica Consistency Service (ORCS), which
automatically direct files to appropriate locations and keep them consistent. Func-
tional details of these services are described below.
• Replica Selection Service: gathers relevant information from the RLS and Infor-
mation Service to determine which sites are better for the ARAM to use for down-
loading files.
• Anticipative-Recursively-Adjusting Mechanism: enables users to download de-
sired data in parallel, dynamically adjusting download speeds according to net-
work bandwidths between server nodes and client nodes, and balancing file site
loadings.
• One-way Replica Consistency Service: keeps files consistent with duplicates stored
in distributed nodes. When one file in a node is updated, it will notify the other
nodes that have the same file to update to the newest version.
• Dynamic Maintenance Service: dynamically replicates, migrates, and deletes grid
environment files according to parameter variations. It reduces execution times,
promotes system stability, and improves storage device usage ratio efficiency.
3.2 ORCS and DMS operation
The DMS maintains replicas; the ORCS keeps file copies consistent. Figure 4 shows
general DMS and ORCS operation. Prior to file maintenance, the Information Service
and Replica Location Service store relevant information in the database for DMS
measurement using the cost model described below. The Information Service and
Replica Location Service functions are described below:
• Information Service [6]: periodically gathers statuses such as CPU idle ratio, mem-
ory usage, storage device free space, and network bandwidth, and records them in
real time in the Information Database (Info. DB) for the DMS to use.
• Replica Location Service (RLS): stores file information such as logical file name,
file size, file physical location, time of file creation or updating, and file access fre-
quency in the File Information Database (File Info. DB). Users can use the Replica
Location Service to search for desired files and the closest sites in the grid envi-
ronment where the files are stored.
Before the Replica Manager triggers the ORCS and DMS, it first queries the In-
formation Service and Replica Location Service, which then separately query the
Information Database and File Information Database to get all file and system status
information. If a Replica Manager determines some files need to be adjusted or kept
consistent, it directs the ORCS and DMS to make the necessary adjustments. After
all adjustments have been made, the ORCS and DMS query the Replica Location
Service to check the new statuses of all files in the grid environment. After checking,
the Replica Location Service records the new information in the File Information
Database.
420 C.-T. Yang et al.
3.3 Parameters and evaluation model
In this subsection we introduce our affect parameters, define measurable parameters,
and present the evaluation models we use to measure the performance of the two
services described above.
3.3.1 Affect parameters
Because grid environments have many factors that affect performance, we calculated
how the following static and dynamic factors affect overall performance.
• Static Parameters: These factors do not change when the grid environment changes.
As Xuanhua Shi et al. indicated in [25], they include system site attributes such
as CPU type and frequency, each storage element’s hard disk capacity, memory
capacity, and network card transfer rate. In general, faster frequency CPUs, larger
memory and hard disk capacities, and network cards with faster transfer rates are
better choices for executing jobs. Since these cannot be major factors in measuring
grid environment performance due to the changeable nature of grid environments,
we focus on the dynamic factors.
• Dynamic Parameters: These factors change when the grid environment changes.
Job execution consumes computing power and uses memory space downloading
or uploading data, and storing computational results. Thus, CPU usage rate, mem-
ory space, bandwidth, and node free space may all change. Among these, network
bandwidth has the most important influence on performance. The NWS [18] mon-
itors and periodically forecasts the performance of various network and computa-
tional elements. Real-time requirements must be met to achieve high performance.
We use the NWS to measure network bandwidth, and the Linux commands “sar”
and “df” to measure CPU, memory, and hard disk free space.
3.3.2 Cost model
Before files are replicated, migrated, or deleted, their affect factors must be measured
to determine what operations are necessary. Below, we define our strategic parame-
ters.
• BWLAN(i − j): LAN connection bandwidth between node i and node j in Mbit
• BWWAN(i − j): WAN connection bandwidth between site i and j in Mbit
• F_size: File size for transfer in MB
• T_trans(i − j): Time to transfer data from node i to node j
• T_auth(i − j): Time for authenticating transfer of data file from node i to node j
• T_replica_local(i − j): Time for local file replication from node i to node j
• T_replica_remote(i − j): Time for remote file replication from node i to node j
• F_space(i): Node i storage device free space
• FA_Min: Minimum file access rate
• FA_Max: Maximum file access rate
• α: Adjustable parameter for checking whether the storage element free space is
sufficient for replication
422 C.-T. Yang et al.
Fig. 5 DMS replication
algorithm Check all file access rates
If (File i′s access rate is greater than FA_Max in site j) Then
{
Check site j storage device free space
If (Not enough free space) Then
Find an alternative site closest to site j
If (The same file exists intra-region) Then
Replicate file i to site j from intra-region file site
Else
Replicate file i to site j from best inter-region file site
Else
If (The same file exists intra-region) Then
Replicate file i to site j from intra-region file site
Else
Replicate file i to site j from best inter-region file site
}
If (File i’s access rate is greater than FA_Min in site j ) and
(File i’s access rate is less than FA_Max in site j ) Then
{
Find the nearest file site j that no longer needs file i
Check site j storage device free space
If (Not enough free space) Then
Find an alternative site closest to site j
Migrate file i to alternative site from file site
Else
Migrate file i to site j from file site
}
If (File i’s access rate is less than FA_Min in site j) Then
{
Check the File Info. DB for another site with the same file
If (A site with the file is found) Then
Delete file i in site j
Else
Keep file i
}
• Replication: If the access frequency for file i at site j exceeds the maximum access
rate FA_Max, the DMS first checks to see if the storage device at site j has enough
free space to store the replicated file. If it does, the DMS duplicates the data to site
j using the intra-region copy of file i if such a copy exists, or it creates a duplicate
of file i at site j in the intra-region. If site j does not have enough free space, the
DMS first checks to see if it can duplicate file i in the inter-region. If not, it stores
the duplicated data in the site closest to site j .
• Migration: When an original file site no longer needs a file, or has insufficient free
space to store duplicated data, temporary data, or computing results, but other sites
still need the file, migration is used to move the file to an appropriate location. This
avoids generation of excessive file copies in the data grid system and saves free
space for storing temporary data and job execution results. If the request frequency
of file i in site j is between FA_Min and FA_Max, the DMS first checks to see if
424 C.-T. Yang et al.
// Once Original Data has been updated
If original data is updated from a super node then
Copy the original data to all master nodes
Add update records to the replication database for tracing
End
// For each Grid Site
If a replica’s access frequency by CN to MN is greater than its threshold then
If the CN has sufficient storage capacity then
Copy a replica from MN to CN
Add a replica update record to the database
Else
Find all replicas with access frequencies smaller than the CN threshold
Sort these CN replica access frequencies in ascending order
Delete replicas one by one from small to large until CN has sufficient storage capacity
If the storage capacity of CN is sufficient then
Copy a replica from MN to CN
Add a replica update record to the database
Else
Copy a replica from MN to a CN with the best resource status
Add a replica update record to the database
End
End
End
Fig. 6 First section of the ORCS algorithm
algorithm must find the nearest CN with the best resource status that has sufficient
storage capacity, as shown in Fig. 11.
In Fig. 12, Node j replica’s access frequency is lower than its threshold value,
thus Node j accesses the MN replica. In contrast, in Fig. 13, grid users access Node j
directly if the last update time of the CN replica is the same as that of the MN replica.
If the latest update times are not equal, the replica will be copied automatically from
the MN to the CN, as shown in Fig. 14. Figures 15 and 16 show the algorithm finding
the nearest CN with the best resource status and sufficient storage capacity because
node j has insufficient storage capacity.
4 Experimental environment and results
We compared and evaluated the performance of the DMS and ORCS algorithms
against other strategies. The Least Frequently Used (LFU), Least Recently Used
(LRU) strategies, and the Bandwidth Hierarchy-based Replication algorithm (BHR)
were tested against the DMS algorithm. The LFU and LRU always replicate when
requests occur, but choose files for deletion differently when storage element free
space is insufficient for replication. LRU chooses the oldest files for deletion, while
LFU chooses the least frequently requested files. The synchronous and asynchronous
consistency strategies were tested against the ORCS. We used a simulator called Op-
torSim, developed by the EU Data Grid [3], to compare the strategies mentioned
above. Our experimental grid environment is shown in Fig. 17. It consisted of four
426 C.-T. Yang et al.
Fig. 9 Operation 2 of the first
section
Fig. 10 Operation 3 of the first
section
Fig. 11 Operation 4 of the first
section
428 C.-T. Yang et al.
Fig. 14 Operation 3 of the
second section
Fig. 15 Operation 4 of the
second section
Fig. 16 Operation 5 of the
second section
The evaluation results are shown in Figs. 18 and 19. Figure 18 shows the execution
time for 500 jobs was best when α was set to 0.9, and Fig. 19 shows the storage
430 C.-T. Yang et al.
Table 1 Parameters used in the
simulation Parameters Values
Number of jobs 500
Number of job types 30
Number of files accessed per job 15
File sizes 250/500/750/1000 MB
Intra-region bandwidth 500 Mbps
Inter-region bandwidth 250 Mbps
Hard disk space at each site 50 GB
FA_Max 10
FA_Min 5
Job Delay 2500 ms
Table 2 Numbers of files in
each size File size Number
250 MB 175
500 MB 90
750 MB 100
1000 MB 85
Fig. 19 Free space for various
alpha values
4.1.2 ORCS parameter setting
In Table 3, we define several parameters used to derive the experimental results shown
below. We submitted 100 writing jobs from the SN and 1000 read jobs from each CN.
Files were 100 MB in size and the access threshold was 10. We used synchronous
replication, asynchronous replication, and the ORCS algorithms in our experimental
environment.
4.2 Results
4.2.1 File management results
Figures 20 and 21 show, respectively, DMS execution times and storage element free
space with and without the migration mechanism. Experimental results show exe-
cution times were better with the migration mechanism than without the migration
432 C.-T. Yang et al.
Fig. 22 Performance
comparison of four strategies
Fig. 23 Free space comparison
of four strategies
Fig. 24 Execution time
comparison for various
bandwidth ratios with 50 G H.D.
porary data and job results were better than those of the DMS, choosing files for
deletion and downloading files for job execution took considerable time. The BHR
strategy caused jobs to spend excessive time getting needed files. Although the BHR
strategy saved a lot of free space, its execution times were greater than those of the
DMS strategy. Thus, the DMS performed more efficiently than other three replication
strategies.
Figures 24 to 28 show the use of various network bandwidths and storage ele-
ment capacities to demonstrate variations in the four strategies’ performance. For all
variations in hard disk size, LFU and LRU performed better when WAN bandwidth
equaled LAN bandwidth. When there was not enough space to store replicas, tempo-
rary files, and job results, computing elements spent less time getting relevant files
434 C.-T. Yang et al.
Fig. 28 Execution time
comparison for various
bandwidth ratios with 200 G
H.D.
cution times decreased gradually as the ratio of WAN bandwidth to LAN bandwidth
was increased.
The DMS replication strategy performed better than other replication strategies
when storage element capacity was small because it provided more storage ele-
ment free space for storing replicas, temporary files, and job results. But increas-
ing the storage element capacity sufficiently provided enough free space for the
LFU and LRU replication strategies to store replicas, temporary files, and job re-
sults, reducing the time computing elements needed to get required files from other
storage elements in the LAN and even in the WAN, thus shortening total execu-
tion times. And increasing storage element capacity also increased DMS replication
strategy execution times. This means that as capacity was increased, DMS repli-
cation strategy performance worsened due to inefficient storage element usage ra-
tios, and that the DMS was more effective when the storage element capacity was
small.
4.2.2 File consistency results
There were three replication algorithms in our experiment, Synchronous, Asynchro-
nous, and our ORCS algorithm. We assumed that each file was 100 MB and the
access frequency threshold was 10. When a replica in the CN was the same as the
one in the MN, no transfer was necessary. The experimental file replication times are
shown in Fig. 29. The ORCS replicated files according to grid users’ needs. It con-
sumed less network bandwidth than the Synchronous algorithm and accessed files
more efficiently than the Asynchronous algorithm.
In the next experiment, we compared the storage capacity usage of our ORCS
with that of the Massive Data Oriented Replication Algorithms (MDORA) proposed
by Changqin Huang et al. in [14]. We assumed CN storage capacities of 80 G, a
replication frequency of 200, and file sizes of 5 MB, 50 MB, 100 MB, 1000 MB,
and 2000 MB. The result is shown in Fig. 30. If the CN storage capacity is inade-
quate, MDORA cannot store replicas. ORCS deletes files in ascending order of ac-
cess frequency until there is adequate storage space for replicas. Thus, replicas can
be written, even though the storage space was initially inadequate.
436 C.-T. Yang et al.
Fig. 31 Replication times for various threshold values
Fig. 32 Numbers of replications for various threshold values
5 Conclusions and future work
This paper presents the Dynamic Maintenance Service (DMS) and the One-way
Replica Consistency Service (ORCS) for improving grid environment performance.
DMS is also aimed at the “one-replica” problem the BHR incurs. It improves grid
system performance and increases storage element usage ratio efficiency by handling
temporary data and results that jobs produce during execution. Via ORCS, we ad-
dressed the principal problems with maintaining consistency among existing repli-
cas. Experimental results show that DMS and ORCS both perform more efficiently
than other strategies and make storage element usage ratios more efficient as well. We
conducted the design and implementation of a data grid system using the components
and services proposed in Sect. 3 to enable general users to use data grid systems and
monitor details of grid resource and file statuses.
438 C.-T. Yang et al.
19. OptorSim. http://edg-wp2.web.cern.ch/edg-wp2/optimization/optorsim.html
20. Park SM, Kim JH, Ko YB, Yoon W-S (2003) Dynamic data grid replication strategy based on Internet
hierarchy. In: The second international workshop on grid and cooperative computing (GCC2003), pp
838–846
21. Park SM, Kim JH (2003) Chameleon: a resource scheduler in a data grid environment. In:
Proceedings of third international symposium on cluster computing and the grid, p. 258.
http://portal.acm.org/citation.cfm?id=792481
22. Rahman RM, Barker K, Alhajj R (2006) Replica placement design with static optimality and dynamic
maintainability. In: Proceedings of the sixth IEEE international symposium on cluster computing and
the grid (CCGRID’06), pp 434–437
23. Rahman RM, Barker K, Alhajj R (2006) Effective dynamic replica maintenance algorithm for the
grid environment. In: Proceedings of advances in grid and pervasive computing, vol 3947: Grid and
pervasive computing 2006 (GPC2006), pp 336–345
24. Ranganathan K, Foster I Design and evaluation of dynamic replication strategies for a high perfor-
mance data grid. In: Proceedings of international conference on computing in high energy and nuclear
physics
25. Shi XH, Jin H, Qiang WZ, Zou DQ (2003) An adaptive meta-scheduler for data-intensive applications.
In: Proceedings of grid and cooperative computing (GCC’03), pp 830–837
26. Stockinger H, Samar A, Allcock B, Foster I, Holtman K, Tierney B (2002) File and object replication
in data grids. J Cluster Comput 5(3):305–314
27. The Globus Alliance. http://www.globus.org/
28. Vazhkudai S, Tuecke S, Foster I (2001) Replica selection in the globus data grid. In: Proceedings of
the 1st international symposium on cluster computing and the grid (CCGRID 2001), pp 106–113
29. Venugopal S, Buyya R, Ramamohanarao K (2006) A taxonomy of data grids for distributed data
sharing, management, and processing. ACM computing surveys, vol 38, Article 3, March 2006
30. Yang CT, Yang IH, Li KC, Wang SY (2007) Improvements on dynamic adjustment mechanism in
co-allocation data grid environments. J Supercomput 40(3):269–280
31. Yang CT, Wang SY, Fu CP (2007) A dynamic adjustment mechanism for data transfer in data grids. In:
Network and parallel computing: IFIP international conference, NPC 2007. Lecture notes in computer
science, vol 4672. Springer, Berlin, pp 61–70. ISSN 1611-3349
32. Yang CT, Yang MF, Chiang WC (2008) Implementation of a cyber transformer for parallel download
in co-allocation data grid environments. In: Proceedings of the 7th international conference on grid
and cooperative computing (GCC2008) and second EchoGRID conference, October 24–26, 2008 in
Shenzhen, Guangdong, China, pp 242–253
33. Yang CT, Yang IH, Chen CH, Wang SY (2006) Implementation of a dynamic adjustment mechanism
with efficient replica selection in co-allocation data grid environments. In: Proceedings of the 21st
annual ACM symposium on applied computing (SAC 2006) – distributed systems and grid computing
(DSGC) track, vol 1, pp 797–804, Dijon, France, April 23–27, 2006
34. Yang CT, Yang IH, Wang SY, Li KC, Hsu CH (2009) A recursively-adjusting co-allocation scheme
with cyber-transformer in data grids. Future Gener Comput Syst 25(7):695–703
35. Ganglia. http://ganglia.info/
Chao-Tung Yang is Professor of Computer Science at Tunghai Uni-
versity in Taiwan. He was born on November 9, 1968 in Ilan, Taiwan,
R.O.C. and received his B.Sc. degree in Computer Science from Tung-
hai University, Taichung, Taiwan, in 1990, and the M.Sc. degree in
Computer Science from National Chiao Tung University, Hsinchu, Tai-
wan, in 1992. He received the Ph.D. degree in Computer Science from
National Chiao Tung University in July 1996. He won the 1996 Acer
Dragon Award for an outstanding Ph.D. dissertation. He has worked
as Associate Researcher for ground operations in the ROCSAT Ground
System Section (RGS) of the National Space Program Office (NSPO)
in Hsinchu Science-based Industrial Park since 1996. In August 2001,
he joined the faculty of the Department of Computer Science at Tunghai
University. He got the excellent research award by Tunghai University
in 2007. In 2007 and 2008, he got the Golden Penguin Award by Indus-
trial Development Bureau, Ministry of Economic Affairs, Taiwan. His
J Supercomput
DOI 10.1007/s11227-009-0307-4
Implementation of a dynamic adjustment strategy
for parallel file transfer in co-allocation data grids
Chao-Tung Yang · Shih-Yu Wang ·
William Cheng-Chung Chu
© Springer Science+Business Media, LLC 2009
Abstract Co-allocation architecture was developed to enable parallel transferring
of files from multiple replicas stored in the different servers. Several co-allocation
strategies have been coupled and used to exploit the different transfer rates among
various client-server links and to address dynamic rate fluctuations by dividing files
into multiple blocks of equal sizes. The paper presents a dynamic file transfer scheme,
called dynamic adjustment strategy (DAS), for co-allocation architecture in concur-
rently transferring a file from multiple replicas stored in multiple servers within a data
grid. The scheme overcomes the obstacle of transfer performance due to idle waiting
time of faster servers in co-allocation based file transfers and, therefore, provides re-
duced file transfer time. A tool with user friendly interface that can be used to manage
replicas and downloading in a data grid environment is also described. Experimental
results show that our DAS can obtain high-performance file transfer speed and reduce
the time cost of reassembling data blocks.
Keywords Data grid · File replica · Parallel file transfer · Co-allocation · Dynamic
adjustment
C.-T. Yang () · S.-Y. Wang · W.C.-C. Chu
Department of Computer Science, Tunghai University, Taichung, 40704, Taiwan
e-mail: ctyang@thu.edu.tw
W.C.-C. Chu
e-mail: cchu@thu.edu.tw
S.-Y. Wang
Communication System Division Service-Oriented Network System Department, Information and
Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, 31040,
Taiwan
e-mail: Laurence@itri.org.tw
Implementation of a dynamic adjustment strategy
completion times. We also present a new toolkit, called cyber-transformer, with a
friendly client-side GUI interface integrated with the information service, replica lo-
cation service, and data transfer service [25] that makes it easy for inexperienced
users to manage replicas and download files in data grid environments. And we pro-
vide an effective scheme for reducing the cost of reassembling data blocks. Experi-
mental results show that our approach is superior to previous methods and achieves
the best overall performance. We also discuss combination cost and provide an effec-
tive improvement.
The remainder of this paper is organized as follows. Related background review
and studies are presented in Sect. 2 and the co-allocation architecture and our research
approaches are outlined in Sect. 3. In Sect. 4, a powerful toolkit, cyber-transformer, is
proposed by us, and experimental results and a performance evaluation of our scheme
are presented in Sect. 5. Section 6 concludes this research paper.
2 Background review
2.1 Data grid and replications
In data grid environments, access to distributed data is typically as important as ac-
cess to distributed computational resources [1–5, 7, 22]. Distributed scientific and
engineering applications require transfers of large amounts of data between storage
systems, and access to large amounts of data generated by many geographically dis-
tributed applications and users for analysis and visualization, among others.
We used the grid middleware Globus Toolkit [9, 10, 12, 13, 16] as our data grid
infrastructure. The Globus Toolkit provides solutions for such considerations as se-
curity, resource management, data management, and information services. One of its
primary components, MDS [7, 13, 16], is designed to provide a standard mechanism
for discovering and publishing resource status and configuration information. It pro-
vides a uniform and flexible interface for data collected by lower-level information
providers in two modes: static (e.g., OS, CPU types, and system architectures) and
dynamic data (e.g., disk availability, memory availability, and loading). And it uses
GridFTP [3, 13, 16] to provide efficient management and transfer data in a wide-area,
distributed-resource environment. We use GridFTP to enable parallel data transfers.
Among its many features, its partial file transfer ability allows files to be retrieved
from data servers by specifying the start and end offsets of file partitions. This pro-
tocol, which extends the standard FTP protocol, provides a superset of the features
offered by the various grid storage systems currently in use.
The data grid community tries to develop secure, efficient data transport mech-
anisms and replica management services. Another key technology from the Globus
project, called the Replica Catalog [16], is used to register and manage complete and
partial copies of data sets. The Replica Catalog contains mapping information from
a logical file or collection to one or more physical files.
Replica management involves creating or removing replicas at a data grid site
[21]. A replica manager typically maintains a replica catalog containing replica site
addresses and file instances. The replica management service is responsible for man-
aging the replication of complete and partial copies of datasets, defined as collections
Implementation of a dynamic adjustment strategy
Fig. 1 The co-allocation
architecture in data grids
service [21] creates a list of the desired files physical locations. The co-allocation
agent then downloads the data in parallel from the selected servers.
Data grids consist of scattered computing and storage resources located in differ-
ent countries/regions yet accessible to users [8]. As datasets are replicated within grid
environments for reliability and performance, clients require the abilities to discover
existing data replicas, and create and register new replicas. A replica location ser-
vice (RLS) [5] provides a mechanism for discovering and registering existing repli-
cas. Several prediction metrics have been developed to help replica selection. For
instance, Vazhkudai and Schopf [19–21] used past data transfer histories to estimate
current data transfer throughputs.
In [17], the author proposes a co-allocation architecture for co-allocating grid
data transfers across multiple connections by exploiting the partial-copy feature of
GridFTP. It also provides brute-force, history-based, and dynamic load balancing for
allocating data blocks.
• Brute-force co-allocation: brute-force co-allocation (see Fig. 2) works by dividing
files equally among “n” available flows (locations). Thus, if the data to be fetched
is size “S” and there are “n” locations to fetch it from, then this technique assigns
to each flow a data block of size, “S/n”. For example, if there are three sources,
the target file will be divided into three blocks equally. And each source provides
one block for the client. With this technique, although all the available servers
are utilized, bandwidth differences among the various client-server links are not
exploited.
• History-based co-allocation: The history-based co-allocation (see Fig. 3) scheme
keeps block sizes per flow proportional to transfer rates predicted by the previous
results of file transfer results. In the history-based allocation scheme, the block
Implementation of a dynamic adjustment strategy
Fig. 4 The conservative load
balancing process
Fig. 5 The aggressive load
balancing process
• Aggressive load balancing: This method is shown in Fig. 5 and adds functions that
change block size in deliveries by: (1) gradually increasing the amounts of data
requested from faster servers, and (2) reducing the amounts of data requested from
slower servers or stopping requesting data from them altogether.
In our previous work [24, 26], we proposed a replica selection cost model and
a replica selection service to perform replica selection. These co-allocation strate-
gies do not address the shortcoming of faster servers having to wait for the slowest
server to deliver its final block. In most cases, this wastes much time and decreases
overall performance. Thus, we propose an efficient approach, called the dynamic ad-
justment strategy, and based on the co-allocation architecture. It improves dynamic
co-allocation and reduces waiting time, thus improving overall transfer performance.
3 The dynamic adjustment strategy
Dynamic co-allocation, described above, is the most efficient approach to reducing
the influence of network variations between clients and servers. However, the idle
time of faster servers waiting for the slowest server to deliver its last block is still
Implementation of a dynamic adjustment strategy
server devices:
Scorei = P CPUi × RCPU + P Memi × RMem + P BWi × RBW,
and RCPU + RMem + RBW = 1 (2)
After getting the scores for all server nodes, the system calculates the weightingi :
weightingi = Scorei
/ n∑
k=1
Scorek (3)
The weighting is then used to determine the size of the next job:
newPT i = ClientBandwidth × weightingi (4)
where newPT i denotes the next job size for server i, and ClientBandwidth denotes
the current client bandwidth.
When server i finishes transferring of a block, it gets a new job whose size is
calculated according to the real-time status of server i. Each time, our strategy dy-
namically adjusts a job size according to source device loading and bandwidth. The
lighter the loading a source device has, the larger job size is assigned. Figure 6 shows
a flowchart illustrating this new strategy.
Next, the average transfer rate of all replicas can be calculated by total transferred
file size divided the cost time ratio of combination of CPU, memory, and network
bandwidth. We used the dynamic adjustment strategy with various sets of replica
servers and measured overall performances, where overall performance is:
Total Performance = File Size/Total Completion Time (5)
4 An efficient toolkit: cyber-transformer
We gave experimental results for cyber-transformer, a powerful new toolkit for replica
management and data transfer in data grid environments. It not only can accelerate
data transfer rate, but can also manage replicas over all various sites. The friendly
interface enables users to easily monitor replica sources, and add files as replicas
for automatic cataloging by our replica location service. Moreover, we provide a
function for administrators to delete and modify replicas. Cyber-transformer can be
invoked with either the logical file name of a data file or a list of replica sources host
names. When users search for a file by its logical file name, cyber-transformer queries
the replica location services to find all the corresponding replicas, and contacts each
replica source to start parallel transfers. The file is then gathered from replica sources
and finally combined into a single file.
4.1 System components
Cyber-transformer is implemented in the Java Cog Kits [13] library. Figure 7 shows
the system stack of cyber-transformer, consisting of three integrated mechanisms:
Implementation of a dynamic adjustment strategy
Fig. 7 The system stack of
cyber-transformer
Fig. 8 The components of cyber-transformer
• Information service: This service is invoked by the information monitor and pro-
vides replica sources statuses allowing users to monitor all replica source sites in
the data grid. Sites status, such as CPU loading, free memory, hard disk free space,
and bandwidth, are gathered by the information service and reported to the infor-
mation monitor.
Implementation of a dynamic adjustment strategy
Fig. 9 The cyber-transformer transaction flow
effort among problem-solving environments, science portals, grid middleware, and
collaborative pilots.
The Java-based application uses the Java CoG kit to connect to the grid system.
The key characteristics include: GridProxyInit, a JDialog for submitting pass phrases
to grids to extend certificate expiration dates, GridConfigureDialog uses the UITool
in the CoG Kit to enable users to configure process numbers and host names of Grid
servers, and GridJob, which creates GramJob instances. This class represents a simple
gram job and allows for submitting jobs to a gatekeeper, canceling them, sending
signal commands, and registering and unregistering from callbacks. The GetRSL,
RSL provides a common interchange language to describe resources.
The Java CoG GridFTP API does not support downloading files in multiple
streams and simultaneously writing them to the same file, which causes some com-
bination overhead after all transmissions. Thus, we needed an effective method for
writing to a file in parallel. To resolve the situation, we analyzed and rewrote the Java
Implementation of a dynamic adjustment strategy
Fig. 11 The file download process
The class, RandomAccessFile in file.seek(bufOffset) provides a function that
can change the write pointer. This allowed us to write another class to inherit
FileRandomIO from Java CoG, and overwrite the method, public synchronized void
write(Buffer buffer). We also added a method to change the write pointer. This gave
us more transmission time to write data to a file at the same time. All streams write
to assigned file positions, not to the beginning of the file. That does not affect other
streams transferring data and writing files. The new code after our changes is listed
below.
protected long filePointer;
/**
* set write pointer
* @param filePointer write pointer
*/
public void setFileOffset(long filePointer) {
this.filePointer = filePointer;
}
Implementation of a dynamic adjustment strategy
The boldface type shows that the key point in measuring the current transfer speed
is using time difference and file-writing duration. This gives the transfer speed during
file transfers, thus there is no separate reassembly time cost. We overcome one co-
allocation shortcoming, and the completion time is just the sum of the authentication
and data transmission times.
5 Experimental results and analysis
In this section, we discuss the performance of our recursive-adjustment co-allocation
strategy. We evaluate four co-allocation schemes: (1) brute-force (Brute), (2) history-
based (History), (3) conservative load balancing (Conservative), (4) aggressive load
balancing (Aggressive) and (5) dynamic adjustment strategy (DAS). We analyze the
performance of each scheme by comparing their transfer finish times, and the total
idle time faster servers spent waiting for the slowest servers to finish delivering the
last block. We also analyze overall performances in the various cases.
5.1 Input parameters
We used the following experiments to determine input parameters for the three factors
in our strategy: CPU idle state, memory free space, and network bandwidth, and
assign ratios to each of the factors.
At first, to determine the effect of network bandwidth on transfer rates, we mea-
sured average rates using various bandwidth ratios. As Fig. 12 shows, there was little
difference for small file sizes, however, as the file size was increased, a curve became
apparent. The transfer rate decreased at bandwidth ratios smaller than 0.6; the peak
transfer rate occurred at a ratio of 0.8. This means that we set RCPU,RMEM, and RBW
in the ratio 0.1:0.1:0.8.
In the second experiment, we assessed the effect of CPU computing power on
transfer rates. We used three machines with different CPU types, memory sizes fixed
Fig. 12 The partition size evaluation result
Implementation of a dynamic adjustment strategy
Fig. 15 Our data grid testbed
portant factor affecting transfer rate, and that the bandwidth ratio should be set larger
than the other two factors. CPU power and memory size can be used to make a dif-
ference when the bandwidths of several servers are very close.
5.2 Experimental environments
We performed wide-area data transfer experiments using our GridFTP GUI client
tool. We executed our co-allocation client tool on our testbed at Tunghai University
(THU), Taichung City, Taiwan, and fetched files from four selected replica servers:
one at Providence University (PU), one at Li-Zen High School (LZ), and the other
one at Hsiuping Institute of Technology School (HIT). All these institutions are in
Taichung, Taiwan, and each is at least 10 km from THU. Figure 15 shows our data
grid testbed, and Table 1 is the detailed listing. All servers had Globus 3.2.1 or above
installed.
In the following experiments, we set RCPU,RMEM, and RBW in the ratio
0.1:0.1:0.8. We experimented with file sizes of 10 MB, 50 MB, 100 MB, 500 MB,
1000 MB, 1500 MB, and 2000 MB. For comparison, we measured the performance
of conservative load balancing on each size using the same block numbers.
Table 2 shows average transmission rates between THU and each replica server.
These numbers were obtained by transferring files of 100 MB, 500 MB, 1000 MB,
and 2000 MB from a single replica server using our GridFTP client tool, and each
number is an average over several runs.
5.3 Results and analysis
We examined the effect of faster servers waiting for the slowest server to deliver the
last block for each scheme. Figure 16 shows total idle times for various file sizes. Note
that our Dynamic Adjustment Strategy performed significantly better than the other
Implementation of a dynamic adjustment strategy
Fig. 16 Idle times for various methods; servers at PU, LZ, and HIT
Fig. 17 Completion times for various methods; servers are at PU, LZ, and HIT
trary, it decreased. We can infer that the co-allocation efficiency reached saturation
in the DAS2_2 case, and that additional flows caused additional overhead and re-
duced overall performance because the PU file site had worse network bandwidth,
and DAS2_1 choosing PU for file transfer led to the differences between DAS2_1 and
DAS2_2. This means that more download flows do not necessarily result in higher
Implementation of a dynamic adjustment strategy
Fig. 19 Comparison of cyber-transformer transmission rate between LAN and WAN
Fig. 20 The rate of overall
performance downgrade
between LAN and WAN
6 Conclusions
Using the parallel-access approach to downloading data from multiple servers re-
duces transfer times and increases server resilience. The co-allocation architecture
provides a co-ordinated agent for assigning data blocks. A previous work showed that
the dynamic co-allocation scheme leads to performance improvements. However, it
cannot handle the idle time of faster servers having to wait for the slowest server to
deliver its final block. This paper proposes the dynamic adjustment strategy (DAS) to
improve file transfer performances using the co-allocation architecture in data grids.
In our approach, the workloads on selected replica servers are continuously adjusted
during data transfers, and our approach can also reduce the idle times spent waiting
for the slowest servers, and thus decrease file transfer completion times.
We also developed a new toolkit, called cyber-transformer that enables even inex-
perienced users to easily monitor replica source site statuses, manage replicas, and
download files from multiple servers in parallel. Experimental results show the effec-
Implementation of a dynamic adjustment strategy
19. Vazhkudai S, Schopf J (2003) Using regression techniques to predict large data transfers. Int J High
Perform Comput Appl (IJHPCA) 17:249–268
20. Vazhkudai S, Schopf J, Foster I (2002) Predicting the performance of wide area data transfers. In:
Proceedings of the 16th international parallel and distributed processing symposium (IPDPS 2002),
April 2002, pp 34–43
21. Vazhkudai S, Tuecke S, Foster I (2002) Replica selection in the Globus data grid. In: Proceedings
of the 1st international symposium on cluster computing and the grid (CCGRID 2001), May 2001,
pp 106–113
22. Venugopal S, Buyya R, Ramamohanarao K (2006) A taxonomy of data grids for distributed data
sharing, management, and processing. ACM Comput Surv 38(1):1–53
23. Wolski R, Spring N, Hayes J (1999) The network weather service: a distributed resource performance
forecasting service for metacomputing. Future Gener Comput Syst 15(5–6):757–768
24. Yang CT, Shih PC, Chen SY (2006) A domain-based model for efficient network information on
grid computing environments. IEICE Trans Inf Syst E89-D(2):738–742. Special issue on paral-
lel/distributed computing and networking
25. Yang CT, Yang IH, Chen CH, Wang SY (2006) Implementation of a dynamic adjustment mechanism
with efficient replica selection in co-allocation data grid environments. In: Proceedings of the 21st
annual ACM symposium on applied computing (SAC 2006)—distributed systems and grid computing
track, April 23–27, 2006, pp 797–804
26. Yang CT, Wang SY, Fu CP (2007) A dynamic adjustment mechanism for data transfer in data grids.
In: Network and parallel computing: IFIP international conference, NPC 2007, September 17–20.
Lecture notes in computer science, vol 4672. Springer, Berlin, pp 61–70
27. Yang CT, Yang IH, Li KC, Wang SY (2007) Improvements on dynamic adjustment mechanism in
co-allocation data grid environments. J Supercomput 40(3):269–280
28. Zhang X, Freschl J, Schopf J (2003) A performance study of monitoring and information services
for distributed systems. In: Proceedings of 12th IEEE international symposium on high performance
distributed computing (HPDC-12 ‘03), August 2003, pp 270–282
Chao-Tung Yang is a professor of Computer Science at Tunghai Uni-
versity in Taiwan. He was born on November 9, 1968, in Ilan, Tai-
wan, R.O.C. and received the B.Sc. degree in Computer Science from
Tunghai University, Taichung, Taiwan, in 1990, and the M.Sc. degree in
Computer Science from National Chiao Tung University, Hsinchu, Tai-
wan, in 1992. He received the Ph.D. degree in Computer Science from
National Chiao Tung University in July 1996. He won the 1996 Acer
Dragon Award for an outstanding Ph.D. Dissertation. He has worked as
an Associate Researcher for ground operations in the ROCSAT Ground
System Section (RGS) of the National Space Program Office (NSPO)
in Hsinchu Science-based Industrial Park since 1996. In August 2001,
he joined the faculty of the Department of Computer Science at Tunghai
University. He got the excellent research award by Tunghai University
in 2007. In 2007 and 2008, he got the Golden Penguin Award by Indus-
trial Development Bureau, Ministry of Economic Affairs, Taiwan. His
researches have been sponsored by Taiwan agencies National Science Council (NSC), National Center
for High Performance Computing (NCHC), and Ministry of Education. His present research interests are
in grid and cluster computing, parallel and multi-core computing, and Web-based applications. He is a
member of both the IEEE Computer Society and ACM.
This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
http://www.elsevier.com/copyright
Author's personal copy
In our previous research work, we presented a method for
regulating next-section workloads by continuously adjusting the
workloads on selected replica servers. The anticipative recursively
adjusting mechanism (ARAM) scheme (Yang et al., 2007a)
measures the actual bandwidth performance during data ﬁle
transfers, and, according to previous transfer ﬁnish rates,
anticipates bandwidth statuses at the next transfer section. The
basic idea is to assign less data to selected replica servers with
greater network link performance variations since links with more
bandwidth variations will have smaller effective bandwidths, as
well as smaller transfer ﬁnish rates. The goal is to make the
expected ﬁnish times of all servers be the same.
In this paper, we ﬁrst present our new approach based on the
ARAM co-allocation strategy for data grid environments. We have
designed and implemented a TCP bandwidth estimation model
and Burst Mode (BM) to enhancing the original ARAM algorithm.
Workloads on all selected replica servers are still adjusted
according to TCP throughputs and packet loss rates, and faster
servers get double or even quadruple throughputs via Burst Mode
enabling. Finally, we present Cyber Transformer, a useful toolkit
for data grid users. Integrated with the Information Service,
Replica Location Service, and Data Transfer Service, its simple,
friendly GUI interface makes it easy for inexperienced users to
manage replicas and download ﬁles in data grid environments.
This tool integrates all strategies based on co-allocation archi-
tectures including our previous and proposed algorithms.
The remainder of this paper is organized as follows. Related
background review and studies are presented in Section 2. Our
new approach is outlined in Section 3. Experimental results and a
performance evaluation of our scheme are presented in Section 4.
Section 5 concludes this research article.
2. Background review and related work
2.1. Co-allocation architecture
The architecture proposed in Vazhkudai (2003) consists of
three main components: an information service, a broker/co-
allocator, and local storage systems. Fig. 1 shows co-allocation of
data grid transfers, an extension of the basic template for resource
management (Vazhkudai et al., 2001; Vazhkudai and Schopf,
2002) provided by the Globus Toolkit. Applications specify the
characteristics of desired data and pass attribute descriptions to a
broker. The broker queries available resources, gets replica
locations from the Information Service (Czajkowski et al., 1999,
2001) and Replica Management Service (Czajkowski et al., 2001),
then gets lists of physical ﬁle locations.
2.1.1. Brute-force co-allocation
The Brute-force co-allocation scheme shown in Fig. 2 divides
ﬁle sizes equally among available ﬂows; it does not address
bandwidth differences among various client–server links.
2.1.2. History-based co-allocation
The history-based co-allocation scheme shown in Fig. 3 keeps
block sizes per ﬂow proportional to predicted transfer rates, and
disregards the inﬂuence of network variations between client and
server.
2.1.3. Conservative load balancing
The conservative load balancing scheme shown in Fig. 4
divides requested data sets into k disjoint blocks of equal size.
Available servers are allocated single blocks to deliver in parallel.
Servers work in sequential order until all requested ﬁles are
downloaded. Loadings on the co-allocated ﬂows are automatically
adjusted because the faster servers deliver larger ﬁle portions
more quickly.
2.1.4. Aggressive load balancing
This method, shown in Fig. 5, adds functions that change block
size in deliveries by: (1) gradually increasing the amounts of data
requested from faster servers and (2) reducing the amounts of
ARTICLE IN PRESS
Application
RLS
Queries
Data Access/Transport using GridFTP
Information
Broker Forecasts Information
Service
Local Storage System
Co-allocator
Fig. 1. Data grid co-allocation architecture.
Client
File Server 1
File Server 2
File Server 3
Fig. 2. The Brute-force co-allocation process.
Client
File Server 3
File Server 2
File Server 1
Fig. 3. The history-based co-allocation process.
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 835
Author's personal copy
The challenge in multiple server–multiple client scenarios is
greater since server selections and data downloads on some
clients can impact server selections and data transfer performance
on other clients.
3. Our approach
3.1. Anticipative recursively adjusting mechanism (ARAM)
The recursively adjusting mechanism reduces ﬁle transfer
completion times and idle times spent waiting for the slowest
server. It also provides an effective scheme for reducing the cost of
reassembling data blocks. However, our scheme did not consider
the potential effect of server links broken or idled during ﬁle
transfers. Therefore, we propose an efﬁcient approach called the
anticipative recursively adjusting mechanism to extend and
improve upon recursively adjusting co-allocation mechanism
(Yang et al., in press). The main idea of the ARAM is to assign
transfer requests to selected replica servers according to the ﬁnish
rates for previous transfers, and to adjust workloads on selected
replica servers according to anticipated bandwidth statuses. In
continuously adjusting selected replica server workloads, the
anticipative recursively adjusting mechanism scheme measures
actual bandwidth performance during data ﬁle transfers and
regulates workloads by anticipating bandwidth statuses for
subsequent transfers according to the ﬁnish rates for previously
assigned transfers. The basic idea is to assign less work to selected
replica servers on network links with greater performance
variability. Links with more bandwidth variation will have smaller
effective bandwidths, as well as smaller ﬁnish rates for assigned
transfers. The goal is to have the expected ﬁnished times of all
servers be the same. Our approach performs well, even when the
links to selected replica servers are broken or idled. It also reduces
the idle time wasted waiting for the slowest server. As appropriate
ﬁle sections are selected, they are ﬁrst divided into proper block
sizes according to the respective server bandwidths, previously
assigned ﬁle sizes, and transfer ﬁnish rates. Initially, the ﬁnish rate
is set to 1. Next, the co-allocator assigns the blocks to selected
replica servers for transfer. At this moment, it is expected that the
transfer ﬁnish times will be consistent with E(t1). However, since
server bandwidths may ﬂuctuate during segment deliveries,
actual completion times may differ from expected times E(t1)
(solid lines in Figs. 8 and 9). When the fastest server ﬁnishes at
time t1, the size of unﬁnished transfer blocks (italic blocks in Figs.
8 and 9) is measured to determine the ﬁnish rate. Two outcomes
are possible: the quickest server ﬁnish time t1 may be slower than
or equal to the expected time, E(t1), indicating that network link
performance remained unchanged or declined during the transfer.
In this case, the difference in transferred size between the
expected time and actual completion time (italic block in Fig. 8)
is then calculated.
The other outcome is that the quickest server ﬁnish time t1
may be faster than the expected time, E(t1), indicating an
excessively pessimistic anticipation of network performance, or
an improvement in replica server network link performance
during the transfer. The difference in transferred size between the
expected time (italic block in Fig. 9) and earlier time is then
measured. If the anticipated network performance was exces-
sively pessimistic, it is adjusted for the next section. The next task
is to assign proper block sizes to the servers along with respective
bandwidths and previous ﬁnish rates, enabling each server to
ﬁnish its assigned workload by the expected time, E(t2). These
adjustments are repeated until the entire ﬁle transfer is ﬁnished.
Looking more closely at ARAM, some parameter deﬁnitions are
shown below:
 A: ﬁle requested by user
 n: selected replica servers
 a: rate that determines how much of the section remains to be
assigned
 Tj: allocated time for section j
 SEj: allocated size for section j
 UnassignedFileSize: portion of ﬁle A not yet distributed for
downloading
 UnﬁnishedFileSize: size of unﬁnished blocks assigned in
previous rounds
 Bji: real-time transfer rate from the selected replica server
 rj: transfer ﬁnish rate
 rj1: server transfer ﬁnish rate for previously assigned
delivered ﬁle
 Bj: bandwidth available for section j
 Sji: block size per ﬂow from SEj for each server i at time Tj
 ETji: expected time for server i at section j
 RTji: real ﬁnish time for server i at section j
 TSji: actual transfer size at real ﬁnish time RTji
 rji: job ﬁnish rate
When a user requests ﬁle A from the data grid environment, the
replica selection server responds with a list of all available servers
deﬁned as maximum performance data sets/servers. Data sets/
servers for the co-allocator to transfer the ﬁle are selected, and the
target ﬁle is then transferred from the chosen replica data sets/
servers.
Assume that n replica servers are selected and Si denotes server
‘‘i’’ for 1%i%n. A connection for ﬁle downloading is then built to
each server.
The anticipative recursively adjusting mechanism process is as
follows. A new section of a ﬁle to be allocated is ﬁrst deﬁned. The
section size is shown as
SEj ¼ ðUnassignedFileSizeþ TotalUnfinishedFileSizeÞa; 0oap1
(1)
ARTICLE IN PRESS
Round 1 Round 2
E (t1) E (t2)t1
Section 1 Section 2 ... ...
...
...
...
File A
Server 1
Server 2 
Server 3
Fig. 8. Later-than-expected-time adjustment process.
Server 3
Server 2
Server 1
Round 1 Round 2
E (t1) E (t2)t1
File A Section 1 Section 2 ... ...
...
...
...
Fig. 9. Earlier-than-expected-time adjustment process.
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 837
Author's personal copy
network bandwidths have been placed in three groups (k ¼ 3).
The simulation results are shown in Fig. 10:
 k: number of partitions
 x: number of points
 Si: partition attributes form a vector space
 mi: the mean point of all of Si points
 xBooleanij: determines whether or not an x point belongs to Si
 V: distance cost function
 d: distance between two point
mi ¼
P
x2Si dðxi;miÞ
jSij
(10)
xBooleanij ¼
1 if kxj ¼ Sik2pkxj ¼ Skk2 8ka1
0 otherwise
(
(11)
V ¼
Xk
i¼1
Vi ¼
Xk
i¼1
X
k;xj2Si
dðxj;miÞ
0
@
1
A (12)
newðmiÞ ¼
1
jSij
X
k;xj2Si
xj (13)
3.4. Burst Mode
Like many network accelerator methods, and multithreading,
Burst Mode ﬁrst splits one huge bandwidth into small
pipelines all working at the same time. Burst Mode focuses on
the fastest group of servers and can differentiate among the
various candidate server network bandwidths. Second, BM
chooses the faster one then others (as shown in Eqs. (10)–(13)).
Ultimately, the BM has made single jobs into many, as shown in
Fig. 11.
The k-means simulation results showed that fewer local replica
servers are high efﬁciency than many remote replica servers.
Accordingly, the main ideas in Burst Mode are to ﬁnd the fastest
server group, and to make it download via multithreading. BM
also deals with cutting blocks properly for various data sets.
Burst Mode function is shown below:
 Ni TCPBW: candidate server bandwidth
 FTS: the fastest group of servers
Ni TCPBW ¼
MSS
RTT
Cﬃﬃﬃ
p
p (14)
FTS ¼ SimaxfS1; S2; . . . ; Sng;mi 2 Si (15)
The algorithm is listed below:
[Initialization]
Measure bandwidths and ﬁnd the fastest servers using Eqs.
(14) and (15).
BigBlockUnit set to 100MB initially
[Allocate blocks to the fastest servers and download via
multithreading.]
Step 1: Group mi and rank the most powerful server FTS
Step 2: Allocate SEj and download via multithreading
Step 3: Monitor job progress statuses
LOOP WHEN (UnassignedFileSize and total UnﬁnishedFileSize are
greater than BigBlockUnit (initial BigBlockUnit ¼ 100MB))
THEN
{
IF (Job ﬁnish rate is just 100% (rji ¼ 1) and UnassignedFi-
leSize and total UnﬁnishedFileSize are greater than BigBlockUnit)
THEN
{
Let data transfer in multiple parts between client and FTS
server
SEj ¼ (UnassignedFileSize+TotalﬁnishedFileSize)a, 0oa
p1 (UnassignedFileSize+TotalUnﬁnishedFileSize)
XBigBlockUnit
}
}
END LOOP;
3.5. Grid network congestion control
Grid network congestion control is concerned with controlling
trafﬁc entry into data grid networks to prevent congestive
collapse by avoiding oversubscription of any grid node processing
or link capacity and taking resource reduction steps, such as
reducing packet sending rates when Burst Mode is active.
The modern theory of congestion control (Kelly, 2003; Mamatas
et al., 2007), describes how individuals controlling their own pack
lost rate can interact to achieve an optimal network-wide rate
allocation. Examples of ‘‘optimal rate’’ allocation are max–min fair
allocation and Kelly’s (2003) suggestion of proportional fair
allocation, although many others are possible. The mathematical
expression (Eq. (16)) for optimal rate allocation is as follows. Let xi
be the rate of ﬂow i. Let x, c and R be the corresponding vectors and
matrix. Let U(x) be an increasing, strictly convex function, called
the utility, which measures how much beneﬁt a user obtains by
transmitting at rate x. The optimal rate allocation will then satisfy:
max
x
X
i
UðxiÞ; Rxpc (16)
3.6. Anticipative recursively adjusting mechanism plus (ARAM+)
3.6.1. Assumptions
We outline our system design model assumptions below.
 All grid nodes are installed GlobusToolkit4 previously.
 All grid nodes are supporting Simple Network Management
Protocol (SNMP).
 The time for transferring, stopping/assigning processes, and
calculating TCPBW to selected replica servers is negligible.
3.6.2. Anticipative recursively adjusting mechanism plus (ARAM+)
The ARAM+ is not merely inherited from ARAM. It has been
enhanced also in the following two areas: its TCP Bandwidth
ARTICLE IN PRESS
Server 1
Server 2
Server 3
Round 1 Round 2
t1 t2
Section 1 Section 2 ... ...
...
...
...
:Burst Mode Enable
File A
80Mbps
5Mbps
2Mbps
Fig. 11. Burst Mode enables higher bandwidths.
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 839
Author's personal copy
management and data grid environment data transfers. It can
accelerate data transfer rates, and also manage replicas over
various sites. The friendly interface enables users to easily
monitor replica sources, and add ﬁles as replicas for automatic
cataloging by our Replica Location Service. Moreover, we provide a
function for administrators to delete and modify replicas. Cyber
Transformer can be invoked with either the logical ﬁle name of a
data ﬁle or a list of replica source host names. When users search
for ﬁles using logical ﬁle names, Cyber Transformer queries the
Replica Location Services to ﬁnd all corresponding replicas, and
ARTICLE IN PRESS
Hsiuping Institute
of Technology, HIT
Da-Li Senior
HighSchool, DL
delta x 4
alpha x 4
beta x 4
gamma x 4
zeta x 5
eta x 8
75 Mbps
37 M
bps
16 Mbp
s
National
Taichung
University,
NTCU
23 Mbps
36
 M
bp
s
40 
Mb
ps
48 M
bps
Lizen High
School, LZ
3.6
 M
pb
s
S1
Tungs’ Taichung
MetroHarbor
Hospital
2.8 Mpbs
2.
1 
M
pb
s
81 Mbps
Tunghai
University, THU
Tung Grid 1
Tung Grid 2
Tung Grid 3
Tung Grid 4
S6
S5
S4
S3
S2 tc4
tc3
tc2
tc1
host 101
host 102
host 103host 104
lz1
lz2
lz3
lz4
Fig. 12. Tiger grid network.
Fig. 13. Tiger grid resources.
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 841
Author's personal copy
force (Brute), history-based (history), conservative load balancing
(conservative), aggressive load balancing (aggressive), dynamic
co-allocation with duplicate assignments (DCDA), recursively
adjusting mechanism (RAM), dynamic adjustment strategy
(DAS), anticipative recursively adjusting mechanism (ARAM),
and anticipative recursively adjusting mechanism plus (ARAM+).
Using the case setups listed in Table 3 for each scheme, we
analyzed their performance by comparing transfer ﬁnish times
and overall performance, as shown Figs. 20 and 21.
We found that ARAM+ performed better than the others. An
interesting outcome shows the Brute scheme’s ‘‘local’’ perfor-
mance differed greatly from its ‘‘mixed’’ performance. ARAM+ is
comparable to Brute or any others. The advantages of ARAM+ are
the following:
 ARAM+ uses TCP bandwidth measurement technology, relia-
bility and accuracy of the best.
 ARAM+ can enhance GridFTP to become multiplexing.
ARTICLE IN PRESS
Fig. 16. Effects of various replica locations on performance results.
Fig. 17. Effects of various replica numbers and selections on performance results.
Fig. 18. Performance results for scenario A.
Fig. 19. Performance results for scenario B.
Table 3
Replica placement and selection plan.
Mix HIT-S1, S2; LZ-1, 2; THU-beta1, beta2
Local HIT-S1, S2, S3, S4, S5, S6
Fig. 20. Comparing 9 schemes on ‘‘local’’ cases.
Fig. 21. Comparing 9 schemes on ‘‘mixed’’ cases.
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 843
Author's personal copy
Yang CT, Yang IH, Wang SY, Hsu CH, Li KC. A recursively-adjusting co-allocation
scheme with cyber-transformer in data grids. Future Generation Computer
Systems, in press (available online 21 January 2007).
Yang L, Schopf J, Foster I. Improving parallel data transfer times using predicted
variances in shared networks. In: Proceedings of the ﬁfth IEEE international
symposium on cluster computing and the grid (CCGrid ’05), 9–12 May 2005.
p. 734–42.
Zhang X, Freschl J, Schopf J. A performance study of monitoring and information services
for distributed systems. In: Proceedings of 12th IEEE international symposium on
high performance distributed computing (HPDC-12 ‘03), August 2003. p. 270–82.
ARTICLE IN PRESS
C.-T. Yang et al. / Journal of Network and Computer Applications 32 (2009) 834–845 845
99 年度專題研究計畫研究成果彙整表 
計畫主持人：楊朝
棟 計畫編號：99-2622-E-029-001-CC2 
計畫名稱：於協同配置資料網格環境中具適應性複本管理的高效能醫療影像儲傳系統之實作(3/3) 
量化 
成果項目 
實際
已達
成數
（被
接受
或已
發
表） 
預期
總達
成數
(含
實際
已達
成
數)
本
計
畫
實
際
貢
獻
百
分
比 
單
位 
備註（質化說明：如數個計畫共同成果、成果列為該期刊
之封面故事...等） 
期刊論文 0 0 100%  
研究報告 /
技術報告 3 3 
10
0% 
於協同配置資料網格環境中具適應性複本管理的高效能醫療影像
儲傳系統之實作(1/3) 
於協同配置資料網格環境中具適應性複本管理的高效能醫療影像
儲傳系統之實作(2/3) 
於協同配置資料網格環境中具適應性複本管理的高效能醫療影像
儲傳系統之實作(3/3) 
研討會論
文 0 0 
10
0% 
篇 
 
論文
著作 
專書 0 0 100%   
申請中件
數 1 1 
10
0% 
專利名稱: 進階預測遞迴式調整協同配置法 公開號 201032150 
公開日 2010/09/01 申請日 2009/02/19 申請號 098105346 專利 已獲得件
數 0 0 
10
0% 
件 
 
件數 1 1 100% 件 
於協同配置資料網格環境中具適應性複本管理的高效能醫療影像
儲傳系統之實作 技術
移轉 權利金 0 0 100% 
千
元  
碩士生 9 9 100% 
陳秋雄 
楊明峰 
林志豪 
羅裕翔 
陳龍騰 
王冠傑 
周威利 
郭政達 
王文瑋 
國
內 
參與
計畫
人力 
（本
國籍） 
博士生 0 0 10
人
次 
 
專書 0 0 100% 
章
/
本 
 
申請中件
數 0 0 
10
0%  
專利 已獲得件
數 1 1 
10
0% 
件 
ANTICIPATIVE RECURSIVELY-ADJUSTING CO-ALLOCATION MECHANISM
Inventors:  Chao-Tung Yang (Taichung City, TW)  Ming-Feng 
Yang (Taichung City, TW)  
IPC8 Class: AH04L1226FI  
USPC Class: 370252  
Class name: Determination of communication parameters  
Publication date: 08/19/2010  
Patent application number: 20100208599  
 
 
Read more: 
http://www.faqs.org/patents/app/20100208599#ixzz1c8oKbE5P
件數 0 0 100% 件  技術
移轉 權利金 0 0 100% 
千
元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研
究員 0 0 
10
0%  
參與
計畫
人力 
（外
國籍） 
專任助理 0 0 100% 
人
次 
 
其他成
果 
(無法以
量 化 表
達 之 成
果 如 辦
理 學 術
活動、獲
得 獎
項、重要
國 際 合
作、研究
成 果 國
際 影 響
力 及 其
他 協 助
產 業 技
 
