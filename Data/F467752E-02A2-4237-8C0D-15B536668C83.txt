the Internet are recorded in 2D formats. Therefore, 
the development of effective mechanisms for 2D to 3D 
image conversion can help people to really enjoy the 
3D experience. 
 
One key technology of the 2D to 3D image conversion 
is the effective estimation of image depth, i.e. 
computing the relative depth of every two-dimensional 
pixel in the three-dimensional space. There have been 
many related algorithms being proposed in the 
literature, but most of them just ignored an 
essential problem in the depth estimation, i.e. which 
or what kind of image features are able to capture 
the depth characteristics in an image more 
effectively. Therefore, this project targets on the 
feature selection problem of depth estimation. 
Meanwhile, this project will investigate the use of 
different machine learning kernels and probability 
models to evaluate the effectiveness of image 
features. Based on our research work, we propose an 
effective set of image features that can precisely 
capture the depth characteristics so as to establish 
a systematic framework for depth estimation from 
single images. 
英文關鍵詞： Stereo vision, monocular vision, depth cues, depth 
estimation, machine learning 
 
 2
(中文摘要) 
影像顯示技術在近年來已經逐步由傳統的 2D 平面顯示走向更具視覺真實感的 3D 立體呈現。相較
於傳統 2D 影像，3D 視覺內容的一大特色在於包含深度資訊，能夠讓多媒體應用的開發者更準確地掌
握視覺影像中所包含的場景結構，進而提供使用者更為真實且多元的多媒體人機互動應用。然而製作
3D 視覺內容的進入門檻較傳統 2D 影像為高，需要較深的專業知識，因此雖然 3D 顯示裝置漸趨普及，
使用者卻面臨 3D 視覺內容缺乏的窘境。此外，無論是使用者本身所擁有的個人數位資產，或是網際網
路上數以千萬計的影像或影片資料，目前絕大多數仍是以 2D 形式存在。因此，2D 至 3D 影像轉換技術
的有效開發， 將有助於讓一般大眾真正享受 3D 體驗的樂趣。 
 
其中 2D 至 3D 影像轉換的一個關鍵核心技術在於影像深度的有效估測，也就是計算出每一個二維
影像像素在三維空間中的相對深度。從過去以來，雖然在文獻上已有許多相關演算法被陸續提出，然
而它們大多忽略了影像深度估測中一個根本性的問題，那就是到底哪些或是甚麼樣的影像特徵值能夠
較為有效地掌握影像中的深度特性。因此，本計畫針對單張影像深度估測中的影像特徵值選擇問題進
行探討，同時利用不同的機器學習核心或機率模型進行影像特徵值的有效性評估。藉由本計畫所進行
的研究，我們提出有效掌握視覺深度特性的影像特徵值，同時建立一個系統化的單張影像深度估測架
構，期待進一步提昇 3D 視覺內容相關研究的技術強度。 
 
 
 
 
 
 
 
 
 
 4
(一)、 報告內容 
A. 前言 
近年來，影像顯示技術的蓬勃發展，已由傳統的 2D 平面顯示逐步走向更具視覺真實感的 3D 立體
呈現，相關的 3D 影像產品與應用也漸趨多元，例如 3D 數位相機 [1]、3D 投影機[2]、3D 筆記型電腦
[3]等等。而自 2009 年第一部 3D 電影阿凡達(Avatar)問世開始，到 2010 年的世界盃足球賽(FIFA World 
Cup)以 3D 方式轉播，更在全世界帶起一股 3D 熱潮。根據市場調查研究機構 Parks Associates 的最新資
料顯示，電視用戶對於觀賞 3D 電視節目抱持高度興趣，參見圖(一)。因此，數位內容的 3D 化，已成
為多媒體領域一個重要的發展趨勢。 
 
 
雖然 3D 顯示技術漸趨成熟，然而製作 3D 立體視覺內容(3D Stereo Digital Content)的進入門檻較傳
統 2D 影像為高，需要較深的專業知識，因此主要仍仰賴專門人員產生[5][6][7]。從技術上來說，3D 視
覺內容所帶來的立體感(Sense of Stereo)，主要是利用人類視覺系統(Human Visual System)的成像原理
[7][8]：由於人的兩眼之間有一定距離(Interaxial Separation)，因此分別從左、右眼觀察場景中的某一點
時存在雙眼視覺像差(Binocular Disparity)，簡稱視差(Disparity)，可用來做為該點深度(Depth)資訊的估
測參考。在圖(二)中，我們以相機取代人眼做為實例來加以說明。給定 P 為三維場景中的某一點，Ol
與 Or 分別為左、右相機成像平面的原點座標，則 pl 與 pr 為 P 在左、右相機的二維投影。假設兩台相
機的焦距均為 f，利用(P,pl,pr)與(P,Ol,Or)為相似三角形的關係，則可推算出 P 點的深度值 Z =
d
Tf , 其
中 d = xr - xl為 P 在兩個相機成像平面上所產生的視差，當 P 點離觀察者越遠則視差越小，反之越大。 
圖(一)、電視用戶對於電視節目 3D 化的感興趣程度[4]。 
 6
讓觀看者感受到影像所包含的內容本身有任何變化。然而，當我們以相同縮放技術直接套用至立體影
像對的左、右影像後，觀看者卻容易察覺到影像中所包含物體的相對深度有所改變，甚至是在物體本
身產生深度上的視覺失真(Visual Distortion)[12]。如圖(三)所示，其主要原因在於視差值與觀看者所感
受到的視覺深度並非呈現等比關係，因此視差值上的線性改變，卻會以非線性方式改變我們所感受到
的深度知覺(Depth Perception)，進而影響到觀看者對於影像內容本身的理解。其他相關的 3D 視覺內容
處理技術與應用也面臨同樣的挑戰，例如 3D 電視場景呈現(3DTV Scene Representation)[11]、3D 影像
調適(3D Image Retargeting)[13]、以及 3D 影片編修(3D Video Authoring)[14]等等。 
 
      
 
 
 
 
 
 
 
 
II. 2D 至 3D 影像轉換 
隨著 3D 顯示裝置的普及化，使用者所面臨的一個問題是 3D 視覺內容的缺乏。此外，無論是使用
者本身所擁有的個人數位資產，或是網際網路上數以千萬計的影像或影片資料，目前絕大多數仍是以
2D 形式存在。因此，2D 至 3D 影像轉換技術的有效開發，不僅有助於解決 3D 視覺內容不足的問題，
同時也能夠協助一般大眾自行轉換其個人 2D 數位資產，享受 3D 體驗的樂趣。如圖(四)所示，2D 至
3D 影像轉換技術的核心，主要是在二維影像之對應深度圖(Depth Map)的產生，也就是估測每一個二維
影像像素(Pixel)在三維空間中的相對深度[8][15][16]。根據給定 2D 影像數量的不同，我們可以將深度
圖產生問題分為兩類，分別是單張影像深度估測 (或稱單眼視覺深度估測，Monocular Depth 
Estimation)，以及二或多張以上影像深度估測(或稱雙眼或多眼視覺深度估測，Binocular or Multi-Ocular 
Depth Estimation)[17][18]。 
用來估測深度的影像視覺特性，我們稱之為深度線索(Depth Cue)[8]，如離焦程度(Defocus)、線性
透視(Linear Perspective)、大氣散射(Atmosphere Scattering)、表面著色(Shading)、圖形紋理(Patterned 
Texture)、重疊現象(Occlusion)、與圖形統計(Statistical Pattern)等等[8][12][17][19][20]。我們可以發現，
以上所列的深度線索大多需要對影像本身有一些較強的假設，且往往存在分析上的侷限性。因此，2D
至 3D 影像轉換技術的相關研究在近年來逐漸捨棄以實際物理特性為基礎的深度估測方式，轉而採用圖
      圖(四)、單張二維影像(左)與其深度圖(右)之範例圖。 
 8
Problem)進行探討，同時利用不同的機器學習核心或機率架構進行影像特徵值的有效性評估。依據有效
性評估的結果以及所選擇影像特徵值的分類特性，例如是屬於空間領域 (Spatial Domain)或是頻率領域
(Frequency Domain)，分析該些影像特徵值所掌握的視覺深度特性，進而嘗試研發更為有效的影像特徵
值。本計畫之重要性在於，針對深度估測問題，目前文獻上仍缺乏關於影像特徵值之估測有效性的大
規模系統性分析(Systematic Analysis)，而本計畫之研究成果將可做為未來相關研究的重要參考，同時
藉以提升相關 3D 多媒體應用與演算法的實用性與準確性。 
 
C. 研究方法與成果 
 
本計畫之具體研究成果為開發出一套用於單張影像深度估測之有效影像特徵值選擇機制。我們
提出了以 SVM (Support Vector Machines)機器學習方法為基底的單一影像深度估測架構，利用許多
近期提出的影像描述方法，可以有效率並且準確的估測影像的深度結構。 
 
 
 
 
整個架構如圖(五)所示，分為訓練階段(Training)以及測試階段(Testing)兩大步驟，各個部分的詳細
研究方法以及成果我們會在以下內容中作詳盡的說明。 
C.1. 【標準深度資料收集】 
影像深度估測的基礎在於是否能夠有效率且正確地取得大量具有代表性的標準深度資料
(Ground Truth)。目前所提出的標準深度資料收集方式均需仰賴特殊或專業的軟硬體設備，例如
使用一般數位相機搭配 3D 雷射掃描器(3D Laser Scanner)取得真實影像場景中每一個像素所對
應的深度值，或是利用光線追蹤技術 (Ray Tracing)合成具已知深度資訊的虛擬影像場景
圖(五)、本計畫所提出以 SVM 為基底的單一影像深度估測架構圖。 
 10
 頻率: 單張影響中的頻率以及相關深度值的關係在[35]中研究過。我們利用離散餘弦
變換(DCT)的參數來當作頻率描述符號 DCTP 。我們針對每個 m×m 的區塊套用二維的
離散餘弦變換，因此每個 m×m 的區塊的 DCTP 的長度是 m×m。 
C.3. 【進階影像特徵值萃取】 
進階的影像描述符號如 SIFT 帶來電腦視覺的新機會。這些進階的描述符號可以有效的幫助
深度估測的演算法。相較於 SIFT，近期被提出的 DAISY 描述符號[34]更適合用於深度估測演算
法使用。給定了 m×m 的區塊， DAISYP 的長度是 8×25= 200。 
C.4. 【影像深度估計】 
如圖(五)所示，本項計畫模組主要是利用標準深度資料收集與一般及進階影像特徵值萃取兩
個部分所產生的結果，建立影像深度估測的機器學習核心與機率架構。我們將每張影像分成 8×8
的大小，每個包含了所有影像描述符號的特徵向量高達了 774 個維度，並且每張影像擁有 43,200 
(90×(640×480)/(8×8))個特徵向量，也因此我們將單張影像的深度估測問題視為一個極大量的分
類問題(Classification Problem)。 
C.5. 【估測品質分析】 
   這個部分主要是負責分析影像深度估測模組所產生的深度品質，將會用以下兩項標準來分析
估測出來的深度值: 
 深度錯誤: |loglog| 1010log dde  , d 代表實際的深度值， d 為估計出來的深度值。 
 相對深度錯誤: 
d
ddeREL
||   
Mixed image descriptor Average loge  Average RELe  
All 0.194 0.232 
All without TextureP  0.287 0.360 
All without ColorP  0.228 0.282 
All without DAISYP  0.261 0.331 
All without DCTP  0.193 0.233 
表(一)、利用混合的影像描述符號平均的深度錯誤以及相對深度錯誤。 
表(一)顯示了本架構所估測深度資訊的品質。我們的架構利用了所有的影像描述符號之後，的確
可以表現得比目前文獻上所提出方法的效能有顯著提升[21,22,23]。 
 
 
 
 12
[17] Y. Furukawa and J. Ponce, “Accurate, dense, and robust multiview stereopsis,” IEEE Trans. Pattern 
Analysis and Machine Intelligence, vol. 32, no. 8, pp. 1362-1376, Aug. 2010. 
[18] D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo 
correspondence algorithms,” Intl. Journal of Computer Vision, vol. 47, no. 1/2/3, pp. 7-42, 2002. 
[19] S.T. Barnard and M.A. Fischler, “Computational stereo,” Computing Surveys, vol. 14, no. 4, pp. 
553-572, Dec. 1982. 
[20] D. Hoiem, A. Efros, and M. Herbert, “Geometric context from a single image,” Proc. 10th Intl. Conf. 
Computer Vision (ICCV), 2005. 
[21] A. Saxena, M. Sun, and A.Y. Ng, “Learning 3-D scene structure from a single still image,” Proc. 
ICCV Workshop on 3D Representation for Recognition (3DRR), 2007. 
[22] A. Saxena, M. Sun, and A.Y. Ng, “Make3D: Learning 3D scene structure from a single still image,” 
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 824-840, May 2009. 
[23] B. Liu, S. Gould, D. Koller, “Single image depth estimation from predicted semantic labels,” Proc. 
IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 1253-1260, 2010. 
[24] Xbox 360 Kinect, Microsoft Corp., http://www.xbox.com/kinect/. 
[25] D. Wagner, G. Reitmayr, A. Mulloni, T. Drummond, and D. Schmalstieg, “Real-time detection and 
tracking for augmented reality on mobile phones,” IEEE Trans. Visualization and Computer Graphics, 
vol. 16, no. 3, pp. 355-368, May-June 2010. 
[26] B. Leibe, N. Cornelis, K. Cornelis, and L. van Gool, “Dynamic 3D scene analysis from a moving 
vehicle,” Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2007. 
[27] A. Ess, B. Leibe, K. Schindler, and L. van Gool, “Robust multiperson tracking from a mobile 
platform,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 10, pp. 1831-1846, Oct. 
2009. 
[28] D. Geronimo, A.M. Lopez, A.D. Sappa, and T. Graf, “Survey of pedestrian detection for advanced 
driver assistance systems,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 7, pp. 
1239-1258, July 2010. 
[29] F. Moreno-Noguer, P.N. Belhumeur, S.K. Nayar, “Active refocusing of images and videos,” Proc. 
SIGGRAPH, 2007. 
[30] I. Mihara, T. Harashima, S. Numazaki, and M. Doi, “Pop.eye: a pop-out video camera system for 
personal use,” Proc. SIGGRAPH, 2002. 
[31] K.C. Zheng, A. Colburn, A. Agarwala, M. Agrawala, B. Curless, D. Salesin, and M. Cohen, 
“Parallax photography: Creating 3D cinematic effects from stills,” Proc. Graphic Interface, 2009. 
[32] D. Nister, O. Naroditsky, and J.R. Bergen, “Visual Odometry,” Proc. IEEE Conf. Computer Vision 
and Pattern Recognition (CVPR), 2004. 
[33] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based object tracking,” IEEE Trans. Pattern 
Analysis and Machine Intelligence, vol. 25, no. 5, pp. 564-575, May 2003. 
[34] E. Tola, V. Lepetit, and P. Fua, “Daisy: An efficient dense descriptor applied to wide-baseline 
stereo,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol.32, no. 5, pp. 815–830, May 
2010. 
[35] A. Torralba and A. Oliva, “Depth estimation from image structure,” IEEE Trans. Pattern Analysis 
and Machine Intelligence, vol. 24, no. 9, pp. 1226–1238, Sept. 2002. 
表 Y04 
議，本次會議並無明確的專題演講(Keynote Speech)議程，而是依主題分散在不同的
Workshop 之間，因此也往往必須事先安排每個時段欲參加的場次，方能避免遺珠之憾。
 
我們此次所發表的論文，主題是關於影像檢索，被安排在會議第三天下午 17:10 到
18:30 的“Optimization, Vision for Robotics, Scene Understanding, Image and Video 
Retrieval”議程，在同一個議程中的其它論文也都是屬於相關主題，因此除了其它會議的
參加者，同一個議程中的報告者彼此之間也都有熱烈的討論與互動，例如我們在報告現
場就與來自其他國內外研究團隊的研究學者有著面對面的直接交流，包括香港中文大學
電子工程學系的王曉剛助理教授、南洋理工大學計算機工程學院的徐東助理教授、美國
University of California at Merced 的楊明玄教授、清華大學資訊工程學系的賴尚宏教授、
以及中央研究院資訊科技創新研究中心的陳祝嵩研究員等等。 
 
 
圖(一)：CVPR 會議現場一隅，由左至右為陳祝嵩研究員、楊明玄教授、報告人、以及
賴尚宏教授。 
 
二、與會心得 
 
在本次會議中，可以發現所發表之論文的研究水準普遍而言相當不錯，而其中一部
分在想法或做法上有較為創新的突破，對於本身未來的研究工作，或是相關領域的研究
發展，均具有相當大的啟發性。例如，在第一天的“Large Scale Learning for Vision”議程
中，邀請到了史丹佛大學的電腦視覺專家 Fei-Fei Li 教授發表題為“Building the Forest: 
Large Scale Data and Modeling in Computer Vision”的專題演說。Li 教授的研究團隊在過
去發表了一個目前在電腦視覺研究領域中被廣泛採用的影像資料庫，稱為 ImageNet。而
Li 教授在本專題演說中，即針對 ImageNet 資料庫在收集影像資料時所採用的原則、與
實際電腦視覺問題的關聯性、ImageNet 影像資料的視覺特性、以及與其它現存影像資料
庫的差異性進行詳盡的介紹。Li 教授其中所提到的一點，是大部分的影像資料庫所包含
的影像資料，在不同物件間的影像數量是大致相同，例如“車”與“花”物件種類均包含五
千張影像，然而真實世界中不同物間種類所包含的影像資料數量是呈現長尾分布
(Long-Tail Distribution)的現象，也就是只有少量的物件種類是包含大量的影像資料，而
大部份物件種類的影像數量是有限的。這個現象對於電腦視覺分析而言，也是一個極大
的挑戰，在部分機器學習的文獻中所探討的非平衡訓練資料問題(Unbalanced Training 
Data Problem)也是這個現象的呈現。因此 Li 教授團隊在收集 ImageNet 的影像資料時，
即故意保留此特性以反映真實的影像資料分布狀況。 
 
在會議第三天的“Stereo and Structure from Motion, Performance Evaluation, Object 
Detection”議程中，美國麻省理工學院的知名教授 Antonio Torralba 也發表了一篇極為有
CVPR 2011 Paper secision
1 message
Wen-Huang Cheng <whcheng@gmail.com> 
Conference Management Toolkit <cmt@microsoft.com> Mon, Feb 14, 2011 at 7:52 AM 
Reply-To: pff@cs.uchicago.edu 
To: whcheng@citi.sinica.edu.tw 
Dear Wen-Huang Cheng,
We are writing to inform you that CVPR decisions have been finalized
and are now available for viewing at the CVPR submission site.
The status of paper 1492 - Unsupervised Auxiliary Visual Words Discovery for Large-Scale Image Object 
Retrieval is Accepted as Poster
Many strong papers that were deemed not quite ready for CVPR were
rejected after consideration by three qualified reviewers, a primary
area chair, and a secondary area chair.  In close cases, a panel of 10
or 12 area chairs discussed the paper and approved the accept or
reject decision.  Since there is no revision process for CVPR, the
best advice to authors whose papers were not able to be accepted is to
revise and submit to the next appropriate venue.
Statistics regarding acceptance and rejection rates will be available
on the CVPR web site within a few days.
More instructions will follow for papers that are being accepted.
CVPR 2011 PC
頁 1 / 1Gmail - CVPR 2011 Paper secision
2012/1/16https://mail.google.com/mail/?ui=2&ik=edab1d4970&view=pt&q=cvpr&qs=true&sear...
tive AVWs across the visual and textual graphs since these
two modalities can boost each other (cf. Figure 3). The two
processes are formulated as optimization formulations itera-
tively through the subtopics in the image collections. Mean-
while, we also consider the scalability issues by leveraging
distributed computation framework (e.g., MapReduce).
Experiments show that the proposed method greatly im-
proves the recall rate for image object retrieval. Specifically,
the unsupervised auxiliary visual words discovery greatly
outperforms BoWmodels (by 111% relatively) and comple-
mentary to conventional pseudo-relevance feedback. Mean-
while, AVW discovery can also derive very compact (i.e.,
1.4% of the original features) and informative feature rep-
resentations which will benefit the indexing structure [14].
The primary contributions of the paper include,
• Observing the problems in large-scale image object re-
trieval by conventional BoW model (Section 3).
• Proposing auxiliary visual words discovery through vi-
sual and textual clusters in an unsupervised and scal-
able fashion (Section 4).
• Investigating variant optimization methods for effi-
ciency and accuracy in AVW discovery (Section 5).
• Conducting experiments on consumer photos and
showing great improvement of recall rate for image ob-
ject retrieval (Section 7).
2. Related Work
Most image object retrieval systems adopt the scale-
invariant feature transform (SIFT) descriptor [8] to capture
local information and adopt bag-of-words (BoW) model
[14] to conduct object matching [1, 11]. The SIFT descrip-
tors are quantized to visual words (VWs), such that index-
ing techniques well developed in the text domain can be
directly applied.
The learned VW vocabulary will directly affect the im-
age object retrieval performance. The traditional BoW
model adopts k-means clustering to generate the vocabu-
lary. A few attempts try to impose extra information for vi-
sual word generation such as visual constraints [13], textual
information [19]. However, it usually needs extra (manual)
information during the supervised learning, which might be
formidable in large-scale image collections.
Instead of generating new VW vocabulary, some re-
searches work on the original VW vocabulary such as [15].
It suggested to select useful feature from the neighboring
images to enrich the feature description. However, its per-
formance is limited for large-scale problems because of
the need to perform spatial verification, which is compu-
tationally expensive. Moreover, it only considers neighbor-
ing images in the visual graph, which provides very lim-
ited semantic information. Other selection methods for the
0
20
40
60
80
100
0 0.2 0.4 0.6 0.8 1 1.2
Images (%)
V
is
u
a
l 
w
o
rd
s
 (
%
)
Flickr11K (11,282 images)
Flickr550 (540,321 images)
Half of the visual words occur in
Flickr11K: 0.106% (12 images)
Flickr550: 0.114% (617 images)
Figure 2. Cumulative distribution of the frequency of VW occur-
rence in two different image databases, cf. Section 3.1. It shows
that half of the VWs occur in less than 0.11% of the database im-
ages (i.e., 12 and 617 images, respectively). The statistics rep-
resent that VWs are distributed over the database images very
sparsely.
useful features such as [6] and [10] are based on different
criteria—the number of inliers after spatial verification, and
pairwise constraints for each image, thus suffer from lim-
ited scalability and accuracy.
Authors in [9] consider both visual and textual informa-
tion and adopt unsupervised learning methods. However,
they only use global features and adopt random-walk-like
process for post-processing in image retrieval. Similar limi-
tations are observed in [16], where only the similarity image
scores are propagated between textual and visual graphs.
Different from the prior works, we use local features for
image object retrieval and propagate the VWs directly be-
tween the textual and visual graphs. The discovered aux-
iliary VWs are thus readily effective in retrieving diverse
search results, eliminating the need to apply a random walk
in the graphs again.
To augment images with their informative features, we
propose auxiliary visual words discovery, which can effi-
ciently propagate semantically relevant VWs and select rep-
resentative visual features by exploiting both textual and vi-
sual graphs. The discovered auxiliary visual words demon-
strate significant improvement over the BoW baseline and
are shown orthogonal and complementary to conventional
pseudo-relevance feedback. Besides, when dataset size be-
comes larger, we can apply all the processes in a parallel
way (e.g., MapReduce).
3. Key Observations—Requiring Auxiliary Vi-
sual Words
Nowadays, bag-of-words (BoW) representation [14] is
widely used in image object retrieval and has been shown
promising in several content-based image retrieval (CBIR)
tasks (e.g., [11]). However, most existing systems sim-
ply apply the BoW model without carefully considering the
sparse effect of the VW space, as detailed in Section 3.1.
906
(a) A visual cluster sample. (b) A textual cluster sample.
Figure 4. Sample image clusters (cf. Section 4.1). The visual clus-
ter groups visually similar images in the same cluster, whereas the
textual cluster favors semantic similarities. The two clusters facil-
itate representative VWs selection and semantic (auxiliary) VWs
propagation, respectively.
4.2. Semantic Visual Words Propagation
Seeing the limitations in BoWmodel, we propose to aug-
ment each image with additional VWs propagated from the
visual and textual clusters (Figure 5(a)). Propagating the
VWs from both visual and textual domains can enrich the
visual descriptions of the images and be beneficial for fur-
ther image object queries. For example, it is promising to
derive more semantic VWs by simply exchanging the VWs
among (visually diverse but semantically consistent) images
of the same textual cluster (cf. Figure 4(b)) .
We actually conduct the propagation on each extended
visual cluster, containing the images in a visual cluster and
those additional ones co-occuring with these images in cer-
tain textual clusters. The intuition is to balance visual and
semantic consistence for further VW propagation and se-
lection (cf. Section 4.3). Figure 5(b) shows two extended
visual clusters derived from Figure 5(a). More interestingly,
image E is singular in textual cluster due to having no tags;
however, E still belongs to a visual cluster and can still re-
ceive AVWs in its associated extended visual cluster. Sim-
ilarly, if there is a single image in a visual cluster such as
imageH , it can also obtain auxiliary VWs (i.e., from image
B and F ) in the extended visual cluster.
Assuming matrix X ∈ RN×D represents the N image
histograms in the extended visual cluster and each image
has D (i.e., 1 million) dimensions. And Xi stands for the
VW histogram of image i. Assume M among N are from
the same visual cluster; for example, N = 8 and M = 4
in the left extended visual cluster in Figure 5(b). The vi-
sual propagation is conducted by the propagation matrix
P ∈ RM×N , which controls the contributions from differ-
ent images in the extended visual cluster.1 P (i, j) weights
1Note that here we first measure the images from the same visual cluster
only. However, by propagating through each extended visual clusters, we
can derive the AVWs for each image.
C
F
A
E
Visual clusters
Textual clusters
H
A D
G
C
F
B
H
E
(a) Visual and textual graphs.
C
F
A
E
From visual clusters
From textual clusters
G
D
B
H
H
B
F
(b) Two extended visual clus-
ters from the (left) visual and
textual clusters.
Figure 5. Illustration of the propagation operation. Based on vi-
sual and textual graphs in (a), we can propagate auxiliary VWs
among the associated images in the extended visual clusters. (b)
shows the two extended visual clusters as the units for propaga-
tion respectively; each extended visual cluster include the visually
similar images and those co-occurrences in other textual clusters.
the whole features propagated from image j to i. If we mul-
tiply the propagation matrix P andX (PX), we can obtain
a new M × D VW histograms, as the AVWs, for the M
images augmented by the N images.
For each extended visual cluster, we desire to find a bet-
ter propagation matrix P , given the initial propagation ma-
trix P0 (i.e., P0(i, j) = 1, if both i and j are semantically
related and within the same textual cluster). We propose to
formulate the propagation operation as
fP = min
P
α
‖PX‖2F
NP1
+ (1− α)‖P − P0‖
2
F
NP2
, (1)
The goal of the first term is to avoid from propagating too
many VWs (i.e., propagating conservatively) since PX be-
comes new VW histogram matrix after the propagation.
And the second term is to keep the similarity to the orig-
inal propagation matrix (i.e., similar in textual cluster).
NP1 = ‖P0X‖2F and NP2 = ‖P0‖2F are two normaliza-
tion terms and αmodulates the importance between the first
and the second terms. We will investigate the effects of α
in Section 7.2. Note that the propagation process updates
the propagation matrix P on each extended visual cluster
separately as shown in Figure 5(b); therefore, this method
is scalable for large-scale dataset and easy to adopt in a par-
allel way.
4.3. Common Visual Words Selection
Though the propagation operation is important to obtain
different VWs, it may include too many VWs and thus de-
crease the precision. To mitigate this effect and remove
those irrelevant or noisy VWs, we propose to select those
representative VWs in each visual cluster. We observe that
images in the same visual cluster are visually similar to each
other (cf. Figure 4(a)); therefore, the selection operation is
to retain those representative VWs in each visual cluster.
908
the matrices and get
(X⊗IM )(XT⊗IM )p = (X⊗IM )vec(PX) = vec(PXXT )
Then,
∇pf(p) = 2α1vec(PXXT ) + 2α2vec(P − P0)
= vec(2α1PXXT + 2α2(P − P0)).
That is, we can update pold as a matrix P old with the gra-
dient also represented in its matrix form. Coupling the up-
date scheme with an adaptive learning rate η, we get update
propagation matrix by
Pnew =P old − 2η(α1P oldXXT + α2(P old − P0)).(6)
Note that we simply initialize pstart to vec(P0).
For the selection formulation (Section 4.3), we can adopt
similar steps with two changes. And let β1 = βNS1 and
β2 =
1−β
NS2
. First, Eq. (6) is replaced with
Snew =Sold − 2η(−β1XTX(S0 − Sold) + β2Sold).(7)
Second, the initial point Sstart is set to a zero matrix since
the goal of selection formulation is to select representative
visual words (i.e., retain a few dimensions).
There is one potential caveat of directly using Eq. (7)
for updating. The matrix XTX can be huge (e.g., 1M ×
1M ). To speed up the computation, we could keep only the
dimensions that occured in the same visual cluster, because
the other dimensions would contribute 0 to XTX .
5.3. Analytic Solver (AS)
Next, we compute the unique optimal solution p∗ of
Eq. (1) analytically. The optimal solution must satisfy
∇pf(p∗) = 0. Note that From Eq. (4),
∇pf(p∗) = Hp∗ − 2α2p0,
where H is the constant and positive definite Hessian ma-
trix. Thus,
p∗ = 2α2H−1p0.
Similar to the derivation in the gradient descent solver, we
can write down the matrix form of the solution, which is
P ∗ = α2P0(α1XXT + α2IM )−1.
For the selection formulation, a direct solution from the
steps above would lead to
S∗ = β1(β1XTX + β2ID)−1XTXS0. (8)
Nevertheless, as mentioned in the previous subsection,
theXTX matrix in Eq. (8) can be huge (e.g., 1M×1M ). It
is a time-consuming task to compute the inverse of an 1M×
1M matrix. Thus, instead of calculating XTX directly, we
transform XTX to XXT which is N by N and is much
smaller (e.g., 100 × 100). The transformation is based on
the identity of the inverse function
(A+BBT )−1B = A−1B(I +BTA−1B)−1.
Then, we can re-write Eq. (8) as
S∗ = β1XT (β1XXT + β2IN )−1XS0. (9)
Note that the analytic solutions of Eq. (1) and (2) are of a
similar form to the solutions of ridge regression (Tikhonov
regularization) in statistics and machine learning. The fact
is of no coincidence. Generally speaking, we are seeking
to obtain some parameters (P and S) from some data (X ,
P0 and S0) while regularizing by the norm of the param-
eters. The use of the regularization not only ensures the
strict convexity of the optimization problem, but also eases
the hazard of overfitting with a suitable choice of α and β.
6. Experimental Setup
6.1. Dataset
We use Flickr550 [20] as our main dataset in the exper-
iments. To evaluate the proposed approach, we select 56
query images (1282 ground truth images) which belong to
the following 7 query categories: Colosseum, Eiffel Tower,
Golden Gate Bridge, Tower de Pisa, Starbucks logo, Tower
Bridge, and Arc de Triomphe. Also, we randomly pick
up 10,000 images from Flickr550 to form a smaller sub-
set called Flickr11K.2 Some query examples are shown in
Figure 7.
6.2. Performance Metrics
In the experiments, we use the average precision, a
performance metric commonly used in the previous work
[11, 20], to evaluate the retrieval accuracy. It approximates
the area under a non-interpolated precision-recall curve for
a query. A higher average precision indicates better retrieval
accuracy. Since average precision only shows the perfor-
mance for a single image query, we also compute the mean
average precision (MAP) over all the queries to evaluate the
overall system performance.
6.3. Evaluation Protocols
As suggested by the previous work [11], our image ob-
ject retrieval system adopts 1 million visual words as the ba-
sic vocabulary. The retrieval is then conducted by compar-
ing (indexing) the AVW features for each database image.
To further improve the recall rate of retrieval results, we
apply the query expansion technique of pseudo-relevance
2http://www.cmlab.csie.ntu.edu.tw/%7Ekuonini/Flickr11K
910
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAP (M)
Alpha
0
0.2
0.4
0.6
0
20
40
60
80
100
MAP by PRF MAP #Features
(a) Alpha (α, for propagation in Eq. (1))
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAP (M)
Beta
0
0.1
0.2
0.3
0.4
0
10
20
30
MAP by PRF MAP #Features
(b) Beta (β, for selection in Eq. (2))
Figure 8. Parameter sensitivity on alpha and beta. (a) shows that
propagating too many features is not helpful for the retrieval accu-
racy. (b) shows that only partial features are important (represen-
tative) to each image. More details are discussed in Section 7.2.
Note that we can further improve retrieval accuracy by iteratively
updated AVW by propagation and selection processes.
features needed to be propagated. Figure 8(a) shows that if
we propagate all the possible features to each image (i.e.,
α = 0), we will obtain too many irrelevant and noisy fea-
tures which is helpless for the retrieval accuracy. Besides,
the curve drops fast after α ≥ 0.8 because it preserved few
VWs which might not appear in the query images. The fig-
ure also shows that if we set α around 0.6we can have better
result but with fewer features which are essential for large-
scale indexing problem.
And for selection formulation, similar to α, β also influ-
ences the number of dimensions needed to be retained. For
example, if β = 0, we will not select any dimensions for
each image. And β = 1 means we will retain all the fea-
tures, and the result is equal to the BoW baseline. Figure
8(b) shows that if we just keep a few dimensions of VWs,
the MAP is still similar to BoW baseline though with some
retrieval accuracy decrease. Because of the spareness of
large VW vocabulary as mentioned in Section 3.1, we only
need to keep those important VWs.
8. Conclusions and Future Work
In this work, we show the problems of current BoW
model and the needs for semantic visual words to improve
the recall rate for image object retrieval. We propose to aug-
ment each database image with semantically related auxil-
iary visual words by propagating and selecting those infor-
mative and representative VWs in visual and textual clus-
ters. Note that we formulate the processes as unsupervised
optimization problems. Experimental results show that we
can greatly improve the retrieval accuracy compared to the
BoW model (111% relatively). In the future, we will fur-
ther look into the problem by L2-loss L1-norm optimiza-
tion which might preserve the sparseness for visual words.
We will also investigate different solvers to maximize the
retrieval accuracy and efficiency.
References
[1] O. Chum et al. Total recall: Automatic query expansion
with a generative feature model for object retrieval. In IEEE
ICCV, 2007.
[2] O. Chum et al. Geometric min-hashing: Finding a (thick)
needle in a haystack. In IEEE CVPR, 2009.
[3] J. Dean et al. Mapreduce: Simplified data processing on
large clusters. In OSDI, 2004.
[4] T. Elsayed et al. Pairwise document similarity in large col-
lections with mapreduce. In Proceedings of ACL-08: HLT,
Short Papers, pages 265–268, 2008.
[5] B. J. Frey et al. Clustering by passing messages between data
points. Science, 2007.
[6] S. Gammeter et al. I knowwhat you did last summer: Object-
level auto-annotation of holiday snaps. In IEEE ICCV, 2009.
[7] J. Hays et al. im2gps: estimating geographic information
from a single image. In IEEE CVPR, 2008.
[8] D. G. Lowe. Distinctive image features from scale-invariant
keypoints. IJCV, 2004.
[9] H. Ma et al. Bridging the semantic gap between image con-
tents and tags. IEEE TMM, 2010.
[10] P. K. Mallapragada et al. Online visual vocabulary pruning
using pairwise constraints. In IEEE CVPR, 2010.
[11] J. Philbin et al. Object retrieval with large vocabularies and
fast spatial matching. In IEEE CVPR, 2007.
[12] J. Philbin et al. Lost in quantization: Improving particu-
lar object retrieval in large scale image databases. In IEEE
CVPR, 2008.
[13] J. Philbin et al. Descriptor learning for efficient retrieval. In
ECCV, 2010.
[14] J. Sivic et al. Video Google: A text retrieval approach to
object matching in videos. In IEEE ICCV, 2003.
[15] P. Turcot et al. Better matching with fewer features: The se-
lection of useful features in large database recognition prob-
lems. In IEEE ICCV Workshop on WS-LAVD, 2009.
[16] X.-J. Wang et al. Multi-model similarity propagation and its
application for web image retrieval. In ACM MM, 2004.
[17] X.-J. Wang et al. Annosearch: Image auto-annotation by
search. In IEEE CVPR, 2006.
[18] X.-J. Wang et al. Arista - image search to annotation on
billions of web photos. In CVPR, 2010.
[19] L. Wu et al. Semantics-preserving bag-of-words models for
efficient image annotation. In ACM workshop on LSMMRM,
2009.
[20] Y.-H. Yang et al. Contextseer: Context search and recom-
mendation at query time for shared consumer photos. In
ACM MM, 2008.
[21] X. Zhang et al. Efficient indexing for large scale visual
search. In IEEE ICCV, 2009.
912
99 年度專題研究計畫研究成果彙整表 
計畫主持人：鄭文皇 計畫編號：99-2218-E-001-012- 
計畫名稱：單張二維影像深度估測技術之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 1 0 100% 
人次 
 
