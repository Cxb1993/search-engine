 2
 
In the manufacturing process of cellular 
phones shown as Figure 1, the Radio 
Frequency (RF) function is a crucial test and 
needs more operation time than any of the 
other inspection processes. In order to save 
inspection costs and shorten production time, 
manufacturers need an effective method to 
reduce the RF function test items. A number 
of soft computing approaches, such as neural 
networks [27], genetic algorithms (GA) [32], 
decision tree and rough sets [22, 23] have 
been widely used to remove irrelevant, 
unnecessary, and redundant attributes (test 
items). However, when these methods are 
applied to real world problems, there are 
many issues that need to be addressed. One 
of them is the “imbalanced data” problem 
which almost all the instances are labeled as 
one class while far few instances are labeled 
as the other class [5, 10]. When learning 
from such imbalanced data, traditional 
classifiers often produce high accuracy over 
the majority class, but poor predictive 
accuracy over the minority class (usually the 
important class).  
In modern production systems, the 
defective rate of products is becoming quite 
low. In the six sigma quality management 
system for example, we should use 
“ppm”(parts per million) instead of “%” to 
calculate the defective rate. In a mature 
manufacturing industry the amount of good 
products far exceeds the defective products. 
This type of data is so-called “imbalanced 
data.” When feature selection approaches 
encounter imbalanced data such as this, it 
becomes difficult to acquire knowledge from 
the few negative examples (defective 
products). Fewer abnormal products will be 
viewed as outliers or bias by feature selection 
methods [17]. This leads to a high level of 
type II errors (customer risks, the probability 
that customers accept defective products) 
which are critical to OEM/EMS companies. 
A high level of type II errors will cause great 
losses, requires compensation and may result 
in the loss of orders from important 
customers.  
In this study, we propose a neural 
network based information granulation 
approach which can effectively reduce RF 
function test items. A real case with 
imbalanced data is studied, and the 
implementation results show that our 
proposed method can find relevant test items 
without losing classification accuracy and 
increasing the type II errors. 
 
三、結果與討論 
 
The actual case comes from a cellular 
phone OEM/ODM company which was 
established in 1984. It is located in Taiwan 
and the company owns several factories in 
mainland China. In 2003, its total annual 
revenue reached US 4.713 billion dollars, 
and it has a worldwide workforce of over 10 
000. The production volume of cellular 
phones in 2004 was about 7.5 million units.  
 
3.1 The Problem 
In this case, the objectives of the 
cellular phone manufacturer are to reduce 
test time and consequently cost. Figure 1 
provides the manufacturing process of the 
cellular phone including the operation time 
of each process. We find that the RF 
functional test is the bottleneck of entire 
process. The RF test is aimed at inspecting 
whether or not the mobile phone 
receive/transmit signal satisfies the enabled 
transmission interval (ETI) protocol on 
different channels and different power levels. 
In order to ensure the quality of 
communication of mobile phones, the 
manufacturers usually add extra inspection 
items, such as several different frequency 
channels and power levels, resulting in the 
inspection time being increased and as a 
result the test procedure becomes a 
bottleneck. 
 If we can reduce the numbers of items 
 4
Rule 1:  
)7446.31,7440.31[725∈B  
AND ,*]1(102114 ∉−H  TEHN Class = 
Good Product [Accuracy: 1.0; Supports: 24] 
Rule 2:  
)7446.31,7440.31[725∈B  AND 
,*]1(102114 ∈−H TEHN Class = Bad 
Product [Accuracy: 1.0; Supports: 2] 
Rule 3:  
)7446.31,7440.31[725∉B  AND 
,*]1(102114 ∉−H  TEHN Class = Bad 
Product [Accuracy: 1.0; Supports: 4] 
Rule 4:  
)7446.31,7440.31[725∉B  AND 
,*]1(102114 ∈−H  TEHN Class = Bad 
Product [Accuracy: 1.0; Supports: 3] 
 
Figure 2(a). Knowledge rules extracted by rough 
sets 
 
Rule 1:  
)7446.31,7440.31[725∈B AND 
,*]1(102114 ∉−H  TEHN Class = Good 
Product [Accuracy: 0.962; Supports: 24] 
Rule 2: 
)7446.31,7440.31[725∉B  TEHN Class = 
Bad Product  
[Accuracy: 0.889; Supports: 7] 
Rule 3: 
,*]1(102114 ∈−H  TEHN Class = Bad 
Product  
[Accuracy: 0.857; Supports: 5] 
 
Figure 2(b). Knowledge rules extracted by 
decision tree (C 4.5) 
 
3.5 Feature Selection and Knowledge 
Acquisition 
 Now three feature selection algorithms, 
rough sets method, decision tree (C 4.5 
algorithm) and neural network, are 
implemented. The inputs and outputs of the 
decision tree and rough sets are 176 
sub-attributes and defined classes 
respectively. In the neural network based 
method, the back-propagation neural network 
with one hidden layer is adopted and 
implemented using Professional II PLUS 
software. All parameters of the BPNN are 
obtained by trial and error, including the 
number of training iterations and the 
structure of the network.  
 
 
 
   Implementation results are shown in 
Tables 2~4. In Tables 2 and 3, our proposed 
approach obviously outperforms the 
traditional approach without granulation, in 
both classification accuracy and type II error. 
In addition, fewer knowledge rules and 
attributes are obtained. In Table 4, the 
classification accuracy and type II error of 
our approach are still better than those by the 
original BPNN. All the attributes, kept and 
ranked by priority, are listed in Table 4. By 
comparing the implementation results of 
these three methods, six attributes {B7211, 
H114-102, B725, B8750, C8750 and E8750} 
are reserved as final test items for the RF 
functional test. The knowledge rules listed in 
Figures 2(a) & (b) are generated by using 
rough sets and decision tree methods. These 
rules may not only help engineers to predict 
the yield rate of products, but may also 
enhance the performance of knowledge 
management. 
 
 
 
 6
Mining (ICDM.01) 2001; 11-18 (San Jose, 
CA, USA). 
[5] Batista, G., Prati, R. C. and Monard, M. 
C., A study of the behavior of several 
methods for balancing machine learning 
training data. SIGKDD Explorations 
2004; 6(1): 20-29.  
[6] Bruzzone, L. and Serpico, S. B., 
Classification of imbalanced 
remote-sensing data by neural networks. 
Pattern Recognition Letters 1997; 18: 
1323–1328. 
[7] Burke, L. and Kamal, S., Neural networks 
and the part family/machine group 
formation problem in cellular 
manufacturing: a framework using Fuzzy 
ART. Journal of Manufacturing Systems 
1995; 14: 148-159. 
[8] Carpenter, G. A., Grossberg, S. and 
Rosen, D. B., Fuzzy ART: fast stable 
learning and categorization of analog 
patterns by an adaptive resonance system. 
Neural Networks 1991; 4: 759-771. 
[9] Castellano, G. and Fanelli, A. M., 
Information granulation via neural 
network-based learning. IFSA World 
Congress and 20th NAFIPS 
International Conference 2001; 5: 
3059-3064. 
[10] Chawla, N. V., Japkowicz, N. and Kolcz, 
A., Editorial: special issue on learning 
from imbalanced data sets. SIGKDD 
Explorations 2004; 6(1): 1-6.  
[11] Han, J. and Kamber, M., Data mining: 
concepts and techniques. Morgan 
Kaufmann publishers, San Francisco; 
2001. 
[12] Hannikainen, M., Hamalainen, T. D., 
Niemi, M. and Sarinen, J., Trends in 
personal wireless data communications. 
Computer Communications 2002; 25: 
84-99.  
[13] Li, R. and Wang, Z., Mining 
classification rules using rough sets and 
neural networks. European Journal of 
Operational Research 2004; 157: 
439-448. 
[14] Oyeleye, O. and Lehtihet, E. A., A 
classification algorithm and optimal 
feature selection methodology for 
automated solder joint defect inspection. 
Journal of Manufacturing Systems 1998; 
17: 251-262. 
[15] Pawlak, Z., Rough sets and fuzzy sets. 
Fuzzy Sets and Systems 1985; 17: 99-102. 
[16] Pedrycz, W. and Vukovich, G., Feature 
analysis through information granulation 
and fuzzy sets. Pattern Recognition 2002; 
35: 825-834. 
[17] Pendharkar, P. C., Rodger, J. A., 
Yaverbaum, G. J., Herman, N., Benner, 
M., Association, statistical, mathematical 
and neural approaches for mining breast 
cancer patterns. Expert Systems with 
Applications 1999; 17: 223-232. 
[18] Provost, F. and Fawcett, T., Robust 
classification for imprecise environments. 
Machine Learning 2001; 42: 203-231. 
[19] Quinlan, J., Induction of decision trees. 
Machine Learning 1986; 1: 81–106. 
[20] Quinlan, J., C4.5: Programs for 
machine learning. Bari: Morgan 
Kaufmann; 1993. 
[21] Reed, R., Pruning algorithms- a survey. 
IEEE Trans. Neural Networks 1993; 5: 
740-747. 
[22] Swiniarski, R. W. and Hargis, L., Rough 
sets as a front end of neural-networks 
texture classifiers. Neurocomputing 2001; 
36: 85-102. 
[23] Swiniarski, R. W. and Skowron, A., 
Rough set methods in feature selection 
and recognition. Pattern Recognition 
Letters 2003; 24: 833-849. 
[24] Su, C.-T., Hsu, J.-H. and Tsai, C.-H., 
Knowledge mining from trained neural 
network. Journal of Computer 
Information Systems 2002; 61-70. 
[25] Su, C.-T. and Shiue, Y.-R., Intelligent 
scheduling controller for shop floor 
control systems: a hybrid genetic 
algorithm/decision tree learning approach. 
International Journal of Production 
Research 2003; 12: 2619-2641. 
[26] Taguchi, G. and Jugulum, R., The 
Mahalanobis-Taguchi strategy: a pattern 
technology system. John Wiley & Sons, 
New York; 2002. 
[27] Verikas, A. and Bacauskiene, M., 
Feature selection with neural networks. 
Pattern Recognition Letters 2002; 23: 
1323-1335. 
[28] VI Services Network, Mobile phone 
online test application, available at: 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
報告人姓名：蘇朝墩 
服務機構及職稱：清華大學工業工程與工程管理系 教授 
會議時間/地點：96/10/20~96/10/23，埃及(Egypt)亞歷山卓(Alexandria) 
會議名稱：(中文)電腦與工業工程第 37 屆國際學術研討會議 
                (英文)The 37th International Conference on Computers and Industrial Engineering 
發表論文題目：A Reformed MTS for Multi-class Data Analysis 
本會核定補助文號：96-2221-E-007-049- 
 
一、參加會議經過 
此次 C&IE 研討會安排於埃及亞歷山卓城市舉辦，而台灣共有 25 篇論文發表。本
人於 10/19 日早上出發，搭乘 KE692 下午 13:25 pm 起飛之韓國航空班機先經韓國首
爾，再經由杜拜等地轉機而於次日早上抵達「埃及開羅」機場，然後再搭乘巴士抵達
埃及第二大城：亞歷山卓。 
本次研討會中，大會安排的 Keynote Speaker 為 E. A. Elsayed，其演講主題為
Quality and Reliability Engineering in the Global Economy。本次研討會所發表論文涵蓋
各種與資訊技術、工業工程與工業管理等相關的主題，其依不同論文主題於三天中分
10 個時段、5 個平行 Sessions 進行發表。本人的論文於 10/21 下午 2:00~3:30 時段發
表。除了個人的論文發表之外，亦選擇與研究興趣關聯性較高之場次進行聆聽。 
整體而言，透過此次研討會，與國內外學者有良好交流，因此瞭解了一些國際學
者在資訊與工業工程相關領域之研究方向，頗有收穫。 
二、與會心得 
藉由此次國際會議之發表以及與其他學者之交流互動，添加許多個人在未來研究
方向之想法。建議國科會多多補助出席國際學術會議。 
 
三、攜回資料名稱及內容 
1. 研討會論文議程一冊、研討會論文 CD一份 
2. 多項研討會與國際期刊 Call for Paper之資訊 
 
 
  2
)( ilqt  Gram-Schmidt coefficients of iMS . 
)( ilξ  SDs of )( )(pl iU  for ip = . 
)(i
rlu  Gram-Schmidt vector of the 
thl  feature in 
any example r  processed by iMS . 
ipη  SN ratio obtained by the MDs from the 
examples in iMS  to pC . 
)(
)(
p
j iMD  MD from the 
thj  example in iMS  to class 
pC . 
+
lSN  Average SN ratio of all runs including the thl  feature. 
−
lSN  Average SN ratio of all runs excluding the thl  feature. 
lGain  Effect gain of the thl  feature. 
δ  Number of features in the reduced model.
)(i
rWMD  WMD from any example r  to iC . 
lw  Weight of the thl  feature. ( )BAH ,  Hamming distance between vector A and 
B. ( )BAS ,  Similarity of vector A and B. 
1. INTRODUCTION 
MTS, which was developed by Dr. Taguchi, is a 
collection of methods that was proposed as a 
forecasting and classification technique using 
multivariate data (Taguchi, 2001; Taguchi and 
Jugulum, 2002). MTS integrates MD and Taguchi’s 
robust engineering. MD is used to construct a 
multidimensional measurement scale and define a 
reference point of the scale with a set of observations 
from a reference group. On the other hand, Taguchi’s 
robust engineering is applied to determine the 
important features and then optimize the system. So 
far, MTS has been successfully used in various 
applications (Srinivasaraghavan and Allada, 2006; 
Riho et al., 2005; Das and Datta, 2006; Taguchi et al., 
2006). Nevertheless, all of these applications are 
restricted to two-class problems due to the limitation 
of MTS algorithm that only one MS can be 
constructed for one problem. 
The multi-class classification problems, a sub-set of 
classification problems, assume the existence of a 
known set of k  ( 2>k ) classes, 
},,,{ 21 kCCCC K= . The goal of multi-class 
classification problems is to find a mapping or 
function, ( )XfCi = , that can predict the associated 
class label iC  of a given example vector X . In this 
view, it is expected that the mapping or function can 
accurately separate the data classes. Such a problem 
is often seen in many real applications, such as 
handwritten digit recognition (Cun et al., 1989), 
image recognition (Chapelle, 1999), speech 
recognition (Shami and Verhelst, 2007), disease 
diagnosis (Selvi et al., 2000), text categorization 
(Lam et al., 1999), and so on.  
On the other hand, a higher number of dimensions 
always put bounds on the applicability of 
multivariate techniques in obtaining meaningful 
information. Feature selection which aims to reduce 
the number of features (dimensions), is a well known 
problem and is usually regarded as a technique to 
improve a data mining algorithm. The reduction of 
feature space may lead to benefits in terms of 
accuracy and simplicity. Furthermore, it will bring 
financial saving from identifying the features which 
have no need to be collected and stored (Justin and 
Victor, 1997). For this reason, feature selection is 
usually merged into the process of establishing a 
classification function. Currently, a number of 
statistics, machine learning, and artificial intelligence 
techniques, such as RST, SLDA, DT, and so on have 
been used to solve multi-class classification problems 
and can simultaneously identify the important 
features. However, many popular classification 
algorithms, such as SVM, MLP, and MTS, were 
originally developed just for two-class problems, and 
the follow-up studies have also mainly focused on 
learning a binary function. Though there is a well 
established understanding on these algorithms for 
two-class problems, there have been a relatively 
smaller amount of investigations on supporting for 
the multi-class ones. To enhance the practicality in 
real world, effectively extending the two-class 
algorithms to support multi-class problem is non-
trivial. However, it is inevitable that the extension 
often leads to unexpected complexities. Moreover, 
carrying out a feature selection on multi-class 
problems is much more difficult in comparison with 
on two-class ones.  
The best and well known strategy to generalize from 
a two-class classification algorithm to a multi-class 
one, the most well-known strategy is to decompose 
the multi-class problems into a collection of two-
class problems. A number of decomposition 
strategies has been proposed, such as one-against-one 
(KreBel, 1999), one-against-all (Vapnik, 1995), P-
against-Q (Ou and Murphey, 2007), and ECOC 
  4
In the third stage, the OAs and SN ratio are applied 
for feature selection and the optimization of the 
multi-class system. Inside an OA, every row (run) 
presents a different combination of features used for 
MS construction. The SN ratio, η , corresponding to 
each run is computed using the concept of the larger-
the-better type and is defined using the following 
equation: 
∑∑ ∑
== =
−
=
≠
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛⋅⋅−=
K
pi
ip
k
pi
n
j
ip
j
ip
j
i
i
i
i
MD
MD
n 1,1, 1
1
)(
)(
10
)(
)(1log10 ηη     (5) 
After the SN ratio of every run is obtained, the effect 
gain of each feature can be calculated using the 
following equation: 
−+ −= lll SNSNGain                                             (6) 
If the effect gain corresponding to a feature is 
positive, the variable my be important and may be 
considered as worth keeping. However, a variable 
with negative effect gain should be removed. 
In the fourth stage, a reduced model measurement 
scale is constructed using the important features, and 
a proposed “WMD” is employed to verify the 
measurement scale and to be the distance metric for 
classification. The WMD from any example r  to iC  
is computed through the following equation: 
∑
∈
⋅⋅=
Rl
l
i
rl
l
i
r
i
uwWMD 2
2)(
)(
)(
1
ξδ                                   (7) 
where 
∑
∈
⋅=
Rl
l
l
l
Gain
Gainw δ                                                  (8) 
By classifying examples into the class with minimum 
WMD, the classification accuracy can be acquired. 
Finally, a test should be implemented using the 
unknown examples to confirm the classification 
ability of the reduced model. 
3. NUMERICAL EVALUATION  
To evaluate the effectiveness of our proposed MMTS, 
we compared the performance of MMTS with other 
well-known classification algorithms which can 
support the multi-class problems on several datasets 
from the Statlog collection and the UCI machine 
learning databases. The algorithms were MDC, SVM, 
BPN, LVQ, C4.5, RST, and SLDA. In addition to the 
multi-class classification, feature selection is one of 
the main functions of our proposed MMTS. In 
addition, many classification algorithms can 
simultaneously satisfy the demand of feature 
selection. However, it should be noticed that stability 
has been extensively studied with respect to the 
outcome of classification, while stability on the 
feature selection is a relatively neglected issue. 
Stability is defined as the sensitivity of a method to 
variations in the training set (Kalousis et al., 2005). It 
is possible that different training examples lead to 
radically different reduced feature sets when a 
feature selection algorithm with low stability is 
utilized. Consequently, there tends to be less 
confidence in the identified feature subsets. Thus, not 
only the class prediction ability but also the feature 
selection efficiency should be simultaneously 
considered when algorithms are evaluated. In this 
study, the main criterion used to judge the k -class 
classification outcome was the BCA which is defined 
as: 
∑
=
=
k
i iclassofexamples
rightpredictediclassofexamples
k
BCA
1 #
#1         (9) 
Additionally, the feature selection efficiency measure, 
FSE, was proposed in this study and is defined as the 
geometric mean of the feature stability and the 
percentage of removed features. A higher FSE is 
preferred in the feature selection. 
To fairly compare the classification results, avoid 
exaggerating the results obtained by chance, and 
estimate the stability of a feature selection for a given 
dataset, we adopted N-fold stratified cross-validation 
( 5=N ). The algorithm outputs a preference feature 
subset and it follows a classification result for each 
of the training folds. The feature stability is then 
obtained simply by using the average similarity of 
the 
( )
2
1−NN
 pairs from the N  feature subsets. Our 
proposed similarity between two feature sets is 
measured adopting the Hamming distance. Let [ ]daaaA ,,, 21 K=  and [ ]dbbbB ,,, 21 K=  be two 
vectors in a d  dimensional space, where { }1,0, ∈ll ba  and 1 denotes the feature that is 
selected while 0 denotes one that is not. The 
Hamming distance, H , between vector A  and 
vector B  is defined as the number of mismatched 
components of A  and B . The similarity, S , 
  6
metric for classification to overcome the drawback of 
Mahalanobis distance which equally adds up the 
variance normalized squared distances of the features 
without considering the relative contribution for 
classification. In order to verify the ability of MMTS, 
several multi-class datasets were used to compare the 
performance of MMTS with that of other popular 
classification algorithms. The results indicated that 
MMTS outperformed the other algorithms not only 
on the classification accuracy but also on the feature 
selection efficiency.  These results led us to conclude 
that our proposed MMTS indeed overcomes the 
limitation of MTS and is a powerful and useful 
algorithm for multi-class classification and feature 
selection. 
REFERENCES 
Anand, R., Mehrotra, K., Mohan, C. K., and Ranka, S., 
(1995), “Efficient Classification for Multiclass 
Problems Using Modular Neural Networks,” IEEE 
Transactions on Neural Networks, vol. 6(1), pp. 117-
124. 
Chang, C. C. and Lin, C. J., (2001), LIBSVM: A Library 
for Support Vector Machines. Software is available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Chapelle, O., Haffner, P., and Vapnik, V. N., (1999), 
“Support Vector Machines for Histogram-based Image 
Classification,” IEEE Transactions on Neural 
Networks, Vol. 10(5), pp. 1055-1064. 
Cun, Y. L., Boser, B., Denker, J., Hendersen, D., Howard, 
R., Hubbard, W., and Jackel, L., (1989), 
“Backpropagation Applied to Handwritten Zip Code 
Recognition,” Neural Computation, vol. 1, pp. 541-
551. 
Das, P. and Datta, S., (2007), “Exploring the Effects of 
Chemical Composition in Hot Rolled Steel Product 
Using Mahalanobis Distance Scale under 
Mahalanobis-Taguchi System,” Computational 
Materials Science, vol. 38(4), Pages 671-677.  
Dietterich, T. G. and Bakiri, G., (1995), “Solving 
Multiclass Learning Problem via Error-correcting 
Output Codes,” Journal of Artificial Intelligence 
Research, vol. 2, pp. 263-286.  
Duda, R., Hart, P., and Stork, D., (2001), Pattern 
Classification, Wiley.  
Ho, T. K. and Basu, M., (2002), “Complexity Measures of 
Supervised Classification Problems,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24(3), pp. 289-300. 
 Hsu, C. W. and Lin, C. J., (2002), “A Comparison of 
Methods for Multiclass Support Vector Machines,” 
IEEE Transactions on Neural Networks, vol. 13(2), 
pp.415-425.  
Justin, C. W. D. and Victor, R. J., (1997), “Feature Subset 
Selection with A Simulated Annealing Data Mining 
Algorithm,” Journal of Intelligent Information Systems, 
vol. 9, pp. 57-81.  
Table 2 Feature selection efficiency 
Dataset MMTS MDC SVM BPN LVQ C4.5 RST SLDA 
Abalone 0.7965 - - - - 0.0000 0.0000 0.6315 
Vehicle 0.4746 - - - - 0.4269 0.5258 0.4049 
Tae 0.4699 - - - - 0.0000 0.1918 0.5879 
Shuttle 0.6740 - - - - 0.5829 0.5648 0.4055 
Average 0.60375 - - - - 0.25245 0.3206 0.50745
 
Table 3 Balanced classification accuracy 
Dataset MMTS MDC SVM BPN LVQ C4.5 RST SLDA 
Abalone 0.2683 0.2310 0.2259 0.1923 0.2263 0.2195 0.2212 0.2550 
Vehicle 0.8352 0.8454 0.8321 0.8038 0.6850 0.7307 0.6930 0.7829 
Tae 0.8352 0.8454 0.8321 0.8038 0.5817 0.7307 0.6930 0.7829 
Shuttle 0.9659 0.9582 0.9792 0.7469 0.7497 0.9988 0.9968 0.7063 
Average 0.7262 0.7200 0.7173 0.6367 0.5607 0.6700 0.6510 0.6318 
