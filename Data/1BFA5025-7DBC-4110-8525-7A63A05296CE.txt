 2
 
利用支援向量迴歸去偵測離異點及其應用之研究(I) 
 
A study on the support vector regression as outliers detection and its 
application (I) 
 計畫編號：NSC 96－2221－E －197 －021－ 
 執行期限：96 年 8 月 1 日 至 97 年 7 月 31 日 
   主持人：莊鎮嘉          職稱：副教授 
 電子信箱：ccchuang@niu.edu.tw 
 執行機構：宜蘭大學電機系 
一、中文摘要 
本計畫分為兩年期計劃。首先，第一年計劃
我們將對支援向量回歸及其參數選擇加以分析。
因為在使用支援向量回歸時，在訓練資料中含有
離異點時，有如果支援向量回歸內部參數(核心函
數參數、遺失函數參數)選擇不當會導致其結果出
現過度貼近及不足貼近狀況。因此，我們將建立
一個新的離異點偵測準則，經由此準則可判定在
訓練資料中之離異點。對於遺失函數參數，我們
將利用統計機率分布－變異數觀念以便得到此參
數。這個想法是利用 1L -基準來估測出輸入資料的
雜訊分佈，當藉由選擇適當的資料集中度(變異數)
的觀念，可以得到遺失函數之參數。對於核心函
數參數，我們將利用訓練資料分佈特性－分群概
念以便得到此參數。我們希望透過上述方法能對
支援向量回歸及其參數有更深入分析及結果。其
次是藉由上述結果，我們將應用離異點偵測上並
與利用統計方式所找到的離異點作一比較與分
析。根據第一年成果，在第二年計劃，我們利用
離異點偵測準則去將離異點從訓練資料中移除。
此做法類似濾波器的概念。並利用移除離異點後
的訓練資料拿來訓練非強健式支援向量回歸，如
SVRNs 與 LS-SVMR，並與強健式支援向量回歸，
如  Robust SVRNs (RSVRNs) 與 Weighted 
LS-SVMR，做一比較與分析。值得注意，一般強
健式支援向量回歸都是利用強健式統計理論去降
低離異點的影響。我們的做法是先利用濾波器的
概念去除離異點，在將此資料拿來訓練非強健式
支援向量回歸。最後我們會針對此兩種方式－訓
練資料中含有離異點作一分析比較並應用於動態
系統的塑模 (Modeling)。 
 
關鍵詞：統計學習理論、支援向量回歸、離異資
料、強健式支援向量回歸。 
 
英文摘要 
This project is two-years projects. In the first 
year of project, the practical selection of support 
vector regression (SVR) parameters are discussed 
and analyzed under the training data includes 
Outliers. Due to the results of SVR appear 
over-fitting and under-fitting situations when the 
SVR parameters (kernel parameter, epsilon and 
regular constant) are improperly chosen and the 
training data includes Outliers. Hence, the SVR 
parameters have to be chosen carefully. In this 
project, we investigate practical selection of SVR 
parameters. The proposed methodology advocates 
analytic parameters selection directly for training 
data rather than re-sampling approaches commonly 
used in support vector machines (SVM) 
applications. By introducing the repeated SVR 
approach and the property of standard deviation 
(std), a suitable selection of epsilon can be 
obtained. The concept is to use the SVR with 
1L -norm to estimate the noise distribution of 
training data. Then, the epsilon value is obtained 
by setting standard deviation (std) of distribution. 
 3
overcome the outlier’s effects [13]. For those 
approaches, the training data that includes outliers 
are aggregately considered in the training process. 
In those approaches, the concepts of robust 
statistic theory are directly use to reduce the 
outlier’s effects.  
In order to obtains the better initialization 
of SVR for outliers detection, some parameters in 
the SVR must be chosen carefully. The parameters 
include the kernel parameter, epsilon in Vapnik’s 
ε -insensitive loss function and regularization 
constant. However, it is still not a structured 
(efficiency) way to select the SVR parameters. In 
the literature, the regularization constant is 
suggested as larger as possible. The epsilon is 
suggested as NNln××= ψτε , where N is 
the number of training data, τ  is a constant and 
ψ  is the input noise level  [14]. Due to the input 
noise level ψ  unknown, the concepts of repeated 
SVR approach is used to estimate ψ . Moreover, 
the number of SVs and the performance of 
approximated function (regression) are controlled 
by the epsilon value. Third parameter is the kernel 
parameter. In the literature, the Gaussian function 
with kernel parameter is used as kernel function 
that often used. Due to the Gaussian kernel 
parameter dependents on the distribution/range of 
training data, the kernel parameter must be 
varying based the distribution property of training 
data. How to chosen those parameters under the 
training data include outliers are important. When 
those parameters properly chosen, the outliers are 
easily detection based on the results of SVR. 
Moreover, the training data except for outliers 
directly applied to training the non-robust 
approaches (i.e. SVR and LS-SVM). This 
concepts similar to filters. It is different from the 
robust support vector regression network 
(RSVRNs) and weighted LS-LSVM approaches. 
In those approaches, the concepts of robust 
statistic theory are directly use to reduce the 
outlier’s effects. 
 
Robust Support Vector Regression Networks 
(RSVRNs) 
 
In [13], the robust learning concept and the 
traditional SVR are combined to form the 
RSVRNs. The learning of the RSVRNs is divided 
into two phases, the initial phase and the robust 
learning phase. The initial phase is to determine 
the network structure and the corresponding initial 
network weights through the traditional SVR as 
stated in the next section. In the robust learning 
phase, the robust leaning algorithm is used to 
improve the learning performance without 
affected by outliers. After applying the traditional 
SVR, the initial structure of RSVRNs can be 
represented as  
( ) ∑ +=
=
P
i
ii bxxKxy
1
),(ˆ rrr β .         (1) 
where ( )⋅K  is a kernel function, ixr  is i-th 
support vectors (SVs), P  is the number of kernel 
functions, which is equivalent to the number of 
SVs, TP ],...,[ 1 βββ =
r
 is the weights vector of 
the network, and yˆ  is the output of the RSVRNs. 
The parameters P, ix
r  and βr  are determined by 
the traditional SVR. To reduce the outlier’s effect, 
the robust leaning algorithm is used to improve 
the learning performance. An important feature of 
those robust learning algorithms is to use a robust 
cost function in place of the quadratic form of the 
cost function in a standard back-propagation 
algorithm. Based on the same idea, a robust cost 
function )(tER  for the learning algorithm of 
RSVRNs is defined here: 
[ ]∑
=
=
N
j
jR teN
tE
1
)(1)( σ ,           (2) 
where N is the number of training data, t  is the 
epoch number, )(ˆ)( tyyte jjj −=  is the error 
between the j-th desired output and the j-th output 
of the RSVRNs at epoch t , and ( )⋅σ  is a robust 
cost function, which directly stems from the 
theory of robust statistics [11]. For the robust 
learning algorithm of RSVRNs, the gradient 
descent method is employed and βr  can be 
updated as: 
)()()( ttt βββ rrr Δ+=+1 ,           (3) 
with      
[ ]∑ ∂∂=∂∂=Δ =
N
j
j
j
R
 
t e
te
N
tEt
1
)(
)(
 
)( )( βϕ
η
βηβ rr
r
, (4) 
 5
kernel ),( ⋅⋅K  is represented as  
.,...,1,),()(),( NlkxxxxK l
T
klk == ϕϕ   (13) 
As a result, the output of LS-SVM for 
regression can be represented as 
∑
=
+=
N
k
kk bxxKxy
1
),()( α ,           (14) 
where kα  and b  are the solution to (12).  
In order to obtain a robust estimate based 
upon the LS-SVM solution, one can weight the 
error variables γα /kke =  by weighting factors 
kv . This leads to the optimization problem: 
∑
=
∗∗∗∗∗ +=∗∗∗
N
k
kk
T
ebw
evwwewJ
1
2
,, 2
1
2
1),(min γ , (15) 
such that 
.,...,1,)( Nkebxwy kk
T
k =++= ∗∗∗ ϕ   
As a result, the Lagrangian function is defined as  
{ }∑
=
∗∗∗∗
∗∗∗∗∗∗
−++
−=
N
k
kkk
T
k yebxw
ewJebwL
1
.)(
),();,,(
ϕα
α
.     (16) 
The unknown variables for this weighted 
LS-SVM problem are denoted by the ∗  symbol. 
From the conditions for optimality and 
elimination of ∗∗ ew ,  one obtains the  
KKT system 
,0
1
10
⎥⎦
⎤⎢⎣
⎡=⎥⎦
⎤⎢⎣
⎡
⎥⎥⎦
⎤
⎢⎢⎣
⎡
+Ω ∗
∗
y
b
Vv
T
v
αγ          (17) 
where the diagonal matrix γV  is given by 
.1,...,1
1 ⎭⎬
⎫
⎩⎨
⎧=
Nvv
diagV γγγ             (18) 
The choice of the weights kv  is determined 
based upon the error variables γα /kke =  from 
the (unweighted) LS-SVM case (12). Robust 
estimates are obtained then (see[5,18]) e.g. by 
taking 
⎪⎪⎩
⎪⎪⎨
⎧
≤≤−
−
≤
=
− otherwise  ,10
ˆ/ if  ,
ˆ/
ˆ/ if          ,1
4
21
12
2
1
csec
cc
sec
cse
v k
k
k
k  (20) 
where sˆ  is a robust estimate of the standard 
deviation of the LS-SVM error variables 
ke : 
.
6745.02
IQRˆ ×=s                    (21) 
The constants 1c  and 2c are typically chosen as 
2.5 and 3, respectivly [17]. This is a reasonable 
choice taking into account the fact that for a 
Gaussian distribution, there will be very few 
residuals larger than 2.5 sˆ . The interquartile range 
IQR is the different between the 75th percentile 
and 25th percentile. In the estimate sˆ  one takes 
into account how much the estimated error 
distribution deviates from a Gaussian distribution. 
Another robust estimate of the standard deviation 
is sˆ  =1.483MAD( ix ) where MAD stands for the 
median absolute deviation [17]. 
本計畫的主要目的是 
第一年：Support vector regression as outliers 
detection 
在第一年中，我們將對目前 SVR 的參數
選擇與特性做一個分析，特別是應用在離異點
的偵測上。一般而言，在實驗過程中，我們所
得到訓練資料會包含離異點。當有離異點出
現 ， 訓 練 後 經 常 會 出 現  overfitting 與 
non-smoothing現象。因此，我們會針對過去SVR 
的參數選擇方法逐一探討與分析。針對於過去
方法在訓練資料含有離異點情況下，提出一個
有效方法來決定 SVR 的參數。當 SVR 的參數
決 定 後 ， 根 據  SVR 的 特 性 及  Slack 
variables/Support vectors (SVs)來區分離異點。
因為在 SVR 的特性中，SVs 有可能是離異點。
因此，我們會利用統計方式對所有 SVs 去做分
析探討，把離異點給找出來。最後，再將我們
所找到離異點與統計方式找出離異點作一比
較。 
在軟體發展過程，藉由 Matlab 軟體的分
析去驗證所發展出來之新的  Support Vector 
Regression 之 參 數 選 擇 及  Support Vector 
Regression as Outliers Detection 之正確性與適
用性。同時，利用統計方式找出離異點與我們
所提出的方法比較並分析其差異。 
 
 
 
 7
points A’ and B’ may be regarded as either the 
outliers or the SVs. To determine which the SVs 
can be regarded as the outlier, the criterion is 
build as follows rule.  
⎩⎨
⎧ ×≥=
otherwise ,     SVs         
) ablesslack vari(3or     Outliers, * std
CR kk
ξξ ,(30) 
where std(slack variables) represented as the 
standard deviation of slack variables *  and kk ξξ  
for k=1,2,…,N. To estimate the slack variables, 
the hyper-parameters (i.e. kernel parameter GKp, 
epsilon and regularization constant C) in the SVR 
approach must chosen. For the epsilon ε , it is 
well-known that the value of ε  should be 
proportional to the input noise level ψ , that is 
ψε ∝ . In this study, the parameter ε  is chosen 
as  
N
Nln××= ψτε .             (31) 
Based on empirical results, the constant value 
3=τ  gives good performance. Due to the input 
noise level ψ  unknown, the concepts of repeated 
SVR approach is used. According to these 
concepts, the input noise level ψ  is estimated as 
Step 1: The traditional SVR with 0=ε  is 
applied to obtain the estimated output 
results. Consequently, the actual and 
expected output corresponding to the 
same input data were used to calculate the 
errors.  
Step2: According to the above errors, the noise 
level ψ  easily obtained by ( )errorsstd  
where std represented as the standard 
deviation. 
 
Based on those procedures, the results of 
proposed approach are shown in figure 2 ~ figure 
5. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: The relational among of the output of 
SVR output )( xf r , ε  and ξ  are 
shown. 
 
Simulation Results 
 
In this study, some of examples are used to 
verify the effectiveness of the proposed approach. 
The index used for evaluating the performance is 
the root mean square error (RMSE) defined as 
( )
N
yy
RMSE
N
j
jj∑
=
−
= 1
2ˆ
, (32) 
where N denotes the number of the test data, jy  
represents the actual output and jyˆ  is the output 
of the SVMR approach for the j-th training data. 
Additionally, the following gross error model [14, 
15] is considered as the outlier model: { }10 ,)1( | ≤≤+−== δδδδ HGDDD ,(33) 
where δ  is the probability of occurrence of an 
outlier, G is a usual distribution (noise) and H is a 
symmetric long tailed distribution (outliers). 
In the first example, the training data sets are 
generated by [5,16]: 
x
xy sin= , ]10,10[−∈x . (33) 
The number of training data that generated by Eq. 
(33) and the gross error model with )1.0,0(NG = , 
)10,0(NH =  and 0.05δ =  are obtained as 1001. 
The number of the used test data is 2001. The 
regularization constant C is chosen as 1024 in the 
y
x
*ξ
ξ
*ξ
ξ
ε
ε
 
A
A
B
B
 9
 
Table 3: The results for the weighted LS-SVMR 
approach, the RSVRNs approach, the 
proposed approach with non-robust 
SVRNs and the proposed approach with 
non-robust LS-SVMR are summarized 
for example 2.  
 
Approaches RMSE The number of 
training data 
Proposed approach 
with non-robust 
SVRNs 
(τ =3, 1237.0=ε ) 
0.0849 814 
Proposed approach 
with non-robust 
LS-SVMR  
(τ =3, 1237.0=ε ) 
0.1117 814 
RSVRNs approach 0.1315 1024 
Weighted 
LS-SVMR 
approach 
0.1337 1024 
 
 
四、結論 
 In the first year of this project, the proposed 
approach is developed to deal with training data set 
with outliers. That is, there are two-stage strategies 
that propose in the proposed approach. In the stage 
I, called as data preprocessing, the support vector 
machines for regression approach is used to filter 
out the outliers in the training data set. Because the 
outliers in the training data set are removed, the 
concepts of robust statistic theory have no need to 
reduce the outlier’s effect. Then, the training data 
set except for outliers is used to training the 
non-robust least squares support vector machines 
for regression or the non-robust support vector 
regression networks in the stage II. Consequently, 
the learning mechanism of the proposed approach 
is much easier than the robust support vector 
regression networks approach and the weighted 
LS-SVMR approach. Besides, based on the 
simulation results, the performances of the 
proposed approach with non-robust LS-SVMR and 
non-robust SVRNs are superior to the weighted 
LS-SVMR approach and the RSVRNs approach 
when the outliers are existed, respectively. 
 
五、參考文獻 
[1] V. N. Vapnik, The nature of statistical 
learning theory, Springer, 1995. 
[2] J. A. K. Suykens, J. Vandewalle and B. D. 
Moor, “Optimal control by least squares 
support vector machines,” Neural Networks, 
vol. 14, no. 1, pp. 23-25, 2001. 
[3] S. Mukherjee, E. Osuna, and F. Girosi, 
“Nonlinear prediction of chaotic time series 
using a support vector machine,” NNSP'97, pp. 
24-26, 1997. 
[4] J.-T. Jeng, C. C. Chuang, and S. F. Su, 
“Support vector interval regression networks 
for interval regression analysis,” Fuzzy Set and 
Systems, vol. 138, vo. 2, pp. 283-300, 2003. 
[5] C. C. Chuang, J. T. Jeng and S. F. 
Su, ”Annealing Robust Radial Basis Function 
Networks for Function Approximation with 
Outliers," Neurocomputing vol. 56, pp. 
123-139, 2004. 
[6] F. E. H. Tay and L. Cao, ”Application of 
support vector machines in financial time 
series forecasting,” The international journal 
of Management Science, Omega 29, p.p. 
309-317, 2001. 
[7] J. A. K. Suykens, T. Van Gestel, J. De 
Brabanter, B. De Moor, J. Vandewalle, Least 
Squares Support Vector Machines, World 
Scientific Pub. Co., Singapore, 2002. 
[8] Dehaene J., Verrelst H., Vandewalle J., 
Timmerman D., ``Ovarian tumor classification 
with assymetric support vector machines'', in 
Proceedings of the International Workshop on 
Advanced Black-Box Techniques for Nonlinear 
Modeling, Leuven, Belgium, Jul. 1998, pp. 
13-16. 
[9] Baesens B., Viaene S., Van Gestel T., Suykens 
J., Van den Poel D., Vanthienen J., De Moor B., 
Dedene G., “`Knowledge discovery using least 
squares support vector machine classifiers : a 
direct marketing case”, in Proc. PKDD2000, 
Fourth European Conference on Principles of 
Knowledge Discovery in Databases, Lyon, 
France, Lyon, France, Sep. 2000, pp. 657-664. 
[10] De Brabanter J., Pelckmans K., Suykens 
 11
 
 2008 International Conference on Fuzzy 
Systems (FUZZ2008)  
2008 IEEE???????? 
 
 
???: ??? 
 
????????? 
???????? 1? 
 
Email: ccchuang@niu.edu.tw 
 
??? IEEE????????????????????????
????????????????????????????????
????????????????????????????????
????????????????????????????????
????????????????????????????????
???????????????????? 
 
??????????????? Jim Bezdek ?? “Visual Cluster 
Validity”? David B. Fogel ?? “The Burden of Proof - Part II” ? Teuvo 
Kohonen ?? “Data Management by Self-Organizing Maps ” ?Takeshi 
Yamakawa  ?? “Bio-inspired Self-Organizing Relationship Network as 
Knowledge Acquisition Tool and Fuzzy Inference Engine”???????
??????????? PANEL???? 
 
?????????????????????????????
??????????? 
 
 
 
 
  
Abstract— In this study, the robust least square support 
vector machines for regression (RLS-SVMR) is proposed to deal 
with training data set with outliers. There are two-stage 
strategies in the proposed approach. In the stage I, called as data 
preprocessing, the support vector regression (SVR) approach is 
used to filter out the outliers in the training data set. Due to the 
outliers in the training data set are removed, the concepts of 
robust statistic theory have no need to reduce the outlier’s 
effect. Then, the training data set except for outliers, called as 
the reduced training data set, is directly used to training the 
non-robust least squares support vector machines for regression 
(LS-SVMR) in the stage II. Consequently, the learning 
mechanism of the proposed approach is much easier than the 
weighted LS-SVMR approach. Based on the simulation results, 
the performance of the proposed approach is superior to the 
weighted LS-SVMR approach when the outliers are existed. 
I. INTRODUCTION 
he support vector machines (SVM) proposed by Vapnik 
and his research groups for solving classification 
problem [1]. The SVM implements the structural risk 
minimization principle that minimizes the upper bound of the 
generalization error. This induction principle is based on the 
fact that the generalization error is bounded by the sum of 
training error and a confidence interval term that depends on 
the Vapnik-Chervonenkis (VC) dimension. For the 
classification problem, the SVM tries to find the optimal 
hyper-plane, which is expressed as a linear combination of a 
subset of training data (called support vectors) by solving a 
linearly constrained quadratic programming (QP) problem 
with a maximum margin between the two classes. With the 
introduction of Vapnik’s ε -insensitive loss function, the 
SVM has been extended to solve a nonlinear regression 
estimation problem, called the support vector regression 
(SVR). Moreover, the SVR approach with the ε -insensitive 
loss function can be use a small subset of the training data, 
called the support vectors (SVs), to approximate the unknown 
functions within a tolerance ε  (epsilon) band. The number 
of SVs is controlled by the values of epsilon. Another, the 
least square (LS) versions of SVM, called as LS-SVM, is also 
investigated for classification [2] and regression [3]. In these 
 
Manuscript received Nov. 30, 2007. This work was supported in part by 
the National Science Council Under Grant 96-2221-E-197-021-.  
 
Chen-Chia Chuang is with the Electrical Engineering Department, 
National Ilan University, Taiwan (phone: 886-952783266; e-mail: 
ccchuang@niu.edu.tw).  
Jin-Tsong Jeng, is with the Computer Science and Information 
Engineering Department, National Formosa University, Taiwan. (e-mail: 
tsong@nfu. edu.tw). 
Mei-Lang Chan is with the Electrical Engineering Department, National 
Ilan University, Taiwan. 
LS-SVM formulations on works with equality instead of 
inequality constraints and a sum squared error (SSE) cost 
function as it is frequently used in the training of traditional 
neural networks. This reformulation greatly simplifies the 
problem in such a way that the solution is characterized by a 
linear system, more precisely a KKT (Karush-Kuhn-Tucker) 
system [4], which takes a similar from as the linear system 
that one solves in every iteration step by the interior point 
method for the standard SVM [5]. This linear system can be 
efficiently solved by iterative methods such as conjugate 
gradient [6]. Then, a LS-SVM is computationally more 
effective than a SVM [7]. While a LS-SVM incorporates all 
training data in the network to produce the result, the 
traditional SVM selects some of SVs that are important in the 
regression. This sparseness of the traditional SVM can also be 
reached with the LS-SVM by applying a pruning method [7]. 
However, the SVR and the least squares support vector 
machines for regression (LS-SVMR) approaches have been 
successfully applied into various applications [8-13]. 
In real applications, the obtained data may be subject to 
outliers. The outliers occur for various reasons, such as 
erroneous measurements or noisy data from the heavy-tails of 
noise distribution functions [14, 15]. To overcome the 
outlier’s effects, the robust version of LS-SVMR, called as 
the weighted LS-SVMR, is proposed [16]. Additionally, the 
robust support vector regression networks (RSVRNs) based 
on the SVR approach and robust learning algorithm is also 
proposed to overcome the outlier’s effects [17]. Due to the 
training data that includes outliers are aggregately considered 
in the training process, the concepts of the robust statistic 
theory are used to reduce the outlier’s effects. When the 
parameters in those approaches are improperly selected, those 
approaches cannot provide a satisfactory performance. 
In this study, the robust least squares support vector 
machines for regression (RLS-SVMR) are proposed to deal 
with training data set with outliers. In the proposed approach, 
two stages are included. In the stage I, it is an important 
matter for removing the outliers. That is, in the stage I, called 
as data preprocessing, the SVR approach is used to filter out 
the outliers in the training data set. Due to the outliers can be 
regarded as the support vectors in the SVR; the criterion is 
proposed to discriminate outliers from SVs in this study. To 
correctly discriminate outliers from SVs, some of parameters 
in the SVR approach must be carefully chosen. In the 
literature, some of approaches for selecting those parameters 
are proposed [18, 19]. Because VCYM approach (i.e. 
practical selection of SVMR parameter by V. Cherkassky and 
Y. Ma [18]) is easily method for selecting those parameters, it 
is adopted in this study. When the outliers are detected, the 
Robust Least Squares-Support Vector Machines for Regression with 
Outliers 
Chen-Chia Chuang, Jin-Tsong Jeng and Mei-Lang Chan 
T 
 
 
N
Nln
××= ψτε . (10) 
Based on empirical results, the constant value 3=τ  gives 
good performance. Due to the input noise level ψ  unknown, 
the concepts of repeated SVR approach is used [17]. 
According to these concepts, the input noise level ψ  is 
estimated as 
Step 1: The SVMR with 0=ε  is applied to obtain the 
estimated output results. Consequently, the 
actual and expected output corresponding to the 
same input data were used to calculate the 
errors.  
Step2: According to the above errors, the noise level ψ  
easily obtained by ( )errorsstd , where std 
represented as the standard deviation. 
Based on the above procedure, the outliers in the training 
data are detected. The training data set except for outliers are 
obtained as the reduced training data set { } DNkDkDk yx 1, =G  with 
input data nDk Rx ∈
G  and output data Ry Dk ∈  in the stage I, 
where DN  is the number of reduced training data. Then, those 
training data points are used training the LS-SVMR 
(non-robust) approach in stage II. 
 
III. LS-SVMR AND WEIGHTED LS-SVMR APPROACHES 
For the learning mechanism of the LS-SVMR (non-robust) 
approach, the training data set { } DNkDkDk yx 1, =G  are considered. 
Then, the following optimization problem in primal weight 
space is considered as 
∑
=
+=
DN
i
i
T
ebw
ewwewJ
1
2
~,~,~
~~
2
1~~
2
1)~,~(min γ , (11) 
such that  
Di
D
i
TD
i Niebxwy ,...,1,~
~)(~~ =++= Gϕ , 
where )(~ ⋅ϕ  is a function which maps the input space into a 
so-called higher dimensional (possibly infinite dimensional) 
feature space, w~  is the weight vector in primal weight space, 
ie~  is the error variables and b
~ is the bias term. Note that the 
cost function J consists of the SSE and a regularization term, 
which is also a standard procedure for the BP learning 
algorithm and is related to ridge regression [24]. The relative 
importance of these terms is determined by the positive real 
constant γ~ . In the case of noisy data one avoids overfitting by 
taking a smaller γ~  value.  In primal weight space, one has the 
model .~)(~~)( bxwxy DTDD += GG ϕ  The weight vector w~  can be 
infinite dimensional, which makes a calculation of w~  from 
Eq. (11) impossible in general. Therefore, one computes the 
model in the dual space instead of the primal weight space. 
One defines the Lagrangian as 
{ }∑
=
−++−=
DN
i
D
ii
D
i
T
i yebxwewJebwL
1
~~~)(~~~)~,~()~;~,~,~( Gϕαα ,  (12) 
where α~  and iα~  are Lagrange multipliers (called support 
values). The conditions for optimality are given by 









==−++→=
∂
∂
=⋅=→=
∂
∂
=→=
∂
∂
=→=
∂
∂
∑
∑
=
=
.,...,1,0~~)(~~0~
,,...,1,~~~0~
,0~0~
),(~~~0~
1
1
D
D
ii
D
i
T
i
Dii
i
N
i
i
N
i
D
ii
NiyebxwL
Nie
e
L
b
L
xw
w
L
D
D
G
G
ϕ
α
γα
α
ϕα
 (13) 
After elimination of w~  and e~ , the solution can be 
represented as 



=






+Ω y
b
Iv
T
v 0
~
~
~
11
10
αγ
, (14) 
where [ ]
DN
yyy ,...,1= , [ ]1,...,11 =v , [ ]DNααα ~,...,~~ 1=  and 
)(~)(~ Dl
TD
iil xx
GG ϕϕ=Ω  for .,...,1, DNli =  According to 
Mercer’s condition, a kernel ),(~ ⋅⋅K  is represented as  
.,...,1,),(~)(~),(~ D
D
l
TD
i
D
l
D
i NlixxxxK ==
GGGG ϕϕ  (15) 
As a result, the output of the LS-SVM approach for regression 
can be represented as 
∑
=
+=
DN
i
DD
ii
DD bxxKxy
1
~),(~~)( GGG α , (16) 
where iα~  and b
~  are the solution to Eq. (14). When a 
Gaussian function is used as a kernel function, the Eq. (16) 
can be written as  
b
xx
xy
DN
i
D
i
D
i
DD ~
~2exp
~)(
1
2
2
+






−−
= ∑
=
σ
α
GGG , (17) 
where σ~  is termed as the GKp for the LS-SVM. 
For the weighted LS-SVMR (robust) approach, the 
robust loss function is applied to modify the errors and is 
defined as: 



≤≤
−
−
≤
=
− otherwise,  10
,~ˆ/~ if  ~~
ˆ/~
,~ˆ/ if          1
)(
4
21
12
2
1
csec
cc
sec
cse
ev
, (18) 
where sˆ  is a robust estimate that defined as 
.
6745.02
IQRˆ
×
=s  (19) 
The constants 1~c  and 2~c are typically chosen as 2.5 and 3, 
respectively [14]. This is a reasonable choice taking into 
account the fact that for a Gaussian distribution, there will be 
very few residuals larger than 2.5 sˆ . The interquartile range 
(IQR) is the different between the 75th percentile and 25th 
percentile. In the estimate sˆ  one takes into account how 
much the estimated error distribution deviates from a 
Gaussian distribution. Another robust estimate of the 
standard deviation is sˆ = 1.483× MAD( ix ) where MAD( ix ) 
stands for the median absolute deviation [15]. 
 
 
 
Control, Special Issue on fundamental issues in control, 
vol. 7, no. 2-3, pp. 311-327, 2001. 
[10] M. Espinoza, J. A. K. Suykens, R. Belmans, B. De Moor, 
Electric Load Forecasting- an Application of Nonlinear 
System Identification and Kernel Based Modeling, 
Internal Report 06-84, ESAT-SISTA, K.U.Leuven 
(Leuven, Belgium), 2006. 
[11] M. W. Chang, C. J. Lin, and R. C. Weng, “Analysis of 
switching dynamics with competing support vector 
machines,” IEEE Transactions on Neural Networks, vol. 
15, pp. 720-727, 2004.   
[12] J. T. Jeng, C. C. Chuang, and S. F. Su, “Support vector 
interval regression networks for interval regression 
analysis,” Fuzzy Set and Systems, vol. 138, vol. 2, pp. 
283-300, 2003. 
[13] C F. Lin and S. D Wang, “Fuzzy support vector 
machines with automatic membership setting,” pp. 233 in 
Lipo Wang (ed.), Support Vector Machines: Training and 
Applications, Springer 2005, Berlin. 
[14] P. J. Rousseeuw, and M. A. Leroy, Robust Regression 
and Outlier Detection, New York: Wiley, 1987. 
[15] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw and W. 
A. Stahel, Robust Statistics, The Approach Based on 
Influence Functions, Wiley, 1986. 
[16] J. A. K. Suykens, J. De Brabanter, L. Lukas, and J. 
Vandewalle, “Weighted least squares support vector 
machines: robustness and sparse approximation,” 
Neurocomputing, Special issue on fundamental and 
information processing aspects of neurocomputing, vol. 
48, no. 1-4, pp. 85-105, 2002. 
[17] C. C. Chuang, S. F. Su, J. T. Jeng, and C. C. Hsiao 
“Robust support vector regression networks for function 
approximation with outliers,” IEEE Trans. on Neural 
Networks, vol. 13, no. 6, 2002. 
[18] V. Cherkarsky and Y. Ma, “Practical selection of SVM 
parameters and noise estimation for SVM regression,” 
Neural Networks, vol. 17, no. 1, pp.113-126, 2004. 
[19] J. T. Jeng, “Hybrid approach of selecting 
hyperparameters of support vector machine for 
regression,” IEEE Trans. on Systems, Man and 
Cybernetics, Part B, vol. 36, no. 3, pp. 699-709, 2006. 
[20] H. H. Tsai and P. T. Yu, “On the optimal design of fuzzy 
neural networks with robust learning for function 
approximation,” IEEE Trans. on Systems, Man and 
Cybernetics, Part B, vol. 30, no. 1, pp. 217-223, 2000. 
[21] C. C. Chuang, S. F. Su and C. C. Hsiao, “The annealing 
robust backpropagation (ARBP) learning algorithm,” 
IEEE Trans. on Neural Networks, vol. 11, no. 5, pp. 
1067-1078, 2000. 
[22] D. S. Chen and R. C. Jain, “A robust back-propagation 
learning algorithm for function approximation,“ IEEE 
Trans. on Neural Networks, vol. 5, no. 3, pp. 467-479, 
1994. 
[23] V. David Sanchez A., “Robustization of learning method 
for RBF networks,” Neurocomputing vol. 9, pp. 85-94, 
1995. 
[24] G. H. Colub, and C. F. Van Loan, Matrix Computation, 
Baltimore MD: Johns Hopkins University Press, 1989. 
[25] http://lib.stat.cmu.edu/datasets/balloon 
 
 
 
 
 
 
 
 
 
Figure 1: The relational among of the output of SVR output 
)( xf G , ε , *ξ  and ξ  are shown. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: The original training data (‘.’) and the reduced 
training data (‘o’) are shown for the example 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y
x
*ξ
ξ
*ξ
ξ
ε
ε
SVR 
output 
A 
A’ 
B 
B’ 
