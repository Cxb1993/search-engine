???????????????????????????????????
?????????(3) 
Computational Intelligence Approaches on Dynamic Functional Clustering Algorithm, 
Clusters of Interval Regression Analysis and Distance-Based Outliers Detection of 
Microarray Data for Ovarian Cancer (3) 
 
?????95-2221-E-197-024- 
?????95 ?8 ?1 ??96 ?7 ?31 ? 
???????   ?????? 
?????ccchuang@niu.edu.tw 
????????????????? 
?????? 
?????????????????
????????????????????
????????????????????
????????????(???????
???????)????????????
????????????????????
????????????????????
???????????????????
????????????????????
???????????????????
????????????????????
???????????????????
????????????????????
????????????????????
???????????????????
???????????????????
????????????????????
????????????????????
????? 
????????????????????
????????????????
????????????????
????? 
Abstract 
In the previous two-years of project, we 
has been proposes the dynamic functional 
clustering (DFC) algorithm with new clusters 
merge approach and functional support vector 
interval regression networks (FSVIRNs). In the 
final year of this project, we will be discussing 
the outlier’s effects in microarray data. Outliers 
in Chip experiments can occur at several levels. 
However, outliers must be detected and removed. 
Basically, we need a statistical model of you data. 
The simplest model is equality among replicates. 
If one replicate (chip, gene, or probe) deviates 
several standard deviation from the mean, then 
such data can be regarded as outliers and remove 
it. Hence, the distance-based (DB) outlier 
detection approach of microarray data is 
proposed and discussed. In this approach, we 
propose the fuzzy weighted support vector 
regression networks (FWSVR) to detecting the 
outliers. Based on the above results, the DB 
outlier detection approach is applied into the 
microarray data of ovarian cancer. Additionally, 
the epsilon in FWSVR approach is obtained by 
repeated SVR method that is regarded as the 
outlier’s threshold. Based on the above results, 
the important genes of ovarian cancer are clearly 
determined and discussed. 
??????? 
A technology for measuring gene 
expression should fulfill a number of demands. 
Obviously, it should be able to detect all 
common types of sequence variation. Also, it 
should use small amounts of biological material 
and be able to detect differences at the 
nucleotide level with very high accuracy. Finally 
it should be fast. The development of microarray 
technologies is an exciting advance in this regard, 
as they are massively parallel and can, in 
principle, survey an entire genome. Gene 
expression data are obtained by comparing for 
each immobilized DNA spot, the relative 
activities for two samples, one reference and one 
test sample. Let us now describe how the 
microarray technique works. To the reference 
sample a redfluorescense label is attached and to 
the test samples a green-fluorescence label. 
When material of a sample has been hybridized 
to a spot, at that place fluorescence will be 
detected. If the fluorescence is strong 
irrespective of the colour, a lot of material has 
co-workers have used Bayesian networks to 
analyze genome-wide expression data in order to 
identify significant interactions between genes in 
a variety of metabolic and regulatory pathways 
[17, 13]. 
K-means Clustering (Partitioning): 
K-means is a divisive clustering approach. It 
partitions data (genes or experiments) into 
groups that have similar expression patterns. k is 
the number of clusters (also sometimes called 
buckets, bins, or classes) into which the user 
believes the data should fall. The number k is an 
input value given to the algorithm by the user. 
The k-mean clustering algorithm is a three steps 
process. In the first step, the algorithm randomly 
assigns all training data to one of the k clusters. 
In the second step the mean inter- and 
intra-class distances are calculated. The mean 
interclass distance of each cluster is calculated 
by firstly calculating a mean vector for each 
cluster and then averaging the distances between 
the vectors (data) of a cluster and its mean vector. 
In expression level data, the mean vector is 
called the average expression vector. The mean 
intra-class distance between two clusters is the 
distance between their respective mean vectors. 
The third step is an iterative step, and its goal is 
to minimize the mean inter-class distances, 
maximize intra-class distances, or both, by 
moving data from one cluster to another. In each 
iteration, one piece of data is moved to a new 
cluster where it is closest to the mean vector of 
the new cluster. Quackenbush [18] discusses a 
few optional steps and variations of this 
algorithm. Tamayo et al. explain that k-means 
clustering is “a completely unstructured 
approach, which proceeds in an entirely local 
fashion and produces an unorganized collection 
of clusters that is not conductive to interpretation 
[19]”. The most recent variant of the k-means 
clustering algorithm (at the time of this survey) 
designed specifically for the assessment of gene 
spots (on the array images) is the work of 
Bozinov et al [20]. The technique is based on 
clustering pixels of a target area into foreground 
and background. The authors report: “results 
from the analysis of real gene spots indicate that 
our approach performs superior to other existing 
analytical methods” [20]. 
Hierarchical Clustering: There are various 
hierarchical clustering algorithms that can be 
applied to microarray data analysis. These 
include single-linkage clustering, 
complete-linkage clustering, average-linkage 
clustering, weighted pair-group averaging, and 
within pair-group averaging [18, 21-23]. These 
algorithms only differ in the manner in which 
distances are calculated between the growing 
clusters and the remaining members of the data 
set. Hierarchical clustering algorithms usually 
generate a gene similarity score for all gene 
combinations, place the scores in a matrix, join 
those genes that have the highest score, and then 
continue to join progressively less similar pairs. 
In the clustering process, after similarity score 
calculations, the most closely related pairs are 
identified in an above-diagonal scoring matrix. 
In this process, a node in the hierarchy is created 
for the highest-scoring pair, the gene expressed 
profilers of the two genes are averaged, and the 
joined elements are weighted by the number of 
elements they contain. The matrix is then 
updated replacing the two joined elements by the 
node. For n genes, the process is repeated n-1 
times until a single element (that contains all 
genes) remains. The first report by Wen et al. 
[21] uses clustering and data-mining techniques 
to analyze large-scale gene expression data. This 
report is significant in that it shows how 
integrating results that were obtained by using 
various distance metrics can reveal different but 
meaningful patterns in the data. Eisen et al. [23] 
also make an elegant demonstration of the power 
of hierarchical clustering in the analysis of 
microarray data. The first problem arises from 
the “greedy” nature of the algorithm. 
Hierarchical clustering is essentially a greedy 
algorithm, and like other such algorithms, it 
suffers from sensitivity to early mistakes in the 
greedy process. Because, by definition greedy 
algorithms cannot go back (backtrack) to redo 
the step that was taken by mistake, “small errors 
in cluster assignment in early stages of the 
algorithm can be drastically amplified” [23]. 
Therefore, the dependence on the results 
produced by certain arbitrarily imposed 
clustering structures (that do not correspond to 
reality) can give rise to misleading results. The 
second drawback of hierarchical clustering is 
best described by Quackenbush [18]: “one 
potential problem with many hierarchical 
clustering methods is that, as clusters grow in 
size, the expression vector that represents the 
cluster might no longer represent any of the 
genes in the cluster. Consequently, as clustering 
progresses, the actual expression patterns of the 
genes themselves become less relevant”. As a 
result, an active area of research in hierarchical 
cluster analysis is in detecting when to stop the 
hierarchical merging of elements. In this 
direction new hybrid techniques are emerging 
that combine hierarchical methodology with 
k-means techniques. 
Mixture models and EM (expectation 
maximization):  
Mixture models are probabilistic models 
built by using positive convex combination of 
distributions taken from a given family of 
distributions [24, 25]. EM algorithm is an 
iterative algorithm that proceeds in two 
correctly separate entities into appropriate 
classes, but also identify instances whose 
established classification is not supported by the 
data. It performs well in multiple biological 
analyses, having remarkable robust performance 
with respect to sparse and noisy data”.  
One of the drawbacks of SVM is its 
sensitivity to the choice of a kernel function, 
parameters, and penalties. For instance, if the 
kernel function is not appropriately chosen for 
the training data, SVM may not be able to find a 
separating hyperplane in feature space. [27] 
Choosing the best kernel function, parameters 
and penalties for the SVM algorithm can be 
difficult in many cases. Because of the 
sensitivity of the algorithm to these choices, 
different choices often yield completely different 
classifications. As a result, “it is necessary to 
successively increase kernel complexity until an 
appropriate (biologically sound) classification is 
achieved” [18]. The ad hoc nature of the penalty 
term (error penalty), the computational 
complexity of the training procedure (a quadratic 
minimization problem), and risk of over fitting 
when using larger hidden layers are further 
drawbacks of this method. 
Other Techniques in Microarray Data Analysis:  
Sasik et al. [25] have presented the 
percolation clustering approach to clustering of 
gene expression patterns base on the mutual 
connectivity of the patterns. Unlike SOM or 
k-means which force gene expression data into a 
fixed number of predetermined clustering 
structures, this approach is to reveal the natural 
tendency of the data to cluster, in analogy to the 
physical phenomenon of percolation. 
GA/KNN is another algorithm described by 
Li, et al [29]. “This approach combines a genetic 
algorithm (GA) and the k-Nearest Neighbor 
(KNN) method to identify genes that can jointly 
discriminate between different classes of 
samples. The GA/KNN is a supervised 
stochastic pattern recognition method. It is 
capable of selecting a subset of predictive genes 
from a set of large noisy data for sample 
classification.” [29]. 
These are many advantages and 
disadvantages for the above approaches. 
However, the above approaches do not consider 
the effects of time for continuous microarray 
samples. In addition, the number of clusters 
must be pre-defined in the clustering algorithm 
and the shape of clusters is easily affected by 
the distribution of microarray data. Hence, we 
will be proposes the dynamic functional 
clustering (DFC) algorithm with new clusters 
merge approach to overcome the problems of 
above approaches in the first year of this project. 
In the DFC algorithm, these are two important 
problems - the structure similarity and 
pointwise similarity that have been defined and 
discussed [30]. Structural similarity relates to 
variety of aspects of the trajectories (functions) 
under consideration: form, evolution, size or 
orientation (of trajectories in nR ) are some 
examples. Pointwise similarity between 
functions is concerned with the closeness of 
functions in the feature space and is based on 
considering functional values directly 
(function’s characteristics or derived functions 
are not relevant in this case). In addition, new 
clusters merge approach is proposed to merge 
clusters. In order to reduce the redundant 
clusters, the similarity measure is used in the 
clustering process. Several measure of 
similarity has been proposed [31, 32]. In [31, 
32], the cardinality concept is incorporate into 
the similarity measure. The proposed similarity 
measure does not depend on the distance 
measure explicitly, and therefore, it can be used 
independently of cluster shape and size.  
Several machine learning techniques have 
been previously used in classifying gene 
expression data for cancer, including Fisher 
linear discriminate analysis, k nearest neighbors, 
decision tree, multi-layer perceptron, support 
vector machine, boosting, and self-organizing 
map. However, those approach mainly focus in 
classification. Hence, we propose, termed as 
support vector interval regression networks 
(SVIRNs) [32], to constructing an interval 
regression (functions) for each cluster. When the 
interval regression of each clusters are 
constructed, the special genes are easily found 
by the gene expression data of ovarian cancer 
microarray. That is, whether the genes 
expression data of ovarian cancer can be fall into 
the interval regression for the same gene or not. 
If genes expression data is fall into interval 
regression for the same gene, we can say that the 
genes are not important gene for ovarian cancer.  
Outliers in Chip experiments can occur at 
several levels. You can have an entire chip that 
bad and consistently deviates from other chips 
made from the same condition or sample. Or you 
can have an individual gene on a chip that 
deviates from the same gene on other chips from 
the same sample. That can be caused by image 
artifacts such as hairs, air bubbles, precipitation 
and so on. Finally, it is possible that a single 
probe, due to precipitation or other artifact, is 
perturbed. However, outliers must be detected 
and removed. Basically, we need a statistical 
model of you data. The simplest model is 
equality among replicates. If one replicate (chip, 
gene, or probe) deviates several standard 
deviation from the mean, then such data can be 
regarded as outliers and remove it. In [34, 35], 
more advanced statistical model have been 
developed that also allow for outliers detection 
)()()()( 2211 qi
q
kikikik xwxwxwxW ⋅⋅⋅⋅=r ,    (7) 
and ( )( ) ( )( ) 







−⋅+
−⋅+
⋅−−
⋅−−= 0,,minmax)( s
k
s
k
s
k
s
i
s
k
s
k
s
k
s
k
s
k
s
k
s
k
s
is
i
s
k
xx
xw βδηβ
δηβ
δηββ
δηβ
, for k= 1, 2, …, C and i= 1, 2, …, p and s= 1, 
2, …, q,                           (8) 
where )(xWk  is the fuzzy weighting of the kth 
local regression model. Then, the overall output 
of the proposed FWSVR is calculated as 
∑
∑
=
== C
k
ik
C
k
i
k
SVRik
i
xW
xLRMxW
xy
1
1
)(
)()(
)(ˆ r
rr
r .           (9) 
The proposed FWSVR approach is 
described in the following.  
Step 1: Set the number of clusters that represents 
the number of the local regressions and 
the overlapped parameterη .  
Step 2: Use the FCM clustering algorithm to 
compute the membership 
iju  using (2), 
the cluster centers jβ
r
 using (14) and the 
spread width sjδ  using (3). 
Step 3: If the stop criterion is not satisfied, then 
go to Step 2; otherwise go to Step 4. 
Step 4: Based on the centers and the spread 
width obtained, the training subsets kψ  
are obtained by (4). 
Step 5: Set the hyper-parameter set {Gaussian 
kernel parameter, regularization 
constant and epsilon} in the SVR 
approach. 
Step 6: Construct each LRM by the SVR 
approach. 
Step 7: Construct the output of the FWSVR 
approach using (8) with the fuzzy 
weighted mechanism using (6). 
In order to deternine the threshold of outliers 
for the Distance-Based Outliers Detection 
method of for Ovarian Cancer Microarray Data, 
the epsilon in the SVR approach is obtained as 
follows:  
Step 1: The SVR with 0=ε  then is applied to 
obtain the estimated results. Consequently, 
the actual and expected output 
corresponding to the same input data were 
used to calculate the regression errors. 
Step 2: Based on the regression errors, the 
appropriate ε  are calculated by 
[ ]( )errors regressionstd×= υε ,  (10) 
where the termed [ ]( )errors regressionstd  
is the standard deviation of the regression 
errors and υ  is a constant .  
 
Figure1 : The structure of FWSVR is shown. 
All microarray procedure is performed in a 
dust/climate control laboratory at China Medical 
University. Microarray design, experimental 
procedures, data processing, and data 
presentation were carefully performed according 
to guidelines of Minimum Information About a 
Microarray Experiment (MIAME). A 
sequence-verified human cDNA library 
containing 9,600 human cDNA clones was a 
kind gift from the National Health Research 
Institute of Taiwan. The clones were originally 
obtained from the IMAGE consortium libraries 
through its distributor (Research Genetics, 
Huntsville, AL). In the experiments the number 
of experiments is 15 and the number of gene n is 
9600. Tissues applied in the current study 
included 5 normal ovaries, 5 benign ovarian 
tumors (OVT), 5 ovarian cancers at stage I 
(OVCAI), and 5 ovarian cancers at stage III 
(OVCAIII). According those data, we propose 
fuzzy weighted support vector regression 
(FWSVR) to detecting the outliers in the 
Distance-Based Outliers Detection method of 
Microarray Data for Ovarian Cancer. Moreover, 
the FWSVR can provide better local 
approximation ability than SVR. Some of results 
are summarized as Table 1~3.  
 
Table 1. The number of up-regulated genes and 
down-regulated genes with proposed method for 
the OVT microarray data. 
OVT epsilon= 
1*std(Y) 
epsilon= 
1.5*std(Y) 
epsilon=
2*std(Y)
The number of 
Up-regulated Genes 
 
1204 
 
356 
 
164 
The number of 
Down-regulated Genes 
 
1121 
 
 
429 
 
179 
 
 
[17] Quackenbush, J. Computational Analysis of 
MicroArray Data (2001) Nature Genetics 2, 
418-427. 
[18] Tamayo, P. et al. Interpreting patterns of 
gene expression with self-organizing maps: 
methods and application to hematopoietic 
differtiation (1999) Proc. Natl. Acad. Sci. 
USA 96, 2907-2912. 
[19] Bozinov D Unsupervised technique for 
robust target separation and analysis of 
DNA microarray spots through adaptive 
pixel clustering (2002) Bioinformatics 
18(5), 747-56. 
[20] Wen, X.et al. Large-scale Temporal Gene 
Expression Mapping of Central Nervous 
System Development (1998) Proc. Natl. 
Acad. Sci USA 95, 334-339. 
[21] Eisen, P. T. et al Cluster Analysis and 
Display of Genome-Wide Expression 
Patterns (1998) Proc. Natl. Acad. Sci. USA 
95, 14863-14868. 
[22] Alon, U.et al. Broad Patterns of gene 
Expression Revealed by Clustering 
Analysis of Tumor and Normal Colon 
Tissues Probed by Oligonucleotide Arrays 
(1999) Proc. Natl. Acad. Sci USA 96, p.p. 
6745-6750. 
[23] Sasik, R. et al. Percolation Clustering: A 
Novel Approach to the Clustering of Gene 
Expression Patterns in Dictyostelium 
Development (2001) PSB Proceedings 6, 
335-347. 
[24] Baldi, P. On the convergence of a clustering 
algorithm for protein-coding regions in 
microbial genomes (2000) Bioinformatics 
16, 367-371. 
[25] Ghosh, D. Mixture modeling of gene 
expression data from microarray 
experiments Bioinformatiocs (2001) 18(2), 
275-286. 
[26] Hastie, T. et al. “Gene Shaving” as a 
Method for Identifying Distinct Sets of 
Genes with Similar Expression Pattern 
(2000) Genome Biology 1, 1-21. 
[27] Brown, M. P. S. et al Knowledge-based 
analysis of microarray gene expression 
data by using support vector machines 
(2000) Proc. Natl. Acad. Sci. USA 97, 
262-267. 
[28] Furey, T.S et al., Support vector machine 
classification and validation of cancer 
tissue samples using microarray expression 
data (2000) Bioinformatics, 16, 906-914. 
[29] Li, et al, Gene selection for sample 
classification based on gene expression 
data: study of sensitivity to choice of 
parameters of the GA/KNN method (2001) 
Bioinformatics 17(12), 131-1142. 
[30] A. Joentgen, et al, Dynamic fuzzy data 
analysis based on similarity between 
functions (1999) Fuzzy sets and systems 
(105), 81-90.  
[31] H. Frigui, et al, A robust algorithm for 
automatic extraction of an unknown 
number of clusters from noise data, Pattern 
Recognition Letters (17), 1223-1232, 1996. 
[32] C. C. Chuang, et al, Support Vector Interval 
Regression Networks for Interval 
regression analysis, Fuzzy Sets and 
Systems (138), 283-300,2003. 
[33] C. C. Chuang, et al, Adaptive Fuzzy 
Regression Clustering Algorithm for TSK 
Fuzzy Modeling, 5th IEEE International 
Symposium on Computational Intelligence 
in Robotics and Automation (CIRA 2003), 
Japan, 2003. 
[34] C. Li, et al, Model-based analysis of 
oligonucleotide array: expression index 
computation and outlier detection, Proc. 
Natl. Acad. Sci. (2001), USA, 31-36. 
[35] C. Li, et al, Model-based analysis of 
oligonucleotide array: model validation 
design issues and standard error application, 
GenomeBioogy (2), 1-11, 2001.  
 
???????? 
??????????????????
????????????????????
????????????????????
????????????????????
?????? 
 FUZZ-IEEE 2007 IEEE International 
Symposium on Fuzzy Systems 
2007 IEEE???????? 
 
 
???: ??? 
 
????????? 
???????? 1? 
 
Email: ccchuang@niu.edu.tw 
 
??? IEEE????????????????????????
????????????????????????????????
????????????????????????????????
????????????????????????????????
????????????????????????????????
?????????????????????????? 
 
?????????????????????? Lotfi Zadeh ?? 
“From Fuzzy Logic to Extended Fuzzy Logic - The Concept of F-Validity and the 
Impossibility Principle”??????????? Rudolf Kruse ?? “The 
Role of Soft Computing in Intelligent Data Analysis” ?????????? 
Jerry Mendel ?? “An Approach to Computing With Words” ??????
? Jim Keller ?? “Recognition Technology for Eldercare”???????
??????????? PANEL???? 
 
?????????????????????????????
??????????? 
1? ????? 
 
??????????????????????? (????
95-2221-E-197-024-)?????IEEE????????????????
????????????????????????????????
????????????????????????????????
???????????????????????????? 
Figure 1 illustrates the block division of CMAC-GBF for a
two-variable case. (Ne=4 and Nb=4). This simple example
has two state variables (x1 and x2) with each quantized into
four discrete regions, called blocks. For instance, x1 can be
divided into A, B, C and D and x2 divided into a, b, c and d.
Areas Aa, Ab,…Dd formed by quantized regions are called
hypercubes. By shifting each block a small interval (called an
element), different hypercubes can be obtained. For instance,
E, F, G and H in the 2nd row for x1 and e, f, g and h in the 2nd
row for x2 are possible shifted regions. Ee, Ef,…Hh are new
hypercubes from the shifted regions. With this kind of
decomposition, if there are Ne elements in a complete block,
we will have Ne layers of hypercubes. In the given example,
there are four layers as shown in Figure l. The state is covered
by Ne different hypercubes, one from each layer. The CMAC
associates each hypercube to a physical memory element.
Information for a discrete state is distributively stored in
memory elements associated with these hypercubes covering
this state
In the original CMAC, a constant value assigned to each
hypercube. By inputting vectors into these hypercubes, no
matter what the positions are, the obtained values are the
same. The output of CMAC is the sum of several values of
hypercube; the input vector is treated as memory addresses.
Therefore, the output is irrelevant to input and the differential
of output to input is also not obtainable. So in CMAC-GBF, a
General Basis Function is used to replace constants in every
hypercube. In this way, the positions of input vector relate to
output value and the learning precision is also increased. In
mathematics, the value iw of a hypercube in a CMAC-GBF
can be expressed as  ss xx iii bvw  , and the output can be
rewritten as follows:
   
 
  .x
vxBa
xwax
,
T
T
 


eN
i
siijs
ss
sss
bva
y
(1)
In this paper, the Gaussian functions are employed as the
basis functions
   


vN
j
jsijsi xb
1
,x  (2)
where
   







  2
2
,
, exp
ij
ijjs
jsij
mx
x 
(3)
ijm is the mean, ij is the variance and vN is the number of
variables in the target function. Consequently, the weight
function is
   


vN
j
jsijisi xvw
1
,x  . (4)
The output from the CMAC with Gaussian basis functions
can be mathematically expressed as
     




 

e vN
i
N
j
jsijiiss xvay
1
,,x  . (5)
The learning procedure are stated as follows. The error
function of CMAC-GBF is defined as
    2T2 xwa
2
1
2
1
sssss yyyE 
 . (6)
The updating rules for iv can be derived as
   










vN
j
jsijissss
e
v
ie
v
i
xay
N
v
E
N
v
1
,,
T xwa 


(7)
where
e
v
N
 is the learning rate for v .
The means and variances of the Gaussian functions can
also be adjusted to increase the approximation capability. The
updating rules for these parameters can be derived as
     







 











3
2
,
1
,,
T 2xwa
ij
ijjs
N
j
jsijiissss
e
ije
ij
mx
xvay
N
E
N
v







(8)
     







 










2
,
1
,,
T 2xwa
ij
ijjs
N
j
jsijiissss
e
m
ije
m
ij
mx
xvay
N
m
E
N
m
v




(9)
where
eN
 and
e
m
N
 are the learning rates for the variances
and means, respectively.
III. SUPPORT VECTOR REGRESSION
Figure 2. Basic idea of SVM to solve the binary classification problem,
separating circular balls from square tiles.
As shown in Fig. 2, the basic idea of support vector
machine (SVM) is to map the training data from the input
space into a higher dimensional feature space via function 
and then construct a separating hyperplane with maximum
margin in the feature space. Given a training set of data
n
i Rx  , li ,...,1 , where l corresponds to the size of the
training data and 1iy class labels, SVM will find a
Feature spaceInput space
554
error and results in a complex model, whereas when C goes to
0, the result would tolerate a large amount of errors and the
model would be less complex.
Now, we have solved the value of w in terms of the
Lagrange multipliers. For the variable b, it can be computed
by applying the Karush–Kuhn–Tucker (KKT) conditions that,
in this case, imply that the product of the Lagrange multipliers
and constrains has to equal to 0
0)),((
0)),((


 bxwy
bxwy
iiii
iiii


,
(17)
and
0)(
0)(



ii
ii
C
C


(18)
where i and i are slack variables used to measure errors
outside the tube. Since i , 0i , and 0i for
),0( Ci  , b can be computed as
),0(for),(
),0(for),( i
Cxwyb
Cxwyb
iii
ii




.
(19)
Putting it all together, we can use SVM or SVR without
knowing the transformation. We need to experiment kernel
function and C, which determines penalties to estimation
errors; and the radius, which determines the data inside the
tube to be ignored in regression.
IV. Integration of CMAC-GBF and Support Vector
Regression Techniques
Figure 4. Basic architecture of the proposed SVR- based CMAC-GBF.
Kernel machines like SVR apply a trick, which is called
kernel trick. They use a set of nonlinear transformations from
the input space to a "feature space". However, it is not
necessary to find the solution in the feature space; instead it
can be obtained in the kernel space, which is defined easily.
We combine the characteristic of SVR with CMAC-GBF to
construct a new scheme and it is named SVR-based
CMAC-GBF.
A SVR-based CMAC-GBF is uncomplicated than a
traditional CMAC-GBF. We just need to evaluate the
basis-function selection vectors of CMAC-GBF by
employing equations (2) and (3), and utilize it as SVR
training data set, not necessarily to pass through the process
of data retrieval and learning.
To build SVR-based CMAC-GBF system shown as figure
4 has three tasks: (1) determine the CMAC hypercubes, (2)
according to CMAC hypercubes, we can obtain the
basis-function selection vectors sets of CMAC-GBF, which
correspond to each input and (3) to obtain support vector
regression.
The steps to build above system are as follow:
1. For a given input S and the target output y (S), define
the dimension of the learning space and the learning
space range, then find all hypercubes that cover the input
S.
2. To obtain the basis-function selection vectors sets of
CMAC-GBF according to CMAC hypercubes.
3. Selecting the parameters of SVR. To build a SVR model
efficiently, SVR's parameters must be specified carefully.
These parameters includes:
1) Kernel function: The kernel function is used to
construct a nonlinear decision hyper-surface on
the SVR input space. Generally, using Gaussian
function will yield better prediction
performance. Therefore, the Gaussian function
is used as the SVR kernel function in this study.
2) Regularization parameter :C C determines
the trade-off cost between minimizing the
training error and minimizing the model's
complexity.
3) Bandwidth of the kernel function (): This
parameter represents the variance of the
Gaussian kernel function.
4) The tube size of -insensitive loss function ():
It is equivalent to the approximation accuracy
placed on the training data points.
4. Input training samples using the CMAC association
vectors set.
5. Training support vector regression.
6. According the input and output samples (S, y ) to
generate test data set, using the data set to test and
evaluate system capability.
As discussed previously, there are many parameters that
must set for SVR. For the SVR, no standard procedure exists
to determine the parameters C , and [6-8]. Hence, we
have tried several combinations for obtaining SVR optimal
parameter sets and finally chose RBF as the kernel. In our
simulation, as training SVR we should pay attention to two
cases. First case is the training data without noise. And let
SVM be -SVR (=0), kernel type be radial basis function.
Besides, kernel parameter  and C are chosen as the
number of layer and 1024, respectively. Second case is the
training data with noise data. And let SVM be -SVR, kernel
type be radial basis function. Besides, kernel parameter 
and C are chosen as the number of layer and 1024,
SVR outy
y
Training SVR with
C , ,
x
Input
Space
a1
a2
a3
aNh
w1
w2
w3
wNh
 
Association Vector of
CMAC
basis-function selection
vectors of CMAC-GBF
556
