3 Research on Hierarchical Data
By hierarchical classification we mean that class labels correspond to nodes in a
rooted tree. For example, a web directory can be represented in a tree format
so that a class “restaurant” has sub-classes like “Italian restaurant” or “Chinese
restaurant.” A Chinese restaurant belongs to both “restaurant” and “Chinese
restaurant” classes. The following figure gives a hierarchical structure of six class
labels (copied from our project proposal):
1
2 3 4
5 6
Figure 1: A structure of six labels
One can ignore this structural information and train the data as a regular
six-class problem. However, most people think that incorporating the hierarchical
information can give a better model. We thought so too and proposed some new
SVM training strategies for hierarchical data.
After some efforts, unfortunately, we failed to publish any work in major con-
ferences or journals. Looking back, several reasons may cause our failures:
1. From Figure 1, we can ignore hierarchical information and train a regular
multi-class model. We find that it is difficult to clearly show the advantage
of incorporating hierarchical information. Using our proposed methods, the
accuracy difference between with and without hierarchical information is
sometimes small.
2. There are not many publicly available data sets. Thus we cannot perform
enough experiments to validate our methods.
3. Among various types of structured data sets, hierarchical data have not been
a very important one. Other types (e.g., search ranking) of problems have
emerged to be more popular nowadays.
2
6 Conclusions
While we are not very successful in studying structured data, this research has lead
us to identify some other important directions. Overall working on this project is
very rewarding.
References
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871–1874, 2008. URL
http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A
dual coordinate descent method for large-scale linear SVM. In Proceedings of
the Twenty Fifth International Conference on Machine Learning (ICML), 2008.
URL http://www.csie.ntu.edu.tw/~cjlin/papers/cddual.pdf.
F.-L. Huang, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. Itera-
tive scaling and coordinate descent methods for maximum entropy.
Journal of Machine Learning Research, 11:815–848, 2010. URL
http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_journal.pdf.
S. S. Keerthi, S. Sundararajan, K.-W. Chang, C.-J. Hsieh, and C.-
J. Lin. A sequential dual method for large scale multi-class lin-
ear SVMs. In Proceedings of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2008. URL
http://www.csie.ntu.edu.tw/~cjlin/papers/sdm_kdd.pdf.
C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region Newton method for large-
scale logistic regression. Journal of Machine Learning Research, 9:627–650,
2008. URL http://www.csie.ntu.edu.tw/~cjlin/papers/logistic.pdf.
4
