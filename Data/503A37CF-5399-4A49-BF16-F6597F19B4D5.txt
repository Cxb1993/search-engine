                                           2
行政院國家科學委員會專題研究計畫報告 
 
總計畫: 結合生物反饋之新世代腦機介面及其在移動載具
控制之應用 
 
子計畫四:以機器視覺為基礎之人意向移動載具獲知與追蹤 
 
Subproject IV: A Machine Vision-Based Human-Intending Moving Vehicle 
Capture and Tracking 
 
計畫編號 : NSC–96–2221–E–009–222 
 
執行期限：96/08/01–97/07/31 
 
主持人：張志永    交通大學電機與控制工程學系教授 
 
 
一、 中文摘要 
本計畫以機器視覺為基礎之人意向移動載
具獲知與追蹤課題。腦波生理訊號，通常受試驗
者的視覺、聽覺、感受等影響，本子計畫的目的，
在於利用視覺狀態回饋給使用者，探討其對的人
腦波訊號之影響。利用本計畫結果，子計畫三可
據此鑑別岀重要視覺腦波生理訊號的位置，即頻
道，及訊號強弱變化，一起與子計畫五方向感腦
波訊號、子計畫二注意力腦波訊號，整體訓練學
習後，能提昇在移動載具的控制效率。本子計畫
執行所需之移動載具，擬以電動代步車為實驗平
台。第一年將探索在室內環境情況下，人坐在移
動載具車之視訊獲知及其方位獲知方法。 
由於近年來智慧型機器人的發展迅速，機器
人與人之間的互動也愈來愈頻繁，當機器人與人
溝通時，必須知道人的位置、距離及面對的方向
等資訊。在此篇論文中，我們結合二維影像輪廓
比對與臉部方位偵測來完成三維人體方位偵
測。首先，任一張影像的前景人物利用一個基於
前後影像比值而建立之統計背景模型抽取出
來，並將抽取出來影像轉換成二值化的影像格
式，進而獲得前景人物的輪廓。透過人體輪廓樣
板比對及線性內插法，可以初步得到前景人物所
朝的方向。當人臉朝向前方 30∘以內時，也可
透過雙眼與臉的三角幾何關係來估算人體所朝
的方向。經實驗證明，我們提出的方法對於人體
方位偵測的準確度相當高，角度誤差低於 4∘。 
 
關鍵詞：人物方位偵測、輪廓對應、臉部與
眼睛抽取、傅利葉描述器 
 
英文摘要 
This project is concerned with machine 
vision-based moving vehicle capture and tracking. 
It is known that brain signals are dependent on the 
excitations of vision, audio, and feeling of a tester. 
The purpose of this project is to provide  various 
feedback to a user and then investigate its effect on 
brain signaling. With an excitation, subproject III 
will identify the responsive brain channel and its 
signaling variation. The brain signaling variations 
of a person invoking a multi-task distraction and 
spatial disorder are studied by subprojects II and V, 
respectively. With these identified responsive brain 
                                                                                      4 
 
                                        
statistics: minimum intensity value  ( )yxn  , , 
maximum intensity value ( )yxm  ,  and 
maximum inter-frame ratio ( )yxd  ,  of a 
background video. Because these three values 
are statistical and obtained from a background 
modeling algorithm [1], we need a background 
video, without any moving objects, for 
background model training. Let I be an image 
frame sequence and contains N consecutive 
images. ( )yxIi ,  be the intensity of a pixel 
which is located at ( )yx,  in the i-th frame of I. 
The background model, ( ) ( ) ( )[ ]yxdyxnyxm  , , , , , , 
of a pixel is obtained by 
( )
( )
( )
( ){ }
( ){ }
( ) ( ){ }
( ) ( )
( ){ }
( ){ }
( ) ( ){ }
1
1
1
max ,
 min ,           if  , , 1
, max , ,
,
max ,,
 min ,                  otherwise   
max , ,
ii
i i ii
i ii
ii
ii
i ii
I x y
I x y I x y I x y
m x y I x y I x y
n x y
I x yd x y
I x y
I x y I x y
−
−
−
⎧ ⎡ ⎤⎪ ⎢ ⎥⎪ ⎢ ⎥ ≥⎪ ⎢ ⎥⎡ ⎤ ⎪ ⎢ ⎥⎪ ⎣ ⎦⎢ ⎥ = ⎨⎢ ⎥ ⎡ ⎤⎪⎢ ⎥⎣ ⎦ ⎢ ⎥⎪ ⎢ ⎥⎪ ⎢ ⎥⎪ ⎢ ⎥⎪ ⎣ ⎦⎩
                   (1) 
 
Foreground subjects can be segmented from 
every frame of the video stream. Each pixel of 
the video frame is classified to either a 
background or a foreground pixel by the 
difference between the background model and 
a captured image frame. We utilize the 
maximum intensity ( )yxm  , , minimum intensity 
( )yxn  ,  and maximum inter-frame ratio ( )yxd  ,  
of the training background model to segment a 
foreground by 
⎪⎪⎩
⎪⎪⎨
⎧
=),( yxB
 
(2) 
where ( )yxIi  ,  be the intensity of a pixel which 
is located at ( )yx, , ( )yxB  ,  is the gray level of a 
pixel in a binary image and k is a threshold. 
Threshold k is determined by experiments 
according to the environments. The value of k 
affects the mount of information retained in 
binary image B. 
 
B. Silhouette Matching Using Fourier 
Descriptor and Linear Discriminant 
Analysis 
Fourier descriptor [2] is a useful 
implement for describe closed curve shape 
which obtained from subject contour. We can 
obtain a periodic function along the closed 
curve and this function can represent by a 
Fourier series. In this paper, we introduce Zahn 
and Roskies’ [3] Cumulative-Angle Approach 
to represent Fourier descriptors. We assume γ  
is a clockwise-oriented simple closed curve 
with parametric representation ))(),(( lylx . Let 
))0(),0(( yx  be the starting point and we denote 
,)()()( ljylxlZ +=  where l is the arc length of the 
starting point to )(lZ  and Ll ≤≤0 . Denote the 
angular direction of γ  at point l by the 
function )(lθ  and let )0(0 θδ =  be the absolute 
angular direction at the starting point )0(Z . We 
now define the cumulative angular function 
)(lφ  as the net amount of angular bend between 
starting point and point l. With this definition 
0)0( =φ  and 0)( δφ +l , is identical to )(lφ  except 
for a possible multiple of π2 . Besides, It is not 
hard to see that πφ 2)( −=L  because all smooth 
simple closed curves with clockwise 
orientation have a net angular bend of π2− . As 
a result, )(Lφ  does not convey any shape 
otherwise.         pixel foreground a    ,255
),(),(),(
or
),(),(),(
 if        pixel background a    ,0
⎪⎩
⎪⎨
⎧
<
<
yxkdyxnyxI
yxkdyxmyxI
i
i
. , ... 2, ,1 Ni =
                                                                                      6 
 
                                        
 
 
 
 
 
 
 
 
Fig. 1.  The representation of a 3D subject by 
2D views. 
. 
III. Face Direction Detection 
In our research, we propose a method to 
estimate the face direction of subject interests 
to enhance the accuracy of direction detection 
of target subject interests. Using image 
processing, the face angle will be estimated 
from the geometric relationship of pupils in the 
face. Firstly, we utilize skin detection formula 
in the rbCYC  color space and edge detection to 
find head region. Secondly, we utilize PCA to 
find eyes position and then calculate the pupil 
centers for estimate the face direction. Conse- 
quently, we can combine object direction from 
FD and face direction to better detect the direc- 
tion of subject/object of interests. 
A. Face Tracking by PTZ Camera 
In our research, we using Pan-Tilt-Zoom (PTZ) 
camera to track human face before face 
direction detection. In order to estimate face 
direction correctly, we have to zoom the face of 
target person in to occupy the image between 
70% to 80% vertically. We utilize rbCYC  skin 
color segmentation method to detect the face 
region initially. There will be some noise or 
other skin color region (e.g. hand, leg, etc.) 
segmented left in the extracted image. We have 
to eliminate noise and non-face region. Then 
calculate the coordinates of the center of the 
face region, which are used to compute the 
distance to the center of image. We recode the 
pixels of x-direction and y-direction from the 
distance which is proportionality to the camera 
pan and tilt time, and compute the occupying 
ratio of the human face in the image. According 
to these parameters, PTZ camera will track 
human face automatically. 
B. Head Region Extraction 
The first stage of the algorithm is to classify the 
pixels of the input image into skin region and 
non-skin region [5-9]. To do this, we obtain a 
skin-color reference map in rbCYC  color space 
[5]. The second stage is to combine the region 
of skin color detected and the result from edge 
detected. Because skin color segmentation is 
unable to get the whole region of head, thus we 
have to utilize the edge detection method, then 
do the union with the face region. Thus, we can 
get a more complete head region. 
 
C. Eye Detection  and Pupil Center Estim- 
ation Using PCA 
Most approaches in computer recognition 
of faces and expressions have been focused on 
detecting individual features such as eyes, head 
outline, mouth, or defining a face model by 
position, size, and relationships among these 
features [10-12]. Features extraction plays an 
essential role in the pre-processing stage. 
Principal component analysis (PCA) has been 
commonly used to face recognition problems. 
Typical PCA algorithm is one of the main 
streams of research on face feature processing 
                                                                                      8 
 
                                        
Consequently, when the human direction angle 
(or face direction angle) is within o30± , the 
face is more accurately than that from the 
subject’s silhouette. On the other hand, the face 
accuracy will decrease quickly when the angle 
is larger than ± 40°. Therefore, we can further 
fuse the face direction detection if the face is 
available and within ± 30°. Hence, we use the 
weights, in which 1w  and 2w , are constrained, 
as  shown in Fig. 4., by the following 
relationship. 
 
 
 
 
 
 
 
 
 
 
Fig. 4.  Weight graph of detection reliability. 
 
The subject direction θ  is estimated by using 
the weights and a linear interpolation method 
as follows. 
,21 αψθ ww +=               (9) 
where ψ  and α  are the estimated face and 
subject direction angles, respectively. Table III 
is the estimated results by the proposed method 
which combine the results of subject and face 
direction detection. 
 
To evaluate the estimation performance, 
we calculated the mean absolute error (MAE) 
of estimated angle of each model. Furthermore, 
we can estimate the average error of these five 
models. When subject direction detection using 
silhouette only, the average error is 4.58°. 
However, we fuse the face and subject 
direction detection, it can reduce 13% of error 
to 3.99°. As shown in Table IV, linear 
combination approach produces better 
performance than that from subject’s silhouette 
only. Table IV shows the comparison of the 
MAE and average error of method 1: using 
only subject direction detection, and method 2: 
combine subject and face direction detection. 
 
V. Conclusion  
In this paper, we have proposed an 
approach which combine subject’s silhouette 
matching and face direction for the human 
direction detection. In our direction detection 
system, we first utilize Fourier descriptor to 
discriminate different directions of target 
subject ranging from 90° to -90°. In addition, 
we exploit the linear discriminant analysis to 
optimize the class separability and improve the 
classification performance. Moreover, we also 
estimate the face direction from one’s pupils 
and pupils’ geometric relationship as well. 
When the human direction angle (or face 
direction angle) is within ± 30°, direction 
detection from the face is more accurately than 
that from subject’s silhouette, and decrease 
quickly when the angle exceeds .40o±  
Therefore, we fuse to detect direction if face 
detection reliable. The experiment results have 
shown that our approach can obtain a high 
accuracy on subject direction detection, with 
mean absolute error less than 4°. 
 
For future work, we shall increase the 
detection from the back subject’s side ranging 
                                                                                      10 
 
                                        
-10 -13.05 -5.97 -5.80 -16.78 -6.55 
-20 -16.87 -15.87 -15.2 -26.43 -16.67
-30 -24.79 -26.68 -33.51 -35.88 -34.20
-40 -35.61 -43.59 -43.76 -44.37 -44.60
-50 -44.01 -54.02 -55.08 -55.17 -53.44
-60 -65.12 -63.9 -63.6 -63.89 -63.96
-70 -74.82 -65.76 -65.67 -85.81 -65.91
-80 -84.69 -85.92 -73.13 -83.99 -71.88
-90 -84.55 -86.78 -84.30 -85.20 -76.28
 
 
 
TABLE II 
THE FACE DIRECTION BY THE PUPIL LOCATIONS 
 
Model 1 Model 2 Model 3 Model 4 Model 5
angel[°] 
ψ [°] ψ [°] ψ [°] ψ [°]°] ψ [°] 
60 12.33 54.23 36.57 27.54 -6.04 
50 12.32 12.85 26-87 33.65 39.45 
40 33.98 37.56 36-75 37.15 33.12 
30 25.43 27.95 30.54 28.68 26.88 
20 15.75 18.38 19.64 20.38 19.03 
10 8 12.17 9.44 13.22 8.35 
0 -2.86 -2.45 3.71 4.55 0.66 
-10 -12.88 -15.97 -10.24 -15.65 -12.73 
-20 -18.39 -22.84 -20.92 -25.14 -19.79 
-30 -26.72 -30 -31.61 -33.54 -28.09 
-40 -39.84 -36.78 -41.81 -29.78 -36.57 
-50 -34.25 -31.69 -32.23 -23.17 -30.17 
-60 -35.3 -24.86 -15.68 -24.7 -45.2 
 
 
 
 
 
TABLE III 
THE ESTIMATED SUBJECT DIRECTION BY THE PROPOSED METHOD 
 
Model Model Model Model Model 
angel[°] θ [°] θ [°] θ [°] θ [°] θ [°] 
90 85 83.56 85.30 83.61 86 
80 74.65 76.33 84.53 82.98 75.9 
70 65.73 74.4 65.67 75.61 74.12 
60 55.13 63.62 64.24 65.10 56.48 
50 53.43 46.24 54.35 54.80 54.07 
40 34.18 37.24 43.86 43.61 43.47 
30 25.74 27.13 28.42 31.56 26.57 
20 15.76 18.38 19.64 20.38 19.03 
10 8 12.17 9.44 13.22 8.35 
0 -2.86 -2.45 3.71 4.55 0.66 
-10 -12.88 -15.97 -10.24 -15.65 -12.73
-20 -18.39 -22.84 -20.92 -25.14 -19.79
-30 -25.75 -28.34 -32.56 -34.7 -31.14
-40 -35.61 -43.59 -43.76 -44.37 -44.60
-50 -44.01 -54.02 -55.08 -55.17 -53.44
-60 -65.12 -63.9 -63.6 -63.89 -63.96
-70 -74.82 -65.76 -65.67 -85.81 -65.91
-80 -84.69 -85.92 -73.13 -83.99 -71.88
 
TABLE IV 
MAE COMPARISON OF DIFFERENT METHODS 
 
Method Model Model Model Model Model Average
Silhouette 
only 
4.61° 4.37° 4.37° 5.31° 4.65° 4.58° 
Combining 4.28° 3.64° 3.43° 4.87° 3.79° 3.99° 
 
                                                                                            2 
一、參加會議經過    
2007 年, 第 十四 屆, 神經資訊處理國際會議 (2007 14th International 
Conference on Neural Information Processing Systems) 它是亞太神經網路聯
合會(APNNA)的年度會議，也是類神經網路領域最重要、歷史最悠久、參
與者極多的國際會議；2005 年由本校本人等主辦，本人為 APNNA 理事，
除 發 表 論 文 ， 將 參 加理監事會議，本人並獲邀為本次會議之
Bioinformatics Special Session 邀稿；本屆 2007 年，於 2007 年 11 月 13
日至 11 月 16 日在日本九州北九州市學研園區 Hibikino 舉行，九州理工學
院 Prof. T Yamakawa 為大會主席，九州理工學院 Prof. M. Ishikawa 與沖繩大學
Prof. K. Dora 博士為大會共同主席， 由亞太神經網路聯合會(APNNA)主辦，
日本類神經網路、九州理工學院、Riken 腦科學研究所、日本模糊理論與智
慧資訊學會(SOFT)等協辦。本人，於 96 年 11 月 9 日上午搭長榮航空赴北九州
福岡市，在附近旅遊數天，於 11 月 13 日至 11 月 16 日參加會議，11 月 15
日參加理監事會議，決定 09 年由泰國主辦，10 年由澳洲主辦； 11 月 17
日上午搭長榮航空返國。此次國內參與會議及發表論文者，另有中研院程爾觀、
劉長萱教授台灣大學劉長遠教授，及台大、清大等校教授研究生數名。  
本次會議共有約 270 篇論文投稿，錄取近 200 篇，錄取率 70%強，相當競爭，
一個較小型但極為相關之腦資訊技術會議伴隨本次會議舉行議程每天有六場論文
發表場次，同時進行，上午一個場次，下午兩個場次場論文發表，口頭與張貼發
表交替舉行，論文發表場次主題涵蓋神經網路重要領域與應用，包括：神經
網路模型 計算神經科學、運動控制與視覺 學習與記憶 學習演算法、
學習理論、計算基因演算法、認知科學、影像處理、生物認證、生物資訊、
圖形識別、電腦視覺、新奇應用、資料探勘與知識獲取、統計學習、模糊
神經系統、神經系統架構與實現等，每天上、下午並安排大會演講一場，大會
演講包括：  
(1) Mitsuo Kawato, ATR Computational Neuroscience Laboratories,  
講題： Cerebellar Long Term Depression as a Supervised Learning Rule with All 
or Nothing Character at Each Synapse；  
(2) Rajesh P. N. Rao, University of Washington, USA,  
                                                                                            4 
演講，以借鏡大師風範，吸收研發最新成果與方向。我的論文，“應用模糊 K-
最近相鄰點法於蛋白質可溶性預測＂我及學生徐家杰、施逸祥合作 (“Fuzzy 
K-Nearest Neighbor Classifier to Predict Protein Solvent Accessibility” by J.Y. Chang, 
Jia-Jie Shyu, and Yi-Xiang Shi) 發表場次，於 6 月 16 日早上生物資訊場次演講發
表， 論文重點如下：蛋白質在生物體中一直扮演著很重要的角色，蛋白質被發現
的數量及其結構逐年增加。隨著蛋白質的應用越來越廣泛，待解決的課題也就越
來越多。例如：蛋白質二級結構預測問題、蛋白質相對溶劑可接觸性預測問題等。
本篇論文，我們利用修改的模糊 K-最近相鄰點法，混合從 PSI-BLAST 產生的位置
加權矩陣，針對蛋白質相對溶劑可接觸性預測問題進行研究。最近 Sim 等人，應
用模糊 K-最近相鄰點法於蛋白質可溶性預測有顯著的效果。我們提出改進之模糊
K-最近相鄰點法，應用在三態相對溶劑可接觸性預測和二態相對溶劑可接觸性預
測，所得到的實驗結果與近幾年的其它方法比較，有較佳的預測正確率。發表演
講期間後，計有紐西蘭學者詢問討論，氣氛極佳。 
 
二、 與會心得 
日本在推展亞太或全世界科技學術活動，一向盡心盡力，不落人後。
本次大會慷慨解囊，資助差旅費給 8 名學生、6 名教授或研究者，主要是開發中
國家，值得鼓勵與學習。日本在參與國際會議向來踴躍，尤其亞太地區會議，
通常占參加者一半以上，例如 ICONIP05 在台北市舉行，約六成是日本參
加者，本人基於回報心理，鼓勵國人一起共襄盛舉，我們參加者，輸給地
主國日本、韓國，居第三名，尚多出大陸(含香港) 參加者，還算踴躍，總算
給予日本相當回報。  
國際觀光旅遊，國際性語言，例如英語，溝通是必要條件。但日本一般民
眾英語極差，不會聽，不能講，國際旅客各方面需求，如交通、點餐、購物等，
幾乎很困難達成，因此國際旅客，除我們與韓國外，極少，我們與韓國勉強可
以手寫漢字溝通，日本有好山好水及極佳市容與治安，觀光旅遊業猶有很大發
展空間。   
 
三、 建議 
