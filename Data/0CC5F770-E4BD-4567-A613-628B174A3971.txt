Computing (CTHPC 05), Tunghai 
University, Taichung, Taiwan, pp. 178-188,
March 17-18, 2005. 
陳棅易 碩士班 
[1] Chao-Tung Yang, Ping-I Chen, Sung-Yi 
Chen, and Hao-Yu Tung, “A Jobs Allocation 
Strategy for Multiple DRBL Diskless Linux 
Clusters with Condor Schedulers,”
Proceedings of the Fifth International 
Conference on Grid and Cooperative 
Computing, China, Oct. 2006. 
[2] Wen-Chung Shih, Chao-Tung Yang, 
Shian-Shyong Tseng, and Ping-I Chen, “A 
Hybrid Parallel Loop Scheduling Scheme 
on Heterogeneous PC Clusters,”
Proceedings of the 6th International 
Conference on Parallel and Distributed 
Computing, Applications and Technologies
(PDCAT 2005), pp. 56-58, December 5-8, 
2005. (EI) 
[3] Chao-Tung Yang, Ping-I Chen, and Ya-Ling 
Chen, “Performance Evaluations of SLIM 
and DRBL Diskless PC Clusters on Fedora 
Core 3,” Proceedings of the 6th 
International Conference on Parallel and 
Distributed Computing, Applications and 
Technologies (PDCAT 2005), pp. 479-482, 
December 5-8, 2005. (EI) 
負責本計畫監視與操控系
統建置及效能測試。 
王世宇 碩士班 
[1] Chao-Tung Yang, I-Hsien Yang, Shih-Yu 
Wang, Kuan-Ching Li, and Ching-Hsien 
Hsu, “A Recursive-Adjustment 
Co-Allocation Scheme with 
Cyber-Transformer in Data Grids,” accepted
and to appear in Future Generation
Computer Systems, 2007. (SCI JCR 
IF=0.555, EI) 
[2] Chao-Tung Yang, I-Hsien Yang, 
Kuan-Ching Li, and Shih-Yu Wang, 
“Improvements on Dynamic Adjustment 
Mechanism in Co-Allocation Data Grid 
Environments,” accepted and to appear in 
The Journal of Supercomputing, December
2006. (SCI JCR IF=0.482, EI) 
[3] Chao-Tung Yang, I-Hsien Yang, 
Chun-Hsiang Chen, and Shih-Yu Wang, 
“Implementation of a Dynamic Adjustment 
Mechanism with Efficient Replica Selection 
in Co-Allocation Data Grid Environments,”
Proceedings of the 21st Annual ACM 
Symposium on Applied Computing (SAC 
2006) - Distributed Systems and Grid 
Computing Track, vol. 1, pp. 797-804, April 
23-27, 2006. (EI) (Paper acceptance rate 
300/927=32%) 
[4] Chao-Tung Yang, Shih-Yi Wang, Che-Hung 
Lin, “Design and Implementation of a 
Toolkit with Efficient Replica Management 
and Data Transfer in Data Grid 
Environments,” 12th Workshop on Compiler 
Techniques for High-Performance 
Computing (CTHPC 2006), Tainan, Taiwan, 
March 16-17, 2006. 
[5] Chao-Tung Yang, Shih-Yu Wang, Che-Hung 
Lin, Ming-Hsiao Lee, and Tsung-Ying Wu, 
“Cyber-Transformer: A Toolkit for Files 
Transfer with Replica Management in Data 
Grid Environments,” Proceedings of the 
Second Workshop on Grid Technologies and 
負責本計畫排程系統及效
能測試。 
備的企劃、整合以及執行能力。因此本計畫參與人員在計畫結束之後，不僅僅學習到「技
術」，而能增強自身的企劃整合能力，並能以宏觀的視野及多樣化的觀察角度分析當前的產
業界，相信在未來必定能提升國家的產業發展。預期可獲之訓練如下： 
z 了解如何建構個人電腦叢集式系統。 
z 熟悉個人電腦叢集系統環境與平行應用程式發展工具。 
z 了解 SCMS 並搭配個人電腦叢集監控管理系統使用。 
z 了解設計與製作發展個人電腦叢集系統之 Scheduler，提供安全性的工作提交機制。 
z 了解設計與製作個人電腦叢集系統狀態資訊擷取應用程式界面(API)，並以 Web 
Service 封裝，用以提供各項存取裝置之監測應用程式呼叫使用。 
 
技術研發成果說明： 
完成之工作項目及成果 
z 設計與建構搭配 Redhat Linux 作業系統與 NFS, NIS, PVM (Parallel Virtual Machine)
或 MPI (Message-Passing Interface)訊息傳遞程式庫的個人叢集式電腦系統。 
z 使用 NFS 平行檔案系統軟體，以達成高效能。 
z 使用 J2ME 行動通訊系統發展軟體於無線行動監控管理系統，以達成能存取個人電
腦叢集系統資料。 
z 發展個人電腦叢集系統之 Scheduler，提供安全性的工作提交機制。 
z 開發建置個人電腦叢集系統狀態資訊擷取應用程式界面(API)，並以 Web Service
封裝，用以提供各項存取裝置之監測應用程式呼叫使用。 
z 完成設計與製作適用於 PDA 與手機的監控管理系統。 
z 完成設計與發展個人電腦叢集系統延伸應用。 
 
技術特點說明： 
本系統特性與整體功能 
近年來，拜莫爾定律所賜，個人電腦的效能快速提升，同時電腦網路的興盛使得個人
電腦叢集（PC Clusters）之勢銳不可擋。簡單的說，個人電腦叢集是一群獨立的單微處理
機亦或對稱式多處理機系統透過高速網路的連結，來做為媲美超級電腦效能的替代方案。
然而，個人電腦叢集已經出現多年了，端看其傳統的運作方式，系統管理者必須為每個節
點安裝作業系統以及相關的軟體套件。近來，運用叢集式電腦系統搭配 Linux 作業系統與
PVM 或 MPI 訊息傳遞程式庫，來執行高速或平行計算已經到達可行階段，目前現存的叢集
電腦之監視器與排程器雖有一定成果，但目前卻無一套整合此兩種系統的出現，故本計畫
的目的在於整合叢集監視系統與排程器，並且改善某些監視系統的缺點。本計畫之系統全
為自行開發，並且監視器有自訂的 Protocol，排程系統為使用自行 Implement 的演算法。本
計畫將陳述本實驗室所建構的兩套各四台主機的個人電腦叢集，並簡介我們如何降低與分
攤系統負擔達到快速監測與控制並強化其高可用度。 
本計畫的目的是希望可以用不同的架構，達到相較於 SCMS (Scalable Cluster 
Management System)較好的高可用度、監測速度，並有較好的工作管理功能。由於在 Linux
作業系統上個人電腦叢集技術已有相當的成熟度，故目前較急切發展的便是較具親和力的
管理、使用、及監測的介面，使得一般使用者能較輕易的操作、管理，並且能夠在最快速
的時間內了解各機器的現況，所以如何避免頻繁的使用 Super User 來管理，導致增加其危
程功能。它的優點在於可以針對多重叢集架構的網格系統的架構做監控，取得有用的系統
資訊，並可將其顯示在網頁上。 
以下是 SCMS 目前所具的問題： 
z 當 Cluster 或 Grid 的規模過於龐大時，對於 Cluster Master node 或結構上較上層的
node 負荷過大，可能會發生無法預期的錯誤。如上圖 1 中的 Information Collector
和 Data Access Server。 
z 一旦底層的 node 發生錯誤，無法即時偵測與恢復。 
z 無法針對系統狀態做動態工作分配。 
z 安裝繁瑣複雜。 
 
 
圖 1：SCMS 系統架構圖 
 
OpenPBS：Open Portable Batch System 是一套為叢集系統開發的遠程排程系統
(Long-term Scheduler)，其系統最大的特色為具有自己的專寫的 Script Parser。系統使用
Multi-Queue，Queue 之間為 Round-Robin，但美中不足的地方為： 
z 確認 Queue 中工作的存在與否使用週期性偵測，如下圖 2。此舉不但影響工作的時
效性，而且會佔用 CPU 使用率。 
z 無法針對各主機狀態有效的分派工作。 
 
 
圖 3：資訊處理階層圖 
 
各階層之間目前使用 Send – Receive 進行資料交換，將來可能會改採 Multicast 的方
式。Observer 使用的 Protocol 詳見附錄一，其中對於 Scheduler 的支援目前採用 Load Average
的分析，以下是分析所採用的公式： 
定義： 
L1 ：1 分鐘內負載 
L5 ：5 分鐘內負載 
L15 ：15 分鐘內負載 
C ：CPU 數量 
S ：Score，總分。分數愈高被指派工作的優先權越高。 
%)25*%50*%25*/( 1551 LLLCS ++= ……………………………………………… (1) 
公式(1)中，5 分鐘內負載的權重較高的原因在於較具代表性。1 分鐘內的負載可能只代
表一時的負載高，而 15 分鐘內負載所經過的時間太久，因此這兩個值得權重較高的話，容
易失去代表性。 
Scheduler：以下將介紹 Scheduler 的技術中比較細節的部份和一些相關的探討，包括了
系統設計上的問題和改進方法，首先介紹此 Scheduler 的系統流程。圖 4 中，Main Thread
即為 Scheduler 這個 Process 的主要流程，當接收到工作要求時，會自動產生一個 Thread 叫
Queue Handler，其功能為將工作需求與內容加入到 Queue 中，並且使用 Signal 將 Scheduler
從 Wait 狀態中叫醒並執行工作。由圖中也可顯示，當有工作需求正在加入時，仍允許其他
需求加入 Queue，不過不會拋棄 Mutual Exclusion 的原則，其方法與原理以下面會逐一介紹。 
Set Re-traversal
Mutex A lock
Enqueue job
Mutex B unlock
Mutex B try lock
Send signal
Mutex A unlock
Lock successful Lock failed
 
圖 5：Queue Handler 流程圖 
 
運用利用這個小技巧，可以準確的知道目前已經有人在等待 signal，因此如果嘗試鎖定
失敗的話，Queue Handler 就會執行 pthread_kill()的動作送出一個 Continue 的信號給對方，
而在不同的作業系統當中，Signal 的編號的設定上有相當多不同的地方，所以在查詢了許
多的相關資料之後，了解到許多的即時作業系統都有一些緊急的 signal 的支援，因為有些
嵌入式的即時作業系統也使用 Linux 作為系統的核心，而其中有一個 Continue 的指令相當
適合此實作所要用到的功能，而所要送出的信號便是這個信號；而先前如果鎖定成功的話，
這表示目前 Job executive 正在執行某些平行程式，但是這個時候可能會因為一些 Scheduling
的時間差，因此造成系統已經檢查過的 Entry 裡面剛剛被塞入了新的 Job 資料，可能使得系
統認為目前沒有工作，而使得剛剛指派的工作被長時間的擱置，直到下次有成功的加入
Queue 的工作資訊並且 Wake up 起來主要的 Job executive 之後，才會被執行到的窘境。 
為了防止上面所說的情況發生，設了一個 Flag 功能的變數，如果程式中的 Job executive
執行程式的時候，有任何一個 Queue Handler 更改到了 Queue 的資訊的話，系統就要設定這
個 Flag 說明目前的 Queue 已經被更動過了，因此 Scheduling 的流程就必須要重新跑一次去
檢查目前的 Queue 裡面的新工作在哪裡，並且在找到之後去執行。 
在前面所提到的 Signal 的等待的 System call 那邊，因為程式如果沒有設定好說一定要
等待 Job executive已經執行過了 sigwait()的 Function之後，有可能會發生因為Queue Handler
跑得比較快，所以先送出了 Signal 但是對方的 Job executive 卻沒有執行到等待 Signal 的
System call 的部份，也因此 Job executive 後來執行到了 sigwait()的時候，不會因此就被 Wake 
up 起來，一樣要等到下次依照成功正確的順序執行的 Queue Handler 加入了新的工作資訊
的時候，Job executive 才有可能再次被 Wake up 起來。 
目前只支援 Cluster 架構，且 Scheduler 可與 Master nodes 做結合，以節省建置成本。同樣
的，Observe Server 與 Web Server 也可作結合。 
 
 
圖 7：系統硬體架構圖 
 
軟體架構由圖 8 中，主要可分為 4 方面的軟體，Slave daemon、Master daemon、Observe 
Server 上負責蒐集資訊的 daemon，最後則是 Web Server。 
 
圖 8：系統軟體架構圖 
Web Server 控制介面展示範例： 
 
採用 SCMS 風格的首頁 
 
點選一套 Cluster 可看到此 Cluster 圖形資訊 
 
再點選其中一部機器，可看到此機器的其他的圖形資訊 
系統資訊 
 
點選 Grid 中之一個 Cluster，系統狀態列會顯示其 Cluster 的系統資訊 
 
點選 Cluster 中之一個 Node，右邊會顯示其 Node 的詳細系統資訊 
 
點選右邊的 Tab Bar，可分別得知詳細的系統資訊 
 
 
Job 網頁控制介面 
各應用領域的學界專家利用到 PC Cluster 最大的潛力來進行叢集系統之研製與應用。 
 
※ 備註：精簡報告係可供國科會立即公開之資料，並以四至六頁為原則，如
有圖片或照片請以附加檔案上傳，若涉及智財權、技術移轉案及專
利申請之資料，請勿揭露。 
configurability. We present our effort to resolve this 
problem by developing a PC cluster monitoring system. 
This system also provides web service and application 
to monitor for large scale clusters or grid environment. 
We conduct our multi-cluster system that with NAT 
mechanism that shows in our testing parallel 
application it has good performance and in the 
additional evaluation we could find that the machine 
availability could reflect the performance on multi-
cluster environment. We also conduct our effort on the 
monitor system of cluster, we use the master-slave 
architecture to implement our monitor and make a 
special client for mobile equipments based on Java. 
We can easily monitor in everywhere even if we don’t 
have computer, we could also receive the cluster status 
on our mobile devices. 
2. Implementation of Multi-clusters with 
NAT Settings 
When we have many single clusters and wonder 
build them as a big and single computation resource 
with NAT, The master node of our single cluster are 
connected with inter- and intra-cluster networks. Key 
point is that how could we configure our master nodes 
of many of our single clusters and make them could 
communicate over the network. 
In general, a PC cluster is constructing by using 
Linux as their OS environment. The kernel version 2.4 
and latter provides a subsystem called Netfilter. It 
operates a packet filtering job in kernel stack.  We use 
this feature for IP-Masquerade, a solution of network 
address translation (NAT), in the setting, we just open 
the capability of packet forwarding module [12]. The 
reason is that we do not need to protect any type of 
attack form Internet; we just want to make the client 
that can communicate around the public network. IP-
Masquerade in Linux has several ways to be enabled. 
You should use the root account to perform the 
following commands. 
The building steps of multi-cluster in each cluster’s 
master node are list below. First, you can use 
“sysctl” command to enable kernel IP forward 
function like this command “sysctl -w 
net.ipv4.ip_forward=1”. Another way, you can 
change the “/proc/sys/net/ipv4/ip_forward”
value to 1. Then, you can use “iptables” command 
to change Netfilter trains, before this configuration you 
should load kernel module include ip_tables and 
iptable_nat through “modprobe” or “insmod”
command, after modules loaded. The command is like 
below: 
iptables -t nat -A POSTROUTING -o <eth> -
s <private_net> -j MASQUERADE
Under the command, the “eth” parameter is the 
mapped to your master node’s public address interface 
and the “private_net” parameter is your cluster’s 
private network address, if your private network is 
192.168.1.0 and its netmask is 255.255.255.0.
You should type “192.168.1.0/24”.
After enabling the NAT support for each cluster’s 
master node, if we want to combine these clusters 
together, we must build a static routing path that 
causes every node in this environment can pass 
messages to each other. We use a simple example to 
explain this concept. For example; we have two 2-
client clusters, named Cluster A and Cluster B. In their 
private network setting, Cluster A and Cluster B 
should configure different private network address like 
as 192.168.1.0 and 192.168.2.0 or other, 
respectively. After IP-Masquerade in Linux kernel, we 
can look each master node as a network router, in this 
concept, we don’t mind that private network couldn’t 
route between public networks because every cluster 
master node in our setting is in public network. Our 
configuration is concentrated on made the routing 
policy to each cluster master. The policy is simple, if 
we must combine N cluster together, each master must 
add (N-1) route trains to satisfy each cluster 
communications together. 
In this example for cluster A, we can add this 
command to add a routing train on this cluster master: 
route add –net 192.168.2.0 netmask 
255.255.255.0 gw <cluster B master IP >
For cluster B, we also add the command on cluster 
master: 
route add –net 192.168.1.0 netmask 
255.255.255.0 gw <cluster A master IP >
After this configuration and master node is set to use 
IP-Masquerade. We easily can combine the two Linux 
PC clusters for applying MPI parallel applications for 
obtain large computation power. 
3. Design and Implementation of 
Monitoring Tool 
The concept of our monitor system is to improve the 
availability of monitor system in the distributed 
computing environment [5, 15]. Nowadays, the 
monitor system is not well developed on user 
requirements. We started on the user interaction and 
the manner of application executing, and developed the 
applications by the portability of the Java Virtual 
Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the Grid Workshops (CCGRIDW'06) 
0-7695-2585-7/06 $20.00 © 2006 IEEE 
listed as below: 
z Slave Daemon: The Slave daemon can obtain the 
related system information of each slave node 
form Kernel, and provide the services to its 
Master node in the PC cluster, 
z Master Daemon: The Master Daemon is 
responsible to collect the system information 
from all slave nodes in its cluster, and put the 
related information into Local Database.  Local 
Database is used for the purpose that will not 
allow the high load of master node for an instant.  
The function of Local Database can be view as a 
buffer, 
z Collect Daemon: This daemon is running on the 
Observe Server. It is used for collecting the 
system information of each master node of 
multiple Linux PC clusters.  It can provide 
services by using database to applications or the 
remote users. 
All server daemons are written in C and the web 
portal interface is written in PHP, We could get some 
information through our web portal and they are 
shown from Figures 4 to 7. For the user without 
computer, we have also implemented a JAVA-based 
PDA version of our monitor shown in Figure 8. 
Figure 4: Home page created with PHP and RRDTool 
Figure 5: Platform information page 
Figure 6: The overall system information webpage of 
one cluster platform 
Figure 7: The detail system information webpage 
Figure 8: Monitoring screen on PDA 
4. Experimental Results 
Table 1 shows the hardware specification of our 
testing environment. The network topology is shown 
as in Figure 9. Our inter-cluster connection are build 
on the Fast Ethernet that giving the speed of 100 Mb/s. 
The configuration steps of multi-cluster are listed 
below. 
z First, get all information of public and private IP 
addresses of these four cluster’s master on 
executing “ifconfig” command, 
Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the Grid Workshops (CCGRIDW'06) 
0-7695-2585-7/06 $20.00 © 2006 IEEE 
  c[i][k]=0.0; 
  for(j=0; j<P; j++) 
   c[i][k]+=a[i][j]*b[j][k]; 
 } 
Its algorithm requires n3 multiplications and n3
additions, leading to a sequential time complexity of 
O(n3). For matrix multiplication, the smallest sensible 
unit of work is the computation of one element in the 
result matrix. It is possible to divide the work into even 
smaller chunks, but any finer division would not be 
beneficial because of the number of processor is not 
enough to process, i.e., n2 processors are needed. 
In the Figures 10, 11, and 12, we could find out the 
amd64-dual01 leads the performance in these four 
single clusters because of its new architecture and the 
optimization for the x86_64 architecture in new kernel 
2.6. The amd-dual1 cluster has the worst performance 
because of its network speed is only 100Mbps speed 
and the CPU clock is the slowest of these four single 
clusters. We can also check the performance of amd-
dual1 and mc8. It shows that when the matrix size 
increased the performance of both cases is lower than 
other cases in the 100Mbps network. 
In the performance evaluation results in Figure 10 
shows the performance benefit in the multi-cluster 
system. First the performances of the mc8, mc16 and 
mc32 clusters are between the performance of amd1 
and amd64-dual01 these two clusters.  When the 
matrix size is not very large, the communication 
overhead occur the execution time of mc32 and mc8 
are very closely. But we can get the better performance 
in the mc16 cluster. 
When the matrix size increased as in Figures 11 and 
12, it shows if we increase our nodes to 16 to execute 
the matrix multiplication program, we could get the 
best performance changes. The case of the matrix size 
increasing to 2048, the performances of single clusters 
are not the same in the Figures 10 and 11, but multi-
cluster also shows the better performance in the mc16 
and mc32 settings.  Though, the case of mc8 shows the 
worst performance because the network speed of the 
mc8 cluster is the Fast Ethernet and in normal LAN 
environment communications that is like the case of 
amd-dual1. 
When we compare our multi-cluster settings with 
single cluster settings with matrix multiplication, our 
program must send the divided matrix as a multiplier 
and the full matrix as multiplicand with traditional 
block send and receive functions. We could find that 
when the matrix size is 512 in Figure 13, sc32 could 
not have the poor performance than the mc32 because 
the communication over the single cluster is running 
through a single switch that has bigger intra-
communication bandwidth. The results in larger matrix 
size are more significant when we have strong intra-
communication bandwidth in Figures 11 and 12. In 
this type of parallel processing type that has large 
message passing over the computation nodes, the 
multi-cluster only has little advantage even has bed 
performance. 
In the experiment on starting node of the MPI job. 
As shown in Figure 13, we respectively conduct the 
relation in the execution time and the availability of the 
MPI job start location. We can get the better 
performance when we choose the amd64-dual01 as the 
node for job starting than other nodes, the amd1 is the 
second fast, the amd-dual01 is the third and the last is 
the amd-dual1. It could be compared with the 
performance of single cluster in Figures 10, 11 and 12. 
We can find out that if we choose node that gets better 
performance in single cluster case to starting the MPI 
job, we can get better performance when we combine 
our single cluster to multi-cluster. 
Matrix Multiplication
0
500
1,000
1,500
2,000
2,500
3,000
3,500
Matrix size = 512
P
ro
ce
ss
in
g 
tim
e 
(m
s)
amd-dual1
amd-dual01
amd1
amd64-dual01
mc8
mc16
mc32
sc16
sc32
Figure 10: Comparison of matrix multiplication when 
the matrix size is equal to 512. 
Matrix Multiplication
0
5,000
10,000
15,000
20,000
25,000
Matrix size = 1024
P
ro
ce
ss
in
g 
tim
e 
(m
s)
amd-dual1
amd-dual01
amd1
amd64-dual01
mc8
mc16
mc32
sc16
sc32
Figure 11: Comparison of matrix multiplication when 
the matrix size is equal to 1024. 
Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the Grid Workshops (CCGRIDW'06) 
0-7695-2585-7/06 $20.00 © 2006 IEEE 
