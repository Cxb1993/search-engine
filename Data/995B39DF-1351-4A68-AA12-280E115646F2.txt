 1
行政院國家科學委員會專題研究計畫成果報告 
 
平行資料程式於計算網格上通訊與 I/O 局部化研究與
應用工具開發(3/3) 
Design and Implementation of Communication and I/O Localization 
Tools for Parallel Applications on Computational Grids (3/3) 
 
計畫編號：NSC95-2221-E-216-006 
執行期限：96 年 8 月 1 日至 97 年 7 月 31 日 
主持人：許慶賢   中華大學資訊工程學系副教授 
 
計畫參與人員：中華大學資訊工程學系研究生 
陳泰龍(博二)、張智鈞(研二)、郁家豪(研二)ヽ蔡秉儒(研二) 
 
 
一、中文摘要 
 
本報告是有關於在異質性計算網格系
統和網路拓樸下開發適應型的評估模組
與通訊局部化的技術之描述。本計畫執行
三年，我們完成自動資料分割工具、平行
資料程式效能預測工具、資料局部化選擇
器、以及針對特殊平行應用程式的資料局
部化學習系統。本項研究所發展的通訊局部
化技術與分析工具有助於提升平行資料程式
在計算網格上的執行效能。執行本計畫所得到
的研究理論、工具開發、與實務經驗亦可作為
相關領域學術研究與教學的素材。 
 
關鍵詞：通訊區域化、平行資料程式、計算網
格、平行 I/O、資料配置、通訊排程、效能預
測、平行編譯器、平行應用、SPMD。 
 
Abstract 
 
This report presents adaptive performance 
models for optimizing communications of real 
world parallel applications on heterogeneous 
grid systems and topologies. This project 
developed tools for automatic data partitioning, 
performance prediction of data parallel programs, 
web-based locality selector and learning systems 
for scientific applications. The integrated 
locality preserving techniques and analysis tools 
developed in this project will facilitate 
development of efficient data parallel 
applications on computational grids.  The 
achievements of theorems, tools and experience 
in this project can be applied in both academic 
teaching and research.  It is the main objective 
of this project. 
 
Keywords: Localized Communication, Data 
Parallel Program, Computational Grid, Parallel 
I/O, Data Distribution, Communication 
Scheduling, Performance Prediction, 
Parallelizing Compiler, Parallel Applications, 
SPMD. 
 
二、緣由與目的 
 
  整合計算資源的觀念使得網格計算成為
廣泛被接受的虛擬高效能計算平台。網格
(Grid Computing)計算系統不同於傳統平行電
腦，它連接分散於不同網域的電腦組成一個具
有高度擴充性的計算平台。叢集式的網格
(Cluster Grid)即是一個典型的系統。對於平行
資料程式(Data Parallel Program)而言，程式執
行的過程中有可能發生資料的切割、資料的交
換，這種情況，在網格系統中，節點之間的通
 3
 P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 I E 
P0 a1 a2 a3          3 0 
P3    b1 b2 b3       3 0 
P6       c1 c2 c3    3 0 
P9          d1 d2 d3 3 0 
P1 e1 e2 e3          3 0 
P4    f1 f2 f3       3 0 
P7       g1 g2 g3    3 0 
P10          h1 h2 h3 3 0 
P2 i1 i2 i3          3 0 
P5    j1 j2 j3       3 0 
P8       k1 k2 k3    3 0 
P11          l1 l2 l3 3 0 
 Cluster-1 Cluster-2 Cluster-3 36 0  
圖三、重新排序處理器 ID 後的資料通訊示意
圖。 
 
    針對每組電腦叢集提供不同數量的
處理器之問題，可利用此做法，提高資料
傳輸效能。 
 為了適用於多維度的處理器編排系
統 ， 我 們 也 提 出 多 維 陣 列
(Multi-Dimensional Array)資料對應模組，
希望可以動態調整通訊的瓶頸，提升程式
的執行效益。圖四是處理器跟資料通訊的
關係，(a)是 2-D 處理器編排系統，可視為
多維系統的表示圖，每個 P 皆視為一個處
理器，其所佔面積等同於二維陣列中所分
配的資料範圍；(b)為資料重新分配時的需
產生資料(m00~m22)示意圖，虛線表示二維
陣列新的分配方式。 
    
(a) 
 
(b) 
圖四、處理器與資料通訊的關係。(a)多維處理器
示意圖；(b)資料通訊示意圖 
 
 為了達到資料配置的要求，處理器經
常移動資料，而花費的通訊成本過高時會
影響執行效能。為此，我們提出了 Local 
Message Reduction Optimization，改善資料
重新配置之排程演算法，並建立效能對照
表。重新計算了每筆通訊的權重，評估並
對排程了所有的通訊，除了可以有效降低
通訊成本，更能避免資料傳輸所產生的衝
突。 
 
四、結論與討論 
 
下面我們歸納本計畫主要的成果: 
z 完成自動資料分割模組的開發 
z 完成平行資料分割效能預測系統的實
作。 
z 提出重新排程與資料重新分配的技術 
z 實作程式階層的效能預測、與其效能監
督工具所提供的資訊，進行程式判別。 
z 發表三篇國際期刊與五篇國際研討會
論文 
Journal Papers: 
 
 Ching-Hsien Hsu, Min-Hao Chen, 
Chao-Tung Yang and Kuan-Ching Li, 
“Optimizing Communications of Dynamic 
Data Redistribution on Symmetrical 
Matrices in Parallelizing Compilers,” 
IEEE Transactions on Parallel and 
Distributed Systems, Vol. 17, No. 11, pp. 
1226-1241, Nov. 2006. (SCI, EI) 
 Ching-Hsien Hsu, Tai-Lung Chen and 
Kuan-Ching Li, "Performance Effective 
Pre-scheduling Strategy for Heterogeneous 
Communication Grid Systems," Future 
Generation Computer Science, Vol. 23, 
Issue 4, pp. 569-579, May 2007. Elsevier 
(SCI, EI) 
 Ching-Hsien Hsu, Shih-Chang Chen and 
Chao-Yang Lan, "Scheduling 
Contention-Free Irregular Redistribution in 
Parallelizing Compilers," The Journal of 
Supercomputing, Kluwer Academic 
Publisher, Vol. 40, No. 3, pp. 229-247, 
June 2007. (SCI, EI) 
 Ching-Hsien Hsu, Tai-Lung Chen and 
Jong-Hyuk Park, “On improving resource 
utilization and system throughput of 
master slave jobs scheduling in 
heterogeneous systems,” Journal of 
Supercomputing, Springer, Vol. 45, No. 1, 
pp. 129-150, July 2008. (SCI, EI). 
 
Conference Papers: 
 
 Ching-Hsien Hsu, Justin Zhan, Wai-Chi 
Fang and Jianhua Ma, “Towards 
Improving QoS-Guided Scheduling in 
 5
Pre-scheduling Strategy for Heterogeneous 
Communication Grid Systems,” Accepted, 
Future Generation Computer Science, Elsevier, 
2007.  (SCI, EI, NSC93-2213-E-216-029) 
[12] Ching-Hsien Hsu, Chih-Wei Hsieh and 
Chao-Tung Yang, “ A Generalized Critical 
Task Anticipation Technique for DAG 
Scheduling,＂ Algorithm and Architecture for 
Parallel Processing - Lecture Notes in Computer 
Science, Springer-Verlag, June 2007. 
(ICA3PP＇07) 
[13] Ching-Hsien Hsu, Chao-Yang Lan and 
Shih-Chang Chen, “ Optimizing Scheduling 
Stability for Runtime Data Alignment, ＂ 
Embedded System Optimization - Lecture Notes 
in Computer Science, Vol. 4097, pp. 825-835, 
Springer-Verlag, Aug. 2006. (ESO＇06) (SCI 
Expanded, NSC92-2213-E-216-029) 
[14] Ching-Hsien Hsu, Guan-Hao Lin, Kuan-Ching 
Li and Chao-Tung Yang, “ Localization 
Techniques for Cluster-Based Data Grid, ＂ 
Algorithm and Architecture for Parallel 
Processing - Lecture Notes in Computer Science, 
Vol. 3719, pp. 83-92, Springer-Verlag, Oct. 
2005. (ICA3PP’05) (SCI Expanded, NSC 
93-2213-E-216-029) 
[15] Ching-Hsien Hsu, Ming-Yuan Own and 
Kuan-Ching Li, “Critical-Task Anticipation 
Scheduling Algorithm for Heterogeneous and 
Grid Computing,” Computer Systems 
Architecture - Lecture Notes in Computer 
Science, Vol. 4186, pp. 97-110, Springer-Verlag, 
Sept. 2006. (ACSAC’06) (SCI Expanded, 
NSC92-2213-E-216-029) 
[16] D.H. Kim, K.W. Kang, “Design and 
Implementation of Integrated Information System 
for Monitoring Resources in Grid Computing,” 
Computer Supported Cooperative Work in Design, 
10th Conf., pp. 1-6, 2006. 
[17] K.C. Li, Ching-Hsien Hsu, H.H. Wang and C.T. 
Yang, “Towards the Development of Visuel: a 
Novel Application and System Performance 
Monitoring Toolkit for Cluster and Grid 
Environments”, Accepted, International Journal 
of High Performance Computing and 
Networking (IJHPCN), Inderscience Publishers, 
2008. 
[18] Emmanuel Jeannot and Frédéric Wagner, “Two 
Fast and Efficient Message Scheduling 
Algorithms for Data Redistribution through a 
Backbone,” Proceedings of the 18th 
International Parallel and Distributed Processing 
Symposium, April 2004. 
[19] C. Lee, R. Wolski, I. Foster, C. Kesselman 
and J. Stepanek, “A Network Performance 
Tool for Grid Computations,” 
Supercomputing '99, 1999. 
[20] K.C. Li, H.H. Wang, C.T. Yang and 
Ching-Hsien Hsu, “Towards the Development 
of Visuel: a Novel Application and System 
Performance Monitoring Toolkit for Cluster and 
Grid Environments,” Accepted, International 
Journal of High Performance Computing and 
Networking (IJHPCN), Inderscience Publishers, 
2007. 
[21] J.M. Schopf and S. Vazhkudai, “Predicting 
Sporadic Grid Data Transfers,” 11th IEEE 
International Symposium on 
High-Performance Distributed Computing 
(HPDC-11), IEEE Press, Edinburg, Scotland, 
July 2002. 
[22] A. Smyk, M. Tudruj, L. Masko, “Open MP 
Extension for Multithreaded Computing with 
Dynamic SMP Processor Clusters with 
Communication on the Fly,” PAR ELEC, pp. 
83-88, 2006. 
[23] H. Stockinger, A. Samar, B. Allcock, I. 
Foster, K. Holtman and B. Tierney, “File and 
Object Replication in Data Grids,” Journal of 
Cluster Computing, 5(3)305-314, 2002. 
[24] M. Tudruj and L. Masko, “Fast Matrix 
Multiplication in Dynamic SMP Clusters with 
Communication on the Fly in Systems on Chip 
Technology,” PAR ELEC, pp. 77-82, 2006 
[25] S. Vazhkudai and J. Schopf, “Using Disk 
Throughput Data in Predictions of 
End-to-End Grid Transfers,” Proceedings of 
the 3rd International Workshop on Grid 
Computing (GRID 2002), Baltimore, MD, 
November 2002. 
[26] Chun-Ching Wang, Shih-Chang Chen, 
Ching-Hsien Hsu and Chao-Tung Yang, 
“Optimizing Communications of Data 
Parallel Programs in Scalable Cluster 
Systems,” Proceedings of the 3rd 
International Conference on Grid and 
Pervasive Computing (GPC-08), LNCS 5036, 
pp. 29-37, May 2008 
[27] C.T. Yang, I-Hsien Yang, Shih-Yu Wang, 
Ching-Hsien Hsu and Kuan-Ching Li, “A 
Recursive-Adjustment Co-Allocation Scheme 
with Cyber-Transformer in Data Grids,” 
Accepted, Future Generation Computer 
Science, Elsevier, 2008. 
[28] Kun-Ming Yu, Ching-Hsien Hsu and 
Chwani-Lii Sune, “A Genetic-Fuzzy Logic 
Based Load Balancing Algorithm in 
Heterogeneous Distributed Systems,” 
Proceedings of the IASTED International 
Conference on Neural Network and 
Computational Intelligence (NCI 2004), Feb. 
2004, Grindelwald, Switzerland. 
 7
 
三、 考察參觀活動(無是項活動者省略) 
 
四、 建議 
 
    看了眾多研究成果以及聽了數篇專題演講，最後，本人認為，會議所安排的會場以及邀請的講席
等，都相當的不錯，覺得會議舉辦得很成功，值得我們學習。 
 
五、 攜回資料名稱及內容 
 
1. Conference Program 
2. Proceedings 
 
 2
where we also present a motivating example to demonstrate the characteristics of the master-slave pre-scheduling 
model. Section 4 assesses the new scheduling algorithm, the Smallest Communication Ratio (SCR), while the 
illustration of SCR on heterogeneous communication is examined in section 5. The performance comparisons and 
simulations results are discussed in section 6, and finally in section 7, some conclusions of this paper. 
2   Related Work 
The task scheduling research on heterogeneous processors can be classified into DAGs model, master-slave paradigm 
and computational grids. The main purpose of task scheduling is to achieve high performance computing and high 
throughput computing. The former aims at increasing execution efficiency and minimizing the execution time of tasks, 
whereas the latter aims at decreasing processor idle time and scheduling a set of independent tasks to increase the 
processing capacity of the systems over a long period of time. 
Thanalapati et al. [13] brought up the idea about adaptive scheduling scheme based on homogeneous processor 
platform, which applies space-sharing and time-sharing to schedule tasks. With the emergence of Grid and ubiquitous 
computing, new algorithms are in demand to address new concerns arising to grid environments, such as security, 
quality of service and high system throughput.  Berman et al. [6] and Cooper et al. [11] addressed the problem of 
scheduling incoming applications to available computation resources. Dynamically rescheduling mechanism was 
introduced to adaptive computing on the Grid.  In [8], some simple heuristics for dynamic matching and scheduling of 
a class of independent tasks onto a heterogeneous computing system have been presented.  Moreover, an extended 
suffrage heuristic was presented in [12] for scheduling the parameter sweep applications that have been implemented 
in AppLeS. They also presented a method to predict the computation time for a task/host pair by using previous host 
performance. 
Chronopoulos et al. [9], Charcranoon et al. [10] and Beaumont et al. [4, 5] introduced the research of master-slave 
paradigm with heterogeneous processors background. Based on this architecture, Beaumont et al. [1, 2] presented a 
method on master-slave paradigm to forecast the amount of tasks each processor needs to receive in a given period of 
time. Beaumont et al. [3] presented the pipelining broadcast method on master-slave platforms, focusing on message 
passing disregarding computation time. Intuitionally in their implementation, fast processor receives more tasks in the 
proportional distribution policy. Tasks are also prior allocated to faster slave processors and expected higher system 
throughput could be obtained. 
3   Preliminaries 
In this section, we first introduce basic concepts and models of this investigation, where we also define notations 
and terminologies that will be used in subsequent subsections. 
3.1   Research Architecture 
We have revised several characteristics that were introduced by Beaumont et al. [1, 2]. Based on the master 
slave paradigm introduced in section 1, this paper follows next assumptions as listed. 
z Heterogeneous processors: all processors have different computation speed. 
z Identical tasks: all tasks are of equal size. 
z Non-preemption: tasks are considered to be atomic. 
z Exclusive communication: communications from master node to different slave processors can not be 
overlapped.  
z Heterogeneous communication: communication costs between master and slave processors are of different 
overheads. 
 
 
 
3.2   Definitions 
 4
 
Fig. 1. Most Jobs First (MJF) task scheduling when 1≤δ . 
 
Lemma 1: Given a master slave system with δ  > 1, in MJF scheduling, the amount of tasks being assigned to 
Pmax+1 can be calculated by the following equation, 
task(Pmax+1) = (BSC − ∑
=
max
1
)(
i
iPcomm ) / Tmax+1_com                                             (1) 
Lemma 2: Given a master slave system with δ  > 1, in MJF scheduling, the period of processor Pmax+1 stays idle 
denoted by MJFidleT  and can be calculated by the following equation,  
MJF
idleT  = BSC − )()( 1max1max ++ − PcompPcomm                                (2) 
Another example of master slave task scheduling with identical communication (i.e., Ti_comm=1) and δ  > 1 is 
given in Fig. 2. Because δ  > 1, according to equation (1), we have task(Pmax+1=P4) = 10. We note that P4 
completes its tasks and becomes available at time 100. However, the master processor dispatches tasks to P3 
during time 100 ~ 110 and starts to send tasks to P4 at time 110. Such kind of idle situation also happens at time 
100~110, 160~170, 220~230, and so on. 
 
Fig. 2. Most Jobs First (MJF) Tasking when 1>δ . 
Lemma 3: In MJF scheduling algorithm with identical communication Ti_comm, when δ  > 1, the completion time of 
tasks in the jth BSC can be calculated by the following equation. 
T(BSCj) =∑
=
max
1
)(
i
iPcomm + ))()(( 1max1max MJFidleTPcompPcommj ++× ++  MJFidleT−                       (3) 
 
 6
 
Fig. 3. Smallest Communication Ratio (SCR) Tasking when 1>δ . 
5   Generalized Smallest Communication Ratio (SCR) 
As computational grid integrates geographically distributed computing resources, the communication overheads 
from resource broker / master computer to different computing site are different. Therefore, towards an efficient 
scheduling algorithm, the heterogeneous communication overheads should be considered. In this section, we 
present the SCR task scheduling techniques work on master slave computing paradigm with heterogeneous 
communication. 
 
Lemma 8: Given a master slave system with heterogeneous communication and δ  > 1, in MJF scheduling, we 
have 
task(Pmax+1) = 
⎥⎥
⎥⎥
⎥
⎦
⎥
⎢⎢
⎢⎢
⎢
⎣
⎢ −
+
=
∑
comm
i
i
T
PcommBSC
_1max
max
1
)(
                                     (8) 
Lemma 9: Given an SCR scheduling with heterogeneous communication and δ > 1, SCRidleT  is the idle time of one 
slave processor, we have the following equation,  
SCR
idleT  = ∑+
=
1max
1
)(
i
iPcomm  − BSC.                                   (9) 
Lemma 10: Given an SCR scheduling with heterogeneous communication and δ > 1, )( jSCRstart BSCT  is the start 
time to dispatch tasks in the jth BSC, we have the following equation,  
)()1()( SCRidlej
SCR
start TBSCjBSCT +×−=                                    (10) 
Lemma 11: Given an SCR scheduling with heterogeneous communication and δ > 1, the task completion time of 
the jth BSC denoted by )( jSCRfinish BSCT , we have 
)( j
SCR
finish BSCT = ∑+
=
1max
1
)(
i
iPcomm +comp(Pk)+ ))()(()1( SCRidlekk TPcompPcommj ++×−         (11) 
 8
 
The MJF scheduling is depicted in Fig. 4(b). According to corollary 5, task(Pmax+1) = task(P4) = 0,  therefore, 
P4 will not be included in the scheduling.  MJF has the task distribution order P1, P2, P3. Another scheduling 
policy is called Longest Communication Ratio (LCR) which is an opposite approach to the SCR method. Fig. 4(c) 
shows the LCR scheduling result which has the dispatch order P1, P2, P4, P3. 
To investigate the performance of SCR scheduling technique, we observe that MJF algorithm completes 16 tasks 
in 90 units of time in the first BSC. On the other hand, in SCR scheduling, there are 19 tasks completed in 73 
units of time in the first BSC. In LCR, there are 19 tasks completed in 99 units of time. We can see that the 
system throughput of SCR (19/73≈0.260) > LCR (19/99≈0.192) > MJF (16/90≈0.178). Moreover, the average 
turnaround time of the SCR algorithm in the first three BSCs is 183/57 (≈3.2105) which is less than the LCR‘s 
average turnaround time 209/57 (≈3.6666) and the MJF‘s average turnaround time 186/48 (≈3.875). 
6   Performance Evaluation 
To evaluate the performance of the proposed method, we have implemented the SCR and the MJF algorithms. We 
compare different criteria, such as average turnaround time, system throughput and processor idle time, in 
Heterogeneous Processors with Heterogeneous Communications (HPHC). 
Simulation experiments for evaluating average turnaround time are made upon different number of 
processors and show in Fig. 7. The computational speed of slave processors is set as T1=3, T2=3, T3=5, T4=7, T5=11, 
and T6=13. For the cases when processor number is 2, 3… 6, we have 1≤δ . When processor number increases to 7, 
we have 1>δ . In either case, the SCR algorithm conduces better average turnaround time. From the above results, 
we conclude that the SCR algorithm outperforms MJF for most test samples. 
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1 2 3 4 5 6
# of nodes
Av
era
ge
 tu
rn
-ar
ou
nd
tim
e
MJF
SCR
 
Fig. 5. Average task turn-around time on different numbers of processors. 
Simulation results present the performance comparison of three task scheduling algorithms, SCR, MJF, LCR, 
on heterogeneous processors and heterogeneous communication paradigms. Fig. 6 shows the simulation results 
for the experiment setting that with ±10 processor speed variation and ±4 communication speed variation. The 
computation speed of slave processors are 1T =3, 2T =6, 3T =11, and 4T =13. The time of a slave processor to 
receive one task from master processor are commT _1  = 5, commT _2  = 2, commT _3  = 1 and commT _4 =3. The average task 
turnaround time, system throughput and processor idle time are measured.   
 
 10
(a) 
0
0.05
0.1
0.15
0.2
0.25
5 10 15 20 25# of Nodes
Th
rou
gh
pu
t
MJF
LCR
SCR
 
(b) 
Fig. 7. Simulation results of throughput for the range of 5~25 processors with ±30 computation speed variation and 
±30 communication variation in 100 cases and 100 BSC (a) system throughput of the cases when 0< iT ≤ 30 and 
0< commiT _ ≤ 5 (b) system throughput of the cases when 0< iT ≤ 5 and 0< commiT _ ≤ 30. 
Fig. 7(a) is the case of 0< iT ≤ 30, 0< commiT _ ≤ 5 and the parameter of computation speed and communication 
speed are to be random and uniformly distributed within different number of nodes and 100 BSC for 100 cases. 
Fig. 7(b) is the case of 0< iT ≤ 5 and 0< commiT _ ≤ 30. The SCR algorithm performs better than MJF method, and 
SCR method has higher throughput than the MJF algorithm as shown in Fig. 7(a) and Fig. 7(b).  From the 
above experimental tests, we have the following remarks. The proposed SCR scheduling technique has better 
task turnaround time and higher system throughput than the MJF algorithm. 
From the above experimental tests, we have the following remarks. 
z The proposed SCR scheduling technique has higher system throughput than the MJF algorithm. 
z The proposed SCR scheduling technique has better task turnaround time than the MJF algorithm. 
The SCR scheduling technique has less processor idle time than the MJF algorithm. 
7   Conclusions 
The problem of resource management and scheduling has been one of main challenges in grid computing. In this paper, 
we have presented an efficient algorithm, SCR for heterogeneous processors tasking problem. One significant 
improvement of our approach is that average turnaround time could be minimized by selecting processor has the 
smallest communication ratio first. The other advantage of the proposed method is that system throughput can be 
increased via dispersing processor idle time. Our preliminary analysis and simulation results indicate that the SCR 
algorithm outperforms Beaumont’s method in terms of lower average turnaround time, higher average throughput, less 
processor idle time and higher processors’ utilization. 
There are numbers of research issues that remains in this paper. Our proposed model can be applied to map tasks 
onto heterogeneous cluster systems in grid environments, in which the communication costs are various from clusters. 
In future, we intend to devote generalized tasking mechanisms for computational grid.  We will study realistic 
applications and analyze their performance on grid system.  Besides, rescheduling of processors / tasks for minimizing 
processor idle time on heterogeneous systems is also interesting and will be investigated. 
References 
1. O. Beaumont, A. Legrand and Y. Robert, “The Master-Slave Paradigm with Heterogeneous Processors,” IEEE Trans. on parallel 
and distributed systems, Vol. 14, No.9, pp. 897-908, September 2003. 
2. C. Banino, O. Beaumont, L. Carter, J. Ferrante, A. Legrand and Y. Robert, ”Scheduling Strategies for Master-Slave Tasking on 
Heterogeneous Processor Platforms,” IEEE Trans. on parallel and distributed systems, Vol. 15, No.4, pp.319-330, April 2004. 
3. O. Beaumont, A. Legrand and Y. Robert, “Pipelining Broadcasts on Heterogeneous Platforms,” IEEE Trans. on parallel and 
distributed systems, Vol. 16, No.4, pp. 300-313 April 2005. 
4. O. Beaumont, V. Boudet, A. Petitet, F. Rastello and Y. Robert, “A Proposal for a Heterogeneous Cluster ScaLAPACK (Dense 
Linear Solvers),” IEEE Trans. Computers, Vol. 50, No. 10, pp. 1052-1070, Oct. 2001. 
 12
 
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 96 年  6 月  20  日   
姓 名 許慶賢 服 務 機 關 名 稱
 
中華大學
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
2007 International Conference on Algorithms and Architecture for Parallel 
Processing, June 11 -14 2007. 
到 達 國 家 
及 地 點 
Hangzhou, China 出 國
期 間
自 96 年 06 月 11 日
迄 96 年 06 月 19 日
內 容 提 要 
這一次在杭州所舉行的國際學術研討會議共計四天。第一天下午本人抵達會
場辦理報到。第二天各主持一場 invited session 的論文發表。同時，自己也
在上午的場次發表了這依次被大會接受的論文。第一天也聽取了 Dr. 
Byeongho Kang 有關於 Web Information Management 精闢的演說。第二天許
多重要的研究成果分為六個平行的場次進行論文發表。本人選擇了
Architecture and Infrastructure、Grid computing、以及 P2P computing 相關場
次聽取報告。晚上本人亦參加酒會，並且與幾位國外學者及中國、香港教授
交換意見，合影留念。第三天本人在上午聽取了 Data and Information 
Management 相關研究，同時獲悉許多新興起的研究主題，並了解目前國外
大多數學者主要的研究方向，並且把握最後一天的機會與國外的教授認識，
希望能夠讓他們加深對台灣研究的印象。三天下來，本人聽了許多優秀的論
文發表。這些研究所涵蓋的主題包含有：網格系統技術、工作排程、網格計
算、網格資料庫以及無線網路等等熱門的研究課題。此次的國際學術研討會
議有許多知名學者的參與，讓每一位參加這個會議的人士都能夠得到國際上
最新的技術與資訊。是一次非常成功的學術研討會。參加本次的國際學術研
討會議，感受良多。讓本人見識到許多國際知名的研究學者以及專業人才，
得以與之交流。讓本人與其他教授面對面暢談所學領域的種種問題。看了眾
多研究成果以及聽了數篇專題演講，最後，本人認為，會議所安排的會場以
及邀請的講席等，都相當的不錯，覺得會議舉辦得很成功，值得我們學習。
出 席 人 所 屬 機 
關 審 核 意 見 
 
層 轉 機 關 
審 核 意 見 
 
研 考 會 
處 理 意 見 
 
 14
approach, information of applications, such as tasks execution time, message size of 
communications among tasks, and tasks dependences are known a priori at 
compile-time; tasks are assigned to processors accordingly in order to minimize the 
entire application completion time and satisfy the precedence of tasks.  Hybrid 
scheduling techniques are mix of dynamic and static methods, where some 
preprocessing is done statically to guide the dynamic scheduler [8]. 
A Direct Acyclic Graph (DAG) [2] is usually used for modeling parallel 
applications that consists a number of tasks.  The nodes of DAG correspond to tasks 
and the edges of which indicate the precedence constraints between tasks.  In 
addition, the weight of an edge represents communication cost between tasks.  Each 
node is given a computation cost to be performed on a processor and is represented by 
a computation costs matrix.  Figure 1 shows an example of the model of DAG 
scheduling.  In Figure 1(a), it is assumed that task nj is a successor (predecessor) of 
task ni if there exists an edge from ni to nj (from nj to ni) in the graph.  Upon task 
precedence constraint, only if the predecessor ni completes its execution and then its 
successor nj receives the messages from ni, the successor nj can start its execution.  
Figure 1(b) demonstrates different computation costs of task that performed on 
heterogeneous processors.  It is also assumed that tasks can be executed only on 
single processor with non-preemptable style.  A simple fully connected processor 
network with asymmetrical data transfer rate is shown in Figures 1(c) and 1(d). 
 
            
 P1 P2 P3 iw  
n1 14 19 9 14 
n2 13 19 18 16.7 
n3 11 17 15 14.3 
n4 13 8 18 13 
n5 12 13 10 11.7 
n6 12 19 13 14.7 
n7 7 16 11 11 
n8 5 11 14 10 
n9 18 12 20 16.7 
n10 17 20 11 16  
(a)                                   (b) 
                  
 (c)                                  (d) 
Figure 1: An example of DAG scheduling problem (a) Directed Acyclic Graph (DAG-1) (b) 
computation cost matrix (W) (c) processor topology (d) communication weight. 
 
The scheduling problem has been widely studied in heterogeneous systems where 
 16
estimated computation time of processor Pj to execute task ni.  The mean execution time 
of task ni can be calculated by the following equation: 
∑== Pj jii Pww 1 ,                         (1) 
Example of the mean execution time can be referred to Figure 1(b).   
 
For communication part, a P × P matrix T is structured to represent different 
data transfer rate among processors (Figure 1(d) demonstrates the example).  The 
communication cost of transferring data from task ni (execute on processor px) to task 
nj (execute on processor py) is denoted by ci,j and can be calculated by the following 
equation, 
yxjimji tMsgVc ,,, ×+= ,                    (2) 
Where: 
Vm is the communication latency of processor Pm, 
Msgi,j is the size of message from task ni to task nj, 
tx,y is data transfer rate from processor px to processor py, 1≤ x, y ≤P. 
 
In static DAG scheduling problem, it was usually to consider processors’ 
latency together with its data transfer rate.  Therefore, equation (2) can be 
simplified as follows, 
yxjiji tMsgc ,,, ×= ,                     (3) 
Given an application represented by Directed Acyclic Graph (DAG), G = (V, E), 
where V = {nj: j = 1: v} is the set of nodes and v = |V|; E = {ei,j = <ni, nj>} is the set 
of communication edges and e =|E|.  In this model, each node indicates least 
indivisible task.  Namely, each node must be executed on a processor from the start 
to its completion.  Edge <ni, nj> denotes precedence of tasks ni and nj.  In other 
words, task ni is the immediate predecessor of task nj and task nj is the immediate 
successor of task ni.  Such precedence represents that task nj can be start for 
execution only upon the completion of task ni.  Meanwhile, task nj should receive 
essential message from ni for its execution.  Weight of edge <ni, nj > indicates the 
average communication cost between ni and nj. 
Node without any inward edge is called entry node, denoted by nentry; while node 
without any outward edge is called exit node, denoted by nexit.  In general, it is supposed 
that the application has only one entry node and one exit node.  If the actual application 
claims more than one entry (exit) node, we can insert a dummy entry (exit) node with 
zero-cost edge. 
 
3. Preliminaries 
This study concentrates on list scheduling approaches in DAG model.  List 
scheduling was usually distinguished into list phase and processor selection phase.  
Therefore, priori to discuss the main content, we first define some notations and 
terminologies used in both phases in this section. 
3.1 Parameters for List Phase 
 18
Pk.  ),( kj PnFT  is defined as follows, 
kjkjkj wPnSTPnFT ,),(),( +=                     (8) 
Definition 4: Given a DAG scheduling system on G = (V, E), the earliest finish time of 
task nj denoted by )( jnEFT , is formulated as follows,  
)},({)( kjPpj PnFTMinnEFT k∈
=                      (9) 
Definition 5: Based on the determination of )( jnEFT  in equation (9), if the earliest finish 
time of task nj is obtained upon task nj executed on processor pt, then the target processor of 
task nj is denoted by TP(nj), and TP(nj) = pt. 
 
4. The Generalized Critical-task Anticipation Scheduling Algorithm 
Our approach takes advantages of list scheduling in lower algorithmic complexity and 
superior scheduling performance and furthermore came up with a novel heuristic 
algorithm, the generalized critical task anticipation (GCA) scheduling algorithm to 
improve the schedule length as well as speedup of applications.  The proposed 
scheduling algorithm will be verified beneficial for the readers while we delineate a 
sequence of the algorithm and show some example scenarios in three phases, 
prioritizing phase, listing phase and processor selection phase.  
In prioritizing phase, the CS(ni) is known as the maximal summation of scores 
including the average computation cost and communication cost from task ni to the 
exit task.  Therefore, the magnitude of the task’s critical score is regarded as the 
decisive factor when determining the priority of a task.  In listing phase, an ordered 
list of tasks should be determined for the subsequent phase of processor selection. The 
proposed GCA scheduling technique arranges tasks into a list L, not only according to 
critical scores but also considers tasks’ importance.  
Several observations bring the idea of GCA scheduling method.  Because of 
processor heterogeneity, there exist variations in execution cost from processor to 
processor for same task.  In such circumstance, tasks with larger computational cost 
should be assigned higher priority.  This observation aids some critical tasks to be 
executed earlier and enhances probability of tasks reduce its finish time.  Furthermore, 
each task has to receive the essential messages from its immediate predecessors.  In 
other words, a task will be in waiting state when it does not collect complete message 
yet.  For this reason, we emphasize the importance of the last arrival message such 
that the succeeding task can start its execution earlier.  Therefore, it is imperative to 
give the predecessor who sends the last arrival message higher priority.  This can aid 
the succeeding task to get chance to advance the start time.  On the other hand, if a 
task ni is inserted into the front of a scheduling list, it occupies vantage position.  
Namely, ni has higher probability to accelerate its execution and consequently the start 
time of suc(ni) can be advanced as well.   
In most list scheduling approaches, it was usually to demonstrate the algorithms 
in two phases, the list phase and the processor selection phase.  The list phase of 
proposed GCA scheduling algorithm consists of two steps, the CS (critical score) 
calculation step and task prioritization step. 
Let’s take examples for the demonstration of CS calculation, which is performed 
in level order and started from the deepest level, i.e., the level of exit task.  For 
example, according to equation (4), we have CS(n10)= 10w = 16.  For the upper 
 20
12.         Put CS(ni) into container C; 
13.       Endif 
14.     Push tasks pred(nj) from C into S by non-decreasing order according to their 
critical scores; 
15.     Reset C to empty; 
16.     /* if there are 2+ tasks with same CS(ni), task ni is randomly pushed into S. 
17. EndWhile 
End_GCA_List_Phase 
 
In processor-selection phase, tasks will be deployed from list L that obtained in 
listing phase to suitable processor in FIFO manner.  According to the ordered list L = 
{n1, n4, n2, n5, n9, n3, n7, n6, n8, n10}, we have the complete calculated EFTs of tasks in 
DAG-1 and the schedule results of GCA algorithm are listed in Table 2 and Figure 2(a), 
respectively.   
Table 2: Earliest Finish Time of tasks in DAG-1 using GCA algorithm 
 
Earliest Finish Time of tasks in GCA algorithm 
n1 n2 n 3 n 4 n 5 n 6 n 7 n 8 n 9 n 10 
9 27 42 19.7 32.7 47.6 53 65.7 54.7 84.7   
 
P1 P2 P3 P1 P2 P3 P1 P2 P3
1
10
20
30
40
50
60
70
80
90
100
110
1
24
5 6
9
8
3
7
10
1
34
2
5
6
9
7
8
10
(a) (b) (c)
24
5
9
3
7
6
8
10
 
Figure 2: Schedule results of three algorithms on DAG-1 (a) GCA (makespan = 84.7) (b) CA 
(makespan = 92.4) (c) HEFT (makespan = 108.2). 
 
In order to profile significance of the GCA scheduling technique, the schedule 
results of other algorithms, CA and HEFT are depicted in Figure 2(b) and 2(c), 
respectively. The GCA scheduling techniques incorporates the consideration of 
heterogeneous communication costs among processors in processor selection phase.  
Such enhancement facilitates the selection of best candidate of processors to execute 
specific tasks.   
 
5. Performance Evaluation 
 22
processor number (P=16) under different number of task (n) are shown in Figure 5.  
The speedup of these algorithms show placid when number of task is small and 
increased significantly when number of tasks becomes large.  In general, the GCA 
algorithm has better speedup than the other two algorithms.  Improvement rate of the 
GCA algorithm in terms of average speedup is about 7% to the CA algorithm and 34% 
to the HEFT algorithm.  The improvement rate (IRGCA) is estimated by the following 
equation: 
IRGCA = ∑
∑∑ −
)(
)()(
CAorHEFTSpeedup
CAorHEFTSpeedupGCASpeedup         (13) 
 
 
Figure 3: PQS: GCA compared with CA (3 processors)   
 
 
Figure 4: PQS: GCA compared with CA (weight = 128) 
 
16 processors
2.00
3.50
5.00
6.50
8.00
20 40 60 80 100
# task
sp
ee
du
p
GCA
CA
HEFT
 
Figure 5: Speedup of GCA, CA and HEFT with different number of tasks (n). 
 24
 
Speedup of the GCA, CA and HEFT algorithms to execute different DAGs with fix processor number (P=16) and 
task number (n=60) under different out-degree of tasks (d) are shown in Figure 6.  The results of Figure 6 demonstrate 
the speedup influence by task dependence.  We observe that speedups of scheduling algorithms are less dependent on 
tasks’ dependence.  Although the speedups of three algorithms are stable, the GCA algorithm outperforms the other two 
algorithms in most cases.  Improvement rate of the GCA algorithm in terms of average speedup is about 5% to the CA 
algorithm and 80% to the HEFT algorithm. 
Figure 7 shows simulation results of three algorithms upon different processor number and degree of parallelization.  
It is noticed that, graphs with larger value of p tends to with higher parallelism.  As shown in Figures 7(a) and (b), the 
GCA algorithm performs well in linear graphs (p=0.5) and general graphs (p=1.0).  On the contrary, Figure 7(c) shows 
that the HEFT scheduling algorithm has superior performance when degree of parallelism is high.  In general, for 
graphs with low parallelism (e.g., p = 0.5), the GCA algorithm has 33% improvement rate in terms of average speedup 
compare to the HEFT algorithm; for graphs with normal parallelism (e.g., p = 1), the GCA algorithm has 20% 
improvement rate.  For graphs with high parallelism (e.g., p = 2), the GCA algorithm performs worse than the HEFT by 
3% performance. 
Speedup of the GCA, CA and HEFT algorithms to execute different DAGs with fix processor number (P=16) and 
task number (n=60) under different out-degree of tasks (d) are shown in Figure 6.  The results of Figure 6 demonstrate 
the speedup influence by task dependence.  We observe that speedups of scheduling algorithms are less dependent on 
tasks’ dependence.  Although the speedups of three algorithms are stable, the GCA algorithm outperforms the other two 
algorithms in most cases.  Improvement rate of the GCA algorithm in terms of average speedup is about 5% to the CA 
algorithm and 80% to the HEFT algorithm. 
 
   
(a)                    (b)                     (c) 
Figure 7: Speedup with different degree of parallelism (p) (a) p = 0.5 (b) p = 1 (c) p = 2. 
The impact of communication overheads on speedup are plotted in Figure 8 by setting different value of CCR.  It is 
noticed that increase of CCR will downgrade the speedup we can obtained.  For example, speedup offered by CCR = 
0.1 has maximal value 8.3 in GCA with 12 processors; for CCR = 1.0, the GCA algorithm has maximal speedup 6.1 
when processor number is 12; and the same algorithm, GCA, has maximal speedup 3.1 for CCR = 5 with 12 processors.  
This is due to the fact that when communication overheads higher than computational overheads, costs for tasks 
migration will offset the benefit of moving tasks to faster processors.   
 
 
(a)                    (b)                     (c) 
Figure 8: Speedup results with different CCR (a) CCR=0.5 (b) CCR = 1 (c) CCR = 5. 
 
 
 
6. Conclusions 
The problem of scheduling a weighted directed acyclic graph (DAG) to a set of heterogeneous processors to 
minimize the completion time has been recently studied.  Several techniques have been presented in the literature to 
improve performance.  This paper presented a general Critical-task Anticipation (GCA) algorithm for DAG scheduling 
 26
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 97 年  4 月  20  日  
姓 名 許慶賢 服 務 機 關 名 稱
 
中華大學
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
The 22nd International Conference on Advanced Information Networking and 
Applications (AINA-08), March 25 -28 2008. 
到 達 國 家 
及 地 點 
Okinawa, Japan 出 國
期 間
自 97 年 03 月 25 日
迄 97 年 03 月 28 日
 
內容提要 
 
一、主要任務摘要（五十字以內） 
    AINA-08 是網路相關研究領域一個大型的研討會。這一次參與AINA-08除了發表
相關研究成果以外，也在會場上看到許多新的研究成果與方向。此外，也與許多學術
界的朋友交換研究心得。 
 
二、對計畫之效益（一百字以內） 
    這一次參與 AINA-08 除了發表我們在此一計劃最新的研究成果以外，也在會場
中，向多位國內外學者解釋我們的研究內容，彼此交換研究心得。除了讓別的團隊
知道我們的研究方向與成果，我們也可以學習他人的研究經驗。藉此，加強國際合
作，提升我們的研究質量。 
 
三、經過 
    這一次在 Okinawa 所舉行的國際學術研討會議共計四天。第一天是 Workshop 
Program。第二天，由Dr. Michel Raynal的專題演講， “Synchronization is Coming Back, 
But is it the Same?” 作為研討會的開始。緊接著是五個平行的場次，分為上下午進
行。本人全程參與研討會的議程。晚上在大會的地點舉行歡迎晚宴。晚上本人亦參
加酒會，並且與幾位國外學者及中國、香港教授交換意見，合影留念。第三天，專
題演講是由 Dr. Shigeki Yamada 針對  “Cyber Science Infrastructure (CSI) for 
Promoting Research Activities of Academia and Industries in Japan”發表演說。本人也參
 28
 
Towards Improving QoS-Guided Scheduling in Grids  
Ching-Hsien Hsu1, Justin Zhan2, Wai-Chi Fang3 and Jianhua Ma4 
 
1Department of Computer Science and Information Engineering, Chung Hua University, Taiwan 
chh@chu.edu.tw 
2Heinz School, Carnegie Mellon University, USA 
justinzh@andrew.cmu.edu 
3Department of Electronics Engineering, National Chiao Tung University, Taiwan 
wfang@mail.nctu.edu.tw 
4Digital Media Department, Hosei University, Japan 
jianhua@hosei.ac.jp 
 
 
Abstract 
 
With the emergence of grid technologies, the 
problem of scheduling tasks in heterogeneous systems has 
been arousing attention. In this paper, we present two 
optimization schemes, Makespan Optimization 
Rescheduling (MOR) and Resource Optimization 
Rescheduling (ROR), which are based on the QoS 
Min-Min scheduling technique, for reducing the 
makespan of a schedule and the need of total resource 
amount. The main idea of the proposed techniques is to 
reduce overall execution time without increasing resource 
need; or reduce resource need without increasing overall 
execution time. To evaluate the effectiveness of the 
proposed techniques, we have implemented both 
techniques along with the QoS Min-Min scheduling 
algorithm. The experimental results show that the MOR 
and ROR optimization schemes provide noticeable 
improvements.  
 
1. Introduction 
 
With the emergence of IT technologies, 
the need of computing and storage are rapidly 
increased.  To invest more and more 
equipments is not an economic method for an 
organization to satisfy the even growing 
computational and storage need. As a result, 
grid has become a widely accepted paradigm 
for high performance computing.   
To realize the concept virtual organization, 
in [13], the grid is also defined as “A type of 
parallel and distributed system that enables the 
sharing, selection, and aggregation of 
geographically distributed autonomous and 
heterogeneous resources dynamically at 
runtime depending on their availability, 
capability, performance, cost, and users' 
quality-of-service requirements”.  As the grid 
system aims to satisfy users’ requirements with 
limit resources, scheduling grid resources plays 
an important factor to improve the overall 
performance of a grid.   
In general, grid scheduling can be 
classified in two categories: the performance 
guided schedulers and the economy guided 
schedulers [16]. Objective of the performance 
guided scheduling is to minimize turnaround 
time (or makespan) of grid applications. On the 
other hand, in economy guided scheduling, to 
minimize the cost of resource is the main 
objective.  However, both of the scheduling 
problems are NP-complete, which has also 
instigated many heuristic solutions [1, 6, 10, 14] 
to resolve. As mentioned in [23], a complete 
grid scheduling framework comprises 
application model, resource model, 
performance model, and scheduling policy. The 
scheduling policy can further decomposed into 
three phases, the resource discovery and 
selection phase, the job scheduling phase and 
the job monitoring and migration phase, where 
the second phase is the focus of this study.  
Although many research works have been 
devoted in scheduling grid applications on 
 30
allocation of grid computing is also presented.  
The scheduling strategy can control the request 
assignment to grid resources by adjusting usage 
accounts or request priorities. Resource 
management is achieved by assigning usage 
quotas to intended users. The scheduling 
method also supports reservation based grid 
resource allocation and quality of service 
feature.  Sometimes the scheduler is not only 
to match the job to which resource, but also 
needs to find the optimized transfer path based 
on the cost in network. In [19], a distributed 
QoS network scheduler (DQNS) is presented to 
adapt to the ever-changing network conditions 
and aims to serve the path requests based on a 
cost function. 
3. Research Architecture 
  
Our research model considers the static 
scheduling of batch jobs in grids.  As this 
work is an extension and optimization of the 
QoS guided scheduling that is based on 
Min-Min scheduling algorithm [9], we briefly 
describe the Min-Min scheduling model and the 
QoS guided Min-Min algorithm.  To simplify 
the presentation, we first clarify the following 
terminologies and assumptions. 
z QoS Machine (MQ) – machines can provide 
special services. 
z QoS Task (TQ) – tasks can be run 
completion only on QoS machine. 
z Normal Machine (MN) – machines can only 
run normal tasks. 
z Normal Task (TN) – tasks can be run 
completion on both QoS machine and 
normal machine. 
z A chunk of tasks will be scheduled to run 
completion based on all available machines 
in a batch system. 
z A task will be executed from the beginning 
to completion without interrupt. 
z The completion time of task ti to be 
executed on machine mj is defined as  
 
CTij = dtij + etij              (1) 
 
Where etij denotes the estimated execution time 
of task ti executed on machine mj; dtij is the 
delay time of task ti on machine mj.   
 
The Min-Min algorithm is shown in Figure 
1. 
 
Algorithm_Min-Min()
{ 
while there are jobs to schedule 
for all job i to schedule 
for all machine j 
Compute CTi,j = CT(job i, machine j)
end for 
Compute minimum CTi,j 
end for 
Select best metric match m 
Compute minimum CTm,n 
Schedule job m on machine n 
end while 
} End_of_ Min-Min  
 
Figure 1. The Min-Min Algorithm 
 
Analysis: If there are m jobs to be scheduled in 
n machines, the time complexity of Min-Min 
algorithm is O(m2n). The Min-Min algorithm 
does not take into account the QoS issue in the 
scheduling.  In some situation, it is possible 
that normal tasks occupied machine that has 
special services (referred as QoS machine).  
This may increase the delay of QoS tasks or 
result idle of normal machines. 
 
The QoS guided scheduling is proposed to resolve 
the above defect in the Min-Min algorithm.  In QoS 
guided model, the scheduling is divided into two classes, 
the QoS class and the non-QoS class.  In each class, the 
Min-Min algorithm is employed.  As the QoS tasks have 
higher priority than normal tasks in QoS guided 
scheduling, the QoS tasks are prior to be allocated on 
QoS machines.  The normal tasks are then scheduled to 
all machines in Min-Min manner.  Figure 2 outlines the 
method of QoS guided scheduling model with the 
Min-Min scheme.   
Analysis: If there are m jobs to be scheduled in 
n machines, the time complexity of QoS guided 
scheduling algorithm is O(m2n).  
Figure 3 shows an example demonstrating 
the Min-Min and QoS Min-Min scheduling 
schemes.  The asterisk * means that 
tasks/machines with QoS demand/ability, and 
the X means that QoS tasks couldn’t be 
executed on that machine.  Obviously, the 
QoS guided scheduling algorithm gets the 
better performance than the Min-Min algorithm 
in term of makespan.  Nevertheless, the QoS 
guided model is not optimal in both makespan 
and resource cost. We will describe the 
 32
 12 
 *M1 M2 
T1 7 4 
T2 3 3 
T3 9 5 
*T4 5 X 
B. The Makespan Optimization 
Rescheduling (MOR) algorithm  
M3 
7 
5 
7 
X 
T5 9 8 6 
*T6 5 X X 
Machine 0 *M1 M2 
*T4 
*T6 
T1 3 
8 
11 
M3 
T3 
T5 
Makespan 
T2 
Machine 
T1 
T2 
T3 
M2 
T5 
M3 
A. The QOS guided scheduling 
algorithm 
*T6 
*T4 
*M1 
Makespan 
12 
8 
3 
 
Figure 4. Example of MOR 
 
Recall the example given in Figure 3, 
Figure 4 shows the optimization of the MOR 
approach.  The left side of Figure 4 
demonstrates that the QoS guided scheme gives 
a schedule with makespan = 12, wheremachine 
M2 presents maximum CT (completion time), 
which is assembled by tasks T2, T1 and T3.  
Since the CT of machine ‘M3’ is 6, so ‘M3’ has 
an available time fragment (6).  Checking all 
tasks in machine M2, only T2 is small enough 
to be allocated in the available time fragment in 
M3.  Therefore, task M2 is moved to M3, 
resulting machine ‘M3’ has completion time 
CT=11, which is better than the QoS guided 
scheme. 
As mentioned above, the MOR is based on the QoS 
guided scheduling algorithm.  If there are m tasks to be 
scheduled in n machines, the time complexity of MOR is 
O(m2n).  Figure 5 outlines a pseudo of the MOR scheme.   
 
Algorithm_MOR()
{ 
for CTj in all machines 
find out the machine with maximum makespan CTmax and 
set it to be the standard 
end for 
do until no job can be rescheduled 
for job i in the found machine with CTmax  
            for all machine j 
  according to the job’s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj < makespan) 
           rescheduling the job i to machine j   
           update the CTj and CTmax 
       exit for 
end if 
            next for 
            if the job i can be reschedule 
find out the new machine with maximum CTmax
            exit for 
end if 
next for 
end do  
} End_of_ MOR  
Figure 5. The MOR Algorithm 
4.2 Resource Optimization Rescheduling (ROR) 
Following the assumptions described in MOR, the main 
idea of the ROR scheme is to re-dispatch tasks from the 
machine with minimum number of tasks to other 
machines, expecting a decrease of resource need.  
Consequently, if we can dispatch all tasks from machine 
Mx to other machines, the total amount of resource need 
will be decreased.  
Figure 6 gives another example of QoS scheduling, 
where the QoS guided scheduling presents makespan = 13. 
According to the clarification of ROR, machine ‘M1’ has 
the fewest amount of tasks.  We can dispatch the task 
‘T4’ to machine ‘M3’ with the following constraint 
 
CTij + CTj <= CTmax             (2) 
 
The above constraint means that the rescheduling can be 
performed only if the movement of tasks does not 
increase the overall makespan.  In this example, CT43 = 2, 
CT3=7 and CTmax=CT2=13.  Because the makespan of 
M3 (CT3) will be increased from 7 to 9, which is smaller 
than the CTmax, therefore, the task migration can be 
performed.  As the only task in M1 is moved to M3, the 
amount of resource need is also decreased comparing 
with the QoS guided scheduling.   
 34
QR=45% (NT=300, NR=50, QT=20%) and (d) QT=15% 
(NT=300, NR=50, QR=40%) have best improvements.  All of 
the four configurations conform to the following relation, 
 
0.4 × (NT × QT) = NR × QR          (3) 
 
This observation indicates that the improvement of MOR 
is significant when the number of QoS tasks is 2.5 times 
to the number of QoS machines.  Tables (e) and (f) 
change heterogeneity of tasks.  We observed that 
heterogeneity of tasks is not critical to the improvement 
rate of the MOR technique, which achieves 7% 
improvements under different heterogeneity of tasks. 
 
Table 2: Comparison of Makespan 
 
(a) (NR=50, QR=30%, QT=20%, HT=1, HQ=1) 
Task Number (NT) 200 300 400 500 600 
Min-Min 978.2 1299.7 1631.8 1954.6 2287.8
QOS Guided Min-Min 694.6 917.8 1119.4 1359.9 1560.1
MOR 597.3 815.5 1017.7 1254.8 1458.3
Improved Ratio 14.01% 11.15% 9.08% 7.73% 6.53%
 
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
Resource Number (NR) 50 70 90 110 130 
Min-Min 1931.5 1432.2 1102.1 985.3 874.2 
QOS Guided Min-Min 1355.7 938.6 724.4 590.6 508.7 
MOR 1252.6 840.8 633.7 506.2 429.4 
Improved Ratio 7.60% 10.42% 12.52% 14.30% 15.58%
 
(c) (NT=300, NR=50, QT=20%, HT=1, HQ=1) 
QR% 15% 30% 45% 60% 75% 
Min-Min 2470.8 1319.4 888.2 777.6 650.1 
QOS Guided Min-Min 1875.9 913.6 596.1 463.8 376.4 
MOR 1767.3 810.4 503.5 394.3 339.0 
Improved Ratio 5.79% 11.30% 15.54% 14.99% 9.94% 
 
(d) (NT=300, NR=50, QR=40%, HT=1, HQ=1) 
QT% 15% 30% 45% 60% 75% 
Min-Min 879.9 1380.2 1801.8 2217.0 2610.1
QOS Guided Min-Min 558.4 915.9 1245.2 1580.3 1900.6
MOR 474.2 817.1 1145.1 1478.5 1800.1
Improved Ratio 15.07% 10.79% 8.04% 6.44% 5.29% 
 
(e) (NT=500, NR=50, QR=30%, QT=20%, HQ=1) 
HT 1 3 5 7 9 
Min-Min 1891.9 1945.1 1944.6 1926.1 1940.1
QOS Guided Min-Min 1356.0 1346.4 1346.4 1354.9 1357.3
MOR 1251.7 1241.4 1244.3 1252.0 1254.2
Improved Ratio 7.69% 7.80% 7.58% 7.59% 7.59% 
 
(f) (NT=500, NR=50, QR=30%, QT=20%, HT=1) 
HQ 3 5 7 9 11 
Min-Min 1392.4 1553.9 1724.9 1871.7 2037.8
QOS Guided Min-Min 867.5 1007.8 1148.2 1273.2 1423.1
MOR 822.4 936.2 1056.7 1174.3 1316.7
Improved Ratio 5.20% 7.11% 7.97% 7.77% 7.48%
 
5.3 Experimental Results of ROR 
Table 3 analyzes the effectiveness of the ROR technique 
under different circumstances.   
 
Table 3: Comparison of Resource Used 
 
(a) (NR=100, QR=30%, QT=20%, HT=1, HQ=1) 
Task Number (NT) 200 300 400 500 600 
QOS Guided Min-Min 100 100 100 100 100 
ROR 39.81 44.18 46.97 49.59 51.17 
Improved Ratio 60.19% 55.82% 53.03% 50.41% 48.83%
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
Resource Number (NR) 50 70 90 110 130 
QOS Guided Min-Min 50 70 90 110 130 
ROR 26.04 35.21 43.65 50.79 58.15 
Improved Ratio 47.92% 49.70% 51.50% 53.83% 55.27%
(c) (NT=500, NR=50, QT=20%, HT=1, HQ=1) 
QR% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 50 50 50 50 50 
ROR 14.61 25.94 35.12 40.18 46.5 
Improved Ratio 70.78% 48.12% 29.76% 19.64% 7.00% 
(d) (NT=500, NR=100, QR=40%, HT=1, HQ=1) 
QT% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 100 100 100 100 100 
ROR 57.74 52.9 48.54 44.71 41.49 
Improved Ratio 42.26% 47.10% 51.46% 55.29% 58.51%
(e) (NT=500, NR=100, QR=30%, QT=20%, HQ=1) 
HT 1 3 5 7 9 
QOS Guided Min-Min 100 100 100 100 100 
ROR 47.86 47.51 47.62 47.61 47.28 
Improved Ratio 52.14% 52.49% 52.38% 52.39% 52.72%
(f) (NT=500, NR=100, QR=30%, QT=20%, HT=1) 
HQ 3 5 7 9 11 
QOS Guided Min-Min 100 100 100 100 100
ROR 54.61 52.01 50.64 48.18 46.53
Improved Ratio 45.39% 47.99% 49.36% 51.82% 53.47%
 36
Processing (JSSPP'05), pp. 146-158, 2005.  
[22] Haobo Yu, Andreas Gerstlauer, Daniel Gajski, "RTOS 
Scheduling in Transaction Level Models", in Proc. of the 1st 
IEEE/ACM/IFIP international conference on Hardware/software 
Codesign & System Synpaper, pp. 31-36, 2003.  
[23] Y. Zhu, "A Survey on Grid Scheduling Systems", LNCS 4505, 
pp. 419-427, 2007. 
[24] Weizhe Zhang, Hongli Zhang, Hui He, Mingzeng Hu, 
"Multisite Task Scheduling on Distributed Computing Grid", 
LNCS 3033, pp. 57–64, 2004. 
行政院國家科學委員會補助專題研究計畫 █ 成 果 報 告   □期中進度報告 
 
 
平行資料程式於計算網格上通訊與 I/O局部化
研究與應用工具開發(3/3) 
 
 
計畫類別：5 個別型計畫  □ 整合型計畫 
計畫編號：NSC95-2221-E-216-006  
執行期間：96 年 8 月 1 日至 97 年 7 月 31 日 
 
計畫主持人：許慶賢   中華大學資訊工程學系副教授 
共同主持人： 
計畫參與人員： 陳泰龍 (中華大學工程科學研究所博士生) 
     張智鈞、郁家豪、蔡秉儒(中華大學資訊工程學系研究生) 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  5完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
5出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年5二年後可公開查詢 
          
執行單位：中華大學資訊工程學系 
 
中 華 民 國    97   年  10    月   31   日 
 3
 
三、 考察參觀活動(無是項活動者省略) 
 
四、 建議 
 
    看了眾多研究成果以及聽了數篇專題演講，最後，本人認為，會議所安排的會場以及邀請的講席
等，都相當的不錯，覺得會議舉辦得很成功，值得我們學習。 
 
五、 攜回資料名稱及內容 
 
1. Conference Program 
2. Proceedings 
 
 2
where we also present a motivating example to demonstrate the characteristics of the master-slave pre-scheduling 
model. Section 4 assesses the new scheduling algorithm, the Smallest Communication Ratio (SCR), while the 
illustration of SCR on heterogeneous communication is examined in section 5. The performance comparisons and 
simulations results are discussed in section 6, and finally in section 7, some conclusions of this paper. 
2   Related Work 
The task scheduling research on heterogeneous processors can be classified into DAGs model, master-slave paradigm 
and computational grids. The main purpose of task scheduling is to achieve high performance computing and high 
throughput computing. The former aims at increasing execution efficiency and minimizing the execution time of tasks, 
whereas the latter aims at decreasing processor idle time and scheduling a set of independent tasks to increase the 
processing capacity of the systems over a long period of time. 
Thanalapati et al. [13] brought up the idea about adaptive scheduling scheme based on homogeneous processor 
platform, which applies space-sharing and time-sharing to schedule tasks. With the emergence of Grid and ubiquitous 
computing, new algorithms are in demand to address new concerns arising to grid environments, such as security, 
quality of service and high system throughput.  Berman et al. [6] and Cooper et al. [11] addressed the problem of 
scheduling incoming applications to available computation resources. Dynamically rescheduling mechanism was 
introduced to adaptive computing on the Grid.  In [8], some simple heuristics for dynamic matching and scheduling of 
a class of independent tasks onto a heterogeneous computing system have been presented.  Moreover, an extended 
suffrage heuristic was presented in [12] for scheduling the parameter sweep applications that have been implemented 
in AppLeS. They also presented a method to predict the computation time for a task/host pair by using previous host 
performance. 
Chronopoulos et al. [9], Charcranoon et al. [10] and Beaumont et al. [4, 5] introduced the research of master-slave 
paradigm with heterogeneous processors background. Based on this architecture, Beaumont et al. [1, 2] presented a 
method on master-slave paradigm to forecast the amount of tasks each processor needs to receive in a given period of 
time. Beaumont et al. [3] presented the pipelining broadcast method on master-slave platforms, focusing on message 
passing disregarding computation time. Intuitionally in their implementation, fast processor receives more tasks in the 
proportional distribution policy. Tasks are also prior allocated to faster slave processors and expected higher system 
throughput could be obtained. 
3   Preliminaries 
In this section, we first introduce basic concepts and models of this investigation, where we also define notations 
and terminologies that will be used in subsequent subsections. 
3.1   Research Architecture 
We have revised several characteristics that were introduced by Beaumont et al. [1, 2]. Based on the master 
slave paradigm introduced in section 1, this paper follows next assumptions as listed. 
z Heterogeneous processors: all processors have different computation speed. 
z Identical tasks: all tasks are of equal size. 
z Non-preemption: tasks are considered to be atomic. 
z Exclusive communication: communications from master node to different slave processors can not be 
overlapped.  
z Heterogeneous communication: communication costs between master and slave processors are of different 
overheads. 
 
 
 
3.2   Definitions 
 4
 
Fig. 1. Most Jobs First (MJF) task scheduling when 1≤δ . 
 
Lemma 1: Given a master slave system with δ  > 1, in MJF scheduling, the amount of tasks being assigned to 
Pmax+1 can be calculated by the following equation, 
task(Pmax+1) = (BSC − ∑
=
max
1
)(
i
iPcomm ) / Tmax+1_com                                             (1) 
Lemma 2: Given a master slave system with δ  > 1, in MJF scheduling, the period of processor Pmax+1 stays idle 
denoted by MJFidleT  and can be calculated by the following equation,  
MJF
idleT  = BSC − )()( 1max1max ++ − PcompPcomm                                (2) 
Another example of master slave task scheduling with identical communication (i.e., Ti_comm=1) and δ  > 1 is 
given in Fig. 2. Because δ  > 1, according to equation (1), we have task(Pmax+1=P4) = 10. We note that P4 
completes its tasks and becomes available at time 100. However, the master processor dispatches tasks to P3 
during time 100 ~ 110 and starts to send tasks to P4 at time 110. Such kind of idle situation also happens at time 
100~110, 160~170, 220~230, and so on. 
 
Fig. 2. Most Jobs First (MJF) Tasking when 1>δ . 
Lemma 3: In MJF scheduling algorithm with identical communication Ti_comm, when δ  > 1, the completion time of 
tasks in the jth BSC can be calculated by the following equation. 
T(BSCj) =∑
=
max
1
)(
i
iPcomm + ))()(( 1max1max MJFidleTPcompPcommj ++× ++  MJFidleT−                       (3) 
 
 6
 
Fig. 3. Smallest Communication Ratio (SCR) Tasking when 1>δ . 
5   Generalized Smallest Communication Ratio (SCR) 
As computational grid integrates geographically distributed computing resources, the communication overheads 
from resource broker / master computer to different computing site are different. Therefore, towards an efficient 
scheduling algorithm, the heterogeneous communication overheads should be considered. In this section, we 
present the SCR task scheduling techniques work on master slave computing paradigm with heterogeneous 
communication. 
 
Lemma 8: Given a master slave system with heterogeneous communication and δ  > 1, in MJF scheduling, we 
have 
task(Pmax+1) = 
⎥⎥
⎥⎥
⎥
⎦
⎥
⎢⎢
⎢⎢
⎢
⎣
⎢ −
+
=
∑
comm
i
i
T
PcommBSC
_1max
max
1
)(
                                     (8) 
Lemma 9: Given an SCR scheduling with heterogeneous communication and δ > 1, SCRidleT  is the idle time of one 
slave processor, we have the following equation,  
SCR
idleT  = ∑+
=
1max
1
)(
i
iPcomm  − BSC.                                   (9) 
Lemma 10: Given an SCR scheduling with heterogeneous communication and δ > 1, )( jSCRstart BSCT  is the start 
time to dispatch tasks in the jth BSC, we have the following equation,  
)()1()( SCRidlej
SCR
start TBSCjBSCT +×−=                                    (10) 
Lemma 11: Given an SCR scheduling with heterogeneous communication and δ > 1, the task completion time of 
the jth BSC denoted by )( jSCRfinish BSCT , we have 
)( j
SCR
finish BSCT = ∑+
=
1max
1
)(
i
iPcomm +comp(Pk)+ ))()(()1( SCRidlekk TPcompPcommj ++×−         (11) 
 8
 
The MJF scheduling is depicted in Fig. 4(b). According to corollary 5, task(Pmax+1) = task(P4) = 0,  therefore, 
P4 will not be included in the scheduling.  MJF has the task distribution order P1, P2, P3. Another scheduling 
policy is called Longest Communication Ratio (LCR) which is an opposite approach to the SCR method. Fig. 4(c) 
shows the LCR scheduling result which has the dispatch order P1, P2, P4, P3. 
To investigate the performance of SCR scheduling technique, we observe that MJF algorithm completes 16 tasks 
in 90 units of time in the first BSC. On the other hand, in SCR scheduling, there are 19 tasks completed in 73 
units of time in the first BSC. In LCR, there are 19 tasks completed in 99 units of time. We can see that the 
system throughput of SCR (19/73≈0.260) > LCR (19/99≈0.192) > MJF (16/90≈0.178). Moreover, the average 
turnaround time of the SCR algorithm in the first three BSCs is 183/57 (≈3.2105) which is less than the LCR‘s 
average turnaround time 209/57 (≈3.6666) and the MJF‘s average turnaround time 186/48 (≈3.875). 
6   Performance Evaluation 
To evaluate the performance of the proposed method, we have implemented the SCR and the MJF algorithms. We 
compare different criteria, such as average turnaround time, system throughput and processor idle time, in 
Heterogeneous Processors with Heterogeneous Communications (HPHC). 
Simulation experiments for evaluating average turnaround time are made upon different number of 
processors and show in Fig. 7. The computational speed of slave processors is set as T1=3, T2=3, T3=5, T4=7, T5=11, 
and T6=13. For the cases when processor number is 2, 3… 6, we have 1≤δ . When processor number increases to 7, 
we have 1>δ . In either case, the SCR algorithm conduces better average turnaround time. From the above results, 
we conclude that the SCR algorithm outperforms MJF for most test samples. 
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1 2 3 4 5 6
# of nodes
Av
era
ge
 tu
rn
-ar
ou
nd
tim
e
MJF
SCR
 
Fig. 5. Average task turn-around time on different numbers of processors. 
Simulation results present the performance comparison of three task scheduling algorithms, SCR, MJF, LCR, 
on heterogeneous processors and heterogeneous communication paradigms. Fig. 6 shows the simulation results 
for the experiment setting that with ±10 processor speed variation and ±4 communication speed variation. The 
computation speed of slave processors are 1T =3, 2T =6, 3T =11, and 4T =13. The time of a slave processor to 
receive one task from master processor are commT _1  = 5, commT _2  = 2, commT _3  = 1 and commT _4 =3. The average task 
turnaround time, system throughput and processor idle time are measured.   
 
 10
(a) 
0
0.05
0.1
0.15
0.2
0.25
5 10 15 20 25# of Nodes
Th
rou
gh
pu
t
MJF
LCR
SCR
 
(b) 
Fig. 7. Simulation results of throughput for the range of 5~25 processors with ±30 computation speed variation and 
±30 communication variation in 100 cases and 100 BSC (a) system throughput of the cases when 0< iT ≤ 30 and 
0< commiT _ ≤ 5 (b) system throughput of the cases when 0< iT ≤ 5 and 0< commiT _ ≤ 30. 
Fig. 7(a) is the case of 0< iT ≤ 30, 0< commiT _ ≤ 5 and the parameter of computation speed and communication 
speed are to be random and uniformly distributed within different number of nodes and 100 BSC for 100 cases. 
Fig. 7(b) is the case of 0< iT ≤ 5 and 0< commiT _ ≤ 30. The SCR algorithm performs better than MJF method, and 
SCR method has higher throughput than the MJF algorithm as shown in Fig. 7(a) and Fig. 7(b).  From the 
above experimental tests, we have the following remarks. The proposed SCR scheduling technique has better 
task turnaround time and higher system throughput than the MJF algorithm. 
From the above experimental tests, we have the following remarks. 
z The proposed SCR scheduling technique has higher system throughput than the MJF algorithm. 
z The proposed SCR scheduling technique has better task turnaround time than the MJF algorithm. 
The SCR scheduling technique has less processor idle time than the MJF algorithm. 
7   Conclusions 
The problem of resource management and scheduling has been one of main challenges in grid computing. In this paper, 
we have presented an efficient algorithm, SCR for heterogeneous processors tasking problem. One significant 
improvement of our approach is that average turnaround time could be minimized by selecting processor has the 
smallest communication ratio first. The other advantage of the proposed method is that system throughput can be 
increased via dispersing processor idle time. Our preliminary analysis and simulation results indicate that the SCR 
algorithm outperforms Beaumont’s method in terms of lower average turnaround time, higher average throughput, less 
processor idle time and higher processors’ utilization. 
There are numbers of research issues that remains in this paper. Our proposed model can be applied to map tasks 
onto heterogeneous cluster systems in grid environments, in which the communication costs are various from clusters. 
In future, we intend to devote generalized tasking mechanisms for computational grid.  We will study realistic 
applications and analyze their performance on grid system.  Besides, rescheduling of processors / tasks for minimizing 
processor idle time on heterogeneous systems is also interesting and will be investigated. 
References 
1. O. Beaumont, A. Legrand and Y. Robert, “The Master-Slave Paradigm with Heterogeneous Processors,” IEEE Trans. on parallel 
and distributed systems, Vol. 14, No.9, pp. 897-908, September 2003. 
2. C. Banino, O. Beaumont, L. Carter, J. Ferrante, A. Legrand and Y. Robert, ”Scheduling Strategies for Master-Slave Tasking on 
Heterogeneous Processor Platforms,” IEEE Trans. on parallel and distributed systems, Vol. 15, No.4, pp.319-330, April 2004. 
3. O. Beaumont, A. Legrand and Y. Robert, “Pipelining Broadcasts on Heterogeneous Platforms,” IEEE Trans. on parallel and 
distributed systems, Vol. 16, No.4, pp. 300-313 April 2005. 
4. O. Beaumont, V. Boudet, A. Petitet, F. Rastello and Y. Robert, “A Proposal for a Heterogeneous Cluster ScaLAPACK (Dense 
Linear Solvers),” IEEE Trans. Computers, Vol. 50, No. 10, pp. 1052-1070, Oct. 2001. 
 12
 
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 96 年  6 月  20  日   
姓 名 許慶賢 服 務 機 關 名 稱
 
中華大學
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
2007 International Conference on Algorithms and Architecture for Parallel 
Processing, June 11 -14 2007. 
到 達 國 家 
及 地 點 
Hangzhou, China 出 國
期 間
自 96 年 06 月 11 日
迄 96 年 06 月 19 日
內 容 提 要 
這一次在杭州所舉行的國際學術研討會議共計四天。第一天下午本人抵達會
場辦理報到。第二天各主持一場 invited session 的論文發表。同時，自己也
在上午的場次發表了這依次被大會接受的論文。第一天也聽取了 Dr. 
Byeongho Kang 有關於 Web Information Management 精闢的演說。第二天許
多重要的研究成果分為六個平行的場次進行論文發表。本人選擇了
Architecture and Infrastructure、Grid computing、以及 P2P computing 相關場
次聽取報告。晚上本人亦參加酒會，並且與幾位國外學者及中國、香港教授
交換意見，合影留念。第三天本人在上午聽取了 Data and Information 
Management 相關研究，同時獲悉許多新興起的研究主題，並了解目前國外
大多數學者主要的研究方向，並且把握最後一天的機會與國外的教授認識，
希望能夠讓他們加深對台灣研究的印象。三天下來，本人聽了許多優秀的論
文發表。這些研究所涵蓋的主題包含有：網格系統技術、工作排程、網格計
算、網格資料庫以及無線網路等等熱門的研究課題。此次的國際學術研討會
議有許多知名學者的參與，讓每一位參加這個會議的人士都能夠得到國際上
最新的技術與資訊。是一次非常成功的學術研討會。參加本次的國際學術研
討會議，感受良多。讓本人見識到許多國際知名的研究學者以及專業人才，
得以與之交流。讓本人與其他教授面對面暢談所學領域的種種問題。看了眾
多研究成果以及聽了數篇專題演講，最後，本人認為，會議所安排的會場以
及邀請的講席等，都相當的不錯，覺得會議舉辦得很成功，值得我們學習。
出 席 人 所 屬 機 
關 審 核 意 見 
 
層 轉 機 關 
審 核 意 見 
 
研 考 會 
處 理 意 見 
 
 14
approach, information of applications, such as tasks execution time, message size of 
communications among tasks, and tasks dependences are known a priori at 
compile-time; tasks are assigned to processors accordingly in order to minimize the 
entire application completion time and satisfy the precedence of tasks.  Hybrid 
scheduling techniques are mix of dynamic and static methods, where some 
preprocessing is done statically to guide the dynamic scheduler [8]. 
A Direct Acyclic Graph (DAG) [2] is usually used for modeling parallel 
applications that consists a number of tasks.  The nodes of DAG correspond to tasks 
and the edges of which indicate the precedence constraints between tasks.  In 
addition, the weight of an edge represents communication cost between tasks.  Each 
node is given a computation cost to be performed on a processor and is represented by 
a computation costs matrix.  Figure 1 shows an example of the model of DAG 
scheduling.  In Figure 1(a), it is assumed that task nj is a successor (predecessor) of 
task ni if there exists an edge from ni to nj (from nj to ni) in the graph.  Upon task 
precedence constraint, only if the predecessor ni completes its execution and then its 
successor nj receives the messages from ni, the successor nj can start its execution.  
Figure 1(b) demonstrates different computation costs of task that performed on 
heterogeneous processors.  It is also assumed that tasks can be executed only on 
single processor with non-preemptable style.  A simple fully connected processor 
network with asymmetrical data transfer rate is shown in Figures 1(c) and 1(d). 
 
            
 P1 P2 P3 iw  
n1 14 19 9 14 
n2 13 19 18 16.7 
n3 11 17 15 14.3 
n4 13 8 18 13 
n5 12 13 10 11.7 
n6 12 19 13 14.7 
n7 7 16 11 11 
n8 5 11 14 10 
n9 18 12 20 16.7 
n10 17 20 11 16  
(a)                                   (b) 
                  
 (c)                                  (d) 
Figure 1: An example of DAG scheduling problem (a) Directed Acyclic Graph (DAG-1) (b) 
computation cost matrix (W) (c) processor topology (d) communication weight. 
 
The scheduling problem has been widely studied in heterogeneous systems where 
 16
estimated computation time of processor Pj to execute task ni.  The mean execution time 
of task ni can be calculated by the following equation: 
∑== Pj jii Pww 1 ,                         (1) 
Example of the mean execution time can be referred to Figure 1(b).   
 
For communication part, a P × P matrix T is structured to represent different 
data transfer rate among processors (Figure 1(d) demonstrates the example).  The 
communication cost of transferring data from task ni (execute on processor px) to task 
nj (execute on processor py) is denoted by ci,j and can be calculated by the following 
equation, 
yxjimji tMsgVc ,,, ×+= ,                    (2) 
Where: 
Vm is the communication latency of processor Pm, 
Msgi,j is the size of message from task ni to task nj, 
tx,y is data transfer rate from processor px to processor py, 1≤ x, y ≤P. 
 
In static DAG scheduling problem, it was usually to consider processors’ 
latency together with its data transfer rate.  Therefore, equation (2) can be 
simplified as follows, 
yxjiji tMsgc ,,, ×= ,                     (3) 
Given an application represented by Directed Acyclic Graph (DAG), G = (V, E), 
where V = {nj: j = 1: v} is the set of nodes and v = |V|; E = {ei,j = <ni, nj>} is the set 
of communication edges and e =|E|.  In this model, each node indicates least 
indivisible task.  Namely, each node must be executed on a processor from the start 
to its completion.  Edge <ni, nj> denotes precedence of tasks ni and nj.  In other 
words, task ni is the immediate predecessor of task nj and task nj is the immediate 
successor of task ni.  Such precedence represents that task nj can be start for 
execution only upon the completion of task ni.  Meanwhile, task nj should receive 
essential message from ni for its execution.  Weight of edge <ni, nj > indicates the 
average communication cost between ni and nj. 
Node without any inward edge is called entry node, denoted by nentry; while node 
without any outward edge is called exit node, denoted by nexit.  In general, it is supposed 
that the application has only one entry node and one exit node.  If the actual application 
claims more than one entry (exit) node, we can insert a dummy entry (exit) node with 
zero-cost edge. 
 
3. Preliminaries 
This study concentrates on list scheduling approaches in DAG model.  List 
scheduling was usually distinguished into list phase and processor selection phase.  
Therefore, priori to discuss the main content, we first define some notations and 
terminologies used in both phases in this section. 
3.1 Parameters for List Phase 
 18
Pk.  ),( kj PnFT  is defined as follows, 
kjkjkj wPnSTPnFT ,),(),( +=                     (8) 
Definition 4: Given a DAG scheduling system on G = (V, E), the earliest finish time of 
task nj denoted by )( jnEFT , is formulated as follows,  
)},({)( kjPpj PnFTMinnEFT k∈
=                      (9) 
Definition 5: Based on the determination of )( jnEFT  in equation (9), if the earliest finish 
time of task nj is obtained upon task nj executed on processor pt, then the target processor of 
task nj is denoted by TP(nj), and TP(nj) = pt. 
 
4. The Generalized Critical-task Anticipation Scheduling Algorithm 
Our approach takes advantages of list scheduling in lower algorithmic complexity and 
superior scheduling performance and furthermore came up with a novel heuristic 
algorithm, the generalized critical task anticipation (GCA) scheduling algorithm to 
improve the schedule length as well as speedup of applications.  The proposed 
scheduling algorithm will be verified beneficial for the readers while we delineate a 
sequence of the algorithm and show some example scenarios in three phases, 
prioritizing phase, listing phase and processor selection phase.  
In prioritizing phase, the CS(ni) is known as the maximal summation of scores 
including the average computation cost and communication cost from task ni to the 
exit task.  Therefore, the magnitude of the task’s critical score is regarded as the 
decisive factor when determining the priority of a task.  In listing phase, an ordered 
list of tasks should be determined for the subsequent phase of processor selection. The 
proposed GCA scheduling technique arranges tasks into a list L, not only according to 
critical scores but also considers tasks’ importance.  
Several observations bring the idea of GCA scheduling method.  Because of 
processor heterogeneity, there exist variations in execution cost from processor to 
processor for same task.  In such circumstance, tasks with larger computational cost 
should be assigned higher priority.  This observation aids some critical tasks to be 
executed earlier and enhances probability of tasks reduce its finish time.  Furthermore, 
each task has to receive the essential messages from its immediate predecessors.  In 
other words, a task will be in waiting state when it does not collect complete message 
yet.  For this reason, we emphasize the importance of the last arrival message such 
that the succeeding task can start its execution earlier.  Therefore, it is imperative to 
give the predecessor who sends the last arrival message higher priority.  This can aid 
the succeeding task to get chance to advance the start time.  On the other hand, if a 
task ni is inserted into the front of a scheduling list, it occupies vantage position.  
Namely, ni has higher probability to accelerate its execution and consequently the start 
time of suc(ni) can be advanced as well.   
In most list scheduling approaches, it was usually to demonstrate the algorithms 
in two phases, the list phase and the processor selection phase.  The list phase of 
proposed GCA scheduling algorithm consists of two steps, the CS (critical score) 
calculation step and task prioritization step. 
Let’s take examples for the demonstration of CS calculation, which is performed 
in level order and started from the deepest level, i.e., the level of exit task.  For 
example, according to equation (4), we have CS(n10)= 10w = 16.  For the upper 
 20
12.         Put CS(ni) into container C; 
13.       Endif 
14.     Push tasks pred(nj) from C into S by non-decreasing order according to their 
critical scores; 
15.     Reset C to empty; 
16.     /* if there are 2+ tasks with same CS(ni), task ni is randomly pushed into S. 
17. EndWhile 
End_GCA_List_Phase 
 
In processor-selection phase, tasks will be deployed from list L that obtained in 
listing phase to suitable processor in FIFO manner.  According to the ordered list L = 
{n1, n4, n2, n5, n9, n3, n7, n6, n8, n10}, we have the complete calculated EFTs of tasks in 
DAG-1 and the schedule results of GCA algorithm are listed in Table 2 and Figure 2(a), 
respectively.   
Table 2: Earliest Finish Time of tasks in DAG-1 using GCA algorithm 
 
Earliest Finish Time of tasks in GCA algorithm 
n1 n2 n 3 n 4 n 5 n 6 n 7 n 8 n 9 n 10 
9 27 42 19.7 32.7 47.6 53 65.7 54.7 84.7   
 
P1 P2 P3 P1 P2 P3 P1 P2 P3
1
10
20
30
40
50
60
70
80
90
100
110
1
24
5 6
9
8
3
7
10
1
34
2
5
6
9
7
8
10
(a) (b) (c)
24
5
9
3
7
6
8
10
 
Figure 2: Schedule results of three algorithms on DAG-1 (a) GCA (makespan = 84.7) (b) CA 
(makespan = 92.4) (c) HEFT (makespan = 108.2). 
 
In order to profile significance of the GCA scheduling technique, the schedule 
results of other algorithms, CA and HEFT are depicted in Figure 2(b) and 2(c), 
respectively. The GCA scheduling techniques incorporates the consideration of 
heterogeneous communication costs among processors in processor selection phase.  
Such enhancement facilitates the selection of best candidate of processors to execute 
specific tasks.   
 
5. Performance Evaluation 
 22
processor number (P=16) under different number of task (n) are shown in Figure 5.  
The speedup of these algorithms show placid when number of task is small and 
increased significantly when number of tasks becomes large.  In general, the GCA 
algorithm has better speedup than the other two algorithms.  Improvement rate of the 
GCA algorithm in terms of average speedup is about 7% to the CA algorithm and 34% 
to the HEFT algorithm.  The improvement rate (IRGCA) is estimated by the following 
equation: 
IRGCA = ∑
∑∑ −
)(
)()(
CAorHEFTSpeedup
CAorHEFTSpeedupGCASpeedup         (13) 
 
 
Figure 3: PQS: GCA compared with CA (3 processors)   
 
 
Figure 4: PQS: GCA compared with CA (weight = 128) 
 
16 processors
2.00
3.50
5.00
6.50
8.00
20 40 60 80 100
# task
sp
ee
du
p
GCA
CA
HEFT
 
Figure 5: Speedup of GCA, CA and HEFT with different number of tasks (n). 
 24
 
Speedup of the GCA, CA and HEFT algorithms to execute different DAGs with fix processor number (P=16) and 
task number (n=60) under different out-degree of tasks (d) are shown in Figure 6.  The results of Figure 6 demonstrate 
the speedup influence by task dependence.  We observe that speedups of scheduling algorithms are less dependent on 
tasks’ dependence.  Although the speedups of three algorithms are stable, the GCA algorithm outperforms the other two 
algorithms in most cases.  Improvement rate of the GCA algorithm in terms of average speedup is about 5% to the CA 
algorithm and 80% to the HEFT algorithm. 
Figure 7 shows simulation results of three algorithms upon different processor number and degree of parallelization.  
It is noticed that, graphs with larger value of p tends to with higher parallelism.  As shown in Figures 7(a) and (b), the 
GCA algorithm performs well in linear graphs (p=0.5) and general graphs (p=1.0).  On the contrary, Figure 7(c) shows 
that the HEFT scheduling algorithm has superior performance when degree of parallelism is high.  In general, for 
graphs with low parallelism (e.g., p = 0.5), the GCA algorithm has 33% improvement rate in terms of average speedup 
compare to the HEFT algorithm; for graphs with normal parallelism (e.g., p = 1), the GCA algorithm has 20% 
improvement rate.  For graphs with high parallelism (e.g., p = 2), the GCA algorithm performs worse than the HEFT by 
3% performance. 
Speedup of the GCA, CA and HEFT algorithms to execute different DAGs with fix processor number (P=16) and 
task number (n=60) under different out-degree of tasks (d) are shown in Figure 6.  The results of Figure 6 demonstrate 
the speedup influence by task dependence.  We observe that speedups of scheduling algorithms are less dependent on 
tasks’ dependence.  Although the speedups of three algorithms are stable, the GCA algorithm outperforms the other two 
algorithms in most cases.  Improvement rate of the GCA algorithm in terms of average speedup is about 5% to the CA 
algorithm and 80% to the HEFT algorithm. 
 
   
(a)                    (b)                     (c) 
Figure 7: Speedup with different degree of parallelism (p) (a) p = 0.5 (b) p = 1 (c) p = 2. 
The impact of communication overheads on speedup are plotted in Figure 8 by setting different value of CCR.  It is 
noticed that increase of CCR will downgrade the speedup we can obtained.  For example, speedup offered by CCR = 
0.1 has maximal value 8.3 in GCA with 12 processors; for CCR = 1.0, the GCA algorithm has maximal speedup 6.1 
when processor number is 12; and the same algorithm, GCA, has maximal speedup 3.1 for CCR = 5 with 12 processors.  
This is due to the fact that when communication overheads higher than computational overheads, costs for tasks 
migration will offset the benefit of moving tasks to faster processors.   
 
 
(a)                    (b)                     (c) 
Figure 8: Speedup results with different CCR (a) CCR=0.5 (b) CCR = 1 (c) CCR = 5. 
 
 
 
6. Conclusions 
The problem of scheduling a weighted directed acyclic graph (DAG) to a set of heterogeneous processors to 
minimize the completion time has been recently studied.  Several techniques have been presented in the literature to 
improve performance.  This paper presented a general Critical-task Anticipation (GCA) algorithm for DAG scheduling 
 26
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 97 年  4 月  20  日  
姓 名 許慶賢 服 務 機 關 名 稱
 
中華大學
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
The 22nd International Conference on Advanced Information Networking and 
Applications (AINA-08), March 25 -28 2008. 
到 達 國 家 
及 地 點 
Okinawa, Japan 出 國
期 間
自 97 年 03 月 25 日
迄 97 年 03 月 28 日
 
內容提要 
 
一、主要任務摘要（五十字以內） 
    AINA-08 是網路相關研究領域一個大型的研討會。這一次參與AINA-08除了發表
相關研究成果以外，也在會場上看到許多新的研究成果與方向。此外，也與許多學術
界的朋友交換研究心得。 
 
二、對計畫之效益（一百字以內） 
    這一次參與 AINA-08 除了發表我們在此一計劃最新的研究成果以外，也在會場
中，向多位國內外學者解釋我們的研究內容，彼此交換研究心得。除了讓別的團隊
知道我們的研究方向與成果，我們也可以學習他人的研究經驗。藉此，加強國際合
作，提升我們的研究質量。 
 
三、經過 
    這一次在 Okinawa 所舉行的國際學術研討會議共計四天。第一天是 Workshop 
Program。第二天，由Dr. Michel Raynal的專題演講， “Synchronization is Coming Back, 
But is it the Same?” 作為研討會的開始。緊接著是五個平行的場次，分為上下午進
行。本人全程參與研討會的議程。晚上在大會的地點舉行歡迎晚宴。晚上本人亦參
加酒會，並且與幾位國外學者及中國、香港教授交換意見，合影留念。第三天，專
題演講是由 Dr. Shigeki Yamada 針對  “Cyber Science Infrastructure (CSI) for 
Promoting Research Activities of Academia and Industries in Japan”發表演說。本人也參
 28
 
Towards Improving QoS-Guided Scheduling in Grids  
Ching-Hsien Hsu1, Justin Zhan2, Wai-Chi Fang3 and Jianhua Ma4 
 
1Department of Computer Science and Information Engineering, Chung Hua University, Taiwan 
chh@chu.edu.tw 
2Heinz School, Carnegie Mellon University, USA 
justinzh@andrew.cmu.edu 
3Department of Electronics Engineering, National Chiao Tung University, Taiwan 
wfang@mail.nctu.edu.tw 
4Digital Media Department, Hosei University, Japan 
jianhua@hosei.ac.jp 
 
 
Abstract 
 
With the emergence of grid technologies, the 
problem of scheduling tasks in heterogeneous systems has 
been arousing attention. In this paper, we present two 
optimization schemes, Makespan Optimization 
Rescheduling (MOR) and Resource Optimization 
Rescheduling (ROR), which are based on the QoS 
Min-Min scheduling technique, for reducing the 
makespan of a schedule and the need of total resource 
amount. The main idea of the proposed techniques is to 
reduce overall execution time without increasing resource 
need; or reduce resource need without increasing overall 
execution time. To evaluate the effectiveness of the 
proposed techniques, we have implemented both 
techniques along with the QoS Min-Min scheduling 
algorithm. The experimental results show that the MOR 
and ROR optimization schemes provide noticeable 
improvements.  
 
1. Introduction 
 
With the emergence of IT technologies, 
the need of computing and storage are rapidly 
increased.  To invest more and more 
equipments is not an economic method for an 
organization to satisfy the even growing 
computational and storage need. As a result, 
grid has become a widely accepted paradigm 
for high performance computing.   
To realize the concept virtual organization, 
in [13], the grid is also defined as “A type of 
parallel and distributed system that enables the 
sharing, selection, and aggregation of 
geographically distributed autonomous and 
heterogeneous resources dynamically at 
runtime depending on their availability, 
capability, performance, cost, and users' 
quality-of-service requirements”.  As the grid 
system aims to satisfy users’ requirements with 
limit resources, scheduling grid resources plays 
an important factor to improve the overall 
performance of a grid.   
In general, grid scheduling can be 
classified in two categories: the performance 
guided schedulers and the economy guided 
schedulers [16]. Objective of the performance 
guided scheduling is to minimize turnaround 
time (or makespan) of grid applications. On the 
other hand, in economy guided scheduling, to 
minimize the cost of resource is the main 
objective.  However, both of the scheduling 
problems are NP-complete, which has also 
instigated many heuristic solutions [1, 6, 10, 14] 
to resolve. As mentioned in [23], a complete 
grid scheduling framework comprises 
application model, resource model, 
performance model, and scheduling policy. The 
scheduling policy can further decomposed into 
three phases, the resource discovery and 
selection phase, the job scheduling phase and 
the job monitoring and migration phase, where 
the second phase is the focus of this study.  
Although many research works have been 
devoted in scheduling grid applications on 
 30
allocation of grid computing is also presented.  
The scheduling strategy can control the request 
assignment to grid resources by adjusting usage 
accounts or request priorities. Resource 
management is achieved by assigning usage 
quotas to intended users. The scheduling 
method also supports reservation based grid 
resource allocation and quality of service 
feature.  Sometimes the scheduler is not only 
to match the job to which resource, but also 
needs to find the optimized transfer path based 
on the cost in network. In [19], a distributed 
QoS network scheduler (DQNS) is presented to 
adapt to the ever-changing network conditions 
and aims to serve the path requests based on a 
cost function. 
3. Research Architecture 
  
Our research model considers the static 
scheduling of batch jobs in grids.  As this 
work is an extension and optimization of the 
QoS guided scheduling that is based on 
Min-Min scheduling algorithm [9], we briefly 
describe the Min-Min scheduling model and the 
QoS guided Min-Min algorithm.  To simplify 
the presentation, we first clarify the following 
terminologies and assumptions. 
z QoS Machine (MQ) – machines can provide 
special services. 
z QoS Task (TQ) – tasks can be run 
completion only on QoS machine. 
z Normal Machine (MN) – machines can only 
run normal tasks. 
z Normal Task (TN) – tasks can be run 
completion on both QoS machine and 
normal machine. 
z A chunk of tasks will be scheduled to run 
completion based on all available machines 
in a batch system. 
z A task will be executed from the beginning 
to completion without interrupt. 
z The completion time of task ti to be 
executed on machine mj is defined as  
 
CTij = dtij + etij              (1) 
 
Where etij denotes the estimated execution time 
of task ti executed on machine mj; dtij is the 
delay time of task ti on machine mj.   
 
The Min-Min algorithm is shown in Figure 
1. 
 
Algorithm_Min-Min()
{ 
while there are jobs to schedule 
for all job i to schedule 
for all machine j 
Compute CTi,j = CT(job i, machine j)
end for 
Compute minimum CTi,j 
end for 
Select best metric match m 
Compute minimum CTm,n 
Schedule job m on machine n 
end while 
} End_of_ Min-Min  
 
Figure 1. The Min-Min Algorithm 
 
Analysis: If there are m jobs to be scheduled in 
n machines, the time complexity of Min-Min 
algorithm is O(m2n). The Min-Min algorithm 
does not take into account the QoS issue in the 
scheduling.  In some situation, it is possible 
that normal tasks occupied machine that has 
special services (referred as QoS machine).  
This may increase the delay of QoS tasks or 
result idle of normal machines. 
 
The QoS guided scheduling is proposed to resolve 
the above defect in the Min-Min algorithm.  In QoS 
guided model, the scheduling is divided into two classes, 
the QoS class and the non-QoS class.  In each class, the 
Min-Min algorithm is employed.  As the QoS tasks have 
higher priority than normal tasks in QoS guided 
scheduling, the QoS tasks are prior to be allocated on 
QoS machines.  The normal tasks are then scheduled to 
all machines in Min-Min manner.  Figure 2 outlines the 
method of QoS guided scheduling model with the 
Min-Min scheme.   
Analysis: If there are m jobs to be scheduled in 
n machines, the time complexity of QoS guided 
scheduling algorithm is O(m2n).  
Figure 3 shows an example demonstrating 
the Min-Min and QoS Min-Min scheduling 
schemes.  The asterisk * means that 
tasks/machines with QoS demand/ability, and 
the X means that QoS tasks couldn’t be 
executed on that machine.  Obviously, the 
QoS guided scheduling algorithm gets the 
better performance than the Min-Min algorithm 
in term of makespan.  Nevertheless, the QoS 
guided model is not optimal in both makespan 
and resource cost. We will describe the 
 32
 12 
 *M1 M2 
T1 7 4 
T2 3 3 
T3 9 5 
*T4 5 X 
B. The Makespan Optimization 
Rescheduling (MOR) algorithm  
M3 
7 
5 
7 
X 
T5 9 8 6 
*T6 5 X X 
Machine 0 *M1 M2 
*T4 
*T6 
T1 3 
8 
11 
M3 
T3 
T5 
Makespan 
T2 
Machine 
T1 
T2 
T3 
M2 
T5 
M3 
A. The QOS guided scheduling 
algorithm 
*T6 
*T4 
*M1 
Makespan 
12 
8 
3 
 
Figure 4. Example of MOR 
 
Recall the example given in Figure 3, 
Figure 4 shows the optimization of the MOR 
approach.  The left side of Figure 4 
demonstrates that the QoS guided scheme gives 
a schedule with makespan = 12, wheremachine 
M2 presents maximum CT (completion time), 
which is assembled by tasks T2, T1 and T3.  
Since the CT of machine ‘M3’ is 6, so ‘M3’ has 
an available time fragment (6).  Checking all 
tasks in machine M2, only T2 is small enough 
to be allocated in the available time fragment in 
M3.  Therefore, task M2 is moved to M3, 
resulting machine ‘M3’ has completion time 
CT=11, which is better than the QoS guided 
scheme. 
As mentioned above, the MOR is based on the QoS 
guided scheduling algorithm.  If there are m tasks to be 
scheduled in n machines, the time complexity of MOR is 
O(m2n).  Figure 5 outlines a pseudo of the MOR scheme.   
 
Algorithm_MOR()
{ 
for CTj in all machines 
find out the machine with maximum makespan CTmax and 
set it to be the standard 
end for 
do until no job can be rescheduled 
for job i in the found machine with CTmax  
            for all machine j 
  according to the job’s QOS demand, find the 
adaptive machine j  
if (the execute time of job i in machine j + the 
CTj < makespan) 
           rescheduling the job i to machine j   
           update the CTj and CTmax 
       exit for 
end if 
            next for 
            if the job i can be reschedule 
find out the new machine with maximum CTmax
            exit for 
end if 
next for 
end do  
} End_of_ MOR  
Figure 5. The MOR Algorithm 
4.2 Resource Optimization Rescheduling (ROR) 
Following the assumptions described in MOR, the main 
idea of the ROR scheme is to re-dispatch tasks from the 
machine with minimum number of tasks to other 
machines, expecting a decrease of resource need.  
Consequently, if we can dispatch all tasks from machine 
Mx to other machines, the total amount of resource need 
will be decreased.  
Figure 6 gives another example of QoS scheduling, 
where the QoS guided scheduling presents makespan = 13. 
According to the clarification of ROR, machine ‘M1’ has 
the fewest amount of tasks.  We can dispatch the task 
‘T4’ to machine ‘M3’ with the following constraint 
 
CTij + CTj <= CTmax             (2) 
 
The above constraint means that the rescheduling can be 
performed only if the movement of tasks does not 
increase the overall makespan.  In this example, CT43 = 2, 
CT3=7 and CTmax=CT2=13.  Because the makespan of 
M3 (CT3) will be increased from 7 to 9, which is smaller 
than the CTmax, therefore, the task migration can be 
performed.  As the only task in M1 is moved to M3, the 
amount of resource need is also decreased comparing 
with the QoS guided scheduling.   
 34
QR=45% (NT=300, NR=50, QT=20%) and (d) QT=15% 
(NT=300, NR=50, QR=40%) have best improvements.  All of 
the four configurations conform to the following relation, 
 
0.4 × (NT × QT) = NR × QR          (3) 
 
This observation indicates that the improvement of MOR 
is significant when the number of QoS tasks is 2.5 times 
to the number of QoS machines.  Tables (e) and (f) 
change heterogeneity of tasks.  We observed that 
heterogeneity of tasks is not critical to the improvement 
rate of the MOR technique, which achieves 7% 
improvements under different heterogeneity of tasks. 
 
Table 2: Comparison of Makespan 
 
(a) (NR=50, QR=30%, QT=20%, HT=1, HQ=1) 
Task Number (NT) 200 300 400 500 600 
Min-Min 978.2 1299.7 1631.8 1954.6 2287.8
QOS Guided Min-Min 694.6 917.8 1119.4 1359.9 1560.1
MOR 597.3 815.5 1017.7 1254.8 1458.3
Improved Ratio 14.01% 11.15% 9.08% 7.73% 6.53%
 
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
Resource Number (NR) 50 70 90 110 130 
Min-Min 1931.5 1432.2 1102.1 985.3 874.2 
QOS Guided Min-Min 1355.7 938.6 724.4 590.6 508.7 
MOR 1252.6 840.8 633.7 506.2 429.4 
Improved Ratio 7.60% 10.42% 12.52% 14.30% 15.58%
 
(c) (NT=300, NR=50, QT=20%, HT=1, HQ=1) 
QR% 15% 30% 45% 60% 75% 
Min-Min 2470.8 1319.4 888.2 777.6 650.1 
QOS Guided Min-Min 1875.9 913.6 596.1 463.8 376.4 
MOR 1767.3 810.4 503.5 394.3 339.0 
Improved Ratio 5.79% 11.30% 15.54% 14.99% 9.94% 
 
(d) (NT=300, NR=50, QR=40%, HT=1, HQ=1) 
QT% 15% 30% 45% 60% 75% 
Min-Min 879.9 1380.2 1801.8 2217.0 2610.1
QOS Guided Min-Min 558.4 915.9 1245.2 1580.3 1900.6
MOR 474.2 817.1 1145.1 1478.5 1800.1
Improved Ratio 15.07% 10.79% 8.04% 6.44% 5.29% 
 
(e) (NT=500, NR=50, QR=30%, QT=20%, HQ=1) 
HT 1 3 5 7 9 
Min-Min 1891.9 1945.1 1944.6 1926.1 1940.1
QOS Guided Min-Min 1356.0 1346.4 1346.4 1354.9 1357.3
MOR 1251.7 1241.4 1244.3 1252.0 1254.2
Improved Ratio 7.69% 7.80% 7.58% 7.59% 7.59% 
 
(f) (NT=500, NR=50, QR=30%, QT=20%, HT=1) 
HQ 3 5 7 9 11 
Min-Min 1392.4 1553.9 1724.9 1871.7 2037.8
QOS Guided Min-Min 867.5 1007.8 1148.2 1273.2 1423.1
MOR 822.4 936.2 1056.7 1174.3 1316.7
Improved Ratio 5.20% 7.11% 7.97% 7.77% 7.48%
 
5.3 Experimental Results of ROR 
Table 3 analyzes the effectiveness of the ROR technique 
under different circumstances.   
 
Table 3: Comparison of Resource Used 
 
(a) (NR=100, QR=30%, QT=20%, HT=1, HQ=1) 
Task Number (NT) 200 300 400 500 600 
QOS Guided Min-Min 100 100 100 100 100 
ROR 39.81 44.18 46.97 49.59 51.17 
Improved Ratio 60.19% 55.82% 53.03% 50.41% 48.83%
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
Resource Number (NR) 50 70 90 110 130 
QOS Guided Min-Min 50 70 90 110 130 
ROR 26.04 35.21 43.65 50.79 58.15 
Improved Ratio 47.92% 49.70% 51.50% 53.83% 55.27%
(c) (NT=500, NR=50, QT=20%, HT=1, HQ=1) 
QR% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 50 50 50 50 50 
ROR 14.61 25.94 35.12 40.18 46.5 
Improved Ratio 70.78% 48.12% 29.76% 19.64% 7.00% 
(d) (NT=500, NR=100, QR=40%, HT=1, HQ=1) 
QT% 15% 30% 45% 60% 75% 
QOS Guided Min-Min 100 100 100 100 100 
ROR 57.74 52.9 48.54 44.71 41.49 
Improved Ratio 42.26% 47.10% 51.46% 55.29% 58.51%
(e) (NT=500, NR=100, QR=30%, QT=20%, HQ=1) 
HT 1 3 5 7 9 
QOS Guided Min-Min 100 100 100 100 100 
ROR 47.86 47.51 47.62 47.61 47.28 
Improved Ratio 52.14% 52.49% 52.38% 52.39% 52.72%
(f) (NT=500, NR=100, QR=30%, QT=20%, HT=1) 
HQ 3 5 7 9 11 
QOS Guided Min-Min 100 100 100 100 100
ROR 54.61 52.01 50.64 48.18 46.53
Improved Ratio 45.39% 47.99% 49.36% 51.82% 53.47%
 36
Processing (JSSPP'05), pp. 146-158, 2005.  
[22] Haobo Yu, Andreas Gerstlauer, Daniel Gajski, "RTOS 
Scheduling in Transaction Level Models", in Proc. of the 1st 
IEEE/ACM/IFIP international conference on Hardware/software 
Codesign & System Synpaper, pp. 31-36, 2003.  
[23] Y. Zhu, "A Survey on Grid Scheduling Systems", LNCS 4505, 
pp. 419-427, 2007. 
[24] Weizhe Zhang, Hongli Zhang, Hui He, Mingzeng Hu, 
"Multisite Task Scheduling on Distributed Computing Grid", 
LNCS 3033, pp. 57–64, 2004. 
