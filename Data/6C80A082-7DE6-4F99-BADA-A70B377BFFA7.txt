vectors can be selected adaptively according to the requirements of
the task, so that the generalization/complexity trade-off can be
well controlled. Further, the lower bound of the number of selected
vectors required to recover the original discriminant function can
be evaluated to help determine the number of selected support
vectors.
The paper is organized as follows. We brieﬂy introduce the SVM
model in the next section. In Section 3, the approximation error are
deﬁned and analyzed, the criterion for an optimal reduced set of
support vectors are described and compared with some others
mentioned in the paper. The proposed reductionmethods and their
implementation strategies are introduced in Section 4. In Section 5,
experiments with some benchmark datasets are given. Finally,
conclusions of this paper are addressed in Section 6.
2. Support vector machines (SVM)
For the sake of simple discussion, we shall concentrate on the
two-class pattern recognition problem. However, the proposed
method can be extended and applied to other multiple-class
problems.
The learning of an SVM is formulated as the following convex
quadratic problem (QP problem) (Cortes and Vapnik, 1995; Platt,
1998; Burges, 1999; Graepel et al., 2000; Ruszczynski, 2006).
Given the training sample fðxi; yiÞ : xi 2 X; yi 2 f1;þ1gNi¼1, ﬁnd
the Lagrange multipliers a ¼ faigNi¼1 that minimize the objective
function:
QðaÞ ¼ 
XN
i¼1
ai þ 12
XN
i¼1
XN
j¼1
aiajyiyjKðxi; xjÞ ð1Þ
subject to the constraint:
XN
i¼1
aiyi ¼ 0; 0 6 ai 6 C for i ¼ 1;2;3; . . . ;N; ð2Þ
where C is a user-speciﬁed positive constant and K (, ) is a positive
semideﬁnite kernel function on a sample space X, for which there is
a Hilbert space H with inner product h, i and a feature mapping /:
X? H such that
Kðx; x0Þ ¼ h/ðxÞ;/ðx0Þi; ðx; x0 2 XÞ: ð3Þ
Let faigNi¼1 be an optimal solution of the above problem, then the
discriminant function takes the form:
f ðxÞ ¼
XN
i¼1
aiyiKðxi; xÞ þ b ¼
XN
i¼1
aiyih/ðxiÞ;/ðxÞi þ b: ð4Þ
Note that 0 6 ai 6 C hold for i = 1,2, . . . ,N. All training samples xi
with ai > 0 are called support vectors. To distinguish between sup-
port vectors with 0 < ai < C and those with ai = C, the former are
called unbounded support vectors while the latter are called
bounded ones (Burges, 1999; Joachims, 1999). In the following,
we assume without loss of generality that 0 < ai 6 C for
i = 1,2, . . . ,n, and ai = 0 for i = n + 1,n + 2, . . . ,N. Thus, the discrimi-
nant function in (4) can be written as follows:
f ðxÞ ¼
Xn
i¼1
aiyih/ðxiÞ;/ðxÞi þ b: ð5Þ
For simplicity, in what follows, we let ki = aiyi so that
ki 2 [C,C]n{0} and that the discriminant function given in (5)
can be written in a further simpler form:
f ðxÞ ¼ bþ h/ðxÞ;wi; ð6Þ
where w ¼Pni¼1ki/ðxiÞ is the normal vector.
3. Reduction of the solutions for SVMs
Li et al. (2007) constructed a reduced set by selecting from the
support vector solutions one element at a time, based on the use of
the vector correlation principle and a greedy strategy. However,
the greedy search is not suitable for the problem and does not
guarantee an optimal solution.
For improving the testing speed of an SVM while remaining the
accuracy acceptable, we propose to ﬁnd a subset of support vectors
that best approximates the discriminant function formed by the
original support vectors.
Let S = {x1,x2, . . . xn} be the support vectors for a sample obtained
by an SVM and let XF ¼ fxF1 ; xF2 ; . . . ; xFmg# Sðm 6 nÞ be a subset of
vectors selected from S. If the mapping selected support vectors
f/ðxF1 Þ;/ðxF2 Þ; . . . ;/ðxFm Þg span the vector space spanned by the
mapping support vectors {/ (x1),/(x2), . . . ,/(xn)}, then any mapping
support vector /(xi) can be exactly approximated by the mapping
selected vectors or exactly expressed as a linear combination of
them:
/ðxiÞ ¼
Xm
j¼1
bij/ðxFj Þ:
As investigated in our previous work (Lin and Yeh, 2009), it is
helpful to evaluate the dimension of the space spanned by the origi-
nal support vectors, which is equal to the rank of the so-called sup-
port matrix U ¼ ½/ðx1Þ /ðx2Þ    /ðxnÞ  formed by the support
vectors; that is, dim(span {/(x1),/(x2), . . . ,/(xn)}) = rank(U).
Althoughwe are not able to acquire the entries in the supportmatrix
U, we may obtain its rank with the fact that rank(U) = rank(UTU),
since the entries ofUTU can be acquired through the kernel function
K: (UTU)ij = h/(xi),/ (xj)i = K(xi,xj). Since the value of rank(U) is the
minimum size of the reduced set of support vectors required for
recovering the original discriminant function, if we desire an
approximation with no error, we have to set the size m of the
reduced set to any number not less than rank(U).
3.1. Error of the reduced set of support vectors
For a reduced set of selected support vectors XF ¼ fxF1 ; xF2 ; . . . ;
xFmg, we are looking for coefﬁcients bij 2 R, 1 6 i 6 n, 1 6 j 6m,
such that each mapping support vector /i = /(xi) can be best
approximated by a linear combination of the mapping selected
vectors /ðXFÞ : /i ﬃ ~/i ¼
Pm
j¼1bij/ðxFj Þ. The coefﬁcients bij are the
least squares solution of the linear system /i ¼
Pm
j¼1/ðxFj Þ
xij; i ¼ 1;2; . . . ;n (Lin and Yeh, 2009). The discriminant function
fXF ðxÞ determined by the selected vectors XF can be expressed as:
fXF ðxÞ ¼ bþ h/ðxÞ;wXF i; ð7Þ
where wXF ¼
Pn
i¼1ki~/i ¼
Pn
i¼1ki
Pm
j¼1bij/ðxFj Þ ¼
Pm
j¼1cj/ðxFj Þ and
cj ¼
Pn
i¼1kibij. For searching the optimal reduced set of vectors we
deﬁne the approximation error dXF as follows:
dXF ¼ kwwXFk2 ¼
Xm
j¼1
cj/ðxFj Þ 
Xn
i¼1
ki/ðxiÞ


2
: ð8Þ
3.2. Evaluation of error
The expression for dXF given in (8) can be expanded as follows:
dXF ¼
Xm
j¼1
cj/ðxFj Þ 
Xn
i¼1
ki/ðxiÞ
 !
;
Xm
j¼1
cj/ðxFj Þ 
Xn
i¼1
ki/ðxiÞ
 !* +
¼
Xm
i¼1
Xm
j¼1
ðcicjh/ðxFi Þ;/ðxFj ÞiÞ  2
Xn
i¼1
Xm
j¼1
kicjh/ðxiÞ;/ðxFj Þi
þ
Xn
i¼1
Xn
j¼1
kikjh/ðxiÞ;/ðxjÞi
 !
: ð9Þ
564 H.-J. Lin, J.P. Yeh / Pattern Recognition Letters 31 (2010) 563–571
4.1.1. Our discrete PSO
A discrete particle swarm works by adjusting trajectories
through manipulation of each coordinate of a particle (Kennedy
and Eberhart, 1997). In a binary space, a particle may be considered
as to move to nearer and farther corners of the hypercube by ﬂip-
ping various numbers of bits, thus, the velocity of the particle over-
all may be described by the change of some bits. Following this
idea, we propose a modiﬁed version of discrete PSO to perform
the search of optimal subset of support vectors, as described in
the following. First, we take the velocity updating formula given
in (19) and set x^ ¼ 0, h1 = h2 = g1 = g2 = 1, to form a tentative
formula:
Uki ¼ ðuki1;uki2; . . . ;ukimÞ ¼ pbestki þ gbestk  2Cki : ð23Þ
Let n1 and n2 denote the number of negative components and the
number of positive components of Uki , respectively, and let
n0 = dmin(n1,n2)/2e. For each component uki;j of Uki , we store its index
j into the array C1 if uki;j > 0; otherwise, into the array C2 if u
k
i;j < 0.
From each of the sets C1 and C2, we randomly select n0 elements to
remain, and thus jC1j = jC2j = n0. We give the velocity updating for-
mula in (24), with which we simply take (20) as the position updat-
ing formula. For simplicity, in the algorithm shown in Fig. 1, we use
T to denote the procedure transforming Uki into
Vki ¼ ðvki1;vki2; . . . ;vkimÞ; i.e., Vki ¼ TðUki Þ. The way acquiring the sets
C1 and C2 is such that the position updating formula (20) would
change the jth component ckij from 0 to 1 when. vkij ¼ 1, and ckij from
1 to 0 when. vkij ¼ 1. Since jC1j = jC2j, the number of 1’s in the
components of Ckþ1i leaves unchanged and C
kþ1
i ensures a feasible
solution.
vki;j ¼
1 if j 2 C1;
1 if j 2 C2;
0 otherwise:
8><
>: ð24Þ
4.1.2. Optimization using PSO
Each particle in a swarm, representing a candidate solution, is
expressed as a binary vector as follows:
Cki ¼ ðcki1; . . . ; ckinÞ; subjectto
Xn
j¼1
cki;j ¼ m; ð25Þ
where cki;j is 1, if the jth support vector is selected and is 0, other-
wise. Based on the optimization problem described in Section 3, if
the particle C represent the reduced set XF then the ﬁtness for C is
deﬁned as ﬁtPSO (C)=ﬁt(XF), where the ﬁtness function ﬁt was given
in (12).
The procedure of the proposed binary PSO algorithm for search-
ing the optimal subset of support vectors is presented in Fig. 1.
4.2. Elitism genetic algorithm
Genetic algorithm (GA) is an adaptive optimization search algo-
rithm simulating the evolutionary ideas of natural selection (Gold-
berg, 1989). The basic idea is to maintain a population of possible
solutions that evolves and improves over time through a process of
competition and controlled variation. Since for such algorithms it is
not guaranteed that they will converge to the global optimum as
the number of iterations increases, and there are always possibili-
ties for such algorithms getting trapped in some local optimum, we
Fig. 1. Algorithm reduction of solutions for SVM by PSO. Fig. 2. Algorithm reduction of solutions for SVM by EGA.
566 H.-J. Lin, J.P. Yeh / Pattern Recognition Letters 31 (2010) 563–571
for the EGA. A ﬂowchart for the hybrid approach EGA–PSO is gi-
ven in Fig. 3.
5. Experimental results and comparisons
Our experiments were implemented on a PC with 2.8 GHz
Pentium Dual-Core processor and 4 GB RAM using Borland
C++ Builder 6.0 compiler. To verify the efﬁciency of the pro-
posed algorithms, we carried out a series of experiments on
some benchmark examples including spirals (Lang and Wit-
brock, 1989), cubic polynomials, concentric ellipses, and cosine
waves. The population size was set to 50 and RBF kernel
K(x,y) = exp(kx  yk2/2r2) is used. The parameter setting is
as following: mutation rate pm = 0.05, crossover rate pc = 0.5,
both population size for EGA and swarm size for PSO are set
to 50, both no. of generations for EGA and no. of iterations
for PSO are set to 200. In Tables 1–4 we compare the pro-
posed methods with our previously proposed method which
has been proven better than Li’s method. For clear explanation,
Table 1
Recognition rates for two spirals (n = 98; rank(U) = 86).
Reduction Method and error
Reduced size m Reduction rate Li’s method EGA EGA PSO EGA–PSO PSO–EGA Testing time (ms)
D1 D2 D3 D3 D3 D3
5 94.8 0.5000 0.5030 0.7395 0.7655 0.8250 0.8300 1.0894
10 89.8 0.5000 0.7375 0.7495 0.8210 0.8290 0.8510 2.1788
15 84.6 0.5315 0.7440 0.7535 0.8605 0.8730 0.8740 3.2682
20 79.6 0.5405 0.9035 0.9035 0.9035 0.9035 0.9035 4.3576
25 74.4 0.5470 0.9300 0.9455 0.9665 0.9900 0.9995 5.4470
30 69.4 0.5475 0.9995 0.9995 0.9995 0.9995 0.9995 6.5364
40 59.2 0.6040 0.9995 0.9995 0.9995 0.9995 0.9995 8.7152
50 48.9 0.6640 0.9995 0.9995 0.9995 0.9995 0.9995 10.8940
60 38.8 0.9900 0.9995 0.9995 0.9995 0.9995 0.9995 13.0728
70 28.6 0.9995 0.9995 0.9995 0.9995 0.9995 0.9995 15.2516
86 12.2 0.9995 0.9995 0.9995 0.9995 0.9995 0.9995 18.7376
Original size n = 98 0.0 Recognition rate of the original support vectors = 0.9995 21.3522
Table 2
Recognition rates for two polynomial graphs (n = 103; rank(U) = 98).
Reduction Method and error
Reduced size m Reduction rate Li’s method EGA EGA PSO EGA–PSO PSO–EGA Testing time (ms)
D1 D2 D3 D3 D3 D3
5 95.1 0.4880 0.4965 0.6200 0.6315 0.6400 0.6085 1.0894
10 90.3 0.4930 0.5205 0.5300 0.5305 0.5315 0.6780 2.1788
15 85.4 0.5080 0.6070 0.6120 0.6185 0.6120 0.7815 3.2682
20 80.6 0.5185 0.6535 0.6535 0.6535 0.6535 0.7945 4.3576
25 75.7 0.5430 0.7040 0.7085 0.7085 0.7085 0.8045 5.4470
30 70.9 0.5805 0.7045 0.7145 0.7145 0.7630 0.8130 6.5364
35 66.0 0.5815 0.7055 0.7420 0.8455 0.8665 0.8865 7.6258
40 61.2 0.5600 0.7085 0.8280 0.8310 0.8980 0.9335 8.7152
50 51.5 0.8235 0.9520 0.9520 0.9520 0.9520 0.9520 10.8940
60 41.7 0.7365 0.9700 0.9700 0.9700 0.9700 0.9700 13.0728
70 32.1 0.9000 0.9700 0.9700 0.9700 0.9700 0.9700 15.2516
80 22.3 0.9300 0.9700 0.9700 0.9700 0.9700 0.9700 17.4304
90 12.6 0.9700 0.9700 0.9700 0.9700 0.9700 0.9700 19.6092
98 4.9 0.9700 0.9700 0.9700 0.9700 0.9700 0.9700 21.3522
Original size n = 103 0.0 Recognition rate of the original support vectors = 0.9700 22.4416
Table 3
Recognition rates for two concentric ellipses (n = 234; rank(U) = 206).
Reduction Method and error
Reduced size m Reduction rate Li’s method EGA EGA PSO EGA–PSO PSO–EGA Testing time (ms)
D1 D2 D3 D3 D3 D3
5 98.3 0.5710 0.8165 0.8165 0.8165 0.8165 0.8165 0.8069
10 95.7 0.5710 0.8345 0.8430 0.8540 0.8545 0.8560 2.0631
15 93.5 0.6360 0.8460 0.8660 0.8765 0.8860 0.8865 2.4184
20 91.5 0.6355 0.9190 0.9190 0.9190 0.9190 0.9190 3.2246
25 89.3 0.6865 0.9365 0.9405 0.9480 0.9560 0.9585 4.0300
30 87.2 0.6855 1.0000 1.0000 1.0000 1.0000 1.0000 4.8369
40 82.9 0.8415 1.0000 1.0000 1.0000 1.0000 1.0000 6.4492
50 78.6 0.9315 1.0000 1.0000 1.0000 1.0000 1.0000 8.0615
55 76.5 0.9925 1.0000 1.0000 1.0000 1.0000 1.0000 8.8677
60 74.4 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 9.6738
206 13.6 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 33.2137
Original size n = 234 0.0 Recognition rate of the original support vectors = 1.0000 37.7281
568 H.-J. Lin, J.P. Yeh / Pattern Recognition Letters 31 (2010) 563–571
Eberhart, 2001; Løvberg and Krink, 2002), in which the parameters
are adjusted based on the feed back from the search process.
References
Angeline, P., 1998a. Evolutionary optimization versus particle swarm optimization:
Philosophy and performance difference. In: Proc. Seventh Annual Conf. on
Evolutionary Programming, pp. 601–610.
Angeline, P., 1998b. Using selection to improve particle swarm optimization. In:
Proc. Internat. Conf. on Evolutionary Computation. Piscataway, New Jersey, USA,
pp. 4–89.
Boser, B.E., Guyon, I.M., Vapnik, V.N., 1992. A training algorithm for optimal margin
classiﬁers. In: Proc. 5th Annual Workshop on Computational Learning Theory,
pp. 144–152.
Burges, C.J.C., 1996. Simpliﬁed support vector decision rules. In: Proc. 13th Internat.
Conf. on Machine Learning, Bari, Italy, pp. 71–77.
Burges, C.J.C., 1999. Geometry and invariance in kernel basedmethod. In: Advance in
Kernel Method-Support Vector Learning. MIT Press, Cambridge, MA, pp. 6–116.
Fig. 4. Examples of reduced sets of SVs given by PSO–GA method: (a) Red and blue dotted lines (each formed by 600 dotted points) represent the original Cosine waves (t,
30cos(t)) and (t, 35 + 30cos(t)), respectively; (b) ‘’ represents SVs given by standard SVM (r = 20, C = 8, n = 83 SV/600 total) and black line represents the hyperplane
(discriminant function) determined by SVs; (c) ‘’ represents selected m = 35 SVs and purple line represents the hyperplane determined by these SVs; (d) ‘’ represents
selected m = 20 SVs and green line represents the hyperplane determined by these SVs; (e) ‘’ represents selected m = 15 SVs and brown line represents the hyperplane
determined by these SVs; (f) hyperplanes are illustrated together: black, purple, green and brown lines represent hyperplanes shown in (b), (c), (d) and (e), respectively.
570 H.-J. Lin, J.P. Yeh / Pattern Recognition Letters 31 (2010) 563–571
 1
 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：100 年 03 月 01 日 
 
 
 
 
 
 
計畫編號 NSC 99-2221-E-243 -003 - 
計畫名稱 使用混合最佳化技術化簡支撐向量機之解 
出國人員
姓名 葉日斌 
服務機構
及職稱 
亞太創意技術學院資訊管理系助理教授 
會議時間 
2011年 2月 26日
至 2011年 2月 28
日 
會議地點 Singapore 
會議名稱 
(中文) 2011 年第三屆機器學習和計算機國際會議  
(英文)2011 3rd International Conference on Machine Learning and Computing(ICMLC 2011) 
發表論文
題目 
(中文)使用模擬退火演算法化簡支撐向量機之解 
(英文)Reducing the Solution of Support Vector Machines Using Simulated Annealing Algorithm 
附件四 
2011 3rd International Conference on Machine Learning and Computing(ICMLC 2011)   
 
- 1 - 
Notification of Acceptance of the ICMLC 2011 
February 26-28, 2011, Singapore 
http://www.icmlc.org/    
          
Dear Jih Pin Yeh and Chiang Ming Chiang, 
Paper ID : L128     
Paper  Title  :  Reducing the Solution of Support Vector Machines Using Simulated Annealing 
Algorithm 
 
Congratulations! The  review  processes  for  2011  3rd  International  Conference  on  Machine  Learning  and 
Computing (ICMLC 2011) have been completed. The conference received submissions from nearly 20 different 
countries and regions, which were reviewed by international experts, and about 150 papers have been selected 
for presentation and publication. Based on  the  recommendations of  the  reviewers and  the Technical Program 
Committees, we are pleased to  inform you that your paper  identified above has been accepted  for publication 
and  oral  presentation.  You  are  cordially  invited  to  present  the  paper  orally  at  ICMLC  2011  to  be  held  on, 
February 26‐28, Singapore. 
The  ICMLC  2011  is  sponsored  by  International  Association  of  Computer  Science  and  Information  Technology 
(IACSIT),  Singapore  Institute  of  Electronics  (SIE)  and  IEEE.  The  ICMLC  2011  has  been  listed  in  the  IEEE 
Conference Calendar.   
(Important) So in order to the register the conference and have your paper included 
in the proceeding successfully, you must finish following SIX steps. 
1. Revise your paper according to the Review Comments in the attachment carefully. 
2. Format your paper according to the Template carefully.   
http://www.icmlc.org/ieee.doc    (DOC Format)  
3. Download and complete the Registration Form. 
http://www.icmlc.org/reg.doc    (English) 
4. Finish  the  payment  of  Registration  fee  at  the  Bank.  (The  bank  transfer  information  can  be  found  in  the 
Registration form)   
http://www.icmlc.org/reg.doc    (English) 
 
5. Finish the IEEE Copyright Form 
http://www.icmlc.org/ieeecopyright.doc       
Reducing the Solution of Support Vector Machines Using Simulated Annealing 
Algorithm 
 
Jih Pin Yeh 
Department of Multimedia and Entertainment Science 
Asian-Pacific Institute of Creativity, 
Toufen Township, Miaoli County, Taiwan 35153, ROC 
yehbin@ms.apic.edu.tw 
 
Chiang Ming Chiang 
Department of Computer Science and Information 
Engineering 
Tamkang University, 
Tamsui, Taipei County, Taiwan 25137, ROC 
cmchiangtw@gmail.com 
 
 
Abstract—Support Vector Machines are a relatively recent 
machine learning technique. One of the SVM problems is that 
SVM is currently considerably slower in test phase caused by 
the large number of the support vectors, which greatly 
influences it into the practical use. To address this problem, we 
proposed a simulated annealing algorithm to reduce the 
solutions for an SVM by selecting vectors from the trained 
support vector solutions, such that the selected vectors best 
approximate the original discriminant function. Experimental 
results show that the proposed method can reduce the solutions 
for an SVM by selecting vectors from the trained support 
vector solutions, confirm the theoretical results and improve 
classification accuracy. 
Keywords-Support vector machine; simulated annealing 
algorithm; discriminant function. 
I.  INTRODUCTION  
Simulated annealing is an instance of a probabilistic 
search method. A probabilistic search method is an algorithm 
that searches the feasible set of an optimization problem by 
considering randomized samples of candidate points in the 
set. The simulated annealing algorithm was first presented 
for optimization by Kirkpatrick et al. [9] according to 
techniques of Metropolis et al. [14]. So far, extensions to 
simulated annealing have been developed. One of the 
successful ones is the application to optimization problem 
was described by Geman and Geman [6]. 
Support vector machines (SVMs) were developed by 
Vapnik [17][18], and are winning popularity due to their 
many charming features and their hopeful empirical 
performance. Since the discriminant function of a SVM is 
parameterized by a large set of support vectors and 
corresponding weights, the machine is greatly slower in the 
test phase than other learning machines like neural network 
and decision trees [2][11][13]. Many methods have been 
suggested to address this disadvantage by approximating the 
discriminant function using a reduced set of vectors 
[2][4][7][12][16]. In this paper, we propose an adjustable 
approach for optimally reducing the support vectors 
solutions using a simulated annealing algorithm based on a 
more reasonable fitness. Searching for an optimal subset of 
vectors chosen from the support vector solutions is usually 
speaking a combinatorial optimization for sets of support 
vectors of large size. In our work, we employ a simulated 
annealing based method to search after the optimal subset of 
a given number of support vectors. The subset made by the 
simulated annealing based method can be used to best 
approximate the original discriminant function. The number 
of the optimal support vectors can be selected adaptively in 
accordance with the requirements of the tasks’ need. 
Consequently, with our proposed method the 
generalization/complexity trade-off can be well controlled 
directly. 
II.  SUPPORT VECTOR MACHINES (SVMS)  
The learning of an SVM is formulated as the following 
convex QP problem [1][3][15]. 
Given the training sample 
( ) { }{ }Niidiii yRxyx 11,1,:, =+−∈∈ , find the Lagrange 
multipliers { }Nii 1=α  that minimize the objective function:  
( ) ( )∑ ∑∑
= = =
+−=
N
i
N
i
N
j
jijijii xxKyyQ
1 1 1
,
2
1
αααα           (1)  
subject to the constraints  
∑
=
≤≤=
N
i
iii Cy
1
0,0 αα   for i=1,2,3,…,N              (2) 
where C is a user-specified positive constant and K(•, •) is a 
positive semidefinite kernel function on a sample space X, 
for which there is a Hilbert space H with inner product <•, •> 
and a feature mapping such that 
( ) ( ) ( ) ( )XxxxxxxK ∈′>=<′ ,,,, φφ                            (3) 
Let { }Nii 1=α  be an optimal solution of the above problem, 
then the discriminant function takes the form: 
( ) ( )
( ) ( ) bxxy
bxxKyxf
N
i
iii
N
i
iyi
+><=
+=
∑
∑
=
=
1
1
,
,
φφα
α
                                       (4) 
Note that hold for i = 1, 2,…, N. All training samples 
with 0>iα  are called support vectors. To distinguish 
2011 3rd International Conference on Machine Learning and Computing (ICMLC 2011)
V1-3925978-1-4244-92 3-4 /11/$26.00 ©2011 IEEE 
Therefore, we can apply the simulated annealing technique 
to the combinatorial optimization problem. 
C. Encoding Scheme of Feasible point 
Each feasible point representing a reduced set of m 
vectors is generated by some encoded form : 
mcccC ....21=  , where jc  is a binary digital corresponding 
to the index of a support vector. And let the feasible point 
FC  represent the reduced set FX , that is, 
fitness ( ) ( )FF XFitC '= . 
IV. PROPOSED SIMULATED ANNEALING METHOD 
Simulated annealing (SA) is a generic probabilistic 
metaheuristic for the global optimization problem of applied 
mathematics, namely locating a good approximation to the 
global optimum of a given function in a large search space. 
The basic steps in our proposed SA-based algorithm are 
described as follows: 
Algorithm Reduction of Solutions for SVM 
The main steps of the SA search algorithm are listed as 
follows. 
Step 1. //Initialization 
Set t := 0; 
Randomly select an initial point  SC ∈)0( . 
Step 2.Pick a candidate point )(tz  at random from  )( )(tCN . 
)( )(tCN  is a neighbourhood of )(tC . 
Step 3.If ))(),(,(() )()( tt CFitzFittprandom < , then set 
)()1( tt zC =+  , else set )()1( tt CC =+ . 
 
)}/))()((exp(,1min{
))(),(,(
)()(
)()(
i
tt
tt
TCFitzFit
CFitzFittp
−−=
 
  is the acceptance probability. 
)2log(
6
+
=
t
Ti , the 
call random() should return a random value in the 
range [0,1]. 
 )1(: += tCbest  
Step 4. If the stopping criterion is satisfied, then stop. 
Step 5. Set t := t + 1, goto Step 2. 
V. EXPERIMENTAL RESULT 
Our experiments were implemented on a PC with 2.8 
GHz Pentium Dual-Core processor and 4 GB RAM using 
Borland C++ Builder 6.0 complier. We carried out 
experiments on spirals[10] and cubic polynomials. Each test 
includes randomly generated 3000 samples, 300 of them 
were randomly chosen as training data, and the remaining as 
test data. For this test we set 8=σ  and 10=C for spirals 
and 40=σ and 1=C  for cubic polynomials. And the 
RBF kernel )2/exp(),( 22 σyxyxK −−= is used. 
The recognition rates presented in Tables 1-2 refer to the 
mean of 30 executions of the algorithm. Tables 1 and 2 we 
compare the proposed methods with Li’s method [11] and 
our previous work [12]. The experimental results show that 
our proposed methods can effectively reduce the solutions 
for a SVM. The parametric equations for each of the testing 
examples are given in the following. 
⎩⎨
⎧
++=−
++=−
))sin()14(),cos()14((),(:2
)),sin()104(),cos()104((),(:1
θθθθ
θθθθ
yxSpiral
yxSpiral         (13) 
⎪⎩
⎪⎨⎧
+−+=−
+−+=−
)001.00312.08.1150,(),(:2
),001.0031.02200,(),(:1
32
32
ttttyxCubicpoly
ttttyxCubicpoly          (14) 
TABLE I.  RECOGNITION RATES FOR TWO SPIRALS 
m Reductionrate (%) 
Recognition rate 
Li’s method 
Our previous 
work 
(GA) 
SA 
5 94.8 0.5000 0.7395 0.8270 
10 89.8 0.5000 0.7495 0.8310 
15 84.6 0.5315 0.7535 0.8655 
25 74.4 0.5470 0.9455 0.9675 
n=98 0.0 Recognition rate of the original support vectors =0.9995 
TABLE II.  RECOGNITION RATES FOR TWO WAVEFORM GRAPHS 
m Reductionrate (%) 
Recognition rate 
Li’s method
Our previous 
work 
(GA) 
SA 
5 95.1 0.4880 0.6200 0.6325 
2
0 80.6 0.5185 0.6535 0.6555 
3
0 70.9 0.5805 0.7145 0.7245 
4
0 61.2 0.5600 0.8280 0.8410 
n
=
1
0
3
0.0 Recognition rate of the original support vectors =0.9700 
VI. CONCLUSIONS 
The simulated annealing algorithm was first presented for 
reducing solutions for SVMs. And the approximation error  
Fδ  which, unlike the one defined by Li et al., directly 
reflects the error between the approximating discriminant 
function and the original discriminant function. Experimental 
results have shown that SA outperforms Li’s method and our 
previous work. Further, our proposed method has many other 
advantages. Unlike other methods [2], [16], the vectors in the 
reduced set produced by our method are from the original 
support vectors. The proposed method can be applied on 
SVMs associated with any kernels. Besides, observing from 
our experimental results, we found that although the 
simulated annealing approach is a good strategy to deal with 
the problem of reducing solutions for SVMs, it occasionally 
has the problem of premature convergence; that is, SA 
usually suffers from premature convergence. Future work 
will concentrate on solving these problems. For the problem, 
we shall consider some hybrid approach or other 
2011 3rd International Conference on Machine Learning and Computing (ICMLC 2011)
V1-394
W. Hu (Ed.): Electronics and Signal Processing, LNEE 97, pp. 203–213. 
springerlink.com                     © Springer-Verlag Berlin Heidelberg 2011 
Optimal Reducing the Solutions of Support Vector 
Machines Based on Particle Swam Optimization 
Jih Pin Yeh1 and Chiang Ming Chiang2 
1 Department of Multimedia and Entertainment Science, Asian-Pacific Institute of Creativity, 
Toufen Township, Miaoli County, Taiwan 35153, ROC 
yehbin@ms.apic.edu.tw  
2 Department of Computer Science and Information Engineering, Tamkang University, 
Tamsui, Taipei County, Taiwan 25137, ROC 
cmchiangtw@gmail.com 
Abstract. SVM has been winning increasing interest in areas ranging from its 
original application in pattern recognition to other applications such as regres-
sion analysis due to its noticeable generalization performance. One of the SVM 
problems is that SVM is considerably slower in test phase caused by large 
number of the support vectors, which greatly influences it into the practical use. 
To address this problem, we proposed an adaptive particle swarm optimization 
(PSO) algorithm which based on a reasonable fitness to optimally reduce the 
solutions for an SVM by selecting vectors from the trained support vector solu-
tions, such that the selected vectors best approximate the original discriminant 
function. Experimental results show that adaptive particle swarm optimization 
algorithm is an optimal algorithm to simplify the solution for support vector 
machines and can reduce the solutions for an SVM by selecting vectors from 
the trained support vector solutions.  
Keywords: Support vector machine, Particle swarm optimization, Optimiza-
tion, Discriminant function. 
1   Introduction 
Support vector machines (SVMs) were developed by Vapnik [1], and are winning 
popularity resulting from their many attractive features and their promising empirical 
performance. Since the discriminant function of a SVM is parameterized by a large 
number of support vectors and corresponding weights, the machine is much slower in 
the test phase than other learning machines such as neural network and decision trees 
[1,2,14,16,18]. A lot of approaches have been proposed to address this disadvantage by 
approaching the discriminant function using a reduced set of vectors. Burges [2] and 
Thies et al. [23] each provided an explicit way to build a reduced set of vectors, which 
are usually not support vectors, and their approaches can only be applied effectively to 
SVMs using quadratic kernels. Downs et al. [7] reduced the solution of support vectors 
base on their linear dependency without any loss of generalization, but their reduction 
rate is not high enough. Guo et al. [8] evaluated the contribution of each support vector 
 Optimal Reducing the Solutions of Support Vector Machines 205 
The objective of a support vector machine is to determine the optimal w and op-
timal bias b such that the corresponding hyperplane separates the positive and nega-
tive training data with maximum margin and it produces the best generation perfor-
mance. This hyperplane is called an optimal separating hyperplane. Using Lagrange 
multiplier techniques leads to the following dual optimization problem [4, 6, 20, 21]. 
2.1   Dual Optimization Problem 
Given the training sample { }Niidiii yRxyx 1}1,1{,:),( =+−∈∈ , find the 
Lagrange multipliers { }Nii 1=α  that minimize the objective function  
∑∑∑
= ==
+−=
N
i
N
j
jijiji
N
i
i xxKyyQ
1 11
),(
2
1)( αααα  (5)
subject to the constraints: 
NiCy
N
i
ii ..., ,3 ,2 ,1for      0       0 i
1
=≤≤=∑
=
αα  (6)
where C is a user-specified positive constant and K(•, •) is a positive semidefinite 
kernel function. For any positive semidefinite kernel K on a sample space X, there is a 
Hilbert space H with inner product <•, •> and a feature mapping HX →:φ such 
that  
)',(,)'(),()',( XxxxxxxK ∈><= φφ  (7)
Let { }Nii 1=α   be an optimal solution of the above problem, then the discriminant 
function takes the form:  
bxxKyxf
N
i
iii +=∑
=1
),()( α  (8)
It is apparent from the constraint (6) that Ci ≤≤ α0   holds for i = 1, 2,…, N. All 
training samples ix   such that 0>iα  are called support vectors. To distinguish 
between support vectors with Ci <<α0  and those with iα = C, the former are 
called unbounded support vectors while the latter are called bounded ones [4, 10, 11]. 
In the following, we assume without loss of generality that Ci ≤<α0    for i = 1, 
2,…, n, and iα = 0 for i = n + 1, n + 2, . . . , N. Thus, the discriminant function in (8) 
can be written as follows: 
∑
=
+=
n
i
iii bxxKyxf
1
.),()( α  (9)
From Eq. (9), we can see that if there are more support vectors then the decision 
function will be more complicated and the classification speed of the SVM will be 
 Optimal Reducing the Solutions of Support Vector Machines 207 
∑
∈
=
Sx
FiX
i
iF
δαδ 2
 (12)
Where  
F
T
F
T
iiiF
T
iii
T
ii
iiiiFi
ΦΦ+Φ−=
−=
βαβφαφφα
φαφαδ
vv 222
2
2
~
 (13)
And the coefficient vector iβ
v
 such that  iFi βφ
v
Φ=~   best approximates iφ  is just 
a least squares solution to the linear system βφ vFi Φ= , which has a unique solution 
as given in (14) if ( ) 1−ΦΦ FTF exists. 
( ) iTFFTFi φβ ΦΦΦ= −1v  (14)
3.3   The Selection of Support Vectors Problem 
As we have shown in above, our goal is to find out an optimal reduced subset of sup-
port vectors such that the discriminant function has the minimal error, therefore we 
propose to select optimal vectors to approximate the original support vectors with 
error 
FX
δ . 
In other words, we can think the question as that we want to select m support 
vectors from n support vectors. This problem is a combinatorial optimization 
problem. Therefore, we can apply the PSO algorithm to the combinatorial 
optimization problem, so the selection of support vectors is formulated as the 
following optimal problem. 
SX F
X F
⊂  subject to
Minimize δ
 (15)
3.4   Evaluation of the Approximation Error 
The error 
FX
δ can be used for performance evaluation and its expression given in (12) 
can be expanded as follows: 
))(( 12 iTFFTFFTiiTi
Sx
iX
i
F
φφφφαδ ΦΦΦΦ−= −
∈
∑  
∑∑
∈
−
∈
ΦΦΦΦ−=
Sx
i
T
FF
T
FF
T
iii
T
i
Sx
i
ii
))(()( 122 φφαφφα  
(16)
 Optimal Reducing the Solutions of Support Vector Machines 209 
In the discrete binary version [13], a particle moves in a state space restricted to ze-
ro and one on each dimension, where each ijv  represents the probability of bit ijc  
taking the value 1. Thus, the step for updating ijv  as shown in (19) remains un-
changed, except that ijpbest and jgbest  are integers in {0, 1} in binary case. The 
resulted changes in position are defined as follows: 
( ) ( )   , )exp(1/1
,,
k
ji
k
ji vvs −+=  (21)
( )( ) 0  else  1    then     if
,,,
==< k ji
k
ji
k
ji ccvsτ  (22)
where τ  is a random number drawn from uniform sequence of U(0,1) . 
A discrete particle swarm works by adjusting trajectories through manipulation of 
each coordinate of a particle[13]. In a binary space, the velocity of a particle may be 
described by the change of some bits. Following this idea, we propose a modified 
version of discrete PSO to perform the search of optimal subset of support vectors, as 
described in the following. First, we take the velocity updating formula given in (19) 
and set ωˆ =0, 1θ = 2θ = 1η = 2η =1, to form a tentative formula: 
( )
  2,....,, 21
k
i
kk
i
k
im
k
i
k
i
k
i CgbestpbestuuuU −+==  (23)
Let n1 and n2 denote the number of negative components and the number of posi-
tive components of kiU , respectively, and let n0  = ( )⎡ ⎤2/,min 21 nn .For each compo-
nent k jiu , of kiU , we store its index j into the array 1Γ if 
k
jiu , > 0; otherwise, into the 
array 2Γ if 
k
jiu , <  0. From each of the sets 1Γ and 2Γ , we randomly select n0 elements 
to remain so that =Γ || 1  =Γ || 2 n0. We give the velocity updating formula in (24), with 
which we simply take (19) as the position updating formula. We use T to denote the 
operation transform kiU  into ( )kimkikiki vvvV ,....,, 21=  and denote kiV = T( kiU ). 
⎪⎩
⎪⎨
⎧
Γ∈−
Γ∈
=
otherwise
jif
jif
vk ji
0
1
1
2
1
,
 (24)
Each particle in a swarm, representing a candidate solution, is expressed as a bi-
nary vector as follows: 
( ) , subject to   ,,...,
1
,1 mcccC
n
j
k
ji
k
in
k
i
k
i ∑
=
==
 (25)
where k jic ,  is1, if the j-th support vector is selected and is 0, otherwise. We define the 
fitness for the particle C corresponding to the reduced set XF as fitPSO (C) = fit(XF), 
where the fitness function fit was given in (18). The procedure of the proposed binary 
PSO algorithm for searching the optimal subset of support vectors is described as 
follows: 
 Optimal Reducing the Solutions of Support Vector Machines 211 
Table 1. Recognition rates for two spirals. 
m 
Reduction rate 
(%) 
Recognition rate 
Li’s method Previous work(GA) PSO 
5 94.8 0.5000 0.5030 0.7485 
10 89.8 0.5000 0.7375 0.7895 
15 84.6 0.5315 0.7440 0.8005 
25 74.4 0.5470 0.9300 0.9350 
n=98 0.0 Recognition rate of the original support vectors = 0.9995 
Table 2. Recognition rates for two waveform graphs. 
m 
Reduction rate 
(%) 
Recognition rate 
Li’s method Previous work(GA) PSO 
5 95.1 0.4880 0.4965 0.6215 
20 80.6 0.5185 0.6535 0.6535 
30 70.9 0.5805 0.7045 0.7145 
40 61.2 0.5600 0.7085 0.8290 
n=103 0.0 Recognition rate of the original support vectors = 0.9700 
6   Conclusions 
We have proposed algorithms for reducing solutions for SVMs. Experimental results 
have shown that PSO outperforms our previously work [16] and Li's method [15]. And 
the PSO method is fast converge performance to find the best solution and take advan-
tage of the easily implementing performance. The simulation results also demonstrated 
that the proposed PSO based algorithm can be easily realized with high efficiency and 
can obtain higher quality solution than our previously work [16] and Li's method [15]. 
Although PSO finds good solutions much faster than other evolutionary algorithms, it 
usually can not improve the quality of the solutions as the number of iterations is in-
creased; that is, PSO usually suffers from premature convergence. Future work will 
concentrate on solving these problems. For the problem, we shall consider some a 
more reasonable fitness to optimally reduce the solutions for an SVM or other swarm 
intelligence (SI) technique further improves the effectiveness of PSO. 
References 
1. Boser, B.E., Guyon, I.M., Vapnik, V.N.: A training algorithm for optimal margin classifi-
ers. In: Proceedings of the 5th Annual ACM Workshop on Computational Learning  
Theory, pp. 144–152 (1992) 
2. Burges, C.J.C.: Simplified support vector decision rules. In: Proceedings 13th International 
Conference on Machine Learning, Bari, Italy, pp. 71–77 (1996) 
 Optimal Reducing the Solutions of Support Vector Machines 213 
23. Thies, T., Weber, F.: Optimal reduced-set vectors for support vector machines with a qua-
dratic kernel. Neural Computation 16, 1769–1777 (2004) 
24. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer, New York (1995) 
25. Vapnik, V.N.: Statistical Learning Theory. Wiley, New York (1998) 
26. Yeh, J.P., Pai, I.C., Wang, C.W., Yang, F.W., Lin, H.J.: Face Detection using SVM-Based 
Classification. The Special Issue of Far East Journal of Experimental and Theoretical Ar-
tificial Intelligence (FEJETAI) 3(2), 113–123 (2009) 
 
2011 3rd International Conference on Machine Learning and Computing(ICMLC 2011)   
 
- 1 - 
Notification of Acceptance of the ICMLC 2011 
February 26-28, 2011, Singapore 
http://www.icmlc.org/    
          
Dear Jih Pin Yeh and Chiang Ming Chiang, 
Paper ID : L128     
Paper  Title  :  Reducing the Solution of Support Vector Machines Using Simulated Annealing 
Algorithm 
 
Congratulations! The  review  processes  for  2011  3rd  International  Conference  on  Machine  Learning  and 
Computing (ICMLC 2011) have been completed. The conference received submissions from nearly 20 different 
countries and regions, which were reviewed by international experts, and about 150 papers have been selected 
for presentation and publication. Based on  the  recommendations of  the  reviewers and  the Technical Program 
Committees, we are pleased to  inform you that your paper  identified above has been accepted  for publication 
and  oral  presentation.  You  are  cordially  invited  to  present  the  paper  orally  at  ICMLC  2011  to  be  held  on, 
February 26‐28, Singapore. 
The  ICMLC  2011  is  sponsored  by  International  Association  of  Computer  Science  and  Information  Technology 
(IACSIT),  Singapore  Institute  of  Electronics  (SIE)  and  IEEE.  The  ICMLC  2011  has  been  listed  in  the  IEEE 
Conference Calendar.   
(Important) So in order to the register the conference and have your paper included 
in the proceeding successfully, you must finish following SIX steps. 
1. Revise your paper according to the Review Comments in the attachment carefully. 
2. Format your paper according to the Template carefully.   
http://www.icmlc.org/ieee.doc    (DOC Format)  
3. Download and complete the Registration Form. 
http://www.icmlc.org/reg.doc    (English) 
4. Finish  the  payment  of  Registration  fee  at  the  Bank.  (The  bank  transfer  information  can  be  found  in  the 
Registration form)   
http://www.icmlc.org/reg.doc    (English) 
 
5. Finish the IEEE Copyright Form 
http://www.icmlc.org/ieeecopyright.doc       
Reducing the Solution of Support Vector Machines Using Simulated Annealing 
Algorithm 
 
Jih Pin Yeh 
Department of Multimedia and Entertainment Science 
Asian-Pacific Institute of Creativity, 
Toufen Township, Miaoli County, Taiwan 35153, ROC 
yehbin@ms.apic.edu.tw 
 
Chiang Ming Chiang 
Department of Computer Science and Information 
Engineering 
Tamkang University, 
Tamsui, Taipei County, Taiwan 25137, ROC 
cmchiangtw@gmail.com 
 
 
Abstract—Support Vector Machines are a relatively recent 
machine learning technique. One of the SVM problems is that 
SVM is currently considerably slower in test phase caused by 
the large number of the support vectors, which greatly 
influences it into the practical use. To address this problem, we 
proposed a simulated annealing algorithm to reduce the 
solutions for an SVM by selecting vectors from the trained 
support vector solutions, such that the selected vectors best 
approximate the original discriminant function. Experimental 
results show that the proposed method can reduce the solutions 
for an SVM by selecting vectors from the trained support 
vector solutions, confirm the theoretical results and improve 
classification accuracy. 
Keywords-Support vector machine; simulated annealing 
algorithm; discriminant function. 
I.  INTRODUCTION  
Simulated annealing is an instance of a probabilistic 
search method. A probabilistic search method is an algorithm 
that searches the feasible set of an optimization problem by 
considering randomized samples of candidate points in the 
set. The simulated annealing algorithm was first presented 
for optimization by Kirkpatrick et al. [9] according to 
techniques of Metropolis et al. [14]. So far, extensions to 
simulated annealing have been developed. One of the 
successful ones is the application to optimization problem 
was described by Geman and Geman [6]. 
Support vector machines (SVMs) were developed by 
Vapnik [17][18], and are winning popularity due to their 
many charming features and their hopeful empirical 
performance. Since the discriminant function of a SVM is 
parameterized by a large set of support vectors and 
corresponding weights, the machine is greatly slower in the 
test phase than other learning machines like neural network 
and decision trees [2][11][13]. Many methods have been 
suggested to address this disadvantage by approximating the 
discriminant function using a reduced set of vectors 
[2][4][7][12][16]. In this paper, we propose an adjustable 
approach for optimally reducing the support vectors 
solutions using a simulated annealing algorithm based on a 
more reasonable fitness. Searching for an optimal subset of 
vectors chosen from the support vector solutions is usually 
speaking a combinatorial optimization for sets of support 
vectors of large size. In our work, we employ a simulated 
annealing based method to search after the optimal subset of 
a given number of support vectors. The subset made by the 
simulated annealing based method can be used to best 
approximate the original discriminant function. The number 
of the optimal support vectors can be selected adaptively in 
accordance with the requirements of the tasks’ need. 
Consequently, with our proposed method the 
generalization/complexity trade-off can be well controlled 
directly. 
II.  SUPPORT VECTOR MACHINES (SVMS)  
The learning of an SVM is formulated as the following 
convex QP problem [1][3][15]. 
Given the training sample 
( ) { }{ }Niidiii yRxyx 11,1,:, =+−∈∈ , find the Lagrange 
multipliers { }Nii 1=α  that minimize the objective function:  
( ) ( )∑ ∑∑
= = =
+−=
N
i
N
i
N
j
jijijii xxKyyQ
1 1 1
,
2
1
αααα           (1)  
subject to the constraints  
∑
=
≤≤=
N
i
iii Cy
1
0,0 αα   for i=1,2,3,…,N              (2) 
where C is a user-specified positive constant and K(•, •) is a 
positive semidefinite kernel function on a sample space X, 
for which there is a Hilbert space H with inner product <•, •> 
and a feature mapping such that 
( ) ( ) ( ) ( )XxxxxxxK ∈′>=<′ ,,,, φφ                            (3) 
Let { }Nii 1=α  be an optimal solution of the above problem, 
then the discriminant function takes the form: 
( ) ( )
( ) ( ) bxxy
bxxKyxf
N
i
iii
N
i
iyi
+><=
+=
∑
∑
=
=
1
1
,
,
φφα
α
                                       (4) 
Note that hold for i = 1, 2,…, N. All training samples 
with 0>iα  are called support vectors. To distinguish 
2011 3rd International Conference on Machine Learning and Computing (ICMLC 2011)
V1-3925978-1-4244-92 3-4 /11/$26.00 ©2011 IEEE 
Therefore, we can apply the simulated annealing technique 
to the combinatorial optimization problem. 
C. Encoding Scheme of Feasible point 
Each feasible point representing a reduced set of m 
vectors is generated by some encoded form : 
mcccC ....21=  , where jc  is a binary digital corresponding 
to the index of a support vector. And let the feasible point 
FC  represent the reduced set FX , that is, 
fitness ( ) ( )FF XFitC '= . 
IV. PROPOSED SIMULATED ANNEALING METHOD 
Simulated annealing (SA) is a generic probabilistic 
metaheuristic for the global optimization problem of applied 
mathematics, namely locating a good approximation to the 
global optimum of a given function in a large search space. 
The basic steps in our proposed SA-based algorithm are 
described as follows: 
Algorithm Reduction of Solutions for SVM 
The main steps of the SA search algorithm are listed as 
follows. 
Step 1. //Initialization 
Set t := 0; 
Randomly select an initial point  SC ∈)0( . 
Step 2.Pick a candidate point )(tz  at random from  )( )(tCN . 
)( )(tCN  is a neighbourhood of )(tC . 
Step 3.If ))(),(,(() )()( tt CFitzFittprandom < , then set 
)()1( tt zC =+  , else set )()1( tt CC =+ . 
 
)}/))()((exp(,1min{
))(),(,(
)()(
)()(
i
tt
tt
TCFitzFit
CFitzFittp
−−=
 
  is the acceptance probability. 
)2log(
6
+
=
t
Ti , the 
call random() should return a random value in the 
range [0,1]. 
 )1(: += tCbest  
Step 4. If the stopping criterion is satisfied, then stop. 
Step 5. Set t := t + 1, goto Step 2. 
V. EXPERIMENTAL RESULT 
Our experiments were implemented on a PC with 2.8 
GHz Pentium Dual-Core processor and 4 GB RAM using 
Borland C++ Builder 6.0 complier. We carried out 
experiments on spirals[10] and cubic polynomials. Each test 
includes randomly generated 3000 samples, 300 of them 
were randomly chosen as training data, and the remaining as 
test data. For this test we set 8=σ  and 10=C for spirals 
and 40=σ and 1=C  for cubic polynomials. And the 
RBF kernel )2/exp(),( 22 σyxyxK −−= is used. 
The recognition rates presented in Tables 1-2 refer to the 
mean of 30 executions of the algorithm. Tables 1 and 2 we 
compare the proposed methods with Li’s method [11] and 
our previous work [12]. The experimental results show that 
our proposed methods can effectively reduce the solutions 
for a SVM. The parametric equations for each of the testing 
examples are given in the following. 
⎩⎨
⎧
++=−
++=−
))sin()14(),cos()14((),(:2
)),sin()104(),cos()104((),(:1
θθθθ
θθθθ
yxSpiral
yxSpiral         (13) 
⎪⎩
⎪⎨⎧
+−+=−
+−+=−
)001.00312.08.1150,(),(:2
),001.0031.02200,(),(:1
32
32
ttttyxCubicpoly
ttttyxCubicpoly          (14) 
TABLE I.  RECOGNITION RATES FOR TWO SPIRALS 
m Reductionrate (%) 
Recognition rate 
Li’s method 
Our previous 
work 
(GA) 
SA 
5 94.8 0.5000 0.7395 0.8270 
10 89.8 0.5000 0.7495 0.8310 
15 84.6 0.5315 0.7535 0.8655 
25 74.4 0.5470 0.9455 0.9675 
n=98 0.0 Recognition rate of the original support vectors =0.9995 
TABLE II.  RECOGNITION RATES FOR TWO WAVEFORM GRAPHS 
m Reductionrate (%) 
Recognition rate 
Li’s method
Our previous 
work 
(GA) 
SA 
5 95.1 0.4880 0.6200 0.6325 
2
0 80.6 0.5185 0.6535 0.6555 
3
0 70.9 0.5805 0.7145 0.7245 
4
0 61.2 0.5600 0.8280 0.8410 
n
=
1
0
3
0.0 Recognition rate of the original support vectors =0.9700 
VI. CONCLUSIONS 
The simulated annealing algorithm was first presented for 
reducing solutions for SVMs. And the approximation error  
Fδ  which, unlike the one defined by Li et al., directly 
reflects the error between the approximating discriminant 
function and the original discriminant function. Experimental 
results have shown that SA outperforms Li’s method and our 
previous work. Further, our proposed method has many other 
advantages. Unlike other methods [2], [16], the vectors in the 
reduced set produced by our method are from the original 
support vectors. The proposed method can be applied on 
SVMs associated with any kernels. Besides, observing from 
our experimental results, we found that although the 
simulated annealing approach is a good strategy to deal with 
the problem of reducing solutions for SVMs, it occasionally 
has the problem of premature convergence; that is, SA 
usually suffers from premature convergence. Future work 
will concentrate on solving these problems. For the problem, 
we shall consider some hybrid approach or other 
2011 3rd International Conference on Machine Learning and Computing (ICMLC 2011)
V1-394
W. Hu (Ed.): Electronics and Signal Processing, LNEE 97, pp. 203–213. 
springerlink.com                     © Springer-Verlag Berlin Heidelberg 2011 
Optimal Reducing the Solutions of Support Vector 
Machines Based on Particle Swam Optimization 
Jih Pin Yeh1 and Chiang Ming Chiang2 
1 Department of Multimedia and Entertainment Science, Asian-Pacific Institute of Creativity, 
Toufen Township, Miaoli County, Taiwan 35153, ROC 
yehbin@ms.apic.edu.tw  
2 Department of Computer Science and Information Engineering, Tamkang University, 
Tamsui, Taipei County, Taiwan 25137, ROC 
cmchiangtw@gmail.com 
Abstract. SVM has been winning increasing interest in areas ranging from its 
original application in pattern recognition to other applications such as regres-
sion analysis due to its noticeable generalization performance. One of the SVM 
problems is that SVM is considerably slower in test phase caused by large 
number of the support vectors, which greatly influences it into the practical use. 
To address this problem, we proposed an adaptive particle swarm optimization 
(PSO) algorithm which based on a reasonable fitness to optimally reduce the 
solutions for an SVM by selecting vectors from the trained support vector solu-
tions, such that the selected vectors best approximate the original discriminant 
function. Experimental results show that adaptive particle swarm optimization 
algorithm is an optimal algorithm to simplify the solution for support vector 
machines and can reduce the solutions for an SVM by selecting vectors from 
the trained support vector solutions.  
Keywords: Support vector machine, Particle swarm optimization, Optimiza-
tion, Discriminant function. 
1   Introduction 
Support vector machines (SVMs) were developed by Vapnik [1], and are winning 
popularity resulting from their many attractive features and their promising empirical 
performance. Since the discriminant function of a SVM is parameterized by a large 
number of support vectors and corresponding weights, the machine is much slower in 
the test phase than other learning machines such as neural network and decision trees 
[1,2,14,16,18]. A lot of approaches have been proposed to address this disadvantage by 
approaching the discriminant function using a reduced set of vectors. Burges [2] and 
Thies et al. [23] each provided an explicit way to build a reduced set of vectors, which 
are usually not support vectors, and their approaches can only be applied effectively to 
SVMs using quadratic kernels. Downs et al. [7] reduced the solution of support vectors 
base on their linear dependency without any loss of generalization, but their reduction 
rate is not high enough. Guo et al. [8] evaluated the contribution of each support vector 
 Optimal Reducing the Solutions of Support Vector Machines 205 
The objective of a support vector machine is to determine the optimal w and op-
timal bias b such that the corresponding hyperplane separates the positive and nega-
tive training data with maximum margin and it produces the best generation perfor-
mance. This hyperplane is called an optimal separating hyperplane. Using Lagrange 
multiplier techniques leads to the following dual optimization problem [4, 6, 20, 21]. 
2.1   Dual Optimization Problem 
Given the training sample { }Niidiii yRxyx 1}1,1{,:),( =+−∈∈ , find the 
Lagrange multipliers { }Nii 1=α  that minimize the objective function  
∑∑∑
= ==
+−=
N
i
N
j
jijiji
N
i
i xxKyyQ
1 11
),(
2
1)( αααα  (5)
subject to the constraints: 
NiCy
N
i
ii ..., ,3 ,2 ,1for      0       0 i
1
=≤≤=∑
=
αα  (6)
where C is a user-specified positive constant and K(•, •) is a positive semidefinite 
kernel function. For any positive semidefinite kernel K on a sample space X, there is a 
Hilbert space H with inner product <•, •> and a feature mapping HX →:φ such 
that  
)',(,)'(),()',( XxxxxxxK ∈><= φφ  (7)
Let { }Nii 1=α   be an optimal solution of the above problem, then the discriminant 
function takes the form:  
bxxKyxf
N
i
iii +=∑
=1
),()( α  (8)
It is apparent from the constraint (6) that Ci ≤≤ α0   holds for i = 1, 2,…, N. All 
training samples ix   such that 0>iα  are called support vectors. To distinguish 
between support vectors with Ci <<α0  and those with iα = C, the former are 
called unbounded support vectors while the latter are called bounded ones [4, 10, 11]. 
In the following, we assume without loss of generality that Ci ≤<α0    for i = 1, 
2,…, n, and iα = 0 for i = n + 1, n + 2, . . . , N. Thus, the discriminant function in (8) 
can be written as follows: 
∑
=
+=
n
i
iii bxxKyxf
1
.),()( α  (9)
From Eq. (9), we can see that if there are more support vectors then the decision 
function will be more complicated and the classification speed of the SVM will be 
 Optimal Reducing the Solutions of Support Vector Machines 207 
∑
∈
=
Sx
FiX
i
iF
δαδ 2
 (12)
Where  
F
T
F
T
iiiF
T
iii
T
ii
iiiiFi
ΦΦ+Φ−=
−=
βαβφαφφα
φαφαδ
vv 222
2
2
~
 (13)
And the coefficient vector iβ
v
 such that  iFi βφ
v
Φ=~   best approximates iφ  is just 
a least squares solution to the linear system βφ vFi Φ= , which has a unique solution 
as given in (14) if ( ) 1−ΦΦ FTF exists. 
( ) iTFFTFi φβ ΦΦΦ= −1v  (14)
3.3   The Selection of Support Vectors Problem 
As we have shown in above, our goal is to find out an optimal reduced subset of sup-
port vectors such that the discriminant function has the minimal error, therefore we 
propose to select optimal vectors to approximate the original support vectors with 
error 
FX
δ . 
In other words, we can think the question as that we want to select m support 
vectors from n support vectors. This problem is a combinatorial optimization 
problem. Therefore, we can apply the PSO algorithm to the combinatorial 
optimization problem, so the selection of support vectors is formulated as the 
following optimal problem. 
SX F
X F
⊂  subject to
Minimize δ
 (15)
3.4   Evaluation of the Approximation Error 
The error 
FX
δ can be used for performance evaluation and its expression given in (12) 
can be expanded as follows: 
))(( 12 iTFFTFFTiiTi
Sx
iX
i
F
φφφφαδ ΦΦΦΦ−= −
∈
∑  
∑∑
∈
−
∈
ΦΦΦΦ−=
Sx
i
T
FF
T
FF
T
iii
T
i
Sx
i
ii
))(()( 122 φφαφφα  
(16)
 Optimal Reducing the Solutions of Support Vector Machines 209 
In the discrete binary version [13], a particle moves in a state space restricted to ze-
ro and one on each dimension, where each ijv  represents the probability of bit ijc  
taking the value 1. Thus, the step for updating ijv  as shown in (19) remains un-
changed, except that ijpbest and jgbest  are integers in {0, 1} in binary case. The 
resulted changes in position are defined as follows: 
( ) ( )   , )exp(1/1
,,
k
ji
k
ji vvs −+=  (21)
( )( ) 0  else  1    then     if
,,,
==< k ji
k
ji
k
ji ccvsτ  (22)
where τ  is a random number drawn from uniform sequence of U(0,1) . 
A discrete particle swarm works by adjusting trajectories through manipulation of 
each coordinate of a particle[13]. In a binary space, the velocity of a particle may be 
described by the change of some bits. Following this idea, we propose a modified 
version of discrete PSO to perform the search of optimal subset of support vectors, as 
described in the following. First, we take the velocity updating formula given in (19) 
and set ωˆ =0, 1θ = 2θ = 1η = 2η =1, to form a tentative formula: 
( )
  2,....,, 21
k
i
kk
i
k
im
k
i
k
i
k
i CgbestpbestuuuU −+==  (23)
Let n1 and n2 denote the number of negative components and the number of posi-
tive components of kiU , respectively, and let n0  = ( )⎡ ⎤2/,min 21 nn .For each compo-
nent k jiu , of kiU , we store its index j into the array 1Γ if 
k
jiu , > 0; otherwise, into the 
array 2Γ if 
k
jiu , <  0. From each of the sets 1Γ and 2Γ , we randomly select n0 elements 
to remain so that =Γ || 1  =Γ || 2 n0. We give the velocity updating formula in (24), with 
which we simply take (19) as the position updating formula. We use T to denote the 
operation transform kiU  into ( )kimkikiki vvvV ,....,, 21=  and denote kiV = T( kiU ). 
⎪⎩
⎪⎨
⎧
Γ∈−
Γ∈
=
otherwise
jif
jif
vk ji
0
1
1
2
1
,
 (24)
Each particle in a swarm, representing a candidate solution, is expressed as a bi-
nary vector as follows: 
( ) , subject to   ,,...,
1
,1 mcccC
n
j
k
ji
k
in
k
i
k
i ∑
=
==
 (25)
where k jic ,  is1, if the j-th support vector is selected and is 0, otherwise. We define the 
fitness for the particle C corresponding to the reduced set XF as fitPSO (C) = fit(XF), 
where the fitness function fit was given in (18). The procedure of the proposed binary 
PSO algorithm for searching the optimal subset of support vectors is described as 
follows: 
 Optimal Reducing the Solutions of Support Vector Machines 211 
Table 1. Recognition rates for two spirals. 
m 
Reduction rate 
(%) 
Recognition rate 
Li’s method Previous work(GA) PSO 
5 94.8 0.5000 0.5030 0.7485 
10 89.8 0.5000 0.7375 0.7895 
15 84.6 0.5315 0.7440 0.8005 
25 74.4 0.5470 0.9300 0.9350 
n=98 0.0 Recognition rate of the original support vectors = 0.9995 
Table 2. Recognition rates for two waveform graphs. 
m 
Reduction rate 
(%) 
Recognition rate 
Li’s method Previous work(GA) PSO 
5 95.1 0.4880 0.4965 0.6215 
20 80.6 0.5185 0.6535 0.6535 
30 70.9 0.5805 0.7045 0.7145 
40 61.2 0.5600 0.7085 0.8290 
n=103 0.0 Recognition rate of the original support vectors = 0.9700 
6   Conclusions 
We have proposed algorithms for reducing solutions for SVMs. Experimental results 
have shown that PSO outperforms our previously work [16] and Li's method [15]. And 
the PSO method is fast converge performance to find the best solution and take advan-
tage of the easily implementing performance. The simulation results also demonstrated 
that the proposed PSO based algorithm can be easily realized with high efficiency and 
can obtain higher quality solution than our previously work [16] and Li's method [15]. 
Although PSO finds good solutions much faster than other evolutionary algorithms, it 
usually can not improve the quality of the solutions as the number of iterations is in-
creased; that is, PSO usually suffers from premature convergence. Future work will 
concentrate on solving these problems. For the problem, we shall consider some a 
more reasonable fitness to optimally reduce the solutions for an SVM or other swarm 
intelligence (SI) technique further improves the effectiveness of PSO. 
References 
1. Boser, B.E., Guyon, I.M., Vapnik, V.N.: A training algorithm for optimal margin classifi-
ers. In: Proceedings of the 5th Annual ACM Workshop on Computational Learning  
Theory, pp. 144–152 (1992) 
2. Burges, C.J.C.: Simplified support vector decision rules. In: Proceedings 13th International 
Conference on Machine Learning, Bari, Italy, pp. 71–77 (1996) 
 Optimal Reducing the Solutions of Support Vector Machines 213 
23. Thies, T., Weber, F.: Optimal reduced-set vectors for support vector machines with a qua-
dratic kernel. Neural Computation 16, 1769–1777 (2004) 
24. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer, New York (1995) 
25. Vapnik, V.N.: Statistical Learning Theory. Wiley, New York (1998) 
26. Yeh, J.P., Pai, I.C., Wang, C.W., Yang, F.W., Lin, H.J.: Face Detection using SVM-Based 
Classification. The Special Issue of Far East Journal of Experimental and Theoretical Ar-
tificial Intelligence (FEJETAI) 3(2), 113–123 (2009) 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：葉日斌 計畫編號：99-2221-E-243-003- 
計畫名稱：使用混合最佳化技術化簡支撐向量機之解 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
