to support 3-D scene inference. 
Based on this BHF framework, we further propose a multi-view 
face detection system, which is capable of detecting all targets’ 
faces in the given images and is able to illustrate the bird-eye view 
direction of each face in the 3-D space in a multi-camera 
surveillance system. Unlike existing approaches, the proposed 
system does not directly detect targets over the 2-D image domain 
nor project the 2-D detection results back to the 3-D space for 
correspondence. Instead, our system searches for the targets over 
small cubes in the 3-D space. Each searched 3-D cube is projected 
onto the 2-D camera views to determine the existence and direction 
of human faces. This approach can help us to efficiently combine 2-
D information from different camera views and to suppress the 
ambiguity caused by 2-D detection errors. 
 
 
主動式多攝影機視訊監控系統之研究 
Visual Surveillance System with Multiple Active Cameras 
計畫編號：NSC 97-2221-E-009-132-MY3 
執行期限：99年8月1日至100年7月31日 
主持人：王聖智 (交通大學電子工程系教授) 
計畫參與人員：黃敬群、戴玉書、陳柏翔、宋秉修(交通大學電子所研究生) 
 
中文摘要 
 在本計畫中，我們提出以貝氏階層式結構
為基礎的分析方法，讓視訊監控系統得以用一
致的架構，同時分析影像內容以及推論空間中
場景的資訊。在真實的場景中，為了實現一套
穩健的視訊監控系統，往往會面臨許多挑戰，
諸如物體間相互遮蔽、前景物體與背景物體外
貌相似而產生的混淆、透視投影所造成的物體
形變、陰影的變化、還有外在光線變化造成的
影像變異。透過將空間場景適當的參數化，並
同時依據場景模型和擷取到的影像資料來進行
分析，系統將能更輕易地處理前面所提及的變
異因素。在貝氏階層式架構中，我們透過階層
式表示法將以像素特徵為基礎的資訊、以區域
影像內容為基礎的資訊、與以物件特性為基礎
的資訊，透過機率的方式進行有系統的整合，
以支援影像內容的分析與場景資訊的推論。透
過所提出的貝氏階層式架構，前面所提到的許
多變異因素可以被有效地解決，除此之外，某
些變異因素還可進一步變成有效的線索來協助
三維場景資訊的推論。 
基於這樣的階層性結構，我們提出一套應
用於多台攝影機之多角度人臉偵測系統。此系
統可根據多攝影機擷取的影像偵測出影像中的
人臉位置，並得到在三維空間中人臉方向的鳥
瞰圖。有別於以往的作法，我們並不在二維的
影像中直接作搜尋與偵測、或是將這些在二維
的影像中的偵測結果投影到三維空間中作結
合；反之，我們的系統直接在三維空間中進行
搜尋，並將三維空間投影回二維影像中加以比
對處理，以判斷在這個三維空間中是否存在著
某種方向的人臉。這樣的做法使得我們的系統
可以有效地結合多攝影機中的二維影像資訊，
並可避免以往因在二維影像中的錯誤偵測所導
致的不明確資訊整合。 
關鍵詞：影像標記、圖學模型、物件偵測、物
件追蹤、影像切割。 
Abstract 
In this project, we present a Bayesian 
hierarchical framework (BHF) to simultaneously 
deal with 3-D scene modeling and image analysis 
in a unified manner. In practice, to develop a 
robust video surveillance system, many 
challenging issues need to be taken into account, 
such as occlusion effect, appearance ambiguity 
between foreground and background, perspective 
effect, shadow effect, and lighting variations. 
Here, we find a way to handle these challenging 
issues by modeling 3-D scene in a parametric 
form and by integrating scene model and image 
observation together in the inference process. In 
the proposed hierarchical framework, we 
systematically integrate pixel-level information, 
region-level information, and object-level 
information in a probabilistic way for the 
semantic inference of image content and 3-D 
scene status. Under this BHF framework, 
occlusion effect, appearance ambiguity, 
approach is used to efficiently fuse consistent 2-D 
foreground detection results from multiple 
camera views. Here, we formulated a posterior 
distribution, named target detection probability 
(TDP), as the message pool to indicate the 
probability of having a moving target at a certain 
ground location. With the TDP distribution, 
candidate targets and their locations can be 
identified in a probabilistic manner. Moreover, 
with the use of 3-D target model, our fusion 
scheme may work well even with imperfect 
foreground extraction.  
After data fusion, a set of candidate targets 
are detected, including both true targets and ghost 
targets. In our system, we use a few 3-D priors 
about the surveillance scenario, such as the 
probability distributions of target height and 
target location, to distinguish true targets from 
ghost targets. By properly integrating these priors 
into the scene knowledge, we can greatly simplify 
the ghost suppression problem. Moreover, in the 
BHF framework, we introduce a labeling layer as 
the interface between scene knowledge and 
multi-camera observations. This 3-layer 
framework unifies the target labeling, target 
correspondence, and ghost suppression into a 
Bayesian inference problem. Besides the 
intermediate role in the hierarchical framework, 
the labeling layer also provides a feedback route 
to refine the scene knowledge based on an EM 
(Expectation-Maximization) mechanism.  
2. INFORMATION FUSION AND 
SUMMARIZATION 
A. Foreground Detection on Single Camera 
To fulfill the speed requirement of a 
real-time multi-camera system, we only consider 
2-D foreground detection results as the 
observation data. For each camera, we build its 
reference background based on the Gaussian 
mixture model (GMM) approach [3]. To remove 
shadows, the frame difference operation is 
performed over the chromatic domain.  
B. Information Fusion 
To improve the accuracy in the estimation of 
target location, we adopt a model-driven approach 
to fuse 2-D information. In the proposed method, 
we define a Target Detection Probability (TDP) 
distribution to estimate the probability of having a 
moving target at a ground location, as expressed 
below: 
);|,,()(~);,,|()( 11  XFFpXpFFXpXG NN 
(1) 
In (1), X represents a location (x1,x2) on the 
ground plane of the 3-D space. N is the total 
number of cameras in the multi-camera system. Fi 
denotes the foreground detection result of the ith 
camera view.  defines the set of camera 
parameters of all N cameras.  
To define Fi, we use (m,n) to represent the 
2-D coordinate system of the ith camera. Based 
on the foreground detection result on the ith 
camera view, we define Fi as 
1       ( , )   ( , )     
( , ) 0       ( , )   ( , )   
      ( , )                                                   
i
L
if m n V and m n  foreground regions
F m n if m n V and m n  background regions
P if m n V
    
(2) 
Moreover, given the location X, we assume the 
foreground detection results are conditionally 
independent of each other.  With this assumption, 
we rewrite (1) as 



N
i
iN XFpXpXFFpXp
1
1 )|()()|,,()(     (3) 
To formulate p(Fi|X), we model a moving person 
at the ground position X as a rectangular pillar, as 
shown in Figure 2. The height H and width R of 
the pillar are modeled as independent random 
1 1
, , , ,
0 0
( ( )( ) ) ( )
s sR R
k k k T
k j k j k j k j
j j
W X X W 
 
 
   C  
(9) 
D. Ghost Object 
From time to time, ghost clusters may occur 
in the TDP distribution. Geometrically, the ghost 
effect happens when the projection of a 
rectangular pillar at an incorrect location 
accidentally matches the foreground detection 
results on the camera views. In Figure 4, we 
present an illustration of the ghost problem when 
trying to reconstruct the 3-D scene based on two 
camera views.  
 
Figure 4. Illustration of the ghost problem. 
 
3. BAYESIAN INFERENCE AND GHOST 
SUPPRESSION 
After information summarization, we 
identify a few candidate targets and their 
locations. For each candidate, we have to decide 
whether its status is “true” or “ghost”. To 
determine the status of candidate targets, we 
consider not only foreground observations and 
geometric consistence but also some helpful prior 
knowledge about the targets. 
A. System Modeling 
A.1. Bayesian Hierarchical Framework 
Here, we propose a 3-layer Bayesian 
hierarchical framework (BHF) to simultaneously 
infer the status of candidate targets. In Figure 5, 
without loss of generality, we consider an 
example of TDP distribution fused from four 
camera views. The top layer of the BHF 
architecture is the scene layer SL that indicates the 
3-D scene knowledge built at the fusion stage. 
The bottom layer is the observation layer OL, 
which contains both the original images and the 
foreground detection results. Here, we define 
Ii(m,n) and Fi(m,n) as the original image and the 
foreground detection result of the ith camera view. 
The value of Fi(m,n) is defined as in Equation (2). 
Between the scene layer and the observation layer, 
a labeling layer LL is inserted to deal with image 
labeling, target correspondence, and ghost 
removal. Here, we define Li(m,n) as the labeling 
image of the ith camera view. 
 
(a) 
 
Scene Layer 
Labeling 
Layer 
Observation Layer 
 
x2 
x1 
s4 
s2 
s1 
s5 
s3 
 
(b) 
Figure 5. (a) An example of TDP distribution fused 
from four camera views. (b) The corresponding 
Bayesian hierarchical framework. 
 
EA[Ii(m,n),Li(m,n);Np] denotes the “adjacency 
energy” that relates the ith original image with 
the ith labeling image by checking the adjacent 
property within the neighborhood Np. 
On the other hand, we define 
ED[Fi(m,n),Li(m,n)] as 
))]},((),,([1{)),(),,(( nmLTnmFnmLnmFE iiiiD  
(13) 
with T(Li(m,n)) being defined as 

 
     otherwise         1
),( if     0
)),(( 0
TnmL
nmLT ii         (14) 
and [pa,qa] being defined as 
1     if    
[ , ]
0     otherwise
a a
a a
p q
p q  
             (15) 
In our system, by taking the original image Ii(m,n) 
into consideration, we define the adjacency 
energy EA[Ii(m,n),Li(m,n);Np] based on a Markov 
random field  to provide a smoothness constraint 
between adjacent labeling nodes [5]. Here, we 
define 
 ],,,,,[
]);,(),,([
nmnmLIC
NnmLnmIE
ii
p
pm
p
pn
A
piiA
  
 

    (16) 
where 
)),(),((    
)]),(),,([1(
 ],,,,,[
nnmmInmIG
nnmmLnmL
nmnmLIC
iiS
ii
iiA



   (17) 
In (16), Np denotes the (2p+1)(2p+1) 
neighborhood around (m,n), and  is a learned 
penalty constant whose value is to be determined 
later. In (17), [pa,qa] is defined as in (15). In our 
system, we design GS(U) to be a discriminative 
function similar to a logistic sigmoid function: 
  ( - ) ( - )( ) 1 (1- ) (1 ) 1th thU C U CSG U Sigm U e e       
(18) 
Here, Sigm(U) outputs a positive value if U is 
smaller than Cth, and outputs a negative value 
otherwise. With this design, CA[.] is equal to zero 
when Li(m,n) and Li(m+m,n+n) are the same. If 
Li(m,n) and Li(m+m,n+n) are different, CA[.] 
gives a larger penalty if the difference between 
Ii(m,n) and Ii(m+m,n+n) is smaller than Cth. 
Hence, Li(m,n) and Li(m+m,n+n) tend to share 
the same label when the difference between Ii(m,n) 
and Ii(m+m,n+n) is small, and tend to have 
different labels otherwise.  
  
Figure 7.  Examples of p(Li(m,n) = Tk | S) 
A.4. Formulation of p(L|S) 
Given a status combination S, we define a 
conditional probability p(Li(m,n)=Tk| S) to express 
the likelihood of having a label Tk at the pixel 
(m,n) of the ith labeling image. Here, with the 
status combination S, we define a few rectangular 
pillars on the ground. The height and width of 
each pillar are sampled from p(H) and p(R). The 
locations of the pillars are sampled from p(X|Tk), 
where Tk indicates the kth target. With the camera 
projection parameters, the expected foreground 
patterns for each target can be generated by 
projecting these rectangular pillars onto each 
camera view. Occasionally, more than two targets 
may project onto the same image region and 
cause occlusion. The inter-occluded patterns can 
be determined by checking the distance from the 
camera to the mean location of the targets. In 
Figure 7, we demonstrate the occlusion effect by 
plotting p(Li(m,n)=Tk| S) individually for each of 
the four targets in Figure 5(b). 
Based on the definition of p(Li(m,n)=Tk| S), 
we define the log probability function ln[p(L|S)] 
as 
is then solved by using a Linear Programming 
method. 
B.3. Optimal Status Inference and Target 
Labeling 
For each status hypothesis SH, we deduce the 
optimal L that maximizes the potential function 
Cp(L,S=SH) in (22). If we treat 
ED(Fi(m,n),Li(m,n)), p(Li(m,n)|S=SH), and p(S=SH) 
as data terms and treat EA[Ii(m,n), Li(m,n);Np] as a 
smoothness term, Cp(L,S=SH) actually follows a 
canonical form that can be maximized based on 
many existing optimization algorithms [5-7]. 
Based on a recent study, the graph cuts method is 
proved to be more efficient in terms of running 
time as compared to loopy belief propagation, 
tree-reweighted, and iterated conditional mode 
algorithms [7]. Hence, in our system, the graph 
cuts algorithm [8-10] is used for the 
maximization of Cp(L,S=SH). 
In our system, the optimal image labeling 
under SH are achieved by assigning to each pixel 
a suitable ID from the set {T0, T1, …, TM}. Based 
on the graph cuts theory [8-10], we form a graph 
in Figure 8 to represent our optimization problem. 
In this graph, a candidate target, say T1, can only 
affect a portion of labeling nodes in the labeling 
image. Through the projection of the 3-D 
candidate target onto the ith camera view, the 
relation is represented by a collection of “t-links”. 
In our system, we combine ED(Fi(m,n),Li(m,n)), 
p(Li(m,n)|S=SH), and the prior p(S= SH) as the 
data term to define the weight of each t-link. 
Moreover, the “n-links” in the graph represent the 
smoothness term, which is modeled as 
EA[Ii(m,n),Li(m,n);Np]. After forming this graph, 
our optimization problem is equivalent to the 
cutting of the t-links and n-links with the minimal 
cost so that all terminals are separated and each 
labeling node Li(m,n) only connects to one 
terminal through a t-link. 
 
Figure 8. The graph cut model for the optimal labeling. 
Moreover, in the graph cuts algorithm, the 
initial guess of L is obtained by finding the 
labeling image of each camera view that 
maximizes the probability function in (19) under 
the status hypothesis SH. That is, we find the 
initial labeling image ( , )iniiL m n  of the ith camera 
view such that 
( , ) arg max ( ( , ) | ).
i
ini H
i iL m n
L m n p L m n S S   
(24) 
Among all status hypotheses, the status 
hypothesis that achieves the maximum posterior 
probability is picked as the optimal status 
combination S*. The optimal labeling of S* is then 
inferred as the optimal labeling L*. 
B.4. 3-D Target Model Refinement 
In our system, the 3-D model of each target 
is a pillar model with parameters height (H) and 
width (R) standing at a location X on the ground 
plane. However, different targets may have 
different heights and widths. In our system, we 
treat these model parameters as latent random 
variables and introduce an EM-based algorithm to 
iteratively refine the parameters. 
Initially, the proposed EM algorithm adopts 
the pre-trained probability distributions p(H) and 
p(R) to model the uncertainty of each target’s 
height and width. Since the BHF framework 
where λ is a normalization constant to make 
the probability summation equal to 1. In (27), x, y, 
and z are the weighting parameters with the value 
5, -3, and 0, respectively, which satisfies the 
relation x > z > y. If we rewrite (24) based on (25), 
we get a likelihood form as follows 
0
1( | ) exp{ ( )}r rk k otherp L Q x N y N z NN
        
(28) 
where Nk, is the number of Tk-labeled pixels, N0 is 
the number of T0-labeled pixels, and Nother is the 
number of the other pixels inside the projected 
regions in all camera views. Basically, (28) 
measures the degree of matching by accumulating 
the weighted sum of different labeling pixels 
inside the projected regions with the weighting 
parameters (x,y,z). Once the likelihood term 
( | )r rkp L Q  is determined, the refined probability 
distribution of the height and width of the kth 
target height in the current iteration can be 
obtained based on (25). The refined models 
 ( | )r rkp H L and
 ( | )r rkp R L  are inputted to the 
proposed BHF for the next iteration of the 
optimal object labeling. In our experiments, it 
usually takes only 2 to 3 iterations to construct 
the refined target model. 
4. MULTI-VIEW FACE DETECTION 
FRAMEWORK 
After the position estimation, we identify the 
ground locations X of detected candidate targets 
on the 3-D ground. However, the head location of 
each candidate is still unknown. Even so, the 
extracted locations are useful for speeding up 
head finding. In next subsections, we aim to 
search head positions and determine the face 
directions. We will formulate the problem and 
explain our multi-view face detection framework. 
 
Figure 10. The sliding cube in 3-D space. 
A. Finding Target Heads and Face 
Directions 
Many detection algorithms are already 
proposed for multi-view face detection. Most of 
these methods based on some learning approaches 
to train suitable detectors. After the training 
process, the trained detectors are able to detect 
specific object based on the sliding window 
approach in the image. However, because of some 
reasons, this sliding window approach in 2-D 
image may not be suitable for our application. 
First, it would not be easy to train a high accuracy 
detector for all face views. Second, from time to 
time, we need to search all scales to detect faces 
with different sizes including very small faces in 
the image. These small faces are usually too small 
to correctly identify. Also, the heavy searching 
time is unwelcome. Third, there could be some 
occlusions in the scene. Sometimes, we may need 
to detect faces that are incompletely observed in 
the image view. Due to the above reasons, 
detecting face directly in 2-D images usually 
generates many inevitable false detection and 
false rejection. 
A multi-camera system may provide us more 
information about the scene and could 
theoretically decrease the false positive rate and 
increase the detection rate. However, the 
performance of the multi-camera system depends 
heavily on the way we utilize the 3-D geometric 
information. A conventional way is to detect faces 
in each 2-D image and then the 2-D detection 
  |
( , ) arg max ( , | , , , ). 
i i
Ti
T T ih H l L
h l p h l D I X C 
 

  (29) 
To numerically analyze this optimization problem 
in (29), we uniformly quantize the solution spaces 
of 3-D head positions and face orientations and 
denote the spaces as L and H respectively. In our 
system, the interesting 3-D space L, bounded by 
the surveillance zone and a user-defined height 
200cm, is divided into 100x100x50 cubes. The 
orientation space H, ranging from zero to 360 
degree, is divided into eight face directions. We 
also define other notations in (29) as below: 
(D): The set of eight image-based face classifiers 
pre-trained for different face orientations.  
(I): The set of multi-camera image views. 
( iX ): The ground location of candidate target Ti. 
(C): Camera layout and geometry information.  
( |
iT
L ): The possible 3-D head positions of 
candidate target Ti. 
Here, | iTL  needs to be detailed. In our system, 
TN
X  indicates the set of estimated ground 
positions of the NT detected targets in the 3-D 
space and could be utilized to reduce the solution 
space of the head locations. We Assume Xi = {xi, 
yi, 0}, where (xi, yi) is the ground position of the 
ith detected target. If we know the mean of 
human height is z0 beforehand, we can reduce the 
interesting 3-D head positions of the target Ti, and 
define the reduced space | iTL  as 
0 0
2 2
| ( , , )  .    2 2
2 2
i
i i
T i i
s sx x x
s sL x y z y y y
s sz z z
                   (30) 
In (30), s defines a search range and is 
determined by the average size of a 3-D head. In 
our system, we set s as three times of the average 
size in order to account for the uncertainty. Also 
in (8), the average 3-D human height z0 is 
obtained through statistical training. In Figure 12, 
we illustrate the reduced position space given the 
plane location (x0, y0, 0) of the first detected 
target. 
To solve (29), we still need to define the 
calculation of p(h,l|D,I,Xi,C). In detail, for each 
hypothesis (h,l) in the 3-D space, we project a 
cube at 3-D location l onto 2-D images to locate 
the focused patches and also generate the 
expected face orientations in different camera 
views based on h, I, and C. Here, we use Equation 
(31) to expresses the 3-D to 2-D projection 
process, where the function B(.) projects the 3-D 
cube and generates the expected face direction; 
In,ED,l indicates the projected image patch with the 
expected face direction EDn in the nth camera 
views of our four-camera system 
, , ( , | , )  |   1,2,3,4  in ED l TI B l h I C l L n     
(31) 
For each camera view, we based on the 
expected face direction ED to select the 
corresponding face classifier from the classifier 
set D. By feeding the image patch In,ED,l into the 
selected classifier, we could evaluate the 
likelihood pn,l,h of the hypothesis (h,l) based on 
information form this camera view. This process 
could be defined as 
, , , ,( ; )     1,2,3,4.    n l h n ED l np D I ED n  (32) 
 
Figure 12. The reduced position space given a detected 
target location (x0, y0, 0). 
for every other 25 frames. To see the details of 
our experimental results, please visit our website 
[16]. 
To understand the process of head localization 
and multi-view face detection in our system, we 
show the projected windows under different 3-D 
location hypotheses in the four camera views. In 
Figure. 14(a), owing to a correct 3-D location 
hypothesis, the projected windows match the face 
regions well, while in Figure 14(b), the projected 
windows shift away from the correct face regions 
due to the wrong 3-D location hypothesis. To 
quantitative compare, we draw the calculated 
likelihood values under both the correct 3-D 
location and the wrong 3-D location over eight 
hypotheses of face orientations as shown in 
Figure. 14(c). Please note the blue curve, 
indicating the values under the correct location, is 
always higher than the green curve, representing 
the values under the wrong location. Also, the 
largest likelihood value over the blue curve 
indicates the optimal hypothesis of face 
orientation.  
We tested our system over the video sequence 
provided by Fleuret’s work [17]. Note the 
sequence contains more than 2000 frames. To 
quantitatively evaluate the detection and 
correspondence performance, false positive rate 
(FPR) and false negative rate (FNR) are used. In 
our system, the target detection and 
correspondence are defined as “correct” when the 
projected regions of the detected target in all 
camera views intersect the same individual and 
the detected face directions match the ground 
truth. Based on this definition, the calculated FPR 
and FNR of all tested sequence are 0.065 and 
0.023. 
We also show some detection results in 
Figure. 15. To clearly present our outputs, we use 
bounding boxes with different colors to indicate 
different targets. We also mark the detected face 
direction onto the bird-eye view of the 
surveillance zone. In this example, there are two 
persons in the scene. As shown in the figure, our 
system can detect faces and identify the face 
directions even if some serious occlusion occurs 
or someone is out of image view. In Figure. 15(a), 
there is an occlusion case in the top-left image 
and there is a missing person in the lower-right 
image. For this example, our system can still find 
the approximate locations of the faces and the 
face directions, as shown in Figure. 15(b). 
Another experimental result is illustrated in 
Figure. 15(c-d). 
 
Figure 14. (a) Detection result at a correct 3-D 
position. (b) Detection result at an incorrect 3-D 
position. (c) The blue line corresponds to the likelihood 
values of eight hypotheses of face orientations at the 
correct position, and the green line corresponds to he 
likelihood values of eight hypotheses at the incorrect 
position. 
計畫成果自評 
在本計畫中，我們驗證了以貝氏階
層式結構為基礎的影像分析架構可以
有效地應用到視訊監控的分析與應用
上。透過此架構，我們將像素層級的色
彩資訊、像素間的區域層級資訊、以及
以物體為基本單位的物件層級資訊有
系統地整合在一起，這樣的整合讓系統
可以擁有更多的資訊，並可以針對較複
雜的影像內容進行準確的推論分析。 
在多攝影機視訊監控系統中，我們
自動地定位、標記、與對應在不同攝影
機監控範圍內的多個物體，同時有效壓
抑因為幾何深度上的不確定性所產生
的假物體。多攝影機視訊監控系統在真
實的應用場景中，往往面臨一些具挑戰
性的議題: (a) 場景中未知物體的數量; 
(b)物體間的相互遮蔽; 以及(c)假物體
的出現。有別於過去的方法，我們提出
了一套包含資訊整合與場景推論的兩
步驟策略。在資訊整合的步驟中，我們
整合來自多攝影機的資訊以建立一機
率分佈，藉以描述物體出現於地面某一
位置的可能性。在場景推論的步驟中，
我們應用貝氏階層式結構將場景模型
納入考量，透過此結構，我們將物件在
影像內的標記議題、物件在多攝影機間
的對應議題、以及假物件的消除議題整
合為單一的最佳化問題。此外，我們進
一步採用期望-最大化架構來調整出更
好的物體三維模型，透過貝氏階層式結
構與期望-最大化架構的結合，我們可
以得到更好的系統效能。實驗結果顯
示，我們的系統可以自動地決定場景中
的運動物體數量、有效地標記並對應出
不同攝影機影像中的多個物體、準確地
定位物體在三維場景中的位置、並且能
有效地清除假物件。 
同時我們以此貝氏階層式結構為基
礎。提出一套應用於多台攝影機之多角
度人臉偵測系統。此系統可根據多攝影
機擷取的影像偵測出影像中的人臉位
置，並得到在三維空間中人臉方向的鳥
瞰圖。實驗結果顯示，我們的系統在物
體相互遮蔽，以及前景區域與背景區域
因為外貌相似而混淆的情況下，仍然可
以得到好的效果。 
   此為三年期計畫，若包含前兩年之
執行成果，本計畫在三年期間一共完成
以下數篇學術論文。 
 
1. 國際期刊論文2篇 
 Hsien Chen and Sheng-Jyh Wang, "An 
Efficient Approach for Dynamic Calibration 
of Multiple Cameras," IEEE Transactions on 
Automation Science and Engineering, VOL. 
6, NO. 1, pp. 187-194, Jan. 2009. 
 Ching-Chun Huang and Sheng-Jyh Wang, 
“A Bayeisan Hierarchical Framework for 
Multitarget Labeling and Correspondence 
with Ghost Suppression over Multicamera 
Surveillance System”, to appear on IEEE 
Transactions on Automation Science and 
Engineering.  
 
2. 國際研討會論文5篇 
 Ching-Chun Huang and Sheng-Jyh Wang, 
“A Monte Carlo Based Framework for 
Multi-Target Detection and Tracking Over 
Multi-Camera Surveillance System,” in 
Proceedings of the 10th European 
Conference on Computer Vision (ECCV) 
Workshop on Multi-camera and Multi-modal 
Sensor Fusion Algorithms and Applications, 
Marseille, France, Oct. 2008.  
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
