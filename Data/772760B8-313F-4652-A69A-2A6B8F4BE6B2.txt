II 
英文摘要 (Abstract) 
Finding an efficient data reduction method for large-scale problems is an 
imperative task. In this project, we propose a similarity-based self-constructing fuzzy 
clustering algorithm to do the sampling of instances for the classification task. 
Instances that are similar to each other are grouped into the same cluster. When all the 
instances have been fed in, a number of clusters are formed automatically. Then the 
statistical mean for each cluster will be regarded as representing all the instances 
covered in the cluster. This approach has two advantages. One is that it can be faster 
and uses less storage memory. The other is that the number of new representative 
instances need not be specified in advance by the user. Experiments on real-world 
datasets show that our method can run faster and obtain better reduction rate than 
other methods. 
Keywords: Large-scale dataset, fuzzy similarity, data reduction, prototype reduction, 
instance-filtering, instance-abstraction. 
2 
資料來當代表點，因此這個方法可以避免為了找出最適合的資料點數目而做的
錯誤嘗試。在實驗中我們以多組真實的資料集做測試，測試的結果顯示出我們
的方法相較於其他的方法在速度上和資料縮減率上更有優勢。 
二、文獻探討 
在 做 資 料 分 類 時 ， 我 們 先 給 定  筆 已 標 記 類 別 的 訓 練 資 料 集
)} ,( ..., ), ,( ), ,{( 2211  yyy xxxtrS ， 其 中 訓 練 資 料 ] , , ,[   ,2 ,1 , niiii xxx x ，
 ..., 2, 1,  i ， n 為資料的特徵數，資料類別以 iy 表示， } . . . , 2, {1, ki y 。
在一般的分類問題中，我們會先拿訓練資料 trS 對分類器做訓練，使的
分類器可以對未知的資料做較準確的預測。而資料縮減的工作則是從
原 始 的 訓 練 資 料 中 產 生 一 組 數 量 較 小 的 新 的 訓 練 資 料 集
)} ,( ..., ), ,( ), ,{(  2211 J
'
J
'''
tr yyyS xxx ， J ，並以
'
trS 取代 trS 對分類器進行
訓練，且期望 'trS 對未知的資料有和 trS 一樣好的預測效果。如果 J 遠小
於  ，則以 '
tr
S 當訓練資料可大大減少運算時所需耗費的資源。目前的
資料縮減方式大致可分為兩種類型 :  (1)資料過濾法  (2)資料萃取法。
在接下來的內容中我們將簡短的介紹幾種和我們的方法比較的資料
萃取技術。  
2.1 資料過濾法 
資料過濾法的概念是從原本的訓練資料
trS 裡挑選部分資料組成
新的訓練資料集 )} ,( ..., ), ,( ), ,{(  2211 J
'
J
'''
tr yyyS xxx 。分層抽樣 (stratiﬁed 
sampling)是一個統計上簡單且常見的資料過濾法，它從每一個類別
jC 裡平均且隨機的抽取部分資料 jh ， kj  , 2, 1,   ， khhhJ         21  ，
此種方法的優點是不需做額外的運算，缺點則是此方法未考慮到資料
的分布特性。  
另一種資料過濾的方法， DROP3 [7]是我們比較的方法之ㄧ，
DROP3 一開始必須先對所有的資料做距離的運算，最後的輸出結果
為 'trS 且 tr
'
tr SS    。DROP3 演算法可分為三個步驟，首先第一個步驟是
先對所有資料做 k-NN，並拿掉預測錯誤的資料，在這個步驟中會將
雜訊和邊緣部份的資料移除。接下來是計算每筆資料與不同類別資料
的最短距離，並以此距離做資料排序的依據，此步驟的目的是希望在
下一個步驟處理資料時先處理距離類別邊界較遠的資料。最後一個步
驟是測試每一筆資料對分類結果的影響，測試的順序則依照前一個步
驟排序後的結果。假若移除某筆資料 x 會導致整體準確率下降則將此
筆資料保留。相反的，則移除此筆資料。DROP3 的優點是在做分群
時有考量到資料的分布特性，且此種方法不用事先決定要取多少代表
資料，缺點則是此方法是以 k-NN 為基礎，因此有運算量過大及較花
時間的問題。  
2.2 資料萃取法 
4 
續型態的特徵， ] ,… , ,[  ,2 ,1 , 11 npnpnp xxx  表示的是 px 離散型態的特徵。每
一群我們則用
Jc , ,c ,c 21  來表示，而群 jc 的中心 (平均值 ) jm 表示為
] ,… , ,  ,… , ,[ = m  ,1+  , ,2 ,1 , njnjnjjj mmmmmj ，標準差 ] ...,  ,[  1n j,2 j,1 j, σσσj  ， js 表
示 jc 群所包含的資料數量。在初始化時，我們定 0  J 表示一開始不存
在任何群，接著讀入第一筆訓練資料，並將它定為第一群
1c 且 11   m x ，
1c   1
yy  ， 01     ， 1  1 s ，在這裡 0 是由使用者定義的一個常數向量
] ...,  ,[  
1n 0,2 0,1 0,0
σσσ 。此時第一群產生， 1  J 。  
Input Layer Layer 1 Layer 2 Layer 3 Layer 4 Output Layer











Cluster 1
Cluster J
C
O(5) = 1: Refine existing cluster
O(5) = 0: Create a new cluster
Assign cluster label
d
i
s
c
r
e
t
e
c
o
n
ti
n
u
o
u
s
P
P
x1










 xn
y
xn1
xn +11
(2)
1O
(2)
jO
(2)
JO
(3)O
(1)
,1JO
(1)
1,1O
(1)
1,nO
(1)
,j iO
(4)O (5)O
(1)
,J nO
 
圖 3.1: 自建構式模糊分群網路 
第一層 :此層執行模糊化運算，每一個節點表示一群，節點的數量是
自動決定的。要注意的是，如果訓練資料集
trS 的資料型態包含離散型
態和連續型態兩種特徵，則歸屬函數必須分開來運算。在計算連續型
態的特徵上，我們使用高斯函數來運算會比用其他函數的效果較好錯
誤 ! 找不到參照來源。。在離散型態特徵的運算上，我們以模糊單值
(fuzzy singleton)的方式來表示。此層連續型態特徵的歸屬值如下 : 

















2
 ,
 , ,(1)
 ,
 
-exp  
ij
ijip
ij
σ
- mx
O  (1) 
6 
1 
  
  
 , ,
 ,



a
ipiaa
ia
s
xms
m  (9) 
i  = 1, 2,  …,  n1，  
i
aa
iaipaiiaa
ia
ss
mxss
 0,
2
 , ,
2
 0, ,
2
 ,  
1) (
)-( )-1)(-(
  

 


  (10) 
i 0, 為使用者定義的初始標準差， 1 , 2, 1,  ni  ，且  
1    aa ss  (11) 
在 1  (5) O 的情況下 am 的離散型態特徵和 J 是不會改變的。 
如果 0  ( 5 )O 表示 px 和所有的群皆未通過相似度的門檻值。在這種情形
下我們認定 px 和現有的群皆不相似，因此我們新增新的一群 1c J 且  
01 1    ,   m    JpJ x  (12) 
此時 px 被判定屬於 1c J 群，並對 1c J 的資料量大小 1Js 做初始化  
1  1 Js  (13) 
而總群數也增加了 1 
1   JJ  (14) 
前面的過程顯示出了 SCFC 處理資料的方式，一次處理一筆資料
直到所有的資料都處理完畢，最終我們會得到資料總群數 J 以及，以
及新的一組訓練資料集 )} ,( ..., ), ,( ), ,{(  2211 J
'
J
'''
tr yyyS xxx ，
'
jx 為 jc 中心點
(平均值)， Jj  , 2, 1,  。當資料完成分群，屬於同一群資料間的相似度會比其
他群的資料高。此外，不論是要更新已存在的群或是新增一群，都不需要拿同一
群裡所有的資料來計算。 
四、結果與結論 
為了驗證我們所提出的方法，我們使用由 MIT Lincoln Labs 提供的
1998 DARPA 入侵偵測資料集錯誤 ! 找不到參照來源。 : 1999 KDD 
Cup 作測試。1999 KDD 的資料集裡包含了 4,898,431 筆訓練資料和
311,029 筆測試資料。不論是訓練或測試資料集裡都包含了正常型態
的 網 路 傳 輸 資 料 (Normal) 以 及 四 種 主 要 的 攻 擊 型 態 資 料 :  
Denial-of-Service(DoS) ， Probing(Prob) ， User-to-Root (U2R) 以 及
Remote-to-Local (R2L)。每一筆資料以 34 個連續屬性和 7 個離散屬
性表示。1999 KDD 資料集的分佈極不平均，約有將近 20%資料屬於
normal 類別， 80%資料屬於 DoS 類別。所有的資料集在做資料縮減
之前我們先將連續屬性的資料正規化到 -1 至 1 之間。  
在本實驗裡我們將比較我們所提出的方法和其他的資料縮減方
法的效果。由於分層抽樣 (SS)和 k-means 必須事先決定所要取的代表
資料數量，因此我們以 SCFC 的數量為標準，SS 和 k-means 每一類別
的代表資料數量皆依照 SCFC 的數量來取。而 RSP3 和 DROP3 這兩
種方法則不必事先決定代表資料的數量。實驗的結果整理在表 2，其
中列出了縮減率、執行時間以及準確度。由於 SS 的結果會受到抽樣
8 
表 3: SCFC 對於 1999 KDD Cup 資料集做資料縮減的詳細結果 
Datasets 1999 KDD Cup – whole data 
Class Original data Reduced data SVM(%) C4.5(%) 1-NN(%) 
C1 972781 1729 98.15 97.48 98.18 
C2 3883370 451 96.96 96.77 97.12 
C3 41102 879 82.50 74.58 81.30 
C4 52 33 7.46 3.95 14.47 
C5 1126 60 9.23 3.32 7.96 
Total 4898431 3152 92.38 91.68 92.42 
五、計畫成果自評 
目前我們的研究成果在研討會來發表，如下所示。 
─ C.-Y. Yeh, J. Ouyang, and S.-J. Lee, "A Clustering-Based Algorithm for Data 
Reduction," 5th International Workshop on Computational Intelligence and 
Applications, pp. 65-70, Nov. 2009. 
六、參考文獻 
[1] C. Cortes and V. Vapnik, “Support-vector networks,” Machine Learning, vol. 20, 
no. 3, pp. 273–297, September 1995. 
[2] P. Datta and D. Kibler, “Symbolic nearest mean classiﬁers,” in Proceedings of 
the 14th International Conference on Machine Learning, pp. 82–87, July 1997. 
[3] J. Macqueen, “Some methods for classification and analysis of multivariate 
observations,” in Proceedings of the 5th Berkeley Symposium on Mathematical 
Statistics and Probability, pp. 281–297, 1967. 
[4] E. Pękalska, R. P. W. Duin, and P. Paclík, “Prototype selection for 
dissimilarity-based classifiers,” Pattern Recognition, vol. 39, no. 2, pp. 189–208, 
February 2006. 
[5] J. Sánchez, “High training set size reduction by space partitioning and prototype 
abstraction,” Pattern Recognition, vol. 37, no. 7, pp. 1561–1564, July 2004. 
[6] V. Vapnik, The nature of statistical learning theory, 2nd ed. New York, NY, USA: 
Springer, November 1999. 
[7] D. R. Wilson and T. R. Martinez, “Reduction techniques for instance-based 
learning algorithms,” Machine Learning, vol. 38, no. 3, pp. 257–286, March 
2000. 
    Keynote Speech請來Prof. Takeshi Yamakawa 大師，來介紹他們所設計的機器人的
腳步運動，會議過程這位教授以生動有趣的影片介紹他如何觀察馬在跑、狗在走(其中
一段還有這位教授與狗在玩的畫面，十分逗趣)等等的生物關節運動，再以數學公式去
模擬關節運動，最後在Demo出一段他們模仿動物的行走奔跑的機器人。或許因為是學
生非控制領域，所以聽懂的並不多，但是學生卻深深的體會到各位學者對於研究的興
趣與熱誠。 
    最後收穫最大與最開心的事就是能在學生發表完論文”Applying Self-constructing 
Clustering to Color Image Quantization”會後，一位天津大學的研究生對於學生所發表的
論文十分感興趣，互相交換許多在分群上的想法與意見，獲取了許多寶貴的意見。 
本次會議共有32個general sessions, 分別為 
A1: Fuzzy Logic (I) ; B1: Signal Processing; C1: Image Processing (I) ; D1: Robotics; A2: 
Fuzzy Logic and Applied Statistics; B2: Data Mining; C2: Classification; D2: Intelligent 
Robotic Control; A3: Neural Networks (I) ; B3: Software Design and Applications; C3: 
Multimedia Systems; D3: Advanced Control (I) ; A4: Innovative Computing and Its 
Applications; B4: Circuit System; C4: Image Processing (II) ; D4: Advanced Control (II) ; 
A5: Neural Networks (II) ; B5: Biological Computing; C5: Ad Hoc and Wireless Sensor 
Network; D5: Fuzzy Control (I) ; A6: Fuzzy Logic (II) ; B6: Management Science (I) ; C6: 
Nature Language Analysis; D6: Fuzzy Control (II) ; A7: Soft Computing; B7: Management 
Science (II) ; C7: Intelligent Systems; D7: Control Application; A8: Optimization; B8: 
Management Science (III) ; C8: Mathematical Modeling and Systems; D8: Modeling and 
Simulation 
此次會議參與成員除了地主國，國外學者有日本、韓國、巴基斯坦、中國、土耳
其、馬來西亞等，台灣的研究學者也不在少數，大多是亞洲和非洲國家，但也不乏歐、
澳洲等國家。會議中討論相當踴躍，休息時間更是彼此寒暄問候，一方面也藉著難得
的機會，互相詢問與指教各國先進的研究趨勢與成果，適度分享研究心得與接受建議。
三、 建議 
1. 此次承蒙國科會工程處計劃補助，方得成行，在此先致上十二萬分的謝意，也
希望爾後國科會對於參加國際知名會議之學者均能大力支持。 
2. 國內有不少研究此方面的學者，而每年在相關的國內、外舉辦的國際性研討會，
皆可看到國內學者的投稿和參與熱忱，建議國科會亦可主辦或協辦此項國際性
的研討會。 
四、 攜回資料名稱及內容 
1.會議論文集 CD。 
2.其他相關會議之 call for paper 資料。 
 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：李錫智 計畫編號：98-2221-E-110-052- 
計畫名稱：機器學習技術應用在網路入侵偵測防護系統之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 2 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
