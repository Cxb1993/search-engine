approaches in the statistics theory [13] and is referred 
to as the initialization problem. Chuang, et al. [9] 
proposed the annealing robust learning algorithm 
(ARLA) learning algorithm that adopts the annealing 
concept into robust learning algorithms to deal with 
this problem. At the same time, the ARLA can 
overcome the problems of initialization and the cut-
off points in the robust learning algorithm. Besides, 
the conventional recurrent neural networks [14-15] 
can deal with modeling problem without outliers. 
That is, various neural networks have been proposed 
to modeling unknown nonlinear dynamic systems. 
Among them, the dynamic recurrent multiplayer 
neural networks have been widely used in modeling 
of nonlinear discrete dynamic systems [16-18]. For a 
general discrete-time dynamic system, the nonlinear 
autoregressive moving average (NARMA) model is 
an exact representation of its input-output behavior 
over the range in which the system operates [16].  
The purpose of this project is to guarantee that 
the universal approximation, both a suitable neural 
network structure and learning algorithm are needed 
for the model of nonlinear dynamic systems with 
outliers. In this project, the annealing robust recurrent 
radial basis function neural networks (ARRRBFNNs) 
based on SVR are proposed to model nonlinear 
dynamic systems with outliers and noise. This 
structure can be determined an initial structure. Hence, 
the proposed structure have fast convergence speed 
and robust against outliers. The ARRRBFNNs based 
on SVR include a RRBFNNs, a SVR and a recurrent 
annealing robust learning algorithm (RARLA). 
Because the RRBFNNs can be regarded as the 
NARMA model and the SVR is equivalent to radial 
basis function networks, the RRBFNNs based on 
SVR are suitable to modeling of nonlinear dynamic 
systems with good initial structure. At the same time, 
the RARLA is applied to the RRBFNNs. That is, the 
ARRRBF neural networks can extend recurrent 
RBFNNs to data with modeling problem with noise 
and outliers. Besides, the annealing robust recurrent 
fuzzy basis function is also proposed in the second 
year. 
三、Research Approach 
Many modeling algorithms are expressed in the 
continuous time domain, using measured variables, 
which assess the state of the nonlinear dynamic 
systems. However, the vast majority of modeling is 
implemented as sampled systems. In this project, the 
nonlinear dynamic systems with outliers can be 
expressed as 
 [ ] [ ] [ ]( )[ ] [ ] [ ]( ) [ ].,11
,1
kvkukxgky
kukxfkx
++=+
=+  (1) 
When the functions f and g in Eq. (1) are unknown, 
the problem of modeling of unknown Dynamic 
systems is needed to do for the design of controller. 
Let the input and output of a time-invariant, causal 
discrete-time dynamical system (1) be [ ] [ ]⋅⋅ yu  and , 
respectively, where [].u  is a uniformly bounded 
function of time, [ ]⋅y  is a target output. At the same 
time, the system is assumed to be stable and the 
disturbances v  is bounded. We propose the 
ARRRBF neural networks for the model of nonlinear 
systems with outliers. The proposed structure is 
shown in Fig. 1, where the new input [ ]yuu ˆ= , 
and the error [ ] [ ] [ ]kykyke ˆ−= . TDL in Fig. 1 
denotes a tapped delay line whose output vector for 
its elements the delayed values of the input signal. 
Besides, we also proposed the annealing robust 
recurrent fuzzy basis function to the model of 
nonlinear dynamic systems with outliers. 
 
Fig. 1. A proposed structure of ARRRBFNs for the 
model of nonlinear dynamic systems with outliers. 
A. ε -SVR for ARRRBF neural networks 
A ε -SVR approach is to approximate an 
unknown function from a set of (input, output) 
samples ( ){ }Niyx ii ,...,1 ,, =r  without learning. 
Assuming that a set of basis function ( ){ }lixgi ,..,1 , =r  is given, there exists a family of 
functions that can be expressed as a linear expansion 
of the basis functions. Then, the problem of function 
approximation transforms into that finding the 
parameters of the following basis function linear 
expansion: 
( ) bxgwwxf l
i
ii += ∑=1),(
rrrr
, (2) 
where ( )lwww ,...,1∈r  is parameter vector to be 
identified and b  is a constant. Then, the solution for 
the problem is to find f that minimizes 
( ) ( )( )wxfyL
n
wR kk
n
k
rrr
,1
1
−= ∑
= ε
, (3) 
subject to the constraint  
.
.
.
.
.
.
.
.
.
u1 un
Σ11
1
1
1
Σ
.
.
.
1
1
1
y1
yp
W
G1
Using An ARLA
 to Update 
the Parameters 
and Weights
G2
Gi
GN
...Nonlinear Dynamic systems
with Outliers
. . .
TDL
^
^
SVR for initial 
C≤2θr , (19) 
where C denotes the regularization constant and ( )⋅εL  represents the ε -insensitive loss function and 
is defined as 
⎩⎨
⎧
−=  ,
 , 0  
)( εε eeL   . 
,  
otherwise
efor ε≤  (20) 
An ε  zone is defined as that if the value of e is 
within the zone, the loss is zero. Otherwise, the loss is 
the magnitude of the difference between the absolute 
value of e and the ε  zone. The parameter ε  can 
be useful if the desired accuracy of the approximation 
can be specified beforehand. In some case, however, 
we just want the estimate to be as accurate as possible, 
without having to commit ourselves to a specific level 
of accuracy. Hence, Ref. [21] presented a 
modification of the SVM that automatically 
minimizes ε , thus adjusting the accuracy level to the 
data at hand. The Ref. [21] introduced a new 
parameter ν  ( 10 ≤≤ν ), which lets one control the 
number of support vectors and training errors. To be 
more precise, they proved that ν  is an upper bound 
on the fraction of margin errors and a lower bound of 
the fraction of support vectors. To get the estimations 
of θr  and b, equation (23) is transformed to the 
primal problem of ν -SVR: 
By using the Lagrange multiplier method, 
minimizing yields the following dual optimization 
problem:  
( ) ( )
( )( ) ( ) ( ) ,
2
1                                    
 ,  Minimize 
1
*
1,
*
1
**
⎥⎦
⎤⎢⎣
⎡−−
+−−=
∑∑
∑
==
=
l
s
vsusvv
p
vu
uu
p
v
vvv
xgxg
y
rrαααα
ααααψ  (21) 
subject to the constraint  
∑∑
==
= p
u
u
p
u
u
11
* αα , ( ) ναα Cp
u
uu ≤+∑=1
* , ⎥⎦
⎤⎢⎣
⎡∈
p
C
u 0
*α , 
for u=1,…,p, (22) 
where α , uα , vα , *α , *uα and *vα  are the 
Lagrange multipliers. The inner product of basis 
functions ( )xgs r  in (21) is replaced by the kernel 
function 
( ) ( ) ( )vkl
s
ukvu xgxgxxK
rrrr ∑
=
=
1
, . (23) 
The kernel function determines the smoothness 
properties of the solutions, and should reflect a prior 
knowledge on the data. Hence, the optimization of 
(21) is rewritten as 
( ) ( )
( )( ) ( ).,
2
1                
,
*
1,
*
1
**
vuvv
p
vu
uu
p
v
vvv
xxK
y
rrαααα
ααααψ
−−+
−−=
∑
∑
=
=  (24) 
Hence, the solution of the ν -SVR approach [21] is 
in the form of the following linear expansion of 
kernel functions: ( ) ( ) bxxKxf kp
k
kk +−= ∑=
rrr
,),,(
1
** αααα , (25) 
The Gaussian function is used as the kernel function 
in this study. Hence, (25) can be rewritten as  
( ) bxxxf p
k
k
kk +⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧ −−−= ∑
=1 2
2
**
2
exp),,( σαααα
rrr , (26) 
where σ  is called the Gaussian kernel parameter. 
C. Recurrent anneal robust learning algorithm 
In general, the most learning algorithm of 
traditional RBF neural networks are based on the LS 
criterion that easily affected by outliers [9]. Hence, 
the robust RBF neural networks are proposed to 
overcome the problems of traditional RBF neural 
networks for function approximation with outliers. 
Those robust RBF neural networks are mainly focus 
on the using robust learning algorithms [9, 22]. These 
robust learning algorithms adopt the concept of the 
M-estimators into the learning algorithms. The basic 
idea of such algorithms is to use the loss function to 
degrade the effects of those outliers. In order to 
overcome the above problems and extend to nonlinear 
dynamic system, a novel ARRRBF neural network 
for modeling of nonlinear dynamic systems with 
outliers is proposed. Let E  be the total error of all 
output neurons k  over all time steps Tt ,,0 L= . 
( )∑
=
= T
t
RARLA tEE
0
. (27) 
A cost function for the RARLA is defined here: 
[ ]∑∑
=×
= N
i
ki
k
RARLA tteNk
tE
1
)();(1)( βρ ,  (28) 
where 
( ) ∑
= ⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧ −−−=−= L
j j
ji
jkiikkiki
mu
wyuyyte
1
2
2
2
expˆ)( σ
,(29) 
t  is the epoch number, )(teki  is the error between 
the i-th desired output k and the i-th output k of the 
ARRBFNs at epoch t , )(tβ  is a deterministic 
annealing schedule acting like the cut-off points and 
[ ]
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛+= β
ββρ 21ln
2
; kiki
ee . Then the error gradient 
of error ( )tERARLA  for an arbitrary parameter p of 
the consider recurrent ARRBF neural network is ( ) ( )∑∑
= ∂
∂
×=∂
∂ N
i
ki
ki
k
RARLA
 p
t e
e
Nkp
tE
1
);(1
 
 βϕ , (30) 
where 
( ) ( )
p
ty
p
te kiki
∂
∂−=∂
∂ ˆ
 
 
, and 
( )( ) ( )( )
( )
( )t
e
te
te
te
te
ki
ki
ki
ki
ki
β
βρβϕ 2
1
 
);( 
;
+
=∂
∂= . When the 
output is single, the synaptic weights iw , the centers 
五、Conclusions 
In this project, the annealing robust recurrent 
radial basis function neural networks are developed to 
overcome the modeling of nonlinear dynamic systems 
with outliers problem. The soft computing learning 
approaches include a recurrent radial basis function 
neural networks, a recurrent fuzzy basis function, a 
support vector regression and a recurrent annealing 
robust learning algorithm. Besides, a new recurrent 
learning algorithm will be also derived under the 
results of the proposed networks. 
六、References 
[1] Michael Negnevitsky, Artificial Intelligence: A 
Guide to Intelligent Systems, Addison Wesley, 
Harlow, England, 2004. 
[2] C.C. Chuang, “Fuzzy Weighted Support Vector 
Regression with a Fuzzy Partition,” IEEE Trans. 
on Systems, Man, and Cybernetics: Part B, Vol. 
37, No. 3, pp. 630-640, 2007. 
[3] C. F. Hsu, C. M. Lin and T. T. Lee, “Wavelet 
Adaptive Backstepping Control for a Class of 
Nonlinear Systems,” IEEE Trans. on Neural 
Networks, Vol. 17, Issue 5, pp. 1175–1183, 2006. 
[4] Y. Jin and J. Branke, “Evolutionary optimization 
in uncertain environments-a survey,” IEEE Trans. 
on Evolutionary Computation, Vol. 9, Issue 3, pp. 
303-317, 2005. 
[5] D. M. Hawkins, Identification of Outliers, 
Chapman and Hall, 1980. 
[6] V. David Sanchez, “Robustization of learning 
method for RBF networks,” Neurocomputing, 
Vol. 9, pp. 85-94, 1995 
[7] C. C. Lee, P. C. Chung, J. R. Tsai and C. I. Chang, 
Robust radial basis function neural networks, 
IEEE Trans. on Systems, Man, and Cybernetics, 
Vol. 29, No. 6, pp. 674-685, 1999 
[8] D. S. Chen and R. C. Jain, “A robust back-
propagation learning algorithm for function 
approximation,” IEEE Trans. Neural Networks, 
vol. 5, no. 3, May, pp. 467-479, 1994. 
[9] C. C. Chuang, S. F. Su and C. C. Hsiao, “ The 
Annealing Robust Backpropagation (BP) 
Learning Algorithm,” IEEE Trans. Neural 
Networks, Vol. 11, No. 5, pp. 1067-1077, 2000. 
[10] J. Liang; Z. Wang; Y. Liu and X. Liu “Robust 
Adaptive Gradient-Descent Training Algorithm 
for Recurrent Neural Networks in Discrete Time 
Domain,” IEEE Transactions on Neural 
Networks, Vol. 19, Issue 11, pp. 1841-1853, 
2008. 
[11] H. Frigui and R. Krishnapuram, “A robust 
competitive clustering algorithm with 
applocations in computer vision,” IEEE Trans. 
Pattern Analysis and Machine Intelligence, Vol. 
21, No. 5, 1999. 
[12] K. Liano, “Robust error measure for supervised 
neural network learning with outliers,” IEEE 
Trans. Neural Networks, Vol. 7, No. 1, January, 
pp. 246-250, 1996. 
[13] H. Zhang, T. Ma, G.B. Huang, and Z. Wang, 
“Robust Global Exponential Synchronization of 
Uncertain Chaotic Delayed Neural Networks via 
Dual-Stage Impulsive Control,” IEEE Trans. on 
Systems, Man, and Cybernetics: Part B, Vo. 40, 
Issue 3. pp. 831-844, 2010. 
[14] M Liu, “Delayed Standard Neural Network 
Models for Control Systems,” IEEE Trans. on 
Neural Networks, Vol. 18, Issue 5, pp. 1376–
1391, 2007. 
[15] M. Brown and C. Harris, Neurofuzzy Adaptive 
Modeling and Control, Prentice-Hall, 1994. 
[16] K.S. Narendra and S. Mukhopadhyay, “Adaptive 
control using neural networks and approximate 
models,” IEEE Trans. Neural Networks, vol. 8, 
no. 3, pp. 475-485, 1997. 
[17] C. Alippi and C. D. Russis, and V. Piuri, “A 
neural-network based control solution to air-fuel 
ratio control for automotive fuel-injection 
systems,” IEEE Trans. Systems, Man and 
Cybernetics, Part C, Vol. 33, No. 2, pp. 259-268, 
2003. 
[18] D.V. Prokhorov, “Training Recurrent 
Neurocontrollers for Real-Time Applications,” 
IEEE Trans. on Neural Networks, Vol. 18, Issue 
4, pp. 1003-1015, 2007. 
[19] J. Smola and B. Schölkopf, A Tutorial on 
Support Vector Regression, Neuro COLT 
Technical Report TR-1998-030, Royal Holloway 
College, 1998. 
[20] V. Vapnik, The Nature of Statistical Learning 
Theory, Springer-Verlag, 1995. 
[21] B.A. Schölkopf, A. Smola, R.C. Williamson and 
P.L. Barlett, “New support vector algorithms,” 
Neural Computation, Vol. 12, No.5, 1207-1245, 
2000. 
[22] C.C. Chuang, J.T. Jeng and P.T. Lin, “Annealing 
Robust Radial Basis Function Networks for 
Function Approximation with Outliers,” 
Neurocomputing, Vol. 56, pp. 123-139, 2004.  
[23] Y.Y. Fu, C.J. Wu, J.T. Jeng and C.N. Ko, 
“Identification of MIMO systems using radial 
basis function networks with hybrid learning 
algorithm,” Applied Mathematics and 
Computation, Volume 213, Issue 1, pp.184-196 , 
Jul. 2009. 
 
整體而言本期計畫在軟式計算方法、模型化非
線性動態系統包含離異點及回授式退火強健學習
演算法上並發表3篇的相關論文、參加一次的國際
會議。同時本年度共發表6篇論文及6篇之國內外
研討會論文。可開發之技術如下： 
a. 軟式計算方法實現(以伺服器為基礎)。 
b. 軟式計算方法實現(以PDA手機為基礎)。 
c. 回授式退火強健學習演算法。 
另外尚有一些成果正在審查中，未來會陸續
發表於相關期刊及研討會上，最後主持人在
此要感謝國科會在這一年之相關補助。 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                         100 年  6  月  15 日 
報告人姓名 鄭錦聰 
 
 
服務機構
及職稱 
國立虎尾科技大學 
教授 
 
     時間 
會議 
     地點 
2011 年 6 月 8-10 日 
在澳門舉行 
本會核定
補助文號
 NSC 99-2221-E150-079 
會議 
名稱 
 (中文) 2011 國際系統科學與工程研討會 
 (英文) 2011 International Conference on System Science and Engineering 
發表 
論文 
題目 
 (中文) 最小截尾平方為基底之 CPBUM 神經網路 
 (英文) Least Trimmed Squares Based CPBUM neural networks  
                                                                                               ICSSE 2011 
Least Trimmed Squares Based CPBUM neural 
networks 
 
Jin-Tsong Jeng* and Chi-Ta Chuang 
Department of Computer Science and Information 
Engineering 
National Huwei University of Science and Technology 
64, Wen-Hua Road, Huwei Jen, Yunlin County, 
TAIWAN 632  
*E-mail: tsong@nfu.edu.tw 
 
Chen-Chia Chuang 
2Department of Electrical Engineering 
National Ilan University 
1, Sec. 1, Shen-Lung Rd., I-Lan, 260, TAIWAN, R.O.C. 
 
 
 
Abstract—In this paper, least trimmed squares (LTS) based 
CPBUM neural networks are proposed to improve the outliers 
and noise problems of conventional neural networks. In general, 
the obtained training data in the real applications maybe contain 
the outliers and noise. Although the CPBUM neural networks 
have fast convergent speed, this model is difficult to deal with 
training data set with outliers and noise. Hence, the robust 
property must be enhanced for the CPBUM neural networks. In 
this paper, the LTS computational architecture is proposed for 
the CPBUM neural networks. That is, the LTS approach can 
trim some large noise and outliers in the training data set. After 
the LTS, the gradient-descent kind of learning algorithms is used 
as the learning algorithm to adjust the weights of the CPBUM 
neural networks. It tunes out that the LTS based CPBUM neural 
networks have fast convergent speed and robust against outliers 
and noise than the conventional neural networks with robust 
mechanism. Simulation results are provided to show the validity 
and applicability of the proposed neural networks. 
Keywords—outliers, robust mechanism, least trimmed 
squares, gradient-descent kind of learning algorithms, CPBUM 
neural networks 
I.  INTRODUCTION 
The conventional neural networks are often used for 
modeling system due to its simplicity and nonlinear [1, 2, 3]. 
These conventional neural networks include multi-layered 
perceptron (MLP), wavelet networks, radial basis function 
network (RBF), piecewise smooth networks (PWSN), time 
delay input multi-layered perceptron, general regression neural 
network (GRNN), recurrent neural networks and so on. That 
is, these methods use the conventional feedforward and the 
recurrent neural networks to modeling of nonlinear systems, 
and these conventional neural networks can approximate 
continuous function with desirable precision. However, these 
methods suffer a major drawback of slow learning speed, lack 
of a systematic design method and difficult to deal with the 
training data with outliers and noise for modeling. Our 
previous results [4, 5] proposed the CPBUM neural networks 
for function approximation. The CPBUM neural networks can 
overcome the above problems. That is, the approximate 
transformable techniques, which consist of both the direct and 
indirect transformation are proposed to obtain the CPBUM 
neural networks for the feedforward/recurrent neural networks 
[4]. Specifically, the unified model can be obtained by way of 
the approximate transformable technique when the activation 
function of neural network is Riemann integrable. Hence, the 
unified model can be represented as a Chebyshev polynomial 
functional link network. Besides, the CPBUM neural networks 
not only have the same capability of universal approximator, 
but also have a faster learning speed than the conventional 
feedforward/recurrent neural networks. For the recurrent 
neural network, the feedback parameter is regarded as input 
parameter in the proposed CPBUM neural networks. In 
addition, the relationship between the single-layered neural 
network and the multi-layered perceptron neural network has 
derived [4]. Besides, Patra et al. [6] used Chebyshev 
functional link network to do identification problem. Though 
the Chebyshev functional link network can overcome the 
learning speed and systematic design method, it is difficult to 
deal with the model with outliers and noise. That is, this study 
extends the CPBUM neural networks for modeling with 
outliers and noise. 
In general, for the scientific and engineering applications, 
the obtained training data maybe subject to outliers. The 
intuitive definition of an outlier [7] is “an observation which 
deviates so much from other observations as to arouse 
suspicions that it was generated by a different mechanism.” 
However, outliers may occur due to various reasons, such as 
erroneous measurements or noisy data from the tail of noise 
distribution functions. When the outliers exist, there still exist 
some problems in the traditional neural networks approaches 
[8, 9]. Hence, various robust learning approaches [8-11] are 
proposed to overcome the problems of traditional neural 
network approaches while facing with outliers. Those robust 
approaches could indeed improve the learning performance 
when training data contain outliers. Besides, the MLP neural 
networks are often referred to as universal approximator. 
Nevertheless, if the used training data are corrupted by large 
noise, such as outliers, traditional backpropagation learning 
978-1-61284-471-8/11/$26.00 ©2011 IEEE
Proceedings of 2011 International Conference on System Science and Engineering, Macau, China - June 2011
187
ICSSE 2011  
speed and robust against outliers and noise than the 
conventional neural networks with robust mechanism. 
III. THE CPBUM NEURAL NETWORKS  
The development of CPBUM in a three-layered 
feedforward/recurrent neural networks can been found in Lee 
and Jeng [4]. Besides, for the general feedforward/recurrent 
neural networks can be found in Jeng and Lee [5]. The 
CPBUM neural networks are summarized as follows: 
Theorem 1. [5] If every activation function satisfies the 
Riemann integrable condition for the standard MLP and the 
recurrent neural network, then the MLP/recurrent neural 
network can always be represented as CPBUM neural 
networks 
( ) ( )( )inin xx iTXWy ˆˆ = . (7) 
where 
( )( ) ( ) ( ) ( )[ "" 12111 ||1 xTxTxTTX ni =inx
( ) ( ) ( ) ( ) ( ) ||11121112 "" nnn xTxTxTxTxT −  
( )( ) ( )( )]T LnoPoP xTxT ×11 " , ( )( )
( )∑
−
−+
=
=
op
k nk
nk
L
0 !1!
!1
,  
[ ]LL wwwwwW 1210ˆ −= " . 
The standard MLP neural network can be described as a 
nested nonlinear function in the following: 
( ) ( )wxw ,ˆ, ininx yN = , (8) 
where ( ) [ ]Tiyyyy ˆˆˆ,ˆ 21 "=wxin , 
 ( ) ( )( ) ⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
= ∑ ∑
−
−
−
n n
nn
h h
hnnhi netffwfwfy ""
1
11
ˆ , 
net W= +1xin θ , xin  is input vector, Wn  is the weight of hidden 
layer to output layer, W1 is the weight matrix of input layer to 
hidden layer, θi  is the bias constant, y is the output vector, 
fi   is the activation function, and h is the number of hidden 
neuron. 
The recurrent neural network can be described as a nested 
nonlinear function in the following: 
( ) ( )wxw ,ˆ, ininx yN = , (9) 
where ( ) [ ]Tiyyyy ˆˆˆ,ˆ 21 "=wxin ,  
( ) ( )( ) ⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
= ∑ ∑
−
−
−
n n
nn
h h
hnnhi netffwfwfy ""
1
11
ˆ , 
net W u i= +1x +in Wj θ ,  and Wj  is the weight matrix of input 
layer to hidden layer for the feedback input. 
According to Theorem 1, we have 
( ) ( )( )inin xiTXWy ˆ,ˆ ≈wx . (10) 
The proposed CPBUM neural networks are shown in Fig. 1. It 
consists of two parts; namely, numerical transformed part in 
part (a) and learning part in part (b). Numerical transformation 
deals with the input layer to hidden layer by the approximate 
transformable method. Hence, there is no learning in this part. 
As a result, the Chebyshev polynomial basis can be viewed as 
a new input vector. Therefore, it will reduce much learning 
time after this transformation. The part (b) is a functional link 
network, based on Chebyshev polynomials. Because this 
model is a single-layered model, its learning speed is fast. 
Note that from Eq. (10), the CPMUM neural network is a 
linear-parameter and a nonlinear-input structure. However, the 
CPBUM neural networks in Fig. 1 are easy to be extended as a 
nonlinear-parameter and a nonlinear-input structure. This 
modified part is output layer in Fig. 1 to use nonlinear 
activation functions such as sigmoid function and hyperbolic 
tangent. This new structure can be represented by 
( )( ) ( )Wayaynew ˆ,,ˆˆ inin x=≈ wx , (11) 
where a is a nonlinear activation function. 
 
Part (a) Part (b)
.
.
.
T0(   )x1
T1 (   )x1
.
.
.
T1 (   )xn
T2 (   )x1
.
.
.
T2 (   )xn
T1 (   )xnT1 (   )x1
.
.
.
.
.
.
Tp (   )x1( )o
Tp (   )xn( )o
.
.
.
x1
xn
Σ
1
1
1
1
1
1
1
1
1
W^
Σ
.
.
.
y1^
yi^
Numerical Transformed Part Learning Part
 
Fig. 1. The architecture of the CPBUM neural networks. 
IV. THE LEAST TRIMMED SQUARES FOR THE CPBUM 
NEURAL NETWORKS 
The LTS estimator is formulated as 
{ }21Minimize ( )h ii r d=∑   
and 2 2 2 21 2( ) ( ) ( ) ( )h kr d r d r d r d≤ ≤ ≤ ≤ ≤" " ,  (12) 
where id  is the thi  observation, 
2 ( )ir d  is the thi  squared 
residual, k  is the length of observations and h is the number 
of data points which are not trimmed from the data set. In 
robust regression analysis [5], the maximum tolerance of the 
LTS estimators to outliers (named Maximum Breakdown 
Point) for any equi-variant regression estimator satisfies  
( )( )( )1Breakdown Point 2 1kk ζ≤ − + , (13) 
189
ICSSE 2011  
 
Example 1. The sinc function is defined as 
( ) [ ]sin ,   10,10 .xy x
x
ε= + ∈ −  (17) 
The parameters in the order of the proposed approach are set 
18 for this case. Besides, we also compare with our previous 
approach. The proposed LTS based CPBUM neural network 
has better performance. TABLE II show the compared results 
for the sinc function contains the Cauchy, student’s t and 
Gaussian noise with different h in the LTS. 
 
Example 2.  Consider the dynamic systems and its model is 
written as 
[ ]( ) [ ]( ) [ ]( )1sin 5 cos 3 1 ,y x t x t x t ε−= − +   (18) 
where 0, ,1000k = … , [ ]x t  is the input sequence generated by 
equally sampling the interval [ ]1,1−  for 0, ,1000k = … . The 
initial condition for the system is [ ]( )0 3y x = − . 
The parameters in the order of the proposed approach are set 
20 for this case. Besides, we also compare with our previous 
approach. The proposed LTS based CPBUM neural network 
has better performance. TABLE III show the compared results 
for the dynamic systems contains the Cauchy, student’s t and 
Gaussian noise with different h in the LTS. 
 
Example 3. The Hermite function is defined as 
( ) [ ]22 21.1 1 2 ,   10,10 .xy x x e xε−= − + + ∈ −   (19) 
The parameters in the order of the proposed approach are set 
60 for this case. Besides, we also compare with our previous 
approach. The proposed LTS based CPBUM neural network 
has better performance. TABLE IV show the compared results 
for the Hermite function contains the Cauchy, student’s t and 
Gaussian noise with different h in the LTS. 
VI. CONCLUSIONS  
In this study, the CPBUM neural networks with the LTS are 
developed to improve the conventional robust neural networks 
for function approximation with outliers and noise. In this 
proposed approach, an LTS is applied to improve the 
performance of the CPBUM neural networks. That is, the LTS 
can be overcome the problems of outlier and noise in the 
conventional robust neural networks. After LTS the proposed 
neural networks can use a simple learning to improve the 
convergence speed. Simulation results are provided to show the 
validity and applicability of the proposed neural networks. 
ACKNOWLEDGMENTS 
This work was supported by National Science Council 
Under Grant NSC 99-2221-E-150-079. 
REFERENCES 
[1]   C.C. Chuang, J.T. Jeng and T.T. Lee, “Robust adaptive tracking 
control via cpbum neural network for mimo nonlinear systems,” 
International Journal of Electrical Engineering, Vol. 12, pp.  
313-324, 2005. 
[2]  C.C. Chuang, J.T. Jeng and P.T. Lin, “Annealing robust radial 
basis function networks for function approximation with 
outliers,”  Neurocomputing, Vol. 56, pp. 123-139, 2004. 
[3]  N. Roy and R.Ganguli, “Filter design using radial basis function 
neural network and genetic algorithm for improved operational 
health monitoring,” Applied Soft Computing, Vol. pp. 154-169, 
2006.  
[4] T.T. Lee and J.T. Jeng, “The chebyshev-polynomial-based 
unified model neural networks for function approximation,” 
IEEE Trans. System, Man, and Cybernatics Part B: Cybernetics, 
Vol. 28, pp. 925-935, 1998. 
[5] J.T. Jeng and T.T. Lee, “Control of magnetic bearing systems 
via the chebyshev polynomial-based unified model (CPBUM) 
neural network,” IEEE Trans. Systems, Man, and Cybernatics 
Part B: Cybernetics, Vol. 30, pp. 85-92, 2000. 
[6]  J.C. Patra, and A.C. Kot, “Nonlinear dynamic system 
identification using Chebvshev functional link artificial neural 
networks,” IEEE Trans. System, Man, and Cybernatics Part B: 
Cybernetics, Vol. 32, pp. 505-511, 2002. 
[7]  D.M. Hawkins, Identification of Outliers, (Chapman and Hall, 
1980). 
[8]  C.C. Lee, P.C. Chung, J.R. Tsai and C.I. Chang, “Robust radial 
basis function neural networks,” IEEE Trans. Systems, Man, and 
Cybernetics, Vol.  29, pp. 674-685, 1999. 
[9]  C.C. Chuang, S.F. Su, J.T. Jeng and C.C. Hsiao, “Robust 
support vector regression networks for function approximation 
with outliers,” IEEE Trans. Neural Networks, Vol. 13, pp. 1322-
1330, 2002. 
[10] D.S. Chen and R.C. Jain, “A robust back-propagation learning 
algorithm for function approximation,” IEEE Trans. Neural 
Networks, Vol. 5 pp. 467-479, 1994. 
[11] Chen-Chia Chuang and Jin-Tsong Jeng, “CPBUM Neural 
Networks for Modeling with Outliers and Noise,” Applied Soft 
Computing, Vol. 7, No. 3, pp. 957-967, 2007. 
[12]  H. Zhao and J. Zhang, “Pipelined Chebyshev functional link 
artificial recurrent neural network for nonlinear adaptive filter,” 
IEEE Trans. Systems, Man, and Cybernetics, Part B: 
Cybernetics, Vol. 40, pp. 162-172, 2010. 
[13] H. Zhao and J. Zhang, “Functional link neural network cascaded 
with Chebyshev orthogonal polynomial for nonlinear channel 
equalization,” Signal Processing, Vol. 88, Issue 8, pp. 1946-
1957, 2008  
[14]  H. Wang and H. Gu, “Chaotic synchronization in the presence of 
disturbances based on an orthogonal function neural network,” 
Asian Journal of Control, Vol. 10, pp. 470–477, 2008. 
[15] P.J. Rousseeuw, and K.V. Driessen, “Computing LTS regression 
for large data sets,” Data Mining and Knowledge Discovery, Vol. 
12, pp. 29-45, 2006. 
[16] Kuo-Lan Su, You-Min Jau and Jin-Tsong Jeng, “Modeling of 
Nonlinear Aggregation for Information Fusion Systems with 
Outliers Based on Choquet Integral,” Sensors, Vol. 11(3), pp. 
2426-2446, 2011. 
 
 
191
1Least Trimmed Squares Based 
CPBUM neural networks
Jin-Tsong Jeng,+ Chi-Ta Chuang+ and Chen-Chia Chuang
+Department of Computer Science and Information Engineering
National Huwei Institute of Technology
Yunlin County, TAIWAN 632. 
Department of Electronic Engineering
Hwa-Hsia College of Technology and Commerce
Taipei Country, TAIWAN 235.
Motivations 
Introduction
CONCLUSIONS
OUTLINE
Simulation Results
Proposed Methods
In order to overcome the drawbacks in NN
slow learning speed and
nonsystematic method in the Neural Network
Why need to improve in Neural Networks ﹖
We proposed CPBUM neural network.
Motivations (1/5)
.
.
.
T0 (   )x1
T1(   )x1
.
.
.
T1(   )xn
T2(   )x1
.
.
.
T2(   )xn
T1(   )xnT1(   )x1
.
.
.
.
.
.
Tp (   )x1( )o
Tp (   )xn( )o
.
.
.
x1
xn
Σ
1
1
1
1
1
1
1
1
1
W^
Σ
.
.
.
u1
ui
( )
( )
( )∑
= −
−+= op
k nk
nkL
0 !1!
!1
( )( )inxiTXWu ˆ≈u1
u2
ui
x1
x2
xn
f
1
(net)
f
h
(net)
Wj Wi
f
2h
(net)
21f (net)
x1
x2
xn
W1
W j
Wn
y1^
y2^
y i^
Motivations (2/5)
3• Outlier Effects in Learning Algorithm
– Non-smoothing Interpolation
– Noise Sensitivity
Introduction
• The Statistical definition of Outliers
“an observation which deviates so much from
other observations  as  to  arouse  suspicions
that it was generated by a different mechanism.”
Majority data set
Outliers
Introduction
• Outlier Models (often used in literature)
– Artificial Outliers Model 
– Gross Error Model (Gaussian Mixture Model)
Outliers are added artificially by simply moving
some  points from their designated locations.                 
{ }10 ,)1( | ≤δ≤δ+δ−==δ HGDDD
δ
G and H are Gaussian distributions
is the probability of occurrence of outliers 
Introduction
The LTS estimator is formulated as
where ri2 is the ith squared residual, h is the 
length of observations which are not trimmed 
from the data set.
{ }2 2 2 2 21 21Minimize ,  h i h ki r r r r r= ≤ ≤ ≤ ≤ ≤∑ L L
Proposed Methods
5The other significant feature of the FAST-
LTS algorithm is the C-step. It is indeed a 
recursive procedure used for increasing 
accuracy of the estimated results and can be 
elaborated by the following procedures.
The C-step:
Figure 2.1. Show the complete procedure for the C-step.
The procedure of the CPBUM neural networks with the LTS are stated as follows:
Algorithm 1
Step 1: Set the maximum iteration number, learning constant and the 
parameters of the CPBUM neural networks. Besides, the CPBUM neural 
network can be described as a nested nonlinear function in Eq. (10).
Step 2: Set LS initial estimates, compare the residues between target and 
LS initial estimates value. Then the squared residuals are sorted in 
ascending order.
Step 3: LTS approach can trim some large noise and outliers in the 
training data set. To determine the h value for the subset under the squares 
residuals.
Step 4: Initialize the CPBUM neural networks structure using the random
weights.
Step 5: Compute the estimated result and its error via RMSE for all 
training data.
Step 6: The synaptic weights are iteratively updated via Eq. (15).
Step 7: Compute the robust cost function EN defined via RMSE.
Step 8: If the termination conditions are not satisfied, then go to Step 5; 
otherwise terminate the learning process.
Simulation Results
7TABLE III. SHOWN THE COMPARED 
RESULTS FOR THE DYNAMIC SYSTENS 
WITH PROPOSED APPROACH THAT 
CONTAINS DIFFERENT NOISE AND H IN 
THE  LTS
0.3537450.2065410.1851040.9
0.4466920.2165570.1873650.8
0.5120.2386250.1973560.7
0.6007380.2628540.1855590.6
0.6490830.2805530.1738480.5
LTS-CPBUM
0.8439780.2904120.213539CPBUM
Cauchy noisesStudent’s t noisesGaussian noiseMethod\Noise
The Hermite function is defined as 
Assume the number of sampling data is 121. 
( )op equals to 18. 
The total number of weights is 19 in the ARCNNs. 
The noise is a normal distribution with mean zero, variance one,
magnitude 1.5 and standard deviation one. 
( ) [ ]22 21.1 1 2 ,   10,10 .xy x x e xε−= − + + ∈ −
TABLE IV. SHOWN THE COMPARED RESULTS FOR 
THE HERMITE FUNCTION WITH PROPOSED 
APPROACH THAT CONTAINS DIFFERENT NOISE
AND H IN THE  LTS
0.230990.1167850.0967560.9
0.2540060.123180.0959660.8
0.2697450.129630.090920.7
0.2979130.1438530.0883030.6
0.3345520.1537850.0892210.5
LTS-CPBUM
0.3525610.1631820.103328CPBUM
Cauchy noisesStudent’s t noisesGaussian noiseMethod\Noise
In this paper, the CPBUM neural networks with the 
LTS are developed to improve the robust neural 
networks for function approximation with outliers. 
A LTS  is developed with a simple gradient-descent 
kind of learning algorithm for CPBUM neural networks. 
Hence, a LTS can deal with the model with 
outliers and noise. 
Consequently, the proposed LTS based CPBUM neural 
networks with gradient-descent kind of learning 
algorithm have fast convergence speed and robust 
against noise and outliers than the conventional neural 
networks. 
CONCLUSIONS
99 年度專題研究計畫研究成果彙整表 
計畫主持人：鄭錦聰 計畫編號：99-2221-E-150-079- 
計畫名稱：軟式計算學習方法在模型化非線性動態系統包含離異點之研究與應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 6 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
