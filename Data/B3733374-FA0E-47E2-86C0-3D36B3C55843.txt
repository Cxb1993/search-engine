 
 
結合多媒體互動與生理回饋技術建立智慧型太極拳輔助學習環境 
─ 以電腦視覺技術進行姿態分析及其於太極拳輔助學習之應用 
 
一、中文摘要 
本計畫利用電腦視覺的技術進行人的肢體動
作分析與姿態還原，並藉此開發能應用於運
動（例如太極拳）的人體姿態分析與追蹤方
法。我們發展了以一個以範例比對的方式，
由教練的姿態資料庫中，藉由單一影像的形
狀比對出與學員姿態最相似的動作影像的方
法。接著，我們更進一步地將其提昇，發展
了透過影像序列來進行姿態辨認的方法。以
便藉由多張連續的影像序列來進行更精確的
比對。我們並研發了一個藉由多攝影機來追
蹤以還原三維軀幹模型的方法。除此之外，
配合總計畫的整合系統發展需求，我們也將
範例影像比對方法進行簡化與加速，俾便能
更快速地分辨出人體姿態的不相似處，以提
供評分之依據。為了將影像序列中前景物體
抽取得更加準確，我們也研發了一個影子模
型學習方法，以利於將容易被誤認為前景的
影子加以濾除。 
 
關鍵詞：電腦視覺、姿態分析、姿態還原、
影像序列分析、特徵比對、運動分割、前景
分割。 
 
英文摘要： 
In this project, we developed computer-vision 
techniques for human posture matching and 
recovery. They can serve as foundations for 
athlete image sequence analysis and tracking 
(such as Tai Chi Chuan). We developed an 
exemplar-based method that can match two 
images, where one is from the input and the 
other is from the database. We then enhanced 
the result by matching two video clips instead 
of individual images, so that the posture 
matching results can be more accurate. We also 
developed a method that can track and recover 
3D gestures from multi-views. To be integrated 
with the main project, we modified the 
exemplar-based matching method, so that 
image-based matching can be performed more 
quickly. In addition, we also developed a cast 
shadow model learning method for on-line 
shadow discovery and elimination, so that 
foreground objects can be extracted more 
accurately. 
 
二、緣由與目的 
人體姿態的追蹤、比對、與還原是電腦視覺
中熱門的研究課題。然而由於其高維度以及
高變化性之故，其挑戰性也很高。本計畫研
究姿態比對相關問題，此方面牽涉到的研究
課題很廣，包含了背景與前景分割，物體追
蹤，外圍輪廓比對，三維姿態還原等。這些
都是目前電腦視覺中重要的研究課題，在世
界上相關研究也正方興未艾。 
過去相關於身體運動姿態（例如太極拳
或其它肢體運動）的教導與學習方式，除教
練的從旁指導外，多以書籍或錄影帶為輔助
自學工具。然而這樣的方式難以讓使用者觀
察或瞭解其本身的姿態，學習模式流於枯燥
被動，也因而影響學習興趣與效果。對於學
習者而言，若能透過智慧型互動學習環境，
即時觀看自身的影像姿態與獲得生理回饋，
可讓學習者自然地融入學習環境，達到較佳
的學習狀態。若能更進一步提供專家姿態的
同步對照與評分機制，有助於使用者察覺自
身姿態與專家姿態的差異，藉此察覺自身缺
失、調整自身的姿態與運動模式。這樣的一
種即時回饋與呈現的融入式 (immersive) 虛
擬實境教學系統需要結合包括姿態分析、師
生姿態同步化、生理訊號測量分析、與人機
介面呈現等不同的模組。而姿態分析是其中
基礎而重要的一環。 
近來許多學者開始應用電腦視覺的技
術，企圖發展出不需穿戴輔助器材的運動追
 
 
warping effect) 之影響。 
而在 [Bobick and Davis, 2001] 的方法
中，在時間軸累計剪影，以二維影像呈現運
動的範圍，再取該影像的 x-y 軸動差作為運動
姿態的特徵向量。然而所有運動資訊被壓縮
在一張影像中呈現，因而動作的先後順序沒
有辦法被保留下來，易造成誤判的結果。 
近年有許多研究者提出以運動過程中產
生的時空資訊 (space-time information) 作為
姿態分析的基礎，從時空中取適當的時間長
度來描述運動特徵。[Blank et al., 2005] 將動
作視為人體外型輪廓在時間軸變化下所形成
的三維資料，再透過 Poission equation 計算出
表達此三維資料的特徵，並取其時間軸與空
間軸的動差作為特徵值；[Wang and Suter, 
2007] 將外型輪廓的序列轉變成一長串資料 
(raw data) 表示，並採用一個流型的拓撲空間
將資料降維至隱含動作的子空間中。 
在我們的研究中，提出一個新的時空資
訊表示法，於影像序列中進行人體運動姿態
的分析。我們以適當的軟性時間區間 (soft 
time interval) 來表示動態的資訊，並善加利用
離線教練姿態  現場學員姿態 
背景重建與前景偵測 
顏色模型學習 
Shape Context 
姿態比對與肢體重建 
肢體模型 
教練姿態資料庫 
評分結果 
控制影片播放 
圖一、單張影像比對示意圖，左側為離線教
練姿態分析，右側為即時學員姿態分析。若
加入色彩模型有助於找出 critical regions。 
 
 
(Background Modeling)來進行背景的學習，並
利用所學習到的背景模型來對後續輸入影像
進行前景偵測，而偵測出的結果可以應用在
許多領域內，例如安全監控系統、行人偵測
與追蹤等。在本計劃的姿態分析系統中，完
整的去除背景將能使姿態擷取與分析的工作
更方便準確。在先前的研究中，有許多建立
背景模型的方法被提出，諸如使用顏色和方
向資訊，或是利用區塊資訊等當成描述背景
的特徵，在描述這些特徵時，最常用的方法
為高斯混合模型(Gaussian Mixtures) [Stauffer 
and Grimson, 2000]，以利於適應背景可能出
現的動態變化，然而此方法並無考慮到前景
物體所造成的影子，針對此一問題，K. Kim
等提出了 CodeBook 背景模型來進行背景學
習與前景偵測 [K. Kim et al., 2005]，其除了可
以學習動態背景(Dynamic Background)的資
訊以克服晃動背景的問題外，也可利用其背
景模型的特性來對環境中影子造成的變化做
更 好 的 處 理 。 在 我 們 的 研 究 中 便 是 以
CodeBook 作為背景重建與前景偵測的工具。 
CodeBook 是以像素(Pixel)為基礎的背景
模型，在建置的過程中，每一個像素所建立
的背景模型稱為 CodeBook，而每個 CodeBook
內存有一個至數個 CodeWords，分別用來代
表此一像素所有可能出現的顏色資訊，每個
CodeWord 分別使用下列參數來描述該像素
在過去時間所發生過的一些資訊： 
z ( )iiii BGR ,,=v ：表示第 i 個像素在此
CodeWord 中儲存的 RGB 彩色資訊。 
z I)：表示過去時間出現過的最大亮度。 
z I(：表示過去時間出現過的最小亮度。 
z f：此 CodeWord 被參考的頻率。 
z λ ：此 CodeWord 最長未被參考到的時
間間隔。 
z p：此 CodeWord 被產生時的時間。 
z q：此 CodeWord 最後一次被參考到的時
間。 
其中，該像素的亮度定義為 RGB 空間中，輸
入的顏色與原點(0, 0, 0)間的距離，此距離定
義為該輸入顏色的亮度。CodeBook 內每個
CodeWord 同時學習了亮度的資訊，這也使得
CodeBook 背景模型較能處理光影的變化；此
外，對於動態亮度變化背景的問題，每個像
素 利 用 學 習 出 多 組 的 CodeWords ， 每 個
CodeWord 儲存不同動態背景的資訊來克服
此一問題，使得此方法也可以應用在含有動
態亮度變化背景的環境中。背景模型示意
圖，見圖二。 
在背景模型建立後，後續輸入影像便可
利用學習到的背景模型進行前景偵測，偵測
的方法為將輸入影像的每一個像素與該像素
相對應的 CodeBook 做比對，看看是否有互相
match 的 CodeWord，若有，則該像素被偵測
為背景，並更新 match 的 CodeWord，反之，
若沒有 match 的 CodeWord，則該像素被偵測
為前景。 
 
  圖二、CodeBook 的背景模型示意圖 [ Kim 
et al., 2005] 
 
有鑑於無論何種姿態分析的方法， 初始工作
是去除影像背景，以便擷取出人體的外圍輪
廓作進一步的分析。而背景重建與前景分割
的方法容易遭受到影子 (cast shadow)的影
響，使得影子被當成錯誤的前景被抽取出
來，從而影響到外圍輪廓的抽取以及影像姿
態的比對。因此除了採用 CodeBook 方法之
 
 
消耗也是必要的。 
    以太極拳姿態為例，見圖四(a)。太極拳
「雲手」、「掤、履、擠、按」等動作，姿
態訊息往往集中於雙手的區域，但腳部僅需
外圍輪廓卻佔人體極大的區域，使得手部訊
息在取樣平均的情形下強度被削弱了，而腳
部也造成了處理對應點時不必要的負擔。有
鑒於此，我們可以加入色彩學習模型，強化
手部區域的資訊，設定取樣點分布的重要程
度。 
 
 
(a) 
 
(b) 
圖四、(a)太極拳動作：右掤。(b) 事先選取的
15 個骨架特徵點。 
 
1.3 學員肢體模型配置 
有了比對後的姿態結果，我們希望藉先前教
練與學員輪廓對應點的資訊，與事先決定的
教練肢體模型，來重建出學員的肢體模型。
由肢體模型的對照，可以更清楚的估測出學
員與教練之間姿態的差異程度。教練的姿態
在離線時已賦予包含 15 個骨架特徵點的肢體
模型(頭部、頸部、腹部、肩膀關節、手肘關
節、手腕關節、髖部關節、膝關節、腳踝關
節)，將特徵點連接即構成肢體架構，見圖四
(b)。 
    由兩張影像已知的對應點，我們希望進
而求得骨架特徵點的對應位置。基本上此為
一個內插 (interpolation) 的問題。在此工作上
我們採用[S. Belongie et al.,2002]的建議，使用
適合於求解具複雜運動軌跡之點集合間的內
插方法：thin plate spline ( TPS ) model，在此
簡介其方法。 
      
( , ),
( , ),
( , ) , ( , )
ix x ix iy
iy y ix iy
i ix iy i ix iy
q f p p
q f p p
p p p P q q q Q
=
=
= ∈ = ∈
 
目的在求得轉換式 xf 與 yf 的參數。我們以
( , )i i iv f x y= 來簡述其方法，TPS 所內插之
( , )x y 滿足 ( , )f x y 之曲率最小化： 
2
2 2 22 2 2
2 22f
f f fI dxdy
x x y yℜ
⎛ ⎞ ⎛ ⎞ ⎛ ⎞∂ ∂ ∂= + +⎜ ⎟ ⎜ ⎟ ⎜ ⎟∂ ∂ ∂ ∂⎝ ⎠ ⎝ ⎠ ⎝ ⎠∫ ∫  
可改寫成： 
1
1
2 2
( , ) ( ( , ) ( , ) ),
( ) is defined by ( ) log  and (0) 0
n
x y i i i
i
f x y a a x a y wU x y x y
U r U r r r U
=
= + + + −
= =
∑
為了滿足 ( , )f x y 為二次可積分函數，做了以
下限制： 
      
1 1 1
0 and 0
n n n
i i i i i
i i i
w w x w y
= = =
= = =∑ ∑ ∑  
對於 TPS 的係數，可用一線性方程組表示： 
 
 
性，選擇ㄧ準確性最高的門檻值。 
    利用上述膚色偵測法，換作訓練衣服顏
色模型後，當學員的手臂區域夠明顯時，亦
可得知手部區域位置，且有助於在姿態分析
中消除因衣服皺摺造成邊緣偵測時影響手部
區域的訊息。手部是人體擺出動作姿勢的關
鍵 ， 即 是 輪 廓 取 樣 的 重 要 區 域 (critical 
region)，此區域取樣數增多使手部輪廓加深，
於姿態比對時能提升準確性，而其餘部份取
樣數減少，則能縮短比對時間。 
1.5 結果與討論 
    我們發展了以姿態比對的方式進行姿態
還原之方法，以便應用於太極拳的動作分
析。目前可針對單一視角的姿態比對。系統
會從教練影像資料庫中，進行所有影像的比
對，取得姿態最相似的影像。由影像中姿態
點集合的對應點，重建出學員的肢體模型，
並以肢體模型比較提供評分機制。 
現階段我們以太極拳的基本動作「雲
手」、「掤、履、擠、按」為學員學習的姿
態。將教練之「掤、履、擠、按」動作的影
片切割後，取出 18 張姿態差異較大的代表影
像。先將姿態點集合訊息擷取並儲存（包含
15 個骨架特徵點）。 
在測試階段，輸入的一張學員影像則與
教練姿態的代表影像皆進行比對，完成比對
後並建立肢體模型。總共有 80 張學員影像進
行測試。在比對的正確性上，我們的方法對
於所有的輸入影像皆可在單一視角的狀況下
找到外圍輪廓正確的影像。結果範例可見圖
七、八所示。然而因為只使用單一視角的影
像，某些手部動作並無法在外圍輪廓上造成
差異（如圖六所示），因此雖可比對出外圍
輪廓上正確的影像，但因手部動作的變異而
未能還原出最佳姿態。儘管如此，我們的方
法在大部分的情況下皆可以讓最佳姿態出現
在前五名。就使用單一攝影機的的方法而
言，此結果已可作為一個不錯的初始資訊，
可作為整合多個攝影機，及動作連貫性方法
的良好基礎。但若對於不易分辨的姿態而言
（如圖五），仍為相當困難的問題。 
   
圖五、太極拳動作：右掤和擠，即使實質動
作仍有不同，但從影像比較，人眼也不易區
分（上圖為掤，下圖為擠）。 
 
Part 2: 結合時空資訊的人體運動姿態辨認 
前面發展的方法是以一張張獨立影像的
角度，求出每張影像的對應姿態，但若遭遇
的動作重複問題與連貫性問題。利用一串連
續影像比對的方式，以便在教練資料庫中先
預測出可能的動作，降低比對範圍提升處理
速度。在此一部份，我們發展了一個以連續
影像作為基礎來進行姿態判斷的方法。本方
法的成果發表於  ICPR 2008 [Hsiao et al., 
2008]。 
當輸入一個視訊片段 (video clip)，我們
希望從中判斷出人的運動姿態 (gesture)。一
般而言，在以外型輪廓為基礎的方法中，人
體方向與速度等全域性資訊在姿態的辨認上
並不關鍵，反而是四肢的擺動時所產生的變
異，對辨認有決定性的影響。為了凸顯四肢
擺動的運動特徵，我們提出了一個新的特徵
表示：時間重疊輪廓圖，Temporal-overlapping 
Contours (TOC) map。 
對於一段含有 n 張影像的 video clip, 
，假設其前景物體的剪影 
(silhouette)為 ，而 
分 別 為 其 外 圍 輪 廓 (contour) 。 當 我 們 將
的重心經由平移疊合在一起時，
其 外 圍 輪 廓 所 形 成 的 重 疊 圖 稱 為  TOC 
map。如下式所示： 
 
 
其中  分別是模糊
區 間 early, middle, late 的 歸 屬 函 數 
(membership function)， 是一個正規化參數
使得 
  
  
 歸屬函數如圖九(a)所示。我們選擇鐘型
的平滑曲線( )以緩和區間與區間之間
的過渡所造成的影響。對於每個區間，我們
分別計算其蘊含了歸屬程度的 TOC map，稱
為 soft TOC (STOC) map： 
 
其中 。 
 
 
(a)                 (b) 
圖九 SSC 特徵向量由 (a)三個歸屬函數定義
的區間，(b)線條方向與 log-polar 空間所組成。 
 
2.1 動作姿態的方法架構 
給定一段 video clip，我們進行以下的流
程以取得描述動態資訊的特徵： 
(1) 從背景中區分出人體外型輪廓：此處我們
透過訓練背景模型的方式，將背景去除以取
得前景之外圍輪廓。 
(2) 進行重心對齊，並求取 STOC map。 
(3) 對於 STOC map 的各局部區域進行特徵
抽取，並進行比對。 
(4) 總合各局部區域的比對結果。 
以下我們分別對於(3)、(4)步驟進行詳述： 
2.2 局部特徵之抽取 
為了捕捉 STOC map 局部上的時空線條
特性，我們將 shape context 的方法應用在 
STOP map，並命名為 soft shape context (SSC)。 
假設有 STOC map 中有 N 個點，我們分
別計算三個區間的 shape context，當區間為
early 時，每一點標示如下： 
  
其中  和 代表該點在空間上的座標，  
代表該點的方向，  是該點在 上
的比重, )(iuearly= 。在我們希望萃取 SSC
特徵的地方，給定一個中心點 ，
將  周邊分成 R 個 log-polar 區域，如圖四
(b)所示，接著在每個一區域 r，計算線條方向
所形成的統計圖 Bin，如以下式子： 
 
原來 shape context 是統計點數；而本研究則
是累計每一點在各區間的隸屬程度，是一種
軟性的投票。串連起 log-polar 中所有區域的
統計圖，則形成 STOC map 在 early 這個區間
的統計圖，如下： 
  
區間 middle 和 late 都用以上方式計算，最後
串連起三個區間的統計圖則形成在  位置
取出的局部時空的 SSC 特徵向量： 
 
在我們的實驗中，log-polar 區域設定為 17，
線條方向量化成 6 個方向，配合三個時間，
形成具有 306 維度的 SSC 特徵。 
2.3 利用 SSC 辨認運動姿態 
接著我們在運動姿態上萃取 SSC 特徵，
亦即如何放置中心點 ，並進行兩兩動作的
比對。 
 
 
類結果列於表一，橫軸代表預測結果，縱軸
代表 10 個不同的動作。在 Pentium D 3.0 GHz
的系統下，每秒鐘可完成 334 個視訊片段的
SSC 特徵萃取，以及 17540 對片段之間的比
對工作。 
 
表一 視訊片段交互比對結果 
 wave2wave1 walk skip side run pjump jump jack Bend
wave2 99.1 0.9 0 0 0 0 0 0 0 0
wave1 0 93.9 0 0 0 0 6.1 0 0 0
walk 0 0 99.1 0 0 0.9 0 0 0 0
skip 0 0 0 87.3 0 7.0 0 5.6 0 0
side 0 0 0 0 100 0 0 0 0 0
run 0 0 1.8 0 0 98.2 0 0 0 0
pjump 0 0 0 0 0 0 100 0 0 0
jump 0 0 0 1.3 0 0 0 98.7 0 0
jack 0 0 0 0 0 0 0 0 100 0
bend 0 5.4 0 0 0 0 0 0 0 94.6
 
表二 影片交互比對結果 
 wave2 wave1 walk skip side run pjump jump jack Bend
wave2 100 0 0 0 0 0 0 0 0 0
wave1 0 100 0 0 0 0 0 0 0 0
walk 0 0 100 0 0 0 0 0 0 0
skip 0 0 0 77.8 0 11.1 0 11.1 0 0
side 0 0 0 0 100 0 0 0 0 0
run 0 0 11.1 0 0 88.9 0 0 0 0
pjump 0 0 0 0 0 0 100 0 0 0
jump 0 0 0 0 0 0 0 100 0 0
jack 0 0 0 0 0 0 0 0 100 0
bend 0 0 0 0 0 0 0 0 0 100
 
為了測試 SSC 特徵是否能夠抵抗時間不
對稱的影響，我們在兩兩視訊片段中的每張
影像各自取出格點上的 shape context 作為特
徵向量，並總和對應的影像上特徵向量之間
的距離作為比對依據。在此實驗中，分類錯
誤的視訊片段提升到 36 個(錯誤率 3.80%)，
由此實驗可知我們提出的 SSC 特徵的確能夠
抵抗時間不對稱的影響。 
 此外，我們還進行了影片比對的實驗，
給定一個測試影片，我們取出其中間 1/3 部分
作為一視訊片段代表該影片。每個測試片段
與資料庫中某一影片進行比對時，影片被劃
分成長度與測試片段一致的多個視訊片段
們，片段之間彼此重疊 5 張影像。我們取測
試片段與影片中多個片段之間的最短距離，
作為片段到影片的距離。同樣透過 nearest 
neighbor 的分類器，如表二，在 90 段影片中
萃取出的 90 個視訊片段中，有 87 段被正確
分類，只有 3 段被誤判，其中 2 段是單腳向
前跳的動作，另一段是跑步動作。 
如同前人的實驗成果，我們發現單腳往
前跳和雙腳往前跳這兩個動作容易彼此誤
判，經由直覺地推斷，這兩個動作之所以相
似是因為動作行為非常相似。除了測試動作
之間的相似度之外，我們將每個格點上的距
離比對結果作為局部一致性的資訊，如圖十
二，其中暗色的區域表示兩個動作之間是不
相似的；而淺色則代表相似。 
skip     jump      walk     walk        
  
   
圖十二 透過我們的比對方法可以突顯兩動
作之間局部不一致的位置。 
2.4.2. 強韌性之探討 
因本研究基於外圍輪廓資訊，在此測試
我們提出的方法能否抵抗外圍輪廓具不規則
的變形。我們取得不同的情境下所拍攝到的
 
 
fps。我們使用 CodeBook 背景去除法 [Kim et 
al., 2005] 對於影片中的每張 frame 進行前
景物體的剪影偵測。 
在教練影片中，我們手動切出五個視訊
片段各自包含五個招式，在手動切割片段的
過程中，我們只將該招式的核心部分取出，
不包含招式與招式之間過渡的部分，因為過
渡的部分可能因人而異，且與太極拳教學的
目的無關；此外，由於外圍輪廓受視角的限
制，我們設定兩台同步的攝影機取得教練與
學員的正面與側面的姿態，並將兩視角的判
斷結果合而為一。 
在取得教練的五個動作片段之後，我們
把每一個動作分別跟學員影片作比對，進行
方式如 1.4.1 節的影片比對方式，學員影片被
切割成與教練的動作同樣長度的視訊片段，
並彼此重疊 5 張影像，學員與教練之間動作
的比對結果如圖十五所示，雖然這五個動作
彼此有相似之處，但我們的方法還是能正確
比出學員與教練之間動作的對應，而比對分
數也可以作為兩者動作之間的差異程度，譬
如學員的「掤」與「擠」的動作作的比較好，
而「履」與「單鞭」的動作與教練所做的略
有點差異。甚至在此學員比錯「按」的動作
時，我們的方法也能呈現這樣的差異性，如
圖十五所示。 
 
PART 3: 即時人體姿態輪廓差異偵測 
Part 1 與 Part 2 所介紹的方法分別可利
用單張影像與影像的連續運動資訊來進行比
對，但其速度較慢，效果雖然較好，若要加
速可能需要利用到硬體。以下我們介紹加速
方法的版本，此部分強調開法即時簡單的方
法，能夠進行系統整合。 
太極拳之特色之一在於能讓使用者找到
最讓自己放鬆省力的使力方法，達到調適自
我呼吸與心靈的狀態，因而不同的人在打太
極拳時的步調速度是不一致的。在總計畫所
 
       
       
圖十五  太極拳運動姿態的分析。(a)表示教練的五個動作與學員影片的比對結
果，置底標示的線段代表人眼觀測到學員的太極拳動作，(b)手動切割出來的教
練的五個標準動作，左至右依序為掤、履、擠、按、單鞭，(c)由左至右依序代
表在學員影片中所偵測到對應教練動作的片段。 
(a) 
(b) 
(c) 
 
 
便我們描述依此需求所發展之即時人體姿態
輪廓差異偵測方法，流程圖見圖十七。 
3.1 輪廓取樣 
    在進行輪廓比對前需要先將抽取出比對
的輪廓，我們同樣利用 CodeBook 背景去除法
[Kim et al., 2005] 來進行背景學習與前景偵
測，取得影像中人物的剪影與輪廓，見圖十
八。我們在輪廓上收集取樣點來表示此輪
廓，決定取樣點主要有兩種方式，一是固定
取樣個數，另一是固定取樣頻率，為了維持
各部位的輪廓取樣不受其他部位影響，我們
採用後者。取樣結果可見圖十九。 
3.2 輪廓描述 
以往輪廓描述方式為了加強確認相似輪
廓的正確性，所使用的是考量整體輪廓關連
性之描述法，如此原本相似部位會受不相似
部位影響，而這些部位應該被視為 outliers。
我們以人體重心為中心，根據不同角度把輪
廓取樣點分成 16 等份，見圖二十，每區域輪
廓將由區域內的取樣點之梯度向量總合表
示，如下式。 
   
  , 1, 2, ,16
  
  
i
i k
k R
i
k
h t i
R i
t k
∈
= =∑ K
為落於 角度區間的取樣點集合
為取樣點 的梯度向量
 
完成輪廓描述後，進行教練與學員姿勢的相
對區域比對。由於教練與學員有身形上的差
異，每區域輪廓取樣個數會不同，進行比對
時需要經過正規化處理。當滿足下面式子時
該區域表示是相似的，否則不相似。 
{ } { }− ≤
= K
# #
1,2, ,16
S C
i i
S C
i i
h h threshold
R R
i
 
其中  ‘s’ 與  ‘c’ 分別代表學員與教練。此
外，為使比對方法更精確，教練的重心位置
與起始角度將能在附近偏移，以姿勢差異最
小為比對結果。 
3.3 偵測結果 
    局部比對之結果將較為破碎時並無實質
意義。我們可以定義相鄰區域的關係，如區
域 1、2、3 定義為左手，區域 4、5 為頭頸，
區域 13、14、15 為左腳，而區域 16 因取樣
點太少無比較必要。人體為左右對稱，相映
區域表示右邊身體。定義區域關連性後，我
們進行後處理，如連續三個區域 1、2、3 中
有兩個區域不同的話，表示學員與教練左手
姿勢不同。 
比較結果可參見圖二十一。偵測出之錯
誤區域可以提供子計畫四的 Particle System，
以視覺化呈現方式提示學員所應修正的部
位。在偵測速度方面，我們先將影像降至
160×120 並去背後，再回復成 320×240 進行比
對。處理速度約為 62 ms/frame。除輪廓差異
偵測外，我們也發展了另一個模組，利用 W4 
的方法 [Haritaoglu et al., 2000] 來判斷使用
者是否進行手部高揮的動作。當此動作碰觸
到介面中所呈現的光環時，代表使用者希望
將系統還原。如圖二十二所示。此模組也與
總計畫整合，以判斷使用者是否有將系統還
原之意圖。 
 
PART 4: 以 3D 模型為基礎的人體姿態追蹤 
在以上兩部份中，我們提出了一個新穎
的利用影像序列進行姿態辨識的方法，以及
發展了適用於系統整合的快速單張影像比對
技術。在第三部份，我們發展藉由 2D 影像復
原 3D 資訊的方法。分別可分為使用單一攝影
機以及多台攝影機等兩種模式。 
在使用單一攝影機的情況下，其基本問
題 為 電 腦 視 覺 中 之 運 動 分 割  (motion 
segmentation) 問題。當給定多個影像中的對
應點，並假設這些對應點是由多個三維空間
中的剛性運動 (3D rigid motion) 所造成時，
運動分割的問題探討如何由這些對應點還原
出這些運動，並進一步藉由不同的運動方式
 
 
此外，在視訊追蹤的部份，我們也探討
另一個相關的問題 ─ 如何利用特徵點之間
的幾何關係來讓追蹤的結果更穩定，以利於
物體的部件追蹤  (part-based tracking)。在 
particle filtering 的架構下，我們發展了一個
可利用影像中特徵點彼此的幾何關係，來加
強 particle 之預測準確性的方法 [Chang et al., 
2008]，發表於期刊 IEEE Trans. SMCB。此方
法 可 以 藉 由 追 蹤 部 件 彼 此 間 的 互 相 預 測 
(cross-prediction) 來提升物體追蹤的準確度。 
然而以上的方法，由於只使用單一攝影
機，仍難以進行完整的 3D 姿態還原。在單一
視角的影像序列上還原人體的 3D 姿態的研
究中，缺點是易受深度與自我遮蔽的影響 
[Deutscher et al., 2000]。為了克服此問題，我
們發展了一個使用多台攝影機來進行 3D 姿
態追蹤的方法。過去某些研究皆發展基於單
一視角的方法，當擁有多視角的資料時，則
求取各視角誤差相加之最小值來進行估測。
但直接累加各視角的誤差的問題在於當姿態
運動差異只在部分視角有意義時，其餘視角
將影響正確估測。在 [Kakadiaris et al., 2000] 
中，便提出考量不同視角的姿態變化，只選
取較具有參考價值的攝影機進行估測。  
因人體肢體間存在著關聯性，可視為一
階層架構，使用人體模型的好處在便於讓人
體姿態能符合運動學限制，這對擁有高自由
度的人體動作有著很重要的助益。雖然我們
的 3D 人體模型基礎的人體姿態追蹤法還在
發展階段，但已有初步成果，接者我們便介
紹目前的研究方法與成果，及未來發展方向。 
我們使用的 3D 人體模型式由簡單的幾
圖二十一、1 和 3 行是學員目前姿勢，2 和 4 行是教練關鍵姿勢。第一列的姿勢為雲手，
第二列的姿勢為太極拳中的擠式。紅線標示出姿勢輪廓差異部分，藍線則為相似部分。
                        (a)                                                    (b)                                                (c) 
圖二十二：(a)  使用者的 Silhouette, (b) X 軸投影長條圖, (c) Y 軸的投影長條圖
 
 
行姿態追蹤。由於人體動作彈性很高，當我
們依人體模型將人體的姿態以有 22 個參數維
度的空間向量表示時，參數空間的分布是呈
現非線性且多模型的。我們使用 Particle Filter
追蹤法 [Isard, M. et al., 1988] 進行姿態追
蹤。然而 Particle Filter 主要的缺點是追蹤準
確性乃根據參數維度所需的 particle 個數，如
此計算時間將隨著 particle 個數需求呈現指數
成長，在人體姿態此類的高維度肢體追蹤是
需要耗費大量時間的，且在高維空間中因
particle 個數覆蓋程度不足，容易找到區域最
佳解而非問題最佳解。有許多相關的研究即
在加速追蹤的進行，提出的解決方式包括有
收集訓練資料訓練出姿態運動模型，限制空
間分布，減少估測範圍，這類方法可以有效
地加快追蹤速度，但最大的缺點是需要大量
訓練資料以及當人體擺出資料庫沒有的姿態
時，便無法作用。 
針對 Particle Filter 本身估測特性從事改
進時，我們使用的方法為階層式的追蹤法，
人體模型為階層架構，當決定軀幹位置時，
其餘四肢將可以分別追蹤，指數成長的難度
將分解成線性成長的難度。我們追蹤的順序
為軀幹、左手、右手、左腳、右腳，我們不
考慮頭部的動作，其位置在軀幹估測完後即
被固定。 
由於我們建立的是 3D 的人體體積，在我
們先對軀幹進行 6 維度的追蹤後，我們將能
夠標記人體 voxels 為軀幹，在下一步的肢體
追蹤時將不用考慮被標記的 voxels，追蹤完的
肢體同樣對 voxels 進行標記。由於被考慮的
voxel 個數愈後面愈少，速度亦可再次增進，
特別是占據大部分 voxels 的軀幹在第一次追
蹤便已完成。 
在利用 particle filtering 的方法中，最常
遇 到 的 困 難 就 是 高 維 度 (high degrees of 
freedom)的問題。我們對於軀幹與四肢的階層
式設計能夠讓高維度的問題更容易獲得解
決。在軀幹的部份有個自由度，而四肢各有 4
個自由度，因而總共為 22 個自由度。我們首
大略追蹤軀幹，再追蹤四肢。而四肢追蹤後，
軀幹的方位也會在隨之調整以便估測得更準
確。方法的流程如圖二十六所示。 
 
 
圖二十六：多攝影機三維姿態追蹤流程圖 
 
我們的方法稱為 soft-joint ICP。原本的
每個四肢部份的自由度為 4。但實際上由於每
個部份的肌肉活動，以及可能的影像前景分
割不準度的影響，若能允許每個關節可額外
在小範圍三度空間內平移，亦即增加三個自
由度（稱為 soft-joint constraint）的話，通常
會 讓 三 維 姿 態 的 估 測 更 容 易 進 行 
[Mundermann et al., 2007]。因此每個四肢的自
由度變成了 7。我們方法的特色在於將其中 6
個自由度利用 iterated closed point （ICP）的
方法來求取。當另外剩下的一個自由度（例
如對於右手臂而言，此自由度為上臂與下臂
之間的夾角）固定時，其餘的六個自由度即
會形成一個剛性變換（旋轉＋平移）。而 ICP
可以比佈建 particle 的方式，更有效率地還原
此 6 個自由度。因此在我們的方法中，只要
針對剩下的一個自由度（1 維空間）佈建 
particle 即可。如此的設計大幅簡化了原先需
要在 7 維空間中進行 particle filtering 的困難。 
當四肢的方位追蹤完畢後，可進一步追
蹤軀幹的位置，因為四肢的方位提供了軀幹
很好的預測值。如圖二十七所示。 
 
 
[Deustcher et al., 2000] Deutscher, J., Blake, A., 
and Reid, I., “Articulated body motion capture 
by annealed particle filtering”, Proc. IEEE 
Conference on Computer Vision and Pattern 
Recognition, CVPR 2000.  
[Efros et al., 2003] Efros, A., Berg, A., Mori, G., 
and Malik, J., “Recognizing action at a 
distance,” Proc. IEEE International Conference 
on Computer Vision, ICCV 2003. 
[Haritaoglu et al., 2000] Haritaoglu, D. 
Harwood, and L. S. Davis, “W4 :real-time 
surveillance of people and their activities,” 
IEEE Trans. on PAMI, vol. 22, no. 8, pp. 
809--830, August 2000. 
[Hasenfratz, J. et al., 2004] Hasenfratz, J., 
Lapierre, M., and Sillion, F., “A Real-Time 
System for Full Body Interaction with Virtual 
Worlds”, Proc. Eurographics Symposium on 
Virtual Environments, 2004. 
[Hsiao et al., 2008] Hsiao P. C., Chen C. S., 
Chang L. W., "Human Action Recognition 
Using Temporal-State Shape Contexts," 
International Conference on Pattern 
Recognition, ICPR 2008. 
[Huang, et al. 2006] Huang, C. R., Chen, C. S., 
and Chung, P. C. “Contrast Context 
Histogram – A Discriminating Local 
Descriptor,” Proceedings of International 
Conference on Pattern Recognition, ICPR 2006. 
[Huang and Chen, 2009] Huang J. B. and Chen 
C. S., "Moving Cast Shadow Detection Using 
Physics-based Features," IEEE Computer 
Society Conference on Computer Vision and 
Pattern Recognition, CVPR 2009. 
[Jian and Chen, 2007] Jian, Y. D. and Chen, C. 
S., “Two-View Motion Segmentation by 
Mixtures of Dirichlet Process with Model 
Selection and Outlier Removal,” Proc. 
International Conference on Computer Vision, 
ICCV 2007. 
[Jones and Rehg, 2002] Michael J. Jones and 
James M. Rehg, ”Statistical Color Models with 
Application to Skin Detection,” International 
Journal of Computer Vision, Vol. 46, No. 1 
2002. 
[Kehl et al., 2005] Kehl, R., Bray, M., and Gool, 
L. V., “Full body tracking from multiple views 
using stochastic sampling”, Proc. IEEE 
Conference on Computer Vision and Pattern 
Recognition, CVPR 2005. 
[Kim et al., 2005] Kim, K., Chalidabhongse, T. 
H, Harwood, D., Davis, L. S., “Real-time 
Foreground-Background Segmentation Using 
Codebook Model,” Real-Time Imaging, 11(3) 
(2005) 167-256. 
[Laptev and Perez, 2007] Laptev, I., and Perez, 
P., “Retrieving actions in movies,” Proc. IEEE 
International Conference on Computer Vision, 
ICCV 2007. 
[Michoud et al., 2007] Michoud, B., Guillou, E., 
Briceño, H., and Bouakaz, S., “Real-Time 
Marker-free Motion Capture from multiple 
cameras”, Proc. IEEE International Conference 
on Computer Vision, ICCV 2007. 
[Mundermann et al., 2007] Mundermann, L.; 
Corazza, S.; Andriacchi, T.P. “Accurately 
measuring human movement using articulated 
ICP with soft-joint constraints and a repository 
of articulated models,” Proc. IEEE 
International Conference on Computer Vision 
and Pattern Recognition, CVPR 2007. 
[Roh et al., 2008] Roh, M.-C., Christmas, B., 
Kittler, J., and Lee, S.-W., “Gesture spotting for 
low-resolution sports video annotation,” Pattern 
Recognition, 41(3):1124–1137, 2008.  
[Sminchisescu et al., 2005] Sminchisescu, C., 
Kanaujia, A., Li, Z., and Metaxas, D., 
“Conditional models for contextual human 
motion recognition,” Proc. IEEE International 
Conference on Computer Vision, ICCV 2005. 
[Stauffer and Grimson, 2000] C. Stauffer and W. 
E. L. Grimson. “Learning patterns of acitivty 
using real-time tracking”. IEEE Trans. on PAMI, 
22(8):747–757, Aug. 2000. 
[Wang and Suter, 2007] Wang, L., and Suter, D., 
“Learning and matching of dynamic shape 
manifolds for human action recognition,” IEEE 
Trans. on Image Processing, 16(6):1646–1661, 
2007. 
 
 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：98 年 10 月 30 日 
國科會補助計畫 
計畫名稱：結合多媒體互動與生理回饋技術建立智慧型太極拳輔助
學習環境 -- 以電腦視覺技術進行姿態分析及其於太極拳輔助學習
之應用 
計畫主持人：陳祝嵩 
計畫編號：95-2221-E-001-028-MY3 學門領域：圖形識別 
技術/創作名稱 由影像序列中前景外圍輪廓進行姿態比對 
發明人/創作人 陳祝嵩、蕭佩琪 
中文： 
本技術首先藉由時間上模糊區間的分割，分別抽取出一個姿態的早
期、中期、與晚期之前景外圍。將每個時期的所有外圍輪廓，透過
模糊權重的方式，疊合成一個時空特徵圖。並藉由 shape context 作
為此時空特徵圖的特徵表示，以從事姿態比對。 
技術說明 
英文： 
We divide a gesture by fuzzy time intervals. In each interval, the fuzzy 
union of all the frames’ foreground contours is constructed and forms a 
temporal overlapping contour map. We use the shape-context method 
to extract the features for the constructed maps. The features of 
different intervals are then integrated for gesture matching. 
可利用之產業 
及 
可開發之產品 
 
技術特點 
本技術透過模糊區間的分割，可降低因動作在時間上不均勻所造成
的比對誤差。 
推廣及運用的價值  
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研發
成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
附件一 
表 Y04 
因此如何透過機器學習或圖形識別的方法，由其中發掘出 common patterns，是一個重
要的研究課題。在本會議中可觀察到這方面與 image category recognition 的研究有日益
密切結合之趨勢。 
 
相對於 Internet 上眾多但不穩定的素材，在本次會議中 Princeton 大學的 Li Fei-Fei 教
授團隊報告其特別發展的一個 ImageNet 的影像資料庫，能提供百萬張等級的清楚標註
的影像資料。此資料庫可類比於文字領域非常著名的 WordNet 資料庫。同時此資料庫
的標註也具有階層化的特性，能夠將一個領域的概念以樹狀的階層性加以表示。如何建
構這麼龐大而清晰的資料庫是一大挑戰，這方面的研究也將對未來 image category 
recognition 提供了非常大的進步空間。 
 
在 image and video search 方面，Oxford University 之 Zisserman 教授的團隊在 pose 
search 的論文中整合了數項技術，發展由視訊中搜尋特定姿態的方法。過去曾有針對特
定重複動作的事訊進行比對與搜尋的研究，但對於一般影片視訊中千變萬化的場景與人
物而言，仍然是一個困難的問題。本篇論文透過  human detection 以及  body-parts 
inference 來發展出特徵向量，並探討如何設計 descriptor 以及相似度量測來達到較為準
確的姿態搜尋。由於我們也正在進行姿態比對之研究，而本論文是嘗試在一般視訊中以
人的姿態來進行搜尋的先驅之作，很具有參考價值。 
 
在影像分割方面，也有更快速的方法被提出。德國波昂大學的團隊提出了一個利用平面
圖的特性來加速 graph cut 的方法，比起過去一般圖形上的 graph cut 方法更有效率。
由於在影像分割與比對的問題中常能夠化簡成為平面圖的模式，因此本方法能夠讓影像
序列中的物體分割與比對更加快速。而在物體辨認方面，如何在部份遮蔽的狀況下進行
辨認，是一個需要探討的問題。Ohio State University 電機系的團隊在機器學習的領域尋
求改進此一問題。他們提出了一個新的 support vector machine 模式，能夠減低部份遮
蔽所造成的影響。 
 
在人的偵測 (human detection) 的探討方面也有日益增溫的趨勢。由於過去人臉偵測的進
步，帶動了許多應用。相較於人臉偵測而言，人的偵測是更困難與具有挑戰性的問題，
也因此吸引了許多關注。馬里蘭大學 Larry Davis 教授的團隊提出以 multiple instance 
learning (MIL) 進行人偵測器學習的方法。由於在訓練資料的蒐集中，常常很難精確地
表 Y04 
常出色的口頭報告，以直覺簡單的方式呈現其背後的創意，獲得滿場喝采。後者也深入
探討了訊號分析裡的難題，blind deconvolution, 發掘與比較許多數學模型背後的意義。
但其概念較為艱深，且報告者並未特別關注直觀上的解釋，因此在現場口頭報告的呈現
上比起前一篇論文明顯要略遜一籌。 
 
整體而言，個人覺得此次會議相當成功。由於許多論文都非常精彩，口頭報告時常常座
無虛席，壁報論文場次也是絡繹不絕，大家無不希望藉此難得的機會，能夠對於先進的
研究觀念作綜合的吸收。本人能夠參加此一高水準的會議，接觸現階段電腦視覺尖端研
究成果的第一手資料，感覺收穫良多，相信對於未來研究必定甚有助益。 
 
攜回資料名稱及內容：CVPR 2009 論文光碟片。 
 
 
 
 
ing a better description of cast shadows, the shadow model
in [6] provided improved performance over the previous ap-
proaches which used linear models.
However, these learning-based approaches [9, 5, 4, 6]
may suffer from insufficient training samples since the sta-
tistical models are learned from background surface varia-
tion under cast shadows. Unlike obtaining samples in ev-
ery frame in background modeling, shadows may not ap-
pear at the same pixel in each frame. A single pixel should
be shadowed many times till its estimated parameters con-
verge, while the illumination conditions should be stable.
Therefore, this kind of pixel-based shadow models require
a longer period of training time when foreground activities
are rare. This problem becomes more serious when video
sequences are captured at a low or unsteady frame rate that
depends on the transmission conditions.
In this paper, we characterize cast shadows with “global”
parameters for a scene. Based on the bi-illuminant dichro-
matic reflection model (BIDR) [7], we first derive nor-
malized spectral ratio as our color features under the as-
sumptions of constant ambient illumination and direct light
sources with a common SPD. The normalized spectral ra-
tio remains constant regardless of different background sur-
faces and illumination conditions. We then model the color
features extracted from all moving pixels using a single
Gaussian Mixture Model (GMM). To further improve the
differentiating ability for cast shadows having similar col-
ors to background, we use a pixel-based GMM to describe
the gradient intensity distortion for each pixel. We update
the pixel-based GMMs using the confidence predicted from
the global GMM through confidence-rated learning to ac-
celerate convergence rates. Contributions are presented in
two key aspects. Firstly, with the global shadow model
learned from physics-based features, our approach does not
require numerous foreground activities or high frame rates
to learn the shadow model parameters. This makes the pro-
posed method more practical than existing works using only
pixel-based models. Secondly, the proposed confidence-
rated learning can be used for fast learning of local features
in pixel-based models. We provide a principled scheme for
the local and global features to collaborate with each other.
The remainder of this paper is organized as follows.
We briefly describe in Section 2 the dichromatic reflection
model [13] and its extension BIDR. In Section 3, we present
the proposed learning approach. The posterior probability
of cast shadows and foreground are developed in Section 4.
Both visual and quantitative results are shown in Section 5
to verify the performance of our method and the robustness
to few foreground activities. Section 6 concludes this paper.
2. Physics-Based Shadow Model
2.1. Bi-illuminant Dichromatic Reflection Model
There are three terms in the Shafer’s model [13]: body
reflection, surface reflection, and a constant ambient term.
Each of the two reflection types can be decomposed into
chromatic and achromatic parts: 1) composition: a relative
SPD cb or cs which depends only on wavelength, 2) mag-
nitude: a geometric scale factor mb or ms which depends
only on geometry. Given a scene geometry, the radiance in
the direction (θe, φe) can be expressed as
I(θe, φe, λ) = mb(θe, φe)cb(λ)+ms(θe, φe)cs(λ)+cd(λ),
(1)
where cd is the constant ambient term.
While this model included a term to account for ambi-
ent illumination, the model did not separate it into body and
surface reflection. Recently, Maxwell et al. [7] proposed
the BIDR model, which contains four terms: two types of
reflection for both the direct light sources and ambient illu-
mination. Then, the BIDR model is of the form:
I(θe, φe, λ) = mb(θe, φe, θi, φi)cb(λ)ld(θL, φL, λ) (2)
+ms(θe, φe, θi, φi)cs(λ)ld(θL, φL, λ)
+cb(λ)
∫
θi,φi
mb(θe, φe, θi, φi)la(θL, φL, λ)dθidφi
+cs(λ)
∫
θi,φi
ms(θe, φe, θi, φi)la(θL, φL, λ)dθidφi,
where (θL, φL) is the direction of the direct light source
relative to the local surface normal, and (θe, φe) and (θi, φi)
are the angles of emittance and incidence, respectively. In
this reflection model, the range of mb, ms, cb, and cs all
lie in [0, 1] (we refer reader to [7] for further details of the
deviation of BIDR model.)
With a specific geometry and representing the two am-
bient integrals as Mab(λ) and Mas(λ), we can simplify the
BIDR model to
I(λ) = cb(λ)[mbld(λ) +Mab(λ)] (3)
+cs(λ)[msld(λ) +Mas(λ)].
Considering only matte surfaces, we can ignore the lat-
ter part of (3). To describe the appearance of cast shadows
on a background surface, we multiply the direct illumina-
tion with an attenuation factor α ∈ [0, 1], which indicates
the unoccluded proportion of the direct light. We assume
that all direct light sources have a common SPD with dif-
ferent power factor and the ambient illumination is constant
over lit and shaded regions (see Fig. 1). This gives us the
simplified form of the BIDR model
I(λ) = αmbcb(λ)ld(λ) + cb(λ)Mab(λ). (4)
2311
pixel in shadow regions using the given image and the ref-
erence background models. The resultant feature distribu-
tions are presented in Fig. 2 (e)-(h). The feature values ex-
tracted from different background surfaces roughly follow
a straight line in the SRSGSB space. Therefore, we can
use the direction of the line as our color feature, which is
roughly the same for all shadow pixels. The direction of the
line in the SRSGSB space can be characterized with two
angles, the zenith and azimuth in the spherical coordinate,
which correspond to the normalized spectral ratio. From the
feature distributions in Fig. 2 (e)-(h), we also observe that
larger feature values tend to be unstable and deviate from
the major direction of most feature values. This is because
there is not sufficient difference between the shaded and lit
pixel value to robustly measure the orientation. In addi-
tion, the value of α(1−α) in the scene can be easily estimated
by intersecting the fitted line with the line passing through
(1,1,1) and the origin.
2.2.2 Gradient Intensity Distortion
Now we have derived color features that are invariant to dif-
ferent background surfaces. This low dimensional (2D) fea-
tures, however, might fail to distinguish foreground with
colors similar to background from cast shadows. Thus,
other shadow-induced properties like edge or gradient in-
formation may be used to further describe the background
appearance variation under cast shadows. In this paper, we
just use a simple gradient intensity distortion as our local
features to demonstrate the improvement by incorporating
additional local features.
For a given pixel p, we define the gradient intensity dis-
tortion ωp as
ωp = |∇(BG)p| − |∇(F )p|, (10)
whereBG and F are luminance channels of the background
image and current frame, and ∇(·) is the gradient operator.
3. Learning Cast Shadows
In this section, we show how to build models for cast
shadows in an unsupervised way. Here, we use GMM to
learn the background surface variation over time. It is also
possible to use other statistical learning method such as ker-
nel density estimation.
3.1. Weak Shadow Detector
To model the cast shadows, impossible shadow sam-
ples that belong to background or foreground (e.g. color
values that are the same as or brighter than background
values) should be excluded. Therefore, we apply a weak
shadow detector that evaluates every moving pixel to filter
out some impossible samples. Since cast shadows reduce
Figure 3. The weak shadow detector. The observation will be con-
sidered as potential shadow point if it falls into the gray area. The
weak shadow detector contains three parameters: maximum al-
lowed color shift, and minimal and maximal illumination attenua-
tion.
the luminance values, the potential shadow values should
fall into the conic volume around the corresponding back-
ground color. The weak shadow detector is illustrated in Fig
3, where values of cast shadows are expected to fall into the
gray conic region. Pixel values that fall into the gray conic
region are considered as potential shadow samples. These
samples are then used to learn the global shadow model for
the scene and the local shadow model for each pixel.
3.2. Global Shadow Model
Using the background surface invariant color features, a
global shadow model is learned for the whole scene. Here,
we model the background color information by the well-
known GMM [15] in the RGB color space. For every frame,
we obtain potential shadow points by applying the weak
shadow detector on moving pixels, which are identified via
background subtraction. We then use one GMM to learn
the normalized spectral ratio rˆ = [γˆR, γˆG]T in the scene.
Note that the reason why we only use two of the three di-
mensional features is that the third component is redundant
since γˆR2 + γˆG2 + γˆB2 = 1. The normalized spectral ratio
in the whole scene is modeled by K Gaussian distributions
with mean vector µk and full covariance matrix Σk. Then,
the probability of the normalized spectral ratio rˆ is given
by:
p(rˆ|µ,Σ) =
K∑
k=1
pikGk(rˆ, µk,Σk), (11)
where µ,Σ denote all parameters of the K Gaussians, pik is
the mixing weight, and Gk is the kth Gaussian probability
distribution. We use the Expectation-Maximization (EM)
algorithm to estimate the parameters in the GMM. The es-
timated parameters in the current frame are propagated to
next frame, so that the EM algorithm can converge quickly.
Since the light sources are usually stable in the scene, we
2313
the learned shadow models. Based on the probability the-
ory, the foreground posterior can be obtained as:
P (FG|xp) = 1− P (BG|xp)− P (SD|xp). (16)
4.3. Summary
Algorithms 1 and 2 summarize in pseudocode the learn-
ing and detection processes of the proposed algorithm. Our
method can be attached to other moving object detection
programs as an independent module. Shadow detection pro-
cess is only applied to moving pixels detected by the back-
ground model and the learning process occurs only when
these moving pixels are considered as shadow candidates.
Consequently, the proposed algorithm is practical, it does
not introduce heavy computational burden and can work ef-
fectively to detect shadows.
Algorithm 1: Learning Process
At time t,
for each pixel p in the frame do
if P (BG|xt(p)) < 0.5 then
if pixel p satisfies shadow property then
-Compute normalized spectral ratio γˆp
-Compute gradient intensity distortion ωp
-Update local shadow model at pixel p
using confidence value C(γˆ) through
confidence-rated learning
end
end
end
Run one EM iteration to estimate the parameters of
global shadow model using the collected color
features.
Algorithm 2: Detection Process
At time t,
for each pixel p ∈ P do
-Obtain background posterior P (BG|xp) from
background modeling
-Compute shadow posterior P (SD|xp)(eq. 14)
-Compute foreground posterior P (FG|xp)(eq. 16)
if P (FG|xp) > P (SD|xp)&P (FG|xp) >
P (BG|xp) then
Label pixel p as foreground
else
Label pixel p as background
end
end
5. Experimental Results
We present the visual results from challenging video se-
quences captured in various environments, including both
indoor and outdoor scenes. We also compare the quantita-
tive accuracy of the proposed method in several videos with
other approaches when the results are available. Previous
approaches using statistical model have higher success rate
in detecting cast shadow when numerous foreground activi-
ties are present. However, we show that our method can deal
with the situation that cast shadows first appear in complex
scenes and unknown illumination conditions as well as rare
foreground activity.
5.1. Qualitative Results
In Figure 4, we show sample cast shadow detection re-
sults from four video sequences. The first three sequence:
Laboratory, Intelligent Room, and Highway I are part of the
benchmark sequences for validating shadow detection al-
gorithm. The last one Hallway is taken from [6]. Figure
4 (a) shows one frame selected from the video, where cast
shadows are present in the scene. The background poste-
rior probability is presented in Figure 4(b), where the dark
region indicates the less probability of belonging to back-
ground. From Figure 4(c)(d), we show the confidence map
of global shadow model and the posterior probability of cast
shadows, respectively. In Fig. 4(e) we show the probability
values of belonging to foreground objects. We can see that
in these video sequences, the proposed algorithm is capa-
ble of detecting cast shadows without misclassifying fore-
ground as shadows. Note that in the second video, Intelli-
gent Room, the man just walks in the room by once. Thus,
there is no chance for the pixel-based shadow model to learn
its parameters. The use global shadow model enables us to
detect shadows first appear in the scene.
To verify the effectiveness of the proposed method, the
results presented here are raw data and without any post-
processing. We can obtain binary results simply with
thresholding the foreground posterior values P (FG|xp).
The posterior probabilities can also be incorporated with
context model that use spatial and temporal coherence to
improve the segmentation accuracy.
5.2. Quantitative Results
The quantitative evaluation follows the method proposed
by Prati et al. [10]. There are two defined metrics for eval-
uating the performance of cast shadow detection algorithm:
shadow detection rate η and shadow discrimination rate ξ.
The formulations of the two metrics are as follows:
η =
TPS
TPS + FNS
; ξ =
TPF
TPF + FNF
, (17)
2315
Table 1. Quantitative results on surveillance sequences
Sequence Highway I Highway II Hallway
Method η% ξ% η% ξ% η% ξ%
Proposed 70.83 82.37 76.50 74.51 82.05 90.47
Kernel [6] 70.50 84.40 68.40 71.20 72.40 86.70
LGf [4] 72.10 79.70 - - - -
GMSM [5] 63.30 71.30 58.51 44.40 60.50 87.00
models are still not built due to the long training time and
disturbance by foreground objects.
5.4. Handling Scene with Few Foreground Activities
Here we use the benchmark sequence, Intelligent Room,
to demonstrate the robustness of our approach to videos
captured at low frame rates and scenes with few foreground
activities. Downsampled sequences with lower frame rates
are obtained by taking a sample from the original image se-
quence for every M ∈ {2, 3, 4} samples. Thus, we have
sequences with 10, 5, 3.33, and 2.5 frame rates, respec-
tively. Figure 6 shows the quantitative results on these se-
quences. We can see that the performance on sequences
lower frame rates degraded slightly, demonstrating that our
approach can still learn and removing cast shadows even in
such low frame rates.
6. Conclusion
In this paper, we have presented a novel algorithm capa-
ble of detecting cast shadows in various scenes. Qualitative
and quantitative evaluation of the physics-based shadow
model validated that our approach is more effective in de-
scribing background surface variation under cast shadows.
The physics-based color features can be used to learn a
global shadow model for a scene. Therefore, our method
does not suffer from the problem of insufficient training
data as in pixel-based shadow models. Moreover, with the
aid of the global shadow model, we can update the local
shadow models through confidence-rated learning, which is
significantly faster than conventional online updating. To
further improve the detection accuracy, more discriminative
features or the spatial and temporal smoothness constraints
can also be incorporated into the detection process in the
future.
Acknowledgement
This work was supported in part by the National Science
Council of Taiwan, R.O.C., under Grant NSC95-2221-E-
001-028-MY3.
References
[1] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati. Detecting
moving objects, ghosts, and shadows in video streams. IEEE
Trans. PAMI, 25(10):1337–1342, 2003. 1
[2] T. Horprasert, D. Harwood, and L. S. Davis. A statistical
approach for real-time robust background subtraction and
shadow detection. In ICCV Frame-rate Workshop, 1999. 1
[3] D. S. Lee. Effective gaussian mixture learning for video
background subtraction. IEEE Trans. PAMI, 27(5):827–832,
2005. 5
[4] Z. Liu, K. Huang, T. Tan, and L. Wang. Cast shadow removal
combining local and global features. In CVPR, pages 1–8,
2007. 1, 2, 7, 8
[5] N. Martel-Brisson and A. Zaccarin. Learning and removing
cast shadows through a multidistribution approach. IEEE
Trans. PAMI, 29(7):1133–1146, 2007. 1, 2, 8
[6] N. Martel-Brisson and A. Zaccarin. Kernel-based learning
of cast shadows from a physical model of light sources and
surfaces for low-level segmentation. In CVPR, June. 2008.
1, 2, 6, 7, 8
[7] B. Maxwell, R. Friedhoff, and C. Smith. A bi-illuminant
dichromatic reflection model for understanding images. In
CVPR, pages 1–8, June 2008. 2, 3
[8] S. Nadimi and B. Bhanu. Physical models for moving
shadow and object detection in video. IEEE Trans. PAMI,
26(8):1079–1087, 2004. 1
[9] F. Porikli and J. Thornton. Shadow flow: a recursive method
to learn moving cast shadows. In ICCV, pages 891–898 Vol.
1, Oct. 2005. 1, 2
[10] A. Prati, I. Mikic, M. Trivedi, and R. Cucchiara. Detecting
moving shadows: algorithms and evaluation. IEEE Trans.
PAMI, 25(7):918–923, 2003. 3, 6
[11] E. Salvador, A. Cavallaro, and T. Ebrahimi. Cast shadow seg-
mentation using invariant color features. CVIU, 95(2):238–
259, 2004. 1
[12] O. Schreer, I. Feldmann, U. Golz, and P. A. Kauff. Fast and
robust shadow detection in videoconference applications. In
IEEE Int’l Symp. Video/Image Processing and Multimedia
Communications, pages 371–375, 2002. 1
[13] S. Shafer. Using Color to Separate Reflection Components.
Color Research Applications, pages 210–218, 1985. 2
[14] J. Stander, R. Mech, and J. Ostermann. Detection of moving
cast shadows for object segmentation. IEEE Trans. Multime-
dia, 1(1):65–76, Mar 1999. 1
[15] C. Stauffer and W. E. L. Grimson. Adaptive background mix-
ture models for real-time tracking. In Proc. CVPR, volume 2,
pages –252, 1999. 4, 5
[16] W. Zhang, X. Fang, X. Yang, and Q. M. J. Wu. Moving cast
shadows detection using ratio edge. IEEE Trans. Multime-
dia, 9(6):1202–1214, 2007. 1
2317
表 Y04 
參加 ICPR2006 報告書 
陳祝嵩 
International Conference on Pattern Recognition 2006 (ICPR)是電腦視覺與圖形識
別領域中具有領導地位的國際會議之一。本次參加的 ICPR2006 會議於香港舉
行。此次會議共接受了 311 篇口頭論文(oral paper)與 857 篇壁報論文(poster 
paper)。本次會議的議題涵蓋了電腦視覺與圖形識別領域上最近的技術與趨勢，
包括 reconstruction, segmentation, geometry, calibration, image and feature analysis, 
image analysis applications, video analysis, structure from motion、video and object 
recognition、learning、faces 及 tracking 等目前研究上熱門之題目。此次會議除了
有邀請國際頂尖學者來給演講外，並安排有分領域的報告以及海報展示的區
域。讓每位與會的人士都能盡情的到各個感興趣的領域聽別人的報告演講或到
海報展示的地點與其他國際專業人士討論最新的技術發明並且交換意見。充分
達到吸收最新知識與學術交流的效果。 
由於 ICPR 收錄的論文橫跨了許多領域，除了電腦視覺的相關應用之外，更包括
了語言處理、機器人學、乃至於（生醫）訊號處理等。整個議程是採 Multi-track
模式進行的，因此同時間有數個不同主題的 sessions 進行，而由每個 session 間
出席人數的多寡，也可以概略地觀察出目前研究方向的趨勢與熱門程度。總體
來說，Face & Gesture Recognition、Surveillance、Learning Algorithm 等都是大家
比較感興趣的主題。一些以應用為導向的主題獲得較多的青睞並不令人意外，
表 Y04 
有興趣的人士直接與本人接洽，進行了長時間的討論。 
由於目前部分研究方向放在表情的分析上，因此會特別注意這方面的報告，其
中印象比較深刻的是 C.S. Lee 等人發表的論文“Nonlinear shape and appearance 
models for facial expression analysis and synthesis”，在論文中他們提到對不同的
人、不同的表情個別做 manifold learning，這個觀念跟我們的想法有些類似。而
F. Dornaika 等人則用了 auto-regressive model 來作表情分析，不過感覺上跟他們
在 ICCV05 的論文很相似，可能需要再仔細研讀比較。另外，也聽了一些 tracking
相關的報告，為了更快速地估測身體的姿態，有些學者採用 depth map 的資訊來
快速判別出身體的位置；為了避免遮蔽影響，Huang 等人提出 data-driven 的觀
念，先偵測出人頭的位置，再依序地利用 spatial 的關係找出身體的其他部位；
Guo 等人則提出 descriminative-based 的方式以 GMM descriptor 來表示姿態的輪
廓，進而對姿態作推理。Osawa 等更採用了多個相機建立出 3D model 及背景資
訊，用以幫助精確的追蹤研究，他們的論文也獲得了最佳論文獎。在 Surveillance
人物追蹤的應用中，不同於之前的以頭或以腳為主的追蹤方法，Peursum 等人則
合併了兩者的概念，並可以隨時做切換，而能達到更佳的效果。 
此外在 Image Analysis Applications 相關論文發表中，許多研究人員提出了有關
image analysis 之應用，例如 Automatic Segmentation of the Knee Bones Using 3D 
Active Shape Models 與 Modeling Crowd Scenes for Event Detection。在 Video 
Analysis and Tracking 相關論文發表中如 Video Local Pattern Based Image 
 Contrast Context Histogram – A Discriminating Local Descriptor for Image 
Matching
Chun-Rong Huang1,2, Chu-Song Chen1 and Pau-Choo Chung2
1. Institute of Information Science, Academia Sinica, Taipei, Taiwan  
2. Dept. of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan 
{nckuos, song}@iis.sinica.edu.tw, pcchung@ee.ncku.edu.tw 
Abstract 
This paper presents a new invariant local descriptor, 
contrast context histogram, for image matching. It 
represents the contrast distributions of a local region, 
and serves as a local distinctive descriptor of this 
region. Object recognition can be considered as 
matching salient corners with similar contrast context 
histograms on two or more images in our work. Our 
experimental results show that the developed 
descriptor is accurate and efficient for matching. 
1. Introduction 
Image matching is difficult due to large variations of 
possible appearances of objects or scenes. The 
appearance variations might be caused by scale 
changes, rotations, different lighting conditions, and/or 
partial occlusions. Recently, invariant local descriptors 
constructed from images have been proposed to 
overcome appearance variations in object recognition 
[1][6][9]. The idea is to detect invariant local 
properties of salient image corners under a class of 
transformations, and then establish discriminating 
descriptors for these corners. In the past, Freeman and 
Adelson [3] developed steerable filters, which 
synthesize filters of arbitrary orientations from linear 
combinations of pixel derivatives in particular 
directions. Belongie et al. [1] proposed shape context 
that is a histogram of edge points with respect to a 
reference point under the log-polar coordinate. Ojala et
al. [10] proposed a circularly symmetric binary pattern 
to discriminate textures. The gray value of the center 
point is subtracted by those of the local neighborhood 
points, and then quantized by a threshold to form a 
binary pattern. Lowe [6] proposed a scale invariant 
feature transformation (SIFT) descriptor that is 
invariant to scale and rotation. He computed 
discriminating image features through the detection of 
scale-space extremes. Then, invariant descriptors are 
constructed by using a weighted orientation histogram 
around the feature point.  
In this paper, we propose a new invariant local 
descriptor called contrast context histogram (CCH) 
and apply it to image matching. CCH exploits contrast 
properties of a local region. Rotation and linear 
illumination changes are considered to make it robust 
against geometric and photometric transformations. In 
the experiments, we use CCH descriptors to represent 
clutter scenes and show its successfulness in image 
matching. 
2. The CCH Descriptor 
A main issue in developing invariant local 
descriptors is to represent a region effectively and 
discriminatively. Color histogram [2] is an option for 
textural description, but it is sensitive to illumination 
changes. Instead, we introduce the concept of contrast 
histogram to describe a component represented by an 
image patch, as shown below. 
In our approach, we assume that there are already 
many salient corners extracted from an image I. For 
each salient corner pc in the center of an n×n local 
region R, we compute the contrast C(p) of a point p in 
R as 
C(p) = I(p) – I(pc),                        (1) 
where I(p) and I(pc) are the intensity values of p and pc,
respectively. 
We then construct a descriptor of pc based on these 
contrast values. In our approach, we separate R into 
several non-overlapping regions, R1, R2,…, Rt.
Without lost of generality, we use a log-polar 
coordinate system (r, θ) to perform the division, as 
shown in Figure 1. Log-polar coordinate system has 
been used in many previous works [1][11], and is more 
sensitive to positions of nearby points respect to the 
center. To make the descriptor invariant to image 
rotations, the direction of θ = 0 in the log-polar 
coordinate system is set to be coincide with the edge 
orientation of pc.
How to represent a sub-region Ri efficiently and 
discriminatively is an important issue. We consider a 
histogram-based representation since histogram is 
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  © 2006
Authorized licensed use limited to: ACADEMIA SINICA COMPUTING CENTRE. Downloaded on July 20, 2009 at 04:02 from IEEE Xplore.  Restrictions apply.
shows some examples of matching results by using 
CCH. The correspondences between two images were 
marked and connected with straight lines. 
The results are shown in Table I. Both of these two 
methods have high matching accuracies on the dataset.  
Although the dimension of CCH is less than that of 
SIFT, it can perform comparable accuracy to SIFT.  
Both methods were implemented by C# under an 
Intel P4 3.4G computer, and the computation time is 
shown in Table II. In this table, the descriptor time is 
the total time for salient corner selection and descriptor 
construction of the whole dataset. The matching time is 
the total time for finding the corresponding pairs of the 
whole dataset.  It can be seen that the descriptor time 
of CCH is much less than that of SIFT because only 
subtraction is required when constructing CCH. By 
contrast, SIFT needs to compute magnitudes and the 
orientations of all pixels in a local sampled region. 
Average descriptor time is the average time to 
construct a descriptor for a salient corner. The 
matching time of CCH is also less because the 
dimension of CCH is smaller than that of SIFT. Our 
experiments show that the CCH descriptor has 
comparable matching accuracy than SIFT, but is more 
efficient to compute and match.  
4. Summary 
In this paper, we introduce CCH that is a new 
invariant descriptor to describe local properties of 
image patches. CCH is efficient to compute since 
simple subtraction is used in its construction. 
Employing positive and negative histogram bins of the 
contrast values makes CCH discriminative for image 
matching. Experimental results and comparative 
studies show that CCH has high matching accuracies 
with less computation time. 
5. Acknowledgement  
This research was supported in part by NSC 94-2422-
H-001-006, NSC 94-2422-H-001-007 and NSC 95-
2752-E-002-007-PAE from the National Science 
Council, Taiwan.  
References 
[1] S. Belongie, J. Malik, and J. Puzicha, “Shape Matching 
and Object Recognition Using Shape Contexts,” IEEE
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24, no.4, pp. 509-522, 2002. 
[2] D. Comaniciu, V. Ramesh, and P. Meer, “Real-time 
Tracking of Non-rigid Objects Using Mean Shift,” In
Proceedings of IEEE Conference on Computer Vision 
and Pattern Recognition, pp. 142-151, 2000. 
[3] W. Freeman, and E. Adelson, “The Design and Use of 
Steerable Filters,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 13, no. 9, pp. 
891-906, 1991. 
[4] C. G. Harris, and M. J. Stephens, “A Combined Corner 
and Edge Detector”, In Proceedings of the Fourth Alvey 
Vision Conference, pp.147-151, 1988. 
[5] T. Lindeberg, “Scale-space Theory: A Basic Tool for 
Analyzing Structures at Different Scales,” Journal of 
Applied Statistic, vol. 21, no. 2, pp. 224-270, 1994. 
[6] D. Lowe, “Distinctive Image Features from Scale-
Invariant Keypoints,” International Journal of Computer 
Vision, vol. 60, no. 2, pp. 91-110, 2004. 
[7] LIBSIFT: http://user.cs.tu-berlin.de/~nowozin/libsift/ 
[8] K. Mikolajczyk, and C. Schmid, “Indexing Based on 
Scale Invariant Interest Points”, In Proceedings of
International Conference on Computer Vision, ICCV’01,
vol. 1, pp. 525-531, 2001. 
[9] K. Mikolajczyk, and C. Schmid, “A Performance 
Evaluation of Local Descriptors,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 27, no. 
10, pp. 1615-1630. 2005. 
[10] T. Ojala, M. Pietikainen, and T. Maenpaa, 
“Multiresolution Gray-scale and Rotation Invariant 
Texture Classification with Local Binary Patterns,” 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24, no. 7, pp. 971-987, 2002. 
[11] S. Pereira, J.J.K.O. Ruanaidh, F. Deguillaume, G. 
Csurka, and T. Pun, “Template Based Recovery of 
Fourier-based Watermarks Using Log-polar and Log-log 
Maps, “In Proceedings of Multimedia Computing and 
Systems, vol. 1,  pp. 870 – 874, 1999. 
Table I. The matching results of CCH in the 
test data set. 
Image set (Resolution) CCH SIFT 
Rotation (512×512) 99.9% 99.7%
View change #1 (440×340) 100% 100%
View change #2 (400×320) 98.8% 98.2%
Image blur #1 (500×350) 99.7% 99.9%
Image blur #2 (500×350) 99.8% 99.9%
JPEG compression (400×320) 99.9% 99.9%
Illumination (460×306) 100% 99.7%
Rotation and zoom 
(420×336) 
97.9% 98.9%
Over all  99.8% 99.8%
Total correspondences 7402 7228 
Table II. The computation time of CCH and 
SIFT in the test data set.  
 CCH SIFT 
(1) Descriptor time (s) 36.3 148.7 
(2) Matching time (s) 66.5 128.6 
Total running time (s) (1)+(2) 102.8 277.3 
Average descriptor time (s)  0.0009 0.0035
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  © 2006
Authorized licensed use limited to: ACADEMIA SINICA COMPUTING CENTRE. Downloaded on July 20, 2009 at 04:02 from IEEE Xplore.  Restrictions apply.
