中文關鍵詞： 通用型圖形處理器運算, 計算統一裝置架構(CUDA), 平行計
算, 電子設計自動化 
英 文 摘 要 ： As the advances of VLSI manufacturing technology, 
multi-core computation becomes an important trend in 
both software developments and algorithm designs. 
Graphics processing unit (GPU) is a specialized 
processor for graphics operation and image 
processing. These operations are with low data 
correlation and high computation demand 
characteristics. Hence, GPU design trend tends to 
become many-core with extremely high throughput. 
Compute unified device architecture (CUDA) is a 
parallel computing architecture for general purpose 
computation on GPU (GPGPU) proposed by nVIDIA. With 
the new programming model, developers can easily 
partition program task into serial part and parallel 
part. The serial part will be assigned to CPU； while 
the parallel part will be assigned to GPU. In this 2-
year project, we have explored feasible data models 
and algorithms on GPU for electronic design 
automation (EDA) field. Our project focuses on the 
following three main parts. 
1. Logic optimization: Conventional logic 
optimizations can be classified into two-level and 
multi-level logic synthesis. In two-level 
optimization, we have developed and evaluate GPU 
optimization algorithms for both sum-of-product (SOP) 
and product-of-sum (POS) Boolean representations. 
Furthermore, we have implemented two frequently used 
algorithms (kernel extraction and algebraic division) 
on GPU for multi-level logic synthesis. 
2. Global and detailed routing: In detailed routing, 
parallelizing maze routing algorithm is a hard 
problem due to its fine-grained dependency 
characteristic. Mikami-Tabuchi algorithm is a 
heuristic with coarse-grained dependency at the cost 
of wire length uncertainty. We developed a new 
routing algorithm on GPU combined the advantages of 
maze routing and Mikami-Tabuchi algorithms. In global 
routing, we have investigated the resource contention 
issue raised by parallel rip-up and re-route and 
□期中進度報告 行政院國家科學委員會補助專題研究計畫 ■期末報告 
 
以圖形處理器加速之運算架構應用於電子設計自動化 (I) 
 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC100－2221－E－155－052 
執行期間：100 年 08 月 01 日至 101 年 07 月 31 日 
 
執行機構及系所：元智大學 資訊工程學系 
 
計畫主持人：劉一宇 
共同主持人： 
計畫參與人員：詹久儀、蕭元佾、陳煒舜、林芳旭、曾嘉傑 
 
 
 
 
本計畫除繳交成果報告外，另須繳交以下出國報告： 
□赴國外移地研究心得報告 
□赴大陸地區移地研究心得報告 
■出席國際學術會議心得報告及發表之論文 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，■一年□二年後可公開查詢 
 
中   華   民   國   一   百   零   一   年   七   月   三   十   日 
 
operation and image processing. These 
operations are with low data correlation and high 
computation demand characteristics. Hence, 
GPU design trend tends to become many-core 
with extremely high throughput. Compute 
unified device architecture (CUDA) is a parallel 
computing architecture for general purpose 
computation on GPU (GPGPU) proposed by 
nVIDIA. With the new programming model, 
developers can easily partition program task into 
serial part and parallel part. The serial part will 
be assigned to CPU; while the parallel part will 
be assigned to GPU. In this 2-year project, we 
have explored feasible data models and 
algorithms on GPU for electronic design 
automation (EDA) field. Our project focuses on 
the following three main parts. 
 
1. Logic optimization: Conventional 
logic optimizations can be classified into 
two-level and multi-level logic synthesis. In 
two-level optimization, we have developed and 
evaluate GPU optimization algorithms for both 
sum-of-product (SOP) and product-of-sum (POS) 
Boolean representations. Furthermore, we have 
implemented two frequently used algorithms 
(kernel extraction and algebraic division) on 
GPU for multi-level logic synthesis. 
2. Global and detailed routing: In 
detailed routing, parallelizing maze routing 
algorithm is a hard problem due to its 
fine-grained dependency characteristic. 
Mikami-Tabuchi algorithm is a heuristic with 
coarse-grained dependency at the cost of wire 
length uncertainty. We developed a new routing 
algorithm on GPU combined the advantages of 
maze routing and Mikami-Tabuchi algorithms. 
In global routing, we have investigated the 
resource contention issue raised by parallel 
rip-up and re-route and evaluate the algorithm 
efficiencies in CPU and GPU versions. 
3. Layout layer operation: We utilized 
GPU computation power for VLSI layout 
polygon and edge layer operations. These layer 
operations can be further utilized in physical 
verifications, such as design rule checking 
(DRC), layout versus schematic (LVS), optical 
proximity correction (OPC), and double 
patterning technology (DPT). 
We have developed data models and 
algorithms on GPU to enhance both run-time and 
solution quality in electronic design automation 
applications. Furthermore, we evaluated other 
possibilities (FPGA) for hardware-accelerated 
design automation. 
 
二、 關鍵詞 
中文關鍵詞 
通用型圖形處理器運算, 計算統一裝置架構
(CUDA), 平行計算, 電子設計自動化 
英文關鍵詞 
GPGPU, CUDA, parallel computing, electronic 
design automation 
 
三、 前言 
近年來，隨著半導體製程技術演進，功率
消耗成為晶片設計最重要的議題之一。傳統透
過提升時脈來增加系統效能的方法在 power 
limit 的物理極限下已經無法使用。因此，CPU
設計趨勢已從原本高效能之單核心逐漸轉為
多個中高效能之多核心設計。另一方面，處理
大 量 圖 形 運 算 的 圖 形 處 理 器  (Graphics 
Processing Unit, GPU) 也在設計上有新的變
革。GPU 主要功能在於影像及畫素運算 
(image and pixel operations)，這一類型的運算
 2
 圖四：CUDA 程式運算流程 
透過 CUDA 技術，在許多領域可以得到
相較於 CPU 數倍甚至數十倍的整體效能提升 
[9]。表一列出 GPU 在運算效能高於 CPU 的應
用及其倍數。App. 表示不同類型之應用、
Archit. Bottleneck 表示目前運算架構的瓶頸、
Simult. T 表示同時可以運行的 thread 數量、
Kernel X 表示採用 GPU 運算的速度可以比
CPU 運算快的倍數、App X 表示整體應用程式
效能提升倍數。 
表一：GPU 與 CPU 的運算效能提升倍數 
 
 
四、 研究目的 
在目前半導體設計自動化領域中，隨著製
程技術演進及設計複雜度提升，造成目前 EDA
軟體的執行時間大幅增加。如何在有限的時間
內透過 EDA 的軟體完成設計，已成為 IC 設計
廠商的重要需求。我們透過本計畫，提出適合
於 GPU 運算的 EDA 資料模型和新的演算法，
運用 GPU 內部多核心運算優勢，達成縮短電
路合成所需的時間和電路優化之目標。在這個
計畫中，我們針對 EDA 的邏輯合成 (logic 
synthesis) 和實體設計自動化 (physical design 
automation) 發展合適於 CUDA 技術之核心演
算法。在邏輯合成方面，我們改進二階層及多
階層邏輯最佳化演算法中花費較多 CPU 資源
的核心運算，使其轉為透過 GPU 平行運算。
在實體設計方面，我們開發適合 GPU 運算的
繞線  (routing) 演算法和 VLSI layout 的 
polygon and edge layer operations，以提升運算
效率。 
目前國內外在嘗試使用 CUDA 技術來提
升 EDA 軟體效能的研究團隊並不多，其中主
要的研究方向為晶片溫度模擬、SPICE 電路模
擬、邏輯錯誤模擬 (fault simulation)、靜態時
序分析 (static timing analysis)、breadth first 
search (BFS)…等。這些研究方向多半透過
GPU 在矩陣運算的優勢來增進軟體效能。目前
產業界中，2008 年 start-up 的公司 Gauda 展示
以 GPU 進行 OPC simulation，最多可以達到
200 倍的效能提升 [10]。Agilent 的軟體 ADS 
(Advanced Design System) 亦可透過GPU進行
EM simulation 加速 [11]。CST 公司在 signal 
integrity 和 power integrity 的分析採用 GPU 運
算 [12]。Synopsys 也在 TCAD Sentaraus 中引
進 GPU 加速 [13]。然而，在邏輯合成和實體
設計自動化方面，學術界和產業界的相關研究
並不多見。如果能在邏輯合成和實體設計自動
化領域發展出合適於 GPU 運算的演算法，將
有益於晶片設計自動化產業。 
 
五、 文獻探討 
A. nVIDIA GPU 硬體架構 
nVIDIA 所設計之 GPU 硬體架構 Fermi 如
圖五所示 [14]。 
 4
憶體存取時間時，SM 會自動切換至另一個
active warp 執行，以便讓 SM 維持運算而不致
於閒置。 
C. CUDA 資料儲存單元 
CUDA 另一個與效能直接相關的特性在
於其獨特的資料儲存單元。在 nVIDIA 的 GPU
架構中，有許多種不同階層的記憶體，如
register、shared memory、local memory、global 
memory、constant memory 和 texture memory。
其中每個 SM 包含有 32768 個 32-bit 的 register
和 48 KB 的 shared memory，這兩種儲存單元
的容量小且速度快。local memory 和 global 
memory 則屬於 off-chip memory (DDR5)，資料
存取時間較長。至於 constant memory 和 texture 
memory 雖然也是 off-chip memory，但由於
constant/texture memory 內含 cache 架構，在
cache hit 情況的存取時間遠較 local/global 
memory 來得低。另外，由於每個 SM 同時管
理多個 block，使得 register 和 shared memory
硬體資源將平均分配給每個 block 使用。假設
每個 block所需的 register數量偏多或是 shared 
memory 的容量偏高時，將導致每個 SM 可以
同時管理的 block 個數減少。這時如果 SM 內
的每一個 warp 都因為 memory access 而造成
stall 時，將導致 SM 使用率下降。圖八為 GPU
的記憶體階層圖，在綠色的 thread 以上的部份
可以視為 on-chip ，其他的部份則視為
off-chip。我們參考 CUDA 的相關文件後，將
各種記憶體之特性整理至表二。由表二可以得
知，如何妥善安排常用資料在 register 和 shared 
memory，同時選擇合適的 thread block size 將
是影響 GPU memory performance 的重要議題。 
 
 
圖八：CUDA 記憶體階層 
表二：CUDA 資料儲存單元之特性列表 
Memory Location Access Scope Clock cycles 
Register On-chip Read/write Thread Immediate 
Shared On-chip Read/write Block 4-6 
Local Off-chip Read/write Thread 400-600 
Global Off-chip Read/write Global 400-600 
Constant Off-chip Read Global hit: 4-6 
miss: 400-600
Texture Off-chip Read Global Hit: 4-6 
miss: 400-600
另 一 個 重 要 的 議 題 為 memory 
coalescing 。當一個 warp 在執行 off-chip 
memory access 的指令時，實質上將有 32 個
thread 在進行 memory access。由於 off-chip 的
memory latency 過高，等待 32 次的 memory 
access 將造成該 warp 長時間處在 waiting 的狀
態。如果一個 SM 中的所有 warp 都處在
memory access 的 waiting 狀態，將導致 SM 使
用效率低落，進而造成系統效能下降。如果能
讓 warp 內部的 32 個 thread 同時存取相鄰的記
憶體位置，將能把 32 次的 memory latency 降
為 1 至 2 次 (視 memory alignment 情形，如果
有 memory alignment，則只需 1 次，否則就需
要 2 次)。由於 nVIDIA 的 off-chip memory 為
row-major，如圖九之左圖所示，當 warp 內部
的 thread 在執行時，每個 thread 將會存取不同
row 的記憶體，因而導致高達 32 次的 memory 
access ， 這 種 現 象 稱 為 memory 
non-coalescing。反之，如圖九之右圖所示，每
個 thread將會存取到同一個 row且相鄰的記憶
體，這時便能夠有效減少 memory access 數
量，這種現象稱為 memory coalescing。因此，
 6
算環境，也有一部份具有 shared memory 的硬
體架構。在不同的計算機架構中，我們需要針
對同步機制 (synchronization)、不可分割運算 
(atomic operation) 和記憶體存取方式的特性
來設計合適的演算法，以提升運算效能。 
讓 CUDA 能發揮高效能 GPU 計算需要大
量執行緒層級平行  (thread-level parallelism) 
且每個資料相依性低的工作特性。常見 EDA
領域所需要處理的電路架構多半以 graph 的形
式呈現，graph 內部的 node 和 edge 本質具有
高度不規則性 (irregularity)，造成 GPU 運算
load unbalance 及 irregular memory access 等議
題 [17] [18]。在相關研究中，有強調矩陣運算
的 SPICE simulation [19]、fault simulation [20] 
[21]、statistical static timing analysis (SSTA) [22]
和 power grid analysis [23]。這些研究成果多半
著重於將 CPU code 轉為 GPU code，利用 GPU
矩陣運算的優勢來提升效能。雖然這類的研究
在演算法的創新可能較少。卻有可能達到數十
倍以上的效能提升。 
除了上述的研究以外，有的團隊期望以
CUDA 運算能力提升 Boolean satisfiability 
(SAT) 問題效能 [24]。初步結果顯示，GPU
運算在 random SAT instance 下的效能比 CPU
運算好，但在 structural SAT instance 的效能比
CPU 運算差。然而，如果希望透過 SAT solver
來輔助邏輯最佳化，我們需要針對處理
structural SAT instance 的 SAT solver。 
另外，有些研究團隊利用稀疏矩陣 
(spares matrix) 來表示gate-level電路拓樸關係 
(circuit topology)，再以稀疏矩陣向量乘法 
(spares matrix vector product, SMVP) 來提升
效能 [25] [26]。其中在 static timing analysis 
(SAT) 可以達到數十倍效能提升，在 linear 
placement 的 Conjugate Gradient Solver (CGS) 
上亦有高於 CPU 數倍的運算優勢。 
在 breadth first search (BFS) 的問題上，目
前有兩個相關研究透過 GPU 進行加速。一個
是採用 spares matrix operation [25]，另一個則
提出 hierarchical queue management [27] 的方
式。採用 sparse matrix 的方法受限於 matrix 形
式，如果 matrix 為 regular (0 與 1 的數目較為
平均)，則 sparse matrix 的運算次數將大幅提
升，反而導致 GPU 運算效能下降。如果能將
hierarchical queue 的運作機制應用在 maze 
routing 上，將有機會透過 GPU 提升 maze 
routing 的運算效能。 
 
六、研究方法及成果 
本計畫針對三個設計自動化流程發展合
適於 GPU 加速之演算法。 
A. Logic optimization on GPU 
傳統 Boolean function optimization 可分為
二階層 (two-level) 和多階層 (multi-level)。常
見的 sum-of-product (SOP) 二階層邏輯最佳化
演算法為 Espresso ，透過重覆執行 cube 
reduce、cube expand 和 cube irredundant 達成邏
輯最佳化之目標。在進行上述流程時，我們可
以使用positional cube representation將Boolean 
function 以 matrix 呈現。由於 cube operation 經
常需要做 cofactor 的運算，只要將 matrix 內每
個 column 都拿來做 bitwise Boolean operations
即可完成 cofactor 運算。由於每個 bit 的運算
都是獨立進行的，只要將每個 column 指定給
不同的 thread，即可提升運算效能。另一方面，
我們也實作了 Quine–McCluskey 演算法，利
用 GPU 產生二階層邏輯 (SOP) 的最佳解。表
三列出目前的初步實驗結果，GPU版本比CPU
版本平均快 8 倍。 
 
 
 
 8
0.0000
30.0000
60.0000
90.0000
120.0000
150.0000
180.0000
210.0000
ben
221
0x2
220
ben
212
0x2
120
ben
234
5x2
134
ben
240
0x2
100
ben
245
0x2
420
ben
270
0x2
525
ben
242
5x2
900
ben
281
3x2
836
ben
280
0x2
550
ben
281
9x2
953
Benchmarks
Ru
n 
Ti
m
e(
m
se
c)
CPU Mikami
CUDA-Mikami
 
圖十四：CPU/GPU Mikami-Router Run Time Comparison 
在 global routing 時，透過我們的 GPU 
Mikami 演算法，有效而且快速地評估 routing 
congesting，並依據 congestion map 來進行
routing region partition，讓 routing 的問題得以
被分割在不同的 routing region 上獨立處理，將
能減輕因為 routing resource 不足而造成的
rip-up and re-route 問題。 
C. Mask layer operation on GPU 
Geometry operations 被大量運用在 design 
rule checking (DRC) 和 layout versus schematic 
(LVS) 之中。部份 layer operation 更可以應用
在 rule-based optical proximity correction (OPC) 
和 double pattern technology (DPT)，以解決微
影製程 (lithographic printing) 的光源繞射及
干涉之問題。隨著晶片設計複雜度增加，layer 
operation 所需的 run-time 因而大幅上升。我們
將 GPU 應用在多邊形  (polygon) 和邊緣 
(edge) 的運算，進而提升 layer operation 之效
率。 
由於 GDS 檔案格式是以逆時針方向儲存
多邊形的點座標。我們依據 GDS 的座標數值
做為 cut lines，所有的 cut lines 將形成 GPU 的
grids。如此將可以減少記憶體使用量，同時，
原本的多邊形將被切割為多個四邊形，並且以
二維grid結構儲存於GPU中。圖十五顯示GDS
的 grid 切割及 GPU 二維 grid 資料型態。圖十
六顯示以 GPU multi-thread 進行資料處理及運
算。 
 
圖十五：多邊形 GDS 資料轉為 grid graph 處理 
 
圖十六：GPU 資料處理及運算 
我們利用 open source 的 Computation 
Geometry Algorithm and Library (CGAL) 做為
geometry operation 開發平台 [37]。如圖十七所
示，我們將電路 b15 的 layer 18 和 layer 28 同
時取出來進行 interact 幾何運算，輸出的結果
再轉為 GDS 型式，最後用業界的 layout editor
呈現。我們的幾何運算結果與 Mentor Graphics
的 Calibre 和 Synopsys 的 Proteus 分別進行比
對，結果均為 XOR-clean。 
 
圖十七：電路 b15 中 layer 18 與 layer 28 的 interact 結果 
(XOR-cleaned) 
 10
21. K. Gulati, S. P. Khatri, "Fault Table Computation on 
GPUs", in Journal of Electronic Testing, V. 26, No. 
2, pp. 195-209, 2010. 
22. K. Gulati, S. P. Khatri, "Accelerating Statistical 
Static Timing Analysis using Graphics Processing 
Units", in Proceedings of IEEE/ACM Asia and 
South Pacific Design Automation Conference, 
260-265, 2009. 
23. Z. Feng and P. Li, "Multigrid on GPU: Tackling 
Power Grid Analysis on Parallel SIMT Platforms", 
in Proceedings of International Conference on 
Computer-Aided Design, pp. 647-654, 2008. 
24. P. Manolios and Y. Zhang, "Implementing Survey 
Propagation on Graphics Processing Units", in 
Proceedings of International Conference on Theory 
and Applications of Satisfiability Testing, pp. 
311-324, 2006. 
25. Y. D. Deng, B. D. Wang, S. Wu, "Taming Irregular 
EDA Applications on GPUs", in Proceedings of 
IEEE/ACM International Conference on 
Computer-Aided Design, pp. 539-546, 2009. 
26. Y. D. Deng, B. D. Wang, S. Wu, Z. H. Wang, 
"Toward EDA computing on GPUs", in Proceedings 
of IEEE International Conference on 
Communications, Circuits, and Systems, pp. 
1119-1123, 2010. 
27. L. Luo, M. Wong, W. M. Hwu, "An effective GPU 
implementation of breadth-first search", in 
Proceedings of ACM/IEEE Design Automation 
Conference, pp. 52-55, 2010. 
28. C. Y. Chan, J. L. Lin, L. S. Chien, T. Y. Ho, Y. Y. 
Liu, "GPU-based line probing techniques for 
Mikami routing algorithm", in Proceedings of 
Workshop on Synthesis And System Integration of 
Mixed Information Technologies (SASIMI), pp. 
340-344, 2012. 
29. P. Y. Hsu, Y. Y. Liu, "Buffer design and assignment 
algorithm for structured ASIC optimization", 
accepted and to appear in Journal of Information 
Science and Engineering (JISE). 
 12
PO-YANG HSU, SU-TING LI AND YI-YU LIU 2 
pense of extra buffer area. Van Ginnecken exploits the special properties of Elmore de-
lay model and proposes a dynamic-programming based algorithm for buffer assignment 
[8]. Gao and Wong optimally solve the buffer planning problem by using a graph-based 
algorithm [9]. In the standard cell design style, there are a set of buffers in a cell library. 
CAD tools select the potential critical paths and place buffers on those paths. Post-layout 
buffer insertion can be done by slightly modifying the placement and routing results. In 
the FPGA design style, plenty of switch buffers reside in the switch blocks. These switch 
buffers explicitly cut off the downstream capacitance at the cost of fairly large gate delay 
overheads. In the structured ASIC design style, the downstream capacitance would be-
come a crucial problem because neither different driving-strength gates nor switch buff-
ers could be used. To solve this problem, dedicated pre-fabricated buffers may be a solu-
tion. The aforementioned issues motivate us to design dedicated buffers for the inter-
connections. We design the layouts of two dedicated buffers and extract the technology 
dependent parameters for evaluations. In addition, buffer assignment and channel migra-
tion algorithms are proposed for timing optimization.  
The rest of this paper is organized as follows. The preliminary background knowl-
edge and the motivation of this paper are given in Section 2. Section 3 presents the lay-
out of our buffered structured ASIC and buffer assignment algorithm. Section 4 illus-
trates our channel migration algorithm. The experimental results are drawn in Section 5. 
Section 6 concludes this paper. 
2. MOTIVATION 
In this section, we give the background of island-style FPGAs followed by is-
land-style structured ASICs. After that, we address the buffer insertion issues for struc-
tured ASIC design. 
 
2.1 Island-style FPGA 
 
Island-style FPGA architecture is a well-known chip level pre-fabricated design 
style [10][11]. There are two major components in an island-style pre-fabrication, logic 
component and routing component. The logic component is composed of configurable 
logic blocks (CLB). Each CLB contains a set of basic logic elements (BLE) to realize 
both combinational and sequential logic functions. Moreover, the routing component 
consists of routing channels, connection blocks, and switch blocks. The routing channels 
are made up of several routing tracks. The connection blocks are used to determine the 
connectivities between the I/O of CLB and the routing tracks. The switch blocks are used 
to determine the connectivities between different routing tracks in different routing chan-
nels. Therefore, with tracks, connection blocks, and switch blocks, routing components 
are capable of interconnecting logic components. Figure 1 briefly illustrates island-style 
pre-fabrication. 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 4 
In a structured ASIC design style, the device masks and the metal masks are 
pre-fabricated to be shared among different designs. In contrast to the device and metal 
masks, only via masks must be re-designed. Hence, the NRE cost of a structured ASIC is 
relatively low compared with that of a standard cell ASIC. 
Since the transmission gates are replaced by vias in a structured ASIC, the diffusion 
capacitance in the interconnections is substantially reduced. The commonly used switch 
buffers in a traditional FPGA design are not required in a structured ASIC design. The 
removal of transistors in SRAMs, transmission gates, and switch buffers greatly im-
proves the chip density as well as the circuit performance. Compared to the FPGA, a 
structured ASIC trades field programmability for circuit performance. 
 
2.3 Buffer Insertion Issues for Structured ASIC 
 
Modern VLSI design must carefully take into account the problem of large down-
stream capacitance due to multiple fanouts and long wires. In the standard cell design 
style, there are different driving-strength logic gates and buffers to be selected in a cell 
library. Hence, the capacitive problem can be alleviated by using the concept of logical 
effort [13]. In the FPGA design style, the capacitive problem is directly neutralized by 
switch buffers, since the switch buffers split a long wire with a high capacitive load into 
several buffered wire segments. However, the problem of a large capacitive load would 
be a serious issue for a cost-efficient structured ASIC design style because neither dif-
ferent driving strength gates nor switch buffers can be used. 
To solve the problem of large downstream capacitance, pre-fabricated buffers are 
proposed [12]. Zhang and Sapatnekar propose a layout-unaware statistical scheme to 
estimate the distribution of pre-fabricated buffers by using Rent's rule. They optimisti-
cally assume all pre-fabricated buffers within a “tile'' can be freely utilized [12]. How-
ever, the exact buffer locations and the pre-fabricated metal wires are key challenges to 
support buffer assignment. In this paper, we will also use pre-fabricated buffers to solve 
the large capacitive load problem. Buffer assignment can be implemented by using 
mask-programmable vias. We will first study where to put the pre-fabricated buffers to 
gain the best performance and then propose an algorithm for buffer assignment based on 
our proposed locations for pre-fabricated buffers. 
3. BUFFER DESIGN AND EVALUATION 
According to island-style architecture, there are two possible candidate locations 
for pre-fabricated buffers, the logic fabric and routing fabric. In this paper, we will 
evaluate the performance when the buffer is in each location. For simplicity, in the fol-
lowing paper, the buffer pre-fabricated in the logic fabric is denoted as the logic fabric 
buffer (LFB), and the buffer pre-fabricated in the routing fabric as the routing fabric 
buffer (RFB). 
In this section, we design a pilot CLB with an LFB. Then, the CLB dimension is 
used to design the RFB. After that, we extract the technology-dependent parameters from 
our designs for simulation. A buffer assignment algorithm is proposed to evaluate three 
types of buffer locations: LFB, RFB, and both LFB and RFB. Finally, we summarize the 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 6 
 
Fig. 2. CLB output buffer comparison 
 
Finally, a via-programmable 4X-buffer and 1X-buffer are placed at the output of the 
CLB. The CLB schematic and corresponding input buffers are drawn in Figure 3. A 
complete CLB layout with TSMC 0.18μm technology is shown in Figure 4. The CLB 
layout dimensions are 33μm × 29μm. We will use these dimensions for routing fabric 
design. 
 
Fig. 3. Schematic of a CLB with output buffers 
 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 8 
 
Fig. 5. The schematic of RFB and sub-channel designs 
 
 
3.3 Simulations on Buffer Assignment 
 
We use an FPGA placement and routing tool, VPR [10], as our simulation platform. 
The technology-dependent parameters from SPICE simulation and the predictive tech-
nology model (PTM) [15] are listed in Table II. All the benchmark circuits from MCNC 
are optimized, mapped, and packed by using SIS [16], FlowMap [17], and T-Vpack [10], 
respectively. After that, we perform placement and routing by using VPR. 
To alleviate the delay impact caused by downstream capacitance, we assign buffers 
to the candidates on the path with the maximum circuit delay. After the buffer assign-
ment, we update the path delay and identify a new path with the maximum circuit delay 
for further buffer assignments. Therefore, the maximum circuit delay can be iteratively 
reduced. Figure 6 illustrates our post-routing buffer assignment algorithm. In this algo-
rithm, C represents the set of all possible buffer assignment candidates. For the LFB as-
signment, C is the set of total CLBs, while for the RFB assignment, C is the set of total 
2-pin wires. P represents a set of processed buffer assignment candidates. S represents a 
set of candidates on the critical paths. The Cth is the threshold capacitance for buffer as-
signment. The Cth can be obtained by computing the optimal buffer insertion under the 
Elmore delay model [13]. Based on our TSMC 0.18μm library, the derived Cth is 28 fF. 
We set Cth to 30 fF in our experiments. If the downstream capacitance of a candidate 
exceeds the threshold Cth, we assign a buffer to cut off the downstream capacitance. For 
the LFB assignment, only one 4X-buffer in CLB can be assigned. For the RFB assign-
ment, the underlying buffers of the 2-pin wire can be assigned. The algorithm repeats 
execution until either the number of candidates or the number of unassigned buffers is 
zero. 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 10 
delay to gate delay are greatly improved when we use large LFB. However, with the size 
of LFB increases, the aforementioned improvement rates are drastically reduced espe-
cially when the LFB size is larger than 4X. Therefore, we cannot tackle capacitive load 
problem by using LFB only. Similarly, according to the result of using unused CLBs as 
buffers, we can slightly improve circuit performance and the ratio of wire delay to gate 
delay. However, the maximum channel width may be increased since extra routing wires 
are required to make use of the unused CLBs. In order to keep the placement and routing 
results unaltered, we suggest not to use the unused CLBs for buffer assignment. Finally, 
the combined LFB and RFB assignment achieves the best results in terms of circuit per-
formance and the ratio of wire delay to gate delay. Based on the above simulation results, 
we select 4X LFB and RFB in our design. 
Table III. Simulation results of different buffer assignments 
Buffer assignment Buffer size Dratio (%) Dw/Dg
LFB 1X 100.0 32.6 
LFB 2X 53.5 16.9 
LFB 4X 29.8 8.2 
LFB 6X 22.1 5.7 
LFB 8X 18.2 3.9 
LFB + unused LFB 4X 27.8 7.6 
LFB + RFB 4X 10.4 0.8 
 
4. CHANNEL MIGRATION 
 
The final layout of our CLB and routing channel is drawn in Figure 7. Since the 
numbers of RFBs are fixed, we cannot use unlimited RFBs in routing channels. In some 
cases, many critical nets assigned to the same sub-channel compete for a limited number 
of buffers. A sub-channel is saturated if all the underlying buffers are assigned. Hence, 
we need to “borrow” buffers to solve this saturation problem. There are two buffer can-
didates of a saturated sub-channel. One is from the adjacent sub-channel within the same 
routing channel and the other is from the adjacent channel as illustrated in Figure 8. To 
utilize the buffer from an adjacent sub-channel, track swapping is required to move the 
critical wire from the saturated sub-channel to an unsaturated sub-channel. Since the 
buffer assignment is within the same routing channel without performance degradation, 
we denote this as intra-channel migration. To utilize the buffer from an adjacent channel, 
we directly assign a buffer from the adjacent channel to the critical wire without per-
forming track swapping. Since the assigned buffer location is changed with some per-
formance degradation, we denote this as inter-channel migration. 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 12 
there is no feasible track for swapping, we assign an unassigned buffer from adjacent 
channel to the critical wire segment (inter-channel migration). 
 
 
Fig. 9. The Channel Migration algorithm 
 
Figure 10 illustrates intra-channel migration. In Figure 10(a), Tracks S and T rep-
resent routing tracks within saturated and unsaturated sub-channels, respectively. The 
line drawn in red indicates the wire segment requires buffer assignment. The line drawn 
in green indicates the migration interval. Assume that the migration interval spans 7 
channels. The vectors in Figure 10(b) indicate the numbers of unassigned buffers and 
required buffers in all sub-channels. Since all elements in vector SU are greater than or 
equal to the corresponding elements in TR (SU ≥ TR) and all elements in vector TU are 
greater than or equal to the corresponding elements in SR (TU ≥ SR), intra-channel migra-
tion is performed in Figure 10(c). Vectors S'U and T'U list the numbers of unassigned 
buffers after intra-channel migration, where S'U = SU - TR and T'U = TU - SR. Figure 11 
illustrates inter-channel migration. The migration interval is the same to that of Figure 
10(a). The vectors in Figure 11(a) indicate the numbers of unassigned buffers and re-
quired buffers. Notice that there is no feasible track for swapping since TU < SR. There-
fore, intra-channel migration is not allowed. Since there are unassigned buffers (SU[4] = 
2) adjacent to the saturated sub-channel (SR[5] = 1), inter-channel migration can be per-
formed. Hence, the numbers of required buffers from track S become S'R, where S'R[4] = 
1 and S'R[5] = 0. As illustrated in Figure 11(b), vectors S'U and T'U list the numbers of 
unassigned buffers after inter-channel migration, where S'U = SU - S'R and T'U = TU - TR. 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 14 
5. EXPERIMENTAL RESULTS 
 
We conduct a constrained RFB assignment to evaluate our structured ASIC design 
and optimization algorithms. The post-routing buffer assignment and channel migration 
algorithms are integrated into VPR [10]. The experiment is performed on an IBM X3550 
2.5GHz Xeon Server with 32 GB memory. All the benchmark circuits from MCNC are 
optimized, mapped, packed, and pre-placed by using SIS, FlowMap, T-Vpack, and VPR, 
respectively. The circuit profiles are summarized in Table IV. Columns CLB, 2-pin, Lev-
el, Dim, and Utilization represent CLB number, 2-pin net number, circuit level, circuit 
dimension, and CLB utilization, respectively. 
In our first experiment, the priority of RFB insertion is the same as the proposed algo-
rithm in Figure 6 until a sub-channel saturation occurs. Then, the channel migration al-
gorithm is performed to seek unassigned buffers from adjacent sub-channels and chan-
nels. Table V shows the results. Columns Baseline, Baseline + unused CLB, and RFB 
represent baseline designs with 4X-buffer at the CLB output, baseline designs taking 
unused CLBs as buffers, and our RFB design with constrained buffer assignment, re-
spectively. Elmore delay model is used to compute the wire delay Dw. The gate delay is 
derived from Dg = gate delay × gate level. The Delay, Dw/Dg, and Dimp columns represent 
circuit delay, ratio of wire delay to gate delay, and delay improvement as compared to 
the baseline designs, respectively. As Table V shows, taking unused CLB as buffers can 
slightly improve circuit performance and the ratio of wire delay to gate delay. Compared 
to the baseline designs, our RFB insertion and channel migration algorithm improve the 
circuit performance by 65.7% and the ratio of wire delay to gate delay from 8.2 to 0.8. 
Table IV. Circuit profiles 
Circuit CLB 2-pin Level Dim Utilization (%) 
apex4 1262 5750 7 36 × 36 97.4 
ex5p 1064 5074 8 33 × 33 97.7 
bigkey 1707 8473 4 54 × 54 58.5 
des 1591 7957 7 63 × 63 40.1 
spla 3690 17514 9 61 × 61 99.2 
frisc 3556 17234 23 60 × 60 98.8 
clma 8383 38940 16 92 × 92 99.0 
diffeq 1497 7234 14 39 × 39 98.4 
dsip 1370 7468 4 54 × 54 47.0 
elliptic 3604 17491 18 61 × 61 96.9 
ex1010 4598 20686 9 68 × 68 99.4 
seq 1750 7984 8 42 × 42 99.2 
misex3 1397 6379 8 38 × 38 96.7 
tseng 1047 5244 13 33 × 33 96.1 
average  87.5 
 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 16 
Table VI. The results of constrained RFB insertion with and without channel migrations 
Without channel migration With channel migration 
Circuit 
Delay Dw/Dg RFB Delay Dimp (%) Dw/Dg RFB Intra Inter 
apex4 7.20E-09 1.0 6655 7.20E-09 0.0 1.0 6670 16 15 
ex5p 9.27E-09 1.2 1629 6.65E-09 28.3 0.6 2528 1 2 
bigkey 4.61E-09 1.2 2289 4.61E-09 0.0 1.2 2289 0 0 
des 5.97E-09 0.9 1582 5.97E-09 0.0 0.9 1582 0 0 
spla 1.76E-08 2.8 5902 1.12E-08 36.5 1.4 10416 3 5 
frisc 1.56E-08 0.3 3376 1.56E-08 0.0 0.3 3376 0 0 
clma 1.37E-08 0.8 43324 1.37E-08 0.0 0.8 43436 94 116 
diffeq 8.84E-09 0.2 1443 8.84E-09 0.0 0.2 1443 0 0 
dsip 4.53E-09 1.2 1870 4.53E-09 0.0 1.2 1870 0 0 
elliptic 1.51E-08 1.0 3575 1.13E-08 25.3 0.5 6111 1 1 
ex1010 1.15E-08 2.1 7844 1.06E-08 8.0 1.3 10785 3 1 
seq 1.31E-08 2.6 1259 6.98E-09 46.9 0.7 3865 2 3 
misex3 1.15E-08 2.1 1969 7.40E-09 35.7 1.0 2766 1 1 
tseng 8.54E-09 0.3 817 8.54E-09 0.0 0.3 817 0 0 
average  1.3 82.5%  12.9 0.8 100%   
 
 
 
(a) Wire delay comparison       (b) Buffer number comparison 
 
Fig. 12. Comparison of buffer assignment algorithms 
 
  
PO-YANG HSU, SU-TING LI AND YI-YU LIU 18 
5. CONCLUSION 
Buffer insertion is a widely used technique for splitting a long wire into several 
buffered wire segments for interconnection delay optimization. We have designed lay-
outs of structured ASIC logic and routing fabrics with pre-fabricated buffers. According 
to our simulation results, 4X-buffers are used as our logic fabric buffers and routing fab-
ric buffers. Additionally, we propose a post-routing buffer assignment algorithm for cir-
cuit performance optimization. Furthermore, intra-channel and inter-channel migration 
algorithms are proposed for buffer assignment within a saturated sub-channel. Com-
pared to the baseline designs with 4X-buffer at the CLB output, our proposed structured 
ASIC design and optimization techniques improve the circuit performance by 65.7% and 
the ratio of wire delay to gate delay from 8.2 to 0.8. Additionally, the circuit performance 
improves 12.9% by using our channel migration algorithm. 
 
Acknowledgement 
 
The authors would like to thank the associate editor and reviewers for their con-
structive comments that help improve the manuscript. 
REFERENCES 
1. C. Patel, A. Cozzie, H. Schmit and L. Pileggi, “An architectural exploration of via 
patterned gate arrays”, in Proceedings of International Symposium on Physical De-
sign, pp.184-189, 2003. 
2. B. Hu, H. Jiang, Q. Liu and M. M. Sadowska, ”Synthesis and placement flow for 
gain-based programmable regular fabrics”, in Proceedings of International Sympo-
sium on Physical Design, pp.197-203, 2003. 
3. N. Jayakumar and S. P. Khatri, “A metal and via maskset programmable VLSI de-
sign methodology using PLAs”, in Proceedings of International Conference on 
Computer-Aided Design, pp.590-594, 2004. 
4. Y. Ran and M. Marek-Sadowska, “Via-configurable routing architectures and fast 
design mappability estimation for regular fabrics”, in Proceedings of International 
Conference on Computer-Aided Design, pp.25-32, 2005. 
5. K. Gulati, N. Jayakumar and S. P. Khatri, “A structured ASIC design approach using 
pass transistor logic”, in Proceedings of IEEE International Symposium on Circuits 
and Systems, pp.1787-1790, 2007. 
6. S. Gopalani, R. Garg, S. P. Khatri and M. Cheng, “A lithography-friendly structured 
ASIC design approach”, in Proceedings of Great Lakes Symposium on VLSI, 
pp.315-320, 2008. 
7. U. Ahmed, G. G. F. Lemieux and S. J. E. Wilton, “Area, delay, power, and cost 
trends for metal-programmable structured ASICs (MPSAs)”, in Proceedings of In-
ternational Conference on Field Programmable Technology, pp.278-284, 2009. 
8. L. P. P. P. van Ginncken, “Buffer placement in distributed RC-tree networks for 
minimal elmore delay”, in Proceedings of International Symposium on Circuits and 
Systems, pp.865-868, 1990. 
  
出席國際學術會議心得報告 
                                                             
計畫編號 NSC-100-2221-E-155-052 
計畫名稱 以圖形處理器加速之運算架構應用於電子設計自動化(I) 
出國人員姓名 
服務機關及職稱 
元智大學 資訊工程學系 劉一宇 助理教授 
會議時間地點 2012/03/08~2012/03/09 日本 別府 
會議名稱 Workshop on Synthesis And System Integration of Mixed Information Technologies (SASIMI) 
 
一、參加會議經過 
 
今年的 SASIMI 會議於三月八日至九日在日本北九州的別府舉行。今年邀請到許多重量
級的學者和業界代表來跟大家分享研究成果。以下是本人的會議參訪報告： 
 
三月八日： 
上午的專題演講邀請到史丹福大學的 Subhasish Mitra來跟大家談可靠/穩定的系統設計方
法。Mitra一開始就舉了一個例子做為開場：現在系統整合越來越多的元件，導致單一元
件出錯的機率必須大幅下降才能維持過去系統的穩定度。透過這樣的介紹，讓與會者瞭
解系統容錯和預測潛在錯誤將會是未來電子系統設計的重要議題。下午的專題演講邀請
了一位來自 IMEC的 Rob van Schaijk，他的研究專長比較偏向物理和材料。演講的主軸
在於介紹目前能量採集技術發展的現況以及瓶頸。這提供給WSN (wireless sensor network)
領域的設計者一個明確的 power consumption budget。 
 
三月九日： 
上午邀請到 UCLA的 Jason Cong來跟大家談平行化、客製化和自動化。演講的內容從多
核心談到異質核心和客製化計算核心 (ex: FPGA) 的硬體系統開發和應用軟體建置。Prof. 
Cong更大膽地預測，未來的系統都應該同時包含異質計算核心和可重組態核心。這樣的
系統才能夠有彈性地適應多種不同類型的運算需求。我個人認為這場演講的收穫很大。
在我們團隊新一期的國科會研究計畫中，就是以建立多核 CPU+GPU+FPGA的運算平台
為目標，這一點與 Jason Cong提到未來運算系統的觀念類似。但透過 Prof. Cong的演講，
讓我們對於運算需求、硬體架構及工作溝通與分派有了更為宏觀的視野，讓我們能進一
步看到這些系統整合將面臨到的挑戰與機會。在 Panel discussion時，Prof. Cong談到他
認為未來值得努力的三大方向  (1) Cloud – Servers (2) Client – mobile devices (3) 
Sensors。下午的演講邀請到參與設計目前世界上最快的高速電腦：K-computer的 Takahide 
Yoshikawa來跟大家分享系統驗證的重要性。 
 
 
(Back to Session Schedule)
Poster III
Time: 10:00 - 11:45 Friday, March 9, 2012
Location: Int'l Conf. Room & Mtg. Room 31
Chairs: Qiang Zhu (Cadence Design Systems, Japan), Kyungsoo Lee (Kyoto Univ., Japan)
R3-1
Title Replacement of Flip-Flops by Latches and Pulsed Latches for Power and Timing Optimization
Author Yao-Ting Wu, *Rung-Bin Lin (Yuan Ze Univ., Taiwan)
Page pp. 300 - 304
Detailed information (abstract, keywords, etc)
R3-2
Title A Routability-oriented Packing Method for FPGA with Fracturable Logic Elements
Author Wei Chen (Waseda Univ., Japan), Yuichi Nakamura (NEC Corp., Japan), *Nan Liu, Takeshi
Yoshimura (Waseda Univ., Japan)
Page pp. 305 - 310
Detailed information (abstract, keywords, etc)
R3-3
Title A Two-Step BIST Scheme for Operational Amplifier
Author *Jun Yuan, Masayoshi Tachibana (Kochi Univ. of Tech., Japan)
Page pp. 311 - 316
Detailed information (abstract, keywords, etc)
R3-4s
Title Circuit Partitioning Methods for FPGA-based ASIC Emulator using High-speed Serial Wires
Author *Katsunori Takahashi, Motoki Amagasaki, Morihiro Kuga, Masahiro Iida, Toshinori Sueyoshi
(Kumamoto Univ., Japan)
Page pp. 317 - 318
Detailed information (abstract, keywords, etc)
R3-5
Title
Timing-aware Description Methods and Gate-level Simulation of Single Flux Quantum Logic
Circuits
Author *Nobutaka Kito, Kazuyoshi Takagi, Naofumi Takagi (Kyoto Univ., Japan)
Page pp. 319 - 324
Detailed information (abstract, keywords, etc)
R3-6
Title Design and Analysis of Via-Configurable Routing Fabrics for Structured ASICs
SASIMI 2012 Technical Program http://www2.infonets.hiroshima-u.ac.jp/sasimi/program/R3.html
1／5 2012/2/17 下午 05:48
Author Hsin-Pei Tsai, *Rung-Bin Lin, Liang-Chi Lai (Yuan Ze Univ., Taiwan)
Page pp. 325 - 329
Detailed information (abstract, keywords, etc)
R3-7
Title
Device-level Simulations of Parasitic Bipolar Mechanisim on Preventing MCUs of Redundant
Filp-Flops
Author *Kuiyuan Zhang, Ryosuke Yamamoto, Kazutoshi Kobayashi (Kyoto Inst. of Tech., Japan)
Page pp. 330 - 333
Detailed information (abstract, keywords, etc)
R3-8
Title A Method of Analog IC Placement with Common Centroid Constraints
Author *Keitaro Ue, Kunihiro Fujiyoshi (Tokyo Univ. of Agri. and Tech., Japan)
Page pp. 334 - 339
Detailed information (abstract, keywords, etc)
R3-9
Title GPU-based Line Probing Techniques for Mikami Routing Algorithm
Author
*Chiu-Yi Chan (Yuan Ze Univ., Taiwan), Jiun-Li Lin (National Cheng Kung Univ., Taiwan),
Lung-Sheng Chien (National Tsing Hua Univ., Taiwan), Tsung-Yi Ho (National Cheng Kung Univ.,
Taiwan), Yi-Yu Liu (Yuan Ze Univ., Taiwan)
Page pp. 340 - 344
Detailed information (abstract, keywords, etc)
R3-10
Title Topology Design for Power Delivery in 3-D Integrated Circuits
Author *Shu-Han Wei, Yi-Hsuan Lee, Chih-Ting Sun, Yu-Min Lee (National Chiao Tung Univ., Taiwan),
Liang-Chia Cheng (ITRI, Taiwan)
Page pp. 345 - 350
Detailed information (abstract, keywords, etc)
R3-11
Title A Spur-Reduction Frequency Synthesizer For Wireless Application
Author *Te-Wen Liao, Jun-Ren Su, Chung-Chih Hung (National Chiao Tung Univ., Taiwan)
Page pp. 351 - 354
Detailed information (abstract, keywords, etc)
R3-12
Title
Definite Feature of Low-Energy Operation of Scaled Cross-Current Tetrode (XCT) SOI CMOS
Circuits
Author *Yasuhisa Omura, Daishi Ino (Kansai Univ., Japan)
Page pp. 355 - 360
Detailed information (abstract, keywords, etc)
R3-13
SASIMI 2012 Technical Program http://www2.infonets.hiroshima-u.ac.jp/sasimi/program/R3.html
2／5 2012/2/17 下午 05:48
Mikami routing algorithm employs line probing tech-
nique to speed-up the run time of BFS wave propagation.
In line probing stage, with setting both source-point and
target-point as base grids, Mikami routing generates two
level-1 cross lines (one horizontal wire and one vertical
wire) for each base grid. All the grids on the level-l cross
lines will become new base grids in next level. Mikami
routing iteratively generates cross lines from base grids
until there exists cross-line intersection from source and
target points. After that, back traces are performed from
the intersection point to source-point and target-point,
respectively. The actual routing path is completed after
back traces. Mikami routing effectively reduces the run
time of maze routing algorithm.
In summary, maze routing guarantees for routing result
with shortest wire length and Mikami algorithm guaran-
tees for minimum bend count. The search space and time
complexity of maze and Mikami algorithms are O(MN)
and O(L), respectively, where M and N are the horizontal
and vertical grid size, and L is the number of Mikami rout-
ing levels. The graph grids data dependency of Mikami
algorithm is lower than that of maze algorithm, since
Mikami algorithm performs cross line propagation and
maze algorithm performs grid-by-grid wave propagation.
B. Graphic Processing Unit
GPU computing and GPGPU (General-Purpose Com-
putation on Graphics Processing Units) are the techniques
of using GPU to perform general purpose computation.
GPU is traditionally used for graphics and image compu-
tations. Nowadays, GPU collaborates with CPU (host)
in a heterogeneous manner for various applications. GPU
computing is challenging due to the intrinsic graphic oper-
ations in GPU. NVIDIA develops “Compute Unified De-
vice Architecture (CUDA)” parallel programming model
to ease the implementation challenges of general purposed
GPU computing [4, 5]. Fermi is the latest CUDA pro-
gramming model [6]. There are plenty of intrinsic func-
tions supported in Fermi to enhance the performance of
GPU computing.
In CUDA, thread is a fine-grained pseudo GPU pro-
cessing unit for each data-parallel and data-independent
operation. Each thread is assigned to a GPU stream-
ing processor (SP) for actual execution. Since each GPU
contains hundreds of SPs, a GPU program achieves high
throughput if most SPs are executing data-parallel tasks
simultaneously. A warp, which contains 32 threads, is
a scheduling unit on NVIDIA’s GPU. Each warp can be
scheduled to a streaming multiprocessor (SM) for execu-
tion if all required data is ready. Taking NVIDIA GTX
580 as an example, there are 16 SMs and each SM con-
tains 32 SPs. Hence, there are total 512 SPs in a GTX
580. To fully utilize a GPU, the most important issue is
to increase the number of threads for SMs execution. We
will use GTX 580 as our implementation platform in this
work.
Fig. 1. Memory coalesced and non-coalesced phenomena.
In addition, memory access is another key issue for
GPU programming. Since a GPU has its own individual
memory hierarchy, data transfer is required between host
and GPU memories. To reduce the data transfer overhead
between two memories, we need to prevent unnecessary
communications between host and GPU. In GTX 580,
each off-chip memory access fetches 128-byte data into a
cache line. Assume that there are 32 active threads within
a warp and each thread requires a 4-byte data. If all 32
required data are not in the same cache line, there are 32
cache misses (off-chip memory accesses) within a warp.
This phenomenon is so-called non-coalesced memory ac-
cess. If all 32 required data are in the same cache line,
there is only 1 cache miss (off-chip memory access) within
a warp. This phenomenon is so-called coalesced memory
access. A warp with non-coalesced memory access will be
deferred for a long memory access latency. If all warps
are waiting for memory accesses (inactive warps), the SM
utilization as well as the GPU data throughput will be
drastically reduced. Therefore, coalesced memory access
within a warp significantly improves GPU performance.
Figure 1 indicates that 7 vertical threads shares one mem-
ory access (cache line). However, 7 horizontal threads
require 7 memory accesses.
III. CUDA-Mikami Routing Algorithm
We propose a parallelized CUDA-Mikami routing al-
gorithm on NVIDIA’s GPU. Our algorithm is composed
of three stages: preparation stage, CUDA-Mikami stage,
and back-trace stage. In the preparation stage, we gen-
erate cross lines from source-point and target-point. All
grids on the lines are taken as the initial base grids for
CUDA-Mikami stage. In CUDA-Mikami stage, vertical
and horizontal segments are generated from base grids by
using our proposed line probing techniques. All grids on
the generated segments are taken as the base grids for
next level line probing. The line probings are iteratively
performed until an intersection point has been found. Fi-
nally, CPU performs back-trace to determine the routing
path. Figure 2 illustrates the overall routing algorithm.
Fig. 5. Example of horizontal line probing.
are de-asserted to 0 for routable grid and asserted to 1
for un-routable grid. The routability result obtained by
ballot() is then used for routable grid marking. There
are two horizontal routing directions, leftward and right-
ward. For leftward routing, the ffs() of routability result
specifies the first un-routable grid. For rightward routing,
the ffs() of bitwise reversed routability result specifies
the first un-routable grid. After that, the warp marks
all routable grids between the base grid and the first un-
routable grid. Each warp continues to handle next 32 hor-
izontal routing grids unless an un-routable grid is found.
Figure 5 illustrates our horizontal line probing. For sim-
plicity, we assume there are four threads in a warp. Sym-
bol ‘S’ and ‘X’ represent the base grid and obstacle grids,
respectively. In Figure 5, the warp checks grid routability
and marks routable grids. The horizontal wire segment
is completed when the first un-routable grid is found in
both directions.
IV. Experimental Result and Race Condition
Issue
We perform the routing experiments on a Linux-based
machine with Intel Core i7 CPU 930 @ 2.80GHz, 8G mem-
ory and NVIDIA GeForce GTX 580. Our CUDA-Mikami
router is compiled with gcc 4.4.5 and nvcc 4.0.
We randomly generate 10 benchmark designs with
thousands of obstacles to evaluate the efficiency of our
CUDA-Mikami router. We also implement Mikami router
in CPU-version for comparisons. The major difference be-
tween CUDA-Mikami and CPU-version Mikami router is
the approach of line probing. In CPU-version, we sequen-
tially generate all the lines one-by-one as the conventional
Mikami algorithm proposed; In CUDA-Mikami, we use
massive threads to generate all the lines simultaneously
on NVIDIA’s GPU.
Table I lists our experimental results. We compare the
results of CUDA-Mikami, denoted GPU, to the result of
CPU Mikami, denoted CPU. Columns No warp and Warp
indicate the results of CUDA-Mikami without and with
warp-level line probing technique during horizontal line
probing, respectively. The ratios of CPU to GPU run-
TABLE I
Run-time comparisons
Benchmarks CPU (msec) GPU (msec) Ratio (%)
No warp Warp No warp Warp
ben2210x2220 75.93 81.53 58.56 93.1 129.7
ben2120x2120 84.40 73.04 49.50 115.5 170.5
ben2345x2134 75.83 75.87 60.19 99.9 126.0
ben2400x2100 80.98 60.69 59.96 133.4 135.1
ben2450x2420 142.90 102.03 77.39 140.1 184.6
ben2700x2525 150.18 91.30 81.03 164.5 185.4
ben2425x2900 169.21 83.65 81.76 202.3 207.0
ben2813x2836 179.46 123.14 94.24 145.7 190.4
ben2800x2550 180.81 88.85 88.54 203.5 204.2
ben2819x2953 200.65 103.80 97.33 193.3 206.2
Average 134.04 88.39 74.85 149.1 173.9
TABLE II
Bend-count comparisons
Benchmarks CPU GPU Difference Ratio (%)
ben2210x2220 345 343 2 0.6
ben2120x2120 358 357 1 0.3
ben2345x2134 301 301 0 0.0
ben2400x2100 253 253 0 0.0
ben2450x2420 190 192 2 1.1
ben2700x2525 226 226 0 0.0
ben2425x2900 232 232 0 0.0
ben2813x2836 267 265 2 0.7
ben2800x2550 188 190 2 1.1
ben2819x2953 222 222 0 0.0
Average 0.4
time improvement are listed in column Ratio. According
to Table I, memory coalesced line probing achieves 173.9%
run-time improvement while memory non-coalesced line
probing achieves only 149.1% run-time improvement.
Our next experiment compares the routing quality of
CPU and GPU Mikami algorithms. The total number of
routing bends are listed in Table II. From Table II, we
notice that the routing bend number of CUDA-Mikami
are slightly greater than that of CPU Mikami. To under-
stand the bend-count differences between the two routers,
we analyze the routing results and identify a race condi-
tion issue during line probing.
Since vertical and horizontal line probings employ dif-
ferent line generating techniques, race condition may oc-
cur when the two line probings are performed simul-
taneously. In horizontal line probing, we use function
ballot() to check the routability and function ffs() to
identify the first un-routable grid. All routable grids, be-
tween the base grid and the first un-routable grid, will be
marked in this level. If an unmarked and routable grid is
read by one vertical thread between horizontal routabil-
ity checking and grid marking, race condition occurs since
the grid can be marked by both vertical and horizontal
line probings. Taking Figure 6 as an example, CUDA-
Mikami marks source bits of all level-1 routable grids to
1 and 0 from source-point and target-point, respectively.
After level-1 cross line generation, all the blue grids be-
come base grids. In level-2 cross line generation, assume
國科會補助計畫衍生研發成果推廣資料表
日期:2012/09/07
國科會補助計畫
計畫名稱: 以圖形處理器加速之運算架構應用於電子設計自動化(I)
計畫主持人: 劉一宇
計畫編號: 100-2221-E-155-052- 學門領域: 積體電路及系統設計
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
透過 CUDA 研究主題之論文發表，認識一位美國 NVIDIA 工程師，共同討論延伸
GPU 應用在 EDA 領域的合作案 (本期正在執行之國科會計畫)。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
