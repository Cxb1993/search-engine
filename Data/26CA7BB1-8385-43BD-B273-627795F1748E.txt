2 
目     錄 
 
中文摘要 1 
Abstract 1 
1 Introduction 3 
2 Motivation 3 
3 Purpose and procedure 3 
Subtitle 1: Overview of the Semi-Humanoid Robot..............................................4 
1.1 Introduction 4 
1.2 System Architecture of the Semi-Humanoid Robot 5 
1.3 Hardware Architecture of the semi-humanoid robot 7 
Subtitle 2: Real-Time Stereo-Vision System for Semi-Humanoid Robot ...........17 
1. Introduction 17 
2. Introduction of Stereo Vision 18 
3. Depth of Stereo Vision 19 
4. Elevator Button Recognition Algorithm 22 
5. Face Detection Algorithm 27 
Subtitle 3: Omni-Directional Trajectory Control for Surveillance and Security 
Robot Team..........................................................................................................33 
1. Design of the Trajectory Control 33 
2. A Fully-Fuzzy Controller Design for the SSR 47 
Subtitle 4: Omni-Directional Trajectory Control for Surveillance and Security 
Robot Team..........................................................................................................55 
1. Design of the Trajectory Control 55 
2. A Fuzzy Controller Design for the Semi-Humanoid Robot Arm 69 
Subtitle 5: Sensor Fusion and Behavior Strategies for Surveillance and 
Security Robot Team............................................................................................75 
1. Introduction 75 
2..1 Sensor Fusion and Behavior Strategies 76 
2.2 Overview of the Fire Sensors System 86 
2.3 Overview of the RFID System 90 
2.4 Overview of the Electric Compass and Infrared Sensor 95 
2.5 Periodic Patrol Behavior 96 
2.6 Emergency Action 98 
2.7 Automatic Recharge Behavior 99 
4 Result and conclusions 99 
1. The subsystems ................................................................................................99 
1.1 Experimental Results of Image Processing 99 
4 
Subtitle 1: Overview of the Semi-Humanoid Robot 
1.1 Introduction 
The enhanced and human-like locomotive ability of the humanoid robot makes it 
possible to delay humanoid robots to place where humans are still working like 
machine. Within the manufacturing industry itself, it is foreseeable that the emergence 
of the humanoid robot will certainly automate more tasks, such as: maintenance, 
diagnostics, security [4, 5] etc.  
We present a design and implementation of sensor based on behavior verification 
system using an environment and manipulation knowledge, which is also used in 
manipulation motion planner [6-9]. We demonstrated a water-pouring task and a 
dishwashing task of the life-sized humanoid robot HRP2-JSK [10, 11] in a real 
environment while verifying its own motion (see Fig. 1.1). Reference [12] presents a 
knowledge-based person-centric human–robot interaction system using facial and 
hand gestures. Understanding the principles of motor coordination with redundant 
degrees of freedom still remains a challenging problem. Reference [13] presents the 
first step towards the development of a working redundancy resolution algorithm 
which is robust against modeling errors and unforeseen disturbances arising from 
contact forces. The path planning algorithm was employed for the humanoid robot 
(Maru-1) arms. The RRT (rapidly exploring random tree) algorithm, which has been 
proven to be an efficient approach to multi-arm motion plans, was selected for use in 
this study [14, 15]. Robots that work alongside us in our homes and workplaces could 
extend the time an elderly person can live at home, provide physical assistance to a 
worker on an assembly line, or help with household chores. Domo (shown as Fig 1.2) 
is able to physically locate the shelf, socially cue a person to hand it an object, grasp 
the object that has been handed to it, transfer the object to the hand that is closest to 
the shelf, and place the object on the shelf [18]. 
  
Fig. 1.1 Vacuuming behavior of HRP2 [12] Fig.1.2 The humanoid robot Domo [16] 
Another scheme is to classify robots according to their structural topologies. A 
robot is said to be a serial robot or open-loop manipulator if its kinematical structure 
6 
semi-humanoid robot is shown on the Fig. 1.5. In stereo vision and speech processing 
level, the robot can distinguish out the faces of people from established database, and 
discern the lift button. When several people are speaking at the same time, the robot 
will be able to separate and recognize all sound sources. In sensor fusion level, the 
robot captures its own approximate location by RFID reader and tag module, and can 
avoid barriers by the ultrasonic sensors. Among motion control module divides into 
two parts: ODMR (Omni-Direction Mobile Robot) control and semi-humanoid robot 
arm control. In this thesis, we explore the semi-humanoid robot arm control only. The 
robot combines the information that is from stereo vision, speech processing, and 
sensor fusion level to decide his behavior strategy. The aim of the thesis is to design 
the tracking trajectory of semi-humanoid robot arm motion control which will be 
introduced in detail in later sections. However, we are not concerned here about the 
rest of other levels. 
 
Fig. 1.4 The system architecture of the semi-humanoid robot 
8 
Table 1.1 The Altera DE2 Development and Education board 
NIOS II - Altera DE2 Board 
FPGA 
Cyclone® II 2C35 FPGA with 
EPCS16 16-Mbit serial 
configuration device 
I/O 
Devices 
Built-in USB-BlasterTM cable for 
FPGA configuration 
10/100 Ethernet 
RS232 
Video out (VGA 10-bit DAC) 
Video in 
(NTSC/PAL/multi-format) 
USB 2.0 (type A and type B) 
PS/2 mouse or keyboard port 
Line in/out, microphone in (24-bit 
Audio CODEC) 
Expansion headers (76 signal pins)
Infrared port 
Memory 
8-MBytes SDRAM, 512K SRAM, 
4-MBytes flash 
SD memory card slot 
Displays 
16×2 LCD display 
Eight 7-segment displays 
Switches 
& LEDs 
18 toggle switches 
18 red LEDs 
9 green LEDs 
Four debounced push-button 
switches 
Clocks 
50-MHz crystal for FPGA clock 
input 
27-MHz crystal for video 
applications 
External SMA clock input 
 
Cyclone II EP2C35 [21] is employed as the main processor. The FPGA is based 
on a 1.2-V, 0.09-μm manufactured with densities up to 33216 logic elements (LEs) 
and up to 472 Kbits of RAM. Phase-locked loops (PLLs) provide clock multiplication 
and division, phase shifting, and external clock outputs. Table 1.2 schedules the 
10 
B. NB 
The on-board NB we utilize to cope with stereo vision and speech processing 
unit by Borland C++ Builder is X32 manufactured by IBM as shown in Fig. 1.9. 
We use two pieces of the Altera DE2 boards for motion control module. One 
controls 12 motors of the two robots arms, and the other deals with omni-directional 
wheels for shifting on the ground. The image and sound of the environment is dealt 
with through the NB which is on-board. From the NB, the command is transmitted to 
the Altera DE2 boards by RS232. While the algorithm of the robot arm trajectory or 
path planning works, the Altera DE2 boards will control the robot to take the elevator, 
catch a baseball by the arms, and patrol everywhere. 
 
1.3.2 Power System and Driver Circuit Board 
 
Table 1.2 The specification of Cyclone II 2C35 
LEs 33216 
Columns 3 
M4L RAM 
Blocks 105 
Total RAM bits 483840 
Embedded Multiplier 
Blocks 
35 
PLLs 4 
Maximum user I/O pins 475 
Available pins 48 
Rows 35 
LAB 
Columns 60 
 
The power supplying for the semi-humanoid robot is a series of batteries 
comprised by two 12V/6Ah recargeable batteries. The reason for using this battery is 
its superior durable. Fig. 1.10 is the power circuit board used for supplying voltage to 
NIOS, RFID, CCD, and ultrasonic sensor module implemented by voltage regulators, 
7800 Series. These regulators employ internal current limiting, thermal shutdown, and 
safe-area compensation. With favorable heat-sinking, output currents can be delivered 
in excess of 1.0 A. Fig. 1.11 is the appearance of the regulators and its application. 
78xx, the two digits of the type number, indicate nominal voltage. 
12 
3863024C motor. We use 024C type of the motor and planetary gear heads 66/1 in our 
SSR which weights about 25 kg for the requiring velocity and larger torque. 
 
Fig. 1.13 The appearance of the DC motor and encoder HEDL-5540 module 
 
Fig. 1.14 The specification of Series 3863024C motor 
1.3.4. Stereo Vision and Speech Guidance Module 
A. Stereo vision module 
The CCD of the Stereo vision module is designed by the WaTec Technology 
Corporation looks upward at the catadioptric mirror to get the image information. The 
model of the CCD is WAT-221S as shown in Fig. 1.15 (a), and it employs Digital 
Signal Processor (DSP) IC for image processing. 
The image grabbing box is USB 2.0 Hi-Speed Video Grabber S-960 produced by 
AME Optimedia Technology grabs the image real-time in 2-D image processing and 
then transmits the data to the external interface. Fig. 1.15 (b) shows the actual photo. 
14 
  
 
Fig 1.17 (a) AWID reader Fig 1.17 (b) RFID tag Fig 1.18 Ultrasonic range finder 
Table 1.3 The specification of AWID UHF long range reader 
AWID UHF Long Range Reader 
Read Range 5(m) 
Protocol ISO-18000(tm) 
Frequency 868MHz，902-928MHz 
RF Power 1.0Watt (+30dBm) 
Multi-tag Read 30 tags/sec 
Power Supply 
6.5DC@1,00mA to 
15DC@450mA 
Operating 
Temperature 
-20℃~ +50℃ 
Storage 
Temperature 
-35℃~ +70℃ 
I/O control RS-232 
Size 20.32L×20.32W×2.9H (cm)
Weight 1575g 
Table 1.4 The specification of RFID tag 
RFID tag 
Material ABS 
Dimensions 8.07×1×0.16 (inch)
1.3.6 Hardware Configuration of the Semi-Humanoid Robot 
We have introduced each part of hardware architecture of semi-humanoid robot 
above. In this section, the mechanism of the components for our semi-humanoid robot 
is represented in Fig. 1.19. 
Stereo vision module is equipped with the head part for the eyes of 
semi-humanoid robot. For two degrees of freedom of the neck, stereo vision module 
on the head part can lift up, bend, and look both sides. The transmission of neck is 
composed of two DC motors, ringlike gear [23], and a wormset manufactured by 
KHK [24]. In the shoulders part, we combine the two pieces of motors and one 
wormset to transmit the torque produced from the whole weight of the arm. Fig. 1.20 
shows the motors and worm gears wrapped in an aluminium ware. A wormset consists 
16 
 
Fig. 1.21 KHK wormset 
 
 
Fig. 1.20 Mechanism of shoulder part Fig. 1.22 Mechanism of upper arm 
. 
  
(a) Grasping (b) Catching (c) Opening 
Fig. 1.23. The behavior of hand part 
Eventually, the chassis of the components for our semi-humanoid robot is as 
follows. Fig. 1.24 shows the omni-directional polyurethane roller wheel. Table 1.5 
lists the specification of the semi-humanoid robot. The semi-humanoid robot made by 
Al-Mg compound metal is weight around 100 kg, height of 130 cm , and 90 cm in 
length and width. It is equipped with four orthgonal DC motors. 
 
 
Fig. 1.24 8cm omni-directional polyurethane roller wheel 
 
 
18 
detector to get the position of the face in the image. Some research addressed in 
developing target-tracking system of security robot [28,29]. Hisato Kobayashi et al. 
proposed a method to detect human being by an autonomous mobile guard robot [30]. 
Similar work can be found in [31] where a detected face region is tracked with 
complexion information. Lang et al. [32] combine several signals including sonar, 
laser scanner, sound localization and color image processing. The work presented here 
is a part of a semi-humanoid robot project, where one task for the mobile robot is to 
detect people in an indoor environment. In this thesis, we address these problems and 
introduce a method to detect a person. 
 
2. Introduction of Stereo Vision 
Using computer vision to achieve the required objective, we must comprehend the 
structure of computer vision [33]. Reconstruction of objects is in the 
three-dimensional, reconstructive process of objects is in the three-dimensional, all 
real objects are in a place, which is three-dimensional coordinates of the system. The 
three-dimensional coordinate is called "World Coordinate System, WCS". 
By the camera, we will obtain a two-dimensional image. The two-dimensional 
coordinate is "Image Coordinate System, ICS". A coordinate of within camera system 
is between WCS and ICS. The coordinate is called "Camera Coordinate System, 
CCS". 
We can not use the information of two-dimensional images to obtain 
three-dimensional coordinate of the world. We transform information of images from 
the ICS to the CCS, and switch information of images from the WCS to the CCS. 
Finally, we will obtain the coordinates of the objects in the world. 
 
Fig 2.2.1 The simple construction of the binocular stereo system 
20 
 
Fig. 2.3.1 The pinhole model is an approximation for a real camera. 
 
 
Fig 2.3.2 Trigonometry of target and two cameras 
This method uses the information of P and angles rθ  and lθ  to obtain the 
information of depth.The other method in common use is that we can use 
two-dimensional projective image and triangularity to obtain the depth, as shown in 
Fig 2.3.3, where the dX is the length of between P and center line of optics. The dx is 
projective length of dX in the image plane. If dX is constant length, then Z and dx are 
inverse proportion. 
 
22 
image plane to convert pixel unit into length unit shown in Fig 2.3.5 Then Eq. (2.9) 
can be rewritten as 
   
2 tan
B SZ
dx θ
⋅= ⋅ ⋅  (2.10) 
 
Fig 2.3.5 Convert length of image into pixel units 
4. Elevator Button Recognition Algorithm 
The most important part of the elevator button is the numeral recognition. Let our 
robot arms can accurate operate the elevator. We use features extraction to recognize 
the number in the elevator button. Image compare is simply and convenient used to 
recognize number. They calculate the amount of different type of each number, and 
the most appear of type of each number is treated as standard type number. Then, the 
input number is mapping with standard type number, if the amount of different pixel 
is less than the threshold, the input number will treat as the same number, or else it 
will treat as the different number. But error rate of the method is relatively high, so we 
use feature extraction to recognition number of elevator button. 
 
Numeral Segmentation and Features Extraction 
Go through a series of pre-process, we can obtain position of each elevator 
button in Fig 2.26. We research the gap of the number and detect the position of gap 
which is lying in between 1/3 and 2/3 in the image, as shown in Fig 2.4.1.  
 
Fig 2.4.1 The gap position of numbers 
We define the line in the 1/3 and 2/3 of height and width image of image. The 
number is segmented eight directions of gap. We calculate the amount of pixel on the 
every direction line. The pixel number is greater than the threshold, we encode 1, or 
else we encode 0. For the digital number 6, shown as Fig 2.4.2, its eight directions 
code is 11101111.  
24 
 
(Continued) 
26 
 
Fig 2.4.5 Thinning and encode of the number 
The result of elevator recognition is shown in Fig 2.4.6. The numbers are the result of 
elevator recognition in the green blocks. The information of elevator buttons is shown 
in Fig 2.4.7. The information is the relative coordinate from left camera to button. 
 
Fig 2.4.6 The result of elevator recognition 
28 
Fig 2.5.1 The cluster of complexion in the CbCr subspace 
 
 
Fig 2.5.2 The results of complexion processing: (a) original image (b) its binary 
image 
 
Localization of Facial Features 
The eyes and lips are the most obvious features for recognition in the several face 
features. Most methods for eye localization are template-based. However, we first 
derive the eyes map, lips map and face mask from both the luminance and 
chrominance of an image, then use the maps to find the eyes and lips. We consider 
only the area covered by a face mask that is built by a series of pre-processing. Fig 
2.5.3 shows an example of the face mask. Because eyes locate on upper half of face 
and lips locates on lower one, we divide the region of face into the two parts for 
reducing the time of locating eyes and lips map, as shown in Fig 2.5.4.  
 
Fig 2.5.3 The face pre-processing 
30 
 
 
Fig 2.5.5 The construction of eye maps 
 
Fig 2.5.6 The construction of lips maps 
32 
We first use the above-mentioned rules to find possible eye candidates and 
reduce the time of processing eye-lips triangle. The eye candidates are also combined 
with lips candidates to find the eye-lips triangles. Suppose we are given the two eye 
candidates (ei, ej) in the set. The following two rules are used to determine if (ei, ej) 
should be added to the set of candidate eye pairs. 
 
1.The ratio of interocular distance to the width of head (see Fig 2.5.8). 
 
Fig 2.5.8 The structure of eye-lips triangle 
10.25 0.65
head
D
D
≤ ≤      (2.13) 
Where  
D1 denotes the distance between two eye candidates. 
Dhead denotes the head width. 
2. The deviation of eye-pair direction from the direction of best-fit ellipse 
 0 dev rangeθ θ≤ ≤  (2.14) 
Where  
devθ  denotes the angle between two eye candidates and head. 
rangeθ  is set as 030  in our experiment 
The eye-lips triangle is verified if they pass the following two criteria: 
1. Isosceles of an eye-lips triangle 
 2 3
2 3
0 0.38
min( , )
D D
D D
−≤ ≤  (2.15) 
2. Range of an angle φ  
 32 68φ≤ ≤D D  (2.16) 
 
34 
 
 
Fig. 3.1.1 The design procedure of the 
trajectory control 
Fig. 3.1.2 The graph and parameter 
definition of the SSR 
 
1.2 Kinematic Model of the SSR 
We make a description of kinematic model of the SSR in the beginning. Let the 
mobile robot move on a horizontal field assumed not being tilt. There are two 
coordinate frames used in the modeling: the robot frame and the world frame. The 
robot frame is fixed on the SSR, and the origin of the robot frame is the mass center 
of the chassis of the robot, as shown in Fig 3.1.2. Wheel 1 is front left wheel, wheel 2 
is front right wheel, wheel 3 is rear right wheel, and wheel 4 is rear left wheel. wΣ  is 
the world coordinate, and rΣ  is the robot coordinate. iR  is the radius of the ith 
omni-directional wheel and is assumed the same as R . L  is the distance from the 
mass center to the center of the corresponding wheel and is also assumed the same. θ  
is the orientation of the robot coordinate with respect to the world coordinate. ψ  is 
the azimuth angle of the SSR. iT  is the driving force (traction force) of the ith motor. 
F  is the compound force. We define the posture and the velocity of the robot in the 
world coordinate frame and in the robot coordinate frame respectively, as shown in 
Table 3.1. 
Table 3.1.1 Definition of variables 
Variable Definition 
[ ]Tw w wX Y θ=wP  The posture of the robot in the world coordinate frame. 
36 
0 0
0 0
0 0
M
M
I
⎛ ⎞⎜ ⎟= ⎜ ⎟⎜ ⎟⎝ ⎠
M
 is a symmetric positive-definite matrix. M is the mass of the 
SSR and I  is the moment of inertia of the SSR. 
It follows that 
w rP pr w→= ℜ   (3.4) 
w rF fr w→= ℜ  (3.5) 
where [ ]rf
T
x y If f M=  is the force vector applied to the robot in the robot 
coordinate system. Take differential of Eq. (3.4) 
w r rP p pr w r w→ →= ℜ + ℜ     
Substitute the result into Eq. (3.3), we obtain 
( )w r rF M p pr w r w→ →= ℜ + ℜ    
From Eq. (3.5) , we know that 
1 1 1 1 [ ( )]   [ ]r w r r r rf F  M p p M p pr w r w r w r w r w r w r w r w
− − − −
→ → → → → → → →= ℜ = ℜ ℜ + ℜ = ℜ ℜ + ℜ ℜ      
1 [ ]r rM p pr w r w
−
→ →= ℜ ℜ +    
Because 
1
r w
−
→ℜ  equals to Tr w→ℜ , thus 
( )
= ( + )r r rf M p p
r r
T
r w r w r r
M x y
M y x
I
θ
θ
θ
→ →
⎛ ⎞−⎜ ⎟⎡ ⎤= ℜ ℜ + ⎜ ⎟⎣ ⎦ ⎜ ⎟⎝ ⎠
 
    

 (3.6) 
Eq. (3.6) can be represented as 
( )x r rf M x y θ= −    (3.6.1) 
( + )y r rf M y x θ=    (3.6.2) 
I IθΜ =  (3.6.3) 
In addition, , ,x y If f Μ  are given by 
38 
2
1
2
2
2
3
4
2
24 0 02 2 2 2
2
2 2 2 2( )
2 2 2 2 2( + )  = 0 4 0
2 2 2 2 2
0 0 4
r r r
w
r r r
u
M x y x
u IkM y x y
uR R
I
uL L L L
L
θ
θ
θ θ
⎛ ⎞⎛ ⎞⎜ ⎟⎛ ⎞ ⎜ ⎟⎜ ⎟− − ⎜ ⎟⎜ ⎟ ⎝ ⎠⎛ ⎞ ⎜ ⎟⎜ ⎟⎛ ⎞− ⎛ ⎞⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎛ ⎞⎜ ⎟ ⎜ ⎟⎜ ⎟− − − ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎝ ⎠ ⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎝ ⎠⎝ ⎠ ⎜ ⎟⎜ ⎟⎝ ⎠ ⎜ ⎟⎜ ⎟⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎜ ⎟⎝ ⎠
  
  
 
2
2
2
2
24 0 0
2
c 20 4 0
2
0 0 4
r
r
x
y
R
L
θ
⎛ ⎞⎛ ⎞⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠⎜ ⎟⎛ ⎞⎜ ⎟⎛ ⎞ ⎜ ⎟− ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠ ⎜ ⎟⎝ ⎠⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠



(3.12) 
Thus, Eq. (3.12) can be simplified as 
1
2
2 2
32 2
4
2 2 2 2
2 2 2 2( ) 2 2
c 2 2 2 2( + ) + 2 + 2  = 
2 2 2 2
4 4
r r r r
w
r r r r
u
M x y x x
uI kM y x y y
uR R R
I L L
uL L L L
θ
θ
θ θ θ
⎛ ⎞− −⎜ ⎟⎛ ⎞⎜ ⎟⎛ ⎞− ⎛ ⎞ ⎛ ⎞ ⎜ ⎟⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟− −⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎜ ⎟ ⎜ ⎟⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠⎝ ⎠ ⎜ ⎟⎝ ⎠⎜ ⎟⎜ ⎟⎝ ⎠
   
   
  
 
Combine the first three left terms into one term. 
2 2 1
2
2 2
3
2 2 4
2 2
2 c( + ) M + 2
2 c( + ) + + 2  = 
c( + 4 ) + 4
w
r r r
w
r r r
w
IM x y x uR R
uI kM y Mx y
uR R R
I uI L L
R R
θ
θ
θ θ
⎛ ⎞−⎜ ⎟ ⎛ ⎞⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎝ ⎠⎜ ⎟⎝ ⎠
Q
  
  
 
 
Take the left term apart according to the same classification. 
2 2 1
2
2 2
3
2
2 4
2 2
2 2c( + ) 0 0 0
2 2c0 ( + ) 0 + 0 =   
4c0 0 ( + 4 ) 0 0
w
r r
w
r r
w
IM M uR Rx x
uI kM y M y
uR R R
I L uI L
R R
θ
θ
θ θ
⎛ ⎞ ⎛ ⎞−⎜ ⎟ ⎜ ⎟ ⎛ ⎞⎛ ⎞ ⎛ ⎞⎜ ⎟ ⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠⎜ ⎟ ⎜ ⎟ ⎝ ⎠⎜ ⎟⎜ ⎟ ⎝ ⎠⎝ ⎠
Q

 
 
 
 (3.13) 
The result is represented as a matrix form:  
r rΝp Κp Ρu+ =   (3.14) 
where
2
2
2
2
2( + ) 0 0
2 = 0 ( + ) 0
0 0 ( + 4 )
w
w
w
IM
R
IM
R
II L
R
⎛ ⎞⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠
Ν
 
2
2
2
2
2 0
2= 0
40 0
c M
R
cM
R
cL
R
θ
θ
⎛ ⎞⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠
Κ
- 

,and 
= k
R
Ρ Q
 
The dynamic equation can be represented in the world coordinate with the state 
variable w w
( ) ( ) ( ) ( )
T
t x t y t tθ⎡ ⎤= ⎣ ⎦x   ,the control input 
[ ]1 2 3 4( ) ( ) ( ) ( ) ( ) Tt u t u t u t u t=u  and the output w w( ) ( ) ( ) ( )
T
t x t y t tθ⎡ ⎤= ⎣ ⎦y   . We 
simplify Eq. (3.14) further as 
( ) ( ) ( )x Ax But t t= +  
40 
2 2 2( +4 )w
LkRb
IR I L
=
, 1 ( cos sin )d θ θ= − + , 2 ( cos sin )d θ θ= − − , 3 (cos sin )d θ θ= − , and 
4 (cos sin )d θ θ= +  
Furthermore, we can make a state equation form 
( ) ( ) ( )x Ax But t t= +  (3.20.1) 
( ) ( )t t=y Cx  (3.20.2) 
where 
1 2
2 1
3
0
0
0 0
a a
a a
a
θ
θ
⎛ ⎞⎜ ⎟= −⎜ ⎟⎜ ⎟⎝ ⎠
A


, 
1 1 1 2 1 3 1 4
1 2 1 3 1 4 1 1
2 2 2 2
b d b d b d b d
b d b d b d b d
b b b b
⎛ ⎞⎜ ⎟= ⎜ ⎟⎜ ⎟⎝ ⎠
B
, 3 3I ×=C  
In [40], the Resolved Acceleration Control was introduced in the dynamic system 
with three omni-directional wheels. If ( , , )w wx y θ   and ( , , )w wx y θ   are known, one 
can obtain the control input iu  by the estimated inverse dynamic model. Accordingly, 
we combine the proposed dynamic model with a fully-fuzzy controller which will be 
talked about in Chapter 4 to track the desired trajectory. The complete architecture of 
the motion control of the SSR system which comprises the dynamic and kinematic 
model is shown in Fig. 3.1.3.  
 
 
Fig. 3.1.3 The overall system block diagram of the SSR 
f  consists of Eqs. (3.22) and (3.24) (see p.53 and p.56), FLC is fuzzy logic 
controller, Eq. (3.23) will also be derived in Subsection 3.4.4, 
-1J  is inverse 
Jacobian matrix,. A(x), B(x), and C are from Eqs. (3.20.1) and (3.20.2), and ∫ is the 
integrator. We will describe the system block diagram in detail in the later sections. 
 
1.4 Design Procedure of Development of NIOS 
42 
bit represented phase A and B, respectively. On the state 1, phase A is 1, and phase B 
is 0, and if the next signal that phase A is 1 and phase B is 1 turns to the state 2, the 
direction equals to 0 defined as counterclockwise. On the contrary, if initial state is 
state 1 and the next state turns to the state 4, the direction equals to 1 defined as 
clockwise. So, if it has the order that 01Æ 11Æ 10Æ 00, we can know it is 
counterclockwise, otherwise, if it has the order that 01Æ 00Æ 10Æ 11, we can know 
it is clockwise. We implement the finite state machine in Verilog-HDL and create a 
block shown as in Fig. 3.1.7. 
  
Fig. 3.1.6 Finite State Machine Fig. 3.1.7 Implement with FSM written 
in Verilog- HDL 
 
The other method to implement the making decision of the direction is to use 
edge trigger detection. When phase A is positive edge trigger, if phase B is 0, it means 
that phase B leads phase A by 90 degrees defined as counter clockwise. In other words, 
when phase A is positive edge trigger, if phase B is 1, it means that phase B lags phase 
A by 90 degrees defined as clockwise. We have eight signal lines of the four encoders, 
so we design eight LPFs to filter the noise. Furthermore, we can get the information 
about the angular velocity of the encoder with the edge trigger detection shown as in 
Fig. 3.1.8. 
  
Fig. 3.1.8 The internal counter Fig. 3.1.9 The four times frequency counter 
 
When phase A is positive edge trigger, the internal counter starts to count the number 
with the 50 times divided-frequency of the system clock, 50 MHz, and ends up 
counting when phase A is negative edge trigger. The number of the counter represents 
the period of the phase A. For example, if the counter is 30, it means that it takes 
30 sμ . If the counter is 50, it means it takes 50 sμ . The whole period is the double 
time which the counter takes because we must consider the rest half period. It goes 
without saying that period is the inversion of the frequency, thus we can get the 
44 
,i iω θ− −
( , , , )w w w wX Y X Y 
1 2 3 4( , , , )u u u u
,d d(  )
u+
iw′ _m dotR
,i iω θ
1 2 3 4( , , , )w w w w
( , )i iw w
−
 
Fig. 3.1.11 The trajectory control algorithm (TCA) 
Basically, the compound velocity _Rm dot  is decided by FLC system, and the 
angle which the SSR should move at is determined by the trigonometric function, 
arctangent function, which substitutes the distance error in the X-direction, _Xm err , 
divide by distance error in the Y-direction, _Ym err , denoted as 
1 _tan ( )
_
Xm err
Ym err
θ −∠ =
.  (3.22) 
θ∠  represents the heading angle to the moving direction as shown in Fig. 3.1.12. 
 
  
Fig. 3.1.12 The heading angle 
representation 
Fig. 3.1.13 The omni-directional mobile 
region 
 
Unfortunately, there exist some exceptions in tangent function when the angle 
46 
implement the behaviors of the omni-directional motion of the SSR by Eq. (3.28) as 
shown in Fig. 3.1.15. The compound velocity _Rm dot  is divided into _Xm dot  
and _Ym dot . 
_ sin
_
_ cos
Xm dot
Rm dot
Ym dot
θ
θ
⎛ ⎞ ⎛ ⎞= ×⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠  (3.28) 
   
(a) (b) (c) 
   
(d) (e) (f) 
  
(g) (h) (i) 
Fig. 3.1.15 The omni-directional motion behavior 
We design some different motion trajectories of the omni-directional behavior, 
such as running a circular trajectory (Fig. 3.1.16(a)(b)), a square trajectory back and 
forth (Fig. 3.1.16(c)), and a diamond trajectory (Fig. 3.1.16(d)) to see the robot’s 
tracking performance. 
         
Fig. 3.1.16(a) The circular trajectory. (with fixed heading angle) 
48 
of the omni-directional mobile robot system [40-42]. [42] presents the dynamic 
models of the omni-directional mobile robot and the control strategy. The LMI 
method is used in general T-S fuzzy design to meet the stability condition. [44] 
proposes a T-S type fuzzy controller with both state of the dynamics and the path 
tracking error.  
In this thesis, a fully-fuzzy logic controller (FFLC) is used not only for velocity 
control but also for position control [45-49]. In Section 2.2, we present a fully-fuzzy 
trajectory tracking system with velocity and position FLCs. In Section 2.3, the 
velocity controller is described. We will compare with P control and FLC for velocity 
control. And then the position FLC is utilized to determine the correction gain of the 
control input in Section 2.4. Finally, Section 2.5 presents the dynamic trajectory 
tracking behavior of the robot with the controllers proposed. 
 
2.2 A Fully-Fuzzy Trajectory Tracking System 
The omni-directional mobile trajectory control algorithm (TCA) for the SSR has 
been introduced in subtitle 3 as shown in Fig. 3.1.16. We have mapped the procedure 
into the block diagram in Fig. 3.1.3. We design an FFLC to track the desired trajectory 
and the desired velocity generated by trajectory generation system at the same time. 
First, the velocity FLC will determine the control output gain according to the errors 
of the velocity. But, if the whole desired trajectory is tracked, the SSR will stop 
moving. Based on the motion law: 0x x vt= + , we can compute the robot’s position in 
the world coordinate immediately. Comparing with the desired trajectory, the error 
distances between the robot and the desired point in X and Y direction can be 
estimated, and therefore, the moving angle can be derived from inverse trigonometric 
function, and the control input gain _Rm dot  can be decided by the position FLC. 
We will describe the two FLCs in the following. 
 
2.3 Velocity Controller- P Controller and FLC 
There are many controllers designed to compensate the error from the difference 
of the reference input and the control plant output. For example, P controller, PI 
controller, PD controller, and PID controller, etc. They can be implemented by the 
summing amplifier, the difference amplifier, the differentiator, and the integrator. P 
controller shown in Fig. 3.2.1 is the simplest one used in the general design. The more 
complicated controller it is, the more expensive and the more difficult to be 
implemented it is. The property of the proportional controller is that the relationship 
of the reference input and the control plant output is a constant. The gain of the 
50 
obtain the most approximately accurate value. 
 
Fig. 3.2.2 The velocity control with P controller Fig. 3.2.3 The algorithm of the 
low pass filter (LPF) 
 
 
Fig.4.4 The total values of PWM for P control Fig.4.5 The velocity control with FLC 
Due to the external force caused by the friction force, the total output iu  is basic 
value of PWM around the value of 30 plus the output of P control shown in Fig. 3.2.4. 
The other controller used for velocity control is fuzzy logic controller (FLC) as shown 
in Fig. 3.2.5. The FLC was presented by Zadeh [51,52] in 1965. The origin of Fuzzy 
control theory was published in “Outline of a new approach to the analysis of 
complex systems and decision processes” [53] in 1973. And the next year, E.H. 
Mamdani used the form of “IF~ THEN…”, based on the linguistic approach and 
fuzzy inference by Zadeh, to describe the skills of the operator and capture the sensor 
52 
 
2.3.2 Decision Making Logic (DML) 
DML which can decide the corresponding output after receiving input data and 
be assisted by the knowledge base is the kernel of the fuzzy logic system. The fuzzy 
output is derived from the fuzzy implication and the compositional operators of fuzzy 
inference. Fuzzy inference is divided into two kinds of methods, one is GMP (General 
Modus Pones) and the other is GMT (General Modus Tollens). GMP is represented as  
( )P P Q Q′ ′∩ → ⇒  called “forward reasoning” which can infer the result from the 
rules with data input and is suitable for the fuzzy control architecture. On the contrary, 
GMT is represented as ( )Q P Q P′ ′∩ → ⇒  called “backward reasoning.”  
GMP(General Modus Ponens ) 
Rule: IF X is A THEN Y is B 
Fact: X is A 
Result: Y is B 
GMT(General Modus Ponens ) 
Rule: IF X is A THEN Y is B 
Fact: Y is B’ 
Result: X is A’ 
The formulation of fuzzy control rules could be obtained by means of a heuristic 
approach which includes an integration of experienced experts or operators using a 
carefully organized questionnaire. In this thesis, the product fuzzy inference method is 
applied to our system. 
 
2.3.3 Knowledge Base (KB) 
The knowledge base, including the knowledge of application domain and the 
objective of control, consists of a data base and a linguistic control rule base. A FLC 
data base provides the definition for linguistic variables as variables discourse, subset 
of language term, and the plan for the membership functions to be managed by the 
language control rule base which are defined based on experience and engineer’s 
judgment. They are generally in the form of IF-THEN rules, which can be easily 
implemented by fuzzy conditional statements. The fuzzy rule base is characterized by 
construction of a set of linguistic statements based on expert knowledge. In order to 
accomplish the inference procedure, DML utilizes the membership state generated by 
the FI (fuzzy inference). Since each discourse is divided into five subsets, denoted as 
NB, NS, Z, PS, PB, two inputs will construct 25 fuzzy rules. The inference rules can 
be illustrated as follows and the corresponding rule table is shown in Table 4.1. 
54 
fuzzy partitions, denoted by VF(very far), F(far), M(medium), C(close), VC(very 
close), and NB(negative big), NS(negative small), Z(zero), PS(positive small), 
PB(positive big), respectively. The control output CO  is decomposed into 6 fuzzy 
partitions, denoted by Z, VS, S, M, B, VB. Fig. 3.2.10 shows the membership 
function shapes of the fuzzy subsets. 
  
Fig. 3.2.9 Definition of the fuzzy input d+  Fig. 3.2.10 The membership function of input 
 
  
Fig.3.2.10The membership function of input Fig.3.2.10The membership function of output 
 
The inference rules can be illustrated as follows and the corresponding rule table is 
shown in Table 4.2. 
{ , , , , }
{ , , , , }
{ , , , , , }
d VC C M F VF
d NB NS Z PS PB
CO Z VS S M B VB
=
=
=
+
+
 
Rule1: IF d+  is VF and d+  is PB, THEN CO  is VB 
Rule2: IF d+  is VF and d+  is PS, THEN CO  is VB 
Rule3: IF d+  is VF and d+  is  Z , THEN CO  is  B 
#  
Table 4.2 The fuzzy rule table for position control 
d+  VF F M C VC 
PB VB VB B M S 
PS VB B M S VS 
56 
surgeon in a number of surgical scenarios, with the system capable of operating in a 
teleoperated as well as a semi-autonomous mode. S. Calinon [55] presents the 
creation of a robot capable of drawing artistic portraits. L. Guilamo [56] has 
implemented a prototype system on the Digital Human version Humanoid Robot 
Platform 2 (DH-HRP2) which he uses to efficiently compute arm inverse kinematic 
solutions. Guilamo demonstrates the benefits of such a system applied to converting 
workspace trajectories to configuration space trajectories. M. Tarokh [57] presents a 
simple decoupled three term controller and a design procedure for accurate and robust 
trajectory tracking without requiring the knowledge of the robot dynamics. Two 
optimization objective functions are proposed aiming at either minimizing extra 
degrees of freedom or minimizing potential energy of a multi-link redundant robot 
[58]. 
 
 
 
Fig.4.2.1The semi-humanoid robot arm Fig.4.2.2The design procedure of trajectory control 
 
In this thesis, the design procedure of the trajectory control is shown in Fig. 4.2.2. We 
first generate the reference trajectories that determine the time history of the robot 
arm along a given path. Once reference trajectories for the robot arm are specified, it 
is the task of the control system to track them. If both of position and orientation of 
the semi-humanoid robot arm are exactly given, then inverse solutions can be easily 
analyzed from kinematic model. Next, the signals of the encoders equipped on the 
motors of the semi-humanoid robot arm are read to count the feedback signals about 
the angular displacement that the motor rotates. And then the corresponding desired 
joint angles are computed by inverse kinematics. The desired joint angles are used as 
58 
id : translational distance between two incident normals of a joint axis. 
1 1i i id O H− −=  is positive if the vector 1 1i iO H− −  points in the positive 1zi−  direction. 
ia : offset distance between two adjacent joint axes, where 1i i ia H O−= . 
iα : twist angle between two adjacent joint axes. It is the angle required to rotate the 
1zi− -axis into alignment with the zi -axis about the positive x i  axis. 
The resulting transformation matrix, 
1i
iA
−
, is given by 
 
1
cos cos sin sin sin cos
sin cos cos sin cos sin
0 sin cos
0 0 0 1
i i i i i i i
i i i i i i ii
i
i i i
a
a
A
d
θ α θ α θ θ
θ α θ α θ θ
α α
−
−⎡ ⎤⎢ ⎥−⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦  (3.1) 
We first establish the joint coordinate frames using the D-H convention. From the 
coordinate system chosen (see Fig 3.5), the link parameters are given in Table 3.1. 
The D-H transformation matrices are obtained by substituting the D-H link 
parameters into Eq. (3.1): 
Fig. 4.2.5 Planar 6 - DOF robot arm 
Table 3.1 D-H parameters of 6-DOF robot arm 
Joint 
i  
iθ (degree) iα (degree) id (cm) ia (cm)
1 90 -90 0 0 
2 90 90 0 0 
3 0 -90 3d  0 
4 -90 0 0 4a  
5 90 90 0 0 
6 0 0 6d  0 
 
60 
 
( )( ) ( )( )( )
( )
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
6 1 2 3 1 3      
xs S C C C C C S S C S S S S C C C S S C S C
C C C S S C
= − − − + − − −
− +  
 
( )( ) ( )( )( )
( )
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
6 1 2 3 1 3       
ys S C C S C C C S S S S S S S C C C S S S C
C S C S C C
= − + − + − + −
− −  (3.5) 
 ( ) ( )( )6 5 2 3 4 2 4 5 2 3 4 2 4 2 3 6zs S C S C C C S S S C S C C S S C= − − − + − +  
 ( )( ) ( )( )5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4xa S C C C C S S C S S C S C C C S S C S C= − − + − +  
 ( )( ) ( )( )5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4ya S C S C C C S S S S C S S C C C S S S C= + − + + +  (3.6) 
 ( ) ( )5 2 3 4 2 4 5 2 3 4 2 4za S S C C C S C S C S C C= − − − −  
 
( )( ) ( )( )( )
( )
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
4 4 1 2 3 1 3 4 1 2 4 3 1 2       
xp d S C C C C S S C S S C S C C C S S C S C
a C C C C S S a C S S d C S
= − − + − +
+ − − +  
 
( )( ) ( )( )( )
( )
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
4 4 1 2 3 1 3 4 1 2 4 3 1 2       
yp d S C S C C C S S S S C S S C C C S S S C
a C S C C C S a S S S d S S
= + − + + +
+ + − +  (3.7) 
 ( ) ( )( )6 5 2 3 4 2 4 5 2 3 4 2 4 4 2 3 4 4 2 4 3 2zp d S S C C C S C S C S C C a S C C a C S d C= − − − − − − +  
1.3 Inverse Kinematic Model of the Semi-Humanoid Robot Arm 
We use a geometric approach to solve the inverse kinematics problem of 6-DOF 
robot arm with rotary joint. As a verification of the joint solution, the arm 
configuration indicators can be determined from the corresponding decision equations. 
These equations are functions of the joint angles. Consider the elbow robot arm 
shown in Fig. 4.2.6, with the component of p . We project p  onto the 0 0-x y  plane 
as shown in Fig. 4.2.7. 
62 
 
2 2 2
sin z z
x y z
p p
R p p p
α = − = − + +
  (3.11) 
 
2 2
2 2 2
cos x y
x y z
p pr
R p p p
α += = + +
 (3.12) 
 
2 2 2 2 22 2 2
3 43 4
2 2 2
3 3
cos
2 2
x y z
x y z
d p p p ad R a
d R d p p p
β + + + −+ −= = + +
 (3.13) 
 
2sin 1 cosβ β= −  (3.14) 
From Eq. (3.10) to (3.14), we can find the sine and cosine function of 2θ : 
 ( )2sin sin sin cos cos sinθ α β α β α β= + = +  (3.15) 
 ( )2cos cos cos cos sin sinθ α β α β α β= + = −  (3.16) 
 
1 2
2
2
sintan
cos
θθ θ
− ⎛ ⎞= ⎜ ⎟⎝ ⎠        2π θ π− ≤ ≤   (3.17) 
Set joint 3 such that a rotation about joint 4 will align the axis of motion of joint 6 
with the given vector a . Mathematically the above is presented respectively: 
 
( )± ×= ×23 2
z a
z
z a           given ( ), ,x y za a a=a   (3.18) 
We look at the projection of the coordinate frame ( )3 3 3, ,x y z  on 2 2-x y  plane and 
from Fig. 4.2.9, it can be shown that the following are true: 
 ( )3 1 1sin cos siny xa aθ θ θ= − ⋅ = ⋅ = −3 2 2z x a y  (3.19) 
 ( )3 1 2 1 2 2cos cos cos sin cos sinx y za a aθ θ θ θ θ θ= ⋅ = ⋅ = + −3 2 2z y a x   (3.20) 
Where 2x  and 2y  are the x  and y  column vector of 
0
2T , respectively, and a  
is the approach vector. 
                    
1 3
3
3
sintan
cos
θθ θ
− ⎛ ⎞= ⎜ ⎟⎝ ⎠   (3.21) 
64 
( )( ) ( )( )
( )
4 1 2 3 1 3 1 2 4 4 1 2 3 1 3 1 2 4
2 3 4 2 4  
x y
z
a S C C C S S C S C a S S C C C S S S C
a S C S C C
= − − − + − + −
+ −  
Where 4x  and 4y  are the x  and y  column vector of 
0
4T , respectively, and a  
is the approach vector. Thus, the solution for 5θ  is: 
 
1 5
5
5
sintan
cos
θθ θ
− ⎛ ⎞= ⎜ ⎟⎝ ⎠        5π θ π− ≤ ≤   (3.27) 
We need to align the orientation of the gripper to ease picking up the object. Looking 
at the projection of the coordinate frame ( ), ,n s a  on the -5 5x y  plane, it can be 
shown that the following are true (see Fig. 4.2.12): 
 ( ) ( ) ( )6 1 2 3 1 3 1 2 3 1 3 2 3sin x y zn C C S S C n S C S C C n S Sθ = ⋅ = − − + − + +5n y  (3.28) 
 ( ) ( ) ( )6 1 2 3 1 3 1 2 3 1 3 2 3cos x y zs C C S S C s S C S C C s S Sθ = ⋅ = − − + − + +5s y  (3.39) 
Where 5y  is the y  column vector of 
0
5T , and n  and s  are the normal and 
sliding vector of 
0
6T . Thus, the solution for 6θ  is: 
 
1 6
6
6
sintan
cos
θθ θ
− ⎛ ⎞= ⎜ ⎟⎝ ⎠         6π θ π− ≤ ≤   (3.30) 
 
 
Fig. 4.2.11 Solution for joint 5 Fig. 4.2.12 Solution for joint 6 
The above derivation of the inverse kinematics solution of the semi-humanoid 
robot arm is based on the geometric interpretation of the position of the end point of 
the hand. The solution of the inverse kinematics for the robot arm is in left-arm 
situation. As a result of that right-arm situation and left-arm situation are symmetrical; 
we derive a series of solutions of left hand to obtain the other. 
If both position and orientation [ ]n s a p  of the semi-humanoid robot arm are 
66 
In Fig. 4.2.15, channel A and channel B are both 50 % duty cycle square wave. On the 
top, channel B leads channel A by 90 degrees and it is defined as clockwise according 
to the specifications of the encoder. On the other hand, channel B lags channel A by 
90 degrees on the bottom and it is defined as counterclockwise. We decide the 
direction according to the state of the A/B channel. Fig. 4.2.16 presents the finite state 
machine diagram. There are two bits, the lower bit and upper bit represented channel 
A and B, respectively. On the state 1, channel A is 1, and channel B is 0, and if the 
next signal that channel A is 1 and channel B is 1 turns to the state 2, the direction 
equals to 0 defined as counterclockwise. So, if it has the order that 01Æ 11Æ 10Æ 00, 
we can know it is counterclockwise. We implement the finite state machine in 
Verilog-HDL and create a block shown as in Fig. 4.2.17. 
  
Fig. 4.2.15 Channel A/B from the encoder Fig. 4.2.16 Finite state machine 
 
  
Fig. 4.2.17 Symbol block of FSM Fig. 4.2.18 The four times frequency counter 
Furthermore, we can get the information about the angle of the encoder with the edge 
trigger detection shown as in Fig. 4.2.18. Based on the specification of the encoder, 
we know that there are 500 pulses in a revolution. We can design an angle counter 
which counts when both positive and negative edge trigger of the channel A and the 
channel B to obtain the accurate data. On the other hand, the timer counts one time 
when the state changes. The counter value can be changed into the angle. Due to the 
difference of the unit, the angle of the encoder, degree, can be derived from Eq. 
(3.31). 
 (degree)  counter value / 4 360   ( 500) Angle N N= × =  (3.31) 
 
1.4.2 The Trajectory Control Algorithm 
68 
chapter. After making decisions from the FLC, the control output iu , described in 
Chapter 4, will be determined. From the information of ( )1 2 3 4 5 6, , , , ,θ θ θ θ θ θ , we can 
implement the behaviors of the robot arm motion of the semi-humanoid robot by Eq. 
(3.32). The compound position p  is composed of the angles ( )1 2 3 4 5 6, , , , ,θ θ θ θ θ θ  of 
the motors by Eq. (3.32) from Eq. (3.7). 
( )( ) ( )( )( )
( )
( )( ) ( )( )( )
( )
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
4 4 1 2 3 1 3 4 1 2 4 3 1 2
6 5 4 1 2 3 1 3 1 2 4 5 4 1 2 3 1 3 1 2 4
4 4 1 2 3 1 3 4 1 2 4 3 1 2
6 5 2 3 4
       
       
x
y
z
d S C C C C S S C S S C S C C C S S C S C
a C C C C S S a C S S d C S
p d S C S C C C S S S S C S S C C C S S S C
p
a C S C C C S a S S S d S S
p
d S S C C
− − + − +
+ − − +
⎛ ⎞ + − + + +⎜ ⎟ =⎜ ⎟ + + − +⎜ ⎟⎝ ⎠ −( ) ( )( )2 4 5 2 3 4 2 4 4 2 3 4 4 2 4 3 2C S C S C S C C a S C C a C S d C
⎛ ⎞⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟− − − − − +⎜ ⎟⎜ ⎟⎜ ⎟⎝ ⎠  (3.32) 
The matrix 
0
6T  from Eq. (3.3) can be used to infer the position and orientation 
of any link of the robot arm. To plan a collision free path we must ensure that the 
robot arm never reaches a danger boundary that cause it to make contact with an 
obstacle. The set of configurations for which the robot collides with an obstacle is 
referred to as the danger boundary obstacle and it is the front view shown as Fig. 
4.2.22. We need the lateral view of robot arm shown as Fig. 4.2.23, too. 
  
Fig. 4.2.22 The front view of robot arm Fig. 4.2.23 The lateral view of robot arm
Then we want to define the area where the semi-humanoid robot arm can move 
about from Fig. 4.2.22 and Fig. 4.2.23. We design some different motion trajectories 
of the semi-humanoid robot arm, such as running a circular trajectory (Fig. 4.2.25), a 
square trajectory (Fig. 4.2.26), and a catching object task (Fig. 4.2.27) to see the robot 
tracking performance. The gripper has the same moving direction each time and black 
point means the point located at the center point of fully closed fingers. 
70 
of the humanoid robot arm system [60-62]. Reference [63] presents a globally stable 
robust control of a class of nonlinear systems proposed and illustrated the efficiency 
of the fuzzy controller applied to a 6-DOF manipulator. In [64], a 
Takagi-Sugeno-Kang-type fuzzy-neural-network control (T-FNNC) scheme is 
constructed for an n-link robot manipulator to achieve high-precision position 
tracking. 
In this thesis, a fuzzy logic controller is used for position control [65, 66] which 
compensates the errors of rotary angles. In Section 4.2, we will present a fuzzy 
trajectory tracking system with position FLC. In Section 4.3, the position FLC is 
utilized to determine the PWM of control output for tracking desired angular 
displacement. Then in Section 4.4, the membership function of FLC is selected by 
experiments. We compare with two kinds of membership function for effective 
response. Finally, Section 4.5 presents the trajectory tracking behavior of the robot 
arm with the proposed controllers. 
 
2.2 A Fuzzy Trajectory Tracking System 
The control algorithm for the semi-humanoid robot arm has been introduced in 
Chapter 3 as shown in Fig. 4.2.21. We have mapped the procedure into the block 
diagram in Fig. 4.2.13. Given a desired trajectory of the end-effector in space, the 
goal of the control system is to compute the angles at the joints needed to move the 
robot arm through this trajectory. If there are n  joints, we need to compute n  
desired joint positions for each instant of time, where the increment between these 
instants depends on the sampling rate of system. Clearly the sampling rate must be 
fast enough compared to the highest frequencies present to avoid aliasing. The fuzzy 
trajectory tracking system is shown in Fig. 4.2. In Fig. 4.1, the inverse kinematics 
block computes the corresponding desired joint angles ( )kTθ . The desired joint 
angles are used as inputs to the each of the n  actuator encoders that compute the 
actual joint positions at each interval. A desired end-point trajectory in Cartesian 
coordinates, ( ) ( ) ( ) ( )kT kT kT kT⎡ ⎤⎣ ⎦n s a p , is computed and sampled at the 
desired update frequency (
1
T ). The index k  is a counter for sampling times, and T  
is the sampling time. 
72 
and the compositional operators of fuzzy inference. Fig. 4.2 shows a basic 
configuration of fuzzy logic control system containing four principal components: a 
fuzzification interface (FI), a decision-making logic (DML), a defuzzification 
interface (DFI), and a knowledge base (KB). In the following, we briefly describe the 
principles of FLC and the procedure of each component. 
 
Fig. 4.2 Basic configuration of FLC system for position control 
 
2.3.1 Fuzzification Interface 
The functions of the fuzzification interface are to perform the following steps, 
such as measuring the values of the input variables from the data acquisition interface, 
quantizing in order to transform the range of the observed values into the 
corresponding discourse of the language variables, and transforming the input data 
into proper linguistic values which can be regarded as a form of fuzzy set. The actual 
joint angle error and the joint angle error difference are used as fuzzy inputs in the 
position FLC. They are both decomposed into five fuzzy partitions, denoted by 
NB(negative big), NS(negative small), Z(zero), PS(positive small), PB(positive big), 
respectively. The control output CO  is decomposed into 5 fuzzy partitions denoted 
as NB, NS, Z, PS, and PB. Fig. 4.3 shows the membership function shapes of the 
fuzzy subsets. 
 
  
Fig. 4.3 The membership function of inputs Fig. 4.3 The membership function of output 
 
2.3.2 Decision Making Logic (DML) 
DML which can decide the corresponding output after receiving input data and 
be assisted by the knowledge base is the kernel of the fuzzy logic system. The fuzzy 
74 
Rule3: IF ie  is NB and ieΔ  is  Z , THEN CO  is NS 
#  
Table 4.1 The fuzzy rule table for position control 
ie  NB NS Z PS PB 
NB NB NM NS NS Z 
NS NM NS NS Z PS 
Z NS NS Z PS PS 
PS NS Z PS PS PM
PB Z PS PS PM PB 
 
2.3.4 Defuzzification Interface 
The DFI is a mapping from a space of fuzzy control action defined over an 
output universe of discourse into a space of crisp control action. The deffuzification 
strategy utilized in our system is depicted as 
1
1
( )
( )
i i i
i i
n
CO e e
i
crisp n
e e
i
W
u W
μ μ
μ μ
Δ
=
Δ
=
× ×
= =
×
∑
∑
 (4.1) 
where n : the number of fuzzy sets of the control input.  
  COi
W
: the weight of each fuzzy set i . 
  
,
i ie eμ   μΔ : the membership function value of the control input. 
 
2.4 Selection of FLC Membership 
Some experimental results are provides here to demonstrate the effectiveness of 
the chosen membership function of FLC. In this thesis, the triangular form is utilized 
as the membership functions of fuzzy system. There two kinds of membership 
functions are constructed from Fig. 4.3(a) and Fig. 4.4, here we name them 
membership function 1 and 2. We verify one of these membership functions which 
suits our FLC. The parameters are chosen based on human knowledge and through 
some trials.  
76 
people in the daily life [70-71]. In order to complete these missions, we should fuse 
many sensors, process patrol behavior; detect fire accident, and automatic recharge. In 
the patrol behavior, the robot should avoid obstacles, localization, plan the patrol path, 
and human identification. When the fire accident is happened, the robot should detect 
it as fast as possible. In order to work continuously, the surveillance and security robot 
team need to recharge power autonomously. 
To solve the problem of the obstacle avoidance, we employ ultrasonic range 
sensor to detect the obstacle and design a fuzzy logic controller to realize obstacle 
avoidance behavior. The fire sensor can help the robot to detect a fire accident in 
initial stage. To realize the region localization and patrol path planning for the robot, 
we employ the RFID system. By combining the vision and RFID system, the robot 
can differentiate between the strangers and families. 
The robot not only protects people, but also protects their house by fire sensors. 
The fire sensors can detect a radiation disaster, gas disaster, heat disaster, and fire 
accident. The mission of the surveillance and security for human needs to rest over 10 
hours, but the surveillance and security robot just needs to recharge 3 hours. By 
automatic recharge behavior and team work, a surveillance and security robot can 
execute mission continuously. 
2..1 Sensor Fusion and Behavior Strategies 
2.1.1 Overview of the Ultrasonic Sensing System 
There are two functions of the ultrasonic sensing system. One is avoiding 
obstacle, the other is measuring the distance from the obstacle to robot. We should 
consider the moving pattern of the surveillance and security robot when design the 
strategy of the obstacle-avoidance. Different moving pattern will cause different 
strategy.  
In order to move faster for the surveillance and security robot team, we use the 
omni-directional wheel as the wheel of the surveillance and security robot team. Since 
the omni-directional wheel moving pattern is different with traditional wheel of robot, 
and the ultrasonic avoiding obstacle strategy also different. In this section, we present 
a novel avoiding obstacle strategy. The moving pattern of traditional wheel and 
omni-directional wheel of robot are depicted in Figure 5.1.1. 
78 
will cause serious noise. The serious noise is called crosstalk noise. The surveillance 
and security robot will make a mistake in distance due to crosstalk noises. In general, 
the crosstalk noise is generated by a nearby object in Figure 5.1.3. 
 
Figure 5.1.3 The crosstalk noise 
The ultrasonic sensor detects objects by emitting a short ultrasonic burst and then 
listening to the echo. Under control of a host microcontroller (trigger pulse), this 
sensor emits a short 40 kHz ultrasonic burst. This burst travels through the air about 
1130 feet per second, hits an object and it bounces back to the ultrasonic sensor. The 
ultrasonic sensor provides an output pulse to the host that will terminate when the 
echo is detected therefore, the width of this pulse corresponds to the distance to the 
target. 
According to this theory of operation, we can control the trigger pulse to change 
ultrasonic sensor firing intervals. Crosstalk noises will be reduced by different firing 
intervals. Figure 5.1.4 shows the timing of the ultrasonic sensor. The maximal period 
of an ultrasonic sensor is tout + tHOLDOFF + tIN-MAX. The maximal period of an 
ultrasonic sensor is about 18.5 ms. We wait for additional 1.5ms for safety, to allow 
the ultrasound energy in the surrounding to decline further. Thus we design a basic 
measurement period of 20 ms. We divide the sensor-firing intervals into two periods. 
Finally a cumulative measurement period is 40 ms. The firing order is clear from 
Figure 5.1.5. 
80 
(2) L1<L2 or R1>R2, which represent the robot need to turn right until L1=L2= 
Dis_L_wall or R1=R2=Dis_R_wall. 
If:  
(a) Dis_R_wall <30cm & Dis_L_wall >30cm 
If: There are obstacles in front of the robot which move shift-right until Dis_L_wall 
>30cm. 
Else: Keep going straight. 
(b) Dis_R_wall >30cm & Dis_L_wall <30cm 
If: There are obstacles in front of the robot which move shift-left until Dis_R_wall 
>30cm 
Else: Keep goes straight 
(c) Dis_R_wall <30cm & Dis_L_wall <30cm 
If: Dis_L_wall >30cm first, the robot moves shift-left until Dis_R_wall >90cm. 
Else If: Dis_R_wall >30cm first, the robot moves shift-right until Dis_L_wall >90cm. 
Else: Keep goes straight. 
(d) Dis_R_wall >30cm & Dis_L_wall >30cm 
If: There are obstacles in front of the robot which uses the fuzzy obstacle-avoiding 
rule. 
Else: Keep goes straight. 
The flow chart of the wall-following mode is depicted in Figure 5.1.7 
 
Figure 5.1.6 The definition of measurement variables for ultrasonic sensor in 
wall-following mode 
 
82 
 
Figure 5.1.8 Basic configuration of fuzzy logic controller 
Pre-Processor (PP) 
The pre-processor selects the nearest obstacle. At first, we divide the region in 
front of the robot into five regions and define the weight of the region as in Figure 
5.1.9. Then we divide the distance into five stages and define the weight of the stage 
as in Figure 5.1.10. The robot is defined to deal with only two obstacles at the same 
time. We define the critical rating (CR) by the following equations: 
( _ ) ( _ )CR region weight stage weight= ×  (5.1) 
 
Figure 5.1.9 The weight of the region 
84 
cm. The length of each stage is the half of the diagonal length. There are 5 stages in 
Figure 5.1.12. Stages are denoted by VS(very small), S(small), M(middle), B(big), 
VB(very big). Figure 5.1.13 shows the angle definition of the robot. Figure 5.1.14 
shows the membership function shapes of the fuzzy subsets adopted in this thesis. 
 
 
Figure 5.1.11 The region in front of the robot 
 
 
Figure 5.1.12 Five stages between robot and obstacle 
 
 
Figure 5.1.13 The angle definition of the surveillance and security robot 
86 
Defuzzification Interface 
A useful defuzzification technique must first add the result of the rules together 
in some way. The max criterion, the mean of maximum (MOM), and the center of the 
area (COA) are the commonly used methods to implement the defuzzification 
interface. In this thesis, the weight average method (WAM) is applied to the 
surveillance and security robot. From the strategies mentioned above, we will develop 
the fuzzy logic control with the simplified fuzzy singleton by using Nios embedded 
system. The fuzzification strategy is the fuzzy number, the inference method is 
simplified fuzzy singleton method, and the defuzzification strategy is the weight 
average method. 
The method yields  
1 1 2 2
1 2
.....
.....
n n
crisp
n
w C w C w CC
w w w
+ + += + + +           (5.2) 
iw  - the antecedent fitness degree of each rule 
iC  - the corresponding consequent output values   1 ~i n=  
In above discussions, we explore the components of basic architecture of the fuzzy 
logic control system. 
 
2.2 Overview of the Fire Sensors System 
2.2.1The Causes of the Fire Accident 
According to the fire accident statistics of the National Fire Agency, Ministry of 
the Interior in 2006, the number of dead people in fire accidents over 4,000 [74]. 
Reach up to fifty percent of the reason of the fire accident is preventable, and 
therefore the prevention of fire accident is very important. 
As we know, prevention is better than cure. In the initial stage of fire accident, it can 
be put it out easily. In order to detect fire accident early, we establish a fire sensor 
system on surveillance and security robot team. In addition to detect fire accident, 
surveillance and security robot transfers fire alert message to web-control center. 
In order to detect the fire accident, we should know the products of the inflammation. 
Flame: 
The heat energy is transmitted by the radiation. In usual, people will be hurt by 
directly burn or indirectly radiant temperature. Our skins will be hurt in one second by 
the temperature over 66ºC or the radiant heat over 3W/cm2. People will be killed by 
the temperature of the fire and the radiant temperature. 
Heat: 
The temperature of the area of the fire accident will rise due to the heat will be 
transmitted by all kinds of ways. Although people can live in the 140 ºC temporarily, 
people will lose his breath and act slowly in the temperature over 66 ºC. 
88 
 
Figure 5.2.1 UV TRON’s spectral response and various light sources 
 
 
Figure 5.2.2 Angular sensitivity (directivity) 
 
Temperature Sensor Circuit: 
The AD590 is a 2-terminal integrated circuit temperature transducer that 
produces an output current proportional to absolute temperature. For supply voltages 
between 4 V and 30 V, the device acts as a high impedance, constant current regulator 
passing 1 μA/K. Laser trimming of the chip’s thin-film resistors is used to calibrate 
the device to 298.2μA, output at 298.2 K (25°C). The AD590 should be used in any 
temperature-sensing application below 150 ° C in which conventional electrical 
temperature sensors are currently employed. 
Gas Sensor Circuit: 
The TG-135 applies for carbon dioxide, ethanol and smoke. The gas sensor is 
n-type semiconductor (SnO2). When a gas molecule close to SnO2, there is an electron 
will be produced. The specification of gas sensor is displayed in Table 5.2. 
90 
2.3 Overview of the RFID System 
2.3.1 The Communication of RFID system 
We use the AWID RFID Common Serial Protocol (RCSP) which defines the 
serial (RS-232) communication protocol to be used in communications among and 
between MPR (Multi Protocol Reader) RFID devices and other HOST systems and 
equipment. A HOST system for purposes of this specification is a personal computer. 
The AWID MPR reads tags of the type are ISO-18000-6 Type B (U-code, HSL). The 
device will be connected the host via RS-232. It will be a three-wire connection (TX, 
RX and GD) with 9600, 8, N, 1 as the default setting. 
Table 5.3 The command of read multiple tag IDs 
IDs (0x0E) 
FROM TO MSG Example RESPONSE Example 
HOST READER 12 11 0E 00 00 00 
00 00 00 00 00 00 
00 00 2B F0 C3 EF
00 or FF 
0E 11 0E 01 A8 E5 8F 80 B8 40 09 (ID Number) 
XX XX (CRC)  
 
Mainly we use the command in Table 5.3 which provides the ability to read 
multiple tag IDs presents in the reading field. 
 
2.3.2 The Identification of Family Members 
We use RFID frequently in our life, but it always passive. RFID host is set up in 
fixed place and wait for people pass through there. Now we make the way positive. 
Surveillance and security robot team equipped RFID system. The robot team can 
identify stranger positively when the robot is in patrol. In order to simplify operation, 
we use Borland C++ Builder to design graphical user interface. As Figure 5.2.4, we 
click button to memory the ID number of family members. When the RFID system 
reads the ID number of family members, it will show the corresponding greeting. For 
example, the greeting of father is “Hi daddy.” 
 
Figure 5.2.4 The graphical user interface of RFID setup 
92 
 
Figure 5.2.6 The strategy of the behavior 
2.3.5 The Structure of Building 
In real world, the structure of the building has many styles. Our strategy can 
adapt to many buildings environment. According the environment of the building, we 
use the different tag and design the different patrol path. 
 The square cyclic structure is the easiest. Square cyclic structure only needs 
transshipment points to finish the patrol. The robot will change direction when the 
robot in the transshipment point. The robot changes direction in four corners 
continuously. The navigational map of the square cyclic structure is depicted in Figure 
5.2.7. 
 
Figure 5.2.7 The navigational map of the square cyclic structure 
Many buildings like to use circular structure, it make people feel soft. Compared 
94 
 
Figure 5.2.9 The navigational map of the U-shape structure 
The structure of the figure eight is composed of two U-shape and fork without 
impasse. The key point of this structure is the fork. The fork has two forward 
directions which makes the patrol path complicated. First, the robot start patrol like a 
circular cyclic structure. The robot will decide the right direction to change when the 
robot in the fork point. Due to the different reference tags, we can design many route 
charts. The navigational map of the figure eight is shown in Figure 5.2.10 
 
Figure 5.2.10 The navigational map of the figure eight 
96 
define the now_angle is the angle data from electric compass. The desired_angle is 
the angle that we need to turn. The equation error_angle is written as follows: 
error_angle = now_angle - desired_angle   
The angle information form the electric compass will be distorted easily due to 
electromagnetic interference. In order to avoid this state, we use a linear filter to 
reduce the noise. 
 
2.4.2 Infrared Sensor Function 
In the automatic recharge process, the surveillance and security robot need 
infrared sensors and ultrasonic sensors at the same time. The path and angle are 
important for the surveillance and security robot in the automatic recharge process. 
We select the long range infrared to make sure of the robot and the recharge station in 
alignment. The ultrasonic sensor can obtain the distance information between the 
robot and the power station. We use the distance information to control the velocity 
and angle of the robot. 
The infrared sensors and ultrasonic sensors are equipped in rear of the surveillance 
and security robot due to the recharge system is equipped in rear of the robot.  
The last mile of the automatic recharge process is a key point for overall process. The 
path of the last mile demand very exactly. We use two ultrasonic sensors to keep the 
robot in right pose. The distance of two ultrasonic sensors will be equaled as in Figure 
5.2.12. The L-ultrasonic and R-ultrasonic will balance the distance to each other. 
Among two ultrasonic is infrared sensor. If infrared sensor identifies the charge 
station, the robot will move backward. The robot will revise pose when it is in the 
wrong pose. The robot will keep moving backward until combine with the charge 
station. 
 
Figure 5.2.12 The automatic recharge system 
 
2.5 Periodic Patrol Behavior 
The regular behavior for surveillance and security robot team is periodic patrol 
behavior. The periodic patrol behavior consists of human identification, change 
direction, avoid obstacle, detect accident, and patrol diary. Every task is completed by 
the cooperation of the different units. Human identification can recognize stranger. 
98 
Detect Accident 
The most important thing is fire accident detecting for the surveillance and 
security robot team. The fire accident detecting is top priority in the Periodic Patrol 
Behavior. Flame sensor detects the radiation of the fire accident. Gas sensor detects 
the smoke of the fire accident. Temperature sensor detects the heat of the fire accident. 
When fire accident or disaster is affirmed, the robot will send a fire alert message to 
local server by wireless communication. Local server receive the fire alert message 
will transmit the command to every robot.  
Human Identification  
As the saying goes “a thief in the family is difficult to detect”. The major 
function of the human identification is to detect a thief in the family or company. 
Human identification consists of vision unit and RFID unit. The robot will search for 
a face in vision unit. The RFID unit will read the tag of the identification and then 
compare the tag number to database. When finding the right data of tag number, the 
robot will recognize identification. When identification is affirmed, the robot will 
record it on the patrol diary. If the burglary is happened, we can check the patrol diary 
to find the thief. 
Change Direction 
In general situation, the robot does not need to change direction. The robot 
always keeps going straight. The robot needs to change direction when it reads the tag 
of the landmark. We use vision unit and RFID unit to locate the region. The RFID unit 
can read tag number in 2m range. The vision unit recognizes image mark stably needs 
to close within least 1m. Due to the sensible distance of the vision unit is smaller than 
the sensible distance of the RFID unit. First, we employ RFID unit to locate the 
region. Then, the robot will recognize the image mark of the environment. The RFID 
unit will read the tag of the landmark and then compare the tag number to landmark 
database. When discovering the match data of tag number, the robot will locate region. 
When location is decided, the robot will search the image mark of the environment 
actively. The vision unit will compare the image mark of region to environment 
database. When the region is affirmed, the robot will change direction. The data of the 
electric compass is important to change direction.  
2.6 Emergency Action 
When the fire accident is happened, the local server will receive the fire alert 
message. According to the fire alert message, the local server will plan the path and 
assign the assistant robot. The local server will transmit the command to every robot. 
According to the command, the robot will execute the corresponding action. We call 
the corresponding action is emergency action. We divide the robot team into two types. 
We call the robot that detect the fire accident is the detector and the other robots are 
100 
 
Fig. 4.1 Experiment results of the face detection and human identification 
 
1.2 Experimental Results of the TCA 
In this section, we present some experiment results. The camera, DCR-PC330 
NTSC, manufactured by SONY is fixed on the 2.5 meter height ceiling. The images 
are captured by the dynamic video recorded. The form of the image is 352×240 pixels. 
Figs. 4.2, 4.3, and 4.4 show the circular trajectory where the SSR moved with radius 
0.75 m by P controller as Kp=1, 0.8, and 0.4, respectively. The field is 2.4×2.2 m2. 
The initial velocity and final velocity is zero. The SSR is attached to a small red piece 
of paper to represent the heading angle. Under the ideal conditions, the heading angle 
will be always constant. Due to the sliding of the wheel, the SSR will diverge from 
the trajectory. Figs. 4.5 and 4.6 show the circular trajectory where the SSR moved 
with radius 0.75 m by FFLC. From the experiments, the FFLC proposed is better than 
P controller. 
  
Fig. 4.2 The circular trajectory compensated by P controller (Kp=1) 
102 
as (50,-30) and the SSR tracks to the origin (0,0) in Fig. 4.7. Later, it will keep 
tracking in the clockwise (CW) direction in Fig. 4.7. Fig. 4.7 shows the counter 
clockwise (CCW) trajectory. 
  
Fig. 4.7 The square trajectory compensated by P controller (Kp=0.4) 
In Fig. 4.8, the SSR is tracking the square trajectory in CW, and Fig. 4.8 is in 
CCW. Comparing Fig. 4.7 and Fig. 4.8, the tracking trajectory in Fig. 4.7 does not 
match the size of 1.5×1.5 m2 because the SSR does not track the velocity. The 
experiments demonstrate the SSR can track the desired trajectory with FFLC more 
accurately than that with P controller. 
  
Fig. 4.8 The square trajectory compensated by FFLC – CW/ CCW 
Figs. 4.9 and 4.10 show the diamond trajectory with the diagonal 2.121m. In Fig. 
4.9, the SSR is tracking the diamond trajectory in CW, and Fig. 4.9 is in CCW. In Fig. 
4.10, the SSR is tracking the diamond trajectory in CW, and Fig. 4.10 is in CCW. 
When tracking the diamond trajectory, the SSR is driven by two wheels not four 
wheels. Therefore, the velocity is slower and affects the experimental results. In Fig. 
4.9, the heading angle of the SSR is bias, so the real trajectory does not match the 
shape of diamond. From the experiments, the FFLC illustrates better performance. 
104 
wavelength 6 cm. In Fig. 4.13, the initial position is set at ( )0 50 0 and the robot arm 
tracks the desired sine wave trajectory along y  axis. Later, it will keep tracking the 
sine wave trajectory in the first period in Fig. 4.13, and Fig. 4.13 is in the second one. 
The sine wave trajectory finished at (0,38,0) and then the robot arm went back to 
initial position. The function of the trajectory is ( ) ( )5sin(2 50 / 6)f x xπ= ⋅ − + . 
  
Fig. 4.11 The circular trajectory compensated by FLC 
  
Fig. 4.12 The square trajectory compensated by FLC 
  
Fig. 4.13 The sine wave trajectory compensated by FLC 
 
 
 
 
106 
108 
[10] K. Okada, M. Kojima, Y. Sagawa, T. Ichino, K. Sato and M. Inaba, “Vision 
based behavior verification system of humanoid robot for daily environment 
tasks,” in Proc. the 2006 IEEE Int. Conf. on Humanoid Robots, pp. 7-12, 2006. 
[11] K. Kaneko, F. Kanehiro, S. Kajita, H. Hirukawa, T. Kawasaki, M. Hirata, K. 
Akachi and T. Isozumi, “Humanoid robot HRP-2,” in Proc. the 2004 IEEE Int. 
Conf. on Robotics & Automation, pp. 1083-1090, LA, 2004. 
[12] M. Hasanuzzaman, T. Zhang, V. Ampornaramveth, H. Gotoda, Y. Shirai and H. 
Ueno, “Knowledge-based person-centric human-robot interaction using facial 
and hand gestures,” in Proc. the 2004 IEEE Int. Conf. on Systems, Man, and 
Cybernetics, Vol. 3, pp. 2121-2127, 2004. 
[13] J. Nakanishi, R. Cory, M. Mistry, J. Peters and S. Schaal, “Comparative 
experiments on task space control with redundancy resolution,” in Proc. the 
2005 IEEE Int. Conf. on Systems, Man, and Cybernetics, pp. 3901-3908, 2005. 
[14] S. U. Ryu, C. J. Kim and K. H. Coi, “Multi-arm path generation method for 
humanoid robots,” in Proc. the 2007 IEEE Int. Conf. on Robot & Human 
Interactive Communication, pp. 224-227, Ro-Man, 2007. 
[15] H. Martínez-Alfaro and S. Gómez-García, “Mobile robot path planning and 
tracking using simulated annealing and fuzzy logic control,” Expert on Systems 
with Applications, Vol. 15, No.3, pp. 421-429, 1998. 
[16] P. Nanua, K. J. Waldron and V. Murthy, “Direct kinematic solution of a stewart 
platform,” IEEE Trans. on Robotics Automation, Vol. 6, No. 4, pp. 438-444, 
1990. 
[17] R. Ricard and C. Gosselin, “On the development of hybrid planar 
manipulators,” in Proc. the 1993 IEEE Int. Conf. on Circuits and Systems, Vol. 
1, pp. 398-401, 1993. 
[18] K. Erbatur, O. Kaynak and I. Rudas, “Fuzzy identifier based inverse dynamics 
control for a 3-DOF articulated manipulator,” in Proc. the 1997 IEEE Int. Conf. 
on Industrial Electronics, Control, and Instrumentation, Vol. 3, pp. 1052-1056, 
1997. 
[18] Robotics Research Corporatio, http://www.robotics-research.com/. 
[20] Beijing Research Institute of Automation for Machinery Industry, 
http://www.robotschina.com/robotcenter/wuliu/enwuliu.htm. 
[21] Robot L. Norton, Machine design: an integrated approach, Prentice-Hall, 
1998. 
[22] AMX, http://www.amx.com.tw/. 
[23] N. Barnes and Z. Q. Liu, “Fuzzy Control for Active Perceptual Docking,” in 
Proceedings of the 10th IEEE International Conference on Fuzzy Systems, pp. 
110 
Robotics and Automation, Vol. 4, pp. 2686-2691, 1999.  
[35] V. Muñoz, A. Ollero, M. Prado, and A. Simón, “Mobile robot trajectory 
planning with dynamics and kinematics constraints,” Proc. of the IEEE Int. 
Conf. on Robotics and Automation, Vol. 4, pp. 2802-2807, May, 1994. 
[36] N. Faiz, and S. K. Agrawal, “Trajectory planning of robots with dynamics and 
inequalities,” Proc. of the 2000 IEEE Int. Conf. on Robotics and Automation, 
Vol. 4, pp. 3976-3982, April, 2000. 
[37] K. L. Moore, and N. S. Flann, “A six-wheeled omni-directional autonomous 
mobile robot,” Control Systems Magazine, IEEE, Vol. 20, No. 6, pp. 53-66, 
2000. 
[38] T. Kalmár-Nagy, R. D’Andrea, and P. Ganguly, “Near-optimal dynamic 
trajectory generation and control of an omnidirectional vehicle,” IEEE Trans. on 
Robotics and Autonomous Systems, Vol. 46, No. 1, pp. 47-64, 2004. 
[39] M. Saito, and T. Tsumura, “Collision Avoidance Among Multiple Mobile 
Robots -A Local Approach Based on Non-linear Programming,” Trans. of the 
Institute of Systems, Control and Information Engineers, Vol. 3, No. 8, pp. 
252-260, Japan, 1990. 
[40] K. Watanabe, Y. Shiraishi, S. G. Tzafestas, J. Tang and T. Fukuda, “Feedback 
Control of an Omnidirectional Autonomous Platform for Mobile Service 
Robots,” J. Intelligent and Robotic Systems, Vol. 22, No. 3, pp.315-330, 1998. 
[41] R. L. Williams II, B. E. Carter, P. Gallina, and G. Rosati, “Dynamic Model 
with Slip for Wheeled Omni-Directional Robots,” IEEE Trans. on Robotics and 
Automation, Vol. 18, No. 3, pp. 285-293, March, 2002. 
[42] T. Tsuji, K. Chigusa and M. Kaneko, “Trajectory generation of moving robots 
using active deformation of artificial potential fields,” J. of Japan Society of 
Mechanical Engineers, Vol. 62, No. 597, pp. 1905-1911, Japan, 1996. 
[43] K. Watanabe, “Control of an Omnidirectional Mobile Robot,” Proc. of the 1998 
IEEE 2nd Int. Conf. on Knowledge-Based Intelligent Electronic Systems, pp. 
51-60, Australia, 1998. 
[44] C. C. Shing, P. L. Hsu, and S. S. Yeh, “T-S Fuzzy Path Controller Design for 
the Omnidirectional Mobile Robot,” in Proc. 32nd Annu. Conf. Industrial 
Electronics, pp. 4142-4147, 2006. 
[45] H. O. Wang, K. Tanaka, and M. F. Griffin, “An approach to fuzzy control of 
nonlinear systems: stability and design issues,” IEEE Trans. on Fuzzy Systems, 
Vol. 4, No. 1, pp. 14-23, 1996. 
[46] K. Tanaka, T. Ikeda, and H. O. Wang, “An LMI approach to fuzzy controller 
designs based on relaxed stability conditions,” in Proc. 6th Int. Fuzzy Systems 
Conf., pp. 171-179, Spain, 1997. 
112 
robot manipulator using inversion of its neural emulator,” IEEE Trans. on 
Neural Networks, Vol. 7, No. 6, pp. 1401-1414, 1996. 
[62] A. Visioli and G. Legnani, “On the trajectory tracking control of industrial 
SCARA robot manipulators,” IEEE Trans. on Industrial Electronics, Vol. 49, 
No. 1, pp. 224-232, 2002. 
[63] M. Hamerlain, R. Ouiguini, Z. Siguerdidjane and S. Saadaoui, “Robust control 
applied to robot arm,” in Proc. the 2005 IEEE Int. Conf. on Intelligent Control, 
pp. 149-154, Portugal, 1997. 
[64] R. J. Wai and P. C. Chen, “Intelligent tracking control for robot manipulator 
including actuator dynamics via TSK-Type fuzzy neural network,” IEEE Trans. 
on Fuzzy Systems, Vol. 12, No. 4, pp. 552-559, 2004. 
[65] P. J. C. Branco and J. A. Dente, “On using fuzzy logic to integrate learning 
mechanisms in an electro-hydraulic system—part II: actuator’s position 
control,” IEEE Trans. on Systems, Man, and Cybernetics, Vol. 30, No. 3, pp. 
317-328, 2000. 
[66] R. J. Wai and M. C. Lee, “Intelligent optimal control of single-link flexible 
robot arm,” IEEE Trans. on Industrial Electronics, Vol. 51, No. 1, pp. 201-220, 
2004. 
[67] L. A. Zadeh, “Fuzzy algorithm,” Information Control, Vol. 12, pp. 94-102, 
1968. 
[68] L. A. Zadeh, “Fuzzy sets,” Information Control, Vol. 8, pp. 338-353, 1965. 
[69] L. A. Zadeh, “Outline of a new approach to the analysis of complex systems 
and decision processes,” IEEE Trans. on Systems, Man and Cybernetics, Vol. 3, 
pp. 28-44, 1973. 
[70] R.C. Luo and K.L. Su, “Autonomous fire-detection system using adaptive 
sensory fusion for intelligent security robot,”, IEEE/ASME Transactions on 
Mechatronics, Volume 12, Issue 3, pp. 274 - 281, June 2007. 
[71] D. Zeng, C. Xie and X. Li, “Design and implementation of a security and patrol 
robot system,” in Proc. IEEE International Conference on Mechatronics and 
Automation, Volume 4, pp. 1745 - 1749, 2005. 
[72] L. A. Zadeh, “Fuzzy Algorithm,” Information Control, Volume 12, pp. 94 - 102, 
1968. 
[73] L. A. Zadeh, “Fuzzy Sets,” Information Control, Volume 8, pp. 338 - 353, 1965. 
[74] http://www.nfa.goc.tw 
114 
七、可供推廣之研發成果資料表 
可供推廣之研發成果資料表 
■ 可申請專利  ■ 可技術移轉                           日期：97 年 10 月 30 日 
國科會補助計畫 
計畫名稱： 
半人形機器人之設計、研製及應用(2/2) 
計畫編號：NSC 95-2221-E-006 -363 –MY2 
執行期限：自民國 95 年 08 月 01 日至民國 97 年 07 月 31 日 
主 持 人：李祖聖 特聘教授 國立成功大學電機工程學系 
學門領域：控制學門 
技術/創作名稱 全方位自主式半人形機器人之設計、研製及應用 
發明人/創作人 李祖聖 
近年來智慧型機器人之開發已蔚為風尚，而系統整合則是機器人研
發的重要課題，如何將各種領域技術整合到機器人上，已成為國際
學術研究之重點。本計畫的目標係將機電整合、運動控制、視覺伺
服、RFID、IC 設計及智慧型控制等領域之技術加以整合，並實現
在半人形導覽保全機器人上。 
技術說明 Design and implementation of intelligent robots have attracted many academics and industries for decades. In fact, system integration is a 
key issue in setting up a robot. The main theme of this project is to 
design and realize a fully autonomous semi-humanoid robot (SHR) 
with guide and security capabilities. To accomplish this purpose, we 
should integrate mechanism, motion control, visual sevoring, voice 
recognition, RFID, IC design, and intelligent control technologies 
 
可利用之產業 
及 
可開發之產品 
導覽、娛樂、居家保全、防災機器人之產業與產品。 
技術特點 
本計畫之技術特點包含機器人機構設計與實現、機器人控制設計與
實現、無線通訊系統之建構、即時影像處理、語音辨識、RFID 定
位、電子電路設計與開發…等。 
