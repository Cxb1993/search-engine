2 
行政院國家科學委員會專題研究計畫成果報告 
應用虛擬因子於小樣本學習—以生產資訊系統之試產為例 
Deriving Virtual Attributes for Small Data Set Problems 
中文摘要 
小樣本問題一直在於最終無法獲得穩健的推論與結論，但此類問題卻常常存在於現實生
活中，例如新種類疾病的診斷、生產線產品試產。本質上，不管是樣本數的不足或是預測變
數的不充份都將造成無法建立可靠的模式來擷取可用的管理知識。 
 
面臨這類的小樣本問題，本研究首先應用極值定理來估算出各屬性的定義域範圍，接著
利用產生虛擬樣本的技術來使現有的小樣本資料平滑化，藉以填補稀少資料之間的資訊空
缺，其次再以正弦函數作為發展虛擬屬性的工具，建立更有效的預測屬性，從而建構一個可
行的知識模型。實務驗證也針對傳統的類神經網路以及大量趨勢擴散法—本研究所改良的方
法—進行比較，結果顯示兩者的預測誤差有顯著的差異，也就利用本研究所改良的方法將可
使小樣本資料有效降低預測誤差。
 
關鍵詞：小樣本、極值定理、正弦函數 
Abstract 
Small data set problems often make it difficult for researchers to obtain robust conclusions. 
However, such problems often arise, such as in the prediction of a terrorist attacks, new disease 
diagnoses, and pilot product management knowledge. Essentially, in such cases, either insufficient 
samples or prediction attributes mean it is statistically hard to build reliable models for extracting 
useful knowledge. 
Faced with a specific small data set problem, this research first applies extreme value theory to 
estimate the domain range of an attribute, and then uses a virtual sample generation technique to fill 
the information gaps in the sparse data. Further, the sinusoid combination of real attributes is 
developed to construct a virtual attribute, which is used to find a more effective attribute to build the 
knowledge model. A real data set is employed to validate the performance of the proposed method 
by comparing it with the Artificial Neural Network and Mega-Trend-Diffusion approaches. The 
results indicate that the prediction error rate can be significantly decreased by applying the proposed 
method to very small data sets. 
 
Keywords: Small data set; Extreme value theory; Sinusoid combination 
4 
2. Literature Review 
Small data set problems are not unusual in the real world; but they are not easily solved by 
traditional optimal or heuristic procedures. In the past decade, more and more small data set 
problems of different types have been explored by researchers. 
Ivănescu et al. [5] proposed a procedure to solve the limited data problem for batch process 
industries. They assumed that the job arrival moments obey a Poisson arrival process and utilized a 
bootstrap procedure to generate another 250 bootstrap jobs. According to their results, the procedure 
they proposed could improve the regression modeling performance. 
Lanouette et al. [6] utilized neural networks to evaluate various modeling strategies when only 
a small experimental data set is available. Their research mainly focused on selecting the optimal 
process model for a wood pulp factory, as because the related experiments are expensive and time 
consuming, only a few samples can be obtained. They concluded that the stacked neural networks, 
proposed by Wolpert in 1992 [7], are the most effective approach. 
Oniśko et al. [8] proposed a method using Noisy-OR gates to learn Bayesian network 
parameters for small data sets. They tested their method on a model for diagnosing liver disorders, 
called HEPAR II. The parameters of HEPAR II were extracted from a real and small data set of 
patient records, and this technique successfully improved the diagnostic accuracy. 
Rhodes and Keefe [9] analyzed data from the Revolutionary Organization November 17 (N17), 
a terrorist organization, to weigh the strengths and weaknesses of the Bayesian approach in 
inferring network structure. They proved that this approach can successfully infer the topology of 
terrorist networks. 
Real world problems are generally complicated and non-linear. Although ANNs have been 
widely utilized to extract management knowledge from acquired data, it requires a sufficient 
number of training samples. With regard to sample sufficiency, computational learning theory [10] 
has been used to examine machine learning problems concerning sample size, such as how many 
training examples are needed to lead to a successful learning, how many reasonable computations 
are needed for successful learning, and what is the estimated misclassification rate in learning. 
Anthony and Biggs [10] developed the probably approximately correct (PAC) concept to identify 
classes of hypotheses that can/cannot be learned from a polynomial number of training examples 
with a reasonable amount of computations. Furthermore, Vapnik [11] considered a sample size 
small if the ratio of the number of training samples to the Vapnik-Chervonenkis dimensions (VC 
dimensions) of a learning machine function is less than 20. 
In our experience, a sufficient training sample set is defined as a sample set which provides 
enough information for a learning method to obtain stable learning accuracy. Thus, the sample size 
is considered closely related to the stability of the system environment, method used, and sample 
dimensions. 
Adding some artificial data to the system is one effective approach to increase learning 
accuracy. The Virtual Data Generation approach was used in Pattern Recognition, in that Niyogi et 
al. [12] used prior knowledge obtained from the given small training set to create virtual examples 
to improve the recognition ability. In their method, from a given 3-D view of an object new views 
6 
3. Proposed Method 
In this section, the proposed method is introduced, including the modified extreme value 
theory and the sinusoid combination of real attributes. In order to explore the effectiveness of the 
proposed method when compared with that in Li et al. [4], this research adopts the hidden 
assumption of the earlier work, that the input attributes are independent. The proposed procedure 
has the following steps: 
Step 1.Utilize the modified extreme value theory to calculate the estimated domain for all 
attributes, including inputs and output. 
Step 2.Randomly generate a value for each attribute within the estimated domain obtained in 
Step 1. 
Step 3.Calculate the sinusoid combination of input attributes to find the virtual attribute value. 
Step 4.Repeat Steps 2 and 3 one hundred times to obtain 100 virtual samples. 
Step 5.Perform the sensitivity analysis of training sample size by setting it as 5, 10, 15, 20, 25 
and 30. 
Step 6.Calculate the average error rate of each training sample size. 
The methods employed in the proposed procedure are explained as follows: 
3.1 The modified extreme value theory 
Traditionally, ANN has been commonly used to derive management knowledge from real 
empirical data. However, a considerable number of samples is needed in the learning process to 
establish a robust network. In practice, collecting data from pilot runs is expensive and 
time-consuming, and consequently the data available is often insufficient for training an ANN. 
Insufficient data usually produces imprecise system information and presents two major problems: 
what is the population domain and how can the data gaps be filled. In order to overcome these two 
problems, this research first adopts the extreme value theory to estimate the input data domains 
systematically, and then virtual samples are produced to fill the data gaps. 
The extreme value theory is used to infer the distribution of extreme values. Jenkinson[19] 
derived the generalized extreme value distribution (GEV) as follows: 
8 
Step 2. Transfer the sample values into iR  and iL  for the samples greater / smaller than 
  2maxmin  , respectively. 
Using the following equations: 
 
2 2
2 2
i i i
i i i
min max min max
R x for x
min max min max
L x for x
 
  
 
  
 (6) 
Let R  and Rs  be the average and standard deviation of iR , andlet L  and Ls  be 
the average and standard deviation of iL . 
Step 3.  
Utilizing L , Ls  and Equation (5), the left end a  is calculated as: 
 
  20
2
6
0.450041 ln( ln(10 ))
L
L L
min max
a E
s L s



 
      
 (7) 
And utilizing R , Rs  and Equation (5), the right end b  is calculated as: 
 
  20
2
6
0.450041 ln( ln(10 ))
R
R R
min max
b E
s R s



 
      
 (8) 
3.2 Production of the virtual attribute 
In this research, it is assumed that there is an unknown critical attribute affecting the output 
and it can be theoretically found by combining all the input attributes obtained.However, under this 
assumption, this goal is not able to be achieved by a linear combination of input attributes, because 
the virtual attribute can be replaced by other inputs, such as using the Principal Component Analysis 
technique in the process of dimensional abstraction. That is to say, there is no further information 
that can be obtained through linear combinations. Obviously, the unknown critical attribute must be 
a function resulting from a nonlinear combination of input attributes. Based on the idea of the 
Fourier series, a function can be decomposed into a set of sines and cosines. In other words, a 
sinusoid combining all the input attributes can construct the virtual attribute. 
However, the approach in this research is unlike the regular Fourier series, which has a 
function that can be decomposed or presented as a set of sines and cosines. In contrast, in this 
research one can only use the input and output attributes to extend the information. Therefore, the 
first step to decide the virtual attribute value is to figure out the corresponding angles of input 
attribute values, and then use a sinusoid combination of input attributes to calculate the virtual 
10 
 
Figure 2:The distribution of virtual attribute values 
3.3 Production of virtual samples 
Once the domains have been estimated and the virtual attribute value has been calculated, the 
information gaps are ready to be filled. First, a random set of values is generated for each input and 
output attribute,  1 2, , , ,nx x x y   L . Second, Equation (10) is used to calculate the virtual attribute 
value, v . These randomly produced values, the virtual attribute value and the random output, 
make a complete virtual sample,  1 2, , , , ,nx x x v y    L . Though it requires further research, 
according to our limited experience, 100 virtual samples are sufficient for training most of the 
ANNs. 
Finally, the set of data  1 2, , , , ,nx x x v yL  and  1 2, , , , ,nx x x v y    L  will be used to train the 
ANN and compare the results with that of the original data 1 2, , , ,nx x x yL . 
12 
4.1Evaluate virtual attribute 
In order to evaluate the effectiveness of the virtual attribute compared with the other attributes, 
this research conducted an special experiment in which the correlation coefficients between inputs 
and output were first calculated. The input attribute with the highest correlation coefficient (the 
most informative attribute) was then deleted. Assume the k th attribute is deleted and the other 
input attributes,  1 2 1 1, , , , , ,k k nx x x x x L L ,are used to figure out the value of virtual attribute, 
vusing Equation (10). In addition, 100 virtual samples,  1 2 1 1, , , , , , , ,k k nx x x x x v y       L L , are 
also produced. The data  1 2 1 1, , , , , , ,k k nx x x x x y L L ,  1 2 1 1, , , , , , , ,k k nx x x x x v y L L , and 
 1 2 1 1, , , , , , , ,k k nx x x x x v y       L L are used to train and test the performance of the ANN. The 
results are summarized and compared with those in Li et al. [4] in Table 2 and Figure4. 
Table 2:The evaluation of the virtual attribute compared with Li et al. [4] 
 Scales of training data sets 5 10 15 20 25 30 
Average 
error rate 
ANN (13 Real attributes) 8.8688% 7.0899% 6.7002% 7.0095% 6.6064% 6.0171% 
Li et al. [4] 7.1977% 6.7202% 6.6342% 6.5832% 5.7297% 4.7335% 
12 Real attributes 
+ 1 Artificial attribute 
6.3780% 6.2883% 5.8388% 5.7003% 5.4999% 4.7837% 
ANN (12 Real attributes) 9.3107% 8.7423% 6.7245% 7.2055% 7.3021% 6.6605% 
14 
5. Conclusions 
The results of Table 1 and 2 and Figure 2 and 3 are combined into Table 3 and Figure 5. 
Table 3:The computational results 
 Scales of training data sets 5 10 15 20 25 30 
Average 
error rate 
ANN (13 Real attributes) 8.8688% 7.0899% 6.7002% 7.0095% 6.6064% 6.0171% 
Li et al. [4] 7.1977% 6.7202% 6.6342% 6.5832% 5.7297% 4.7335% 
13 Real attributes 
+ 1 Artificial attribute 
6.1587% 6.0465% 5.4589% 5.2259% 4.7121% 4.1848% 
12 Real attributes 
+ 1 Artificial attribute 
6.3780% 6.2883% 5.8388% 5.7003% 5.4999% 4.7837% 
ANN (12 Real attributes) 9.3107% 8.7423% 6.7245% 7.2055% 7.3021% 6.6605% 
 
Figure 5:Comparisons of the computational results 
Table 3 and Figure 5reveal that the proposed method successfully explores the hidden critical 
information, which includes the attribute domain and unknown attribute. In the real world, there are 
many small data set problems that are often associated with negative outcomes. This research is an 
attempt to solve such problems. In this work, the MLCC problem is used to illustrate the proposed 
method. Much remains to be explored, such as prediction of typhoon damage, terrorist attacks, and 
outbreaks of new diseases. Few studies examine small sample problems, and thus there is a lot of 
potential to seek better methods to obtain a higher rate of accuracy. At present, we believe that the 
proposed procedure can be applied as the pilot research, and in the future it is hoped that more 
16 
References 
[1] P.P.Balestrassi, E. Popova, A.P. Paiva, J.W. Marangon Lima, Design of experiments on 
neural network's training for nonlinear time series forecasting,Neurocomputing, 72 (2009) 
1160-78. 
[2] F.Bellini, G. Figa-Talamanca, Runs tests for assessing volatility forecastability in financial 
time series, European Journal of Operational Research, 163 (2005) 102-14. 
[3] M.Qi, G.P. Zhang, An investigation of model selection criteria for neural network time series 
forecasting, European Journal of Operational Research, 132 (2001) 666-80. 
[4] D.C.Li,T.I.Tsai, S.Shi, A prediction of the dielectric constant of multi-layer ceramic 
capacitors using mega-trend-diffusion technique in the powder pilot runs: Case 
study,International Journal of Production Research,47(2009) 51-69. 
[5] V.C. Ivănescu, J.W.M. Bertrand, J.C. Fransoo, J.P.C. Kleijnen, Bootstrapping to solve the 
limited data problem in production control: an application in batch process industries, Journal 
of the Operational Research Society, 57(2006)2-9. 
[6] R. Lanouette, J. Thibault, J.L. Valade, Process modeling with neural networks using small 
experimental datasets, Computers & Chemical Engineering, 23(1999) 1167-1176. 
[7] D. Wolpert, Stacked generalization, Neural networks, 5(1992) 241-259. 
[8] A. Oniśko, M.J. Druzdzel, H. Wasyluk, Learning Bayesian network parameters from small 
data sets: application of Noisy-OR gates, International Journal of Approximate Reasoning, 
27(2001) 165-182. 
[9] C.J. Rhodes, E.M.J. Keefe, Social network topology: a Bayesian approach, Journal of the 
Operational Research Society, 58(2007) 1605-1611. 
[10] M.Anthony and N.Biggs, Computational Learning Theory (Cambridge University Press, 
1997). 
[11] N.V.Vapnik, The Nature of Statistical Learning Theory (Springer, New York, 2000). 
[12] P.Niyogi, F. Girosi,P. Tomaso, Incorporating prior information in machine learning by 
creating virtual examples, Proceeding of the IEEE,(1998)275-298. 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/11
國科會補助計畫
計畫名稱: 應用虛擬因子於小樣本學習—以生產資訊系統之試產為例
計畫主持人: 林耀三
計畫編號: 99-2221-E-273-002- 學門領域: 資訊系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
