 Problem-aware Resource Selection Policies in 
Computational Grids  
Woei Lin
*Department of Computer Science, National Chung Hsing University, Taiwan, R.O.C. 
Email: wlin@nchu.edu.tw 
 
Abstract—The goal of the resource selection policy is to 
minimize the schedule length for a number of tasks running in 
the selected resources. The time complexity of task scheduling 
can be reduced mostly by performing resource selection. In this 
report, the problem-aware resource selection policy is proposed 
base on some basic methods discussed in our previous works. 
The grid routing network is modified to fit the actual Internet 
topology. The impact of routing delay in the bottom level 
equation is discussed in this research. The average schedule 
length with routing cost is proposed to approximate the 
schedule length of the improved list scheduling method, and 
simulation result shows the accuracy of this approximation. The 
effect of routing delay is investigated in the experiment. The 
performance results show the efficiency of proposed resource 
selection policy. 
Index Terms—resource selection policy, routing delay, 
average schedule length. 
I. INTRODUCTION 
THE computational grid will be the next generation of 
computing infrastructure [1]. Resource selection policy is the 
mechanism to select a number of suitable resources from 
multiple available candidates in the grid environment. Task 
scheduling is the assignment of a set of tasks to a certain 
number of resources and its goal is to minimize the schedule 
length. Task scheduling problem has been proved as NP-hard 
[2]. The resource selection policy can reduce the complexity 
of task scheduling. 
 
Some related resource selection schemes are summarized 
as follows:  
 
1) Usage Pattern Based: the node selection mechanism 
based on the usage pattern of computational nodes on 
Campus Grid is proposed [3]. The node usage pattern is 
represented by a history of CPU load. The length of the 
usage pattern is a critical problem in this scheme. 
 
2) Communication Pattern Based: the method selects 
the resource by analyzing the network status and the 
communication pattern used by the application [4]. The 
drawback is that the method depends on the accuracy of 
the network status information is measured and the future 
performance is predicted. 
 
3) Mapping Strategy Based: the resource selector 
service [5] selects the resource set that satisfies the 
requirements by adopting a mapping strategy. The 
problem is that an efficient general mapping algorithm 
suitable for all applications is difficult to find. Random 
Based: resources are selected by random policy. It is 
simple but hard to analyze. 
 
4) Genetic Algorithm (GA): the resource selection agent 
uses a genetic algorithm is developed [6]. The generating 
time of genetic algorithm is an issue for resource 
selection. 
 
5) Multi-site Resource Selection: the clustering-based 
grid resource selection algorithm is proposed by adopting 
multiple sites [7]. There is a critical issue that accuracy of 
predicted execution time will determine the correctness of 
the algorithm. 
 
6) Pull-Based: A resource broker called Surfer [8] is 
implemented for resource selection and ranking resources 
to meet constraints. 
 
The topology of routing network has a great impact on the 
communication performance. The routing network model can 
be classified into three categories: the first is the flat random 
model (such as the Pure Random Model [9], and the Waxman 
Model [10]). The second is the hierarchical network model 
[11] (such as the N-Level Method, and the Transit-Stub 
Method). The last is the real internet model [12]. 
 
This rest of this report is organized as follows. In Section II, 
the system model is defined. Section III introduces the 
problem-aware resource selection policy. In Section IV, a 
new bottom level method for task priority is proposed. In 
Section V, the approximation of scheduling length is 
described. Section VI is the simulation result. The report ends 
with a brief conclusion and future works in Section VII. 
II. SYSTEM MODEL 
The system model consists of five components:  grid 
resource model, problem model, resource selected policy, 
scheduling model and routing model. The grid resource 
model is represented by a routing network with resources. 
The problem model is represented by a directed acyclic graph 
(DAG) of tasks. Resource selected policy is used to select the 
computational nodes from the grid resource model. 
Scheduling Model performs the assignment of tasks in the 
problem model to the selected computational nodes. The 
routing model can provide the information about routing path 
to the scheduling model. 
 
 1
 ability parameter than B will be thought that A is faster 
than B. 
 
2) Near Node First (NNF) Policy: the top n  near 
computational nodes will be selected from m  available 
nodes in the resource model, and m  is greater than n . 
The computing node A is nearer than B, if the 
communicational cost of A is less than B. 
3)  Fast Among Near (FAN) Policy: we select r  nodes 
using NNF policy, and then select top  fast nodes from n
r  nodes, where r  is greater than n . 
4) Near Among Fast (NAF) Policy: we select r  nodes 
using FNF policy, and then select top  near nodes from n
r  nodes, where r  is greater than n . 
5) Random Selection Policy: Select randomly n  
computing nodes from  available nodes in the topology, 
and m  is greater than . 
m
n
 
C. Constraints on Selected Nodes 
Before task scheduling, the computational ability of the 
selected computational node c  should satisfy the following 
condition to avoid the schedule length get worse: 
( ) ( ) ( ) CoCCC VcPtVcP ˆ×≥ , (1) 
Where  is the original node and  is the total 
computational volume of a problem. The condition is soft 
limit if 
oc CVˆ
( ) ( ){ TttVtV iiCC ∈= min }. It is hard limit if ( )tVC  is 
maximal. 
IV. IMPROVED BOTTOM LEVEL 
The list scheduling algorithm can achieve reasonable 
worst-case performance bound in grid environments with 
large applications and the schedule length does not impacted 
significantly by the heterogeneous communication [15]. 
The bottom level of list scheduling algorithm is used to set 
the priority of the tasks in the DAG. The priority of the task 
in the DAG topology is expressed as follows: st
( ) ( ) ( ) ( ){ }ddsblcommsblcomps tBLeCosttCosttBL ++= ,max , (2) 
Where the task  is the successor of the task , 
 is the computational cost of the task , and 
dt st
( )sblcomp tCost st( )dsblcomm eCost ,  is the communicational cost of the edge from 
 to .  st dt
However, in the computational grid environment, there is a 
problem that computational cost and communicational cost 
are undetermined until assigning tasks to the computational 
nodes. Our solution is to provide average cost parameters for 
the bottom level before performing the task scheduling. 
 
A. Computational Cost 
Let CP  denote the average computational ability of all 
selected computational nodes. The computational cost for 
each task  in the DAG is defined as follows: st
( ) ( ) CsCsblcomp PtVtCost =  (3) 
 
B. Critical Link 
The critical link  is the link with minimal bandwidth in 
the routing path 
ji,l
ji,Ρℜ  that is from the source computational 
node  to the destination computational node . ic jc
 
C. Communicational Cost  
For each pair of  selected nodes, a proper path can be 
produced by the routing algorithm adopted. Let 
SN ( )jiBP ,l  be 
the bandwidth of the critical link .Let  denote the set 
of selected computational nodes. The average critical link 
bandwidth of all pairs of selected computational nodes  and 
ji ,l SR
i
j  is: 
( ) ( ) (∑∈∀×−×= SRji jiBssjiB PNNP , ,, 1
1 ll )  (4) 
Let ( )sout td  denote the outgoing degree of the task . The 
serialization cost 
st( )dsbls eCost ,  is defined as: ( ) ( ) ( )( ) ( )jiBsoutsMdsbls PtdtVeCost ,, l×=  (5) 
Let us define β  is the routing cost factor. It is the ratio of 
the routing cost to the serialization cost. The routing cost for 
the edge  that is form the task node  to  is defined as: dse , st dt( ) ( )dsblsdsblr eCosteCost ,, ×= β  (6) 
In the absence of contention, the communicational cost for 
each directed edge  that is from task  to the task  in a 
DAG is defined as: 
dse , st dt
( ) ( ) ( ) ( ) ( )dsblsdsblrdsblsdsblcomm eCosteCosteCosteCost ,,,, 1 β+=+=  (7) 
 
V. APPROXIMATION OF SCHEDULE LENGTH 
A. Assumptions 
After performing resource selection policy, assume  
computational nodes are selected. For each edge  in a 
DAG, we have a pair of tasks i  and task
SN
jie ,
j . Assume that two 
tasks i  and j are assigned to the computational node s  and 
 respectively after performing task scheduling. The 
mapping function  can be defined to express the task 
assignment. For example, 
d
(.)f
( )ifs = , and . ( )jfd =
 
B. Average Computational Cost 
Let CV  denote the average computational volume of all 
tasks in the DAG. The average computational cost is defined 
as: 
CCcomp PVCost =  (8) 
 
C. Average Communicational Cost 
The serialization cost for each edge (  form the task i  
to the task 
)ji,
j  can be determined by the critical link 
 3
 E. Accuracy of Average Schedule Length 
Figure 3 is the approximation of the schedule length for all 
selection policies. In Figure 5, the average difference of ASL 
and SL is about 30% because of the schedule length 
decreases rapidly, while the number of resources is from 2 to 
average 9.4=α . These two measurements differ by less than 
30% in the range from α  to 30. 
 
[11] Ellen W. Zegura, Kenneth L. Calvert, and Michael J. Donahoo, “A 
Quantitative Comparison of Graph-Based Models for Internet 
Topology,” IEEE/ACM Transactions On Networking, Vol. 5, No. 6, 
December 1997. 
[12] The Cooperative Association for Internet Data Analysis (CAIDA), 
http://www.caida.org. 
[13] Thomas H. Cormen, Charles E. Leiserson and Ronald L. Rivest, 
Introduction to Algorithms, The MIT Press, 1990. 
[14] Uei_Ren Chen, Chien_Hsun Wang, and Woei Lin, “Average Schedule 
Length and Resource Selection Policies on Computational Grid,” 
Submitted to International Conference on Grid and Pervasive 
Computing (GPC) 2006, Taiwan. F. Effect of Routing Cost Factor 
For computational-oriented problem, the effect of routing 
cost can be observed in Figure 6. Figure 7 is the usage of the 
selected computational nodes. For communication-oriented 
problem, the effect of routing cost is in Figure 8, and usage of 
the selected nodes is in Figure 9.  
[15] Keqin Li, “Job Scheduling for Grid Computing on Metacomputers,” 
Proceedings of the 19th IEEE International Parallel and Distributed 
Processing Symposium, 2005. 
[16] Lloyd Allison’s web site, 
http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Graph/DAG/. 
 
VII. CONCLUSION 
The conclusions of this report are made as follows: The 
problem-aware resource selection policy can achieve the 
bound of schedule length efficiently. The number of selected 
resources can be determined by adopting assignable factor α  
and using these resources can achieve the bound of schedule 
length above 90%. The improved bottom level can resolve 
the resource undetermined problem for list scheduling 
algorithm in the grid environment. The Average Schedule 
Length is proposed to approximate the schedule length of 
improved list scheduling algorithm. The accuracy of the 
Average Schedule Length is evaluated and the maximal 
difference to schedule length is less than 30%. 
 
 
REFERENCES 
[1] Ian Foster and Carl Kesselman, The Grid: Blueprint for a New 
Computing Infrastructure, Morgan Kaufmann, San Francisco, CA, 
1999. 
[2] J. D. Ulman, “NP-Complete Scheduling Problems,” Journal of 
Computing System Science, Vol. 10, 1975. 
[3] Hiroshi Arikawa, Kazutoshi Fujikawa and Hideki Sunahara, “A Node 
Selection Mechanism based on the Node Usage Pattern on Campus 
Grid,” IEEE Pacific Rim Conference on Communications, Computers 
and signal Processing, 2003. 
[4] Srikanth Goteti, and Jaspal Subhlok, “Communication pattern based 
node selection for shared networks,” Proceedings of the Autonomic 
Computing Workshop, 2003. 
[5] Chuang Liu, Lingyun Yang, Ian Foster and Dave Angulo, “Design and 
Evaluation of a Resource Selection Framework for Grid Applications,” 
Proceedings of the 11th IEEE International Symposium on High 
Performance Distributed Computing HPDC-11, 2002. 
[6] HwaMin Lee, KwangSik Chung, SungHo Chin, JongHyuk Lee, 
DaeWon Lee, Seongbin Park and HeonChang Yu, “A resource 
management and fault tolerance services in grid computing,” Journal of 
Parallel and Distributed Computing, 2005. 
[7] Weizhe Zhang, Binxing Fang, Hui He, Hongli Zhang and Mingzeng 
Hu, “Multisite Resource Selection and Scheduling Algorithm on 
Computational Grid,” Proceedings of the 18th International Parallel 
and Distributed Processing Symposium, 2004. 
[8] Paul Z. Kolano, “Surfer: An Extensible Pull-Based Framework for 
Resource Selection and Ranking,” IEEE International Symposium on 
Cluster Computing and the Grid, 2004. 
[9] GT-ITM project of Goeorigia Institute of Technology. 
http://www.cc.gatech.edu/projects/gtitm/. 
[10] B.M. Waxman, “Routing of multipoint connections,” IEEE Journal on 
Selected Areas in Communications, 1988. 
 5
