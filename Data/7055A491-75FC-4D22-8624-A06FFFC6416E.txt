1 
目錄 
壹、前言............................................................ 2 
貳、研究目的........................................................ 2 
參、文獻探討........................................................ 3 
肆、研究方法........................................................ 4 
伍、研究成果........................................................ 6 
陸、結論與建議...................................................... 6 
柒、計畫成果自評.................................................... 7 
參考文獻........................................................... 10 
附錄一：高光譜遙測影像辨識研究之影像............................... 11 
附錄二：實驗結果................................................... 13
3 
bagging 的不足。 
2.  針對  WRSM  只能對特徵作抽取而無考慮樣本方面的缺點，融入  bagging  來改善 
WRSM的不足。 
3.  討論分類器個數對分類效能的影響與變化。 
本研究中所採用高斯分類器(quadratic Gaussian density classifier, qdc)、k最近鄰分類器(k 
nearest neighbors classifier, knnc)和支撐向量機分類器(support vector machine classifier,  svc)三 
種分類器，結合 bagging 與 WRSM應用在多重分類器上來驗證所提方法的可用性。 
參、文獻探討 
一、多重分類器與結合策略 
分類器的結合最主要是結合多重分類器之個別的決策，而如何結合分類器則是影響多重 
分類器效能的主要議題[8]。而使用多重分類器較單一分類器佳的原因，可以從統計上及計算 
上來分析。統計上的原因是因為假如無足夠的訓練樣本可以訓練分類器，對於分類器本身的 
建構會相當不穩定，進而造成分類效果會有偏差，於是乎組合多組分類器即可解決此問題。 
在計算方面來說，若各分類器由解空間的邊界開始搜尋求解，會因為訓練時起始點的不同而 
得到不同的解，所以會常發生無法選取適當的初始訓練點或是不良的搜尋方式，最後會陷入 
區域解。因此基於這些理由來選擇建構多重分類器。 
小樣本高維度的情形會造成所使用的單一分類器估計偏誤及不穩定，以多重分類器來取 
代原有的單一分類器方法可以提高分類器的性能及穩定性。根據先前的研究，結合分類器的 
方法可分為 1.序列式結合(serial combination)、2.分類器融合或並列式結合(classifier fusion or 
parallel combination)，以及動態分類器選取(dynamic classifier selection)。 
分類器結合策略其目的是將多組的分類器進行結合的動作，考慮對資料點 X 做分類，有 
j 個可能的類別  j y  ,  L j  ,..., 1 = ，而資料點 X 只會被指派到其中的一個類別。共有 B 個分類器， 
每個分類器對此資料點 X 都有不同的測量向量  b x  ,  B b  ,..., 1 = 。因此可以得到機率密度函數 
) | (  j b  y x p  與先驗機率  ) (  j y P  。在此用到主要投票法(majority vote)的結合策略，結合策略中為 
了正確的利用所有可得的資訊來做出決策，也必需同時考慮所有的觀測值並計算其後驗機率 
) | (  b j  x y P  ，利用後驗機率產生二進位的輸出 
ï î 
ï 
í 
ì = = D = 
otherwise , 0 
) ( max ) ( if , 1 
1 
b j 
L 
j 
b j 
jb 
|x y P |y y P
5 
將彌補兩種方法個別不足的地方，一次做結合的動作，產生一個新的多重分類器系統，以求 
解決兩種方法個別不足的地方。 
本研究提出新的多重分類器之結合，對資料使用兩階段取樣步驟，先行使用 bagging的 
技巧，進行對訓練樣本的抽樣並產生新的訓練資料集，而新的資料集再分別應用 WRSM步 
驟，對特徵進行抽樣，最後再使用主要投票法做出最後決策，以下為演算法之介紹。 
一、Bagging and random subspace method based on kernel smoothing algorithm 
Bagging和 RSM由核平滑化法所估計的子空間維度分佈 R 來自動選取子空間維度。 
[BG­RSM­KS演算法] 
1.  Repeat for  B b  ..., 2 , 1 = 
(a)Take a bootstrap replicate  b X  of the training data set X 
(b)RSM­KS procedure using training data set  b X 
2.  Combine classifiers  B b X C b  ..., 2 , 1 ), ( =  by majority voting (the most often predicted label) to 
a final decision rule 
å 
Î 
= 
b 
y x C 
L y 
b x  ), ( 
} ,..., 1 { 
max arg ) ( d b 
where  j i, d  is the Kronecker symbol, and  L y  ..., 2 , 1 =  is a decision (class label) of the classifier 
for two­class problem. 
二、Bagging and weighted random subspace method based on kernel smoothing algorithm 1 
Bagging和使用訓練樣本分類正確率作為特徵重要程度分佈W 的加權。 
[BG­WRSM­KS1演算法] 
1.  Repeat for  B b  ..., 2 , 1 = 
(a)Take a bootstrap replicate  b X  of the training data set X 
(b)WRSM­KS1 procedure using training data set  b X 
2.  Combine classifiers  B b X C b  ..., 2 , 1 ), ( =  by majority voting (the most often predicted label) to 
a final decision rule 
å 
Î 
= 
b 
y x C 
L y 
b x  ), ( 
} ,..., 1 { 
max arg ) ( d b 
where  j i, d  is the Kronecker symbol, and  L y  ..., 2 , 1 =  is a decision (class label) of the classifier 
for two­class problem.
7 
分類器的分類效能，利用 3種不同的分類器，並對不同的訓練樣本數及分類器個數做討論， 
檢視其中的差異性及其效能為何。就實驗結果顯示出，本研究所提之方法，在Washington DC 
Mall資料集中，qdc及 svc分類器與原方法效能相似，而在 knnc則能超越原方法的表現。在 
Indian Pine Site資料集中，svc分類器與原方法效能相似，而在 qdc及 knnc則能超越原方法的 
表現。在不同的訓練樣本數及分類器個數的影響，可以發現到在 Indian Pine Site資料集中， 
發現當訓練樣本數較少時，分類器個數越多則對分類表現會有較顯著影響，而訓練樣本數較 
多時，則用較少的分類器個數即可達到最佳值；而在 Washington DC Mall資料集中，訓練樣 
本數的多寡與分類器個數的表現結果無太大差異。於是可以得知需視樣本的情形，對訓練樣 
本數的大小來調整分類器個數。而在各分類器中 knnc可以得到較顯著的改善效能。 
在未來研究與發展方面，建議可以對分類器個數的大小和樣本數大小，及對不同的分類 
器結合理論做更進一步的討論，另外還可以加入不同的特徵加權方式，以及不同的樣本抽取 
方式，考慮對樣本作分配來更新抽取方式或許也能提高分類器的效能。 
柒、計畫成果自評 
本研究計畫主要依據計畫書所規劃事項，完成最後一階段任務，工作進行相當順利，相 
關成果與發表論文亦相當豐碩，累積計有 SCI­Expanded 論文 6篇，EI論文 20篇及其它研討 
會論文數篇。參與計畫研究人員得到完整且良好研究訓練，亦順利考取國立大學博士班數名， 
累計有國立台灣大學資工所一名、國立交通大學電機所兩名、國立中興大學電機所一名、國 
立中正大學資工所一名。 
本研究計畫三年期間已投稿及出版的論文如下： 
Bor­Chen Kuo, Member, IEEE, and Kuang­Yu Chang. (2007). Feature Extractions for Small Sample Size Classification 
Problem. IEEE Transactions on Geoscience and Remote Sensing, Vol. 45, No. 3, pp.756­764. (SCI and EI) 
Bor­Chen Kuo, Hsin­Hua Ho, Cheng­Hsuan Li, and Ya­Yuan Chang. (2006). A Novel Nearest Neighbor Classifier 
Based on Adaptive Nonparametric Separability. Lecture Notes in Artificial Intelligence,Vol.4304. pp.204­213(SCI 
and EI) 
Kuo, B­C., and Li, C­H. (2005). Kernel Nonparametric Weighted Feature Extraction for Classification. Lecture Notes 
in Artificial Intelligence, Vol.3809, pp. 567­576. (SCI and EI) 
Bor­Chen Kuo, and Chang, K­Y. (2005). Regularized Feature Extractions and Support Vector Machines for 
Hyperspectral Image Data Classification. Lecture Notes in Computer Science, 3681, pp. 873­879. (SCI­Expanded) 
Ko, L­W., Kuo, B­C., and Lin, C­T. (2005).An Optiaml Nonparametric Weighted System for Hyperspectral Data 
Classification. Lecture Notes in Computer Science, 3681, pp. 866­872. (SCI­Expanded) 
Kuo, B­C., Ko, L­W., and Sheu, T­W. (2004). Two­stage Classification Systems for Hyperspectral Image Data. 
(submitted to IEEE Transactions on Geoscience and Remote Sensing, SCI). 
Bor­Chen Kuo, Ming­Hung Chi, Jinn­Min Yang, and Chih­Wei Yang, (2007). Hierarchical Classification Systems for
9 
Eigenvalue Decomposition Methods, and Regularization Techniques in Feature Extractions for Small Sample Size 
Classification Problem. Proceedings of International Geoscience and Remote Sensing Symposium, July. 25­29, 
2005, (EI). 
Bor­Chen Kuo, Chun­Hao Chang, Tian­Wei Sheu, Chih­Cheng Hung (2005). Feature Extractions Using Labeled and 
Unlabeled Data. Proceedings of International Geoscience and Remote Sensing Symposium, July. 25­29, 2005, (EI). 
Kuo, B­C., Pai, C­H., Sheu, T­W., and Chen, G­S. (2004). Hyperspectral Data Classification Using Classifier 
Overproduction and Fusion Strategies. Proceedings of International Geoscience and Remote Sensing Symposium, 
Sep. 20­24, 2004, (EI). 
Chen, G­S., Ko, L­W., Kuo, B­C., and S­C., Shih. (2004). A Two­stage Feature Extraction for Hyperspectral Image 
Data Classification. Proceedings of International Geoscience and Remote Sensing Symposium, Sep. 20­24, 2004, 
(EI). 
Chun­Hsiang Chuang, Wen­Chun Huang, Bor­Chen Kuo.(2007) Multiple classifiers fusion method based on a fuzzy 
measure. 第二十屆電腦視覺、圖學暨影像處理研討會論文集(CVGIP 2007)，2007  年8  月19~21  日，聯合大 
學。 
Cheng­Hsuan Li, Hsin­Hua Ho, Bor­Chen Kuo.(2007) Hyperspectral image classification using kernel­based 
nonparametric weighted feature extraction with optimized kernels. 第二十屆電腦視覺、圖學暨影像處理研討會論 
文集(CVGIP 2007)，2007  年8  月19~21  日，聯合大學。 
Hsin­Hua Ho, Cheng­Hsuan Li, Bor­Chen Kuo, and Ya­Yuan Chang. (2006) An adaptive nearest neighbor classifier 
based on nonparametric separability. 第十九屆電腦視覺、 圖學暨影像處理研討會論文集(CVGIP 2005)，2006  年 
8  月13~15  日，中原大學。 
Ming­Hung Chi, Jang Yu­Ching, Bor­Chen Kuo, and Ya­Yuan Chang. (2006) Two­stage hierarchical schemes with 
maximal likelihood classifierin hyperspectral image. 第十九屆電腦視覺、圖學暨影像處理研討會論文集(CVGIP 
2005)，2006  年8  月13~15  日，中原大學。 
Pai, C­H., Kuo, B­C., Sheu, T­W., and Chen, G­S. (2004). Hyperspectral Image Classification Using Dynamic 
Classifier Selection with Multiple Feature Extractions. International Computer Symposium. Dec. 15­17, 2004, 
Taipei, Taiwan. 
Kuo, B­C., Chang, K­Y., Chang, C­H., and Hsieh, Y­C. (2004). Hyperspectral Image Data Classification Using Feature 
Extractions and Support Vector Machines. 第十七屆電腦視覺、圖學暨影像處理研討會論文集(CVGIP 2004)， 
2004年 8月 15~17日，東華大學。 
Ko, L­W., Kuo, B­C., and Yang, J­M. (2004). A Two­stage Classification System Using Likelihood Ratio Reject 
Option for Hyperspectral Image Data. 第十七屆電腦視覺、圖學暨影像處理研討會論文集(CVGIP 2004)，2004 
年 8月 15~17日，東華大學。
11 
附錄一：高光譜遙測影像辨識研究之影像 
經調查遙測領域相關期刊及研討會論文集內高光譜遙測影像辨識研究中常採用之高光譜 
遙測影像而且是免費可獲的，本研究擬採用二影像資料，其中一為農業用地影像，另一為都 
市區域影像，簡介如下。 
1. Indian Pine Site: 
Indian  Pine  Site 影像是由印第安那州西北部之農業用地中選取一百平方英里範圍， 
並於 1992年 6月收集完成，為一混合森林和農業區域的 AVIRIS空載高光譜影像，如圖 
2所示。此影像具有 220個有效頻譜，包含 9個類別，分別為：Corn­no till，Corn­min till， 
Grass/Pasture，Grass/Trees，Hay­windrowed，Soybean­no till ，Soybean­min till ，Soybean­clean 
till和 Woods。此影像中各類別所含有之圖素(pixels)如表 1所示，本研究欲進行之 Indian 
Pine  Site 影像實驗，乃由各類別中的圖素隨機選取所需之樣本數，形成訓練樣本和測試 
樣本。 
表 1、Indian Pine Site影像各類別所含有之圖素 
類別名稱 圖素數 
1  Corn ­ no till  1428 
2  Corn ­ min till  830 
3  Grass / Pasture  483 
4  Grass / Trees  730 
5  Hay – windrowed  478 
6  Soybean ­ no till  972 
7  Soybean – min till  2455 
8  Soybean – clean till  593 
9  Woods  1265 
全部  9234 
圖 2、Indian Pine Site 影像
13 
附錄二：實驗結果 
本研究所提出新方法與舊方法之實驗比較結果，分類器分類之正確率如表 3~8 所示，粗 
體部份為擁有最佳分類正確率的組合。 
Washington DC MALL分類正確率(表 3~5)，在小樣本中(實驗 1)類別訓練樣本為 20時， 
單一分類器的最佳分類正確率為  0.838(knnc)，使用  WRSM  最佳分類正確率為 
0.912(WRSM­KS2)，經由新演算法可提升至 0.932(BG­WRSM­KS2)有 2%的提升水準。在 qdc 
及 svc 分類器的新方法與舊方法表現方面則無太大差異，圖 7~9 則是取在實驗 1 當中有較明 
顯差距分類效果的 knnc分類器之分類結果圖。 
表 3 Washington DC Mall (Ni=20)分類正確率 
Classifier qdc knnc(k=1) svc 
B Algorithm BG WRSM BG- 
WRSM 
BG WRSM BG- 
WRSM 
BG WRSM BG- 
WRSM 
1 SINGLE 0.143 0.838 0.562 
20 KS 0.143 0.923 0.915 0.836 0.856 0.860 0.783 0.875 0.910 
20 KS1 0.143 0.935 0.934 0.836 0.849 0.856 0.783 0.912 0.926 
20 KS2 0.143 0.930 0.936 0.836 0.907 0.928 0.783 0.934 0.935 
50 KS 0.143 0.931 0.925 0.837 0.858 0.869 0.799 0.903 0.914 
50 KS1 0.143 0.941 0.936 0.837 0.857 0.867 0.799 0.927 0.936 
50 KS2 0.143 0.936 0.938 0.837 0.911 0.932 0.799 0.935 0.936 
100 KS 0.143 0.932 0.929 0.838 0.859 0.870 0.804 0.907 0.918 
100 KS1 0.143 0.943 0.937 0.838 0.856 0.868 0.804 0.931 0.935 
100 KS2 0.143 0.937 0.938 0.838 0.912 0.932 0.804 0.942 0.941 
0.8 
0.85 
0.9 
0.95 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 4 Washington DC Mall 分類正確率比較圖(實驗 1, Ni=20,B=20) 
分
類
正
確
率
% 
分類器
15 
圖 8 Wrsm­KS2 dc1N20 knnc的分類結果圖 
圖 9 BG­Wrsm­KS2 dc1N20 knnc的分類結果圖 
在實驗 2 中，單一分類器的最佳分類正確率為 0.880(knnc)，使用 WRSM 最佳分類正確 
率為  0.935(WRSM­KS2)經由新演算法可提升至  0.952(BG­WRSM­KS2)有  1.7%的提升水準， 
在 qdc 及 svc分類器的新方法與舊方法表現方面則無太大差異，不過在 svc分類器之中，新演 
算法還是可以達到最佳分類正確率。圖 13~15 則是取在實驗 2 當中有較明顯差距分類效果的 
knnc分類器之分類結果圖。
17 
0.8 
0.85 
0.9 
0.95 
1 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 12 Washington DC Mall 分類正確率比較圖(實驗 2, Ni=40,B=100) 
圖 13 dc1N40 knnc的分類結果圖 
圖 14 Wrsm­KS2 dc1N40 knnc的分類結果圖 
分
類
正
確
率
% 
分類器
19 
0.88 
0.9 
0.92 
0.94 
0.96 
0.98 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 16 Washington DC Mall 分類正確率比較圖(實驗 3, Ni=100,B=20) 
0.9 
0.92 
0.94 
0.96 
0.98 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 17 Washington DC Mall 分類正確率比較圖(實驗 3, Ni=100,B=50) 
0.9 
0.92 
0.94 
0.96 
0.98 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 18 Washington DC Mall 分類正確率比較圖(實驗 3, Ni=100,B=100) 
分
類
正
確
率
%
 
分
類
正
確
率
%
 
分
類
正
確
率
% 
分類器 
分類器 
分類器
21 
表 6 Indian Pine Site (Ni=20)分類正確率 
Classifier qdc knnc(k=1) svc 
B Algorithm BG WRSM BG- 
WRSM 
BG WRSM BG- 
WRSM 
BG WRSM BG- 
WRSM 
1 SINGLE 0.111 0.692 0.750 
20 KS 0.111 0.753 0.757 0.689 0.693 0.685 0.775 0.717 0.707 
20 KS1 0.111 0.763 0.771 0.689 0.694 0.686 0.775 0.719 0.716 
20 KS2 0.111 0.717 0.724 0.689 0.693 0.689 0.775 0.719 0.710 
50 KS 0.111 0.782 0.774 0.691 0.695 0.688 0.777 0.727 0.720 
50 KS1 0.111 0.793 0.774 0.691 0.694 0.691 0.777 0.739 0.735 
50 KS2 0.111 0.752 0.737 0.691 0.695 0.697 0.777 0.729 0.730 
100 KS 0.111 0.803 0.775 0.692 0.696 0.692 0.778 0.745 0.740 
100 KS1 0.111 0.812 0.777 0.692 0.694 0.691 0.778 0.746 0.741 
100 KS2 0.111 0.780 0.737 0.692 0.696 0.699 0.778 0.742 0.738 
0.6 
0.65 
0.7 
0.75 
0.8 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 22 Indian Pine Site 分類正確率比較圖(實驗 4, Ni=20,B=20) 
0.6 
0.65 
0.7 
0.75 
0.8 
0.85 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 23 Indian Pine Site 分類正確率比較圖(實驗 4, Ni=20,B=50) 
分
類
正
確
率
%
 
分
類
正
確
率
% 
分類器 
分類器
23 
0 
0.2 
0.4 
0.6 
0.8 
1 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 25 Indian Pine Site 分類正確率比較圖(實驗 5, Ni=40,B=20) 
0 
0.2 
0.4 
0.6 
0.8 
1 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 26 Indian Pine Site 分類正確率比較圖(實驗 5, Ni=40,B=50) 
0.65 
0.7 
0.75 
0.8 
0.85 
0.9 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 27 Indian Pine Site 分類正確率比較圖(實驗 5, Ni=40,B=100) 
實驗 6 單一分類器的最佳分類正確率為 0.794(knnc)，單獨使用 WRSM 最佳分類正確率 
為 0.807(WRSM­KS2)經由結合策略可提升至 0.816(BG­WRSM­KS2) 有 0.9%的提升水準，使 
用 qdc分類器時並且在分類器個數為 20的時候WRSM 分類正確率為 0.874(WRSM­KS1)經由 
結合策略可提升至 0.893(BG­WRSM­KS1)有 1.9%的提升，而在其他個數的分類器一樣能達到 
分類最佳值，也發現在 Indian Pine Site資料集當中使用(BG­WRSM­KS2)演算法能達到分類最 
佳值 0.904。在 svc分類器中分類效果大致相同。 
分
類
正
確
率
%
 
分
類
正
確
率
%
 
分
類
正
確
率
% 
分類器 
分類器 
分類器
25 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
qdc knnc svc 
Rsm-KS 
Wrsm-KS1 
Wrsm-KS2 
BG-Rsm-KS 
BG-Wrsm-KS1 
BG-Wrsm-KS2 
圖 30 Indian Pine Site 分類正確率比較圖(實驗 6, Ni=100,B=100) 
分
類
正
確
率
% 
分類器
