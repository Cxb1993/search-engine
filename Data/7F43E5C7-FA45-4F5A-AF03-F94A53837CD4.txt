I 
 
摘  要 
 
資料錯誤有些會造成商業上的傷害，有些則否；因此資料品質測量不應把這些錯誤
一視同仁。本研究首先提出了品質指示關鍵數據中，以成本為考量之下，可將錯誤資料放
大之一品質指標；並透過資料倉儲其資料產生的過程為例，來了解資料品質如何透過所提
的指標模型在一程序中演進。基於這樣的分析，資料品質管理團隊可將可用的資源直接使
用在最關鍵的資料產生過程中，以提高整體資料品質。其次，本研究提出一個範例流程模
式，具體化有關資料品質控制活動的標準化與制度化方式，以獲得全面性的資料品質管
理。另外，我們實作並分析一個企業中的例子，來說明提出此方法的理由，並討論其如何
相應到商業上的決策。 
 
關鍵詞：資料倉儲,資料品質,品質測量,流程方法 
 
 
ABSTRACT 
Some data errors have more business impact, some do not. Data quality measures thus 
should not treat all errors equally. This article first presents a cost and operational level of 
quality indication that amplifies the errors on critical data. With the focus on data warehouse, 
a process centric analysis is conducted for realizing how data quality evolves along a set of 
pipeline processes. Based on such an analysis, the data quality management team can direct 
the available resources to the most critical data production processes for enhancing total data 
quality. Second, this paper provides an empirical process model featuring standardization and 
institutionalization for data quality control action to gain a comprehensive data quality 
management. An analytical business example is implemented for illustrating the rationale of 
the proposed measure and discussing how business decisions are impacted correspondingly. 
 
 
Key Words: Data Warehousing, Data Quality, Quality Measure, Process Approach 
 
 
1 
 
1. Introduction 
Data quality (DQ) is a critical issue that impacts us in many ways. Many decisional 
problems due to poor DQ have been evinced and quantified, e.g. (English, 1999) (Firth, 1996) 
(Redman, 1996) and (Wang, et al, 2001). Other famous examples that show its criticality are 
the two tremendous tragedies: the explosion of the space shuttle Challenger and the shooting 
down of an Iranian Airbus by the USS Vincennes, which have been shown that poor DQ is 
one of the reasons (Fisher and Kingman, 2001). Another example includes the “dead people 
still eat” news from NBC reporting that poor DQ on government databases have caused 
billions of dollars of loss (Huang et al., 1999).  
Academically, a number of research reports analyzed or used simulations to show 
impacts on businesses and decision-making due to DQ problems, e.g. (Svanks, 1998) (Wang, 
et al. 1995) (Wand and Wang, 1996) (Kesh, 1995) (Srinivasan, 1999) (Strong, 1997) (Chen, 
2005) et al. As more and more damages, failures, and cost impacts resulting from poor DQ are 
concluded and reported, businesses and organizations are paying more attention to the quality 
of the data they use or produce. 
As business operations rely on data more highly and heavily, the quality of the data 
become more critical. Among these data-intensive business activities, data warehousing (DW) 
is a typical task that deals with tremendous size of data. DW is a “subject-oriented, integrated, 
time-variant, and nonvolatile collection of data in support of management’s decision making 
process (Inmon et al., 1996)”. With the help of DW, customer demands, sales opportunities 
and business rules can be identified based on the data derived from various, transactional or 
historical sources (Sen et al., 2006).  
In DW, the data are presented in a view or a fact table with multi-dimensional data 
models such as star schema, snowflake schema, and fact constellation schema in a 
materialized or virtual way (Inmon et al., 1997) (Gardner, 1999). However, data quality tends 
to be a major problem in data warehouse development (Hatcher, 2003). 
 
2. Related Work 
2.1  What affects DW’s DQ? 
If “information is viewed as a product (Wang, 1998) (Wang et al., 1998)”, then 
information manufacturing can be viewed as a set of processes that transform a set of data 
units into final information products. It is very helpful to view data as an intermediate or final 
output of a series of data manufacturing processes. In order to understand where data errors 
come from, how they are generated, and how they affect the data quality of final information 
products, a process might be a good unit to look into for a better data quality control. 
Therefore, there is a need for a simple but effective mechanism to track and evaluate data 
quality at each process stage.  
Based on the process-by-process tracking and evaluation, the information manufacturers 
can then decide if the quality of information requires improvement or is acceptable for every 
next information manufacturing process. In general, the information manufacturing of DW 
applications is complicated. It comprises several components and process elements. As Figure 
1 shows, “data loading” is a process that imports and selects data from external sources. The 
“views” refer to star schema, multi-dimensional fact table, materialized or virtualized views 
that present desired data for further decision support. 
3 
 
1999) (Redman, 1996) (Huh et al., 1990). Moreover, some stochastic observations of error 
distribution after data are selected are made (Guynes, 1996) (Firth, 1996) (Bowen et al., 
1998). 
2.2.1  The need of focusing on process 
When it comes to DW, the first question becomes difficult to answer due to, e.g. the 
complexity of DW process composition, multi-sources with different data quality levels, and 
the volume of data to be processed. Since data production is complicated in DW, it therefore 
plays a critical role in affecting the quality of the resulting data. Currently there is little 
research for studying what happens to the final DQ after going through the aforementioned 
process variables. Although several DW quality metrics have been seen, e.g. (Si-Said Cherfi 
and Part., 2003) (Calero et al., 2001), they are schema level and do not deal with data level or 
“instance level (Rahm and Do, 2000)” quality problems. Instead, Vassiliadis et al. provide an 
initial work that quantifies the quality level of the entire dataset by considering multi-sources 
and the process of combining the data. Figure 2 illustrates such a process-centric measure 
(Vassiliadis et al, 2000). 
 
 
 
 
 
 
 
 
2.2.2  The need of amplifying the errors on critical data 
With regard to DW managers’ second question, it is true that some data fields have 
critical impacts on the DW project; some do not. For example, for a customer table that is 
prepared for analyzing the average income levels vs. different cities, the data fields such as 
Salary, City or ZIP Code are relatively more critical than other fields such as Name, 
Telephone, Street or Gender. In spite of this, however, current DQ measures present only one 
single value representing the total quality of the entire dataset, perhaps in the size of millions 
of records but with most errors located in trivial fields. 
Even in the case that the crucial fields are obvious; the current generic-type quality 
measures might be still misleading, i.e. current DQ measures do not reflect the corresponding 
criticality of different data fields. As a result, relevant stakeholders who prepare the data 
simply ignore the errors when the quality level is acceptable even most errors come from the 
critical data. This could be very risky to the final decision makers and the business decisions 
in that the data suppliers do not know and may not care the context of ad-hoc business 
impacts due to poor DQ. 
Since it is unnecessary to indicate trivial errors, and since errors in critical data fields 
need more attention, DQ measures thus should not equally treat all errors. In other word, a 
“quality” data quality value (i.e. the quality about data quality) not only measures the degree 
of the agreement between the data and the real world, but it also should reflect the degree of 
impact of errors that bring to the real world.  
Source 1 
Source 2 
View1 (V1) 
View 2 (V2) 
View 3 (V3)
Accuracy = 45% 
Accuracy = 90% 
Analytically: 
Accuracy = 90% 
Analytically: 
Accuracy = 45% 
Size = 20M 
Size = 80M 
V3 = V1∪ V2 
size(V1)*acc(V1) + size(V2)*acc(V2)
size(V1) + size(V2) 
Accuracy (V3) = 
Accepted when 
Accuracy [85% - 100%] 
Figure 2: A quantitative model proposed by Vassiliadis et al. 
5 
 
where n is the number of records that are loaded, p is the probability of having an erroneous 
data item on each scan, and x is the number of erroneous data items actually loaded. Function 
P(x) denotes the error rate distribution with the loading process. When n is very large, P(x) 
has a probability density function (C.D.F) as illustrated in Figure 4.  
 
 
 
 
 
 
 
 
Then, εinitial can be estimated by the expected value of x according to the following equation:  
 
 
The expected value of x can be computed by the following equation: 
)1()1(1
1
11
)1(
)]!1()1[()!1(
)!1(
)1(
)!(!
!)()(
−−−−
=
−
==
−−−−−
−⋅=
−−⋅=⋅=
∑
∑∑
xnx
n
x
xnx
n
x
n
x
pp
xnx
nnp
pp
xnx
nxxPxxE
 
And, letting y = x-1 and m=n-1, this becomes 
npppnppp
ymy
mnpxE mymy
m
y
=−+⋅=−−⋅=
−
=
∑ )]1([)1(]![! !)( 0  
Accordingly, the initial error rate εinitial becomes: 
 
 
 
Therefore, the initial error rate analytically equals to the original error rate during a data 
loading process. Therefore, without cleaning and syntactic validation, after data has been 
selected, extracted, projected, or subset, the post error rate is statistically equivalent to the 
original error rate when the volume of the loaded data is very large. 
Step II: Determining the start error rate: 
A DW validation process should consider reliability problems, that is, Type I errors and 
Type II errors. This research demonstrates the work of validation control process by 
employing a single validation control loop from Cushing’s models. With the probabilities P(e) 
and P(s) representing Type I and Type II error respectively, a probability tree of error rate 
after the work of the validation process is illustrated in Figure 5. The post error rate ε for each 
field in a DW can be obtained as: 
Figure 4: The CDF of the error rate distribution after a loading process 
Number of errors in P (X) 
Probability 
P(x) 
xi 
pi 
[2] 
   
[1]n
xE
initial
)(=ε
p
n
np
initial ==ε
7 
 
Suppose that there are n fields A[] = [A1, A2, A3,…and An] in a star schema with sizes 
of ri, r2, r3, … and rn data items and error rates ε1, ε2, ε3,… and εn respectively with impact 
costs c1, c2, … and cn associated with the n data fields, then the relative criticality index for 
A[i] can be written as: 
 
 
 
Then the accuracy value ℜ is defined as the cost-effective believability regarding the 
accuracy level of the data on the view. Therefore, the prioritized error rate, εweighted, can be 
expressed as: 
 
 
 
Then according to [3], the ℜ can be further formulated as: 
 
 
 
 
 
4. Establishing data quality control (DQC) procedure 
model 
Without control action, data quality management is incomplete. In addition to the 
proposed quality indication method, this research establishes an empirical quality control 
procedure model (DQC) for standardizing and institutionalizing the quality control action for 
data warehouse applications. 
4.1  Standardization vs. institutionalization 
Most organizations have standard processes to ensure best practices are there. However, 
it is institutionalization that ensures such best practices can be truly and stably carried out 
under different circumstances, e.g. different people, different project sizes, or under different 
stress of time. With proper institutionalization practices, it would help reduce process 
variation of DW data quality control. 
This research inherits the concept of generic practice from CMMI (SEI, 2007). CMMI is 
a process reference model that is used for managing software development processes. 
However, the institutionalization concept and framework is applicable to other practices due 
to its generalization and process-focus nature. 
ωi = ci / Σ A[i]  for data field A[i] [5] 
/ ( Σ ci ωi  ri)       
     Σ  [(1 – εinitial)(P(e)) + εinitial (1 - P(s))] ci ωi ri)    i=1 
n 
= 1 – ( [7] 
ℜ = 1 –  εweighted 
9 
 
5. A Case Study 
To demonstrate the proposed work, a case study is conducted and is presented in this 
section. For the purpose of simplicity and ease of understanding, this article provides only a 
cursory presentation of the study characteristics and findings.  
CM is a motor dealership company that sells cars for domestic and imported cars. The 
company would like to have an end-of-year sale on a particular luxury SUV model. So CM 
prepares very fine advertisement brochures regarding the SUV and mail to each of the 
selected potential customers. CM employs a data warehousing company, MIT Service 
Corporation to prepare and analyze the expected potential customer list. 
The DW project imports 100,000 local resident records from 2 external data sources, S1 
and S2. Data source S1 maintains numeric (e.g. age, annual income, mortgage level, et al.) data 
and S2 maintains qualitative data (e.g. name, address, occupation, education, et al.) of the 
residents. The initial error rates for S1 and S2 are provided by the data supplier and are 0.003 
and 0.019 respectively. The historical record regarding the performance of the validation 
processes shows a value of 0.00081 for P(e) and 0.00051 for P(s). The DW process is 
indicated in Figure 4, and the star schema is shown in Figure 6. 
 
 
 
 
 
 
 
 
5.1  The institutionalized DW DQC process 
MIT has a standard DQ-control (DQC) process that controls the quality of the data on the 
schemas. DQC includes the following tasks: (1) validating the initial data quality information, 
which is to ensure the creditability of the DQ information provided by the sources; (2) 
establishing criticality indices and identifying critical data fields, which is to realize the loss 
for the DW customer resulting from erroneous data in the critical fields; (3) computing and 
evaluating the ℜ value, which is to objectively understand the quality level of the entire fact 
table or star schema; and (4) taking quality control actions, which is to engage proper  QC 
tasks according to the criticality of the quality levels.  
Moreover, in order to maintain the stability of the DQC process performance, the 
company applies several generic practices from CMMI (SEI, 2007) and instantiates several 
generic practices for institutionalizing and managing the DQC processes. The practices are: 
(QGP-1) establishing the incentive and audit policies; (QGP-2) planning and tailoring, 
depending on the size e.g. the volume of data  of the DQC process each time for a DW 
project (Sen et al., 2006); (QGP-3) providing resources for performing the DQC process; 
(QGP-4) assigning responsibilities; (QGP-5) training people that are capable of performing 
the DQC job; (QGP-6) managing configuration of the data resulting from each DQC job; and 
(QGP-7) auditing the DQC process. Table 2 on the next page illustrates such an 
institutionalized DW DQC process. 
The Star Schema
SSN Salary Age # of Children SSN Name Address 
Data 
Source 1 
Data 
Source 2 
Figure 6: The data-join example in the case study 
Name Address Salary Age # of Children 
11 
 
Table 3: DQ Control Strategies 
Criticality 
index 
DQ Control Action Required  Resources
Acceptable 1. Do nothing N/A 
Marginal 2. Identifying new associative relationships 
3. Manual inspection with sampling method on the critical fields 
4. Value imputation for missing data 
5. Run the DC validation tools again 
Software tools, 
statistical tools 
Risky 6. Identifying new associative relationships 
7. Manual walkthrough on the critical fields 
8. Value imputation for missing data 
9. Edit check, imputation techniques 
10. Run the DC validation tools again 
The internship, 
software tools, 
statistical tools 
In the task of evaluating the quality level of the entire fact table, this study compares the 
values resulting from the proposed method and the traditional measure. They are described as 
follows.  
The objective of the DW project is to provide quality data for the car dealer to determine 
if a resident is qualified (i.e. has a higher chance to buy the vehicle) by looking at the field 
“Salary”. After potential customers are identified, the dealer sends out and mails 
advertisement packages including a dedicated brochure with the discount coupon for each 
qualified customer. The cost (including the material costs and the postage) of mailing an 
advertisement package is about 2 US dollars, and the revenue of selling a SUV is 20,000 US 
dollars. The past experiences show that qualified (pre-selected) customers (including anyone 
who obtains the coupon from the pre-selected customer) have a chance of 0.3 that will buy the 
luxury SUV. 
Case I: The existing way 
If the impact cost is not considered, then the error rate (ε), i.e. the accuracy value of 
the data on the star schema, according to [4] and Vassiliadis’s expression, is:  
(100000*2*0.019+ 100000*3*0.003) / (100000*2 + 100000*3) = 0.0094. 
Case II: Considering amplifying the errors on the critical data 
Suppose an error in the “address” field would lead to the return of the package, which 
loses the chance of the potential sale (0.3) and the waste of the advertisement cost and the 
postage. Therefore, the expected impact cost for a wrong address is:  
20,000*0.3 + 2 = 6,002 
13 
 
6. Discussion  
Several management insights are obtained with the proposed data quality measure and 
the case study: 
1. It reflects the actual criticality of a dataset. In this example, both cases have the 
same dataset, but the proposed measurement method indicates a worse quality condition 
(0.014152 in Case II). The reason for this is that the measure considers business impact for 
each data field. In this simple example, the critical field, for example, “address”, is from data 
source 1, which has higher error rate (0.019). This amplifies the data errors of that field and 
thus the quality value is higher, which truly reflects the actual impact of the dataset. 
 2. It leads to an effective focus on the right (critical) data. Finding and correcting errors 
in a dataset is effort-consuming work, particularly when the dataset is large and the company 
has limited resources for performing data quality improvement. Suppose the company 
determines whether or not to perform data improvement by a threshold level of 0.01, then in 
this case, the value 0.0094 of Case I would mislead to a wrong decision (i.e. to ignore the 
errors). Besides, the results (Case II and Table 1) from the proposed measure would help 
relevant stakeholders identify critical data fields. 
3. It helps reduce the loss of revenue due to the false signal. In a case when errors are 
located in the critical fields (e.g. Address, and Income in this example), the actual need of 
quality improvement may be ignored due to the false signal (0.0094, suppose the risk control 
threshold is 0.01). As a result, potential business revenue are not earned due to this kind of 
Type-I error (Error of omission). On contrary, if the proposed measure indicates an 
unacceptable value, then it is relatively more reliable to make such a decision of taking 
improvement efforts due to cost–effective concern. 
4. It helps avoid the waste of business resources due to the false signal. In another case 
when errors are located in the uncritical fields (e.g. Name and Age in this example) and the 
traditional measure indicates an unacceptable value, then DQ improvement would be 
triggered. As a result, efforts are spent on trivial errors and business resources would be 
wasted on detecting and correcting trivial errors due to this kind of Type-II error (Error of 
redundancy). On contrary, if the proposed measure indicates an acceptable value, then it is 
relatively more reliable to make such a decision of ignoring the errors and saving unnecessary 
DQ improvement efforts due to cost–effective concern. 
5. The process-centric measure indicates critical stages of error rate evolution. 
Another way to obtain the quality level on the final table (fact table) is to directly sample and 
calculate the error rates for the fields. By doing so we are not able to know where and which 
critical process component that contribute the data errors. This is particularly true when the 
data generation process is complicated. With the proposed process-centric quality measure, 
15 
 
[2] Ballou, D.P. and Tayi, G.K. (1999). Enhancing DQ in DW Environment, 
Communications of the ACM, 42(1), 73-78. 
[3] Ballou D.P. and Pazer, H. (1985). Modeling Data and Process Quality in Multi-Input, 
Multi-Output Information System, Management Science, 31(2), 150-162. 
[4] Bowen, P. et al., (1998). Continuously Improving Data Quality in Persistent Databases, 
Data Quality Journal, 4(1). 
[5] Chen, C.Y. et al. (2005). An Object Oriented Quality Framework with Optimization 
Models for Managing Data Quality in Data Warehouse Applications, International 
Journal of Operations Research, 2(2), 1-8. 
[6] Chen, Z. (2002). Intelligent Data Warehousing: From Data Preparation to Data Mining, 
CRC Press: FL, USA. 
[7] Cushing, B. (1974). A Mathematical Approach to the Analysis and Design of Internal 
Control Systems, The Accounting Review, January, 24-41. 
[8] English, L. (1999). DW and Business Information Quality. John Wiley & Sons Inc.: New 
York. 
[9] Eckerson W. (2002). Data quality and the bottom line, Application Development Trends, 
May 2002. 
[10] Firth C.P. (1996). Data quality in practice: experience from the frontline, The 1996 
Information Quality Conference, Massachusetts Institute of Technology, October 25-26. 
[11] Fisher, C.W. Kingma, E.R. (2001). Criticality of data quality as exemplified in two 
disasters, Information and Management, 39(2), 109-116. 
[12] Gardner, S.R. (1999) Building the Data warehouse, Communications of the ACM, 
41(9), pp.52-60. 
[13] Guynes, C.S. (1996). Evolving DQ Considerations for Client/Server Environments, Data 
Quality Journal. 2(1), 21-27. 
[14] Hamlen, S. (1980). A Chance-Constrained Mixed Integer Programming Model for 
Internal Control Design, The Accounting Review, Vol. LV, No. 4, October, 578-593. 
[15] Hatcher, D.  (2003). Sharing the Info Wealth, ComputerWorld, July 2, 2003. 
[16] Hernandez, M. and Stolfo, S. (1998). Real-World Data is Dirty: Data Cleansing and the 
Merge/Purge Problem, Data Mining and Knowledge Discovery, Vol.2, pp.9-37.  
[17] Huang, H.T., Lee, Y.W. and Wang, R (1999). Quality Information and Knowledge, 
Prentice Hall Inc.   
[18] Huh, Y.U. et al. (1990). Data Quality, Information and Software Technology, 32(8), 
559-565. 
[19] InduShobha, C.S. et al. (1999). The Impact of DQ Information on Decision Making: An 
Exploratory Analysis, IEEE Transactions on Knowledge and Data Engineering, 11(6), 
853-864. 
[20] Inmon, W.H. (1996). Building the Data Warehouse. John Wiley & Sons Inc.: New York, 
USA. 
17 
 
[39] Vassiliadis, P. et al. (2000). Toward quality-Oriented Data Warehouse Usage and 
Evolution, Information Systems, 25(2), 89-115. 
[40] Wiederhold, G. (1992). Mediators in the Architecture of Future Information Systems, 
IEEE Computer, March, 38-49. 
[41] Wang, R. (1998). A product perspective on total data quality management, 
Communications of the ACM, 41(2), 58-65. 
[42] Wang, R. et al. (1998). Manage your information as a product, Sloan Management 
Review, 39(4), 95-105. 
[43] Wang, R. et al. (2001). Data Quality. Massachusetts: Kluwer Academic Publishers: 
Boston. 
[44] Wang, R., Stroey, V., and Firth, C. (1995). A framework for analysis of data quality 
research, IEEE Transactions on Knowledge and Data Engineering, 7(4), 623-640. 
[45] Wand, Y. and Wang, R. (1996). Anchoring data quality dimensions in ontological 
foundations, Communications of the ACM, 39(11), 86-95. 
[46] Yu, S. and Neter, J. (1973). A Stochastic Model of the Internal Control System, Journal 
of Accounting Research, Autumn, 273-295.  
 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2628-E-008-076-MY2 
計畫名稱 資料品質之研究：建立一個能反映資料錯誤對於決策實際影響程度之品質量測方法 
出國人員姓名 
服務機關及職稱 
陳仲儼 
會議時間地點 新加坡 
會議名稱 IEEE International Conference on Industrial Engineering and Engineering Management 
發表論文題目 Organizing event-based, meeting-oriented project group communication holistically: a preliminary study 
 
一、參加會議經過 
 
個人此次參與的國際研討會(IEEE International Conference on Industrial Engineering 
and Engineering Management)屬 EI。會議期間是 12/3~5 日，由新加坡國立大學主辦。在
會議中發表論文。並參加了第一天的開幕大會，以及中午的學術宴會，收穫豐碩。以下
的貼圖即為本人研究報告的場次： 
 
