and their variations. 
 
一.緣由與目的 
The objective of rate control at the encoder is 
to maximize the perceptual video quality under a 
given encoding target bit rate. Such adaptive 
encoding may be achieved by the methods of 
either adjust quantization parameters or control 
video frame rate (frame skipping) [6]. By looking 
at most encoders [1-2, 12-13]], it can be seen that 
the only block available for the rate control 
mechanism is to reasonably modify the discrete 
cosine transform (DCT) coefficient quantizer. 
Because the quantization matrix scale may be 
changed on a picture basis or a macroblock (MB) 
basis, encoder rate control algorithms to provide 
dynamic control over the relative buffer fullness 
commonly use these parameters. In this manner, a 
constant bit-rate may be provided by the output of 
the encoder buffer, yet underflow or overflow 
may be prevented without severe quality 
degradation such as repeating or skipping video 
frames. Although rate control algorithms are 
necessary in fixed bit-rate applications, they are 
encoder vendors＇ proprietary, and the subject of 
current research. A list of some of the more 
well-known general algorithms may be found in 
[3, 10-15]. 
Analysis and estimation of the R-D 
functions [7-9], [11] have important applications 
in video coding and communication. First, with 
the estimated R-D functions we can adjust the 
parameters of the model and control the output bit 
rate or frame quality according to the channel 
conditions, the storage capacity, or the user＇s 
requirement. Second, based on the estimated R-D 
functions, bits allocation procedures can be 
performed to improve the efficiency of the coding 
algorithm and, consequently, to improve the 
image quality or video presentation quality. All of 
the R-D models attempt to find the formula 
relating the coding bit rate R and the distortion D 
with the quantization parameter q. A study of the 
rate and distortion as functions of q , denoted by 
( )R q  and ( )D q  , is termed q-domain R-D 
analysis. In the other words, the conventional R-D 
analysis is carried out in the q-domain. 
In the classical R-D formula, the only 
parameter that describes the input source is the 
variance of the source data. It is well known that 
only variance itself is insufficient to characterize 
the input source data and to determine the final 
coding bit rate. In addition, the analytic entropy 
formula does not take into account the coding 
behavior of a specific coding algorithm. Since the 
analytic entropy formula does not work well in 
practical coding applications, more sophisticated 
rate formulas have been developed and used in the 
literature and applied to rate control for video 
coding applications. To estimate the rate function 
more accurately, some new models have also been 
proposed. However, it's  not difficult to find that 
in order to improve the rate estimation accuracy 
for coding algorithm, the rate models are          
becoming very complex , so that with complex 
and highly nonlinear expressions, the estimation 
and rate control process becomes very 
complicated and even unstable with the image 
dependent variations. It should also be noted that, 
for different coding algorithms, the q-domain R-D 
models and its rate control algorithms are 
heterogeneous to each other. It would be better to 
develop a simple and accurate rate model for 
transform coding systems. 
In the transform coding of images and 
videos, it is observed that the percentage of the 
zeros among the quantized DCT coefficients, 
denoted by ρ , has a critical effect on the coding 
bit rate. Note that, if we assume the distribution of 
The bit amount allocated for a VOP is 
usually determined by the weighting summation 
of the number of macroblocks, the motion vector 
and the square of the mean absolute difference of 
the VOP. The three weights are used to adjust the 
relative emphasis of the associated parameters in a 
regression manner. In contrast, our proposed bit 
allocation uses the a factor defined as the sum of 
the MB number and the mean absolute difference 
(MAD) to assign bit amount for a VOP. The 
number of macroblocks in a VOP changes as an 
object zooms in or zooms out. In other words, the 
object size proportionally determines the bits 
count required for a VOP. On the other hand, the 
magnitude of the motion vector of a VOP does not 
necessary proportionally determine the bit count 
required for the VOP. This is argued as follows. 
In the frame/object edges, poor motion 
compensation results in large bit counts. In the 
case when an object is purely translated, good 
motion compensation results in small bit counts. 
In general, the user is more interested in the 
moving foreground objects in the scene and 
assigns more bits to code these objects with higher 
fidelity. This can be realized by using a weighting 
for each object to indicate its relative importance. 
Obviously, the objects of interest should have 
relatively larger weights to guarantee that they are 
coded with less distortion. It's also worth to notice 
that when the size of each object is distinctly 
different, we used to give higher weights for 
smaller objects, especially for foreground parts. 
With this consideration, the distortion sensitivity 
can be reduced (ie. the smaller object allocates but 
it's easy to introduce obvious distortion because of 
suddenly occurrence variance.).  
In a stationary video sequence (e.g. Akiyo 
sequence), object size and MAD are more fixed 
for all VOPs in a GOP. On the other hand, in a 
motion sequence, object size (e.g. Stefan sequence) 
usually enlarges or reduces continuously. 
Conventional bit allocation methods usually fix bit 
count ratio among objects during the period of a 
GOP. This VO first method tends to make object 
quality disproportional when object size changes 
greatly in a GOP. To comeback this drawback, we 
proposed a hybrid bit allocation method as shown 
in Fig. 2. 
 
 
Figure 2 Flowchart of the Proposed Hybrid 
Bit Allocation 
 
In Fig. 2, two paths instead of one path are 
considered. If the object size change is larger than 
a threshold value, the sequence is considered 
dynamic and the frame first path is taken. On the 
other hand, if the object size change is smaller 
than the threshold, the sequence is considered 
stationary and the VO first path is taken. Figs. 3 
bit rate, the qualities are smoother. 
 
Table 1 PSNR and Standard Deviation of the 
VM18 and the Proposed Method 
 
  “Stefan＂is a sequence of quite complexity. 
Therefore we need higher coding rate to avoid 
excessive frames skipping. Experiments with 
target bit rates over 500kbps was performed. 
Table 2 shows that more stable (less buffer 
variance) results are obtained for higher target bit 
rates. It should be noted that with the same 
amount of data bits, our method coded more 
frames than VM18, yet still produced much higher 
average PSNR in most cases. The improvement of 
our proposed bit allocation strategy becomes more 
noticed while the object size changes greatly 
during the last 100 frames. It should be noted that 
the PSNR of our method sometime might be 
lower than of the VM18. This is due to the fact 
that more frames are coded by our method than by 
VM 18. 
 
 
Figure 5 Buffer Fullness vs. Frame Number 
(800kbps) 
 
Figure 6 Bit Rate vs. Frame Number (800kbps) 
 
Figure 7 (a) Foreground PSNR vs. Frame Number 
(800kbps) 
 
