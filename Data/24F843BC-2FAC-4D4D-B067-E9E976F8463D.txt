 1 
行政院國家科學委員會補助專題研究計畫 
■ 成 果 報 告   
□期中進度報告 
 
模型式 Java 程式測試工具的研製（三） 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 99－2628－E－194－006－ 
執行期間：99 年 08 月 01 日至 100 年 07 月 31 日 
 
計畫主持人：林迺衛 
共同主持人： 
計畫參與人員：簡志瑋、蕭世杰、柯廷諭、林志遠 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
 
中   華   民   國  99 年  10 月  29 日 
 3 
testing activity usually takes about half of the cost of 
the entire software process. This high cost of the 
software testing activity is the main reason to hinder 
the achievement of high quality software. 
Many software testing techniques have been 
developed during the last several decades. However, 
most of these software testing techniques are still 
performed manually at present. This is why the cost 
of the software testing activity is so high. The 
automation of the software testing activity should be 
able to significantly reduce the cost of the software 
testing activity. Many research works have strived to 
develop software testing tools to automate the 
software testing activity as much as possible. 
However, the available software testing tools are still 
very primitive at present. There is still a long way to 
go to achieve the full automation of the software 
testing activity. 
Software testing is a systematic attempt to find 
faults in the software systems. A failure of a software 
system is any deviation of the observed behavior from 
the specified behavior. A fault, or defect, or bug, of a 
software system is the cause of a failure of the system. 
A test case is a pair of input and expected output that 
can be used to detect faults by revealing failures 
caused by faults. The goal of software testing is to use 
a suite of test cases to maximize the number of faults 
detected in order to assure the quality of the software 
systems as high as possible.  
Software testing consists of the design of test 
cases and the execution of test cases. There are two 
types of techniques for designing test cases: 
black-box testing (or functional testing, or 
specification-based testing) and white-box testing (or 
structural testing, or program-based testing) [8]. The 
black-box testing techniques are based on the 
functional specification of a software system and 
focus on the coverage of the specified external 
behaviors of the software system. The white-box 
testing techniques are based on the source code and 
focus on the coverage of the internal structure of the 
source code. These two types of testing techniques 
are complementary. The white-box testing techniques 
alone may fail to reveal that some portions of the 
specification are missed in the source code. On the 
other hand, the black-box testing techniques alone 
may fail to reveal that some portions of the source 
code are not contained in the specification. Therefore, 
employing both types of testing techniques is 
necessary to assure high quality software. 
Model-based testing is a form of black-box 
testing [13]. A model is a description of the generic 
structure and meaning of a software system. A model 
is always an abstraction of the software system that 
captures the essential aspects of the system and 
ignores some of the details [36]. In model-based 
testing, the suite of test cases is derived from the 
models that specify the behaviors of the software 
systems. This derivation of test cases is expected to 
be performed automatically or semi-automatically. 
Model-driven software development has been 
increasingly adopted by industry. Model-based testing 
should soon become the prevailing testing technique 
in industry. 
The Unified Modeling Language (UML) is a 
general-purpose visual modeling language that can be 
used to specify, visualize, construct, and document 
the artifacts of a software system [36]. The UML has 
been widely used as the functional specifications of 
object-oriented software systems. The UML provides 
several types of diagrams to model different aspects 
of software systems. For example, use case diagrams 
are used to specify the functional behaviors of the 
system from the viewpoint of users. Class diagrams 
are used to describe the static structure of the system 
in terms of objects, classes, attributes, methods, and 
their associations. Activity diagrams are used to 
formalize the dynamic behaviors of a method in a 
class by specifying the logical relationships between 
the input and output of the method. State diagrams are 
used to formalize the dynamic behaviors of an 
individual object of a class by specifying a number of 
states and transitions between these states. Sequence 
diagrams are used to formalize the dynamic behaviors 
of a group of objects of different classes by 
specifying the interactions among these objects. The 
UML diagrams have also been widely used to 
facilitate the analysis, design, and testing of 
object-oriented software systems [9]. The UML also 
provides a constraint language, Object Constraint 
Language (OCL), to enable the more detailed 
functional specifications by using constraints on 
elements in the UML diagrams [27]. 
In model-based testing, test cases, which consist 
of pairs of input and expected output, are generated 
iteratively in the following three steps. First, a test (or 
execution) path is generated from the model. Each 
test path covers a sub-domain of the input. This step 
is implemented as a test path generator. Second, the 
satisfiability of the set of logical predicates appearing 
on the test path is checked. If the set of logical 
predicates is satisfiable, this test path is feasible. In 
this case, a solution of the set of logical predicates is 
solved and acts as a representative input from the 
sub-domain. Otherwise, the test path is infeasible and 
is abandoned. This step is implemented as a test data 
generator. Third, the expected output with respect to 
the input is determined. This step is implemented as a 
test oracle. These three steps are repeated until the set 
of generated test cases satisfies a specific test 
coverage criterion. Most model-based testing tools 
focus on test path generation. The jobs of test data 
generators and test oracles are still usually performed 
manually by programmers or testers. 
There are many possible test coverage criteria for 
model-based testing: all-node, all-edge, all-definition, 
all-use, and all-path [8, 16]. The all-node coverage 
criterion requires that the set of test cases covers all 
nodes in the model. The all-edge coverage criterion 
requires that the set of test cases covers all edges in 
 5 
providers. Operating under an open source paradigm, 
with a common public license that provides royalty 
free source code and world-wide redistribution rights, 
the Eclipse platform provides tool developers with 
ultimate flexibility and control over their software 
technology. Eclipse provides an extensible framework 
for building tools. Tool builders contribute to the 
Eclipse platform by wrapping their tools in pluggable 
components, called Eclipse plug-ins, which conform 
to Eclipse‘s plug-in contract. We will develop our 
testing tool as Eclipse plug-ins.  
This project also aims to provide high 
interoperability between this tool and other tools. 
This project adapts the widely used modeling 
language UML to model software systems and adapts 
the widely used data exchange language XML to 
represent models that can be exchanged among 
different modeling tools [28]. 
The testing activity in model-based testing 
consists of model construction, test case generation, 
and test case execution, as shown in Figure 1. This 
project uses the EclipseUML to construct the UML 
models [25]. EclipseUML is an Eclipse plug-in. This 
project uses the JUnit framework to automate test 
case execution [5]. JUnit is also an Eclipse plug-in. 
This project has developed a test case generator to 
automatically generate test cases. This test case 
generator is also implemented as an Eclipse plug-in. 
The test cases are generated based on UML models. 
The test cases will be classified into three categories: 
test cases for unit testing of a single method, for unit 
testing of objects of a single class, and for integration 
testing of a group of objects of multiple classes. This 
project aims to develop a testing tool that supports all 
three categories of test cases using a unified approach. 
All three categories of test cases are generated based 
on UML diagrams and OCL. We will first use UML 
class diagrams to specify the interfaces of classes and 
the interfaces of methods in each class. The unit 
testing of a single method is based on UML activity 
diagrams and OCL. The unit testing of objects of a 
single class is based on UML state diagrams and OCL. 
The integration testing of a group of objects of 
multiple classes is based on UML sequence diagrams 
and OCL. This is shown in the Table 1. 
Table 1. The modeling language of the testing tool. 
Method 
Level Unit 
Testing 
Class Level 
Unit 
Testing 
Integration 
Testing 
activity 
diagrams 
and OCL 
state 
diagrams 
and OCL 
sequence 
diagrams 
and OCL 
 
For each category of test cases, the test case 
generation consists of three components: a test path 
generator, a test data generator, and a test driver 
generator, as shown in Figure 2. The test path 
generator repeatedly selects a test path from the 
model until the selected test paths satisfies a specific 
coverage criterion. The test data generator constructs 
the set of logical predicates on the selected test path 
and solves the set of logical predicates using the 
constraint logic programming system ECLiPSe. If the 
set of logical predicates is satisfiable, then each of the 
solutions provides a pair of test input and expected 
Model Construction 
(EclipseUML) 
 
Test Case Generation 
(Our Testing Tool) 
 
Test Case Execution 
(JUnit) 
 
UML Models 
Test Cases 
Figure 1. The testing activity in model-based testing. 
 
 7 
of integer elements. The variable stack represents the 
sequence of integer elements in the stack. The type 
Sequence is an abstract ordered-bag collection type 
that can be implemented using a suitable Java 
collection type. The constructor Stack() creates a 
Stack object with zero element. The method 
push(int element) pushes an element element into 
the stack. The method pop() pops an element out of 
the stack. The method peek() returns the top element 
of the stack. The method size() returns the number of 
elements in the stack. The methods can be divided 
into two categories: update and query methods. The 
update methods like push and pop will change the 
state of the object. The query methods like peek and 
size only return the status of the state of the object. 
The OCL specification for the methods in the 
class Stack is given as follows: 
 
context Stack::Stack() 
post: stack = Sequence{} and size = 0 
 
context Stack::push(element: Integer): Stack 
post: stack = stack@pre->prepend(element)  
and size = size@pre + 1  
and result = self 
 
context Stack::pop(): Stack 
pre: size > 0 
post: 
stack = stack@pre->subSequence(2, size@pre) 
and size = size@pre – 1  
and result = self 
 
context Stack::peek(): Integer 
pre: size > 0 
post: result = stack->first() 
 
context Stack::isEmpty(): Boolean 
post: result = (size = 0) 
 
context Stack::size(): Integer 
post: result = size 
 
context Stack 
inv: size >= 0 
 
In our testing tool for the unit testing, these OCL 
specifications are transformed into CLP predicates. 
These predicates can be used to generate test input 
and expected output at the same time. For the class 
Stack, the OCL specifications are transformed into 
the following CLP predicates. Each stack object is 
represented as a term, stack(Stack, Size), where 
Stack is a list containing the elements of the stack 
and Size is the size of the stack. 
The class PostfixExpr implements postfix 
expressions of digits. The method evaluate() will use 
a stack to evaluate the value of a postfix expression of 
digits. The class diagram for the class PostfixExp is 
given as follows: 
PostfixExp 
exp: String 
PostfixExpr(str: String) 
evaluate(): Integer 
 
The specifications for the methods of class 
PostfixExpr can be given using OCL as follows: 
 
context PostfixExp::PostfixExp(str: String) 
pre: isValidPostfixExp(str) 
post: exp = str 
 
context PostfixExp::evaluate(): Integer 
post:  
result = exp->iterate( 
ch: String; stack: Stack = Stack() |  
if Sequence{„0‟,„1‟,„2‟,„3‟,„4‟,„5‟,„6‟,„7‟,„8‟,„9‟} 
->includes(ch) then 
stack.push(StringToInteger(ch)) 
else  
let opr1: Integer = stack.peek(),  
s1: Stack = stack.pop(), 
opr2: Integer = s1.peek(),  
s2: Stack = s1.pop() 
in  
if ch = „+‟ then s2.push(opr1 + opr2) 
else if ch = „-‟ then s2.push(opr1 - opr2) 
else if ch = „*‟ then s2.push(opr1 * opr2) 
else s2.push(opr1 / opr2) 
endif endif endif 
endif)->peek() 
 
The OCL specification for the constructor is 
transformed into the following CLP predicate: 
 
postfixExp_PostfixExp(Str, postfixExp(Str)). 
 
Since the behavior of a method invoked on an 
object depends on the state of the object, the 
specification of interactions among objects needs to 
specify the complete state changes before the current 
state for all objects involved in the interaction. 
Sequence diagrams can be used for such 
specifications. The sequence diagram for the 
evaluation of postfix expressions of digits is given in 
Figure 3. This diagram specifies the whole life of a 
PostfixExp object exp and a Stack object stack. 
4. THE TEST PATH GENERATOR 
The test path generator consists of two steps. 
First, it reads in a sequence diagram and its 
corresponding OCL specification. Second, it then 
enumerates all the possible test paths in the sequence 
diagram. 
The enumeration schemes of test paths depend on 
the test coverage criterion chosen. The test coverage 
criterion can be control-flow based: all-decision, 
all-condition, or all-decision-condition, or data-flow 
based: all-define, all-use, or all-define-use.  
 
 9 
In this article, all-decision test coverage criterion is 
used. 
Based on the sequence diagram, the test path 
generator repeatedly generates test paths in the 
breadth-first fashion. For example, there are 10 
feasible test paths that go through the loop construct 
only once and go through the alternative for digits in 
Figure 3. For example, one of these feasible test paths, 
path1, is as follows: 
 
PostfixExp(str),  
Stack(),  
str = {ch1},  
ch1 = „0‟,  
stack.push(0),  
stack.peek(). 
 
Similarly, there are many feasible test paths that 
go through the loop construct three times in Figure 3. 
They must go through the alternative for digits in the 
first two times and go through the alternative for 
operators in the third times. For example, one of these 
feasible test paths, path2, is as follows: 
 
PostfixExp(str),  
Stack(),  
str = {ch1, ch2, ch3},  
ch1 = „1‟,  
stack.push(1),  
ch2 = „2‟,  
stack.push(2),  
ch3 = „+‟,  
opr1 = stack.peek(),  
s1 = stack.pop(), 
opr2 = s1.peek(),  
s2 = s1.pop(), 
s2.push(opr1 + opr2),  
stack.peek() 
 
Note that although there are many test paths that 
go through the loop construct twice in Figure 3, these 
test paths are infeasible test paths. For example, one 
of these infeasible test paths, path3, is as follows: 
 
PostfixExp(str),  
evaluate(),  
Stack(),  
str = {ch1, ch2},  
ch1 = „1‟,  
stack.push(1),  
ch2 = „+‟,  
opr1 = stack.peek(),  
s1 = stack.pop(), 
opr2 = s1.peek(),  
s2 = s1.pop(), 
s2.push(opr1 + opr2),  
stack.peek() 
 
The feasibility of a test path is determined by the test 
data generator. 
5. THE TEST DATA GENERATOR 
The test data generator generates a sequence of 
arguments for the sequence of method invocations in 
each test path passed from the test path generator. 
The sequence of arguments is generated by solving 
the set of constraints on the test path. This set of 
constraints is first transformed into ECLiPSe 
predicates and these predicate are then solved by the 
powerful constraint solving capability of the ECLiPSe 
system. 
The powerful constraint solving capability of the 
ECLiPSe system can be illustrated by the following 
example. 
 
%Include constraint solving library 
:- lib(ic). 
 
foo(A, B, C) :- 
% Domains of variables 
[A, B, C] :: 1 .. 3, 
% Constraints on variables 
A #= B + C, 
% Solving constraints 
labeling([A, B, C]). 
 
where ic is the library for solving constraints on 
finite domains. The ECLiPSe system supports several 
other libraries. Based on the unification mechanism of 
logic programming, the predicate foo/3 can be 
queried with any number of variable arguments. For 
example, the predicate foo/3 can be queried using any 
of foo(3, 1, 2), foo(3, 1, C), foo(3, B, C), foo(A, B, 
C), and so on. Namely, each argument can be either 
used as an input when it is a constant or used as an 
output when it is a variable. The unification 
mechanism allows us to solve the test input and 
expected output simultaneously. By specifying the 
domains of variables, the constraint solving library 
can find all the solutions satisfying the constraints in 
the predicate. For the example, the ECLiPSe system 
will return 
A = 3, 
B = 1, 
C = 2, 
and 
A = 3, 
B = 2, 
C = 1, 
as solutions for the query foo(A, B, C). 
For each method method in the class class, the 
test data generator will convert its postcondition into 
an ECLiPSe predicate classMethod(OState, 
Arguments, Result, NState). The argument OState 
is the state of the object right before the method 
invocation. The state consists of the list of attribute 
values of the object. The argument Arguments is the 
list of arguments to the method invocation. The 
argument Result is the returned value of the method 
 11 
Generation and Execution of Test Suites for 
DIstributed Component-based Software project 
(AGEDIS) sponsored by the European Commission 
from 2000 [1]. The project is headed by the IBM 
Research Laboratory in Haifa, and contains academic 
partners at the Oxford University Computing 
Laboratory, the Verimag laboratory at Universite 
Joseph Fourier in Grenoble. The major industrial 
partners are France Telecom R&D, the IBM 
development Laboratory in Hursley Park (UK), and 
Intrasoft International with headquarters in 
Luxembourg and Athens. The AGEDIS project has 
developed a suite of tools: a graphical interface, a 
model simulator, a test case generator, a test executor, 
a coverage analyzer, and a defect analyzer. The test 
case generation of the AGEDIS is based on model 
checking [19]. The results of the AGEDIS 
experimentation with these tools indicate that they 
still have a long way to go before these tools are 
ready for widespread adoption by industry. 
There have been many other research works on 
model-based test case generation. The majority of 
these works is based on state diagrams or finite state 
machines [1, 12, 15, 18, 20, 23, 26, 31, 38]. A few 
works are based on sequence diagrams [14, 35]. 
However, these works only addresses automatic test 
path generation. The automatic generation of test 
input and expected output is still rarely addressed. 
The works on automatic generation of test input 
are mostly on white-box testing [17, 21, 22, 30]. 
Their approaches can also be used on black-box 
testing. The problem of automatic generation of test 
input can be formulated as the constraint satisfaction 
problem. Each of these works proposes a specific 
approach to solving the constraint satisfaction 
problem in a specific domain. A more complete 
constraint solving system is required to handle a large 
variety of programs. Recently, there are a few works 
that have applied constraint logic programming to 
automatically generate test input [25, 34]. They use a 
more complete constraint solving system, the 
constraint logic programming system ECLiPSe, to 
generate test input [4].  
There are two approaches for automatic 
generation of expected output. One approach is based 
on executable specification or modeling languages. 
One work that has addressed the automatic generation 
of expected output is based on the Abstract State 
Machine Language (AsmL) [5]. The AsmL is an 
executable specification language that combines the 
Abstract State Machine paradigm and an algebraic 
specification language. The models specified in 
AsmL can be automatically transformed into finite 
state machines. The language xtUML is a subset of 
UML that is executable [24]. The xtUML interpreter 
can also be used as a test oracle if the xtUML is used 
as a specification language. Another work that has 
addressed the automatic generation of expected 
output is based on an algebraic specification language, 
the Java Modeling Language (JML) [10, 11]. The 
pre-conditions and post-conditions of each method 
are specified. The test input is provided by the user. 
The post-conditions are used to generate expected 
output with respect to the test input. The JML uses 
Java syntax and is described as comments in the Java 
code. This restricts the JML to be used only for Java 
programs. Another approach for automatic generation 
of expected output is based on neural networks [2, 
39]. This approach uses the learning algorithms of 
neural networks to model the input and output 
relationships of a software system. This model can 
then be used to generate an approximate expected 
output. 
8. CONCLUSION 
Automatic generation of input data and expected 
output is the most difficult tasks in software testing 
activity. The automation of any task depends on the 
formal specification of the task. This article uses 
UML sequence diagrams and the Object Constraint 
Language as a formal language to specify the 
dynamic behavior of a group of different Java objects. 
This article uses the powerful constraint solving 
capability of constraint logic programming to 
automatically generate test input and expected output 
for test cases. This approach can generate the test 
input and expected output simultaneously. This article 
shows that the integration of UML sequence diagrams, 
OCL, and CLP makes the design and implementation 
of an automatic testing tool for Java classes possible. 
REFERENCES 
[1] AGEDIS – Automatic Generation and Execution 
of Test Suites for Distributed Component-based 
Software. http://www.agedis.de/. 
[2] K. K. Aggarwal, Y. Singh, A. Kaur, and O. P. 
Sangwan, ―A Neural Net Based Approach to 
Test Oracle,‖ ACM SIGSOFT Software 
Engineering Notes, 29(4): 1-6, 2004. 
[3] A. V. Aho, R. Sethi, and J. D. Ullman, 
Compilers: Principles, Techniques, and Tools, 
Addison Wesley, 1986. 
[4] K. R. Apt and M. G. Wallace, Constraint Logic 
Programming Using ECLiPSe, Cambridge 
University Press, 2007. 
[5] M. Barnett, W. Grieskamp, L. nachmanson, W. 
Schulte, N. Tillmann, and M. Veanes, ―Towards 
a  Tool Environment for Model-Based Testing 
with AsmL,‖ Proceedings of the Third 
International Workshop on Formal Approaches 
to Testing of Software, Quebec, Canada, LNCS 
2931, 252-266, 2003. 
[6] K. Beck and C. Andres, Extreme Programming 
Explained, 2nd Edition, Addison Wesley, 2004. 
[7] K. Beck and E. Gamma, JUnit Cookbook, 
http://junit.sourceforge.net/. 
[8] B. Bezier, Software Testing Techniques, 2nd 
Edition, Van Nostrand, 1990. 
[9] M. Blaha and J. Rumbaugh, Object-Oriented 
Modeling and Design with UML, 2nd Edition, 
Pearson Prentice Hall, 2005. 
 13 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：林迺衛 計畫編號：99-2628-E-194-006- 
計畫名稱：模型式 Java 程式測試工具的研製 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
