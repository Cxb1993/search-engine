 II
Abstract 
 
 Randomness has become a useful resource in computer science. However, random sources we have 
access to are usually imperfect. We say that a weak source has (statistical) min-entropy b if every string 
occurs with probability at most 2-b. From such weak sources, we would like to use a deterministic 
function, called an extractor, to extract some almost perfect randomness. During the past decade, the 
research about extractors for various kinds of sources has received much attention, and obtains many 
important results. 
 
First, we construct a fingerprint authentication system based on the fuzzy extractor. Furthermore, we 
use list decodable codes to raise the identification rate of the system. 
 
Next, we consider the task of deterministically extracting randomness from sources consisting of a 
sequence of n independent symbols from {0,1}d. The only randomness guarantee on such a source is that 
the whole source has min-entropy b. We give an explicit deterministic extractor which can extract almost 
all randomness from an independent-symbol source as long as logcb n≥ , for some constant c>0. 
Moreover, we show the existence of a non-explicit deterministic extractor which can extract 
( )( )log 1/m b ε≥ −Ο  bits whenever ( )( )log /b d nω ε= + , where ε is the error. Finally, we show that even 
to extract from bit-fixing sources, any extractor, seeded or not, must suffer an entropy loss 
( )( )log 1/b m ε− = Ω . This generalizes a lower bound of Radhakrishnan and Ta-Shma with respect to 
general sources.  
 
Finally, we consider the following definition of computational min-entropy. For a function f, we say 
a source (f(X)|X) has computational min-entropy b if for any Boolean circuit C with size complexity 2b, 
Prx[C(X)=f(X)]<2-b. We show that we can extract randomness from two sources with computational 
min-entropy.  
 
 
 
 
Keywords: extractors, deterministic extractors, fuzzy extractors, biometric authentication, bit-fixing 
sources, independent-symbol sources, upper bound on entropy loss, lower bound on entropy loss, 
computational min-entropy. 
 1 
一、 研究計畫之背景 
I. 弱隨機源(Weak random sources) 
隨機性(Randomness)已被證實在資訊科學(computer science)的許多領域中都是非常有用
的。舉個例子來說，在密碼學上，我們常使用隨機性來隱藏秘密，以避免在傳輸的過程中被
第三者竊聽而洩露資訊。此外，在許多情況下，使用隨機演算法(probabilistic algorithm)
比我們已知的傳統決定性演算法(deterministic algorithm)在時間以及空間的複雜度(time 
and space complexity)上來得更有效率[MR95, Gol98]。而這些隨機性的演算法都需要使用
所謂的”真正的隨機元(truly random bits)”。不幸地，在真實的世界中並不存在真正的隨機
元，因此，我們只能退而求其次的使用一個決定性的函數(deterministic function)從一些
可取樣的弱隨機源(weak random sources)中萃取出真正的隨機元，其中，弱隨機源所能保證
的事只有,任一個字串出現的機率都不會非常高。如果我們說一個弱隨機源的 min-entropy 為
b 則表示每個字串出現的機率都不會超過2 b− ，直觀地我們認為這樣的弱來源”包含” b 個隨機
元。下面是一些弱隨機源的例子，這些例子都是分布在長度為 n 的字串，且”包含” 長度為 b < 
n 的隨機性，我們稱這樣的來源為(n,b)-source。 
 馬可夫鏈的來源(A Markov-chain source): 
為一個在 { }0,1 n 上的分布 ( )1, , nX X X= ⋯ ，其中 1, , nX X⋯  是由在一個轉變矩陣
(transition matrix)為 ( )ijP p= 的 Markov-chain 上走 n 步所獲得的，而其中
,    1ij
b bi j p
n n
∀ < < − 。此種來源可參考[Blu84]。 
 無法預測的來源(An unpredictable source): 
為一個在{ }0,1 n 上且滿足下列性質的分布 ( )1, , nX X X= ⋯ ， { }1 11  and , , 0,1ii n x x −∀ ≤ ≤ ∈⋯ ， 
[ ]1 1 1 1Pr 1| , , 1 .i i ib bX X x X x
n n
− −
< = = = < −⋯  
換句話說，對每一個位元而言，即使已經知道之前的位元，亦幾乎無法猜測現在的位元會
為何。此種來源可參考[SV86]及[CG88]。 
 固定某些位元的來源(A bit fixing source): 
為一個在{ }0,1 n 的分布 ( )1, , nX X X= ⋯ ，其中 n-b 個位元是固定的，而其餘的 b 個位元則
是均等的(uniform)且彼此獨立的(independent)。此種來源請參考[CGH+85]及[CW89]。 
 
而我們的目的在於設計出可以從上述的例子以及其他的弱隨機源中萃取出隨機性的萃取器
(extractor)，則我們即可利用萃取出的隨機元來執行隨機演算法。接下來我們會考慮一般的弱隨
機源，而把上述的弱隨機源當作是特例。 
1988年時，Chor 等人證明不存在一個決定性的函數(deterministic function)可以從單一個
其 min-entropy < n 弱隨機源中萃取出一個隨機元[CG88]。由這個觀察留下三個可以從弱隨機源
中萃取真正隨機元的方向: 第一、不考慮一般的弱隨機源，而考慮一些條件更加嚴格的弱隨機源，
 3 
 種子長度的下限 
每 一 個 1 2ε ≥ 的 non-trivial (b, ε )- 種 子 萃 取 器 ， 他 的 種 子 長 度
( ) ( ) ( )log 2log 1 1d n b ε≥ − + −Ο 。 
 Entropy損失(entropy loss)的下限 
所謂的 entropy 損失指的是 b + d – m 的值，亦即，在萃取的過程中所損失的
隨機性的個數。Radhakrishnan 等人證明每一個(b,ε )-種子萃取器不可能萃取出所有
輸入的隨機性，而且 entropy損失為 ( ) ( )2log 1 1ε −Ο 。 
在近幾十年來，國內外的理論學家們都致力於建造出使用最少種子，而能從各種弱隨機源中萃
取出非常靠近真正隨機元的種子萃取器。我們可以發現由剩餘雜湊引理所得到的強萃取器，其
entropy 損失是最佳的 ( ) ( )( )2log 1 1m b d ε= + − − Ο ，然而他需要很長的種子 ( )d n= Ο 。下面我們
列舉兩類滿足最佳參數的萃取器，為了方便起見，我們將固定誤差ε 為一個非常小的常數。 
 最少的種子長度 
從上述種子長度的下限中，我們得知一個最佳的種子萃取器其種子長度為
( ) ( )log 1d n b= − + Ο 。而目前我們已知明確的造法其種子長度為 ( )logd n= Ο 。 
當 ( )b n= Ω 時，Zuckerman 的萃取器[Zuc97]需要 ( )logd n= Ο 個種子，可以萃取
( )1m bδ= − 個隋機元，其中 0δ > 可以是任何常數。而 Ta-Shma 等人[TSZS01]的萃取
器則減少種子長度為 ( )log log logd n n= + Ο ，可萃取 ( )m b= Ω 個隨機元。 
如果現在我們放寬 b 的限制，使 b 可以為任何小於 n 的數，則最好的結果為
Reingold 等人[RSW00]的萃取器，其使用需要 ( )logd n= Ο 個種子，可萃取 logm b n=
個隨機元。 
 最大的輸出長度 
一個最佳的種子萃取器其輸出長度為 ( )1m b d= + −Ο 。而目前我們已知明確的造
法其輸出長度為 ( )1m b d= + − Ο 。 
當 ( )( )1 1b nο= − 時，Reingol 等人的萃取器[RVW00]需要 ( ) ( )1logd n bΟ= − 個種
子，可以萃取 ( )1m b d= + −Ο 個隨機元。 
如果現在我們放寬 b 的限制，使 b 可以為任何小於 n 的數，則最好的結果為
 5 
為了克服這個問題，他們提出一個新的物件，稱為”種子獲取器(seeded obtainer)”。 
簡單的說，種子獲取器是一個將原本的固定某些位元的來源轉成兩個輸出字串 X’和 Y 的一個
函數，其中 X’和 Y必須滿足下列條件: 
 X’是一個(n, b’)-固定某些位元的來源，而且 'b b≈ 。 
 Y是一串短的真正隨機元。 
 X’和 Y是獨立的。 
Gabizon 等人是將原本 Kamp 萃取器轉成種子獲取器後將固定某些位元的來源 X 轉成一個
min-entropy 幾乎維持不變的固定某些位元的來源 X’，以及一個短的種子 Y，由於種子獲取器的特
性，我們知道 X’和 Y是獨立的，因此利用 X’和 Y執行種子萃取器，即可萃取出幾乎所有的隨機性。
圖一即為 Gabizon 等人建造方式的簡圖。 
 
 
IV. 從兩個甚至多個弱隨機源中萃取隨機元 
 從兩個弱隨機源的萃取器(Two-sources-extractors) 
1988 年，Chor 等人[CG88]率先利用 Hadamard matrix (其為一個任兩行和任兩列都互相垂直
的 1± 矩陣)造出一個，只要兩個弱隨機源的長度一致( 1 2n n n= = )，且其 min-entropy 的和超過它
們的長度( 1 2b b n+ > )，即可以從這兩個弱隨機源萃取出很靠近真正隨機元的兩個弱隨機源的萃取
器(two-sources-extractor)，。但是此種方法只能萃取出一個隨機元，亦即輸出隨機元的長度為
1(m=1)。 
在2003年時，Dodis等人[DO03]在原本Hadamard matrix的架構上又加入了線性除錯碼(linear 
error correcting codes)的概念將原本只能萃取一個隨機元地缺點改進為可以萃取出多個隨機
X 
(n,b) 
KZ 
extractor 
Seed 
obtainer 
X’ 
(n,b’) 
Y 
(seed) 
X’ & Y indep. 
Seeded 
extractor 
Almost all the 
randomness 
of X’ 
圖一、從固定某些位元的來源中萃取出幾乎所有隨機性的方法 
 7 
somewhere-condenser 以及兩個弱隨機源的萃取器。在介紹 somewhere-condenser 前，我們必須先
介紹 somewhere random source。一個 ( )1:, kn b -source 是一個隨機變數 ( )1, , kX X X= ⋯ ，其中每一
個 iX 的長度均為 n，且至少有一個 iX 的 min-entropy b≥ ；換句話說，在 1, , kX X⋯ 中至少有一個是
(n,b)-source。下面我們接著介紹 Raz 造法的元件，以及如何將這些元件合成起來，得到一個新的
多弱隨機源萃取器。 
簡單的說，somewhere-condenser 的功能為將一個弱隨機源，轉成一個具有很高 min-entropy 
rate 的 somewhere random source，其中 somewhere-condenser 的造法是基於[BIW04]造多弱隋機
源萃取器的造法加以改變所得。至於兩個弱隨機源的萃取器，則使用我們上述所提的[Raz05]兩個
隨機源萃取器。首先先將從第一個弱隨機源 ( )1 1,n nδ -source 中取樣的 1x 送進 somewhere-condenser 
C，我們令其輸出為 ( ) ( ) ( )( )1 1 11 , , kC x C x C x= ⋯ ，接著對所有 2 1i k≤ ≤ + ，將 ( )1 1iC x − 以及從第 i 個
弱隨機源取樣的 ix 送進兩個弱隨機源的萃取器 iE ，最後再將此 k 個結果做互斥或即為最後的輸出。
我們將此造法的簡圖列於圖二。 
 
 
V. 應用 
在探討了萃取器的歷史背景以及重要的造法後，我們也列出一些萃取器的主要應用。在理論學
上，BPP 等不等於 P 是一個大家極感興趣的問題，因此如何減少 BPP 所需的隨機元就成了一個重
要的目標，這就是有名的解隨機 BPP(Derandomized BPP)問題。Zuckerman 證明可以用一個具有足
夠 min-entropy 的弱隨機源去模擬 BPP[Zuc91]，考慮一個計算函數 f(x)的 BPP演算法(一個多項
C 
C(x1)1 
C(x1)k
1 
…… 
E2 
Ek 
x1 
x2 
xk 
⊕ 
…
…
 
圖二、 Raz 多個弱隨機源萃取器的建造略圖 
 9 
 
 
 
 
 
在 2004 年時，Dodis 等人利用萃取器造出模糊萃取器(fuzzy extractors)，並將之應用到生
物認證上[DRS04]。模糊萃取器＜Gen, Rep＞為一個隨機函數，其中Gen可從一個生物認證的輸入(如
指紋)w 萃取出一個隨機值 R 並輸出一個公開值 P (亦即， ( ) ,Gen w R P= )，而 Rep 則保證將來只
要輸入一個跟原來的 w 很靠近的一個 w’，就可以利用 P 將 w’還原成 w 再重新產生 R（亦即，
Rep( ', )w P R= ）。如此即可避免在傳統的認證系統上，使用者必須輸入與原來設定絲毫不差的密碼
才能通過認證的問題。圖五與圖六分別表示傳統的密碼認證法以及使用模糊萃取器的認證法的簡
圖。 
然而，Dodis 等人所建造的模糊萃取器在使用多次之後，就會有洩露資訊的危機。為此，Boyen
造出更為安全的模型，並將之應用於一個有第三方檢定的遠端生物認證協定(a remote biometric 
authentication protocol with third party certification)，在此協定中，使用者僅需記住她
的一開始所萃取出的隨機值Ｒ，即可透過一個可靠的第三者向遠端的另一使用者認證自己的身分，
此認證行為可重複多次且不需擔心有洩露資訊的危機[Boy04]。 
圖四  在不安全的通道中建立秘密金鑰的協定 
 11 
 節省隨機性(Randomness-efficient)的未覺察到的取樣(oblivious sampling)以及
隨機演算法成功機率的決定性擴大(deterministic amplification)[Nis96, Zuc97, 
NTS99]。 
 無條件的建造偽亂數產生器(pseudorandom number generator)使得對空間限制下的
隨機演算法來說很難分辨偽亂數產生器的輸出與真正的亂數[NZ96, Nis96, NTS99]。 
 用來建造深度為 2 的 superconcentrator，以及 expander graphs(其擴張性質有時
甚至比使用特徵值方法所造出的 expander graphs 更強)[Nis96, WZ99, NTS99]。 
 用來建造擁有很強的列表式編解碼(list decoding)性質的除錯碼(error correcting 
codes)[TSZ01]。 
 另一個不同的証明BPP PH⊆ 的方法[GZ97]。 
 
 
二、 研究目的 
I. 建造模糊萃取器 
密碼認證系統是最常見的認證系統。然而由於使用者設定之密碼的亂度不足，可能導致密
碼系統的安全性不如預期。因此，近年來大家開始關注生物認證，如虹膜、指紋或手型等。
Dodis 等人提出由模糊萃取器來擷取生物特徵，並將之應用到生物認證上[DRS04]。本計劃以
模糊萃取器為基礎，建構一個指紋認證系統。 
 
II. ”廣義的”固定某些位元來源: 
我們發現固定某些位元的來源以及多個隨機源的來源可以下列觀點看成是兩種極端。這兩
種來源均包含多個部分而且這些部份彼此都是獨立的。固定某些位元的來源可看成是包含許
多個部份，且每個部分只有單一個隨機或者為固定的位元。而多個隨機源的來源則可看成包
含相對少數個部份，可是每個部份為多個位元且含足夠數量的隨機元。本計劃考慮一個介於
其中的來源，稱為獨立符號的來源(independent-symbol source)，寫成 (n,D,b)-來源，其
為一個在[D]
n
 ([D]={1,2,…,N}) 上的分布 ( )1, , nX X X= ⋯ ，其中 1, , nX X⋯ 是彼此獨立的，
且 X 的 min-entropy 是 b。 我們不難看出一個(n, b)-固定某些位元的來源其實就是一個
(n,2,b)-來源。換句話說，固定某些位元的來源只是我們所考慮的獨立符號來源的一個特例。
而對較小的 n 及較大的 D，此種來源即可涵蓋多個隨機源的來源。關於其他範圍的 n和 D之
結果，我們所之極少，因此在本計劃中，我們將考慮從此種來源中萃取出隨機元，亦即我們
將建造針對此種來源的萃取器。 
為了得知我們所建造的決定性萃取器的優劣，我們並證明決定性萃取器之 entropy損失的
存在性的上限。此外為了證明上述的上限是嚴格的(tight)，我們亦證明一個與之成對的針對
固定某些位元之來源的萃取器其 entropy損失的下限。 
 
III.Computational min-entropy: 
在本計劃中，我們將由以往傳統的統計架構轉為計算性架構。我們考慮那些可能並不含任
何 statistical min-entropy但在計算複雜度有限的觀察者看來有點隨機的來源，並試著從
 13
接著，我們使用機率方法來證明 entropy 損失的存在性的上限。亦即，我們隨機地產生
一個不需種子的萃取器，並且證明有大於零的機率，它可以應用於所有我們考慮的獨立符號
來源。更精確地說，針對每一個來源，我們證明一個隨機選的函數只有很小的機率會無法從
此來源中萃取出隨機元。然而此種來源有無限多個，所以我們接著證明，事實上，我們只需
考慮一小群的來源即可，因為任何我們所考慮的來源都很靠近此群來源的 convex 
combination。任何在此群的來源其性質為對每個 i∈[D]，Xi 是一個幾乎均等(almost flat)
的分布，且其 min-entropy 的值只侷限於很少的可能值。  
為了證明相對的 entropy 損失的下限，給定任何一個函數 { } { } { }: 0,1 0,1 0,1n s mExt × → ，其
中 ( )( )log 1/m b s o ε≥ + − ，我們使用機率方法證明存在一個 min-entropy 為 b 的固定某些位元
來源，使得當 Ext應用於此來源上時，其誤差一定會超過ε。我們產生一個來源的方法為隨
機挑 n-b 個隨機元，並且固定此 n-b 個隨機元為某個定值，而剩下的 b 個位元即為真實的隨
機元。我們的困難點主要在證明任何此種 Ext 都有大於零的機率無法從隨機挑選出來的來源
中萃取出隨機元。此種機率跟某種”幾乎” t-wise independent space(亦即在大部份的
t-dimension 上此分布很靠近均等分布)的大小有關。我們證明一個此種空間大小的下限，由
此我們即可推出 entropy 損失的下限。詳細證明請參考附錄。 
III.Computational min-entropy 
由於我們已知兩個弱隨機源 U 和 V 的內積可以萃取隨機元[CG88、DO03、LLTT05]，因此
我們希望能證明即使當兩個隨機源均只擁有 computational min-entropy 時，其內積依然可
以萃取出隨機元。我們發現此問題跟學習理論(learning theory)中一個重要的問題有關。在
學習理論中，如何經由一些可能有誤的訓練範例(training example)學習一個 parity 函數
(parity function)是一個基本的問題，並有許多相關結果 [BKW03、FGKP06、KMV08]。我們
將設計一個可由一些可能有誤的訓練範例學習一個線性函數(linear function)的演算法，並
利用此演算法造出針對只擁有 computational min-entropy 的來源之萃取器。 
 
 
四、 研究計畫之結果與討論 
I. 建造模糊萃取器 (此部分結果為畢業論文[Lai05]) 
我們建造一個指紋認證系統其包含一個可處理此生物數據的模糊萃取器。我們分別使用單一
編解碼(unique decodable code)及列表法編解碼(list decodable code) 作為建造模糊萃取器所
需的除錯碼(error correcting codes)。 
本計劃中，我們假設已存在一個可以幫助我們從指紋圖中萃取出生物數據的方法。事實上，
此方法亦在認證系統中扮演一個重要的角色。因此未來可以考慮如何設計一個可以萃取出足以反
應指紋特徵之生物數據的方法。 
 
II. ”廣義的”固定某些位元來源 (此部分結果已發表[LLT06]，原文請見附錄) 
 首先，我們建造出一個從 min-entropy 為 b 的獨立符號來源中萃取出大約 logb個隨機元
 15
取器亦可以從一個擁有 computational min-entropy 為 b 的隨機源與另一個獨立的擁有
computational min-entropy為某個 ( )n b o b− + 的來源中萃取出m個隨機元，使得對任何size 
complexity 為 ( )2n b o b− + 的 Boolean circuit 看來，此 m 個隨機元為真實的隨機元。 
 
 
 17
properties: A quality-size trade-off for hashing. Random Structures & 
Algorithms, 11(4):315-343, 1997. 
[HLR07] C. Y. Hsiao, C. J. Lu, and L. Reyzin. Conditional computational entropy, or 
toward separating pseudoentropy from compressibility. In Proc. Advances in 
Crptology- EUROCRYPT, 169-186, 2007. 
[ILL89] R. Impagliazzo, L. A. Levin, and M. Luby. Pseudorandom generation from one-way 
functions. In Proceedings of the 21
st
 ACM Symposium on Theory of Computing, 
1989. 
[KMV08] A. Kalai, Y. Mansour and E. Verbin. On agnostic boosting and parity learning. 
In Proc. 40
th
 STOC, 629-638, 2008. 
[KZ03] J. Kamp and D. Zuckerman. Deterministic extractors for bit-fixing sources and 
exposure-resilient cryptography. In Proceedings of the 44
th
 Annual IEEE 
Symposium on Foundations of Computer Science, 2003. 
[Lai05] C. M. Lai. Authentication using fuzzy extractor with list decodable codes. 
Master Thesis, National Chiao Tung University, 2005.  
[LLT06] C. J. Lee, C. J. Lu, and S. C. Tsai. Deterministic extractors for 
independent-symbol sources. In Proc. 33
rd
 ICALP, 84-95, 2006.  
[LLTT05] C. J. Lee, C. J. Lu, S. C. Tsai, and W. G. Tzeng. Extracting randomness from 
n independent weak random sources. IEEE Transactions on Information Theory 
(SCI), 51(6) 2224-2227, 2005. 
[MR95] R. Motwani and P. Raghavan. Randomized algorithms. Cambridge University press, 
1995. 
[Nis96] N. Nisan. Extracting randomness: How and why: A survey. In Proceedings 11
th
 
Annual IEEE Conference on Computational Complexity, 44-58, 1996. 
[NTS99] N. Nisan and A. Ta-Shma. Extracting randomness: A survey and new constructions. 
JCSS, 58, 1999. 
[NZ96] N. Nisan and D. Zucherman. Randomness is linear in space. Journal of Computer 
and System Sciences, 52(1):43-52, 1996. 
[Raz05]  R. Raz. Extractors with Weak Random Seeds. Proceeding of the 37th STOC, 2005, 
pp. 11-20. 
[RSW00] O. Reingold, R. Shaltiel, and A. Wigderson. Extracting randomness via repeated 
condensing. In Proceedings of the 41
st
 Annual IEEE Symposium on Foundations 
of Computer Science, 2000. 
[RTS00] J. Radhakrishnan and A. Ta-Shma. Tight bounds for depth-two 
superconcentrators. SIAM Journal on Discrete Mathematics, 13(1):2-24, 2000. 
[RVW00] O. Reigold, S. Vadhan, and A. Wigderson. Entropy waves, the zig-zag graph 
product, and new constant-degree expanders and extractors. In Proceedings 
of the 41
st
 Annual IEEE Symposium on Foundations of Computer Science, 2000. 
[Sha02] R. Shaltiel. Recent Developments in Explicit Constructions of Extractors. 
Bulletin of the EARCS 77: 67-95, 2002. 
 19
計畫成果自評 
一、假設已存在一個可以幫助我們從指紋圖中萃取出生物數據的方法，我們建造一個指紋認證系統
其包含一個可處理此生物數據的模糊萃取器，並使用列表法編解碼(list decodable code) (在
此我們使用的是 Reed-Solomon Codes) 作為建造模糊萃取器所需的除錯碼(error correcting 
codes)來提升其復原能力。此外，由於從指紋圖中萃取出生物數據的方法在認證系統中扮演一
個非常的重要的角色，因此未來可以考慮如何設計一個可以萃取出足以反應指紋特徵之生物數
據的方法。 
二、我們發現一個新的特殊隨機源，稱為獨立符號來源，其可以不靠種子的幫助而萃取出隨機元。
我們觀察出延伸[KZ03]及[GRS04]中針對固定某些位元來源萃取出隨機元的方法，即可建造出
針對獨立符號來源的決定性萃取器。然而值得注意的是，[KZ03]中的方法僅適用於固定某些位
元的來源，而無法應用於獨立符號來源，即使是在 D=2的情況底下。 
三、在[BIW04]中，他們利用”加”與”乘”這兩個運算子來增加目的來源的 min-entropy。然而在我
們建造第一個萃取器的過程中，我們發現，其實只靠”加”即可增加 min-entropy。但是，這種
方式所增加的 min-entropy量較少，所以相較於[BIW04]中只需常數個隨機源即可萃取出隨機
元，我們需要很多的隨機源才能做到。不過，我們的方法也許可以當作將來建造多個隨機源萃
取器的基礎。 
四、建造萃取器的目標為所萃取出的隨機元個數要越多越好 (換句話說，即為減少 entropy 損
失)，這引發一個新的問題: 針對一個獨立符號隨機元，我們最多能夠從中萃取出多少個隨機
元。為此，我們證明了 entropy 損失的上限。由此 entropy 損失的上限，我們發現所建造的
決定性萃取器並非最佳的，故未來我們將繼續思考如何建造出 entropy損失最少的決定性萃取
器。 
五、我們亦證明一個相對於 entropy損失上限的 entropy損失下限。藉此，我們可以證明上述所得
到的上限是一個嚴格的上限。在證明的過程中，我們得到一個”幾乎” t-wise independent 
space大小的下限。由於此種 ”almost” t-wise independent space很類似標準的 approximate 
t-wise independent space (即在每個 t-dimension 上此分布均很靠近均等分布) ，只
是 ”almost” t-wise independent space 的條件更為放寬。所以，此大小的下限即為
approximate t-wise independent space 的一個下限。 
六、由計算的角度，我們證明[LLTT05]所提出的萃取器亦可從一個擁有 statistical min-entropy
的隨機源及一個只擁有 computational min-entropy 的隨機源中萃取出隨機元。我們證明的方
法與學習理論 (learning theory)中由一些可能有誤的訓練範例(training example)學習一個
線性函數(linear function)的演算法有關，而設計此種演算法本身也是一種有趣的問題。 
七、更進一步地，我們證明[LLTT05]中的萃取器也可以作用於兩個只擁有 computational 
min-entropy 的隨機源。 
 
 
  
 
Deterministic Extractors for Independent-Symbol Sources 85
extractor which extracts a string with distribution close to uniform. However,
it is well known that one cannot deterministically extract even one bit from
an n-bit source with min-entropy n − 1 [6]. In contrast, it becomes possible if
we are allowed a few random bits, called a seed, to aid the extraction. Such
a procedure is called a seeded extractor. During the past decades, a long line
of research has worked on using a shorter seed to extract more randomness
(e.g. [20,19,22,10,24,29,27,26]), and recently an optimal (up to constant factors)
construction has been given [17].
The problem with a seeded extractor is again to get a seed which is perfectly
(or almost) random. For some applications, this issue can be taken care of (for
example, by enumerating all possible seed values when the seed is short), but for
others, we are back to the same problem which extractors are originally asked to
solve. This motivates one to consider the possibility of more restricted sources
from which randomness can be extracted in a deterministic (seedless) way.
One line of research studies the case with multiple independent sources. The
goal is to have a small number of independent sources with a low min-entropy
requirement on sources, while still being able to extract randomness from them.
With two independent sources, the requirement on the min-entropy rate (average
min-entropy per bit) stayed slightly above 1/2 for a long time [6,8], but this
barrier has been broken by a recent construction which pushes the requirement
slightly below 1/2 [5]. The requirement on min-entropy rate can be lowered to
any constant when there are a constant number of independent sources [3], and
the number of sources has recently been reduced to three [4].
The other line of research considers the case of bit-ﬁxing sources. In an obliv-
ious bit-ﬁxing source, each bit is either ﬁxed (containing no randomness) or
perfectly random, and is independent of other bits. From such a source of length
n with min-entropy n1/2+γ , for any constant γ ∈ (0, 1/2), Kamp and Zucker-
man [13] gave a seedless extractor which can extract Ω(n2γ) bits of randomness.
Building on this result together with some new idea, Gabizon et al. [9] were
able to extract even more randomness. In particular, when the source has min-
entropy k > n1/2+γ , they can extract k − n1/2+γ bits and when k > logc n for
some constant c, they can extract k − kΩ(1) bits.
Note that the two lines of research discussed above can be seen as belonging
to two extremes of a spectrum in the following sense. Sources in both cases
consist of multiple parts which are mutually independent. In the ﬁrst case, one
usually has in mind sources with relatively few parts while each part is long and
contains a substantial amount of randomness. In the second case, a bit-ﬁxing
source consists of many parts, while each part is only a single bit which is either
random or ﬁxed. We would like to put both cases in the same framework and
study sources that lie in between these two extremes.
We consider the following more general class of sources, characterized by the
parameters n, d, k ∈ N. Each source in the class consists of n mutually indepen-
dent parts, each of length d, and the whole source has min-entropy k. For small
n and large d, this covers sources of the ﬁrst type, while for large n and d = 1,
this covers sources of the second type. For other ranges of n and d, very little
Deterministic Extractors for Independent-Symbol Sources 87
sources and even allowing a seed of length s, any extractor can only extract
k+s−Ω(log(1/ε)) random bits. That is, even to extract from bit-ﬁxing sources,
any extractor, seeded or not, must suﬀer an entropy loss of Ω(log(1/ε)). This
generalizes the result of Radhakrishnan and Ta-Shma [23], which has the same
bound for seeded extractors on general sources.
Our techniques. Our ﬁrst extractor, which extracts about log k bits, was inspired
by that of Kamp and Zuckerman [13], but our approach is quite diﬀerent. Instead
of taking a random walk on an odd cycle, we walk on the group ZM for a prime
M . More precisely, given a source X = (X1, . . . , Xn), we see each Xi as an
element of ZM and outputs X1 + · · · + Xn over ZM . As in [13], we will show
that each step of our walk brings the distribution closer to uniform when the
symbol from the source contains some randomness. However, even for the case
of d = 1, we cannot use the analysis from [13], which is based on bounding the
second eigenvalue of the transition matrix for a perfectly random step on a cycle.
This is because we may walk in a highly biased way as each bit of our source
can have an arbitrary bias. Our proof is very diﬀerent and elementary, and has
the following interesting point. The recent breakthrough construction of multi-
source extractors [3] and its subsequent works all relied on using both sums and
products to increase entropy. Our analysis shows that in fact even doing sums
alone can increase entropy. The increase, however, is slower, so we need a larger
number of sources (as opposed to a constant number in [3]).
Then we apply the technique of [9] to extract more randomness. Our two
extractors generalize the corresponding ones in [9]. The only diﬀerence is that
we deal with a more general classes of sources, do a more careful analysis, and
use our ﬁrst extractor instead of that in [13] as a building block.
Our existential upper bound on entropy loss is proved via a probabilistic
argument. That is, we generate a seedless extractor randomly, and show that it
works for all of our sources with a positive probability. For each source, we can
show that it fails with a small probability. However, the number of all possible
sources is in fact inﬁnite. Nevertheless, we show that it suﬃces to consider only
a small set of sources, since any source is close to a convex combination of
them. Sources in this set are those with the property that their distributions
in each dimension are “almost ﬂat” and have only a small number of possible
min-entropy values.
Our lower bound proof of entropy loss follows the outline of that in [23].
Namely, given any function Ext : {0, 1}n × {0, 1}s → {0, 1}m with m ≥ k +
s − o(log(1/ε)), we show the existence of a bit-ﬁxing source with min-entropy
k on which the error of Ext exceeds ε, again using a probabilistic argument.
We generate a source by randomly picking n− k bits from the source and ﬁxing
them to some random values; the remaining k bits are left free and given a
uniform distribution. The diﬃcult part is to show that any such Ext fails on
such a randomly chosen source with a positive probability. This probability turns
out to be related to the size of some “almost” t-wise independent space, whose
distribution is close to random on most sets of t dimensions. This can be seen as
a relaxation of the standard notion of approximate t-wise independent space, in
Deterministic Extractors for Independent-Symbol Sources 89
3 Extractor from Random Walk
In this section, we give an explicit seedless extractor for independent-symbol
sources, which works for any min-entropy k but only extracts about log k bits.
Theorem 1. For any n, k,D ∈ N and any prime number M ≥ D, there is an
explicit (n,D, k, ε)-extractor Ext0 : [D]n → [M ], with ε ≤
√
M · e−k/(8M2 logD).
Proof. We will work on the group ZM , for a prime M , and see any symbol
Xi ∈ [D] of the source as an element in ZM . Throughout this section, operation
+ or − on elements in ZM is understood as an operation over the group ZM .
Our extractor Ext0 : [D]n → [M ] is then deﬁned as Ext0(X ) =
∑
i Xi, which
can be seen as taking an n-step walk on the group ZM , using the n symbols from
the source in the following way. Each time when we are at some state v ∈ ZM
(initially at 0 ∈ ZM ) and read a symbol a from the source, we go to the state
v + a ∈ ZM . The extractor of Kamp and Zuckerman [13] for bit-ﬁxing sources
can be seen as a special case of ours, with D = 2 and Xi ∈ {−1, 1}.
As in [13], we will show that each step of the walk brings the distribution closer
to uniform if the symbol read from the source contains some randomness, but
our analysis is totally diﬀerent. See a distribution over ZM as an M -dimensional
vector in the natural way. Suppose the current distribution is P = (P1, . . . ,PM )
and the next symbol in the source has a distribution β = (β1, . . . , βM ) (let
βi = 0 for D+1 ≤ i ≤ M). Then the next distribution is P¯ = (P¯1, . . . , P¯M ) with
P¯i =
∑
j∈ZM βjPi−j for i ∈ ZM . Let U denote the uniform distribution over
ZM . Let δ = P − U and δ¯ = P¯ − U , i.e., δi = Pi − 1/M and δ¯i = P¯i − 1/M for
i ∈ ZM . The following lemma, to be proved in Section 3.1, shows the progress
we can make after each step.
Lemma 1. ‖δ¯‖22 ≤ ‖δ‖22 · (1 −H∞(β)/(4M2 logD)).
This implies that ‖Ext0(X ) − U‖22 ≤
∏
t∈[n](1 − H∞(Xt)/(4M2 logD))
≤ e−
∑
t∈[n] H∞(Xt)/(4M2 logD). Since the n symbols of the source are indepen-
dent of each other, we have
∑
t∈[n] H∞(Xt) = H∞(X ) = k, so the bound above
becomes e−k/(4M
2 logD). Then by Cauchy-Schwartz inequality, we have the the-
orem. 	unionsq
3.1 Proof of Lemma 1
Note that for i ∈ ZM , δ¯i =
∑
j∈ZM βjδi−j . So ‖δ¯‖22 =
∑
i(
∑
j βjδi−j)
2 =
∑
i
∑
j β
2
j δ
2
i−j +
∑
i
∑
j = βjβδi−jδi− which, using the equality ab = (a
2 +
b2 − (a− b)2)/2 on the second term, equals
∑
j
β2j
∑
i
δ2i−j +
∑
j =
βjβ
∑
i
(δ2i−j + δ
2
i− − (δi−j − δi−)2)/2
=
∑
j
β2j ‖δ‖22 +
∑
j =
βjβ‖δ‖22 −
∑
j =
βjβ
∑
i
(δi−j − δi−)2/2
= ‖δ‖22 −
∑
j =
βjβ
∑
i
(δi − δi+j−)2 /2,
Deterministic Extractors for Independent-Symbol Sources 91
a (non-explicit) seedless extractor for independent-symbol source with entropy
loss O(log(1/ε)).
Theorem 4. Suppose k ≥ c log(Dn/ε) for a large enough constant c. Then there
exists an (n,D, k, ε)-extractor Ext : [D]n → {0, 1}m with m ≥ k −O(log(1/ε)).
The proof is somewhat standard, and due to the space limitation, we only sketch
the idea here. The existence of such an extractor is guaranteed by a probabilistic
argument: we show that a randomly chosen function is an (n,D, k, ε)-extractor
with a positive probability. Our ﬁrst step is to show that for any given (n,D, k)-
source, a randomly chosen function fails on it with a small probability. However,
the number of such sources is inﬁnite. Our next step is to show that it suﬃces
to consider a much smaller class of sources, namely, those (n,D, k)-sources X
with the property that for each i ∈ [D], Xi is an “almost ﬂat” distribution and
H∞(Xi) is a multiple of some number α. This is guaranteed by the fact, which
is our main technical contribution in this section, that any (n,D, k, ε)-source is
close an (n,D, k, ε)-source which can be expressed as a convex combination of
sources with this property. The number of such sources is small, and the theorem
then follows from a union bound.
6 Lower Bound on Entropy Loss
In this section, we show that the existential upper bound on the entropy loss
in Section 5 is tight by giving a matching lower bound. In fact, we show that
even for bit-ﬁxing sources and even allowing a seed, any extractor must suﬀer
an entropy loss of Ω(log(1/ε)).
Theorem 5. Let Ext : {0, 1}n × {0, 1}s → {0, 1}m be an (n, 2, k, ε)-extractor
for bit-fixing sources, with n, s,m ∈ N, k = ω(1), and ε ∈ (0, 1). Then m ≤
k + s−Ω(log(1/ε)).
We will basically follow the proof idea in [23]. Brieﬂy speaking, given any Ext :
{0, 1}n × {0, 1}s → {0, 1}m with m exceeding the bound, we will show the
existence of a bit-ﬁxing source of min-entropy k on which Ext fails, using a
probabilistic argument. Before giving the proof, let us ﬁrst state some deﬁnitions
and lemmas which will be needed. For any z ∈ {0, 1}m, let S(z) denote the set
{x ∈ {0, 1}n : ∃y ∈ {0, 1}s s.t. z = Ext(x, y)}, and we say that z is δ-missed
by X ⊆ {0, 1}n if |Prx∈S(z) [x ∈ X ] − Prx∈Un [x ∈ X ] | ≥ δ. We will rely on the
following lemma from [23].1
Lemma 2. Suppose X is the uniform distribution over a set X ⊆ {0, 1}n with
|X | = 2k, and ‖Ext(X ,Us) − Um‖1 ≤ ε. Then at most 4√ε fraction of z ∈
{0, 1}m can be (2−(n−k)√ε)-missed by X.
1 Note that this lemma does not appear explicitly in [23] but corresponds to Claim
2.7 there, which is stated in a graph-theoretical term and says that any extractor
gives rise to some kind of “slice-extractor”.
Deterministic Extractors for Independent-Symbol Sources 93
tight lower bound on their size is known [2,7], and we would like to apply it to
get our bound. However, there are two diﬃculties in front of us. One is that S
only guarantees some randomness property on most, instead of all, collections of
t dimensions. The other is that the randomness property only guarantees being
close to random instead of perfectly random. We get around these by showing
that for some properly chosen r < t, S embeds many disjoint copies of r-wise
independent spaces.
We say that S is
√
δ-uniform on I ∈ P ([n], t) if the fraction of u ∈ {0, 1}t being
(2−tδ)-biased in SI is at most
√
δ. From the property of S, a Markov inequality
shows that S is not
√
δ-uniform on at most 8
√
δ fraction of I ∈ P ([n], t). By
an average argument, there must exist some J ∈ P ([n], t − r), for some r to
be determined later, such that S is not
√
δ-uniform on J ∪ T for at most 8√δ
fraction of T ∈ P ([n] \J, r). Fix one such set J . We will partition S into subsets
SJ,v = {x ∈ S : xJ = v}, for v ∈ {0, 1}t−r, and show that many of them embed
an r-wise independent space.
Let us focus on the set J¯ = [n] \ J and those subsets T ∈ P (J¯ , r). Let
k = n− t, so |J¯ | = k+ r. Call T ∈ P (J¯ , r) nice if S is √δ-uniform on J ∪T . Call
v ∈ {0, 1}t−r bad for T if (v, w) is (2−tδ)-biased in SJ∪T for some w ∈ {0, 1}r.
Then for any nice T , the fraction of v ∈ {0, 1}t−r bad for T cannot exceed 2r√δ.
Thus, the fraction of v ∈ {0, 1}t−r bad for at least 2r+1√δ fraction of nice T ’s is
at most 1/2. Fix any v ∈ {0, 1}t−r which is bad for at most 2r+1√δ fraction of
nice T ’s, and thus is bad for at most α = 2r+1
√
δ + 8
√
δ fraction of all possible
T ’s in P (J¯ , r). Next, we show that |SJ,v| ≥ 2r(1/δ)Ω(1).
Assume without loss of generality that |SJ,v| < 2r/(6δ) (otherwise, we are
done), which means that 2−r(6δ) < 1/|SJ,v|. Then we have the following.
Claim. Suppose v is not bad for T ∈ P (J¯ , r). Then ∀w ∈ {0, 1}r, Prx∈SJ,v [xT =
w] = 2−r.
Proof. Suppose v is not bad for T , so ∀w ∈ {0, 1}r, |Prx∈S[(xJ , xT ) = (v, w)]−
2−t| ≤ 2−tδ. This implies that |Prx∈S [xJ = v] − 2−(t−r)| ≤ 2−(t−r)δ. Then for
any w ∈ {0, 1}r, Prx∈SJ,v [xT = w] = Prx∈S [(xJ , xT ) = (v, w)]/Prx∈S [xJ = v]
is at most 2−r(1 + δ)/(1 − δ) ≤ 2−r(1 + 3δ) and at least 2−r(1 − δ)/(1 + δ) ≥
2−r(1− 2δ). That is,
∀w ∈ {0, 1}r ,
∣
∣
∣
∣ Prx∈SJ,v
[xT = w]− 2−r
∣
∣
∣
∣ ≤ 2−r(3δ) < 1/(2|SJ,v |). (1)
Consider the 2r probabilities Prx∈SJ,v [xT = w], for w ∈ {0, 1}r, which are
all multiples of 1/|SJ,v|. If they were not all equal to 2−r, there must exist
w,w′ ∈ {0, 1}r such that Prx∈SJ,v [xT = w] < 2−r < Prx∈SJ,v [xT = w′]. Then
the diﬀerence between 2−r and one of these two probabilities must be at least
half their gap, which is at least 1/(2|SJ,v|), a contradiction to condition (1)
above. Therefore, these 2r probabilities must all be equal to 2−r. 	unionsq
From the claim, we next show that SJ,v embeds an r-wise independent space,
which then implies a lower bound on |SJ,v|. We diﬀerentiate two cases ac-
cording to the range of δ. In the ﬁrst case, when δ < 1/(2k)8, we choose
Deterministic Extractors for Independent-Symbol Sources 95
9. A. Gabizon, R. Raz, and R. Shaltiel. Deterministic extractors for bit-ﬁxing sources
by obtaining an independent seed. FOCS’04, pp. 394–403.
10. R. Impagliazzo, R. Shaltiel, and A. Wigderson. Extractors and pseudo-random
generators with optimal seed length. STOC’00, pp. 1–10.
11. S. Jukna. Extremal Combinatorics. Springer-Verlag, 2001.
12. J. Kamp, A. Rao, S. Vahan, and D. Zuckerman. Deterministic extractors for small-
space sources. STOC’06, to appear.
13. J. Kamp and D. Zuckerman. Deterministic extractors for bit-ﬁxing sources and
exposure-resilient cryptography. FOCS’03, pp. 92–101.
14. R. Ko¨nig and U. Maurer. Generalized strong extractors and deterministic privacy
ampliﬁcation. In Proc. Cryptography and Coding, pp. 322–339, 2005.
15. C.-J. Lee, C.-J. Lu, S.-C. Tsai, and W.-G. Tzeng. Extracting randomness from
multiple independent sources. IEEE Transactions on Information Theory, 51(6),
pp. 2224–2227, 2005.
16. C.-J. Lu. Encryption against storage-bounded adversaries from on-line strong ex-
tractors. J. Cryptology, 17(1), pp. 27–42, 2004.
17. C.-J. Lu, O. Reingold, S. Vadhan, and A. Wigderson. Extractors: Optimal up to
constant factors. STOC’03, pp. 602–611.
18. J. Naor and M. Naor. Small-bias probability spaces: eﬃcient constructions and
applications. SIAM J. Comput., 22(4), pp. 838–856, 1993.
19. N. Nisan and A. Ta-Shma. Extracting randomness: A survey and new construc-
tions. J. Comput. Syst. Sci., 58(1), pp. 148–173, 1999.
20. N. Nisan and D. Zuckerman. Randomness is linear in space. J. Comput. Syst. Sci.,
52(1), pp. 43–52, 1996.
21. R. Raz. Extractors with weak random seeds. STOC’05, pp. 11–20.
22. R. Raz, O. Reingold, and S. Vadhan. Extracting all the randomness and reducing
the error in Trevisan’s extractors. STOC’99, pp. 149–158.
23. J. Radhakrishnan and A. Ta-Shma. Bounds for dispersers, extractors, and depth-
two superconcentrators. SIAM J. Discrete Math., 13(1), pp. 2–24, 2000.
24. O. Reingold, R. Shaltiel, and A. Wigderson. Extracting randomness via repeated
condensing. FOCS’00, pp. 12–14.
25. R. Shaltiel. Recent developments in explicit constructions of extractors. Bulletin of
the European Association for Theoretical Computer Science, 77, pp. 67–95, 2002.
26. R. Shaltiel and C. Umans. Simple extractors for all min-entropies and a new
pseudo-random generator. FOCS’01, pp. 648–657.
27. A. Ta-Shma, C. Umans, and David Zuckerman. Loss-less condensers, unbalanced
expanders, and extractors. STOC’01, pp. 143–152.
28. A. Ta-Shma and D. Zuckerman. Extractor codes. STOC’01, pp. 193–199.
29. L. Trevisan. Extractors and pseudorandom generators. JACM, 48(4), pp. 860–879,
2001.
30. S. Vadhan. Constructing locally computable extractors and cryptosystems in the
bounded-storage model. J. Cryptology, 17(1), pp. 43–77, 2004.
31. A. Wigderson and D. Zuckerman. Expanders that beat the eigenvalue bound:
Explicit construction and applications. Combinatorica, 19(1), pp. 125–138, 1999.
32. D. Zuckerman. General weak random sources. FOCS’90, pp. 534–543.
33. D. Zuckerman. Simulating BPP using a general weak random source. Algorithmica,
16(4/5), pp. 367–391, 1996.
34. D. Zuckerman. Randomness-optimal oblivious sampling. Random Structures and
Algorithms, 11, pp. 345–367, 1997.
一、 參加會議經過 
此次會議包含 3 個不同的主題，條列如下: 
 Track A - Algorithms, Automata, Complexity and Games 
 Track B - Logic, Semantics, and Theory of Programming  
 Track C - Security and Cryptography Foundations:  
   
與我的研究方向最相關的是 Track A，其研究內容可細分為下列各項: 
• Algorithmic Aspects of Networks 
• Algorithmic Game Theory  
• Analysis of Heuristics  
• Automata Theory  
• Combinatorics in Computer Science  
• Computational Biology  
• Computational Complexity  
• Computational Geometry  
• Data Structures  
• Design and Analysis of Algorithms  
• Internet Algorithmics  
• Machine Learning  
• Parallel and Distributed Computing  
• Quantum Computing  
會議的投稿者不乏國際理論界知名的學者，此外，Cynthia Dwork ，Alon Noga，Prakash 
Panangaden 以及 Simon Peyton Jones 多位知名學者更應邀演講。因此，此次論文能被接
受可說是非常難能可貴的。 
以下簡述各天重要之論文發表 
1. 第一天，在開幕式後緊接著是 Alon Noga 的應邀演講，演講題目為 Additive 
Approximation for Edge-Deletion Problems。下午的第二場演講即是本次的論文發表，發
表內容為 Deterministic Extractors for Independent-Symbol Sources。發表時間大約 20 分
鐘。由於是第一次在許多學者面前用英文演講，所以我非常緊張。幸好在出發前，老
師們花了許多時間陪我練習，所以報告順利完成。 
2. 第二天有 Cynthia Dwork 的應邀演講， 演講題目為 Differential Privacy。 
3. 第三天有 Simon Peyton Jones 的應邀演講，演講題目為 Composable memory 
transactions。 
4. 第四天有 Prakash Panangaden 的應邀演講，演講題目為 The One Way to Quantum 
Computation。 
 
出席國際會議報告
報告人姓名 蔡錫鈞 報告日期 7/6-11/2008
系所 資訊工程學系 核定文號 年　月　日
連絡電話 03 5131551 電子信箱 sctsai@csie.nctu.edu.tw
會議期間 7/6/2008~7/11/2008 會議地點 多倫多,加拿大
會議名稱 （中文）電機電子學會2008資訊理論國際研討會（英文）2008 IEEE International Symposium on Information Theory
