                            
行政院國家科學委員會多年期專題研究計畫期末報告 
高樣本效率回歸決策樹之發展與應用 
Development of Sample-Efficient Regression Tree (SERT) and Its Applications 
計畫編號：NSC 94-2212-E-002-064 
執行期限：94年 8月 1日至 95年 10月 31日 
主持人：陳正剛  
(e-mail: achen@ntu.edu.tw) 
執行機構及單位名稱：國立台灣大學工業工程學研究所 
 
中文摘要 
迴歸樹 (Regression Trees) 是已知很有效的資料礦
掘工具，用以輔助工程上或管理上的決策。迴歸樹是利
用不斷的因子選擇與資料分群來建構一階層式的樹結構
模型，其基本的假設，也是在某些應用上最大的好處，
是每一個樹節點都有屬於自己的資料群與迴歸模型，也
就是說整個樹是由不同階層的資料群與迴歸模型組成。
由於樹的階層是由不斷的資料分群往下演衍，其最大的
缺點是資料群的樣本大小隨著階層的不斷分枝兒快速減
小。反觀統計上的逐步迴歸分析 (Stepwise Regression 
Analysis)，利用同一組大小的樣本選擇所有有影響的因
子，但是迴歸分析所選擇的因子卻只能建構單一的迴歸
模型，也就是說傳統的逐步迴歸分析無法將資料分群以
建構不同資料群下的個別模型。本研究便是嘗試著去結
合兩者的好處，發展出一具高樣本利用率的迴歸樹 
(Sample-Efficient Regression Tree, SERT)。本研究的第一
年，首先提出整合的因子選擇條件，進而在迴歸樹中運
用逐步迴歸分析的技巧。我們同時也發展出一測試方
法，以測試資料分群的必要性，這個測試將幫助我們避
免不必要的資料分群所造成的樣本快速遞減。最後，我
們用一些模擬案例與實際的半導體良率分析案例來驗證
所提出的高樣本利用率迴歸樹。 
 
關鍵詞：分類迴歸樹、迴歸樹、資料礦掘、樣本大小、
逐步迴歸分析 
 
Abstract 
Regression trees have been known to be effective 
data-mining tools that help both engineering and managerial 
decision making. The regression tree is built by iteratively 
splitting data set and selecting attributes into a hierarchical 
tree model. The fundamental assumption, also an advantage 
in some cases, is that each tree node with a data subset in the 
hierarchy has its own governing model and the entire tree 
consists of governing models in different hierarchical levels. 
The shortcoming of regression trees is that the sample size 
reduces sharply after few levels of data splitting. With the 
quickly decreasing sample size, attribute selection in the 
lower hierarchical levels becomes extremely unreliable. In 
contrast, forward stepwise regression analysis selects the 
influential attributes all the way using the entire data set. 
However, the attributes are evaluated and selected with only 
one regression model. That is, the stepwise regression is not 
capable of splitting data to subsets with different governing 
models. In this research, we attempt to develop a 
Sample-Efficient Regression Tree (SERT) that combines the 
stepwise regression and regression tree methodologies. In the 
first year of this research, we first propose a unified selection 
criterion. With that, we are able to unify the regression tree 
splitting decision with stepwise regression selection. We then 
develop a test to determine whether or not data splitting is 
needed for a selected attribute. The test can help avoid 
unnecessary data splitting and thus preserve the sample size. 
The proposed sample-efficient tree will be tested with two 
simulated cases and one actual semiconductor yield analysis 
case. 
Keywords：CRT, Regression Tree, Data Mining, Stepwise 
Regression, Sample Size 
 
1. Introdution 
Once the data is collected, the analysis tools must then 
be capable of finding patterns in the data to explain the 
observed behavior. The process of finding hidden patterns in 
the data is called data mining. There are several well-known 
methods such as decision tree, neural network, association 
rule, clustering, etc. Each of them has its own inherent pros 
and cons. 
Regression tree, a hierarchical structure, is built based 
on the divide-and-conquer conception. It is used to separate 
data into several similar groups with smaller intra-subset 
variance and derive a sequence of rules. Typically, in order to 
monitor the upcoming observation, each rule assigns a single 
value to each leaf node. 
In the past, the famous Classification and Regression 
Trees (CART) [2] [8], first derived in 1984, is concerned with 
both the discrete and continuous response variables. In 
parallel, a basic ID3 procedure similar to CART was devised 
by Ross Quinlan in 1986. A comprehensive description of 
this method was embodied in C4.5 which deals with the 
categorical response variable, and appears along with the full 
source code in the 1990s. [5] The descendant method M5 
constructs tree-based piecewise linear models. It integrates 
the concept of parametric model to estimate the target value. 
Then, model tree [4] [7] is introduced to process the data 
with piecewise linear functions. Although the decision tree 
has been a popular technique for data analysis, there still 
exist some shortcomings to be addressed. 
To begin with, the regression tree selects the most 
significant “variance reduction” attribute and the data is 
separated into two or more groups. The data assigned to each 
group are “purer” than in the parent level. And it does the 
same thing conditionally until the data sample size or the 
variance depletes. Owing to the inherence in the tree 
structure, the sample size reduces rapidly and leads to the 
unreliable splitting decision. Here, we use an example to 
 
combination to prevent sample-size depleting caused by tree 
splitting. 
0 1 
1 
x3 
x2
0 
Size:  
4 
Size:  
4
Size:  
12 
 
Figure 3  The sample size depleting in three-way interaction 
 
To address the above problems and develop a software 
system for the newly proposed sample-efficient regression 
tree, this research will be conducted in three-year span. In the 
first year, we first propose a unified attribute selection 
criterion by examining and comparing the attribute selection 
criteria by regression trees and by stepwise regression 
analysis. We also attempt to develop a test to determine 
whether or not data splitting is needed for a selected attribute. 
The test can help avoid unnecessary data splitting and thus 
preserve the sample size. Only binary attributes will be 
considered in the first year. 
 
2. Methodology 
2.1 A unified attribute selection criterion 
We first examine and compare the selection of the first 
attribute in the regression tree and in the stepwise regression 
analysis. Let inii xxx ,...,, 21  be observations of n  potential 
binary attributes ),...,,( 21 nxxx  thought to be related to a 
continuous response variable y  with observations iy  
where mi ~1= . For each attribute, there is an underlying 
simple linear regression model shown in Equation (2.1). 
 
nkmixy kikki <<=++= 1  ,~1for    0 εββ   (2.1) 
 
In the stepwise regression analysis, we iteratively fit 
these models by each of the independent variable (or 
attributes). The first step of the stepwise rgression analysis 
chooses a variable with the largest partial F-value. 
The regression tree, on the other hand, is formed by 
iteratively splitting nodes so as to divide them into the purer 
subsets. It selects the attribute with the maximum “variance 
reduction”. In this research Task, we show that the partial 
F-value criterion and the “variance reduction” criterion are 
equivalent in selecting a variable. 
The partial F-value is therefore selected as the unified 
selection criterion. 
 
2.2. Algorithm of Sample-Efficient Regression Trees for 
Binary Attributes 
We conclude the selection procedure to be: Attribute 
Selection, Splitting Test, and Re-split as shown in Figure 4.  
 
3. Results 
3.1 Validation of SERT 
It’s natural to measure the performance in terms of the 
error rate. Error rate on the raw data is not likely to be a good 
indicator of the future performance. Since the estimation has 
been learned from the very same training data, any estimate 
of performance based on that data will be “optimistic”. To 
predict the performance of the tree, we need to assess its 
error rate on an independent and similar dataset which is 
called the “test set”. Usually, a certain amount is held over 
for testing and remainder used for training, called the 
“holdout” procedure. And, the real problem occurs when 
there is not a vast supply of empirical data available. The 
empirical criterion of predicting error rate of a learning 
technique given a single, fixed sample of data is to use 
stratified tenfold cross-validation. The data is divided 
randomly into ten segments. Each segment is held-out in turn 
and the learning scheme trained on the remaining nine-tenths, 
and then its error rate is calculated on the holdout set. Thus 
the estimating procedure is executed a total of ten times on 
different training sets. Finally, the ten error estimates are 
averaged to yield an overall error estimate. Here, we adopt 
cross-validation as our validation rule. [8] 
We adopt the error estimation indicator in CART. [2] 
The accuracy index named the relative mean square error 
)(dRE  is shown in Equation (2.2). Let the predictor )(τd  
of the dataset τ  be measured by: 
 
m
y
y 
datasetinyd
m
dy
dR
m
yy
R
m
i
i
m
i
i
m
i
i
∑
=
∑ −
=
∑ −
=
=
=
=
1
i
1
2
1
2
           
        of predictor   the:)(   where
))((
)(
)(
)(
ττ
τ
µ
  
 
and 
 
)(/)()( µRdRdRE =    (2.2) 
 
3.2 Case study 
In this research task, we plan to use two simulated cases 
and one semiconductor yield analysis case to validate the 
proposed tree. 
 
Simulated Case 1 
We generate a dataset by simulating four binary 
attributes and a continuous response variable. 1x  and 2x  
are in-node attributes and the entire dataset is split by 1x  
into two sets. The left child is a dataset with 1x =1. 3x  and 
4x  are determinate splitting attribute for each child. The 
model is shown in Figure 5 with which 200 instances are 
generated with error )1,0(~ N
iid
. 
 
sample size after few levels of splitting. After a sequence of 
calculation using the SERT algorithm, regression tree, and 
stepwise regression analysis, we obtain the relative mean 
squared error from cross-validation estimation with 100 
simulations. The average error rate for each method is shown 
in Figure 8. 
0.009097135 0.00931861
0.068307209
0.19556766
0
0.05
0.1
0.15
0.2
0.25
True Error SERT CART Stepwise
 
Figure 8 Average RE (d) comparison for Case 2 
 
Real Case in Semiconductor Fabrication 
In a typical integrated circuit fabrication process, a 
wafer lot goes through more than three hundred process steps. 
Wafer lots may be processed by different machines at each 
step, and result in different final yields. To find at which step 
and by which machine the yield is lost, engineering data is 
collected and stored in the database for yield analysis. The 
engineers use the engineering database to investigate events 
and comments logged to the tool history during the suspect 
time period. They attempt to discover clues by monitoring 
the circuit probe (CP) yields using statistical screening graph 
like box plots, scatter plots, and trend charts along with some 
basic statistical analysis methods. With the enormous and 
complex manufacturing data, such analyses are often a 
time-consuming and error-prone task. The dataset consists of 
200 attributes and 154 wafer lots. Each attribute is a binary 
variable with value 1 to indicate that the wafer lot was 
processed by a certain machine at certain step and value 0 
indicating otherwise. The semiconductor yield data will be 
used as a case study here. 
The semiconductor yield data will be used as a case 
study here. The dataset consists of 200 attributes and 154 
wafer lots. The attributes record whether the wafer lot was 
processed by a certain machine at some steps or not. We use 
four-fifth of the data as the training data set and one-fifth as 
the testing data set. After analysis, we obtain a five-level 
regression tree using the regression tree which has about 9 
attributes. We also obtain a three-level SERT with 9 attributes 
in Figure 9. The interpreted model for SERT is in Equation 
(2.3). In addition, the the stepwise analysis also selects 22 
attributes. 
X49, X68, X52 
X14 
X134 X57 
X22 X4 X6 
1 0 0 1 
X52=1 X52=0 
 
Figure 9 SERT result for the semiconductor yield case 
 
40,0149
0575
31,068
5236824910
20,147
11344
11,1276
5752
52
5752
13452
52
13452
|                                                                   
|                                              
|                                                                      
|                                                                      
|                                              
|                                                                      
εβ
β
εβ
ββββ
εβ
β
εβ
++
+
++
+++=
++
+
++
==
=
==
==
=
==
xx
x
xx
xx
x
xx
x
x
x
xxxy
x
x
x
(2.3) 
 
We list the estimates of the parameters of (2.3) in Table 
1. 
Table 1 Model Estimate 
Coefficients 
0βˆ  83.204 5βˆ  -11.67
1ˆβ  -9.7 6βˆ  29.17
2βˆ  -7.71 7βˆ  2.93
3βˆ  1.15 8βˆ  7.74
4βˆ  -42.66 9βˆ  1.48
 
The comparison of these three algorithms is shown in 
Table 2. 
 
Table 2 Comparison of Algorithms 
 SERT Regression 
Tree 
Stepwise 
Regression 
Level 3 6 1 
Number of 
variables 
9 9 22 
RE(d) for 
Training set 
0.1956 0.1608 0.192 
RE(d) for 
Testing set 
1.177 1.3955 1.4562 
  
When using the training data set to find the model, the 
regression tree has the smallest relative mean square error. 
However, with 6 levels, the regression tree model may have 
been over-fitted. Testing with one-fifth of the dataset 
confirms that the regression tree may have been over-fitted 
since the relative mean square error has become much higher 
than the SERT model. The errors in the stepwise regression 
analysis are larger than the other two methods with both the 
training and test datasets. 
The case study also shows two advantages of SERT. In 
the SERT model, we use fewer levels to interpret the data, 
and fit the model with fewer attributes than the stepwise 
regression analysis. And in SERT, there are three in-node 
attributes staying in the root node and shared the entire data 
set with a sample size of 119 instances. Both estimation of 
in-node attributes effects and selection of the determinate 
splitting attribute are based on the 119 instances. That is, the 
SERT will efficiently use the 119 instances to analyze the 
effects of the three in-node attributes as opposed to the 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
94年 10月 24日 
報 告 人 
姓 名 
陳正剛 服務機構
及 職 稱
台灣大學工業工程研究所
副教授 
會 議 
時間 
地點 
2006/09/25-2006/09/27 
日本東京 
本會核定
補助文號
NSC 94-2212-E-002-064 
會 議 名 稱 
(中文) 第十五屆半導體製造國際研討會 
 
(英文) 15th International Symposium on semiconductor Manufacturing 
發表論文題目 
(中文)  
 
(英文) Sample Efficient Regression Trees (SERT) for Yield Loss Analysis 
報告內容應包括下列各項： 
 
一、參加會議經過 
見附頁 
 
二、與會心得 
見附頁 
 
三、建議 
見附頁 
 
四、攜回資料名稱及內容 
1. 15th International Symposium on Semiconductor Manufacturing Proceedings 
2. ISSM2006 Proceedings CD 
 
 
 
表一 Lithography光源發展與預測 
 
 從表一可看到，藉著更種輔助技術的發展，光源波長的壽命遠比原本預測來的長很
多，193nm 波長的光源將持續為 2007 年的主流。兩項主要輔助技術為 Immersion 與 
double exposure/patterning兩項技術，尤其是 Immersion技術將使 193nm ArF持續用到
65nm的 technology node. 圖一為光源波長與相對應 technology node的發展趨勢。 
圖一 Lithography光源波長 vs. Technology Node 
 
 Ushida-san 將過往的發展策略分成 change wavelength as soon as possible (ASAP) 
與 use one wavelength as long as possible (ALAP) 兩種策略，並製作了下圖： 
