行政院國家科學委員會補助專題研究計畫
■ 成 果 報 告   
□期中進度報告 
 
 
 
RFID數位家庭中網路技術與應用之整合及個人化服務平台之建置 
子計畫一:數位家庭之視覺式保全系統 
 
 
 
 
計畫類別：□個別型計畫  ■整合型計畫 
計畫編號：NSC 97-2221-E-259-012-MY3 
執行期間：   97年 8月 1日至   100年 7 月 31日 
 
 
計畫主持人：楊茂村 
共同主持人：無 
計畫參與人員：鄭意如、林華振、戴佑安、方浤丞、楊子皙、林坤逸、盧俊佑、劉玲伊、
陳浩瑋、蔡孟儒、鄭景瑜、紀昭宇、廖萬哲 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列管計畫
及下列情形者外，得立即公開查詢 
 □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
         執行單位：國立東華大學資訊工程學系 
中   華   民   國    100 年   10  月    6  日 
研究計畫成果報告英文摘要 
 
One of the fundamental obstacles to realizing the power of intelligent digital home is the difficulty to 
detect, identify, and track human. RFID approaches can identify human effectively but users are forced to 
bear RFID tags all the time. Vision-based approaches provide a natural and nonintrusive solution but it is 
sensitive to illumination and occlusion. We propose to combine the RFID and vision-based techniques to 
develop a reliable human detection, identification, and tracking system for digital home guard. However, the 
problems of active camera, moving cast shadow, occlusion, template drifting and human body 
scaling/rotation/deformation can significantly increase the error during the process of video analysis. In this 
project, we propose to utilize appearance templates of full human body to solve these problems. In the first 
year, we developed a human recognition technique based on gait/posture analysis, and implemented a human 
following robot. In the second year, we exploited and integrated the color, lightness, texture, motion, and 
shape information to detect human dynamically. In the third year, we handled the occlusion problem by 
constructing Flattened Cylindrical Template (FCT) and Multiple View Template (MVT). The FCT records 
the appearance information of a person with a view of 360 degree. The MVT models the geometric 
information of a person by depth-from-stereo technique from several viewpoints. The geometric information 
is ideal for tracking and the appearance information is important for identification. The proposed solutions is 
general and practical so that they can be applied to other applications such as education, entertainment, and 
human-computer interface in digital home. 
 
Keywords: digital home, human detection, human identification, human tracking, video surveillance. 
how(如何回應?)等問題。透過視覺式人物樣板與RFID的輔助，可以解決who的問題。透過機器人定位，
可以解決where的問題。我們進一步探討如何利用視覺式行為分析(behavior analysis)解決what的問題，
並利用路徑規劃(path planning)與人工智慧回答how的問題。我們期望智慧型機器人除了保全外，亦可
提供數位家庭中兒童娛樂、成人學習或老人照護等服務。 
近年來，視訊監控(Video Surveillance)是越來越熱門的一項研究，重要的應用如監視系統、影片搜
索、交通監控系統等。如圖0-1所示，數位家庭中之攝影機包含四大類:第一類是靜態攝影機(Static 
Camera)，第二類是可原地3D旋轉的攝影機(PTZ Camera)，第三類是可自由移動的動態攝影機(Active 
Camera)，第四類是立體攝影機(Stereo Camera)。對於靜態攝影機，我們曾經提出一個前景與背景分割
的方法[124]，利用多種特徵之融合，有效的擷取前景區塊。 
人物偵測、追蹤與辨識系統一直是許多研究人員努力的研究方向，其應用範圍相當的廣泛，例如：
機器人、視訊會議、人機介面、保全監視、流量統計、運動分析、軍事用途等，不過其所面臨之困難
包括：第一、人的動態及外觀：人的外表包括動作、穿著、造型、年齡、裝飾等，具有相當大的可變
動性，也因此造成了辨識及追蹤上的困難。第二、環境有相當大的可變動性，例如光源、陰影等，都
很容易影響到人物偵測的正確性。第三、人在環境中活動時，很容易就會有被環境中的物體所遮蔽或
人跟人之間彼此遮蔽的情況發生，偵測的結果將受到影響。第四、人物的偵測及追蹤系統通常都需要
對大量的影像進行數學和邏輯的運算及處理，可是許多應用皆要求系統的即時性，因此縮短運算時間
也是必要處理的問題。 
此外，人物的偵測、追蹤與辨識也牽涉到許多非常有趣的議題，例如一個人物該用什麼樣模式的
資料來記錄、如何利用樣本比對來判定人物的身份等。人物的偵測、追蹤與辨識系統要保持其穩定性，
就必須針對各種可能出現的問題，如人物遮蔽、旋轉與大小比例縮放的問題，做準確的分析和適當的
處理，再針對每個狀況即時做出正確的回應，以維持系統的效能。我們擷取人物顏色(color)、亮度
(lightness)、材質(texture)、運動(motion)與形狀(shape)資訊來動態更新與建構人物樣板，利用
這些視覺式人物樣板，我們可以準確的辨別與追蹤每位不同的人物，且不會因為許多人物同時在場景
中交錯移動而影響其效果。 
可用於人物偵測及追蹤的判別特徵有很多種，其中最廣泛被應用的四種分別為膚色特徵、移動特
徵、形狀特徵和距離特徵。在單一特徵擷取的人物偵測系統中，由於環境因素的改變，經常導致偵測
結果不夠理想；例如，膚色特徵在動態光源下不穩定、運動特徵易受雜訊干擾、形狀特徵很難描述可
大幅度變形的人物肢體、距離特徵計算複雜度過高等。距離特徵係指經由立體視覺的方法計算兩台不
同位置的攝影機所攝取之影像的視差(Disparity)，再經由視差還原出影像中物體與攝影機的相對距離(Z
軸資訊)，以獲得三度空間座標。 
除了上述特徵之外，亦可經由攝影機拍攝從外部靠近居家的行人，藉由判斷行走的風格，在遠距
離就先辨識是否是家庭成員，假如系統判定不是家庭成員，則可以先提醒家中之人，以提高警覺。此
系統也可以輔助其他的辨識系統，以減少其他辨識系統的錯誤率。由於每個人的行進過程中的行走姿
勢，行走習慣都會有些許的不同，我們常常可以藉由這些特性而從遠方觀察就知道遠方行走的人物的
身份，而不需要看到遠方的人他的面貌長相。 
 
 
三、文獻探討 
近年來有很多關於機器人視覺方面的應用，如追蹤、導航、導覽、遊樂、建模、巡邏等，而其中
動態攝影機人物跟隨(human following)技術，大部分都使用雷射掃描、音頻分析、紅外線等非視覺
感測器，或是採用多個攝影機，以得到深度(depth)資訊，但也因此有硬體與應用上的限制。以下針
對基於單一攝影機且純粹視覺追蹤的相關文獻以顏色(color)、亮度(lightness)、材質(texture)、
運動(motion)與形狀(shape)等特徵分類做說明: 
三(一)、顏色 (Color)  
使用顏色特徵的文獻相當多，Mean-Shift 演算法[170]以記錄目標的色彩直方圖去對整個影像作
背投影，得到一個機率密度影像，針對此影像以遞迴的方式將框選位置移到機率密度最高的區域，能
簡單有效的達到追蹤目的，但卻不能解決部分遮蔽(partial occlusion)、自我遮蔽(self occlusion)、
外型大小變化(scale)、照明(illumination)變化、背景與前景顏色類似時容易追丟的問題；
CamShift[136]改良 Mean-shift 的演算法，使用 HSV 色彩空間的 Hue，去除顏色特徵維度過高造成計
算量過大的情況，採用橢圓形框選並且可動態調整其大小、位置與方向，解決了外型大小變化問題，
能更好的框選出目標物，同時限制掃描區域為前一個 frame 追蹤結果的附近，以避免較遠區域顏色相
同的雜訊。 
將顏色在空間上的分布結合進來的方法中，[182]提出把目標物以 Y 軸灰階變化劇烈的方式來判
斷分段，並且針對每個部分個別比較，改進 Mean-Shift 內部判斷的方式，以處理部分遮蔽，也可以
應用在移動的攝影機下，但只針對 Y軸來分段的方式造成了此追蹤法的侷限性，無法適用於各種目標。
[183]提出一個基於 Mean-Shift 的多片段追蹤法，採用不重疊且平均切割的方式將目標切成多片段，
可追蹤各種不同的目標，解決部分遮蔽以及外型姿勢的變化。[176]提出一個可以同時兼顧解決部分
遮蔽與外型大小變化的方法，以避免過度縮小的問題，可是卻不能解決目標放大與光影變化劇烈的問
題。 
[171]提出一種改良 Mean-Shift 的方式，對目標區域外在框選一個背景區域，以此同時考慮目標
與附近背景的顏色，去改良目標顏色的權重，能針對比較有分辨性的顏色來判斷，將這些特徵各自得
出的權重影像使用 Mean-Shift 做追蹤，再將各自結果以相同權重合併成唯一結果，如此可以大幅降
低背景顏色與目標相似造成追丟的問題，但是每個選出來的顏色特徵之間分辨性是不同的，比較好的
特徵可能被較差的特徵所拖累，造成合併的結果並非理想。 
[167]結合 AdaBoost與 Mean-Shift的概念，提出兩個改良演算法 Discrete AdaBoost(DAB)和 Real 
AdaBoost(RAB)，DAB 與 RAB從背景與目標區域中訓練出目標顏色，並且對每個選出來比較有分辨性的
顏色特徵作更細緻的分辨性的分析，給每個特徵一個權重，則這些特徵各自算出來的 likelihood image
就可以使用他的權重來做合理的合併，再交由 Mean-Shift 做追蹤，如此可以很好的分出目標與背景，
而不受顏色相近影響，缺點是計算量龐大，且每個 frame 都得訓練會造成效能大幅下降，執行速度上
與其他方法相比仍相對較低。 
針對強烈分辨性顏色的區分的方法中，[173]先將目標在色彩空間的分布中比較相近的分類為一
群，針對這些群使用 Independent Component Analysis(ICA)算出直方圖，則能對顏色中具有獨立成
分部分加強考慮，更好的分辨出顏色的不同，可是計算複雜使得執行時間太高，無法使用於即時性的
應用。EM-Shift[163]也是基於色彩特徵去做追蹤，使用 EM 的概念，E-Step將目前框選的顏色與所要
比較的顏色做比較，算得相似度，對目前框選內的每個顏色調整權重，M-Step針對每個顏色新的權重
去計算出應該移動到的位置與框選範圍，如此不斷重複直到位置與範圍收斂，可解決外型大小變化的
三(五)、外型 (Shape)  
針對要被追蹤的物件進行外型分析比對有許多的方法，如 Template matching[172]、Shape 
fitting[134]和 Human modeling[128]等，這些方法靠人物的外型去追蹤，所以必須要有一般大眾認
知的人體外型；[172]事先將所有行人可能出現的外型以樹狀結構分類，之後訓練出每一類的樣板
(template)，以樹狀結構判斷後以樣板比對的方式框出行人，雖然可以在移動攝影機的情況下作行人
偵測，可是卻會因為場景中有類似行人外型的光影變化，而造成誤判；因此[134]先作前景背景切割，
切割出來的前景再去做判斷是否為人型，進而判斷此人型的變化以確認追蹤是否正確，如果能在移動
攝影機的情況下切割出目標，則針對此目標做外型分析以減少雜訊的影響；[128]以人體結構組成的
方式把人體拆成數個部位，如頭、身體、上手臂、下手臂、手、大腿、小腿、腳等，彼此有一定關係
的連接，將抓取到的外型進一步分析各自為何種部位，以此拼湊出人體目標外觀，但是移動中的人物
外型會不斷變化，若是場景複雜則不容易分辨。 
三(六)、多模組融合 (Multi-modal fusion) 
結合多特徵或多種模型的追蹤方法有相當多，大略可以分為線性(Linear)與非線性(Non-Linear)
兩大類；線性(Linear) 的方法是將每個特徵或模型以平行的方式執行，並且以一種權重的方式線性
合併；而非線性(Non-Linear) 的方法則是以經驗法則將各種特徵以一種前後順序來排列，比如先做
膚色偵測再作人臉辨識，這樣的方法則必定要使得先作的方法較為寬鬆，而後作的方法較為嚴謹。 
線性(Linear)的結合方法中，[174]提出一基於 Mean-shift結合顏色與位置以預測的方式去做追
蹤，並且可以靠輔助物件來協助目標物的追蹤，若目標物被遮蔽或是離開視線時，則可經由輔助物件
去預估他可能出現的位置，針對這位置不斷做搜尋，而不會直接判斷為追丟，如此可以解決部分遮蔽、
離開視線(out-of-range)的問題，此法必須靠人為手動去選擇輔助物件，且預測方式太過簡化，若兩
人穿相同顏色衣服面對面接近後各自折回，則會出現追錯的情形。 
[181]是一個同時考慮顏色與材質資訊以追蹤人臉的方法，材質資訊使用 Local Binary 
Patterns(LBP)，並且將其應用於 Particle filter的架構上，以 Bhattacharyya distance 去衡量相
似度，以去除單調顏色或材質會遇到的問題，此法可用於動態攝影機追蹤上，但是並未對部分遮蔽與
自我遮蔽做處理，也不足以應付環境光影變化的問題，且計算較複雜，執行速度上無法達到即時性。 
[179]結合 LBP、Mean-Shift 和 Kalmen filter，多加了門檻值以去除雜訊，結合[177]與[178]
的方法將 LBP分類，針對目標的鄰居順時鐘環繞一圈計算 0與 1的變化次數，以代表邊、點、角落等
資訊，以此材質直方圖與 HSV的 hue所得到的直方圖合併，並且只對 Kalmen filter 所預測的區域做
搜尋。因為使用的特徵少又有明顯的分辨性，跟單純的 RGB 作 Mean-Shift 比較起來較快，可以減少
疊代次數並提升正確率，但是沒有解決部分遮蔽、自我遮蔽、光影變化等問題。 
非線性(Non-Linear)的結合方法中，[169]是基於兩個攝影機作移動追蹤，採用人臉辨識結合 LK
演算法，並且以深度、移動向量等資訊對可能非目標物上的特徵點作刪除，以保障特徵點在目標身上，
優點是特徵點比顏色不易受到光影變化的影響，解決了部分遮蔽、自我遮蔽、外型大小變化，並且提
出改善特徵點在多個 frame之後可能會追丟的問題，缺點是此法需要一開始人臉面對攝影機，若中途
追丟且目標又沒有將臉面向攝影機就離開可視範圍，則將永遠追丟。 
[149]的方法是採用GMM作前景背景切割，然後使用EM-Shift與LK演算法互補針對前景做動態追蹤，
在frame與frame之間使用LK演算法追蹤，而每個frame都採用追蹤出來的特徵點去定義位置與形狀大
小，以EM-Shift演算法去重新框選位置與形狀，而LK演算法在新框選的範圍中選定新的特徵點，如此
四、研究方法 
本計劃以三年的時間，針對 RFID 整合視覺式偵測、辨識與追蹤技術之關鍵性的問題，研發解決
的方法。本計劃第一年已嘗試針對人物步伐(gait)進行分析，探索人物外觀(appearance)資訊建模
與肢體姿勢(posture)參數擷取，並實作移動機器人之人物追蹤與跟隨；人物追蹤系統要保持其
穩定性，就必須在追蹤與辨識過程中針對各種可能出現的問題，如人物遮蔽問題與人物大小比例縮
放的問題，做準確的分析和適當的處理，再針對不同狀況做最出即時正確的回應，以維持追蹤系統
的效能。為了解決這些問題，本計劃第二年已嘗試擷取人物顏色(color)、亮度(lightness)、材質
(texture)、運動(motion)與形狀(shape)資訊來動態更新與建構人物特徵資訊，融合這些視覺式人
物特徵資訊來辨別與追蹤不同的人物。本計劃第三年專注於開發立體視覺之人物多視角樣板技術，
以解決更困難的遮蔽問題。攤平圓柱樣板(FCT)整合記錄人物周身 360 度的顏色資訊，多視角樣板
(MVT)分別記錄人物多個視角的外觀資訊；利用這些新的視覺式人物樣板技術，我們可以準確的辨別
每位不同的人物，且不會因為許多人物同時在場景中交錯移動、互相遮蔽，而影響其辨識與追蹤的
效果。若 RFID 系統成功偵測到人物 ID，則著手以人物前景區域逐步建構該人物 ID 的 FCT 或 MVT
樣板，並更新資料庫。反之，若 RFID 系統未偵測到人物 ID，則利用人物前景區域來比對資料庫中
存在的 FCT 或 MVT 樣板，辨識出相似度最高的人物 ID 並進行追蹤。如此，家中成員即使未配戴個
人 RFID Tag 時，仍可由視覺系統正確辨識；而未配戴 RFID Tag 之侵入者，也會被視覺系統發現並
進一步以移動式機器人追蹤與監控。 
第一年的研究成果分成三個部份；監視影片透過背景減法(background subtraction)找出有改變
的前景區域，並動態選擇性地更新背景之後，再針對所取出的前景區域進行分析；研究成果(一)嘗試
對人物步伐進行分析與辨識；研究成果(二)進行人物外觀資訊建模與肢體姿勢參數擷取；研究成果(三)
實作移動機器人之人物追蹤與跟隨。第二年的人物樣本建構系統成果分成三個部份；研究成果(四)
利用顏色(color)與亮度(lightness)資訊進行人物樣本建構；研究成果(五) 利用材質(texture)與運
動(motion)資訊進行人物樣本建構；研究成果(六)多特徵融合 (multi-modal fusion)與人形
(human shape)確認 (verification)。第三年的人物外觀樣本建構、辨識與追蹤系統成果分成三個
部份；研究成果(七)人物攤平圓柱樣板之建構、辨識與追蹤；研究成果(八)人物多視角樣板之建構、
辨識與追蹤；研究成果(九)、人物遮蔽之偵測與處理；分別詳述如下: 
 (a)        (b) 
 
(c)                            (d) 
圖 1-1. 高斯背景建模 (a)(c)為偵測之前景 (b)(d)為經過膨脹與侵蝕後的結果 
       
(a)      (b)          (c)             (d) 
圖 1-2. 分別為人物行進中前腳與後腳張最開的時候的影像以及雙腳幾乎密合的影像。 
 
 研究成果(二)、 人物外觀(appearance)資訊建模與肢體姿勢(posture)參數擷取 
首先對輸入影像進行 Codebook 背景減除法[147]，這是一個跑起來相當快的方法，可以在 2005
年前的機器，以 30FPS 來執行，其原理在於對像素做適當的 codeword 編碼、更新，之後再對 pixel
以查表的並做適當的判斷後，決定像素是前景或是背景，如圖 2-1所示；除了相當快之外，對記憶體
的用量也遠小於 GMM，且對整體光源的變化，可以動態的適應，因此在實做上，我們採用 codebook做
前景擷取，再對出現的物體做 contour 分析，之後對最大的 contour所在的區塊來做姿勢辨識。如前
所提，這裡的方法是使用 DDMCMC 來做辨識，其所依據的 observation 有四種： 
1. Face Detection : 這裡使用的臉部偵測，是使用 Adaboost 法的臉部偵測；只要偵測到臉部之後，
即可以確定出頭的位置，如圖 2-2(a) 所示。 
2. Head-Shoulders Contour Matching： 在經過臉部偵測之後，可以藉由頭跟身軀的關係，簡單的
找出頭、脖子、肩膀的部分。 
3. Skin Detection： 這個階段則是盡可能得找出有皮膚的部分，讓相對應部份的 observation 能
得到較正確的機率分布，如圖 2-2(b) 所示。 
4. Contour Observations：相較於[148]無法得知圖片中何處是人體，我們可以分割出前景人物，
故不用再做人體位置的判斷，只要在此區域中做即可，如圖 2-2(c) 所示。 
在每個關節點角度計算的時候，同時亦會透過 direct Inverse Kinematics(IK)來做角度的估計，
原理是應用人體骨骼運動的限制及產生一個 2D 影像時的 3D 位置有限，以縮小搜尋空間並加快速度，
而得到最高機率的角度 [156]，並在獲得的 candidate 中取出和前一個 State(四肢或軀體)中深度差
異最小的當作目前的角度。然而[148]是假設人物和攝影機為正交投影，這在一般 surveillance 系統
中是不太可能發生的，因此還需要加上對 Projection的角度做修正[160]，這樣就可以得到近似的人
物姿勢。 
姿勢參數的獲取之後，我們考慮使用 DirectX的人物架構.X檔來表現人物的動作，例如使用 3d max
之類的 3D model 製作軟體，做出有結構性的 3D 的  .X 檔案之後，使用 Direct3D 的
D3DXLoadMeshHierarchyFromX 函式[8]，即可利用 3D model 表現出相對應的姿勢，如圖 2-3所示。目
前已完成前處理的部分和呈現 3D 的部份。即將進行的是姿勢參數的獲取，並對身體各部份，以各個
時間的視角所獲得的色彩資訊，對 3D model 作相對應的 mapping，如此即可使用此 3D model 對影片
中的人物做身分辨識、行為分析(behavior analysis)等重要的應用。 
我們的研究結果發現相較於人物步伐(gait)與肢體姿勢(posture)，人物外觀(appearance)包含更
多有用的資訊，可以進一步有效利用。 
  
研究成果(三)、 移動機器人之人物追蹤與跟隨  
我們使用的機器人是 Dr.Robot 公司所出產的 WiRobot X80機器人，從側面看來，上方為機器人
的視訊頭，下方則是放置超音波與紅外線模組的區域，而左右各有一個輪子，可控制機器人前後左右
移動，後面的一個小輪子是用來負責輔助支撐使機器人能站穩，機器人高度為 255mm，長度為 380mm，
輪子高度為 170mm。圖 3-1(a)為此機器人的俯視圖，其中標記為綠色的 B、C、E、F、K、L、J為紅外
線模組；而 A、D、G為超音波模組；H、I為聲吶模組；紅色的 J與 K為直流馬達模組。我們以這些模
組完成偵測四周障礙物的有無，將此參數傳回來，針對機器人目前所要做的移動來做安全上的判斷，
以保證機器人不會撞到障礙物，同時輔助目標物追蹤上的控制。圖 3-1(b)則是此機器人的影音交流部
分，其中標記為藍色的 O、P部份可用來控制此視訊攝影機的轉動，對於機器人人性化的表現有很大
的幫助，而標記為黃褐色的模組 Q、R、S分別為攝影機、麥克風、擴音器。連線方式有三種：RS232
連線、Wi-Fi連線、Bluetooth連線。由於 RS232會受到線路的限制，在測試上會有些不方便，而
Bluetooth 連線法目前相容性不穩定，所以這裡我們使用較穩定的 Wi-Fi連線。大略硬體連線架構如
圖 3-2 所示。 
    
(a)          (b) 
圖 3-1. 機器人元件分布圖 (a)機器人俯視圖 (b)影音交流部分  
 
圖 3-2. 硬體連線架構圖 
AP 
Wireless network 
(median filter)濾除椒鹽雜訊(salt & pepper noise)，再使用型態學(morphological filter)的封
閉(closing)算子，先做擴張(dilation)運算來增強前景面積，然後使用收縮(erosion)運算濾除遠處
的小雜訊，如圖3-4所示。當這些前處理結束取得必要的前景後，計算切出前景的大小範圍，當長寬
在一定程度且長與寬的比例為3比1時，我們認定此前景為人的大小，之後將此前景取出，然後進入
Block 2。 
Block 2：追蹤前景 
由於目前針對動態攝影機的追蹤演算法都須要大量運算，而一般的追蹤演算法則各有缺點，因此
我們提出結合兩種演算法的資訊，以達到在動態攝影機下即時追蹤移動物體的目的，以下分別介紹此
兩種演算法： 
Lucas-Kanade optical flow algorithm[155]： 
1. 選出比較具代表性(也就是光影變化強烈)的地方當作特徵點，並且只取其中光影變化特別明
顯的特徵點。 
2. 假設特徵點在兩個 frame 之間的移動夠小且強度不變，根據上一個 frame的特徵點座標，計
算目前 frame的特徵點座標。 
3. 為了得到更精準的結果，演算法的實作以 Newton-Raphson法反覆做到收斂或我們設定的遞迴
次數。 
4. 使用金字塔形的結構去克服兩個相鄰 frame之間的特徵點移動可能過大的問題。 
5. 追蹤過程中，可能有些特徵點對於估算的移位向量的強度與上一個 frame的強度差異太大，
造成這些特徵點在成本函數(SSD)計算上誤差過大，則稱此特徵點為信心度不足，不是適合追
蹤的點，我們使用 block matching 將這些因為 optical flow 無法追蹤到的特徵點做錯誤回
復的動作，使這些特徵點能在上一個 frame的位置附近找到最佳的移動距離。 
此方法本身有些問題，第一，特徵點本身並不能代表任何有意義的資訊，它不能讓你知道此點在
人身上是屬於手、腳、頭等特徵。第二，此法在連續幾個 frame之後則容易因為移動變化造成原本的
邊、角消失或遇到相似背景，使得特徵點追丟、追錯，所以並不適合長時間或變化量大的追蹤。圖 3-5(a)
是一開始選出特徵點的樣子，可以看出人身上有很多特徵點，但是當人往右移動後原本邊緣的特徵點
會追到背景相似或身上其他相似的位置，如圖 3-5(b)(c)(d)所示，這邊是將(b)與(a)比較、(c)與(b)
比較、(d)與(c)比較，將追到背景相似特徵的特徵點以紅色框選出來，把追到目標其他相似特徵以藍
色框選出來。。 
3. 選擇 2D Mean-shift臉部顯示框的初始位置。 
4. 計算搜尋視窗在 2D範圍中的中心位置的顏色機率分布(為 Back projection 運作；ROI要些微
大於 Mean-shift視窗大小)。 
5. 執行 Mean-shift演算法以發現搜尋視窗中心，存入新追蹤到的面積(或大小)和中心位置。 
6. 對下一個 frame，集中被存於步驟 4的搜尋視窗的平均位置，且設定視窗大小給一個函式(此
函式可以發現初始物件的位置)，若位置與大小已經收斂或是重複到上限次數則代表追到，反
之回到步驟 4。 
此方法的問題在於當背景與前景顏色相同時會追丟。圖 3-6(c)可以看到切出來的前景相當正確，
圖 3-6(d)可以看到取出的前景 Hue 的直方圖也是正確的，但是在圖 3-6(e)中，由於背景有一個藍色
椅子上放黑色電腦包，此物體的顏色與要追蹤的目標顏色結構相似，造成了 CamShift 所追蹤的結果
產生誤判而沒有框選到正確的目標。 
  
(a)                   (b)                   (c) 
 
(d)         (e) 
圖 3-6. 使用 CamShift執行的錯誤結果圖。(a)GMM切出來並做過前處理的前景，(b)GMM 採用的背景，
(c)取出的前景，(d)前景使用 Hue計算出的直方圖，(e)追蹤錯誤結果。 
由上可知， LK判斷特徵點卻不考慮色彩資訊；CamShift 使用色彩追蹤但不考慮邊角等特徵點；
若能結合這兩個演算法，則可互補對方的不足，以移動資訊做一個整體判斷，並且結合這兩個演算法
所得到的目標位置與範圍大小，讓追丟機率下降，適用於移動攝影機上。 
我們希望以簡單且計算量少的方法，因此不採用完整的對影像做移動補償。假設機器人追蹤且跟
 圖 3-7. 結合 LK演算法與 CamShift 演算法的流程圖 
  
Lost 
Initial object position and ROI 
Not Lost 
Block 1 
Update the object 
histogram. 
camshift 
LK 
Have lost ? 
Compute the new position and ROI. 
Block 3 
Remove feature 
outside the ROI. 
Add new feature if 
necessary. 
id
d v
iv
W
W
  
                (3-2) 
 
(a)                              (b) 
圖 3-9. 機器人與目標物在實際上與影像中的轉動角度俯視圖。其中的 Wiv 為影像一半寬度，Wid 為目
標物中心與影像中線的距離，θv為影像中線到最遠可視區域的角度，θd為目標物與影像中線的角度。
(a)實際上機器人與目標之間的關係，(b)影像中機器人與目標之間的關係。 
這裡我們的 θv為 45 度，Wiv為 160 像素寬，而 Wid 可以在執行中得到。假設 R 為輪軸的半徑，r
為輪胎的半徑，當機器人原地旋轉 360 度時，一個輪胎所需的移動距離為 2πR，輪胎轉一圈的移動距
離為 2πr，則機器人原地旋轉 360 度時，一個輪胎必須轉動(2πR)/(2πr)圈，而轉動角度 θd與轉一圈
的角度的比例必定等於輪胎轉動圈數 Nr與自轉一圈所需轉動輪胎的圈數的比例，所以旋轉角度 θd與輪
胎轉動圈數 Nr的關係可以得到下式：  
=
2 R2
2
d r
r



N
 即   
R
2
d
r
r


N =
            (3-3) 
如此即可以角度大小去控制轉動幅度的大小與方向。結合公式(3-1)所得的 Do即可對機器人做前
進後退轉彎幅度方位的控制，假設目前是往左轉，則轉動圈數公式如下: 
左輪轉動圈數=
2
o
r
D
N
r
             (3-4) 
右輪轉動圈數=
2
o
r
D
N
r
             (3-5) 
由於輪子高度為 170mm，所以半徑 r為 85mm，如此即可算出左右輪所需轉動的次數。若為右轉，則將
公式(3-5)當作左輪轉動圈數，公式(3-4)當作右輪轉動圈數即可。若距離太近需要後退，則將上述所
得結果加上負號即可。  
θv 
θv 
θd 
目
標 
Wid 
機器人 
Wiv 
 
圖 4-1. 顏色特徵背投影(back-projection)。(a)輸入影像，(b)目標物的顏色直方圖， 
(c)目標物在輸入影像中的顏色機率分布圖。 
 單獨考慮某一種特徵時，若遇到此特徵無效的時候，例如目標與背景的顏色相同，則此時結果必
定會很差，所以我們計算一個信任度來表示此特徵應該被看重的程度，信任度計算目標與背景的顏色
直方圖的差距，差距越大，代表取出來的特徵越有分辨性，即信任度越高，其定義如下: 
    
    
360
0
360
0
,
1
,
o bi
H
o bi
MIN H i H i
CV
MAX H i H i


 


    
亮度其實是最早被用來分辨目標的辦法，通常都用於靜態攝影機，如 frame difference，而動態
攝影機追蹤這部分，因為亮度資訊太過簡化，通常都會做其他處理後才用於追蹤上，像是如 template、
texture、LBP 等。我們擷取 HSL 中的亮度(Lightness)資訊，紀錄此目標以及背景的亮度直方圖，
並將亮度分為 360種，之後採用背投影的方式，原理與上述顏色的方法相同。亮度機率分佈圖顯示於
圖 4-2。亮度的 log likelihood ratio 定義如下:  
 
  
  
max ,
log
max ,
o
o
b
L i
L i
L i


 
  
 
 
     
亮度的信任度與顏色是相同的概念，差別只在於改為針對亮度的目標與背景來考慮，其定義如下: 
    
    
360
0
360
0
,
1
,
o bi
L
o bi
MIN L i L i
CV
MAX L i L i


 


  
 
(a) 
(b) (c) 
研究成果(五)、 材質(texture)與運動(motion)資訊之利用 
Tan 基於 Local Binary Patterns(LBP)提出一個更完整的材質特徵擷取方法，叫做 Local Ternary 
Patterns (LTP)[178]，其概念為使用一個門檻值 T 以濾除雜訊，並且考慮鄰居比自己小的情況，此
法定義如下: 
   1, 0, 2
P p
P R c c p cp
LTP x y s g g


       
 
1        ,  
' , 0       ,  
-1      ,  
p c
c c p c
p c
g g T
s x y g g T
g g T
 

  

 
       
其中 c為目前位置，P 為鄰居的集合，R為鄰居與自己的距離，g代表要對應位置的值，t 為一個
範圍門檻值，代表鄰居與自己的差距至少要高於 t才需要考慮，此大小決定了 LTP 的抗雜訊能力。我
們將 R 設為 1，t設為 3。由於此定義會得到界於-255~255 之間的值，以此結果加上 255，則可以將所
有材質分類為 0~510共 511種材質，沒有負數的情況下才可以直接拿來記錄成直方圖，將上式改為: 
   1, 0, 255 2
P p
P R c c p cp
LTP x y s g g


       
材質特徵亦可比照顏色與亮度的方法取出有分辨性的特徵，如下式: 
 
  
  
max ,
log
max ,
o
o
b
T i
T i
T i


 
  
 
 
       
 
圖 5-1. 材質特徵背投影(back-projection)。(a)輸入影像，(b)目標物的材質直方圖， 
(c)目標物在輸入影像中的材質機率分布圖。 
 材質機率分佈圖顯示如圖 5-1。材質的信任度與顏色、亮度是相同的概念，差別只在於改為針對
材質的目標與背景來考慮，其定義如下式: 
(a) 
(b) (c) 
研究成果(六)、 多特徵融合 (multi-modal fusion)與人形 (human shape)確認  
上述四種特徵的機率密度分布圖計算完成後，應融合為一張最終的機率密度圖。首先顏色、亮度、
材質這三個特徵都是從目標物身上所取得，可以個別特徵之信任度當成權重做線性合併，但是移動特
徵只是單純針對移動的區域做處理，所以不能將移動資訊與其他三種特徵以相同方式合併，結合這四
種特徵的方法如下式: 
                    ,H H L L T T M MP t MAX W t P t W t P t W t P t CV t P t  
 
H
H
H L T
CV
W
CV CV CV

 
 
L
L
H L T
CV
W
CV CV CV

 
         
 
T
T
H L T
CV
W
CV CV CV

 
 
其中的 P(t)為時間 t時合併的機率密度分布圖，PH(t)、PL(t)、PT(t)、PM(t)分別為在時間 t的顏
色、亮度、材質、移動機率密度分布圖，WH、WL、WT分別為將顏色、亮度、材質三種特徵正規化的信任
度。合併後，使用一個遮罩的方式將背景框框之外的部分設定為零，以避免其他區域雜訊的影響，結
合的流程如圖 6-1所示。 
 
圖 6-1. 多特徵合併示意圖 
最後將針對合併的結果使用是先訓練好的人型樣板(template)去做比對，此人型樣板稱為 Humans，
當目標的外型變化成非人型的時候，則判斷為追丟，以避免追到非人的物體。但當目標與另外一個人
接近時，則會使得 Humans有機會框到錯誤的地方，為了解決此問題，我們不直接把 Humans拿來做追蹤，
而是採用一個可以隨著時間而更新目標的樣板，稱作 Us(t)，以此來做追蹤，更新之後再將 Us(t)與
Humans來做比對，人型比對流程如圖 6-2所示。Us(t)的更新方法如下式: 
 
     
 
max1      ,  if C >
1
                            ,  others
s s
s
s
b t U t
U t
U t
   
  

 
WH WL WT 
MA mas
研究成果(七)、人物攤平圓柱樣板之建構、辨識與追蹤 
 攤平圓柱樣板(FCT)代表著每個特定人物的顏色特徵，FCT 的基本概念是將每個人物視為一個圓
柱體，圓柱體的圓柱面上每個像素點的值儲存著從圓柱中心往外投影所得到的顏色值，而將整個圓柱
攤平之後所得到的平面影像就是我們所謂的 FCT。針對每一個新出現的人物，若 RFID 系統成功偵測
到人物 ID，則著手以人物前景區域逐步建構該人物 ID 的 FCT，並進行樣本資料庫的更新。在系統的
初始時，影片中的每個框架經由人物偵測所獲得的人物範圍區塊內都只含有該人物目前某個角度投影
在平面影像上的顏色資訊，為了要獲得完整的 FCT 影像，我們必須利用兩張前後不同時間的框架，分
析這兩張框架中同一個人物的範圍區塊彼此間的差異性，藉此比較該人物在目前的框架中與前張框架
估算轉動角度，並將新資訊記錄起來，藉此逐步建立 FCT 更完整的資訊。 
建立 FCT 前必須先將在影片框架中所獲得之人物區塊影像 I’(上標代表切割影像)轉換成攤平影像
F’，考慮從人物的正上方往下俯視之頭部視角，假設 I’區塊寬度等於人物頭部寬度(2r)，I’平面上所記
錄的顏色資訊是將該人物半圓柱面垂直投影到框架平面所獲得的區塊影像，而 F’則是將人物的半圓柱
面完全攤平所獲得的攤平影像，F’ 區塊的寬度為 w (w=πr)，其中 x,y,r 為已知值，我們可由下式求出
dr: 
( , ) ( , )I dr y F x y   
sin
x
dr r
r
 ．  
 將 I’轉換成 F’後，就可以開始建立 FCT，假設我們已有了某個人物前一個框架的人物範圍區塊影
像 IP’跟目前框架的人物範圍區塊影像 IC’(下標 p 代表前一張框架、c 代表目前框架)，我們先將 IP’跟 IC’
轉換成攤平影像 FP’跟 FC’，接著計算兩張攤平影像之間的比對值 TM (Template Matching Value)，TM
也代表著這兩張攤平影像彼此之間的相似度，該值越大則兩張影像越相似。假設 M 為遮罩(mask)，而
M 則是 M 的反相(取距離近者為 1，遠者為 0)，則 TM 的估算是利用遮罩M 計算兩張影像中所得之相
近點總數目，再除以影像交集比對之面積所得之值，如下式: 
 1-BS BSM M  
/ 2 | |/ 2
/ 2 / 2
( ( min( ,0), ), ( max( ,0), ))
( )
( | |)
w dh
BS P C
y h x w
M F x d y F x d y
TM d
h w d

 
  


 
．
 
上式中的 d 為以 FC’中心點為主所得 FP’中心點的位置，若 d 值為負代表 FP’中心點在 FC’中心點
左方，反之則為正值；假設在影片的每個框架之間人物的轉動角度不可能太大，α 是人物在前後一個
框架之間我們自訂所允許的轉動角度範圍，在正負α範圍之內我們搜尋某個平移量d’可得最大TM值，
針對每一個新出現的人物，若 RFID 系統未偵測到人物 ID，我們利用人物前景區域來比對資料庫
中存在的人物攤平圓柱樣本，採用工作項目四中所定義的 TM 值來當作相似度標準，辨識出相似度最
高的人物 ID，對該人物 ID 區塊做顏色統計，再用中心移動演算法(Mean Shift Algorithm)[16]進行追蹤。
在資料庫比對時如果順利找出正確之 FCT 資料，我們可以採用比對時所得到最高相似度的角度範圍區
塊(該範圍區塊所框出的 FCT 影像正好代表著該人物在場景中正面向某個角度)，並記錄其顏色分佈當
作該人物的目標模組，進一步將該目標模組使用中心移動演算法計算之後，得到該人物在新的框架中
的新位置，針對該位置的人物範圍區塊影像跟 FCT 再做一次比對，假設該人物有轉動的話，新的人物
範圍區塊的影像應會在 FCT 上不同的位置得到相似度最大值，我們將最新的人物範圍區塊當作新的目
標區塊，比對 FCT 重新找出系統最新的目標模組。如果 FCT 的資訊還不足以用來做追蹤，我們可以
直接拿偵測到的人物範圍區塊的影像來建模並做追蹤的工作，邊追蹤邊建立 FCT，直到 FCT 的資訊足
夠為止。 
中心移動演算法的基本精神在於參考一個搜尋視窗內所有像素的顏色分佈，根據物件顏色樣本給
予不同權重，來計算出中心偏移向量(Mean Shift Vector)，藉以追蹤物件。我們的系統架構中，追蹤所
需要的目標模組的來源有兩個，一個是直接使用框架影像中所抓到的人物範圍區塊影像，另一個是使
用 FCT 上比對分數最高的人物角度範圍區塊影像。第一個狀況是在 FCT 剛開始建立時，因為 FCT 的
資訊還不足以利用，只能直接先以在影片框架中所抓到的人物範圍區塊影像當作目標模組；另外一個
狀況是 FCT 已經建立完成的情況，我們會直接採用 FCT 中比對分數最高的人物角度範圍區塊影像來
當作目標模組，並且在接下來每個框架做完追蹤後，更新該角色最新朝向的角度。無論是在哪一種狀
況取得的區塊，紀錄該影像顏色分佈的目標模組 Q 的建立方法如下式:  
,
1... , ...
2 2
{ }u y h h
m y
Q q
 

u
 
2
,
2
(| |) [ ( , ) ]
w
u y
w
x
q C k x I x y u

   
上式所得的Q 會紀錄區塊影像當中的顏色分佈(以該區塊中心為原點，長度 h，高度 w，顏色值的
分佈為 u=1…m)，也就是該影像的顏色統計圖(Histogram)；k 是輻射對稱的核心函式(isotropic kernel) ，
其所帶有的特性是像素特徵位置距離區塊視窗中心點較近者給予較高的權重，這使得區塊週邊的背景
像素得到較小的權重，因此對效能影響跟著變小。而 δ 則為脈衝函式(Knonecker delta function)；如果
I(x,y)= u，則 δ[I(x,y)-u]為 1，反之則為 0。C 則是一個常數，用來做正規化，使得 1
1


m
u
uq 。 
在大多數的中心移動演算法的研究當中，其系統裡所使用的核心函式大多都是使用 Epanechnikov
])([
)(
*)||(||
**)||(||
)(
1
1
2
1
2
uxb
yp
q
w
wxk
xwxk
yM
i
m
u u
u
i
n
i
ii
n
i
iii









 
其中 ωi 為 xi 的權重，y 為追蹤時新的影像的初始追蹤位置，也就是前一張影像物件區塊所在的中心，
而 M(y)則是以初始位置為中心的搜尋視窗的新質心座標，兩者相減就可得到中心偏移向量(Mean Shift 
Vector) MV=M(y)-y，做為目前該視窗追蹤所得到的結果，接著我們將求得的 M(y)位置當做新的 y 值，
重新代入再求得新的 M(y)值，一直持續做下去直到 M(y)收斂到我們所設定的程度(或中心偏移向量夠
小)為止。我們在條件限定的狀況之下(無追蹤狀態，單人影像)所建立的 FCT 影像，如圖 7-2
所示。 
         
         
         
         
圖 7-2. FCT影像的建立，在單調的背景中只進行轉動所建立的 FCT影像，該人物為順時鐘轉動，FCT
影像往右更新。 
  
Frame 75 
Frame 144 
Frame 207 
Frame 274 
假設 C(x,y)為影像序列中的影像，Bm(x,y)為我們所取出來的背景影像模型，Bs(x,y)則是初步的影像
前景切割結果。接下來我們利用立體視覺計算將影像像素轉換成三度空間的座標點群，假設 ( , )tp x y 為
一連續的影像序列中在第 t 個時間點的影像，經由立體視覺計算之後， ( , ) ( , , )t tp x y P X Y Z ，可得其
相對應的三度空間座標 ( , , )tP X Y Z 及該點的顏色C( , , )x y z ： 
C( , , ) = C( , )   ( , ) ( , , )X Y Z x y p x y P X Y Z ｔ ｔ   
其中C( , )x y 表示該點在影像中的顏色，C( , , )X Y Z 表示該點轉換到三度空間之後的顏色，假設點在二
度影像空間的顏色與點在三度空間的顏色應該相同，我們利用立體視覺計算的方法將背景影像模型也
轉換成三度空間的座標點群，接著利用下式將前景做過濾： 
( , )                    ( , ) ( , , )
( , , )     
0                                                                                
s Bs BmB x y if Z Z p x y P X Y ZF X Y Z
otherwise
   
 

ｔ ｔ， －
，
 
其中 F(X,Y,Z)表示我們所過濾出來的前景切割結果在三度空間的表示法，ZBs 與 ZBm 分別代表前景切割
結果 Bs(x,y)的三度空間點群之 Z 座標值與背景影像模型 Bm(x,y)的三度空間點群之 Z 座標值， 為門檻
值，此法主要是藉由立體視覺計算，將前景影像作一個雜點及光影影響過濾的動作。得到 F(X,Y,Z)之
後，我們已經擁有前景物體的三度空間點資訊了，接下來要做的就是從前景物體中做人物的偵測。人
物的偵測在一般的偵測與追蹤系統中扮演著相當重要的一環，而通常可能會對偵測造成影響的因素有
很多，例如：遮蔽現象、影像大小縮放問題、姿勢變動等。Beymer[9]提出佔有圖(Occupancy map)和
Harville[45]提出高度圖(Height map)的觀念，皆是利用三度空間資訊，試圖解決上述問題。 
相對於佔有圖和高度圖的投影方式，我們提出了一個新的前向圖(Front Map)投影方式作為人物偵
測的依據。前向圖的觀念很簡單，主要也是在於三度空間資訊的利用，一般而言影像大小縮放的問題
是起因於第三維座標的變化，因此我們將前景物體的三度空間點群取最近者當成標準作一個前向投影，
然後再將點群中的各個點依寬度為 的直條(Bin)為單位來轉換 X 座標，並且將 Y 座標依人的大致身高
h 為門檻值做切割，最後將可以得到經由前向圖投影之後所轉換的點座標，轉換的方法如下式： 
min
min max
min min
( )
 0.5                   &  
 
                                                   
( )                                ( )  
 
front
front
X X
if X X X X
x
do nothing otherwise
Y Y if Y Y h
y

 
     




－
，
，
－ ， －
max
( , ) ( , , )
           
                                        
min         
( , )                    
          
front frontx y X Y Z
front front
do nothing otherwise
Z if Z Z
Z x y
do nothing otherwise







 

   


，
，
，
在完成立體影像之人物偵測之後，可以得到數個人物候選區塊，我們使用多視角人物樣本(Multiple 
View Template, 簡稱 MVT)，來代表每一個人物的模型，在每個人的 MVT 之中，可以存有多張不同視
角的影像，儲存的影像數量 i 可以視需求而定，第 m 個人的 MVT 的一般式定義如下式：  
Qm,n={Im
1
, Im
2,…, Im
i
,candn} 
為了方便比較第 n 個人物候選區塊與第 m 個 MVT，我們除了原本 MVT 中的 i 張影像之外，另外
再加入第 n 個人物候選區塊的影像 candn。MVT 可以依照需求設定佇列中存有的影像數量，不過當
MVT 的總數或者 MVT 中的影像數量過多的話，勢必會增加儲存空間的需求，因此在我們將影像存入
MVT 之前，先將影像轉換成直方圖(Histogram)以利儲存。一般的直方圖都是藉由統計各個顏色點的總
點數來製成，其橫座標為顏色，縱座標為點數，定義如下式： 
*
0 0
( )   1  ( , , )   ( , , ) ( , , ( , ))
h s
t t
A front front front front
b a
G color if C X Y Z color P X Y Z P x y Z x y

 
    ，  
 其中 A 為 Qm,n 中的一張影像，我們藉由此式將前向圖中偵測到的人物作顏色上的統計，其中 h 為
前向圖長條高度，s為人物所佔的長條數量，δ為長條寬度，color可以視需求而為灰階或者色調值(Hue)。
經由此直方圖的顏色統計，我們可以得知整體的顏色分布。此種直方圖雖然能夠得知構成影像的主要
顏色，可是卻沒有辦法掌握這些顏色的分布狀況，換句話說，即使構成的主要顏色相同，但是這些顏
色經由排列組合之後，可能出現的情況卻不只有一種。因此，使用這種直方圖的作法極有可能會造成
人物跟人物之間的混淆。為了改善這種情況，我們提出一個改良的直方圖，以幾何資訊來增加直方圖
對於顏色分佈的可讀性，然後再將此兩種直方圖作為 MVT 的儲存依據。 
 在前向圖的直條中，同一個直條內可能會包含有很多個點，這些點經由立體視覺計算及前向圖投
影的座標轉換之後所各自擁有的 Y 座標值(即 yfront)也不盡相同，所以為了加入幾何資訊，在這裡我們
針對每一個高度，即每一個 yfront 值去做顏色統計，經由對同一列所有點的顏色統計，我們可以找出該
列數量最多的顏色，然後將此顏色作為該列的代表顏色。我們將直方圖的橫向座標改成 yfront值，換句
話說也就是人的高度，而直方圖的縱向座標改成顏色；經過改良之後的直方圖，將可以很輕易的看出
人物高度與該高度代表顏色的對應關係，改良的直方圖定義如下式： 
( )  arg  max  {( , , ) |  ( , , )  , }A front front
C
H y X Y Z C X Y Z C Y y     
其中 A 為 Qm,n 中的一張影像，而 C 是任意一種顏色資訊，根據需求的不同，C 可以是灰階也可以
是色調值(Hue)。對於同一個 yfront 我們蒐集所有顏色相同的點，然後對每一個從 Y 轉過來的 yfront 都可
以找到一個對應的顏色 C 使得 C 在同樣 yfront 高度的點中為數量最多的顏色，此顏色即為高度 yfront的
代表顏色。經由上面所述的兩種直方圖轉換之後，我們可以很輕易的將原本要存入人物 MVT 之中的
現的人物，並將此新人物與其所有子區塊顏色分怖登錄進資料庫。圖 8-2 為多視角人物樣本之追蹤與
辨識的實驗結果。 
 
在找到 candn 的 ID 之後，我們必須考慮是否對 candn 所對應的個人 MVT 進行更新，因此我們將
candn 加入其所對應的個人 MVT 中，與原本 MVT 中的 i 張影像成為新的影像集合，以再次計算 candn
所對應的個人 MVT 中的各個影像相互之間的距離，藉此來判斷 MVT 是否該對 candn 來進行更新。距
離的計算方法是利用平方差和之和(Sum of Sum of Squared Difference, SSSD)，這邊需要注意的是這裡
所說的平方差和之和與之前的直方圖的平方差和相加結果，雖然計算方法類似，但是在實質上的意義
不同。距離的平方差和計算如下式: 
,
2
0
( , )  ( ( ) ( ))
m n
front
h
Q J front K front
y
SSD J K H y H y

  －  
其中 J 與 K 分別為 Qm,n中的一張影像，經由上式可以得到 Qm,n 中所有影像相互計算出來的平方差
和，由於在 Qm,n 中一共有 i+1 張影像，所以 J 對 K 會產生 i+1 個平方差和，我們將此 i+1 個平方差和
累加起來便是 J 的平方差和之和，又 J 也有 i+1 種可能性，所以一共會有 i+1 個平方差和之和，我們
所要做的就是在這 i+1 者之中，保留住其中平方差和之和最大的 i 個，因為平方差和之和越大就表示
該影像與其他 i 張影像的差異度越大，意即相較於其他 i 張影像較有 candn 的代表性，所以將平方差和
之和最小者挑選出來，並予以剔除，挑選最小者的方法如下式: 
, ,
, , ,
,
arg  min  ( , )    ( , )  
   
                                                                    
m n m n
m n m n m n
m n
Q Q n
J Q K Q K Q
Q
n
SSD J K if SSD cand K
U
cand otherwise

  
 

 

 ，
，
 
上式從 Qm,n 中找到一個 J 使得 J 與 Qm,n 中的所有影像的平方差和為最小，
,m nQ
U 即為從 Qm,n 找到需
要被更新的 J，不過如果當 candn 與 Qm,n 的平方差和之和大過我們所設置的門檻值 時，表示此 candn
很有可能雜點過多或者根本是垃圾資訊，此時則直接選擇將 candn 丟棄。如果我們找到的
,m nQ
U 就是
candn 的話，則直接丟棄 candn，不需要更新個人 MVT，否則的話表示 candn 是我們需要的影像，以 candn
(a)       (b)    (c)        (d)      (e) 
圖 8-2. 多視角人物樣本之追蹤與辨識的實驗結果 (a) 原始影像 (b) 前景之人物遮罩 (c) 
以顏色區分之人物辨識結果 (d) 第一人之多視角樣本 (e) 第二人之多視角樣本 
研究成果(九)、遮蔽之偵測與處理 
利用中心移動演算法做追蹤之後，每一個人物區塊都會各自得到一個新的位置值，而在正常的情
況之下，區塊的移動向量應該是伴隨著人物的移動向量往同樣的方法前進，但是仍然不能排除有些特
別的情況發生，例如人物的某一區塊被背景物或是被另外一個人物所影響，使得該區塊往不可預測的
方向移動，或是因為遮蔽(Occlusion)而使得某些區塊的追蹤結果失敗，我們針對這些個別狀況找到解
決之道，我們必須判斷每個人物區塊是否有發生遮蔽的情形，做為是否要更新色彩樣本資訊或是否需
要進行復原的依據；追蹤時有可能因為被遮蔽或是其他狀況導致相似值變低，我們應對各個人物區塊
判斷是否有遮蔽發生。在我們的系統下，可能發生遮蔽的情況有兩種，第一種狀況是一個人被另一個
人所遮蔽，另一個狀況是一個人被物件所遮蔽。如果人物是被另一個人所遮蔽，這時只要做兩個人物
的 bounding box 的交集，再加上分析可能碰撞區塊的相似度變化；相似度變小的區塊，代表著該區塊
所屬的人物發生被遮蔽的狀況。而如果人物是被物件所遮蔽的話，則只要去觀察區塊相似值的變化即
可判定。 
遮蔽一旦已經發生，追蹤的效果則開始大打折扣；當人物區塊在追蹤過程中相似值開始下降的時
候，只要其值降到一個門檻值以下，我們直接判定該人物區塊被遮蔽，被遮蔽的人物區塊不再進行追
蹤，而是利用其他的線索來推測人物所移動的位置；可以利用該人物區塊還沒被遮蔽前的框架的移動
位量來計算其速度向量，推測現在的框架該人物應該移動到的位置，在這些位置上設一圓型搜尋視窗
來做搜尋，找到相似度值最高的位置，如果還是找不到，則回到原來該人物區塊未被遮蔽前的位置做
搜尋，系統亦應對之前失敗的人物區塊做復原的動作。 
在一般物件追蹤的系統當中，最常碰到需要處理的問題就是遮蔽跟比例變化這兩種狀況。在我們
的系統中，我們利用了在前面所講過的 TM 值，再加上一個由我們所定義的密度值 DV(Density Value)，
假設目前框架執行中心移動演算法所找到的最終位置為(x’,y’)，且該位置之範圍區塊影像為 IC’，又目
前 FCT 之角度區塊允許之 α 範圍寬度的影像為 IFCT’。我們定義 IFCT’影像上所有像素點顏色集合為
{ ( , ),( , ) }FCT FCT FCTC I x y x y I   ，則我們可以利用此集合獲得跟影像 IC’同大小的二元遮罩 MDV，而 DV
值則是總合所有 MDV 上所有值為 1 的點，再除以 MDV 的面積即可得。DV 值的求法如下式: 
1, ( , )
( , ) ,    (( , ) )
0,
FCT
DV
f I x y C
cM I x y x y I
c therwise c
 
  

　　i 　
（ ）
　　o
 
/ 2 / 2
/ 2 / 2
( ( , ))
( )
*
h w
DV c
y h x w
c
M I x y
DV I
w h
 

 
 
 
TM 曲線跟 DV 曲線是藉由 TM 遮罩跟 DV 遮罩所求得的曲線，我們將 TM 或 DV 遮罩依高度畫分
成 h 列，每列都去計算該列像素點數值的總合，最後再除以該列的寬度，即可得到介於 0~1 之間的數
值，將分數值當做 x 軸，該人物的高度當做 y 軸，即可得該曲線。 
在每個框架跑完一次追蹤演算法之後，我們便可以針對最新的人物範圍區塊影像所計算出的 TM
值跟 DV 值去分析目前的框架該人物是否有狀況發生，一旦偵測出有人物比例縮放的情形，我們應根
據狀況將範圍區塊影像縮放(|DV- (DV_basic)| / 2)倍，並計算依比例縮放之後所求得新的 TM 值跟新的
DV 值是否介於 TM 值跟 DV 值的上下界範圍之內，如果是的話才會將其比例成功轉換，反之則不做任
何動作；而如果是部份遮蔽狀況的話，我們會忽略不理，因為即使有部份被遮蔽了，還是有很多其他
的部分可以用來追蹤；而如果是全部遮蔽的狀況，我們則是會利用前面幾張框架所得到的人物位置去
預測人物下個框架應該會移動的位置。一般而言，一旦我們發現相似度 TM 值過低時，就會終止追蹤
動作，判定該人物離開場景。 
圖 9-3 是在較低的場景複雜度下，以建好的 FCT 影像來追蹤人物的結果，由計算得到的
TM-DV 曲線圖中可看出，FCT 影像可以求得正確的 TM-DV 曲線而使我們做更精確的遮蔽判斷，
右上角的紅字是根據 TM值、DV 值跟 TM-DV曲線所得的自動判定結果。 
(c)          (d) 
圖 9-2. 模擬四種不同的狀況底下應得的 TM值跟 DV 值的變化 (a) 正常狀況 (b) 人物變小  
(c) 人物變大 (d) 被遮蔽 
TM ? 
 
DV > DV 上界 
 
TM < TM 下界 
 
DV < DV 下界 
 
TM 下界<TM<TM 上界 
 
DV 下界<DV<DV 上界 
 
TM < TM 下界 
 
DV < DV 下界 
 
(a)          (b)  
 五、結果與討論 
圖 10-1為執行程式時的介面，由圖左上角可知目前我們已經取出所有機器人感測模組的數據，
機器人的攝影機所看到的影像可由右上角的區域看到，並且可以以手動方式做簡單機器人移動、發聲
等控制。圖 10-1的下方包含有前景背景切割的演算法、兩種人物追蹤的演算法、與移動機器人的演
算法(結合所有流程)。 
圖 10-2 為目前執行結果，由於我們並不採用機器人本身的攝影機，而採用外接在筆電上的攝影
機(webcam)，所以我們另外將攝影機影像顯示於螢幕左上角。而各個部分的執行結果視窗依序由左到
右再由上到下的順序排列在介面視窗，圖 10-2 可以看到完成了 Block 1(前景背景切割並取出所需要
的前景)以及 Block 2(追蹤前景)的 camshift algorithm和 LK algorithm部分，至於合併之演算法及
Block 3(控制機器人移動)部分則尚未實作。 
目前實做發現可能會遇到一些問題，第一，所選的特徵點常可能會滑動到相似的地方，除了背景
之外，就算同樣是目標區域但是卻不同位置的地方，造成特徵點密集在某些區塊，但是某些地方卻可
能沒有特徵點，造成得到的新目標區域變小；第二，由於每個 frame 都把可能追丟的特徵點丟棄，則
特徵點所框選的區域範圍會越來越小。 
上述兩個問題都會使得追蹤目標框選範圍變小，解決辦法目前可分兩個方向；首先，針對特徵點
何時需要重新給予做比較精細的判斷，像是當特徵點之間不是較為分散的狀態，則考慮重新給予特徵
點；第二，針對移動控制方面，假設目標物移動速度不會變化劇烈，也就是說距離不會突然從很近變
很遠，我們可設定一個門檻值，讓新框選的區域不會對移動速度上造成太劇烈的影響，以避免爆走急
停的現象。 
 
圖 10-1. 系統介面圖  
 
    
圖 10-3. 執行結果圖，順序為由左到右。(a)為追蹤結果影像，(b)(c)(d)(e)分別為顏色、亮度、材
質、移動的機率密度分布圖，(f)為合併後的機率密度分布圖。 
  
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
   
   
(a) 
   
   
(b) 
圖 10-6.  顏色特徵必要性的個別特徵結果圖。 
有使用顏色特徵(Ours-M)的結果顯示於(a)。 
不使用顏色特徵(Ours-C-M)的結果顯示於(b)。 
圖片中上排由左至右依序為結果影像、顏色特徵、亮度特徵， 
而下排依序為材質特徵、移動特徵、所有特徵合併結果。 
  
   
   
(a) 
   
   
(b) 
圖 10-9. 亮度特徵必要性的個別特徵結果圖。 
有使用亮度特徵(Ours-M)的結果顯示於(a)。 
不使用亮度特徵(Ours-L-M)的結果顯示於(b)。 
圖片中上排由左至右依序為結果影像、顏色特徵、亮度特徵，而下排依序為材質特徵、移動特徵、所
有特徵合併結果。 
  
    
   
(a) 
   
   
(b) 
圖 10-12. 材質特徵必要性的個別特徵結果圖。 
有使用材質特徵(Ours-M)的結果顯示於(a)。 
不使用材質特徵(Ours-T-M)的結果顯示於(b)。 
圖片中上排由左至右依序為結果影像、顏色特徵、亮度特徵，而下排依序為材質特徵、移動特徵、所
有特徵合併結果。 
  
   
   
(a) 
   
   
(b) 
圖 10-15. 移動特徵必要性的個別特徵結果圖。 
我們的完整方法(Ours)的結果顯示於(a)。 
不使用移動特徵(Ours-M)的結果顯示於(b)。 
圖片中上排由左至右依序為結果影像、顏色特徵、亮度特徵，而下排依序為材質特徵、移動特徵、所
有特徵合併結果。 
  
 
圖 10-17.自我遮蔽的偏移量曲線圖。 
當找不到目標物時將偏移量設定為 400。 
 
 
圖 10-18.自我遮蔽的正確率曲線圖。 
當找不到目標物時將正確率設定為 0。 
  
0
5
10
15
20
25
30
35
60 67 74 81 88 95 10
2
10
9
11
6
12
3
13
0
13
7
14
4
15
1
15
8
16
5
17
2
17
9
18
6
19
3
P
os
it
io
n 
of
fs
et
 
Frame number 
Ours
Ours-M
Ours-C-M
Ours-L-M
Ours-T-M
0
0.2
0.4
0.6
0.8
1
60 67 74 81 88 95 10
2
10
9
11
6
12
3
13
0
13
7
14
4
15
1
15
8
16
5
17
2
17
9
18
6
19
3
C
or
re
ct
 r
at
e 
Frame number 
Ours
Ours-M
Ours-C-M
Ours-L-M
Ours-T-M
 圖 10-20.部分遮蔽的偏移量曲線圖。 
當找不到目標物時將偏移量設定為 400。 
 
 
圖 10-21.部分遮蔽的正確率曲線圖。 
當找不到目標物時將正確率設定為 0。 
  
0
5
10
15
20
25
30
35
P
os
it
io
n 
of
fs
et
 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
C
or
re
ct
 r
at
e 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
 
圖 10-23.暫時遮蔽的偏移量曲線圖。 
黃色區塊所框選的範圍在 frame 123到 180為目標完全遮蔽時。 
當找不到目標物時將偏移量設定為 400。 
 
 
圖 10-24.暫時遮蔽的正確率曲線圖。 
黃色區塊所框選的範圍在在 frame 123到 180為目標完全遮蔽時。 
當找不到目標物時將正確率設定為 0。 
 
  
0
5
10
15
20
25
30
35
70 78 86 94 10
2
11
0
11
8
12
6
13
4
14
2
15
0
15
8
16
6
17
4
18
2
19
0
19
8
20
6
21
4
P
os
it
io
n 
of
fs
et
 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
0
0.2
0.4
0.6
0.8
1
70 78 86 94 10
2
11
0
11
8
12
6
13
4
14
2
15
0
15
8
16
6
17
4
18
2
19
0
19
8
20
6
21
4
C
or
re
ct
 r
at
e 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
 
圖 10-26.超出視角的偏移量曲線圖。 
黃色區塊所框選的範圍在 frame 120到 200為目標離開視角時。 
當找不到目標物時將偏移量設定為 400。 
 
 
圖 10-27.超出視角的正確率曲線圖。 
黃色區塊所框選的範圍在 frame 120到 200為目標離開視角時。 
當找不到目標物時將正確率設定為 0。 
從圖 10-26.與圖 10-27.中可以看到不使用亮度特徵(Ours-L)的方法一開始沒多久就追蹤失敗好多
次，最後完全追蹤失敗。這與 4.4.2.暫時遮蔽(Temporary occlusion)的解釋一樣，由於目標物身上穿著
全黑色，且背景正好為正常亮度的紅色與綠色，因此使得使用色度的顏色特徵無法分辨出來，加上目
標身上沒有花紋，使得材質特徵也無法分辨，而為有亮度特徵在這時候可以明顯的分辨出目標，移動
特徵此時用處不大是因為攝影機不斷移動，而且背景的紋路複雜，造成移動特徵的信任度被壓抑下來，
若移動特徵考慮太多會產生過多雜訊影響追蹤，我們這邊將不使用亮度特徵(Ours-L)的方法在 frame 73
時目標詳細的個別特徵結果圖如圖 10-25.所示。 
  
0
5
10
15
20
25
30
35
70 79 88 97 10
6
11
5
12
4
13
3
14
2
15
1
16
0
16
9
17
8
18
7
19
6
20
5
21
4
22
3
23
2
24
1
25
0
P
os
it
io
n 
of
fs
et
 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
0
0.2
0.4
0.6
0.8
1
70 79 88 97 10
6
11
5
12
4
13
3
14
2
15
1
16
0
16
9
17
8
18
7
19
6
20
5
21
4
22
3
23
2
24
1
25
0
C
or
re
ct
 r
at
e 
Frame number 
Ours
Ours-C
Ours-L
Ours-T
Ours-M
表 10-3. 靜態攝影機影像序列的平均正確率。1為完全正確，0為完全追丟。 
 Need color Need 
lightness 
Need  
texture 
Need  
motion 
Self-occlusi
on 
Ours 0.770349 0.716157 0.804062 0.669978 0.755731 
Ours-M 0.772456 0.716157 0.812207 0 0.756158 
Ours-C-M 0.0527781 0.705924 0.839049 0 0.78257 
Ours-L-M 0.698978 0.24869 0.838317 0 0.758342 
Ours-T-M 0.59117 0.716802 0.240654 0 0.691473 
表 10-4. 動態攝影機影像序列的平均正確率。1為完全正確，0為完全追丟。 
 Partial- occlusion Temporary occlusion Out of range 
Ours 0.726706 0.609931 0.702324 
Ours-C 0.653021 0.693619 0.680567 
Ours-L 0.698306 0.20446 0.0222397 
Ours-T 0.673741 0.640696 0.64581 
Ours-M 0.697088 0.610968 0.669856 
我們提出的方法中有許多特殊使用特徵的方式，例如將亮度、材質資訊使用背投影的方式取得，
移動資訊採用 MHI 的方法取得，這些方法雖然簡單卻容易造成大量雜訊，產生的資訊也不夠完整，
但是我們採以簡單且合理的方式同時採納各種特徵的資訊，就能夠達到增強準確率的結果， 
 接下來的表 10-5.與表 10-6.記錄了我們演算法在所有影像序列的平均執行速度，可以看到我們
的方法執行起來相當快速，其中最完整且執行速度最慢的所有特徵全部使用的情況下仍能保持每秒超
過 20 個 frame，符合當前追蹤所要求的即時性(Real time)。 
表 10-5. 靜態攝影機影像序列的平均執行效率(FPS) 
 Need color Need 
lightness 
Need  
texture 
Need  
motion 
Self-occlusi
on 
Ours 23.2963 28.5534 29.2591 31.7179 29.6572 
Ours-M 31.8046 31.2333 46.3499 63.7231 31.6244 
Ours-C-M 61.0839 31.963 32.0209 60.0739 31.8492 
Ours-L-M 31.97 30.6549 31.9813 63.5148 31.8636 
Ours-T-M 53.9603 37.4803 53.832 63.4375 32.0132 
表 10-6. 靜態攝影機影像序列的平均執行效率(FPS) 
 Partial- occlusion Temporary occlusion Out of range 
Ours 25.2903 37.7635 43.2463 
Ours-C 32.2581 42.4366 43.3367 
Ours-L 31.3913 60.6695 55.4856 
Ours-T 33.7602 48.6012 70.52 
Ours-M 32.032 41.8149 42.8296 
本計劃第一年已嘗試針對人物步伐(gait)進行分析與辨識，探索人物外觀(appearance)資訊建模
與肢體姿勢(posture)參數擷取，並實作移動機器人之人物追蹤與跟隨；研究結果發現人物外觀
比人物步伐包含更多有用的資訊，可以進一步有效利用。人物追蹤系統要保持其穩定性，就必須在追
蹤與辨識過程中針對各種可能出現的問題，如人物遮蔽問題與人物大小比例縮放的問題，做準確的分
析和適當的處理，再針對不同狀況做最出即時正確的回應，以維持追蹤系統的效能。 
為了解決第一年遭遇的問題，本計劃第二年已嘗試擷取人物顏色(color)、亮度(lightness)、材
質(texture)、運動(motion)與形狀(shape)資訊來動態更新與建構人物特徵資訊，利用並融合這些視
覺式人物特徵資訊來迅速且準確的追蹤不同的人物。 
根據前兩年的研究發現，本計劃第三年專注於開發立體視覺之人物外觀樣板技術以解決更困難的
遮蔽問題。攤平圓柱樣板(FCT)整合記錄人物周身 360 度的顏色資訊，多視角樣板(MVT)分別記錄人物
多個視角的外觀資訊；利用這些新的視覺式人物樣板技術，我們可以辨別不同的人物，且不會因為許
多人物同時在場景中交錯移動、互相遮蔽，而影響其辨識與追蹤的效果。 
若 RFID 系統成功偵測到人物 ID，則著手以人物前景區域逐步建構該人物 ID 的 FCT 或 MVT 樣
板，並更新資料庫。反之，若 RFID 系統未偵測到人物 ID，則利用人物前景區域來比對資料庫中存在
的 FCT 或 MVT 樣板，辨識出相似度最高的人物 ID 並進行追蹤。如此，家中成員即使未配戴個人 RFID 
Tag 時，仍可由視覺系統正確辨識；而未配戴 RFID Tag 之侵入者，也會被視覺系統發現並進一步以移
動式機器人追蹤與監控。 
經由實驗結果，我們可以了解利用我們所提出的人物外觀樣板技術進行人物偵測、辨識與追蹤是
可行的，可以處理發生遮蔽現象之後所產生的誤判狀況，但是還無法對遮蔽現象做出更進一步的分析
與還原，這一部分將是未來值得探討的方向；另一個值得改進的地方便是系統的執行時間，由於人物
外觀樣板的建構與搜尋必須進行相當複雜且龐大的運算，因此如何改進系統的執行速度以求達到即時
運算，也是值得繼續思考的方向。 
 
六、參考文獻 
[1] B. Abboud and G. Chollet. Appearance based lip tracking and cloning on speaking faces. ISPA, 2005. 
[2] M. Agrawal, K. Konolige, and L. Iocchi. Real-time detection of independent motion using stereo. IEEE 
Workshop on Motion and Video Computing, 2005. 
[3] P. Aleksic, J. Williams, Z. Wu, and A. Katsaggelos. Audio-visual speech recognition using MPEG-4 
compliant visual features. EURASIP Journal of Applied Signal Processing, pp. 1213-1227, 2002. 
[4] S. Bahadori, G. Grisetti, L. Iocchi, G. Leone, and D. Nardi. Real-time tracking of multiple people 
through stereo vision. IEE International Workshop on Intelligent Environment, 2005. 
[5] S. Bahadori, L. Iocchi, G. Leone, D. Nardi, and L. Scozzafava. Real-time people localization and 
tracking through fixed stereo vision. International Conference on Industrial and Engineering Applications of 
Artificial Intelligence and Expert System (IEA/AIE), 2005. 
[6] K. Balci. Xface: MPEG-4 based open source toolkit for 3d facial animation. Working Conference on 
Advanced Visual Interfaces, 2004. 
[7] S. Battiato, A. Capra, S. Curti, and M. Cascia. 3D Stereoscopic Image Pairs by Depth-Map Generation. 
International Symposium on 3D Data Processing, Visualization, and Transmission, 2004. 
[8] D. Beymer and K. Konolige. Real-time tracking of multiple people using continuous detection. IEEE 
Frame Rate Workshop, 1999. 
[9] D. Beymer. Person counting using stereo. Workshop on Humann Motion, pp.127-133, 2000.  
[10] N. Bird, O. Masoud, N. Papanikolopoulos, and A. Issacs. Detection of Loitering Individuals in Public 
Transportation Areas. IEEE Transactions on Intelligent Transportation Systems, 2005. 
[11] C. Bregler and Y. Konig. Eigenlips for Robust Speech Recognition. International Conference on 
Acoustics; Speech and Signal. Processing, pp. 669-672, 1994. 
[12] L. Brethes, P. Menezes, F. Lerasle, and J. Hayet. Face tracking and hand gesture recognition for 
boosting. Computational Learning Theory: Eurocolt, pp. 23-37. Springer-Verlag, 1995. 
[40] G. Fung, N. Yung, G. Pang, and A. Lai. Towards detection of moving cast shadows for visual traffic 
surveillance. IEEE International Conference on Systems, Man, and Cybernetics, pp. 2505 – 2510, Vol.4, Oct 
2001. 
[41] B. Funt and G. Finlayson. Color Constant Color Indexing. IEEE Transactions on Pattern Analysis and 
Machine Intelligence,  Issue 5, pp. 522 – 529, May 1995.  
[42] T. Gandhi, M. Yang, R. Kasturi, O. Camps, and L. Coraor. Detection of Obstacles in the Flight Path of 
an Aircraft. IEEE Transactions on Aerospace and Electronic System, Vol. 39, no. 1, pp. 176-191, January, 
2003. 
[43] T. Gandhi, M. Yang, R. Kasturi, O. Camps, L. Coraor & J. McCandless. Performance Characterization 
of the Dynamic Programming Obstacle Detection Algorithm. IEEE Transactions on Image Processing, Vol. 
15, no. 5, pp. 1202-1214, May, 2006.  
[44] M. Gordan, C. Kotropoulos, and I. Prtas. Pseudo automatic Lip Contour Detection Based on Edge 
Direction Patterns. International Symposium on Image and Signal Processing and Analysis, Pula, Croatia, 
138–143, 2001. 
[45] M. Harville. Stereo person tracking with adaptive plan-view statistical templates. ECCV Workshop on 
Statistical Methods in Video Processing, pp. 67-72,June, 2002. 
[46] M. Harville and D. Li. Fast,integrated person tracking and activity recognition with plan-view templates 
from a single stereo camera. International Conference on Computer Vision and Pattern Recognition, 2004. 
[47] M. Harville. Stereo person tracking with short and long term plan-view appearance models of shape and 
color. IEEE Conference on Advanced Video and Signal Based Surveillance, 2005. 
[48] R. Herbert, S. Thomas, B. Csaba, W. Martin, and B. Horst. Shape -based Detection of Humans for Video 
Surveillance Applications. IEEE Conference on Computer Vision and Image Processing, 2003. 
[49] E. Hjelmas and B. Low. Face detection : a survey. Journal of Computer Vision and Image 
Understanding, 2001. 
[50] D. Hoffman and W. Richards. Parts of Recognition. Cognition, Vol.18, pp.65-96,1984.  
[51] X. Hou, S. Li, H. Zhang, and Q. Cheng. Direct Appearance Models. IEEE Conference on Computer 
Vision and Pattern Recognition, Vol. 1, pp. 828-833, 2001. 
[52] C. Hsieh, Y. Huang, H. Zhao, J. Chen, and Y. Tsai. Multiple-person tracking system using support vector 
machine. Proceedings of CVGIP, 2001. 
[53] J. Hsieh, S. Yu, Y. Chen, and W. Hu. A Shadow Elimination Method for Vehicle Analysis. International 
Conference on Pattern Recognition, 2004. 
[54] A. Hulbert and T.Poggio. Synthesizing a color algorithm from examples. Science, Vol. 239, pp. 482-485, 
Jan. 1988. 
[55] L. Iocchi and R. Bolles. Integrating plan-view tracking and color-based person models for multiple 
people tracking. IEEE International Conference on Image Processing, 2005. 
[56] M. Isard and A. Blake. Contour tracking by stochastic propagation of conditional density. European 
Conference on Computer Vision, 1996. 
[57] Y. Ishii, H. Hongo, K. Yamamoto, and Y. Niwa. Face and Head Detection for a Real-Time Surveillance 
System. International Conference on Pattern Recognition, 2004. 
[58] R. Ishiyama, and S. Sakamoto. Fast and Accurate Facial Pose Estimation by Aligning a 3D Appearance 
Model. International Conference on Pattern Recognition, 2004. 
[59] ISO/IEC FDIS 14496-2:2001 Visual, ISO/IEC JTC1/SC29/WG11 N2502, Nov. 2001. 
[60] P. Juell and R. Marsh. A hierarchical neural network for human face detection. Pattern Recognition, pp. 
781-787, 1996. 
[61] T. Kanade, A. Yoshida, K. Oda, H. Kano, and M. Tanaka. A stereo machine for video rate dense depth 
mapping and its new applications. IEEE International Conference on Computer Vision and Pattern 
Recognition, 1996. 
[62] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models. International Journal of 
Computer Vision, pp. 321-331, 1988. 
[63] R. Koenen. Overview of the MPEG-4 Standard. ISO/ IEC JTC1/ SC29/WG11 N4668, Mar. 2002. 
http://www.chiariglione.org/mpeg/standards/mpeg-4/mpeg-4.htm. 
[64] K. Konolige. Small Vision System： Hardware and implementation. Robotics Research, 1997. 
[65] J. Krumm, S. Harris, B. Brumitt, M. Hale, and S. Shafer. Multi-camera multi-person tracking for 
easyliving. International Workshop on Visual Surveillance, 2000. 
International Conference on Image Processing, 2005. 
[94] Y. Shih and M. Yang. A Collaborative Virtual Environment for Situated Language Learning Using 
VEC3D, Journal of Educational Technology and Society, vol. 11, issue 1, pp. 56-68, 2008. 
[95] S. Shimizu, K. Yamamoto, C. Wang, Y. Sato, H. Tanahashi, and Y. Niwa. Moving object detection with 
mobile stereo omni-directional system(SOS) based on motion compensatory inter-frame depth subtraction. 
International Conference on Pattern Recognition, 2004. 
[96] K. Siala, M. Chakchouk, O. Besbes, and F. Chaieb. Moving Shadow Detection with Support Vector 
Domain Description in the Color Ratios Space. International Conference on Pattern Recognition, pp. 384 – 
387, Vol.4, Aug. 2004.  
[97] M. Singh, G. Seyranian, and D. Hoffman. Parsing Silhouettes: the Short-Cut Rule. Perception and 
Psychophysics, Vol.61, No.4, pp.636-660, May 1999. 
[98] L. Sirovinch and M. Kirby. Low-dimensional procedure for the characterization of human faces. Journal 
of Opt. Soc. Amer.,pp. 519-254, 1987. 
[99] X. Song and R. Nevatia. Combined Face-body Tracking in Indoor Environment. International 
Conference on Pattern Recognition, 2004.  
[100] M. Soriano, B. Martinkauppi, S. Huovinen, and M. Laaksonen. Skin detection in video under 
changing illumination conditions. IEEE International Conference on Pattern Recognition, 2000. 
[101] J. Stauder, R. Mech, and J. Ostermann. Detection of Moving Cast Shadows for Object 
Segmentation. IEEE Transactions on Multimedia, 1(1):65 - 76, Mar 1999. 
[102] M. Stegmanm. Analysis and Segmentation of Face Images using Point Annotation s and Linear 
Subspace Techniques. Technical Report, DTU, 2002.  
[103] M. Swain and D. Ballard. Color Indexing. International Journal of Computer Vision, Vol.7, 
pp.11-32, 1991. 
[104] H. Tao, H. Chen, W. Wu, and T. Huang. Compression of MPEG-4 Facial Animation Parameters for 
Transmission of Talking Heads. IEEE Trans. Circuit and Systems for Video Technology, Vol. 9, no. 2, pp. 
264-276, Mar. 1999. 
[105] D. Terzopoulos and K. Waters. Analysis and synthesis of facial image sequences using physical and 
anatomical models. IEEE Transactions on Pattern Analysis Machine Intelligence, Vol. 25, pp. 569-579, 1993. 
[106] Y. Tian, T. Kanade, and J. Cohn. Recognizing lower face action units for facial expression analysis. 
IEEE International Conference on Automatic Face and Gesture Recognition, 2000. 
[107] D. Toth, I. Stuke, A. Wagner, and T. Aach. Detection of moving shadows using mean shift 
clustering and a significance test. International Conference on Pattern Recognition, pp. 260 – 263, Vol.4, 
Aug. 2004. 
[108] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, pp. 71-86, 
1991. 
[109] M. Turk and A. Pentland. Eigenfaces for Recognition. Journal of Cognitive Neuroscience, Vol. 3 , 
No. 1, pp. 71–86, 1991. 
[110] M. Turk and A. Pentland. Face Recognition Using Eigenfaces. Computer Vision and Pattern 
Recognition, 1991. 
[111] Tyzx Inc. http://www.tyzx.com 
[112] Videre Design. http://www.videredesign.com 
[113] P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer Vision, 
2001. 
[114] D. Vukadinovic and M. Pantic. Fully Automatic Facial Feature Point Detection Using Gabor 
Feature Based Boosted Classifiers. International Conference on Systems, Man and Cybernetics, 2005. 
[115] T. Wakasugi, M. Nishiura, and K. Fukui. Robust lip contour extraction using separability of 
multi-dimensional distributions. IEEE International Conference on Automatic Face and Gesture Recognition, 
pp. 415-420, May 2004. 
[116] C. Wang, Y. Chang, and Y. Chen. Moving object extraction using mosaic technique and tracking 
with active camera. Proceedings of CVGIP, 2001. 
[117] G. Wang, M. Yang, C. Chiang, and W. Tai. A Talking Face Driven by Voice Using Hidden Markov 
Model. Journal of Information Science and Engineering, Vol. 22, pp. 1059-1075, September, 2006.   
[118] J. Wang and S. Chen. Progressive background image generation. CVGIP, 2001. 
[119] J. Wang, Y. Chung, C. Chang, and S. Chen. Shadow Detection and Removal for Traffic Images. 
International Conference on Networking, Sensing and Control, pp. 649 – 654, Vol. 1, Mar. 2004. 
[120] G. Welch and G. Bishop. An introduction to the Kalman filter. Technical report, TR 95-041, 
[147] K. Kim, T. Chalidabhongse, D. Harwood, L. Davis. Real-time foreground-background 
segmentation using codebook model. Elsevier, 2005 
[148] M. Lee and I. Cohen. Proposal Maps driven MCMC for Estimating Human Body Pose in Static 
Images. CVPR, 2004 
[149] M. Liem, A. Visser, F. Groen. A Hybrid Algorithm for Tracking and Following People using a 
Robotic Dog. 3rd ACM/IEEE international conference on Human robot interaction, pp. 185-192, 2008. 
[150] Y. Liu, R. Collins, and Y. Tsin. Gait sequence analysis using frieze patterns. Proc. Eur. Conf. 
Computer Vision, pp. 659–671, May 2002.  
[151] Z. Liu & S. Sarkar. Improved Gait Recognition by Gait Dynamics Normalization. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, Vol. 28, Issue 6, pp. 863 - 876, June 2006 
[152] Z. Liu & S. Sarkar. Outdoor recognition at a distance by fusing gait and face. Image Vision 
Comput., Vol. 25,  pp. 817–832, 2007 
[153] W. Long, and Y.H. Yang. Stationary Background Generation：An Alternative to the Difference of 
Two Images. Pattern Recognition, Vol. 23, pp. 1351-1359, 1990. 
[154] K. Okuma, A. Taleghani, N. Freitas, J. Little, and D. Lowe. A Boosted Particle Filter: Multitarget 
Detection and Tracking.ECCV, pp. 28-39, 2004. 
[155] J. Shi and C. Tomasi. Good Features to Track. IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 593-600, 1994.  
[156] C. Sminchisescu, G. Triggs. Kinematic Jump Processes For Monocular 3D Human Tracking. CVPR, 
2003 
[157] Z. Tang & Z. Miao. Fast Background Subtraction and Shadow Elimination Using Improved 
Gaussian Mixture Model. IEEE International Workshop on Haptic, Audio and Visual Environments and 
Games, 12-14, pp. 38 – 41, Oct. 2007 
[158] P. Vadakkepat, S. Member, & P. Lim. Multimodal Approach to Human-Face Detection and 
Tracking. IEEE Journal on Industrial Electronics, pp. 1385-1393, 2008. 
[159] A. Yuille, D. Kersten. Vision as Bayesian inference: analysis by synthesis? July. Elsevier, 2006 
[160] X. Zhou and Z. Zhao. Construction of 3D Virtual Humans from 2D Images. ICNSC, 2008 
[161] S. Zhu, F. Delleart and Z. Tu. Markov Chain Monte Carlo for Computer Vision. 
http://civs.ucla.edu/MCMC/MCMC_tutorial.htm, ICCV, 2005 
[162] Z. Zivkovic. Improved adaptive Gaussian mixture model for background subtraction. IEEE 
Conference on Pattern Recognition, pp. 28-31, 2004. 
[163] Z. Zivkovic and B. Krose. An EM-like algorithm for color-histogram-based object tracking. IEEE 
Conference on Computer Vision and Pattern Recognition, pp. 798-803, 2004. 
[164] B. Zou, S. Chen, C. Shi, U. Marie Providence. Automatic reconstruction of 3D human motion pose 
from uncalibrated monocular video sequences based on markerless human motion tracking. Dec.19.2007 
[165] A. Adam, E. Rivlin & I. Shimshoni, Robust Fragments-based Tracking using the Integral 
Histogram, IEEE Computer Society Conference of Computer Vision and Pattern Recognition, vol. 1, pp. 
798-805, 2006. 
[166] B. Babenko, M. Yang & S. Belongie, Visual Tracking with Online Multiple Instance Learning, 
IEEE Conference of Computer Vision and Pattern Recognition, pp. 983–990, 2009. 
[167] H. Baskoro, J. Kim & C. Kim, Mean-Shift Object Tracking with Discrete and Real AdaBoost 
Techniques, ETRI Journal, vol. 31, issue. 3, pp. 282-291, 2009 
[168] O. Bimber, R. Raskar, Tracking multiple people with recovery from partial and total occlusion, 
ACM Journal of Pattern Recognition, vol. 38, issue 7, pp. 1059-1070, 2005 
[169] Z. Chen, S. T. Birchfueld, Person Following with a Mobile Robot Using Binocular Feature-Based 
Tracking, IEEE/RSJ International Conference of Intelligent Robots and Systems, pp. 815-820, 2007. 
[170] Y. Cheng, Mean Shift, Mode Seeking, and Clustering, IEEE Journal of Pattern Analysis and 
Machine Intelligence, vol. 17, issue 8, pp. 790-799, 1995.  
[171] R. Collins, Y. Liu & M. Leordeanu, Online Selection of Discriminative Tracking features, IEEE 
Journal of Pattern Analysis and Machine Intelligence, vol. 27, issue 10, page 1631-1643, 2005. 
[172] D. Gavrila, A Bayesian, Exemplar-Based Approach to Hierarchical Shape Matching, IEEE Journal 
of Volume Pattern Analysis and Machine Intelligence, vol. 29, issue 8, pp. 1408-1421, 2007. 
[173] P. Li, An adaptive Binning Color Model for Mean Shift Tracking, PhD thesis, IEEE Journal of 
Circuits and Systems for Video Technology, vol. 18, issue 9, pp. 1293-1299, 2008. 
[174] H. Liu, L. Zhang, Z. Yu, H. Zha & Y. Shi, Collaborative Mean Shift Tracking Based on Multi-Cue 
 國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
研究進行順序配合實際狀況有所調整，但研究內容與原計畫大致相符，計畫中工作項目
一～三已於第一年度執行完成，工作項目四～六已於第二年度執行完成，工作項目七～九則
於第三年度執行完成。三年來實際已完成之工作成果整理如下: 
 研究成果(一) 人物步伐(gait)之分析與辨識 
 研究成果(二) 人物外觀(appearance)資訊建模與肢體姿勢(posture)參數擷取 
 研究成果(三) 移動機器人之人物追蹤與跟隨  
 研究成果(四) 顏色(color)與亮度(lightness)資訊之利用 
 研究成果(五) 材質(texture)與運動(motion)資訊之利用 
 研究成果(六) 多特徵融合 (multi-modal fusion)與人形 (human shape)確認  
 研究成果(七) 人物攤平圓柱樣板(FCT)之建構、辨識與追蹤 
 研究成果(八) 人物多視角樣板(MVT)之建構、辨識與追蹤 
 研究成果(九) 遮蔽之偵測與處理 
附件二 
行政院國家科學委員會補助國內專家學者出席國際學
術會議報告 
報告人姓名 
 
楊茂村 
 
服務機構 
及職稱 
 
國立東華大學資工系副教授 
 
會議時間 
會議地點 
July 9~14, 2011 
Orlando,U.S.(美國奧蘭多) 
本會核定 
補助文號 
於99 年度專題研究計劃中核定 
97-2221-E-259-012-MY3 
會議 
名稱 
 (中文)2011人機介面國際會議 
 (英文 ) International Conference on Human Computer Interface (HCI International 
2011) 
發表 
論文 
題目 
 (中文) 針對學習狀態分析之人臉表情辨識 
 (英文) Facial Expression Recognition for Learning Status Analysis 
Facial Expression Recognition for Learning Status Analysis      3 
Facial Expression Recognition for Learning Status 
Analysis 
Mau-Tsuen Yang, Yi-Ju Cheng, & Ya-Chun Shih 
National Dong-Hwa University, Taiwan 
mtyang@mail.ndhu.edu.tw 
Abstract. Facial expression provides an important clue for teachers to know the 
learning status of students. Thus, vision-based expression analysis is valuable 
not only in Human-Computer Interface but also in e-Learning. We propose a 
computer vision system to automatically analyze learners’ video to recognize 
nonverbal facial expressions to discover learning status of students in distance 
education. In the first stage, Adaboost classifiers are applied to extract 
candidates of facial parts. Then spatial relationships are utilized to determine 
the best combination of facial features to form a feature vector. In the second 
stage, each feature vector sequence is trained and recognized as a specific 
emotional expression using Hidden Markov Model (HMM). The estimated 
probabilities of six expressions are combined into an expression vector. The last 
stage is to analyze the expression vector sequence to figure out the learning 
situation of the student. Gaussian Mixture Model (GMM) is applied to evaluate 
three learning scores (Understanding, Interaction, and Consciousness) that are 
integrated into a status vector. Each evaluated status vector reflects the learning 
status of a student and is helpful to not only teachers but also students for 
improving teaching and learning.  
Keywords. Facial expression recognition, Learning status analysis 
1. Introduction 
Most existing e-Learning systems focus on the use of instructor’s video. However, 
learners’ videos are critical for instructor to know the learning status of students. 
Advances in computer processing speed and imaging technology make it possible to 
capture each learner’s video and estimate learning status using low-cost hardware 
(off-the-shelf PC and webcam). The estimated learning statuses provide valuable 
information for bi-directional online interaction and distance learning evaluation.  
In order to automatically discover learning status from learner’s video in real time, 
we propose a facial expression recognition system for learning status analysis. Fig. 1 
shows the flowchart of the proposed system. In the first stage, Adaboost classifiers are 
applied to extract candidates of facial parts. Then spatial relationships are utilized to 
determine the best combination of facial features to form a five-dimensional vector 
called feature vector. In the second stage, each feature vector sequence is trained and 
Facial Expression Recognition for Learning Status Analysis      5 
Appearance Model (AAM) for face detection but it is only effective for specific 
trained person. Rowley et al. [13] applied neural network to detect face but an 
individual neural network need to be trained for each head orientation. Viola and 
Jones [17] presented a face detection system called AdaBoost that constructs a strong 
classifier by linking a boosted cascade of simple haar classifiers. AdaBoost achieved 
rapid and stable face detection so that it is widely used for face detection nowadays. 
Moreover, Peng et al. [23] combined color information with AdaBoost in order to 
increase the robustness. Cristinacce and Cootes [22] integrated AdaBoost and shape 
constrains to improve the performance of face detection. Wilson and Fernandez [20] 
refined the AdaBoost algorithm to address the problem of head rotation.  
Facial expression recognition can be performed on either a static image or an 
image sequence. Several methods only rely on spatial information in a still image to 
recognize facial expressions. Saatci and Town [14] used Support Vector Machine 
(SVM) to separate difference expressions by estimating hyper planes. Besides, gender 
classifier is applied to improve the expression recognition performance. Liu et al. 
proposed Fusion Neural Network (FNN) [10] to integrate multiple Gabor features for 
expression recognition. They also utilized Gaussian Mixture Models (GMM) to 
analyze eigenvector of facial expressions [9]. Cheon and Kim [4] integrated K-
Nearest Neighbors (KNN) and manifold learning to classify different facial 
expressions. Cao and Tong [3] proposed spatial Embedded Hidden Markov Model 
(EHMM) to recognize facial expressions. Jung et al. [7] used AdaBoost algorithm to 
classify expression and showed the practicability of AdaBoost for not only detection 
but also recognition. 
In addition to the spatial information in an image, temporal information extracted 
from an image sequence also provides valuable clues for expression recognition. 
Datcu and Rothkrantz [5] compared and showed that the expression recognition 
performance based on video sequences is much better than those based on still 
images. Ofli et al. [12] constructed several temporal parallel HMM to recognize 
different expressions. Wang and Ju [19] proposed a hybrid model which combined 
KNN with HMM to increase the recognition accuracy. 
Recent advances in computer vision technology gradually reveal its potentials on 
e-Learning improvement. Walczak et al. [18] proposed a VR-based framework of 
network service for distance education. It constructed a virtual classroom in that 
students can learn by interacting with the virtual objects in the classroom. Calvi et al. 
[21] developed an eye tracking device to find learner’s interest and make a warning if 
the learner missed his attention. Loh et al. [11] proposed an expression recognition 
system for e-Learning based on Gabor wavelet and Neural Network. However, their 
database contains only static images with four facial expression (neutral, sleepy, 
confuse, and smile). Relatively, we propose a facial expression recognition system 
working on real-time video. Based on the recognized facial expression, we further 
evaluate three learning scores including understanding, interaction, and consciousness 
to encourage online interaction and enhance the quality of e-Learning. 
Facial Expression Recognition for Learning Status Analysis      7 
 
 
 
 
 
 
( )
Blink
Wrinkle
Shake
Nod
Yawn
Talk
p t
p t
p t
Ev t
p t
p t
p t
 
 
 
 
  
 
 
 
  
      
(2) 
5. Status Estimation 
In the stage of learning status analysis, Gaussian Mixture Model [2] is used to 
estimate the learning status by analyzing the expression vector sequence. Since the 
learning status is a lasting state, it is discovered by observing the sequence of the 
expression vectors for a period of time. In a fixed time interval (30 seconds in our 
experiments), the number of times that the expression probability is higher than a 
threshold is counted and record in a counting vector. EM algorithm [6] is applied to 
train the GMM parameters for each kind of learning status based on the counting 
vectors. Then the trained GMM is used to estimate the probability of respective 
learning status. Three learning scores (Understanding, Interaction, and 
Consciousness) are outputted as a sequence of three-dimensional status vectors: 
 
 
( )
( )
Interaction
Understanding
Consciousness
p t
Sv t p t
p t
 
 
  
  
     
(3) 
6. Experiments and Applications 
Our preliminary experiments show that AdaBoost facial feature extraction combined 
with spatial relationship filtering can locate the facial features robustly. Also, the 
trained facial expression HMMs can recognize facial expressions reliably even for 
untrained people. It should be noted that it is possible to recognize several expressions 
with the same sequence because several expressions can appear concurrently. For 
example: one could blink and talk at the same moment. Finally, the GMMs can 
effectively estimate three learning scores: interaction, understanding, and 
consciousness. 
It is very important for a teacher to know the learning status of students in order to 
improve the learning effectiveness. The proposed system has been implemented to 
enhance an online Virtual English Classroom called VEC3D [16]. Instructor as well 
as each distant student uses his or her own computer and webcam to login VEC3D 
online as a virtual avatar. A live facial image can be transmitted and shown above 
Facial Expression Recognition for Learning Status Analysis      9 
 
 
 
Fig. 3. Three typical learning situations when a group discussion is taking place.  
(a) Ideal case (b) Confused case (c) Quiet case 
0
0.2
0.4
0.6
0.8
1
20 70 120
st
at
u
s 
d
eg
re
e 
time 
(a) Ideal Case 
Interaction Understanding Consciousness
0
0.2
0.4
0.6
0.8
1
20 70 120
st
at
u
s 
d
eg
re
e 
time 
(b) Confused Case 
0
0.2
0.4
0.6
0.8
1
20 70 120
sa
tu
s 
d
eg
re
e 
time 
(c) Quiet Case 
Facial Expression Recognition for Learning Status Analysis      11 
8. Kumar, C., and Bindu, A.: An Efficient Skin Illumination Compensation Model for Efficient 
Face Detection. IEEE Annual conference on Industrial Electronics, pp. 3444-3449, Nov. 
(2006) 
9. Liu, W., Lu, J., Wang, Z., & Song, H.: An Expression Space Model for Facial Expression 
Analysis. IEEE Congress on Image and Signal Processing, vol. 2, pp. 680-684 (2008) 
10. Liu, W., and Wang, Z.: Facial Expression Recognition Based on Fusion of Multiple Gabor 
Features. IEEE International Conference on Pattern Recognition, (2006) 
11. Loh, M., Wong, Y., & Wong, C.: Facial Expression Recognition for E-learning System 
using Gabor Wavelet & Neural Network. International Conference on Advanced Learning 
Technologies (2006) 
12. Ofli, F., Erzin, E., Yemez, Y., & Tekalp, A.: Estimation and Analysis of Facial Animation 
Parameter Patterns. IEEE International Conference on Image Processing 2007, vol. a, pp. 
IV-293-IV296, Oct. (2007) 
13. Rowley, H., Balujia, S., & Kanade, T.: Neural Network-Based Face Detection. IEEE 
Journal of Pattern Analysis and Machine Intelligence , vol. 20, issue 1, pp. 23-28, Jan. 
(1998) 
14. Saatci, Y., and Town, C.: Cascaded Classification of Gender and Facial Expression using 
Active Appearance Models. IEEE International Conference in Automatic Face and Gesture 
Recognition  (2006) 
15. Seo, K., Kim, W., Oh, C., & Lee, J.: Face Detection and Facial Feature Extraction Using 
Color Snake. IEEE International Symposium Industrial Electronics, pp. 457-462 (2002) 
16. Shih, Y., and Yang, M.: A Collaborative Virtual Environment for Situated Language 
Learning Using VEC3D, Journal of Educational Technology and Society, vol. 11, issue 1, 
pp. 56-68 (2008) 
17. Viola, P., and Jones, M.: Rapid Object Detection using a Boosted Cascade of Simple 
Features. IEEE Computer Vision and Pattern Recognition Society Conference, pp.  511-518 
(2001) 
18. Walczak, K., Wojciechowski, R., & Cellary, W.: Dynamic Interactive VR Network 
Services for Education. ACM symposium on Virtual reality software and technology, pp. 
277-286 (2006) 
19. Wang, Q., and Ju, S.: A Mixed Classifier Based on Combination of HMM and KNN. IEEE 
4th International Conference on Natural Computation, vol. 4, 18-20, pp. 38-42, Oct. (2008) 
20. Wilson, P., and Fernandez, J.: Facial Feature Detection Using Haar Classifiers. Journal of 
Computing Sciences in Colleges, vol. 21, issue 4, pp. 127-133, Apr. (2006) 
21. Calvi, C., Porta, M., & Sacchi, D.: e5Learning, an E-Learning Environment Based on Eye 
Tracking. IEEE International Conference on Advanced Learning Technologies, pp. 376-
380, Jul. (2008) 
22. Cristinacce, D., and Cootes, T.: Facial Feature Detection Using AdaBoost with Shape 
Constraints. British Machine Vision Conference (2003) 
23. Peng, Y., Jin, Y., He, K., Sun, F., Liu, H., & Tao., L.: Color Model Based Real-time Face 
Detection with AdaBoost in Color Image. International Conference on Machine Vision, pp. 
40-45, Dec. (2007) 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：楊茂村 計畫編號：97-2221-E-259-012-MY3 
計畫名稱：RFID 數位家庭中網路技術與應用之整合及個人化服務平台之建置--子計畫一:數位家庭之
視覺式保全系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 5 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 5 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 13 13 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
