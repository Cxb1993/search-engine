                              共 1 頁   第 1 頁 
中文摘要：  
關鍵詞：類神經網路，特徵萃取，維度化約，生成模型，機率密度函數估計，平均
場退火理論 
 
本計劃主要透過監督式資料分析的生成模型之建構，藉以探討並解決函數近似及分
類之問題，並進一步利用所建構之生成模型，探討真實世界中成對訊號，彼此之間造成
影響之主要成份。我們將建構監督式資料產生之生成模型，進而利用條件機率來模擬監
督式資料之參數化機率密度函數(parameterized probability density function)。在作法上，
我們將以多個比重不同的常態分佈(Normal distribution)之總合來估計資料之密度函數，
以大量之資料樣本，遵循最大概似原理(maximal likelihood principle)，推導混合式線性整
數規劃模型，求取最佳化的生成模型；在計算組織上，則是透過連續型的線性變數及離
散型的整數變數分別對學習機制的幾何與組合特性進行內部編碼，並以混合式平均場退
火理論(mean field theory)與梯度遞減法(Gradient descend method)對複雜的線性整數規劃
模型進行最佳化，發展具相互作用的多組動態方程組，在平均場退火理論的架構下求取
相關係數的最佳解。 
接著我們藉由控制函數近似的生成模型中之接受體(receptive field)數量，將原始的高
維度ℜn的資料，投影到低維度ℜm上(m<<n)，更進一步利用模型最佳化特性搭配固定之
接受體參數，去除所萃取出的特徵彼此間之相關性，亦即所萃取出的特徵彼此間的相關
性，在不影響與輸出之對應關係上，將儘可能的達到不相關，甚至是彼此獨立，亦即可
以利用此架構，找出影響監督式資料對應關係的主要特徵。 
我們將以數值模擬方法具體分析乳癌(Breast Cancer)資料，建立相關的參數化近似函
數及完成判讀，並與先前的研究結果進行比較。 
2 
 
其生成模型之結構，與 Wu,Lin和 Hsu[16]所提在 M=1的情況下一樣。在此，我們
著重在 M>1的情況下，圖 1，為其所對應之生成模型，在 M>1時，對應的生成模
型只單純的增加接受場的數目而非增加條件式多工選擇器的數目，所以在學習的
過程中，我們將其分為二個學習階段，分述如下： 
第一階段：特徵萃取階段 
在此階段中，我們將 K-state function 的參數固定，均勻分散在ℜM的單位
體中，所以此時模型的參數包括接受場 W、期望值向量 r 和變異數σy及σh，此階
段的目的在於最小化目標函數 
E 𝐰, 𝛅, 𝐫, 𝛔 =
1
2Tσh
2   δk t  𝐖𝐱 t − 𝐜k 
2
kt
+ logσh
+
1
2Tσy2
  δk t  𝐲 t − rk 
2
kt
+ logσy 
其中常數項已忽略，且 
 δk t 
k
＝１ for all t  
δk t ∈  0,1  for all k, t  
利用一混合了均場理論及最陡坡降法的最佳化方法，我們可推導出相應之最佳化
疊代過程如下： 
μk t =
∂E 𝐰, 𝐯, 𝐫, 𝛔 
∂𝑣𝑘 𝑡 
      
=
1
2Tσh
2
 𝐖𝐱 t − 𝐜k 
2 +
1
2Tσy2
 𝐲 t − rk 
2 
𝑣k t =
exp −βμk t  
 exp −βμk t  
K
l=1
 
𝑟k =
 𝑣k t 𝐲 t t
 𝑣k t t
 
𝜎h =  
1
N
  𝑣𝑘 𝑡  𝐖𝐱 t − 𝐜k 
2
kt
 
1
2
 
𝜎y =  
1
N
  𝑣𝑘 𝑡  𝐲 t − rk 
2
kt
 
1/2
 
𝐰 = A+𝐛 
4 
 
模擬實驗： 
在數值實驗中，我們分成了二個部份，第一個部份利用生成的訊號，來呈現
所提之架構在特徵萃取上特異能力；第二個部份則實際的將模型應用到真實訊號
Wisconsin的乳癌資料[11]，所提之架構在 Wisconsin 的乳癌資料分析上，表現
十分優異。 
人造訊號： 
在此實驗中，我們產生一棋盤狀之資料  𝐱 t , y t  |𝐱 t ∈ ℜ2 , y t ∈
 −1,1  
t=1
900
，如圖 2 所示，資料中輸入值 x 本身呈現正相關。在設定 K=3的情況
下，所萃取出來的特徵𝐡 t ，如圖 3所示，呈現的是不相關之資料型態。在此，
生成模型所萃取出來的特徵，不僅僅侷限於輸出及輸入的對應關係，更進步的萃
取出儘可能是獨立或是不相關的特徵值。 
Wisconsin Breast Cancer： 
Wisconsin 的乳癌資料，總共 699筆資料，每筆資料包含了 9個特徵值作為
預測良性或惡性乳癌之依據，分成 458良性乳癌及 241筆惡性乳癌資料。9個特
徵值分別為 clump thickness, uniformity of cell size, uniformity of cell 
shape, marginal adhesion, single epithelial cell size, bare nuclei, bland 
chromatin, normal nucleoli and mitosis，其值範圍從 1到 10。在最初的實
驗中，Wolberg-Mangasarian[11]應用多面向方法，得到 6%的錯誤率。後來 Malini 
Lamego[5]利用類神經網路搭配代數廻圈(algebraic loops)在前 683筆資料上作
實驗（前 483筆作為訓練集，後 200筆作為測試集），得到的錯誤率在訓練及測
試集分別為 2.3%和 4.5%。對於同樣的訓練集和測試集，Wu[12]利用所推導的
PottsNDA方法，在 42個代表點的模型下，在訓練及測試集的錯誤率分別為 1.4%
和 1.0%，如果再把最後的 16筆資料包含到測試集的話，PottsNDA的測試集的錯
誤率為 1.39%。在此，我們依據先前相關實驗之資料集分法，在設定 M=1，K=2
的情況下，所得到之實驗數據為學習集 6.4%，測試集 0.5%，也就是說測試資料
中只有 1筆資料被誤判，如果再把最後的 16 筆資料也加到測試集，我們發現仍
舊只有原先的一筆資料被誤判，測試集的錯誤率下降為 0.46%。相關的實驗數據
如下表所示： 
 
 
 
6 
 
Transactions on Neural Networks, 12(1), 33-42. 
6. Mansuripur, M., (1987), Introduction to information theory, 
Prentice-Hall, INC. 
7. Parzen, E., (1962), On Estimation of a Probability Density Function 
and Mode, Annals of Math. Statistics, 33, 1065-1076. 
8. Peterson C. & Söderberg B. (1989) , A new method for mapping 
optimization problems onto neural network, Int. J. Neural Syst. 
1(3). 
9. Revow M, Williams CKI and Hinton GE, (1996), Using generative models 
for handwritten digit recognition, IEEE Transactions on Pattern 
Analysis and Machine Intelligence 18(6), 592-606. 
10. Verbeek, J.J., Vlassis, N., & Krőse, B.(2003), Efficient Greedy 
Learning of Gaussian Mixture Models, Neural Computation, 
15,469-485. 
11. Wolberg WH and Mangasarian OL,(1990), Multisurface method of pattern 
separation for medical diagnosis applied to breast cytology, 
Proceedings of the National Academy of Science 87, 9193-9196. 
12. Wu JM,(2002), Natural discriminant analysis using interactive Potts 
models, Neural Computation 14(3), 689-713. 
13. Wu, J.M., & Chiu, S.J., (2001), Independent component analysis using 
Potts models, IEEE Transactions on Neural Networks, 12(2). 
14. Wu JM and Lin ZH,(2004), Global and local feature extraction by 
natural elastic nets, IEICE Transactions on Information and Systems 
E87D(9), 2267-2271. 
15. Wu JM and Lin ZH,(2002), Learning generative models of natural images, 
Neural Networks 15(3), 337-347. 
16. Wu JM, Lin ZH and Hsu PH, (2006), Function approximation using generalized 
adalines, IEEE Transactions on Neural Networks, 17(3), 541-558. 
  
8 
 
 
  
圖 3：在設定 M=2，K=3 的情況
下，所萃取出來的特徵𝐡 t ，在
第１及第２維度的相關性已消
除，呈現的是不相關之資料型
態。 
