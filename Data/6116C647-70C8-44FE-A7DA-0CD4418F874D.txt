II 
 
中文摘要 
 
自我組織圖(self-organizing map)為一長久以來廣泛應用之類神經網路模式，具有維度縮
減(dimension reduction)與拓樸維持(topology preservation)等主要特性。運用此模式，高維度
資料空間得以在訓練後對應至一低維度空間。且原空間內之拓樸關係在對應後亦可保存。
因此自我組織圖常被用來進行資料分群(clustering)與視覺化(visualization)用途。然而其主要
缺點為需預先制定圖中神經元之數量與結構，而此兩者並無可靠法則據以求得。針對此缺
陷，數種改善方法已被提出，例如增長式／擴充式自我組織圖 (growing/extendable 
self-organizing map)，階層式自我組織圖(hierarchical self-organizing map)，與增長階層式自
我組織圖(growing hierarchical self-organizing map)等。此類方法皆在訓練過程中進行動態之
結構增長與改變，並已達不錯之效能。其結構改變之依據，多以資料分布特性為主。換言
之，可視為以資料為導向之自我組織圖。 
本研究發展一應用於文件分群之以主題為導向進行自我組織圖增長與結構變化之方
法。有別於上述資料導向方法，本方法在訓練過程中，依據所偵測之文件文題，進行動態
結構配置。此結構配置，因其依據所偵測之文件主題，故可視為主題導向之方法，以別於
傳統資料導向方法。本計畫以方法之理論發展與模擬建置為主，完成本方法之演算法規劃
與實作。本研究發展一新的自我組織圖訓練方法，並可應用於文件探勘上，預期可對相關
領域提供可行之解決方案。本研究成果已於二國際會議上發表，並已撰寫成期刊論文投稿
至類神經網路國際知名期刊中。 
1 
 
1. 前言 
 
自我組織圖近年來被廣泛的運用於文件的分群或文件的分類應用上。文件的分類所注重
的是如何依照文件的內容、特性與性質將文件分類至某一既定的類別中。將所有文件進行
適當的分類後，便有利於我們有效的對其文件之儲存與檢索。文件分類有一重要的性質，
即同類別底下之所有文件應具有該類別的共同主題；另一方面，這些文件所形成的群集，
該群集內的所有文件擁有相似的內涵。因此文件的分類替這些文件提供了某種相關的知識。 
藉由文件分類獲得知識之前，需要運用一些知識使得文件得以正確分類。至少需要兩種
知識以進行文件分類：一，可使用哪些類別？二，各類別間之關係為何？第一種知識包含
了一組主題以供分類使用，這些主題構成了文件分類的基礎，擁有相同主題的文件將被分
類於同一類別中。第二種知識依類別間之關係程度建立其結構。也就是說，相似的類別應
被安排在該結構中以某種方式所定義之„相近‟位置。藉由這樣的結構我們可以有效的進行文
件的儲存與檢索之外，這種結構更利於人類瞭解。上述的知識傳統以來都是由人類專家指
定或透過結合人類知識及計算技術之半自動的方式獲得。例如著名的文件集 MEDLINE 就
需要相當多的勞力進行分類。然而，類別主題與結構之全自動產生是非常困難的。其原因
有二：一、必須選擇一些重要的字做為類別字(或稱類別主題)。這些字可用來代表類別的主
題以及檢索該類別文件。一個好的主題選擇將反應在分類的結果與效度。然而，必須對語
言的內涵有相當深入的了解才能選擇出適當的單一字或單一詞代表該類別主題，由於過去
都必須依靠人類語言專家來決定最具代表性和區分性的字詞來表示該類別主題，因此難以
將主題選得之過程完全自動化。二、為了利於人類的瞭解，常以階層式的方式來表示類別。
階層式架構可以輕易的表示類別之間的關係。高階層之類別代表的主題較為廣泛，而低階
層之類別的主題則較為狹義。同時，子階層類別之共同主題即代表著母階層之主題。當文
件以此結構方式儲存時，則檢索某一特定主題之文件將會變得十分有效率。為了建立一個
接近理想的架構以及避免母階層之下有著不相關的子類別，因此必須對文件進行全盤的語
意分析，這過程之自動化是非常困難的。目前大部分的文件分類研究中都是以既定好的類
別及架構進行文件的分類，而非自文件中產生類別與其結構。 
若想應用自我組織圖於文件的分群與文件階層產生上，除了以其將文件分群外，還必須
能夠自動的發掘一隱含知識，即每一文件群集之主題，即所謂之主題之偵測 (theme 
identification)。主題偵測乃針對文件群集進行分析以找出該群集中重要關鍵字或關鍵詞作為
該群集之主題。以往這些被偵測出來的主題通常僅用於標記所屬的群集以利於瀏覽或檢
索。在自我組織圖的訓練過程中，並無偵測主題且用於導引訓練過程之作法。 
本研究主要目的在於在文件分群領域中發展一新的自我組織圖訓練演算法並在其訓練
過程中導入文件主題之偵測技術。主要流程為先經過自我組織圖訓練分群後，偵測分析每
個群集中的所有文件之內涵，給定每個群集一適當的主題。當每個群集都給予適當之主題
後，再基於主題之文件群集相似比對方法來擴展群集結構與產生階層，以達到自動文件分
群的目的。此演算法與傳統自我組織圖之主要不同是不需事先給定既有的類別主題與結構
大小，當訓練完成後，即已產生一個分類的架構，便可使用此分類架構進行分類達到自動
分類的效果。 
 
3 
 
High Error
New node
New node
 
圖 2-3 GSOM 網路增長 
2.3 階層的產生 
雖然自我組織圖的增長可以更確切表達資料間的關係，但仍然無法呈現過於複雜之資料
集關係，且大量的文件資料中可能存在著階層的關係，而解決此缺陷的方法為使用獨立之
自我組織圖表示階層關係，如階層式特徵圖(Hierarchical Feature Map)[4]或樹狀結構自我組
織圖(Tree Structured Self-Organizing Map)[5]。 
階層式特徵圖可以將資料以階層方式呈現，產生每一階層都一樣大小的自我組織圖之固
定對稱結構，如圖 2-4 所示。 
Layer 3
Layer 2
Layer 1
 
圖 2-4 階層式特徵圖 
階層式特徵圖採由上而下(top-down)方式進行訓練。亦即，首先形成最上層的圖，再依
其所隱含的類別來導引下一層的圖之形成，依此類推直至到達最下層為止。在上層的圖可
視為只進行高階或粗略的分群，而愈低階的圖則進行低階或精細分群。階層式特徵圖的限
制在於其圖之大小（不論在那一層）其大小皆為固定的。樹狀結構自我組織圖亦為類似的
作法。低層的自我組織圖只會運用標記於在上一層中其所對應的神經元作訓練。其結構一
般而言亦是固定的，並非動態增長。 
2.4 增長層級式自我組織圖 
地圖的增長克服了自我組織圖的固定結構，而階層的產生則可以表達大量的複雜文件資
料間的階層關係，所以 Rauber 等學者結合了此兩種方法，提出增長層級式自我組織圖
(Growing Hierarchical Self-Organizing Map；GHSOM) [6][7]。GHSOM 是基於 SOM 的一種網
路模式，由多個階層所組成，每個階層個別由獨立的自我組織圖所構成，且結構大小會依
據資料而動態增長，屬一個動態演算法。訓練初期給予少量神經元數量(通常為 23 神經元
數量)進行訓練，訓練過程中分析所有神經元，找出未被正確分群之錯誤神經元 e 與其最不
相似鄰居 d，在 e 與 d 之間插入新的一列或一行，直到所有神經元都符合條件即終止神經元
之成長，如圖 2-5 所示。 
5 
 
3. 研究目的與方法 
 
在經過增長層級式自我組織圖與文件分群演算法相關文獻探討分析後，在分群領域中發
展一新的自我組織圖訓練演算法為本研究主要目標，其中主題偵測導入訓練過程是本研究
之主要概念，以主題來導引訓練過程。本章節將依序介紹每個處理過程，其訓練過程如圖
3-1 所示。 
文字文件
文件向量
前置處理
自我組織圖
訓練
主題偵測
橫向擴展 橫向擴展
階層擴展階層擴展
訓練完成
是
是
否
否
 
圖 3-1 研究架構圖 
3.1 文件前置處理 
一般的自然語言文件內容不易由電腦使用進行分類，因此需要將文件內容經過整理成為
分類訓練所需的資料格式，而資料前置處理(data preprocessing)是將文件內容中萃取重要的
特徵集合來取代原始文件。藉由這些特徵(feature)辨別的方法可以給予自然語言文件適當的
主題類別。經過此項步驟，系統較容易處理資料。反之，無此步驟則系統不易處理資料，
可能導致分類的結果成效不彰。 
3.1.1 斷詞與關鍵字選取 
文件經過斷詞步驟處理後的詞彙之集合可以代表該文件特徵。但這些詞彙之集合並非全
部都與該文件之涵義顯著相關，部分詞彙甚至不具意義，而且詞彙過多將導致分類演算法
的效率不彰，此時必須決定哪些重要的詞彙可以做為文件的特徵，也稱之為關鍵字
(keyword)。傳統上，英文關鍵字之選擇包含幾個主要的步驟：停用字(stopword)去除、名詞
選擇與字根還原(stemming)等步驟。首先，文件必須先停用字去除，停用字是將一些不具太
7 
 
件分群之新的自我組織圖訓練演算法，稱之為主題導向式自我組織圖。由於本研究所發展
之演算法是以自我組織圖為基礎所發展出來的演算法，而過程中會依據情況地圖擴展與階
層擴展，因此我們先介紹自我組織圖之演算法，接著在介紹本研究之主題導向式自我組織
圖的演算方式。 
3.2.1 主題導向式自我組織圖 
本研究之主題導向式自我組織圖(Topic Oriented Self-organizing Map, TOSOM)為發展一
種根基於 SOM 之類神經網路模式，他除了可以使眾多的資料以前述 SOM 的特徵地圖二維
呈現外，並可以依據階層結構分群呈現，可利於資料分析與探索，是可處理高維度特徵空
間之穩定且可調整的模式。TOSOM 的階層結構中具有多個階層，每個層級皆由數個獨立
之自我組織圖所構成。每個自我組織映特圖之拓樸大小會依據神經元所包含之關鍵字主題
特性而有所成長。在地圖中每個神經元視其底下輸入資料之特性可決定是否產生另一個獨
立的自我組織圖以往下形成第二個階層，如果資料間的同質性高的話，便無須更進一步的
往下擴展到下一階層，透過這些準則便可讓文件除了以二維帄面方式視覺化外，也可將每
個群集中資料間之階層特性視覺化呈現，而主題導向式自我組織圖之階層架構如圖 3-2 所
示。 
Layer 2
Layer 1
Layer 3
Layer 0
 
圖 3-2 主題導向式自我組織圖結構圖 
TOSOM 可以克服前述需事先固定地圖大小與非階層式調整地圖結構之問題，TOSOM
可以根據資料間的關鍵字主題涵義去發展地圖大小與階層架構，layer 0 代表整個輸入文件
群集，是控制階層成長是必要的一層。圖 3-2 中的 layer 1 的地圖大小為 23，為輸入向量
之主要分群，初始之地圖大小不宜太大，在過程中會視情況調整大小，若 layer 1 之地圖中
某個神經元符合向下擴展之條件，則往下產生 layer 2 階層，並將該神經元底下所有文件當
成輸入向量進行 SOM 訓練產生新的特徵圖使之更相近於文件資料。若該特徵圖還有神經元
可以擴展至下一層，則繼續到 layer 3 產生新地圖更接近資料，整個過程在當地圖成長與階
層成長停止完畢時，即訓練結束。在主題導向式自我組織圖中每個地圖的大小是根據資料
間的主題涵義而有所不同，因此除了可以減輕自我組織圖需事先給定地圖大小的負擔外，
還可以給予群集間充分的內隱知識。 
TOSOM 演算法其學習過程中包括了自我組織圖訓練、主題偵測、橫向擴展與階層擴展
四個部分，其演算法步驟如下： 
A. 自我組織圖訓練 
文件轉換為文件向量後，便依傳統自我組織圖進行訓練。初次訓練時，僅有一自我
組織圖且其包含極少數之神經元，例如為一 22 之神經元架構，稱為初始自我組織
圖(initial SOM)。而後自我組織圖與神經元數量皆會依情形擴充。其中由於我們所
9 
 
如 Wordnet，來決定關鍵字間之關聯性。當二關鍵字間不具有直接之關聯性時，則
判定其不相容。其二為藉由檢驗神經元鍵結權重中對應於關鍵字之成份分布來判
定。所謂分布，在此有兩種作法，一種為關鍵字 kj 於該神經元底下每個文件之值排
列構成直方圖。例如關鍵字 ki 之分佈為(Di1, Di2, …, Dij)。另一種乃指將該成份於不
同神經元之值依序排列而構成之直方圖(histogram)。例如關鍵字 kj 之分布即為(w1j, 
w2j, …, wMj)。當兩關鍵字之分布差距過大時，即判定為不相容。此處要注意關鍵字
之分布必須先行正規化(normalization)以獲得較正確的結果。本研究以關鍵字分布
差距是否過大作為是否地圖擴展之判斷，當神經元經過主題偵測後，獲得一關鍵字
之主題集合後。此時針對該神經元之鏈結權重中，對應於主題集合中之關鍵字之值
與其他神經元之鏈結權重對應該關鍵字之值，構成關鍵字分布，而該神經元之主題
集合中擁有的關鍵字數量即為關鍵字分布之筆數。而判斷是否橫向擴展時，則計算
關鍵字分布彼此間帄均歐氏距離得到一個關鍵字分布值，亦即 
,
2
1
i
i j
i j C
d d
n


    (8) 
其中 n 為該關鍵字分布兩兩距離之個數，Ci 為神經元 i 之主題集合，di 與 dj 為屬於
該神經元主題集合中之關鍵字的關鍵字分布，2 為門檻值。此值若大於門檻值，則
該神經元將進行橫向擴展。 
當所有神經元之關鍵字分布值計算完後，選擇關鍵字分布值最大之神經元，也稱之
為錯誤神經元 e (error unit)進行擴展，依據 GHSOM 之最差鄰法(worst neighbor)，找
出與此神經元鄰近中最不相似的神經元 d (dissimilar neighbor unit)，在兩神經元之
間插入一排神經元，而新的神經元之權重向量被初使化為其鄰居的帄均值，此時再
將新的地圖結構進行 SOM 訓練，而為了控制地圖最大結構，因此加入一倍數門檻
值4 來控制地圖結構，當目前地圖結構大小之ㄧ定倍數大於訓練資料數量時便停止
擴展，亦即 
4
Vector
MAP
  (9) 
其中 Vector 為訓練資料向量之筆數，MAP 為目前地圖結構大小，也就是目前神經
元總數，而4 為門檻值。其地圖之橫向擴展成長步驟如下： 
 Step 1：地圖執行 SOM 訓練。 
 Step 2：找出錯誤神經元 e 與最不相似神經元 d。 
 Step 3：在 e 與 d 之間插入新的一列或一行神經元。新的神經元之權重為其鄰近神
經元的帄均權重。 
 Step 4：重複 Step 1 至 Step 3，直到所有神經元之關鍵字分布值小於門檻值2 或當
前神經元數量之4 倍數大於訓練資料筆數即橫向擴展結束。 
D. 階層擴展 
最後一階段為階層擴展，在此一樣進行階層擴展之判斷。在此偵測分析每個神經
元，當有某群集(神經元)過大或群集中文件不一致性過高時，則該神經元需進行階
層擴展。判斷條件為二，兩條件成立時，一神經元將進行階層擴展。條件其一為標
記於該神經元之文件不一致性高於門檻值。在此文件之不一致性定義如下： 
11 
 
4. 結果與討論 
本研究將對英文文件進行 TOSOM 分群。研究所使用支實驗資料集為 Reuters-21578，其
內容為路透社至 1987 年 2 月至 1987 年 10 月中所收集之新聞，總共為 21578 篇新聞文件。
Reuters-21578 資料集是由 David D. Lewis[10]與路透社人員所整理而成。本研究選取該資料
集內容 100 份與 500 份，所選取之文件皆會進行斷詞之步驟。英文斷詞採用 TAIParse 來進
行斷詞之處理。文件經過斷詞與名詞選擇後，其中 100 份文件與 500 份文件各有 1290 與
3924 個關鍵字詞從文件中淬取出來。接著我們使用二元向量的方式將文件表示為特徵向
量，再根據這些特徵向量進行 TOSOM 文件分群與階層的產生。 
首先，我們給於 TOSOM 初始參數設定中的初始神經元大小，為 2×2 神經元，訓練過程
中，經過傳統 SOM 的訓練階段發現，傳統 SOM 無法將輸入資料有效的分群，經過不斷的
反覆微調與測試中，發現到造成 SOM 無法有效的分群之結果在於輸入的資料向量。資料集
透過前置處理轉換成資料向量後，深入分析資料向量中發現該資料向量為一個稀疏矩陣，
單個文件向量其擁有之關鍵字與關鍵字維度相比之下相對的少很多。由於關鍵字維度偏
高，文件擁有關鍵字相對偏少，而所有文件彼此間關鍵字重疊的數量偏低，而以傳統 SOM
中以歐式距離篩選優勝神經元訓練後，造成文件被那些數量偏高之關鍵字值為 0 的影響了
該文件被標記於正確神經元，因此我們將用歐式距離最小篩選優勝神經元之方式改用以內
積值最大之方式篩選優勝神經元。 
在 3.2.1 中主題偵測部分中提到主題關鍵字選擇之方法，起初我們使用以該神經元權重
值來篩選主題關鍵字集合，經過實驗結果發現，每個神經元得到的主題關鍵字與鄰近神經
元的主題關鍵字相似度不高，如圖 4-1 所示。而為了取得更可靠之主題關鍵字，其根據 SOM
的特性，每個鄰近神經元都有一定的相似程度，因此我們修正為將鄰近神經元之權重值加
進來計算帄均值來篩選主題關鍵字，結果顯示每個神經元之主題關鍵字與鄰近神經元之主
題關鍵字相關程度較高，如圖 4-2 所示。 
      
圖 4-1 神經元權重值之主題關鍵字 圖 4-2 鄰近權重值帄均之主題關鍵字 
13 
 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本研究所發展技術，除了提出新的自我組織圖訓練演算法外，亦可為文件分群與
組織提供有效的解決方案，在文本探勘(text mining)領域提供可行之技術。當今
愈來愈多的文件以無結構或半結構之方式存在，如何將此類文件進行分群並發掘
其間之關聯性已成為亟需解決的問題，文本探勘之目的便在於此。目前雖已存在
多種運用自我組織圖進行文本探勘之研究，但並能完全發揮其成效。本研究將文
本探勘過程導入自我組織圖之訓練當中，期待二者相輔相成，在訓練後便可直接
獲得隱藏於文件間之關聯。另在訓練過程中使用文本探勘技術，動態的擴展與建
立自我組織圖結構，亦可收加速訓練過程之效。預期本研究之進行，可為類神經
網路與文本探勘之研究皆提供可行之解決方案。 
15 
 
 
五、攜回資料名稱及內容 
會議論文集暨光碟：包含本會議全部發表論文。 
 
六、其他 
 
 
會議論文集暨光碟：包含本會議全部發表論文。 
 
六、其他 
 
 
 
hierarchical structure. Thus it is also well fit for tasks regarding
text documents such as text categorization, text clustering, and
text hierarchy generation, etc.
The following text is divided into four sections. Sec. II
describes some works related to our research. In Sec. III we
will introduce the proposed topic-oriented self-organizing map
algorithm. Sec. IV gives the experimental result. Finally, we
give conclusions and discussions in the last section.
II. RELATED WORK
We briefly review some related works here.
A. Adaptable SOM
To overcome the two major disadvantages, namely static
map structure and lack of hierarchical relationships, of tra-
ditional SOM [3], [1], many models have been proposed.
Some models use adaptable map size, e.g. the growing grid
model [4]. Another approach is to use hierarchical arranged
maps, such as hierarchical feature map [5] and tree-structured
self-organizing map [6]. Hybrid approaches were also de-
veloped, e.g. growing hierarchical self-organizing map (GH-
SOM) [7], [8], [9].
B. Text Organization Based on SOM
Here we refer text organization to the efforts involved in
organizing a corpse of texts into some predefined structures.
Typical text organization processes include text clustering, text
categorization, and text structure generation. Text organization
research has been active for several decades. Many methods
have been proposed to conquer this problem. Here we men-
tion some works that make use of SOM. SOM was widely
used in text clustering. A famous example is the WEBSOM
project [10]. Liu et al. [11] proposed ConSOM which use
concepts along with traditional features to guide the learning
process. Their method demonstrated better result compared to
traditional SOM due to its semantic sensibility.
III. TOPIC-ORIENTED SOM ALGORITHM
In this work we try to develop a new training method for
SOM. The core of the proposed approach is the identification
of topics which are used to guide the training process as
well as to change the structure of the map. Fig. 1 depicts
the flowchart of our method. The detailed explanations of the
steps are described as follows.
A. Preprocessing
We need to transform the text documents into vectors before
training since the input to SOM need to be vector-form. In this
step common procedures for processing text documents, such
as word segmentation, stopword elimination, and stemming,
were first applied to obtain a set of keywords that can describe
the contents of a document. All keywords were collected into
the vocabulary of the corpse. A document is then transformed
into a vector according to the keywords it contains. Let
document dj = {kij |1 ≤ i ≤ nj}, 1 ≤ j ≤ N , where N
is the number of documents, nj is the number of distinct

	


	

















Fig. 1. Flowchart of the proposed method.
keywords in dj , and kij represents the ith keyword in dj .
The vocabulary, denoted by V , is just the union of all dj , i.e.
V =
⋃
j dj = {ki|1 ≤ i ≤ |V |}. A document is encoded into
a binary vector according to those keywords that occurred in it.
When a keyword ki occurs in this document, the ith element of
the vector will have value 1; otherwise, the element will have
value 0. With this scheme, a document dj will be encoded
into a binary vector dj .
B. SOM Training
The document vectors were trained by classical SOM al-
gorithm [3]. The initial training was performed on a small
map, says a 2× 2 map, named initial SOM. The initial SOM
will be expanded when training proceeds. When the training
process is accomplished, we then perform a labeling process
on the trained map to establish the association between each
document and one of the neurons. The labeling process is
described as follows. The feature vector of document dj , dj =
(xji), 1 ≤ i ≤ |V |, 1 ≤ j ≤ N , is compared to the synaptic
weight vectors of every neuron in the map. We then label dj to
the lth neuron if its synaptic weight vector is closest to dj , i.e.
||dj −wl|| = argminm ||dj −wm||, where wm is the synaptic
weight vector of neuron m, wm = (wmi), 1 ≤ i ≤ |V |. After
the labeling process, each document is labeled to some neuron
or, from a different point of view, each neuron is labeled by
a set of documents. We record the labeling result and obtain
the document cluster map (DCM). In the DCM, each neuron
is labeled by a list of documents which are considered similar
and are in the same cluster.
We will also construct the keyword cluster map (KCM)
World Academy of Science, Engineering and Technology 65 2010
356
n
m 

n+2
m+2
(a) (b) 
Fig. 2. The omnidirectional lateral expansion approach. (a) The size of the
original map is m × n. The black disk depicts the neuron to be expanded.
(b) After expansion, the size of the map becomes (m + 2) × (n + 2). The
grey disks depict the added neurons.

n
m 

n+1
m
(a) (b) 
Fig. 3. The worst-neighbor lateral expansion approach. (a) The size of the
original map is m × n. The black disk depicts the neuron to be expanded.
The slashed disk is its worst neighbor. (b) After expansion, the size of the
map becomes m × (n + 1) since the worst neighbor is in the same row of
the neuron. The grey disks depict the added neurons.
where dl denotes the average of document vectors in Al,
i.e. dl =
1
|Al|
∑
j∈Al
dj .
When a neuron l needs hierarchical expansion, we should
create an initial SOM as described in Sec. III-B. This SOM is
then trained by the documents labeled to this neuron, i.e. Al.
IV. EXPERIMENTAL RESULT
We performed the experiments on two corpora which were
used in our previous research [14], [15] for comparison
purpose. The first corpus (C1) contains 100 Chinese news
articles posted in Aug. 1, 2, and 3, 1996 by Central News
Agency2. The second corpus (C2) contains 3268 documents
posted during Oct. 1 to Oct. 9, 1996. A word extraction process
is applied to the corpora to extract Chinese words. There are
1475 and 10937 words extracted from C1 and C2, respectively.
To reduce the dimensionality of the feature vectors we dis-
carded those words which occur only once in a document. We
also discarded the words appeared in a manually constructed
stoplist. This reduces the number of words to 563 and 1976
for C1 and C2, respectively. A reduction rate of 62% and 82%
are achieved for the two corpora, respectively.
We trained the corpora with the proposed algorithm. The
sizes of the initial SOMs for C1 and C2 are 2 × 2 and
4× 4, respectively. Table I shows the parameters used in the
experiments.
2http://www.cna.com.tw/
TABLE I
TRAINING PARAMETERS USED IN OUR EXPERIMENTS.
Corpus C1 C2
Size of Initial SOM 2× 2 4× 4
Number of synapses in a neuron 563 1976
Initial training gain for SOM training 0.4 0.4
Maximal training time for SOM training 200 500
TABLE II
THE RESULT OF TRAINING PROCESS
Corpus C1
Topic identification scheme NNWT ANWT ADWT LabelSOM
Average number of topic
in each neuron
2.37 2.78 3.67 3.23
Average number of neu-
rons in each map
5.75 6.24 5.47 6.45
Average number of map in
each training
4.32 3.67 5.27 6.31
Average depth of hierar-
chies
2.3 3.21 3.45 5.76
Corpus C2
Topic identification scheme NNWT ANWT ADWT LabelSOM
Average number of topic
in each neuron
4.32 5.68 6.57 6.54
Average number of neu-
rons in each map
8.81 9.27 9.31 11.46
Average number of map in
each training
7.37 8.26 9.31 12.29
Average depth of hierar-
chies
3.25 3.94 4.37 7.58
After SOM training, we performed topic identification using
three topic identification schemes described in Sec. III-C. We
then find neurons which need lateral expansion according to
the topic incompatibility defined in Sec. III-D. The thresholds
for topic identification were set to 0.8, 0.85, and 0.8 for
NNWT, ANWT, and ADWT schemes, respectively. Besides,
the threshold for topic incompatibility was set to 0.7. Both
omnidirectional and worst neighbor schemes were adopted
to expand a neuron. The hierarchical expansion was then
performed with document inconsistency threshold been set
to 0.65. A neuron was expand to a lower SOM with sizes
initialized as those in Table I. We retrained the two corpora 100
times and obtained statistical information about the proposed
algorithm. Table II shows the statistics of the training process.
Note that the values are averaged over 100 times of training.
We also conducted the same experiments using GHSOM,
which is denoted as LabelSOM in Table II.
V. CONCLUSIONS AND DISCUSSIONS
The self-organizing map model has been widely and suc-
cessfully used in data clustering and visualization, as well as
other wide scope of applications. Traditional SOM models
cluster data according to their similarity. Besides, the structure
of the map is always fixed. Various models have been proposed
to overcome such insufficiency. In this work, we try to
develop a novel learning algorithm which is suitable for text
organization. When a set of text documents are being trained,
we will identify the topics of a neuron which represents
a document cluster after SOM training. We then use such
topics to decide if a neuron needs to be lateral expanded or
World Academy of Science, Engineering and Technology 65 2010
358
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/11
國科會補助計畫
計畫名稱: 主題導向式自我組織圖模式之研究
計畫主持人: 楊新章
計畫編號: 98-2221-E-390-040- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
