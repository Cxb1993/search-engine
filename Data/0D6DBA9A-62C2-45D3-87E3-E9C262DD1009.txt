 1
中文摘要 
自首封垃圾郵件出現開始，其數量便不斷成長。迄今，垃圾郵件氾濫現象已然成為每
一位電子郵件使用者最迫切希望解決的問題。為此，部分研究者沿用既有文件分類演算法
進行垃圾郵件分類。多數文件分類方法著重於發掘正常郵件或垃圾郵件共同特徵，爾後以
此為依據進行分類。然而並非所有特徵都對垃圾郵件分類有所助益，有些甚至可能導致分
類正確率下降或延遲分類機制作動時間。針對該問題，本計畫以兩階段的方式，以演化式
計算的技術結合文件分類的方法，從事垃圾郵件的特徵分析後進而產生最適的特徵集合；
然後從此特徵集合中，嘗試建立用以分類垃圾郵件的規則。 
在第一階段中，我們以演化式計算技術中的粒子群尋優演算法為要角，配合支撐向量
機的分類技術，從一群垃圾郵件中，篩選出最能代表這群郵件的關鍵字集合。由於利用文
件分類技術亦能挑選出為數相當的特徵；因此，此階段的目的乃設定為特徵集合維度的縮
減，透過我們的方法可以達到以較少的特徵數量，達到最大的垃圾郵件分類效果。至於第
二階段，是將第一階段所得到的結果，輔以布林運算元進行相結合，以造就出適合一般使
用者閱讀且容易設定的分類規則。這個將關鍵字與布林運算元相結合的動作，則需仰賴我
們所設計的巢狀型式的演化式演算法。經由上述兩個階段的處理，我們便可以建置一套符
合使用者最佳需求的個人化垃圾郵件分類系統。 
關鍵詞：垃圾郵件、演化式計算、粒子群尋優演算法、支撐向量機、文件分類 
 
Abstract 
The increasing volume of unsolicited bulk e-mail (also known as spam) has generated a need 
for reliable anti-spam filters. Using a classifier based on machine learning techniques to 
automatically filter out spam e-mails has drawn many researchers’ attention. Some text 
categorization methods were used to select common features of spam e-mails and to classify 
spam e-mails according to these features. Unfortunately, in some cases, there are too many 
features to decrease the classification accuracy. In this project, we try to design a two-stage 
approach to overcome the problem above mentioned. The proposed approach integrates 
evolutionary computation technique and text categorization method, and the purposes of it are to 
reduce the dimension of feature space and to automatically generate the classification rules. 
In the first stage, the particle swarm optimization (PSO) is used to select the proper features 
in spam e-mails. PSO is a relatively new population-based evolutionary computational model. In 
contrast to other evolutionary computational techniques which exploit the competitive 
characteristics of biological evolution, PSO is motivated from the simulation of cooperative and 
social behavior. The integration of PSO and support vector machine (SVM) can produce proper 
and representative features in the spam e-mails corpora. In the second stage, we will use the 
results from the first stage to automatically generate the classification rules for anti-spam filtering. 
Here we propose a novel technique, called nested evolutionary algorithm (NEA), and the Boolean 
operators are also considered simultaneously to accomplish the task. NEA is a special model in 
which there is an EA embedded in another EA. We believe that the proposed approach can 
provide a good way to construct a personalized and adaptive anti-spam filtering system. 
Keywords: spam, evolutionary computation, particle swarm optimization, support vector 
machine, text categorization 
 
 3
合中，利用巢狀式演化式計算(nested evolutionary computation, NEC)的方式，將關鍵
字搭配布林運算元，自動建構出容易閱讀且適合設定的分類規則(classification 
rules)，以利使用者在自己的郵件軟體中進行設置。 
 
2. The Proposed Approach 
第一階段將以演化式計算中的粒子群尋優演算法與支撐向量機技術，進行垃圾郵件特
徵之分析，以尋求最適(或最小)分類的特徵維度。在第二階段中，則利用第一階段所建
立的最適垃圾郵件分類用之特徵集合，應用巢狀式演化式演算法(nested evolutionary 
algorithm, NEA)[74]技術，轉換成垃圾郵件分類所用的規則(classification rules)。重點在
於，如何將分類用的特徵(feature)，也就是一些獨立的關鍵字集，變成一些可用的分類規
則。 
(1) 首先，我們將針對取得的測試郵件集進行前處理(pre-processing)，步驟如下： 
Step 1. 郵件集前置處理：將垃圾與正常郵件集逐封進行 stemming、stopping、去除數
字、特殊符號與全字小寫等。 
Step 2. 關鍵字選取：對於經過前置處理之郵件集，使用文字分類上常使用的特徵選取
方法，例如：document frequency (DF), term frequency-inverse document frequency 
(TF-IDF), information gain (IG), mutual information (MI)等方法進行關鍵字選取。 
Step 3. 關鍵字後續處理：對於選出之所有關鍵字進行垃圾與正常郵件相同關鍵字比
對，去除無意義之關鍵字(例如：選出之字彙為 xxxyyy，此種在英語中不具意
義之字)等後續處理。 
Step 4. 產生可供實驗使用之關鍵字。 
(2) 然而如前所述，並非每一個關鍵字對於分類都有所助益；再則，經由作法(1)中篩選而
得的關鍵字數量可能相當龐大。因此，如果我們可以尋找出最適用(或最佳)的特徵組
合，可以縮減分類時的時間，亦能獲取最大的分類效能。我們將採取對最佳化問題有
良好效能之 DPSO，輔助篩選既有特徵；步驟如下所述。 
Step 1. 產生可能的特徵組合：由於關鍵字的數量可能很多，因此手動調整組合方式絕
對不可行。藉助適當的演算法則進行選取，才能達成自動化的目的。我們在此
計畫中，選定了 DPSO 演算法，以其進行合適關鍵字集組合的選取。將粒子的
位置值視為特徵，用意為是否採用該項特徵為產生特徵組合之依據，意即當某
欄位數值為 1 時，代表所對應之特徵將被使用，數值為 0 時則不使用。 
 
 
 
 
 
圖 1 Particle swarm 表示垃圾郵件之特徵組合 
0 1 0 0 1...
1 0 0 1 1...
1 1 0 1 0...
keywords:   money      sex       friend                                love  Viagra     
population: 
k particles
 5
其中， S Sn → 為被正確分類的垃圾郵件數量， S Ln → 為被誤判的垃圾郵件。 
 單純使用 Precision 或 Recall 來評估分類系統的好壞，有時候會有不太客觀
的情形。如果可以調和二者的結果，便可以充分顯示出該分類系統的特點所
在。因此，有學者提出了所謂的 F-Measure 如下式所示 
SLLSSS
SS
nnn
n
→→→
→
++
⋅=+
××=
2
2
Recall Precision 
RecallPrecision2  Measure-F  (4) 
 高召回率和高準確率一直是每位研究者所期望的目標，但事實上一高一低，
或二者皆低之情況均有可能發生。因此可由該 2 個項目得知實際分類情形，
並分析可能原因。Androutsopoulos et al. [68, 69, 70, 71]提出高分類率與低錯
誤率會造成對分類實驗的誤解，衡量分類系統優劣的基礎，應該建立在郵件
被誤判率(垃圾郵件判定成正常郵件，或正常郵件被視為垃圾郵件)的降低。
因此 Androutsopoulos 等人引入權重概念於正確率及錯誤率公式，以加重計
分方式強調分類正確的重要性以及分類錯誤的嚴重性。 
此外，他們亦提出包含權重值概念的正確率(WAcc)與錯誤率(WErr)。以及可
清楚得知郵件誤判情況，包含 bWAcc 與 bWErr 的基準線(baseline)公式。最終，
他們結合 WErr 與 bWErr ，提出總值比例(total cost ratio, TCR)公式，可就 TCR
值的高低評估分類系統的優劣。以下將對 WErr, bWErr 與 TCR 進行說明。 
WErr 為考慮所有郵件數量中，被誤判郵件所佔的比例；其中，屬於正常郵
件的部分必須有權重上的考量。 
SL
LSSL
NN
nnWErr +⋅
+⋅= →→λ
λ , (5) 
其中， LNλ ⋅ 為加重計分之正常郵件總數量， SN 垃圾郵件總數量， L Snλ →⋅ 為
加重計分之被誤判正常郵件之數量， S Ln → 為被誤判垃圾郵件之數量。 
bWErr 在於衡量加重正常郵件的記分設定下，垃圾郵件佔所有郵件的比例。
其數值越小，代表分類出之垃圾郵件比例越高。 
SL
Sb
NN
N WErr +⋅= λ , (6) 
其中 LNλ ⋅ 為被正確分類且加重計分的正常郵件數量， SN 為垃圾郵件總數
量。 
總值比例結合 WErr 與 bWErr ，其目的在於計算所有垃圾郵件數量與所有誤
判郵件數量之比例，以所得數值之高低，衡量分類效能的優劣。總值比例越
高，代表該系統設計越精良。 
Step 5. 進行 DPSO 的相關參數更新：依據 Step 4 所得到適應值(fitness)，檢查是
否滿足實驗設定，以進行 DPSO 相關參數的數值更新程序。更新步驟如
下所示： 
 7
新設計交配運算元，以發揮最大的功效。 
Step 6. 至於突變運算元則需分上下層個別考慮：在上層的演化式演算法中，由於染色
體的組成單元是來自於第一階段所獲得的關鍵字集；因此，最簡單的突變運算
元可設計成重新從關鍵字集中任意找尋一個字來進行替換即可。至於下層的演
化式演算法中，最簡單的方法亦可仿效上層的動作，從所考慮的布林運算單元
中擇一取代原有的運算元；或者，亦可設計成任意選擇突變位置後，將該位置
上的布林運算元互相交換。 
Step 7. 經過上述 Step 1 ~ 6 的動作後，我們將得到以最適關鍵字集中的關鍵字搭配布
林運算元的分類法則。然而我們可以透過一後續處理動作(post-processing)，來
構建出更具意義且容易設定的分類規則。我們將檢查規則中的關鍵字，分析其
究竟出現在郵件中的那個位置(Header, Subject, or Body)，如此，就可以產生出
如下的垃圾郵件分類法則： 
Rule_k: IF (Subject contains "Information" AND Body contains "message" AND 
"job") THEN (give the mail a high rating for spam) 
3. Experiments 
實驗中所使用之郵件集分別來自 spamassassin, i-config 與 E. M. Canada 網站所提供之
部分公開正常與垃圾郵件集(public corpus)，依據後續實驗的需求，我們規劃了以下 4 個
郵件集，如表 1 所示。 
表 1 實驗用郵件集相關細節表 
編號 郵件屬性 郵件數量 郵件出處 
正常郵件 2,501 spamassassin 2003_0228_easy_ham。 C1 垃圾郵件 501 spamassassin 2003_0228_spam。 
正常郵件 1,401 spamassassin 2003_0228_ easy_ham_2。 C2 垃圾郵件 1,397 spamassassin 2003_0228_spam_2。 
正常郵件 2,412 Ling-Spam 郵件集中的正常郵件。 C3 垃圾郵件 481 Ling-Spam 郵件集中的垃圾郵件。 
正常郵件 2,501 spamassassin 2003_0228_easy_ham。 C4 垃圾郵件 4,441 E. M. Canada 2005 年 5 月份垃圾郵件集。 
本實驗主要用意，在於證明所提方法確實能有效縮減關鍵字特徵的維度，並產生與使
用所有關鍵字進行分類時，相同甚至更高之效能。首先由 DF, TF-IDF 與 IG 等既有關鍵字
特徵選取方法，對於 C1-C4 等 4 個郵件集進行關鍵字特徵選取，接著使用上述 3 個方法選
出前之 100 個關鍵字，配合 SVM 進行郵件分類，並以正確率、準確率、召回率與總值比例
等項目，配合關鍵字數量進行效能評估。DPSO 實驗則全部進行 200 次疊代，依上述評估
項目取其中表現最佳粒子進行比較。 
表 2 為實驗的結果數據，其中 DF, TF-IDF 與 IG 欄位表示利用前述方法選出之前 100
個關鍵字進行郵件分類之結果，DPSO 欄位則表示應用所提方法進行關鍵字再次選取的結
果。根據對所有數據的觀察發現以下現象： 
z 4 個郵件集中，以所提方法對於以 C1 與 C3 進行之分類實驗，其正確率、準確率、
召回率與總值比例等 4 方面的表現均相當優良，C2 與 C4 則差強人意。若引進垃
圾郵件比例(即為所有郵件中垃圾郵件佔的比例)概念，經計算後發現 C1 為
16.69%、C2 為 49.93%、C3 為 16.63%，C4 為 63.97%。因此，垃圾郵件比例的高
 9
網路化的普及，使得電子郵件已成為每位使用者不可缺少的溝通工具之一；然而，垃圾郵
件的問題，卻使得使用者面臨前所未有的大問題，各國無不絞盡腦汁以因應此現象。本計
畫利用演化式演算法的特性，結合目前廣為使用的支撐向量機作為核心技術，可以有效地
縮減用以篩選垃圾郵件的特徵維度，並且自動地產生對應的分類規則。因此，我們很肯定
此一方法確實可以應用在此實用性甚高的研究題目上。 
 
5. Self-evaluation 
共發表下列論文: 
1. Chih-Chin Lai* and C.-H. Wu, "Particle swarm optimization-aided feature selection for 
spam email classification," in Proc. The Second International Conference on Innovative 
Computing, Information and Control, Kumamoto City International Center, Kumamoto, 
Japan, Sep. 5-7, 2007. (NSC95-2221-E-024-015) 
2. Chih-Chin Lai* and C.-H. Wu, "Feature selection using particle swarm optimization with 
application in spam filtering," submitted to Information Processing and Management, 
2007. (NSC95-2221-E-024-015) 
3. Chih-Chin Lai* and C.-Y. Chang, "A hierarchical evolutionary algorithm for automatic 
medical image segmentation," submitted to Expert Systems with Applications, 2007. 
(NSC95-2221-E-024-015) 
 
References 
[1] Gfi Mail Essentials http://www.gfi.com/mes/anti-spam.htm?adclickid=182636 
[2] SpamCatcher, http://www.mailshell.com/spamcatcher/desktop_fd2.html 
[3] Inboxer, http://www.inboxer.com/index.shtml 
[4] SpamKiller, http://us.mcafee.com/root/package.asp?pkgid=156 
[5] Kill The Spams, http://www.zipstore.com/page7.html 
[6] SpamInspector, http://www.spamfilterreview.com/ 
[7] SpamEater Pro, http://www.hms.com/spameater.asp 
[8] Spam Buster, http://www.contactplus.com/products/spam/spam.htm 
[9] Email Protect, http://www.spamfilterreview.com/ 
[10] Choice Mail One, http://www.spamfilterreview.com/ 
[11] SpamAssassin, http://www.spamassassin.org/ 
[12] SpamPal, http://www.spampal.org 
[13] POP3 Agent, http://pop3agent.sourceforge.net/ 
[14] Spamless, http://www.qoosoft.com/ 
[15] MailWasher, http://www.mailwasher.net/ 
[16] K9, http://keir.net/k9.html 
[17] Bogofilter, http://bogofilter.org/ 
[18] ASSP, http://assp.sourceforge.net/ 
[19] SpamBayes, http://www.spambayes.org 
[20] E-Mail Relay, http://emailrelay.sourceforge.net/ 
[21] Mailfilter, http://mailfilter.sourceforge.net/ 
[22] Cohen, W., “Learning rules that classify e-mail,” in AAAI Spring Symposium on Machine 
Learning in Information Access, pp.18-25, 1996. 
[23] Brutlag, C. and J. Meek, “Challenges of the email domain for text classification,” 
Proceedings of Seventeenth Int'l Conf. on Machine Learning, 2000. 
[24] Ng, H. T., W. B. Goh and K. L. Low, “Feature selection, perception learning, and a usability 
case study for text categorization,” ACM SIGIR Forum, Vol. 31, Issue SI, pp. 67-73, 1997. 
[25] Payne, T. and P. Edwards, “Interface agents that learn: an investigation of learning issues in 
a mail agent interface,” Applied Artificial Intelligence, Vol. 11, pp. 1-32, 1997. 
[26] Boone, G., “Concept features in re:agent, an intelligent email agent,” Proceedings of 
Second Int'l Conf. on Autonomous Agents, 1998. 
[27] Pantel, P. and D. Lin, “Spamcop: A spam classification & organization program,” 
 11
ACM, vol. 35, pp.48-63, 1992. 
[51]Yang, Y., "Expert network: effective and efficient learning from human decisions in text 
categorization and retrieval," in 17th Ann Int'l ACM SIGIR Conf. on Research and 
Development in Information Retrieval, pp.13-22, 1994. 
[52] Lewis, D. D. and M. Ringuette, "Comparison of two learning algorithms for text 
categorization," in Proc. the Third Annual Symposium on Document Analysis and 
Information Retrieval, 1994. 
[53] Tzeras, K. and S. Hartman, "Automatic indexing based on bayesian inference networks," in 
Proc. 16th Ann Int'l ACM SIGIR Conf. on Research and Development in Information 
Retrieval, pp.22-34, 1993. 
[54] Wiener, E., J. O. Pedersen and A. S. Weigend, "A neural network approach to topic 
spotting," in Proc. the Fourth Annual Symposium on Document Analysis and Information 
Retrieval, 1995. 
[55] Apte, C., F. Damerau, and S. Weiss, "Towards language independent automated learning of 
text categorization models," in Proc. the 17th Annual ACM/SIGIR Conf., 1994. 
[56] Cohen, W. W. and Y. Singer, "Context-sensitive learning methods for text categorization," 
in SIGIR'96: Proc. the 19th Annual Int'l ACM SIGIR Conf. on Research and Development 
in Information Retrieval, pp.307-315, 1996. 
[57] Moulinier, I., G. Raskinis, and J. Ganascia, "Text categorization: a symbolic approach," in 
Proc. the Fifth Symposium on Document Analysis and Information Retrieval, 1996. 
[58] Lewis, D. D., R. E. Schapire, J. P. Callan, and R. Papka, "Training algorithms for linear text 
classifiers," in SIGIR'96: Proc. the 19th Annual Int'l ACM SIGIR Conf. on Research and 
Development in Information Retrieval, pp.298-306, 1996. 
[59] Yang, Y. and J. O. Pedersen, "A comparative study on feature selection in text 
categorization," in Proc. 14th Conf. Machine Learning, 1997. 
[60] Kennedy, J. and R. C. Eberhart, “Particle swarm optimization,” Proceedings of IEEE 
International Conference on Neural Networks, Vol. IV, pp. 1942-1948, 1995. 
[61] Katirai, H., “Filtering junk e-mail: A performance comparison between genetic 
programming & naive bayes,” Available: http://members.rogers.com/ 
hoomank/papers/katirai99filtering.pdf, 1999. 
[62] Sousa, T., A. Silva and A. Neves, “A Particle Swarm Data Miner,” Proceedings of 11th 
Portuguese Conference on Artificial Intelligence, pp. 43-53, 2003. 
[63] Sousa, T., A. Silva and A. Neves, “Particle swarm based data mining algorithms for 
classification tasks,” Parallel and Nature-inspired Computational Paradigms and 
Applications, Vol. 30, Issue 5-6, pp. 767-783, 2004. 
[64] Fukuyama, Y., “Fundamentals of particle swarm techniques,” Modern Heuristic 
Optimization Techniques with Applications to Power Systems, IEEE Power Engineering 
Society, pp. 45-51, 2003. 
[65] Kennedy, J. and R. Eberhart, "A discrete binary version of the particle swarm optimization 
algorithm," in Proc. of the 1997 Conf. on Systems, Man, and Cybernetics, pp.4104-4109, 
1997. 
[66] Crstianini, N. and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other 
Kernel-based Learning Methods, Cambridege Univ. Press. 
[67] Scholkopf, B. and A. J. Smola, Learning with Kernels – Support Vector Machines, 
Regularization, Optimization, and Beyond, The MIT Press, 2002. 
[68] Androutsopoulos, I., J. Koustantinos, K. V. Chandrinos, G. Paliouras and C. D. Spyropoulos, 
“An evaluation of Naïve Bayesian anti-spam filtering,” in Proc. of the Workshop on 
Machine Learning in the New Information Age, 11th European Conference on Machine 
Learning, pp. 9-17, 2000. 
[69] Androutsopoulos, I., G. Paliouras, V. Karkaletsis, G. Sakkis, C. D. Spyropoulos and P. 
Stamatopoulos, “Learning to filter spam e-mail: A comparison of a Naive Bayesian and a 
memory-based approach,” in Proc. of the Workshop on Machine Learning and Textual 
Information, 4th European Conference on Principles and Practice of Knowledge Discovery 
in Databases, pp. 1-13, 2000. 
[70] Androutsopoulos, I., J. Koutsias, K. V. Chandrinos and C. D. Spyropoulos, “An 
experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with 
 13
 
可供推廣之研發成果資料表 
□ 可申請專利  ; 可技術移轉                                      日期：96 年 8 月 27 日 
國科會補助計畫 
計畫名稱：應用演化式計算於網路不當資訊的特徵分析與分類規則
之產生 
計畫主持人：賴智錦 
計畫編號：NSC 95-2221-E-024-015- 
學門領域：Web 技術 
技術/創作名稱  
發明人/創作人  
中文： 
在本次的計畫中，我們以兩階段的方式，以演化式計算的技術
結合文件分類的方法，從事垃圾郵件的特徵分析後進而產生最適的
特徵集合；然後從此特徵集合中，嘗試建立用以分類垃圾郵件的規
則。 
技術說明 英文： 
In this project, we try to design a two-stage approach to overcome the 
spam emails problem. The proposed approach integrates evolutionary 
computation technique and text categorization method, and the 
purposes of it are to reduce the dimension of feature space and to 
automatically generate the classification rules. 
可利用之產業 
及 
可開發之產品 
網際網路相關產業、網路安全、垃圾郵件濾除軟體 
技術特點 
本研究計畫從事"應用演化式計算於網路不當資訊的特徵分析與分
類規則之產生"的技術研究。其研究的主要目的是利用演化式演算
法中的粒子群尋優演算法，及巢狀型式演化式演算法等具有強韌且
搜尋全域最佳解答的能力，可以提供使用者更精確的資訊。垃圾郵
件的問題，卻使得使用者面臨前所未有的大問題。本計畫利用演化
式演算法的特性，結合目前廣為使用的支撐向量機作為核心技術，
可以有效地縮減用以篩選垃圾郵件的特徵維度，並且自動地產生對
應的分類規則。 
推廣及運用的價值 
可視為一種新型態的垃圾郵件濾除技術 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
 
三、建議與結語 
 
 
四、攜回資料 
SCIS & ISIS 2006 會議議程資料一份。 
SCIS & ISIS 2006 Proceedings 光碟一份。 
相關國際性研討會資料數份(Call-for-Paper)。 
 
三、建議與結語 
 
 
四、攜回資料 
SCIS & ISIS 2006 會議議程資料一份。 
SCIS & ISIS 2006 Proceedings 光碟一份。 
相關國際性研討會資料數份(Call-for-Paper)。 
Section 2, the basic concept of the hierarchical differential
evolution algorithm is introduced and then the proposed ap-
proach is described. The experimental results and discussions
are given in Section 3. The conclusions are summarized in
Section 4.
II. THE PROPOSED INTELLIGENT CLUSTERING APPROACH
Clustering process is ubiquitous in many real-world prob-
lems, especially those labeled as pattern recognition problems
at an abstract level. When we applied any clustering approach
to partition a data set, determining the proper or optimal
number of clusters for a data set is one of the more difficult
aspects to the clustering process. The hierarchical differential
evolution algorithm with the properties of exploration and
exploitation capabilities is properly used to simultaneously
determine the numbers of clusters as well as properly partition
a data set. In this section, we first present the hierarchical dif-
ferential evolution algorithm, and then introduce the proposed
clustering approach.
A. Hierarchical differential evolution
Differential evolution (DE) [22] is a relatively new type of
Evolutionary algorithms (EAs), which are a set of randomized
search and optimization techniques guided by the principles
of evolution and natural genetics, and have a large amount of
implicit parallelism. They provide near optimal solutions of an
objective or fitness function in complex, large, and multimodal
landscapes. The DE algorithm resembles the structure of an
EA, but differs from traditional EA in its mutation scheme and
the ‘greedy’ selection process. The former makes DE has self
adaptive property and the latter provides significant advantage
of converging performance over genetic algorithms.
The HIDE algorithm is a comparatively simple variant of
the DE algorithm. The main difference between the former
and the latter is the structure of the chromosome. In HIDE
algorithm, the chromosome consists of two types of genes –
the control genes and the parametric genes. The purpose of
control genes is designed to determine which parameter gene
should be utilized and which one can be disabled during the
process by HIDE. Generally, the control genes are coded as
binary digits, while the parametric genes can be coded as any
type of data structure. If the value of a control gene is “1”,
then the associated parametric gene is activated; otherwise, the
associated parametric gene is disabled.
B. The proposed approach
When we apply the HIDE algorithm to the automatic clus-
tering problem, we must consider the following components:
(1) a genetic representation of solutions to the problem, (2)
one way to create the initial population of solutions, (3) an
evaluation function that rates all candidate solutions according
to their “fitness”, (4) genetic operators that alter genetic
composition of children during reproduction.
  Solution representation: In the proposed approach, the
chromosome is made up of binary digit (the total number
of “1” implicitly represents the number of clusters) as
well as real numbers (representing the estimated coordi-
nates of the cluster centers). The number of control genes
is decided by the soft estimation of the upper bound of
the number of clusters. An example of the hierarchical
chromosome structure in our approach is illustrated as
follows, where it represents a partition of a give data set
with two clusters, and the associated cluster centers are
     and     	, respectively.

 

 
 	
   
    

  	
   
              	  	   
  Initial population: A HIDE algorithm requires a pop-
ulation of potential solutions to be initialized at the
beginning of the evolution process. Here we adopt the
random generation scheme to create the initial population
until all chromosomes in a population are created.
  Fitness function: A fitness function is the survival arbiter
for chromosomes. Three different cluster validity indices
used in our approach are described in the next section. For
each chromosome, the centers encoded in it are extracted,
and then a partition is obtained by assigning the data
points to a cluster corresponding to the closest center.
Given the above partition, and the number of clusters,
the value of the cluster validity index is computed and
used as the fitness of the selected chromosome.
  Mutation: The mutation operator is needed to explore
new areas of the search space and helps the search
procedure avoid sticking in local optima. Here we just
apply the mutation scheme to the parametric genes.
Assume the parametric genes in a chromosome can
be viewed as a 	-dimensional vector 

 
, here  de-
notes the generation number of the algorithm. Accord-
ing to the mutation scheme, for each vector 

 
  

          (population size), a new mutated vector
is created by

 
 

 



 
 




  




 (1)
where 

 

 

           are mutually different
random indices and they also differ from the current index
.  is a real and constant factor which controls the
amplification of the different variation [22].
  Crossover: Following the mutation phrase, the crossover
operator is applied to the population. The crossover
operator randomly pairs chromosomes and swaps parts of
their genetic information to produce new chromosomes.
A trial vector is

 
 

  
 
 
 
 
 
        
 
 
 (2)
is generated, where

 
 




 
 
 if (   CR ) or  
  



 
 if (   CR ) and  
   (3)
where  
          	;      is the th evaluation
of a uniform number generator; CR is the crossover
- 1919 -
TABLE I
DESCRIPTION OF THE DATA SETS
Data Set No. of points No. of clusters Points per cluster
D1 540 3 120, 120, 300
D2 480 4 120
D3 275 5 55
D4 500 10 50
Iris 150 3 50
Cancer 683 2 444, 239
TABLE II
ACTUAL AND COMPUTED NUMBER OF CLUSTERS FOR THE DATA SETS
Data Set Actual No. DB-index Dunn’s index CH-index
of Clusters
D1 3 3 6 3
D2 4 4 6 4
D3 5 5 5 5
D4 10 8 2 10
Iris 3 2 2 2
Cancer 2 2 2 2
of the Iris data, 2 or 3 clusters are preferred depending on the
validity index used [16].
On the other hand, to show that the proposed approach
can be applied to other application, a CT head image in
which there exist a number of tiny tissues was used for the
experiment. Figure 2 is the segmented result of our method.
From the result, we can see that the proposed approach
can correctly segment the image into three distinct regions
(clusters) and obtain a clear and complete segmentation image.
V. CONCLUSION
In this paper, an intelligent clustering approach using hi-
erarchical differential evolution algorithm was proposed to
solve the automatic clustering problem. Clustering plays an
important role for knowledge discovery and is being applied in
a wide variety of engineering and scientific disciplines. Many
clustering algorithms can be found in the literature. However,
most of the clustering algorithms suffer from the drawback
that the number of clusters has to be known in advance. In
many applications, this knowledge is not available. In the
proposed clustering approach, it can automatically search for a
proper and correct number of clusters. On the other hand, the
centers of clusters are also determined, and then a partition of
given data set is done by assigning the points to corresponding
center. The hierarchical differential evolution algorithm is the
conventional differential evolution algorithm with hierarchical
genetic structure. The proposed clustering approach has been
applied to the several artificial and real-life data sets. Another
interesting real life application is that HIDE clustering method
has the ability to automatically segment a CT image.
ACKNOWLEDGMENT
This work was partially supported by the National Science
Council, Taiwan, R.O.C. under grant NSC 94-2213-E-024-
005.
REFERENCES
[1] C.L. Blake, C.J. Merz, UCI Repository of machine learning databases
[http://www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: Uni-
versity of California, Department of Information and Computer Science
(1998).
[2] J.C. Bezdek, N.R. Pal, “Some new indexes of cluster validity,” IEEE
Trans. Syst., Man Cybern., vol.28, no.3, pp.301–315, 1998.
[3] S. Bandyopadhyay, U. Maulik, “Nonparametric genetic clustering: com-
parison of validity indices,” IEEE Trans. Syst., Man Cybern., vol.31,
no.1, pp.120–125, 2001.
[4] S. Bandyopadhyay, U. Maulik, “Genetic clustering for automatic evolu-
tion of clusters and application to image classification,” Patt. Recong.,
vol.35, pp.1197–1208, 2002.
[5] R.B. Calinski,J. Harabasz, “A dendrite method for cluster analysis,”
Comm. in Statistics, vol.3, pp.1–27, 1974.
[6] L. Davis, Handbook of Genetic Algorithm, Van-Nostrand, New York,
1991.
[7] D.L. Davies, D.W. Bouldin, “A cluster separation measure,” IEEE Trans.
Patt. Anal. MAch. Intell., vol.1, pp.224–227, 1979.
[8] K.A. DeJong, W.M. Spears, An analysis of the interacting roles of
population size and crossover in geneitc algorithms. in Parallel Problem
Solving from Nature, H.-P. schwefel and R. Manner, Eds., Springer-
Verlag, Berlin, 1990, p.38.
[9] J.C. Dunn, “A fuzzy relative of the ISODATA process and its use in
detecting compact well-separated clusters,” J. Cybern., vol.3, pp.32–57,
1973.
[10] R.A. Fisher, The use of multiple measurements in taxonomic problems,
Ann. Eugen. 3 (1936) 179–188.
[11] D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Ma-
chine Learning, Addison-Wesley:Reading, MA, 1989.
[12] D.E. Goldberg, K. Deb, “A comparison of seletion schemes used in
genetic algorithms,” in Foundations of Genetic Algorithm 1, pp.69–93,
1991.
[13] J.J. Grefenstette, Optimization of control parameters for genetic algo-
rithms. IEEE Trans. Systems, Man, and Cybern. 16(1) (1986) 122–128.
[14] M. Herbin, N. Bonnet, P. Vautrot, “Estimation of the number of clusters
and influence zones,” Pattern Recognition Letters, vol.22, pp.1557–1568,
2001.
[15] A.K. Jain, M.N. Murty, P.J. Flynn, “Data clustering,” ACM Computing
Surveys, vol.31, no.3, 1999. (1999) 264–323.
[16] R. Kothari, D. Pitts, “On finding the number of clusters,” Patt. Recog.
Lett., vol.20, pp.405–416, 1999.
[17] D.J.C. MacKay, Bayesian Methods for Adaptive Models, Thesis, Cali-
fornia Institute of Technology, Pasadena.
[18] K.F. Man, K.S. Tang, S. Kwong, Genetic Algorithms: Concepts and
Designs, Springer-Verlag:London, 1999.
[19] U. Maulik, S. Banyopadhyay, “Genetic algorithm-based clustering tech-
nique,” Pattern Recognition., vol.33, pp.1455–1465, 2000.
[20] M. Michalewicz, Genetic Algorithms + Data Structure = Evloution
Program, Springer-Verlag:Berlin, 1996.
[21] S.K. Pal, P.P. Wang (Eds.), Genetic Algorithms for Pattern Recognition,
CRC Press, Boca Raton, 1996.
[22] R. Storn and K. Price, “Differential evolution – a simple and efficient
adaptive scheme for global optimization over continuous spaces,” Tech-
nical report, International Computer Science Institute, Berkely, 1995.
[23] M. Sarkar, B. Yegnanarayana, D. Khemani, “A clustering algorithm
using an evolutionary programming-based approach,” Patt. Recog. Lett.,
vol.18, pp.975–986, 1997.
[24] J.T. Tou, R.C. Gonzalez, Pattern Recognition Principles, Reading,
MA:Addison-Wesley, 1974.
[25] L.Y. Tseng, S.B. Yang, “A genetic approach to the automatic clustering
problem,” Patt. Recog., vol.34, pp.415–424, 2001.
- 1921 -
