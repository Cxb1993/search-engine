[11-13]。而在人臉偵測方面，Fan and Paindavoine 
[14]採用TI TMS320C62及FPGA實作一個利用RBF
類神經網路的即時人臉追蹤與辨識系統。Chen et 
al. [15]則是使用兩顆TI TMS320C6201 DSPS作平
行處理，實作出即時人臉偵測系統。Ming-Hsuan 
Yang等人[16]，曾探討近年來人臉偵測的技術，
主要包括以知識、特徵、樣版、訓練等四類為基
礎的方式，其中Viola[13]所提出的AdaBoost演算
法即是以訓練為基礎的方法，透過對大量影像的
訓練，以取得一些特徵參數，來實現人臉的偵
測。本系統即是利用此方式偵測出人臉，再以相
對位置的劃分，進而偵測眼睛以及鼻子的實際位
置，用以判斷駕駛者的轉頭角度。 
在取得駕駛者轉頭角度及道路線的方向兩項
資訊後，再來則是分析兩者間的差異，最後結合
前車及行人資訊，做為判斷駕駛安全性的依據。 
 
二、研究架構與方法 
  
1. 系統架構 
 本系統所開發的演算法以TI DaVinci 嵌入式
系統為發展平台。DaVinci為雙核心處理器設計，
有ARM926EJ-S及C64x+兩個CPU。ARM-SIDE負
責影像擷取、控制與影像輸出，DSP-SIDE負責影
像處理。ARM-SIDE透過stubs 把資料傳到DSP透
過skeleton 接收資料進行處理，如圖 1 所示。圖 2
為影像處理的流程，GPP為ARM-SIDE，包括
Control thread 、Video thread、Display thread ，
在Video thread 部分連接到DSP-SIDE 。 
   
圖 1 Adapter in Remote Execution  
 
圖 2 Application Data Flow 
2. 道路線偵測演算法 
 
為偵測道路線，我們使用 Sobel edge detector
進行垂直邊緣偵測及影像二值化，如圖 3(a)所
示。接著使用型態學運算子，填補空隙部分，同
時也消除部份雜訊。另為了避免因虛線而造成偵
測不到道路影像的結果，我們將前十五張的邊緣
偵測結果做 OR 運算，以便將中斷的道路線連接
起來，如圖 3(b) 所示。用 labeling 把相連像素歸
在一類，取出長度最常當作車道線，如圖 3(c) 所
示。最後經由反透視轉換取得道路線真實空間位
置，再由兩條道路線獲得道道路中線，如圖 3(d) 
所示。 
   
(a)   (b) 
  
(c)   (d) 
圖 3 道路線偵測結果，(a)邊緣偵測結果，(b)OR
運算結果，(c)偵測結果，(d)反透視轉換結果 
 
2.3 車輛偵測演算法 
在車輛偵測部分，首先使用水平 Sobel filter 
擷取車輛車尾的水平邊緣，如圖 4(a)所示。然後
使用水平形態學運算子消除非水平邊緣，並修補
水平邊緣的破損，如圖 4(b) 所示。另外使用
Sobel edge detector 進行垂直邊緣偵測及影像二值
化，如圖 4(c)所示。用 labeling 運算子把水平邊緣
相連像素歸在一類，當作候選區域，並去除過短
部份。計算候選區域垂直邊緣的垂直投影量，以
峰值當作左右邊界。最後，使用 AdaBoost 分類器
確認候選區域是否為車輛，如圖 4(d)所示。 
   
(a)   (b) 
    
(c)           (d) 
圖 4 (a)水平邊緣偵測；(b)水平形態學運算；(c)垂
直邊緣偵測；(d)車輛偵測結果 
 
位為7，欄位的設定與之前所介紹之轉頭角度相
同。在道路線的相似度模型中，扇形欄位為9，因
此每個角度均為20度，如圖12所示，半圓半徑一
樣為原點到圖形左上角的距離的一半。其中道路
中線與駕駛者視線之相關係數依式(2)所算出來的
相似度為Cdriver，道路中線與行車線之相關係數依
式(2)所算出來的相似度為CLane。 
     
圖11 視線相似度欄位   圖12 車道線相似度欄位 
 
藉由這兩個相關係數的關係，即可用來判斷駕駛
者的安全性，如式(3)所示。 
 
)1()1( +×+= DriverLaneDanger CCC    (3) 
利用CDanger的數值大小來判斷駕駛狀況是否安全，
在數值最大及較大時給予駕駛者警告音，使駕駛
者注意車輛行駛的情況，減低駕駛者因注意力不
足而造成意外的情況。其定義的範圍如表1所示。 
 
表1 駕駛狀態範圍 
 CLane CDriver CDanger 
安全 80以下 60以下 4800以下 
可能危險 80～180 60～120 4800～21600
危險 180以上 120以上 21600以上 
 
三、實驗結果 
本研究所用實驗平台為TI DaVinci發展平
台，此平台為雙核心設計，一個ARM9及C64X+ 
DSP。ARM處理器負責整個系統的控制、影像擷
取及顯示，而DSP負責各種演算法的處理。使用
一個CCD攝影機及反射鏡擷取前方道路影像及駕
駛者影像，解析度為320X240。本文實驗分為五
個部份：車道線偵測、前車偵測、行人偵測、駕
駛者視線偵測及駕駛狀況監控實驗，並在不同天
氣情況下測試，驗證其系統的強健性。 
 
3.1 車道線偵測結果 
在此我們將車道線情況分為直線、右轉以及
左轉，利用此三種不同情況的相關係數CLane，來
說明行車的狀況。並在不同的天氣情況下測試，
包括晴天，陰天及雨天。 
圖13(a)為駕駛在直線路段，其相關函數值較
小，圖13(b)正中央淺白色的直線為行車方向；左
右白色的線段為所偵測到之道路線；中間白色的
直線，是根據所偵測到道路線所預測的道路中線
。圖13(b)相關係數為13.26，為較小的值。 
   
(a)   (b) 
圖13 直線路段1，(a)道路影像，(b)道路中線 
   
(a)     (b) 
圖14 直線路段2，(a)道路影像，(b)道路中線 
   
(a)     (b) 
圖15 直線路段3，(a)道路影像，(b)道路中線 
   
(a)     (b) 
圖16 直線路段4，(a)道路影像，(b)道路中線 
 
圖17(a)為行駛於轉彎道路，圖17(b)為17(a)之
道路中線及行車方向，圖17(b)相關係數69.88。因
為車輛行駛於轉彎道路中，因此所得之相關係數
值比前一個直線路段所得之相關係數值還要大。 
   
(a)    (b) 
圖17 左轉彎影像之行車線與道路方向，(a)道路影
像，(b)道路中線 
   
(a)     (b) 
圖 22 行人影像測試結果，(a)垂直邊緣偵測，(b)
垂直形態學運算，(c)影像標籤化結果，(d)
偵測結果 
 
3.4 駕駛者影像實驗結果 
我們利用AdaBoost演算法偵測人臉，並依照
人臉相對的位置，接著對左右眼矩形區域使用
AdaBoost演算法來做眼睛的偵測，將其範圍再次
縮小，以二值化投影的方式，找出雙眼以及鼻子
。人臉偵測及雙眼偵測結果如圖23所示。  
   
  
  
圖23 人臉五官偵測結果 
 
我們利用雙眼兩段的比率，來判斷駕駛者的
視線方向。利用鼻孔判斷出鼻子中間的位置將雙
眼區分為兩段距離。另外我們也定義了七個視線
方向的區域。如表4-1所示，分別為 ±12°、-12°
～-24°、-36°～-24°、-90°～-36°、12°～24°、24°
～36°以及36°～90°。其區間的閥值定義如圖24所
示。 
 
 
圖24 區間閥值定義 
 
在決定駕駛者視線的角度後，利用視線方向
與道路方向來計算相關係數CDriver。當車輛偏離車
道時，計算相關係數CDriver，若此相關係數值較小
，則表示在注視著道路狀況。 
 
3.5 駕駛狀況監控實驗結果 
 如圖25所示，為駕駛狀態偵測結果，圖25(a)
為駕駛者影像，駕駛者看正前方，偵測結果如圖
25(b)所示，圖25(c)為道路影像，圖25(d)為道路方
向偵測結果，為直線道路，其相關係數CLane為
13.34，其相關係數CDriver為0.55；所得之CDanger為
22.227。 
   
(a)     (b) 
   
(c)    (d) 
圖25 駕駛狀態偵測結果(1)，(a)人臉影像，(b)視
線方向，(c)道路影像，(d)道路中線 
 
圖26所示為另一狀況駕駛狀態偵測結果，圖
26(a)為駕駛者影像，駕駛者看左前方，偵測結果
如圖26(b)所示，圖26(c)為道路影像，圖26(d)為道
路方向偵測結果，為左轉彎的道路，其相關係數
CLane為115.96，其相關係數CDriver為0.62；所得之
CDanger為189.48。 
   
(a)    (b) 
   
(c)    (d) 
圖26 駕駛狀態偵測結果(2) ，(a)人臉影像，(b)視
線方向，(c)道路影像，(d)道路中線 
 
圖27所示為另一狀況駕駛狀態偵測結果，圖27(a)
為駕駛者影像，駕駛者看右前方，偵測結果如圖
27(b)所示，圖27(c)為道路影像，圖27(d)為道路方
向偵測結果，為直線道路，其相關係數CLane為
38.69，其相關係數CDriver為132.57；所得之CDanger
為5301.39。 
 
   
(a)     (b) 
12 
24
3612 24 36 
-0.30.7 0.5 0.3 R 
轉頭角度
1 
2 3 4 5 6
7
 
-0.7 -0.5
Sensing and Control, vol. 2, pp. 1323-1328, 
2004. 
[13] Paul Viola and Michael Jones, “Rapid object 
detection using a boosted cascade of simple 
features”, Proceedings of IEEE Conference on 
Computer Vision and Pattern Recognition 
(CVPR), vol. 1, pp. 511-518, 2001. 
[14] Y. Fan and M. Paindavoine, “Implementation 
of an RBF Neural Network on Embedded 
Systems: Real-Time Face Tracking and Identity 
Verification”, Proceedings of IEEE 
Transactions On Neural Networks, Vol. 14, No. 
5, pp. 1162- 1175, September 2003. 
[15] B. Y. Chen, G. D. Su, and Y.F. Deng, 
“2-Channel Parallel Face Detection Board 
Using Two TMS320C6201 DSPS”, 
Proceedings of International Conference on 
Machine Learning and Cybernetics, vol. 7, pp. 
4446-4451, 2005. 
[16] Ming-Hsuan Yang, David J. Kriegman and 
Narendra Ahuja, “Detecting Faces in Images: A 
Survey”, IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 24, No. 
1, pp. 34-58, January 2002. 
 2
分鐘的說明來解答聽眾的提問。 
會議中所發表的此篇論文，我們提出了一個有效率的演算法利用
哈爾小波轉換擷取輸入向量的特徵，利用這些特徵在向量量化的過程
可以提早移除不可能的 codewords，從實驗的結果可以證明我們的方
法相較於 DHSS 演算法，Lai’s演算法， HTPDE演算法， WTPDE
演算法都有較佳的效率。 
附表為本次出席國際會議之行程說明 
台北時間 
07/13 
13:50 到達桃園中正國際機場，確認預備登機動
作;15:10搭乘韓亞航 OZ712; 預計 2小時 20分後抵達
韓國仁川機場。 
韓國時間 
07/13 
韓國時間 07月 13日晚上 6點 30分左右抵達韓國仁川
機場，辦理完入境手續後，搭乘高速巴士，約 70分後
抵達首爾市區，再搭乘 KTX高鐵，約 50分後抵達大
田市區，再搭乘捷運系統，約 10分後抵達會議地區，
並步行前往 YOUSUNG HOTEL辦理住宿事宜，居住
為期 4天。 
韓國時間 
07/14 
早上 10點搭乘計程車，前往 Daejeon Convention 
Center，領取會議資料及參加開幕式。 
韓國時間 
07/15 
早上整理並複習相關研究報告，並開始熟讀在國內已
經準備好的演講稿。 
