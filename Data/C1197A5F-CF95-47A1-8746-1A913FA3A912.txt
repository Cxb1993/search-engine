 2
 
中英文摘要 
型樣探掘是圖形識別之一延伸‧主要在一大型資料倉儲中挖掘隱藏的結構‧如同圖形識別，
包含三個主要部分：因素分析，聚類，及歸類‧人類原本認知的過程即是一連串的辨識不
同種類資訊的過程‧而此議題之所以益形重要，主要乃因網際網路之普及，幾致成為不可
或缺的之溝通媒介與傳訊通路‧大量的資訊與純語意的傳輸使得資料辨識的困難倍增‧如
何有效且迅速的將資料內涵精確的勾勒出來並加應用成為當今知識管理上重要的工作‧而
由於類神經網路分析再處理大型資料上有其難以取代優點，因此，在此三年計劃中將在類
神經網路的架構下逐年進行以下研究： 
《第一年》、建立減低維度分析機制‧並建構挖掘完全且獨立因素的可能性分析模式‧ 
《第二年》、根據可能的因素集合建立聚類準則及趨勢模式，以利分析型樣之潛在變動‧ 
《第三年》、根據所分析之型樣及其可能變動預測新資訊之可能歸屬‧並將與現存方法作一
比較與評估‧ 
 
關鍵詞 : 模糊類神經網路、型樣識別、聚類與推理規則、比較與評估 
 
英文摘要 
Pattern Mining is an extension of Pattern Recognition, which consists of factor (feature) 
analysis, clustering, and classification, is a process that deals with searching hidden structures in 
a huge number of data. It becomes an important issue especially when internet becomes a widely 
used communication media. With relevant issues in internet world, linguistic expression in the 
global communication requires accurate interpretation. Fuzzy logic analysis provide a tool for 
this purposes when investigating the data pattern in which the data are linguistic or fuzzily 
expressed. Hence, our interests in this 3-year project correspond to  
(1st ). To design a dimensional reduction process in the framework of neural network architecture. 
Then, the hidden factors will be extracted with their degrees of possibilities. 
(2nd ) Based on these possible factors, a clustering algorithm will be designed in which 
formulation of membership function will be focused and developed by a neural network..  
(3rd) A classifier will be developed based on the proposed inference rules, which will be 
compared and evaluated with respect to other methods.  
 
Keywords : FNN, Pattern Recognition, Clustering and Inference Rules, Comparison and    
Evaluation 
 
 
 4
neural networks has drawn much more attention on research. . Buckley and Hayashi [1] have 
devoted to both learning algorithms and applications of fuzzy neural networks. They suggested to 
apply more general fuzzy sets in order to facilitate wider applications, such as the possibility of 
using more generalized fuzzy numbers for fuzzy signals/weights. However, this would mean to 
deal with the complex computations of fuzzy arithmetic. Thus, the research on fuzzy neural 
network has put forth for investigating learning algorithms with fuzzy arithmetic. The general 
approach to this issue is to perform approximated interval arithmetic on level-cut sets of fuzzy 
numbers [3,9,10,11,12]. Although these algorithms have shown their capabilities in learning, the 
restrictions on specific forms of fuzzy weights in models or the considerable computational 
complexity caused by calibration of a fuzzy weight at level sets have been considered as the 
drawbacks. Then, Wang and Kuo [13] proposed an approximate fuzzy arithmetic of L-R fuzzy 
numbers, considered as the most general form of fuzzy numbers, for fuzzy neural networks and 
consequently a more effective learning algorithm can be developed. 
   To compare this learning algorithm with other commonly used learning algorithms, numerical 
analysis is carried out in this paper so that the evidence on the performance, when the compared 
algorithms all reach to a required convergence level, their accuracy and efficiency can be 
compared and analyzed.  
. The rest of the paper is organized as follows : we first describe the basic notations of fuzzy 
neural networks and discuss three representative learning algorithms. Then, we outline the 
proposed learning algorithm [13] in Section 3. Then, the efficiency and accuracy of these learning 
algorithms are compared by using two numerical examples of nonlinear mapping of fuzzy 
numbers and realization of fuzzy IF-THEN rules in Section 4. Finally, we conclude this study in 
Section 5 and discuss the possible aspects for further research. 
 
2. Fuzzy Neural Networks 
 
For a neural network called a fuzzy neural network is a multi-layer feedforward neural network in 
which the signals and/or the weights must be fuzzy sets. Fuzzy neural networks have been 
recently proposed with many different structures listed in Table 1 [1]. Since a real number can be 
viewed as a special fuzzy number, our study focuses on FNN3 whose architecture is shown in 
Figure 1, a fully fuzzified neural network considered as the most general form of FNN with fuzzy 
inputs, weights and outputs. 
 
Table 1. Types of Fuzzy Neural Networks 
 Inputs Weights Outputs 
Conventional NN Real number Real number Real number 
FNN1 Real number Fuzzy number Fuzzy number 
FNN2 Fuzzy number Real number Fuzzy number 
FNN3 Fuzzy number Fuzzy number Fuzzy number 
 6
back-propagation algorithm suggested the fuzzy weights to be symmetric triangular fuzzy 
numbers in order to improve the resultant distorted fuzzy weights in learning (Figure 2). Then, 
Ishibuchi and Nii extended the FNN learning algorithm in [3, 10] to cope with the asymmetric 
triangular and trapezoid fuzzy weights [11, 12]. Moreover, Dunyak and Wunsch [9] notified that 
the condition of distorted fuzzy weights (Figure 1) was caused by the level-cut operations and 
they proposed a learning algorithm that relaxed the level-cut constraints by a transformation and 
thus 
( ) ( ) U
kj
hU
kj
hU
kj
hU
kj
hL
kj
hL
kj
hL
kj
hL
kj
h wwwwwwww HHHH 121121 ≤≤≤≤≤≤≤ −− "" ,     (4) 
where h = 1, 2, …, H describes the number of level cuts used in interval arithmetic on fuzzy 
numbers. The cost function of Dunyak and Wunsch’s learning algorithm was weighted by 
positive weights 1hω  and 2hω  allowing a relative weighting of the importance of different h 
levels instead of the degrees of level-cut, h, in Eq. (3). 
( ) ( ) ( )( )∑ ∑ ∑ −+−=
= =q
H
h
n
k
Uq
k
hUq
k
h
h
hLq
k
hLq
k
h
h
S
ydyd
H 1 1
2)()(
2
2)()(
12
1~ ωωξ w .    (5) 
These algorithms [3, 9, 10, 11, 12], however, performed the fuzzy outputs with the interval sets 
which could not be described by definite membership functions and showed a common difficulty 
in training when the fuzzy weights were described by their level-sets. 
 
Fuzzy
     Weight
Fuzzy
     Weight
h
0
1 1
h
M
em
be
rs
hi
p
M
em
be
rs
hi
p
 
Figure 2. The Distorted Fuzzy Weights in Learning of a Triangular Fuzzy Weight by Level Sets 
 
On the other hand, another calculation of fuzzy arithmetic used in the fuzzy neural network 
performing definite membership functions of the fuzzy outputs was based on Dubios and Pride’s 
approximate formulas [2]. Li et al. [4] proposed a learning algorithm of fuzzy neural network 
based on such kind of fuzzy number operations and presented the learning algorithm by the cost 
function below. 
( ) ( ) { }RMLUyd
q k
Uq
k
Uq
k ,,  where,2
1~ 2)()( ∈∑∑ −=wξ .      (6) 
The superscripts L, M, R denoted the left, mean, and right values of a fuzzy number, respectively. 
Since their proposed fuzzy number operations were based on Dubios and Pride’s approximate 
formulas, they assumed that the fuzzy numbers in the fuzzy neural network were all positive. 
Besides, there was a restriction that the spreads compared to the mode of fuzzy numbers should 
be relatively small. The learning algorithm has shown its capability and was less time-consuming 
 8
⎪⎪⎩
⎪⎪⎨
⎧
+≥⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
+−
+≤⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−+
== +
21
21
21
21
21
21
~)(~~
,)(
,)(
)()(
ccxccxR
ccxxccL
xx BAZ
ββ
ααµµ .      (8) 
 
Besides, assume Z~  is the approximate fuzzy multiplication of A~  and B~ , BAZ ~)(~:~ ⋅= . Then, 
Z~  is approximated by the L-R fuzzy number, Zˆ~ , with membership function of 
(i) when c1 ≥ 0 and c2 ≥ 0, 
 
⎪⎪⎩
⎪⎪⎨
⎧
≥⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
≤⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
=
21
1221
21
21
1221
21
~ˆ
,
,
)(
ccx
cc
ccxR
ccx
cc
xccL
x
Z
ββ
ααµ ;       (9) 
(ii) when c1 < 0 and c2 ≥ 0, 
 
⎪⎪⎩
⎪⎪⎨
⎧
≥⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
≤⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
=
21
1221
21
21
1221
21
~ˆ
,
||
,
||
)(
ccx
cc
ccxR
ccx
cc
xccL
x
Z
βα
αβµ ;       (10) 
(iii) when c1 < 0 and c2 < 0, 
 
⎪⎪⎩
⎪⎪⎨
⎧
≥⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
≤⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
−
=
21
1221
21
21
1221
21
~ˆ
,
||||
,
||||
)(
ccx
cc
ccxR
ccx
cc
xccL
x
Z
αα
ββµ .       (11) 
 
The fuzzy arithmetic approximation had been shown and evaluated in details in [13]. When it was 
adopted to a single layer perceptron, we have shown that the approximation errors will not cause 
any effect in learning when comparing with the level-sets approach by using the same error 
criterion (cost function). In the following, the approximate fuzzy arithmetic of the L-R fuzzy 
numbers will be used in our learning model. 
 
3.2 The learning algorithm 
 
Consider a fully fuzzified neural network showed in Figure 1, the forward operations of node j in 
sth layer of Eq. (1) by the 3-parameters fuzzy arithmetic approximation are 
(i) the mode, 
( )∑ +⋅= −
=
1
1
)(1)(1)(1)(1 s
n
i
q
j
q
ji
q
i
q
j wyv θ ;           (12) 
 10
the defined cost functions in Eq. (16) is 
)(
)(
)(
)(
)()(
)(
~
~
~
~
~
)~(
~
)~(~
q
ji
q
j
q
j
q
j
q
j
q
ji
q
ji w
v
v
y
yw
w ∂
∂⋅∂
∂⋅∂
∂⋅=∂
∂⋅=∆ ww ξηξη .       (19) 
 
Therefore, the back-propagation learning rules are as the follows. 
(i) If jy~  is output of the network, then 
 )(1)(1)(1)(1)(1)(1 )1()( qi
q
j
q
j
q
j
q
j
q
ji yyyydw ⋅−⋅⋅−⋅=∆ η ,        (20) 
 
⎪⎪⎩
⎪⎪⎨
⎧
<<
≥<
<≥
≥≥
⋅−⋅⋅−⋅=∆
0 ,0,0
0 ,0,0
0 ,0,
0 ,0,
)1( )(
)(1)(1
)(1)(1
)(1)(1)(1
)(1)(1)(1
)(
~
)(
~
)(
~
)(
~
)(
~
q
ji
q
i
q
ji
q
i
q
ji
q
i
q
i
q
ji
q
i
q
i
q
y
q
y
q
y
q
d
q
w
wy
wy
wyy
wyy
jjjjji
ααααηα ,  (21) 
 
⎪⎪⎩
⎪⎪⎨
⎧
<<
≥<
<≥
≥≥
⋅−⋅⋅−⋅=∆
0 ,0,0
0 ,0,
0 ,0,0
0 ,0,
)1()(
)(1)(1
)(1)(1)(1
)(1)(1
)(1)(1)(1
)(
~
)(
~
)(
~
)(
~
)(
~
q
ji
q
i
q
ji
q
i
q
i
q
ji
q
i
q
ji
q
i
q
i
q
y
q
y
q
y
q
d
q
w
wy
wyy
wy
wyy
jjjjji
ββββηβ .  (22) 
(ii) If jy~  is output of hidden layer, then 
 )(1
1
)()(
1
)(1)(1)(1
)1(
)~()1( qi
n
k
q
kj
q
k
q
j
q
j
q
ji ywyyw
s ⋅∑ ⋅⋅−⋅⋅=∆ +
=
δη ,      (23) 
 
⎪⎪⎩
⎪⎪⎨
⎧
<<
≥<
<≥
≥≥
⋅∑ ⋅⋅−⋅⋅=∆ +
=
0 ,0,0
0 ,0,0
0 ,0,
0 ,0,
)()1(
)(1)(1
)(1)(1
)(1)(1)(1
)(1)(1)(1
1
)(
~
)(
2
)(
~
)(
~
)(
~
)1(
q
ji
q
i
q
ji
q
i
q
ji
q
i
q
i
q
ji
q
i
q
i
n
k
q
w
q
k
q
y
q
y
q
w
wy
wy
wyy
wyy
s
kjjjji
αδααηα , (24) 
 
⎪⎪⎩
⎪⎪⎨
⎧
<<
≥<
<≥
≥≥
⋅∑ ⋅⋅−⋅⋅=∆ +
=
0 ,0,0
0 ,0,
0 ,0,0
0 ,0,
)()1(
)(1)(1
)(1)(1)(1
)(1)(1
)(1)(1)(1
1
)(
~
)(
3
)(
~
)(
~
)(
~
)1(
q
ji
q
i
q
ji
q
i
q
i
q
ji
q
i
q
ji
q
i
q
i
n
k
q
w
q
k
q
y
q
y
q
w
wy
wyy
wy
wyy
s
kjjjji
βδββηβ . (25) 
where )(3
)(
2
)(
1  , ,
q
k
q
k
q
k δδδ  are the propagated local gradients of the mode, left spread and right spread 
in neuron k, respectively. 
 
3.3 Evaluation of the proposed model 
 
Since the operations of fuzzy numbers are done by the three parameters of the mode, left spread 
 12
The parameters of a three-layered FNN were: (1) number of hidden neurons = 6; (2) learning 
constant η= 0.5; (3) momentum constant γ = 0.9; and (4) stopping condition was 10000 
iterations of the learning algorithm. After training the FNN by the above three data pairs, ( )11 ~ ,~ dx , 
( )33 ~ ,~ dx  and ( )55 ~ ,~ dx , two new fuzzy inputs, 2~x  = [0.35, 0.1, 0.1] and 4~x  = [0.65, 0.1, 0.1], 
were used to test the model in order to map the nonlinear fuzzy input-output relations. Table 2 
and Figure 4 show the learning results of fuzzy outputs with the three fuzzy patterns used to train 
in the learning process and two new fuzzy patterns in the testing process. The small errors have 
shown the learning capability of the proposed algorithm; and the performance of mapping new 
fuzzy inputs to fuzzy outputs has shown the applied ability of the proposed model. 
 
0.0 1.0
1.0
Input Value
Ou
tpu
t V
alu
e
M
em
be
rs
hi
p 
D
eg
re
e )~,~( 11 dx
)~,~( 33 dx
)~,~( 55 dx
 
Figure 3. Fuzzy Training Input-output Pairs in Example 1 
 
Table 2. The Learning Results and The New Fuzzy Patterns in Example 1 
 Fuzzy outputs y~  Errors 
 
Fuzzy inputs Desired outputs Mode 
value 
Left 
spread
Right 
spread ii xd
11 −  ii xd ~~ αα −  ixid ~~ ββ −
[ ]0.1 ,1.0 ,2.0~1 =x [ ]0.15 0.15, 0.35,~1 =d 0.350 0.152 0.150 0.000 0.002 0.000 [ ]0.1 ,1.0 ,5.0~3 =x [ ]0.1 0.1, 0.7,~3 =d  0.700 0.097 0.100 0.000 -0.003 0.000 Training fuzzy 
patterns [ ]0.1 ,1.0 ,8.0~5 =x [ ]0.05 0.05, 0.45,~5 =d 0.450 0.056 0.051 0.000 0.006 0.001 
[ ]0.1 ,1.0 ,35.0~2 =x - 0.354 0.145 0.144 - - - New fuzzy 
patterns [ ]0.1 ,1.0 ,65.0~4 =x - 0.599 0.060 0.079 - - - 
 
0.0 1.0
1.0
Input Value
Ou
tpu
t V
alu
e
M
em
be
rs
hi
p 
D
eg
re
e )~,~( 11 yx )~,~( 55 yx
)~,~( 33 yx)~,~( 22 yx )~,~( 44 yx
New fuzzy patterns
Training fuzzy patterns
 
 14
0 0.5 1
small largemedium medium
large
medium
small1
Membership
Degree
 
Figure 5. The Membership Functions of Linguistic Values in Example 2 
 
Figure 6. The Membership Functions of the Learning Fuzzy Output of Example 2 
 
Table 4. The Complete Fuzzy IF-THEN Rule Table Generated by the FNN of Example 2 
  x2 
  S MS M ML L 
S L L ML M M 
MS L M S S S 
M ML MS S S S 
ML M MS S S S 
x1   
L M MS S S S 
 16
Figure 7. 95% Confidence Interval Plots of Single-factor ANOVA for Example 1 
 
One-way ANOVA (Example 1 – Learning errors): 3-parameter, Ishibuchi, Li et al. 
 
Analysis of Variance 
Source     DF        SS        MS        F        P 
Factor      2  0.276308  0.138154   294.06    0.000 
Error      87  0.040874  0.000470 
Total      89  0.317182 
                                      Individual 95% CIs For Mean 
                                      Based on Pooled StDev 
Level       N      Mean     StDev  ---------+---------+---------+------- 
3-parame   30   0.01561   0.00792   (-*-)  
Ishibuch   30   0.03823   0.01412         (-*-)  
Li et al   30   0.14281   0.03387                                   (-*-)  
                                      ---------+---------+---------+------- 
Pooled StDev =  0.02168                     0.040     0.080     0.120 
 
One-way ANOVA (Example 1 – No. iterations in convergence): 3-parameter, 
Ishibuchi, Li et al. 
 
Analysis of Variance 
Source     DF        SS        MS        F        P 
Factor      2 995688913 497844457     5.29    0.007 
Error      87 8.182E+09  94051525 
Total      89 9.178E+09 
                                      Individual 95% CIs For Mean 
                                       Based on Pooled StDev 
Level       N      Mean     StDev  -----+---------+---------+---------+- 
3-parame   30     17411     15296                      (--------*-------)  
Ishibuch   30     16125      6170                   (-------*--------)  
Li et al   30      9801      3179   (--------*-------)  
                                      -----+---------+---------+---------+- 
Pooled StDev =     9698                 8000    12000     16000     20000 
 18
Paired samples T-test 
Null hypothesis (H0) e1 ≥ e2 e1 ≥ e3 e2 ≥ e3 
P-value 0.49 (*) 9.7×10-20 1.3×10-21 
 
Table 8. Comparison of Learning Algorithms in Number of Iterations of Example 2 
Example 2: Number of iterations (q) 
 Our algorithm (q1) Ishibuchi et al. (q2) Li et al. (q3) 
Mean 7.9×103 2.4×104 2.0×104 
Variance 7.5×107 1.9×108 1.0×108 
Paired samples T-test 
Null hypothesis (H0) q1 ≥ q2 q1 ≥ q3 q2 ≤ q3 
P-value 3.2×10-5 3.6×10-5 0.10 (*) 
 
5. Conclusion 
 
In this study, we compared a learning algorithm based on a developed approximate fuzzy 
arithmetic, called 3-parameters fuzzy arithmetic approximation, for fully fuzzfied neural 
networks with others algorithms on both efficiency and accuracy. The fully fuzzified neural 
network with fuzzy signals described by L-R fuzzy numbers were considered because it is seen as 
the most general model of fuzzy neural networks. Based on the fuzzy arithmetic approximation, 
the calculated fuzzy outputs could be directly described by three parameters of the mode, the left 
and the right spreads of the fuzzy numbers so that the membership functions could be presented 
with definite L-R functions. This reduced the computation complexity from formal methods 
learning by the level-sets computations. Also, using a back-propagation algorithm by minimizing 
the defined cost functions to train a fuzzy neural network not only has facilitated the automatic 
learning of the fuzzy neural network model but also ensured the accuracy of the learning results. 
In computer simulations, two numerical examples had been used to illustrate the proposed 
learning algorithm and the results had been also used to compare the accuracy and efficiency of 
different learning algorithms of fuzzy neural networks. The simulation results have shown that 
our proposed learning algorithm is comparatively efficient and accurate. 
 
Further studies would focus on the development of 4-parameters fuzzy arithmetic approximation 
for which fuzzy numbers with the interval modes are considered, so that the trapezoidal type of 
fuzzy numbers can be applied. Besides, although the proposed model has shown its adaptive 
ability in learning, the approximation of the activation function in Eq. (2) might lead to a large 
error. Hence, how to obtain a better approximation of the activation function will be also studied 
in the near future. 
 
Acknowledgement 
The authors acknowledge the financial support from the National Science Council with Project 
Number NSC95-2221-E007-100. 
 
參考文獻 
