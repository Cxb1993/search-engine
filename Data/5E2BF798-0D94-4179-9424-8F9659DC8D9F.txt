 I
口述對話系統中語意分析與不流暢語音模型化之研究（2/2） 
 
中文摘要 
 
語音辨認技術在過去多著重於辨認模式之研究，然而要真正將語音辨認技術實用化便不
能不面對口語對話中所產生之不流暢問題，並將其語意解析以達初步語意理解之目的。 
本計畫於第一年度之目標為提出一以語意相依圖形之對話歷程解析模式。本計畫將對話
歷程視為語音動作序列後，描述對話歷程遂轉化為語音動作序列之辨識問題。統計模式被有
效地應用於語意相依圖形的建構以描述對話者語句中詞與詞之間的語意關係. 在中心詞語
與其他修飾語之語意相依關係可以利用語意相依文法加以偵測。為了有效驗證所提方法是否
可行，我們建立了一個以醫院掛號為主要應用之口語對話系統，並蒐集相關語料。由實驗結
果可以得知就語音動作偵測之正確率可達 95.6%，工作完成率為 85.24%。就平均對話長度
而言，使用本計畫所提的方法所得之平均對話長度為 8.3 回合(turns) 。相較於以貝式分類器
或部分樣本樹之方法，本計劃所提之方法就語音動作之辨識率分別提升 14.9 與 12.5。顯示
本計劃在第一年所提之對話歷程模式確實可以有效地辨認語音動作。 
在第二年的研究中，本計劃提出一新穎之不流暢語流偵測及修正方法。首先利用聲學特
徵之假設檢定來預判可能之潛在中斷點，再利用修正現象中關於詞序之規律性並以上位辭與
詞類來對詞歸類以解決統計資料不足之問題，以對數線性組合來混合修正後之語言模型與修
正中對應模型藉以偵測並修正不流暢語流之現象。實驗結果顯示本計劃所提之方法在中斷點
與修正詞之錯誤率分別為 0.33 與 0.21，相較於 DF-Gram 為佳，顯示本計劃所提之方法在分
析對話中關於不流暢語流模型之分析是確實有效的。 
本計劃於兩年研究期間，將語意分析與不流暢語流模型整合至一對話系統中。結果顯示
經不流暢語流修正之語意分析可以有效解決不流暢語流之語意誤解問題，配合語意相依圖形
對於口述對話系統中關於語意擷取與分析可以達到更佳的效果。 
 
關鍵詞: 語意分析，語意相依圖，口述對話系統，語音轉錄，不流暢語流，語言模型，對正
模型，潛在中斷點偵測  
 III
目錄 
中文摘要                 I 
英文摘要                    II 
報告內容                  1 
1 Introduction                 
 1 
2 Semantic Dependency Graph             4 
 2.1 Speech act identification using semantic dependency with discourse analysis  
 5 
 2.2 Semantic dependency analysis using word sequence and discourse    
 6 
3 Edit Disfluency Detection and Correction Model          7 
 3.1 Potential Interruption Point Detection           8 
  A. Probability of Interruption Point using Posterior Probability of Silence Duration   
  B. Syllable-based Acoustic Feature Extraction             
  C. Gaussian Mixture Model for Interruption Point Detection     
    D. Potential Interruption Point Extraction         
 3.2 Lingusitic Processing for Edit Disfluency Correction        
     A. Language Model of Cleanup Utterance             
  B. Alignment model between Deletable Region and Correction      
4 Experiments                    15 
 4.1 Analysis of corpus for semantic dependency graph            
  A. Precision of speech act identification related to the corpus size    
      B. Performance analysis of semantic dependency graph     
       4.2 Data Preparation and Speech Recognition used in the disfluency analysis     
    A. Potential Interruption Point Detection       
       B. Clean-up Disfluency using Linguistic Information     
       C. Interruption Pont Detection          
5 Conclusion                             26 
Reference                 27   
計畫成果自評                   32 
可供推廣之研究成果資料表                   33 
出席國際學術會議心得報告                                                    34 
發表之論文                                                                  35 
 2
between concepts in the utterance are also taken into consideration. Instead of semantic frame/slot, 
semantic dependency graph can keep more information for dialogue understanding. 
 
As information exchangeability and communication becomes increasingly human-oriented, 
human-computer interfaces that provide inter-connective services are increasingly important. 
Significant effort has been made in the last few decades to develop automatic speech recognition 
technology to enrich the output in the most informative manner using spontaneous speech. The 
transcription, that converts speech into written or typewritten form, symbolically represents an 
acoustic signal for indexing, retrieval and analysis [17]. Rich transcription has emerged as an 
inter-disciplinary field combining automatic speech recognition, speaker diarization and natural 
language processing. Moreover, the concept of rich transcription comprises generating an 
orthographic transcription of a spontaneous speech utterance enriched with side information 
concerning the speaker, the presence of background sound, the topics and any information related 
to the structure of the speech utterance [18-19]. Shriberg defined and analyzed the disfluencies of 
spontaneous speech in 1994. The annotated information includes observable characteristics of the 
speech domain, speaker, sentence in which a disfluency occurs and word-related and simple 
acoustic properties of the disfluency [20]. Recent research has attempted to enhance acoustic and 
language modeling for speech recognition and novel algorithms for extracting structural metadata 
sentence boundaries and disfluencies. Combined with the linguistic applications, rich transcription 
can be applied to dictation systems, speech-to-speech translation, spoken language understanding 
and transcription system for meeting.  
 
Two essential conceptual issues in rich transcription are Speech-to-Text (STT) and Metadata 
Extraction (MDE). Well-known research projects such as EARS (Effective, Affordable, Reusable 
Speech-to-text Program), which is supported by DARPA, have developed many tools and corpora 
for rich transcription especially in speech recognition related work. A famous workshop, RT (The 
Rich Transcription evaluation series), which opened a rich transcription track by NIST since 2002, 
provides a rich corpus and evaluation material. Speech recognition systems have been an 
important issue for rich transcription research in past RT workshops. The DiSS (Disfluency in 
Spontaneous Speech) workshop considers the challenge of research of spontaneous speech, 
particularly in disfluency. While these workshops have undoubtedly provided important research 
on rich transcription, there must be considerable doubt as to deal with the problems resulted from 
the disfluency without exception. One of the essential issues is how to model the edit disfluency. 
Conversational utterances have several problems, including interruption, correction, filled pauses 
and grammatically incorrect sentences. The definitions of disfluencies have been discussed in 
SimpleMDE [21], where simple edit disfluencies are divided into three categories, namely 
repetitions, revisions or repairs and restarts. Each category is illustrated as follows: 
 
(1)Repetition: the abandoned words repeated in the corrected portion of the utterance, as 
depicted in Fig. 1(b). 
(2)Revision or repair: although similar to repetitions, the corrected portion that replaces the 
abandoned constituent modifies its meaning, rather than repeating it. Figure 1(c) shows an 
example. 
(3)Restarts or false starts: a speaker abandons an utterance, and neither corrects it nor repeats it 
partially or wholly, but instead restructures the utterance and starts over, as illustrated in Fig. 1(d). 
 4
telephone speech using LDA [40]. Harper et al. utilized parsing approaches to rich transcription 
[41]. Liu et al. not only detected the boundaries of sentence-like units using the conditional 
random fields [42], but also compared the performances of HMM, maximum entropy [43] and 
conditional random fields on disfluency detection [44].  
 
This study focuses on the detection and correction of the edit disfluency based on the word 
order information. The first process attempts to detect the IPs based on hypothesis testing. 
Acoustic features, including duration, pitch and energy features, are adopted in hypothesis testing. 
To circumvent the problems resulting from disfluency, particularly in edit disfluency, a reliable 
and robust language model must be adopted to correct speech recognition errors. To handle 
language-related phenomena in edit disfluency, a cleanup language model characterizing the 
structure of the cleanup sentences and an alignment model for aligning words between deletable 
region and correction part are proposed for detecting and correcting edit disfluency. 
2 Semantic Dependency Graph 
Since speech act theory is developed to extract the functional meaning of an utterance in the 
dialogue [45], discourse or history can be defined as a sequence of speech acts,  
1 2 1{ , ,.... , }t t tH SA SA SA SA−= , and accordingly the speech act theory can be adopted for discourse 
modeling. Based on this definition, the discourse analysis in semantics using the dependency 
graphs tries to identify the speech act sequence of the discourse. Therefore, discourse modeling by 
means of speech act identification considering the history is shown in Equation (1). By 
introducing the hidden variable Di, representing the i-th possible dependency graph derived from 
the word sequence W. The dependency relation, rk , between word wk and headword wkh is 
extracted using HowNet and denoted as ( , )k kh kDR w w r≡ . The dependency graph which is 
composed of a set of dependency relations in the word sequence W is defined as 
1 1 1 2 2 2 1 1 ( 1)( ) { ( , ), ( , ),..., ( , )}
i i i
i h h m m m hD W DR w w DR w w DR w w− − −= . 
 The probability of hypothesis SAt given word sequence W and history Ht-1 can be described in 
Equation (1). According to the Bayes’ rule, the speech act identification model can be 
decomposed into two components, ( )1| , ,t tiP SA D W H − and ( )1| , tiP D W H − , described in the 
following.  
( )
( )
( ) ( )
* 1
1
1 1
arg ax | ,
arg ax , | ,
arg ax | , , | ,
t
t
i
t
i
t t
SA
t t
i
SA D
t t t
i i
SA D
SA m P SA W H
m P SA D W H
m P SA D W H P D W H
−
−
− −
=
=
= ×
∑
∑
 
(1)
where SA* and SAt are the most probable speech act and the potential speech act at the t-th 
dialogue turn, respectively. W={w1,w2,w3,…,wm} denotes the word sequence extracted from the 
user’s utteance without considering the stop words. Ht-1 is the history representing the previous t-1 
turns.  
 
2.1 Speech act identification using semantic dependency with discourse analysis 
In this analysis, we apply the semantic dependency, word sequence, and discourse analysis to the 
identification of speech act. Since Di is the i-th possible dependency graph derived from word 
 6
1
1 ( , )( | )
( )
t t
t t
t
C SA SAP SA SA
C SA
−
− =  (8)
where ( )C ⋅  represents the number of events in the training corpus. According to the definitions 
in Equations (7) and (8), Equation (6) becomes practicable. 
2.2 Semantic dependency analysis using word sequence and discourse 
Although the discourse can be expressed as the speech act sequence 1 2 1{ , ,.... , }t t tH SA SA SA SA−= , 
the dependency graph iD  is determined mainly by W, but not 
1tH − . The probability that defines 
semantic dependency analysis using the words sequence and discourse can be rewritten in the 
following: ( )1 1 2 1| , ( | , , ,..., ) ( | )t t ti i iP D W H P D W SA SA SA P D W− − −= ≅  (9)
And 
( , )( | )
( )
i
i
P D WP D W
P W
=  (10)
Seeing that several dependency graphs can be generated from the word sequence W, by 
introducing the hidden factor Di, the probability ( )P W  can be the sum of the 
probabilities ( , )iP D W as Equation (11). 
 : ( )
( ) ( , )
i i
i
D yield D W
P W P D W
=
= ∑  (11)
Because Di is generated from W, Di is the sufficient to represent W in semantics. We can estimate 
the joint probability ( , )iP D W  only from the dependency relations Di. Further, the dependency 
relations are assumed to be independent with each other and therefore simplified as  
1
1
( , ) ( ( , ))
m
i
i k k kh
k
P D W P DR w w
−
=
=∏  (12)
The probability of the dependency relation between words is defined as that between the concepts 
defined as the hypernyms of the words, and then the dependency rules are introduced. The 
probability ( | ( ), ( ))k k khP r f w f w  is estimated from Equation (13). 
( ( , )) ( ( ( ), ( )))
( | ( ), ( ))
( , ( ), ( ))
( ( ), ( ))
i i
k k kh k k kh
k k kh
k k kh
k kh
P DR w w P DR f w f w
P r f w f w
C r f w f w
C f w f w
≡
=
=
 
(13)
According to Equations (11), (12) and (13), Equation (10) is rewritten as the following equation. 
11
11
1 1
 : ( )  : ( )1 1
( , ( ), ( ))
( ( , ))
( ( ), ( ))( | )
( , ( ), ( ))( ( , ))
( ( ), ( ))
i i i i
mm
i k k kh
k k kh
kk k kh
i m m
i k k kh
k k kh
D yield D W D yield D Wk k k kh
C r f w f w
P DR w w
C f w f wP D W
C r f w f wP DR w w
C f w f w
−−
==
− −
= == =
= =
∏∏
∑ ∑∏ ∏
 (14)
where function, ( )f ⋅ , denotes the transformation from the words to the corresponding semantic 
classes. 
 8
The edit disfluency correction module has two processing stages, cleanup and 
alignment. The cleanup process divides a word string into three components, the deletable 
region (delreg), the editing term and correction, according to the locations of potential IPs 
detected by the IP detection module. The cleanup process is performed by shifting the 
correction part and replacing the deletable region to form a new cleanup transcription. The 
edit disfluency correction module comprises an n-gram language model and the alignment 
model. The n-gram model considers the cleanup transcriptions as fluent utterances, and 
models their word order information. The alignment model finds the optimal 
correspondence between the deletable region and the correction in edit disfluency.  
 
3.1 Potential Interruption Point Detection  
Since precise IP detection using acoustic features is still an open issue, this study 
considers whether linguistic features could be integrated to provide a more reliable 
solution. Rather than detecting exact IPs, additional candidates called potential IPs are 
selected for further linguistic processing. The location information of a potential IP can 
help the language model verify an IP reliably and therefore correct the edit disfluency. 
Since the IP is the point at which the speaker breaks off the deletable region and the 
correction, it has some characteristic acoustic events. In a syllabic language like Chinese, 
every character is pronounced as a monosyllable, while a word comprises one or more 
syllables. The speech input of a syllabic language with n syllables can be described as a 
sequence, syllable_silence 1 1 2 2 n-1 nSeq  {syllable , silence , syllable , silence ,...,silence , syllable }≡ , 
This sequence can then be separated into a syllable sequence   
syllable 1 2 nSeq  {syllable ,  syllable , ..., syllable }≡ , and a silence sequence 
silence 1 2 n-1Seq  {silence ,  silence ,...,silence }≡ . The interruption detection problem is 
modelled as a choice between H0 , which is the IP that is not embedded in the silence 
hypothesis, and H1, which is the IP that is embedded in the silence hypothesis. The 
likelihood ratio test is employed to detect the potential IPs. The function 
( )syllable_silenceSeqL  is called the likelihood ratio, since it indicates the likelihood of 1H  
versus the likelihood of 0H  for each value of syllable_silenceSeq . 
( ) ( )( )
_ 1
_
_ 0
;
;
syllable silence
syllable silence
syllable silence
P Seq H
L Seq
P Seq H
=  (15)
By introducing the threshold γ  to adjust the precision and recall rates,  
( )1 syllable_silence: SeqH L γ≥  means the IP is embedded in silencek, which is conceptually a 
potential IP. Under the assumption of independence, the probability of IP appearing in 
silencek can be considered as the product of probabilities obtained from silencek and the 
syllables around it. The probability density functions (PDFs) under each hypothesis are 
denoted and estimated as  
( )
( )
( ) ( )
1_
_
;
|
| |
syllable silence
syllable silence ip
silence syllableip ip
P Seq H
P Seq E
P Seq E P Seq E
=
= ×
 (16)
And 
 10
 
Figure 3. Flow of acoustic feature extraction 
Since the durations of syllables are not the same even for the same syllable, the 
duration ratio is defined as the average duration of the syllable normalized by the average 
duration over all syllables. 
( )
( )
.
1
.
1 1
i
i i
n
i j
j
duration syllable n
i j
i j
duration syllable
nf
duration syllable
=
= =
≡
∑
∑ ∑
 (20)
where syllablei,j denotes sample j of syllable i in the corpus; |syllable| is the total number 
of the syllables in the corpus; ni represents the number of the syllable i in the corpus. 
Similarly, frame-based statistics are adopted used to calculate the normalized energy and 
pitch for each syllable. The estimation is defined as follows. 
( )( )
( )( )
,
,
, ,
1 1
, ,
1 1 1
log
log
= =
= = =
≡
∑ ∑
∑ ∑ ∑
i ji
i i ji
framen
i j k
j k
pitch framesyllable n
i j k
i j k
ptich frame
nf
ptich frame
 (21)
( )
( )
.
1
.
1 1
i
i i
n
i j
j
energy syllable n
i j
i j
energy syllable
nf
energy syllable
=
= =
≡
∑
∑ ∑
 (22)
 12
( ) ( ) ( ) ( )
1
1/ 2dim/ 2
1 1; , exp
22
T
t i i t i i t i
i
O O Oμ μ μπ
−⎛ ⎞Ν ∑ = − − ∑ −⎜ ⎟⎝ ⎠∑  (31)
where iμ  and i∑  are the mean vector and covariance matrix of component i, and tO  
denotes observation t in the training corpus. The parameters [ ], , ,  1..i i iw i Mθ μ= ∑ = can 
be estimated iteratively using the EM algorithm [49] for mixture i  
( )
1
1ˆ | ,
N
i t
t
w P i O
N
λ
=
= ∑  (32)
( )
( )
1
1
| ,
ˆ
| ,
N
t t
t
i N
t
t
P i O O
P i O
λ
μ
λ
=
=
=
∑
∑
 (33)
( )( )( )
( )
1
1
ˆ ˆ| ,
ˆ
| ,
N
T
t t i t i
t
i N
t
t
P i O O O
P i O
λ μ μ
λ
=
=
− −
∑ =
∑
∑
 (34)
where ( ) ( )
( )
1
|
| ,
|
t i
t W
t j
j
P O w
P i O
P O w
λλ
λ
=
=
∑
 and N denote the total numbers of feature 
observations. 
 
D. Potential Interruption Point Extraction 
Based on the assumption the detected IPs that do not appear in the word boundary 
can be removed by assuming that an IP usually occurs at the boundary of two successive 
words. After eliminating unlikely IPs, the remaining IPs are kept for linguistic processing. 
Since the word graph or word lattice is obtained from the speech recognition module, 
every path in the word graph or word lattice forms its potential IP set for an input 
utterance.  
 14
 
Figure 4 (c) Linguistic processing for edit disfluency correction 
 
A. Language Model of Cleanup Utterance  
Statistical language models have previously been applied to speech recognition, and 
have significantly improved the recognition results. Stolcke and Shriberg first proposed 
speech disfluency detection using a statistical language model [51]. Speech disfluencies 
are represented by the probabilistic events occurring within the word stream. However, 
estimating the probability of word sequences can be expensive, and always suffers from 
the problem of data sparseness. In practice, the statistical language model is frequently 
approximated by the class-based N-gram model with modified Kneser-Ney discounting 
probabilities [52] for further smoothing. 
( )
( )( ) ( )( ) ( )( )
1 2 1 2 2 1
1 1
1 1 1 1 1
1 2
, ,... , ,... , ,....
| | |
t n n t n t N
t N
i t t j
i n j n
i j n
P w w w w w w w
P w Class w P w Class w P w Class w w
+ − − +
− −
+ +
= = +
=∏ ∏  (36)
Here, ( )Class ⋅  is the conversion function that translates a word sequence into a word 
class sequence. This study employs two word classes, semantic and part-of-speech (POS) 
class.  A semantic class, such as the synset in WordNet (http://wordnet.princeton.edu/), 
contains words that share a semantic property based on semantic relations, for example, 
hyponym and hypernym. A POS is a syntactic or grammatical category, and is defined as 
the role played by a word in a sentence such as noun, verb or adjective.  
 The other essential issue of n-gram models for correcting edit disfluency is the 
number of orders in the Markov model. Since an IP is the point at which the speaker 
breaks off the deletable region and the correction that consists of the portion of the 
utterance repaired by the speaker, the cleanup utterance can be considered fluent by 
removing the words in the deletable region. However, removing part of the word string 
shortens the string, and increases the probability of a shorter word string. Consequently, 
short word strings are favored. To deal with this problem, we can increase the order to 
constrain the perplexity and normalize the word length by aligning the deletable region 
and the correction. 
 
 
 
 
 
 16
 
  
Figure 5 An example of dialog  
 
12 Speech acts are defined and shown in Figure 6. Every service corresponds to the 12 
speech acts with different probabilities.  
 
Figure 6. Speech acts corresponding to multiple services in the medical domain 
 
The acoustic speech recognition engine embedded in dialog system based on Hidden 
Markov Models (HMMs) was constructed. The feature vector is parameterized on 26 
MFCC coefficients. The decoding strategy is based on a classical Viterbi algorithm. The 
evaluation results by the character error rate (CER) for a Chinese speech recognition 
system is 18.3 percent and the vocabulary size of the language is 25,132.  
 
4.1 Analysis of corpus for semantic dependency graph 
The training corpus was collected using the on-line recording from National Cheng Kung 
University Hospital in the first phase and the Wizard-of-Oz method in the second phase. 
Totally, there are 1,862 dialogues with 13,986 sentences in the corpus. The frequencies of 
the speech acts used in the system are shown in Figure 7.  
System:午安。需要服務嗎? (Good Afternoon. May I help you?) 
User:是的，我要看醫生。 (Yes, I should be diagnosed by doctor.) 
System:請問您要掛哪一科? (Which department would you like to make an appointment for? ) 
User:我不知道。(I do not know.) 
System:請問你有那些症狀？ (What symptoms are with you? ) 
User:我喉嚨痛又發燒。而且全身酸痛。(I am with a sore throat and a fever. I ached all over, too. )  
System:好的，這些症狀多久了？(Well, how long have you had these symptoms? ) 
User:好幾天了。 (For several days. ) 
System:建議您掛內科醫師。 (My suggestion is that you may visit the Internal Medicine.) 
User:請問內科何時有門診？(What’s the visiting schedule of the Internal Medicine? )  
System:您可以參考醫院門診表，決定您的時間，我們的時間有…。  
You can check the Services Schedule and decide a convenient time for you. The Available time for 
now is.... 
 18
50
62.5
75
87.5
100
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Size of corpus
        (the number of sentence, in thousands)
S
pe
ec
h 
ac
t i
de
nt
ifi
ca
tio
n 
ra
te
 (%
)
semantic dependency graph with
discourse analysis
semantic dependency graph
without discourse analysis
 
Figure 9. The relation between the speech act identification rate and the size of training 
corpus  
 
B. Performance analysis of semantic dependency graph 
To evaluate the performance, two systems were developed for comparison. One is based 
on the Bayes’ classifier [55], and the other is the use of the partial pattern tree [6] to 
identify the speech act of the user’s utterances. Since the dialogue discourse is defined as 
a sequence of speech acts. The prediction of speech  
act of the new input utterance becomes the core issue for discourse modeling. The 
accuracy for speech act identification is shown in Table 1.  
According to the observation of the results, semantic dependency graphs obtain obvious 
improvement compared to other approaches. The reason is that not only the meanings of the words 
or concepts but also the structural information and the implicit semantic relation defined in the 
knowledge base are needed to identify the speech act. Besides, taking the discourse into 
consideration will improve the prediction about the speech act of the new or next utterance. This 
means the discourse model can improve the accuracy of the speech act identification, that is to say, 
discourse modeling can help understand the user’s desired intension especially when the answer is 
very short.  
 
 
 
 
 20
 
We found that the dialogue completion rate and the average length of the dialogs using the 
dependency graph are better than those using the Bayes’ classifier and partial pattern tree 
approach. Two main reasons are concluded: First, dependency graph can keep the most 
important information in the user’s utterance, while in semantic slot/frame approach, the 
semantic objects not matching the semantic slot/frame are generally filtered out. This 
approach is able to skip the repetition or similar utterances to fill the same information in 
different semantic slots. Second, the dependency graph-based approach can provide the 
inference to help the interpretation of the user’s intension.  
 
For semantic understanding, correct interpretation of the information from the user’s 
utterances becomes inevitable. Correct speech act identification and correct extraction of 
the semantic objects are both important issues for semantic understanding in the spoken 
dialogue systems. Five main categories about medical application, clinic information, 
Dr.’s information, confirmation for the clinic information, registration time and clinic 
inference, are analyzed in this experiment.  
Table 3 Correction rates for semantic object extraction 
 SDG PPT Bayes’ 
Dr. and Clinic  95.0 89.5 90.3 
Dr.’s information 94.3 71.7 92.4 
Confirmation (Clinic) 98.0 98.0 98.0 
Clinic information 97.3 74.6 78.6 
Time 97.6 97.8 95.5 
SDG:With discourse analysis 
 
According to the results shown in Table 3, the worst condition happened in the query for 
the Dr.’s information using the partial pattern tree. The misdentification of speech act 
results in the un-matched semantic slots/frames. This condition will not happen in 
semantic dependency graph, since the semantic dependency graph always keeps the most 
important semantic objects according to the dependency relations in the semantic 
dependency graph instead of the semantic slots. Rather than filtering out the unmatched 
semantic objects, the semantic dependency graph is constructed to keep the semantic 
relations in the utterance. This means that the system can preserve most of the user’s 
information via the semantic dependency graphs. We can observe the identification rate of 
the speech act is higher for the semantic dependency graph than that for the partial pattern 
tree and Bayes’ classifier as shown in Table 3. 
 
4.2 Data Preparation and Speech Recognition used in the disfluency analysis 
The Mandarin Conversational Dialogue Corpus (MCDC) [56], gathered from 2000 to 
2001 at the Institute of Linguistics of Academia Sinica, Taiwan, comprises 30 digitized 
conversational dialogues numbered from 01 to 30 of a total length of 27 hours. Sixty 
subjects living in Taiwan were randomly chosen. The annotation in [56] gives concise 
explanations and detailed operational definitions of each tag. Corresponding to 
SimpleMDE, direct repetitions, partial repetitions, overt repairs and abandoned utterances 
are considered as the edit disfluency in MCDC. Spontaneous speech is usually divided 
 22
Table 5. Speech recognition performance with fluent utterances 
 Acc. Del. Sub. Ins. 
TCC-300 (Top 1)  89.51 0.15 9.55 0.79 
TCC-300 (Top 3) 89.76 0.15 9.32 0.77 
TCC-300 (Top 5) 90.38 0.15 8.82 0.65 
MCDC   (Top 1) 52.83 7.79 32.35 16.36 
MCDC   (Top 3) 53.27 7.75 32.32 16.32 
MCDC   (Top 5) 53.92 7.75 31.42 16.26 
 
A. Potential Interruption Point Detection 
An observation window over several syllables is adopted because IP detection can be 
treated as a position determination problem. In this observation window, the values of 
pitch and energy of the syllables just before an IP are usually larger than that after the IP. 
This phenomenon indicates that the pitch reset and energy reset co-occur with IP in the 
edit disfluency. This phenomenon generally arises in the syllables of the first word 
immediately after the IP. The pitch reset event is very obvious when the disfluency type is 
repair. The energy plays the same role as the pitch when the edit disfluency appears, but 
its effect is not as obvious as that of the pitch. The filler words or phrase after the IP are 
lengthened to strive for the time for the speaker to construct the correction and attract the 
listener’s attention. This factor can significantly improve the IP detection rate.  
The hypothesis testing, combined with the GMM model with four mixture 
components using the syllable features, determines if the silence contains the IP. The 
parameter γ  should be determined to improve the result. The overall IP detection error 
rate defined in RT’04F [60] is the sum of the missed detection and false alarm rates: 
M IP FA IP
IP
IP
n nError
n
− −+=  (40)
where M IPn −  and FA IPn −  denote the numbers of missed detections and false alarms 
respectively, and IPn  represents the number of reference IPs. The threshold γ  in Eq. 
(15) can be adjusted for M IPn −  and FA IPn − , as shown in Table 6. Since the goal of the IP 
detection module is to detect the potential IPs, false alarms for IP detection is a less 
serious problem than missing detection errors. That is, the aim of this work is to achieve a 
high recall rate without significantly increasing the false alarm rate. Finally, the threshold 
γ  was set to 0.25.  
 24
alignment outperforms other combination, because 3-gram with more strict constraints can 
reduce the false alarm rate for edit word detection. A 4-gram model was also used to 
attempt to gain further improvement over 3-gram, but the excessive computation meant 
that the improvement was less significant than expected. Additionally, the 4-gram 
statistics were sparser than those of the 3-gram model. These findings indicate that the 
best combination in edit disfluency correction module is 3-gram and semantic class.  
Table 7. Results (%) of linguistic module with equal weight 0.2α =  for edit word 
detection on REF and STT conditions. 
 Human generated 
transcription (REF) 
Speech-to-text recognition 
output (STT) 
 −M EWD
EWD
n
n
−FA EWD
EWD
n
n EWDError
−M EWD
EWD
n
n
−FA EWD
EWD
n
n
 
EWDError
1-gram1 0.18 0.14 0.32 0.49 0.48 0.97 
1-gram2 0.14 0.18 0.32 0.46 0.48 0.94 
2-gram1 0.12 0.16 0.28 0.39 0.44 0.83 
2-gram2 0.14 0.17 0.32 0.41 0.43 0.84 
3-gram1 0.13 0.16 0.29 0.37 0.41 0.78 
3-gram2 0.29 0.12 0.41 0.43 0.39 0.82 
DF-gram (3-gram)1 0.18 0.11 0.29 0.46 0.28 0.74 
DF-gram (3-gram)2 0.12 0.14 0.26 0.35 0.34 0.69 
 alignment1 0.15 0.16 0.31 0.37 0.36 0.73 
2-gram+ alignment 0.17 0.12 0.29 0.40 0.32 0.72 
2-gram+ alignment1 0.09 0.15 0.24 0.36 0.32 0.68 
2-gram+ alignment2 0.10 0.21 0.31 0.34 0.54 0.88 
3-gram+ alignment 0.16 0.12 0.28 0.31 0.35 0.66 
3-gram+ alignment1 0.09 0.12 0.21 0.32 0.32 0.64 
3-gram+ alignment2 0.07 0.16 0.23 0.28 0.30 0.58 
1: word class based on part of speech (POS)   2: word class based on the semantic class 
According to the analytical results shown in Table 7, the values of the probabilities of 
the n-gram model are much smaller than those of the alignment model. Since the 
alignment can be taken as the penalty for edit words, the effects of the 3-gram and the 
alignment with semantic class should be balanced by a log linear combination weight α . 
To optimize the performance, α  was empirically estimated by minimizing the edit word 
errors. The result is shown in Figure 5, in which the weight α  was specified as 0.2 to 
optimize performance.  
 26
model shown in Fig. 2. 
0
0.2
0.4
0.6
0.8
1
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
Tr
ue
 P
os
iti
ve
 R
at
e
ME without word boundary
GMM without word boundary
GMM with word boundary
Proposed
 
Figure 7. Receiver Operator Characteristic (ROC) curves for interruption point detection 
by four criteria 
 
    The maximum entropy model [44] was performed to compare the performance of the 
interruption point detection model. The acoustic features are also adopted to train the 
model. Acoustic GMM models with and without word boundary constraints have been 
described. Instead of the first best hypothesis to detect the interruption point, the word 
lattice generated by the speech recognition engine is considered to postpone the disfluency 
detection decision to the language model, rather than annotating the first best hypothesis 
in the proposed model. 
 The results in Fig. 7 indicate a significant rise in the true positive rate of the 
maximum entropy model when the false positive rate is less than 0.1. In this case, the 
curve of the maximum entropy model intersects that of the GMM model without word 
boundaries when the values of true and false positive rates are 0.6 and 0.073, respectively. 
The maximum entropy model clearly outperforms the GMM model when the false 
positive rate is small. The observations indicate that the precision rate of maximum 
entropy is better than that of GMM model, but the recall rate is not. However, the recall 
rate of the maximum entropy model can clearly be refined by adding additional features. 
As revealed in Section V.B, the performance can be improved by introducing the word 
boundaries. The proposed approach achieves the best performance by integrating the 
language and alignment models.  
5 Conclusion  
This paper has presented a semantic dependency graph that robustly and effectively 
deals with a variety of conversational discourse information in the spoken dialogue 
systems. By modeling the dialogue discourse as the speech act sequence, the predictive 
method for speech act identification is proposed based on discourse analysis instead of 
 28
[7] M. F. McTEAR. 2002. Spoken Dialogue Technology: Enabling the Conversational 
User Interface. ACM Computer Surveys, Vol 34, No. 1,  90-169.. 
[8] B. Rajesh, and B. Linda. 2004. Taxonomy of speech-enabled applications 
(http://www106.ibm.com/developerworks/wireless/library/wi-tax/) 
[9] Y.-Y. Wang and A. Acero. 2003. Combination of CFG and N-gram Modeling in 
Semantic Grammar Learning, In Proceedings of the Eurospeech Conference. Geneva, 
Switzerland. September 2003.  
[10] T. Kudo and Y. Matsumoto. 2000. Japanese Dependency Structure Analysis Based 
on Support Vector Machines. In Proceedings of the EMLNP.  18–25 
[11] K. Hacioglu and W. Ward. 2003. Target word detection and semantic role chunking 
using support vector machines. In HLT-03. 
[12] J. Gao, and H. Suzuki. 2003. Unsupervised learning of dependency structure for 
language modeling. In Proceedings of ACL 2003, 521-528.   
[13] D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. 
Computational Linguistics, 28(3). 245–288. 
[14] K. Hacioglu, S. Pradhan, W. Ward, J. Martin, and D. Jurafsky. 2003. Shallow 
semantic parsing using support vector machines. Technical Report TR-CSLR-2003-1, 
Center for Spoken Language Research, Boulder, Colorado. 
[15] C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING/ACL. 86-90 
[16] Z. Dong and Q. Dong. 2006. HowNet and the computation of meaning. World 
Scientific Publishing Co Inc. 
[17] W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic, D. Oard, M. Picheny, J. 
Psutka, B. Ramabhadran, D. Soergel, T. Ward, and Z. Wei-Jin, "Automatic Recognition 
of Spontaneous Speech for Access to Multilingual Oral History Archives," IEEE Trans. 
on Speech and Audio Processing, Vol. 12, No. 4, pp. 420-435, 2004. 
[18] D. Jones, F. Wolf, E. Gibson, E. Williams, E. Fedorenko, D. Reynods, and M. 
Zissman, “Measuring the Readability of Automatic Speech-to-Text Transcripts.” In 
Proceedings of Eurospeech 2003. pp. 1585-1588. 
[19] Y. Liu, “Structural Event Detection for Rich Transcription of Speech,” Ph.D. thesis, 
Purdue University, 2004. 
[20] E. E. Shriberg, “Preliminaries to a Theory of Speech Disfluencies,” Ph.D. thesis, 
Department of Psychology, University of California, Berkeley, CA, 1994. 
[21] S. Strassel, “Simple Metadata Annotation Specification Version 6.2,” 
(http://www.ldc.upenn.edu/Projects/MDE) Linguistic Data Consortium, Feb. 3, 2004.  
[22] C.-H. Wu and G.-L. Yan, “Speech Act Modeling and Verification of Spontaneous 
Speech With Disfluency in a Spoken Dialogue System,“ IEEE Transaction on  Speech 
and Acoustic Processing, Vol. 13, No. 3, 2005. pp 330-344 
[23] E. Shriberg, A. Stolcke, D. Hakkani-Tur, & G. Tur, Prosody-based automatic 
segmentation of speech into sentences and topics”, Speech Communication, 32(1-2), 
2000, pp. 127-154. 
 30
[40] H. Soltau, , H. Yu, F. Metze, C. Fugen, J. Qin,  S.-C. Jou, “The 2003 ISL rich 
transcription system for conversational telephony speech,” In Proceedings of Acoustics, 
Speech, and Signal Processing 2004 (ICASSP), 2004, pp. 17-21 
[41] M. Harper, B. J. Dorr, J. Hale, B. Roark, I. Shafran, M. Lease, Y. Liu, M. Snover, L. 
Yung, A. Krasnyanskaya, R. Stewart, "Final Report on Parsing and Spoken Structural 
Event Detection", Johns Hopkins Summer Workshop, 2005. 
[42] Yang Liu, Andreas Stolcke, Elizabeth Shriberg, Mary Harper, “Using Conditional 
Random Fields for Sentence Boundary Detection in Speech” In Proceedings of the 43nd 
Annual Meeting of the Association for Computational Linguistics: ACL 2005. 
[43] J. Huang and G. Zweig, “Maximum Entropy Model for Punctuation Annotation from 
Speech.” In Proceedings of ICSLP 2002, pp. 917-920. 
[44] Y. Liu, E. Shriberg, A. Stolcke and M. Harper, “Comparing HMM, Maximum 
Entropy, and Conditional Random Fields for Disfluency Detection,” in Proc. of 
Eurospeech, 2005, pp. 3313-3316 
[45] J. Searle. 1979. Expression and Meaning: Studies in the Theory of Speech Acts. New 
York, Cambridge University Press. 
[46] K. J. Chen, C. R. Huang, F.Y. Chen, C. C. Luo, M. C. Chang, and C.J. Chen. 2001. 
Sinica Treebank: Design Criteria, representational issues and immplementation. In Anne 
Abeille, editor, Building and Using Syntactically Annotated Corpora. Kluwer. 29-37 
[47] Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, D. Hillard, M. Ostendort, M. 
Tomalin, P.l Woodland, and M. Harper, “Structural Metatada Research in the EARS 
Program”, Invited paper. ICASSP 2005.  
[48] A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. Hakkani, M. Plauche, G. Tur, and 
Y. Lu. “Automatic detection of sentence boundaries and disfluencies based on 
recognized words.” In Proceedings of the International Conference on Spoken 
Language Processing, 1998, pp. 2247--2250. 
[49] A.P. Dempster, N.M. Laird, and D.B. Rubin, “Maximum-likelihood from incomplete 
data via the EM algorithm. J. Royal Statist.”  Soc. Ser. B. 1977, pp 1-39 
[50] Yang Liu, Qun Liu, Shouxun Lin, “Log-Linear Models for Word Alignment,” In 
Proceedings of the 43rd Annual Meeting of the Association for Computational 
Linguistics: ACL 2005. 
[51] A. Stolcke, E. Shriberg, "Statistical Language Modeling for Speech Disfluencies, “ In 
Proceedings of ICASSP-96, Atlanta, GA, I, May 1996, pp. 405-408. 
[52] S.F. Chen, and J. Goodman, “An empirical study of smoothing techniques for 
language modeling.” Technical Report TR–10–98, Center for Research in Computing 
Technology (Harvard University), 1998. 
[53] H. Ney, S. Niessen, F. J. Och, H. Sawaf, C. Tilhnmm, S. Vogel, “ Algorithms for 
statistical translation of spoken language.” IEEE Transactions on Speech and Audio 
Processing, vol. 8, no. 1, 2000, pp. 24- 36. 
[54] Z. Wu and M. Palmer. “Verb semantics and lexical selection.” In Proceedings of the 
32nd  ACL, 1994, pp. 133–138. 
[55] M. A. Walker, D. Litman, C. Kamm, and A. Abella, 1997. PARADISE: a general 
framework for evaluating spoken dialogue agents. In Proceedings of the ACL, 271–280 
 32
計畫成果自評 
在本計劃兩年的執行過程中，建立一具實用性之醫療院所口述語言掛號系統可有
效整合多種服務。為了有效推論應診科別，症狀、疾病之推論規則庫及其推論模式
被整合到系統中。其中為了理解語者之語意，本計劃完成統計式中心詞分析模組並
提出語意相依圖之語意理解方法。針對口述語言中關於不流暢(disflencies)之分析，
本計劃除自行收集語料外，亦採用中研院所蒐集標註之”現代漢語連續口語對話語音
語料庫”(Mandarin Conversational Dialogue Corpus) ，作為分析不流暢現象之用。而
在偵測與修正不流暢語流的方法，本計畫提出ㄧ以語言模型與對正模型為主，輔以
聲學特徵之不流暢語流處理模型。 
 
此一計畫之研究屬於計算語言學之重要領域之一，國際知名大學如 MIT 等均投入
大量研究能量於此，國際知名軟體公司微軟科技之領導人比爾蓋之也預言未來十年
將為語言和語音技術發展快速的時期。但由於語言會依國家或地區不同而有所差
異，自立研究永續發展變成了在自然語言處理此一領域之原則。所以以此計畫之執
行將可提升口述語言對話系統尤其在語言處理部份之學術水準，亦可讓中文資訊技
術尤其式繁體中文及台灣地區之口語文化不至於在國際智慧型人機介面之研究上缺
席。並藉由系統的實現，可提升醫療院所線上掛號系統介面至具知識能力與人性化
之智慧型系統。本計劃之執行總計有一 IEEE 期刊論文、ACL 國際研討會論文並有
一部分成果被收錄至計算語言領域之國際書籍中，其學術研究成果詳列如下: 
 
1. Jui-Feng Yeh and Chung-Hsien Wu, “Edit Disfluency Detection and Correction Using a Cleanup 
Language Model and an Alignment Model,＂ IEEE Trans. Audio, Speech, and Language Processing, 
Vol. 14, No. 5, September, 2006, pp.1574~1583. (SCI, EI).  
2. Liang-Chih Yu, Chung-Hsien Wu, and Fong-Lin Jang, “HAL-based Cascaded Model for 
Variable-Length Semantic Pattern Induction from Psychiatry Web Resources,＂ COLING/ACL 2006, pp. 
937–944, Sydney, July 2006. 
3. Chung-Hsien Wu, Jui-Feng Yeh and Gwo-Lang Yan, "Speech act modeling and verification in spoken 
dialogue systems," to appear in Book Chapter of Advances in Chinese Spoken Language Processing, 
World Scientific Publishing Co. 2006. 
 
在這兩年中，參與研究之人員瞭解口述語言理解之基本理論與應用，並對語意中
心詞之萃取及語意相依圖之表示有深刻瞭解。對系統之架構及整合技術有極為豐富
之實際經驗。 
 34
行政院國科會補助專家學人出席國際學術會議報告 
         
94 年 9 月 19 日  
報告人姓名 吳宗憲 服務機構 及  職  稱 
國立成功大學 
資訊工程學系 
時間 
會議 
地點 
94 年 9 月 5 日至 9 月 8 日 
 
 
葡萄牙 里斯本 
本會核定 
補助文號 NSC 94-2213-E-006-018
會   議   名   稱
(中文)  第九屆歐洲語音通訊與技術國際會議 
(英文) 9th European Conference on Speech Communication and Technology
(Interspeech 2005) 
發表論文題目 
(中文) 應用語音辨認及場景偵測於電視新聞之影音摘要 
(英文) AUDIO-VIDEO SUMMARIZATION OF TV NEWS USING
SPEECH RECOGNITION AND SHOT CHANGE
DETECTION 
 
報告內容應包括下列各項: 
一、參加會議經過 
二、與會心得 
三、考察參觀活動(無是項活動者省略) 
四、建議 
五、攜回資料名稱及內容 
六、其他 
 
 
 
 
 
 
 
 
 36
1. Speech recognition-Language modeling,  
2. Prosody in language performance,  
3. Spoken language extraction/retrieval,  
4. Multi-modal/multimedia processing,  
5. Multilingual TTS,  
6. Text-to-speech,  
7. Multimodal dialogue systems. 
 
(三) 參觀活動 
 
     大會於會議期間，安排多家公司產品展示，其中包括軟體、書籍及相關應用產品。 
 
(四) 建議 
 
   筆者非常感謝國科會的補助，才能夠參加此次國際性會議，藉由此次之參與將對未來
之研究有莫大之助益，因此像這種大型的、有代表性國際會議應多鼓勵國內學者多參與，以
提昇國際地位，並提高國內之研究水準。 
 
(五) 攜回資料名稱 
 
研討會論文摘要一本及 CD 一片 
 
 38
 
2. THE PROSODED SCHEME 
 
As shown in Figure1, a news video is divided into two components: the anchor speech and the field report video. In 
anchor speech summarization, anchor speech is recognized into transcripts. Four confidence scores for anchor speech 
summarization are estimated and used to choose the best summarization result. In video skimming of the field report, 
this study selects the image sequences by minimizing the visual redundancy in the field report videos. Finally, the 
summarized anchor speech is synchronized with the video summarization result to provide a concise audio-video 
summarization output.  
 
2.1. Speech Summarization 
 
Given an anchor speech utterance with N  words, the corresponding transcription 1 2( , ,..., )NX w w w= is obtained 
using an LVCSR. A dynamic programming algorithm is applied to find a speech summary with highest confidence 
score. Given the compression ratio ∂ , a summarized word sequence 1 2( , ,..., )MY w w w=  with ( )M N= ×∂  words 
which maximizes the following four summarization scores is obtained: 
2 1
1
1
( ) { ( ) ( ) ( | , )
                ( , )}
M
C m R m L m m m
m
B SDG m m
S Y C w R w L w w w
B w w
λ λ λ
λ
− −
=
−
= + +
+
∑  (1) 
 
where ( )mC w  denotes the confidence score of word mw obtained from the LVCSR. ( )mR w denotes the word 
significance score. 2 1( | , )m m mL w w w− −  represents the trigram probability and 1( , )SDG m mB w w− is the semantic 
dependency score. 
 
Fig.1: Flowchart of News Video Summarization 
 
1) Speech Recognition Confidence: In order to obtain the text information of the TV news, an LVCSR is used to 
automatically translate the speech into transcriptions. In order to evaluate the assurance of the recognition result, the 
 40
( ( ), ( ))
( , ( , ))
( ( ( ), ( ))) / ( ( ), ( ))
r
i a b
j
i a bDR H w H w
r
i a b a b
f T S w w
F DR H w H w F H w H w=
 (5) 
 
where ( , ( ), ( ))ri a bF DR H w H w denotes the frequency that dependency relation ( ( ), ( ))ri a bDR H w H w  happens in the 
training corpus. ( ( ), ( ))a bF H w H w denotes the co-occurrences of ( )aH w and ( )bH w in the training corpus.  
 
 
Fig. 2: Example of the semantic dependency graph 
 
Figure 2 shows the example of semantic dependency graph which is constructed from the Chinese sentence “我們
(We) 遊覽(go sightseeing) 台北(Taipei) 各個(every) 景點(scenic spot).” 
 
2.2. Video Summarization 
 
For video summarization, this approach extracts the key frames of the field report according to the length of 
the extracted anchor speech. There are three steps in the video summarization procedure: First, this study employs the 
color histogram-based shot boundary detection algorithm to segment video shot. Second, we analyze the image 
sequence of each shot and extract the desired length of the image sequence. Finally, the summarized anchor speech is 
combined with the field report video to give a concise summarization result.  
1) Shot Boundary Detection: In general, there are three kinds of shot changes: hard cuts, fades and dissolves [4]. 
In the application of news video summarization, the hard cuts method is able to obtain good result for video 
skimming. In this study, we measure the color histogram difference to detect the shot change boundary. This basic 
idea is that the color content does not change rapidly within but across shots. In the video decoding procedure, we 
can get the image sequence with 256 pixels of color BMP format ( , , )iP R G B . Each color component is composed 
of ( )R red , ( )G green  and ( )B blue , and the value is ranged from 0 to 255. The color histogram difference 
1( , )i idiff P P−  is estimated between two images iP  and 1iP−  as follows: 
255
1 1
0
( , ) | |R Ri i R i i
k
diff P P P P− −
=
= −∑  
255
1 1
0
( , ) | |G Gi i G i i
k
diff P P P P− −
=
= −∑  
255
1 1
0
( , ) | |B Bi i B i i
k
diff P P P P− −
=
= −∑    (6) 
, ,
1 1( , ) ( , )
R G B
i i i i kk
diff P P diff P P− −= ∑  
 
If the difference 1( , )i idiff P P− exceeds a threshold θ  and the number of successive image frames is larger 
than a threshold NC , a hard cut will be detected. According to our preliminary experiment, we chose a setting of 
 42
As shown in Figure 3, the deletion and insertion error are due to subjective variation between different persons. The 
substitution error is caused by mis-recognition and therefore some important information is missing. 
 
3.2. MOS Evaluation of Video Summarization 
 
The performance of video summarization was evaluated by the Mean Opinion Score (MOS) test.  Eighteen graduate 
students were invited to evaluate the video summarization results. Then, they were asked to subjectively evaluate the 
results according to the following criterions: fluency (FLU), favorite (FAV) and average (AVG). For each evaluation, 
evaluators can assign the level from 0.0 to 10.0. The comparison result is shown in Figure 4. 
 
0 2 4 6 8 10 (MOS Score)
AVG
FAV
FLU
 
Fig. 4: Experimental results for subjective evaluation 
 
FLU is decided by the smoothness of speech summarization and video skim. FAV depends on personal favorite for 
video summarization result. Finally, AVG is used to measure the average of fluency and favorite scores. 
 
4. CONCLUSION 
 
This study has presented an approach for audio-video summarization of TV news using speech recognition and shot 
change detection. The TV news program is chosen as the experimental materials and is divided into two parts: the 
anchor speech and the field report videos. A speech summarization method is used to extract key speech segments 
from the anchor speech. Then, a video skimming method is applied to minimize dispensable images. Finally, the 
extracted anchor speech and the field report image sequence of TV news are aggregated into a summarization output. 
Experimental results from subjective and objective evaluation demonstrate that the proposed framework achieves a 
satisfactory performance. 
 
5. REFERENCES 
 
[1] I. Manu and M. Maubury, Advances in Automatic Summarization. Cambridge, MA: MIT Press, 1999. 
[2] C. Hori and S. Furui, “A new approach to automatic speech summarization,” IEEE Trans. on Multimedia, vol. 5, 
no. 3, pp. 368-378, 2003. 
[3] Sangkeun Lee; Hayes, M.H, "An application for interactive video abstraction," in Proc. Of ICASSP, vol. 5, pp. 
17-21, 2004. 
[4] R. Lienhart. "Comparison of automatic shot boundary detection algorithms." In Image and Video Processing VII 
1999, Proc. SPIE 3656-29, January 1999. 
[5] T. Kemp and T. Schaaf, “Estimating confidence using word lattices,” in Proc. 5th Eurospeech, vol. 2, Rhodes, 
Greece, 1997, pp. 827-830. 
[6] C.H. Hsieh, C.L. Huang and C.H. Wu, “Spoken document summarization using topic-related corpus and 
