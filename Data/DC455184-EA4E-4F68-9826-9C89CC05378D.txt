2 
摘要 
許多生物發出聲音之目的，主要是為了彼此之溝通或是某些特定行為所發出的聲音，
像進食、移動、飛行、求偶和警戒等等，而生物聲紋自動辨識之研究對於生物學、生態學
和環境監控應用上是相當重要的，特別是應用在偵測生物之種類和生物之分佈定位上。本
計畫提出一套自動辨識生物聲紋特徵之系統，此系統可用以辨識蛙聲和蟋蟀聲。對於輸入
一生物聲音，首先我們將此聲音之每一音節切取出來，然後對每一個音節分析其音色特徵，
我們是以整個音節之平均線性倒頻譜係數(ALPCC)及梅爾倒頻譜係數(AMFCC)當做此音節
之音色特徵向量。然後以線性區別分析演算法來提升辨識之正確率，此一演算法可以縮小
同類特徵向量之距離而且加大不同種類特徵向量之距離，因此可以減少特徵向量維度之情
況下提高辨識率。 
 
一. 報告內容 
1. 前言 
在日常生活中，我們能夠聽到許多生物的聲音，像是人類講話的聲音、狗吠聲、鳥鳴
聲、蛙叫聲、蟬聲和蟋蟀聲等等。許多生物發出聲音之目的，主要是為了彼此之溝通或是
某些特定行為所發出的聲音，像進食、移動、飛行、求偶和警戒等等，而生物聲紋自動辨
識之研究對於生物學、生態學和環境監控應用上是相當重要的，特別是應用在偵測生物之
種類和生物之分佈定位上。一般而言，人們只會聽到生物發出的聲音，較少看到生物本身，
除 此 之 外 ， 生 物 的 發 聲 (animal vocalizations) 早 已 進 化 成 與 特 定 之 物 種 相 關
(species-specific)，也就是不同之物種之聲音會有所不同，因此利用生物的聲音來辨識生物
種類是非常自然不過的，另外還能夠應用在生物種類統計(ecological censusing)、環境監控
(environment monitoring)和生物多樣性評估(biodiversity assessment)等方面。 
通常在做生態調查時都是靠人力在白天用拍攝方式去做生態記錄，雖然現今科技進
步，即使在晚上也可以利用紅外線拍攝，但是拍攝者必須花費很大的耐心和時間，且必須
小心翼翼在不驚動到動物的情況下去做拍攝，但如果是記錄動物的聲音則不同，通常我們
在野外容易聽得到動物的聲音，若想見其行蹤，則是難上加難，如果我們可以利用動物聲
音之記錄來做生態評估，應該能省下許多的時間精力，且可以更有效的從事生態物種記錄，
因此利用聲音去辨識物種做生態評估是近來常用的方法。由於生物種類繁多，不同物種間
的棲息環境及生活方式也都有所差異，因此眾多研究人員投入研究生物叫聲的差異性，希
望依此發現新的物種，然而目前所使用生物聲音的辨識方法，多採用人工至野外錄音，再
回實驗室做人工的識別，如此相當沒效率且不方便做大規模之調查，因此我們提出一套自
4 
Step 3. 針對所有的 )  ,( tf ，找出振幅強度之最大值，即 ),(),( tfMtfM nn     ≥ ，且將第 n
個音節的位置記錄為 )  ,( nn tf 。 
Step 4. 求得 )  ,(log20)0( 10 nnn tfMA = dB，當 β−< )()( 0A0A 0n  dB，停止切割動作，這代
表第 n 個音節的強度太小了因此就不需再切割出任何音節了， β 為一門檻值
(threshold)且其值設為 20。 
Step 5. 從 )  ,( nn tf 的 nt 位置找出 ntt < 的 )  ,( tfM 最大值，直到 dB )0()( β−<− nnn AttA ，同
時也尋找出 ntt > 的 )  ,( tfM 最大值，直到 dB )0()( β−<− nnn AttA 。這個步驟是要
找出第 n 個音節的起始時間 )( sn tt − 和結束時間 )( en tt + 。 
Step 6. 儲存第 n 個音節的時間點軌跡 ensnn tt...,,tt τA +−=       ),( 其中τ 。 
Step 7. 令 0)]...,,[,( =+−        snsn ttttfM ，目的是要將第 n 個音節的部份給清空，並令
n=n+1 且回到 Step 3 尋找下一個音節。 
 
2.2 特徵擷取 
當每一動物聲音之音節已切割出來後，我們先將每一音節分為一個個部分重疊之音框
(frame)，對每一個音框，我們以 LPCCs/MFCCs 為其特徵向量，而所有屬於同一音節中音
框之 LPCCs/MFCCs 的平均值則視為此一音節之特徵向量，因此所有音節之特徵向量長度
是固定的，與音節之長短無關。 
(a)線性倒頻譜係數(Linear Predictive Cepstral Coefficients, LPCCs) 
線性預測編碼 (linear predictive coding, LPC) 已經被廣泛的應用在語音辨識的技術
上，可以說是語音分析技術上最重要的一項表示法，線性預測編碼的基本觀念，在於一個
語音的訊號可以由前面 p 個語音訊號的線性組合來預測，而其做法是將預測的訊號值與實
際的訊號值的誤差值減至最小，如此就可以得到最佳的預測器，而預測器內的係數即為線
性組合所需的係數稱為線性預測編碼係數。 
線性倒頻譜係數(Linear prediction cepstral coefficients, LPCCs)是一種較可靠之特徵而
且已被證明比起線性預測編碼係數更能應用在語音辨識上，並且更能表現出語音訊號中頻
譜的波峰與細部的變化。計算線性倒頻譜係數之詳細步驟如下: 
Step 1. Preemphasis 
使用一階的 FIR 濾波器: 
  )1(~)()(~ −−= nsansns  
其中 )(ns 為輸入的聲音訊號， a~ =0.95。 
6 
 ∑−
=
−⎟⎠
⎞⎜⎝
⎛+=
1
1
m
k
kmkmm acm
kac ,  pm ≤≤1  
其中 2σ 為與語音發聲模型之增益值(gain term): 
 ∑
=
−=
p
k
k krar
1
2 ][]0[σ  
(b)梅爾倒頻譜係數（Mel-scale Frequency Cepstral Coefficients, MFCCs） 
梅爾倒頻譜係數已被廣泛的利用在語音辨識上，事實上，梅爾倒頻譜係數在語音辨識
上是非常有用的而且可以用一組頻帶來描述一段聲音訊號。mel 來是用以表示聽覺上對一
個 tone 感覺上的的音高(pitch)或者是頻率的計算單位，在人類聽覺系統中，人類對於一個
tone 的實際頻率(physical frequency)之反應並不是呈現線性變化，而實際頻率和梅爾頻率之
間的對應關係在頻率低於 1 KHz 時呈現線性變化，但在高頻的部份是呈現對數變化的。而
實際頻率和梅爾頻率之間的關係圖顯示於圖二，其數學式如下: 
 )
700
1(log2595 10
fmel += , 
 )110(700 2595 −=
mel
f , 
f 代表實際的頻率值。人類之聽覺系統可將聲音之頻率分為一個個臨界頻帶(critical band)，
位於同一臨界頻帶內之頻率聲音對人耳聽起來是相似的，因此我們可以用一組濾波器來過
濾每一臨界頻帶之聲音訊號，另外每一個臨界頻帶的頻寬會隨著頻率值而改變，圖三顯示
一組三角臨界頻帶濾波器之形狀及頻寬，表一是每個臨界頻帶濾波器的頻寬範圍。 
 
圖二 實際頻率和梅爾頻率之間的關係圖 
 
8 
求取 MFCC 之步驟如下： 
Step 1: 預強調 (Pre-emphasis) 
 ][ˆ][][ˆ 1nsansns −−=  
][ns  為我們輸入訊號， aˆ的預設值為 0.95。 
Step 2: 取音框 (Framing) 
將每一個音節切割成一個一個的音框，大小為 512，而且為了讓每個音框的差異性
不大，我們又讓每個音框重疊一半。 
Step 3: 乘上漢明視窗(Hamming Windowing)  
為了來消除每個音框與開始與結束的不連續性，每個音框都乘上一個漢明視窗，
漢明視窗式子如下。 
 10  ),
1
2cos(46.054.0][ −≤≤−−= NnN
nmw π  
Step 4: 快速傅立葉轉換(FFT)  
將音訊訊號從時域轉換成頻率域 
      0   ,][~][
1
0
2
NkenskX
N
n
n
N
kj <≤= ∑−
=
− π
  
N：音框大小, ][~ ns ：離散訊號 
Step 5: 三角帶通濾波器(Triangular band-pass filter)  
由於人耳對聲音的頻率的解析度不是呈線性關係，而是呈現對數(logarithm)變化，利
用三角帶通濾波器將聲音訊號分成一個個頻帶，並算出每個頻帶的能量: 
         0  ,)(
1
0
JjXkE
K
k
kjj <≤= ∑−
=
φ , 
J 為三角帶通濾波器之個數, kA 為 ][kX 的振幅: 
              2/0,|][| 2 NkkXAk <≤=  
而 jφ 為第 j 個濾波器: 
  
⎪⎩
⎪⎨
⎧
≤≤−−
≤≤−−
≥≤
=
j
h
j
c
j
c
j
h
j
h
j
c
j
l
j
l
j
c
j
l
j
h
j
l
j
IkIIIkI
IkIIIIk
IkIk
k
  )/()(
                 )/()(
or   if                         0
][φ  
在這裡， jlI , jcI 和 jhI 分別代表第 j 個濾波器之低頻索引值，中間頻率索引值，和
高頻索引值: 
10 
  minmax
min
mm
mm
m ff
fFF −
−=∧ , 
其中 maxmf 和 minmf 為第 m 個特徵的最大值和最小值。 
 
2.3 線性區別分析演算法(Linear Discriminant Analysis, LDA) 
線性區別分析演算法之目的是將一個高維度的特徵向量轉換成一個低維度的向量，並
且增加辦識的準確率，線性區別分析演算法主要處理不同類別間的區別程度而不是用於不
同類別之表示方式。線性區別分析演算法的主要精神是要把同類之間的距離最小化，並且
把不同類別之間的距離給最大化，所以，必需決定一個轉換矩陣(transformation matrix)來將
維度 n 的特徵向量轉換成維度 d 的向量，在這裡 nd ≤ ，透過這樣的轉換我們能夠增強不同
類別之間的差異性。最常使用的轉換矩陣主要依據 Fisher criterion FJ 來求得: 
 ))()(()( ASAASAtrAJ B
T1
W
T
F
−= , 
其中， WS 和 BS 分別代表的是同類別之散佈矩陣(within-class scatter matrix)和不同類別之散
佈矩陣(between-class scatter matrix)，而同類別之散佈矩陣的公式如下: 
 ∑∑
= =
−−=
C
j
N
i
T
j
j
ij
j
iW
i
S
1 1
))(( μxμx , 
而 jix 代表在類別 j 中的第 i 個特徵向量， jμ 為第 j 類的平均向量(mean vector)，C 為類別的
數目， jN 為類別 j 裡的特徵向量個數。而不同類別之散佈矩陣公式如下: 
  ∑
=
−−=
C
j
T
jjBS
1
))(( μμμμ ,     
μ 為所有類別的平均向量。線性區別分析演算法的目的是要去求出能夠使不同類別之散佈
矩陣和同類別之散佈矩陣的比值為最大值轉換矩陣(transformation matrix) optA ，而其維度大
小為 dn× : 
 
.
)(
)(
maxarg
ASAtr
ASAtrA
W
T
B
T
opt
A
=
 
此一轉換矩陣，可經由求出 B1W SS − 的 eigenvectors 來得到，而 optA 之 d 個行向量為前 d 個最
大 eigenvalue 值所對應之 eigenvector。假設 eigenvalues 的順序為非遞增的，則所保留之
eigenvector 個數可由以下公式求得: 
∑∑
==
≥
n
i
i
d
i
i
11
95.0 λλ , 
iλ 為第 i 個 eigenvalue。 
12 
表二 青蛙聲音資料庫 (SC 為代碼) 
SC Scientific name (Popular name) Ns 
1 Bufo bankorensis (Central Formosan Toad) 38 
2 Bufo melanosticus (Spectacled Toad) 48 
3 Hyla chinensis (Chinese Tree Frog) 46 
4 Microhyla butleri (Butler’s Narrow-Mouthed Toad) 15 
5 Microhyla heymonsi (Heymonsi’s Narrow-Mouthed Toad) 235 
6 Microhyla ornata (Ornate Narrow-Mouthed Toad) 193 
7 Rana adenopleura (Olive Frog) 36 
8 Rana catesbiana (American Bull Frog) 13 
9 Rana guentheri (Guenther’s Amoy Frog) 15 
10 Rana kuhlii (Kuhli’s Wart Frog) 52 
11 Rana latouchii (Brown Wood Frog) 95 
12 Rana limmocharis (Indian Rice Frog) 38 
13 Rana rugulosa (Chinese Bull Frog) 98 
14 Rana swinhoana (Swinhoe's Frog) 13 
15 Rana sauteri (Sauter’s Frog) 132 
16 Rana taipehensis (Taipei Grass Frog) 27 
17 Buergeria japonica (Japanese Tree Frog) 60 
18 Buergeri robusta (Brown Tree frog) 67 
19 Chirixalus eiffingeri (Eiffinger’s Tree Frog) 10 
20 Chirixalus idiootocus (Meintein Tree Frog) 112 
21 Polypedates megacephalus (White lipped Tree Frog) 23 
22 Rhacophorus arvalis (Farmland Tree Frog) 46 
23 Rhacophorus Aurantiventris (Orange-Belly Tree Frog) 38 
24 Rhacophorus moltrechti (Moltrecht’s Tree Frog) 191 
25 Rhacophorus prasinatus (Emerald Tree Frog) 52 
26 Rhacophorus taipeianus (Taipei Green Tree Frog) 49 
27 Microhyla steinegeri (Steinger's Narrow-Mouthed Toad) 3 
28 Kaloula pulchra (Malaysian Narrow-Mouthed Toad) 6 
29 Rana longicrus (Long-Legged Frog) 9 
30 Rana psaltes (Harpist Frog) 65 
 
14 
 表四 青蛙聲音之辨識正確率 
SC HMM ALPCC AMFCC 
1 66% 89% 97% 
2 98% 91% 100% 
3 89% 97% 100% 
4 40% 100% 100% 
5 76% 71% 95% 
6 82% 80% 97% 
7 53% 58% 100% 
8 54% 100% 100% 
9 93% 100% 100% 
10 50% 76% 100% 
11 80% 66% 100% 
12 68% 100% 100% 
13 81% 97% 100% 
14 62% 46% 38% 
15 77% 87% 88% 
16 63% 85% 92% 
17 75% 100% 100% 
18 96% 100% 97% 
19 100% 100% 100% 
20 99% 100% 100% 
21 43% 60% 56% 
22 15% 30% 73% 
23 79% 68% 100% 
24 94% 96% 99% 
25 67% 96% 96% 
26 94% 85% 100% 
27 0% 100% 100% 
28 0% 66% 100% 
29 89% 88% 100% 
30 98% 69% 100% 
Average 78.9% 83.9% 96.2% 
 
16 
[5] A. L. McIlraith and H. C. Card, “Birdsong recognition with DSP and neural networks,” 
Proceedings of the IEEE International Conference on Communications, Power, and 
Computing, vol. 2, pp. 409-414, May 1995. 
[6] A. L. McIlraith and H. C. Card, “Birdsong recognition using backpropagation and 
multivariate statistics,” IEEE Transactions on Signal Processing, vol. 45, Issue 11, pp. 
2740-2748, Nov. 1997. 
[7] A. L. McIlraith and H. C. Card, “Bird song identification using artificial neural networks 
and statistical analysis,” Proceedings of the IEEE 1997 Canadian Conference on Electrical 
and Computer Engineering, vol. 1, pp. 63-66, May 1997. 
[8] K. Sasaki and M. Yamazaki, “Vector compression of bird songs spectra in water sites by 
using the linear prediction method and its application to an automated Bayesian species 
classification,” Proceedings of the 38th Annual Conference on SICE Annual, pp. 1083-1088, 
July 1999. 
[9] C. Rogers, “High resolution analysis of bird sounds,” Proceedings of the 1995 International 
Conference on Acoustics, Speech, and Signal Processing, vol. 5, pp. 3011-3014, May 1995. 
[10] A. Harma and M. Juntunen, “A method for parametrization of time-varying sounds,” IEEE 
Signal Processing Letters, vol. 9, Issue 5, pp. 151-153, May 2002. 
[11] R. Vergin, D. O'Shaughnessy, and A. Farhat, “Generalized Mel Frequency Cepstral 
Coefficients for Large-Vocabulary Speaker-Independent Continuous-Speech Recognition,” 
IEEE Trans. on Speech and Audio Processing, Vol. 7, No. 5, 1999, pp. 525 –532. 
[12] L. Rabiner and B. H. Juang, Fundamentals of Speech Recognition. Englewood Cliffs, NJ: 
Prentice-Hall, 1993. 
[13] J. W. Picone, “Signal modeling techniques in speech recognition,” Proceedings of the IEEE, 
Vol. 81, pp. 1215–1247, 1993. 
[14] T. Tolonen and M. Karjalainen, “A computationally efficient multipitch analysis model,” 
IEEE Trans. Speech Audio Processing, Vol. 8, pp. 708–716, Nov. 2000. 
[15] Cristianini, An Introduction to Support Vector Machine, Cambridge, 2000. 
[16] L. Lu, S.Z. Li and H.J. Zhang, “Content-based audio segmentation using support vector 
machines,” Proceedings of the IEEE International Conference on Multimedia and Expo, pp. 
749-752, Aug. 2001. 
[17] Hung Wei Ng, Y. Sawahata and K. Aizawa, “Summarization of wearable videos using 
support vector machine,” Proceedings of the IEEE International Conference on Multimedia 
and Expo, pp. 325-328, Aug. 2002. 
[18] S. I. Hill, P. J. Wolfe and P. J. W. Rayner, “Nonlinear perceptual audio filtering using support 
vector machines,” Proceedings of the 11th IEEE Signal Processing Workshop on Statistical 
18 
[1] C. H. Lee, C. H. Chou, and R. Z. Huang, “Automatic Recognition of Bioacoustic Sounds: an 
Experiment on the Frog Vocalizations”, in Proceedings of the 17th IPPR Conference on 
Computer Vision, Graphics, and Image Processing, Hualien, Aug. 15-17, 2004. 
[2] C. H. Lee, C. H. Chou, C. C. Han, and R. Z. Huang, “Automatic Recognition of Frog Calls 
Using Averaged MFCC and Linear Discriminant Analysis”, in Proceedings of the 9th 
Conference on Artificial Intelligence and Applications, Taipei, Nov. 5-6, 2004. 
 
以下為發表之期刊論文的全文 
 
Once the syllables have been properly segmented, a set
of features will be calculated to represent each syllable.
The most well-known features for speech/speaker recogni-
tion are linear predictive coeﬃcients (LPCs) (Rabiner and
Juang, 1993) or Mel-frequency cepstral coeﬃcients
(MFCCs) (Picone, 1993; Rabiner and Juang, 1993; Vergin
et al., 1999). In this paper, we use the averaged MFCCs in a
syllable to identify animals from their sounds due to the
fact that MFCCs can represent the spectrum of animal
sounds in a compact form. In the next section, we will
describe the proposed recognition method for animal
vocalizations.
2. The proposed recognition method for animal
vocalizations
The recognition system consists of two parts: the train-
ing part and the recognition part. The training part is com-
posed of three main modules: syllable segmentation,
averaged MFCCs extraction, and linear discriminant anal-
ysis (LDA). The recognition part consists of four modules:
syllable segmentation, averaged MFCCs extraction, LDA
transformation, and classiﬁcation. A detailed description
of each module will be described below.
2.1. Syllable segmentation
The input acoustic signal is ﬁrst segmented into a set of
syllables (Harma, 2003). Each syllable is regarded as the
basic acoustic unit for recognition. The syllable segmenta-
tion method based on the frequency information is de-
scribed as follows:
Step 1. Compute the spectrogram of the input bioacoustic
signal using short-time Fourier transform (STFT).
We denote the spectrogram a matrix S(f, t), where f
represents frequency index and t is the frame
index.
Step 2. Set n = 0.
Step 3. Find fn and tn, such that jS(fn, tn) j P jS(f, t)j, for
every pair of (f, t). Set the position of the nth sylla-
ble to be (fn, tn).
Step 4. Compute the amplitude An(0) = 20 log10jS(fn, tn)j
dB and set the frequency parameter Wn(0) = fn.
If An(0) < A0(0)  20 dB, stop the segmentation
process. This means that the amplitude of the nth
syllable is too small and hence no more syllables
need to be extracted.
Step 5. Starting from (fn, tn), trace the maximal peak of
jS(f, t)j for t < tn until An(t) < An(0)  ßdB, where
ß is the stopping criteria and its default value is
20. Next, trace the maximal peak of jS(f, t)j for
t > tn until An(t) < An(0)  ßdB. The step is to
determine the starting time (tn  ts) and the ending
time (tn + te) of the nth syllable around tn.
Step 6. Store the amplitude trajectories corresponding to
the nth syllable in function An(s), where
s = tn  ts, . . . , tn + te.
Step 7. Set S(f, [tn  ts, . . . , tn + te]) = 0 to delete the area
of nth syllable. Set n = n + 1 and goto Step 3 to
ﬁnd the next syllable.
Fig. 1 shows the waveform of Olive Frog (Rana adeno-
pleur) as well as the segmentation results by using the
energy information and the spectrogram frequency infor-
mation. It is evident that a better result can be obtained.
After segmenting each syllable, the averaged MFCCs are
extracted to represent the syllable.
2.2. Averaged MFCCs extraction
MFCCs have been the most widely used features for
speech recognition (Picone, 1993; Rabiner and Juang,
1993; Vergin et al., 1999), bird song recognition (Kogan
and Margoliash, 1998), and audio retrieval (Slaney, 2002)
due to their ability to represent the signal spectrum in a com-
Fig. 1. (a) The waveform of Olive Frog (Rana adenopleur). (b) The segmentation result by using the energy information. (c) The segmentation result by
using the spectrogram frequency information.
94 C.-H. Lee et al. / Pattern Recognition Letters 27 (2006) 93–101
syllables. From this table, we can see that these feature vec-
tors are very close. Table 2 shows the distance between
each feature vector extracted from these syllables and the
representative feature vectors corresponding to the 30
kinds of frogs. From this table, we can see that the correct
frogs (indexed with subject code 7) can be identiﬁed using
Euclidean distance between two feature vectors.
2.3. Linear discriminant analysis (LDA)
LDA (Baker, 2004; Duda et al., 2000; Slaney, 2002) aims
at improving the classiﬁcation accuracy at a lower-dimen-
sional feature vector space. LDA deals with discrimination
between classes. The goal of LDA is to minimize the within-
class distance while to maximize the between-class distance.
Table 1
Feature vectors for the extracted syllables (SC denotes the subject code)
SC S1 S2 S3 S4 S5 S6 S7 S8 S9
f1 0.627 0.667 0.606 0.713 0.727 0.791 0.843 0.829 0.680
f2 0.680 0.684 0.784 0.768 0.784 0.793 0.783 0.801 0.782
f3 0.290 0.242 0.342 0.325 0.361 0.310 0.283 0.296 0.511
f4 0.404 0.456 0.241 0.309 0.305 0.256 0.264 0.270 0.284
f5 0.368 0.324 0.504 0.463 0.416 0.467 0.417 0.383 0.455
f6 0.156 0.098 0.418 0.406 0.346 0.462 0.291 0.303 0.439
f7 0.273 0.250 0.266 0.251 0.162 0.260 0.183 0.154 0.258
f8 0.457 0.459 0.487 0.536 0.467 0.511 0.510 0.493 0.550
f9 0.592 0.617 0.351 0.512 0.439 0.562 0.760 0.734 0.478
f10 0.809 0.915 0.641 0.662 0.699 0.698 0.873 0.908 0.676
f11 0.671 0.658 0.420 0.490 0.577 0.456 0.466 0.485 0.475
f12 0.506 0.479 0.653 0.661 0.686 0.609 0.426 0.463 0.727
f13 0.523 0.357 0.559 0.541 0.501 0.511 0.437 0.409 0.525
f14 0.451 0.477 0.390 0.453 0.337 0.453 0.450 0.462 0.396
f15 0.593 0.614 0.410 0.418 0.491 0.425 0.467 0.483 0.482
Table 2
Distance between each feature vector extracted from the example syllables and the 30 representative feature vectors with the minimum distance highlighted
SC S1 S2 S3 S4 S5 S6 S7 S8 S9
1 0.7415 0.8188 0.6039 0.5936 0.6587 0.5493 0.7214 0.7292 0.6717
2 0.8217 0.9170 0.6944 0.6371 0.7520 0.6797 0.8420 0.8740 0.7215
3 0.7595 0.8941 0.8354 0.8194 0.8182 0.8604 0.9300 0.9503 0.8349
4 0.6782 0.7808 0.7048 0.6665 0.7097 0.6718 0.7571 0.7804 0.7471
5 0.6993 0.7981 0.6162 0.5661 0.6487 0.5693 0.7071 0.7289 0.5668
6 0.8232 0.8872 0.6899 0.6597 0.7372 0.6684 0.8217 0.8341 0.6709
7 0.3099 0.3917 0.4410 0.2813 0.2748 0.2685 0.2748 0.2622 0.3777
8 0.9130 1.0213 0.6975 0.6980 0.7500 0.6806 0.8556 0.8842 0.6811
9 0.8558 0.9303 0.6713 0.6936 0.7061 0.6909 0.8221 0.8495 0.7285
10 0.9542 1.0621 0.7159 0.7853 0.7150 0.7850 0.9780 0.9661 0.7123
11 0.7517 0.8106 0.5296 0.5299 0.6268 0.5263 0.7261 0.7387 0.6011
12 1.0211 1.1240 0.8332 0.8651 0.9174 0.8173 0.9882 1.0073 0.9110
13 0.6558 0.7210 0.6242 0.5835 0.6120 0.5959 0.6438 0.6716 0.6317
14 0.6947 0.8062 0.6483 0.6258 0.6534 0.6830 0.8248 0.8373 0.6077
15 0.5267 0.5939 0.6179 0.5592 0.5456 0.6100 0.7183 0.7158 0.5603
16 0.7915 0.8734 0.7215 0.6223 0.7144 0.6161 0.7501 0.7792 0.6388
17 0.6448 0.7294 0.6343 0.5876 0.6302 0.6157 0.7493 0.7423 0.5883
18 0.6568 0.7204 0.5167 0.5368 0.5833 0.5677 0.7192 0.7357 0.6165
19 1.2702 1.3241 1.3281 1.3180 1.3344 1.3571 1.4431 1.4364 1.2818
20 1.1940 1.2704 1.2935 1.3012 1.3162 1.3569 1.4272 1.4301 1.2355
21 0.6753 0.7434 0.4479 0.4495 0.5182 0.4171 0.5986 0.6129 0.5046
22 0.8649 0.9368 0.7933 0.7902 0.8599 0.8153 0.9374 0.9430 0.7289
23 0.7865 0.8738 0.7021 0.6580 0.6753 0.6612 0.7866 0.7947 0.5952
24 1.0395 1.1656 0.7796 0.8132 0.9295 0.7812 0.9507 0.9850 0.8778
25 0.7724 0.8922 0.7335 0.6541 0.7303 0.6930 0.8256 0.8609 0.6843
26 0.8508 0.9501 0.6928 0.6290 0.7261 0.6534 0.8123 0.8423 0.6436
27 1.0673 1.0742 0.9840 0.8746 1.0122 0.8696 1.0030 0.9894 0.9164
28 0.9436 1.0374 0.7056 0.7272 0.7848 0.7024 0.8491 0.8737 0.7178
29 0.7372 0.8596 0.5467 0.5054 0.6029 0.5086 0.7217 0.7424 0.4600
30 0.9759 1.0893 0.8906 0.8582 0.9431 0.8531 0.9991 1.0190 0.7676
96 C.-H. Lee et al. / Pattern Recognition Letters 27 (2006) 93–101
F km is the mth feature of the kth species, and r is the subject
code for the rth species as shown in Tables 3 and 4.
3. Experimental results
Two audio databases of 30 frog calls and 19 cricket
calls derived from compact disk are used for the experi-
ments (see Tables 3 and 4). The sampling frequency is
44,100 Hz and each sample is digitized in 16 bits. Most
of the calls are ﬁeld recordings with additional sounds in
the background. Some of the calls are generated by multi-
ple individuals vocalizing simultaneously. Each acoustic
signal is ﬁrst segmented into a set of syllables, in which
half is used for training and half for testing. Two scenarios
for dividing the syllables equally into training and testing
sets are conducted: (1) the syllables are alternately divided
into training and testing sets, that is, the odd-numbered
syllables are used for training whereas the even-numbered
syllables for testing; (2) the ﬁrst-half of the syllables are
used for training and the second half of the syllables are
used for testing. In the second scenario, the training and
testing syllables may be extracted from the calls of dif-
ferent frogs/crickets. The classiﬁcation accuracy (CA) is
deﬁned as
CA ¼ NCA
N S

 100; ð15Þ
where NCA is the number of syllables which were recog-
nized correctly and NS is the total number of test syllables
(shown in Tables 3 and 4).
3.1. Experiment 1—alternate training and testing
In this experiment, the syllables are alternately divided
into training and testing sets. The odd-numbered syllables
are used for training whereas the even-numbered syllables
for testing. Tables 5 and 6 compare the recognition results
of the proposed averaged MFCC (AMFCC) method with
HMM and averaged LPC (ALPC) for 30 frog calls and
19 cricket calls, respectively. In addition, multiple feature
vector templates were compared in these two tables, where
LPC-i and MFCC-i denote that i (i > 1) vector templates
are used to model the syllables derived from the same spe-
cies. From these two tables, we can see that AMFCC
greatly outperforms HMM and ALPC. HMM, which ex-
ploits the temporal information among the frames within
a syllable, does not provide better performance than ALPC
or AMFCC. Since variations between consecutive frames
within a syllable are not regular and thus temporal infor-
mation extracted for identiﬁcation purpose is not very
essential. In addition, most of the sounds are recorded in
the ﬁeld with additional sounds/noise in the background.
Thus, the feature vector extracted from each syllable is
not so stable. On the other hand, the averaged LPC/MFCC
can attenuate the eﬀect of background noise by averaging
the feature vectors of all frames within the syllable. There-
fore, the proposed AMFCC is adequate for the identiﬁca-
tion of frogs and crickets. From Tables 5 and 6, we can
also see that if each species is modeled by more than one
vector template, the classiﬁcation accuracy will increase
as well.
3.2. Experiment 2—progressive training and testing
In this experiment, the ﬁrst-half of the syllables are used
for training and the second half of the syllables are used for
testing. Tables 7 and 8 compare the recognition results of
AMFCC with HMM and ALPC for 30 frog calls and 19
cricket calls, respectively. In addition, multiple feature vec-
tor templates were compared in these two tables. From
these two tables, we can see that AMFCC greatly out-
performs HMM and ALPC. However, if each species is
Table 4
Cricket call database (SC denotes the subject code)
SC Scientiﬁc name (popular name) Ns (Experiment 1) Ns (Experiment 2)
1 Gryllotalpa fossor (Mole Cricket) 79 80
2 Teleogryllus occipitalis (Oil guord) 24 24
3 Teleogryllus mitratus (Oil guord) 3 3
4 Teleogryllus emma (Oil guord) 9 10
5 Gryllus bimaculatus (Painted Mirror) 5 6
6 Brachytrupes portentosus (Formosan Giant Crickets) 66 67
7 Loxoblemmus equestris (Coﬃn-headed cricket) 9 9
8 Dianemobius ﬂavoantennalis (Flowered Bell) 22 22
9 Homoeogryllus japonicus (Horse bell) 2 3
10 Scleropterus punctatus (Rocky bell) 6 7
11 Oecanthus longicaudus (Bamboo bell) 6 7
12 Xenogryllus marmoratus (Pagoda Bell) 5 6
13 Anaxipha pallidula (Yellow Bell) 57 58
14 Svistella bifasciatata (Golden Bell) 14 15
15 Homoeoxipha lycoides (Inky bell) 4 4
16 Mecopoda elongta L. (Weaving Lady) 74 74
17 Gryllotalpa fossor (Mole Cricket) 135 135
18 Teleogryllus occipitalis (Oil guord) 1 2
19 Teleogryllus mitratus (Oil guord) 5 5
98 C.-H. Lee et al. / Pattern Recognition Letters 27 (2006) 93–101
and testing, the diversity of the training syllables is not
large enough to learn all the possible calls, especially for
the experiment on cricket calls.
Comparing Tables 3 and 5 (respectively, Tables 4 and 6),
we can see that alternate training and testing performs better
than progressive training and testing. This is due to the fact
Table 7
Classiﬁcation accuracy of 30 frog calls for progressive training and testing
SC HMM (%) ALPC (%) LPC-2 (%) LPC-3 (%) AMFCC (%) MFCC-2 (%) MFCC-3 (%)
1 66 89 97 95 97 100 100
2 98 91 96 94 100 100 100
3 89 97 100 100 100 100 100
4 40 100 100 100 100 100 100
5 76 71 70 87 95 89 97
6 82 80 84 86 97 95 93
7 53 58 53 94 100 100 100
8 54 100 100 92 100 100 100
9 93 100 100 100 100 100 100
10 50 76 98 100 100 100 100
11 80 66 74 75 100 100 100
12 68 100 100 100 100 100 100
13 81 97 94 100 100 100 100
14 62 46 54 46 38 46 54
15 77 87 87 91 88 92 75
16 63 85 85 85 92 96 93
17 75 100 100 100 100 100 100
18 96 100 99 100 97 97 97
19 100 100 100 100 100 100 100
20 99 100 100 100 100 100 100
21 43 60 74 78 56 48 74
22 15 30 48 50 73 72 61
23 79 68 87 82 100 100 100
24 94 96 98 98 99 99 100
25 67 96 96 96 96 100 100
26 94 85 92 90 100 100 100
27 0 100 100 33 100 100 0
28 0 66 100 50 100 100 83
29 89 88 89 56 100 100 33
30 98 69 80 69 100 100 98
Average 78.9 83.9 86.8 89.8 96.2 95.5 94.6
Table 8
Classiﬁcation accuracy of 19 cricket calls for progressive training and testing
SC HMM (%) ALPC (%) LPC-2 (%) LPC-3 (%) AMFCC (%) MFCC-2 (%) MFCC-3 (%)
1 90 92 94 96 98 100 100
2 83 100 83 83 100 100 100
3 100 100 100 100 100 100 100
4 50 20 90 90 100 100 100
5 50 100 100 100 100 100 100
6 99 97 94 99 100 100 100
7 89 100 89 89 88 89 89
8 68 86 86 95 100 100 100
9 33 100 33 67 100 33 67
10 86 85 86 29 85 86 0
11 71 100 100 0 100 100 0
12 100 100 100 67 100 100 17
13 93 94 98 36 100 100 45
14 87 93 87 7 93 93 7
15 100 100 100 0 100 100 0
16 99 98 93 92 98 99 77
17 100 97 99 84 100 100 46
18 0 100 100 100 100 100 100
19 20 80 60 80 80 60 0
Average 91.2 94.6 94.0 79.5 98.9 98.5 69.1
100 C.-H. Lee et al. / Pattern Recognition Letters 27 (2006) 93–101
