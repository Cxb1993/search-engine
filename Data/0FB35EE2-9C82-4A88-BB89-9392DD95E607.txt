  
realized a crucial fact that the Kalman gain vector can 
be calculated as a particular set of normalized 
least-squares interpolation errors. This research 
develops the SRF QRD-LSL interpolation algorithm 
first. Then the Kalman gain vector of the RLS algorithm 
can be computed by employing the SRF QRD-LSL 
interpolation algorithm that can produce the normalized 
least-squares interpolation errors of various orders. Like 
the Kalman gain estimation (KaGE) RLS algorithm 
developed in [3], which is based on the QR decomposi-
tion-based RLS (QR-RLS) algorithm, the proposed RLS 
algorithm also requires 2( log )O N N  operations per 
iteration. 
                        
2.  The proposed algorithms 
                         
2.1.  Least squares lattice interpolators 
                         
The least-squares solution of the interpolation coeffi-
cients can be determined by minimizing 
2
,
1
( )
n f
I
p f
i f
e i
−
= −
∑ , 
which results in the augmented asymmetric interpolation 
normal equations 
                   
    1 , ,( ) ( ) ( )N p f p fn n f n f+ − = −Φ b i           (3) 
                 
where , ,( )  ( ) 
TT T
p f f p f pn f I n f⎡ ⎤− = −⎣ ⎦i 0 0  is the ( 1) 1N + ×  
vector in which , ( )p fI n f−  is the sum of interpolation 
error squares of order ( , )p f  and f0  and p0  are 
column vectors of f and p zeros, respectively, 1( )N n+Φ  
is the ( 1) ( 1)N N+ × +  time-average correlation matrix 
of the input data samples ( )x i . 
    A computationally efficient LSL interpolator devel-
oped in [1] requires only O(N) operations via the 
“intermediate predictions”, where N is the order of the 
interpolation lattice.  The LSL interpolator computes 
the order-recursive interpolation errors as an additional 
past data sample and an additional future data sample 
are taken into account, respectively, 
                     
1, , 1, , 1
*( ) ( ) ( ) ( , )I Ip f p f p f b Ne n f e n f q n e n n f+ + +− = − − −     (4) 
 
, 1 , , 1 , 1
*( 1) ( 1) ( ) ( , 1)I Ip f p f p f f Ne n f e n f q n e n n f+ + +− − = − − − − −   
                        (5) 
 
where both 1, ( )p fq n+  and , 1( )p fq n+  are scalar coeffi-
cients; 
                    
1
, 1 1,
1
1
*( , 1) ( ) ( ) ( ),1
N
f N N k
k
k f
e i i f x i a n x i k i n
+
+ +
=≠ +
− − = + − ≤ ≤∑  
                 
is referred to as the ( 1)stN order+ −  intermediate for-
ward prediction error as it is the least-squares prediction 
error of ( )x i  based on a weighted linear combination 
of the (N+1) previous data samples without taking the 
present data sample x(i-f-1) into consideration, where 
1, ... ...( ), 1,2, , , 2, ,N k f fa n k N+ = +  are ( 1)stN order−+  
intermediate forward prediction coefficients; the 
( 1)stN order+ −  intermediate backward prediction 
error is defined as the least-squares prediction error of 
x(i-N-1) based on a weighted linear combination of 
x(i-N), x(i-N+1),…,x(i-f-1), x(i-f+1),…,x(i) without tak-
ing the data sample x(i-f) into consideration: 
                         
1
, 1 1,
1
1
*( , ) ( 1) ( ) ( 1)
N
b N N k
k
k p
e i i f x i N c n x i k N
+
+ +=≠ +
− = − − + + − −∑  
 
1 i n≤ ≤ , where 1,2,..., , 2,...,k p p N= +  are ( 1)stN +  
− order intermediate backward prediction coefficients. 
In order to compute the order-updated interpolation 
errors in (4) and (5), both the intermediate forward and 
backward prediction errors must be computed first. 
They can be computed by using both the forward and 
backward prediction as follows [1] 
                
, 1 , 1 , 1 ,
*( , ) ( ) ( ) ( )Ib N b N b N p fe n n f e n l n e n f+ + +− = + −       (6) 
 
, 1 , 1 , 1 ,
*( , 1) ( ) ( ) ( 1)If N f N f N p fe n n f e n l n e n f+ + +− − = + − −    (7) 
 
where , 1( )b Nl n+  and , 1( )f Nl n+  are scalar coefficients; 
, 1( )b Ne n+  and , 1( )f Ne n+ , which are backward and forward 
prediction errors, respectively, are directly accessible 
from an ( 1)stN order+ −  LSL predictor that can be 
embedded into an LSL interpolator. Notably, both 
, ( )
I
p fe n f−  and , ( 1)Ip fe n f− −  are already computed 
from the previous interpolation lattice stage of the LSL 
interpolator. 
                
2.2 The proposed RLS algorithm 
The ( 1)stN order+ −  adaptive filter in the RLS algo-
rithm with the tap-weight vector at time n, 1( )N n+w , can 
be calculated recursively in time via [2].                    
*
1 1 1( ) ( 1) ( ) ( )N N Nn n n nς+ + += − +w w k          (8) 
where 
1
1 1 1( ) ( ) ( )N N Nn n n
−
+ + +=k Φ x                (9) 
                         
is the Kalman gain vector and               
1 1( ) ( ) ( 1) ( )
H
N Nn d n n nς + += − −w x  
              
is termed the a priori estimation error in which d(n) is 
the desired response. Evidently, the majority of the 
computation of the RLS algorithm is in the calculation 
of the Kalman gain vector in (9) that is required at each 
time step to update the filter weights. As mentioned 
earlier the Kalman gain vector can be calculated as a 
particular set of normalized least-squares interpolation 
errors (i.e., the set of normalized a posteriori LS 
interpolation errors of ( , )thp f order−  by adjusting 
the values of p and f with p f N+ = ) [3]. Herein we 
adopt an approach different from [3] to show this fact. 
From (3), we have  
             
1
, 1 ,( ) ( ) ( )p f N p fn f n n f
−
+− = −b Φ i  
  
The above equations summarize the complex version of 
the SRF Givens rotation with feedback. 
                 
2.4 SRF QRD-LSL interpolation algorithm 
                    
The SRF QRD-LSL interpolation algorithm summarized 
in Table I consists of six blocks, i.e., ①  forward 
prediction (FP) ②  backward prediction (BP) ③ 
intermediate forward prediction (IFP) and interpolation 
[Int(F)] as ( ) ( ),   , 1p f p f→ +  ④ intermediate back-
ward prediction (IBP) and interpolation [Int(P)] as 
( ) ( ),   1,p f p f→ + . Both FP and BP must always be 
executed to compute both forward and backward predic-
tion errors from which a SRF QRD-LSL predictor can 
be constructed and then embedded into the underlying 
SRF QRD-LSL interpolator. 
As an additional “future” stage is increased, then 
both IFP and Int(F) must be executed to compute the a 
posteriori interpolation error 
 
 , 1 , 1 , 1( 1) ( 1) ( 1)
I I
p f p f p fe n f k n f n fε+ + +− − = − − ⋅ − −  
 
where throughout this paper the notations “e” and “ ε ” 
represents the “a posteriori” and “SRF an-
gle-normalized” versions of the estimation errors, 
respectively. As an additional “past” stage is increased, 
then both IBP and Int(P) must be executed to compute 
the a posteriori interpolation error 
 
1, 1, 1,( ) ( ) ( )
I I
p f p f p fe n f k n f n fε+ + +− = − ⋅ −        
 
The SRF QRD-LSL interpolation algorithm can be 
derived from the SR QRD-LSL interpolation algorithm 
[1] by employing SRF Givens rotation with feedback. A 
detailed derivation is lengthy and, therefore, is omitted 
herein. However, a quick outline of the derivation may 
be grasped by letting 2i =  and 
 
, ( 2)a p fk I n fλ= − − , ,
' ( 1)a p fk I n f= − − , , ( 1)b p fk k n f= − − ,
2 , 1 ( 1)f Na n+= Δ − , 2 , 1
' ( )f Na n+= Δ , 1 , ( 1)
I
p fb n fε ∗= − − ,
2 , 1
* ( , 1)f Nb n n fε += − − , 2 , 1
*' ( )f Nb nε += , 
 
and employing the SRF Givens rotation with feedback, 
then the IFP block of the SRF QRD-LSL interpolation 
algorithm can be derived as shown in Table I. Other 
blocks of the algorithm can be similarly derived. 
Notably, when employing 1' ,  2,...,i i ib b b a i p= − = , 
which can be written as 
 
, 1 , 1 , , 1( ) ( , 1) ( 1) ( 1)
I
f N f N p f f Nn n n f n f nε ε ε+ + +∗= − − − − − Δ −       
in order to compute the intermediate forward prediction 
error, we have “reversed” the above equation in 
formulation such that, given the forward prediction error, 
the intermediate forward prediction error can be com-
puted as 
    
 , 1 , 1 , , 1( , 1) ( ) ( 1) ( 1)
I
f N f N p f f Nn n f n n f nε ε ε+ + +∗− − = + − − Δ −   
This corresponds to the “reversed/modified cell” in [3] 
and is no longer with error feedback. It has been shown 
in [3] that the error growth in this reversed formulation 
is linear with time in nature in a finite precision environ-
ment. In contrast, the error growth in a square-root-free 
with feedback (SRFF) form is bounded for all time in a 
finite precision environment. Note that the FP, BP, 
Int(F), and Int(P) blocks belong to SRFF formulation 
whereas the IFP and IBP blocks belong to the reversed 
formulation. Owing to this reversed formulation, the 
SRF QRD-LSL interpolation algorithm may inherently 
exhibit linear error growth with time in a finite precision 
environment and the proposed RLS algorithm, which is 
based on the SRF QRD-LSL interpolation algorithm, 
may also exhibit error growth with time in a finite 
precision environment. However, our simulation results 
show that the proposed RLS algorithm using only 5, 6, 8, 
and 12 mantissa bits did not exhibit linear error growth 
and any sign of divergence. 
                        
3. Simulations 
 
We present results of computer simulations of adaptive 
equalization of a linear channel having unknown 
distortion. To observe finite-precision effects, the 
effective number of mantissa bits in the floating-point 
representation is reduced by truncating the mantissa at a 
predefined position without affecting the exponent. Note 
that in floating-point operations, addition and 
multiplication both introduce errors. A polar form 
pseudo-random signal ( )u n  is applied to a channel 
having unit pulse response [6] 
1 21+cos ( 2) , n=1,2,3
2 W
0 ,                                   
n
n   
h
otherwise
π⎧ ⎡ ⎤⎛ ⎞−⎪ ⎢ ⎥⎜ ⎟= ⎨ ⎝ ⎠⎣ ⎦⎪⎩   
where W was set equal to 3.5 to provide for an eigen-
value spread of 46.8216 corresponding to an 
ill-conditioned channel. The observation ( )x n  is the 
sum of the channel output and an independent white 
Gaussian noise with variance 0.001. The adaptive 
equalizer attempts to correct the distortion produced by 
the channel and the additive noise. In all simulations 
11-tap equalizers were used. Figure 2 depicts the error 
growth with time (by ensemble averaging 200 individual 
absolute difference between the p mantissa bit precision 
solutions using the proposed RLS algorithm and the full 
32-bit precision solution using the conventional RLS 
algorithm in logarithmic scale) of the a posteriori 
interpolation error 5,5 ( 5)
Ie n − , which is element 6 of the 
11-th order Kalman gain vector. Surprisingly, our 
simulation results show that although the error growth 
of element 6 increases with time at the beginning, it re-
mains bounded for the rest of the time and the final error 
is not large enough to make the proposed RLS algorithm 
  
          
2
1 1 1 , 1( ) ( 1) ( 1) ( )m m m f mF n F n k n nλ ε− − − −= − + −  
, 1
, 1 1
1
( )
( ) ( 1)
( )
f m
f m m
m
n
s n k n
F n
ε −
− −
−
∗
= −  
, , 1 , 1 , 1( ) ( 1) ( ) ( 1)b m b m f m b mn n n nε ε ε π− − −∗= − − −  
, 1 , 1 , 1 ,( ) ( 1) ( ) ( )b m b m f m b mn n s n nπ π ε− − −∗ ∗= − +  
             
B. Interpolations      
            
B.1.  ( ) ( ), , 1p f p f→ +    
                          
B.1.1.  IFP block 
      
, ,
2
, ,
,
, ,
,
, 1 , 1 , , 1
( 1) ( 2)
                         ( 1) ( 1)
( 1)
( ) ( 1)
( 1)
( , 1) ( ) ( 1) ( 1)
p f p f
I
p f p f
I
p f
I N p f
p f
I
f N f N p f f N
I n f I n f
k n f n f
n f
s n k n f
I n f
n n f n n f n
λ
ε
ε
ε ε ε+ + +
∗
∗
− − = − −
+ − − − −
− −= − − − −
− − = + − − Δ −
 
, 1 , 1 , , 1( ) ( 1) ( 1) ( )f N f N I N f Nn n s n nε+ + +∗ ∗Δ = Δ − + −  
          
B.1.2.  Int(F) block 
              
1 1
2
, , 1
( , 1) ( 1, 2)
                             ( 1) ( , 1)
N N
p f f N
F n n f F n n f
k n f n n f
λ
ε
+ +
+
− − = − − −
+ − − − −  
1
, 1
1
, 1
, 1 ,
1
'
'
( 1, 2)( )
( , 1)
( , 1)
( ) ( 1)
( , 1)
N
f N
N
f N
f N p f
N
F n n fc n
F n n f
n n f
s n k n f
F n n f
λ
ε
+
+
+
+
+
+
∗
− − −= − −
− −= − − − −
 
, 1 ,
, 1 , 1
( 1) ( 1)
                         ( , 1) ( 1)
I I
p f p f
f N p f
n f n f
n n f n
ε ε
ε ρ
+
+ +
∗
− − = − −
− − − −  
, 1 , 1 , 1 , 1
'( ) ( 1) ( ) ( 1)Ip f p f f N p fn n s n n fρ ρ ε+ + + +∗ ∗= − + − −  
, 1 , 1 ,
'( 1) ( ) ( 1)p f f N p fk n f c n k n f+ +− − = − −    
B.2.  ( ) ( ), 1,p f p f→ +  
   
B.2.1.  IBP block 
 
2
, , , ,
,
, ,
,
( ) ( 1) ( ) ( )
( )
( ) ( )
( )
I
p f p f p f p f
I
p f
I N p f
p f
I n f I n f k n f n f
n f
s n k n f
I n f
λ ε
ε ∗
− = − − + − −
−= − −
 
, 1 , 1 , , 1( , ) ( ) ( ) ( 1)
I
b N b N p f b Nn n f n n f nε ε ε+ + +∗− = + − Δ −  
, 1 , 1 , , 1( ) ( 1) ( ) ( )b N b N I N b Nn n s n nε+ + +∗ ∗Δ = Δ − +  
 
B.2.2.  Int(P) block 
 
1 1
2
, , 1
( , ) ( 1, 1)
                        ( ) ( , )
N N
p f b N
B n n f B n n f
k n f n n f
λ
ε
+ +
+
− = − − −
+ − −  
1
, 1
1
, 1
, 1 ,
1
'
'
( 1, 1)( )
( , )
( , )
( ) ( )
( , )
N
b N
N
b N
b N p f
N
B n n fc n
B n n f
n n f
s n k n f
B n n f
λ
ε
+
+
+
+
+
+
∗
− − −= −
−= − −
 
1, , , 1 1,( ) ( ) ( , ) ( 1)
I I
p f p f b N p fn f n f n n f nε ε ε ρ+ + +∗− = − − − −  
1, 1, , 1 1,
'( ) ( 1) ( ) ( )Ip f p f b N p fn n s n n fρ ρ ε+ + + +∗ ∗= − + −  
1, , 1 ,
'( ) ( ) ( )p f b N p fk n f c n k n f+ +− = −  
 
C. Initializations 
      
C.1.  For order m = 1,2,…,M, 
    
, 1 , 1(0) (0) 0f m b mπ π− −= = , , ,(0) (0) 0f m b mΔ = Δ =  
, (0) 0p fρ = , for all p and f.    
   
C.2.  For order m = 0,1,…,M, 
    
( 1) (0) (0)m m mB B F δ− = = = , (0,0) (0,0)m mB F δ= =  
, ( )p fI n δ= , for 0 n ≤ and all p and f. 
where  is a small positive constant.δ  
     
C.3.  For n = 1,2,… 
 
,0 ,0( ) ( ) ( )f be n e n x n= = , 0,0 ( ) ( )Ie n x n=  
, ( )
I
p fe n 0,  for 0 and all  and .n p f= ≤  
0 ( )k n 1,  0,1,...n= = , 0,0 ( )k n 1,  1,2,...n= =  
, ( )p fk n 1,  for 0 and all  and .n p f= ≤  
