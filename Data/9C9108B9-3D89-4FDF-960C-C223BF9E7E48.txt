2 
目錄 
目錄..............................................................................................................................................2 
一、 中文摘要......................................................................................................................3 
二、 緣由與目的..................................................................................................................2 
三、 文獻探討......................................................................................................................2 
3.1. 目標物之偵測與辨識..............................................................................................2 
3.2. 移動抓取控制..........................................................................................................2 
四、 研究方法及成果..........................................................................................................1 
4.1. 全向式移動機械臂之設計......................................................................................3 
4.1.1. 座標轉換.........................................................................................................4 
4.1.2. 手臂順/逆運動學推導 ...................................................................................4 
4.1.3. 抓取動作路徑規劃.........................................................................................6 
4.2. 目標物辨識..............................................................................................................7 
4.2.1. 特徵點擷取與比對.........................................................................................8 
4.2.2. 定義中心點.....................................................................................................8 
4.2.3. 取得中心點 3D座標......................................................................................9 
4.3. 視覺伺服控制器設計............................................................................................10 
4.4. 實驗結果................................................................................................................12 
4.4.1. 平台移動抓取實驗.......................................................................................12 
4.4.2. 自主式抓取與避障整合測試.......................................................................13 
五、 結論與討論................................................................................................................15 
六、 參考文獻....................................................................................................................15 
2 
 
二、 緣由與目的 
相較於過去傳統僅在工廠中固定式的工
業機器人，現在已經有越來越多的服務型機
器人開始走進人類的生活。帶有手臂的機器
不僅可以協助人類搬運東西，更可以進一步
處理人類所賦予的各種任務。然而工業用機
器手臂主要是運作在結構化的環境中，若要
能讓機器人實際走入人類的居家生活中，其
不僅需要擁有足以應付非結構化而多變的環
境的能力，還需要進一步提升各項與人互動
的能力。其中一項關鍵技術是結合視覺之自
主移動抓取物體，此項技術讓具有手臂之機
器人藉由影像所提供的豐富資訊可以辨識並
抓取一般環境中的各種物體; 而抓取能力結
合移動能力，更可將整個工作範圍擴大，可
以隨著人們的需求移動至環境中的不同地
方，執行取物等功能。 
過去機器人影像伺服控制之研究大多集
中在僅是針對固定式的手臂進行抓取物體的
控制，並沒有考慮到機器人本身所具有的可
移動性與環境之不確定性，現有許多研究成
果無法運用在現實的生活中之移動式抓取。
本計畫之主要目的即在發展一套具備影像處
與辨識系統，以解決移動式抓取的問題，其
技術內容包括：擷取影像中物體特徵、在環
境中搜尋辨識物體、影像伺服控制等。綜合
運用這些能力，能確保機器人在環境中能搜
尋到目標物，並且能夠透過適當的移動到達
目標物所在位置，進而協助人類拿取所需物
體，並與人們互動。因此本計畫將逐步發展
出一套具備影像處理與辨識的移動式雙臂機
械人，以達到能在現實生活環境中進行自主
式抓取物體，進而協助人類拿取特定物體與
人們互動。 
三、 文獻探討 
要完成一個移動式機械臂之基於視覺定
位抓取物品之行為，必須考慮之設計重點包
含(1)目標物之偵測與辨識和(2)移動抓取控
制。有了這些能力的綜合運用，才能確保機
器人在環境中能搜尋到目標物，並且能夠透
過適當的運動控制到達目標物所在位置，進
而達到抓取該目標的目的。 
3.1. 目標物之偵測與辨識 
近年來目標物體辨識的研究主要都已如
Scale-invariant feature transform (SIFT)[1]、
Speeded Up Robust Features (SURF)[2]等強健
特徵點偵測比對演算法為基礎。文獻[3-5]以
立體攝影機擷取影像資訊將目標物周邊的環
境建立起來，知道了整體環境情況之後就能
針對抓取最適當的路徑規劃。文獻[3]除了利
用 SIFT 的特徵點資訊之外還加上了幾何的
特徵，將物體的邊緣描繪出來，如此一來目
前影像中的環境資訊就能更清楚的顯示出
來，此外作者們還提出了一個方法能夠根據
目前影像來決定下一時刻最好的視角位置。
文獻[4]所用的方法是將所有物體的以方塊的
型式描繪出來，再將目標物標色，模擬出機
械手臂抓取的路徑，找出最適當的抓取角
度。文獻[5]能夠做到即時的在整個工作區域
當中辨識出目標物並將其替換成三維模型。
文獻[6]以 SIFT 作為其能夠在複雜環境中辨
識出物體的方法，以三個目的建立其視覺系
統，分別是辨識目標物，追蹤目標物及克服
複雜環境。文獻[7]發展出一套結合反投影直
方圖及 SURF 的辨識方法，讓救援機器人能
夠在一個未知的環境中辨識出危險標誌，自
我定位，建出環境地圖使能對災害現場有一
個參考的依據。Changhyun Choi等人提出一
套可以同時用在單眼視覺跟立體視覺之中的
即時目標物姿態估測跟追蹤方法[8]，作者們
採用目標物上的自然特徵，以這些特徵當作
追蹤點，使攝影機在不同角度的情況下，都
能將目標物的姿態清楚的描繪出來。Tomono
等人提出利用一個移動式機器人建立整體環
境的物體地圖，其藉由雷射掃描整個環境，
建立出整體二維環境的地圖，接著透過 dense 
shape model 的方式建立物體的模型再加上
SIFT的特徵點擷取，便可以判斷出該目標物
以及所在位置[9]。 
3.2. 移動抓取控制 
在移動抓取的設計上，Yamazaki等人利
用 Structure from motion (SFM)描繪物體的
3D model[10]，並描述物體每個像素上的
Oriented points，但亦由於資料量較大，因此
透過 Voxel model 將資料量降低，並計算在
滿足所設定的條件下，所有可能的抓取方
式，並從中找尋最佳的姿態 (Pose) 。
Christensen 等人發展出一具備定位、導航、
4 
上的需要去撰寫程式。 
4.1.1. 座標轉換 
 本計畫中所使用的實驗平台由數個部分
組合起來，包括立體攝影機、機器人頭部、
機械手臂，移動平台，為了整合機器人的各
個部分需要知道每個座標間的轉換關係，以
利於整體的控制設計。機器人的座標原點定
在雙臂肩關節的連線，跟頭部旋轉軸的交叉
點，其中攝影機是主要的資訊來源，所以必
須知道攝影機座標到手臂及平台間的座標轉
換。整個移動式機械臂之間的座標關係，如
圖 4所示，其中的符號定義如下，C：立體視
覺攝影機、B：機械手臂本體中心、E：機械
手臂的手腕點、G：機械手臂的夾爪末端、P：
全向式移動平台、O：欲抓取的目標物體，
而 A
B
H 矩陣代表的意義是從A座標系轉換到
B 座標系之轉換矩陣。我們得知了機器人各
個部位的座標轉換關係之後，就能夠針對控
制的需求，將命令轉換到適當的座標系，例
如:當機器人位於機械手臂的工作範圍之外
時，我們就必須將攝影機所觀測到的座標轉
換到移動平台的座標系，給出適當的命令，
導引平台移動到手臂的工作範圍之內。 
4.1.2. 手臂順/逆運動學推導 
基於雙臂機器人之對稱性，我們只需針
對單邊機械手臂來進行分析。此處將針對右
臂進行運動學的分析。在計算上，採用的是
Denavit-Hartenberg(DH)參數來建立出機器手
臂的模型，這是一種較為受限的運動學模型
建立方式，但對於手臂的操作有相當好的效
果。利用 DH model參數所表示的座標系之
間，其轉換矩陣為： 
圖 3、機器人硬體系統架構圖 
 
圖 2、機器人實體 
6 
構成的三角形的面積
r
Δ r
 
( ) ( ) ( )r wrist u fs s r s r s rΔ = ⋅ − ⋅ − ⋅ −r
r r r
 
(3) 
其中
( )
2
wrist u fr r r
s
+ +
=
r r r
 
可解出: 
2
r
u
wrist
R
r
Δ
=
⋅
r
r
r
 
(4) 
接著藉由
u
R
r
作為媒介可以得到肘位置
到肩膀與手腕連線的的直線上面的正交的點
m
p 。其中 
22 2
2
( ) ( ) ( )
2
u f wrist
rm
wrist
r r r
t
r
− +
=
⋅
r r r
r  
(5) 
接著定義通過
m
p 且平行於水平面的α向
量。由於是平行於水平面的向量，所以 z分
量一定為零。再來，定義 y分量為 factorC ，以
右手為例，Y軸正向的定義是朝左邊(如圖 6
所示)，所以 factorC 定義為”-1”，若為左手的話
則 factorC 定義為”1”)。而我們定義α向量要垂
直
wrist
r
r
，因此就可以求出α向量
[ ( ) , ,0]
wy
factor factor
wx
p
C C
p
α ⋅= −
r
 
(6) 
再來由於β向量同時垂直於α向量與
w ris t
r
r
向量，所以利用α向量與
w ris t
r
r
向量做外
積就可以求得β向量 
wrist
rβ α= ×r r r
 
(7) 
再由α向量與β向量做向量加法後可得
到γ向量，我們可以透過α向量與β向量的
係數 A與 B來控制γ向量的方向，而由前面
已知
u
R
r
的長度，可以算出γ向量與
u
R
r
向量
的倍數關係，因此就可以求得手肘的位置
( , , )
ex ey ez
p p p 。接著就可以分別推導出
1 2 3
, ,θ θ θ 如下: 
1
tan 2( )ex
ez
p
a
p
θ −=
−
 (8) 
1
2
cos
tan 2( )
ey
ez
p
a
p
θ
θ =
−
 (9) 
2 1 2 2 1
3
1 1
sin sin cos sin cos
tan 2( )
cos sin
wx wy wz
wx wz
p p p
a
p p
θ θ θ θ θ
θ
θ θ
+ +
=
−
      (10) 
在推導手腕的逆運動學方面，所需之各
向量間的關係如圖 8所示，首先由肩膀當作
原點a，加上已知的座標有手軸位置b跟手腕
位置 c，將此三點組成一個平面
1
E
uuv
，算出此
平面的法向量
1
n ba ca= ×
ur uur uur
。接著由空間中向
量跟平面的夾角關係，如圖 9所示，求出手
腕兩自由度所需旋轉的角度。在抓取的任務
當中，本計劃所採用的方法是假設瓶子直立
於桌面的情況下來抓取，所以我們希望手掌
部分保持跟地面平行，也就是跟 XY平面平
行，以便於在抓取目標物的時候能夠穩定的
夾住物體。在求得
1
V
ur
跟
1
n
ur
之後，分別求出與
XY平面法向量( ( )0, 0, 1Z =ur )的夾角，就
可以得到手腕的兩自由度在使手掌平行 XY
平面時，所需要的角度。(11)是手腕旋轉的角
度，(12)是手腕上下轉動的角度。 
1 1
5
1
sin
| | * | |
n Z
n Z
θ −=
ur ur
ur ur  (11) 
1 1
6
1
sin
| | * | |
V Z
V Z
θ −=
ur ur
ur ur  (12) 
 
4.1.3. 抓取動作路徑規劃 
 由於機械手臂的機構限制，機器人無法
在每次抓取目標物時都保持夾爪正對著目標
物，也就是手掌平行 X軸垂直 Y軸的方向，
因此在抓取方面我們需要在手爪到達
pre-grasp 位置之後，做最後夾取的路徑規
 
圖 7、由手臂構成的三角形 
圖 8、α
r
向量與各向量關係 
8 
面轉換矩陣包含著縮放及旋轉因子，所以我
們可以由這個平面轉換矩陣準確的框出目標
物在左右攝影機影像中的區域，接著再取其
四個角點的平均，就可以得到一組穩定的對
應點，也就是目標物的中心點，由這組中心
點就能算出控制所需要的穩定參考點。 
4.2.1. 特徵點擷取與比對 
而在本研究中，我們期望能在較少的先
期資訊之下，不必耗費很多時間在建立資料
庫影像。利用物體本身所具有特徵點的辨識
方法上，其優點在於可以運用簡單的辨識方
法達成，以利於機器人在即時的環境下進行
反應。尤其對於辨識環境中 3D的物體而言，
還需考量如何可以盡量達到不受物體旋轉、
視角改變以及遮蔽物的影響，正確的辨識出
物體。 
本計畫採用了在運算上較為快速的
SURF 來作為本計畫的特徵點擷取演算法。
然而雖然經過 SURF 處理後的特徵向量具有
高度的差異，但其亦有可能因為來自於背景
或者是雜訊恰巧相同等情形出現錯誤比對的
特徵點。由於成對的特徵點在之後 3D 座標
計算上扮演著非常重要的角色，所以我們必
須提高這些特徵點的品質，追求更加穩定跟
準確的結果。本計畫在此採用 RANSAC 跟
homography來做特徵點的二次篩選加強最後
辨識的結果。其演算步驟簡述如下: 
1. 隨機挑選 4 對以上在第一階段比對
成功的對應點，求出目前影像跟資
料庫影像之間的平面轉換矩陣 H。 
2. 將所有對應點使用步驟 1 求得的平
面轉換矩陣進行位置轉換，將資料
庫影像的對應點代入轉換矩陣得到
另一組配對點，之後計算這些經過
轉換的配對點座標與原本目前影像
中的配對點座標之間的距離。 
3. 統計步驟 2 中所有配對點間的距
離，其配對點的距離若小於某個自
訂的臨界距離Dτ，則表示此組配對
點符合這個平面轉換矩陣，然後統
計這個平面轉換矩陣的正確配對數
量。 
重複上述步驟多次之後，若某次的正確
配對點數量最多，且大於某個自訂的臨界數
量 Sτ時，則採用此次的平面轉換矩陣，再將
這些配對點拿去求得最終的平面轉換矩陣。 
 在本計畫中，參雜雜訊的資料樣本就是
第一次特徵比對的結果，如圖 13所示，可以
看出當中存在在錯誤比對的特徵點，而模型
就是轉換矩陣 (Homography matrix)，將
RANSAC作為計算轉換矩陣的演算法，如此
一來就可以得到對應正確比例最高的一群特
徵點，用 Homography 來框出在目前影像平
面上的目標物時，位置也能更加準確，如圖
14，剔除錯誤比對特徵點之後，再將剩餘比
對成功的特徵點拿來算轉換矩陣，就可以準
確的框出目標物的位置，在分別對左右攝影
機影像目標物的四個角點取平均後，就能得
到中心點，在平面轉換矩陣計算的相當準確
的情形下，目標物的範圍可以清楚的被框出
來，所以用這個範圍的平均位置來當作參考
點是比較穩定的結果。將這對中心點代入三
維座標估測公式，如此一來就能夠得到穩定
的控制參考點，將會對整個移動式抓取的控
制上有很大的助益。 
實際的運算結果如圖 15及圖 16所示，
我們將立體攝影機所讀取到的左右兩個影
像，分別跟資料庫影像做前述的處理，框出
目標物後，分別求得目標物在左右攝影機影
像的中心點，接著將這對中心點代入第三章
將會提到的三維座標估測公式，就可以知道
目標物在世界座標中的位置，做為導引機器
人往前抓取目標物的控制命令輸入。 
4.2.2. 定義中心點 
在移動式抓取的研究上，必須將機器人
本身所有的部件彼此間的座標轉換關係弄清
100 200 300 400 500 600
50
100
150
200
圖 13、用 SURF做特徵比對的結果 
100 200 300 400 500 600
50
100
150
200
圖 14、用 RANSAC做 homography 
estimation的結果 
10 
又因為左右攝影機看到的 Z座標值皆相同 
2 1
z z z= =  (22) 
將(21)代入(20)得到 
2
1 2
( )
u
x B zλλ+ = −  (23) 
整理之後，就得到 Z座標的公式 
l r
B
z
u u
λλ= −
−
 (24) 
然而，以前述的公式求出對應點的世界
座標與實際位置之間仍存在誤差，因此我們
透過實際量測來校正 3D 座標，以更精確得
到目標物在空間中實際的三維座標。 
4.3. 視覺伺服控制器設計 
機器人抓取功能的核心在於如何利用攝
影機所取得的影像來導引移動式機械臂手爪
到達目標所在位置，實際上是視覺伺服
(Visual servo)的問題[17,18]。依照控制器輸入
資訊的不同，可分為兩類，分別是基於位置
的 視 覺 伺 服 (Position-based visual servo, 
PBVS)和基於影像的視覺伺服(Image-based 
visual servo, IBVS) [19,20]。在 PBVS 架構
中，會先藉由影像估測目標物體之位置與姿
態，以此建立機器人抓取物體時所需要的位
置與姿態，再導引控制機器人直到機器人當
前姿態符合所預定的姿態。而在 IBVS 的架
構中，則藉由攝影機取得影像後，經過特徵
點擷取得到目前目標的特徵點座標，將其直
接與事先所定義好的目標特徵點座標進行相
減，便可以得到兩者之間的誤差；利用此誤
差作為特徵空間中的輸入訊號，藉此產生適
當的控制訊號，以修正攝影機與目標物之間
的位置，達成將機器人導引至最後預定到達
的位置。 
本研究採用視覺伺服控制為 PBVS 架
構，藉由計算出空間中實際的座標關係來對
機器人做控制。與一般 PBVS 架構的不同地
方在於本計畫針對欲抓取的目標物並不是先
建立其模型，而是建立一張目標物的影像於
資料庫中，作為特徵點比對的依據，藉由整
個目標物辨識與座標估測，可以算出所有對
應點的 3D 座標。至於要如何從這群座標中
挑選出抓取所需要的參考點，本計畫引入了
[20]所提到中心點(Center point)的概念，運用
Homography 可以框出在左右攝影機影像平
面中目標物的位置，並以此二方框的中心點
當作我們抓取所需的參考點，由這組對應的
中心點所得到的 3D 座標，來當作機器人跟
目標物位置關係的依據，藉由軸編碼器回傳
手臂位置跟攝影機量測目標物的位置之間的
誤差是否已經小於某一個誤差值，來判斷是
否完成抓取的任務。 
機器人的抓取行為模式如圖 20所示，由
於機器人並非一開始就位於機械手臂的工作
範圍之內，目標物也可能會移動，所以針對
這些情況在影像伺服的過程中設計了不同的
模式，機器人根據目前辨識的結果選擇最適
當的模式來操作。 
 
( , )
l l l
p u v
( , )
r r r
p u v
( , , )w x y z=
 
圖 18、立體影像模型[16] 
 
B
X
Zλ
λ
Left image
Right image
Orign of world 
coordinate system
( , )
l l
u v
( , )
r r
u v
( , , )w x y z=
圖 19、攝影機跟世界座標對應上視圖[16]
12 
是否開始執行抓取的動作等等。當移動抓取
完成時，所有的位置都跟我們的估測結果一
致，誤差達到最小，任務完成。 
4.4. 實驗結果 
4.4.1. 平台移動抓取實驗 
此實驗主要目的在於驗證當機器人判斷
目標物所在位置超出機械手臂工作範圍時，
機器人會控制移動平台向前移動到機械手臂
的工作範圍之內，再進行最後的抓取任務。
實驗環境方面，目標物離地約 105 公分，機
器人距離目標物約為 60cm。圖 22 為建立目
標物資料庫之特徵點影像。在實驗的過程中
透過攝影機讀取目前的影像跟資料庫的影像
做比對，算出對應的特徵點之後，由RANSAC
得到平面轉換矩陣，接著框出目標物在影像
平面上的位置，以此框框的平均位置當作中
心點，將左右眼的中心點代入座標估測公
式，求出目標物在空間中的座標，重複上述
步驟，不斷的更新，當辨識的結果穩定後進
入到抓取的模式，移動機械臂到目標物前
方，並做抓取路徑規劃，手爪沿路徑前進之
後，闔上夾爪，完成任務。 
在本實驗中，我們在機器人前方放置一
個瓶子，作為抓取的目標物，而這次瓶子放
置在離機器人約 93公分處，離地高度為 105
公分。 
整個實驗過程中，從開始到完成抓取動
作，所計算出目標物的中心點在影像平面上
的位置記錄下來。在起點時攝影機擷取到的
畫面如圖 23，此時計算出目標物的中心點位
置如圖 24，是在整個影像平面上半部的位
置。由於此時算出機器人跟目標物的距離超
出手臂工作範圍，於是計算出要移動的距離
之後，控制平台往前移動。到達工作範圍後，
攝影機所截取到的畫面如圖 25，此時紀錄的
中心點位置如圖 26，中心點在影像平面上的
位置向下移動。由於攝影機傾斜的角度固
定，機器人在移動的過程中，中心點在影像
平面上的位置也有所變化，目標物在影像平
面上的位置隨著距離的減少，y值有所變動，
而 x 值則變化不大，顯示出機器人是往正前
方移動。在座標量測方面，隨著機器人的移
動，目標物在影像平面上的位置有所變動，
大致分為兩個區間，分別是在起點時，及到
達工作範圍之後，所以在圖 27中數據有一個
  
(a)                (b) 
圖 22、(a)資料庫物件及(b)其特徵點位置
圖 23、移動抓取實驗中，起點時攝影機所
擷取的畫面及目標物位置 
圖 24、攝影機在起點所量測出之目標物在
影像平面上的中心點 
圖 25、移動抓取實驗中，到達工作範圍時
攝影機所擷取的畫面及目標物位置 
圖 26、攝影機在工作範圍所量測出之目標
物在影像平面上的中心點 
14 
實際機器人移動抓取時的情形則如圖
31(a)-(h)所示。一開始機器人並未偵測到目標
物體，因此機器人往前移動並搜尋目標物，
直到偵測到目標物位置後將其設定為目的地
並前往抓取。過程之中機器人透過避障導航
系統閃避周圍障礙物，最到達目的地並開始
控制手臂抓取物體。抓取過程的移動軌跡如
圖 32所示，抓取時手臂的移動軌跡則如圖
33所示。以上實驗的影片可以於下列網址下
載: 
物體辨識
當前目標
資料
已學習之
物體資料
影像伺服追蹤
避障導航行為
目標物體姿態
與位置估測
特徵萃取
行為融合
全向移動平台
運動控制器
手臂運動控制器
雷射掃瞄儀
物體特徵擷取與影像辨識攝影機
影像擷取
與前處理
移動抓取行為
機器人運動控制系統
環境資訊萃取
與前處理
物體相對位置座標(x,y,z)
 
圖 30、自主式抓取與避障整合系統架構 
0
50
100
150
200
250
300
350
400
-200 -150 -100 -50 0 50 100 150 200
y
x
(a)
(b)
(c)
(d)
(e)
(f)(g)(h)
圖 32、抓取過程中機器人軌跡圖 
(單位:cm) 
 
圖 33、抓取過程中手臂各軸軌跡圖 
(單位:mm) 
(a)      (b) 
(c)      (d) 
(e)      (f) 
(g)      (h) 
圖 31、平台自主避障移動抓取實驗 
16 
Manipulation,” in Proceedings of IEEE 
International Conference on Intelligent 
Robots and Systems, San Diego, CA, USA, 
2007, pp. 1320-1325. 
[13] T. Grundmann, R. Eidenberger, R. D. 
Zoellner, X. Zhixing, S. Ruehl, J. M. 
Zoellner, R. Dillmann, J. Kuehnle and A. 
Verl, “Integration of 6D Object 
Localization and Obstacle Detection for 
Collision Free Robotic Manipulation,” in 
Proceedings of IEEE International 
Symposium on System Integration, Japan, 
2008, pp. 66-71. 
[14] P. Azad, T. Asfour and R. Dillmann, 
“Stereo-Based 6D Object Localization for 
Grasping with Humanoid Robot Systems,” 
in Proceedings of IEEE International 
Conference on Intelligent Robots and 
Systems, San Diego, CA, USA, 2007, pp. 
919-924. 
[15] S. Benhimane and E. Malis, 
“Homography-based 2D Visual Tracking 
and Servoing,” International Journal of 
Robotics Research, 2007, pp.661-676. 
[16] K.S. Fu, R.C. Gonzalez, C.S.G. Lee, 
Robotics Control, Sensing, Vision, and 
Intelligence, McGraw-Hill Book Company, 
1987. 
[17] Richard P. Paul, Robot Manipulators: 
Mathematics Programming and Control, 
Cambridge, MA: MIT Press, 1981.  
[18] A. Ude, D. Omrčen, and G. Cheng, 
“Making Object Learning and Recognition 
an Active Process,” International Journal 
of Humanoid Robotics, vol. 5, pp. 267-286, 
2008 
[19] H. Fujimoto, L. C. Zhu and K. 
Abdel-Malek, “Image-based Visual 
Servoing for Grasping Unknown Objects,” 
in Proc. of 26th Annual Conference of the 
IEEE Industrial Electronics Society, 
Nagoya, Japan, 2000, pp.876-881. 
[20] K.T. Song and C.H. Yeh, “Behavior-Based 
Mobile Manipulation Using Real-Time 
Vision,” in Proc. of 13
th
 IEEE 
International Conference on Advanced 
Robotics, Jeju, Korea, 2007, pp. 791-796. 
[21] K. T. Song and J. Y. Lin, “Behavior Fusion 
of Robot Navigation Using a Fuzzy 
Neural Network,” in Proc. of IEEE 
International Conference on Systems, Man 
and Cybernetics, Taipei, Taiwan, 2006, pp. 
4910-4915. 
 
 
 

其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
2008年上銀智慧機器手 競賽佳作獎 
2010年上銀智慧機器手 競賽佳作獎 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
