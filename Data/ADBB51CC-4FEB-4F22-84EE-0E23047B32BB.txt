摘要 
支持向量群聚演算法因為可將資料以任意形狀輪廓來呈現群聚結果之特殊表現，在理
論發展和實際應用上已經被廣泛地研究。支持向量群聚演算法包含三個主要的步驟: 1) 藉
由解最佳化對偶問題來找到ㄧ個超球體、2) 將資料點根據群聚的結果作正確地歸類分配、
以及 3)藉由調整核心函數的參數來找到一個令人滿意的群聚結果。由於這三個步驟的限制
使得支持向量群聚演算法在處理大量資料上變得沒有效率。根據上述的問題，本計畫提出
了兩個不同的機制來簡化支持向量群聚演算法的運算複雜度。首先，本計畫提出了一有效
率的支持向量選擇機制，並藉此機制發展了以支持向量為基礎的群聚演算法。在支持向量
的選擇機制中，運用了資料點與資料點在高維空間中的相關係數及距離，以去除內部點為
首要目標，一旦去除內部點，外部點即可視為邊界點去建構出各群集的輪廓線。接著，本
計畫亦提出一有效率的資料前處理方法消除訓練資料中不重要的資料，並且不會影響到最
後的群聚架構。由於資料量的減少，支持向量群聚演算法在解最佳化問題與群聚歸屬分配
的運算負擔也會大幅降低。經由電腦模擬結果可以發現，本計畫所提出之有效率的支持向
量選擇機制，在支持向量選擇上的結果是良好的，而且運算時間相較於原始支持向量群聚
演算法來得短許多。而另一本計畫所提出之有效率的資料前處理法在電腦模擬中，亦能大
大改善支持向量群聚演算法的效能並加速其訓練過程。截至目前為止，我們已經投稿且發
表我們目前的研究成果在幾個國際研討會與期刊上。 
期刊論文: 
1. Jeen-Shing Wang and Jen-Chieh Chiang, “A cluster validity measure with a hybrid 
parameter search method for support vector clustering algorithm,” Pattern Recognition, 
vol. 41, no. 2, pp. 506-520, 2008. 
2. Jeen-Shing Wang and Jen-Chieh Chiang, “A cluster validity measure with outlier 
detection for support vector clustering,” IEEE Transactions on Systems, Man, and 
Cybernetics-Part B, vol. 38, no. 1, pp. 78-89, 2008.  
3. Jen-Chieh Chiang and Jeen-Shing Wang, “An efficient data preprocessing procedure for 
SVC algorithm,” Journal of Universal Computer Science, vol. 15, no. 4, pp. 705-721, 
2009. (NSC 97-2221-E-006-157) 
國際研討會論文: 
1. Li-Ying Wu and Jeen-Shing Wang, “A simplified support vector clustering algorithm,” in 
Proc. of 2008 IEEE International Conference on Systems, Man & Cybernetics, 
Singapore, Oct. 12-15, 2008, pp. 1259-1264. (NSC97-2221-E-006-157) 
Abstract 
Support vector clustering (SVC) has been widely researched in both theoretical development 
and practical applications due to its outstanding features—arbitrary-shaped cluster 
representations. SVC involves three main steps: 1) finding the hyper-sphere by solving the Wolfe 
dual optimization problem, 2) identifying the clusters by labeling the data points with cluster 
labels, and 3) searching a satisfactory clustering outcome by tuning kernel parameters. These 
three steps make using SVC to process large datasets inefficient and time-consuming. Based on 
the above problems, we propose two different procedures to reduce the computational complexity 
of SVC in this project. First, we present an effective support-vector-selection scheme, 
cross-correlation coefficients and feature distances are employed for eliminating interior points. 
In the proposed algorithm, eliminating as much as interior points is the prior task to conquer. 
After data point elimination, the remaining points are considered as the support vectors for 
constructing cluster contours. Next, an efficient data preprocessing procedure is proposed to 
eliminate insignificant data points from training datasets without significantly affecting the final 
cluster configuration. Since the size of dataset is reduced, the computational burden for solving 
the optimization problem as well as cluster labeling can be greatly decreased. In the simulation 
results on benchmark datasets, the results of an effective support-vector-selection scheme are 
satisfactory, and computation time is much less than that of SVC. In the simulation results, an 
efficient data preprocessing procedure also improves the performance of SVC and accelerates its 
training procedure. So far, we have submitted and published our current research results in 
several international conference as well as journal publications. 
Journal Papers: 
1. Jeen-Shing Wang and Jen-Chieh Chiang, “A cluster validity measure with a hybrid 
parameter search method for support vector clustering algorithm,” Pattern Recognition, 
vol. 41, no. 2, pp. 506-520, 2008. 
2. Jeen-Shing Wang and Jen-Chieh Chiang, “A cluster validity measure with outlier 
detection for support vector clustering,” IEEE Transactions on Systems, Man, and 
Cybernetics-Part B, vol. 38, no. 1, pp. 78-89, 2008.  
1.  Introduction 
Cluster analysis plays an important role in solving many problems for diverse scientific 
domains as well as commercial sectors. Clustering algorithms partition a data set into clusters or 
classes, where similar data are grouped into the same cluster and dissimilar data are grouped into 
different clusters. 
The support vector clustering (SVC) algorithm, first proposed by Ben-Hur et al. [1], [2], is a 
non-parametric clustering algorithm and automatically determines the number of clusters for a 
given dataset by a unified framework. The SVC algorithm was originated from support vector 
machines (SVMs) [3]. SVMs have attracted a great deal of attentions from many research 
communities because of many desirable properties, including fast convergence, good 
generalization performance and robustness for noises. SVMs are classified as a supervised 
clustering technique. That is, to employ this technique, the correct labels of data are assumed to 
be given, which may not be practical for some applications. As the result of this fact, an 
adaptation of SVMs into an unsupervised approach—the SVC algorithm becomes an appealing 
idea. In the SVC algorithm, data points are mapped from the data space to a high-dimensional 
feature space via a kernel transformation. The kernels can be selected from polynomials, 
Gaussian, or sigmoid functions. The idea of the SVC is to search the smallest sphere in the 
feature space that encloses the images of the data. The contours of clusters in the data space can 
be generated by finding the support vectors on the sphere surface in the feature space. These 
contours are interpreted as cluster boundaries. Moreover, the SVC algorithm possesses an extra 
salient feature—arbitrary-shaped cluster contours, whereas other algorithms use a fixed 
geometric representation [1]. The SVC algorithm involves three main steps: 1) finding the 
hyper-sphere by solving the Wolfe dual optimization problem, 2) identifying the clusters by 
labeling the data points with cluster labels, and 3) searching a satisfactory clustering outcome by 
tuning of kernel parameters. 
However, there is a common agreement in the SVC research community—solving the 
optimization problem and labeling the data points with cluster labels are time-consuming in the 
SVC training procedure. The above limitations make the SVC algorithm inapplicable for large 
datasets. Hence, reducing computational complexity of SVC is the main goal of this project.  
growing model which maps data points to a high dimensional feature space through a desired 
kernel function. The most important advantage of this extension over the existing SVC is that the 
multiple-sphere SVC is capable of identifying cluster prototypes through a membership 
computation procedure explicitly for further cluster analysis or classification. A new approach 
proposed by Sun and Huang [11] combined the SVC algorithm with a two layered neural network 
for multi-class classification problems. They selected a suitable width of Gaussian kernels 
according to different regions formed. The aim of their parameter selection was to choose the 
parameter that can produce an optimum region so that the performance of generalization is the 
best. However, the approach only considered the effect of the width of Gaussian kernels, and the 
selection was solely based on a trial-and-error process. The significance of the soft margin 
constant was ignored. Lee and Daniels [12] presented a secant-like numerical algorithm 
generating an increasing sequence of SVC kernel width values. The authors proposed an estimate 
of sequence length depends on spatial characteristics of the data instead of the number of data 
points or the data dimensionality. The proposed algorithm utilizes the relationship between the 
kernel width value and the radius of the minimal sphere enclosing the images of data points in a 
high-dimensional feature space. Although the authors applied their proposed algorithm only on 
low dimensional dataset, they indeed provided useful information of parameters. However, for 
further applications, large databases clustering should also be discussed. 
In the last decade, a number of clustering algorithms have been proposed for dealing with 
large databases [13, 14, 15]. These algorithms are capable of finding clusters with different 
shapes, sizes, densities, and even in the dataset with presence of noise and outliers. Although 
these algorithms can handle clusters with different shapes, they still cannot produce arbitrary 
cluster boundaries to adequately capture or represent the characteristics of clusters in the dataset. 
Although SVC can overcome the limitation of these clustering algorithms [2, 16], the 
effectiveness for dealing with large datasets is still a worthy research topic. Nath and Shevade [17] 
presented a novel approach that increases the efficiency of the SVC scheme. The geometry of the 
dataset was exploited to reduce the training data size. Their experiments showed that the 
preprocessing procedure drastically decreases the run-time of the cluster algorithm. However, 
different pre-specified parameters could produce totally different clustering results.  
feature space by a kernel function to find a smooth but tight contour. In general, Gaussian kernels 
are considered as the kernel function for the SVC [10], [18]. Using Gaussian kernels, we can 
comprehend that the data points in the feature space are enclosed by a hypersphere. According to 
the Lagrangian theorem, we can derive the following equation. 
2 2 2( || ( ) || ) ,Φ x aj j j
j
L R R                                                        (2) 
where βj is a Lagrange multiplier that is always bigger than or equal to 0. If we add a penalty 
term jC  for a better approximation, where C is a constant, (2) becomes: 
2 2 2( || ( ) || ) ,j j j j j j
j
L R R C            Φ x a

 (3) 
where μj is another Lagrange multiplier whose value is always bigger than or equal to 0. Taking 
the derivatives of (3) with respect to R, a, and j , we can get the following Lagrangian 
conditions: 
2 2 0 1,j j
j j
L R R
R
           (4) 
2 ( ) 2 0 ( ),Φ x a a Φ x
a j j j jj j
L            (5) 
1 0 .j j j j
j j jj
L C C   
              (6) 
By the Karush-Kuhn-Tucker (KKT) theorem, we can obtain 
0,j j     (7) 
2 2( || ( ) || ) 0.Φ x aj j jR         (8) 
  Based on the above derivations, we can derive the following conditions for the three 
categories of the mapped data points: 1) When 0i C   , 0i  , and 2 2|| ( ) ||i RΦ x a  , the 
image of a point xi lies outside the feature-space sphere, and is called a bounded support vector 
(BSV). 2) When 0iC   , 0i   and 2 2|| ( ) ||i RΦ x a  , the image of a point xi lies on the 
feature-space sphere, and is called a support vector (SV). 3) Otherwise, when βi = 0, the data 
point lies inside the sphere. 
According the Wolfe Duality, we can turn (3) into the following equation: 
  
(a) (b) 
Fig. 1. The simulation results using cross-correlation coefficients. (a) q = 0.6. (b) q = 0.65.
 
with other points. In [19], the authors utilized the cross-correlation coefficient to analyze the 
associative distribution. Also, Shrout and Fleiss [20] introduced the intraclass correlation 
coefficient for measuring differences between data. For extracting points that are more likely to 
be on the surface of the hypersphere, cross-correlation coefficients are used to analyze the data 
distribution in the feature space. As the aforementioned distribution properties, the data point 
having high cross-correlation with more points is much more likely to be inside the cluster. To 
examine which points possess more resembling mapping conditions, we should compute the 
cross-correlation coefficients, γ, as described in (12). 
2
2 .xy
xx yy
SS
SS SS
    (12) 
where var( ),  var( ),  cov( , )xx yy xySS N SS N SS N  X Y X Y , N stands for the number of data 
points here, and X and Y are denoted as the column vectors in the kernel matrix. Afterwards, data 
points that contain many higher correlation coefficients with other points are denoted as boundary 
vectors. The terms used to describe the strength of the correlations are as follows: a) strong, |γ| ≥ 
0.7; b) moderate, |γ| ≥ 0.5; c) weak, |γ| ≥ 0.3; and d) and no appreciable correlation, |γ| ≤ 0.3 [21]. 
Thus, 0.7 is set as the criterion to leave points out in this study. Here, the concept is adopted on an 
artificial dataset with two clusters in different densities. The dataset includes 20 data points, while 
10 distribute compactly and the other 10 are in looser phase. The clustering results are displayed 
in Figs. 1(a) and (b). In the following figures, ‘+’ is the original data points of the dataset and ‘◊’ 
represents the boundary vectors that are selected to form the cluster contour. 
where
1
.
M
i ij
j
n z

  zij indicates whether the data point xj belongs to the ith cluster. Specifically, if 
the data point xj belongs to the ith cluster, zij = 1; otherwise, zij = 0. 
Apparently, the main idea of (13) is similar to (11). The objective function provided in (13) 
is to minimize the feature distances between the mapped data points and the centroid of the 
feature space. The feature can be written as follows: 
2
1 1 1
2
1 1 1
2
1 1
( )
1 1 1( ) ( ) 2 ( ) ( ) ( ) ( )
2 1( ) ( ) ( ) ( ) ( ) ( )
2 1( , ) ( , ) ( , )
j i
M M M
j j j ik k ik k il l
k k li i i
M M M
j j ik k j ik il k l
k k li i
M M
j j ik k j ik il k l
k li i
v
z x z x z x
n n n
z x z z x x
n n
K z K z z K
n n
x
x x x
x x x
x x x x x x

  
  
 
 
          
         
   
  
 

1
.
M
k

 (15) 
Note that (15) is also similar to (11) in SVC. The major difference between (11) and (15) is 
that the values of zij represent the belongingness (or membership) to clusters. Searching the 
values in (15) is equivalent to solving the optimization problem in SVC. 
2.2.3  Support-Vector-Selection Scheme 
In the above proposed scheme, we can notice that although the preliminary scheme considered 
many properties in the feature space, the computational complexity is not low enough. In order to 
reduce the computational complexity, the composed algorithms are interchanged and the steps are 
reduced to compare with the preliminary scheme. From the evaluation of the performance of each 
step, the order of those algorithms employed is rearranged. First, from the discussion in Section 
2.2.1, the cross-correlation coefficients are capable of illustrating the data distribution both in the 
feature space and the corresponding data space. Figure 3 and Table 1 use the Bensaid dataset to 
display the relationship between the cross-correlation coefficients and the data distribution 
condition. The data points are numbered for comparisons with Fig. 3. There are two columns in 
Table 1, and the left column shows the numbered data points. In addition, the right column of this 
table represents the number of data points that have strong correlation, where the 
cross-correlation coefficient is over 0.7 with each corresponding numbered point. As the 
aforementioned ideas regarding the correlation coefficient presented in Section 2.2.1, the data 
 Fig. 4. The selected boundary vectors in Bensaid dataset. 
Table 2. The sorted feature distances and corresponding numbered data points 
Data point Dist. Data point Dist. Data point Dist. Data point Dist. Data point Dist. 
27 0.328 32 0.471 15 0.575 9 0.700 6 1.056 
24 0.334 13 0.477 10 0.581 20 0.701 5 1.059 
33 0.357 14 0.483 39 0.594 36 0.703 4 1.064 
29 0.370 38 0.487 41 0.610 46 0.703 2 1.067 
34 0.394 35 0.522 17 0.618 31 0.719 3 1.067 
21 0.396 40 0.524 37 0.648 43 0.722 1 1.073 
26 0.411 22 0.530 45 0.674 18 0.728 47 1.193 
16 0.412 25 0.537 7 0.683 8 0.734 48 1.195 
19 0.420 23 0.540 28 0.684 44 0.768 49 1.206 
30 0.445 42 0.549 11 0.696 12 0.812   
 
 From the above simulation results, we can conclude that the discussions in Section 2.2.1 are 
reasonable. The inner points are most likely to be eliminated if they have larger number of 
strongly correlated points. On the other hand, we can observe from Fig. 4 that most of the inner 
points are eliminated, and the remaining points are almost boundaries. Moreover, the boundary 
vector selection is effective from the cross-correlation coefficient approach, and the 
computational complexity of the cross-correlation coefficient is not high. As a result, the 
cross-correlation coefficient approach is set as the first step conducted in the proposed scheme. 
 After we employ the cross-correlation coefficient approach for eliminating inner data points, 
the feature distances are considered as another criterion for support vector selection. Table 2 and 
Fig. 5 also depict the corresponding data points and their feature distances computed via the 
kernel k-means algorithm, using (15). The 10 numbered data points selected with smaller feature 
 We can infer from Fig. 6 that the proposed scheme is effective and it can also lead to 
acceptable the support vector selection results.  
2.2.4  Parameters Searching and Contour Construction 
In the SVC algorithm, the optimization procedures are capable of searching for parameters and 
selecting the support vectors as well. In the proposed schemes, boundary points are selected first 
and then used for searching parameters. Thus, the proposed procedure for parameter searching 
should be ensured properly before conducting further procedures. The verification results are 
depicted in figures and the approach developed for parameter searching is introduced in the 
following subsections. 
A. Parameters Searching Mechanism 
 As introduced in Section 2.2.2, feature distances can be computed in (15). To enclose all the 
mapped data points within a hypersphere, the largest distance between the centroid and the 
mapped data points is selected as the radius of the sphere. Searching for the parameters that make 
the distances computed by the selected boundary data exactly the same as the radius is then 
conducted. That is, all the boundary points, xi, should satisfy the following equation: 
2
1 1 1
,( , ) 2 ( , ) ( , )
n n n
i i j j i j jk k
j j k
r K K K  
  
   x x x x x x  () 
where γi is the parameter that we want to search. Specifically, r is the mean value of feature 
distances calculated by (15) using boundary vectors selected. In the proposed mechanism, since 
we only used those chosen boundary vectors to search parameters, the number of equations is 
equal to the number of boundary vectors. On the other hand, in SVC, the number of parameters 
that need to be searched is the same as the number of data points in the dataset. Since we only 
searched the parameters that correspond to the boundary points in the feature space, the number 
of the parameters searched in the proposed approach is thus greatly reduced.  
If we compare (11) and (16), we can see that they are almost the same. In (11), however, the 
multipliers, βi, must be solved by the Wolfe dual optimization problem. In general, solving the 
optimization problem is time-consuming. Therefore, the proposed approach can exempt from this 
tedious procedure without much distortion in the clustering outcome. Nevertheless, in the last 
B.  Contour Determination 
As introduced previously, the radius of the hypersphere is used to form the circumference as 
the cluster contour in the feature space. This contour is then transformed back to the data space to 
form cluster contours in the data space. Ideally, if we can find γi’s to satisfy (16), we can find a 
perfect contour that depicts the hyper-circumference of the hypersphere connecting all the 
boundary vectors. All the data points located on the hyper-circumference are transformed back to 
the data space to form the corresponding cluster contours. To obtain these contours, the level set of 
(16) at r is setup. The detail of the concept of level sets can be found in [23]. In order to verify the 
validity of parameter searching mechanism to contour determination, the proposed scheme is fed 
by the support vectors selected by the SVC algorithm. Figures 7(a)-(c) are the example results that 
employed the proposed scheme onto the support vectors of three benchmark dataset. ‘ ’ denotes ◇
the selected support vectors, and ‘+’ is data point. Green and black lines are contours derived from 
the proposed scheme and the SVC algorithm, respectively. 
From Figs. 7(a)-(c), we can observe that the sets of contours constructed by the proposed 
scheme have high similarity to that of SVC. Through this verification, we can conclude that if the 
selected support vectors are exactly the same as that of the SVC algorithm, the contours derived 
would be the same as SVC. 
3  An Efficient Data Preprocessing Procedure for SVC 
Solving the optimization problem and labeling the data points with cluster labels are 
time-consuming in the SVC training procedure. This makes using the SVC algorithm to process 
large datasets inefficient. Thus, how to exclude redundant data points from a dataset is an 
important issue for minimizing the time spent in solving the optimization problem of the SVC 
algorithm. The research challenge in this topic is how to identify insignificant data points so that 
the removal of these data points does not significantly alter the final cluster configuration. This 
idea is to eliminate insignificant data points, such as noise and core points, from the training 
datasets, and use the remaining data points to do the SVC analysis. Due to the size reduction of 
the training datasets, the computational effort for solving the optimization problem can be greatly 
decreased. To fulfill the idea, the shared nearest neighbor (SNN) algorithm [24], [25] is explored 
11  1
1
( , )
,
km
i j
i j
similarity
m k
a = == ´
åå x x
 (18) 
where m is the total number of data points. If the strength, similarity(r, s), between the two points 
is greater than α, these two points are close to each other. 
Step 3: Removal of noise points. First, a threshold δ that is used to define low-similar data points.  
,mean stdS Sd= -                                               (19) 
where 
1
,
1  1=
km
i j
i j
mean
Count
S
m
= =
åå
,  (20) 
and  
1
1
2 2
,
1  1
1= .
1
km
std i j mean
i j
S Count S
m = =
ì üï ïé ùæ öï ï÷çï ïê ú÷-çí ý÷ê úç ÷ï ïç- è øê úï ïë ûï ïî þ
å å  (21) 
Counti,j in (20) and (21) is defined as: 
,
1, if ( , )  
= ,
0, otherwise
i j
i j
similarity
Count
aì ³ïïíïïî
x x
                         (22) 
where 11  and 1 .i m j k£ £ £ £  For a specific point xi, if the strength of xi,
1
,
 1
k
i j
j
Count d
=
£å , xi is 
defined as a noise point and is removed from the dataset. 
Step 4: End of the SNN algorithm. After eliminating the noise points, the SNN algorithm is 
completed. 
3.2  Elimination of Core Points by the Concept of Unit Vectors 
After the SNN algorithm is performed, most of noise points are removed from the datasets. 
The proposed data preprocessing procedure is hoped to not significantly alter the final cluster 
configurations but can save the computational time of SVC. Therefore, this study needs to 
Using the SNN algorithm 
to remove noise points
Using the concept of unit 
vectors to remove core 
points
Input the 
dataset
{ } dix
Final 
clustering 
results
Perform SVC algorithm
 
Fig. 9. The flowchart of the proposed efficient data preprocessing procedure for SVC. 
which equals to θ1 plus a standard deviation of λp. For a data point xp, if λp is bigger than θ2, it is 
classified as a noise point and is removed from the dataset. The above procedure for removing 
core and noise points is summarized as follows. First, for each of the remaining data points, say xi 
(i = 1, …, h), the summation of the unit vectors is calculated from its k2 nearest neighbors, 
denoted as λi. The θ1 and θ2 can be computed according to (24) and (25). Each data point is 
classified by the following conditions: 
1. If λi  θ1, xi is classified as a core point. 
2. If λi  θ2, xi is classified as a noise point. 
3. Otherwise, xi is a representative point of the dataset and will be used in the SVC training 
procedure. 
The elimination of insignificant data points, e.g., noise and core points, from the dataset will 
not alter the final cluster configuration of the SVC algorithm but greatly improve its efficiency. 
There is because the computational complexity of solving the optimization problem and labeling 
the data points with cluster labels in the SVC algorithm can significantly decreased by reducing of 
the size of the training dataset. Figure 9 shows the flowchart of the proposed data preprocessing 
procedure for SVC. 
 (a) 
 
(b) 
 (a) 
 
(b) 
 
(b) 
 
(c) 
Fig. 12. The support vector selection results in different proportions of second elimination step, 
while (a) q = 0.2, W1 = 20%, (b) q = 0.8, W1 = 20%, and (c) q = 0.2, W1 = 40%. 
 
From the experiments, the range of parameters, q, W1, and W2 that would not eliminate data 
points that are selected as support vectors by SVC are searched. Table 3 displays the final results 
of some parameters. The ‘percentage’ row in the table stands for what percentage in the selected 
support vectors contains the support vectors selected by SVC. 
B. Contour Construction Results 
As introduced in the previous sections, support vectors selected by the proposed scheme can be 
determined by both correlation coefficients and feature distances. In accordance with the proposed 
parameter searching mechanism, contours can be constructed after the support vectors are selected. 
Figures 13(a)-(c) are simulation results of the Bensaid dataset in different parameters. Figures 13(a) 
and (b) are results while the values of q are the same. Meanwhile, Figs. 13(a) and (c) are results 
with different q but same elimination percentage in the cross-correlation coefficient step. Likewise, 
the following pairs of Figs. 14 and 15 that also applied the proposed scheme to Crescent dataset 
and Five-cluster dataset are presented. 
 
 
(a) 
 (a) 
 
(b) 
 (b) 
 
(c) 
Fig. 15. The clustering results of the Crescent dataset after contour construction in different 
proportions on data elimination, while (a) q = 2, W1 = 10%, (b) q = 4, W1 = 10%, and (c) q = 6, 
W1 = 15%. 
4.2  The Simulation Results of an Efficient Data Preprocessing Procedure for SVC   
The effectiveness of the proposed data preprocessing procedure for the SVC algorithm has 
been validated through extensive computer simulations of different examples. The proposed 
approach with the HRE method [17] is compared in this study. The HRE method is an efficient 
−0.5 0 0.5 1 1.5
−0.5
0
0.5
1
1.5
x
1
x
2
 
−0.5 0 0.5 1 1.5
−0.5
0
0.5
1
1.5
x
1
x
2
 
(a)                                 (b) 
Fig. 17. (a) More data points are removed if the value of α is large. (b) More noise points are 
retained if the value of α is small. 
−0.5 0 0.5 1 1.5
−0.5
0
0.5
1
1.5
x
1
x 2
−0.5 0 0.5 1 1.5
−0.5
0
0.5
1
1.5
x
1
x 2
 
(a)                                (b) 
Fig. 18. (a) The core and noise points marked with circles can be removed from the artificial 
dataset. (b) The final clustering result obtained by SVC. Cluster boundaries are denoted by the 
contours. 
examples that contain the artificial, and benchmark datasets [29]. These 2-dimensional datasets 
contain 3000 to 5000 points with arbitrary shapes of clusters, various densities, and much noise. 
A.  Artificial Dataset 
The artificial dataset consists of 3000 data points in a two-dimensional space. There are five 
different sizes of clusters in the dataset. k1 = 40 and k2 = 10 was set in this example and obtained 
α = 23.29, δ = 13.06, and λp = 3.77 from (18), (19), and (23), respectively. Figure 16(a) shows the 
distribution of the original artificial dataset. The SNN algorithm is used to eliminate the noise 
points and the result is shown in Fig. 16(b). In this step, a total of 529 data points were eliminated. 
Next, the concept of unit vectors is used to eliminate the core points and 712 data points were 
retained. The result is shown in Fig. 16(c). The execution time of the proposed method was 1.88 
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
 
                       (a)                                 (b) 
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
 
                          (c)                                (d) 
Fig. 20. (a) The original benchmark dataset I. (b) The noise points marked with circles are
eliminated. (c) The distributions of core and representative points. The core points marked with
circles are eliminated from the dataset. (d) The final clustering result obtained by SVC. 
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
0 100 200 300 400 500 600 700 800
0
20
40
60
80
100
120
140
160
x
1
x 2
 
(a)                                  (b) 
Fig. 21. (a) The core and noise points marked with circles can be removed from the benchmark 
dataset I. (b) The final clustering result obtained by SVC.  
20(b) indicates the result obtained by the SNN algorithm with the removal of 788 noise points 
from the original dataset. There were 1176 data points retained after using the concept of unit 
vectors to eliminate the core points. The result is shown in Fig. 20(c). The execution time of the 
proposed method was 43.87 sec. Figure 20(d) illustrates the clustering results obtained by the 
0 100 200 300 400 500 600
50
100
150
200
250
300
x
1
x 2
0 100 200 300 400 500 600
50
100
150
200
250
300
x
1
x 2
 
 (a)                                (b) 
0 100 200 300 400 500 600
50
100
150
200
250
300
x
1
x 2
100 200 300 400 500 600
50
100
150
200
250
300
x
1
x 2
 
                         (c)                               (d) 
Fig. 23. (a) The original benchmark dataset II. (b) The noise points marked with circles are 
eliminated from the dataset. (c) The distributions of core and representative points. The core 
points marked with circles are eliminated from the dataset. (d) The final clustering result 
obtained by SVC. 
0 100 200 300 400 500 600
0
50
100
150
200
250
300
x
1
x 2
0 100 200 300 400 500 600
0
50
100
150
200
250
300
x
1
x 2
 
(a)                                 (b) 
Fig. 24. (a) The core and noise points marked with circles can be removed from the benchmark
dataset II. (b) The final clustering result obtained by SVC.  
and OPTICS. The proposed method can find the correct cluster number but HRE, DBSCAN and 
OPTICS cannot do so for different parameter selections.  
The simulation results of the benchmark datasets confirm that the proposed methods can 
References: 
[1] A. Ben-Hur, D. Horn, H. T. Siegelmann, and V. Vapnik, “A support vector clustering method,” 
in Proceedings of International Conference on Pattern Recognition, vol. 2, 2000, pp. 
724–727. 
[2] A. Ben-Hur, D. Horn, H. T. Siegelmann, and V. Vapnik, “Support vector clustering,” Journal 
of Machine Learning Research, vol. 2, pp. 125–137, 2001. 
[3] C. Cortes and V. Vapnik, “Support-vector network,” Machine Learning, vol. 20, no. 3, pp. 
273–297, 1995. 
[4] D. W. Kim, K. Y. Lee, D. Lee, and K. H. Lee, “Evaluation of the performance of clustering 
algorithms in kernel-induced feature space,” Pattern Recognition, vol. 38, no. 4, pp. 607–611, 
2005. 
[5] B. Schölkopf, A. Smola, and K. R. Müller, “Nonlinear component analysis as a kernel 
eigenvalue problem,” Neural Computation, vol. 10, pp. 1299–1319, 1998. 
[6] B. Schölkopf, S. Mika, C. J. C. Burges, P. Knirsch, K. R. Müller, G. Ratsch, and A. J. Smola, 
“Input space versus feature space in kernel-based methods” IEEE Trans. Neural Network, vol. 
10, no. 5, pp. 1000–1017, 1999. 
[7] I. S. Dhillon, Y. Guan, B. Kulis, “Kernel k-means: spectral clustering and normalized cuts,” in 
Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery 
and Data Mining, 2004, pp. 551–556. 
[8] K. R. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf, “An introduction to 
kernel-based learning algorithms,” IEEE Trans. Neural Networks, vol. 12, no.2, pp. 181–201, 
2001. 
[9] T. Ban and S. Abe, “Spatially chunking support vector clustering algorithm,” in Proceedings 
of 2004 IEEE International Joint Conference on Neural Networks, 2004, pp.413–418. 
[10] J. H. Chiang and P. Y. Hao, “A new kernel-based fuzzy clustering approach: Support vector 
clustering with cell growing,” IEEE Trans. Fuzzy Systems, vol. 11, no. 4, pp. 518–527, 2003. 
[11] B. Y Sun and D. S. Huang, “Support vector clustering for multiclass classification 
problems,” The Congress on Evolutionary Computation, 2003, pp.1480–1485. 
