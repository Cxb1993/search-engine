行政院國家科學委員會專題研究計畫結案報告 
在網格環境下高效率程式級排程之研究與設計 
Investigations on High-Performance Application-Level Task Scheduling for 
Grid Computing Environments 
Abstract 
Parallel and distributed computing environments are essential and utilized to meet the needs 
of a wide variety of high-throughput applications. Scheduling strategies are important in order 
to efficiently utilize resources and to improve response times, throughput and utilization of 
computing platforms. The task scheduling problem has been investigated for quite a long time, 
and it has been proved to be NP-hard or NP-complete. A number of heuristics have been 
proposed to pursue a suboptimal solution within acceptable computation complexity bounds.  
Many of heuristic scheduling algorithms and systems are proposed to address this 
problem. Unfortunately, most of these proposed scheduling algorithms so far are for dedicated 
systems. By the term “dedicated”, we mean the resources are dedicated for a given application, 
and can execute that give application exclusively. In contrary to most of current distributed 
systems available today, these computing environments are non-dedicated and 
computationally shared. Good scheduling in a shared environment involves the integration of 
application specific information and system specific information.  
In this research project, one of main goals was to propose a hierarchical method for 
scheduling of independent coarse-grained tasks in grid environments. Under this proposed 
architecture, the Grid Scheduler (master node of upper-level) proceeds with distribution of 
tasks to computing sites, while the Local Resource Manager (master node of lower level) 
assigns this task to an available computing node according to a given threshold. We have also 
investigated a performance-prediction based task scheduling system, which provides task 
allocation and scheduling based on application-level and system-level performance data.  
Experimental and simulation results are have been conducted, and later they are 
compared with those obtained from well known traditional scheduling algorithms, the 
effectiveness of the proposed method consistently shows a benefit from the approaches 
proposed. The experimental platform considered for our experiments was PCGrid grid 
  
1. Introduction 
Parallel and distributed computing environments are essential and utilized to meet the needs of 
a wide variety of high-throughput applications. Scheduling strategies are important in order to 
efficiently utilize resources and to improve response times, throughput and utilization of computing 
platforms.  
As one of the means to obtain inexpensive computational cycles, grid technology has emerged 
to fulfill the needs for solving large-scale computing intensive high-throughput applications, 
through the aggregation of a number of available and low-cost resources. Multiple users can 
simultaneously utilize any of resources interconnected to execute these large-scale parallel 
applications. In order to effectively utilize hybrid heterogeneous computational resources, resource 
management and task scheduling are fundamental factors for achievements in grids.  
Due to wide distribution and heterogeneity characteristics of grid platforms, loosely coupled 
parallel applications are better suited for execution on this platform than tightly coupled ones. In 
particular, task independent applications such as data mining, Monte Carlo, image manipulation are 
most suitable class of parallel applications for current design of grid environments. 
Scheduling task independent applications is still far to be considered well-established. Finding 
optimal scheduling is an NP-complete problem, and researchers have still resorted to devising 
efficient heuristics, which are based on diverse assumptions; they differ in their functionalities as 
well. 
Simulation and modeling have been dedicated and extensively used by professionals in 
different application fields, particularly, in the area of computer science, e.g., for microprocessor 
design and network protocol design, in which simulation and modeling have been used for decades. 
They are convenient and cost effective. In grid computing, several widely used and acknowledged 
simulations have been commonly used to evaluate tasks scheduling and load balancing. 
There exist a number of complex issues related to the development of parallel applications as 
any application developer may face, which may vary from the modularization, in the design stage, 
to the task allocation of appropriate resources and distribution of these tasks in unutilized resources 
in Grid platforms. Effective partitioning, allocation and scheduling of application programs on these 
platforms are crucial to obtain good performance, since the performance is very sensitive to the 
strategy used to distribute data among computing nodes or processors. 
 
 due to its simplicity. However, due to dynamic behavior and the heterogeneity of resources, not 
only they may not provide similar performance for all applications, but also the contention created 
among applications running on same shared resources, causing delays and affecting the QoS.  
We propose a method for scheduling of task independent parallel applications in grid 
environments. The Task Threshold-based Mapping method (TMM) distributes tasks to given node 
processors based on defined thresholds. That is, given a set of independent tasks, each task of this 
set is classified into one of five defined levels, say A, B, C, D and E (A is most time demanding 
while E is less demanding), according to the execution time needed. Computing nodes in each site 
of grid platform are rated according to their computing power. That is, given MCC as Maximum 
Computing Capability of any node in a grid platform, computing nodes are rated and classified in a 
particular class if this node’s computing power is X% of MCC. Table 1 shows the suggested 
classification (although other scales for levels can be defined). 
 
Node Computing Capability Level of Classification 
0%~20% MCC E 
21%~40% MCC D 
41%~60% MCC C 
61%~80% MCC B 
81%~100% MCC A 
 
In order to classify a computing site, we need to look at highest rank achieved by any of 
computing nodes inside this site. For instance, a site contains 3 computing nodes, with levels of 
classification B, C, and D. Thus, this site is ranked with level B. That is,  
Level Classification SITE =  MAX level {node1, node2, …, noden} 
The proposed method work as follows. Tasks are distributed, and each of them is shown to 
Grid scheduler. Based on Round Robin technique, the Grid scheduler selects next suitable site to the 
execution of that given task, matching a suitable site to the demand need for the given task. As for 
matching process, a task ranked with level A is expected to be distributed to a site ranked with level 
A, while task ranked with level D is expected to be distributed to sites ranked with levels A, B, C 
and D.  
As soon as the task is assigned to a particular site, the LRM (Local resource manager) of that 
site accepts that task, and then assigns it to the next available computing node that meets such 
threshold. See Figure 1 as illustration for the proposed method. 
 
In Figure 2, the number of tasks are varied ranging from 1000 to 50000 tasks, and the number 
of sites is varied from 2, 4, 8 and 16, as presented respectively in figures 2 (A), (B), (C) and (D). 
The vertical axis represents the execution time of all tasks of the given set.  Tasks are randomly 
generated and feed to grid scheduler in the order they are generated. Experimental results shows 
that TTM method gives the best result among the three methods, in all cases with different number 
of independent tasks (see figure 2).  
In figure 3, the experiment setup and conditions are similar to those in figure 2, except that 
tasks are generated and sorted according to their workload. Starting from small-sized tasks, they are 
fed to the grid scheduler in ascending order, according to task size.  
 
 
(A) 1000 tasks        (B) 5000 tasks       (C) 10000 tasks      (D) 50000 tasks 
Figure 3. Execution of sorted random-sized tasks in platform MODE-A (tasks are distributed in 
ascending order to grid scheduler) 
 
4. Conclusions and Discussion 
In this research project, the research results have been achieved are as follows. 
1. From experimental results, we note that the proposed algorithm (section 3.2) produces 
better maximum completion times than other referenced algorithms, considering that it is 
efficient yet low complexity, feasible for application in real platforms, 
2. A middleware has been developed, where users may request this system to automatically 
select computing nodes (a number of parameters and strategies), and the job tasks are 
scheduled and entering in the grid platform, 
3. Based on the scheduling techniques and strategies investigated, future studies for 
economic Grid / Cloud are straightforward, once the mechanism for pricing the job tasks 
are defined, 
4. A highly responsive decision strategy on process re-scheduling and load balancing in a 
 
 6.3  Journal Papers 
1. Francisco Isidro Massetto, Liria Matsumoto Sato, Kuan-Ching Li, “A Novel Strategy for 
Building Interoperable MPI Environment in Heterogeneous High Performance Systems”, The 
Journal of Supercomputing, Springer. [SCI, SCIE] (accepted, in press) 
Available Online DOI: 10.1007/s11227-009-0272-y 
2. Kuan-Ching Li, and Tien-Hsiung Weng, “Performance-based parallel application toolkit for 
high-performance clusters”, The Journal of Supercomputing, vol. 48, no. 1, pp. 43-65, Springer, 
2009. [SCI, SCIE] 
3. Kuan-Ching Li, Hsiao-Hsi Wang, Kuo-Yang Cheng, and Tsung-Ying Wu, “Strategies toward 
Optimal Access to File Replicas in Data Grid Environments”, JISE Journal of Information 
Science and Engineering, vol. 25, no. 3, pp. 747-762, Academia Sinica, 2009. [SCI, SCIE] 
4. Hsiao-Hsi Wang, Kuan-Ching Li, Chun-Chieh Yang, Ssu-Hsuan Lu, “Towards Implementation 
of a Novel Scheme for Data Prefetching on Distributed Shared Memory Systems”, The Journal 
of Supercomputing, vol. 47, no. 2, Springer, 2009. [SCI, SCIE]  
5. Myungho Lee, Neungsoo Park, Wonwoo Ro, Kuan-Ching Li, “Performance Evaluation of 
Programming Models in SMP-Based Clusters”, Journal of the Chinese Institute of Engineers 
(中國工程學刊), vol. 31, no. 7, Chinese Institute of Engineers (中國工程師學會), 2008. [SCI, 
EI] 
6. Hsiao-Hsi Wang, Kuan-Ching Li, Ssu-Hsuan Lu, Chun-Chieh Yang, and Jean-Luc Gaudiot, 
“Design and Implementation of an Agent Home Scheme Strategy for Prefetch-Based DSM 
Systems”, International Journal of Parallel Programming, vol. 36, no. 6, pages 521-542, 
Springer, 2008. [SCI, SCIE, EI] 
7. Kuan-Ching Li and Hsun-Chang Chang, “The Design and Implementation of Visuel 
Performance Monitoring and Analysis Toolkit for Cluster and Grid Environments”, The 
Journal of Supercomputing, vol. 40, no. 3, pp. 299-317, Springer, 2007. [SCI, SCIE] 
 
6.4  Lecture Notes in Computer Science / Springer 
1. Tien-Hsiung Weng, Sheng-Wei Huang, Ruey-Kuen Perng, Ching-Hsien Hsu, and Kuan-Ching 
Li, “A Practical OpenMP Implementation of Bit-reversal for Fast Fourier Transform”, 
Infoscale'2009 The 4th International Conference on Scalable Information Systems, LNICST, 
Springer, Hong Kong, 2009. [EI] 
 
 CIC’2006 The Fourth International Conference on Cooperative Internet Computing, 
Electronic Proceedings, Hong Kong, China, 2006. 
 報告內容應包括下列各項： 
一、參加會議經過 
 
In opening ceremony, we had been informed with some numbers of this conference: 4 keynote 
speeches and a total of 120 papers were accepted for presentation during this conference 
(main conference, symposia and workshops), coming from more than 15 countries, distributed 
in more than 30 sessions. Additionally, we received the information that the total number of 
submissions was over 300 (summed up from main conference, symposia and workshops), 
making the authors with papers accepted proud to have the quality of their papers recognized.
 
The keynote speeches were delivered by Ivan Stojmenovic (University of Ottawa, Canada), 
Hans Zima (JPL/Caltech, USA), Philippe Remy Bernard Devloo (Unicamp, Brazil) and Mark 
Perry (University of Western Ontario, Canada).  Interactions among researchers coming 
from a diverse of countries, such as Sweden, Taiwan, Canada, Malaysia, Australia, France, 
US, Norway, China and others in technical sessions and coffee breaks confirms the success on 
the organization of this conference.  
 
The presentation of my paper took place on July 18 (FRI) afternoon, and researchers from 
Sweden, Brazil, US and France attended it. Technical issues arose into discussion that may 
open to new topics of research and therefore, advancing not only our researches shown in that 
paper, but also specific topics of research. Our paper received “Outstanding Paper” Award, 
presented by Steering Committee Co-Chair, Dr. Siang Wun Song, during conference banquet.
 
二、與會心得 
 
Attending several other presentations, I could have annotated a number of researches being 
conducted by research teams from other countries, which may be considered to exploit 
collaboratively through idea exchanges and discussions with these teams.  
 
The invitation for General Co-Chair position of the CSE’08 conference came during May 
2007, when Dr. Laurence Yang (STFX, Canada) contacted me with this invitation. All 
dedication and efforts over 12 months of preparation has been exposed in a 3-day conference. 
During this period, persistence and hard working were fundamental elements in this unique 
experience. 
Toward an Efficient Middleware for Multithreaded Applications in
Computational Grid
Jose´ Augusto Andrade Filho, Rodrigo Fernandes de Mello, Evgueni Dodonov
University of Sa˜o Paulo
Sa˜o Carlos, SP, Brazil
E-mail: {augustoa, mello, eugeni}@icmc.usp.br
Luciano Jose´ Senger
State University of Ponta Grossa,
Ponta Grossa, PR, Brazil
ljsenger@uepg.br
Laurence Tianruo Yang
St. Francis Xavier University,
Antigonish, NS, Canada
lyang@stfx.ca
Kuan-Ching Li
Providence University,
Shalu, Taichung, Taiwan
kuancli@pu.edu.tw
Abstract
Recent researches on parallel and distributed systems
present drawbacks and limitations regarding to the analy-
sis, design, implementation and execution of high perfor-
mance parallel applications. Complex issues related to
the development of such high performance parallel appli-
cations vary from the modularization, in the design stage,
to the task allocation and distribution. Such difficulties
have motivated the design and implementation of MidHPC
(Middleware for High Performance Computing), a middle-
ware which automatically and transparently balances pro-
cess workloads using knowledge obtained from techniques
to extract and predict application behaviors. Through this
middleware, application tasks transparently communicate
among each other through a distributed shared memory sys-
tem. In order to better take advantage of the MidHPC mid-
dleware, parallel applications are written using the POSIX
threads standard. Applications developed following this
programming model can be executed in SMPs, Clusters and
Grids without any source code modification nor any ad-
ditional change. Experimental evaluation of the proposed
middleware shows not only good scalability and perfor-
mance can be achieved, as also efficient process migration.
1 Introduction
The availability of low cost microprocessors and the evo-
lution of computer networks have made economically fea-
sible the development of distributed systems with high per-
formance, such as Clusters and Grids. In such systems, pro-
cesses are executed over computing nodes interconnected
and exchanging messages to each other, in order to accom-
plish one computing task (7).
Since late 1980s, researchers have been adopting the dis-
tributed system concepts to develop high performance ap-
plications, as they realized the low cost to obtain good com-
putational power. The adoption of such concepts resulted
in several research topics, such as: load balancing, single
system imaging, low latency protocols, communication li-
braries and high performance hardware architectures. Such
researches are relevant, although they present limitations re-
lated to the analysis, design, implementation and execution
of high performance distributed applications.
The analysis of distributed applications is the stage
where the problem is defined as well as inherently high-
level computing issues to solve it. The design stage aims at
modularizing the parallel application to achieve high perfor-
mance, considering task characteristics such as processing,
communication, memory and storage accesses. The im-
plementation is the stage where the parallel application is
written following a programming model, as PVM (6), MPI,
OpenMP (18).
In order to execute a parallel application in a distributed
system, tasks must be scheduled among the available re-
sources. Traditionally, the developer must evaluate the re-
source performance and define the location where each sys-
tem module will be executed. This definition is highly com-
plex, as the developer must know how each module will be-
have during execution, as well as the available computing
resources. This is due to developers must avoid unbalanced
resource utilization, that is, resources may be idle while
some others are overloaded (8).
The presented limitations regarding to the analysis, de-
2008 11th IEEE International Conference on Computational Science and Engineering
978-0-7695-3193-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CSE.2008.56
147
Authorized licensed use limited to: PROVIDENCE UNIVERSITY. Downloaded on October 30, 2009 at 21:28 from IEEE Xplore.  Restrictions apply. 
Once selected individuals for reproduction, it is neces-
sary to modify their genetic characteristics using reproduc-
tion techniques known as genetic operators. The most com-
mon operators are crossover and mutation.
The crossover operator allows to exchange genetic ma-
terial between two individuals, known as parents, by com-
bining their information in a way which provides a good
chance of creating better individuals (15).
The mutation operator is used for changing a single gene
value for a new random one. Let an individual represented
by a bitmap, this operation consists of randomly selecting a
gene and swapping its value from 1 to 0 (or from 0 to 1, cor-
respondingly). The goal of the mutation is to maintain the
diversity of a population, always allowing a chromosome to
cover a significantly large search space (15). It is usually
applied at a low rate, as at high ones the results tend to be
random.
4 MidHPC Middleware
MidHPC middleware aims at simplifying the analysis,
design, implementation and execution of multithreaded par-
allel applications on distributed heterogeneous systems,
such as Grids. For this purpose, firstly we design MidHPC
to support multithreaded applications without any source
code modification. This brings up relevant contributions
to software developers who do not need to have specific
knowledge about parallel libraries nor grid toolkits.
Besides allocating such applications in the grid, users
and developers do not need to know the communication,
processing and I/O access behaviors. MidHPC monitors
such information to make predictions and optimally dis-
tribute application tasks among available resources. In this
way, any multithreaded application can be executed through
MidHPC on Grids or distributed systems. Therefore, devel-
opers does not need to learn a new software toolkit, they just
have to use an accessible and known paradigm: concurrent.
MidHPC is composed of Shell, Broker, Scheduler, Mon-
itor and IBL modules. Shell is responsible for the appli-
cation submission, interfacing with users and developers.
Broker joins networks to compose the Grid and it is di-
vided into two other modules, in which are defined later.
Scheduler gets application and resource information to take
scheduling decisions. Such information is obtained by a
Monitor and used for predicting the future behavior of ap-
plication tasks (using IBL technique (20)), thus, MidHPC
can plan task migration to optimize resource usage. All ap-
plication tasks transparently communicate through the dis-
tributed shared memory support (DSM), which distributes
data in the grid to optimize read and write accesses based
on task locality. DSM runs on top of a distributed filesys-
tem (DFS) which is responsible for distributing data on the
grid.
The sections 4.1, 4.2, 4.3 and 4.5 discuss each module
in details, while the section 4.6 explains how those modules
interact among them.
4.1 Broker
The Broker module is used to transparently connect com-
puting nodes in networks, to build a Grid computing plat-
form. This module is composed of two different managers:
BG (Global Broker) andBL (Local Broker). ManyBLs can
be linked to a single BG.
The BL manager is responsible for offering and con-
trolling grid access to resources from local area networks.
This manager has a list containing information about all re-
sources running the Scheduler daemon (what is a require-
ment to participate on the grid). Such information includes
MIPS capacity, load, memory and hard disk read/write
throughput which is summarized, in means and standard de-
viations. This summary and the number of computer in the
BL network are submitted and stored by the BG manager.
The connection among Brokers through the Internet is
held by the BG manager. Information regarding applica-
tions running in the grid and the summarized information
from BLs are stored by BG manager. This manager also
stores the location and replicates the summary information
of other BGs. This replication keeps the environment run-
ning when any Broker fails. Only applications in BLs of
such Broker stop running what will be studied and may be
solved in future work.
As user send requests coming from Shell, they are served
by a BG manager. The requests start applications or get
information from Broker managers.
The BL manager contains detailed information about
their computers, such as: MIPS capacity, memory (main
and swap) used and performance. BG manages some BLs,
in which summarizes their information into a single entry.
Such entry is replicated among BGs for fail-over reasons.
4.2 Shell
To simplify user interaction, we designed a Unix-like
shell to submit and control application execution. This shell
runs on top of a distributed filesystem containing common
directories of a Unix tree such as /bin, /lib, /usr, /home and
/tmp. All file contents can be distributed on hard disks of
grid computers. This distribution allows replications to op-
timize read and write access in the grid what is considered
for further work.
When a user wants to run parallel applications, he/she
must only copy necessary binaries and libraries to the
filesystem tree. After that, the user launches the applica-
tion running start myapp and MidHPC middleware deals
149
Authorized licensed use limited to: PROVIDENCE UNIVERSITY. Downloaded on October 30, 2009 at 21:28 from IEEE Xplore.  Restrictions apply. 
some size, i.e., the number of parallel application tasks) and
each vector element represents where the process will be
assigned to. For instance, the process of index 0 will be as-
signed to the computer 0, the one of index 1 to the computer
3 and so on.
The proposed genetic algorithm considers the multi-
objective fitness function (Fi) presented in the equation 2,
where: MRTi is the response time of the highest compu-
tational cost process of a given parallel application, LVi is
the load variance that the proposed solution causes to the
environment (equation 3), and η is a parameter to avoid the
division by zero.
Fi =
1.0
MRTi + η
+
1.0
LVi + η
(2)
LVi =
|i|∑
j=1
(Cj −
∑|i|
j=1 Cj
j
)2j (3)
The response time MRTi is obtained by the equation 4
where Cj is the total execution cost for a task j. The equa-
tion 5 defines Cj , where PMj is the total processing cost of
task j, CAPd is the capacity of the computer d where the
task j is allocated, and CCj is the communication cost of
the task j to any other task of the same parallel application
running on other computers.
MRTi = max(Cj), j = 0, ..., |i| − 1 (4)
Cj =
PMj
CAPd
+ CCj (5)
The equation 6 defines how the communication cost is
obtained where CPS(ij , ik) is the communication cost be-
tween the tasks at indexes j and k of the chromosome i
which run, respectively, on the computers ij and ik.
CCj =
|i|∑
j=1
|i|∑
k=1,j 6=k
CPS(ij , ik) (6)
This multi-objective function aims at minimizing the re-
sponse time of the highest cost process (MRTi) and causes
the smallest load variance in the environment, which mean a
better balancing. High Fi values represent better solutions.
After executing the genetic algorithm for several gener-
ations, where in each one n individuals are created, crossed
and mutated, it converges to a valid solution which presents
the highest fitness value. Higher fitness values, in this case,
means better process allocation in grids.
This algorithm is used in the first process distribution of
a parallel application in grids. After that, processes can be
migrated (in accordance with the migration factor presented
in (11)) among neighbor computers, as the original Route.
4.4 Extraction, Classification and Predic-
tion
The extraction, classification and prediction follows the
model proposed in (12), which uses neural networks and
statistical approaches. This model is composed of three
phases.
The first phase consists of an automatic extraction of ap-
plication behavior by using a profiling suite based on the
GridBox project (13). This suite is specifically tuned to
monitor and register all operations, with no source code
modification. All GridBox functionalities occur transpar-
ently to applications and the following operations are regis-
tered: Sequence of operations during execution, allowing to
restore the application behavior afterwards; CPU time, be-
tween consecutive communication operations, used to de-
termine the application processing cost at any given point
of execution; Location and size of modified data for each
distributed operation, which is used to determine and pre-
dict the application access patterns; I/O, memory and net-
work usage for each distributed operation, which is used to
improve the application scheduling and load balancing for
different heterogeneous networks. This information is used
to make predictions about application future behavior and
also to fill out the IBL database used to take scheduling de-
cisions for the first allocation.
After that, there is the phase of application behavior clas-
sification. It reduces the data dimensionality, simplifying
the next phase (prediction). Having the application execu-
tion trace collected, the model classifies its behavior using
the ART-2A neural network (17). The pattern classification
is performed by analyzing the sequence of application be-
havior which is grouped into clusters of similar behavior.
With a vigilance parameter, the application behavior is clas-
sified into clusters and then a transcription data is gener-
ated, describing the sequence of application operations per-
formed in the form of cluster transitions.
In the last phase, two different prediction techniques are
employed. The first approach generates Markov Chains for
a statistical approach and uses an incidence matrix, which
contains all application state transitions, calculating the
probability for each state transition. This allows to predict
the states that can be reached by the application at any given
execution point and the probability of accessing them. This
technique does not consider the application execution his-
tory, offering stochastic results. The second approach uses
the classified application behavior which is transformed into
a time series using TDNN neural network, showing how the
application behavior varies during execution. This time se-
ries is structured as a set of vectors of size n (formally de-
fined as lag (22)), such as v0 = e0, e1, ..., en−1 where ek is
a pattern classified by ART-2A, to predict the next point en.
This model uses application execution history (repre-
151
Authorized licensed use limited to: PROVIDENCE UNIVERSITY. Downloaded on October 30, 2009 at 21:28 from IEEE Xplore.  Restrictions apply. 
consumption means and standard deviations of CPU, mem-
ory, hard disk and network. The network consumption
is used to define the neighborhood for parallel application
task distribution. Brokers where the network bandwidth is
higher than the network consumption are considered neigh-
bors. If IBL returns no value, i. e., the database does have
information on the requested application, the neighborhood
is the minimum, which means that is composed of Sched-
uler daemons from the same network.
After that, the application is started with minimum pri-
ority (set by using the nice tool, from GNU/Linux), so it
does not unbalance the environment, as well as the monitor
also starts filling the database with the application events.
Scheduler daemon requires an application identification to
the primary Broker (the global id – gid) and to its processes,
also named tasks (the process id – pid). The primary con-
sistently replicates such information to others.
With the application running, RouteGA starts executing
which considers all information obtained by IBL. It is nec-
essary to make remarkable that RouteGA considers all com-
puters running Scheduler daemons in Broker neighbor net-
works.
The mean occupation values obtained from IBL are used
as parameters to the genetic algorithm, and a near optimum
task distribution is obtained. According to this distribution,
tasks are migrated to neighbor computers.
The process checkpoint, used in migration, is performed
by a checkpoint tool (nowadays CryoPID has been used)
that stores the process state in the DFS (distributed filesys-
tem) as a binary file. Such file is used to restart the process
in another computer. Scheduler daemon is also responsi-
ble for updating information about processes on the Broker.
Periodically, a verification is performed in order to keep the
balanced environment (10).
5 Programming Model
PVM (6), OpenMP and MPI (18) are examples of en-
vironments, with their own programming models, used to
build distributed applications.
The MidHPC programming model follows the concepts
of the concurrent paradigm and does not require spe-
cific knowledge of the programming environment as listed
above. When the user launches an application, a query,
composed of the user login, group and the application pa-
rameters, is submitted to the IBL knowledge base, which
replies with information about the application behavior. For
instance, if an application uses 2Mb of memory, no hard
disk operation and requires a bandwidth of 10Mbps (due to
the race condition control) then the MidHPC DSM creates
a 2Mb file f which will be used as the shared memory, and
when effectively starting the application, a signal is trig-
gered and the file is referred as the main memory.
Firstly, the application starts at a computer α at the min-
imum priority, just to intercept thread creation. It is im-
portant to notice that as the variables are created, they are
allocated in the file f , previously created by the DSM. The
producer and consumer threads start their execution in the
same computer α. When a pthread create() function is
performed, its call is intercepted and then, a process is cre-
ated instead of a thread, and such processes use the same
DSM file f as main memory. Any memory read-write op-
eration is executed on such file and the DSM guarantees
its consistency by using the sequential model. The main
POSIX thread calls are supported by MidHPC.
With the information from the IBL knowledge base, the
scheduler defines the best computer neighborhood, and the
RouteGA algorithm distributes the process on it. Now, the
application is parallely run on different computers with dis-
tributed shared memory.
It is important to observe that the example does not differ
from an usual C program using threads, what allows the ex-
ecution of legacy applications on the MidHPC Grid. In this
way MidHPC transparently balances the workload and sup-
ports distributed shared memory differing from other Grid
supports such as Globus (14), X-Grid (4) and OurGrid (3).
MidHPC and other environment suffer of the same
modularization problem where developers have to design
considering communication and processing, although just
MidHPC supports legacy applications considering the be-
havior extraction and prediction to balance workload.
6 Implementation Considerations
MidHPC modules (Shell, Brokers, Scheduler, IBL and
RouteGA) are implemented in Java. Monitors and thread
interception functions are implemented in C/C++.
For the first version, the filesystem under the DSM (sec-
tion 4.5) is the NFS (Network File System). However, any
filesystem with distributed data access can be used without
modifications, such as SSHFS, CODA, and AFS.
To install MidHPCmiddleware, there is a singleMakefile
that compiles and install all modules.
Each module has a configuration file that contains infor-
mation to be filled out by the user. The Shell configuration
file needs the primary Broker name and the Broker config-
uration file also needs this name. Scheduler configuration
file needs only the IP address (or name) of the Broker on its
network.
The IBL knowledge base is stored in the same computer
where is the Broker (BG) in order to centralize applica-
tion information and can be accessed by the Scheduler dae-
mon to take load balancing decisions. Besides such central-
ization, we may consider a distributed database in further
work.
153
Authorized licensed use limited to: PROVIDENCE UNIVERSITY. Downloaded on October 30, 2009 at 21:28 from IEEE Xplore.  Restrictions apply. 
 行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
 
                                                             97 年 6 月 22 日 
報告人姓名  
李冠憬 
 
服務機構 
及職稱 
靜宜大學 資訊工程學系 
副教授 
     時間 
會議 
     地點 
98 年 6 月 10 日-6 月 11 日 
香港 
本會核定
補助文號
 
 
會議 
名稱 
第四屆擴充性資訊系統國際研討會 
The 4th International ICST Conference on Scalable Information Systems 
發表 
論文 
題目 
使用 OpenMP 實做快速傅立葉轉換中 Bit-reversal 演算法 
A Practical OpenMP Implementation of Bit-reversal for Fast Fourier Transform
 P. Mueller, J.-N. Cao, and C.-L. Wang (Eds.): Infoscale 2009, LNICST 18, pp. 206–216, 2009. 
© Institute for Computer Science, Social-Informatics and Telecommunications Engineering 2009 
A Practical OpenMP Implementation of Bit-Reversal for 
Fast Fourier Transform 
Tien-Hsiung Weng1, Sheng-Wei Huang1, Ruey-Kuen Perng1, Ching-Hsien Hsu2,  
and Kuan-Ching Li1 
1
 Department of Computer Science and Information Engineering Providence University 
Shalu, Taichung 43301 Taiwan 
{thweng,rkperng,kuancli}@pu.edu.tw 
2 Department of Computer Science and Information Engineering Chung Hua University 
Hsinchu 300 Taiwan 
robert@grid.chu.edu.tw 
Abstract. In this paper, we describe our experience of creating an OpenMP im-
plementation of Bit-reversal for Fast Fourier Transform programs from the ex-
isting un-parallelizable sequential algorithm.  The aim of this work is to present 
an analysis of a case study showing the development of a shared memory paral-
lel Bit-reversal for the FFT parallel code with practical and efficient use of 
multi-core machines.  We present our implementation and discuss the results of 
the case study in terms of program improvement that may be needed to help 
parallel application developers with similar high performance goals.  Our pre-
liminary studies, results and experiments based on FFT code running on a  
four 4-cores Intel Xeon64 CPUs /Dell 6850 platform. The experimental results 
show that the performance of our new parallel code on 16 cores shared-memory 
machine are promising. 
Keywords: Shared-memory parallel programming, OpenMP, Bit-reversal, FFT. 
1   Introduction 
The Fast Fourier Transform (FFT) is one of the most important algorithms used in 
many fields of science and engineering, especially in signal processing and computa-
tional fluid dynamics for solving PDEs.  The FFT [2] uses a divide and conquer strat-
egy to evaluate a polynomial of degree n at the n complex nth roots of unity. FFT is 
easier to parallelize and many parallel FFT algorithms on shared memory machines 
have been well studied and developed. But practical implementation of The FFT pro-
grams consist of two main parts. First, data reordering in the Fast Fourier Transform 
by permuting the element of the data array using Bit-reversing of the array index. This 
first stage involves finding the DFT of the individual values, and it simply passes  
the values along. Next, each remaining stages the computation of a polynomial of 
degree n at the n complex nth roots of unity is used to compute a new value depends 
on the values of the previous stage; this process, we called butterfly operation. A Bit-
reversal is an operation for an exchange data between A[i] and A[bit-reversal[i]], 
where the value of i is from 0 to n-1 and value of n is usually 2 to the power of b. 
208 T.-H. Weng et al. 
 
In this paper, we proposed our OpenMP implementation of Bit-reversal for the FFT 
using the so-called SPMD (Single Program Multiple Data) style of OpenMP, in which 
reducing number of cache misses and data locality are the main concern in the design 
of our code. The SPMD style of OpenMP code is distinct from ordinary OpenMP code. 
In most ordinary OpenMP program, shared arrays are declared and parallel for direc-
tives are used to distribute work among threads via explicit loop scheduling. In the 
SPMD style, systematic array privatizations by creating private instances of sub-arrays 
gives opportunities to spread computation among threads in the manner that ensures 
data locality [5]. An in depth study about SPMD style of OpenMP can be found in [5]. 
Programs written in SPMD style of OpenMP has been shown to provide scalable per-
formance that is superior to a straightforward parallelization of loop for ccNUMA 
systems [6][11]. More advantages of using OpenMP to parallelize our code are port-
ability, easy to use, easy to maintain as well as incremental parallelization. OpenMP 
[7] is an industry standard for shared memory parallel programming agreed on by a 
consortium of software and hardware vendors. It consists of a collection of compiler 
directives, library routines, and environment variables that can be easily inserted into a 
sequential program to create a portable program that will run in parallel on shared-
memory architectures. It is considerably easier for a non-expert programmer to develop 
a parallel application under OpenMP than under either Pthreads or the de facto mes-
sage passing standard MPI. OpenMP also permits the incremental development of 
parallel code. Thus it is not surprising that OpenMP has quickly become widely ac-
cepted for shared-memory parallel programming. 
3   OpenMP Implemention 
In this section, we give an overview of original un-parallelizable sequential Bit-
reversal programs developed by Rodriguez [8].  Next, in Section 3.2 we present and 
discuss our OpenMP implementation.  We describe the steps taken to create the 
OpenMP program as well as how we rewrote the program from the un-parallelizable 
one.  We also explain our development of parallel Bit-reversal implementation using 
OpenMP SPMD style by examples. 
3.1   Brief Overview of Sequential Bit-Reversal 
Our work is based on an improved bit-reversal algorithm for the FFT by Rodriguez 
[8].  In their original algorithm, computation of the bit-reversal of index for data reor-
dering calculates only the required bit-reversal of indices, which also eliminates the 
number of unnecessary bit-reversal and swaps.  The bit-reversal is computed as bi-
trev= ∑
=
−−
k
p
k
kpb
0
1 2 ; this corresponds to the sequential pseudo-code as shown in Figure 1.  
It uses only array A to store its input data and final results.  When the algorithm uses 
only array A, the data reordering must perform the exchange between elements of A. 
Even though the swapping is only a simple exchange between the two elements of an 
array, it actually involves three assignment statements or copies actions.  For in-
stances, the swap(A[1],A[2]) produces the copy A[1] to Temp, then A[2] to A[1], and 
then Temp to A[1].   
210 T.-H. Weng et al. 
 
3.2   Our OpenMP Version of Bit-Reversal 
We implemented our parallel code in this paper; it is quite similar to the one devel-
oped by Rodriguez, but with modification as shown in Figure 3 and 4.  The original 
program designed by Rodriguez was not parallelizable because there are true data 
dependences, and therefore it is impossible to directly add the parallel OpenMP direc-
tive to this original for loop. 
Figure 3 shows part of the FFT code that only computes data reordering performed 
by permuting the element of the data array A into Mf using Bit-reversing of the array 
index. That is Mf[i]=A[Bit-reverse[i]]. Instead of putting the result in A, we store the 
result into Mf. Both arrays have size of n, where n is 2 to power of k, where k is the 
number of bit to be reversed.  In our program, we use more memory spaces in order to 
parallelize the code, reduce the total number of data copy; so we trade the space for 
shorter execution time. 
In order to write a Bit-reversal parallel code using the SPMD style of OpenMP 
from the existing un-parallelizable sequential code, we first remove the true data  
 
FFT(*A,n,nthreads) {  
struct COMPLEX Mf[n],Nth[n/2],Tmp[n/2];  
chunksize = n / nthreads; 
int offset[nthreads];  
for(i=0;i<nthreads;i++) { 
if(i==0) j=0; 
else { 
          k=nthreads/2; 
          while(k<=j) { 
              j=j-k; k=k/2; 
          } //end while  
          j=j+k; 
} //end else 
offset[i]=j;
} // end for 
#pragma omp parallel private(threadid, address) 
  { threadid = omp_get_thread_num(); 
    address = chunksize * threadid; 
    Bit_reverse(A,Mf+address, 
                   n,chunksize,offset[threadid]); 
}
...
...
}
 
Fig. 3. Main function of the OpenMP FFT  
212 T.-H. Weng et al. 
 
equal to number threads for offset, in this case, int offset[4].  Before we perform the 
Bit-reversal in parallel using OpenMP SPMD style to perform the permutation, we 
compute only four values to store in four elements of the offset, in this examples, we 
obtain 0, 2, 1, and 3 for offset[0], offset[1], offset[2], and offset[3] respectively.  In-
side the OpenMP parallel region where the pragma omp parallel directive is added, 
four threads will be created; each thread executes Bit-reversal function in Figure 4 
with parameters to perform different part of data computation.  In this case, thread 0, 
1, 2, and 3 will call Bit-reverse with parameters (Mf+(0*8) and value of offset[0] is 
0), (Mf+(1*8) and the value of offset[1] is 2), (Mf+(2*8) and offset[2] is 1), (Mf+(3*8) 
and the value of offset[3] is 3) respectively. 
Hence, as shown in Figure 5, master thread(or thread 0) will handle the Mf[0] cor-
responds to Mf+0;  then it computes Mf [i]=A[j+offset]  for i from 0 to number of 
chunk size;  where the offset[0] is 0, then Mf[0]=A[0+0], Mf[1]=A[16+0], Mf[2]= 
A[8+0], Mf[3]=A[24+0], up to Mf[7]=A[28+0]. 
Thread 1 will process Mf[0] corresponds to Mf+8, is the pointer to actual location 8 
of Mf, Mf [1] is location 9 of Mf, and so on.  The offset[1] is 2.  So, Mf[0]=A[0+2], 
Mf[1]=A[16+2], Mf [2]=A[8+2], Mf[3]=A[24+2], up to Mf[7]=A[28+2].  Note that 
Mf[0] is actually the Mf[8], Mf[1] is Mf[9], Mf[2] is Mf[10], etc. Other threads are also 
performed in similar manner 
Thread Starting  Value  Mapping 
ID      address    of 
                    offset 
0        0         0      0  16   8  24  4  20  12  28 
1        8         2      2  18  10  26  6  22  14  30 
2       16         1      1  17   9  25  5  21  13  29 
3       24         3      3  19  11  27  7  23  15  31 
 
Fig. 5. The result of mapping computed by threads  
Note that in Figure 5, the variables n, chunksize, offset, i, j, and k are private vari-
ables, only array A and Mf are shared variables.  Private data is usually stored locally 
to thread, which promote data locality. 
As the results, each of the four threads will call Bit-reverse function that is shown 
in Figure 5 and they are run in parallel.  Thread 0 computes the mapping of 0-0, 1-16, 
2-8, 3-24, 4-4, 5-20, 6-12, 7-28.  During the generation of this mapping i-j, it copy 
Mf[i]=A[j].    Thread 1 computes 8-2, 9-18, 10-10, 11-26, 12-6, 13-22, 14-14, 15-30, 
and so on.  The complete computation of the mapping of the data reordering is shown 
in Figure 5. 
4   Experiments 
Our experimental results based on data reordering by Bit-reversal are performed on a 
four 4-cores CPUs 2.6Ghz Intel Xeon64 / Dell 6850 platform with 4 GB of main 
214 T.-H. Weng et al. 
 
to longer execution time, which is twice as much as the number of size increases. This 
overhead is amortized by the increasing number of threads. With increased number of 
threads, the total number of cache misses is reduced to as the number of chunk size. 
For instance, with n equals to 224, with one thread, the number of cache miss will be 
224 times, and with 16 threads, the cache misses will reduce to 224/16 times. 
Bit-reversal for FFT
0
5
10
15
20
25
30
35
1 2 4 8 16
Number of threads
E
x
e
c
u
t
i
o
n
 
t
i
m
e
 
i
n
 
s
e
c
o
n
d
s
2^24
2^25
2^26
2^27
 
Fig. 7. Data reordering in FFT by permuting by bit-reversal 
5   Possible Improvement 
First we discuss the short coming encounter for the original code. In Figure 1, when 
we call this Bit-reverse with N=16 and p=4, we obtain the swap of A between 1-8, 2-
4, 3-12, 5-10, 7-14, and 11-13.  The upper bound index is 11, the value calculated for 
the variable, last.  This program can significantly reduce the number of swaps from N 
to (N-N2)/2.  Even though it reduces the number of swaps, in reality the swap involves 
three copy actions.  But, since the algorithm reduces the number of swap, there are 
only N/2 numbers of cache misses.   
In our sequential code, the problem will occur when N is significantly large. Then, 
the swap between the two array elements involved exchange of the far distant element 
which causes large number of cache misses.  For example, when N=226, then there are 
assignment statements as the following: Mf[1] = A[33554432], Mf [2] = 
A[16777216], and Mf [3] = A[50331648], and so on.  The copy of Mf[1] from 
A[33554432], cause the first cache miss from reading from location 33554432, this 
causes it to load a consecutive block of array from location 33554432 into the cache 
probably up to the cache size.  Then second assignment also cause the cache miss 
from reading A[16777216], which also load consecutive block of array from location 
33554432 into the cache probably up to the cache size.  Similar situations occur for 
the next iteration.   
