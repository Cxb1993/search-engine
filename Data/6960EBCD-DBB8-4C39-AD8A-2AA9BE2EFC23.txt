 2
視訊摘要(video abstraction)意指自冗長
的影片中，取出重要的靜態影像或是動態片
段，以利快速瀏覽影片的內容，或做為建立
索引的依據。視訊摘要大體可分為靜態摘要
(video summarization) 與 動 態 摘 要 (video 
skimming)。靜態摘要，顧名思義，即指取
得的摘要為一張張的靜態影像，稱之為關鍵
畫面(keyframe)，且建立摘要的過程通常不
會考慮聲音方面的資訊。相對地，動態摘要
則是建立一段時間較短、可以播放的影片，
且通常播放時會保留原來的聲音。 
視訊檢索旨在自資料庫中，儘快搜尋需
要的影片或片段。雖然視訊檢索可視為靜態
影像檢索的延伸，但由於需考慮時域上的特
性，查詢方式亦較為複雜。檢索可分為語義
上的檢索、及以內涵式檢索。語義上的檢索
大部份是以關鍵字查詢，故資料庫事先需對
每個影片下註解；由於關鍵字的註解過程耗
時，且許多影片內容不易以文字表達（例如
揮桿的姿勢），因此以內涵資訊就變得十分
重要。內涵式檢索事前必須對某個影片擷取
具有代表性的特徵，並以特徵建立索引，如
此在比對影片時，只需比對事先算出來的特
徵即可，運算的時間自然少了許多。 
經由審視視訊摘要與檢索的各種議
題，不難發現其基礎皆為視訊分析(video 
analysis)，視訊分析旨在分析與擷取視訊影
片中的各種特徵，進而建立該視訊資料的特
性結構，以利各種應用所需。如視覺串流的
空間域特徵分析(spatial feature analysis)、時
域分析(temporal feature analysis)與視訊切割 
(video segmentation)等。就影片的結構性方
面，我們希望根據影片的拍攝特性，自動偵
測具備高階語義的場景；就影片內物件的表
示法，我們利用物件追蹤，記錄某物件隨時
間變化的運動情形，以利做特定物件的特徵
擷取，如運動軌跡或物件的視覺特徵。此
外，我們亦將同時考慮時域與空間域上的運
動資訊，以期進行有效的自動分類。 
 
三、結果與討論 
 
我們本年度的研究包含： 
 
1. 視訊摘要 
2. 相關回饋於個人化分析與檢索之研究 
3. 影片中的語義事件偵測 
 
現分項敘述如下： 
 
視訊摘要 
 
視訊摘要(video abstraction)意指自冗長
的影片中，取出重要的靜態影像或是動態片
段，以利快速瀏覽影片的內容，或做為建立
索引的依據。視訊摘要大體可分為靜態摘要
(video summarization) 與 動 態 摘 要 (video 
skimming)。 
視訊靜態摘要主要目的在於提供使用者
快速判斷一段視訊串流(video stream)是否
符合他所期望，或是該視訊串流內是否有符
合他所期望尋找的鏡頭或場景。我們可以想
像有一張電影的宣傳海報，上面有電影中的
精彩畫面，當我們看見這張海報時就可以大
概了解這部電影的劇情。將影片摘要以靜態
的影像呈現，雖然沒有聲音的配合，但是可
以提供一目了然的資訊。 
視訊動態摘要主要的目的是將一段較長
的視訊資料精簡為一段使用者自訂的長
度，以幫助使用者判斷這一段視訊是否符合
他的期望。一個理想的動態摘要系統，必須
符合以下條件：(1)精簡後的視訊只包含有資
訊且有意義的資料；(2)最後展現的視訊內容
長度，必須能夠配合使用者調整，從最精簡
到最完整播出；(3)在聲音的部分音效或是對
話必須要能夠連續。 
基於第一年度「分鏡偵測與場景分析」
的研究成果，我們除了可以從一段影片中偵
測其分鏡的轉換方式之外，還能擷取出該段
影片中在不同場景的內容(content)下所具有
的特徵，這些特徵包含了運動情形(motion)
以及聲音(audio)資訊等等，並利用這些特徵
做為偵測並分類這些場景的基礎。 
然而，上述特徵雖足以代表影片不同場
景中的內容，但對於人類的認知來說，是很
難從這些特徵中辨別出分鏡甚至是場景的
轉變情形，相較於這些低階特徵(low-level 
feature)，我們必須將人類的語義(semantic)
和這些低階特徵產生連結，方能從影片中選
取出人類感興趣的精彩畫面，並進而達到視
訊摘要的目標。 
我們第一年時以運動賽事的影片為主要
研究目標，實驗的影片擷取自於棒球比賽的
 4
    承上所述，當使用者提供一張範例影像
進行檢索時，我們會先使用 intra-session 
learning 的方法進行第一次的比對搜尋，並
讓使用者選取回饋的影像；接著，我們再利
用 SVM 學習新的 hidden semantic concept，
此時SVM的 training data來自於每張回饋影
像的 hidden semantic feature vector，根據所
學得的 2-class classifier，即可判斷出每張影
像與搜尋目標的相關性，此即我們提出的
inter-session learning 架構。然而，隨著檢索
過 程 增 加 ， hidden semantic space 的
dimension 會越來越高，故我們需要一個有
效的 dimension reduction 技術來解決這個問
題。由於[46]以SVD的方法來實施dimension 
reduction 之後，並無法保持 SVD 執行前的
語義，故我們採用群聚(clustering)的方法來
達到 dimension reduction 的目的。 
    在我們第二年度「相關回饋於內涵式檢
索的研究」中，我們針對 dimension reduction
所提出的方法為：如果有很多張資料庫影像
的 hidden semantic feature vector 很像，代表
它們應該擁有相同的 hidden semantic 
concept，因此我們對所有資料庫影像的
hidden semantic concept 進行 support vector 
clustering (SVC)，每個 cluster 即代表一新的
hidden semantic concept。我們只需利用新得
到的 hidden semantic concepts 重新對所有資
料庫影像進行 indexing，即可再度進行檢索
的工作。然而，SVC 方法具有兩項缺點尚待
改進，一個是其時間複雜度(time complexity)
較高，另一個則是不易設定其控制參數。 
    因此，在本年度中，我們提出了利用兩
種 graph partition 的方法來做 dimension 
reduction ： 一 為 cluster-based similarity 
partition algorithm (CSPA) ， 另 一 則 為
meta-clustering algorithm (MCLA)。以 CSPA
來說，該方法將一個 hidden semantic concept
視為一個 graph，而屬於該 hidden semantic 
concept 的每張影像則視為 graph 上的一個
頂點(vertex)，影像之間 co-associate matrix
的相似程度 (similarity) 則是 graph 上的
edge，再藉一些 clustering 的方法來將相似
的影像群聚起來。另一方面，MCLA 的方法
是將一個 hidden semantic concept 當作一個
頂點，每個 hidden semantic concept 之間的
相似程度當作 edge，並以這些頂點和 edge
去 建 構 成 一 個 meta-graph ， 透 過
meta-graph，我們可以藉由 graph partition 找
出由數個相近的 hidden semantic concept 組
成的meta-cluster，再根據影像和meta-cluster
的相似程度，將影像歸類給最適合的
meta-cluster。利用新得到的 hidden semantic 
concept 重 新 對 所 有 資 料 庫 影 像 進 行
indexing，便可再度進行檢索的工作。 
    我們將提出的相關回饋技術以及
dimension reduction 方法應用於檢索上，除
了和未使用 dimension reduction 的 initial 
hidden semantic space 之檢索結果做比較之
外，也和只使用 low-level 特徵的檢索結果
做比較，如圖二所示，可以發現以 graph 
partition 來做 dimension reduction 在檢索的
結果表現上會有較高的正確率，其次是未使
用 dimension reduction 的 initial hidden 
semantic space 和以 SVC 來做 dimension 
reduction 的檢索結果，最差的則是僅使用
low-level 特徵所得出的檢索結果，由於 SVC
在使用上比較不容易找到適當的控制參
數，而這些參數又對檢索的結果具有相當程
度的影響，可能是造成其正確率偏低的原
因。由於使用者在標記相關與不相關的過程
中可能有出錯的情形發生，此時便可能會影
響整個相關回饋的檢索結果，我們所提出的
方法能夠抵抗相當程度的錯誤(noise)，如圖
三和圖四的結果所示，以 graph partition 來
做 dimension reduction 在檢索的結果表現上
仍會具有較高的正確率。另外，如表一所
示，CSPA 和 MCLA 和 SVC 比起來確實具
有更低的時間複雜度。 
 
影片中的語義事件偵測 
 
    影片中的語義事件偵測(semantic event 
detection)旨在藉由分析影片中的物件在空
間與時間上的變化情形，找出和人類語義上
具有連結性的關鍵特徵，進而從影片中偵測
出事件的發生。語義事件的偵測在監控保全
系統(video surveillance system)的應用中尤
其重要，當影片的拍攝時間越長，且數量越
多時，若直接以人為的方式去找尋影片中發
生某事件的片段，是相當不理想的作法，如
能利用語義事件偵測的方式來搜尋這些片
段，將會達到事半功倍的效果。 
    依照事件的型態來區分，事件的偵測可
 6
析與內涵式檢索的研究，到目前為止我們已
有相關期刊[1-3]及重要國際會議論文[4-13]
發表；而學生畢業論文方面，相關的則有博
士論文一篇[14]及碩士論文多篇[15-21]。其
中論文[17] 獲得 2005 年清華大學電資院碩
士論文優等獎，[19]獲得 2004 年中華民國資
訊學會最佳碩博士論文佳作獎，[20]獲得國
科會九十二年度碩士論文獎及 2003 年清華
大學電資院碩士論文優等獎。此外，我們另
有一篇已 submit 的期刊論文[22]目前正在
revise 中。 
 
 
五、參考文獻 
 
[1] C.T. Hsu and C.Y. Li, "Relevance Feedback 
Using Generalized Bayesian Framework with 
Region-Based Optimization Learning," IEEE 
Trans. Image Processing, vol. 14, no. 1, Oct. 
2005, pp. 1617-1631.  
[2] C.C. Cheng and C.T. Hsu, “Fusion of Audio and 
Motion Information on HMM-Based Highlight 
Extraction for Baseball Games,” IEEE Trans. 
Multimedia, vol. 3, no. 3, June 2006, pp. 
585-599. 
[3] C.T. Hsu and M.S. Hsieh, “Region Tracking for 
Non-Rigid Video Objects in a Non-Parametric 
MAP Framework,” Signal Processing: Image 
Communication, vol. 21, no. 3. pp. 235-251, Mar. 
2006. 
[4] Y.J. Yeh and C.T. Hsu, “Online Selection of 
Tracking Features Using Adaboost,” Proc. 
ICCCN, Aug. 2007. 
[5] S.Y. Yang and C.T. Hsu, “Background Modeling 
from GMM Likelihood Combined with Spatial 
and Color Coherency,” Proc. ICIP, 2006. 
[6] J.W. Tung and C.T. Hsu, "Learning Hidden 
Semantic Cues Using Support Vector 
Clustering," Proc. ICIP 2005, Genova, Italy, Sep. 
2005. 
[7] C.Y. Li and C.T. Hsu, "Soft Region 
Correspondence Estimation for Graph-Theoretic 
Image Retrieval Using Quadratic Programming 
Approach," Proc. ICME 2005, Amsterdam, 
Netherlands, July 2005.  
[8] C.Y. Li and C.T. Hsu, "Region Correspondence 
for Image Retrieval Using Graph-Theoretic 
Approach and Maximum Likelihood Estimation," 
Proc. ICIP 2004, Singapore, Oct. 2004.  
[9] C.Y. Li, M.C. Shih, and C.T. Hsu, "Image 
Retrieval with Relevance Feedback Based on 
Graph-Theoretic Region Correspondence 
Estimation," Proc. ICPR 2004, Cambridge, UK, 
Aug. 2004.  
[10] C.T. Hsu and C.W. Lee, "Statistical Motion 
Characterization for Video Content 
Classification," Proc. ICME 2004, Taipei, 
Taiwan, June 2004.  
[11] C.T. Hsu and M.S. Hsieh, "Segmentation of 
Non-Rigid Object in a Non-Parametric MAP 
Framework," Proc. ICIP 2003, Barceloma, Spain, 
Sep. 2003. 
[12] C.T. Hsu, and S. J. Teng, “Motion trajectory 
based video indexing and retrieval”, Proc. ICIP 
2002, Richester, NY, USA, Sep. 2002. 
[13] C.T. Hsu and Y.C. Tsan, “Mosaics of Video 
Sequences with Moving Objects,” Proc. ICIP 
2001, Thessaloniki, Greece, Oct. 2001.   
[14] C.Y. Li, “A Study on Region-based Issues for 
Intra-session and Inter-session Learning in 
CBIR,” PhD thesis, Department of Computer 
Science, National Tsing Hua University, 2006. 
[15] T.C. Lee, “Video Event Detection Using Color 
and Motion Temporal Histogram,” Master thesis, 
Department of Computer Science, National Tsing 
Hua University, 2007. 
[16] C.H. Yang, “Hidden Semantic Learning using 
Graph-based Cluster Ensemble,” Master thesis, 
Department of Computer Science, National Tsing 
Hua University, 2006. 
[17] J.W. Tung, “Learning Hidden Semantic Cues 
Using SVC,” Master thesis, Department of 
Computer Science, National Tsing Hua 
University, 2005. 
[18] T.L. Wang, “Kernel-Based Object Tracking 
Using Bayesian Framework,” Master thesis, 
Department of Computer Science, National Tsing 
Hua University, 2004. 
[19] C.C. Cheng, "Fusion of Audio and Motion 
Information on HMM-Based Highlight Extraction 
for Baseball," Master thesis, Department of 
Computer Science, National Tsing Hua 
University, 2004. 
[20] M.S. Hsieh, “Segmentation of non-rigid video 
object in a non-parametric MAP framework,” 
Master thesis, Department of Computer Science, 
National Tsing Hua University, 2003.  
[21] M.C. Shih, "Image Retrieval with Relevance 
Feedback Based on Region Correspondence 
Estimation," Master thesis, Department of 
Computer Science, National Tsing Hua 
University, 2003. 
[22] C.Y. Li and C.T. Hsu, “Image Retrieval with 
Relevance Feedback Based on Graph-Theoretic 
Region Correspondence Estimation, under 
revision in IEEE Trans. Multimedia. 
[23] P. Chang, M. Han and Y. Gong, “Extract 
Highlights from Baseball Game Video with 
Hidden Markov Models,” Proc. ICIP 2002, Sep. 
2002. 
[24] S.C. Chen, M.L. Shyu, W. Liao and C. Zhang, 
“Scene Change Detection by Audio and Video 
Clues,” Proc. ICME’02, 2002. 
[25] P. Bouthemy and R. Fablet, “Motion 
characterization from temporal cooccurrences of 
local motion- based measures fir video indexing,” 
Proc. ICPR’ 98, pp. 908-908, Aug. 1998. 
[26] R. Fablet, P. Bouthemy, and P. Pérez, 
“Nonparametric motion characterization using 
 8
表一、三種 dimension reduction 方法的時間複雜度
比較表，表格中所記錄的為執行 dimension 
reduction 所需的時間(秒)。 
 
 noise 
free 
5% 
noise 
10% 
noise 
average
CSPA 24.5 24 24 24.2 
MCLA 3 4 3 2 
SVC 15635.5 15529 17324 16162.8
 
 
 
 
 
 
圖一、我們提出的 hidden semantic space 示意圖。 
 
 
 
Average precison of 100 random queries with precision scope 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 1 2 3Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
(a) 
Average precison of 100 random queries with precision scope 70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 1 2 3Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
 
(b) 
 
圖二、(a) 50 個 query session 下 noise-free 的相關
回饋檢索效能分析；(b) 70 個 query session 下
noise-free 的相關回饋檢索效能分析。 
 
Average precison of 100 random queries with precision scope 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 2 3
Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
 
(a) 
Average precison of 100 random queries with precision scope 70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 1 2 3
Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
 
(b) 
 
圖三、(a) 50 個 query session 下 noise 為 5%的相關
回饋檢索效能分析；(b) 70 個 query session 下 noise
為 5%的相關回饋檢索效能分析。 
 
 
 
Average precison of 100 random queries with precision scope 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 1 2 3
Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
 
(a) 
Average precison of 100 random queries with precision scope 70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 1 2 3
Iteration times
pr
ec
isi
on
Low Level
Original Semantic
CSPA Reduced Semantic
MCLA Reduced Semantic
SVC Reduced Semantic
 
(b) 
 
圖四、(a) 50 個 query session 下 noise 為 10%的相
關回饋檢索效能分析；(b) 70 個 query session 下
noise 為 10%的相關回饋檢索效能分析。 
 10
 
 
圖九、車輛突然轉向事件的偵測示意圖。最上方的圖為 likelihood 的變化情形；中間的圖為事件發生的
真實情形(ground truth)；最下方的圖為我們偵測的事件發生情形。 
 
 
 
圖十、車輛在路口迴轉事件示意圖。最上方的圖為 likelihood 的變化情形；中間的圖為事件發生的真實
情形(ground truth)；最下方的圖為我們偵測的事件發生情形。 
session 中，很高興和多位此一領域的專家學者交換心得，對於我們未來在此方向的研究
頗有助益。 
此次 ICIP 大會出了一些小狀況，首先，在第一天 plenary talk 時大會即宣佈因大會
會議論文 CD 資料片出現了多篇論文無法讀取的情形，日後將再寄發更新過的 CD 片給
大家。此外，因首日的 oral session 場地由於 Marriott Marquis 旅館施工的關係，導致講
者及聽眾飽受干擾，因此，第二天起，有幾場 oral session 的場地更動，造成小小的混亂，
使得大家得在地下一二層原會場及第 40 層的兩間新場地之間奔波。 
 
二、考察參觀 
Atlanta 市區除了眾多的旅館及會議中心外，並無其它令人印象深刻之處。每日所見
多是從外地到 Atlanta 開會或洽公的人，如在我所住的旅館及路上最常見就是身上帶著
不同會議的 badge 或是 poster 捲筒的人。會場附近的餐廳就座落在旁邊的兩三條街上，
唯一可買到日用品的地方就在 Peachtree Center 中的 mall 裏，那裏也是大多數人那幾天
主要活動的範圍。 
 
三、攜回資料 
 
攜回資料有大會會議論文 CD 資料片一片。 
 
              
此次發表之論文的 poster                
each cluster as the GMM likelihood in the cluster mode. 
The background likelihood can be seen as a smoothed 
version of the GMM likelihood in terms of the color and 
spatial coherency.  Finally, we develop an adaptive 
thresholding scheme using Markov Random Field (MRF) 
for object detection. Moreover, in order to reduce the 
computation load in the clustering process, we also propose 
a filtering step to filter out some pixels from the clustering 
process. 
The rest of this paper is organized as follows.  Section 
2 presents our proposed method, which includes:  1) GMM 
learning and updating, 2) background modeling by 
clustering in the hybrid feature space, 3) the adaptive 
thresholding, and 4) the filtering step. Section 3 shows our 
experimental results and comparison. Finally, Section 4 
concludes this paper. 
 
2. PROPOSED METHOD 
 
2.1. GMM learning and updating 
We first use the adaptive GMM [1] to model the variation of 
each background pixel along the time. Let { }tvvv ,,, 21 L  be 
the history of visual features at a certain pixel observed over 
t  previous frames. The probability of observing tv  of 
current background pixel at location ),( yx=x  is 
approximated by 
( ) ( )∑
=
Σ∗=
K
i
titittit bgP
1
,,, ,,μvvx ηω ,                  (1) 
where K  is the number of Gaussians, ti ,ω  is an estimated 
weight of the i th Gaussian at time t , ti ,μ  and ti ,Σ  are the 
mean vector and covariance matrix of the i th Gaussian at 
time t , and  η  is  
( )
( )
( ) ( )⎭⎬⎫⎩⎨⎧ −Σ−− −
Σ
=Σ titti
T
tit
ti
dtitit
,
1
,,2
1
2
1
,2
,, exp
2
1,,
μvμv
μv
π
η .   (2) 
In (2), d  is the dimension of the feature vector tv . In this 
paper, we set K  to five and extract a 3-dimensional (i.e. d = 
3) color feature vector tv  in the perceptually uniform 
*** vuL  space. Assume that the three color channels *** vuL  
are nearly independent, the covariance matrix becomes  
{ }),,,( 2 ,,2 2,,2 1,,, dtitititi diagonal σσσ L=Σ .                       (3) 
The feature vector in each new pixel is then compared 
against the K  Gaussians to find a matched distribution. 
Here, we determine the new pixel matches a Gaussian when 
the pixel feature is within 2.5 standard deviation of a 
distribution [1]. 
In a dynamic scene, as the color feature of each pixel 
changes along the time, the weight ti,ω  is updated by 
( ) tititi M ,1,, 1 αωαω +−= −                       (4) 
where α  is the learning rate, tiM ,  is 1 when the current 
pixel matches the i th Gaussian and is 0 when unmatched.  
Note that, after the updating, the weights are renormalized 
to ensure that 1
1
, =∑
=
K
i
tiω . Similarly, when the current pixel 
matches the i th Gaussian, ti ,μ  and ti ,Σ  are updated as 
follows: 
( ) ttiti vμμ ρρ +−= −1,, 1                                 (5) 
( ) ( ) djv jtijtjtijti ,,2,1    , 1 2,,,2 ,1,2 ,, L=−+−= − μρσρσ        (6) 
where 
( )titit ,, ,, σαηρ μv= .                            (7) 
2.2 Background modeling 
Instead of using the learned GMM background likelihood to 
determine whether a pixel belongs to background or not, 
here we propose to include also the spatial and color 
features to construct the background model.  
Let [ ]kttkt bgP k xvvu x ,),(, =  be the 6-dimensional 
hybrid feature vector for the current pixel, where 
),( kkk yx=x is the spatial coordinates, tv  is the *** vuL  
feature vector, and )( bgP tk vx  is the GMM likelihood.  The 
density of current video frame in the hybrid feature space is 
estimated by  
                ( ) ( )∑
=
−=
N
l
ltktHkt f
K
N
f
1
,,,
1 uuu ,                      (8) 
where N  is the number of pixels in a video frame, lt ,u  is 
the hybrid feature vectors of the pixel lx , and fHK  is a 
kernel function with bandwidth 
fH .   
Next, we use mean-shift procedure [3] to classify each 
hybrid feature vector to its corresponding local maximum 
along the gradient direction. Thus, a set of neighboring 
pixels associated with the same local maximum (i.e. mode) 
is highly similar in this hybrid feature space. We then assign 
the pixel-wise background likelihood )( bgP tvx′  for each 
pixel by the GMM likelihood of its mode. Thus, the 
resulting background likelihood indicates a smoothed 
version of GMM in terms of spatial and color coherency.    
2.3 The adaptive thresholding 
Finally, we proposed using MRF [4] to design an adaptive 
threshold for object detection. In order to achieve a spatially 
