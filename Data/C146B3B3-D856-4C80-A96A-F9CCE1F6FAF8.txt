 2
行政院國家科學委員會專題研究計畫成果報告 
研製具有探索未知室內環境功能之影像導航自走車 
計畫編號：NSC 96-2221-E-129 -007 
執行期限：96 年 8 月 1 日至 97 年 9 月 30 日 
主持人：簡忠漢   聖約翰科技大學電機工程系 
計畫參與人員：陳冠廷、謝明錦、潘冠元 
 
一、中、英文摘要 
本計畫預計發展輪型智慧型機器人(或稱
自走車)自主探索室內環境之適應能力。使自
走車能透過各類感測器探索環境，據而自主
執行其任務。本年度已完成一部適合於室內
環境之差動式驅動自走車雛型。並結合多種
價格低廉之感測器，包括超音波感測器、電
子羅盤、CCD/CMOS 攝影機等。透過感測資
料融合以互補單一感測器在資料正確性與完
整性之不足。藉由超音波感測資訊我們完成
一沿牆直行並與牆壁保持一段安全距離之導
航控制方法。此外，我們亦提出一規避靜態
障礙物之方法。而搭配攝影機連續擷取之環
境影像，我們提出一套影像感測追蹤牆壁邊
線與沿牆之影像伺服探索技術。 
 
關鍵詞：追蹤牆壁邊線、規避靜態障礙物、
感測融合、影像伺服探索。 
 
Abstract 
In this project we present the development 
of an indoor exploration system for a mobile 
robot. We develop one exploration strategy by 
utilizing range-finding ultrasonic sensors and 
another exploration strategy by additionally 
incorporating vision sensors to navigate the 
mobile robot moving along the wall baselines. 
For the former strategy, we use the ultrasonic 
sensors to measure distances to the wall and 
apply the wall-following strategy to navigate the 
mobile robot based on the potential field method. 
In addition, we design an obstacle avoidance 
algorithm to avoid collisions. With the aid of 
vision sensors, we implement a visual tracking 
algorithm based on the randomized Hough 
transform technique to detect and track wall 
baselines of the environment. 
Keywords: wall-baseline detection, sensor 
fusion, visual navigation, obstacle avoidance. 
 
二、緣由與目的 
 
根據國際機器人協會/聯合國歐洲經濟委
員會(IFR/UNECE) 於 2004 年出版之統計資
料指出[1]：2003 年全球服務用智慧型機器人
已達 132 萬台，總價值約 37.9 億美元。預估
2004 至 2007 年服務用智慧型機器人將達
670.4 萬台，總價值約有 96.7 億美元，是一個
極俱發展潛力的市場。因此世界先進國家(如
美國、日本、歐盟、南韓)已積極展開許多智
慧型機器人之研發計畫[2]-[5]。 
雖然目前學界對於智慧型機器人的定義
莫衷一是，然而一般認定智慧型機器人應具
備自主性與適應性等兩項基本條件是無庸置
疑的。所謂自主性係指機器人本身的決策與
行為，完全由機器人本身運算處理或經由集
中式電腦處理並透過無線通訊傳達控制命
令，而非由人工操控。而適應性則是指機器
人對周圍環境變動能有察覺及因應之能力。
換言之，智慧型機器人必須能透過各種感官
來認知環境與現狀，且做為其正確的決策與
行為之依據，進而達成感知、規劃、反應之
行為模式。 
本計畫之題目為「研製具有探索未知室
內環境功能之影像導航自走車」，即希望發
展輪式智慧型機器人(簡稱自走車)具備「經由
各類感測器資訊以自主導航探索室內未知環
境」之適應能力。而在眾多感測技術中，我
們著重於發展機器人視覺感測技術及其應
用。藉由攝影機連續擷取之環境影像可用於
辨識地標、追蹤標線、與重建環境資訊等。 
國內外利用影像視覺於機器人探索室內
環境之相關研究眾多[6]。室內環境探索的相
關研究文獻可再細分為已知環境地圖之探索
[7-11]、結構化(structured) 未知環境之探索
[12-16]與未結構化(unstructured)未知環境之
探索[17-20]。本計畫初期主要著重於發展結
構化未知環境之探索。 
 4
2. Pan-Tilt 攝影機參數校正，IPM 模組程
式撰寫 
於本年度計畫中我們架設 Pan-Tilt 攝影
機於自走車前方，並完成攝影機參數校正與
反透射法(IPM)模組程式撰寫。其可用於追蹤
室內特定目標或鎖定特定特徵，做更精細之
分析辨識。此外經由適當之校正，亦可利用
攝影機取得之影像，透過 IPM 技術量測室內
地面目標之位置，其實作技術說明如下。 
藉由 IPM 轉換之技術，可將一投影至影
像座標平面之環境道路邊點，轉換為其相對
於攝影機之座標位置，反之亦然。如圖 4 所
示，我們在自走車上安裝一具固定式攝影
機，高度為H ，其中，m、n 為攝影機之像素
解析度。 
由於我們採用之攝影機並未提供攝影機
視角(水平視角 xφ ，俯仰視角 yφ )之資訊，且不
知架設於自走車上之攝影機座標系統{C}與
自走車座標系統{R}是否有水平偏移( γ )角或
俯仰偏移(θ )角之情況發生，故我們需設計一
攝影機校正之實驗，來驗證其參數資訊，使
反透視(IPM)轉換或透視(PM)轉換後之目標
值，更為精確。 
理論上，假若攝影機座標系統原點與自
走車座標系統原點是重合( 0==θγ )而無偏移
之情形發生，則相對於自走車座標系統之無
窮遠處點 ),0( ∞ ，其投射至影像平面之遁點
(vanishing point)座標應為 )
2
1,
2
1( −− nm ，意即為
影像平面中心點。不過，為考慮到可能有攝
影機偏移情形發生情況下，我們將遁點之座
標定為 ),( 00 βα ，則座標轉換公式如下式： 
 
1
2
tan
1
2
tan
22
1
0
1
0
−
+
−
+=
−
+=
−
−
n
yx
H
m
y
x
y
x
φββ
φαα
 (1) 
其中， x 與 y 為相對於自走車座標系統之環境
座標點， xφ 為攝影機水平視角， yφ 為攝影機
俯仰視角，α 與 β 則為環境座標投影至影像平
面之影像座標。 
 
圖 3 自走車電控系統架構圖 
θ
 
圖 4 環境平面之ㄧ點(0,L)投影至影像平面之示意圖 
若我們令
1
2
−= ma
xφα ， 1
2
−= na
yφ
β ， 0ααα ⋅= ab ，
0βββ ⋅= ab ，將(1)式改寫成 
 
ββ
αα
β
α
ba
yx
H
ba
y
x
−=+
−
−⋅=
−
−
22
1
1
tan
tan
 (2) 
我們目前已完成攝影機參數校正實驗，
其利用位於自走車前方地磚方格中心點，當
作環境已知參考座標點，於影像中量測其位
置，如圖 5(a)-(b)。量測方法包括對影像進行
邊緣化、二值化、侵蝕、膨脹與濾除非方格
點部分之前置處理工作，接著利用影像處理
中之標記(label)演算法，來分類出每個方格
點，以得到方格點中心之影像座標( ii ,βα )，
如圖 5(c)所示，總共得到 56 筆參照點數據。
將此數據代公式(2)計算，形成一過度限定
(over-determined)方程組，如式(3)所示。然後
以最小平方差(least square)之解法，解出攝影
機之校正參數，其結果如表 1 所示。 
表 1 攝影機之校正參數數據 
0α (影像平面遁點 x 座標，pixel) 158.8011
0β (影像平面遁點 y 座標，pixel) 84.7243 
xφ (攝影機水平視角，degree) 44.3015 
yφ (攝影機俯仰視角，degree) 29.9600 
 
 6
4.  運動規劃模組-沿牆前進法實作 
本年度計畫已完成利用超音波感測器之
資訊，達成自走車自主性巡邏室內環境之功
能。我們採用沿牆前進法做為運動規劃策
略，其可保證自走車成功環繞封閉性室內環
境一周，避免自走車在探索環境過程中，陷
入進退維谷之情況。 
如圖 7 所示，我們以本校電資大樓二樓
環境解說沿牆前進法之探索策略，假設自走
車於第(A)點位置開始環境探索，藉由一沿左
右牆前進並保持一段安全距離之準則，可使
自走車沿著如圖 7 中之紅色箭頭方向，由第(A)
點環境位置探索至第(B)點、第(C)點…直到第
(I)點，然後回到第(A)點之環境目標位置，故
成功達成一自主性之封閉迴路環境巡邏。 
自走車在探索環境過程中，藉由超音波
感測器以自走車車體為中心掃描量測
D8~D22 方向上之障礙物距離值，以判斷自走
車目前所身處之環境位置，並執行對應之探
索控制策略(如自走車直行或右轉…等)。其探
索流程如圖 8，執行步驟如演算法 4-1 所示。
如圖 9 所示，我們應用前述技術，使自走車
於本校電資大樓二樓巡邏室內環境。 
演算法 4-1：超音波感測器探索室內環境 
步驟一、首先利用超音波感測器細部掃描以
自走車車體為中心沿 D8~D22 方向上之障
礙物距離量測值，以蒐集環境空間之資
訊，並據以決定一導航控制策略。 
步驟二、執行步驟一所判斷之導航控制策
略，並且在自走車執行導航過程中，反覆
經由超音波進行粗略量測 D8(左)、D15(前)
與 D22(右)方向，藉以發現是否有異常狀況
之發生，例如環境空間資訊突然改變(進入
叉路口)或是發現障礙物等。如此則自走車
將會停止航行，回到步驟一重新細部掃
描。若無異常狀況之發生，則自走車將會
依循原本之導航控制策略，繼續探索航行
室內環境。 
E201
250
350
120 120
420(A) (B) (C)
(D)
(F)
(G)
(H)
(E)
(I)
 
圖 7 聖約翰科技大學電資學院二樓環境平面圖 
No
Yes
No
Yes
 
圖 8 基於超音波感測器探索室內環境之程式流程圖 
   
(1)                  (2)                 (3) 
   
(4)                  (5)                 (6) 
   
(7)                  (8)                 (9) 
   
(10)                  (11)                 (12) 
   
(13)                  (14)                 (15) 
 8
四、計畫成果自評 
 
在本年度的計畫中，我們主要針對室內
保全之應用場合，發展自走車自主性探索環
境與巡邏之功能。我們首先自行組裝設計一
自走車，並於其上架設超音波感測器、電子
羅盤、紅外線感測器與 CCD 攝影機，並自行
設計上述各感測器之軟、硬體介面。接著我
們提出一套依據超音波感測器量測周遭環境
之樣板資訊，判定自走車所處位置之環境幾
何資訊並制定因應之自走車導航策略。並採
用類似向量場之技術，設計一沿牆直行並與
牆壁保持一段安全距離之導航控制方法，可
使自走車能快速並順暢地探索室內環境空
間，並保證其可成功巡邏一封閉性室內環
境。此外我們亦提出一規避靜態障礙物之方
法，以避免自走車在探索環境過程中，發生
碰撞之情形。 
此外，我們亦提出一套影像感測追蹤牆
壁邊線與沿牆之影像伺服探索技術，可用於
克服超音波感測器偵測環境掃描周期較長之
缺失，且透過影像處理即時偵測牆壁邊線，
將可提供自走車探索環境時之自我定位。 
為驗證上述技術之效能，我們並於本校
電資大樓二樓環境進行自走車自主性探索環
境並巡邏之實驗。經由實驗結果驗證本計畫
所研製的環境探索技術，能有效達成室內探
索與巡邏功能。預期對於智慧型機器人於室
內探索環境之應用，例如保全巡邏、導覽服
務等具有實質之貢獻。 
將來我們亦藉由本文所研製之自主性探
索環境與巡邏的技術，來建立起自走車執行
任務時所需之環境地圖資訊，並嘗試發展特
徵定位演算法，來解決智慧型機器人在探索
環境與巡邏時，所遇到之 SLAM(simultaneous 
localization and mapping)的問題，以完成一具
有自主性探索環境與巡邏之保全機器人。 
本計畫有關影像感測追蹤牆壁邊線與沿
牆之影像伺服探索技術之成果已發表於 SICE 
Annual Conference 2007 [23] (如附件一)。 
 
五、參考文獻 
 
[1] 白忠哲，”智慧型服務機器人商機可期”，ITIS 產業
觀察 
[2] K. Chinzei, N. Hata, , F. Jolesz, and R. Kikinis, 
“Surgical Assist Robot for the Active Navigation in 
the Intraoperative MRI: Hardware Design Issues”, 
IEEE International Conference on Intelligent Robots 
and Systems, pp. 727-732, Oct. 2000. 
[3] K. Wada, T. Shibata, T. Saito, and K. Tanie, “Robot 
Assisted Activity for Elderly People and Nurses at a 
Day Service Center,” IEEE International Conference 
on Robotics & Automation, pp.1416–1421, May 2002. 
[4] B. Graf and A. Hans, “Robotic home assistant 
Care-O-bot II,” Second Joint Conference and the 24th 
Annual Fall Meeting of the Biomedical Engineering 
Society, pp. 2343–2344, 2002. 
[5] K. Osuka, “Rescue robot contest in 
RoboFesta-KANSAI 2001”, 26th Annual Conference 
of the IEEE Industrial Electronics Society, pp. 
132–137, Oct. 2000. 
[6] G. N. Desouza and A.C. Kak,; “Vision for mobile 
robot navigation: a survey,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, Volume 
24, Issue 2, Feb. 2002 Page(s):237 – 267 
[7] S. Atiya and G.D. Hager, “Real-Time Vision-Based 
Robot Localization,” IEEE Trans. Robotics and 
Automation, vol. 9, no. 6, pp. 785-800, Dec. 1993. 
[8] A. Kosaka and A.C. Kak, “Fast Vision-Guided 
Mobile Robot Navigation Using Model-Based 
Reasoning and Prediction of Uncertainties,” 
Computer Vision, Graphics, and Image 
Processing—Image Understanding, vol. 56, no. 3, pp. 
271-329, 1992. 
[9] M. Meng and A.C. Kak, “NEURO-NAV: A Neural 
Network Based Architecture for Vision-Guided 
Mobile Robot Navigation Using Non-Metrical Models 
of the Environment,” Proc. IEEE Int’l Conf. Robotics 
and Automation, vol. 2, pp. 750-757, 1993. 
[10] M. Meng and A.C. Kak, “Mobile Robot Navigation 
Using Neural Networks and Nonmetrical 
Environment Models,” IEEE Control Systems, pp. 
30-39, Oct. 1993 
[11] M.R. Kabuka and A.E. Arenas, “Position 
Verification of a Mobile Robot Using Standard 
Pattern,” IEEE J. Robotics and Automation, vol. 3, no. 
6, pp. 505-516, Dec. 1987. 
[12] H.P. Moravec and A. Elfes, “High Resolution Maps 
from Wide Angle Sonar,” Proc. IEEE Int’l Conf. 
Robotics and Automation, pp. 116-121, 1985. 
[13] H. Choset, I. Konukseven, and A. Rizzi, “Sensor 
Based Planning: A Control Law for Generating the 
Generalized Voronoi Graph,” Proc. Eighth IEEE Int’l 
Conf. Advanced Robotics, pp. 333-8, 1997. 
[14] E. Chown, S. Kaplan, and D. Kortenkamp, 
“Prototypes, Location, and Associative Networks 
(PLAN): Towards a Unified Theory of Cognitive 
Mapping,” Cognitive Science, vol. 19, no. 1, pp. 1-51, 
Jan. 1995. 
[15] S. Thrun, “Learning Metric-Topological Maps for 
Indoor Mobile Robot Navigation,” Artificial 
Intelligence, vol. 99, no. 1, pp. 21-71, 
[16] N. Ayache and O. D. Faugeras, “Maintaining 
Representations of the Environment of a Mobile 
Robot,” IEEE Trans. Robotics and Automation, vol. 5, 
no. 6, pp. 804-819, 1989. Feb. 1998. 
[17] Michael Csorba , “Simultaneous Localisation and 
Map Building,” PhD Thesis, the Department of 
Engineering Science, University of Oxford. 
SICE Annual Conference 2007 
Sept. 17-20, 2007, Kagawa University, Japan 
1. INTRODUCTION 
 
Incidents of home invasion, fire, or lethal gas leakage 
have been reported everyday in the newspaper and may 
happen at anytime to anyone at home. Home security 
has been the main concern of almost all persons today in 
order to protect their safety and valuable property. In 
recent years, many security robots have been developed 
[1]-[17] to assist the repetitive and hazardous tasks of a 
patrol guard for home security. 
For an intelligent patrol robot to provide a wide area 
of applications, it must be equipped with high level of 
autonomy and adaptation to the changing environment. 
Typically, the robot may need to gather environmental 
information through a variety of sensors, build up an 
understanding of its surroundings based on an internal 
model for the environment and make decisions to take a 
subsequent action. All of these must be done 
automatically by the processing units of the robot with 
little a priori knowledge about the environment. 
Besides performing the boring patrol tasks, an 
intelligent patrol robot in home environment may be 
further required to carry out some human-robot 
interactions to create a comfortable experience for 
people and provide appropriate feedback to remote 
operators. 
In this paper we mainly focus on developing three 
functionalities, namely, home security monitoring, 
network assisted interactions, and self-navigation patrol, 
for a wheeled mobile robot toward the goals of the 
afore-mentioned intelligent patrol robot system.  
The remainder of the paper is organized as follows. 
The hardware and software architecture of the mobile 
robot to provide home security function is introduced in 
Section 2. Two kinds of network assisted interactions 
are presented in Section 3. An self-navigation patrol 
system using ultrasonic and vision data fusion is 
described in Section 4. Finally, conclusions are given in 
Section 5. 
 
2. HOME SECURITY MONITORING 
 
Home security is the main goal of our patrol robot 
system. Therefore, we first integrate a variety of sensors 
on the robot, including CCD camera, ultrasonic sensors, 
temperature sensor, human sensors, and gas detectors to 
gather environmental information and detect abnormal 
events including fire alarm, intruder alert and lethal gas 
leakage. 
We adopt an X80 robot by Dr. Robot Inc. [4], to 
provide a testbed for subsequent experiments. The 
mobile robot has two rear driving wheels and one front 
castor wheel. The hardware setup and sensor allocation 
of the mobile robot is shown in Fig. 1. The sensor 
readings except the visual data are collected by a 
PMS5005 controller and are periodically accessed by a 
monitor program running on an onboard computer. The 
vision data from the CCD camera are also captured into 
the onboard computer by use of the monitor program. 
In addition, the monitor program transmits the sensor 
data to the remote control center through wireless 
network. We design a networked graphical user 
interface on the remote control center, as shown in Fig. 
2, to display readings of the sensor measurements, 
monitor the environmental images, send commands to 
the robot, and allow the remote operator to manually 
control the movement of the robot. 
When abnormal events are detected, the monitor 
program will make a loud noise and send a message to 
the control center via the Internet. 
Development of a Patrol Robot for Home Security with Network Assisted Interactions 
Chia-Wei Chang, Kuan-Ting Chen, Hsiu-Li Lin, Chih-Kai Wang, and Jong-Hann Jean 
Department of Electrical Engineering, St. John’s University, Taiwan 
(Tel : +886-2-2013131; E-mail: jhjean@mail.sju.edu.tw) 
 
Abstract:  In this paper we present the development of a patrol robot system for home security with some 
considerations on its interaction functionalities. The system integrates a variety of sensors to gather environmental 
information and detect abnormal events including fire alarm, intruder alert and lethal gas leakage. To facilitate a 
security robot to live with people in a home environment, we implement some dedicated human-robot interactions, 
including a face mask with several facial expressions and a force feedback steering wheel controller. All of the sensor 
measurements and the dedicated interactions can be remotely accessed via the Internet. We further design an indoor 
patrol algorithm that can autonomously command the mobile robot to move randomly, avoid obstacles, or steer along a 
wall baseline based on ultrasonic and vision data fusion. Finally, we apply the patrol robot system to patrol the second 
floor of EE building of our campus and provide several experimental results for validating its performance. 
 
Keywords: intelligent robots, human-robot interaction, networked systems, visual navigation. 
 
operated in self-patrol mode or remote control mode. In 
remote control mode, the remote operator manually 
navigates the robot by sending commands via wireless 
network with the aid of the monitored environmental 
images. 
In self-patrol mode, the mobile robot moves 
randomly at first. When there is an object detected in 
the front direction by the ultrasonic sensor, the robot 
will further check all sensor readings to make clear if 
the object is an obstacle or a wall. When obstacles are 
encountered, the mobile robot performs the obstacle 
avoidance algorithm developed in [18] to bypass the 
obstacle and avoid collisions. 
When a wall is detected, the mobile robot takes the 
wall-following action, because wall-following is a 
simple but robust navigation strategy and has been 
shown to be appropriate in indoor environments [19]. 
The reason why we use the wall-following method is 
that it does not require the knowledge about the layout 
or shape of the indoor environment. As a result, there is 
no need to have a CAD model of the environment in 
advance and it can be easily applied to patrol various 
complex environments with simple preparation. 
Then we apply a previously developed visual 
navigation controller [18], as will be briefly described in 
the next subsections, to steer the robot moving along a 
wall baseline based on ultrasonic and vision data fusion.  
While performing the patrol task, the mobile robot 
detects abnormal events by use of the temperature 
sensor, gas detector, and human sensors. When 
abnormal events are detected, it will send a message to 
the control center to notify the remote operator.  
 
4.2 Detection of wall baselines by ultrasonic and 
vision data fusion 
 
As indicated in Fig. 7, we can detect a wall in a 
corridor by visually detecting the wall baseline. Its 
advantage over detecting by ultrasonic sensors is that 
the wall baseline provides not only the positional 
information but also the relative orientation between the 
mobile robot and the wall, which is helpful for 
subsequent robot navigation. 
For this purpose, we apply a line detection technique 
[20] based on the RHT method to the edge image of the 
corridor. However, it seems difficult to extract wall 
baselines purely from visual information, especially in a 
cluttered corridor, because many background lines may 
be detected from the edge image, and it is not easy to 
identify which are the true wall baselines.  
Start
Remote 
Control
Self 
Patrol
Obstacle 
Detected
Random 
Walk
Obstacle 
Avoidance
Wall Baseline 
Detected
Wall 
Following
No
Yes
Yes
No
Yes
No
Sensor 
Measurements
Fire Alarm
Intruder Alert
Gas leakage
Send Message to 
Control Center
Yes
Yes
Yes
No
No
No
Send 
Message to 
Operators
 
Fig. 6 Flowchart of the self-navigation patrol algorithm 
 
   
(a)        (b)      (c) 
   
(d)         (e) 
Fig. 7 Application of the wall-baseline detection system 
to a typical image of a corridor. (a) Original image. (b) 
Edge image. (c) Filter out edges above the floor. (d) 
Lines detected by applying the RHT method. (e) Retain 
possible lines according to the range information 
obtained by ultrasonic sensors. 
As a remedy, we first use the inverse perspective 
mapping (IPM) technique [21] to filter out edges above 
the floor and then incorporate the range information 
obtained by the ultrasonic sensors to retain possible 
baseline edges. 
 
4.3 Wall-following by visual navigation 
 
In the following, we apply a previously developed 
visual servo controller [21] to achieve wall-following of 
the mobile robot. 
Consider the mobile robot navigates along a wall 
baseline in a corridor, as depicted in Fig. 8. The wall 
baseline has an inclined angle φ  with respect to the 
z-axis of {R0}. Moreover, for compensating the time 
Feb. 2005, pp. 97 – 105. 
[12] A. Treptow, G. Cielniak, and T. Duckett, “Active 
people recognition using thermal and grey images 
on a mobile security robot,” 2005 IEEE/RSJ 
International Conference on Intelligent Robots and 
Systems, Aug. 2-6, 2005, pp. 2103 – 2108. 
[13] J. Huang, S. Ishikawa, M. Ebana, H. Li, and Q. 
Zhao, “Robot Position Identification by Actively 
Localizing Sound Beacons,” Proceedings of the 
IEEE Instrumentation and Measurement 
Technology Conference, April 2006, pp. 1908 – 
1912. 
[14] Y. Kim, H. Kim, S. Yoon, S. Lee, and K. Lee, 
“Home Security Robot based on Sensor Network,” 
2006 SICE-ICASE International Joint Conference, 
Oct. 2006, pp. 5977 – 5982. 
[15] Y. Kim, H. Kim, S. Yoon, S. Lee, and K. Lee, 
“Ubiquitous Home Security Robot Based on 
Sensor Network,” IEEE/WIC/ACM International 
Conference on Intelligent Agent Technology, Dec. 
2006, pp. 700 – 704. 
[16] T. Chien, K. Su, and J. Guo, “The multiple 
interface security robot - WFSR-II,” IEEE 
International Workshop on Safety, Security and 
Rescue Robotics, June 2005, pp. 47 – 52. 
[17] D. Zeng, C. Xie, and X. Li, “Design and 
implementation of a security and patrol robot 
system,” 2005 IEEE International Conference on 
Mechatronics and Automation, Vol. 4, 2005 
pp.1745 - 1749. 
[18] Jong-Hann Jean, Jian-Hong Lai, and Chia-Wei 
Chang, “Development of a Self-navigation Patrol 
Robot in Indoor Environments Based on Ultrasonic 
and Vision Data Fusion,” 13-th International 
Conference on Advanced Robotics, Aug. 21-24, 
2007 
[19] J. C. Latmobe and N. Ahuja, “A potential field 
approach to path planning,” IEEE Transactions on 
Robotics and Automation, vol. 8, no. 1, pp. 23-32, 
Feb. 1992. 
[20] L. Xu, E. Oja and P. Kultanen, “A new curve 
detection method: Randomized Hough transform 
(RHT)”, Pattern Recognition Letters, No. 11, pp. 
331-338, 1990. 
[21]  Jong-Hann Jean, Tien-Pao Wu, and Feng-Li Lian, 
“A Visual Servo Controller for Lateral Navigation 
of Mobile Vehicles in Path Tracking Applications,” 
2006 IEEE Conference on Systems, Man, and 
Cybernetics, Taipei, Taiwan, Oct. 8-11, 2006. 
 
報告內容
 
一、 參加會議經過 
 
2008年日本 SICE學會年會暨精密儀器、控制與資訊科技國際會議於 97年 8月 18日至
8月 22日於日本東京近郊調布市的 University Electro-Communications舉行。日本 SICE學會
為中華民國自動控制學會之姐妹會，會員高達 8,000 餘人。每年我國的自動控制學會年會暨
自動控制研討會，SICE學會皆會組團參與，足見其與我國自動控制學界之友好關係。 
 
本屆 SICE2008會議之議期主要有 4天，會議前一天(19日)為大會的Workshop，主題為
Workshop on Model-Based Development of Embedded Systems。晚上為Welcome Reception，其
後的三天即為主要的議程，此次共計有來自 29個國家近 760篇論文長摘要(extended abstract)
的投稿，錄取了 693篇論文，分散在 79個一般分組討論及 54個規劃分組討論中進行口頭報
告，另外大會安排了 3場 Plenary Talk、6場 Invited Speech與 1場 Invited Special Speech，平
均配置於 3 天，議程緊湊而豐富，為因應如此多的論文報告，大會特地將每天的報告時間分
為三個時段，而每個時段分為 21個平行場次，為聆聽此等論文的報告，個個與會者均在會場
穿梭奔波，不亦忙乎。 
 
本次大會的 General Chair為 Prof. Kojiro Hagino (The Univ. of Electro-Communications 
(UEC), Japan)，而最辛苦的 Program Chair則為同校的 Prof. Takashi Kida，值得一提的是大會
的 Advisory Board也扮演相當重要的角色，此次兩位 Co-Chairs分別為 SICE去年的 President, 
Prof. Susumu Tachi，另外一位則是 SICE今年的 President, Dr. Kazuo Kyuma。由於 SICE年會
為日本控制學界的盛事，參加此項會議已形成日本控制學者的傳統，因此此次會議與會註冊
人數近千人。為增加會議的知名度及國際性，主辦人除上述論文報告外，另外安排了三場
Plenary Talks，分別是(1) Dr. Hiroaki Kitano (The Systems Biology Inst., Japan)，講題為 
“Biological robustness”，(2) Prof. Martti Hallikainen (Helsinki Univ. of Technology, Finland)，講
題為 “Microwave Radiometry for Remote Sensing of the Earth Surfaces”，(3) Prof. John Baillieul 
(Boston Univ., USA), 講題為“The Psychology of Human-Robot Interaction”，至於 Invited Special 
Speech的 Speaker則為 Prof. Kohzo Hakuta (The Univ. of Electro-Communications, Japan)，講題
為 “Manipulating and Controlling Atoms and Photons Using Optical Nanofibers”。參加完了會議
之後，深感收獲豐富，亦同時達到了交友與交流的目地，可謂一舉數得。 
 
此次會議我參與投稿由連豊力教授彙整組織之「Emergent Control Technologies for 
Human Living Environment」專題。此專題乃是針對多個互動主體形成的系統，分別由不同角
度探討其特性與控制設計，共由六篇論文組成。此議程被安排在 20日下午 15點 30到 17點
10 分的場次報告，詳細議程如表 1。希望經由參與此學術會議加強台灣與日本控制界學者相
互之間的交流，進一步討論未來國際合作之契機。 
 
 
 
二、與會心得 
 
日本 SICE學會為中華民國自動控制學會之姐妹會，會員高達 8,000餘人。每年我國的自
動控制學會年會暨自動控制研討會，SICE學會皆會組團參與，足見其與我國自動控制學界之
友好關係。 
 
本次會議大體上可以看出，許多來報告的有年輕的學者及博士生，日本人在研究的主題
上，和歐美不同，有其執著之處，可長期有一批學者做深入的探討，以致於其研究成果經歷
年來的累積，相當豐碩。 
 
在此次的參與活動中，除了能夠同時聆聽相關研究課題的論文發表之外，同時，難得有
機會與日本控制界友人，交換第一手的的研究資料，並且一起討論相關研究方向與研究心得
等活動，此項交流乃對於我國學術的成長與發展有著良性的助力。同時，就由本團隊的參與
此次活動，能夠為臺灣於國際間學術活動的提高能見度，而且與國際間重要學者的意見交流
亦會強化我國在此項研究領域的學術地位，讓國際間瞭解我國的學術研究成果與進展。 
 
此次電氣通信大學的會議場地並不理想，距離住宿的旅館約 40 分鐘的路程，交通的不
便，多少影響了參加會議的行程。東京地區不愧是日本首善之區，此行有幸經過，讓人對日
本交通基礎建設與繁華卻整潔的市容，印象深刻。 
 
三、考察參觀活動
 
無 
 
四、建議 
 
無 
 
五、攜回資料名稱及內容 
 
SICE Annual Conference 2008 Proceedings (CD) 
 Call for Paper of SICE Annual Conference 2009 
 相關研究計畫成果報告書面簡報 
 
六、其他 
感謝國科會贊助此次參與國際會議之旅費與此項研究計畫之研究經費。 
所發表之學術論文資料如附件。 
- 1438 -
programmable multi-functional I/Os, an eight-channel 
10-bit ADC, two 10-bit DACs, a UART receiver and 
transmitter, and plenty of clock sources to generate 
periodic interrupts. The operating frequency of the 
SPCE061A is up to 49.152MHz, which assures its 
capability of real-time digital audio signal processing. 
Consequently, the SPCE061A is applicable to the areas 
of digital sound processing and voice recognition. 
The overall system block diagrams are depicted in 
Fig. 3. The speech data is sampled in 8 KHz for speech 
recognition through the microphone input channel 
(ADC channel 7) of SPCE061A, which has built-in 
amplifier and auto gain controller (AGC). The sound 
signal is also preprocessed by three microphone 
modules mounted around the entertainment robot and 
then sampled by three AD channels of SPCE061A to 
detect the sound direction. The robot responds the 
interactive dialogue by sending the voice data from the 
database through the two DAC channels of SPCE061A. 
There is also an animated head, as shown in Fig. 4, 
mounted on the top of the robot body. The robot 
controls the head gesture and its facial expression by 
sending commands to the servo controller of the head 
system through UART channel of SPCE061A. 
 
3. SPEECH RECOGNITION AND 
DIALOGUE SYSTEM 
 
Vocal communication is the most effective way in 
human communication. Consequently, we first try to 
provide capabilities of speech recognition and 
interactive spoken dialogue for the entertainment robot. 
The flowchart of the speech recognition and dialogue 
system is depicted in Fig. 4. To implement the 
interactive dialogue system, we first record the voice 
commands and the corresponding voice replies, 
compress these speech data by the SUNPLUS 
SACM_S480 algorithm, and store the compressed data 
in the built-in 32K-word flash ROM of the SPCE061A. 
The SUNPLUS SACM_S480 algorithm is a speech 
compression algorithm which can retain a good 
resolution of the original speech/audio with a low data 
compression rate of 4.8 Kbps, and hence dramatically 
save memory usage. Besides the built-in 32K-word 
flash ROM of SPCE061A, we expand the voice storage 
space up to 512 KB with an additional SPR4096 module, 
as shown in Fig. 5. 
For recognition of voice commands, we utilized the 
speaker-dependent speech recognition library of 
SPCE061A, which supports training and recognition up 
to 6 voice commands. Each voice command is trained 
twice and only the voice data of the first 1.3 second will 
be recorded for subsequent recognition process. 
During interactions, the robot samples the voice 
command input through the MIC_In ADC channel of 
SPCE061A and compares the input voice command 
with the ones previously stored in the flash ROM. Once 
a command is detected and recognized, the robot will 
play back the corresponding voice reply through the two 
DACs of SPCE061A. 
 
Fig. 1 System setup of the entertainment robot 
 
 
Fig. 2 SPCE061A EMU board developed by SUNPLUS 
Technology Co., Ltd. 
 
 
Fig. 3 The overall system block diagrams 
 
Speech 
Data
Voice attribute
extraction
Flash
Memory
Write
Train
Speech 
Data
Voice attribute
extraction
Read
Trigger
Speech Recognition
End
 
Fig. 4 The flowchart of the speech recognition and 
dialogue system 
 
- 1440 -
 
 
Fig. 7 Parallax Servo Controller 
 
 
Fig. 8 The microphone module used for detecting arrival 
time of a sound signal. 
 
Fig. 9 The layout of three microphone modules mounted 
around the robot body 
 
Fig. 10 Output waveforms when the sound source is 
located near one of the two microphone modules 
 
Fig. 11 Output waveforms when the sound source is 
located in the middle of two microphone modules 
 
 
Fig. 12 Output waveforms to measure the duration of a 
sound event 
 
 
REFERENCES 
 
[1] J.L. Burke, R.R. Murphy, E. Rogers, V.J. 
Lumelsky, and J. Scholtz, “Final report for the 
DARPA/NSF interdisciplinary study on 
human-robot interaction,” IEEE Transactions on 
Systems, Man, and Cybernetics, Part C: 
Applications and Reviews, Vol. 34, Issue 2, May 
2004 pp.103 – 112. 
[2] K. Wada, T. Shibata, K. Sakamoto, and K. Tanie, 
“Quantitative Analysis of Utterance of Elderly 
People in Long-term Robot Assisted Activity”, 
Proceedings of  2005 IEEE International 
Workshop on Robots and Human Interactive 
Communication, August 13-15, 2005, pp. 267 
-272. 
[3] T. Miyashita and H. Ishiguro, “Human-like natural 
behavior generation based on involuntary motions 
for humanoid robots,” Robotics and Autonomous 
Systems, Vol. 48, Issue 4, pp.203-212, 2004 
[4] http://www.aibosite.com/ 
[5] http://www.wowee.com/ 
[6] http://www.pleoworld.com/ 
[7] http://www.zmp.co.jp/e_home.html 
[8] http://www.lego.com/ 
[9] http://paro.jp/english/ 
[10] T. Shibata, et al., Ϙ Emotional Robot for 
Intelligent System -Artificial Emotional Creature 
Project,ϙ Proc. of 5th IEEE Int'lWorkshop on 
ROMAN, pp. 466-471 (1996) 
[11] T. Shibata, et al., “Mental Commit Robot and its 
