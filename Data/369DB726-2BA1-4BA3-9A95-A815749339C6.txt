如真實影片般逼真的互動式立體虛擬世界
此計畫之目標是在我們新成立的虛擬實境實驗室建立一組虛擬實境系統，包含以下新
的主要特色︰第一，以高解析度如相片影像般真實的 3D 虛擬世界來呈現場景以及其中可與
使用者互動的物件；第二，那些在場景中會因自然界因素而運動的物件，例如葉子，雲朵
或者溪流等，也將以動態呈現以增加其真實感；第三，依據觀賞者的位置自動調整影像視
差，以改善立體視覺舒適性以及 3D 立體景深；最後，發展以影像為基礎的人機互動方法，
讓使用者能在這 3D 虛擬世界中“自然地”自由遊走。兩大最具挑戰性的問題在於全自動化
場景建構與無需配戴任何硬體的人機互動模式。
關鍵詞: 虛擬實境、立體環場影像、影像式電腦繪圖技術
Video-realistic Interactive Stereo Virtual World
The goals of this project is to establish a video-realistic interactive virtual reality (VR)
system, which consists of the following major new features: first, high-resolution panoramic
images will be used for representing both the 3D scenes and the manipulatable objects; second,
the moving objects in the scene caused by nature’s forces, such as leaves, clouds or streams, will
be animated by image-based rendering techniques to enhance the realistic appearance of the
virtual environment; third, the image parallax will be auto-adjusted according the viewer’s
position to improve the stereo-viewing comfort and to correct 3D perception (when necessary);
finally, purely vision-based navigation and human-computer interaction methods will be provided
to allow a device-free navigation within the virtual world. The developed VR system will be
especially suitable for virtual touring or 3D game applications where the realism of the scene is
critical and spatially limited navigating paths are sufficient.
Keywords: virtual reality, stereo panoramic images, image-based rendering
2度的圓柱環場影像，且相機水平擺放，並利用影像的極線幾何，在多個環場影像中找出其
相對應點關係，依找出的相對應點，轉換成單位向量，使用八點演算法和基本矩陣來求解，
進而求得相機在不同位置間的相對關係，並利用此資訊來做重建工作。而 L.Smadja 等[4]
在 2006 年提出，使用線性相機，其方法為單中心的環場拍攝，相機在二個不同的位置且
可以有不同的角度，拍攝圓柱體環場影像，利用影像的相對應點，並使用基本矩陣來求解
而估測相機的相對位置。另外 R. MOLANA 等[5]在 2006 年提出在影像中以相對應點的位置
找出相對應射線的關係來估測相機間的位置，而這種方法並不須知道相機的鏡頭的特性，
只要能找出其射線關係就能做估算。H.Ishiguro 等[6]在 1992 年提出在二個位置使用多
中心的拍攝方法來估測相機間的相對位置，多中心的拍攝方式為相機和旋轉軸有一半徑距
離，相機繞著半徑對場景拍攝環場影像，再經由對應點的關係來估測相機的位置。
動態環場影像製作
為使環場影像有更逼真的效果，加入動態影像使場景中的景物也會自行運動。這樣混
合照片與影片的表現方法在 1996 年與 1998 年便有人試著做過[7,8]，也有人將整個環場
影像做成動態，但這需要有專業設備的支援且變動具有有限的持續時間[9,10]。在影像變
動的持續時間上，2000 年， A. Schödl 等人分析一小段影片的結構，然後將它合成一段
看起來相似且任意長度的新影片，他們稱為 Video Textures，它可以提供影像流連續不斷
地變化，為靜態影像增添動態特性[11]。2005 年， A. Agarwala 等人將一台攝影機平移
拍攝的影片自動化地建立成 Panoramic Video Textures [12]。他們在任意時間拍攝場景
的一小部份，如此，把大量資料經過最佳化處理後，將選擇的影片碎片做時間上與空間上
的接合，使結果影片能遍佈在場景中且同時地表現運動。
立體環場影像景深研究
Chiao Wang 和 A. A. Sawchuk[13]在 2006 年提出以區塊為基礎做視差調整，論文中
左右環場影像利用 Mean-shift 演算法的分割，然後在利用光流（Optical Flow）演算法
找出左右環場影像中區塊的移動，再增大其視差，讓整個立體環場影像更具深度感。Colin
Ware[14]在 1998 年使用 3D model 建購的三維場景提出一個演算法，取決於三維場景顯示
時的深度，自動調整立體觀測的參數，讓觀看者保持最佳的視差。而 Chiao Wang 和 Shmuel
Peleg 所提出的視差調整方法[13][15]，皆是在事前先做好調整，而不是在觀看時即時做
變動。
4接著我們加入動態影像於立體環場影像中，使畫面更自然，但若顯現的時間有限則會
降低了真實度，如果準備長時間的影片又會因檔案大而影響執行速度且佔空間，所以折衷
採重複播放的方式，然後處理反覆點令影片自然順暢，觀賞者不會因反覆點出現中斷而破
壞融入感。將 graphcut 延伸到三度空間上來處理影片頭尾部分的接合，為了使動態部分的
播放為自然的連續循環，我們將動態影片的頭尾循環的部分做連接。首先把影片的前後選
擇適當部分重疊，然後取出重疊的部份來接合處理，接著找出前後重疊部分相鄰相似的面
後，去掉前後影片多餘的部份做接合，由於切面附近相似，所以接合後使畫面從頭播放能
自然連接顯示成為循環影片，如圖四。
圖四 循環式影片製作
在立體景深方面，因為影像式的虛擬實境當觀看者需要拉近（或拉遠）景物的效果，
通常都會透過左右兩眼影像直接的放大（或縮小）來達到這樣的效果，但是放大（或縮小）
僅僅只是將影像放大，利用「近景成像較大；遠景成像較小」的心理因素來達到拉近（或
拉遠）景物的效果，但是因為視差也相對的改變了，所以並沒有使虛擬世界中物件真正的
拉近或拉遠。換句話說，就是本來想要拉近的物件，因為放大而便更遠了；反之，因為縮
小而便更近了。
我們可以根據觀看者所站的位置計算出基準視差D所形成的視深，在縮放m倍後，視
差改變成mD。可以經由公式一 得知基準視差D所形成的視深 Z，「如圖五（左）所示」。
DTZ
B D
  公式一
若是放大的話，則將視深往前移 ( ) /Z T m ；若是縮小的話，則將視深往後移
( ) /Z T m 。移動 ( ) /Z T m 後會得到一個合適的視深 Z ，可以經由公式二得知。
Z TZ T
m
   公式二
64. 結果與討論
我們提出立體環場影像和水平環場影像兩種位置估測的方法，這兩種方法都需要事前
的相機內部參數的校正，此外水平環場影像法的前提是所有影像在拍攝過程必須要保持水
平，接著就可以利用提出的方法來估測影像間的相對位置。而在實驗的設計上，我們把可
能影響到估測結果的參數都以模擬實驗的方式加以討論，例如半徑、相機本身的旋轉角度，
和相對應點的誤差等，而實驗結果顯示半徑、相機本身的旋轉角度的錯誤是對估測結果影
響比較小的，如圖七與圖八，相對應點的誤差則是會對估測結果影響較大，並由實驗數據
結論至少須要 30 個相對應點才能得到比較可靠的估測結果，如圖九。
（ａ） 水平環場影像ω的誤差為±5度
（b） 水平環場影像ω的誤差為±50度
圖七 相機旋轉角度的誤差對估測結果的影響
8由於環場影像拍攝地點大多在室外，所以場景內容多為自然景物。因此，我們主要目
標為針對自然景物做動態影像處理，而自然環境的場景也較令觀賞者有身處在另一個遼闊
空間的感覺。除此之外，我們也不僅僅以自然景物為處理對象，亦針對有週期性運動的自
然景物為處理對象。這是由於不具週期性運動的景物，在令其重複循環顯示時，會因其動
作重複，造成與我們印象中的常識不同，而明顯察覺與現實情況不同，破壞融入感。所以
在我們的情況中，選擇具有週期性運動的對象較為合適，如圖十，透過實驗結果，我們可
以發現利用處理過的影片，加上循環播放模式，大大減少了在動態影像取得上的困難度。
並且可以提升環場影像中靜態圖片的真實度。
（ａ）水波影片擷取畫面 （ｂ）樹木影片擷取畫面
（ｃ）海浪影片擷取畫面
圖十 循環性動態影像範例成果
我們利用縮放後的左右兩張影像做 shiftD 的相對平行移動，若是放大的話，左邊影像往
右平行移動 shiftD ，右邊影像往左平行移動 shiftD ；若是縮小的話，則反之平行移動 shiftD 。如
此一來，不僅可呈現景物的縮放，也可以讓立體圖在縮放後的深度有正確的顯示。圖十一
是我們在戶外場景以樹為主要物體所拍攝的一組左右影像。表一為視差調整的數據分析，
圖十一（a）紅藍線在原圖的時候距離為 0.03 公尺（基準視差）其相對的基準視深為 2.571
公尺，經由放大後 1.4 倍後視差增大成 0.042 公尺如圖十一（b），經由公式的運算我們接
下來做 0.026 公尺的視差調整如圖十一（c）其相對的視深為 5.487 公尺。經由調整後的
視差為 0.016 公尺，而調整後的視深為 0.979 公尺，由這個變小的視差可以使人產生較近
視深的景物（5.487 公尺變為 0.979 公尺），來符合我的利用放大後 1.4 倍影像，且將樹
木向前移。
10
Conferences:
1. F. Huang, R. Klette, and Y. H. Xie. "Sensor Pose Estimation from Multi-Center
Cylindrical Panoramas", in PSIVT’09, Tokyo, Japan, January, 2009, LNCS 5414,
Springer, pp 48~59. [EI]
2. F. Huang, R. Klette, and Y. H. Xie. "Pose Estimation for Sensors Which Capture
Cylindric Panoramas", in CIARP’08, Havana, Cuba, September, 2008, LNCS 5197,
Springer, pp 593~601. [EI]
3. F. Huang and Z.-H. Lin. "Stereo Panorama Imaging and Display for 3D VR System", in
CISP’08, Sanya, Hainan, China, May, 2008, Vol. 3, pp 796~800. [EI]
學生畢業論文:
。王浩駿. "Video Textures Synthesis by Vector Quantization and Image Inpainting", master
thesis, CSIE, NIU, 2009.
。林思慧. "Panoramic Image-based Virtual Reality System", master thesis, CSIE, NIU,
2008.
。林智浩. "Stereo Panorama Acquisition and Visualization for Virtual Reality
Applications", master thesis, CSIE, NIU, 2008.
。張殷瑋. "Rotating Sensor-line Camera Calibration and Pose Estimation", master thesis,
CSIE, NIU, 2009.
。謝允浩. "Sensor Pose Estimation from Cylindrical Panoramas", master thesis, CSIE, NIU,
2008.
12
計畫成果自評
此計畫之目標(灰底部分)與成果如下，綜合自評緊接在後。
(一) 以高解析度如相片影像般真實的 3D 虛擬世界來呈現場景以及其中可與使用者互動的
物件。
實作 - 所開發的技術應用於建構宜蘭香格里拉休閒渡假飯店虛擬實境網站
宜蘭香格里拉飯店外觀
宜蘭香格里拉飯店大廳
宜蘭香格里拉飯店客房
互動物件舉例︰翻書與茶杯
理論 - 林思慧 "Panoramic Image-based Virtual Reality System", master thesis, 2008.
14
理論 - S.-W. Sun, H.-Y. Lo, H.-J. Lin, Y.-S. Chen, F. Huang, and M. Liao. "A
Multi-Camera Tracking System That Can always Select A Better View to
Perform Tracking", in APSIPA ASC’09, October, Japan.
林宏儒 "View Selection in Multiple Cameras People tracking System", master
thesis, 2008.
(五) At the least two publications in international conferences.
During the project period, we have published seven high-quality international conference
papers which were more or less relevance to this project topic. The listed three are the most
representative international conference papers.
。F. Huang, R. Klette, and Y. H. Xie. "Sensor Pose Estimation from Multi-Center
Cylindrical Panoramas", in PSIVT’09, Tokyo, Japan, January, 2009, LNCS 5414, Springer,
pp 48~59. [EI]
。F. Huang, R. Klette, and Y. H. Xie. "Pose Estimation for Sensors Which Capture Cylindric
Panoramas", in CIARP’08, Havana, Cuba, September, 2008, LNCS 5197, Springer,
pp 593~601. [EI]
。F. Huang and Z.-H. Lin. "Stereo Panorama Imaging and Display for 3D VR System", in
CISP’08, Sanya, Hainan, China, May, 2008, Vol. 3, pp 796~800. [EI]
(六) At the least one SCI or SCIE journal publication.
The book Panoramic Imaging: sensor-line cameras and laser range finders, by F. Huang, R.
Klette and K. Scheibe, published by Wiley, West Sussex, England, 2008, has been finalized
during the project period. Four manuscripts have been submitted to SCI or SCIE journals
also during this period of time, one of them has been accepted (below), one is still under
second reviewing, and the other two are still under their first review.
。F. Huang and R. Klette. "Stereo Panorama Acquisition and Automatic Image Disparity
Adjustment for Stereoscopic Visualization", to appear in Special Issue on Emerging
Multimedia Applications of MULTIMEDIA TOOLS AND APPLICATIONS. [SCIE]
行政院國家科學委員會補助國內專家學者出席國際學術會議報告
98 年 10 月 21 日
報告人姓名 黃于飛 Fay Huang 服務機構及職稱
國立宜蘭大學
資訊工程研究所
助理教授
時間
會議
地點
January 13th~16th, 2009
Tokyo, Japan
本會核定
補助文號
專題研究計畫
NSC 97-2221-E-197 -024
會議
名稱
The Third Pacific-Rim Symposium on Image and Video Technology (PSIVT2009)
發表
論文
題目
1. Sensor Pose Estimation from Multi-Center Cylindrical Panoramas
and
2. Monocular 3D Reconstruction of Objects Based on Cylindrical Panoramas
Attended poster and demo sessions
January 15 (Thursday) attended the Invited Talk of Professor Shigeo Morishima (School of Science
and Engineering, Waseda University).
Title: Instant Casting System “Dive Into the Movie”
Attended oral (OS6: Detection and Tracking) and poster sessions
二、 與會心得
It has been the first time that I served as a co-program chair for an international conference. There
were a lot of duties during the paper reviewing period. I have learnt many things from the other two
more experienced co-program chairs. By attending the conference, I have also learnt how to
organize a conference through our discussions in the VIP dinner.
PSIVT2009 has been a successful conference; a total of 247 paper submissions underwent the
full review process. Each of these submissions was evaluated in a double-blind manner by a
minimum of three reviewers. The review assignments were determined by a set of 2-4 chairs for
each of the eight themes. Final decisions were jointly made by the theme chairs. In the end, there
were 39 papers accepted for oral presentation and 57 for poster presentation. The review process
was highly selective, yielding an acceptance rate of less than 40%.
During the PSIVT2009 conference I have established many new connections, which has been
very helpful for my later conference ArtsIT2009 in September. In particular, I invited Prof. Igarashi
to give a keynote speech in ArtsIT2009, whom I first met in PSIVT2009.
三、 建議
One thing I found that PSIVT2009 did very well is the local map and information. A detailed street
map and names of the near-by restaurants are provided, including some convenient stores. This is
something we can do while organizing an international event in the future.
Sensor Pose Estimation from Multi-center
Cylindrical Panoramas
Fay Huang1, Reinhard Klette2, and Yun-Hao Xie1
1 Institute of Computer Science and Information Engineering,
National Ilan University, Taiwan, R.O.C.
fay@niu.edu.tw
2 Department of Computer Science,
The University of Auckland, New Zealand
r.klette@auckland.ac.nz
Abstract. Cylindrical panoramas can be classiﬁed into various types ac-
cording to their basic scanning properties and mutual spatial alignment,
such as single-center (e.g., as in QTVR), concentric, multi-center, sym-
metric, or (after a transformation onto a cylinder) catadioptric panoramas.
This paper deals with a solution of the sensor pose estimation problem us-
ing (somehow calculated) corresponding points in themulti-center panora-
mas. All other types of panoramas are able to be described by this general
multi-center model. Due to the non-linearity of the multi-centered projec-
tion geometry, themodeling of sensor pose estimation typically results into
non-linear and highly complicated forms which incur numerical instabil-
ity. This paper shows that there exist linear models for sensor pose estima-
tion under minor geometrical constraints, namely symmetric and leveled
panoramas. The presented approaches are important for solving the 3D
data fusion problem for multiple panoramas; it is also fundamental for an
in-depth analysis of multi-view geometry of panoramic images.
1 Introduction
Panoramic images have been studied with respect to 3D scene visualization, nav-
igation and reconstruction for more than a decade. Applications include stereo-
scopic visualization, stereo reconstruction, walkthrough or virtual reality.
Various types of panoramic images are proposed to match particular applica-
tions. This paper focuses on those using a cylindrical representation. See Table 1
for a classiﬁcation of cylindrical panoramas and their applications. A 360◦ cylin-
drical panoramic image can be acquired by various means, such as a rotating
video or matrix-sensor camera, a catadioptric sensor (with a subsequent map-
ping onto a cylinder), or a rotating sensor-line camera, as commercially available
from various producers since the late 1990s. For simplifying our discussion, we
assume a model close to the latter one which has a ﬁxed rotation axis and takes
images consecutively at equidistant angles. (Rotating sensor-line cameras allow
maximum accuracy, and have been used, e.g., in major architectural photogram-
metric projects; see [1]). The projection center of the camera does not have to
T. Wada, F. Huang, and S. Lin (Eds.): PSIVT 2009, LNCS 5414, pp. 48–59, 2009.
c© Springer-Verlag Berlin Heidelberg 2009
50 F. Huang, R. Klette, and Y.-H. Xie
O
C
Rotation axis
Base circle
Base plane
X
Image columns
X
Y
Z
R
f
Fig. 1. Basic sensor model of multi-center cylindrical panoramas: the origin of the
sensor coordinate system is at O. Three image columns are shown with their projection
centers.
cylindrical panorama is discussed in Section 3. Two general cases of multi-view
panoramas are elaborated in Section 4. Section 5 informs about experiments
using recorded or synthetic images. There are also concluding remarks.
1.1 Sensor Model and Notations
The sensor model used generalizes various panoramic imaging models [3,4,8].
The model consists of multiple projection centers and a cylindrical image sur-
face; see Fig. 1. C denotes a projection center. Projection centers are uniformly
distributed on the base circle. This circle is incident with the base plane. O de-
notes the center of the base circle; it is also the origin of the sensor coordinate
system. The oﬀ-axis distance R (radius of the base circle) describes the distance
between any projection center and the rotation axis.
A cylindrical panorama is partitioned into image columns of equal width which
are parallel to the rotation axis. The number of image columns is the width Wof
the panorama. There is a one-to-one ordered mapping between image columns
and projection centers. The distance between a projection center and its associ-
ated image column is called the eﬀective focal length, and is denoted by f (see
Fig. 1). The principal angle ω is between a projection ray in the base plane,
emitting from C, and the normal vector of the base circle at point C. The four
intrinsic sensor parameters, R, f , ω, and W characterize how a panoramic image
EP(R, f , ω, W ) is acquired.
Consider two panoramas, EP1 and EP2 . The geometric relationship between
both sensor coordinate systems can be described by a 3 × 3 rotation matrix R
and a 3 × 1 translation vector T. The rotation matrix is given by three row
vectors [rT1 rT2 rT3 ]T, and the translation vector equals (tx, ty, tz)T.
2 General Multi-view Case
Consider a pair of panoramas, EP1(R1, f1, ω1, W1) and EP2(R2, f2, ω2, W2),
which are taken at arbitrary poses in 3D space. Let (x1, y1) and (x2, y2) denote
52 F. Huang, R. Klette, and Y.-H. Xie
3.1 Two Symmetric Pairs
EP1(R, f , ω, W ) and EP2(R, f , -ω, W ) deﬁne a symmetric pair of panoramas,
both deﬁned on the same sensor coordinate system. (Symmetric pairs can easily
be acquired using a single oﬀ-shelf camera, e.g., see the approach in [5].) Epipolar
curves are in this case lines which may be identiﬁed with image rows (see proofs
in [2,7,10]). Therefore, dense image correspondences can be calculated by using
stereo matching algorithms as developed for stereo pairs of planar images.
If 3D data are collected from multiple pairs of symmetric panoramas, acquired
at diﬀerent locations, then data fusion becomes a challenge, and the registration
step requires that the sensor pose estimation problem to be solved in advance.
The basic idea of our sensor pose estimation approach is as follows: ﬁrst, for
each symmetric pair, transform pairs of corresponding image points into direc-
tional unit vectors pointing to the reconstructed 3D points; second, establish a
geometric relation between these two bunches of unit vectors that are respec-
tively deﬁned in two sensor coordinate systems. The following theorem shows
how the directional unit vector of a 3D point (with respect to O) is derived
from a pair of corresponding points (x1, y) and (x2, y) on symmetric panoramas
EP1(R, f , ω, W ) and EP2(R, f , -ω, W ). Let u be the directional unit vector of
that 3D point which projects into (x1, y) and (x2, y).
Theorem 1. This directional vector can be calculated as follows:
u =
(
sinω sinα, yf sinβ, sinω cosα
)T
√
sin2 ω + y
2
f2 sin
2 β
(3)
where α = (x1+x2)πW and β =
(x2−x1)π
W .
O
C2
C1
XR
X
(A) (B)
O
P
C1
C2
Base plane
P0
P0
(C)
C1
P
P0
f
D
H
H (D)
O
P
C1
C2
(x1, y)
(x2, y)
Base circle
X
Y
Z
Z
B
C y
Fig. 3. Geometric interpretation of directional unit vector calculation, as described in
the proof of Theorem 1
54 F. Huang, R. Klette, and Y.-H. Xie
Proof. Each pair of corresponding image points in a pair of symmetric panora-
mas can be transformed into an directional unit vector by Theorem 1. Figure 4
shows a point P that deﬁnes two corresponding directional unit vectors u1 and
u2 with respect to sensor coordinate systems O1 and O2, respectively.
Assume that coordinate system O1 coincides with the world coordinate sys-
tem. Let R and T describe the orientation and translation of the sensor co-
ordinate system O2 with respect to the world coordinate system. Since any
pair of corresponding directional unit vectors is coplanar in 3D (i.e., epipo-
lar constraint), we have that uT1 (˙T × R−1u2) = 0. We can rewrite this as
uT1 T×R−1u2 = 0, where T× is the skew symmetric matrix of vector T.
Then, we have uT1 Eu2 = 0, where E = T×R−1. Here, the matrix E is equiv-
alent to the essential matrix in multiple planar image geometry [11], thus the
normalized 8-point algorithm applies to solve the sensor pose estimation problem
in this symmetric case.
Hence, we may conclude that eight pairs of corresponding image points are
suﬃcient to determine the relative poses of both sensors up to a scale factor,
where the value of sensor parameter R needs not to be known.
Moreover, if all the sensor parameter values (including R) are pre-calibrated and
given, then the exact sensor pose associated to O2 can be recovered with respect
to the world coordinate system (deﬁned at O1). This is because those parameter
values allow to calculate the exact distance between any two 3D points. Then,
these distances can be used as a reference to recover the unknown scale factor
in Theorem 2.
3.2 One Leveled Pair
In this second common approach for capturing multi-view panoramas, the only
constraint is that all associated base planes have to be parallel (say, to the sea
level), to be guaranteed by a lever. Figure 5 sketches a leveled pair of panoramas.
Leveled panoramas are common for virtual navigation [12,13] or reconstruc-
tion [8,9] of a large scale environments. Leveled panoramas allow large “over-
lapping” ﬁelds of views. The larger the common ﬁeld of view, the higher the
probability that object surfaces are visible in more than one panorama. Hence,
this supports more reliable stereo reconstruction and smooth view-transitions
between multiple panoramas in a walk-through simulation.
O1
O2
(x1,y1)
(x2,y2)
P
Fig. 5. A pair of leveled panoramas and a pair of corresponding image points
56 F. Huang, R. Klette, and Y.-H. Xie
Fig. 6. Two symmetric leveled panorama pairs acquired at diﬀerent locations (top:
right panorama of the ﬁrst pair, bottom: right panorama of the second pair), all marked
with 40 corresponding points
Fig. 7. Illustration of three epipolar curves calculated based on the pose estimation
results
were calibrated separately in advance; thus the camera’s intrinsic parameters
were known and kept unaltered during image acquisition. Figure 6 illustrate one
example of a leveled pair taken by a line-camera at diﬀerent locations in the
same room. Actually, at each location, a pair of stereo panoramas were acquired
for experiments, and those shown in Fig. 6 are the “right” panoramas only. In
this particular example, we used R = 100 mm, f = 21.7 mm, and ω = ±155◦.
Each panorama has an image resolution of 324 × 1, 343. We identiﬁed manually
in total 40 corresponding points (marked as stars).
The true rotation matrix R and translation vector T of these two symmetric
panorama pairs were measured with less than ±1% error, and we have φ = 50◦
and (tx, ty, tz) = (−1, 000,−45,−1, 000) in mm. The estimated sensor pose is de-
noted as Rˆ and Tˆ. The error measurements for rotation and translation are de-
ﬁned as arccos
((
tr(RRˆ
T
) − 1
)
/2
)
and arccos
(
T · Tˆ/‖T‖‖Tˆ‖
)
, respectively,
both in degrees.
58 F. Huang, R. Klette, and Y.-H. Xie
the quadratic programming approach is more sensitive to input errors than the
SVD method. Also, the assignment of initial values has signiﬁcant impact onto
the estimation result. According to our experiments, the estimation result was
mostly sensitive to the ‘sign’ of the initial values but not to their quantities nor
inter-ratios. In particular, zeros were not good for an initial guess in our case.
The plots in this case indicate that we had input errors of about eight to nine
pixel in our real-world experiment, which are close but slightly bigger than the
conclusion drawn in the symmetric-panorama case. Error analysis on R and ω
was carried out as well. It concludes that the error of R has a very minor impact
on the pose estimation results. Moreover, a k-degree error of ω would cause
about a k-degree error in the estimated Tˆ, for any real number k, but an error
in ω has very little impact on the estimation of R. The conclusion drawn here
is coherent to the symmetric-panorama case.
Finally, more synthetic experiments were designed and performed for diﬀerent
panorama conﬁgurations (i.e., diﬀerent poses, diﬀerent sensor parameter values,
and etc.). They lead to conclusions that the resolution of the input panoramic
images, and the distribution of the selected corresponding points are also two
critical factors for pose estimation. The panoramic image resolution, especially
the width, should be as large as possible. The corresponding points should be
distributed uniformly and sparsely on the entire panoramic images. A larger
set of corresponding points, say greater than 100, would not guarantee a better
estimation result. A much better result can be achieved if image resolution of
1, 000 × 10, 000 is used instead, and the nearest scene point is no less than four
meters from both sensors. The estimation errors can be less than 0.5 degrees for
both R and T, allowing for both cases even up to ten-pixel input error.
5 Conclusions
This paper presented approaches for pose estimation of multi-view (i.e., also
multi-center) cylindrical panoramas. Two geometric constraints were used: one
was by coplanarity of corresponding projection rays, and the other was by in-
tersection of corresponding projection rays.
The ﬁrst constraint was used in the case of two symmetric pairs, and we ob-
tained a “linear” solution for the sensor pose estimation problem. We showed
that the (common) normalized 8-point algorithm can be utilized in this case.
Experimentally, we found that the normalization step of the normalized 8-point
algorithm for improving the accuracy and satiability was ignorable in our case,1
and this makes a diﬀerence to the planar image case. The second constraint was
applied to cases of a general pair, or a leveled pair of cylindrical panoramas.
Rather poor estimation results were obtained in the case of general pairs, and
we did not include those here. In contrast to that, the result for the leveled-
panorama case was greatly improved and reasonably stable. The proposed ap-
proaches are able to achieve high accuracy of less than 0.5 degree error in general,
1 The corresponding image points on panoramas is likely not as skewed or clustered
as in the planar image case.
Monocular 3D Reconstruction of Objects
Based on Cylindrical Panoramas
Ralf Haeusler1, Reinhard Klette1, and Fay Huang2
1 The University of Auckland, Computer Science Department, New Zealand
r.haeusler@cs.auckland.ac.nz
2 CSIE, National Ilan University, Yi-Lan, Taiwan
Abstract. This paper discusses ways of using a single panoramic image
(captured by a rotating sensor-line camera having very-high spatial reso-
lution) for the geometric shape recovery of a shown object. The objective
is to create a sparse polyhedral model, only allowing a few interactive
user inputs for a given single panoramic image. The study was motivated
by the general question whether a single panoramic image projection al-
lows some kind of 3D shape recovery, possibly beneﬁtting from available
monocular approaches for standard (say, pinhole-type) camera models.
Keywords: Monocular 3D reconstruction, cylindrical projection,
panorama, rotating sensor-line camera.
1 Introduction
The computation of 3D structure from stereo images receives increasingly at-
tention due to the enormous progress recently in this area. However, the task
of retrieving 3D information from a single image seems to be a rather ill-posed
problem, yet scientiﬁc interest herein dates back many centuries [2]. In fact,
so-called monocular reconstruction cannot work without some kind of a-priori
knowledge (i.e., some assumptions about geometric properties or shapes of the
shown objects, or about surface reﬂectance).
Apart from utilizing geometric constraints for speciﬁed classes of objects (see,
for example, [7,8]), a popular approach to monocular 3D understanding applies
the concept of vanishing points (see, for example, [4,5]), as introduced by painters
in the renaissance.
Of course, talented artists may often be successful in modelling manually a
scene from a single photograph, by using common 3D clues for the human visual
system [10].
This paper deals with monocular reconstruction based on images of very high
resolution and with a wide ﬁeld of view. Such images may be recorded with so-
called rotating sensor-line cameras [6], and the resulting images are also called
cylindrical panoramas. The question arises whether such images, projected onto
a straight cylinder, provide better opportunities for understanding the 3D struc-
ture from only a single image compared to images recorded with a ‘normal’ (say,
pinhole-type) camera. [6] uses cylindrical panoramas for 3D modelling of (large)
T. Wada, F. Huang, and S. Lin (Eds.): PSIVT 2009, LNCS 5414, pp. 60–70, 2009.
c© Springer-Verlag Berlin Heidelberg 2009
62 R. Haeusler, R. Klette, and F. Huang
Fig. 2. Camera model of a cylindrical projection with a single projection center: Δϕ
denotes the angular increment, f the eﬀective focal length of the lens, τ is the physical
size of a pixel on the sensor line (assumed to be constant), and l the total physical
length of the sensor line
(with the recorded panorama) and a circular path, respectively. The recorded
panorama is composed line by line, after (or during) such a rotation (typically
of full 360◦).
If the projection centers of all the recorded lines are at the rotation axis, then
they all coincide, and the circular path degenerates into a single point. Such a
case of a single projection center is illustrated in Figure 2.
The main advantage of such a camera system is its very large spatial reso-
lution. By specifying the number of recorded lines (columns), the wide ﬁeld of
view of the recorded panorama may even extend beyond 360◦, by recording into
some directions more than once. The data volume of a single 360◦ panorama
is in the range of several gigabytes for contemporary sensor lines of about 10k
color pixels. The main disadvantage is the long exposure time, limiting its use
for dynamic scenes (but also allowing interesting eﬀects such as having a person
repeatedly in a recorded panorama). Some of the intrinsic parameters (such as
focal length, angular increment, size of a pixel) are also illustrated in Figure 2.
As it is most appropriate to record images with square pixels,2 a common
target is to specify the number of columns using an angular increase of
Δϕ = 2 · arctan
( 1
2τ
f
)
for image recording. We assume (and used) a 360◦ image with pixels known to
have square shape, and this speciﬁes the used intrinsic parameters for monocular
reconstruction (up to a scaling factor). Of course, this ignores some possible
(minor) errors, such as having the projection center always exactly at the rotation
axis. We assume a camera center O which identiﬁes the unique origin of all
projection rays.
2 To be precise, these are actually ‘cylindrical squares’ on a cylindrical surface.
64 R. Haeusler, R. Klette, and F. Huang
Obviously, from a single image, a reconstruction is only possible up to a scaling
factor. Thus, without restriction of the generality, it can be assumed that λ4 = 1.
This deﬁnes a linear equational system
⎡
⎣
t1x −t2x t3x
t1y −t2y t3y
t1z −t2z t3z
⎤
⎦ ·
⎡
⎣
λ1
λ2
λ3
⎤
⎦ =
⎡
⎣
t4x
t4y
t4z
⎤
⎦ (1)
The unique solution λ1, . . . , λ4 describes the position of those 3D rectangular
vertices up to a scale factor μ as follows: rh = O+ μ · λh · th. Scale factor μ can
be determined only if object dimensions are known for real world scenes (e.g.,
height or width of objects in the real world).
However, applied to an object that is composed of several ‘connected’ rec-
tangles, a reconstruction result is not satisfactory if every single rectangle is
reconstructed separately. The ﬁrst reason is that every single rectangle would
have a diﬀerent scaling factor μ as one of the λh values was set to be equal to one.
Adjusting the scale factors μ over all rectangles based on ‘connectedness’ (i.e.,
sharing of edges) properties of faces of the object still does not allow for a closed
reconstructed object surface due to unavoidable reconstruction inaccuracies.
The following is now our proposition for solving this problem. From an object
consisting of q rectangles with n vertices, a single linear equational system T·λ =
t is derived as follows: An instance of vector t contains data from the ‘ﬁrst’
projection ray to a vertex which may be incident with up to q rectangles. (The
component λ1 of vector λ is set to be equal to one due to scale ambiguity.)
Assuming that q is the maximum for all considered rays, we have a matrix T
composed of n − 1 columns and 3 · q rows. These contain information about all
the n projection rays, with up to q rectangles in each case.
All the equations of the derived system are as follows:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
t12x −t13x t14x · · · t1nx
t12y −t13y t14y · · · t1ny
t12z −t13z t14z · · · t1nz
t22x −t23x t24x · · · t2nx
t22y −t23y t24y · · · t2ny
t22z −t23z t24z · · · t2nz
...
...
...
...
...
...
...
. . .
...
...
...
...
...
tq2x −tq3x tq4x · · · tqnx
tq2y −tq3y tq4y · · · tqny
tq2z −tq3z tq4z · · · tqnz
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
·
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
λ2
λ3
...
λn
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
t1x
t1y
t1z
t2x
t2y
t2z
...
...
...
tqx
tqy
tqz
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
T is in general a sparse matrix, as one projection ray is often connected to
not more than four rectangles. (For implementation, the multimap-datastructure
from the Standard Template Library [1] may be recommended.)
66 R. Haeusler, R. Klette, and F. Huang
Table 1. Image coordinates of pixels selected in Figure 3 together with results of the
3D reconstruction process
Point 2D 3D
i j x y z
1 11189 2029 40.47 -50.11 16.84
2 11191 7368 40.49 -50.13 -21.66
3 18703 7414 -12.09 -61.76 -21.83
4 18711 2117 -12.04 -61.67 15.75
5 38702 2098 -36.27 51.42 15.64
6 38698 7409 -36.17 51.39 -21.83
7 46171 7314 16.94 62.70 -22.23
8 46171 2002 16.97 62.75 16.80
Fig. 4. Reconstructed cuboidal room with mapped textures. Circular regions on the
ﬂoor and the ceiling were not recorded by the rotating sensor-line camera, and texture
information is thus not available in these areas. (The ceiling is shown to indicate the
reconstructed 3D volume.)
4 Pinhole-Type versus Cylindrical Camera
The example illustrated that it is possible to generate a full 3D volume model
from a single 360◦ panoramic image, what is, of course, not possible with a single
image of a pinhole-type camera. For pointing out whether the cylindrical pro-
jection itself is already advantageous compared to the standard pinhole model,
we look at panoramic images with a viewing angle less than 360◦.
For 360◦ cylindric images with square pixels, relevant intrinsic camera para-
meters were assumed to be given in Section 2. However, angular increment and
focal length of a rotating line camera may also be estimated based on given
(recorded) images.
68 R. Haeusler, R. Klette, and F. Huang
have to be collinear. This infers that
j′3 − j′1
i′3 − i′1
=
j′2 − j′1
i′2 − i′1
Note that for i′3 = i
′
1 or i
′
2 = i
′
1, no information about Δϕ can be derived as
vertical lines in the world remain straight on the image cylinder provided that
the rotation axis is perfectly upright.
We are able to estimate Δϕ numerically by applying interval bisection, with
j′3 − j′1
i′3 − i′1
− j
′
2 − j′1
i′2 − i′1
≤  ≤ 10−5
being the stop criterion.
Note that, although the method is usable for all ‘bent straight segments’
in the cylindrical image, it yields most accurate results for strongly bended
‘horizontal’ segments. In this case, precisions of up to 99.8 % were achieved in
our experiments.
This is only the most simple method for estimating Δϕ. Signiﬁcant improve-
ments concerning the precision can be made by taking more pixels into account
(potentially all available pixels along a bended line segment), and also using
more advanced approximation techniques.
4.2 Estimation of Focal Length
Concerning the focal length, from Equations (2) we see that parameter f is only
a linear coeﬃcient in the projection Π , and therefore cannot be estimated from
curved lines. Normally we also do not know the length l of the sensor line. How-
ever, there is anisotropic scaling depending on the focal length, and this allows
to estimate the (dimensionless) ratio l/f also using a-priori knowledge about
aspect ratios of shown real-world objects (absolute length cannot be estimated
in general due to scale ambiguity of the recorded 3D scene).
Given four vertices r1, ..., r4 of a rectangle and a-priori knowledge about the
ratio
Ξ =
|k1|
|k2| =
|r4 − r1|
|r2 − r1|
of two of its edges, the ratio l/f can be estimated such that edge ratio Ξ ′,
resulting from the reconstruction of image points of r1, ..., r4, is equal to Ξ.
In the reconstruction process of image points of r1, ..., r4, value l/f is the only
unknown as Δϕ was already estimated, independently from f , in the previous
step. A square-pixel assumption (for the panoramic image) also supports an
initialization for a computationally inexpensive iterative search procedure (e. g.,
interval bisection).
4.3 Use of Vanishing Points
Monocular reconstruction for pinhole-type cameras often utilizes vanishing points.
Those are also of beneﬁt for cylindrical images. As for pinhole-type camera images,
