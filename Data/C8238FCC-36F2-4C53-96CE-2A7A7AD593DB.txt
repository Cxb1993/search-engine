view video coding standard. Except for stereo 
visualization, this research will further explore 
another research domain for H.264/MVC. In this three-
year research project, the yearly-designed research 
topics are enlisted as the follows: 
In the first year, the goal is to establish the 
technology to reconstruct the intermediate view for 
free viewpoint from multiview video row data. By 
using motion search techniques in our lab, we will 
further develop fast and high efficient methods for 
generation of disparity map. After the reconstruction 
of range images from camera parameters, we will 
conduct the background separate research and derive 
some formulas to build the 3D model. Finally, we will 
develop fast immediate view interpolation algorithms 
by combination of interpolation computations. 
In the second year, we plan to extend the research 
results obtained in the first year with multi-view 
video coding system after the H.264/MVC decoder to 
achieve a complete system, which can decode the 
compressed data stream to the generation of the free 
viewpoint immediate view directly. Beside, we will 
also investigate from traditional H.264/AVC bitstream 
to develop a synthesized process from H.264/AVC 
compression stream to H.264/MVC compression stream. 
Beside the synthesis of the single immediate view, we 
also plan to extend our research develop to generate 
the two-view immediate videos. 
In the third year, we want to implement a real system 
for practical applications with free viewpoint 
displayer and video server. Including in mobile 
devices, we will realize a free viewpoint system, 
which can show intelligent real-time applications 
from streaming video in the Internet. Hence, we will 
develop algorithms, which can track the eye sight and 
lock at the video object to provide some different 
viewing method. The segmentation and tracking of the 
object in the three dimensional space are also the 
main research focuses in this year. 
英文關鍵詞： Multi-View, Multi-View Video Coding, Joint Video 
Team, Free View-Point, Motion Search, Immediate View, 
 I
行政院國家科學委員會補助專題研究計畫 ■成果報告   □期中進度報告 
 
互動自由視角系統之應用研究與其先進多維度視訊之處理壓縮(3/3) 
Researches and Applications on Interactive Free View Video Systems and  
Their Related Advanced Multiview Video Coding and Processing (3/3) 
 
計畫類別：□個別型計畫   ■整合型計畫 
計畫編號：NSC 97-2221-E-006-109-MY3 
執行期間：99 年 8 月 1 日至 100 年 10 月 31 日 
 
執行機構及系所：國立成功大學電腦與通信工程研究所 
 
計畫主持人：楊家輝 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
 
中   華   民   國 101 年 1 月 12 日 
 III
Abstract 
After the invention of advance video coding standard, H.264/AVC, Joint Video Team (JVT) further starts 
the design of H.264/MVC standards. The H.264/MVC provides a solution for compressing multi-view video 
sequences. The goal of this research is focused on free view-point video applications from the multi-view 
video coding standard. Except for stereo visualization, this research will further explore another research 
domain for H.264/MVC. In this three-year research project, the yearly-designed research topics are enlisted as 
the follows: 
In the first year, the goal is to establish the technology to reconstruct the intermediate view for free 
viewpoint from multiview video row data. By using motion search techniques in our lab, we will further 
develop fast and high efficient methods for generation of disparity map. After the reconstruction of range 
images from camera parameters, we will conduct the background separate research and derive some formulas 
to build the 3D model. Finally, we will develop fast immediate view interpolation algorithms by combination 
of interpolation computations. 
In the second year, we plan to extend the research results obtained in the first year with multi-view video 
coding system after the H.264/MVC decoder to achieve a complete system, which can decode the compressed 
data stream to the generation of the free viewpoint immediate view directly. Beside, we will also investigate 
from traditional H.264/AVC bitstream to develop a synthesized process from H.264/AVC compression stream 
to H.264/MVC compression stream. Beside the synthesis of the single immediate view, we also plan to extend 
our research develop to generate the two-view immediate videos. 
In the third year, we want to implement a real system for practical applications with free viewpoint displayer 
and video server. Including in mobile devices, we will realize a free viewpoint system, which can show 
intelligent real-time applications from streaming video in the Internet. Hence, we will develop algorithms, 
which can track the eye sight and lock at the video object to provide some different viewing method. The 
segmentation and tracking of the object in the three dimensional space are also the main research focuses in 
this year.  
 
Keywords: Multi-View, Multi-View Video Coding, Joint Video Team, Free View-Point, Motion Search, 
Immediate View, Interpolation, 3D TV, Range Image, Texas Instrument DaVinci Platform。 
 1
計畫之背景及目的 
近年來，在資訊科技數位化的狂潮帶領之下，各種新興的視訊標準被訂定，
不管是傳輸或壓縮等技術都不斷進步，畫面內容的品質及解析度也持續提高，隨
著有線或無線網路的頻寬及處理器速度的加快，在影音娛樂方面能提供的服務越
來越多，對消費者而言，除了觀賞的品質之外，更開始能提供其他不同於以往的
應用，如多視角，立體視覺，行動式，互動式服務等等；至此數位電視的普及已
經成為一種趨勢，不只將更豐富廣大群眾的生活，顛覆傳統靜態畫面的視覺接收
觀念，也為科技業者帶來了無限商機。  
市場研究公司Informa Telecoms & Media在其最新公佈的研究報告中指出，
到2007年年底，全球數位電視家庭用戶將增加5400萬戶，使全球數位電視家庭用
戶達到2.39億戶，普及率為22％，其中，北美地區的普及率超過了68％，西歐的
普及率則為52％。而到2008年全球數位電視家庭用戶將再增加5200萬戶。報告並
且預估，全球數位電視家庭用戶由2007年至2012年將增加2.64億戶，其中，中國
將增加6000多萬戶，約佔全球1/4。報告也另外提到，美國的數位電視家庭用戶
將增加3900萬戶，日本的數位電視家庭用戶將增加2000萬戶，印度的數位電視家
庭用戶將增加2300萬戶。中國、美國、日本與印度四個國家所增加的數位電視家
庭用戶將佔全球新用戶54％。 
 
圖(一)  H.264/MVC 編碼架構圖 
隨著數位傳輸的可能提升，各種先進的壓縮標準也隨之訂定，繼H.264/AVC
之後，JVT(Joint Video Team)更持續定義了H.264/SVC與H.264/MVC兩種不同的
壓縮標準，其中H.264/MVC為多維度影像的編碼標準，架構上承襲了原本的
264/AVC，延伸成多維度的視訊編碼方式。為目前多維度視訊壓縮中的一大主
流，圖(一)為H.264/MVC的編碼架構圖。 
多維度影像編碼標準的產生，也造成新的運用，廣泛的分佈於例如:教育、
醫療、監控系統、通訊、娛樂…等。尤其是在行動通訊跟傳統電信通訊方面。多
 3
研究成果 
1. 快速H.264/MVC編碼之投射式視角預測搜尋演算法 
摘要 
本計畫主要提出一種投射式視角預測搜尋演算法，是利用相機之內部參數和
外部參數產生極線以及更精確的視角向量預測子以縮小搜尋視窗，並在不影響影
像品質的前提下，節省 H.264/MVC 中視角間預測之運算複雜度，進而增進
H.264/MVC 的執行效能。另外，亦參考影像之紋理分布提出新的移動向量預測
子，此預測子不僅能在框間預測時增強物件的凝聚性，更能藉由物件內像素的群
聚性減少移動估測的搜尋範圍，節省移動估測所需時間，並且重建出品質優良的
影像。 
根據我們於 JMVC2.5 上的模擬結果，在不同的量化參數以及不同的測試樣
本中，本方法皆可節省約 70%至 80%的運算量。在影像品質方面，PSNR 僅下降
0.01dB，位元率僅增加 0.3%，此幅度在人眼所能接受的範圍之內；換言之，本
計畫所提出之投射式視角預測搜尋演算法能在影像效果幾乎沒有變差的前提之
下，有效提升 MVC 的編碼效率。 
MVC 的基本概念 
  如前所述，圖(一)顯示 MVC 中的基本編碼結構。而對於各個視角則使用階
層 B 的預測結構。每個視角可分為兩種類別；如 S0 及 S2 主要用以作時間預測
的主要視角，以及如 S1 可參照相鄰主要視角的輔助視角。這些視角將產生巨量
的多視角影片數據。如果視角的數目提高，需要處理的數據隨之增加。然而，若
各個視角皆紀錄同一時間下的相同場景，則多視角影片數據中的空間殘值、時間
殘值以及視角間的殘值結束。 
 
圖(三) 多視角影像編碼結構 
 5
 
圖(五)  預測結果在極線上 
    另一方面，如果當前塊的 DVP 指到的區塊不在參考視圖的極線上，如圖
(六)，搜尋範圍變成平行四邊形的形狀。首先，DVP 表示的像素位置記為 P(x, y1)。
這個像素垂直投影到極線上的點 P(x, y2)。如果 y1 比 y2 大，那麼 P(x, y1)就在 
P(x, y2)之上，在兩像素間的距離 D 為 
21D yyy  .                                                 (1) 
然後，可以分別計算 y1’ 和 y2’去得到點 P(x, y1’) 和點 P(x, y2’) 
yyy D1
'
1  ,                                                 (2) 
和 
       yyy D2
'
2  .                                                (3) 
P(x, y1’) 和 P(x, y2’)的距離 Sy為 
yy Dyy  3S '2'1 ,                                           (4) 
如果 y2 比 y1 大，那麼 P(x, y2)就在 P(x, y1)之上，在兩像素間的距離 D 則為 
12D yyy  ,                                                 (5) 
然後，可以分別計算 y1’ 和 y2’去得到點 P(x, y1’) 和點 P(x, y2’) 
yyy D-1
'
1  ,                                                  (6) 
和 
yyy D2
'
2  .                                                  (7) 
P(x, y1’) 和 P(x, y2’)的距離 Sy為 
yy Dyy  3S '1'2 ,                                           (8) 
 7
(2) 背投影(Back projection) 
  背投影的概念如圖(八)所示，其中三個視角的視訊資料皆已編碼。順序為先
編碼中央視角(center view, CV)接著為右視角(RV)，最後為左視角(LV)。使用框
內及框外預測來移除左、中、右視角上時間域與空間域的冗餘資訊。除此之外，
也採用視角間(inter-view)預測來移除視角間的冗餘資訊，中視角可以作為右視角
的編碼參考，因為中視角最先被編碼；同樣的，中視角與右視角都可以作為左視
角的編碼參考。由於相機幾何，本論文導出專用於左視角視差估計的新型 DVP
與窄型搜尋範圍。採用新型 DVP 與搜尋範圍，將可以獲取中視角與左視角間更
精確的視差向量。 
 
圖(八) 背投影的概念 
  中視角與右視角編碼完成後，包含於中視角的位置關聯(co-located)MB 也將
包含於右視角。每個 MB 左上方的點可以被定義為目前中視角上的 PCC (xcc, ycc)
與位置關聯的右視角 MB 的 PCR (xcr, ycr)。要找到中視點與右視點的視差向量，本
論文先轉換目前方塊(current block)的 PCC (xcc, ycc) 與位置關聯方塊的 PCR (xcr, ycr)
至世界座標(world coordinate)。根據幾何原則，如果某一存在於兩個維度的點被
投影至世界座標，此點可以轉換為一條線。所以在投影之後的世界座標，PCC被
轉換至線 Lc3d，而 PCR被轉換至線 Lr3d。線 Lc3d 與 Lr3d 的交點可以計算得出 PIN (xin, 
yin, zin)，如圖(九)所示。 
 
圖(九) 中視點與右視點投影 
 9
圖(十一)為視差向量 DVC→L及 DVL→C。 
 
圖(十一) 視差向量 DVC→L及 DVL→C 
  在中視角與右視角的編碼程序後，左視角方塊的視差向量 DVL→C可由右視
角的位置關聯方塊、右視角的目前方塊與相機幾何導出。當進行左視角的編碼程
序時，視差向量 DVL→C可用於視差搜尋時作為新的 DVP。 
  當進行左視角視差搜尋時，DVP 可藉由背投影求得。如果目前方塊的 DVP
指向位於參考幅(reference frame)上的極線上之方塊，視差搜尋可以直接在極線上
進行，如圖(十二)所示。搜尋範圍可由(2p + M) × (2p + N)簡化至(2p + M) × 1。另
一方面，如果目前方塊的 DVP 沒有指向位於參考幅上的極線上之方塊，搜尋範
圍則為如章節 3.1 所述的平行四邊形區域。為了將背投影應用於視差搜尋，搜尋
範圍可由矩形區域簡化置一條線或平行四邊形區域，運算複雜度可以減少約
90%。 
 
圖(十二) 由左視角至中視角的視差向量 
(3) 移動向量預測器 (Motion vector predictor) 
 如前所述，內部預測(intra prediction)和互相預測(inter prediction)的碼率失真
成本(RD cost)對材質複雜度很敏感。文獻中也有提到，材質切割的臨界值
(threashold)已經被計算。根據內部預測的碼率失真成本，材質臨界值可以被分成
高、低、中區域。 
 考慮高材質區塊(H)與低材質區塊(L)，H 的內部預測通常採用 intra4×4、 
 11
MVP 就可以用上一個區塊的移動向量來表示。當原本的 MVP 由 JMVC 產生後，
新的 MVP 可用利用此時和前一個區塊的關係來產生。 
 提出的方法採用螺旋搜尋法(Based Spiral Search，BSS)[34]。假設現在的畫
面與參考畫面有很高的相關，最匹配的區塊通常跟現在的區塊位於相同座標。二
階相關的區塊是八個區塊最接近最高相關的區塊。內部相關區塊是十六的區塊最
鄰近八個區塊。所以為了找最匹配的區塊，最高相關區塊可以當作起始點，而搜
尋次序與螺旋相同，如圖(十六)所示。 
 
圖(十五) 區塊間相關性. 
 
圖(十六) 螺旋搜尋次序 
 螺旋搜尋的過程中，被原 MVP 指定的區塊(Bs)用來當作移動估測螺旋搜尋
的起始點。接著 Bs 左上的區塊(B1)為第二搜尋區塊、第三區塊(B2)位於 BS 上方、
第四區塊(B3)位於 Bs右上方…，直到結束區塊(Bt)被新的 MVP 指定開始搜尋。
如果新的 MVP 與原 MVP，搜尋點根據平行四邊形減少到一個點，搜尋次序如圖
(十七)所示。為了採用這個演算法，移動估測的搜尋視窗從平行四邊形的(2p + M) 
× (2p + N)個點減少到一個到二十五個點，編碼時間也可以被降低。另外，這個
演算法強調物體本身的特性，所以物體品質能夠被提升。 
 13
 
表(三)   QP=30 時，提出的演算法比較結果 
※單位:  Δ PSNR (dB), Δ BR (%), Δ T (%) 
 
 
表(四)   QP=35 時，提出的演算法比較結果 
※單位:  Δ PSNR (dB), Δ BR (%), Δ T (%) 
 
表(五)   QP=40 時，提出的演算法比較結果 
※單位:  Δ PSNR (dB), Δ BR (%), Δ T (%) 
 圖(十八) 至圖(二十一)為所節省時間的比較結果，圖(二十二) 至圖(二十五)
則顯示出不同結果所呈現的 RD 曲線圖。根據所節省時間的直方圖及 RD 曲線
圖，模擬結果為:無論何種相機排列和影像特性，我們所提出的演算法只會有些
微的品質降低，便可大幅減少編碼時間。 
Sequences 
QP = 30 
Proposed method 
ΔPSNR1 ΔBR1 ΔT1 ΔPSNR2 ΔBR2 ΔT2 
Ballroom -0.004 0.15 -74.86 -0.003 -0.07 -21.23 
Exit 0.003 -0.38 -75.08 0 0 -18.93 
Vassar -0.002 0.38 -76.00 0 0 -19.98 
Break-dancer -0.020 0.29 -74.35 0.001 0.12 -26.40 
Sequences 
QP = 35 
Proposed method 
ΔPSNR1 ΔBR1 ΔT1 ΔPSNR2 ΔBR2 ΔT2 
Ballroom -0.010 -0.24 -74.86 -0.002 -0.07 -19.22 
Exit 0.001 -0.10 -75.09 0 0 -17.51 
Vassar 0.002 -0.10 -76.12 0 0 -19.12 
Break-dancer -0.010 0.17 -74.24 0.003 0.18 -13.15 
Sequences 
QP = 40 
Proposed method 
ΔPSNR1 ΔBR1 ΔT1 ΔPSNR2 ΔBR2 ΔT2 
Ballroom -0.010 0.22 -75.43 0.0004 0.01 -18.69 
Exit -0.004 0.13 -74.97 0 0 -15.65 
Vassar 0.003 -0.03 -76.01 0 0 -17.66 
Break-dancer -0.020 -0.66 -74.02 0.0003 -0.03 -9.78 
 15
 
圖(二十)  “Vassar” 利用所提出的演算法節省時間比較圖 
 
圖(二十一) “Break-dancer” 利用所提出的演算法節省時間比較圖 
 17
 
圖(二十四)  “Vassar” 利用所提出的演算法失真率曲線圖 
 
 
圖(二十五)  “Break-dancer” 利用所提出的演算法失真率曲線圖 
從以上結果可知 ，我們所提出的演算法在 full search、diamond search 中的 RD 曲
線大致上是相同的。然而，相較於 full search 和 diamond search， 我們提出的
演算法可以減少更多計算複雜度。 
 19
(1) 向前/向後預測 
  為了使深度圖追蹤物件在相對幅中的移動估計的速度和準確之間保持平
衡，我們採用的是廣泛應用於視訊壓縮中以區塊為基礎的移動估計與補償概念，
並採用以下式子為當前區塊 bcur 及參考區塊 bref 之間的相似性測量(similarity 
measure)。  
       
1 ,1
1 ,1
min( ( , ), ( , ))
( , )
max( ( , ), ( , ))
cur ref
x W y L
cur ref
cur ref
x W y L
x y x y
Similarity
x y x y
   
   



b b
b b
b b .
                   
(17) 
其中區塊b中位於(x, y)位置的像素定義為b(x, y)，而W及L分別代表著區塊的寬度
與長度。一般的情況之下，兩個區塊越相似，相似性越大。一旦在Fk中的區塊b
在Fk-1中找到最佳的匹配區塊b*，估計的深度區塊d*將為與相對應的Dk-1中所找
到的b*相同的位移區塊。相同的程序也適用於估算Ok。除此之外，為了加速搜
尋過程，我們也實行了數種如three-step search、early-stop、skip-criterion的加速
演算法。 
(2) 區塊及物件優化 
  在 ME/MC 程序之後，由於旋轉與仿射移動，參考幅中的最佳匹配區塊與目
前區塊仍有可能是不一致的。如圖(二十七)所示，此種不一致使得估計的深度圖
難以適當地在目前的幅上顯示正確的深度。若以錯誤的深度圖作為下一幅的參
考，依次估計的深度圖將維持錯誤且導致更嚴重的誤差。 
 
圖(二十七)  ME/MC 的錯誤估計 
  因此，進一步優化產生的深度圖是必要的。為了解決這個問題，首先檢查物
件圖中的每個區塊，觀察是否需要進一步的優化。只有當區塊包含物件與背景兩
個部分且相對應的 ME_cost 大於預先設定的門檻值時，才會執行優化程序。根
據實驗，我們建議將門檻值設定為 0.95。在此情況下，目前區塊中的像素將進一
步分類為前景與背景，假設目前區塊為 b，b(x, y)表示位於(x, y)位置上的像素值，
而 o 代表由 ME/MC 獲得之物件圖中的目前區塊。雖然在 o 中的輪廓並不正確，
仍可用來提供作為分類的預前知識。假設f 及b 分別是 o 中前景與背景的像素
集合，我們可以獲得中f 的平均向量 mf 和協方差矩陣(covariance matrix)Cf，以
及b 中的 mb 和 Cb。接著我們可以得到如下所示的 priori 機率 
 21
(3) 深度圖挑選機制 
 從兩個估計的深度候選中選擇一個較佳的幅，延伸出如何如何估算深度圖的
問題。在此，估算的概念是檢查深度的邊緣是否匹配原始幅上的邊緣。讓Ek、
Ek,F及Ek,B代表Fk、Dk,F及Dk,B的Sobel邊緣圖，然後為了向前及向後的候選我們推
導出以下兩個分數 
,
( , )
,
( ( , ) ( , ))k F k
x y
F
k F
x y x y
Score
N
 

 E E
E
                              (24) 
,
( , )
,
( ( , ) ( , ))k B k
x y
B
k B
x y x y
Score
N
 

 E E
E
,                              (25) 
其中NEk,F及NEk,B代表在Ek,F及Ek,B中邊緣像素的總合。在(24)及(25)中，ScoreF跟
ScoreB分別指出在Ek,F及Ek,B中邊緣像素匹配於Ek的百分比例。選擇何者為較佳幅
的依據取決於何者的分數較高。若兩者的分數相同，則k在小於N/2的情況下選擇
Dk,F，反之則選擇Dk,B。 
(4) 實驗結果 
所提出的方法是使用配備 1G 記憶體的 Intel Core2 8300 個人電腦進行測試。結
果顯示在三個方面：(1)區塊及物件優化的改善，(2)幅選拔機制與人眼挑選比較
的正確性，以及(3)數幀幅的深度。由於 Philips 3D 顯示器只接受解析度為 960x540
的限制，ME/MC 及區塊優化使用尺寸為 15x15 的區塊。為了證明所提出的方法
適用於各種影片，使用隨機選取的資料：電影 Vanhelsing 的預告片。 
(A) 區塊及物件優化的結果 
 為了驗證實驗結果，每一個由半自動(即由人的努力)而得之正確的深度是必
須的。每一階段中深度的 PSNR 如表(六)所示。明顯的，產生的深度品質逐步地
變好。有時在區塊優化後將導致雜訊點出現以及 PSNR 降低。然而，當雜訊點經
由物件優化消除時，深度圖的品質回復甚至優於透過 ME/MC 處理。 
(B) 評估深度圖挑選機制 
 我們透過比較由所提出的方法及經由人眼所選取的幅來評估幅的選拔機制。表
(七)顯示由幅 644 至幅 655 的選取結果。從表(七)中，我們可以發現由所提出的
方法選取的幅幾乎與由人眼所選取的幅相同，這也代表我們所提出方法的正確
性。 
 
 
 
 
 23
 
 
 
 
 
                 (a) 原始影像         (b) 產生的深度圖 
圖(二十九) 產生的深度圖及其原始影像 
 
結論 
    延續第二年之成果，本年度我們首先針對 H.264/MVC 提出了一種投射式視
角預測搜尋演算法，利用相機之內部參數和外部參數產生極線以及更精確的視角
向量預測子以縮小搜尋視窗，並在不影響影像品質的前提下，節省 H.264/MVC
中視角間預測之運算複雜度，進而增進 H.264/MVC 的執行效能。 
另外，針對深度圖的估計，我們也提出了一新型的自動深度序列插補系統，
在只有最前與最後兩張深度已知的情況，能逐步的將中間影格的深度重建出來。
相較於其他現有方法，本系統可適用於更多情況並產生適合的深度序列，可廣泛
的應用於 2D 轉 3D 技術，對於未來 3D 影片之生成將有相當大的助益。 
 25
architecture for stereo video hybrid coding systems,” IEEE Trans. Circuits Syst. 
Video Technol., vol. 16, pp.1324-1337, Nov. 2006. 
[12] T. Wiegand, G. J. Sullivan, G. Bjøntegaard, and A. Luthra, “Overview of the 
H.264/AVC video coding standard,” IEEE Trans. Circuits Syst. Video Technol., 
vol. 13, pp. 560-576, July 2003. 
[13] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. 
Cambridge, U.K.: Cambridge Univ. Press, 2000. 
[14] X. Xu and Y. He, “Fast disparity motion estimation in MVC based on range 
prediction,” in Proc. IEEE ICIP, Oct. 2008, pp. 2000-2003. 
[15] Guofeng Zhang, Wei Hua, Xueying Qin, Tien-Tsin Wong, and Hujun Bao,” 
Stereoscopic video synthesis from a monocular video,”   IEEE Transactions on 
Visualization and Computer Graphics, vol. 13, no. 4, pp. 686-696, August 2007. 
[16] H. Inoue, T. Tachikawa, and M. Inaba.“Robot vision system with a correlation 
chip for real-time tracking, optical flow and depth map generation,” in 
Proceedings of IEEE International Conference on Robotics and Automation, pp. 
1621–1626, May 1992. 
[17] G. Zhang, J. Jia, T. T. Wong and H. Bao, "Consistent depth maps recovery from a 
video sequence", IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 31, no. 6, pp. 974 - 988, June 2009. 
[18] J. Y. Chang, C. C. Cheng, S. Y. Chien, and L. G. Chen, “Relative depth layer 
extraction for monoscopic video by use of multidimensional filter,” Proceedings 
of IEEE International Conference on Multimedia and Expo, pp. 221-224, 2006. 
[19] Y. L. Chang, W. Y. Chen, J. Y. Chang, Y. M. Tsai, C. L. Lee, and L. G. Chen, 
“Priority depth fusion for the 2D to 3D conversion system”,  SPIE Proceedings 
on Three-dimensional Image Capture and Applications , pp. 680513.1-680513.8, 
Jan. 2008. 
 27
研究成果相關發表 
1. H. M. Wang, C. H. Huang and J. F. Yang, “Block-based Depth Maps Interpolation 
for Efficient Multiview Content Generation," IEEE Trans. on Circuits and Systems 
for Video Technology, vol. 21, no. 12, pp. 1847-1858, December 2011.  
2. A. T. Chiang, H. M. Wang and J. F. Yang, “A New Checkerboard-based Adaptive 
Stereo Packing Format for Efficient Stereo Video Coding," International 
Conference of 3D Systems and Applications, June 2011. 
3. H. M. Wang, C. H. Huang and J. F. Yang, “ Depth Maps Interpolation From 
Existing Pairs of Keyframes and Depth Maps for 3D Video Generation," IEEE 
International Symposium on Circuits and Systems, pp. 3248-3251, May 2010.  
4. C. C. Liao, H. M. Wang, Y. H. Chiu, W. S. Wang, and J. F. Yang, “3D Computer 
Graphic Special Effect Editing System for Multi-View Video Production," 2011 
National Symp. on Telecommunications (2011全國電信研討會), Hualien, Taiwan, 
Nov. 2011.  
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/12
國科會補助計畫
計畫名稱: 互動自由視角系統之應用研究與其先進多維度視訊之處理壓縮
計畫主持人: 楊家輝
計畫編號: 97-2221-E-006-109-MY3 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
