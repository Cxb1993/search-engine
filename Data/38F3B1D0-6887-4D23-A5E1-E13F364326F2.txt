 I 
行政院國家科學委員會補助專題研究計畫
■成果報告   
□期中進度報告 
 
     智慧型計算於知識萃取之整合研究-子計畫二：以 SVM 
     為基之模糊塑模及其在財務與醫學知識萃取之應用 
 
 
計畫類別：□個別型計畫   ■整合型計畫 
計畫編號：NSC 100－2221－E－155－016- 
執行期間： 100年 08月 01日至 101年 07月 31日 
 
執行機構及系所：元智大學資訊工程系 
 
計畫主持人：賴國華 
共同主持人： 
計畫參與人員: 陳宗信(碩士生)、廖廷翊(碩士生)、楊南英(碩士生) 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國    101  年  10  月  20  日 
 - 1 - 
1. 緣由與目的 
 
  人類終其一生，約有三分之一的時間是在睡夢中，其睡眠品質之良窳實攸關健康與
否。睡眠品質不佳會造成白天嗜睡，頭痛、注意力、記憶力下降，甚至高血壓、心肌梗塞、
中風等心血管疾病，引起頻尿、性功能障礙等泌尿科問題，甚至有猝死的危險性[1]。而睡
眠呼吸中止症是所有睡眠疾病當中盛行率最高的，其所造成的國家人力損失及家庭影響，
難以估計。因此如何及早發現，以因應與治療，是本研究的動機之一。 
  根據美國睡眠協會的調查，嗜睡是導致每年 20-40萬件交通事故發生的主要原因，且
其中一半的事故是致死性的；因嗜睡而發生工傷事故的經濟損失更是高達 640 億美元；花
在檢測睡眠呼吸中止症上的費用，每年達到 3億美金之譜；再則因打鼾嗜睡所引發的車禍，
更是造成 120 億美金的損失。在我國，行政院勞工委員會勞工安全研究所曾在 2007年做了
一項調查[2]，針對 719位職業客運駕駛做研究，保守估計約有 11%的駕駛患有阻塞性睡眠
呼吸中止症(Obstructive Sleep Apnea,OSA)。客運駕駛擔負著大多數人民的生命財產安全，
11%的比率，實在令人不得不憂心忡忡，此是本研究的動機之二。 
  據美國 Terry Young研究員在 1993年所做的調查[3]，30-60歲的美國中年人裏，符合
睡眠呼吸中止症診斷標準的男性佔 24%，女性則佔 9%；之中患有阻塞性睡眠呼吸中止症
的男性有 4%，女性則有 2%。以此數據換算臺灣人口，約有四十萬人恐怕罹患 OSA，超過
一百五十萬人必須接受檢查。目前的檢查大都必須待在睡眠研究室，但其成本是相當高昂，
亦需耗費多時量測諸多生理數據。尚且不論成本，各醫學中心根本也應付不了如此多的人
數。因此開發一種低成本、簡便且正確的檢測方式，是本研究的主要目的。 
2. 睡眠呼吸中止(Sleep Apnea) 
   
  一般對呼吸中止症的定義：呼吸中止(Apnea)，口、鼻的氣流停止流動超過十秒；呼
吸不足(Hypopnea)，十秒以上的換氣量降低了 50%或以上。睡眠呼吸中止指數
(Apnea-hyponea Index, AHI)，是指每小時的睡眠當中，呼吸中止加上呼吸不足的次數。當
睡眠呼吸中止指數大於 5 以上，就有睡眠呼吸中止症。根據美國睡眠醫學會的定義，睡眠
呼吸中止指數在 5到 15之間是輕度，15到 30之間是中度，30 以上是重度[1]。 
  當睡眠呼吸暫停時，心跳速率會增快，呼吸恢復正常時，心跳速率亦恢復成正常狀態，
故心電圖是判斷是否有睡眠呼吸中止症的一項非常重要的指標。在 2000年，Computers in 
Cardiology(CNIC) 和 PhysioNet 共同舉辦一次比賽，利用電腦計算，針對 ECG 來判斷是
 - 3 - 
及一分鐘註記一次的註記檔，該註記檔記錄著每一分鐘是否發生睡眠呼吸中止的現象。從
其他生理訊號分析中可得知當發生睡眠呼吸中止症時，會產生心跳速率增加的現象(如圖
2)。 
 
圖 1：實驗流程圖 
 
圖 2：受測者一分鐘心跳(ECG)；通過口鼻之氣流訊號(ONR)；及血氧濃度( )。當呼吸中止
情況發生時會產生心跳加速、口鼻之氣流訊號震盪、以及血氧濃度降低等症狀。 
 
 - 5 - 
因原始資料是採一分鐘註記是否發生呼吸中止，故亦以一分鐘之信號作為輸入。其分解
後，重建信號 S = (CA6 + CD1+ CD2 + CD3 + CD + CD5 + CD6 )解決了基線漂移。結果如下圖
(圖 5) 
 
圖 5：基線飄移分解與重建 
3.2.2 處理 ECG 雜訊 
根據上述步驟重建信號S，使用Matlab內建函數wavedec()，一樣使用db4為母函數以解決ECG 雜
訊問題，其結果如圖6。 
 - 7 - 
 
表格來源：2009, Vol.52, No6  “台灣醫界” p13  
表 2：頻域(frequency domain)分析指標 
 
表格來源：2009, Vol.52, No6  “台灣醫界” p13  
 
在心跳中，R 波是辨識度最高的，且未來希望能居家、出外都可檢測，配合已成熟的行動
裝置科技，不必繁雜計算就能預測。所以本實驗只針對 R 波，分別以時域跟頻域分析丟入 SVM
 - 9 - 
參考文獻 
[1] 陳濘宏, “阻塞型睡眠呼吸中止症,” Formosan J Med, 2005 Vol.9 No.3 
[2] 石東生,劉紹興,”職業駕駛員睡眠品質管理研究”,行政院勞工委員會勞工安全研究所,2008
年 3月 
[3] T Young, M Palta, J. Dempsey, J. Skatrud, S. Weber, and S. Badr,”The occurrence of 
sleep-disordered breathing among middle-aged adults,”N.Engl. J. Med., vol. 328, pp. 1230–1235, 
1993. 
[4] T. Penzel, J. McNames, P. de Chazal, B. Raymond, A. Murray, and G. Moody, “Systematic 
comparison of different algorithms for apnoea detection based on electrocardiogram recordings,” 
Medical & Biological Engineering & Computing 2002, Vol. 40 
[5] Haitham M. Al-Angari and Alan V. Sahakian, “Use of Sample Entropy Approach to Study Heart 
Rate Variability in Obstructive Sleep Apnea Syndrome,” IEEE Transactions on Biomedical 
Engineering, Vol. 54, No. 10, pp. 1900-1904.2007 
[6] Martin O. Mendez, Davide D. Ruini, Omar P. Villantieri, ” Detection of Sleep Apnea from 
surface ECG based on features extracted by an Autoregressive Model,” Proceedings of the 29th 
Annual International Conference of the IEEE EMBS Cité Internationale, Lyon, France August 
23-26, 2007. 
[7] Kris Nilsen, Eugene. Zilberg, David Burton, Ahsan H.,” Variations in the Accuracy of the ECG 
Based Detection of Obstructive Sleep Apnoea (OSA) for Different Numbers of ECG Leads and 
Categories of OSA Events,” 30th Annual International IEEE EMBS Conference Vancouver, 
British Columbia, Canada, August 20-24, 2008 
[8] J. Corthout, S. Van Huffel, M.O. Mendez, A.M. Bianchi, T. Penzel and S. Cerutti, ” Automatic 
screening of Obstructive Sleep Apnea from the ECG based on Empirical Mode Decomposition 
and Wavelet Analysis,” 30th Annual International IEEE EMBS Conference Vancouver, British 
Columbia, Canada, August 20-24, 2008 
[9] J Gubbi, A Khandoker, M Palaniswami,” Classification of Obstructive and Central Sleep Apnea 
Using Wavelet Packet Analysis of ECG Signals,” Computers in Cardiology 2009;36:733−736. 
[10] Martin O. Mendez, Anna Maria Bianchi,” Sleep Apnea Screening by Autoregressive Models 
From a Single ECG Lead ,” IEEE Transactions on Biomedical Engineering, VOL. 56, NO. 12, 
December 2009 
[11] Khandoker AH, Gubbi J, Palaniswami M.,” Automated Scoring of Obstructive Sleep Apnea and 
Hypopnea Events Using Short-Term Electrocardiogram Recordings,” IEEE Transactions on 
Information Technology in Biomedicine, VOL. 13, NO. 6, NOVEMBER 2009 
 - 11 - 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
 ■達成目標 
□ 未達成目標（請說明，以 100字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 ■未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100字為限） 
 
   研究成果之期刊論文已投稿。 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
 
所提出之 SVM-based Fuzzy Modeling，搭配其他次要的方法，來提升效能的
手法，據實驗結果，雖然預測的正確率不是很突出，但僅就 R波來做分析尚
能達到 86%，如果能夠從前置處理上著手，比較小波不同的基底函數，或者
嚐試其他的信號轉換，相信假以時日，必能突破現在成績，更進一步凸顯其
實用價值。 
 
 
附件一 
歷史文化以及環境的保護，仍然相當不錯，值得我們學習。在會議中，參與的 sessions，主要 
包括： 
 第一天(7/16)上午參加兩個 Keynote Speeches，Prof. Dimitar Filev 談的主題為：Evoloving 
Systems for Practical Adaptive Control。主要是介紹 evoloving paradigm for the development of 
intelligent systems。同時，也舉出幾個應用，說明其實用性。接著，Prof. Chih-MinLin的 keynote 
重點是：Cerebellar Model Neural Networks (CMNN) and their Applications on Control, Signal 
Processing, and Image Classification，介紹幾個 CMNN-based的 adaptive learning systems, 可
分別應用於 control, signal processing, and image classification 等領域。下午則參加三個
technical sessions，主題分別為：Intelligent Systems、Neural Networks and Support Vector 
Machines、與 Fuzzy Set Theories and Methods，內容豐富，相當精彩。 
 第二天(7/17)上午參加兩個 Keynote Speeches，Prof. Sam Kwong談的主題是 Techniques for 
Evolutionary Multi-objective Optimization (EMO) State-of art and some development，講者除了
介紹 EMO相關研究議題及發展之外，也介紹最近發展的幾個 EMO techniques，包括：hyper 
volume based technique, adaptive operator algorithms, and improved jumping gene paradigm。之
後參加 Prof. A Min Tjoa的 Keynote，主題為：How Linked Open Data (LOD) Can Empower 
Decision Making? 主要討論 LOD的潛能及未來發展方向。中午則在一個 technical session發
表一篇論文，針對 fuzzy constraint-directed agent negotiation提出其概念架構。 
 第二天(7/17) 下午則訪問西安交大的信息學院，討論合作的機會。晚上則參加會議的
banquet.。 
 第三天(7/18) 由西安出發到機場，搭機經香港回國.。 
在積極的聆聽他人的演講或報告後，對這個領域的研究有更深的認識，也看到各國在先進 SMC, 
intelligent systems等相關領域的研究水平，會後也與其他的研究者一同討論與研究相關的議題與 
想法，積極與他人的互動，對於研究有很大的助益。以下分為二個部分說明此次的與會心得：(1) 
精進個人研究：參與四場著名學者的演講，對於智慧型系統的實用性有更深的瞭解；另外，對於 
現況與未來研究也有更深的認知。而在各個 session中也仔細聆聽其他學者的研究報告，更獲得 
許多先進的想法，會後也與其他學者共同探討，除了能自省目前的研究狀況，更衍生未來的研究 
方向。(2) 提昇國際交流：此次會議中認識了多位各國的學者，對於各國的研究方式與文化有更 
進一步的接觸，著實讓我增進個人的國際觀。 
三、考察參觀活動(無是項活動者略) 
  西安交大屬中國大陸 1st tier的研究型大學，學生素質好，師資的流動性偏高，值得發展 
    合作交流。 
四、建議 
國內學者在 SMC相關研究領域，在國際上已占有一席之地，相關單位應持續重點支持，爭取 
更高的榮譽。 
  
A FRAMEWORK OF FUZZY CONSTRAINT-DIRECTED AGENT 
NEGOTIATION WITH LEARNING ELEMENT 
TING JUNG YU 1, K. ROBERT LAI 2 
1 2 Department of Computer Science & Engineering, Yuan Ze University, Taiwan  
E-MAIL: s899409@mail.yzu.edu.tw, krlai@cs.yzu.edu.tw 
Abstract: 
This paper presents a framework of fuzzy constraint- directed 
agent negotiation with learning element to improve the quality of 
negotiation. The learning element involves: 1) fuzzy probability 
constraint for regularizing the opponent’s behavior to decrease the 
noisy beliefs about the opponent, 2) instance matching method for 
reusing the prior opponent knowledge to infer the similar feasible 
actions from similar situations, and 3) the proposed adaptive 
interaction for specifying the appropriate tradeoff among feasible 
proposals to reach an agent’s local or global goal.  
Keywords: 
Intelligence systems, multi-agent systems, agent negotiation,  
fuzzy constraints. 
1. Introduction 
Agent negotiation is an iterative process through which a 
joint decision is made by two or more agents in order to reach a 
mutually acceptable agreement [1]. Since an agent typically has 
imperfect information about other agents, such incomplete 
comprehension normally leads to inefﬁcient and ineffective 
negotiation. Many researches has proposed to solve such 
problems via enabling an agent with the capability of learning 
to operate in initially unknown environments and to become 
more competent than its initial knowledge alone might allow. 
Many machine learning models [2, 3, 4, 5, 6] in a multi- 
agent system have been proposed for predicting opponent’s 
beliefs, including reinforcement learning model [7, 8, 9], 
model-based learning approach [10, 11, 12], and Bayesian 
learning algorithm [13] to reach the expected agreement. In the 
standard reinforcement learning model, the agent receives an 
indication of the current state of the environment as an in- put 
in each step of the interaction. Then, the agent performs an 
action to generate an output and to change the state of the 
environment toward a more desirable resulting state by 
providing a reward or penalty. However, the problem of slow 
convergence that is particular to reinforcement learning 
methods also reveals formidable computational obstacles to 
developing such a model. The model-based learning approach 
yields an explicit framework for modeling the interaction 
between agents and predicting counterpart’s actions.  Carmel 
et al. [3] and Stone et al. [14] present an architecture by which 
an agent can learn the models of rival agents. The learning 
models of regular opponents are used to infer a best-response 
strategy and an algorithm is speciﬁed to elucidate an opponent’s 
strategy from the earlier beliefs of the opponent. Even so it 
suffers from two primary problems, complexity and risk [15, 
16]. Bayesian learning algorithm [17] manipulates occurrences 
of interest using probability distributions. These probabilities 
and observed data are used in making a decision. Zeng [13] 
presents a sequential decision-making negotiation model, in 
which a Bayesian framework is applied to update the 
knowledge and belief that each agent has about the 
environment and other agents. An agent’s partial belief is 
represented by a set of hypotheses. After an agent receives an 
offer from an opponent, it analyzes the offer and modiﬁes its 
belief. The updated belief then becomes the agent’s a priori 
knowledge.  Nevertheless, Bayesian learning algorithm 
represents uncertainty with probability. It cannot handle the 
imprecision or marginal values, and needs a large set of training 
examples to converge toward a correct prediction. 
This paper presents a framework of fuzzy constraint- 
directed agent negotiation with learning element for making an 
efﬁcient and effective agent negotiation during an incomplete 
information environment. The main purpose of this work is to 
improve the negotiation competence of the agents at negotiation, 
by enabling them to learn from their interactions with other 
agents. The problem of negotiation is formulated as a fuzzy 
constraint satisfaction problem (FCSP) ﬁrst, and then a learning 
element for opponent’s beliefs is proposed.  The proposed 
learning element not only uses the fuzzy probability constraint 
to decrease the noisy beliefs, but also utilizes the instance 
matching to reuse the prior opponent knowledge for speeding 
up the problem-solving. Meanwhile, for acquiring an interested 
performance, the proposed adaptive interaction speciﬁes the 
appropriate tradeoff among feasible proposals to reach an 
agent’s local or global goal.  
The rest of this article is organized as follows. Section 2 
introduces the theoretical basis of fuzzy constraint-directed 
negotiation. Section 3 discusses how an agent with learning 
element improves the quality of negotiation followed by 
conclusions in Section 4. 
2. Fuzzy Constraint-Based Agent Negotiation 
Agent negotiation is closely related to a distributed fuzzy 
constraint satisfaction processing in that coming up to a 
mutually acceptable agreement between two or more agents is 
  
the consensus among agents is achieved and vl will be 
approved as the final agreement. For the agent k, an 
evaluation function [0,1]k kC CΨ =∏ → represents the 
aggregated satisfaction value of agent k over the potential 
agreement in a ki C∏ . The aggregated satisfaction value 
(ASV) expresses the measurement of human’s preference. 
Given an offer (counteroffer) ku , the aggregated 
satisfaction value of ku for agent k can be defined as 
1( ) ( )
k km
jk k jC C j
u uµ=Ψ =∑ . 
The development of the negotiation process is governed by 
consecutive offers. However, consecutive offers are derived 
from the negotiation strategies.  These strategies determine 
how agents evaluate and generate offers to reach an agreement 
that is most in their self-interest.  In agent negotiation, the 
concession strategy serves as the offer-generation to explicitly 
present negotiator’s expectation, and the tradeoff strategy 
works as the offer-evaluation to intimate one’s implication. 
Following the iterative offers exchange, an agent may reason an 
opponent’s beliefs to discover regularities via these strategies 
embedded in negotiation processes. 
An agent’s concession strategy is employed to generate a 
new proposal by reducing the agent’s demands, thereby moving 
the negotiation toward a consensus.  In the proposed approach, 
an agent makes a concession by decreasing its previously 
aggregated satisfaction value to generate an offer from a certain 
solution space. In that space, the satisfaction degrees of the 
constraints associated on the solutions equal or exceed a certain 
threshold of acceptability. Even if no solution enables the 
preference within the proposal space to be met, an agent can 
use self-relaxation to lower gradually the threshold of 
acceptability and thus generate new, feasible proposals without 
giving up on any of the agent’s demand. Therefore, the set of 
feasible concession proposals is deﬁned as follows. 
Deﬁnition 1 Set of feasible concession proposals: Given the 
tht offer ut and a threshold kiα  of agent k, the set of feasible 
concession proposals at the threshold kiα   for the (t + 1)th offer 
ut+1 of agent k, given by 
1
k
k uti
C
α +
, can be defined as 
( ) ( ){ }| ( ) ( ) ( )1 1 11k k k k kC u c u u uk u t t i t tti µ α γα = ≥ ∧ Ψ =Ψ −+ + ++
 
Where γ is the concession value.  
 A tradeoff strategy is one by which an agent generates an 
alternative without reducing its demands. Agents can consider 
options for reaching a mutual satisfactory outcome by 
reconciling their interests. In the proposed method, the agent 
generates and develops alternative in a speciﬁc solution space 
with- out reducing its aggregated satisfaction value.  In that 
space the degrees of satisfaction in the constraints associated 
with the solutions equal or exceed a particular threshold.  Thus, 
this method can reach an agreement that acceptably beneﬁts all 
agents. The set of feasible tradeoff proposals is deﬁned as 
follows. 
Deﬁnition 2 Set of feasible tradeoff proposals: Given the 
latest offer u and a threshold kiα of agent k, the set of feasible 
tradeoff proposals at threshold kiα  in response to the 
alternatives of agent k, denoted by kk u
iα
ξ , is defined as 
{ | ( ( ) ) ( ( ) ( )},
k k k k k
k u ii
v c v v u
α
µ αξ = ≥ ∧ Ψ = Ψ  
The overall process of agent negotiation is a series of 
counteroffer-evaluation and offer-generation. This work tries to 
enhance the capability of learning model to improve the 
negotiation quality. The further computation model of learning 
model is described as follows. 
3. Agent Negotiation with Learning Element 
Although an agent has incomplete information of other 
agents in agent negotiation, it can improve its ability to act in 
the feature through diligent study of its own experiences. By 
enabling an agent with the capability of learning, the agent can 
operate in initially unknown environments and become more 
competent than its initial knowledge alone might allow. This 
section mainly describes how an agent applies learning element 
to improve the quality of negotiation in Section 2. 
In agent negotiation, a preference function of issue 
represents an agent’s degree of favor which is applied as the 
basis of evaluation. If an agent knows the opponent agent’s 
preference functions, it may either cater to the opponent agent’s 
favor or conflict with the opponent agents favor based on its 
negotiation goal. However, since a preference function of is- 
sue expresses just the extent of favor for the designate issue, the 
relative importance of issues is used to identify the order of 
preference among negotiating issues. While an agent acquires 
the relative importance of issues for the opponent agent, it may 
realize how the opponent agent selects a proposal from the 
extension of intent at current state. Additionally, a strategy 
mainly inﬂuences the behavior state reasoning. If an agent is 
familiar to the opponent agent’s strategy, it can predict the 
opponent’s intent at next state to handle the opponent’s 
behavior regularity and then adapt itself to achieve its goal.  
The pro- posed framework utilizes the concession value as the 
transition of behavior state. Therefore, by reasoning the 
concession value at next round, an agent may acquire a more 
concrete intent for the opponent agent.  The beliefs of the 
opponent’s strategy and the next concession value often need a 
sufﬁciently large set of training examples to learn and to update 
iteratively for making suitable predictions. Knowing the 
opponent’s intent at next round, an agent needs a simulation 
mechanism to generate the opponent’s extension of intent. It is 
  
indicates a fuzzy probability. 
The linguistic term “high” is deﬁned as in Figure 2 and the 
membership functions of fuzzy probability can be denoted as 
Figure 3 which is used to tell the occurrence frequency of fuzzy 
event. 
The proposed model regards a series of satisfaction degrees of 
counteroffers from the ﬁrst to the ﬁnal rounds in each 
negotiation as training instances residing in instance repository. 
Each instance in instance repository also records the strategies 
of negotiating agents and the result of negotiation. The proposed 
instance matching calculates the differences of satisfaction 
degrees of counteroffers between the current and each of the one 
in instance repository via Euclidean distance method. Each 
distance means the extent of proximity between the current and 
the one in instance repository.  Usually an agent may adopt the 
similar possible actions from similar situations. Therefore, an 
agent may deduce the opponent agent’s behavior features from 
the proximate instances with a certain extent of proximity.  
Because each instance in instance repository records the 
strategies of negotiating agents, the beliefs of the opponent’s 
strategies can be identiﬁed via the following constraint. 
1...arg ( )( )ii n proj fc e= Ω Ξ , 
where n denotes the number of instances in instance repository; 
ie means the ith difference of satisfaction degrees be- tween the 
current and the training instances; fc ( )⋅ is a fuzzy clustering 
function; Ξ represents a logical operation such as max, min, and 
so on ; ( )projΩ ⋅ is a project function to report the set of the 
opponent agent’s strategies. 
The differences of satisfaction degrees between the current 
and the training instances mean the extent of proximity among 
instances. Therefore, the opponent agent’s strategies can be 
predicted via a certain extent of proximity. The proposed fuzzy 
clustering function ( )fc ⋅  derives from the fuzzy c-means(FCM) 
method which allows the number of clusters to be automatically 
adjusted during the iteration by merging similar clusters and 
splitting clusters with large standard deviations, to relax the 
boundary between clusters. The logical operation Ξ is 
responsible for deciding the threshold for the extent of proximity. 
Any strategy out of the beliefs of the set of opponent’s strategies 
will be excluded for reducing the computational complexity. 
The opponent agent’s behavior state reasoning is often 
implemented via the concession value in most of researches. 
Knowing the opponent agent’s next concession value will be 
able to enable an agent to infer the opponent’s possible intent at 
next round. The proposed framework also utilizes the con- 
cession value as the transition of behavior state. Similarly, the 
formula of Euclidean distance can be applied to compute the 
differences of opponent’s concession values at each round. A 
concession value denotes the difference of aggregated 
satisfaction values which normalized the sum of satisfaction 
degrees of negotiating issues. Hence, an agent may infer the set r 
of the opponent’s next concession values via the 
constraint 1..argi n=  projγ  (Ξ ( )ifc r ), in which ir means the 
ith difference of concession values between the current and the 
training instances and the projection function projγ (•) is used 
to report the set of concession values. To obtain a more concrete 
con- cession value, the opponent’s next concession value r∗ can 
be predicted from the set of the opponent’s next concession 
values by the weighted average method as follows. 
( )*
( )
k k k
k k
r rr
r
µ
µ
⋅∑=
∑
 
In which kr r∈ . 
 
Based on the concrete intent, the mechanism of reasoning 
the opponent’s next concession value may enable an agent to 
look ahead about the opponent’s possible intent at next state and 
adapt dynamically along with the change of intent. How- ever, 
the beliefs of the set of the opponent’s next feasible proposals 
are hard to learn under the situation of imperfect information. 
Such a situation normally leads the beliefs of the set of the 
opponent’s next feasible proposals to be imprecise and 
ineffective. Therefore, based on the similar inference of 
psychological activity, this work simulates the mechanism about 
the opponent’s behavior state reasoning to learn more concrete 
intent of the opponent agent and provides ﬁve aspects of beliefs 
percept, the beliefs of the opponent agent’s preference functions, 
the beliefs of importance of issues about the opponent agent, the 
belief of the opponent agent’s next concession value, the agent’s 
next concession value, and the agent’s goal to explore the 
opponent agent’s possible actions. The beliefs of the opponent 
agent’s preference functions are used to generate the tuples of 
offer-values as feasible proposals. The beliefs of importance of 
issues about the opponent agent are applied to construct the 
possible variations of issues in feasible proposals.  The belief 
of the opponent agent’s next con- cession value is utilized for 
computing the opponent agent’s aggregated satisfaction value at 
current state. All these beliefs are regarded as the opponent 
agent’s mental state. Addition- ally, the agent’s next concession 
value, and the agent’s goal are modeled as responsive state of its 
rival agent.  According to these simulated mental and 
responsive states, the opponent agent’s next feasible proposals 
are inferred. It is helpful for an agent to consider the potential 
situations that an agent could encounter and specify the agent’s 
behavior suitably in advance. 
In proposals selection, the proposed model applies a utility 
function, called adaptive interaction, to provide a way in which 
the possibility of success can be evaluated against the 
importance of the goals. To maximize the learning agent’s 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/23
國科會補助計畫
計畫名稱: 子計畫2:以SVM為基之模糊塑模及其在財務與醫學知識萃取之應用(III)
計畫主持人: 賴國華
計畫編號: 100-2221-E-155-016- 學門領域: 資訊系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
