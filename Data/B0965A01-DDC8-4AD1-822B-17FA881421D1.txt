 2 
中文摘要 
 
由於數位影像技術的提升與大容量儲存媒介價格的下降，大量的影像及視訊資
料的儲存愈來愈普遍。隨著網際網路的普及通訊品質的提升，有愈來愈多的數位媒
體資料，如文字、影像、視訊等被存放於網際網路上。如何精確、有效且快速地從
網路獲得想要的資訊，已是網路搜尋服務亟欲解決的問題。本研究首先針對 Corel 影
像資料庫，於 94 年度研發建立了以 Visual keyword 作為影像索引的影像搜尋引擎。
初步實驗結果顯示，利用 Visual keyword 搜尋 Corel 影像資料庫的命中率超過九成。
隨後，將此技術拓展於網際網路上的影像搜尋上，建立了由 20 台影像伺服器所組成
的分散式影像檢索系統，搜尋的範圍擴展到 45,000 張以上的 Internet 或 WWW 影
像。初步的搜尋命中率已達六成以上，每次查詢平均需時 6 秒。本年度擬研究在目
前以圖像 Visual keyword 為基礎的檢索（VBIR）技術上，結合「圖像標誌檢索」
（TBIR，tag based image retrieval）技術的網際網路搜尋引擎，完成一個能搜尋網際
網路影像與視訊的檢索系統。應用「圖像標誌檢索」技術需將圖像先加注標誌，本
計劃將研發 Web 2.0 的標誌系統：由進入網頁的使用者來協助注釋的工作，不斷地對
小部份的影像加上標誌，再以機器學習的方式將大部分的影像自動的加上標誌。由
於網路上的使用者興趣與喜好會隨時改變，因此人工方式的標誌將由各地上網使用
者不斷的協助進行，隨時隨地適應網際網路上多變化的環境。因此「Web 2.0 影像標
誌系統之建構」將為本研究計劃書所規劃之創新研究工作。 
 
關鍵詞：類神經網路，分散式影像資料檢索，影像分群，分散式視訊資料檢索，圖
像標誌 
 4 
一、前言 
由於數位影像技術的提升與大容量
儲存媒介價格的下降，大量的影像及視訊
資料的儲存愈來愈普遍。隨著網際網路的
普及通訊品質的提升，有愈來愈多的數位
媒體資料，如文字、影像、視訊等被存放
於網際網路上。如何精確、有效且快速地
從網路獲得想要的資訊，已是網路搜尋服
務亟欲解決的問題。目前雖已有許多網路
搜尋引擎，如 Google [10]，YaHoo [11]
等，以文字做為關鍵字來幫助使用者於網
際網路上搜尋其所要的資料。若所要搜尋
的為影像或視訊資料時，由於目前並沒有
很成熟完美的自動影像或視訊文字索引建
置技術，以文字做為關鍵字的搜尋技術常
常無法精確的找到使用者所期望的影像或
視訊資料。因此，目前有越來越多關於這
方面的研究出現，如以影像內容為基礎的
搜 尋 方 式 （ Content-Based Image 
Retrieval）研究 [1-9] 與以視訊內容為基
礎 的 搜 尋 方 式 （ Content-Based Video 
Retrieval）的研究 [12-19]。 
目前雖已有許多以文字為基礎的影
像及/或視訊搜尋引擎[10][11]。然而，用
影像或視訊內容為基礎的大量資料的搜尋
引擎系統則還未看到。因此，於 93 年度
計畫中首先針對 Corel 影像資料庫[20]開
始分析 [24]，於 94 年度研發建立了以
Visual keyword 作為影像索引的影像搜尋
引擎[25]。Visual keyword 整合了影像低
階特徵，並以類同於文字搜尋的「關鍵
字」的觀念來為影像作索引。我們所提出
Visual keyword 的觀念及抽取法也已提出
了專利申請 [21, 22, 23]。 
隨後，並實作了以 1000 張 Corel 影
像做為搜尋對象的類神經網路影像搜尋系
統（Neural network based image retrieval 
system ： NNIRS http:// 140.113.216.78/ 
imagequerysystem/ )。初步實驗結果顯
示，利用 Visual keyword 的搜尋命中率超
過 90%。 
為了驗證所提之索引技術在大型資
料庫之運作情形，本年度擬延伸目前完成
之以視訊內容為基礎的影像搜尋引擎的技
術，研究開發能結合圖像標誌檢索技術的
網際網路搜尋引擎，完成一個能搜尋網際
網路上影像與視訊的檢索系統。 
 
二、研究目的 
如眾所知，人工注釋是可以將影像
或視訊檔案做最完善的描述，但這個工作
相當費時及費(人)工。因此多年來有很多
的研究在探討如何利用電腦來自動化「影
像注釋」，但到目前為止並沒有看到很好
或可用的成果。而且利用注釋文字來檢索
網路上的影像時，結果常常並不很理想
的。因為用這種方法用到網路影像的檢索
時，並沒有考慮到網際網路的影像會隨時
改變，以及網頁瀏覽者在檢索想要的相關
影像時，他們的興趣也會隨時有所改變，
因此常常得不到想要的結果。 
目前在網路上進行內容檢索時，都
強調要以彈性方式來產生所需的標籤
（Tagging) [26, 27]。有鑑於此，本計劃將
以人工注釋的方式，不斷地對小部份的影
像加上標誌，而以機器學習的方式將大部
分的影像自動的加上標誌。由於網路上的
使用者興趣與喜好會隨時改變，因此人工
方式的標誌最好由各地網頁瀏覽者不斷續
地來做，方能隨時隨地適應網際網路的現
況。因此「Web 2.0 影像標誌系統之建
構」為本研究計劃書所規劃之主要工作。 
 
 
三、研究方法 
如前所述，網際網路上累積的影像
資料已近一兆頁。從隨機收集到的影像資
料可發現，有些影像資料有隨附的注釋文
字。這些注釋文字中多包含有與圖像相關
的字詞。一般來說，在做圖像搜尋時可以
利用文字「資料探勘」(Data mining) 技術
將一些相關「字詞」抽取出來做該圖像的
標誌，然後用關鍵字「全文比對」或「標
誌比對」的方式將相關圖像檢索出來。然
而，根據 [28] 指出，用「全文比對」這
樣找到的相關圖像並不理想。 
 6 
舉例如下，假設圖三為圖像 經過圖像
區塊標記字詞及權重建構後的結果，
, { , ,..., }IxR x A B F∈ 為圖像  的圖像區塊資
訊 ， 即 1 3{ 1, 3, 6; , ,I A AA T TR T T T w w= 6}ATw ，
1 2 4 8{ 1, 2, 4, 8; , , , }I B B B BB T T T TR T T T T w w w w= ，以此
類 推 ， 則 圖 像 I 的 註 解 字 詞 為
{ 1, 2, 3, 4, 5, 6, 8}T T T T T T T ，而其對應到一
組權重值 1 2 3 4 5 6{ , , , , , ,I I I I I IT T T T T TW W W W W W 8}ITW
其 中 1 1 1 1 1max{ , , , }I A B C ET T T T TW w w w w= ，
2 2 2max{ , }I B DT T TW w w= ，以此類推，而 ITtW 值
越大就表示Tt 這個標記字詞適用於圖像 I
的可能性越高，在產生圖像註解字詞時，
便可使用 ITtW 來決定其排序優先順序。 
 
圖三：圖像區塊標記字詞及權重建構示意圖。 
然而在實作系統時，圖像分割的結果通常
不會如圖三那樣的理想化，很有可能會產
生許多零碎、較小且不具語意的圖像區
塊，因此在對圖像做自動圖像註解時，我
們另外對圖像區塊的大小做限制，令其必
須大於一定值才進行圖像標記字詞及權重
建構。 
 
四、結果與討論 
本研究的實驗平台為：在硬體方面使
用 Intel Pentium-4 3.0GHz 時脈速率中央處
理器的個人電腦，搭配有 1Gigabytes 主記
憶體，作業系統為  Fedora Core 5 搭配 
MySQL 資料庫；在圖像資料來源方面，使
用 Corel 圖像資料庫中 28 個圖像類別集
合，每個類別集合有 100 張圖像，並對這
2800 張圖像切割，總共分成約 21 萬個圖
像區塊，在實驗中我們以人工標記字詞作
為標準答案，而設定人工標記字詞的標準
如下： 
(1) 針對每張圖像中較顯著的圖像區
塊設定一個或多個人工標記字詞； 
(2) 對於每張圖像設定 1~7 個不同的
人工標記字詞，即使用 1~7 個不同的字詞
描述一張圖像。 
如此一共對約 16000 個圖像區塊設定
了人工標記字詞，其中包含約 300 種不同
的標記字詞，實驗時從每個類別集合中隨
機選取 80 張圖像做為訓練資料，即已存在
資料庫中的圖像，其餘做為測試資料。 
為評估產生圖像自動註解字詞的效
能，我們讓系統給出 1 至 15 個不同的自動
註解字詞。將測試資料圖像單張輸入系統
中進行自動圖像註解，完畢後將其移除換
下一張測試資料圖像，圖四、五分別為資
料庫未經過標記字詞及權重建構與資料庫
經過標記字詞及權重建構的正確率與召回
率比較，橫軸為系統給的註解字詞個數。 
 
圖四：自動圖像註解正確率。 
 
 
圖五：自動圖像註解召回率 
 8 
[7] C. Frankel, M. J. Swain, and V. 
Athitsos, “Webseer: An image search 
engine for the world wide web,” Tech. 
Rep. TR-96-14, Computer Science 
Department, University of Chicago, 
1996.  
[8] F. Rabitti, P. Stanchev, 
“GRIM_DBMS: a graphical image 
database mangement system”, Visual 
Database System, Elsevier Science 
Publishers, B. V., IFIP, 1989, P415-
430. 
[9] Chad Carson, Megan Thomas, Serge 
Belongie, Joseph M. Hellerstein, and 
Jitendra Malik, “Blobworld: A system 
for region-based image indexing and 
retrieval,” Third International 
Conference on Visual Information 
Systems, June 1999, pp. 509–516 
[10] Google, http://www.google.com 
[11] YaHoo, http://tw.yahoo.com 
[12] M. Flinker, H. Samhey, W. Niblack et 
al., “Query by Image and Video 
Content: The QBIC System”, IEEE 
Computer, 28,（Sept. 1995), pp. 23-
32. 
[13] A. Hampapur, A. Gupta, B. Horowitz, 
C-F. Shu, C. Fuller, J. Bach, M. 
Gorkani, and R. Jain, “Virage Video 
Engine”, SPIE Vol. 3022, 1997. 
[14] D. Ponceleon, S. Srinivasan, A. Amir, 
D. Petkovic, and D. Diklic, “Key to 
Effective Video Retrieval: Effevtive 
Cataloging and Browsing”, ACM 
Multimedia, ’98, pp. 99-107. 
[15] S-F. Chang, W. Chen, H. Meng, H. 
Sundaram, and D. Zhong, “A Fully 
Automated Content Based Video 
Search Engine Supporting Spatio-
Temporal Queries”, IEEE Transaction 
on Circuits and Systems for Video 
Tecnology, Vol. 8, No. 5, Sept., 1998. 
[16] S. Adali, K. S. Candan, S-S. Chen, K. 
Erol, and V. S. Subrahmanian, 
“Advanced Video Information System: 
Data Structure and Query Processing”, 
Multimedia System Vol. 4, No. 4, Aug. 
1996, pp. 172-86. 
[17] C. Decleir, M-S. Hacid, and J. 
Kouloumdjian, “A Database Approach 
for Modelling and Querying Video 
data”, LTCS-Report 99-03, 1999. 
[18] H. Jiang and A. Elmagarmid, “Spatial 
and temporal content-based access to 
hypervideo databases” VLDB Journal, 
1998, No. 7, pp. 226-238. 
[19] J. Z. Li, M. T. Ozsu, and D. Szafron, 
“Modeling of Video Spatial 
Relationships in an Object Database 
Management System”, Proc. of Int. 
Workshop on Multi-media Database 
Management Systems, 1996, pp. 124-
132. 
[20] Corel Gallery 1000000, 
http://www.corel.com . 
[21] 台灣專利申請一件。專利名稱：
「圖像檢索之方法及系統」，申請
日 為 2006/6/8 ， 申 請 案 號 為
095120438。 
[22] 中國專利申請一件。專利名稱：
「圖像檢索之方法及系統」，申請
日 為 2007/4/11 ， 申 請 案 號 為
200710090547.5。 
[23] 美國專利申請一件。專利名稱：
“OBJECT-BASED IMAGE SEARCH 
SYSTEM AND METHOD”，申請日
為 2007/06/07 ， 申 請 案 號 ：
11/759,911。 
[24] 國科會結案報告, “類神經網路影像
檢索之研究,” NSC 93-2221-E-009 -
060, 2004。 
[25] 國科會結案報告, “類神經網路影像
檢索之研究(II):影像資料索引之建
構 ,” NSC 95-2221-E-009 -138, 
2005。 
[26] Tim O'Reilly, “What is Web 2.0?” 
http://www.oreillynet.com/pub/a/oreill
y/tim/news/2005/09/30/ what-is-web-
20.html?CMP=&ATT=2705. 
[27] Ulises Ali Mejías, “Tag Literacy,” 
http://ideant.typepad.com/ideant/2005/
04/tag_literacy.html. 
[28] Apostol（Paul) Natsev, "Multimodal 
Search for Effective Video Retrieval", 
in Proceeding of 5th International 
Conference, CIVR 2006, Tempe, AZ, 
USA, July 13-15, 2006. 
（甲）出席 NISS2009 會議 
參加會議經過: 
NISS2009 會議自 98 年 6 月 30 日開幕 98 年 7 月 2 日閉幕， 於中國北京市舉行 3
日，由 AICIT 協會主辦， IEEE，IPM，CAS， 及 GERI 協辦。NISS 是「信息和服務
科學新趨勢」（New trends in information and service science）的簡稱。NISS2009 會議全
名為 2009 International Conference on New Trends in Information and Service Science。
NISS2009 是一個展望消費者導向及以服務為導向技術的國際會議。 
會議討論的主題包括五項： 
主題 1 ─ SC: Service Computing（服務計算） 
主題 2 ─ SIS: Service Oriented Information Science （服務導向的信息科學） 
主題 3 ─ CSE: Consumer and Service Oriented Communication， Network and 
Electronics （消費者和服務導向的通信、網絡和電子） 
主題 4 ─ IMS: Consumer Oriented Information Management and Service （消
費者導向的信息管理與服務） 
主題 5 ─ ICS: Service Oriented Industry and Convergence Science （服務導
向的工業和收斂科學） 
會議討論分為兩個軌道(Track)： 
軌道 1─ 研究成果：研究成果軌道的目的在介紹和討論多學科和混和/收斂研究在
消費者導向和服務導向技術的研究成果。  
 
軌道 2─ 技術經驗：技術經驗軌道的目的在建立一個論壇提供從業人員和研究人員
在不同的組織及環境、不同的系統、或不同的文化中有用的解決方案。報
告的內容分為兩類：（1）案例研究及（2）經驗報告。 
NISS2009 會議議程包括有主題演講，專題 workshop 及論文報告 
(A) 主題演講 
K-1 講題：Heterogeneous Information Integration and Mining ? A Review 
and Case Study on Real-time Incident Management 
講者：Dr. Gang Kou， School of Management and Economics， 
University of Electronic Science and Technology of China 
摘要：Information integration and mining of multiple autonomous and heterogeneous sources is a 
challenging research problem and is essential to achieve efficient information sharing. This 
talk has two parts. The first part reviews the progress of information integration research in 
last decade (e.g.， Semantic Web， Ontology， Mash-up， and so on) and describes the state 
of the art Data mining techniques (e.g.， Web Mining， Text and Multimedia Mining， Social 
Networks mining) which play key roles in automatic information integration systems. The 
second part presents the structure and implementation of an information integration and 
mining system on real-time incident management. 
K-2 講題：The Review and Application of Frontier Methodology on 
Neo-Spatial Economics 
講者：Dr. Rui Chen， Director of CCCOD， IPM， Chinese Academy of 
Sciences， China 
摘要：Spatial economics (SE) research field is focused on the optimal resource allocation in 
space-time. Although the location theory has a long history， different from the research of 
time series， which has not been the mainstream of economics at all times. However， in 
1991 and 1995， Krugman's Geography and Trade， Development， Geography， and 
Economic Theory have led the economic geography and spatial economics to revival. At 
VIDEOFARM: Playing video and display associated information 
 
 
Hsin C. Fu,  
Dept. of computer science  
National Chiao-Tung University 
Hsinchu, Taiwan, ROC 
hcfu@cs.nctu.edu.tw 
Hsiao T. Pao 
Dept. of management science  
National Chiao-Tung University 
Hsinchu, Taiwan, ROC 
htpao@cc.nctu.edu.tw 
Yeong Y. Xu  
Dept. of computer science 
HungKuang University 
Taichung, Taiwan, ROC 
yyxuster@gmail.com
 
 
Abstract 
 
This paper addresses a new type of video sharing 
platform-VideoFarm. In addition to just playing user 
uploaded video clips, the VideoFarm also displays 
the associated information of currently playing video 
contents. The associated information includes the 
general description of the video clip, a dynamic 
electronic map which shows the location of the 
current video contents. The location associated 
information, such as local stores, restaurants, 
lodging, and near by tour attractions are provided 
and displayed in parallel with the playing video clip. 
Through the  URLs  of  the  associated   
information, a  viewer may  collect  plenty of  
tour  information, when (s)he  is  enjoying an  
interesting  video  clip.            A  
prototype  of  VideoFarm  has been implemented 
at http://140.113.216.55/videomap for public trial, 
evaluation, comments, and suggestions. 
 
1. Introduction 
 
In recent years, video sharing becomes one of 
popular services over Internet.  For example, in 
January 2008 alone, nearly 79 million users had 
made over 3 billion views at YouTube Website [1]. 
There are many reasons that a Web site can attract a 
crowd of people. The main reasons are: it is easy to 
upload videos, it is easy to find desired movies, and 
the most important is to have interesting films. For 
example, an 8 minute video called "Battle at Kruger" 
[2] was posted on YouTube in 2007. Up to November 
of 2008, the video received more than 38 million 
views, and 41 thousand comments. This is an 
unprecedented record. However, after watching the 
short film, how many people know where on the 
earth the Kruger Park is? 
Since most of videos on the web were made by 
amateurs, there is neither a map nor a description to 
depict where the movie contents were taken place 
like some commercial films such as Discovery 
Channel’s “Pilot Guides” series [3]. Some people 
may say: “The video itself already possess both space 
and time scenarios, why additional maps are 
needed?” In fact, the space scenario includes local 
and macro views. In a video or a film, most of views 
are limited to local scenery; people can hardly 
deliberate macro-geographical aspects from local 
scenes.  
In particular, some of foreign place names, 
which usually are transliterations, may often just a 
sequence of audio sound to most movie viewers. 
Therefore, video contents involved in the unfamiliar 
names, dynamic changes in orientation and locations, 
often need supporting information to point 
audience's vision to a better sense of direction, to a 
solid sense of stereoscopic view on the Earth, and to 
bring some background stories for better 
understanding on the video contents. When a film is 
playing, if augmented information is presented in 
time, in particular the detailed location of a building, 
and its surrounding environment, and/or people’s life 
style, even a not very eye-catching film may also be 
immense.  
Generating associated messages with the 
contents of a movie is very difficult, because a 
number of qualifications and tremendous knowledge 
are required to provide proper information for films 
from all over the world: 
(1) One has to deeply understand the film’s 
contents; otherwise it would not be possible 
to select impressive scenes or attractions for 
interesting messages.  
(2) One has to familiar with the background 
stories of a film, so as to associate scenes or 
attractions with the corresponding geographic 
locations and related information.  
In the following sections, we will present a Web 
2.0 based platform for video playing and augmented 
information displaying for the associated video 
contents. In Section 2, basic concepts of our approach 
are described. A walk through of the prototype 
VideoFarm platform is depicted in Section 3. 
Concluding remarks and future works are drawn in 
Section 4. 
 
2. Our approach 
 
Our approach to present online information with a 
video is to deploy a Web 2.0 based platform, called 
VideoFarm, which contains a video player and an 
online multi-information display board. We intend to 
congregate Internet users to contribute interesting 
be displayed, as shown in Figure 3. In addition to 
upload a video clip, the user may input a brief 
description of the uploaded video.  
 
3. Walk through of VideoFarm 
 
A prototype of VideoFram has been implemented 
at http://140.113.216.55/videomap to show the 
proposed four features described in Section 2. When 
a user clicks on a preferred video icon in either Home 
page or in Video page, the video begins playing and 
the information board shows the e-map of the 
associated video contents first. Then, it may switch to 
a URL list of geographic information, such as local or 
nearby scene spots, tour guides, lodging and food 
services, souvenir shops, or even associated 
commercial ads, as shown in Figure 2. While 
watching a vedio, a user sees a scene, such as a house, 
a car, a celebrity, a fashion show or anything that is 
impresive to motivating background stories, 
commercial ads, or something worthy to be shared 
with friends, or other people, (s)he may click the 
“Add an Attraction” button to add an attraction 
picture and associated information to the Attraction 
list, and to signal the information displaying 
mechanism to include new attraction related 
information contents. Also, an attraction dialogue box  
as shown in Figure 4, is opened to receive input from 
the user. The input includes: 1) the title of the 
attraction, 2) the location of the attraction, and 3) 
brief introduction or description of the attraction. 
Among the request information, only the title of the 
attraction is mandatory to initial an attraction 
suggestion, and the rest of information can be filled 
in by other users who know more or better of the 
suggested attraction.  When the user finished and 
closed the attraction suggestion dialogue box, the 
video continues playing, and the user may enjoy the 
movie and/or make a new suggestion of another 
attraction. In the meantime, the provided text 
information of the attraction is analysed to extract 
keywords for the searching of various associated 
multimedia contents.  
It is common to see that a person’s favored 
attractions may be other’s aversion. Thus, presenting 
attractions to a Web video browser needs to be 
carefully screening to avoid counter effect. In other 
words, it is needed to match attractions with 
browsers’ preference. Tagging mechanism is 
deployed in both attraction description and user 
registration. On the attraction side, keywords that are 
extracted as tags from the attraction descriptions by 
using IR techniques [4], and recently developed 
content tagging techniques [5, 6] are also used to 
build tags from the frame images of selected 
attractions. In order to tagging user’s preference, a 
user is asked to fill in the preference-related tag 
words during the registration. In addition, when a 
user suggests attractions, the tags associated with 
these attractions are collected and attached to the 
user’s tag list.  
 
 
 
Figure 4: Dialogue box for attraction 
description. 
 
4. Concluding remarks 
 
The walk through presentation in Section 3, 
shows a great success and future potential of the 
augmented information board in extending a video 
sharing web site into a multimodal information 
provider and various e-commerce agents. The 
prototype system also validates the combination of 
recently advanced WWW search techniques, and 
newly developed content tagging schemes to achieve 
a satisfactory level of personalized multimedia 
information retrieval platform. Currently, we are 
working on the English language version of the 
VideoFarm, and in the near future, we plan to include 
channels and community pages to enhance the social 
networking capability of the VideoFarm platform. 
 
Acknowledgement 
 
（乙） 訪問北京交通大學計算機科學系 
（1）交流訪問經過: 
報告人在出席 NISS2009 會議結束後，接受北京交大計算機科學系主任于劍教授邀請，
赴北京交大計算機科學系訪問一週(7/3~7/8)。期間工作項目包括：參觀北京交大計算機學
院重點實驗室，專題報告(演講)，及與計算機科學系教師討論合作研究事宜。茲分別報告
如后。 
1. 參觀重點實驗室 
報告人在北京交大參觀下述重點實驗室 
 北京市現代信息科學與網絡技術重點實驗室 
 鐵路信息科學與工程鐵道部重點實驗室 
 高速鐵路網絡管理教育部工程研究中心（籌） 
 認知計算與機器學習實驗室 
 高性能計算與應用實驗室 
 多模信息處理實驗室 
報告人在參觀過這幾間重點實驗室後，瞭解到北京交大在支援軌道行車訊息處理及
認知、計算及機器學習及多模信息處裡技術及學理方面都有很先進的成就。 
2. 專題報告(演講)  
報告人在北京交大做了二次專題報告（演講）： 
1) 影音園地網站之建構及目標， 
2) Multimodal search for effective image search。 
    在演講中，北京交大師生對報告人所設計及架設的大資料量影音處理及播放分散式叢
集伺服器頗有興趣，因而引發了後續幾天在交大與幾位教師洽談有關研究事宜。 
3. 洽談有關合作研究事宜 
在洽商合作研究的過程中，報告人得知北京交大計算機學院在訊息處理、機器學習及多
模信息的理論研究有很好的基礎及成果，然而在實施這些理論上的發現就比較力不從心。
因此報告人在洽談的過程中也與幾位北京交大的教授們分享個人在建構影音伺服叢集的心
得，也同時規劃了幾項未來合作研究題目: 
1) 認知科學在影像搜尋的實施即應用， 
2) 影像搜尋於鐵道管理之應用， 
3) 多模信息處理於影音搜尋方面之研究。 
（2） 交流訪問心得及建議 
