the face image. Moreover, the Gradient-
faces is adopted to extract the features for 
face recognition under varying illumina-
tion. Experimental results show that the 
recognition rate of the proposed system is 
97.9% for our private database. In addition, 
an effective wall following control system 
for mobile robots is proposed. The genetic 
algorithm is adopted to improve the per-
formance of the fuzzy controller. Moreover, 
a local reference path is designed based on 
morphological image processing to obtain 
a smooth wall following path from the raw 
laser range finder readings. After we obtain 
the accurate reference path, the robot can 
be controlled to approach the desired path. 
 
Keywords: face detection, face recogni-
tion, stereo camera, and Gradientfaces, 
fuzzy controller, genetic algorithm, path 
planning, wall following, mathematical 
morphology, laser range finder. 
 
一、前言 
本子計畫之主要目的為發展以計算
智慧（computational intelligence）為基礎
的機器人視覺與聽覺系統。其中機器人
視覺包含機器與人的互動與環境的認
知。而人體與目標物的偵測是一重要的
前處理步驟，藉由取得景深的資訊並計
算出前方景物與機器人的距離關係，可
以使機器人有能力自行判斷出前方是否
有人或目標物的存在，然後進一步求取
人體位置的資訊。利用所得的人體位置
資訊後，對於後續人臉的偵測與辨識就
相對方便許多。人臉的偵測與辨識則是
對於提供服務以及環境監控等應用所必
須具備的基本功能。人臉偵測與辨識的
目地在於偵測出人臉的位置以及方向，
以提高人臉辨識的準確率。其結果可以
用來驗證人體偵測的正確性以及做為環
境監控的依據。因此，本計畫著重於人
臉偵測與辨識系統之設計。 
3D立體視覺一直是許多科學家研究
的目標的探討的主題，其中的立體視覺
技術則可廣泛應用於諸如三維影像物件
偵測、追蹤與辨識、三維物體與場景重
建、機器人視覺導航以及汽車之駕駛安
全輔助系統等不同領域。本計畫首先利
用雙眼立體視覺相機來取得立體像差影
像，接著使用 X-D 平面的二維模糊歸屬
直方圖來分析影像中可能存在人體的部
分，同時利用大小適應性濾波器來針對
符合人體平均大小的區域做加強，使其
更有利於直方圖的分析以及準確的偵測
出人體物件。 
人臉偵測目的在於從包含複雜背景
之影像中，找出其人臉可能出現在位
置，進而可以用來確認人體偵測的正確
性，並有利於人臉辨識與手勢擷取的進
行。本計劃採用結合特徵臉(Eigenface)
演算法與類神經網路的人臉偵測系統來
找出各張輸入影像中的人臉座標與大
小。接著，我們利用 disparity 影像資訊
與轉角偵測法(corner detection)找出人臉
特定之特徵點三維座標。並且，利用彩
色空間中人體膚色的分佈情形，分割出
屬於膚色的人臉部分。最後，依照
disparity的資訊，將分割出來的人臉部分
作距離的正規化，使每張偵測出來的人
臉都與攝影機有相同的距離。在切割出
人臉區域，並且完成距離正規化之後，
我們可以利用偵測出的人臉特徵點以及
其三維座標，估測出影像中人臉在三度
空間的旋轉角度(yaw, pitch and roll)。最
後，我們利用特徵萃取方法，抽取出代
表人臉影像的特徵。 
在比對模組方面，我們在系統資訊
庫中，每位不同人員擁有五張人臉影像
之特徵向量。此五張人臉影像分別以不
同對攝影機之偏離角(yaw)作取樣。一張
測試影像經過前述前處理與特徵萃取之
後，將與資料庫中，每位人員的 yaw 角
度最接近之人臉特徵進行比對。 
 機器人在接受指令後，需移動至目
標區執行任務，因此路徑規畫（path 
planning）是一個很重要的議題。在控制
系統方面，我們藉由模糊控制器的設計
並且搭配基因演算法能自我演化的特
性，產生出更貼近於環境的模糊控制參
數，使移動式機器人在一個未知的環境
中，於設定的安全距離內達到沿牆行走
之行為（wall-following behavior）。 
方法來得優異。 
 
(五) 路徑規劃 
在移動式機器人領域中，路徑規劃
是重要的議題之一。近年來，許多的學
者發表了許多對於機器人路徑規劃系統
的研究，像是經驗示搜尋（ heuristic 
searching）應用於網狀地圖（grid-type 
maps）[25]、勢能場（potential field）[26]、
向量場（vector field）[27]、組態空間
（configuration space）[28]、數學型態學
（mathematical morphology）[29]等。然
而，大多數的路徑規劃都集中在獲得最
短路徑上。要獲得符合沿牆行走之行為
路徑，就必需去考量到機器人與障礙物
邊緣的距離[30]。 
 
三、研究方法 
 
立體影像的形成是利用人類的視覺
原理，人類的雙眼就像兩台攝影機，各
自拍攝形成影像，成像落於視網膜中。
在此計畫中，首先我們利用經過校正後
的雙眼鏡頭來進行人體的拍攝，取得立
體視訊影像後求取像差圖。接著針對像
差資訊求取二維 X-D 平面直方圖，透過
分析直方圖來分割出立體影像中的人體
物件的部份，系統流程如圖 一所示。另
外，在人臉辨系統的部份，使用
Bumblebee BB2 stereo vision camera來模
擬人類雙眼視覺，藉由 Bumblebee BB2 
stereo vision camera拍攝得到所需的人臉
影像以及相關資訊來達成人臉偵測與人
臉辨識的目的。而移動式機器人沿牆走
行為模式之控制器的設計上，利用簡單
的模糊以及基因演算法來設計出控制
器。另外，透過移動式機器人所搭載的
雷射測距儀與影像的形態學處理來規劃
出精確平滑的參考路徑，讓機器人能依
所規劃的路徑達成自我沿牆走之行為模
式。以下將詳細說明計畫中所使用的方
法。 
 
 
圖 一：方法流程圖 
 
 
（一）X-D二維像差直方圖 
像差影像可由經過校正的雙眼攝影
機，透過其介面軟體直接取得。由於人
類物件類似於一個橢圓柱體，主要在水
平面上活動，因此只要有水平方向上的
資訊，再加上物件與攝影機的距離便可
以大致決定物體的所在位置。在此我們
利用將像差影像的像差值結合水平方向
的資訊的方式來得到水平-深度(X-D)的
二維平面直方圖，亦即計算影像水平 X
軸與深度D軸出現的像素個數，求得X-D
二維平面直方圖資訊 。為了使資料的分
佈較為平均，我們在直方圖統計的部份
加入了模糊歸屬函數的概念，如圖 二所
示。圖 三則表示直接求得的直方圖結果
(a)與加入模糊歸屬的直方圖結果(b)，從
中我們可以發現加入模糊歸屬後的結果
可使直方圖變得更加平滑而易處理。 
 
 
圖 二：模糊歸屬函數 
 
其中不只存在一個人體物件，此時便用
分水嶺法對直方圖做分群，直到符合條
件。如此便能順利將不同物件區隔開來。 
 
 
 
圖 四：(a) dg 變化示意圖  (b) xg 變
化示意圖 
 
 
 
圖 五：經濾波後之 X-D直方圖 
 
 
(四) 分水嶺處理 
  
在將單一或兩個以上之人類物件成
功的區分成獨立的直方圖椎體後，接著
我們便去找尋各個椎體的邊界，由於 X
軸表示的為影像上的水平資訊，而 D 軸
表示的為像差的大小，即深度的資訊，
因此我們將得到的邊界資訊即人類物件
所在的水平範圍以及深度範圍還原至像
差影像中，只留下水平資訊中人類物件
存在的部份中人類物件所在的像差範
圍，便可得到只有人類物件存在的像差
影像。原始像差影像如圖 六 (b)，經過
處理便成為僅有人類物件存在的像差影
像如圖 七 (a)。對此像差影像做邊界的
搜尋，由此邊界資訊建方框將原始影像
中的人類物件圈選起來，圖 七為圈選後
的結果。 
 
 
 
 
 
圖 六：(a) 原始影像  (b) 像差影像 
 
在攝影機成像中所用的有三個座標
系統，分別為世界座標系統、攝影機座
標系統以及影像座標系統。所謂的世界
座標系統就是在真實世界中的座標，依
據物體的三維空間座標，分別以 wx ， wy
與 wz 來表示 xyz 三個座標軸的方向，另
外以 wo 來表示世界座標系統的原點。攝
影機座標系統就是真實座標系統，透過
CCD攝影機鏡片的成像，以 co 來表示攝
影機座標系統中的參考點，這個參考點
通常是以攝影機的中心為參考點，其他
cx ， cy 與 cz 來表示 xyz 三個座標軸的方
向。最後，影像座標系統就是真實座標
系統上的點，對應到攝影機成像的畫面
上，就稱為影像座標系統，並使用v與u
分別來表示 row和 column兩個方向。三
個座標系統組合來使得攝影機成像之關
係如圖 十所示。 
影像平面上的點為二維座標系統，
在平面上通常以座標平面的左上角的點
（0,0）作為參考點，  ,m v u 為攝影機座
標系統中的原點 co 與 cX 之連線穿過影
像平面 I 的交點， cX 與 wX 分別為攝影機
座標系統與世界座標系統中的觀察點，
光軸是通過攝影機座標系統原點與影像
平面 I 中心的軸線。 
在 SDK視覺系統上測定物體到攝影
機的距離是使用影像與幾何攝影機之間
的位移（displacement）來決定的。Triclops 
library提供了深度圖（depth maps）轉換
成距離影像的函式。在 Triclops library影
像與世界座標系統的關係圖如圖 十一
所示。 
 
 
 
圖 十：攝影機成像關係圖。 
 
 
圖 十一：影像座標系與世界座標系之關
係。 
 
對於 Stereo Vision SDK視覺系統內
包含了兩個處理區塊，主要可以分為影
像前置處理（image preprocessing）與立
體影像處理（stereo processing）兩個主
要的處理程序，在處理程序上的詳細流
程圖如圖 十二所示，人臉在拍攝後會產
生彩色影像、左邊鏡頭與右邊鏡頭影
像，藉由這三張影像去執行視覺系統內
的兩個處理程序來得到所需的影像資訊
與影像深度。 
 
 
圖 十二：Stereo Vision SDK視覺系統圖。 
 
當原始影像輸入後，先進行影像前
置處理流程，前置處理包含了低通濾波
（low-pass filtering）、校正（rectification）
以及邊緣偵測（edge detection）。低通濾
波主要是做影像的調整，使影像比較平
滑（smooth）。由於相機鏡頭會造成原始
影像的失真與扭曲，所謂的失真與扭曲
舉例來說，在原始影像上的直線，經拍
攝後，原本的直線會出現彎曲的情況，
尤其是在角落的直線更是明顯。校正的
主要原因是要避免影像上誤差的產生，
其他像是魚眼效應（radial lens distortion）
所產生的失真、在相同空間大小的物體
其像素長寬不平均效應（non-unity pixel 
aspect ratio）在座標換後空間大小的不相
同而產生像素數目不相同或是投影中心
偏移（offset optical center）所造成的對
上，舉例來說，在左右影像上有特徵點
A，左邊影像的 A點定義成 Aleft，右邊影
像的 A點定義成 Aright，那在 A特徵點的
視差值定義為 D(A)，式子如下： 
 
D(A) = Aleft - Aright 
 
利用上述的方式就可以對於左右兩張影
像來計算每個像素點上的視差值，然後
就可以得到影像上的視差圖（disparity 
map）。 
 
(六) 人臉與眼睛偵測 
 
在經過影像前處理以及立體影像處
理後，我們可以取得深度影像 (depth 
image)以及其對應的彩色影像與灰階影
像。在我們的影像擷取系統中，前述之
影像對應至立體雙眼攝影機之右方攝影
鏡頭。接著，我們可以利用深度影像，
計算出影像各像素點在攝影機座標系統
之三維座標。也就是，估算出各點相對
應至右方鏡頭之三度空間距離。 
首 先 ， 我 們 採 用 結 合 特 徵 臉
(Eigenface)與類神經網路之人臉偵測演
算法[31]，在灰階影像中偵測並定位出人
臉位置。此演算法結合了特徵臉、倒傳
遞網路(back-propagation network)以及人
臉驗證(verification)技術，可以在影像找
出正視與側面之人臉位置，並且估測出
人臉區域之半徑大小。如圖 十三所示，
紅色框代表演算法偵測出人臉所在之區
域。 
在偵測出人臉區域之後，我們可以
藉由人臉的幾何位置，透過搜尋人眼深
色瞳孔與虹膜所在之影像座標。接著我
們使用移動窗(moving window)，在灰階
影像中人臉區域的上半部找出區域最小
值，分別代表人類的左右眼的候選位
置。在[32]中提到，在人臉影像中眼睛算
是最暗的區域之一，眼睛位於虹膜的外
圍，由於虹膜與鞏膜在人臉影像中較為
明亮的區域，透過閥值運算（thresholding 
operation）後可以找到眼睛的所在位置。
人眼的黑眼球經常會反射出環境光源。
因此，找出的候選眼睛位置，此附近必
須包含有一光源反射的亮點，將人臉影
像經過梯度（gradient）的運算過後，可
以較為容易的找到眼睛上亮點的位置。
由圖 十四中可以清楚看到，經過梯度運
算前後的影像可以清楚的看到取梯度後
的影像上眼睛的亮點，利用這兩個亮點
來協助移動窗找尋眼睛的位置。如此，
可以避免將頭髮與眉毛等深色區域錯辨
為人類黑眼球的情況發生。圖 十五中的
藍點表示系統偵測出的人眼位置。 
 
 
 
 
 
圖 十三：人臉偵測結果。 
 
 
 
 
（a）梯度運算前之人臉區域影像。 
 
（b）梯度運算後之人臉區域影像。 
圖 十四：人臉區域影像梯度運算結果。 
 
Yaw: 16.7258, Pitch: -3.3553, Roll: -6.5564
100 200 300 400 500 600 700 800
100
200
300
400
500
600
 
圖 十六：嘴角偵測結果。 
 
 
(八) 膚色分割與人臉正規化 
 
為了更精準找出影像中屬於人臉之
區域，我們採用膚色分割法，將原先偵
測出的人臉區域及鄰近區域中，屬於膚
色部分影像獨立分割出來，並且去除其
他非膚色區域。 
首先，我們將 RGB彩色影像轉換至
L*a*b*之色彩空間。接著，分別對 L*與
a*做二值化，並將此兩張二值影像各個
對應的像素相乘。相乘後的影像稱為人
臉膚色區域遮罩，其中人臉膚色區域為
1，其他區域則為 0值。因此，我們可以
此遮罩分割出人臉影像。 
找出人臉膚色區域後，我們以鼻尖
投影至人臉平面的投影點作為參考，估
算此投影點至攝影機中心至距離，並將
此距離正規化至固定大小。換言之，我
們將人臉正規化至與攝影機相同距離，
以達成尺度不變性。最後，我們對 Roll
角正規化，也就是將人臉影像以鼻尖至
人臉平面的投影點為中心，將 Roll 角旋
轉至 0 度。並且依鼻尖投影點為中心，
切出固定大小之影像，作為後續特徵萃
取的輸入影像。膚色分割並經正規化後
的人臉影像如圖 十七所示。 
 
20 40 60 80 100 120 140
20
40
60
80
00
20
 
圖 十七：人臉膚色分割與正規化之結
果。 
 
(九) 人臉特徵萃取 
 
在分割並正規化人臉影像之後，我
們必須在人臉影像中，萃取出足以代表
不同人臉獨特性的特徵向量。在本計劃
中，我們採用 Zhang 等人所提出的
Gradient-faces[36]的方法。此演算法對於
光度的變化較不敏感，可以在不同的亮
度大小、方向等影響下，仍達到極高的
正確辨識率。 
為了在梯度域(gradient domain)萃取
出對照度不敏感之特徵，Zhang等人推導
出以下的定理：給定任意一張在照明環
境下取得的影像  ,I x y ，此影像的 y方向
梯 度   ,I x y x  與 x 方 向 梯 度
  ,I x y y  的比例，是一種照度不敏感
的測量方法。由於在實務上，y方向梯度
與 x 方向梯度的比例可能是無限大(在 x
方向梯度為零時)。因此，以下的定義了
Gradientfaces (G)的估算法： 
 
 arctan ,    0,2y gradient
x gradient
I
G G
I


     
 
 
其中， x gradientI  與 y gradientI  分別代表 x方向
與 y方向之梯度。 
在實作中，我們必須先對人臉影像 x
與 y 方向做梯度運算。接著，可以利用
上式計算出梯度臉(Gradientfaces)。為了
更穩定的計算影像梯度，我們可以先利
用以高斯核方程式的平滑濾波器，對輸
入影像進行平滑化。平滑化的好處可以
使梯度臉更能抵抗影像雜訊，同時也能
(十一) 機器人沿牆走行為模式之控制系
統 
 
 我們所提出的基因最佳化模糊控制
系統如圖 十九所示，其中，本系統利用
雷射測距儀所測得之環境資料來規劃出
一條參考路徑。並且利用模糊控制器來
控制機器人的行走路徑能更為接近參考
路徑。此外，利用基因演算法能自我演
化的能力來針對模糊控制參數進行訓
練，最後能獲得一組最佳化後的控制參
數來提升系統的效能。 
 
 
 
圖 十九：機器人模糊控制系統流程圖 
 
 
(十二)區域參考路徑制定 
 
 首先，利用雷射測距儀，以由左至
右 180 度的方式偵測掃描後就能獲得一
組 180 度的極座標距離序列資料，再經
過座標轉換，就能預先知道前方環境資
訊，並且預先規劃出區域參考路徑（local 
desired path）。座標轉公式如下： 
 
( , )LaserD r   
 
r為機器人與障礙物之距離；為機器人
雷射掃描角度。如圖 二十所示藍色點為
牆面掃描取樣資料。透過將目前的環境
範圍進行偵測取樣，並將雷射偵測資料
量化至相近的格點上，就可以將感測範
圍視為一張二值化影像，雷射資料點在
二值化影像上表示為 1。 
 接著，再針對二值化影像進行膨脹
（dilation）運算，我們使用圓形結構元
素對 180 個取樣點進行膨脹運算。經過
運算後，牆的邊緣點就會產生出一條厚
厚的帶狀參考路徑，如圖 二十一所示。 
 由於經過膨脹運算後的參考路徑
上，在轉角處的部份不夠圓滑。因此，
再將膨脹後的參考路徑進行閉合
（closing）運算。如圖 二十二，紅色虛
線為閉合運算後的參考路徑。 
 最後，參考路徑是選定機器人與牆
面最接近的邊界。如圖 二十三所示，紅
色虛線為最後的參考路徑。 
 在制定參考路徑中所使用的形態學
運算定義如下： 
a) 膨脹：通常是用來使影像中的物體變
大或變厚的運算。厚化的程度主要是由
結構元素來決定的。在此方法中，我們
使用圓形的結構元素，圓形結構的半徑
我們設定為機器人與牆面/障礙物之間的
距離。在數學上，我們將膨脹定義如下： 
 
A B  
 
，其中A為物體影像，B為結構元素。 
b) 閉合：是指先經過膨脹後再進行侵蝕
的混合式運算。經過閉合運算後，會使
物體的輸廓更平滑，也能將小的缺口填
補、連接起來。在數學上，我們將閉合
定義如下： 
 
A‧B=(A㊉B)㊀B 
 
 當參考路徑經過膨脹後，參考路徑
上可能會含有類似鋭角的轉彎，鋭角的
轉彎路徑對於定速的機器人而言，在速
度上是無法應付。為了獲得更可行的路
徑，對於膨脹後的參考路徑再進行使用
圓形結構的閉合運算，使得參考路徑更
加平滑，讓機器人能順利轉向。其中，
結構半徑設定為大於在定速下機器人轉
彎所需半徑。 
 
 
 
圖 二十：雷射掃描取樣示意圖 
Medium)、小負值(NS: Negative Small)、
零值 (ZR: Zero)、小正值 (PS: Positive 
Small)、中正值(PM: Positive Medium)、
大正值(PB: Positive Big)。相對應的歸屬
函數如圖 二十八所示。我們根據人工實
驗與5 5 或 7 7 的與D歸屬函數，分
別制定出5 5 與 7 7 的初始模糊控制規
則表，如表 二與表 三所示。制定初始
模糊控制規則表後，我們使用基因演算
法，將規則表當成染色體，透過演化的
方式去改善並產生新的模糊控制規則
表。在控制規則表的優化過程中，5 5 與
7 7 的每一個規則在染色體上都被視為
一個獨立的參數，所以模糊控制規則表
可以用表 四與表 五來表示。 
 
 
 
圖 二十四：5 5 輸入參數歸屬函數圖 
 
 
 
圖 二十五：5 5 輸入參數D歸屬函數圖 
 
 
 
表 二：5 5 初始模糊控制規則表 
 
 
圖 二十六：7 7 輸入參數歸屬函數圖 
 
 
圖 二十七：7 7 輸入參數D歸屬函數圖 
 
 
 
表 三：7 7 初始模糊控制規則表。 
 
 
 
圖 二十八：輸出值之相對應歸屬函數
圖。 
 
 
表 四：5 5 模糊控制規則表。 
 
 
圖 三十一：機器人右轉修正行走示意
圖。 
 
 
(十四)基因演算法 
 
利用模糊控制器來做為機器人的行
為控制器，效果雖然相當優秀，但是機
器人的行為模式是根據使用者的經驗法
則來制定微調，最後才選定一組最適合
的參數。但是這種經驗法則的制定方
式，設計者需要花費較多的時間來調整
參數，缺少了效率與彈性。因此，我們
將透過一個適應性學習的方法，讓機器
人自我學習，自我演化出一組最適合環
境的行為模式參數。 
基因演算法（GA：Genetic Algo-
rithm）的特性就是能透過自我演化的方
式去產生一組最佳的基因。在我們的系
統中，使用基因演算法去獲得一組最適
合環境的參數。 
我們把系統要調整的參數視為生物
中的基因，之後透過選擇一組較佳的母
代基因以及隨機的方式進行交配，進而
產生優秀的子代基因，經過不斷的重複
上述的步驟，最後得到一組適應性最強
的參數。 
如圖 三十二所示，為整個基因演算
法的流程圖，依照這個流程圖，系統就
能讓機器人自我學習，演化出一組最符
合環境的參數。 
基因演算法的執行步驟如下： 
a) 產生初始族群：首先，我們隨機
產生出 40 個染色體（chromosome），這
40 個染色體是根據先前所產生表 四與
表 五的模糊控制規則表中的參數 ijm 做
為初始染色體，並針對初始染色體的參
數隨機加減一個 2 2  之間的常數
值，依序產生 39個染色體，做為初始族
群（Initial Population）。因此，5 5 與7 7
的規則表的每個染色體分別包含25與77
個實數參數。如圖 三十三所示。 
 
 
 
圖 三十二： 基因演算法的流程圖。 
 
 
 
 
圖 三十三：基因演算法產生初始族群示
意圖。 
 
 
圖 三十六：實驗結果-兩人並排行走 
 
 
(二) 人體物件偵測：一對一行走 
 
第二組實驗之結果，環境為本校電
機大樓之大廳，一人類物件站立不動，
另一人類物件在其周圍行走，一開始距
離攝影機約 2 公尺。其中某些影像由於
其一人類物件大部份皆位於另一人類物
件的後方，導致水平像差分佈上的遮
蔽，因此無法準確判斷是否為人類物
件。在此我們的設定誤差為 50%，即未
達 0.5 倍平均人體寬度皆判定不為單一
人類物件。結果如圖 三十七所示。 
 
 
圖 三十七：實驗結果-一對一行走 
 
(三) 人體物件偵測：三人於不同距離 
 
第三組實驗之結果，環境為本校電
機大樓之大廳，三個人類物件各在不同
距離開始向攝影機靠近，由於人類物件
較靠近的關係，導致其像差在直方圖的
分佈上出現重疊的情形，除了原本設定
好之分水嶺法，再加入人體於不同深度
的平均寬度資訊條件，以便將各個人類
物件做一區分。一開始距離攝影機約 3.5
公尺，行走至約離 1.5公尺。結果如圖 三
十八所示。 
 
 
圖 三十八：實驗結果-三人於不同距離 
 
(四) 人臉資料庫 
 
本計畫中，我們使用 Bumblebee BB2 
stereo vision camera 來擷取人臉立體影
像。每位受測者分別在不同時期進行兩
次的拍攝取樣工作。在第一個時期，受
測者分別將頭旋轉五個角度(yaw角)，取
得五張影像(如圖 三十九)。在第二個時
期，我們以連續拍攝的方法，請受測者
緩慢將臉從左側轉向右側，最後再轉回
左側，利用這樣子的方式來達成多角度
連續影像拍攝的結果。因此，第二時期
的取樣，每位受測者因轉速的不同，會
取得不同數量之人臉影像。 
在徵求多位志願者拍攝後，我們的
資料庫由原本的 43 組人臉類別新增至
 
（a） 轉角 1。 
 
（b）轉角 2。 
 
（c）轉角 3。 
 
（c）轉角 4。 
 
（d）轉角 5。 
圖 四十二：五種典型轉角訓練路徑圖。 
 
圖 四十三：機器人模擬環境圖。 
 
(七) 機器人模擬結果 
 
在模擬實驗中，我們使用MobileSim
模擬軟體來模擬 Pioneer3-DX 機器人，
Pioneer3-DX 機器人如圖 四十四所示。
機器人上搭載 SICK-2000 雷射測距儀，
其有效距離與範圍分別為 32公尺與 180
度。模擬地圖大小為1700cm 1700cm ；
另外，設定機器人為定速行駛，速度為
150 (mm/sec)。 
 
 
 
圖 四十四：Pioneer3-DX機器人。 
 
 
在未知的環境中，我們使用模糊控
制器來控制機器人的行進角度以利避障
以及沿牆行走。機器人必須要能針對直
角、鈍角與鋭角等三種角度的轉彎來進
行轉彎，並且能行走於曲線與直線的路
徑。對於這些不同情況的路徑與環境，
我們希望基因演算法能夠選出一組好的
模糊控制參數使機器人能在不同的未知
環境下順利的行走。為了進行比較，我
們分別使用5 5 與 7 7 的模糊規則表來
進行實驗。如圖 四十五所示，在控制參
件存在的部份確實有人型的分割區域產
生。經過邊界的搜尋動作，將其結果還
原至原始影像，其人體部份皆被圈選出
來，如此已達成我們所需要的成果。 
在 第 二 年 計 畫 中 ， 我 們 使 用
Bumblebee BB2 stereo vision camera來擷
取人臉立體影像。每位受測者分別在不
同時期進行兩次的拍攝取樣工作。在第
一個時期，受測者分別將頭旋轉五個角
度(yaw角)，取得五張影像(如圖 四十)。
在第二個時期，我們以連續拍攝的方
法，請受測者緩慢將臉從左側轉向右
側，最後再轉回左側。因此，第二時期
的取樣，每位受測者因轉速的不同，會
取得不同數量之人臉影像。在徵求多位
志願者後，我們的資料庫擁有 101 位不
同的測試者，也就是有 101組人臉類別。
在人臉辨識方面，我們設定梯度臉影像
相異度測量的 penalty 值為 8 、影像距
離正規化為一公尺、每張梯度臉影像大
小為  80 80 像素。實驗結果顯示，以單
張影像測試，本系統正確辨識率為
97.9%。 
計畫中我們根據人臉立體影像資訊
來選擇最適當的比對影像，可以大幅提
高辨識正確率；並且在測試階段時，可
隨意錄取一序列影像進行比對，因此可
以不需要受測者固定特定姿態，使用上
較為方便，以一序列影像測試，本系統
正確辨識率為 100%。如此已達成我們所
需要的成果。 
在第三年計畫中，我們假設機器人
在接受指令後，需移動至目標區執行任
務，因此我們透過移動式機器人搭載的
雷射測距儀（laser range finder）與影像
處理之形態學結合的方式，能即時的規
劃出一條參考路徑，藉由模糊控制器的
設計並且搭配基因演算法能自我演化的
特性，產生出更貼近於環境的模糊控制
參數，使移動式機器人在一個未知的環
境中，於設定的安全距離內達到沿牆行
走之行為（wall-following behavior）。 
 此計畫共衍生出三篇會議論文
[37][38][39]，兩篇期刊論文整理投稿中。 
 
 
 
圖 四十七：global與 local參考路徑示意
圖。 
 
g 5×5 rule table 7×7 rule table 
15
 
 
圖 四十八：5 5 與7 7 之基因演算法模
擬驗證。 
 
 
表 六：基因演算法模擬平均誤差表。 
 
圖 四十九：5 5 基因演算法 0代之模擬
誤差圖。 
 
rence on Pattern Recognition, vol. 3, pp. 
627-630, 2002. 
[10] F. Deschen, D. Ziou, ”Homotopy-based 
computation of defocus blur and affine 
transform,” in Proceedings of the 2003 
IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition 
(CVPR’03), vol. 1, pp. I-398-404, 2003. 
[11] L. Li, Y. T. Koh, S. S. Ge, and W. 
Huang, “Stereo-based human detection 
for mobile service robots,” Proc. IEEE 
Conferecne on Control, Automation, 
Robotics, and Computer Vision, pp. 
74-79, Dec. 2004. 
[12] L. Li, S. S. Ge, T. Sim, Y. T. Koh, and 
X. Hunag, “Object-oriented 
scale-adaptive filtering for human de-
tection from stereo images,” Proc. 
IEEE Conference on Cybernetics and 
Intelligent Systems, pp. 135-140, Dec. 
2004. 
[13] Y. Suzuki and T. Shibata, “Multiple-clue 
face detection algorithm using edge-based 
feature vectors,” Proceedings of IEEE In-
ternational Conference on Acoustics, 
Speech, and Signal Processing, vol. 5, pp. 
V-737-740, 2004. 
[14] M. Yagi and T. Shibata, “An image re-
presentation algorithm compatible with 
neural-associative-processor-based hard-
ware recognition systems,” IEEE Trans-
action on Neural Networks, vol. 14, no. 5, 
pp. 1141-1161, 2003. 
[15] H. A. Rowley, S. Baluja, and T. Kanade, 
“Neural network-based face detection,” 
1996 IEEE Computer Society Conference 
on Computer Vision and Pattern Recogni-
tion, pp. 203-208, 1996. 
[16] S. Karungaru, M. Funumi, and N. Aka-
matsu, “Human face detection in visual 
scenes using neural networks,” Transac-
tion of Institute of Electrical Engineers of 
Japan （IEEJ）, vol. 122c, pp. 995-1000, 
2002. 
[17] G. Yang and T.-S. Hung, “Human face 
detection in complex background,” Pat-
tern Recognition, vol. 27, no. 1, pp. 53-63, 
1994. 
[18] A. Lanitis, C. J. Taylor, and T. F. Cootes, 
“An automatic face identification system 
using flexible appearance models,” Image 
and Vision Computation, vol. 15, no. 5, pp. 
393-401, 1995. 
[19] C. Lin and K.-C. Fan, “Human face detec-
tion using geometric triangle relation-
ship,” Proceedings of the 15th Interna-
tional Conference on Pattern Recognition, 
vol. 2, pp. 941-944, 2000. 
[20] J. Yang, W. Lu, and A. Waibel, 
“Skin-color modeling and adaptation,” 
Proceedings of ACCV'98, Vol. II, pp. 
687-694, 1998. 
[21] K.-W. Wong, K.-M. Lam, and W.-C. Siu, 
“An efficient color compensation scheme 
for skin color segmentation,” Proceedings 
of IEEE International Symposium on Cir-
cuits and Systems （ISCAS2003）, vol. II, 
pp. 676-679, 2003. 
[22] R. L. Hsu, A. M. Mohamed, and A. K. 
Jain, “Face detection in color images,” 
IEEE Transaction on Pattern Analysis and 
Machine Intelligence, vol. 24, no. 5, pp. 
696-706, 2002. 
[23] W. Zhao, R. Chellappa, P. J. Phillips, and 
A. Rosenfeld, “Face Recognition: A Lite-
rature Survey,” ACM Computing Survey, 
pp. 399-458, 2003. 
[24] X. C. He and N. H. C. Yung, “Corner de-
tector based on global and local curvature 
properties,” Optical Engineering, vol. 47, 
no. 5, pp. 057008, May 2008. 
[25] A. Stentz, "Optimal and efficient path 
planning for partially-known 
environments," in Proc. 1994 IEEE Int. 
Conf. Robotics and Automation, 1994, pp. 
3310-3317 vol.4. 
[26] Y. K. Hwang and N. Ahuja, "A potential 
field approach to path planning," IEEE 
Trans. Robot. Autom., vol. 8, pp. 23-32, 
1992. 
[27] J. Borenstein and Y. Koren, "The vector 
field histogram-fast obstacle avoidance for 
mobile robots," IEEE Trans. Robot. 
Autom., vol. 7, pp. 278-288, 1991. 
[28] S. M. Lavalle, "Motion planning," 
Robotics & Automation Magazine, IEEE, 
vol. 18, pp. 79-89, 2011. 
[29] S.-C. Pei, C.-L. Lai, and F. Y. Shih, "A 
morphological approach to shortest path 
planning for rotating objects," Pattern 
Recognition, vol. 31, pp. 1127-1138, 1998. 
[30] H. Y. Lin, J. S. Taur, W. Z. Chen, and C. W. 
Tao, "Path-planning using the behavior 
cost and the path length with a 
multi-resolution scheme," in 2010 IEEE 
Int. Conf. Systems Man and Cybernetics 
(SMC), 2010, pp. 35-42. 
[31] C. C. Tsai, W. C. Cheng, J. S. Taur, and C. 
W. Tao, “Face detection using eigenface 
and neural network,” IEEE Conf. on Sys-
tems, Man, and Cybernetics, pp. 921-926, 
Oct. 12-15, 2008, Singapore. 
[32] H. Badakhshannoory, M. Safayani, M. T. 
Manzurei-Shalmani, “Using genmentry 
modeling to find pose invariant features in 
face recognition,” IEEE Conf. on Intelli-
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/27
國科會補助計畫
計畫名稱: 子計畫四：智慧型機器人之視覺與聽覺系統設計
計畫主持人: 陶金旭
計畫編號: 97-2221-E-005-072-MY3 學門領域: 智慧型機器人
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
兩篇期刊論文整理投稿中 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
