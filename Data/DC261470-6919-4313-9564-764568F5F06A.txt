 2 
Content-aware multimedia video retrieval system. 
Each of the three research topics has rich research 
outcome that will be illustrated briefly in the following 
report. 
 
I. Research in Multi-Standard Video 
Decoder IP Design 
 
ABSTRACT 
This paper proposes a multi-standard 
(JPEG/MPEG-1/2/4/H.264) video decoder that comprises 
252kgates and 4.9kB SRAM in a core size of 4.2x1.2mm2 
using 0.13um 1P8M CMOS technology. The power 
consumption is 71.1mW@120MHz/1V and 
7.9mW@20MHz/0.8V for real-time HD1080 and D1 video 
decoding, respectively. 
 
1. Introduction and Motivation 
Modern video coding standards like MPEG-4 and H.264 
AVC provide both high video coding quality and high data 
compression rate at the cost of high algorithmic complexity. 
For achieving real-time processing, dedicated VLSI 
implementation is always adopted, which usually results in 
very complex electronic circuits. However, the hardware 
cost and power consumption must be reduced as most as 
possible especially for hand-held applications.  
From the view point of video coding standards, a video 
coding system can be classified into some critical 
components like inter-prediction, intra-prediction, 
de-blocking filtering, and entropy coding, which occupy 
most of the computation complexity and attract much 
researching interest in the past for supporting high video 
resolution up to high definition (HD) video with low 
hardware cost and low power consumption. However, these 
researches try to solve the encountered problems from the 
component point of view instead of systematic point of view, 
which might cause that the advantages of the proposed 
solutions for the critical video coding components are not 
feasible when integrating into a video coding system.  
Hence, it is necessary to consider the design exploration 
from systematic point of view before making the best design 
trade-off to optimize a video coding system for high 
performance, low hardware cost and low power 
consumption. 
In addition, the compressed multimedia data nowadays are 
transmitted through heterogeneous networks. This causes a 
new issue that users may face video data compressed in 
different video coding standards. To decode those data, a 
versatile decoder that supports multiple-standard video 
decoding is necessary. This goal can be easily achieved in a 
programmable system such as the Personal Computer (PC) 
that has abundant resource by installing several video coding 
programs. However, the hand-held video devices have 
limited hardware and software resources, which makes the 
above solution becomes impractical. In addition, the low 
power design consideration is of great importance for 
hand-held devices in portable video coding applications. 
Therefore, a dedicated video decoding engine with 
configurable parameters to support different video coding 
standards could be a good solution for achieving the design 
goals of high performance, low hardware cost, and low 
power consumption, which is also the topic of this paper.    
On realizing the popular video decoders like JPEG, 
MPEG, and H.264, there have been some video decoders 
[1-5] proposed in the past years, which shows that dedicated 
hardware is a good solution to low-power implementation 
for high definition (HD) video applications. One solution to 
low power video decoders is to reduce the clock rate of 
external memory access for reducing the major power 
consumption, especially for HD video decoding. The design 
[4] used single external memory with clock rate up to 
200MHz, which suffers from high power consumption. The 
designs [1-3, 5] used dual external memories to reduce the 
clock rate of memory access (100MHz~170MHz), which 
suffers from high IC package cost.  
 4 
conventional prefix decoding for maintaining the required 
throughput of 1sym/cycle. Since the LUT is partitioned into 
multiple smaller ones, the unused LUTs are disabled to 
further reduce the power consumption. Figure 2 shows the 
proposed PNM method used in HVLD.  
Figure 2: Structure of the proposed PNM method in HVLD. 
2.1.2   Shared Adder-based Filter Structure 
 
Figure 3: Proposed shared adder-based filter structure. 
 
By observing a fact that most of the operations used in the 
texture coding, intra prediction, interpolation in inter 
prediction, and in-loop filtering can be viewed as filtering 
operations with fixed coefficients, we propose a shared 
adder-based filter structure to realize all the data-paths of 
4x4/8x8 IDCT in HTD, intra prediction and inter prediction 
in HPC, and 1-D filter in ILF using only additions and shift 
operations. We first generate the common terms in realizing 
these operations and then use parallel adder trees to compute 
the output values for the filters. In this way, we can share the 
common intermediate results to reduce hardware complexity. 
Using the shared adder-based filter structure contributes 
49%, 56%, 64%, and 58% of complexity reduction in 
4x4/8x8 IDCT, intra prediction, inter prediction, and 1-D 
ILF filter, respectively. Figure 3 shows the shared 
adder-based filter structure we use to reduce the hardware 
cost of the proposed design.  
2.1.3   Reusable Data Manager 
In the video decoding process, some decoded data and 
information need to be temporarily stored for the later 
decoding. For example, pixels in the left and upper adjacent 
MBs are used for the filtering in intra prediction and in-loop 
filter. Some existing designs [1, 4] stored these data in the 
internal memory. However, the storage size required for 
storing these reusable data extensively increases when 
decoding the high resolution videos. In addition, the channel 
information such as frame sizes, quantization tables, and 
other global parameters also need to be kept when switching 
to different channels for multi-channel applications. This 
implies that the increase of cost and power consumption 
cannot be prevented if we store all of these data in on-chip 
SRAM. To reduce internal memory, the intermediate data for 
future decoding are buffered in external memory through a 
DMA-like Reusable Data Manager (RDM) if these data are 
not used immediately. 
  
2.2   Low Memory Bandwidth Design Techniques  
The bandwidth of frame memory access dominates the 
performance of a video decoder. Minimizing memory 
bandwidth is of great significance in reducing power 
consumption since most of the power in pixel compensation 
is consumed by memory access. We propose two techniques, 
i.e. hybrid block access with cache and dual block access 
with same MV as shown in Figure 4, to minimize memory 
bandwidth for doing the pixel compensation that is the most 
bandwidth consuming operation in video decoding.  
 6 
organization and memory addressing order used in the 
proposed design. We store the frame data in raster-scan 
order to facilitate the video display. The order of memory 
addresses is defined according to the consecutive row with 
interlaced banks. Through this kind of data arrangement, the 
next non-continuous data are located at either the row of 
present accessing or the row belongs to a different memory 
bank.  
 
Figure 5: Address bus organization and memory addressing. 
 
With the addresses of impending burst access operation, 
two techniques are proposed for achieving low memory 
access latency as shown in Figure 6. One is Burst Terminates 
Burst (BTB) for eliminating the overhead cycles between 
two consecutive burst access operations by truncating the 
current burst with a subsequent bust. The other is 
Anticipative Row Activation (ARA) to activate the row of 
memory bank for the impending access operation in advance, 
which eliminates the overhead cycles of row activation in 
SDR memory. Combining these two techniques together 
contributes about 41% and 38% reduction of memory 
bandwidth for MPEG-4 and H.264 video decoding, 
respectively.  
 
Figure 6: Proposed low memory latency access techniques. 
 
3. Design Implementation  
In a project development, the effective communication 
between team members is a very important issue to improve 
the quality and efficiency of the project outcome. In this 
project we adopt the Concurrent Versions System (CVS) 
tool for file version control. By using CVS tool, we make the 
debugging progress smoother and easier. In this project, the 
proposed design can be delivered as IP format with the 
design documents including design specification, user guide 
and coding guidelines. The detail implementation of the 
proposed design is described in the following sub-sections.  
Figure 7: Design flow of the proposed multi-standard video 
decoder. 
 
 8 
 
Figure 8: Chip prototyping of the proposed design. 
 
4. Performance Comparison  
Figure 9 shows the die photo and implementation results 
of the proposed multi-standard video decoder chip. The core 
size of the chip is 4.2x1.2mm2 using the TSMC 0.13um 
1P8M CMOS technology. The hardware cost of the chip 
consists of 252K gates and 4.9K bytes of SRAM. The power 
consumption is 71.1mW@120MHz/1V, 7.9mW@20MHz/ 
0.8V, 2.7mW@6MHz/0.7V for real-time HD1080, D1, and 
CIF video decoding, respectively. Table I shows the 
comparison of the proposed design with the state-of-the-art 
multi-standard and H.264 video decoders for both HD video 
[1, 2, 4, 5] and portable video [3]. From the comparison 
results, we conclude that the proposed design outperforms 
the multi-standard video decoder [5] in terms of 72% in 
gate-count and 87% in power consumption. Compared to the 
H.264 video decoders [2, 4], the proposed design owns 34% 
reduction in power consumption over the design [2], and 
16% reduction in gate-count as well as 56% reduction in 
power consumption over the design [4]. Compared to the 
MPEG-2/H.264 video decoder [3] for mobile applications, 
the proposed design owns 17% reduction in gate-count. 
Moreover, it dissipates 7.9mW of power consumption that is 
lower than the 12.4mW reported in the design [3] for 
real-time H.264 decoding on D1 video, which shows its high 
feasibility for portable video in addition to HD video 
applications.  
 
5. Conclusion  
In this paper, a low cost, low power multi-standard video 
decoder supporting JPEG, MPEG-1/2/4, and H.264 for HD 
video applications is proposed. The proposed design is 
optimized through reducing memory bandwidth by 
increasing both data reuse amount and burst length of 
memory access. In addition, a low latency memory control 
scheme is used to eliminate cycle overhead in data access for 
supporting HD video decoding with single AHB-based SDR 
memory at 120MHz. By exploiting the low cost design 
techniques and hardware sharing between different standards, 
the proposed multi-standard video decoder implemented 
through a 0.13um CMOS technology consumes 71.1mW 
with the cost of 252K gates and 4.9KB internal memory for 
HD1080 video decoding. The comparison results listed in 
Table I show that the proposed design is more efficient than 
the existing video decoders targeted at HD1080 video 
coding applications.  
 
6. Acknowledgements  
The authors thank the National Science Council, the 
Ministry of Economic Affairs, and the National SoC 
Program of Taiwan for funding the research. We also thank 
Chip Implementation Center for supporting chip fabrication. 
 
7. References  
[1] T. W. Chen, Y. W. Huang, T. C. Chen, Y. H. Chen, C. Y. Tsai, 
and L. G. Chen, “Architecture Design of H.264/AVC Decoder 
with Hybrid Task Pipelining for High Definition Videos,” Proc. 
ISCAS, vol. 3, pp. 2931-2934, May, 2005. 
[2] C. C Lin, J. I. Guo, H. C. Chang, Y. C. Yang, J. W. Chen, M. C. 
Tsai, and J. S. Wang, “A 160kGate 4.5kB SRAM H.264 Video 
Decoder for HDTV Applications,” Proc. ISSCC, pp. 406-407, 
2006. 
[3] T. M. Liu, T. A. Lin, S. Z. Wang, W. P. Lee, K. C. Hou, J. Y. 
Yang, and C. Y. Lee, “A 125μW, Fully Scalable MPEG-2 and 
H.264/AVC Video Decoder for Mobile Applications,” Proc. 
ISSCC, pp. 402-403, 2006. 
 10 
II. A Quality-Scalable H.264/AVC Baseline 
Intra Encoder for High Definition Application 
 
ABSTRACT 
In this paper, we propose a quality scalable H.264/AVC 
baseline intra encoder with two hardware sharing 
mechanisms and three timing optimizing schemes. The 
proposed hardware sharing schemes share the common terms 
among intra prediction of different modes to reduce the 
hardware cost. The proposed timing optimizing schemes are 
used to improve the data throughput rate. The proposed 
design supports different clock rates of 26/33/47 MHz and 
70/85 MHz to encode SD and HD720 video sequences with 
30fps respectively with different qualities. According to a 
0.13 mm CMOS technology, the proposed design costs 170K 
gates and 4.43 KB of internal SRAM. 
 
1. INTRODUCTION AND MOTIVATION  
MPEG-4 AVC/H.264 gets more and more interests in 
realizing multimedia systems aimed at high-throughput 
design for High Definition (HD) video and low-power design 
for portable devices [1-2]. The MPEG-4/H.264 intra coding 
supports three intra macroblock modes, including intra4x4 
mode, intra16x16 mode, and chrominance mode denoted as 
I4MB, I16MB, and Chroma, respectively. When the best 
intra macroblock is decided, it takes much time to perform 
texture coding to obtain the reconstructed pixels. The 
phenomenon makes the hardware idle and leads to low 
hardware efficiency. In addition, the computation of bilinear 
approximation in plane mode complicates the hardware 
realization for high resolution video applications. 
In the literature, there are some H.264 intra encoder 
designs proposed for low complexity intra coding [3-4, 8]. 
For reducing the complexity of computation, the design [3] 
eliminates the I16MB/Chroma plane mode from intra 
prediction and adopts DCT for mode decision. It causes 
apparent quality drop. Besides, there exists much idle time in 
the encoding processing of designs [3-4] when doing texture 
coding. For low operating frequency, the design [8] 
optimizes the processing cycles with variable pixel 
scheduling and modified three-step algorithm. 
In the paper, we propose some optimizing schemes to solve 
the above mentioned problems. In addition, we propose a 
design feasibility called quality scalable intra coding to 
dynamically adjust the video encoding quality in terms of 
configured parameters in the proposed design by trading off 
different power consumption for different applications. The 
proposed design supports different clock rates of 26/33/47 
MHz and 70/85 MHz to respectively encode SD and HD720 
video sequences with 30fps. According to a 0.13um CMOS 
technology, the proposed design costs 170K gates and 4.43 
KB internal SRAM. As compared to the existing designs 
[3-4], the proposed design owns 42% and 52% reduction in 
terms of the cycle count per MB encoding, respectively. As 
compared to the design [8] that is optimized for low clock 
frequency, the proposed design provides the flexible quality 
scalability for different video coding applications. 
  
2. PROPOSED INTRA CODING 
ALGORITHM 
For both reducing the complexity of the H.264 intra coding 
and providing the scalable quality of the intra coding from 
algorithmic point of view, we develop different fast 
algorithms for different configurations in the proposed design. 
Fig. 1 shows the proposed low complexity search algorithm 
in luminance intra coding. For reducing the complexity, we 
propose two search techniques for luminance intra mode 
decision, i.e. Context Correlation Search Algorithm (CC-SA) 
and Probability Context Correlation Search Algorithm 
(PCC-SA) [5-6]. By taking advantages of spatial correlation 
of texture between current block and neighboring blocks, 
CC-SA searches less prediction modes (3~7 modes) 
according to intra prediction mode in the neighboring blocks 
(Upper block and Left block), as indicated in the right part of 
Fig. 1. In addition, PCC-SA exploits the statistics of intra 
coding modes in real sequences to only search high 
probability modes for further reducing complexity. For intra 
 12 
 
Fig. 3: The scalable architecture for Intra Coding module in Fig. 2 
 
3.1   Shared Item Mechanism of Intra Pixels Generator 
(SIMIPG) 
Fig. 4 shows the propose IPG design. The proposed IPG 
can generate all intra predictors for I4MB, I16MB and 
Chroma. Observing the formula of I4MB prediction, we find 
that we can share some intermediate results in computing the 
predictors to reduce hardware cost. Fig. 5 shows the example 
of I4MB diagonal down-right mode. Predictors of a, b, c and 
d are computed by “I+2X+A”, “X+2A+B”, “A+2B+C”, and 
“B+2C+D”, respectively. We find that there are the same 
terms of “X+A”, ”A+B”, and ”B+C” between predictors of 
“a and b”, “b and c” and “c and d”, respectively. Based on 
this feature, we only compute the same terms one time and 
share them in the computing the other predictors, as shown in 
Fig. 6. With this mechanism, we can eliminate three adders 
compared to the original design. 
 
Fig. 4: Proposed Intra Pixel Generator (IPG) design  
 
Fig. 5: Example of I4MB diagonal down-right mode 
 
 
Fig. 6: Architecture in computing the shared terms in IPG 
 
3.2   Plane Mode Shared Mechanism (PMSM)  
Because the I16MB plane mode needs complex 
computation in intra prediction, the design [3] does not 
support intra16x16 plane mode for reduce the hardware cost. 
As for the design [4], it supports the plane mode operation 
with additional hardware. By analyzing the formula of 
I16MB plane mode shown in Eq. (1), we find that we can 
compute the values of H, V, a, b and c with the same 
architecture of IPG and without additional hardware. For 
example, we divide the equation in computing H value into 
four groups as shown in Eq. (2) and assign them to four 
data-paths as shown in Fig. 8. In Fig. 8, we realize the 
multiplications with shifting and addition/subtraction 
operations. When accumulation is finished, it needs one cycle 
to sum the four results (R1, R2, R3, and R4) of data paths 
together. So, it totally takes 10 cycles to compute each H 
value in the proposed data-path. The computation of V value 
is the same as that for H value. The values of a, b, and c 
could be also calculated by using the shifters and adders in 
IPG.  
 
 14 
for luminance blocks or for the I16MB is finished, we 
pre-load the current luminance data of next MB. In the case 
of encoding the first MB, we load current luminance data 
before doing mode decision and texture coding. Using the 
PCDS can save about 96 cycles per MB in average. Fig. 13 
shows the schedule when I4MB is the best intra mode. 
In summary, we have illustrated three timing optimizing 
schemes for the proposed design to reduce about 173 cycles 
per MB as compared to the original schedule shown in Fig. 9. 
Table 1 shows the cycle count needed in the tasks of different 
quality scalable modes. The average cycles needed in the 
proposed design when operated at QS0, QS1 and QS2 are 
1112, 760 and 626, respectively. It also shows that we can 
obtain 1.46 times and 1.77 times of throughput improvement 
when the proposed design operates at QS1 and QS2 as 
compared to the QS0 at the cost of finite PSNR drop. This 
feature offers a large flexibility for trading off power 
consumption with quality in various applications [6]. 
 
 
Fig. 9: Original processing schedule in the proposed design  
 
Fig. 10: Proposed schedule with HLIS 
 
Fig. 11: Proposed schedule with both CIS and HLIS 
 
Fig. 12: Proposed schedule with PCDS, CIS, and HLIS when the 
best intra mode of the MB is I16MB 
 
Fig. 13: Proposed schedule with PCDS, CIS, and HLIS when the 
best intra mode of the MB is I4MB 
 
5. IMPLEMENTATION RESULT AND 
COMPARISON  
To support a complete intra encoding system, we also 
implement In Loop Filter, PDSB, AHB interface and other 
System components in the proposed design. The proposed 
intra encoder is implemented by 0.13um CMOS technology 
with the cost of about 170K gates and 4.43 KB of internal 
SRAM at clock rate of 130MHz, as shown in Table 2. The 
hardware cost of the intra coding module is only 80.54K 
gates. The proposed encoder can respectively encode SD and 
HD720 video sequences at clock rates of 26/33/47 MHz and 
70/85 MHz with different quality levels. Table 3 shows the 
proposed design compared with the other designs. We 
support the same encoding flow as H.264 reference software 
JM93 [7] including Hadamard Transform and I16MB plane 
mode. Exploiting the proposed timing optimizing schemes 
and low complexity search algorithms, it can respectively 
encode SD and HD720 video sequences at the lowest clock 
rate of 26MHz and 70MHz. As compared to the existing 
designs [3-4], the proposed design owns 42% and 52% 
reduction in terms of the cycle count per MB encoding, 
respectively. As compared to the design [8] that is optimized 
for low clock frequency, the proposed design not only 
supports low frequency mode (QS2) but also supports high 
 16 
[5] J. W. Chen, C. H. Chang, C. C. Lin, J. I. Guo, and J. S. 
Wang, “A Condition-Based Intra Prediction Algorithm 
for H.264/AVC,” Proc. ICME’2006. 
[6] H. C. Chang, J. W. Chen, C. L. Su, Y. C. Yang, Y. Li, C. 
H. Chang, Z. M. Chen, W. S. Yang, C. C. Lin, C. W. 
Chen, J. S. Wang and J. I. Guo, “A 7mW-to-183mW 
Dynamic Quality-Scalable H.264 Video Encoder Chip,” 
Proc. ISSCC’2007. 
[7] H.264/MPEG-4 AVC reference software JM9.3. 
 
III. Low Power High Performance Mixed 
Static/Dynamic Circuit Design 
Abstract－ Low power dissipation used by conventional 
CMOS circuits can be achieved by using the ramped type 
power supply. We propose a self-stabilization ramp voltage 
circuit technique called CKVdd, the swing voltage could 
reduce the power dissipation noticeably, which compared to 
conventional CMOS use constant Vdd. The design techniques 
validated by using a MPEG variable length video decoder, to 
obtain over 24% power saving without delay penalty. 
I. INTRODUCTION 
With the advantage of portable electronic devices and 
high density VLSI circuits, power dissipation has emerged as 
a major design concern. In this paper, we propose a 
methodology to reduce the power dissipation. We call this 
CKVdd. The structure of this paper is organized as follows. 
Section 2 introduces our proposed method. Section 3 
presents our experimental results. Section 4 demonstrates our 
tape-out chip. Section 5 concludes this paper. 
 
II. The Proposed method 
There are two normal power supply circuit structures 
shown in Figure1. A conventional CMOS circuit uses 
constant supply voltage (Vdd) provided to the circuit. 
Power-gate is a technique for reducing leakage power by 
shutting off the idle blocks. The implementation of 
power-gating requires a high-performance sleep transistors 
(PG) connected between the circuit blocks and the power 
supply. 
 
Low power dissipation using conventional CMOS 
circuits can be achieved by using the ramped power supply [l] 
[2]. Pulse power supply CMOS technique describes that the 
energy of the transition through the ramped power supply 
will dissipate small amounts of energy when the transition is 
made slowly enough. When use a ramp voltage with rise time 
Δt apply to a RC network, the dissipated energy of the 
resistor R can be reduced. As Δt increased much larger than 
the RC time constant, the energy dissipated in the resistor can 
be represent as:  
    ( )2
2
112 CVdd
tt
RCEnergy
DD
=  
 
 
 
 
 
 
 
 
 
 
 
 
 
In Figure2, we propose a ramp-voltage self-stabilization 
power supply technique (CKVdd), the circuit structure 
connects the system clock signal to an inverter (INVP) to 
generate reverse clock signal fanin power-gate PG transistor. 
Using this method to generate ramp supply voltage ( SWV ) 
can reduce the dynamic power dissipation for conventional 
CMOS circuits. 
 
We use a simple inverter circuit to illustrate the CKVdd 
theorem as shown in Figure 3. During CK is high, the 
transistors MN=ON, MP=OFF, PG=ON, then Vdd charge 
parasitic capacitances SWC  let SWV =Vdd. When SWV  
reaches Vdd, all gates are enabled as conventional CMOS 
circuits and are defined in the Valid phase. When CK signal 
goes low (the Evaluate phase), the transistor MN=OFF, 
MP=ON, the charge stored in the parasitic capacitances 
SWC  will do charge sharing to CPG. The rising VPG voltage 
will turn off PG to cut off the current supply path. CKVdd 
technique might let SWV  voltage drop bring circuit  
 
 18 
function fail during evaluate phase. The feedback
self-stabilization interconnection design connect SWV  to 
INVP will turn on PG to supply current for preventing the 
functional fault due to voltage drop of complexity circuit, to 
keep the circuit stable operation. The ramp voltage shows 
when CK signal entering next CK valid phase from voltage 
level drop of previous CK cycle. Follows test circuits are 
synchronous operating at positive edges. It could compare 
the CKVdd and Vdd circuit, the CKVdd technique reduce 
dynamic power due to less peak/average current by applying 
the slow rise Vdd voltage, but this issue also bring outputs 
signal performance degradation penalty. 
The PG transistors consume power also impact the 
circuit performance. We compromise the circuit 
power/performance by using the PMU and feedback 
self-stabilization interconnection (SS) mechanism as shown 
in Figure 4. The self stabilization feedback interconnection 
design is useful for complex circuit. The interconnection 
could elevate higher SWV  voltage, when SWV  voltage 
drop by large current requirement of test circuit, SS will turn 
on PG again to raise the SWV  voltage level. By using SS 
mechanism in fast-speed clock design, it could find large 
power current difference within CKVdd and Vdd design in 
all clock rates. So, the high speed circuit could work 
normally from using the self stabilization mechanism. The 
power management unit (PMU) is used to control the turned 
on PG number, PMU will lift the SWV  voltage by letting 
more PG gates turned on simultaneously. We need more PGs 
turned on in complex or fast-speed clock test circuit, and less 
PGs turned on for simple or slow-speed clock circuit. When 
more PG turned on, the higher level SWV  and IVdd 
provided to the test circuit. For using same number PGs, 
compare to Vdd design, it could find the CKVdd design 
have lower SWV  and IVdd of fast-speed clock design, so 
there is more power saving benefit of high speed design to 
adopt the CKVdd technique. 
 
IV. Experimental Demonstrations 
We use two different size test circuits to estimate the 
efficiency of CKVdd technique in Figure 5. The delay, 
power and average current factors are used in the follows 
comparisons. The small-size test circuit is 4-bit shift adder 
(838 transistors), and medium-size circuit is Variable Length 
Video Decoder (VLVD) [3] with PMU (61350 transistors). 
The bar charts show the simulation results under applied 
different clock frequency. The ratios above on each bar are 
comparing use original (Vdd) circuit. From the ratio values, 
positive/negative values represent large/less than original 
circuit. From comparing the power saving and delay 
increased ratio for the same clock-frequency, the power 
saving benefits are larger than delay increased penalty for 
medium-size circuit, but nearly equal for small-size circuit. It 
means the advantage by using CKVdd than conventional 
Vdd for large designs; we will demonstrate this observation 
by a large-size test circuit. This effect is clearly exposed in 
fast-speed clock frequency cases, but we also find for the 
faster clock frequency, partial outputs function fail by using 
 20 
IV. Content-aware multimedia video retrieval 
system    
ABSTRACT 
In this report, we propose a fast inter-mode decision 
scheme based on reliable spatio-temporal predictions of 
neighboring macroblocks and an early termination scheme 
for the algorithm-level optimization. We use a commercial 
profiling tool to identify most time consuming modules and 
then apply code-level optimization techniques including 
frame-memory rearrangement and 
Single-Instruction-Multiple-Data (SIMD) implementation 
based on the Intel MMX/SSE/SSE2 instruction sets to 
achieve further speed-up. Simulation results show that our 
proposed joint optimization H.264 encoder achieves a 
speed-up factor of up to about 20 compared to the reference 
encoder, without introducing significant quality degradation 
We also propose a novel face authentication scheme using 
the Active Appearance Model (AAM) and the Hidden 
Markov Model (HMM). The proposed face authentication 
system can be divided into two parts. First, the AAM is used 
to extract the low-dimensional feature vectors including 
combined texture and shape information of individual face 
images. The extracted feature vectors are further classified 
into several clusters using vector quantization. The clustered 
feature vectors are then characterized using HMMs to make 
full use of the temporal information across the face images. 
After all parameters in the HMMs are calculated, we can 
dynamically determine the thresholds for face authentication. 
An iterative algorithm is also proposed to automatically 
determine a suitable number of HMM states and a suitable 
number of observation classes to achieve good 
authentication accuracy. 
 
I. Introduction 
In the three-year project, we investigate efficient 
schemes for video surveillance and retrieval. We have 
developed in the first two years several efficient tools for 
video surveillance and retrieval, such as object tracking, 
compressed-domain fall incident detection, shot-based video 
retrieval. In this year, the goal of this project is to develop a 
computation-efficient H.264 codec such that it can be used 
in realtime video surveillance applications. Besides, we also 
develop an accurate person authentication scheme using face 
video. 
We first propose a fast inter-mode decision scheme 
based on reliable spatio-temporal predictions of neighboring 
macroblocks and an early termination scheme for the 
algorithm-level optimization. We use a commercial profiling 
tool to identify most time consuming modules and then apply 
code-level optimization techniques including frame-memory 
rearrangement and Single-Instruction-Multiple-Data (SIMD) 
implementation based on the Intel MMX/SSE/SSE2 
instruction sets to achieve further speed-up. Simulation 
results show that our proposed joint optimization H.264 
encoder achieves a speed-up factor of up to about 20 
compared to the reference encoder, without introducing 
significant quality degradation. 
We also propose a novel face authentication scheme 
using the Active Appearance Model (AAM) and the Hidden 
Markov Model (HMM). The proposed face authentication 
system can be divided into two parts. First, the AAM is used 
to extract the low-dimensional feature vectors including 
combined texture and shape information of individual face 
images. The extracted feature vectors are further classified 
into several clusters using vector quantization. The clustered 
feature vectors are then characterized using HMMs to make 
full use of the temporal information across the face images. 
After all parameters in the HMMs are calculated, we can 
dynamically determine the thresholds for face authentication. 
An iterative algorithm is also proposed to automatically 
determine a suitable number of HMM states and a suitable 
number of observation classes to achieve good 
authentication accuracy. 
We have integrated all the components developed in the 
three years into an intelligent realtime video surveillance 
system, comprising a video codec, streaming protocol stack, 
object tracking, fall detection, and person authentication. 
The report is organized as follows. In Section II, we 
brief review our results in the first two years’ projects. 
Section III elaborates on the results of this year’s project 
about H.264 codec optimization. Our results about face 
video-based person authentication are provided in Section 
VI. Conclusions about this 3-year project are drawn in 
Section V. 
 
II. Brief Review of the Results in FY94 and FY95 
II.1. FY94 (2004/8~2005/7) 
In FY94, we developed a feature-based 
compressed-domain fall-down detection scheme for 
intelligent surveillance applications. The proposed scheme 
involves two steps: compressed-domain object extraction 
and fall incident detection. In the object extraction step, the 
MVs and the DC+2AC image of each frame are firstly 
extracted. GME is then performed to distinguish moving 
object MBs from background MBs to obtain a rough object 
segmentation mask. The CDM is then used to refine the 
object mask. Should the video shot contain GMs, the GM 
compensation is performed prior to the change detection 
operation. Finally, object clustering is performed to separate 
the object mask into multiple individual objects. In the 
second step, three feature values: the change ratio of the 
centroid of a human object, the change ratio of the maximum 
of vertical projection histogram, and the duration of an event 
detected are used to identify and locate fall-down events. 
Our proposed object segmentation method can extract 
moving objects with or without cameral motions, thereby 
being useful for video surveillance applications equipped 
with still or pan-tilt cameras. Our experimental results show 
that the proposed method can detect fall incidents with high 
accuracy in real-time. 
 
 22 
Conversely, for the top-down splitting method, the largest 
block-size mode is chosen as the initial block-size mode. For 
merging-and-splitting, the middle block-size is chosen. By the 
assumption that the costs behave monotonically, some 
unlikely modes are excluded. In addition, early termination is 
used to exclude more unlikely modes. In general, for higher 
bit rates, smaller block size modes are preferred, that is, the 
bottom-up merging method can save more computation. For 
lower bit rates, the top-down splitting method may be better. 
Since multimedia applications are getting increasingly 
popular, most modern microprocessors have been embedded 
with specific multimedia instructions to speed up image and 
video processing programs. The 
Single-Instruction-Multiple-Data (SIMD) model is available 
in Intel processors. Utilizing the SIMD technology (e.g., the 
Intel MMX/SSE/SSE2 instructions [20]), several 
data-independent instructions can be executed in parallel. In 
video coding applications, a large number of small-size 
native data type operations are performed frequently, and the 
operations on different data are independent to others. These 
features make it suitable to exploit the parallelism with the 
SIMD technologies to optimize video codecs [22]-[27]. The 
method in [23] speeds up video encoding by adopting 
optimization techniques such as reduced-range 
block-matching, parallel DCT/IDCT with MMX/VIS, and 
code optimization techniques such as loop unrolling, data 
type optimization, and redundant operation reduction. In [25], 
several optimization methods are introduced, including data 
alignment for MMX, avoiding branch using SIMD 
conditional select masks, omitting 4´4 block-type, temporary 
pixel window for deblocking, etc. [26] proposes some SIMD 
implementation methods for fractional pixel interpolation, 
integer transform, etc. to optimize the H.264 decoder for 2-4 
times faster decoding speed. [27] proposes an optimized 
H.264 encoder utilizing a hyper-threading scheme (exploiting 
parallelism at the MB level) which achieves a speed-up 
factor of 1.2´. 
Before applying optimization to the encoder, complexity 
analyses have to be performed first to identify the 
computationally critical paths. Space and time complexity 
analyses for the H.264 codec on a tool-by-tool basis are 
reported in [28]. Theoretical complexity analyses of the 
H.264 baseline decoder are presented in [29][30]. It is shown 
in [26][27] that the most time-consuming modules of an 
H.264 encoder are Motion Estimation, Interpolation, SATD, 
and DCT. Therefore, these modules should be put into the 
top priority list to optimize. 
In this work, both algorithm-level and code-level 
optimization techniques are applied to accelerate the H.264 
software encoder on a commercial personal computer that 
supports SIMD instruction sets as shown in Fig. 1. We 
propose a fast inter-mode decision scheme based on Reliable 
Spatio-Temporal Prediction (RSTP) taken from neighboring 
MBs for the algorithm-level optimization. Besides, we 
implement the hybrid Unsymmetrical-cross 
Multi-Hexagon-grid Search (UMHexagonS) [10] that is 
adopted in the H.264 JM9.6 reference coder to reduce the 
computation. We then apply several code-level optimization 
techniques, including frame-memory rearrangement and 
SIMD implementations based on the Intel MMX/SSE/SSE2 
instruction sets to further accelerate the most time-critical 
modules. 
 
Fig. 1. Flowchart of the proposed joint optimization 
algorithm. 
III.2. Fast Block-Size Mode Decision Using Reliable 
Spatio-Temporal Predictions (RSTP) 
The conventional exhaustive mode-decision analyzes 
seven possible block-size modes and selects the best one 
from the seven modes for encoding. The proposed method, 
on the other hand, need to analyze only a subset of the seven 
modes by taking spatio-temporal predictions from adjacent 
blocks such that the time for encoding can be reduced 
drastically. As shown in Fig. 2, the coding modes of five 
spatio-temporally neighboring MBs: the left, upper, upper 
left, and upper right blocks of the current block, and the 
block at the same location in the previous frame, are used for 
prediction in our method. Table 1 shows the percentages of 
the coding mode obtained by the full-mode search being the 
same as one of the spatio-temporal predictions. The numbers 
in the parentheses of the first row indicate the number of 
predictions taken from the neighboring macroblocks for each 
 24 
Table 2 
Average accuracy of using spatio-temporal predictions to 
replace the full-mode search under three reliability 
conditions 
MV Variance 
Absolute 
MV Difference 
Majority Mode 
QP = 28 
P(T|A) P(F|A) P(T|B) P(F|B) P(T|C) P(F|C) 
Foreman 81% 19% 75% 25% 66% 34% 
Coastguard 84% 16% 70% 30% 62% 38% 
Carphone 85% 15% 78% 22% 70% 30% 
Container 92% 8% 93% 7% 93% 7% 
Akiyo 87% 13% 94% 6% 93% 7% 
Average 86% 14% 82% 18% 77% 23% 
A: MV_VARcur > THVAR 
B: AMVDcur < THAMVD 
C: more than half of the predicted modes are the same (i.e., a 
majority mode exists) 
T: predicted block-size mode is correct 
F: predicted block-size mode is incorrect 
 
 
Fig. 4. Illustration of motion blocks that have a large MV 
difference and a small MV difference. 
 
Fig. 5. Flowchart for the reliability test. 
As described above, only the reliabilities of the reference 
block-sizes smaller than 16´16 are determined by the MV 
variance in our method. For the 16´16 mode, the reliability is 
determined from the absolute MV difference of the current 
16x16 block and its adjacent prediction 16x16 block as 
defined in (2). It is known that if two adjacent blocks belong 
to a same object or have a same motion trajectory, the chance 
of the two adjacent blocks using a same block-size mode for 
encoding will be very high. Accordingly, the MVs of the two 
adjacent blocks are also similar, leading to a small absolute 
MV difference. On the contrary, if the MVs of the two 
adjacent blocks are different, it can be predicted that the two 
adjacent blocks have different motion trajectories, that is, we 
should not use the same block-size mode for encoding. Based 
on this concept, if the absolute MV difference of a current 
block from its adjacent block is smaller than a threshold 
value THAMVD, the current block-mode is then considered as 
a reliable one. It can be seen that the average accuracy is 
about 82% when the reference blocks are determined reliable 
under this reliability test. The process of reliability check is 
summarized in Fig. 5. The two thresholds, THVAR and 
THAMVD, used for the reliability test are determined 
empirically. 
By analyzing the predicted information, a number of 
prediction modes are obtained. Generally, if more than half 
of the prediction MBs use the same block-size mode, the 
possibility that the current encoding MB uses the same 
block-size mode for encoding is very high. This assumption 
is proved by the experimental results shown in Table 2, 
which indicates the average probability that the current MB 
uses the majority of the prediction information from its 
surrounding prediction MBs is about 77% for the five test 
sequences. Accordingly, if more than half of the prediction 
MBs use a majority block-size mode, the method then 
determines whether this majority mode is reliable using the 
reliability check method described in Fig. 5. If the majority 
mode is reliable, the method then uses the majority mode to 
encode the current block. If the majority mode is unreliable, 
however, the method then has to perform a full-mode search 
on the current MB to select the best block-size mode. 
The detailed flowchart of the proposed RSTP algorithm 
is illustrated in Fig. 6.  First, the process obtains prediction 
block-size modes according to the predicted information 
from the neighboring MBs. The sub-8´8 prediction modes 
(i.e., the 8´4, 4´8, and 4´4 modes) are all replaced with the 
8´8 mode. The method then determines whether or not more 
than half of the prediction modes are the same. If so (i.e., 
along the left branch of the flowchart), the process performs 
motion estimation for the majority reference block-size mode, 
and then checks the reliability of the majority reference mode 
according to the process described in Fig. 5. Should the 
majority reference mode be reliable, the best reference mode 
is used as a basis for determining the final coding mode; 
otherwise, a reduced full-mode search over the 16´16, 16´8, 
 26 
 
(a) 
 
(b) 
Fig. 7. Two frame arrangement schemes: (a) the 
arrangement in the JM7.3 reference encoder; (b) rearranged 
frame storage in the proposed method. 
 
In order to avoid the inefficient memory access due to 
noncontiguous memory access, the way the fractional pixels 
are stored is rearranged in the proposed encoder as shown in 
Fig. 7(b). Sixteen image memories, which are of the same 
size as the source video picture, are allocated for the storage 
of each type of fractional pixels. In this way, 16 pixels of a 
4´4 block, regardless of integer-pixel or fractional-pixel 
samples, are now distributed contiguously in the same frame 
memory. Besides the improvement of the cache or memory 
access efficiency, loading 16 pixels of a 4´4 block now takes 
only 4 memory-load operations if the SIMD parallel-load 
instruction is exploited; whereas this instruction cannot be 
utilized with the original storage arrangement. 
 
B. Interpolation with SIMD instructions 
After rearranging the array locations of the interpolated 
image, we modify the interpolation function 
(“UnifiedOneForthPix” in the reference software) according 
to the new memory arrangement so as to take advantage of 
the Intel SIMD technology on speed optimization. In the 
reference encoder, the fractional-pixel interpolation process 
can be divided into the following three steps: 
Step 1.  The half-pixel samples labeled with ‘b’ in a frame 
are interpolated first. The interpolated pixels and 
the integer samples are then stored into a 
temporary image together, which is as large as 
two times of the original image size. 
Step 2.  The half-pixel samples labeled with ‘h’ and ‘j’ in 
a frame are then interpolated by applying the 
6-tap FIR filter vertically to the temporary image 
which is created in Step 1. All integer and half 
pixels are then stored into the up-sampled image. 
Step 3.  The quarter-pixels are subsequently interpolated 
by applying the bilinear filter and stored in the 
up-sampled image. 
In Step 1 of the proposed SIMD implementation, eight 
half-pixels (namely, half-pixel labeled with ‘b’ in Fig. 7) will 
be obtained in parallel, except that the image-boundary cases 
are still implemented by non-parallel C functions since less 
parallelism could be exploited around the boundary. Eight 
integer-pixels are loaded into the SSE2 registers before being 
packed into 16-bit short words. The 6-tap FIR filter is 
applied with only shift and add/subtract operations on the 
integer-pixel samples. It is done by loading six rows of eight 
integer-sample pixels into six SSE2 registers. After applying 
the FIR filter to each one utilizing the shift and add 
operations, intermediate values are obtained by summing up 
these data in the registers in parallel. The final half-pixel 
values are obtained by performing parallel shift on the 
intermediate values, and are then stored into a temporary 
memory together with the integer-pixel samples. 
In Step 2, the eight pixels, including four half-pixel 
samples labeled with ‘h’ and four labeled with ‘j’, are 
interpolated in parallel using a procedure similar to Step 1, 
except that the FIR filter is applied vertically. After the 
parallel interpolation is done, all the half-pixel samples and 
integer-pixel samples are stored in the rearranged memory 
array. Note that fractional-pixel samples of the same type are 
stored contiguously in the rearranged memory arrays, thereby 
facilitating the implementation of the interpolation process 
with the SIMD instructions (contiguous pixels can be loaded 
into the SSE2 registers with one instruction rather than 
multiple memory accesses). 
In Step 3, the parallel-average (PAVG) instruction 
depicted in Fig. 8 is utilized for the quarter-pixel 
interpolation. 
 
 28 
independent, making it suitable for the SIMD implementation. 
In our method, the PSAD instruction provided in MMX and 
SSE2 are utilized according to the size of the matching block. 
The optimized algorithm for the SAD calculations is 
described as follows: 
Case 1: If the size of the matching block is 4´4, perform 
SAD calculation in C. 
Case 2: If the block-size is 8´8,  
A. Load the 8 pixels of current block into the 
MMX1 register and the 8 pixels of reference 
block into the MMX2 register. (8 bits/coef.) 
B. Perform the PSAD instruction on MMX1 
and MMX2. 
Case 3: If the block-size is 16´16,  
A. Load the 16 pixels of current block into 
XMM1 register and the 16 pixels of the 
reference block into XMM2 register. (8 
bits/coefficient) 
B. Perform the PSAD instruction on XMM1 
and XMM2. And sum up the two intermediate 
SAD values in the high/low word of the 
destination register. 
 
III.4. Experimental Results 
The experiments are performed on a Pentium-4 2.4 GHz 
personal computer equipped with 512 MB main memory and 
the Windows XP OS. All codes are compiled by the Intel® 
compiler [31]. The run-time complexities are profiled using 
the Intel® VTune [32] performance analyzer. Five QCIF 
(176´144) sequences of 150 frames, including Foreman, 
Coastguard, Carphone, Container, and Akiyo, are used in our 
experiments. Table 3 lists the detailed setting of coding 
parameters. The two thresholds used in are empirically set as 
THADMV = 2 and THVAR = 5. 
Table 3 
 Setting of encoding parameters 
QP 20, 24, 28, 32, 36 
Search range ±16 
No of reference frames one 
Frame rate 30 Hz 
GOP structure IPPP…IPPP 
GOP size 30 frames 
Entropy coder CAVLC 
R-D Optimization (RDO) off 
 
Table 4 shows the R-D performance comparison between 
the proposed RSTP mode decision and a reduced exhaustive 
search with a selected subset of the seven block-size modes. 
In the reduced exhaustive search, we choose the most 
frequently used modes in the full-mode search to form the 
subset. The number of modes is chosen as the closest integer 
to the average number of modes used in the RTSP method. 
For example, in Table 4, the average number of block-size 
modes searched using the RTSP scheme for Foreman is 3.61. 
We thus pick only the 4 most frequently used modes for the 
reduced exhaustive-search mode decision. Table 4 and Fig. 10 
show that the proposed method outperforms the reduced 
exhaustive search with a similar number of modes in a wide 
range of bit-rates for different test sequences. In practice, the 
encoder does not possibly know which modes are the most 
frequently used ones without doing full-mode search for the 
whole frame. This comparison shows that the proposed RSTP 
method obtains reliable block-size modes with a significantly 
reduced number of search modes compared to the full-mode 
search. The RSTP method also significantly improves the 
R-D performance of the direct spatio-temporal prediction that 
does not perform mode reliability check, while keeping the 
computational complexity comparable. 
Table 4 
 R-D performance comparison between the proposed RSTP 
mode decision and a reduced exhaustive search with a 
similar number of modes 
PSNR difference Bit-rate difference 
Average no. of 
modes 
QP = 28 
RSTP Most freq.  RSTP 
Most 
freq.  RSTP 
Most 
freq. 
Foreman -0.09 dB -0.12 dB 1.16 % 1.81 % 3.61 4 
Coastguard -0.04 dB -0.05 dB 1.81 % 2.76 % 3.17 3 
Carphone -0.13 dB -0.18 dB 1.38 % 2.03 % 3.06 3 
Container -0.04 dB -0.09 dB 3.45 % 5.46 % 2.06 2 
Akiyo -0.04 dB -0.07 dB 1.42 % 2.57 % 1.95 2 
 
FOREMAN
33
34
35
36
37
38
39
40
41
75 125 175 225 275 325
Bitrate
PS
N
R
JM7.3
RSTP Mode Decison
4 most frequently used modes
Direct  Prediction
Ref. [17]
 
(a) 
 30 
dB dB dB % % % 
Akiyo -0.04 dB 
-0.05 
dB 
-0.07 
dB 
1.42 
% 
1.58 
% 
1.57 
% 
 
FOREMAN
33
34
35
36
37
38
39
40
41
75 125 175 225 275 325
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(a) 
COASTGUARD
30
31
32
33
34
35
36
37
38
39
40
100 200 300 400 500 600
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(b) 
CARPHONE
34
35
36
37
38
39
40
41
42
50 100 150 200 250
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(c) 
CONTAINER
33
34
35
36
37
38
39
40
41
25 45 65 85 105 125 145
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(d) 
AKIYO
35
36
37
38
39
40
41
42
43
20 30 40 50 60 70 80 90
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(e) 
Fig. 12. Average PSNR performance comparison using 
the reference software, the proposed fast mode decision 
(FMD), and the joint optimization for five test sequences: (a) 
Foreman; (b) Coastguard; (c) Carphone; (d) Container; (e) 
Akiyo. 
III.5. Summary 
We proposed a fast block-size mode decision algorithm 
based on reliable spatio-temporal predictions and early 
termination. We also proposed efficient code-level 
optimization techniques, including the frame memory 
rearrangement and several optimizations with the SIMD 
technology, etc. We have implemented a highly optimized 
coder by integrating the proposed fast mode decision and 
code-level optimization schemes with a hexagon-search fast 
motion estimation scheme. Experimental results show that 
the optimized encoder achieves a speedup factor of up to 
about 20´ compared to the reference encoder without 
introducing significant quality degradation, making it 
suitable for real-time applications. 
 32 
AAM [42][43] is an extension to the ASM. Instead of 
manually selecting feature points in SAM, it is used to 
extract the representative features of the object in an image 
automatically. The appearance model is built based on a set 
of labeled images, where the landmark points are marked on 
each face. After labeling all the sample images, Procrustes 
analysis is utilized to align each face according to a mean 
shape. Principal Component Analysis (PCA) is then applied 
to effectively reduce the dimensionality for both shapes and 
shape-free textures. Thus, the shape x and texture g of a face 
image can be represented by a compact feature vector c as 
   
s
g
= +
= +
x x Q c
g g Q c
                           (3) 
where x  is the mean shape, g is the mean of shape-free 
texture, and ,s gQ Q  are matrices describing the modes of 
variation derived from the training set [43]. 
 
Fig. 14. A 3-state left-right HMM. 
The Hidden Markov Model (HMM) is a statistical 
model used to characterize a signal as a parametric random 
process. In past ten years, the Hidden Markov Model 
approach has been successfully and widely used for many 
applications because the models are very rich in 
mathematical structure. In the speech recognition domain, 
Rabiner [45] proposed the approaches for speech recognition 
by using HMM. The method proposed in [46] uses the 
embedded HMM for facial expression. In the face 
recognition domain, Liu and Chen [40] proposed an 
approach for video-based face recognition by using adaptive 
HMM, and each HMM is adapted with the test video 
sequence. 
An HMM can be viewed as an unobservable Markov 
chain with a finite number of states. An HMM can be 
described by a transition probability matrix A, an initial state 
probability distribution p, and a set of probability density 
functions for observations B. Fig. 14 shows the diagram of 
HMM with three states. 
 
IV.3. Proposed Video-Based Face Authentication 
IV.3.1. System Overview 
Fig. 15 shows the block diagram of our proposed face 
authentication system. The proposed scheme can be divided 
into two steps. First, the features are extracted by using the 
appearance models. Second, the AAM parameters are 
classified by HMM for authentication. During the 
authentication process, the user is asked to show his/her front 
face to the camera and say a secret password. A face video 
with mouth motions corresponding to the secret password is 
captured for authentication. At the first stage, a 
skin-color-based face detector is used to locate the face 
region which is used to determine the initial AAM shape 
model. Based on the initial model, an iterative AAM mode 
refinement algorithm is performed to locate accurate feature 
point locations as well as to extract the low dimensional 
features of each face image. The model parameters of all 
face images in a video sequence are then clustered using 
vector quantization to obtain a reduced number of 
observation vectors. These observation vectors are fed into a 
set of HMM classifiers to decide whether the incoming face 
sequence matches the features of an authorized user.  
 
Fig. 15. Block diagram of the proposed face 
authentication system. 
In this work, we propose an adaptive scheme to determine 
the thresholds in authenticating a face. We also propose an 
iterative algorithm with a set of training sequences to 
determine a suitable hidden state number in HMM and a 
suitable class number of observations.  
IV.3.2. Face Authentication Using HMM 
As shown in Fig. 3, the sequences in the database are 
divided into two parts: one part for training and the other 
part for testing. In the training stage, we first extract the 
features of each training face image and use these features to 
construct an appearance model. Then we can easily obtain 
the appearance parameter c in (3) by using this model. After 
all the appearance parameters are computed, we use the 
vector quantization approach to separate these feature 
vectors into N classes. Thus, we can obtain all the 
observations O, and each training face image sequence can 
be represented as a symbol sequence. Finally, we use these 
symbol sequences for training the HMM. 
When we train the HMM, a transition probability matrix A, 
an initial state probability distribution B, and a set of 
probability density functions p are initialized. Here we set 
the initial value of p = [1,0,0,…,0], because the left-to-right 
HMM is suitable for our application. We then use the 
training symbol sequences and the Expectation 
Maximization (EM) algorithm to calculate the final 
parameter vector (A, B, p) of each trained HMM. After the 
face authentication system is built, we need to determine the 
suitable parameters of HMM. Finally, we use the test 
sequences with the suitable parameters in HMM to test the 
system and check if the system is stable for face 
authentication. 
 34 
method. The experiments show that the proposed method 
achieves significantly better FRR and FAR combinations 
most of the time. 
 
Class Number = 42 , State Number = 15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.02 0.04 0.06 0.08 0.1
FAR
F
R
R
Proposed Texture Shape  
(a) 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.02 0.04 0.06 0.08 0.1
FAR
F
R
R
Proposed : ClassNum=42, StateNum=15
Texture : ClassNum=45, StateNum=19
Shape : ClassNum=42, StateNum=8
 
(b) 
Fig. 19. Performance comparison of the proposed method, 
AAM shape + HMM, and AAM texture + HMM: (a) (state 
number, class number) = (15,42) for all the three methods; (b) 
each method uses its optimum class number and state 
number. 
 
IV.5. Summary 
We proposed a novel video-based face authentication 
scheme using AAM and HMMs, in which AAM is used to 
extract the low dimensional features of a face image. After 
extracting model parameters of each face image in a face 
video sequence, a reduced set of observations of each 
sequence are obtained using vector quantization to cluster all 
these feature vectors. Using HMMs to characterize the 
temporal dynamics of these observations, we can extract 
useful features for face authentication. We proposed a 
scheme to adaptively determine the thresholds used in the 
system. We have also proposed an iterative algorithm to 
determine a suitable hidden state number in HMM and a 
suitable class number of observations using test sequences. 
The experimental results show that the proposed method 
achieves very low FAR and FRR.  
 
V. CONCLUSIONS 
In this three-year project, we have developed efficient 
tools for video surveillance and retrieval. In the first year 
(FY94), we developed a compressed-domain object 
segmentation and fall incident detection scheme for 
homecare applications. In the second year (FY95), we 
developed a shot-based coarse-to-fine video retrieval to fast 
search similar video clips in video database. In the final year, 
we have developed a highly optimized coder by integrating 
the proposed fast mode decision and code-level optimization 
schemes with a hexagon-search fast motion estimation 
scheme. Our encoder achieves a speedup factor of up to 
about 20´ compared to the reference encoder without 
introducing significant quality degradation. In addition, we 
proposed a novel video-based face authentication scheme 
using AAM and HMMs that can achieve achieves very low 
FAR and FRR. 
We have integrated all the components developed in the 
three years into an intelligent realtime video surveillance 
system, comprising a video codec, streaming protocol stack, 
object tracking, fall detection, and person authentication. 
Our results have resulted in several publications, including 
eight international conference papers [47]-[54] and four 
international journal papers (three papers published or 
accepted and one revised) [52]-[58]. The results meet the 
goals of this project quite well. 
REFERENCES 
[1] Joint Video Team of ITU-T and ISO/IEC JTC 1, Draft 
ITU-T Recommendation and Final Draft International 
Standard of Joint Video Specification (ITU-T Rec. 
H.264 | ISO/IEC 14496-10 AVC), Doc. JVT-G050, 
Mar. 2003. 
[2] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. 
Luthra, “Overview of the H.264/AVC video coding 
standard,” IEEE Trans. Circuits Syst. Video Technol., 
vol.13, no. 7, pp. 560-576, July 2003. 
[3] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. 
J. Sullivan, “Rate-constrained coder control and 
comparison of video coding standards,” IEEE Trans. 
Circuits Syst. Video Technol., vol. 13, pp. 688–703, 
July 2003. 
[4] M. Ravasi, M. Mattavelli, and C. Clerc, “A 
computational complexity comparison of MPEG4 and 
JVT codecs,” Joint Video Team (JVT) of ISO/IEC 
MPEG and ITU-T VCEG, Doc. JVT-D153r1-L, July 
2002. 
[5] X. Zhou, E. Q. Li and Y. K. Chen, “Implementing 
H.26L decoder on general-purpose processors with 
media instructions”, SPIE Conf. on Image and Video 
Communications and Processing, San Diego, USA, 
Jan. 2003. 
[6] T. Koga, K. Iinurna, A. Hirano, Y. Iijima, and T. 
Ishiguro, “Motion-compensated interframe coding for 
video conferencing,” Proc. Nat. Telecommun. Conf., 
 36 
IEEE, vol. 83, no. 5, pp. 705-741, May 1995. 
[35] A. Nefian, A Hidden Markov Model-Based Approach 
for Face Detection and Recognition, PhD thesis, 
Georgia Institute of Technology, Atlanta, GA, August 
1999. 
[36] Z. Li, H. Ai, and G. Xu, "Training support vector 
machines for video based face recognition," In Proc. 
IEEE Int. Conf. Image and Graphics, 2002. 
[37] Y. Li, S. Gong, and H. Liddell, “Video-based online 
face recognition using identity surfaces,” in Proc. 
IEEE Int. Conf. Recognition, Analysis, and Tracking of 
Faces and Gestures in Real-Time Systems, pp. 40-46, 
July 2001. 
[38] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman, 
“Video-based face recognition using probabilistic 
appearance manifolds,” in Proc. IEEE Int. Conf. 
Computer Vision and Pattern Recognition, vol. 1, pp. 
313-320, June 2003. 
[39] X. Tang and Z. Li, “Frame synchronization and 
multi-level subspace analysis for video based face 
recognition,” in Proc. IEEE Int. Conf. Computer Vision 
and Pattern Recognition, vol. 2, pp. 902-907, July 
2004. 
[40] X. Liu and T. Chen, “Video-based face recognition 
using adaptive hidden Markov models,” in Proc. IEEE 
Int. Conf. Computer Vision and Pattern Recognition, 
vol. 1, pp. 340-345, June 2003. 
[41] T. F. Cootes and C. J. Taylor, Statistical Models of 
Appearance for Computer Vision, PhD thesis, 
Manchester, U.K., March 2004. 
[42] T. F. Cootes, G. J. Edwards, and C. J. Taylor, "A 
comparative evaluation of active appearance model 
algorithms," in Proc. British Machine Vision 
Conference, 1998. 
[43] T. F. Cootes, A. Hill, C. J. Taylor, and J. Haslam, "The 
use of active shape models for locating structures in 
medical images," Image & Vision Computing, July 
1994. 
[44] H. Kang, T. F. Cootes, and C. J. Taylor, “A comparison 
of face verification algorithms using appearance 
models,” in Proc. BMVC, vol. 2, pp. 477-486, 2002. 
[45] L. R. Rabiner, “A tutorial on hidden Markov models 
and selected applications in speech recognition,” Proc. 
IEEE, vol. 77, no. 2, pp. 257-286, Feb. 1989. 
[46] X. Zhou, X. Huang, B. Xu, and Y. Wang, “Real-time 
facial expression recognition based on boosted 
embedded hidden Markov model,” in Proc. IEEE Int. 
Conf. Image and Graphics, pp. 290-293, Dec. 2004. 
[47] Y.-H. Ho, W.-R. Chen, and C.-W. Lin, “A 
rate-constrained key-frame extraction scheme for 
channel-aware video streaming,” in Proc. IEEE Int. 
Conf. Image Processing, Oct. 2004, Singapore. (top 
10% paper, Nominee of Student Paper Awards) 
[48] J.-F. Chen, H.-Y. M. Liao, and C.-W. Lin, “Fast video 
retrieval via the statistics of motion,” in Proc. IEEE Int. 
Conf. Acoustics, Speech & Signal Processing, Mar. 
2005, Philadelphia, PA, USA. 
[49] C.-W. Lin, Z.-H. Ling, Y.-C. Chang, and C. J. Kuo, 
“Compressed-domain fall incident detection for 
intelligent home surveillance,” in Proc. IEEE Int. 
Symp. Circuits and Systems, May 2005, Kobe, Japan. 
[50] Y.-H. Ho, C.-W. Lin, J.-F. Chen, and H.-Y. M. Liao 
“Fast coarse-to-fine video retrieval via shot-level 
statistics,” in Proc. SPIE Conf. Visual Communication 
and Image Processing, July 2005, Beijing, China. 
(won the Young Investigator Award) 
[51] Y.-L. Lai, Y.-Y. Tseng, C.-W. Lin, Z. Zhou, and M.-T. 
Sun, “H.264 encoder speed-up via joint 
algorithm/code-level optimization,” in Proc. SPIE 
Conf. Visual Communication and Image Processing, 
July 2005, Beijing, China. 
[52] C.-W. Su, H.-Y. M. Liao, K.-C. Fan, C.-W. Lin and 
H.-R. Tyan, “A motion-flow-based fast video retrieval 
system,” in Proc. 7th ACM SIGMM International 
Workshop on Multimedia Information Retrieval, Nov. 
2005, Singapore. 
[53] K.-Z. Chen, Y.-J. Chang, and C.-W. Lin, “Video-based 
authentication using appearance models and HMMs,” 
in Proc. IEEE Int. Symp. Circuits and Systems, May 
2006, Island of Kos, Greece. 
[54] C.-W. Lin and Z.-H. Ling, "Automatic fall incident 
detection in compressed video for intelligent 
homecare," in Proc. Int. Workshop Multimedia 
Analysis & Processing, August 2007, Hawaii, USA. 
[55] Y.-H. Ho, C.-W. Lin, J.-F. Chen, and H.-Y. M. Liao 
“Fast coarse-to-fine video retrieval using shot-level 
statistics,” IEEE Trans. Circuits and Systems for Video 
Technology, vol. 16, no. 5, pp. 642-648, May 2006. 
[56] C.-W. Lin, Z.-H. Ling, Y.-C. Chang, and C. J. Kuo, 
“Compressed-domain fall incident detection for 
intelligent homecare,” accepted and to appear in 
Journal of VLSI Signal Processing Systems for Signal, 
Image, and Video Technology  (Special Issue on 
Audio-Visual Signal Processing for Intelligent 
Security Systems) 
[57] C.-W. Su, H.-Y. M. Liao, H.-R. Tyan, C.-W. Lin, 
D.-Y. Chen, and K.-C. Fan, “Motion flow-based video 
retrieval,” accepted and to appear in IEEE Trans. 
Multimedia. 
[58] Y.-L. Lai, Y.-Y. Tseng, C.-W. Lin, Z. Zhou, and M.-T. 
Sun, “H.264/AVC encoder speed-up using joint 
algorithm/code-level optimization,” Journal of Visual 
Communication and Image Representation. (revised, 
June 2007) 
