多張透視投影影像間強健式點特徵抽取 
與快速特徵比對技術 
Robust Point Feature Extraction and High-Efficiency Point 
Matching for Perspective View Registration 
計畫編號： NSC 95-2221-E-009-216-MY2 
執行期限：95 年 8 月 1 日至 97 年 7 月 31 日 
  主持人：陳稔教授 / 國立交通大學資工系 
E-mail: zchen@csie.nctu.edu.tw Tel: 03-5731875 
Abstract 
The fundamental problem of point matching for 
image registration is to recover the 2D spatial 
transformation between two images taken under 
different viewing specifications. There are various 
image registration methods in the literature. 
Different methods were proposed to meet different 
requirements. However, there is still a need to find 
the optimal solution under the more desirable 
conditions. In this project, a computational efficiency 
approach for image registration to register two 
images under homography transformation is 
proposed. Meanwhile, the method also deals with the 
problems of image noise and partial overlapping 
using offline planning strategies. Computer 
simulation results with real and synthetic data 
demonstrate the features of our method. 
 
1. Introduction 
In computer vision applications related to 
object pose determination, motion tracking, 
camera parameter estimation, object recognition 
and perspective reconstruction, we need feature 
points with discriminative power to perform these 
tasks, no matter whether we use stereo views, 
multiple views or a view sequence. Also, we need 
to find the point correspondences across the 
views. Due to noise and occlusion and other 
reasons not all the feature points extracted are 
commonly visible across the different views, so 
the point correspondence finding is not an easy 
task. The conventional point matching method is 
based on image correlation, plus the use of 
ordering constraint and eipolar constraint added 
to aid the matching [1]-[13]. A dynamic 
programming[36] is applied to speed up the point 
matching search. However, when the camera 
viewpoints change widely, the above method 
becomes not suitable.  
In the previous NSC research project we 
have developed the Gabor-filtering technique 
using the multi-scale and multi-orientation 
concepts to find the robust feature points. These 
feature points reflect the local pattern structure 
information to facilitating the point matching. 
The fundamental problem of point matching for 
image registration is to recover the 2D spatial 
transformation between two images taken under 
different viewing specifications. There are 
various image registration methods in the 
literature.  
  Different methods[1]-[13] were proposed to 
meet different requirements. However, there is 
still a need to find the optimal solution under the 
more desirable conditions. In the following we 
shall consider four general issues simultaneously 
in the view registration problem: 
(1) The generality of spatial transformation: The 
spatial transformation between two images is 
often approximated by an affine 
transformation or a similarity 
transformation[12, 13]. However, both of 
them are special cases of a homography (2D 
projective matrix). The homography deals 
with the projective distortion problem in 
addition to the effects of rotation, translation 
and scaling. 
(2) View registration time complexity: There are 
two classes of image registration algorithms in 
estimating a particular 2D spatial 
their principal orientations to reduce the 
combinational complexity. Next, we refine the 
approximate solution under a homography 
transformation by applying an iterative process, 
called ICPM. Section 4 proposes five strategies 
for selecting two good matching point pairs for 
the image registration in an off-line fashion. 
Section 5 gives our overall registration algorithm 
including the off-line planning strategies and the 
on-line registration steps. The experimental 
results are reported in Section 6. Finally, a 
conclusion is given in Section 7. 
 
2. The Gabor-based feature point 
In our previous work [35], we apply a 
multi-scale and multi-orientation Gabor filtering 
technique to obtain a set of energy maps of a 
given image and then manage to get a single 
‘maximum energy map’ by retaining the 
maximum energy at the principal scale (sd) for 
each image point. We extract a set of Gabor-based 
feature points from the maximum energy map. 
We use a feature vector 
ip
V
r
consisting of L 
filter responses to a set of Gabor filters with an 
incremental orientation step of π/L to characterize 
the local pattern around the feature point ( , )i i ip x y . 
The principal scale 
i
d
ps  and the principal 
orientation 
i
d
pθ  are iteratively tuned to a high 
accuracy. The mathematical form of a 
Gabor-based feature point is given as 
1 1( , ) ( , )( , )[ ( , ), ( , ), ( , )]
d d d dd d p p p pi i i ip pi i
i
Ls ss TL L
p i i i i i iV R x y R x y R x y
θ π θ πθ −+ +=r K  
The similarity ( , )
i jp q
S V V
r r  between feature 
points 
ip
V
r
and
jq
V
r
 can be measured by the 
normalized cross correlation: 
( , ) i j
i j
i j
p q
p q
p q
V V
S V V
V V
⋅= ⋅
r r
r r
r r  
 
3. The homography registration 
3.1The approximate image registration: an 
initial solution  
We shall approximate a homography by an 
affine transformation, and, in turn, an affine 
transformation by a similarity one. For the time 
being, we assume that the images are free from 
noise and missing object part. 
3.2 The affine transformation matrix 
determination 
Under an affine transformation with 
anisotropic scaling factor (0< 1s < 2s )，, we observe 
that some of the Gabor-based feature points 
invariant under the similarity transformation 
remain almost invariant due to the negligible 
energy change when their principal orientations 
are nearly perpendicular to the direction of the 
smaller scaling 1s . 
To estimate the matrix of the affine 
transformation, we can use merely two point pairs 
plus their accompanied principal orientations to 
replace the conventional set of three point pairs. 
The merit of using only two point pairs is to 
reduce the combinational complexity from 3nC  to 
2
nC , where n is the total number of feature points. 
 
1 1 11 12 13
2 2 21 22 23
cos sin
sin cos
0 0 1 1 0 0 1 11
i x i i
i i y i i i
x s s t x a a a x
X y s s t y a a a y AX
θ θ
θ θ
′ −⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥′ ′= = = =⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦⎣ ⎦
r r
   
In Figs. 1(a) and 1(b), the unit vectors 
, , ,
k l k l
e e e e′ ′r r r r are the principal orientations of point 
, , ,k l k lp p p p′ ′r r r r , respectively. Under the affine 
transformation the relationship between the two 
corresponding principal orientations, ( , )
k k
e e′r r  or 
( , )
l l
e e′r r , can be shown to be  
 
k kk i k i
p p e A p p e′ ′ ′ =uuuuur uuuuurv v  
 
In a matrix notation,  
 
11 12 13
21 22 23
0 0 10 0
k k
k k
e e
k i
e e
k i
x xa a ap p
y a a a y
p p
′
′
′⎡ ⎤ ⎡ ⎤⎡ ⎤′ ′ ⎢ ⎥ ⎢ ⎥⎢ ⎥′ =⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥⎣ ⎦⎣ ⎦ ⎣ ⎦
r r
r r
uuuuur
uuuuur
 
 
Let /k k i k is p p p p′ ′=
uuuuur uuuuur  which is unknown. 
  Then, we have the following system of linear 
the size of the final ( )fCPS  as a stopping 
criterion. The algorithm is described as follows: 
 
Algorithm ICPM (Iterative Closest Point 
Matching) 
Input:  
1. Two feature point sets P′ =
1 2{ , , , }qNp p p′ ′ ′L  
and P =
1 2{ , , , }pNp p pL  in the form of feature 
vectors. 
2. The initial estimate of the transformation 
(0)T . 
Output:  
1. The corresponding point set ( )fCPS  
2. The final transformation matrix ( )fT  
Initialization: 
k = 1  
 
0 {( , ), ( , )}k k l lCPS p p p p′ ′= ; 0 20d =  
Begin 
Repeat until ( ) ( 1) ( 1)k k kCPS CPS CPS− −=U  
1. Construct the corresponding points set 
( ) ( 1)
1 ( , ( ( ), )p
Nk k
i i iCPS p CC T p P
′ −
= ′ ′= U  
where ( 1)( ( ), )k i iCC T p P p− ′ =  
if the following conditions are satisfied: 
(a) Distance constraint 
  ( 1) ( )k i iT p p− ′ − kd≤  
where ( 1) ( )k i iT p p− ′ − = ( 1) ( )
i
k
i ip
min T p p− ′ −  
0 / 2
k
kd d=  
(b) Similarity constraint 
 ( , )i i ThresholdS p p′ >  
where ( , ) i i
i i
i i
p p
p p
p p
V V
S V V
V V
′
′
′
⋅= ⋅
r rr r
r r   
2. Compute the new transformation matrix 
( )kT  using all points in the ( )kCPS , 
Update 1k k= + . 
End 
( ) ( )f kT T= ; ( ) ( )f kCPS CPS=  
End 
 
4. The off-line planning strategies for selecting 
the good starting point pairs  
By now, we know that the success of the whole 
image registration is determined by the two 
starting matching point pairs in the two images. 
In the following we shall propose five strategies 
for the selection of two good starting points in the 
reference image that results in an initial solution 
and a fast final registration result.  
 
4.1 Planning strategies for the image 
registration free from noise and missing object 
part 
Strategy 1: Select the two reference feature 
points leading to a large equilateral triangle 
As shown in Fig. 2, the two points ip
r  and
jp
r , 
and their principal orientations 
ip
er and 
jp
er  form 
a triangle (shown by solid lines) having an 
area ( , )i jA p p
r r . We measure the similarity of this 
triangle to the equilateral triangle defined by the 
longest side length maxl  of the triangle (shown by 
dashed lines). The area of the equilateral triangle 
is 2
max( , ) ( 3 / 4)e i jA p p l=r r . Therefore, the equilateral 
triangle similarity measure is given by 
 
( , )
( , )
( , )
i j
i j
e i j
A p p
S p p
A p p
=
r rr r r r = 2
max
( , )
( 3 / 4)
i jA p p
l
r r
; 0 ( , ) 1i jS p p≤ ≤r r  
 
 
 
Fig. 2 The triangle formed by the two 
points ip
r and
jp
r , and their principal orientations
ip
er  
and 
jp
er and an equilateral triangle defined by the 
longest side
maxl of the triangle. 
 
Moreover, the larger the equilateral triangle is, 
the more reliable the estimation result is. 
Therefore, the effective triangle index is defined 
by  
 
( , ) ( , )e i j i j i jS p p S p p p p= −r r r r v v  
 
ip
v
jp
v
ip
ev
jp
ev
ijA
rtA
maxl
ji PP
rr −
 
many point pairs having no matching points in 
the sensed image for each of the six sorted point 
pair lists to detect the partial overlapping 
sub-regions. We define the overlapping index as  
( )
( )
( )
c
ic
i initial
i
SPP
OI
SPP
= , 1, 2,...,6i =  
 
5. Registration algorithm  
The overall registration algorithm is as follows: 
Input: The reference feature point sets 
P = 1 2{ , , , }pNp p pL  and the sensed feature 
point set P′ =
1 2{ , , , }pNp p p ′′ ′ ′L , each point is 
associated with its feature vector. 
Output: The final corresponding point set 
( )fCPS  and the final homography matrix 
( )fT . 
 
Method: 
Step.1. Randomly select a point pair 
( , )k lp p with intersected principal 
orientations ( , )
k l
e ev v . 
Step.2. Search the feature point in the sensed 
image by computing the feature vectors 
correlation. Once finding a candidate 
matching pair ( , )k lp p′ ′ , check if the 
convex/concave compatibility is 
satisfied: ( )( ) 0
k l k lp p p p
e e e e′ ′⋅ ⋅ ≥v v v v ? If 
passed, execute the following steps. 
(2.1) Compute the approximate 
transformation matrix (0)T  by 
substituting the two point pairs 
( , )k lp p′ ′  and ( , )k lp p  into Eq. (10) 
(refer to Section 2). 
(2.2) Apply the ICPM algorithm to 
determine the homography matrix 
using (0)T  as the initial solution, 
and get the output { ( )fCPS , ( )fT } 
(refer to Section 3). 
(2.3) Check the stopping criteria for the 
registration refinement: 
If the size of the corresponding point 
set ( )fCPS  is greater than a 
pre-defined threshold (10 point pairs 
in our case), terminate and return the 
output; if not, go to Step.2. 
Step.3. Check if all point pairs have been done? 
If yes, return with “failure” and stop; if 
not, go to Step.1. 
 
6. Experiments 
Computer experiments using synthetic and real 
image data were conducted to validate our 
method. All the experiments were executed on a 
PC, running under the Windows XP operating 
system and featured with an AMD K-7 1.2G Hz 
CPU and 512 MB RAM. 
Experiment 1 for the image registration under 
the homography transformation  
Figs. 3(a) show a reference aerial image of size 
500 by 500, which is superimposed by the 
extracted Gabor-based feature points. The feature 
points are labeled with identification numbers and 
attached with arrows to indicate their principal 
orientations. Moreover, the numbers in the 
parentheses indicate their principal scales. The 
total number of the feature points is 22 points. A 
synthetic image with severe perspective 
deformation is generated to be the sensed images, 
as shown in Figs. 3(b). 
The first selected starting point pair happens to 
result in a good approximate transformation (0)T . 
The result is shown in the first row of Table 1. 
Then, the iterative algorithm ICPM refined the 
approximate transformation and terminated 
within two iterations, producing sixteen 
corresponding point pairs (CPS) and a small root 
mean square distance error of 0.75 pixels. Table 1 
also gives the two transformation matrices 
( )iT obtained at the two iterations i = 1 and 2. Fig. 
4 shows the convergence of the feature points and 
the boundaries of the reference image under the 
three spatial transformations (0)T , (1)T  and (2)T , 
respectively. The good overlapping between the 
two images implies that the homography 
estimation is correct. 
 
Table 1: The transformation parameters in three iterations 
3 3M ×    
11m  12m  13m  21m  22m  23m  31m  32m  33m  
(0)T  -0.6110 -1.4876 664.0500 1.0264 -0.8557 161.9824 0 0 1 
(1)T  -0.7977 -0.9341 674.8476 1.0080 -0.7286 173.6283 -0.0005 0.0018 1 
( 2)T  -0.7995 -0.9120 682.9316 1.0400 -0.7318 175.3778 -0.0005 0.0020 1 
 
Experiment 2 on image noise resistance  
To demonstrate the usefulness of the strategies 
2 and 3 in combating with image noise, we 
generate 100 noisy images by adding Gaussian 
noise to the reference image in Fig. 3(a), each 
with three different noise levels such that the 
signal-to-noise ratios are 9.7, 6.2 and 4.7 db, 
respectively. 
First, we compute the principal orientations of 
the feature points for the 100 noisy images. In Fig. 
5(a) the horizontal axis shows the ranking of the 
feature points according to the stability of the 
principal orientation ( )iO p , and the vertical axis 
shows the standard deviations of the principal 
orientations over the 100 noisy versions. The 
result reflects that the points with higher stability 
value of the principal orientation ( )iO p will result 
in small variation of principal orientation under 
the noise effect. Meanwhile, this ranking leads to 
the more stable triangles used to compute the 
approximate transformation in the noisy images. 
Next, we examine the energy change of the 
feature points under the noise effect. The solid 
line in Fig. 5(b) shows the energy value of the 
noiseless reference feature points, and the two 
short horizontal lines show the energy variation 
range of the feature points under the noise effect 
with a signal-to-noise ratio 6.2 dB. The result 
indicates that the feature points with higher 
energy remain strong with the addition of noise. 
Thus, they should be used in the image 
registration. Finally, we examine the points 
according to the index product of ( )kE p  and 
( )kO p . The result is shown in Fig. 5(c) whose 
annotation is same as Fig. 5(b). The result 
indicates that the points with larger index product 
have smaller variation 
Experiment 3 on partially overlapping images 
We use images of 3D building and images of 
2D landscape to demonstrate the capability of our 
method in handling the partial matching. Figs. 6(a) 
and 6(b) show the two building images 
superimposed with the information of the feature 
points for image registration. The left image 
serves as the reference image. As shown in Table 
2, the on-line registration process first takes a 
point pair from the sorted point pair list 6SPP  
corresponding to the sub-regions 3R  and 4R . The 
result shows that there is no matching point in the 
sensed image for the reference point No. 40 and 
No. 57. Therefore, any entries associated with the 
two points in the six 1SPP ~ 6SPP link lists are 
deleted, and the overlapping indices 1OI ~ 6OI  are 
updated. Next, the point pair from 1SPP  with the 
largest 1OI  value is fetched for the image 
registration. The process is repeated until the 
third point pair 18 38( , )p p  fetched from 5SPP  is 
used. This point pair ends up with a successful 
image registration with a final size of CPS being 
23. The execution time for on-line registration 
process is about 1.5 sec. Fig.7 shows the final 
registration result. Another experiment is to 
register three synthetic landscape images shown 
in Figs. 8(a)-8(c). The image in Figs.8 (a) is the 
reference image. Fig.9 shows the final 
registration result. 
 
Computational performance evaluation 
Table 3 gives the computational performance 
evaluation of the application of the five off-line 
planning strategies. We evaluate the performance 
through the comparison of the total number of 
point-pair selections (PS) and the total 
computational time (T) for finding the final image 
Table 2: The matching sequence in the registration process 
Overlapping index Sequence Fetch 
from 
Selected point pair 
label (sub-region)
Matching 
ambiguity
CPS 
size 1OI 2OI 3OI  4OI  5OI 6OI
1 6SPP  40 3( )R  57 4( )R 0 0 0 1 1 1 1 1 1 
2 1SPP  20 1( )R  7 2( )R  0 5 0 1 0.95 0.79 0.76 0.83 0.47
3 5SPP  18 2( )R  38 4( )R 1 6 23 0.8 0.75 0.67 0.76 0.83 0.47
 
 
 
(a) 
 
(b) (c) 
Fig. 8 Three landscape images used for image registration 
 
 
 
Fig. 9 The registration result 
 
 
 
Brazilian Symposium on Computer Graphics and 
Image Processing X , pp. 219 –226,1997. 
[17] J. Zhou and J. Shi, “A robust algorithm for feature 
point matching“, Computers & Graphics, vol. 26, 
pp. 429-436, 2002. 
[18] P. Bao and D. Xu, “Complex wavelet-based 
image mosaics using edge-preserving visual 
perception modeling”, Computer & Graphics, vol. 
23, pp. 309-321, 1999. 
[19] J. W. Hsieh, H. Y. Liao, K. C. Fan, M. T. Ko and 
Y. P. Hung, “Image Registration Using a New 
Edge-based Approach”, Computer Vision and 
Image Understanding, vol. 67, no. 2 pp. 112-130, 
Aug. 1997. 
[20] Q. Zheng and R. Chellappa, “A computational 
vision approach to image registration”, IEEE Tran. 
Image Processing, vol. 2, no. 3, pp. 311 -326, 
1993. 
[21] J. M. Chiu, Z. Chen, J. H. Chuang and T. L. Chia 
(1997), “Determination of feature 
correspondences in stereo images using a 
calibration polygon,” Pattern Recognition, vol. 10, 
no.9, pp. 1387-1400. 
[22] G. Lei, “Recognition of planar objects in 3-D 
space from single perspective views using cross 
ratio,” IEEE Trans. Robotics and Automation, Vol. 
8, No. 4, pp. 432-437, 1990. 
[23] Y. Liu and M. A Rodrigues, “Eliminating false 
matches in image registration through geometric 
histograms from reflected correspondence vectors”, 
Proc. IEEE Conf. Intelligent Robots and Systems, 
pp. 1997-2002, 2001. 
[24] P. Werth and S. Scherer, “Robust subpixel stereo 
matching by relaxation of match candidates”, Proc. 
Fist Int’l workshop on image and signal processing 
and analysis, pp. 14-15, 2000. 
[25] F. Ola and J. A. Marchant, “Matching feature 
points in image sequences through a region-based 
method”, Computer Vision and Image 
Understanding, vol. 66, no. 3, pp. 271-285, 1997. 
[26] H. Lamdan, J. T. Schwartz and H. J. Wolfson, 
“Affine invariant model-based object recognition”, 
IEEE Trans. Robotics and Automation, vol. 6, no. 5, 
pp.  578-589, Oct. 1990. 
[27] T. Suk and J. Flusser, “Point-based projective 
invariants”, Pattern Recognition, vol. 33 pp. 
251-261, 2000. 
[28] S. Irani , P. Raghavan, “Combinatorial and 
experimental results for randomized point matching 
algorithms”, Computational Geometry , vol. 12 , pp. 
17–31, 1999. 
[29] S. H. Chang, F. H. Cheng W. H. Hsu and G. Z. Wu, 
“Fast Algorithm for Point Pattern Matching: 
Invariant to Translation, Rotation and Scale 
Changes”, Pattern Recognition, vol.30, no.2, 
pp.311-320, 1997. 
[30] F. H. Cheng, “Point pattern matching algorithm 
invariant to geometrical transformation and 
distortion”. Pattern Recognition Letter, vol.17 
pp.1429-1435, 1996. 
[31] D. M. Mount, N. S. Netanyahu and J. L. Moigne, 
“Efficient algorithm for robust feature matching”, 
Pattern Recognition, vol.32, pp.17-38, 1999. 
[32] J. Flusser, “A moment-based approach to 
registration of images with affine geometric 
distortion”, IEEE Trans. Geoscience and Remote 
Sensing, vol. 32, no. 2, pp. 382-387, 1994. 
[33] E. De Castro and C. Morandi, ‘‘Registration of 
translated and rotated image using finite Fourier 
transform,’’ IEEE Trans. Pattern Analysis and 
Machine Intelligence, vol. 9, no. 5, pp. 700–703, 
Sept. 1987. 
[34] B. S. Reddy and B. N. Chatterji, “An FFT-based 
technique for translation, rotation and 
scale-invariant image registration”, vol. 5, no. 8, 
pp. 1266-,1271, 1996. 
[35] S. K. Sun, Z Chen and T. L. Chia, “Invariant 
feature extraction and object shape matching using 
Gabor filtering”, Lecture notes in computer science, 
Springer, vol. 2314, pp. 95-104, 2002. 
[36] Y. Ohta and T. Kanade, “Stero by intra- and 
inter-scanline search,” IEEE TPAMI, 7(2), pp. 
139-154, 1985. 
[37] S. Christy and R. Horaud, “Euclidean shape and 
motion from multiple perspective views by affine 
iterations”. IEEE TPAMI, Vol 18, N0, 11, pp. 
1098-1104,1996. 
[38] H. Aanas, R. Fisker, and K. Astrom, “Robust 
factorization,” IEEE TPAMI, Vol. 24, No. 9, pp. 
1215 -1225, 2002. 
[39] J.P. Costerira and T. Kanade, “A multibody 
factorization method for independently moving 
objects,” IJCV, 29(3), pp. 159-179, 1998. 
[40] R.C. Bolles and M.A. Fischler, “ A 
RANSAC-based approach to model fitting and its 
application to finding cylinders in range data,” 
Proc. International Joint Conf. on Artificial 
Intelligence, 1981. 
 
(transmission/broadcast)，以及三維呈現(3Ddisplay)。這些技術問題都是有別於傳
統 2D 電視系統，當然都是此次學術會議的重點內容。有關這些技術的標準規格
制定更是落實技術與商品化的重要工作。國際組織 MPEG 工作小組至今召開約
九十次 MPEG Meetings，徵求對 Multi-view Video Coding(MVC)，Joint Multi-view 
video Model(JMVM)I.O 等規格草案。 
  目前提出的 3DTV 系統主要有 Free-viewpoint TV(FTV) 及 3D Video 
System (3DTV)二種。 
(1) FTV System 
 
(2) 3DTV System 
 
會議現場有 philip 公司出品的一部九路 3D TV 監視器做實際展示。觀賞人可
以裸眼觀賞 3D 節目，只要觀賞人頭的搖擺不超過約 15 度，即可平順地看到生動
的 3D 節目，一旦頭超過上述角度，則 3D 節目會中斷，變成九路中另外一路的
3D 節目。 
本次會議最主要心得是有關 3DTV 系統的實現機會有多大? 有多快?我們知
道 3D 電玩遊戲現在已取代了 2D 電玩，故 3D 數位內容制作不再生疏。另外，
電影院戴紅藍立體眼鏡看立體電影也隨時可見。就數位內容而言，3D 能帶來震
撼的視覺效果，也更能看清影像的 3D 內容。實現機會是必然，但目前要克服的
問題是(1)不必戴立體眼鏡，(2)可以由任意視角觀視 3D 數位內容，(3)真正 3D 數
Correction 
/Conversion 
Capture 
3D Video 
Encoder 
Interpolation 
Depth 
Transmission 
/Storage 
Decoder 3D Display 
Correction 
Captured Images Correction 
Acquisition 
Camera Array 
Decoding 
Ray-Space 
Generation 
Free-Viewpoint Images 
