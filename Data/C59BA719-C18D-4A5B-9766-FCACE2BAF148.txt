computer graphics, even with the help of dedicated 
graphics hardware still lags behind those in offline 
rendering. Therefore one of the hot topics in 
computer graphics is to develop real-time global 
illumination programs to achieve both high-quality 
rendering and real-time interactive rates. 
Another recent trend is the growing popularity of 
smart phones such as Apple iPhone and Google Android 
handsets. With the rich multimedia contents on the 
smart phones, the demand for 3D graphics also 
increases. The availability of cloud computing 
service on the smart phones also shifts the focus of 
the application programs from the typical office 
applications to the entertainment applications. 
Therefore we take a leap of faith to predict that the 
real-time global illumination algorithms will be at 
the core of the rendering function in the future 
smart phones. 
The goal of this project is to develop advanced 
computer graphics functions, such as environment 
lighting and indirect illumination. We will also 
investigate how to adjust the rendering quality to 
meet the requirement of low power consumption. Our 
algorithm is based on the ray tracing method, which 
is much more flexible than the traditional GPU-based 
rasterization method in terms of adaptive rendering 
quality. We will also work with subprojects C and D 
to pursue heterogeneous computing on both multicore 
CPU and GPU using OpenCL. Another interesting topic 
to be investigated in this project is to utilize the 
cloud computing infrastructure, and to adjust the 
rendering quality based on the available network 
bandwidth. 
 
英文關鍵詞： computer graphics, global illumination, graphics 
processor, GPU, multicore processor 
 
目錄 
計畫中文摘要......................................................................................................................................................... 1 
計畫英文摘要......................................................................................................................................................... 2 
報告內容 ................................................................................................................................................................ 3 
1. 前言 ............................................................................................................................................................................. 3 
2. 研究目的 ..................................................................................................................................................................... 4 
3. 研究方法 ..................................................................................................................................................................... 5 
4. 結果與討論 ................................................................................................................................................................. 5 
參考文獻 .......................................................................................................................................................................... 8 
附錄 .................................................................................................................................................................................. 9 
 
 
 
 2
（二）計畫英文摘要 
The development of 3D computer graphics has been following two separate paths, one along 
the improvement of rendering quality and one along the speedup of rendering. The former leads to 
impressive results such as those in commercial 3D animation feature movies, but demands huge 
computational resources. The latter leads to the booming of the computer gaming industry and 
graphics processor industry. However the rendering quality in real-time computer graphics, even 
with the help of dedicated graphics hardware still lags behind those in offline rendering. Therefore 
one of the hot topics in computer graphics is to develop real-time global illumination programs to 
achieve both high-quality rendering and real-time interactive rates. 
Another recent trend is the growing popularity of smart phones such as Apple iPhone and 
Google Android handsets. With the rich multimedia contents on the smart phones, the demand for 
3D graphics also increases. The availability of cloud computing service on the smart phones also 
shifts the focus of the application programs from the typical office applications to the entertainment 
applications. Therefore we take a leap of faith to predict that the real-time global illumination 
algorithms will be at the core of the rendering function in the future smart phones. 
The goal of this project is to develop advanced computer graphics functions, such as 
environment lighting and indirect illumination. We will also investigate how to adjust the rendering 
quality to meet the requirement of low power consumption. Our algorithm is based on the ray 
tracing method, which is much more flexible than the traditional GPU-based rasterization method 
in terms of adaptive rendering quality. We will also work with subprojects C and D to pursue 
heterogeneous computing on both multicore CPU and GPU using OpenCL. Another interesting 
topic to be investigated in this project is to utilize the cloud computing infrastructure, and to adjust 
the rendering quality based on the available network bandwidth. 
 
Keywords: computer graphics, global illumination, graphics processor, GPU, multicore 
processor
 4
在近年來頗為熱門的雲端計算架構下，行動裝置的角色逐漸朝功能簡易輕薄的方向發
展。若干業者(如 OnLive[10]、NVIDIA[9]等)也提出將產生進階繪圖效果的功能移至雲端伺服
器的想法。然而這類的作法必須大量依賴充分網路頻寬的支援，在實用性上的可行性尚未成
熟。另外，我們也認為在可預期的未來，人機圖形介面仍將於行動裝置上扮演重要角色，對
進階繪圖功能的需求也將日益提高。因此本子計畫研究的另一個議題，則是如何在配合有限
網路的頻寬下，適時的將繪圖計算量在使用者端(client)和雲端伺服器端(server)之間調配。 
 
2、 研究目的 
本計畫的目的在於發展多核心架構上的圖學(computer graphics)子系統及應用程式，並在
考量嵌入式系統的低功率或低成本的條件下，提供一個在畫面品質和效能上超越現有行動裝
置上(如 Apple iPhone)的硬體加速方案。值得一提的是我們將脫離移植 GPU 上 OpenGL 或
DirectX 的傳統 graphics pipeline 至多核心處理器的思維，而是採用 real-time ray tracing 的想
法，以發揮多核心架構的優勢。 
本計畫主要有三個工作項目： 
(1) 在多核心架構上的即時全域照明(real-time global illumination)程式發展。 
(2) 開發可依低功率需求或網路連線品質調適的圖學演算法。 
(3) 研究適用於雲端計算架構的圖學應用程式。 
計畫的第一年，我們首先將著手於進階全域照明演算法的研究和開發，特別是分析程式
中的分支分歧(branch divergence)程度和記憶體存取的模式(memory access pattern)，以提供子
計畫 C 和子計畫 D 作為設計編譯工具(compiler)和硬體架構時的依據。 
計畫的第二年，我們將著重於系統設計時期的相關議題探索(Design Time Exploration)。
我們將配合子計畫 C 和子計畫 D，詳細比較以 SIMD 方式、MIMD 方式、或是採取一種混
合式架構來支援可適性高的光線追蹤繪圖技術，並以實作來驗證所選取的架構。在評估過程
中，應用程式撰寫的生產力也應納入考量。譬如同質多核心搭配 Neon SIMD 的做法因為使
用同樣的記憶層，比較容易規劃程式，也比較容易編譯和優化.。 
計畫的第三年，我們將著重於程式執行時期的相關議題探索(Run Time Exploration)。由於
在程式執行時，我們可由子計畫 B 的耗電量測器取樣來了解程式的耗電情形。耗電監管系統
掌控系統的用電實況，並將收集的資訊傳送給運行管理系統 (Runtime management / 
optimization system)，由運行管理系統決定是否須要作調整(作程式碼的微調或是選擇不同的
程式碼)。運行管理系統除了作耗電節能的管理，也對效能提昇進行監控。它可由效能量測
器取樣來了解程式的效能概況，經由選擇不同版本的程式碼或經由程式碼微調來減少耗電或
增進執行效能.從程式撰寫開始，應用程式開發者就可以敘述在儲電不足，或通訊頻寬受限
時，該如何降低輸出圖形的品質，來維持一個較平順的用戶使用經驗。或是在通訊頻寬充裕
時，如何與雲端配合達到高解析，低耗能的理想績效。 
 
 
 
 
 
 
 
 6
 
下圖是上述方法與相關技術的比較，圖中呈現的是在不同取樣數(橫軸)情況下，所殘餘的
雜訊比較(縱軸)，我們所提出的方法和常用的 bilateral filtering 比較，顯然有極大的改善。 
 
我們提出的方法有兩項特色，第一是該方法並不僅限於靜態場景，在動態場景也可適用，
下圖即為物件移動時的成果: 
 
 
 8
參考文獻： 
[1] Bavoil, L., Sainz, M., and Dimitrov, R. 2008. Image-space horizon-based ambient occlusion. 
In ACM SIGGRAPH 2008 Talks (Los Angeles, California, August 11 - 15, 2008). 
SIGGRAPH '08. 
[2] Chen, Y-C., Chang, C-F., Ma, W-C. Asynchronous Rendering. In Posters and Demos of the 
2010 Symposium on interactive 3D Graphics and Games (Washington, District of Columbia, 
February 19 - 21, 2010). I3D '10. 
[3] Ikrima Elhassan. An Analysis Of GPU-based Interactive Raytracing. The University of Texas 
at Austin, Department of Computer Sciences. Honor thesis, June 12, 2006. 
[4] Kautz, J., Sloan, P., and Lehtinen, J. 2005. Precomputed radiance transfer: theory and practice. 
In ACM SIGGRAPH 2005 Courses (Los Angeles, California, July 31 - August 04, 2005). 
[5] Keller, A. 1997. Instant radiosity. In Proceedings of the 24th Annual Conference on Computer 
Graphics and interactive Techniques International Conference on Computer Graphics and 
Interactive Techniques. 
[6] Kontkanen, J. and Laine, S. 2005. Ambient occlusion fields. In Proceedings of the 2005 
Symposium on interactive 3D Graphics and Games (Washington, District of Columbia, April 
03 - 06, 2005). I3D '05. 
[7] William R. Mark and Donald Fussell. Real-Time Rendering Systems in 2010. The University 
of Texas at Austin, Department of Computer Sciences. Technical Report TR-05-18, May 2, 
2005. 
[8] NVIDIA CUDA. http://developer.nvidia.com/object/cuda.html 
[9] NVIDIA RealityServer. http://www.nvidia.com.tw/object/realityserver_tw.html 
[10] OnLive. http://www.onlive.com 
[11] Owens et al.  A Survey of General-Purpose Computation on Graphics Hardware.  In 
Graphics Forum 2007, Volume 26. 
[12] T D. Pham et al., The Design and Implementation of a First-Generation CELL Processor. 
ISSCC 2005. 
[13] Purcell T. J.: Ray Tracing on a Stream Processor. PhD thesis, Stanford University, Mar. 2004. 
[14] Ritschel, T., Grosch, T., and Seidel, H. 2009. Approximating dynamic global illumination in 
image space. In Proceedings of the 2009 Symposium on interactive 3D Graphics and Games 
(Boston, Massachusetts, February 27 - March 01, 2009). I3D '09. 
[15] Seiler, L., Carmean, D., Sprangle, E., Forsyth, T., Abrash, M., Dubey, P., Junkins, S., Lake, A., 
Sugerman, J., Cavin, R., Espasa, R., Grochowski, E., Juan, T., and Hanrahan, P. 2008. 
Larrabee: a many-core x86 architecture for visual computing. In ACM SIGGRAPH 2008 
Papers (Los Angeles, California, August 11 - 15, 2008). SIGGRAPH '08. 
[16] Anand Lal Shimpi. Understanding the Cell Microprocessor. 
http://www.anandtech.com/cpuchip sets/showdoc.aspx?i=2379 
[17] Whitted, T.. An improved illumination model for shaded display. Communications of the 
ACM 23, 6 (June 1980), 343–349. 
 
DOI: 10.1111/j.1467-8659.2012.02094.x COMPUTER GRAPHICS forum
Volume 31 (2012), number 1 pp. 189–201
Spatio-Temporal Filtering of Indirect Lighting for Interactive
Global Illumination
Ying-Chieh Chen1, Su Ian Eugene Lei1 and Chun-Fa Chang2
1National Tsing Hua University, Taiwan
2National Taiwan Normal University, Taiwan
{louis, zenith}@ibr.cs.nthu.edu.tw, chunfa@ntnu.edu.tw
Abstract
We introduce a screen-space statistical filtering method for real-time rendering with global illumination. It is
inspired by statistical filtering proposed by Meyer et al. to reduce the noise in global illumination over a period of
time by estimating the principal components from all rendered frames. Our work extends their method to achieve
nearly real-time performance on modern GPUs. More specifically, our method employs the candid covariance-
free incremental PCA to overcome several limitations of the original algorithm by Meyer et al., such as its high
computational cost and memory usage that hinders its implementation on GPUs. By combining the reprojection
and per-pixel weighting techniques, our method handles the view changes and object movement in dynamic scenes
as well.
Keywords: global illumination, real-time rendering, statistical filtering
ACM CCS: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Raytracing
1. Introduction
The rendering of indirect lighting is generally a time-
consuming process. One of the primary challenges is to
solve the incoming radiance integral of the rendering equa-
tion [Kaj86] in a complex environment. In practice, the ra-
diance integral is evaluated using Monte Carlo integration.
Many different rendering methods such as photon mapping
[Jen96] and path tracing [LW93] use this method. However, it
is computationally expensive to generate a visually plausible
result without excessive noise.
Most real-time ray tracing techniques use geometry-aware
filtering [RGK*08, RGS09, YSL08] within a single frame to
reduce noise or flickering. However this does not negate the
fact that its rendering quality suffers from the low number
of initial samples. Progressive approaches [HOJ08, HJ09,
DWWL05] can improve the rendering quality over time,
but must be recomputed completely if the lighting condition
or viewing angle changes, leading to incoherency between
frames.
Statistical filtering [MA06] improves the quality of under-
sampled ray tracing in a sequence of offline rendered images,
using principal component analysis (PCA). Here the princi-
pal components represent the lighting change of the entire
sequence. By adding cross-frame analysis, this approach re-
duces the noise not only in the spatial domain, but also in
the temporal domain. We present a framework for adapting
the statistical filtering to interactive or real-time applications
that use graphics hardware acceleration. Our framework can
amortize the lighting calculation among a number of frames
without sacrificing the quality. In essence, we provide an in-
cremental approach for global illumination that can maintain
coherency in an interactive and dynamic environment.
Unlike a pre-rendered sequence, real-time changes of the
scene in an interactive application are unknown before-
hand. Therefore the PCA process must be approximated
incrementally. We use a screen-space approach to avoid
redundant calculation of lighting in a dynamic scene. By
using Candid covariance-free incremental PCA (CCIPCA)
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA. 189
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 191
an undersampled animation sequence. They then remove the
noise by projecting these noisy renderings onto the basis
functions. However, their method mainly focuses on filtering
a sequence of offline rendered images. The PCA process is
costly in both time and memory, especially when processing
larger images (which leads to high-dimensional data).
3. Statistical Filtering
In [MA06], an image sequence is approximated by a
weighted sum of bases using principle component analysis
(PCA):
ˆI(x, t) =
N∑
i=1
wi(t)Bi(x), (1)
where ˆI(x, t) is the noisy image sequence that is rendered
with a low number of sampling rays, x is the pixel loca-
tion and t is time. N is the number of basis functions and
Bi(x) are the basis functions computed by PCA. In a PCA
constructed basis, the lower numbered functions capture the
slowly varying components of the signal. Because indirect
lighting is generally of low-frequency, it is mostly contained
in the lower numbered basis functions although the higher
numbered bases mainly capture the noise of the original se-
quence. Therefore if we choose a subset of the PCA basis
functions, we can project the noisy image sequence onto this
truncated set of bases to produce a smoother image sequence.
[MA06] refers to this process as statistical filtering.
3.1. Screen-space statistical filtering
As mentioned in previous sections, our method is inspired
by [MA06] that aims at producing flicker-free animation al-
though using the Monte Carlo techniques to calculate global
illumination. However their method [MA06] has the fol-
lowing limitations: First, it requires all training images to
be available before the principal components are estimated,
which is only feasible with offline rendering of an anima-
tion sequence. Secondly, PCA requires a large covariance
matrix, which reaches O(n2) entries for an image of size n.
Furthermore, the matrix must be completely recalculated if
a new image frame is added to the sequence. It consumes
large amount of memory and is computationally expensive.
Thirdly, previous methods extend the sample points from
image space to object space to support dynamic scenes and
moving cameras, which may expose previously occluded
points. This is to cope with the constraint that traditional
PCA does not allow unknown elements in the input samples.
However the performance and quality may suffer if a large
portion of these sample points ends up being occluded or
outside of the camera view.
Our method seeks to improve the rendering quality of
indirect lighting when a low number of sampling rays are
used, which is typical of interactive or real-time applications.
Figure 2: System overview.
The large covariance matrix in PCA and the performance hit
from the additional object space samples to support dynamic
scenes are both undesirable in real-time applications. There-
fore we take an incremental approach that can estimate the
principle components, and also cope with the problem of
potential missing data because of camera movement and dy-
namic objects.
CCIPCA [WZsH03], a method for incrementally approxi-
mating PCA, offers a solution to update the PCA bases with-
out having to access all previous samples. Its covariance-free
property also makes it memory-efficient. We use a combi-
nation of CCIPCA [WZsH03], reprojection and per-pixel
weighting techniques to approximate the missing data be-
tween frames for CCIPCA input. Therefore our method can
support dynamic lighting and moving objects with a short
convergence time.
Figure 2 shows our framework and it works as follows:
• Warp the basis functions from the previous frame using
the reprojection map provided by the renderer.
• Update the basis functions by applying incremental PCA
on them.
• Project the noisy image onto the updated bases and re-
construct the noise-free image.
In the following sections, we elaborate on these three steps
(incremetal PCA, reconstruction and reprojection) in more
detail.
3.2. Incremental PCA
The first eigenvector: The basic idea of CCIPCA is to update
the PCA bases iteratively. This approach processes the input
data incrementally and avoids using the covariance matrix.
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 193
Figure 4: Pixel movement and dimension changing of PCA
basis during the reprojection.
3.4. Reprojection for dynamic scenes
After camera or object movement, the same point may project
into different pixels in consecutive frames. This significantly
complicates the analysis as mentioned in [MA06]. To fo-
cus on the lighting change, Meyer et al. use the same set of
object space positions in every frame as PCA inputs to anal-
yse the lighting change at these points. To achieve the same
result in screen-space, our approach uses the reverse projec-
tion method [NSL*07] to obtain the relationship of pixels
between two consecutive frames. With this relationship, we
warp the PCA basis functions from the previous frame to the
current frame. Figure 4 shows an example of the pixel and
dimension movement on image space and on the PCA basis
functions.
However, this reprojection causes two problems to hinder
the convergence of CCIPCA—non-orthogonal basis func-
tions that are caused by adding new dimensions, and an
inconsistent number of observed input samples for each di-
mension. These two problems have negative impact on the
convergence process of CCIPCA. The following subsections
explain how we modify CCIPCA to overcome them.
3.4.1. Basis functions orthogonalization
In the reprojection process, there are three possible scenar-
ios that image pixels are related between consecutive frames
(see Figure 4): a new pixel (one without a corresponding
old pixel) that appears on the next frame, an old pixel that
disappears in the next frame or a pixel that changes its posi-
tion between two frames. To complete the reprojection, three
operations (add, delete, swap) may be performed on these
pixels and their corresponding PCA bases. Pixels appearing
or disappearing correspond to the add and delete operations,
respectively. In the case of pixel position change (for exam-
ple, from p to q), two operations would be applied in this
sequence: swap(p,q) and delete(p). Unfortunately, accord-
ing to the properties of PCA, any two dimensions of all basis
functions are interchangeable (swap), but arbitrarily remov-
ing or adding dimensions would compromise the mutually
orthogonal property between basis functions. Because each
pixel corresponds to a dimension in the PCA basis, the add
and delete operations will dramatically prolong the conver-
gence time, even though CCIPCA can eventually correct the
non-orthogonal problem by its iterative refinement process.
We modified the CCIPCA algorithm to orthogonalize the
warped PCA bases by applying Algorithm 2 to these basis
functions. After that, the bases are mutually orthogonal. The
algorithm simply disables the basis updating steps (Line 7
and 8) of the original Algorithm 1. Because it requires no
extra memory or complex computation, it is efficient on our
GPU implementation.
Algorithm 2: Orthogonalization algorithm
1: for all u(n) do
2: for i = 1min(k, n) do
3: ui (n) = u(n)
4: if i == n then
5: vi = ui (n)
6: else
7: ui (n) = ui (n) − (ui (n) · vi ) vi||vi ||2
8: end if
9: end for
10: end for
3.4.2. Per-pixel weighting
After reprojection, we may have some new pixels that did
not appear in the previous frame. Recall that each pixel cor-
responds to a dimension in the PCA basis functions, and
these new incoming dimensions have fewer observed data
than the others. The original CCIPCA algorithm treats every
dimension equally by applying a uniform weighting during
the converging process. In Section 3.2, we know this weight
affects the step size of the estimated basis moving towards
the real basis, as shown in Figure 3. The convergence speed
of these younger dimensions will be negatively affected by
the older ones, because the younger ones need a larger step
than those limited by the older ones. To solve this prob-
lem, we introduce per-pixel weighting to replace the uniform
weighting in CCIPCA. With this modification, the younger
dimensions obtain larger weights although updating their ba-
sis functions, thus converging to the target faster (even with
non-optimal initial guess). This modification does not affect
the result of CCIPCA, because the weight difference will be
reduced gradually over time. Figure 5 shows effectiveness
of our per-pixel weighting method. After the wall is disoc-
cluded from the ball, the single-weight method (bottom row)
needs about 50 iterations to reach the image quality that is
achieved by the new per-pixel weighting method (top row)
after merely 5 iterations.
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 195
Figure 6: Ghosting artefacts and a comparison of our removal approach. (a) Artefacts-free reference image. (b) Ghosting
artefacts before removal. (c) Basis filtering. (d) Adaptive sampling (120 samples/pixel at newly unoccluded area). (e) Adaptive
sampling and basis filtering combined.
Table 1: Performance of our method (in milliseconds) at various
resolutions, using 3, 5 and 7 basis functions. The number in paren-
thesis of the first column is the number of the basis functions used in
IPCA.
Res.(Basis) Reproj. IPCA Recon. Total
512 × 512 (3) 1.7 5.0 0.18 6.9
512 × 512 (5) 2.2 5.9 0.26 8.4
512 × 512 (7) 2.7 6.9 0.36 10.0
640 × 480 (3) 1.7 8.1 0.22 10.1
640 × 480 (5) 2.4 9.2 0.31 11.9
640 × 480 (7) 3.0 10.3 0.42 13.7
768 × 768 (3) 2.5 11.0 0.39 13.9
768 × 768 (5) 3.4 12.9 0.57 16.9
768 × 768 (7) 4.4 14.8 0.79 20.0
1080 × 720 (3) 3.0 14.2 0.51 17.6
1080 × 720 (5) 4.0 16.7 0.73 21.4
1080 × 720 (7) 5.3 19.2 0.97 25.5
1024 × 1024 (3) 3.8 19.5 0.69 24.0
1024 × 1024 (5) 5.3 22.9 1.03 29.2
1024 × 1024 (7) 6.8 26.4 1.42 34.6
unit: ms
using 3 sampling rays per pixel although other methods gen-
erally require 32. The cost for rendering indirect lighting
under various sampling density is shown in Table 2. Con-
sidering the time it takes to render 3 and 32 rays per pixel
(46.3 ms and 277 ms, respectively in 512 × 512), ultimately
our method can save up to 5/6 of rendering time in indirect
lighting.
4.2. Flickering elimination
Our method provides temporal coherence between frames
where geometry-aware filtering cannot. Figure 7 shows the
comparison of our approach and other filtering-based meth-
ods. We measure the flickering using the mean squared dif-
ference between consecutive frames. The flickering becomes
barely notable when the measured difference is less than
2.5 ∗ 10−5. Without temporal coherence, the geometry-aware
Table 2: The rendering time of rendering indirect lighting image
with varying number of samples at different resolutions.
Indirect Lighting Rendering Time
# samples per pixel
Resolution 3 10 32 64
512 × 512 46.3 141.4 277.0 694.4
640 × 480 58.3 147.1 288.2 724.6
768 × 768 109.3 302.1 609.8 1562.5
1080 × 720 116.1 312.5 625.0 1587.3
1024 × 1024 186.9 526.3 1063.8 2777.8
unit: ms
filters (joint bilateral filters) with window sizes of 11× 11 and
17 × 17 require more than 64 and 32 sample rays per pixel,
respectively, to reach this threshold.
The spatio-temporal up-sampling method proposed by
[HEMS10] performs better than geometry-aware filters when
the number of samples per-pixel is low, but still suffers
from the flickering problem when the lighting changes
rapidly. Their approach generates the final rendering us-
ing the weighted sum of 16 previous frames. The weight
is assigned using the colour difference between consecutive
frames (greater weight for similarly coloured pixels). When
the lighting changes rapidly, so does the colour. This leads to
a very low weight for the affected pixels, meaning most of the
previous data would be discarded. Therefore the final filter-
ing result would be less desirable because most weight would
be assigned to the current frame, which is undersampled. On
the other hand, our approach is nearly flickering-free under
similar conditions, even with few samples per pixel (3 in our
experiments).
4.3. Image quality
To show how statistical filtering helps improving our ren-
dering, we compare our results to geometry-aware filtering,
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 197
Figure 9: (a) Is our reference of indirect lighting, using 2048 sample rays per pixel. (b),(c) and (d) are the results of our method,
the method in [HEMS10] and the geometry-aware filtering method. (e) shows one of the noisy input images (3 sample rays per
pixel). (f), (g) and (h) show the difference images (scaled by a factor of 5). Although all methods demonstrate some error at the
edges, our method provide a better quality at the low frequency areas such as the ground and the whiteboard. Others have spots
of error at these areas.
Figure 10: Mean squared errors between a ground truth
(rendered using 8192 rays per pixel) and various filtering
methods. Green: [HEMS10]; Blue: joint bilateral filtering
with window size of 11 × 11; Red: 17 × 17; Purple: our
method.
irrelevant information (such as noises) is filtered away in
the projection step. Although our statistical method is more
complicated in implementation, our method does not suffer
from undersampling because of the loss of older information.
Therefore, our approach requires a much lower number of
samples in the original rendering. Because indirect lighting
is still the bottleneck of real-time global illumination, we can
improve the overall rendering speed with no loss in quality by
reducing the time we needed to generate the indirect lighting
samples.
5.2. Comparison with offline statistical filtering
Previous work in statistical filtering [MA06] has shown that
PCA and its reconstruction can work well on indirect lighting.
However, applying statistical filtering to real-time rendering
requires significant engineering effort. Assume that the image
size is N and the first M basis functions are (e.g. M is 8 in
[MA06]), the memory requirement for the traditional PCA
and the CCIPCA are O(N 2) and O( N × M), respectively.
The memory requirement makes PCA unsuitable for GPU
impementation. Another reason why the PCA is not suitable
for real-time application is that it needs a full recomputation
of bases when new input is added to the working set. As stated
earlier, our choice of CCIPCA is essential for achieving the
real-time performance. Our GPU-based implementation of
CCIPCA further improves its performance. For example, our
GPU implementation of CCIPCA takes 7.4 ms to estimate 8
eigenvectors for each 512 × 512 input image, although the
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 199
Figure 13: Left: the kitchen scene rendered with our method.
The upper-right image shows the direct lighting and the
lower-right shows the indirect lighting.
Figure 14: This figure shows our method also works with
environment lighting (from a cubemap).
basis functions, our method would fail to reconstruct those
lighting effects. We could choose to use more basis functions
to cover wider frequency range, but this would have nega-
tive impact on performance and might produce more noisy
results.
Because our method is a statistical approach, our modified
IPCA requires a period of time (about 10 iterations in our
experiments) to collect sufficient information and to reach
convergence of IPCA. However, if a rapidly moving or de-
forming object stays within a general area of the screen (such
as wobbling), that areas may never reach convergence, thus
leading to visible artefacts. Therefore our method has also a
limitation in handling certain types of dynamic scenes.
Figure 15: The first 4 scaled eigenimages obtained by PCA
(top row) and CCIPCA (bottom row) after 70 epochs. The
images reconstructed with the basis functions of PCA and
CCIPCA have similar quality. (The intensity range has been
rescaled for visualization.)
6. Conclusion and Future Work
We have presented a framework for improving the quality
of real-time indirect lighting by denoising in the tempo-
ral domain. Our approach can generate nearly noise-free
and flickering-free results using a much lower number of
sampling rays comparing to geometry-aware filtering (3–5
rays vs. 32 or more rays in our experiments). The incre-
mental PCA approach makes our method GPU-friendly and
memory-efficient. The reprojection techniques make it pos-
sible to support dynamic scene and camera movement. How-
ever our rendering speed is still limited by the renderer that
produces the raw input images. A more efficient and sophis-
ticated renderer can further benefit from our algorithm in
terms of performance and image quality.
We have successfully eliminated most of the artefacts dur-
ing the reprojection phase by applying adaptive sampling
and basis filtering. In some extreme cases of dynamic scenes
such as those with rapidly deforming objects, our artefact
removal mechanism could potentially be overwhelmed and
some ghosting artefacts may become notable. We hope to
further minimize this type of artifact in the future by dy-
namically adjusting the parameters of adaptive sampling and
basis filtering.
Acknowledgments
We thank the anonymous reviewers for the insightful com-
ments, and Hsiang-Ting Chen for proofreading. This work
is supported in part by National Science Council (Taiwan)
under grant NSC 100-2219-E-003-002.
REFERENCES
[CPC84] COOK R. L., PORTER T., CARPENTER L.: Distributed
ray tracing. SIGGRAPH Computer Graphics 18, (January
1984), 137–145.
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
Y.-C. Chen et al. / Spatio-temporal Filtering of Indirect Lighting 201
approach to illumination. ACM Transactions on Graphics
24, (July 2005), 1098–1107.
[WH08] WARD G. J., HECKBERT P. S.: Irradiance gradi-
ents. In ACM SIGGRAPH 2008 Classes (New York,
NY, USA, 2008), SIGGRAPH ’08, ACM, pp. 72:1
–72:17.
[WRC88] WARD G. J., RUBINSTEIN F. M., CLEAR R. D.: A ray
tracing solution for diffuse interreflection. In Proceedings
of the 15th Annual Conference on Computer Graphics
and Interactive Techniques (New York, NY, USA, 1988),
SIGGRAPH ’88, ACM, pp. 85–92.
[WZsH03] WENG J., ZHANG Y., sHIUAN Hwang W.: Candid
covariance-free incremental principal component analy-
sis. IEEE Transactions on Pattern Analysis and Machine
Intelligence 25 (2003), 1034–1040.
[XP05] XU R., PATTANAIK S.: A novel monte carlo noise
reduction operator. Computer Graphics and Applications,
IEEE 25, 2 (March–April 2005), 31–35.
[YSL08] YANG L., SANDER P. V., LAWRENCE J.: Geometry-
aware framebuffer level of detail. Computer Graphics
Forum (Proc. of Eurographics Symposium on Rendering
2008) 27, 4 (2008), 1183–1188.
[ZW01] ZHANG Y., WENG J.: Convergence analysis of com-
plementary candid incremental principal component anal-
ysis. Tech. Rep., Computer Science Engineering, Michi-
gan State University, East, 2001.
c© 2012 The Authors
Computer Graphics Forum c© 2012 The Eurographics Association and Blackwell Publishing Ltd.
 出席國際學術會議報告 
 
報 告 人 
姓 名 
張鈞法 
服 務 機 構 
及 職 稱 
台灣師範大學資工
系副教授 
會 議 時 間 
地 點 
2012/06/25~2012/06/28 
捷克 Pilsen 市 
會議名稱 
20th International Conference on Computer Graphics, Visualization and 
Computer Vision (WSCG) 2012 
發表論文題目 Subpixel Reconstruction Antialiasing for Ray Tracing 
 
一、參加會議經過 
 
此屆會議是由University of West Bohemia & VSB-Technical University聯合主辦，
Eurographics Association 協辦。主要目的是提供世界各地在電腦圖學及電腦視覺學者的
專家交換研究發展心得，今年為第二十屆。 
 
今年會議於6月25日至6月28日在捷克Pilsen城市的Primavera Hotel and Congress 
Centrum 飯店暨會議中心舉行。Pilsen 附近較知名的城市是布拉格(Prague)，距離 Pilsen
大約兩小時的車程。由於此會議論文接受通知時程較晚，我於 5月 10日獲知論文錄取
即開始規畫旅程，但當時由台北飛往捷克或附近的維也納、慕尼黑等機場的班機皆已
客滿，只能接受候補，因此我最後排上的機位是 6月 23日飛往維也納，7月 1日 回程
的長榮班機(7月 1日以前回程的班機仍然客滿)。我於 6月 23日晚上搭乘長榮 BR61 班
機於台北出發，24日抵達維也納機場，於維也納過夜後於 25 日改搭火車前往布拉格，
再於布拉格轉搭另一班火車前往 Pilsen，於 25 日下午到達 Pilsen 市的會場所在地
Primavera Hotel，隨即辦理會議的報到和註冊。 
 
我於 6月 26日至 28日參加會議，並於 27 日上午 Session H發表論文“Subpixel 
Reconstruction Antialiasing for Ray Tracing”，於 7月 1日下午搭乘火車回到維也納，再
搭乘長榮 BR62班機離開維也納，並於 7月 2日凌晨抵達台灣。 
  
二、與會心得 
  
WSCG2012第一天會議先由兩個平行sessions開始，Session A是五篇full papers，另
一個Session B則是五篇communication papers。接下來由大會主席University of West 
Bohemia的Vaclav Skala致簡短的歡迎詞開幕後，即進行第一場的keynote speech，主講
者是目前任教於Max-Planck-Institut für Informatik的Karol Myszkowski教授，主題是
三、攜回資料名稱及內容 
 
WSCG2012會議議程及論文集光碟片一片。 
 
四、結語 
 
非常感謝國科會提供補助，使得我得以成行。也使得我們有機會與國外同領域的
學者展示我們的研究成果和交換computer graphics技術發展及研究的心得。 
 
 
 
Subpixel Reconstruction Antialiasing for Ray Tracing
Chiu, Y.-F
National Tsing Hua
University, Taiwan
yfchiu@ibr.cs.nthu.edu.tw
Chen, Y.-C
National Tsing Hua
University, Taiwan
louis@ibr.cs.nthu.edu.tw
Chang, C.-F
National Taiwan
Normal University,
Taiwan
chunfa@ntnu.edu.tw
Lee, R.-R
National Tsing Hua
University, Taiwan
rrlee@cs.nthu.edu.tw
ABSTRACT
We introduce a practical antialiasing approach for interactive ray tracing and path tracing. Our method is inspired
by the Subpixel Reconstruction Antialiasing (SRAA) method which separates the shading from visibility and ge-
ometry sampling to produce antialiased images at reduced cost. While SRAA is designed for GPU-based deferred
shading renderer, we extend the concept to ray-tracing based applications. We take a hybrid rendering approach
in which we add a GPU rasterization step to produce the depth and normal buffers with subpixel resolution. By
utilizing those extra buffers, we are able to produce antialiased ray traced images without incurring performance
penalty of tracing additional primary rays. Furthermore, we go beyond the primary rays and achieve antialiasing
for shadow rays and reflective rays as well.
Keywords: antialiasing, ray tracing, path tracing.
1 INTRODUCTION
With the abundance of computation power and paral-
lelism in multicore microprocessors (CPU) and graph-
ics processors (GPU), achieving interactive photoreal-
istic rendering on personal computers is no longer a
fantasy. Recently, we have seen the demonstration of
real-time ray tracing [6, 17] and the emergence of real-
time path tracing with sophisticated global illumination
[2, 20]. Though real-time path tracing can produce
rendering of photorealistic quality that include com-
plex lighting effects such as indirect lighting and soft
shadow, the illusion of a photograph-like image breaks
down quickly when jaggy edges are visible (Figure 1
shows an example).
Jaggy edges are one of the typical aliasing artifacts
in computer generated images. A straightforward an-
tialiasing technique is to increase the sampling rate by
taking multiple samples uniformly at various subpixel
positions. However this approach induces significant
performance penalty that makes it an afterthought in
real-time ray tracing. A more practical approach is
to increase subpixel samples adaptively for image pix-
els where discontinuity is detected. Although adaptive
sampling approach avoids the huge performance hit of
the multisampling approach, it still requires additional
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for profit
or commercial advantage and that copies bear this notice and
the full citation on the first page. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.
subpixel samples and introduces large variation to the
estimation of rendering time.
In this work, we introduce an antialiasing approach that
works well for real-time ray tracing and path tracing.
We take a hybrid rendering approach in which we add a
GPU rasterization step to produce the depth and normal
buffers with subpixel resolution. By utilizing those ex-
tra buffers, we are able to produce antialiased ray traced
images without incurring performance penalty of trac-
ing additional primary rays. Our method is inspired by
the Subpixel Reconstruction Antialiasing (SRAA) [3]
which combines per-pixel shading with subpixel vis-
ibility to produce antialiased images. While SRAA
is designed for GPU-based deferred shading renderer,
we extend the concept to ray-tracing based applica-
tions. Furthermore, we apply our antialiasing approach
to shadow and reflection which SRAA cannot resolve
with its subpixel buffers.
Our main contributions in this work are:
• We propose an efficient antialiasing technique which
improves the perception of photorealism in interac-
tive or real-time ray tracing without sacrificing its
performance.
• Unlike adaptive sampling or subpixel sampling, our
approach does not penalize the performance of a
CPU ray tracer because no additional primary ray
needs to be traced. Our hybrid rendering approach
obtains the necessary subpixel geometric informa-
tion by leveraging the GPU rasterization pipeline.
• While SRAA works well for improving the sam-
pling on image plane, we extend its application be-
yond the primary rays and achieve antialiasing for
shadow rays and reflective rays as well.
upscale shading information using subpixel geometric
information that is obtained from the GPU rasterization
pipeline. It is based on the assumption that the subpixel
geometric information could be obtained much more
easily without fully going through the expensive shad-
ing stage. SRAA can produce good edge antialiasing
but it cannot resolve shading edges in texture, shadow,
reflection and refraction. Our work follows the same
assumption by avoiding emitting subpixel samples for
the primary rays. This maintains the advantage over
adaptive sampling because no subpixel ray needs to be
traced.
3 ANTIALIASING
SRAA [3] relies on the fact that shading often changes
more slowly than geometry in screen space and gen-
erates shading and visibility at different rates. SRAA
performs high-quality antialiasing in a deferred render-
ing framework by sampling geometry at higher reso-
lution than the shaded pixels. It makes three modifi-
cations to a standard rendering pipeline. First, it must
produce normal and depth information at subpixel res-
olution. Second, it needs to reconstruct the shading val-
ues of sampled geometric subpixel from neighboring
shaded samples with bilateral filter using the subpixel
geometric (normal and depth) information. Finally, the
subpixel shading values are filtered into an antialiased
screen-resolution image.
SRAA detects the geometric edges with geometric in-
formation to resolve aliasing problem. However, the
edges of shadow and reflection/refraction could not be
detected by the subpixel geometric information gener-
ated from the eye position. For example, the shadow
edges mostly fall on other continuous surfaces that have
slowly changing subpixel depths and normals. To ex-
tend the SRAA concept to ray-tracing based applica-
tions, we perform antialiasing separately for primary
rays, shadow rays and secondary rays to resolve this
issue. The following subsections offer the detail.
3.1 Primary Ray
Like SRAA, our goal is to avoid the performance
penalty of shading subpixel samples. In Figure 2,
geometric information and shading are generated at
different rates. Each pixel has 4 geometric samples
on a 4× 4 grid and one of those geometric samples
is also a shaded sample. The shading value at each
geometric sample is reconstructed by interpolating all
shaded neighbors in a fixed radius using the bilateral
weights. We take both depth and normal change into
account when compute the bilateral weight. A neigh-
boring sample with significantly different geometry is
probably across a geometric edge and hence receives a
low weight.
wi j = G(σz(z j− zi))G(σn(1− sat(n j ·ni))) (1)
In Equation 1, G(x) is the Gaussian function of the form
exp(−x2). zi and ni are the depth and normal of the ith
subpixel sample. σz and σn are the scaling factors for
controlling how quickly the weights fall off and allow-
ing us to increase the importance of the bilateral filter.
We set σz to 10 and σn to 0.25 in all our testing. The
sat(x) function is implemented as max(0,min(1,x)).
The result wi j is the weight associated with the jth sub-
pixel sample while performing shading reconstruction
for the ith subpixel sample.
For tracing the primary rays that are emitted from the
eye position, we use a hybrid rendering approach that
utilizes the GPU to generate the subpixel geometric in-
formation including position, normal and depth. We
create 3 auxiliary geometric buffers to store position,
normal and depth by GPU rasterization with the same
resolution as the shaded buffer. Each geometric buffer
is rendered with a subpixel offset applied to the pro-
jection matrix. The subpixel offset is applied not only
to form a 4× rotated-grid but also to do pixel align-
ment between rasterization and ray tracing rendering.
Since the GPU rasterization pipeline produces the sub-
pixel geometric information very efficiently, this over-
head is insignificant when compared to the ray tracing
stage.
3.2 Shadow Ray
As mentioned above in Section 3, the shadow edges
cannot be detected by the geometric information that
is generated from the eye position alone. What we need
is subpixel information that is more meaningful to the
shadow edges. The naive solution for shadow antialias-
ing is through a shadow map drawn at a higher reso-
lution. However, this approach is inefficient because
the increased resolution of the shadow map (from the
light’s view) does not contribute directly to the subpix-
els at the screen space. Therefore, we generate subpixel
shadow information by ray casting and combine this
shadow value with the bilateral filter weighting equa-
tion as shown in Equation 2. The subpixel shadow rays
are generated by utilizing the position information in
the geometric buffer as mentioned in Section 3.1.
Figure 2 shows our algorithm reconstructs the color
value of a geometric sample in a non-shadowed area
not only by taking the Euclidean distance and the nor-
mal change between the source and the target samples
but also under the influence of shadow boundaries to ex-
clude the neighboring samples in shadowed area. This
is the reason why the original SRAA adds excessive
blur to the shadow boundaries, yet our method achieves
a better quality that is comparable to 16× supersam-
pling.
Rendering Pass
Resolution 1st 2nd Total
256x256 18 5 23
512x512 35 14 49
768x768 69 28 97
1024x1024 116 49 165
unit: millisecond
Table 1: Time measurement of our method for render-
ing the Sponza scene in Figure 5.The first pass is ge-
ometric information generation and the second pass is
antialiasing process. Note that the time shown in first
pass is measured with raytracer solution.
Figure 3: Performance comparison between NoAA (no
antialiasing applied ), our method with GPU hybrid ap-
proach, and SSAA (16× supersampling antialiasing)
for rendering the Sponza scene under various output
resolutions. The vertical axis is the rendering time
in millisecond. The overall rendering performance of
our method with a GPU hybrid approach is about 6×
speedup in average compared to the 16× supersampling
approach.
tion and achieves about 6× speedup in average com-
pared to the 16× supersampling approach.
5 CONCLUSION
We introduce the concept in SRAA to path-tracing
based rendering methods for antialiasing. Our method
extends the subpixel geometric sampling concept
beyond the primary rays and achieves antialiasing for
shadow rays and reflective rays as well. By adopting
a hybrid approach, our method improves the image
quality without incurring performance penalty of
tracing additional primary rays. We hope our method
encourages the adoption of antialiasing even for the
computationally constrained real-time ray tracing or
path tracing.
6 ACKNOWLEDGEMENTS
This work is supported in part under the “Embedded
software and living service platform and technology de-
velopment project” of the Institute for Information In-
dustry which is subsidized by the Ministry of Econ-
omy Affairs (Taiwan), and by National Science Council
(Taiwan) under grant NSC 100-2219-E-003-002.
7 REFERENCES
[1] Carsten Benthin. Realtime Ray Tracing on Cur-
rent CPU Architectures. PhD thesis, Saarland
University, 2006.
[2] Jacco Bikker. Arauna real-time ray tracer and
Brigade real-time path tracer.
[3] Matthäus G. Chajdas, Morgan McGuire, and
David Luebke. Subpixel reconstruction antialias-
ing for deferred shading. In Symposium on Inter-
active 3D Graphics and Games, I3D ’11, pages
15–22, 2011.
[4] Marko Dabrovic. Sponza atrium,
http://hdri.cgtechniques.com/ sponza/files/, 2002.
[5] Johannes Gunther, Stefan Popov, Hans-Peter Sei-
del, and Philipp Slusallek. Realtime ray tracing on
gpu with bvh-based packet traversal. In Proceed-
ings of the 2007 IEEE Symposium on Interactive
Ray Tracing, pages 113–118, 2007.
[6] Daniel Reiter Horn, Jeremy Sugerman, Mike
Houston, and Pat Hanrahan. Interactive k-d tree
gpu raytracing. In Proceedings of the 2007 sym-
posium on Interactive 3D graphics and games,
I3D ’07, pages 167–174, 2007.
[7] Jorge Jimenez, Diego Gutierrez, Jason Yang,
Alexander Reshetov, Pete Demoreuille, Tobias
Berghoff, Cedric Perthuis, Henry Yu, Morgan
McGuire, Timothy Lottes, Hugh Malan, Emil
Persson, Dmitry Andreev, and Tiago Sousa. Fil-
tering approaches for real-time anti-aliasing. In
ACM SIGGRAPH Courses, 2011.
[8] Bongjun Jin, Insung Ihm, Byungjoon Chang,
Chanmin Park, Wonjong Lee, and Seokyoon Jung.
Selective and adaptive supersampling for real-
time ray tracing. In Proceedings of the Confer-
ence on High Performance Graphics 2009, HPG
’09, pages 117–125, 2009.
[9] Gábor Liktor and Carsten Dachsbacher. Decou-
pled deferred shading for hardware rasterization.
In Proceedings of the ACM SIGGRAPH Sympo-
sium on Interactive 3D Graphics and Games, I3D
’12, pages 143–150, New York, NY, USA, 2012.
ACM.
[10] Don P. Mitchell. Generating antialiased images
at low sampling densities. In Proceedings of
the 14th annual conference on Computer graph-
In
pu
t
N
o
A
A
O
ut
pu
t SR
A
A
O
ur
s
R
ef
er
en
ce
16
sa
m
pl
es
(a) Primary Shading (b) Shadow (c) Reflection (d) Reflected Shadow
Figure 5: Quality comparison between our method and the other antialiasing techniques in highlighted areas of
primary shading, shadow, reflection, and reflected shadow. (Row 1) No antialiasing, (Row 2) SRAA: one subpixel
with shading value and 4 subpixels with primary geometric information, (Row 3) Ours: one subpixel with shading
value and 4 subpixels with geometric information for primary, shadow and secondary rays, (Row 4) Reference
image: 16× supersampling.
國科會補助計畫衍生研發成果推廣資料表
日期:2012/03/03
國科會補助計畫
計畫名稱: 子計畫一：用戶端多核心嵌入式系統三維圖型應用程式發展(1/3)
計畫主持人: 張鈞法
計畫編號: 100-2219-E-003-002- 學門領域: 嵌入式軟體(網通國家型)
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
研究成果所發表的兩篇期刊論文發表於 ACM Transactions on Graphics 與
Computer Graphics Forum，根據 JCR2010 之 impact factors 分別為 3.632 及
1.476，皆為同領域排名前 30%之重要期刊。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
