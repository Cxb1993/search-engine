Of these data analysis techniques, the capability of K-means and Kohonen’s SOM for clustering 
large-scale databases has been confirmed [11]. Even though SOM or conventional clustering methods 
have their superior features for clustering analysis, their combinations into two-stage methods are 
generally much more powerful than individual methods. Not only do two-stage methods conquer the 
drawbacks of conventional hierarchical and partitive clustering methods, but their clustering results are 
more accurate [13]. A two-stage method usually combines either a conventional hierarchical method or 
SOM with a partitive method. The function of the hierarchical method or SOM is commonly used to 
determine the number of groups in the first stage, and the K-means method is used to implement the 
clustering process in the second stage. 
Generally, a two-stage method including a SOM-based algorithm has two ways of functioning. In 
the first way, SOM is initially adopted to determine the number of groups and the initial group centers of 
K-means, i.e. the respective weight vectors of the groups, from the network topology. The merit of SOM 
is that it faithfully presents the structure of the original data set through a simple two or three-
dimensional network topology [9,15]. This advantage overcomes the problems of K-means in 
appropriately determining the number of groups and their initial group centers because these parameters 
substantially affect the final result [10,12]. Based on the results from SOM, the K-means method then 
further clusters the data set unambiguously so that its is clear why each data sample is assigned to its 
group according to the mapping results on the network topology, especially since some data samples are 
located near the border between two groups [14].  
In the second way, SOM initially maps a large-scale data set upon the network topology and 
generates the topological coordinates of prototypes for further clustering in the second stage. The 
method employed in the second stage may be SOM, a hierarchical agglomerative clustering method, or a 
partitive clustering method like K-means. The main advantage of the SOM-based two-stage method is 
savings of time without the considerable computations required by conventional clustering methods for 
large and complicated data sets [16]. Therefore, this research proposes an effective and efficient method 
for preprocessing a large-scale data set under the limited time conditions to complete the purpose of data 
mining and knowledge discovery. 
This paper has five sections. Section 1 is the introduction to the research motivation and objectives. 
In Section 2, the proposed SOM algorithm based on an ant colony optimization is described in detail. In 
Section 3, the two-stage method of ant colony SOM and K-means using four public datasets is verified 
by comparison with the two-stage method of Kohonen’s SOM with K-means. Finally, conclusions and 
future study are presented. 
 
 
2. ABSOM Algorithm 
 
In this study, we propose an Ant-Based Self-Organization Map (ABSOM), which embeds the 
concept of ACS (Ant Colony System) in the SOM algorithm, in order to avoid falling into local 
minimum during the solution search process [1,2]. The detailed algorithm is presented below. 
 
Step 1: Parameter setting 
(a) Topological network setting 
The topological network M = {c1, c2… cj… cm} is defined so that M consists of m = m1 × m2 map 
units where m1 and m2 represent the number of units located on the two sides of the network, 
respectively. According to the probability distribution )( iP ξ  where iξ  stands for an input vector, the n-
dimensional weight vector  of every map unit  is initialized. Set the start time t = 0 and the 
initial neighborhood radius 
n
c Rw j ∈ jc
)0(λ . 
(b) Pheromone matrix setting 
 2
)()1( tt ελλ =+        (8) 
))(/exp())(,()1( trtrfth
jj ccrs
λλ −==+      (9) 
where )(tλ : the neighborhood radius at time t;  
ε : the decreasing learning rate of neighborhood radius )1,0(∈ε ; 
jc
r : the topological distance between the winner (or the BMU) and unit 
jc  on the map topology. 
Step 7: Check the stopping criteria , . 
 
dlenN trainlenN
 
3. Evaluation of ABSOM + K-means 
 
Although an artificial data set with given solution might be created to examine the feasibility of a 
developed model, we consider that it will be more meaningful to test the model directly on practical data 
sets. Thus, four public data sets [15] are employed below. Those are Fisher’s iris data (Iris), Forina’s 
wine recognition data (Wine), optical recognition of handwritten digits (Optical) and pen-based 
recognition of handwritten digits (Pen-Based).  
As mentioned, the visualization results from the ABSOM and SOM can be used to determine the 
number of groups and initial center of each group for clustering in the following stage, K-means. To 
verify the usefulness of visualization, the DB (Davies-Bouldin) index values from K-means for these 
two algorithms are measured and compared [5]. The verification procedure has two stages. In the first 
stage, the ABSOM and SOM are conducted for clustering the four data sets. The results of their QE 
(Quantization Error) and TE (Topological Error) values [6,8] are shown in Table 1. Then, the U-Matrix 
method, contained in the SOM Toolbox 2.0, is adopted to depict the similarity of the mapped data by the 
ABSOM and SOM in terms of the whole variables and individual variable.  
Comparing grey level and shape of data distribution for each variable with those for the overall 
variables, we can easily identify where the key (principal) variables come from and eliminate those 
unimportant variables from original data set if any. If the conclusion can be confirmed by using 
Principal Component Analysis (PCA) [7] or general regression method, the less important variables may 
be eliminated to reflect a positive influence on the clustering purpose. Then, the U-Matrix method is 
combined with the block diagram method to generate the topological maps from the ABSOM and the 
SOM for the Optical data set shown in Fig. 1 and 2, respectively. The blocks in Fig. 1 and 2 represent 
the mapping results from the ABSOM and the SOM. 
 
Table 1. QE and TE values of the four data sets from the SOM and ABSOM before visualization process 
 Data set I (Iris) Data set II (Wine) 
Algorithm SOM ABSOM SOM ABSOM 
Q.E. 0.08451 0.08367 0.5001 0.4971 
T.E. 0.2889 0.2753 0.4398 0.3901 
 Data set III (Pen-Based) Data set IV (Optical) 
Algorithm SOM ABSOM SOM ABSOM 
Q.E. 0.4108 0.3756 0.9471 0.8777 
T.E. 0.0799 0.0938 0.0624 0.1059 
 
 4
To determine initial center of each group for the K-means, different combinations of possible group 
centers for each data set obtained from the ABSOM and SOM are tested. The tuple set (M1, M2,…, Mn) 
represents the combination of the averages of all the possible group centers if different combinations of 
initial group centers exist. The best result from ABSOM+K-means or SOM+K-means for each data set is 
the combination with the lowest DB value. The results show that the ABSOM+K-means always 
performs better than the SOM+K-means. In addition, the combination of averages of all the possible 
group centers, represented by (M1, M2,…, Mn), obtains the best DB value in the Pen-based and Optical 
data sets; however, it obtained the worst DB value in the Iris and Wine data sets.  
To compare the performance of the ABSOM+K-means to the one of SOM+K-means, K-means is 
also executed alone using the same four data sets. The number of groups is set to be the same as that 
used in the ABSOM or the SOM experiments. The initial group centers are those data samples, selected 
from the beginning of the original data set, which have the same number as the number of groups. The 
final results are shown in Table 2 in which we found that the ABSOM+K-means performs better than the 
SOM+K-means while the SOM+K-means performs better than the K-means alone.  
 
 
Table 2. Comparison of K-means, SOM+K-means and ABSOM+K-means using the four data sets 
Data set I(Iris) II(Wine) III(Pen-Based) IV(Optical) 
Cluster 3 3 12 13 11 12 
K-means 0.7337 1.3627 1.6688 1.7288 1.9616 
1.909
2 
SOM +  
K-means 0.7990 1.3627 
1.401
3 N/A N/A 
1.474
6 
ABSOM +  
K-means 0.7189 1.3627 N/A 1.3650 1.2975 N/A 
 
Thus, we can confirm that the ABSOM performs better than the conventional SOM on the 
evaluation criterion QE. In addition, after comparing the ABSOM+K-means and SOM+K-means, we 
conclude that the ABSOM in all respects performs better than the conventional SOM in clustering 
analysis for the four data sets. 
 
 
4. Application of the ABSOM to Grade Li-ion Cells 
 
In this study, the Lithium-ion polymer cell is our subject that we choose from various batteries due 
to its market share and multi-purposed applications.  
 
4.1 Quality Characteristics and Measurements 
 
In order to realize the actual cell performance, new battery cells are selected for the experiments. 
Because of time-consuming and confidential problems, we understand the difficulty of gathering data 
from the industry, so that this research uses a particular electronic instrument for cell performance 
measurements, the PPS-1202GS manufactured by American Reliance, Inc., to test the battery cells.  
As we know, various cell characteristics may influence the final product quality. Some common 
and vital characteristics include output voltage, capacity, energy density, power density, life cycle times 
etc. According to [17], we determine eight characteristics for the experiments besides four from [16]: (1) 
voltage variation in one charge/discharge cycle, (2) internal resistance variation in one charge/discharge 
 6
during the charge process  
(C slope in Charge) 
discharge cycle, and measure capacity 
increasing slope during the charge process 
within first 10 minutes 
Capacity declining slope 
during the discharge 
process 
 (C slope in Discharge) 
Execute capacity test of discharge-charge-
discharge cycle, and measure capacity 
declining slope during the discharge process 
within first 10 minutes 
Smaller-the-Better 
 
 
4.2 Grading Result for Li-ion Cells 
After performing the application of the ABSOM and the K-means Algorithms for clustering battery 
cells, the most appropriate number of clusters and their corresponding centers (or prototype vectors) can 
be acquired. In this case, the 53 battery cells are groups into ten clusters, of which the prototype vectors 
are shown in Table 4. The next step is to examine the average performance of the clusters by ordering 
the prototype vectors using the TOPSIS method. The results shown in Table 5 are the ten normalized 
prototype vectors and the sets of positive ideal solutions A* and negative ideal solutions A-.  
 
Table 4 Ten Prototype vectors after conducting the ABSOM and K-means algorithms 
Cluster △V △R R1C △C V slope in Charge
V slope  
in Discharge
C slope 
in Charge 
C slope 
in Discharge
1 0.32144  0.23398  0.71887 0.10406 0.81363 0.66054  0.37871  0.56419  
2 0.18537  0.72507  0.29454 0.33888 0.60903 0.38397  0.33914  0.66715  
3 0.41905  0.60661  0.41619 0.27937 0.41027 0.52778  0.56616  0.42779  
4 0.78632  0.67444  0.11905 0.40050 0.16339 0.22222  0.73441  0.23457  
5 0.62187  0.34960  0.20248 0.40211 0.65815 0.23611  0.31872  0.73359  
6 0.26927  0.56843  0.17631 0.45482 0.68948 0.27692  0.63043  0.11630  
7 0.66170  0.88021  0.12381 0.65190 0.09424 0.30385  0.35475  0.53111  
8 0.71642  0.16885  0.49455 0.47803 0.26280 0.60317  0.19188  0.70476  
9 0.67549  0.23757  0.40615 0.71062 0.45690 0.35705  0.73187  0.20889  
10 0.30339  0.21036  0.70164 0.85284 0.29375 0.69792  0.61807  0.30225  
 
 
Table 5 The sets of Ideal solutions A* and negative-ideal solutions A- from the TOPSIS 
Cluster △V △R R1C △C V slope in Charge
V slope 
in Discharge
C slope 
in Charge 
C slope 
in Discharge
1 0.00629 0.00514 0.02758 0.00390 0.02158 0.02167 0.00871 0.01662 
2 0.00472 0.01943 0.00749 0.01072 0.01850 0.00978 0.00916 0.01830 
3 0.01148 0.01663 0.01462 0.00743 0.01062 0.01477 0.01780 0.00971 
4 0.01841 0.01791 0.00542 0.00953 0.00572 0.00800 0.01844 0.00723 
5 0.01572 0.00872 0.00756 0.01102 0.01879 0.00874 0.00785 0.02055 
 8
40 2 B 6 1 C 21 7 C 48 10 D 
41 2 B 17 1 C 22 7 C 8 9 E 
45 2 B 42 3 C 24 7 C 10 9 E 
26 5 B 43 3 C 51 7 C 31 9 E 
30 5 B 44 3 C 23 4 D 33 9 E 
32 5 B 46 3 C 50 4 D 39 9 E 
34 5 B 47 3 C 52 4 D 
38 5 B 53 3 C 9 10 D 
49 5 B 25 6 C 11 10 D  
 
 
5. Conclusions and Future Study 
 
This research proposes and evaluates a two-stage clustering method that combines an ant colony 
SOM and K-means. The SOM clustering model, ABSOM, embeds the exploitation and exploration rules 
of state transition into the conventional SOM algorithm. As shown by applying the method to four 
practical data sets, the ABSOM itself not only performs better than Kohonen’s SOM but also it works 
very well in the two-stage clustering analysis when it is used as the preprocessing technique for the 
ABSOM+K-means method. 
In the evaluation process, the U-Matrix method is combined with the block diagram method to 
generate a U-Matrix block diagram on the network topology, which facilitates the visualization ability in 
determining initial prototypes (or group centers) for the K-means in the following clustering analysis. 
The results show that the proposed ABSOM+K-means method performs better than the SOM+K-means 
method for the clustering analysis. 
As indicated by other recent studies, the fixed structure of the conventional SOM neural networks 
does not afford to respond to actual data distribution in the original data set, if the data set is constituted 
by a number of groups containing vague boundaries and complicatedly hierarchical relations. Therefore, 
our future research work will extend the proposed ABSOM approach to create a “growable” ant colony 
SOM such as growing grid in [16] to deal with this type of problem. 
Moreover, as mentioned in the literature review, there are many indices in addition to the Davis-
Bouldin index, such as Dunn’s index, or RMSSTD, that can be utilized to conduct the performance 
evaluation of a clustering method after/during the clustering process. The use of such performance 
indices will assist direct and real-time evaluations for clustering quality. 
With the strict requirements for battery cell performance, this research employs the proposed 
ABSOM to investigate the relationship the quality characteristics and performance grading of Li-ion 
cells. According to the results from Section 4.2, the sampled cells with the measurements of the 
characteristics are designated into five groups. After performing the correlation analysis, the relationship 
between the grouping result and these characteristics has been established Based on the results from this 
research, the battery cell manufacturers can utilize them as guidance for solving the quality classification 
problem of Li-ion battery cells and further build an instantly testing system. 
 
Publications 
 
The result obtained from this research project has been published in the international conferences, 
WSOM 2005 and KES 2006. The extended papers have also been submitted to two international 
journals for reviewing. 
 
 
References 
 
 10
