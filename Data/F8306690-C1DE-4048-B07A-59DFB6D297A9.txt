 2
Abstract  
As technology advances, the 
bandwidth of internet has grown quickly 
and becomes connectible. However, it 
brings a large amount of data flows into 
the internet fast. Every network system 
has to face these problems and also 
needs to provide quality of service 
between the internet service providers 
and users. 
For solving these problems, we 
design a policy-based network 
management system on virtual network. 
This system could collect information in 
a large network efficiently. We could 
also implement relative algorithm or 
mechanism on it. We calculate and 
analyze these data flows to find a 
method that detects and determines 
various internet applications. Therefore, 
we use performance testing equipments 
and software to design network 
architectures in experiment to measure 
the performance of NetFPGA in section 
3, and analyze the results of experiments 
to determine whether NetFPGA is useful 
in the internet.  
In addition to continuing research 
results of section 3 in section 4, we also 
study Simple Network Management 
Protocol (SNMP), OpenFlow Protocol 
and NOX controller. Finally, we 
integrated NetFPGA, modified SNMP, 
OpenFlow and NOX controller, to 
implement a policy-based network 
management on Virtual Network for 
education and discipline. 
Section 5 described the architecture 
of policy-based network management on 
virtual network and illustrates how to 
manage a network system through the 
hierarchical network management 
system in the future. 
 
Keyword: network management, policy, 
virtual network, NetFPGA, OpenFlow, 
NOX controller, SNMP 
 
2. 緣由及目的 
近年來網路的發展一日千里，有
線網路頻寬已達到 10 Gbps以上，網路
使用者與日俱增，使網路架構越趨龐
大，次世代網路 (Next Generation 
Networking，簡稱 NGN)已逐漸成型。
加上網路服務的需求日漸增加，以及
點對點傳輸(Peer-to-Peer)、網路電話
(VoIP)、網路電視(IPTV)以及影音多媒
體等新興服務的誕生，每一種服務對
網路特性的需求不盡相同，為了因應
以及滿足這些需求，將不同類型的網
路流(Network Flow)分類，往不同方向
傳送，顯然已成為十分重要的課題。
同時，訓練網路管理者如何運用網路
管理，將取得的網路資訊進行有效的
管理決策，提高網路管理者之素質，
使其具備管理網路之能力，以提升網
路品質，也是相當重要的一環。 
由於在從事網路相關議題的研究
時，普遍在測試環境的建立上遭遇重
重困難，以及如何模擬真實世界的流
量特性來發展系統及演算法。以當前
網路研究來說，普遍採用網路模擬器
如 ns-2[1] 、 QualNet[2] 等 ， 或 是
Benchmark的方式來評估其效能，但以
現今網路的複雜度與應用的多元化來
看，這些模擬的結果是否真的符合真
 4
的開放平台[10]，透過 FPGA硬體可程
式化的特性，可進行各項高速網路路
由架構實驗性開發研究[11]，也可利用
多個NetFPGA與其他網路設備組成大
型實驗網路平台，進行各項教學與研
究實驗[12]。 
NetFPGA 可提供教學研究用途，
目前已於世界各地舉辦多場教學研討
會，如劍橋、西雅圖、北京、印度等。
提供許多學生及研究者任意新型態網
路研發的平台，透過 NetFPGA我們將
可實作任何網路模型，而只要撰寫
Verilog 語言並燒錄至可程式化的
FPGA上即可。目前 NetFPGA已有為
數可觀的廠商予以支援，如思科
(CISCO)、華為 (HUAWEI)、賽靈思
(Xilinx)、博通(Broadcom)、瞻博網路
(Juniper Networks)等等。 
在 2008年由美國史丹佛大學所提
出的 OpenFlow[13]計畫的概念，是因
為網路普及以及相關網路服務的風
行，目前無論在個人、學校或是企業
之中，網路佈建已成為一不可缺少的
基礎建設之一，而這些進步，都是有
賴於網路相關研究人員的努力。在研
發這些網路流系統架構的同時，網路
研究人員常常會遇到一個問題，即是
在研擬出一個新型或是修正原有網路
流架構的同時，研究人員必須花費太
多的時間與金錢成本在重製新的硬體
實驗平台和佈署建置，而改善此缺點
即 OpenFlow 計畫概念。OpenFlow 計
畫的目的即是讓開發人員能夠迅速的
建構出實驗型的網路流架構，並且真
正的測試其可行性以及效能。 
OpenFlow為一開放的設計平台，
研究人員可在這開放平台上撰寫 Flow 
Table，此 Flow Table可適用在不同廠
牌的網路路由器以及網路交換器，因
此，研究人員可以在不同的網路路由
器以及網路交換器上自行設計其獨特
的網路路由，在網路系統中的各個路
由器以及交換器，只需要確認此封包
是屬於 Flow Table中的哪一個項目，
即可令其繞行自定的路由方式。並且
若將 OpenFlow 運用在 NetFPGA 上
[14]，可比傳統方法得到較佳的效率與
管理。如圖 3所示，OpenFlow的網路
元件由三個主要的部份所組成，包括： 
1. Flow Table：主要是在網路交
換器上記錄各個 flow的進入以及離開
的 port。 
2. Secure Channel：讓管理者能
以遠端的方式對 OpenFlow 的網路交
換器進行管理。 
3. OpenFlow Protocol：讓研究開
發者直接在不同網路交換器自行撰寫
管理系統的平台。 
以政策為基礎之網路管理[15]主
要根據已經定義好的規範，提供一個
方式來配置網路資源、服務品質保證
及資訊安全等，當 VoIP或其他即時服
務對於服務品質保證的需求增加時，
以政策為主的頻寬分配需求也相對增
加。倘若政策訂定失當，會造成資源
分配不均、網路壅塞等嚴重後果。 
以政策為基礎之網路管理系統讓
管理者可針對各種需求在系統中定義
與管理政策，這些規則類似「若符合(狀
況)，則進行(動作)。」，如圖 4所示，
狀況可以是使用者、群組、日期時間、
應用程式類型或網路位址等等，之後
政策將會被散佈到各個網路元件上。
以政策為基礎之網路管理系統對於擁
有大量網路設備的大型網路非常實
用，可方便在中央控制端進行控管。 
 6
4.1. NetFPGA 網路開發平台之實驗與
效能量測 
以往發展網路架構時，因受限於
一般商業產品之硬體限制，多使用模
擬軟體進行可行性評估、特性評估與
效能分析，研究人員所發展出的新架
構與新應用難以直接在現有網路架構
下進行整合測試。而在這篇論文當
中，我們探討 NetFPGA這個新的開發
平台是否可應用以及如何應用於網路
實驗中，從而解決上述之難題。在此
篇論文當中，我們利用 NetFPGA這個
新的開發平台，將 NetFPGA 設定成
Router 或 Switch，建構出基本的網路
架構。 
在實驗架構中，裝載  NetFPGA 
的電腦規格配備及系統環境如表 1 所
示，搭配此規格配備並利用 SmartBits
與 iperf，設計 throughput、latency 與
packet loss三個實驗。依此三項實驗測
量設定成 Switch 以及 Router 的
NetFPGA 的效能，並分析兩者之間的
差異。 
 
表 1 裝載 NetFPGA電腦規格配備及
系統環境 
規格、環境 名   稱 
中央處理器 
Intel Core™2 Quad 
CPU Q8200 2.33GHz 
記憶體 2 Gigabytes 
作業系統 
(核心) 
CentOS 5.3 
(2.6.18-128.el5) 
NetFPGA 
Project 
reference_router 
reference_switch 
 
在這三項實驗測試中，我們建立
六項實驗架構，以下將逐一解說。在
第一項實驗架構中，如架構 A 所示，
透過網路線，我們將 SmartBits的輸出
埠與輸入埠連接至設定成 Router 的
NetFPGA 上的任意兩個連接埠，使
SmartBits可以對NetFPGA進行網路效
能測試。 
第二項實驗架構如架構 B 所示，
我們將NetFPGA設定成Switch並以和
架構 A 同樣方式進行測試其網路效
能。 
第三項實驗架構如架構 C 所示，
將兩台電腦的網路連接埠，透過網路
線任意連接設定成 Router的 NetFPGA
兩個網路連接埠，再使用 iperf 去測量
NetFPGA的網路效能。 
第四項實驗架構如架構 D 所示，
將 NetFPGA設定成 Switch，並如架構
C 同樣方式連接 NetFPGA，再利用
iperf測試 Hub之網路效能。 
第五項實驗架構如架構 E 所示，
我們將兩台 SmartBits的輸出與輸入埠
任意連接至設定成 Router的 NetFPGA
上的四個網路連接埠，同時使用相同
方式向 NetFPGA 灌輸網路流量，藉此
測試在干擾之下設定為 Router 的
NetFPGA的網路效能。 
第六項實驗架構如架構 F 所示，
將 NetFPGA設定成 Switch，並與架構
NetFPGA 同 樣 方 式 連 接 兩 台
SmartBits，再以同樣方式向 NetFPGA
灌輸網路流量，藉此測試在干擾之下
設定為 Switch的 NetFPGA網路效能。 
當實驗環境架設完成後，我們分
別對設定成 Router 或 Switch 的
NetFPGA 進行效能測試。其測試實驗
我們分為 packet loss、throughput以及
latency 三個部分，以下將對三個實驗
進行說明。 
 
 8
NetFPGA 效能架構。當實驗環境架置
完成後，利用 SmartBits 產生封包輸出
至測量目標量測 packet loss 數值，其
SmartBits的測試環境參數如表2所示。 
測試完成後，從實驗產生的 packet loss
數據圖 5中發現，代表架構 A數據的
實心圓紅線、代表架構 B數據的空心
圓虛線、代表架構 E的倒三角型綠線
以及代表架構 F的正三角形虛線的
packet loss率都重疊在 0數值上，表示
設定成 Router與設定成 Switch的
NetFPGA效能都足以負載網路流量。
PacketLossTest
frame size(bytes)
0 200 400 600 800 1000 1200 1400 1600
P
a
c
k
e
t
 L
o
s
s
Lo
ss
 
Ra
te
(%
)
-0.5
0.0
0.5
1.0
架構A
架構B
架構E
架構F
measured by
 SmartBits
圖 5. 以 SmartBits測試 Packet Loss數
據圖 
 
表 2 SmartBits 測試數值 
數值名稱 數值說明 數值設
定 
Test duration 
time (sec) 
封包輸出
的持續時
間 
20 
Minimum 
frame size 
(byte): 
封包最小
值 
64 
Maximum 
frame size 
(byte): 
封包最大
值 
1518 
Step frame 
size (byte): 
調整封包
大小值 
Custom 
(2) Throughput 測試實驗 
在 Throughput 測試實驗中，我們
分別建立兩個實驗架構，分別是如架
構 C所示的以 iperf量測設定成 Router
的 NetFPGA效能架構，以及如架構 D
所示的以 iperf 量測設定成 Switch 的
NetFPGA 效能架構圖。當實驗環境架
置完成，由其中一端電腦利用 iperf設
置成 server，另一端則利用 iperf 的
client功能向 server 灌輸流量。 
測試完成後，觀察由實驗產生的
throughput 數據圖 6 發現，當 TCP 
window size設定 8k的情況下，代表架
構 C的實心圓紅線，1s到 2s之間傳送
可有較高的速度，2s 之後趨於平坦。
代表架構 D 的空心圓虛線，傳送速度
則無較大起伏。從圖 7中可知，當 TCP 
window size設定為 32k時，可發現雖
然兩條線的速度並沒有太大起伏，但
卻具有落差，其差距約 50Mbps。從圖
8則發現，當 window size設定 64k時，
兩條線在 1s到 2s之間的速率比 2s到
9s較低，而 9s到 10s之後則逐漸偏高。
綜合圖 6、圖 7與圖 8的結果，可知設
定成 Router 與設定成 Switch 的
NetFPGA，其兩者效能在 throughput
的壓力測試下，是具有些微的差異性。 
 
Throughput(TCP window size=8KB)
time(sec)
0 2 4 6 8 10
Th
ro
u
gh
pu
t
M
bp
s
244
246
248
250
252
254
256
258
260
262
264
266
架構C
架構D
measured by iperf
圖6. 以 iperf 傳送8k TCP window size
測試 Throughput數據圖 
 10
方面，從兩者的線幾乎重疊的情況也
可進一步證實兩者效能沒有太大的差
異性。由以上實驗可知，可設定成
Router與 Switch的 NetFPGA，既可任
意變換網路型態，又不會導致效能相
差過大，這些特點使 NetFPGA 優於其
他相同功能之網路設備。 
 
4.2 於 NetFPGA 上教學用之網路管理
實作與實驗 
由於網路服務需求量日漸增多，
越來越多種不同的網路服務，使得訓
練管理者如何有效掌控網路資訊，藉
此管理網路流量與服務，並使每個網
路服務需求者可享有較佳的網路服
務，是日趨重要的一個課題。 
我們延續上篇論文研究之成果，
將 NetFPGA設定成 Router或 Switch，
建構出基本的網路架構，另外修改使
用於 Linux 平台上之 SNMP 套件，以
此取得網路管理者需要的網路資訊。
除此之外，我們亦將 NetFPGA設定為
openflow_switch ， 並 透 過 NOX 
controller控制NetFPGA之網路流量與
流向，使其成為網路管理系統。最後
結合修改過之 SNMP 以及簡單之網路
管理政策，使網路管理者可依此網路
資訊，管理與 NOX controller 相接之
NetFPGA，以提供一簡單網路管理系
統供網路管理者使用與訓練。 
以 下 將 分 為 整 合 SNMP 與
NetFPGA、OpenFlow實驗架構與測試
及整合 openflow_switch、SNMP 與
Policy三個章節說明。 
(1) 整合 SNMP 與 NetFPGA 
網路管理者進行管理時，需藉由
網路資訊，以進行網路決策的判斷，
例如藉由網路流量資訊，決定該如何
限制以及放寬流量，以達到較佳的網
路狀況或給與網路服務需求者較好的
使用狀況，即是一種依可取得之網路
資訊進行的網路管理決策。為了滿足
網路管理者之需求，我們整合 SNMP
協定以及 NetFPGA，取得網路管理者
所需之資訊。 
 
表3 裝載NetFPGA電腦規格配備及系
統環境 
 
名稱 
中央處理器 
Intel(R) CoreTM2 
CPU 6420 
2.13GHz 
記憶體 2 Gigabytes 
作業系統(核心) CentOS 
5.3(2.6.18-128.el5) 
NetFPGA 
Project 
reference_nic 
reference_switch 
reference_router 
 
裝載NetFPGA的電腦規格配備與
系統環境如表 3 所示，我們在此規格
配備上，透過網路取得並安裝開放原
始碼(open source)的 SNMP軟體，擷取
網路流量之資訊。當 SNMP 協定已正
確運作，則如圖 10示，由 ObjectID為
sysUpTimeInstance 可看到系統已經正
確運行的時間。透過 NetFPGA 的
reference_nic 、 reference_router 與
reference_swi tch三個不同專案，將其
設定為網路卡、Switch 與 Router 三種
網路設備，以期獲得網路資訊。 
 
 
圖 10. SNMP正確運作圖 
 12
目的地。因此當 NetFPGA 設定為
Router時，會將 Routing table與 ARP 
table 儲存至本身擁有的 Register，
NetFPGA只需對照 Register上的 table
資訊，即可將封包送至目的地。 
所以當封包目的地位址不是
NetFPGA 本身時，則封包不會送至
Linux主機，所以 SNMP套件無法取得
網路流量之資訊。 
為解決此問題，我們修改 SNMP
套件的原始碼，使其可存取 NetFPGA
上存放網路資訊的記憶體暫存器的記
憶體區塊，使資料可以真實反映目前
的流量現況。由於修改部分僅限於存
取流量資訊，所以 SNMP 套件被修改
部分在不影響NetFPGA原本功能的情
況下，讓所擷取資料為正確來源。如
圖 14與圖 15所示，設定為 Switch與
Router 的 NetFPGA，已可藉由 SNMP
取得正確之資訊。 
(2) OpenFlow 實驗架構與測試 
為了驗證 OpenFlow 可正確無誤
的運作於 NetFPGA 上，並與 NOX 
controller 連結，我們簡單建立一個實
驗架構，實際測試 openflow_switch以
及 NOX controller 兩者的運作。 
 
NetFPGA Switch 封包流量
Sec0 100 200 300 400 500 600
to
ta
l t
hr
o
u
gh
pu
t (
M
B
yt
e)
0
50000
100000
150000
200000
250000
SNMP
counterdump
圖 13.  SNMP與 counterdump於
NetFPGA Switch封包流量數據圖 
NetFPGA Router 封封封封
Sec0 100 200 300 400 500 600
to
ta
l t
hr
o
u
gh
pu
t (M
B
yt
e)
0
50000
100000
150000
200000
250000
SNMP
counterdump
圖 14. SNMP與 counterdump於
NetFPGA Router封包正確流量數據圖 
 
NetFPGA Switch 封包流量
Sec0 100 200 300 400 500 600
to
ta
l t
hr
o
u
gh
pu
t (
M
B
yt
e)
0
50000
100000
150000
200000
250000
SNMP
counterdump
圖 15. SNMP與 counterdump於
NetFPGA Switch封包正確流量數據圖 
 
3
NOX controller
使用者B
4
1
OpenFlow
Switch
eth0
eth0
使用者A
nf2c0
nf2c1
2
 
圖 16. OpenFlow實驗架構圖 
 
實驗架構如圖 16所示，安裝 NOX 
controller 之電腦規格配備如表 4 所
示。在圖 16的架構底下，使用者 A與
 14
NetFPGA openflow_switch 封包流量
Sec0 100 200 300 400 500 600
to
ta
l t
hr
o
u
gh
pu
t (
M
B
yt
e)
0
20000
40000
60000
80000
100000
120000
140000
160000
SNMP
counterdump
圖 19. 流量符合 policy之流量圖 
 
NetFPGA openflow_switch 封包流量
Sec0 100 200 300 400 500 600
to
ta
l t
hr
o
u
gh
pu
t (
M
B
yt
e)
0.0
0.5
1.0
SNMP
counterdump
圖 20. 流量不符合 policy之流量圖 
 
(3) openflow_switch、SNMP 與 policy
整合 
網路管理者透過 openflow_switch
中的 ofprotocol 軟體可控制流量的進
出，但是 openflow_switch卻沒有提供
適當的 policy 可對網路進行智慧化的
控管。對此問題，我們將在這部分說
明如何利用 SNMP 所讀取的流量資
訊，結合 ofprotocol的操作以及簡單的
policy制定，實作一簡單的網路管理，
並解決 OpenFlow 實驗架構與測試部
分所面臨的問題。 
如圖 18所示，在 openflow_switch
架構上，我們提供一個 flow monitor
監測網路流量。flow monitor結合修改
過的 SNMP 擷取網路流量資訊，解決
原本網路流量無法讀取的問題，再利
用 ofprotocol 此操作 openflow_switch
之程式，並透過簡單的 policy，實現簡
單的網路管理。 
當使用者 A透過 openflow_switch
欲往使用者 B 傳送流量時， flow 
monitor會透過修改後的 SNMP即時監
控使用者 A 傳送的網路流量，當流量
符合 policy的規定時，flow monitor則
不執行任何動作，openflow_switch 則
照原本之行為與 NOX controller 溝通
並傳送流量至使用者 B，實驗結果如
圖 19 所示。當流量不符合 policy 規
定，則 flow monitor則透過 ofprotocol
之程式，將使用者 A 所使用的
NetFPGA 網路埠關閉，以防止使用者
A 所傳送大量的網路流量影響整個網
路環境，實驗結果如圖 20所示。 
 經過以上三個部分之發展，實作
出一個教學用以政策為基礎之網路管
理，不僅可作為網路管理教學參考，
並借此訓練網路管理者之能力與素
質，使其具備網路管理能力。未來如
何在NetFPGA實現更完備的網路管理
者之訓練與教育，將是網路管理教育
發展之重點。 
 
4.3 規劃建置於虛擬化網路架構之以
政策為基礎之網路管理系統 
在此章節中，將描述已規劃之虛
擬化之網路架構、結合虛擬化網路架
構之網路管理系統，以及整合以政策
為基礎之網路管理系統。 
虛擬化之網路架構基礎模型可由
圖 21 表現出，從實體網路架構中，可
切割出多個不同之虛擬網路 VN 1 及
VN 2，使多個使用者在同一實體網路
架構中，但是卻處於不同的虛擬網路
中。即不同虛擬網路可使用相同實體
節點，但彼此間互相獨立。 
實驗環境利用四片 NetFPGA 以
 16
法達到虛擬網路架構上之階層式網路
管理。 
網路管理系統之架構、模組互動
以及與被管理元件之溝通，如圖 24所
示，此架構提供 Web-based 的使用者
介面供各個管理者操作，如虛擬網路
管理介面 (virtual management user 
interface) 、 政 策 管 理 介 面 (policy 
management user interface)、控制管理
介 面 (control management user 
interface)，透過這些介面以及驗證、授
權 與 稽 核 模 組 (Authentication, 
Authorization and Accounting 
Module，簡稱 AAA 模組)，可控制階
層式網路管理架構中，各虛擬網路管
理者之權限。 
Policy 模組及 Policy 資料庫負責
政策規則之制定、執行與管理，如資
源存取或權限控制等政策，政策則儲
存在資料庫中。由 Virtual Network模
組取得 Virtual Network狀態，並進行
Virtual Network之控制，如建立 Virtual 
Network 或刪除 Virtual Network 等動
作。另外並透過 Control模組進行網路
元件之控制與管理，如控制 SNMP 套
件取得資料或開關網路卡之網路埠
等，並透過 OpenFlow Protocol與被管
元件溝通。 
被管理之元件則透過 Controlling 
Interface 對外透過 OpenFlow Protocol
及 SNMP Protocol 與網路管理系統溝
通，對內則與作業系統之 Linux Kernel
及 PC 上裝載之 NetFPGA設備互動，
控制實體網路，並完成建立 Virtual 
Network 等功能，而 NetFPGA 也可直
接透過 Kernel Module 直接由 Linux 
Kernel 取得相關資料，或提供資料給
Linux Kernel。 
5. 結論 
在 4.1以及 4.2章節所做之研究成
果，已分別發表至「2010 全國網際網
路暨通訊科技研討會」，以及投稿至
「2010 數位生活科技研討會」審查。
未來將會針對虛擬化網路架構及以政
策為基礎之網路管理系統持續進行研
究、設計與實作。 
現行之網路架構越來越複雜龐
大，需求日趨複雜，網路環境對於網
路管理只會越趨苛求。如何讓使用者
享受速度更快、頻寬更大且隨時隨地
都可以存取的服務，是未來在網路管
理中一個重點研究的方向。 
 
6. 參考資料 
[1]. The Network Simulator - ns-2. http: 
//www.isi.edu/nsnam/ns/ 
[2]. QualNet. http://www.scalable-netw 
orks.com/products/qualnet/ 
[3]. Stanford University, NetFPGA 
Homepage. http://www.netfpga.org 
[4]. G. Gibb, J. W. Lockwood, J. Naous, 
P. Hartke, and N. McKeown, 
“NetFPGA－An Open Platform for 
Teaching How to Build 
Gigabit-Rate Network Switches 
and Routers,” Journal of IEEE 
Transactions on Education, vol. 51, 
no. 3, pp. 364-369, Aug. 2008. 
[5]. SmartBits. http://www.spirent.com/ 
Solutions-Directory/Smartbits.aspx 
[6]. iperf Project. http://iperf.sourcefor 
ge.net/ 
[7]. G. Watson, N. McKeown, and M. 
Casado, “NetFPGA: A Tool for 
Network Research and Education,” 
Proceedings of the 2nd Workshop 
