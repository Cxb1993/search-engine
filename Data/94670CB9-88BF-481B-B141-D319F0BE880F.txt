I 
 
行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   
□期中進度報告 
 
一個關於不使用影片分割遮罩的場景背景建立技術 
與快速場景背景建立之研究 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 97－2221－E－009－137－ 
執行期間： 97 年 8 月 1 日至 98 年 7 月 31 日 
 
計畫主持人：陳玲慧 教授 
共同主持人： 
計畫參與人員：  
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
■赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立交通大學資訊科學與工程研究所 
 
中   華   民   國  98  年  8  月   30  日 
 
II 
Abstract 
 
Sprite coding adopted in MPEG-4 can increase the coding efficiency of backgrounds greatly. 
The averaging blending used in MPEG-4 VM’s sprite generator makes some places ever 
occupied by moving objects look blurring, and providing automatically segmented masks for 
moving objects is suggested. However, automatic segmentation can not produce perfect object 
segmentation masks. Segmentation faults in masks causes some moving objects being blended, 
and makes ghost-like shadows appear in a sprite. A sprite generation without segmentation masks 
is proposed. The proposed generator consists of two novel methods: a balanced feature point 
extraction method and an intelligent blending method. The feature point extraction method 
estimates the motion of background, and excludes pixels of moving objects from the feature 
points. Intelligent blending blends only background pixels into a sprite by a counting schema. 
Experiments show the feature points extracted by the proposed method increases the accuracy of 
global motion estimation, and the quality of generated sprites is increased. The proposed 
intelligent blending excludes pixels of moving objects directly in the blending procedure, and 
shadows caused by segmentation faults is not exist. The visual quality of our sprite is close to that 
using manually segmented masks and is better than that generated by Smolic et al.’s method. 
Due to the geometric transformation applied in sprite coding, a sprite is distorted and its 
available view angles are restricted. To solve this problem, multiple sprite generation method is 
proposed by Farins with an exhaustive search to find the optimal partition and reference frames. 
Let N be the number of frames, it requires O(N3) time and O(N2) space. A fast multiple sprite 
partition method is proposed to reduce the complexity. The proposed method includes a fast 
partition point finding method and a fast reference frame finding method. The proposed partition 
point finding method measures translation and scaling between frames and finds candidate 
partition points. The final partition positions are decided from these candidate points, and 
reference frames of each partition are found by the proposed fast reference frame selecting 
method. Let M candidate partition points are found, the proposed method requires only O(M2N) 
in time and O(M2)+O(N) in space. The total size of generated sprites is only slightly higher than 
that of Farin’s method. 
2 
blending, a frame is divided into reliable, unreliable, and undefined regions according to the 
segmented masks, as Fig. 2.1 shows. 
    
(a)                       (b) 
Fig. 2.1.  The reliability masks.  (a) A segmentation mask.  (b) Extracted reliability mask. 
 
The reliable and unreliable pixels are average-blended separately, and the blended pixels 
with the highest reliability are chosen into the sprite. The undefined pixels do not contribute to 
the sprite blending. The given distance from the mask border must be large enough to cover all 
segmentation faults, or the generated sprite will have ghost-like shadows in some places. 
However, it is hard to decide the distance automatically. 
 
2.2 Survey on Multiple Sprite Generation 
The performance of a sprite generator is limited to the perspective motion model applied in 
the MPEG-4 VM’s framework. The perspective model projects each frame of a video sequence 
into a planar coordinate system, which is the coordinate system of the reference frame, as shown 
in Fig. 2.2. The reference imaging coordinate system for a sprite is assumed to be the first frame 
that is denoted as frame A. All the following frames must be projected to this reference system by 
geometric transformation. As the camera rotates, transformed frames are geometrically distorted, 
this phenomenon can be found between frame B and transformed frame B. If camera rotation 
continues, from Fig. 2.2, we can see that frame C can not be projected to the reference system. 
 
Fig. 2.2  Geometric distortions using two sprites. 
 
Massey and Bender proposed to use the middle frame of a video sequence as the reference 
frame [15]. The generated sprite will be much symmetric and the boundary area of the generated 
sprite becomes much smaller if the background of the frames in the video sequence pans toward 
reference coordinate of sprite #1 
frame A and transformed A on sprite #1 
transformed B on sprite #2 
frame B 
camera 
frame C 
transformed C on sprite #2 
reference coordinate 
of sprite #2 
transformed B on sprite #1 
4 
 
The border area of a frame is excluded. The rest of frame is divided into non-overlapping 
blocks. For each block, the gray value variance is calculated to test its homogeneity. The 
homogeneous blocks are discarded, and feature points are extracted in the non-homogeneous 
blocks only. 
Suppose that we want to extract N feature points from K non-homogenous blocks, N/K 
pixels with largest absolute Hessian values are chosen in each non-homogeneous block. Feature 
points extracted from Fig. 3.1(a) using the proposed method are shown in Fig. 3.1(c). In contrast 
to the result of using the MPEG-4’s method shown in Fig. 3.1(b), the distribution of feature 
points using the proposed method is more balanced. Several points on the white line in the 
half-bottom of the frame are extracted as feature points, this will make the white line registered 
well and will significantly improve the visual quality of the generated sprite. 
However, from Fig. 3.1(c), we also find some points on the player located. These points 
should be removed according to the information in segmentation masks, but we do not want to 
have masks in our system. These points reduce the accuracy of estimated GMPs. 
Since the motions of moving objects usually differ from the motion of background, this 
provides us a clue to remove these outliers. Traditional translation-based motion estimation is 
applied on each feature point to find the motion vector relative to the previous frame. In order to 
reduce the searching time, a global translation is found based on some selected feature points first, 
then a full search around the global translation for each feature point is preformed. First, the 
global translation is found by the most occurrence motion vector of 100 pixels with the largest 
absolute Hessian values in the feature points found previously. Then motion vector of each 
feature point is compared with the global translation. Feature points with motion vector differ 
from the global translation is classified as moving objects and is discarded from the feature points. 
Fig. 3.2(a) illustrates the object pixels found from the feature points shown in Fig. 3.1(c). The 
feature points on the player are detected successfully. These object pixels are removed from the 
original feature points and the final feature points are shown in Fig. 3.2(b). 
   
(a)                      (b) 
Fig. 3.2  Outlier removing.  (a) Detected object pixels in Fig. 3.1(c).   
(b) The feature points after removing outliers from Fig. 3.1(c). 
 
Figs. 3.3(a) and (b) show a close view of sprites generated using the MPEG-4 VM’s method 
and proposed balanced feature point extraction method, respectively. The same number of feature 
points is used in both methods. From the figures, we can see that those white lines in the sprite 
generated by the proposed method are registered very well. While the same place in sprite 
generated by conventional method looks blurred. The proposed method achieves much better 
visual quality. 
6 
 
Fig. 3.5  Flowchart of the intelligent blending. 
 
In the starting of the blending, since S is undefined, it is set to be gray value of the first 
incoming pixel and CS is set to one. After filling S, the similarity check is conducted. If an 
incoming pixel is unlike S (i.e. DS is greater than a preset threshold T) and C is undefined, C is set 
to the gray value of the incoming pixel and CC is set to one; otherwise, if DS is smaller than T and 
DC, the incoming pixel is considered as a background pixel and is blended into S, CS is increased 
by 1. If DC is smaller than T and DS, the incoming pixel is blended into the candidate C, CC is 
increased by 1. Two counters are compared when a pixel is blended into the candidate C. If the 
candidate counter is larger than the sprite counter, the sprite and the sprite counter are replaced by 
the candidate and the candidate counter. Then the candidate and its counter are reset to undefined 
and zero, respectively. This replacement is based on the fact that being described before. Since 
the candidate appears more frequently than the current sprite, the candidate is more likely to be 
the background. 
If both DS and DC are larger than T, the candidate is replaced by the incoming pixel, and the 
candidate counter is set to one. With a series of incoming object pixels, the candidate is 
continuously replaced by the incoming pixels until a background pixel is replaced into the 
candidate. If the background pixels appear continuously, the accumulation of candidate counter 
will begin. 
Blending results using the Smolic et al.’s reliability-based blending and proposed intelligent 
blender are shown in Fig. 3.6. We can see that the ghostlike shadows are eliminated and the sprite 
is still clear and sharp. 
   
(a)                      (b) 
Fig. 3.6  Generated sprite using different blending methods.   
(a) Smolic’s reliability blending.  (b) The proposed method. 
 
8 
   
(a)                    (b) 
   
(c)                    (d) 
Fig. 3.8  Close views of the generated sprites.   
(a) Part A by Lu et al.  (b) Part A by the proposed method.   
(c) Part B by Lu et al.  (d) Part B by the proposed method. 
 
4. Proposed Fast Multiple Sprite Generation Method 
In this section, a fast multiple-sprite partition method is proposed. The proposed method 
reduces the searching time for finding an applicable partition for multiple sprite generation, and 
the memory required during the searching is also decreased in contrast to the optimal partition 
method. It consists of two algorithms: video partition algorithm and a reference frame selection 
algorithm. The video partition algorithm is developed based on the characteristics of frame 
translations and scaling. Since the geometric distortion depends on the accumulated global 
translation relative to the reference frame, the accumulated global translation provides a good 
measurement on the distortion. The effect of frame scaling caused by camera zoom-in or 
zoom-out can be employed in a similar way. A reference frame selection algorithm is developed 
based on the idea of Messey and Bender’s work. In their work, the middle frame of a video 
sequence is suggested as the reference frame, since its background has higher possibility to be 
located at the center of a generated sprite. The proposed algorithm extends this idea by taking the 
frame with its background most likely being at the center of the corresponding sprite as the 
reference frame. 
 
4.1 Proposed Feasible Partition Points Selecting Method 
Two types of feasible partition points, translation-based and scaling-based, are found in this 
section. These points are possible partition points of the video, and the proper partition points are 
decided later in Section 4.3. 
 
4.1.1 Translation-based Feasible Partition Points 
The geometric projection distortion comes from the camera rotation. Farins’ experiments [16] 
show that the sprite area grows exponentially as camera pan angle increases. Thus the selecting of 
partition frames must be highly related to the effect of camera rotation. In order to capture the 
10 
feasible partition points (x-axis and y-axis) are found. The final feasible partition points are the 
union of the x- and y-axis partition points (FPX and FPY). 
 
4.1.2 Scaling-based Feasible Partition Points 
The scaling is calculated in a similar way as the translations. Each frame is geometrically 
transformed to its previous frame and the area of transformed frame is calculated. The scaling 
factor is calculated by dividing the area of transformed frame to the area of its previous frame. 
Then the accumulated scaling is calculated by similar recursive procedure: 
 )1()1(
1
)1( --
+=
- ´== Õ jjij
j
ik
kkji sassas , (4.3) 
where )1( -jjs  is the scaling factor of frame j versus frame j-1. The accumulated scaling factors 
are illustrated in Fig. 4.2. 
 
0 50 100
1
2
3
4
5
frame number
ac
cu
m
ul
at
ed
 s
ca
lin
g 
fa
ct
or
sequence 'tabletennis'
#50
#76
#51
#78
 
Fig. 4.2  Accumulated scaling factors of ‘tabletennis’. 
 
The quality degradation of a reconstructed frame is higher as the scaling between the frame 
and its reference frame increases. Therefore, the accumulated scaling of frames in a single sprite 
is limited by the ratio of the maximum and minimum accumulated scaling factors of frames in a 
sprite generated from frame k to frame h. That is, 
 
{ }
{ } thikas
hikas
iki
iki <
£<
£<
|min
|max
, (4.4) 
where t is a threshold. We process frames in a video sequence sequentially. When Eq. (4.4) is not 
satisfied at a certain frame h, frame h is considered as a feasible partition point. We keep 
processing the remaining frames with frame h as the starting frame to find all feasible partition 
points based on Eq. (4.4). 
The fixed threshold t in Eq. (4.9) has a disadvantage: the scaling ratio of the last partition 
will be smaller than that of the other ones. In the other words, the last partition covers less scaling 
range than others. This can be solved by applying an adjusted threshold 
 { }L ii Niast ££=¢ 1|max 1 , (4.5) 
where L is the number of feasible partition points found based on Eq. (4.4) and threshold t, and N 
is the number of frames in the video sequence. The feasible partition points, FPS, based on the 
accumulated scaling factors are searched again by recalculating Eq. (4.4) with the adjusted 
12 
 
Table 4.2  Experimental results of sequence ‘tabletennis’. 
 Partitions (reference frames) Total sprite size (bytes) 
Executing time 
(seconds) 
Using a single sprite - 620,044 - 
Farin et al.’s optimal method 1-49 (48), 50-75 (52), 76-131 (76) 177,766 95 
Proposed method 1-51 (4), 52-77 (52), 78-131 (78) 220,964 2.7 
 
4.5 Complexity Analysis 
Complexity can be discussed in two different ways: time and space. Both complexities of 
the proposed and optimal method are discussed. 
The complexity of Farins’ optimal method is divided into two parts: the building of coding 
cost matrix and the optimal partition algorithm. While building the cost matrix, the coding cost of 
all sub-sequences beginning at frame i and ending at frame k with reference frame r must be 
computed. Suppose that the sequence has N frames, the time and space complexity of building a 
cost matrix will be N3. However, they had developed a method to reduce the space required to N2. 
The optimal partition algorithm finding the best partition frame-by-frame takes N2 time. 
The proposed method calculates the accumulated translation and scaling first, and both of 
them take linear time. The finding of candidate partition points is also linear time because it only 
observes the changes of accumulated translation and scaling once. Let M be the number of 
candidate partition points found, finding reference frame for all possible sub-sequences takes 
M2N time. Finally, the Farins’ optimal partition is applied. Since only M candidate partition points 
are involved, it takes only M2 time. The accumulated translations and scalings must be hold in 
memory and the coding-cost matrix must be generated. These will need 2N+M2 space. 
Table 4.3 shows the complexity of both methods. Since M is usually very small in contrast to 
N in practical, for example, M=9 and N=300 in the sequence ‘stefan’, this makes the complexity 
of the proposed method better than the optimal method. 
 
Table 4.3  Complexity comparison. 
 Time Space 
Farin et al.’s optimal method O(N3) O(N2) 
Proposed method O(M2N) O(M2)+O(N) 
 
14 
Technology, vol. 16, no. 4, pp. 492-506, Apr. 2006. 
 
 
已發表論文 
 
(1) I.S. Kuo and L.H. Chen, “A High Visual Quality Sprite Generator using Intelligent Blending 
without Segmentation Masks,” International Journal of Pattern Recognition and Artificial 
Intelligence, vol. 20, no. 8, Dec. 2006, pp. 1139–1158. 
 
(2) I.S. Kuo and L.H. Chen, “A Fast Multi-Sprite Generator with Near-Optimum Coding 
Bit-Rate,” International Journal of Pattern Recognition and Artificial Intelligence, vol. 23, no. 2, 
Apr. 2009, pp. 331-353.  
 
16 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：98年 8月 30日 
國科會補助計畫 
計畫名稱：一個關於不使用影片分割遮罩的場景背景建立技術與快
速場景背景建立之研究 
計畫主持人：陳玲慧 教授 
計畫編號：NSC 97－2221－E－009－137－ 學門領域：影像處理 
技術/創作名稱 無需物件分割遮罩之場景背景產生系統 
發明人/創作人 陳玲慧 
中文：我們將提出一個不需要物件分割遮罩的場景背景產生系統。
所提出的系統包含兩個新方法：均勻化特徵點擷取方法與智慧型影
像疊合方法。兩個方法均能夠自動排除物件對場景背景的影像，並
有效的產生高品質場景背景。 
 
技術說明 
英文：A sprite generation without segmentation masks is proposed in 
this project. The proposed sprite generator consists of two novel 
methods: a balanced feature point extraction method and an intelligent 
blending method. The two proposed methods exclude the moving 
objects automatically from the generated sprite. The system achieves 
high visual quality sprites. 
可利用之產業 
及 
可開發之產品 
MPEG-4編碼軟體、硬體，硬體編碼晶片製作。 
技術特點 
1. 平衡式的特徵點選取系統能夠較均勻的抽取畫面中的特徵點，
並提高所產生之全域運動向量(global motion vector)之精確度。 
2. 智慧型影像疊合系統，利用簡單的計數方式，避免移動物件被
混合入場景背景當中。 
推廣及運用的價值 
本技術不需要物件分割遮罩亦可產生場景背景。同時亦可提高產生
之場景背景的視覺品質。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 1
出席國際學術會議及參訪心得報告 
2009/7/7~2009/7/16 
                                                             
計畫編號 NSC 97-2221-E-009 -137  
計畫名稱 一個關於不使用影片分割遮罩的場景背景建立技術與快速場景背景建立之         研究 
出國人員姓名 
服務機關及職稱   陳玲慧   交通大學資訊工程系 教授     
會議時間地點 大陸保定  並參訪北京交通大學及保定河北大學 
會議名稱 機器學習與控制理論國際會議 
發表論文題目 
共四篇 
1. A new steganography method via various animation timing effects in 
powerpoint files 
2. A novel method for shoeprints recognition and  classification 
3. Solving Japanese puzzles with logical rules and depth first search algorithm 
4. A real-time unusual voice detector based on nursing at home 
 
 
一、參加會議經過 
 這次的大陸之旅先抵達北京，拜訪了北京交通大學且參觀了計算機與訊息技
術學院，並與須德教授進行了學術交流。須德教授主要研究領域為：MIS、Database
和 Multimedia technology。在這次的交流中，須教授展示了 Color Consistence 和
Video Segmentation 方面的成果，而我們則展示了在 Flower identification 和 Face 
Recognition 方面的成果。除了展示成果外，雙方老師學生交流互動良好。之後
跟隨須德教授和趙耀教授參觀了他們的實驗室。在趙教授的實驗室看到了使用手
機透過無線網路傳輸資料的即時監控系統，雖然傳輸的畫面看起來有點 Lag，但
是整體而言勉強可以達到即時的標準。此外還看到了校園 3D Model 參觀系統，
透過 3D 立體眼鏡確實可以感受到整個校園的建築變得更為立體化。3D Model
的製作可以稱得上是細緻，不過整體立體感有點太過度了，缺乏真實性。但是這
應該只是參數上設定的問題，瑕不掩瑜，實為一佳作。在參觀實驗室的過程，也
了解到了北京交通大學與我們交通大學之間的差別。實驗室條件，電腦設備等硬
體條件上，北京交通大學至少落後我們五到十年。但是，學生的求學態度和實際
在實驗室從事研究的時間，比起我們的學生有過之而無不及。根據須德教授、趙
耀教授及兩位的博士班學生的描述，他們大約都早上八、九點來實驗室開始從事
研究，直到晚間十一、二點才離開。這也許是跟北京交通大學的修業年限有關，
 3
學者參加。 
 
另外此次研討會涵蓋領域很廣。擴展了我們的視野，也認識了不少同領域及不同
領域之學者。在和北京交通大學交換意見時，談到大陸的急速發展，產生很多問
題，如倫理觀念漸失，對台灣的民主、有制序、整潔及服務頗為欣賞。整體而言，
此次出國，不管在學術方面或與國際學者交流都有不少的收穫。 
 
三、攜回資料名稱及內容  
1. 2009 機器學習與控制國際研討會（論文集）。  
2. 2009 機器學習與控制國際研討會（論文光碟）。  
 
 2
他們的博士班學生平均四、五年畢業，最多八年內要畢業。相比於我們的博士班
學生平均五、六年畢業，最多十年畢業，中間有著兩年左右的差距。這部份值得
作為我們改進教育的一個參考點。 
 
 結束北京的行程之後前往保定參加 ICMLC2009，並於此國際會議 present 四
篇論文。ICMLC2009 的論文分佈在很多領域：Intelligent Systems、Imaging 
Processing、Data Mining and Knowledge Discovery、Stochastic Systems Algorithm 
and Applications、System Controls and Signal Processing、Fuzzy System、Feature 
Selection and Manifold Learning、Data Fusion and Ensemble Methods、
Computational、…etc。其中有幾個有趣的議題：Jose M. Carmena 所提出的
Brain-Machine Interfaces (BMI) ，這個技術是利用電子訊號來模擬人腦所發出的
訊號，為近幾年來廣為發展。 
 
在我們這次發表的四篇論文中，以居家看護即時聲音監視系統的引起最多的
注意和興趣，此論文提出一有效的方法，能即時分析聲音資訊，從中辨識出咳嗽
聲、求救聲、呻吟聲和喘息聲這四種緊急求救訊號，並給予監護人員通知。有提
問者提到是否能在原有的四種求救訊號之外增加其他種類的求救訊號之辨識，此
部份將會是未來研究的一個方向。另外也有提問者也建議可向政府安全衛生單位
提計畫，畢竟即時監視系統是目前的熱門科技，而即時監護系統則是一個非常熱
門且重要的項目。 
 
 最後在保定前往了河北大學，先參觀計算機中心所提供的電腦教室的電腦設
備，螢幕大多還停留在 CRT 階段，只有約 1/4 更換為 LCD。根據當時電腦教室
的管理員的敘述，只有少部份的學生擁有自己的個人電腦，半數以上的學生都還
是前來電腦教室使用電腦，設備水平大概落後台灣五到十年。不過儘管是暑假期
間，大部分的實驗室還是看得到許多博士班學生在唸書進行研究。之後參觀了圖
書館，其中古代文獻收藏室，收藏了大量的以前的文獻，最古老的可追述至宋朝。
但是，收藏室設備並不先進，進去即可聞到大量的樟腦味。珍貴古書也皆未數位
化，僅以玻璃櫃保存，大多的古書文獻只是放置於書架之上，跟浩然圖書館的一
般圖書並無兩樣。 
 
二、與會心得 
 
此次出國主要是參訪北京交通大學及保定河北大學，並參加於保定舉辦的機器學
習與控制理論國際會議(ICMLC)，ICMLC 是機器學習與控制理論的主要國際會
議。已為EI Compendex收錄。每年有來自全世界的研究人員聚集一起，相互交流。
此次本人共發表了四篇論文，由本人指導的三位博士生分別上台以英文發表論
文，三位博士生都有不錯的表現。也和一些國外學者做些討論。台灣也有數十位
Proceedings of  the Eighth International Conference on Machine Learning and Cybernetics, Baoding, 12-15 July 2009 
978-1-4244-3703-0/09/$25.00 ©2009 IEEE 
2368 
A REAL-TIME UNUSUAL VOICE DETECTOR BASED ON NURSING AT 
HOME 
MIN-QUAN JING, CHAO-CHUN WANG, LING-HWEI CHEN 
Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 300, ROC 
E-MAIL: mqjing@msn.com, lupin@debut.cis.nctu.edu.tw, lhchen@cc.nctu.edu.tw 
Abstract: 
In this paper, we will propose a method to detect an 
unusual voice for nursing system. Based on the healthy 
condition of a person, we define four kinds of unusual voices 
including cough, groan, wheeze and cry for help. When the 
person nursed sends out the unusual voices, we judge that his 
health condition have a doubt, and need someone to pay 
attention. In order to detect the unusual voices, we extract five 
features on audio waveform, including the number of 
segmented parts, duration of waveform, mean of volume, zero 
crossing rate and correlation. Experimental results show that 
the detection rate is 94%~97% for these four kinds of unusual 
voices. In false alarm, there are only 0.08% of wrong rates.  
Keywords: 
Nursing system; Cough; groan; Wheeze; Cry for help; 
Zero crossing and correlation 
1. Introduction 
While the physically handicapped are receiving 
increasing concern in modern social welfare, it is necessary 
to develop a home nursing system to reduce the social costs 
and labor costs. Most home nursing systems track user 
health condition by means of video or electronic devices. 
Huang et al. [1] employ video to track if users get the 
attention of medical staff for their alleged health problems 
with voluntary warnings signified by special actions, such 
as pointing out their index finger. Such an approach is 
questionable. First, whether or not a person having a 
physical problem and needing immediate medical attention 
can correctly make a help sign is an open case. Second, 
users under video surveillance have no privacy at all. 
Huang and Wu et al. [2] installed a device on user to detect 
the electrocardiograph (ECG) in order to determine the 
health condition and send back to a medical center. In this 
case, users are in a passive situation and are given no 
choice in voluntarily sending critical voice signals. In 
current state, voice detection techniques can be used on 
nursing system. Zwicker [3] points out that an ordinary 
human voice falls within a lower frequency range. Based on 
their work, audio signals at a lower frequency can be 
considered as speeches. Saunders [4] employs the 
normalized zero-crossing rate (ZCR) as the feature to 
distinguish speeches from non-speeches. Abu-El-Quran and 
Goubran [5] identify speeches and non-speeches by 
calculating the pitch of audio signals with the pitch ratio. 
However, these two-way classification methods are 
inadequate in practice. The multi-way classification 
methods are receiving increasing concern. Wyse and 
Smoliar [6] classify ordinary audio signals into music, 
speech and other. Kimber and Wilcox [7] further classify 
audio signals into speech, silence, laughter and non-speech, 
marking a step forward in the two-way classification. Lastly, 
Lin and Chen [8] classify audio signals into five classes: 
pure speech, music, solo (unaccompanied) human singing, 
background music speech, and background noise speech. 
Though these techniques can effectively classify audio 
signals, they fail to further classify pure speech in greater 
detail. In this paper, we develop a voice-detection-based 
nursing system which classify pure speeches into severe 
coughing, groaning, wheezing and help-asking sound, and 
sounds that do not belong to the these four classes of 
signals. Experimental results show that the delectability of 
the proposed method is up to 94-97%; while the false alarm 
rate is only 0.08%.  
The remaining of the paper is described as follows. In 
Section 2, the feature vectors of the four critical voices 
(help-asking, groaning, coughing, and wheezing) and the 
extraction method are defined. Section 3 presents an 
algorithm to classify critical voice signals with these 
features. In Section 4, some experimental results are given 
to demonstrate the effectiveness of the proposed method. 
Section 5 makes a conclusion. 
 
2. Feature Extraction and Selection 
Four critical voice signals are defined as audio 
keywords: help-asking sound, groaning, severe coughing 
Proceedings of  the Eighth International Conference on Machine Learning and Cybernetics, Baoding, 12-15 July 2009 
2370 
five-second audio signal; S(n) and E(n) the start 
and end points of the n-th segment expressed in 
sampling point; and f(x) the volume value. 
z Duration ( nD ): the duration of each voiced segment 
calculated as shown in Eq. (2-4): 
)()( nSnEDn −=  , (2-4)
where n is n-th segment expressed in sampling 
point. 
z Correlations (C): the correlations between the form of 
voiced segments and distinctive form of various 
sounds. How to obtain the distinctive form of various 
sounds and calculate their correlations are described 
below. 
z Normalized ZCR: the frequency of the up and down 
vibrations passing through the zero point of all 
segmented voiced segments divided by the total 
duration of all voiced segments as shown in Eq. (2-5): 
( )∑
=
−
= N
n
nSnE
ZCRNZCR
1
)()( 
, 
(2-5)
where ZCRN is the frequency of the up and down 
vibrations passing through the zero point of all 
segmented voiced segments within a five-second 
audio signal. 
2.3. Feature Statistics and Selection 
Table 1 shows the statistics results on the samples 
according to the features. We can see that the mean volume 
and normalized ZCR of coughing is significantly different 
from that of other sounds. Therefore, these two items will 
be the distinctive features of coughing. 
 
Table 1. Feature Statistics 
 N Mean Volume
Duration 
nD  
Normalized 
ZCR 
Help-asking 
Sound 3~4 
1000 
~1250
11800 
~14900 
0.08 
~0.12 
Groaning 1~2 800 ~2200
17800 
~29200 
0.11 
~0.115 
Coughing 3~11 5500 ~7500
1200 
~18400 
0.14 
~0.19 
Wheezing 5~6 400 ~1000
3500 
~6500 
0.21 
~0.24 
Next, the number of segments (N) and segment duration 
( nD ) are selected as the distinctive features of help-asking 
sound, groaning and wheezing. We explain the 
determination of the thresholds and the threshold ranges in 
Tables 2-6.  
 
Table 2. Mean, Variant and Feature Threshold of the Mean 
Volume of Coughing. 
Mean Volume 
 
MMean  MVariance  
Coughing
23
23
1
∑
=f
fM
 
2/123
1
2
23
)(
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−∑
=f
Mf MeanM
 
 
Table 3. Mean, Variant and Feature Threshold Range of the 
Normalized ZCR of Coughing. 
Normalized ZCR 
 
ZCRMean ZCRVariance  
Coughing
23
23
1
∑
=f
fZCR
2/123
1
2
23
)(
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−∑
=f
ZCRf MeanZCR
 
Threshold 
Range ZCRZCRZCRZCRZCR
VarianceMeanTRVarianceMean +≤≤−
 
Table 4. Mean, Variant and Feature Threshold Range of the 
Duration of Help-asking Sound. 
Duration  
HDMean HDVariance  
Help-asking 
Sound DN
HD
DN
f
f∑
=1
 
2/1
1
2)(
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−∑
=
DN
MeanHD
DN
f
HDf
 
Threshold 
Range HDHDHDHDHD
VarianceMeanTRVarianceMean +≤≤−
 
Table 5.  Mean, Variant and Feature Threshold Range of the 
Duration of Groaning. 
Duration  
GDMean  GDVariance  
Groaning DN
GD
DN
f
f∑
=1
 
2/1
1
2)(
⎟⎟
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜⎜
⎜
⎝
⎛
−∑
=
DN
MeanGD
DN
f
GDf
 
Threshold 
Range GDGDGDGDGD
VarianceMeanTRVarianceMean +≤≤−
 
 
 
 
 
 
Proceedings of  the Eighth International Conference on Machine Learning and Cybernetics, Baoding, 12-15 July 2009 
2372 
the following equation is introduced 
)(
)()(
iRSN
iDiR n= ; where 
3 ,2 ,1=i (1: help-asking sound; 2: groaning; and 3: 
wheezing). In practice, )(iR  intervals on the waveform 
are merged into one point, and the mean of the absolute 
values of )(iR  interval is the value of the merged point. As 
a result, the help-asking sound, groaning and wheezing are 
expressed in 13, 23 and 5 sampling points respectively. 
Observation shows that there are 3 subclasses in the 
similarity of the spectrogram of segmented voiced segments 
of help-asking sound as shown in the first voiced segment 
in Fig. 3. The same result is also found in groaning, which 
is also further classified into 3 subclasses as shown in the 
first voiced segment in Fig. 4.  
 
  
(a) Waveform of 
Subclass 1 of 
Help-asking. 
(b) Waveform of 
Subclass 2 of 
Help-asking. 
(c) Waveform 
of Subclass 
3 of Help- 
asking. 
Figure3. The waveforms of subclasses of Help-asking Sound.
 
  
(a) Waveform of 
Subclass 1 of 
Groaning. 
(b) Waveform of 
Subclass 2 of 
Groaning. 
(c) Waveform 
of Subclass 
3 of 
Groaning. 
Figure4. The waveforms of subclasses of Groaning Sound. 
 
The voiced segments of wheezing are the most stable 
and require no further classification. The mean voiced 
segment of each subclass is selected as the control voiced 
segment for the system to perform comparisons. Details of 
the calculation of the mean voiced segment are as follows. 
All voiced segments in a subclass: nk XXXX ""21 ; 
where n  is the number of segments within the voiced 
interval. ),( )(21 iRSNkkjk
kk xxxxX ""= , nk ≤≤1 , where 
k
jx  is the re-sampling point and i  is the class. Mean 
voiced segmet =X ),( )(21 iRSNl xxxx "" , )(1 iRSNl ≤≤ , 
where =lx  ),,( 21 nlll xxxMEAN " . When running the 
program, the system will re-sample all segmented voiced 
segments into )(iRSN  number of sampling points to 
perform the correlation calculation of the mean voiced 
segment of the i -th class. The correlation calculation is 
shown in Eq. (3-1), where X is the mean voiced segment, 
and Y the re-sampled voiced segment segmented by the 
system. Then, the class of the segmented voiced segment, 
i.e. help-asking sound, groaning or wheezing, is determined 
based on the greatest value. 
)()(
),(),(
YVarXVar
YXCovYX =ρ  (3-1)
3.2. Determination Criteria and System Process of 
Various Sounds 
Concluding the above analysis results (Table 7), and 
the results of calculation of the correlations between the 
mean voiced segments and samples of various classes in the 
database, the distinctive features and optimal threshold 
range of various critical voice signals are determined. The 
main judgment process of the system is illustrated below. 
 
 Silence Detection: N=0; when there is no voiced 
segment segmented in a five-second audio signal after 
processing, it is determined as silence. 
 Coughing Detection: (M>5500) and (0.15<ZCR<0.2); 
when the mean volume and normalized ZCR of a 
five-second audio signal after processing fall within the 
threshold range, it is determined as coughing. 
 Correlation Calculation: is the calculation of the 
correlations between the voiced segments segmented 
from a five-second audio signal and the control mean 
voiced segments of help-asking sound, groaning and 
wheezing in the system. The system will continue to 
compare the following criteria if one of the help-asking 
voiced segments is greater than 0.7; one of the 
groaning voiced segments is greater than 0.9; or one of 
the wheezing voiced segments is greater than 0.8. 
 Help-asking Sound Detection: (N=3 or 4) and 
(Dn=12000~15000); when a five-second audio signal 
meets the correlation calculation requirements of 
help-asking sound, and the number of segments and 
duration of one of the voiced segments falls within the 
