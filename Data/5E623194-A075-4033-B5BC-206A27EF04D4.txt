行政院國家科學委員會補助專題研究計畫  成果報告
用於支援數位生活之智慧型校園視訊監控系統 
Intelligent Campus Video Surveillance System for Digital Life  
計畫編號：NSC96－2221－E－133－001－ 
執行期限：96 年 8 月1 日至97 年7 月31 日 
主持人： 蔡俊明  臺北市立教育大學資訊科學系 
 
一、中文摘要 
傳統的校園大樓門禁系統是以磁卡、鑰匙或密碼為主，其缺點是必須隨身攜帶，容易
造成遺失或忘記，人員管理上，也會有漏洞。由於科技的進步，影像辨識技術可以應用於
門禁監控系統，減少上述問題的發生。所以，本文提出以智慧型的影像辨識技術為基礎，
來做校園大樓門禁監控系統。本系統主要包括人臉偵測和人臉辨識，其中人臉偵測包括膚
色偵測和人臉確認，在膚色偵測中，因人的膚色容易受亮度的影響，所以，首先在不同亮
度下，分別訓練出不同亮度的CbCr值，來做膚色偵測；接著，近距離的人臉，利用人臉中
有眼睛和嘴唇以及上方有頭髮三種特徵，來確認出人臉區域，遠距離的人臉，利用變異度
和頭髮，來驗證出人臉；最後，在人臉辨識中，利用主成份分析法和灰關聯分析法，做人
臉辨識。經由實驗結果顯示，動態人臉偵測率和人臉辨識率分別達97.0％和94.3%。 
 
二、研究背景與目的 
現在的學校，為了學校財產和學童的安全，都裝設了視訊監控系統，當學校遭受小偷
入侵，甚至財產失竊時，可以事後調閱錄影帶，提供警方辦案線索。但是，這是屬於事後
再調查，視訊監控系統無法提供『即時資訊』，立即通知警衛和警方馬上處理。另外，校
園警衛人數不多，要照顧到全校安全、巡邏校園、門口管制人員和車輛的進出等工作，非
常繁雜，所以，學校常被宵小入侵，偷竊財產的事，時有所聞。另外，雖然有裝設視訊監
控系統，但是因為錄到的影像品質不佳，或是因為光線問題，或是因為監控位置設計不佳，
錄影到的影像不是很理想，加上需要額外人力去看影片，造成相關單位許多工作負擔。 
傳統的校園大樓門禁系統是以磁卡、鑰匙或密碼為主，其缺點是必須隨身攜帶，容易
造成遺失或忘記，人員管理上，也會有漏洞。以影像為基礎的生物辨識系統，是近幾年熱
烈研究的題目，它是一個最方便的生物認證方式。各級學校的大樓，如果都能使用人臉辨
識系統，那門禁進出管理會變得很方便很自然，也能節省大樓警衛人力。所以，本計畫提
出一以膚色偵測、人臉確認和人臉辨識為基礎的智慧型校園大樓門禁監控系統，希望來解
決上述傳統的校園門禁管理問題。 
 圖1.1是所提出用於支援數位生活之智慧型校園視訊監控系統之智慧型校園大樓門禁
監控系統流程圖，主要包括兩大部分：系統使用前的訓練和線上系統測試。在使用前的訓
練階段，要先將現有編制人員或新進人員的影像擷取到電腦中，再利用膚色偵測和人臉確
認組成的人臉偵測演算法將人臉偵測出來，最後，抽取特徵存入人臉特徵資料庫中。在線
上系統測試階段，利用CCD擷取影像，同樣，利用人臉偵測模組來偵測人臉，再利用人臉
辨識模組抽取特徵與人臉資料庫比對，若是校方人員，解除門禁，讓其進出，並記錄進出
時間，若不是校方人員，則通知警衛處理，以下章節分別介紹之。 
表 3.1 不同亮度下膚色群聚表 
n 亮度值(Y) Cb 膚色範圍 Cr 膚色範圍 
0 Y ≤50 108≤ Cb ≤131 120≤ Cr ≤145 
1 51≤ Y≤110 83≤ Cb ≤132 123≤ Cr ≤175 
2 111≤ Y ≤180 75≤ Cb ≤135 125≤ Cr ≤185 
3 181≤ Y ≤205 68≤ Cb ≤138 115≤ Cr ≤160 
4 206≤ Y ≤219 65≤ Cb ≤140 115≤ Cr ≤158 
5 Y ≥220 65≤ Cb ≤141 115≤ Cr ≤153 
 
範圍，可以偵測出正確的膚色範圍，其式子如 3.1 所示。 
G),BR/(RRn
220],y)[Y(x, if    5
219],y)Y(x,[206 if    4
205],y)Y(x,[181 if    3
180],y)Y(x,[111 if    2
110],y)Y(x,[51 if    1
50],y)[Y(x, if    0
n
otherwise,    0
], 1b)(a,Map and 0.6)Rn[(0.36 if    1
y)Face(x, n
++=
⎪⎪
⎪⎪
⎩
⎪⎪
⎪⎪
⎨
⎧
≥
≤≤
≤≤
≤≤
≤≤
≤
=
⎩⎨
⎧ =≤≤=
 (3.1) 
其中 a 和 b 分別指 Cb、Cr 的膚色範圍，Y 表示在輸入影像(x,y)位置中的亮度值，n 是指不
同的亮度等級，式子 3.1 的意思是輸入ㄧ個 pixel 並計算它的 Rn 值，假如其 Rn 值的範圍
介於 0.36~0.6 之間，再計算它的亮度值，並判斷它是屬於 0~5 那一個 map 的範圍，再判斷
它的 Cb、Cr 範圍是不是屬於這個 map，若是的話就判定為膚色。 
在偵測完膚色後，因為在背景中會有一些 pixels 的顏色屬於膚色的範圍，這會產生雜
訊。另外，也會有一些膚色沒有事先被訓練到，而造成膚色破碎。所以，利用形態學
（morphology）的膨脹(Dilation) 和侵蝕(Erosion)來處理這些問題。接下來，利用連通成分，
找出膚色的連通區域，因為在背景中可能出現和膚色相接近的原木色桌子或是類似膚色的
壁紙，這些與膚色相近的背景，會產生大的連通成分。依照人體計測資料庫[21]的說明，
人臉的長寬比是固定的，所以，利用長寬比，可以排除非人臉的膚色連通成分，最後，就
得到人臉候選區域。 
(二) 人臉特徵抽取 
 
圖3.1 人臉平均值和標準差的二維分佈圖。 
睛的pixels數目等於 2％候選區的pixels數目，這時的閥值就是最適當的閥值，反之，當眼
睛的pixels數目小於 2％候選區的pixels數目，我們就把閥值加 1 再重新計算一次，直到當
眼睛的pixels數目大於 2％候選區的pixels數目，如此就可以偵測出眼睛後選區。因為眉毛
和鼻孔的灰階度也低，容易被誤認為眼睛，所以，本文利用幾何規則，來確認出真正眼睛
所在，所用的幾何規則如下：兩眼距離是臉寬的 0.2~0.8 倍，兩眼間的斜率必須小於 0.5(27
°)，嘴唇位置在兩眼間，兩眼中心點到嘴唇的距離大於 0.3 倍臉長。 
上述方法是當人臉候選區面積大時，因為人臉上的特徵比較清楚，所以可以抽出頭
髮，嘴唇和眼睛，但是，當人臉候選區太小時，人臉上的特徵不明顯，則無法抽出特徵，
對於這種情況，在此提出以人臉變異度為特徵的抽取方法。首先，我們觀察人臉無論是在
遠方或是近處，或是有旋轉角度，它都具有旋轉不變(Rotation-invariant)[25]的特徵存在，
所以，可以應用此特徵來做人臉驗證工作。由實驗結果發現，人臉候選區保有一定的變異
度，所以，可以事先以訓練的方式找出人臉候選區的變異度範圍，在動態測試的時候，就
可以直接算變異度的範圍，且並判定待測人臉候選區的變異度是否在範圍之內，我們提出
的人臉變異度抽取方法介紹如下。 
首先，輸入800張人臉，並將每一張人臉影像轉成灰階，再來，對灰階影像做Sobel邊
緣偵測，再來，利用低通濾波器來降低高頻雜訊，這樣處理之後，人臉五官的邊緣特徵會
特別明顯，接著，計算和統計每一張人臉的平均值和標準差，其分佈如圖3.1所示，在此圖
中可以看出，由800張影像所描繪出的資料點，其分佈像是兩條拋物線所限制的範圍，所以，
利用拋物線的方程式來逼近分佈圖的上下線，將資料點帶入拋物線標準方程式，可以推導
如公式(3.4)與(3.5)的結果。所以，這兩條拋物線，就是800張人臉影像訓練出的變異度範圍。 
2326.160744.10076.0 1
2
11 ++−= μμσ  (3.4) 
7674.10744.10076.0 2
2
22 −+−= μμσ  (3.5) 
其中σ1, μ1和σ2, μ2分別表示上下拋物線的標準差和平均值。經過上述的訓練後，得到人臉
變異度的範圍，所以，往後測試時，就可以直接做計算，並判定是否符合在兩條拋物線之
間的範圍。 
(三) 人臉確認 
     CWWW T
w
opt maxarg=          (3.9) 
由線性代數的理論中得知，轉換矩陣W恰為共變異矩陣C的特徵向量所構成之矩陣，而其
所相對應的特徵值，為其向量所代表的能量。但是共變異矩陣C是一個非常巨大的矩陣，
很難計算其特徵向量，因此在[11]中使用AAT 矩陣來求特徵向量，這是因為AAT中的M個
特徵向量恰好是AAT 的N2 個特徵向量中，最大的前面M個特徵向量。所以，可以在這M個
特徵向量中，選取K個特徵向量{u1, u2, …, uk}，來構成較低維度的子空間，每個差異向量
便可以由式子(3.10)來表示： 
                    (3.10) 
其中 稱為主成份(principal component)。 
∑Φ
=
∧ =−
K
j
jjuw
1
ψ
i
T
jj uw Φ=
(六) 灰關聯分析法 
傳統的辨識比對法則大都是採用歐式距離，本文採用灰關聯分析法[23-24]做為辨識時
的決策法則。灰色系統理論(Grey System Theory)，主要是在不完整或沒有足夠資訊下，用
來進行系統分析的理論。灰關聯分析(Grey Relational Analysis)是在灰色系統理論中，用來
分析資料序列的工具，在給定一參考序列和一比較序列，透過灰關聯分析法就可以知道這
些序列之間的關係。假設給定一參考序列集合為Sr = {Sr1, Sr2, …, Srn}，其中n≥1，其序列為
Sri = {Sri(1), Sri(2), …, Sri(p)}；給定比較序列集合為Sc = {Sc1, Sc2, …, Scm}，其中m≥1，且其
序列為Scj = {Scj(1), Scj(2), …, Scj(p)}。上述p都是有限的正整數，比較序列與參考序列之間
的關係稱為灰關聯度，其值域為0到1，定義如式子(3.11)所示： 
               (3.11) 
其中β
∑
=
=
p
k
ijkcjri kSS
1
)(),( γβγ
k為一權重值，定義為 。式子(3.11)常簡化為式子(3.12)式: 
    
1
1
=∑
=
p
k
kβ
∑
=
=
p
k
ijcjri kp
SS
1
)(1),( γγ            (3.12) 
其中γij稱為灰關聯係數(Grey Relational Coefficients)，定義為式子(3.13)[23]： 
    
max
maxmin
)(
))(),(( Δ+Δ
Δ+Δ= ζ
ζγ
k
kSkS
ij
cjri   (3.13) 
其中 
    )()()( kSkSk cjriij −=Δ , 
    , 
    , 
ζ為區別係數，其值介於0到1之間。但是式子(3.13)的分佈不夠線性，在此情況下，計算灰
關聯度時，會因為加總的動作而扭曲了最後的結果，所以在[24]中提出了新的灰關聯係數
的計算： 
)(maxmaxmax kij
mj
Δ=Δ
∀∈
)(minminmin kij
mj
Δ=Δ
∀∈
   
 (a)  (b)  (c) 
圖5.1 不同拍攝距離的人臉偵測結果(a)1公尺(b)3公尺(c)5公尺。 
表 5.1 是動態的人臉偵測實驗的結果。在此實驗中可以發現當拍攝距離約 1 公尺時，
人臉偵測率高達 97%，這是因為(1)使用事先訓練在不同亮度下膚色的 CbCr 值，相較於不
分亮度的膚色訓練，可以提高膚色在不同亮度下的偵測，(2)使用自適性的閥值來偵測人臉
特徵，對於每張不同的影像我們都分別給予不同的閥值來抽取眼睛和嘴巴，相較於使用固
定的閥值，可以得到較高的偵測。對於沒有偵測到人臉影像的原因，是因為實驗者有閉眼
的情況導致特徵點遺失。另外，因為拍攝距離較遠，偵測到的人臉越來越小，偵測率會下
降，這是因為使用色彩分析來偵測人臉特徵，人臉影像越來越小時人臉特徵相對變小，當
人臉特徵太小時就會被當成雜訊去除掉了，所以我們的系統在近距離時有較高的偵測率，
但是遠距離時，系統就不能有效偵測人臉特徵，造成偵測率無法達到理想的效果。另外，
對於處理的速度是根據連通成分的數量以及大小來決定，連通成分數量越多，後面處理步
驟要花費較多時間，連通成分面積越大代表要處理的影像也變大，所以速度會降低下來，
所以可以看到人臉在近距離時，FPS 比較慢，但是隨著人臉在距離的增加，FPS 會有提高
的趨勢，所以，如何在偵測率與處理速度之間取得平衡，是一項直的研究的問題。圖 5.1
是在不同拍攝距離(1、3、5 公尺)的人臉偵測結果。 
     
 (a)  (b)  (c)  
   
 (d)  (e) (f) 
圖5.2 移動中的人做人臉偵測結果。(a)到(f)是人由遠到近。 
    
 (a) (b)  (c) 
     
 (d) (e) (f) 
圖5.5 多人的人臉偵測結果。 
線變化的情形，也可以偵測到人臉。 
在對不同角度拍攝的人臉做人臉偵測實驗中，因為本系統是根據臉部膚色和特徵來偵
測和驗證人臉，可以偵測的角度在 45 度到-45 度間，在這個角度範圍內，兩眼和頭髮的特
徵都可以偵測到，當大於這個角度時，眼睛特徵就會偵測不到，所以只討論 45 度到-45 度
的人臉偵測。對不同角度拍攝的人臉做人臉偵測，其結果如圖 5.4 所示，頭由左轉到右的
偵測結果，最左邊角度 45 度，最右邊角度也是 45 度 
在對多人做人臉偵測實驗時，多人影像也是可以偵測出來，其結果如圖 5.5 所示，在
此圖中發現，有些人臉無法偵測出來，如圖 5.5(b)所示，右上方的人臉傾斜，嘴唇張開，
表情高興，所以無法偵測到。另外，在圖 5.5(c)中，左下方的人臉被上方的手影響到，發
生重疊現象，導致也偵測不到人臉。所以，未來要針對表情和重疊的人臉問題，進行研究
表 5.2 不同拍攝距離的人臉辨識結果 
距離 1 公尺 2 公尺 3 公尺 
張數 1500 1500 1500 
正確 1415 1305 1250 
錯誤 85 195 250 
正確率 94.3% 87.0% 83.3 % 
FPS 5 5 5 
 
來，在最好的實驗條件下，動態人臉偵測率可達97％。另外，在動態人臉辨識中，在近距
離以及比較正面取像情形下，最好的辨識率94.3%。 
當有人經過監控的門時，此人可以被系統偵測和辨識出來，並經由實際錄影資料來驗
證此系統的可行性，由實驗結果可以看出本系統是有很高的完成度，符合本計畫的第一年
目標。可惜，本計畫僅給一年，原本的計畫還有兩年，雖然得不到國科會的關愛眼神，未
來我們將繼續結合車牌偵測與車牌辨識系統，發展實際的應用。 
本計畫的研究成果投稿會議有三篇：NCS 2007一篇、 ICMLC 2008一篇、CVGIP 2008
一篇，2008年5月發表一篇於國際期刊IEEE Transactions on Consumer Electronics，另外，有
三篇在準備中，將投稿於國際期刊。 
 
六、重要參考文獻 
 
[1]  M.H. Yang, D.J. Kriegman, and N. Ahuja, “Detecting faces in images: a survey,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, Vol. 24, No. 1, pp. 34-58, 2002. 
[2]  Y. Nara, J. M. Yang, and Y. Suematsu, “Face detection using the shape of face with both 
color and edge,” Cybernetics and Intelligent Systems, 2004 IEEE Conference on, Vol.1, pp. 
147-152, 2004. 
[3]  R.L. Hsu, M. Abdel-Mottaleb, and A.K. Jain, “Face detection in color images,” IEEE Trans. 
Pattern Analysis and Machine Intelligence, Vol. 24, No. 5, pp. 696-706, 2002. 
[4]  C.H. Lin and K C. Fan, “Triangle-based approach to the detection of human face,” Pattern 
Recognition, Vol. 34, pp. 1271-1284, 2001. 
[5]  H. Wu, Q. Chen, and M. Yachida, “Face detection from color images using a fuzzy pattern 
matching method,” IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.21, 
No.6, 1999. 
[6]  K. Anderson and P.W. McOwan, “A real-time automated system for the recognition of 
human facial expressions,” Systems, Man and Cybernetics, Part B, IEEE  Transactions on, 
Vol. 36, pp. 96-105, 2006. 
[7]  H.A. Rowley, S. Baluja, and T. Kanade, “Rotation invariant neural network-based face 
detection,” in Proceedings of Computer Vision and Pattern Recognition, pp. 38-44, 1998. 
[8]  C.A. Waring, X.W. Liu, “Face detection using spectral histograms and SVMs,” Systems, 
Man and Cybernetics, Part B, IEEE Transactions ,Vol. 35,  Issue 3,  pp.467-476, 2005. 
[9]  A.N. Rajagopalan, R. Chellappa, and N.T. Koterba, “Background learning for robust face 
recognition with PCA in the presence of clutter,” IEEE Transactions on Image Processing, 
Vol. 14, Issue 6, pp.832-843, 2005. 
[10]  R. Chellappa, C.L. Wilson, and S. Sirohey, “Human and machine recognition of faces: a 
Survey,” Proceedings of the IEEE, Vol. 83, Issue 5, pp. 705-741, 1995. 
[11]  M.A. Turk, A.P. Pentland, “Face recognition using eigenfaces,” in Proceedings of Computer 
Vision and Pattern Recognition, pp. 586-591, 1991. 
[12]  P.N. Belhumeur, J.P. Hespanha and D.J. Kriegman, “Eigenfaces vs. Fisherfaces: Recognition 
using class specific linear projection,” IEEE Transactions on Pattern Analysis and Machine, 
Vol. 19, No. 7, pp. 711-720, 1997. 
