 1
由電影配樂中探勘音樂情緒之研究 
 
 
ABSTRACT 
Movies play an important role in our life nowadays. How to analyze the emotional content of the 
movies becomes one of the major issues. Based on film grammar, there are many audiovisual cues in 
movies helpful for detecting emotions of scenes. In this research, we investigate the discovery of the 
relationship between audiovisual cues and emotions of scenes and automatic annotation of the emotion 
of the scene is achieved. First, the training scenes are labeled with the emotions manually. Second, six 
classes of audiovisual features are extracted from all scenes. These classes of features consist of color, 
light, tempo, close-up, audio, and subtitle. Finally, the graph-based approach, Mixed Media Graph 
approach is modified to mine the association between audiovisual features and emotions of scenes. The 
experiments show that the accuracy is up to 70%. 
 
1. Introduction 
With the development of content digitalization, the increase of computer storage capacity and the raise 
of network bandwidth, digital video collections grow rapidly in recent years. Instead of time-consuming 
and tedious manual search for video clips, various techniques for content-based analysis have been 
developed to automatically analyze and index multimedia data [12][14][16][20]. 
The purpose of content-based video analysis is to obtain a structured organization of the original 
video content or to realize its semantics meaning. Content-based video indexing is the task of tagging 
semantic video units obtained from content analysis to achieve convenient and efficient content retrieval.  
The techniques for content-based analysis so far enable us to easily access the events, people, objects, 
and scenes captured by the camera.  For example, we can retrieve the most exciting parts of a sport 
game [12], or to efficiently generate abstracts or summaries of movies or sports game.  In addition, 
multimedia semantic retrieval is getting popular recently.  Today, movies play an important role in our 
life.  How to analyze the emotional content of the movies becomes one of major issues. 
A film was composed of many elements.  Some important aspects in film production are the color 
composition in the mise-en-scene, lighting, sound, editing, and narrative et al.  The relationships 
 3
Nowadays, many approached have been proposed to analyze affective content 
[11][12][14][15][16][27].  Hanjalic and Xu [11][12][13] exploit motion activity, cut density and sound 
energy to form the so-called affect curve, which maps to the two-dimensional (Arousal-Valence) 
emotion model.  The affect curve on the two-dimensional model will show us the emotion distribution. 
A number of works on affective classification of films have been done.  Kang [16] proposed a 
new technique for detecting affective events such as Fear, Sadness, and Joy using Hidden Markov 
Models (HMM).  He performed empirical study on the relationship between emotional events and 
low-level features of video content.  These low level features consisting of color, motion and shot cut 
rate were computed for each shot.  The feature vector of each shot was transformed to observation 
vector sequences using vector quantization.  He introduced two HMM topologies to detect emotional 
events.  The experiments on six thirty-minute video for emotional event classification show about 
70.2% and 78.73 separately. 
Based on film theories and psychological models, Wei et al. [39] proposed a color content-based 
system to analyze the color distribution and related feelings brought to viewers.  This system uses a set 
of color features for color-mood analysis and subgenre discrimination.  They introduced two color 
representations for scenes and full films for extracting the essential moods from the films – Movie 
Palette Histogram and Mood Dynamic Histogram.  Movie Palette Histogram is a global measure for 
the color palette while Mood Dynamic Histogram is a distinguishing measure for the transitions of the 
moods in the movie.  The dominant color ratio and the pace of the movie are also captured for 
classification.  They exploit eight mood types that are defined by psychologist Plutchik - Anger, Fear, 
Joy, Sorrow, Acceptance, Rejection, Surprise, and Expectancy.  In this paper, emotion and mood are 
regarded as different concepts.  Many emotion terms associated with the eight mood types are selected 
to describe the eight mood types.  Each window consist of six video shots is mapped into a mood type 
according to the emotions of the six shots.  c-SVC Support Vector Machine (SVM) [4] is adopted for 
mood classification.  Their experiments on fifteen full-length films for mood type classification of the 
window (group of six shots) level show about 80% accuracy. 
Wang and Cheong [14] proposed an affective scene classification system that is a complementary 
approach grounded in the fields of cinematography and psychology.  This system exploits a number of 
effective audiovisual cues and an appropriate set of affective categories that are identified for scene 
classification.  For each scene, the audio and the visual signal were processed separately.  The visual 
signal was segmented into shots (represented as key-frames) to computing visual cues.  The audio 
 5
In film, the basic film grammar is defined as follows. A film is composed of many scenes, and a 
scene consists of many shots.  A scene is defined as a meaningful story unit while a shot is defined as a 
stream of many frames continuously recorded by a single camera.  Based on how many subject matters 
or human figures are included within the frame of the screen (not the distance between the camera and 
the object photographed), most shots can be designed and subsumed under the six basic categories: (1) 
the extreme long shot, (2) the long shot, (3) the full shot, (4) the medium shot, (5) the close-up, and (6) 
the extreme close-up.  As showed in Table 3.1 [10].  Figure 3.1 shows the examples of shots. 
Table 3.1 Six basic categories of shots in cinema [10] 
Shot Description 
Extreme Long 
Shot 
The extreme long shot is taken from a great distance, sometimes as far 
as a quarter of a mile away.  It is also called “establishing shots” because of 
being taken at the exterior space, and serve as spatial frames of reference for 
the closer shots. 
Long Shot 
Usually, the distance of long shot is between the audience and the stage 
in the live theater.  The closest distance in long shot is equal to full shot that 
just includes human body in full, with the head near the top of the frame and 
the feet near the bottom. 
Full Shot 
The distance of full shot is between medium shot and long shot.  A full 
shot contains over three figures. 
Medium Shot 
The medium shot contains a figure from the knees or waist up. It is also 
called “functional shot”, useful for shooting exposition scenes and for 
dialogue. 
Close-Up 
The close-up emphasizes little on the background or external location, 
but on a relatively small object - the human face, for example.  The close-up 
shot enhances the importance of things, often suggesting a symbolic 
significance by enlarge the size of an object. 
Extreme 
Close-Up 
The extreme close-up is a variation of close-up.  Therefore, the 
extreme close-up might show only a person’s eyes or mouth instead of a face. 
 
 7
The audiovisual features we adopted consist of several classes of visual features (color, light, tempo, 
and close-up), one class of audio feature (sound), and one class of textual feature (subtitles).  Each 
class of feature may have more than one feature representation.  In total, there are fifteen feature 
representations which are shown in Table 3.2. 
3.3 Visual Features 
Visual features include four classes of features – color, light, tempo, and close-up. 
Color Class 
People are often influenced by color in the subconscious.  Psychologists have discovered that 
most people actively try to interpret the lines of a composition, but they accept color passively, 
permitting it to suggest moods rather than objects. 
Table 3.2 Audiovisual features in our work 
Types Classes Feature Representations 
Movie Palette Histogram (MPH) 
Mood Dynamics Histogram (MDH) 
Family Histogram (FH) 
Color 
Dominant Color Ratio (DCR) 
Light Lightness 
Shot Length (SL) 
Tempo 
Number of Shots per Minute (NSPM) 
Close-Up (CU) 
Visual 
Close-Up 
Close-Up Ratio (CUR) 
Zero-Crossing Rate (ZCR) 
Spectral Roll-off point(SR) 
Spectral Centroid (SC) 
Spectral Flux (SF) 
Audio Sound 
Mel Frequency Cepstral Coefficients (MFCCs) 
Textual Subtitle Feeling words 
 
 9
 ⎪⎩
⎪⎨⎧ ×>=
otherwise
ThPHPHifP
P
k
k
k
k
k
k
,0
])([)(, 233
3  
 where Th : 0~1, a fixed threshold.  
 
2) After computing Dominant Color Palette, we can get dominant color bin counts : 
 )( 11
k
k
k PHD =  
 ⎪⎩
⎪⎨⎧ ×>=
otherwise
ThPHPHifPH
D
k
k
k
k
k
kk
,0
])([)(,)( 122
2  
 ⎪⎩
⎪⎨⎧ ×>=
otherwise
ThPHPHifPH
D
k
k
k
k
k
kk
,0
])([)(,)( 233
3  
 where Th : 0~1, a fixed threshold. 
3) Based on dominant color, Representative Dominant Color Sequence for the shot k , )( kRDCS , is 
defined as: 
∑
= ++=×=
3
1 321
,)(
l
kkk
k
lk
l
k
l
k
l DDD
DWWPkRDCS  
   for k = 1 ~ N, and l  = 1 ~ 3. 
4) Next, we get Movie Palette (MP): 
 }]))(,)(([min{arg)( 12~1)(
mRkRDCSdiskMP
mmR =
=  , for k = 1~N 
)(mR : Pre-defined colors, which uniformly divide the CIELUV color space, m = 1 ~ 12. 
*)(*,dis : the distance between two color, is defined as Euclidean distance. 
5) Finally, MPH is derived from MP: 
 
 ∑
= ⎩⎨
⎧
≠
==
N
k mRkMPif
mRkMPif
mMPH
1 )()(,0
)()(,1
)( , for m = 1~12 
where N is the number of shots in the scene. 
 11
The tempo feature consist of two representation, Shot Length (SL) and Number of Shots Per 
Minute (NSPM), defined as follows, 
y Shot Length: Shot length is defined as the number of frames in the shot. 
y Number of Shots Per Minute: NSPM is a scene level feature, defined as the number of shots per 
minute in the scene.  The more shots one scene has in a minute, the more excited the audiences feel. 
Close-up Class 
It is essential for a filmmaker to concern what kind of shot to use to convey the action of a scene.  
When we see a close-up of the character, it implies that the filmmaker forces us to care about him or her 
and to identify with his or her feelings.  If the character is a villain, the close-up shot can make an 
emotional revulsion in us.  For example, when a threatening character is so close to us, he seems to 
encroach on our space. 
Generally speaking, the greater the distance between the objects and the camera, the more 
emotionally neutral we remains.  One of Chaplin’s most famous pronouncements is “Long shot for 
comedy, close-up for tragedy”.  This principle appears to make sense for when an action “a person is 
slipping on a banana peel” is close to us, it’s hardly funny because the person’s safety will become our 
first concern.  But if we see this event from a greater distance, it seems to be a comical act to us.   
The close-up feature comprises Close-Up and Close-Up Ratio (CUR) representations. 
y Close-Up: it describes whether the shot is a close-up.  Close-Up for shot k: 
- ⎩⎨
⎧
−
−=−
.,0
.,1
)(
upcloseanotisshottheif
upcloseaisshottheif
kUpClose  
y Close-Up Ratio: CUR is a scene level feature.  Because the audience have an inclination toward 
identification with the character in a close-up, close-up ratio may be helpful in determining emotion of 
the scene.  CUR is defined as follow: 
- 
scenetheinshotsofnumberTotal
scenetheinupcloseofnumberTheRatioUpClose −=−  
 
3.4 Audio Features 
Sound plays a very essential role in films.  In sound, sound effect and music are the most 
influential on the audience emotion.  Famous director Akira Kurosawa had said “Cinematic sound is 
that which does not simply add to, but multiplies, two or three times, the effect of the image.”  
Although the emotion of audience is usually governed by sounds in films, they are often not aware of it. 
 13
 
 
Figure 3.3 Zero-crossing rate during voiced speech region (top, ZCR = 432) and unvoiced speech 
region (button, ZCR = 7150) [31] 
 
y Spectral Roll-off point: spectral roll-off point is the frequency below which 95th percentile of the 
power in the spectrum resides [37].  This is a measure of the "skewness" of the spectral shape - the 
value is higher for right-skewed distributions [37].  SR can tells voiced speech from unvoiced speech 
as unvoiced speech has a high proportion of energy contained in the high-frequency range of the 
spectrum, while most of the energy for unvoiced speech and music is contained in lower bands of 
spectrum.  Table 3.4 shows the twelve statistics of SR in our work.  Figure 3.4 shows the energy 
spectrum (cumulative energy) along frequency with 95% spectral roll-off frequency.  The spectral 
roll-off value for a frame is computed as follows: 
 15
Table 3.4 Spectral Roll-off point statistics 
 Name Num of Values
1 Spectral Roll-off Point Overall Average 1 
2 Spectral Roll-off Point Overall Standard Deviation 1 
3 Derivative of Spectral Roll-off Point Overall Average 1 
4 Derivative of Spectral Roll-off Point Overall Standard Deviation 1 
5 Running Mean of Spectral Roll-off Point Overall Average 1 
6 Running Mean of Spectral Roll-off Point Overall Standard Deviation 1 
7 Standard Deviation of Spectral Roll-off Point Overall Average 1 
8 Standard Deviation of Spectral Roll-off Point Overall Standard Deviation 1 
9 Derivative of Running Mean of Spectral Roll-off Point Overall Average 1 
10 Derivative of Running Mean of Spectral Roll-off Point Overall Standard 
Deviation 
1 
11 Derivative of Standard Deviation of Spectral Roll-off Point Overall Average 1 
12 Derivative of Standard Deviation of Spectral Roll-off Point Overall Standard 
Deviation 
1 
 
y Spectral Centroid: the “balancing point” of the spectral power distribution.  Many types of music 
involve percussive sounds which push the spectral mean higher by including high-frequency noise 
[37].  The spectral centroid for a frame is computed as follows: 
- ∑
∑ ⋅
=
k
k
kX
kXk
SC
][
][
 
Where: k is an index corresponding to a frequency, or small band of frequencies within the overall 
measured spectrum, and X[k] is the power of the signal at the corresponding frequency band.  Table 3.5 
shows the twelve statistics of SC in our work. 
y Spectral Flux statistic: the average variation value of spectrum between the adjacent two frames in 
one second window [21].  Table 3.6 shows the twelve statistics of SF in our work. 
-    
where X means the magnitude of FFT coefficients. 
N is the number of frames in one window. 
δ is a little value to avoid log zero. 
n is the frame index, and k is frequency index. 
 
 17
coefficients of the Mel cepstrum.  MFCCs are commonly derived as follows: 
1. Take the Fourier Transform of (a windowed excerpt of) a signal. 
2. Map the log amplitudes of the spectrum obtained above onto the mel scale, using triangular 
overlapping windows.  
3. Take the Discrete Cosine Transform of the list of mel log-amplitudes, as if it were a signal.  
4. The MFCCs are the amplitudes of the resulting spectrum. 
Table 3.7 shows the twelve statistics of MFCCs in our work. 
Table 3.7 MFCCs statistics 
 Name Num of Values
1 MFCCs Overall Average 13 
2 MFCCs Overall Standard Deviation 13 
3 Derivative of MFCCs Overall Average 13 
4 Derivative of MFCCs Overall Standard Deviation 13 
5 Running Mean of MFCCs Overall Average 13 
6 Running Mean of MFCCs Overall Standard Deviation 13 
7 Standard Deviation of MFCCs Overall Average 13 
8 Standard Deviation of MFCCs Overall Standard Deviation 13 
9 Derivative of Running Mean of MFCCs Overall Average 13 
10 Derivative of Running Mean of MFCCs Overall Standard Deviation 13 
11 Derivative of Standard Deviation of MFCCs Overall Average 13 
12 Derivative of Standard Deviation of MFCCs Overall Standard Deviation 13 
3.5 Textual Feature 
Monologue and dialogue are two types of spoken languages in films.  Monologue happens when 
the character talks to self or the off-screen person narrates the background story to facilitate our 
understanding about the scene or the film.  Dialogue can be the conversation among more than two 
actors. 
The textual feature for each scene is defined as a set of the feeling words appearing in the caption 
stream of the scene.  The feeling word list we used for textual feature extraction is collected by Hein 
[42].  There are over 3000 words in total.  We divided these feeling words into two classes – positive 
and negative feelings for similarity measure.  Each feeling word belongs to one class. 
4. Emotion Discovery 
 19
 
 
Figure 4.2 Illustration of the 2-D emotion space (from Dietz and Lang [7]) 
Thayer [38] adapted Russell's model to divide music mood into four classes based on a 
two-dimensional Energy-Stress mood model.  The Energy dimension corresponds to the arousal, while 
Stress corresponds to valence in Russell’s model.  As shown in Figure 4.3, Contentment refers to happy 
and calm music, such as Bach’s “Jesus, Joy of Man’s Desiring”; Depression refers to tired and tense 
music, such as the opening of Stravinsky’s “Firebird”; Exuberance refers to happy and energetic music 
such as Rossini’s “William Tell Overture”; and Anxious/Frantic refers to tense and energetic music, 
such as Berg’s “Lulu”. 
 
Figure 4.3 Illustration of Thayer’s two-dimensional mood model [38] 
Based on human facial expressions of emotion, Ekman [9] identified the six basic emotions: Happy, 
Surprise, Anger, Sad, Fear and Disgust.  The set of emotions is found to be universal among humans. 
 21
DEFINITION 4.1: The domain Di of (set-valued) attribute i is the collection of atomic values that 
attribute i can choose from.  The values of domain Di will be referred to as the domain tokens of Di.  
Di can consist of categorical values, numerical values, or numerical vectors. 
ASSUMPTION 4.1: For each domain Di (i = 1, ... , m), we are given a similarity function si(*, *) which 
assigns a score to each pair of domain tokens. 
As shown in Figure 4.4, there are two types of vertices in MMG: object nodes and attribute value 
nodes.  Object nodes stand for multimedia object (e.g. O1, O2, O3), and attribute value nodes represent 
associated attribute values of object nodes.  For example, there are two types of attributes value node: ri 
(i = 1, 2, … , 8) and ej (j = 1, 2, 3, 4).  It is permitted that object nodes have missing attribute value 
nodes.  For object nodes with m types of attributes, MMG will be an (m+1)-layer graph with m types 
of nodes and one more type of nodes for the objects. 
             
Figure 4.4 Mixed Media Graph with two attribute type: r and e 
 
The edges in MMG are treated as un-directional.  There are two types of links in MMG: 
object-attribute-value link (OAV-link), and nearest neighbor link (NN-link).  OAV-link is the link 
between an object node and an attribute value node, represented by solid arc.  NN-link is the link 
between two similar attribute value nodes that are in the same type, represented by dashed arc.  Each 
attribute value node has k NN-link linking to k most similar attribute value nodes.  Note that some 
attribute value nodes have degree greater than k because the edges are un-directional and nearest 
neighbor relationship is not symmetric. 
After constructing MMG, the “random work with restart” mechanism is used for estimating the 
affinity of attribute value node to the query node.  In detail, take Figure 4.4 for example, to compute 
 
O1 O2 O3 
r3 r4 r5 r7 
e1 e2 e3 e4 
r2 r8 r6 r1 
 23
ASSUMPTION 4.2: For each domain Di (i = color, light, tempo, close-up, audio, text and emotion), we 
are given a similarity function si(*, *) (i = color, light, tempo, close-up, audio, text and emotion) which 
assigns a score to each pair of domain tokens. 
Graph Construction 
1. Object-Attribute-Value link 
In scene affinity graph, each object node stands for one scene (video clip).  For each object node 
in SAG with seven-attribute, there are seven types of attribute value nodes – color, light, tempo, 
close-up, audio, text and emotion.  Given one object node (i.e. one scene), the number of attribute value 
nodes in each attribute is given as follows: 
- For emotion attribute: because each scene belongs to one emotion, the object node has one 
emotion attribute value node.   
- For color (light, tempo, close-up) attribute: the number of color (light, tempo, close-up) attribute 
value nodes depends on the number of shots in the scene. 
- For audio attribute: the number of audio attribute value nodes is based on the number of shots 
that is no less than one second in the scene.   
- For textual attribute: the number of textual attribute value nodes is based on the number of 
feeling words in the scene.  One textual attribute value node stands for one feeling word and the 
number of feeling words in a scene could be zero. 
2. Nearest Neighbor link 
- For emotion attribute: the edge between two emotion attribute value nodes are linked only when 
they are of the same emotion. 
- For color (light, tempo, close-up, audio) attribute: the edge between two color (light, tempo, 
close-up, audio) attribute value nodes are constructed based on k-nearest neighbors. 
- For textual attribute: the edge between two textual attribute value nodes is linked when these two 
nodes belong to the same class (positive or negative feeling). 
3. Similarity measure for seven attributes 
- color attribute: the similarity of two nodes is the average of the four features’ similarity –FH, 
MPH, MDH and DCR.  The similarity measure for the four features is defined as follows,  
y FH, MPH and MDH: the similarities measures for the three features are the same.  It is 
defined as bin-wise histogram intersection [8]:  
- For two node x, y in color attribute with color histogram xH , yH  
 25
y CUR: the ratio of smaller value over larger value. 
- For two node x, y in close-up attribute with CURx, CURys 
),(max
),(min
),(
yx
yx
CUR CURCUR
CURCUR
yxsim =  
- audio attribute: the similarity of two nodes of audio attribute is defined as the average of five 
audio features’ similarities.  The similarities of audio features are defined as Euclidian Distance. 
- textual attribute: for two nodes t1, t2 in textual attribute (i.e. two feeling words): 
⎪⎩
⎪⎨
⎧
=
.,0
.,8.0
.,1
),( 21
otherwise
categorysamethetobelongbutdifferentareyandxif
sametheareyandxif
ttsimT  
- emotion attribute: for two node e1, e2 in emotion attribute (i.e. two emotion words): 
⎩⎨
⎧=
.,,0
.,,1
),( 21 differentareyxif
sametheareyxif
eesimE  
 For example, Figure 4.5 illustrates the constructed SAG with seven-attribute with respect to the 
query scene object Q, in which there are two training scene object nodes - {SE1, SE2}.  Scene SE1 has 
two shots and both are more than one second.  Scene SE2 has two shots and only one shot is more than 
one second.  There are one and two feeling words in SE1 and SE2 respectively.  For scene Q, it has two 
shots and both are more than one second.  There is one feeling word in Q.  In this case,  
- SE1 has color attribute nodes - {cr11, cr12}, light attribute nodes - {lt11, lt12}, tempo attribute 
nodes - {tp11, tp12}, close-up attribute nodes - {cp11, cp12}, audio attribute nodes - {a11, a12}, 
textual attribute nodes - {t11, t12}, and emotion attribute node - e11  
- SE2 has color attribute nodes - {cr21, cr22}, light attribute nodes - {lt21, lt22}, tempo attribute 
nodes - {tp21, tp22}, close-up attribute nodes - {cp21, cp22}, audio attribute node - {a21}, textual 
attribute node - {t21, t22}, and emotion attribute node - e21. 
- The query scene object Q has color attribute nodes - {crq1, crq2}, light attribute nodes - {ltq1, ltq2}, 
tempo attribute nodes - {tpq1, tpq2}, close-up attribute nodes - {cpq1, cpq2}, audio attribute node - 
{aq1, aq2}, and textual attribute nodes - {tq1}.  In Figure 5.2, the number of nearest-neighbors, k, 
is set to one.    
 27
Graph Construction 
1. Object-Attribute-Value link 
In scene affinity graph, each object node stands for one scene (video clip).  For each object node 
in SAG with four-attribute, there are four types of attribute value nodes – vision, audio, textual and 
emotion.  Each vision attribute value node is composed of color, light, tempo, and close-up features.  
Given one object node (one scene), the number of attribute value nodes in each attribute is: 
- For emotion attribute: because each scene belongs to one emotion, the object node has one 
emotion attribute value node.   
- For vision attribute: the number of vision attribute value nodes depends on the number of shots in 
the scene. 
- For audio attribute: the number of audio attribute value nodes is based on the number of shots 
that are more than or equal to one second in the scene. 
- For textual attribute: the number of textual attribute value nodes is based on the number of 
feeling words in the scene.  One textual attribute value node stands for one feeling word and 
feeling words in a scene could be more than or equal to zero. 
2. Nearest Neighbor link 
- For emotion attribute: the edge between two emotion attribute value nodes are linked only when 
they are the same emotion. 
- For vision attribute: the edge between two vision attribute value nodes are constructed based on 
k-nearest neighbors. 
- For textual attribute: the edge between two textual attribute value nodes is linked when they 
belong to the same class (positive/negative). 
3. Similarity measure for four attributes 
The similarity measure for all attributes in SAG with four-attribute is the same with above except 
vision attribute.  Vision attribute consists of nine features, so the similarity is defined as the average of 
the nine features’ similarity defined above. 
Take Figure 4.6 for example, Figure 4.6 illustrates the constructed SAG with four-attribute with 
respect to the query scene object Q revised from Figure 4.5, in which there are two training scene object 
nodes - {SE1, SE2} and one testing scene Q the same as Figure 4.5.  In this case, 
- SE1 has vision attribute nodes - {v11, v12}, audio attribute nodes - {a11, a12}, textual attribute 
nodes - {t11, t12}, and emotion attribute node - e11. 
 29
segmented into shots. 
Step 3: The visual, audio and caption streams are used to compute fifteen features stated in Section 3.   
Step 4: After feature extraction, the output features of all scenes and the emotions of the training scenes 
are used to construct SAG to output the emotion of the testing scene. 
 
Table 5.1 Twelve movies used in our system 
Genre movies 
Minority Report Action The Patriot 
Titanic 
In Her Shoes 
Life Is beautiful 
The Departed 
The Pursuit of Happyness 
Drama 
The Sixth Sense 
Must Love Dogs Romance Bruce Almighty 
House Of Wax  Horror I Know What You Did Last Summer 
 
Table 5.2 Emotion Categories vs. Viewer’s feeling [14] 
Emotion Categories Viewer’s Feeling 
Joyous exuberance, joyous, enjoyment, happy 
Tender Affection heart-warming, tender, sentimental, relaxed 
Sad depressed sad, bad, hopeless 
Fear scary, fearful, terrified 
Anger exciting, dangerous, aggressive, angry 
Surprise surprised, tense, anticipation 
 
5.1.1 Preprocessing 
The visual, audio and caption stream of each scene are processed separately.  The visual stream is 
segmented into shots according to frame difference, and for each shot, one key-frame is extracted to 
represent the visual detail of the shot.  The audio stream is also segmented according to shot boundaries 
while segments less than one second are being dropped.  For caption stream, all words belong to this 
scene were picked up for later process. 
 31
The audio segments that are more than one second are reserved for feature extraction.  Each audio 
segment is 48kHz sample rate, mono channel and 16bit per sample.  In audio feature extraction, five 
features are extracted from each audio segment.  Each audio segment is stands for one audio attribute 
node in SAG.  Therefore, each audio attribute node in SAG is represented as a vector of 204 
dimensions, as shown in Table 5.4. 
 
Table 5.4 Number of values in audio type. 
Type Feature Representations Num of values 
Zero Crossing Rate 12 
Spectral Roll Off  12 
Spectral Centroid 12 
Spectral Flux 12 
Audio
Mel Frequency Cepstral Coefficients 156 
 
5.1.4 Textual Feature Extraction 
After preprocessing step, the subtitles belong to one scene are collected.  In textual feature 
extraction, these subtitles are compared with the reference feeling word list [42].  If the word (in 
subtitles) is found in the reference list, it will be picked up from the subtitles of the scene.  The 
reference feeling word list is over 3,000 words.  Owing to the limitation of space, we only show the 
feeling words appearing in our training movies, as shown in Table 5.5.  We classified the feeling words 
of all training scenes into two categories – positive and negative for the similarity measure of two nodes 
in SAG’s textual attribute. 
5.1.5 Emotion Discovery 
The emotion discovery is performed by the Scene Affinity Graph (SAG) mentioned in section 4.3 - 
SAG with seven-attribute and SAG with four-attribute.  In average, the SAG with seven-attribute 
contains 18300 nodes, and the SAG with four-attribute contains 7400 nodes. 
The output emotion of the testing scene is determined by two criteria.  The first criterion is the 
emotion node with the maximal probability in SAG, the other is the emotion category with the maximal 
cumulative probability. 
Based on the two proposed topologies, we present two experiments - SAG with seven-attribute and 
SAG with four-attribute. 
 33
Table 5.5 Feeling words appear in out work (cont.) 
mature poor sad Sunny unlucky 
mean positive safe Super upset 
mediocre powerful sane Sure V 
messy precious satisfied Surly violent 
mighty pretty scared Sweet vital 
moping professional screwed T vivid 
N promiscuous secure talented voluptuous 
naked promised serious Tender vulnerable 
nasty proper sharp Tense W 
natural protected shocked Terrible wacky 
negative proud sick Terrific wanting 
nervous psychotic simple terrified warm 
nice punished smart thankful wasted 
normal pure smooth Thrilled weak 
nuts R soft Tired weird 
O ready solid Torn welcome 
okay real sorrow Tough willing 
open reasonable sorry true wise 
P regret stranded trusting wonderful 
pain reliable strange typical worried 
panic rich strong U worse 
panicked ridiculous stubborn uncomfortable worthy 
passionate right stuck uncovered wrong 
perfect romantic stunning undesirable  
pissed rough stupid unfair  
plain rude subtle unfit  
pleasure S suffering unique  
 
5.2 Experiment on SAG with seven-attribute 
This experiment results show that the weights for color, light, tempo, close-up, audio, textual, and 
emotion in SAG are 0:0:1:0:5:0:5 respectively. 
As Figure 5.1 shows, firstly we can see there is no clear relationship between the accuracy and the 
restart probability in the experiment based on the emotion node with the maximal probability as the 
square nodes shows.  Second, as the restart probability increase, the accuracy based on the emotion 
with the maximal cumulative probability will raise as the triangle nodes shows.  The maximal accuracy 
is about 61% with 0.9 restart probability. 
As Figure 5.2 shows, generally, based on the two accuracy measures, the accuracy declines as the 
value of k increase. 
 35
SAG with seven attribute, k = 5,  5 output emotions
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Restart probability
Ac
cu
rac
y
accumulative
maximum
maimum
 
Figure 5.3 Accuracy with different restart probability 
 
SAG with seven attribute, Restart probability = 0.9,  5 output emotions
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15
k
Ac
cu
rac
y
accumulative
maximum
maximum
 
Figure 5.4 Accuracy with different k value 
 
5.3 Experiment on SAG with four-attribute 
This experiment results show that the weights for vision, audio, textual, and emotion in SAG are 
0:1:0:1 respectively.  
As Figure 5.5 shows, when the restart probability increase, the accuracy raise based on both criteria.  
The maximal accuracy is about 64% with 0.9 restart probability. 
 
 37
SAG with four attribute, k = 2,  5 output emotions
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Restart probability
Ac
cu
rac
y
accumulative
maximum
maximum
 
 Figure 5.7 Accuracy with different restart probability. 
SAG with four attribute, restart probability = 0.9,   5 output emotions
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 7 9 11 13 15
k
Ac
cu
rac
y
accumulative
maximum
maximum
 
 Figure 5.8 Accuracy with different restart probability. 
As mentioned in Section 5.2, we combine Fear and Surprise emotion into one emotion.  Figure 
5.7 and 5.8 show the corresponding modified results.  We can see that the maximal accuracy is about 
70% when k = 2, 5% better than before. 
The experiments above show that the accuracy is closer with the restart probability than the value 
of k in both topologies.  The output emotion of the testing scene is determined based on the emotion 
category with the maximal cumulative probability always has higher accuracy.  And we find out that 
the audio features are the most influential. 
 39
[7] R. Dietz and A. Lang, “Affective Agents: Effects of Agent Affect on Arousal, Attention, Liking and 
Learning,” Proceedings of Cognitive Technology Conference, San Francisco, CA, 1999. 
[8] N. Dimitrova, J. Martino, H. Elenbaas, and L. Agnihotri, “Color SuperHistograms for Video 
Representation,” IEEE International Conference on Image Processing (ICIP ‘99), 1999. 
[9] P. Ekman, “Universals and Cultural Differences in the Judgments of Facial Expressions of Emotion,” 
J. Personality Social Psych., Vol. 54, No. 4, pp. 712–717, Oct. 1987. 
[10] L. Giannetti, Understanding Movies, Prentice Hall, Englewood Cliffs, NJ, 2005. 
[11] A. Hanjalic and L. Q. Xu, “Extracting Moods from Pictures and Sounds: Towards truly 
personalized TV,” IEEE Signal Processing Magazine, Vol.23, No.2, pp. 90-100, Mar. 2006. 
[12] A. Hanjalic and L. Q. Xu, “Affective Video Content Representation and Modeling,” IEEE 
Transaction on Multimedia, Vol.7, No.1, pp.143-154, February 2005. 
[13] A. Hanjalic and L. Q. Xu, “User-oriented Affective Video Content Analysis,” Proceedings of IEEE 
CBAIBL, Kauai, Hawaii, pp. 50-57, December 2001. 
[14] H. L. Wang and L. F. Cheong, “Affective Understanding in Film,” IEEE Transactions on Circuits 
and Systems for Video Technology, Vol.16, No.6, pp.689-704, 2006. 
[15] H. B. Kang, “Affective Content Retrieval from Video with Relevance Feedback,” International 
Conference on Asian Digital Libraries, Kuala Lumpur, Malaysia, pp. 243-252, December 2003. 
[16] H. B. Kang, “Affective Content Detection using HMMs,” Proceedings of 11th ACM International 
Conference on Multimedia, Berkeley, California, U.S.A, November 2003, pp. 259-262. 
[17] G. Kirouac, Les émotions: Monographies de psychologie.. Sillery: Presses de l’Université du 
Québec, 1992. 
[18] F. F. Kuo, M. F. Chiang, M. K. Shan, and S. Y. Lee, “Emotion-based Music Recommendation by 
Association Discovery from Film Music,” Proceedings of ACM International Conference on 
Multimedia, pp. 507-510, Singapore, November 2005. 
[19] P. J. Lang, “The Emotion Probe: Studies of Motivation and Attention,” American Psychologist, 
Vol.50, No.5, pp.372-385, 1995. 
[20] Y. Li, S. H. Lee, C. H. Yeh, and C. C. J. Kuo, ”Techniques for Movie Content Analysis and 
Skimming,” IEEE Signal Processing Magazine, Vol. 23, No. 2, pp. 79-89, March 2006 
[21] L. Lu, H. Jiang and H. J. Zhang, "A Robust Audio Classification and Segmentation Method," 
 41
Classification,” IEEE Transaction on Circuits and Systems for Video Technology, Vol. 15, No. 1, pp. 
52–64, Jan. 2005. 
[34] J. A. Russell and A. Mehrabian, “Evidence for a Three-Factor Theory of Emotions,” J. Res. 
Personality, Vol. 11, pp. 273–294, 1977. 
[35] A. Salway and M. Graham, “Extracting Information about Emotions in Films,” Proceedings of 11th 
ACM International Conference on Multimedia, Berkeley, California, pp. 299-302, November 2003. 
[36] J. Saunders, “Real-Time Discrimination of Broadcast Speech/Music,” Proceedings of IEEE 
International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’96), Vol. 2, Atlanta, 
Ga, pp. 993-996, May 1996. 
[37] E. Scheirer and M. Slaney, “Construction and Evaluation of A Robust Multifeature Speech/Music 
Discriminator”, Proceedings of IEEE International Conference on Acoustics, Speech, and Signal 
Processing (ICASSP ’97), Munich, 1997. 
[38] R. E. Thayer, The Biopsychology of Mood and Arousal, New York, Oxford University Press, 1989. 
[39] C. Y. Wei, N. Dimitrova, and S.F. Chang, “Color-Mood Analysis of Films on Syntactic and 
Psychological Models,” Proceedings of IEEE International Conference on Multimedia and Expo. 
(ICME ’04), Taipei, Taiwan, pp. 831-834, June 2004. 
[40] H. Zettl, Sight Sound Motion: Applied Media Aesthetics, 3rd ed., Belmont, CA: Wadsworth, 1998. 
[41] http://www.intel.com/technology/computing/opencv/index.htm 
[42] http://eqi.org/fw.htm 
 
 43
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：97年 7 月 31 日 
國科會補助計畫 
計畫名稱：由電影配樂中探勘音樂情緒之研究(一) 
計畫主持人：沈錳坤 
計畫編號：NSC 95-2221-E-004-009-MY2 學門領域：資訊工程 
技術/創作名稱 基於電影拍攝手法之電影場景自動標記情緒系統 
發明人/創作人 沈錳坤、廖家慧 
技術說明 
電影場景自動標記情緒系統主要分為三個模組，鏡頭切割模組、特
徵值擷取模組與情緒標記模組。 
欲查詢的電影場景透過鏡頭切割模組切割出此場景之特徵值擷取
模組產生四類特徵值，這四類特徵值乃根據電影的拍攝手法所決
定。擷取出來的特徵值將輸入到情緒標記模組來產生此電影場景的
情緒。情緒標記模組我們採用Mixed Media Graph(MMG)演算法。
可利用之產業 
及 
可開發之產品 
1. 可利用於文化、娛樂與音樂教育產業 
2. 電影搜尋與音樂配樂 
技術特點 運用資料探勘技術，對電影視覺內容分析。  
推廣及運用的價值 
電影情緒的判斷可協助電影配樂情緒的判斷，進而搭配音樂。 
 
 
附件二 
ˇ
