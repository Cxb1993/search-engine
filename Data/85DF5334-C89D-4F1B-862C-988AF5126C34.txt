on an associated latent topic Tk. The conditional probability of a
document di generating a term tj thus can be parameterized by
P (tj |di) =
KX
k=1
P (tj |Tk)P (Tk|di). (1)
Notice that this probability is not obtained directly from the
frequency of the term tj occurring in di, but instead through
P (tj |Tk), the probability of observing tj in the latent topic Tk, as
well as P (Tk|di), the likelihood that di addresses the latent topic
Tk. The PLSA model can be optimized with the EM algorithm by
maximizing a carefully defined likelihood function [10].
2.2. Automatic Generation of Titles and Summaries for Spo-
ken Documents
The titles exactly complement the summaries for the user during
browsing and retrieval. The user can easily select the desired doc-
ument with a glance at the list of titles. He can then look through
or listen to the summaries in text or speech form for the titles he
selected.
Substantial efforts have been made in automatic generation of
titles and summaries for spoken documents [11, 12, 13, 14, 15]. In
this research, it was found that the topic entropy of the terms esti-
mated from probabilities obtained in PLSA analysis is very useful
in finding the key terms of the spoken documents in automatic
generation of titles and summaries [16][17]. The topic entropy of
a term tj is evaluated from the topic distribution {P (Tk|tj), k =
1, 2, . . . ,K} of the term obtained from PLSA and defined as:
EN(tj) = −
KX
k=1
P (Tk|tj) logP (Tk|tj) (2)
Clearly, a lower entropy implies the term carries more topical in-
formation for a few specific latent topics, thus is more significant
semantically.
In this research, it was found that the sentences selected based
on the topic entropy can be used to construct better summaries for
the spoken documents [16]. For title generation, a new delicate
scored Viterbi approach was developed in this research based on
the concept of the previously proposed statistical translation ap-
proach [12]. In this new approach, the key terms in the automati-
cally generated summaries are carefully selected and sequenced by
a Viterbi beam search using three sets of scores. This new delicate
scored Viterbi approach was further integrated with the previously
proposed adaptive K-nearest neighbor approach [18] to offer better
results [17].
2.3. Global Semantic Structuring for the Spoken Document
Archive
The purpose of global semantic structuring for spoken document
archives is to offer an overall knowledge of the semantic con-
tent of the entire spoken document archive in some form of hi-
erarchical structures with concise visual presentation to help the
user to browse across the spoken documents efficiently. In this
research, we developed successfully a new approach to analyze
and structure the topics of spoken documents in an archive into a
two-dimensional tree structure or a multi-layer map for efficient
browsing and retrieval [19]. The basic approach used is based
on the PLSA concept. In the constructed two-dimensional tree
structure, the spoken documents are clustered by the latent topics
they primarily address, and the clusters are organized as a two-
dimensional map. The nodes on the map represent the clusters,
each labeled by several key terms with the highest scores for the
cluster. The nodes are organized on the map in such a way that the
distances between nodes have to do with the relationships between
the topics of the clusters, i.e., closely located nodes represent clus-
ters with closely related topics. Every node can then be expanded
into another two-dimension map in the next layer with nodes rep-
resenting finer topics. In this way the entire spoken archive can
be structured into a two-dimensional tree, or a multi-layered map,
representing the global semantics of the archive [19]. This is very
useful for browsing and retrieval purposes.
2.4. Query-based Local Semantic Structuring for Spoken
Documents Relevant to User’s Interests
The global semantic structure mentioned above is very useful, but
not necessarily good enough for the user regarding his special in-
formation needs , very often represented by the query he entered
to the information retrieval engine. The problem is that the query
given by the user is usually very short and thus not specific enough,
and as a result a large number of spoken documents are retrieved,
including many noisy documents retrieved due to the uncertainty
in the spoken document retrieval. However, as mentioned above,
the spoken documents are very difficult to be shown on the screen
and very difficult to browse. The large number of retrieved spoken
document therefore becomes a difficult problem. It is thus very
helpful to construct a local semantic structure for the retrieved spo-
ken documents for the user to identify what he really needs to go
through or to specify what he really wish to obtain. This semantic
structure is localized to user’s query, constructed from those re-
trieved documents only, thus needs to be much more delicate over
a very small subset of the entire archive. This is why the global
semantic structure proposed above in section 2.3 cannot be used
here. Instead in this research we propose to construct a very fine
topic hierarchy for the localized retrieved documents. Every node
on the hierarchy represents a small cluster of the retrieved docu-
ments, and is labeled by a key term as the topic of the cluster. The
user can then click on the nodes or topics to select the documents
he wishes to browse, or to expand his query by adding the selected
topics onto his previous query [20].
The approach we used in this research for topic hierarchy con-
struction is the Hierarchical Agglomerative Clustering and Parti-
tioning algorithm (HAC+P) recently proposed for text documents
[21]. This algorithm is performed on-line in real time on the re-
trieved spoken documents. It consists of two phases: an HAC-
based clustering to construct a binary-tree hierarchy and a parti-
tioning (P) algorithm to transform the binary-tree hierarchy to a
balanced and comprehensive m-ary hierarchy, where m can be dif-
ferent integers at different splitting nodes. The first phase of HAC
algorithm is kind of standard, based on the similarity between two
clusters Ci and Cj and is performed bottom-up, while the second
phase of partitioning is top-down. In this second phase, the binary-
tree is partitioned into several sub-hierarchies first, and then this
procedure is applied recursively to each sub-hierarchy. The point
is that in each partitioning procedure the best level at which the
binary-tree hierarchy should be cut in order to create the best set
of sub-hierarchies has to be determined based on the balance of
two parameters: the cluster set quality and the number preference
score [20].
3. An Initial Prototype System
An initial prototype system has been successfully developed. The
broadcast news are taken as the example spoken/multi-media doc-
uments. The broadcast news archive to be summarized includes
two sets, all in Mandarin Chinese. The first has roughly 85 hours
of about 5,000 news stories, recorded from radio/TV stations in
Taipei from Feb. 2002 to May 2003. No video signals were kept
with them. The character and syllable error rates of 14.29% and
8.91% respectively were achieved in the transcriptions. The sec-
ond set has roughly 25 hours of about 800 news stories, all includ-
ing the video signal parts, recorded from a TV station in Taipei
from Oct. to Dec. 2005. The character and syllable error rates for
this set is 20.92% and 13.90%.
For those news stories with video signals, the video signals
were also summarized using video technologies, for example,
Table 1: Evaluation results for title generation.
Approaches Precision Recall F1 Relevance Readability
ST 0.0783 0.1513 0.1032 3.615 1.874
AKNN 0.1315 0.1074 0.1183 3.594 4.615
Proposed 0.2176 0.2119 0.2147 4.102 4.332
Table 2: Evaluation results for automatic summary generation.
ROUGE-1 ROUGE-2 ROUGE-3 ROUGE-L
Summarization 10% 30% 10% 30% 10% 30% 10% 30%
Ratio
Significance 0.27 0.48 0.18 0.40 0.16 0.36 0.26 0.47
Score
Proposed 0.36 0.54 0.30 0.47 0.29 0.44 0.36 0.53
are shown in table 2 for summarization ratios of 10% and 30%.
Here listed are two different ways to perform the automatic sum-
marization: the well known and very successful significance score
[14, 15], and the approach proposed in this research respectively.
It can be found that the proposed approach is better in all scores.
4.2. Performance Evaluation of Global Semantic Structuring
The performance evaluation for the global semantic structuring
was performed on the TDT-3 Chinese broadcast news corpus [19].
A total of 47 different topics have been manually defined, and each
news story was assigned to one of the topics, or as “out of topic”.
These 47 classes of news stories with given topics were used as
the reference for the evaluation. We define the “Between-class to
within-class” distance ratio as in equation (3),
R = d¯B/d¯W , (3)
where d¯B is the average of the distances between the locations of
the two clusters on the map for all pairs of news stories manually
assigned to different topics, and d¯w is the similar average, but over
all pairs of news stories manually assigned to identical topics. So
the ratio R in equation (3) tells how far away the news stories
with different manually defined topics are separated on the map.
Apparently, the higher values of R the better. On the other hand,
for each news story di, the probability P (Tk|di) for each latent
topic Tk, k = 1, 2, . . . ,K, was given. Thus the total entropy for
topic distribution for the whole document archive with respect to
the organized topic clusters can be defined as:
H = −
NX
i=1
KX
k=1
P (Tk|di) log(P (Tk|di)), (4)
where N is the total number of news stories used in the eval-
uation. Apparently, lower total entropy means the news stories
have probability distributions more focused on less topics. Table
3 lists the results of the performance measure for the proposed ap-
proach as compared to the well-known approach of Self-Organized
Map (SOM) [23] for different choices of the “term” tj used, i.e.,
W(words), S(2)(segments of two syllables), C(2)(segments of two
characters), and combinations.
4.3. Performance Evaluation of Query-based Local Semantic
Structuring
The performance evaluation for the query-based local semantic
structuring was performed using 20 queries to generate 20 topic
Table 3: Evaluation results for the global semantic structuring.
Choice of Distance Ratio (R) Total Entropy
Terms Proposed SOM (H)
(a) W 2.34 1.11 5135.62
(b) S(2) 3.38 1.04 4637.71
(c) C(2) 3.65 1.03 3489.21
(d) S(2)+C(2) 3.78 1.02 4096.68
hierarchies [20]. The average values of correctness (C) and cov-
erage ratio (CR) were obtained with some manual efforts. The
correctness (C) is the measure if all the key terms in the topic hi-
erarchy is correctly located at the right node position. It can be
evaluated by counting the number of key terms in the topic hierar-
chy which have to be moved manually to the right node position
to produce a completely correct topic hierarchy. The coverage ra-
tio (CR) is the percentage of the retrieved news stories which can
be covered by the key terms in the topic hierarchy. On average a
correctness (C) of 91% and a coverage ratio (CR) of 97% were
obtained.
5. Conclusion
In this paper we proposed a whole set of technologies to offer
multi-layered summarization for entire spoken archives for the
purposes of efficient browsing and retrieval. This includes (1) Au-
tomatic Generation of Titles and Summaries for each of the spoken
documents, (2) Global Semantic Structuring of the spoken docu-
ment archive and (3) Query-based Local Semantic Structuring for
the subset of spoken documents relevant to user’s query. An ini-
tial prototype system was completed, and satisfactory performance
was obtained.
6. References
[1] L.-S. Lee and B. Chen, “Spoken document understanding and organization,”
IEEE Signal Processing Magazine, vol. 22, no. 5, Sept. 2005.
[2] CMU Informedia Digital Video Library project [online]. Available :
http://www.informedia.cs.cmu.edu/.
[3] Multimedia Document Retrieval project at Cambridge
University [online]. Available : http://mi.eng.cam.ac.
uk/research/Projects/Multimedia Document Retrieval/.
[4] D.R.H Miller, T. Leek, and R. Schwartz, “Speech and language technologies
for audio indexing and retrieval,” Proc. IEEE, vol. 88, no. 8, pp. 1338–1353,
2000.
[5] S. Whittaker, J. Hirschberg, J. Choi, D. Hindle, F. Pereira, and A. Singhal,
“Scan: Designing and evaluating user interface to support retrieval from speech
archives,” in Proc. ACM SIGIR Conf. R&D in Information Retrieval, 1999, pp.
26–33.
[6] A. Merlino and M. Maybury, “An empirical study of the optimal presentation of
multimedia summaries of broadcast news,” in Automated Text Summarization,
I. Mani and M. Maybury, Eds., pp. 391–401. Eds. Cambridge, MA:MIT Press,
1999.
[7] SpeechBot Audio/Video Search at Hewlett-Packard (HP) Labs [online]. Avail-
able : http://www.speechbot.com/.
[8] S. Furui, “Recent advances in spontaneous speech recognition and understand-
ing,” Proc. ISCA & IEEE Workshop on Spontaneous Speech Processing and
Recognition, pp. 1–6, 2003.
[9] Columbia Newsblaster project at Columbia University [online]. Available :
http://www1.cs.columbia.edu/nlp/newsblaster/.
[10] T. Hofmann, “Probabilistic latent semantic analysis,” Uncertainty in Artificial
Intelligence, 1999.
[11] R. Jin and A. Hauptmann, “Automatic title generation for spoken broadcase
news,” in Proc. of HLT, 2001, pp. 1–3.
[12] M. Banko, V. Mittal, and M. Witbrock, “Headline generation based on statisti-
cal translation,” in Proc. of ACL, 2000, pp. 318–325.
[13] B. Dorr, D. Zajic, and R. Schwartz, “Hedge trimmer: A parse-and-trim ap-
proach to headline generation,” in Proc. of HLT-NAACL, 2003, vol. 5, pp. 1–8.
[14] S. Furui, T. Kikuchi, Y. Shinnaka, and C. Hori, “Speech-to-text and speech-
to-speech summarization of spontaneous speech,” IEEE Trans. on Speech and
Audio Processing, vol. 12, no. 4, pp. 401–408, 2004.
