摘要 
 
本研究主要探討的問題為如何從深層的網路資料中去抓取與主題相關的資料，用於輔助各種資訊
檢索的應用。許多學者提出主題式網頁爬行研究方向。主題式網頁爬行器與傳統搜尋引擎的網頁爬行
器差別在於前者是以主題相關的資料來做擷取，以提昇擷取資料的相關度。研究重點是如何在擷取過
程中判斷文章是否相關及文章中是否出現與主題相關的線索。本研究運用鏈結分析來判斷網頁的重要
性，並透過重要性高的網頁涵蓋的向內鏈結(Inward links)及向外鏈結(Outward links)來擷取網頁特徵，
並運用支援向量機(Support vector machine, SVM)的演算法來建立網頁分類模型，透過建立出來的分類
器來將網頁爬行器擷取回來的網頁資料網頁分類，並區分為相關/不相關兩種類別，提昇針對主題資料
擷取的資料精確度。 
 
關鍵詞：鏈結分析、支援向量機、主題式網頁爬行 
 
1. 前言 
 
由於網際網路的蓬勃發展，使得網路的使用者增加並且資訊量呈現快速成長的趨勢，而為了能夠
有效地搜尋到網路上相關的資訊，近年來搜尋引擎及入口網站的開發提供給使用者一個相當便利的網
路資料搜尋工具，例如:Google、Yahoo 等大型入口網站；對於使用者而言，搜尋引擎提供基本搜尋和
進階搜尋的功能來輔助使用者於資料搜尋上的便利性，但是使用者在輸入查詢詞搜尋時，經常使用許
多語意模糊的查詢詞(inexact query)而非精確的查詢詞(exact query)，相對地搜尋引擎的檢索效能會降低
並且增加使用者的瀏覽時間；因此，近年來許多學者們進一步針對主題式爬行(Topic crawling)作為研
究方向，以主題做搜尋的依據來提升抓取回來的資料精確度及降低儲存成本作為考量。 
在本研究中主要會針對網頁的重要性來分析網頁間鏈結的相關性，並藉由不同網頁特徵來判斷擷
取回來的網頁資料的相關程度做實驗及分析，以提昇擷取回來的資料相關度及引導主題式網頁爬行器
進一步往有相關線索的網站或網頁作擷取。接下來，在第二節的部分會針對本研究的研究目的作概述，
在第三節的部分我們會針對一般的網頁爬行器和主題式網頁爬行的相關文獻作介紹及說明，在第四節
的部分會針對本研究使用的分類相關技術及系統架構做詳細的說明，在第五節的部分主要是實驗運用
支援向量機(SVM)建立網頁特徵模型，並運用此特徵模型來針對擷取回來的網頁特徵作實驗證明及評
估分析，最後為本研究的結論及未來系統的發展方向和規劃。 
 
2. 研究目的 
 
一般傳統搜尋引擎其優點在於能夠掌握網路上大部分的資料，由於資料量龐大的關係，相對地資
料類型會呈現五花八門的狀態，因此對於搜尋到的資料精確度相對就會降低，甚至於搜尋的結果與查
詢詞毫無關聯性。而主題式網頁爬行器主要是運用分辨不同的網頁特徵或其他判斷網頁相關度的機制
來擷取許多與主題或查詢詞相關的資料，以提供精確度較高的資料給使用者。因此，如何從大範圍的
網頁資料中擷取到與查詢詞相關或主題相關的資料，並且有效地引導網頁爬行器往有相關網頁線索的
區域擷取更多相關的網頁資料為本論文主要的研究目的。 
 
3. 文獻探討 
 
以下部分會針對網頁爬行器(Web crawling)及主題式爬行器(Topic web crawling)的相關文獻及相關
研究情況作詳細說明及介紹。 
 
3.1 一般網頁爬行器(web crawling) 
 
現今的搜尋引擎應用在擷取網路資料的程式，一般被稱為網頁爬行器(web crawler)或網路蜘蛛(web 
spider)，網頁爬行(Web crawling)是一種從網路擷取網頁回來，並建成索引及提供資料給搜尋引擎運作
的程序。網頁爬行的目的在於能快速和有效率的去擷取網路上可能有用的頁面，並能夠透過鏈結架構
去連結到抓取的頁面。運用網頁爬行器的程式來針對圖形般的網路架構進行探勘時，一開始藉由提供
G.
C. 
Ha
Bo
H
 
4. 研
 
4.1 
 
以區
(SV
將一
超平
量機
機為
超平
資料
下：
yi為
找出
4.2 
 
器擷
以區
集由
主題
練階
網頁
 
4.2.
 
 Almpanidi
Kotropoulo
I. Pitas 
/2007[3] 
i-Tao Zhen
-Yeong Kan
ong-Gee K
/2008[17]
究方法 
支援向量機
本研究我們
分為相關
M)；支援向
群在 Rd 空
面(Hyperp
需要經由
監督式學
比較圖 1 左
面(實線)之
點區分間
已知Traini
標籤(Labe
一最佳的
系統架構 
在本研究中
取資料的
在訓練階段
分未知的
Open dire
相關的網
段使用支
資料與主
1 訓練階段
步驟1：從
s、 
s、 結合
語
g、
g、
im 
 
運用
徵
(Support
主要是將
(Positive)/
量機是一
間中的資
lane)，並且
訓練的過程
習方法中的
以及圖 1
間距離邊
隔較大的超
ng Data Se
l))，i 為訓
超平面，以
我們將系
階段(Craw
主要是採
網頁資料為
ctory proje
址(URL)的
援向量機(
題相關或不
(Training
ODP的網站
本文和鏈
間潛在語意
知識本體
擷取的語
 vector ma
網頁爬行器
不相關(Ne
種可用於
料，經由
希望藉由
來建立模
一種。 
右，我們可
界(Margin
平面，因
ts:：{xi,yi},i
練資料集的
利將未知
圖 1  比
統架構區分
ling phase)
用機器學習
相關與不
ct(ODP)[1]
集合。在網
SVM)學習
相關。 
 phase)[12
中取得R
結分析於詞
的分析。
論來提昇特
意關聯。
chine, SVM
擷取回來
gative)兩種
分類(classi
事先準備的
超平面可
型，而建立
以發現左
)較近，而右
此我們認為
=1,2,...,n,xi
數量，d
的 Xi歸類
較兩個不
為兩個階
；以下會針
中的分類
相關兩種類
的網站中取
頁爬行器
訓練建構的
] 
DF型式的資
 
將網頁
作 SV
縮減，
縮減後
詞
使用知
體論來
詞語和
的關
)[4,5,6,7,1
的網路資
類別或-1
fication)和
資料訓練
以將這群資
模型前需
圖所找出的
圖則具有
右圖所找
אRd, yiא{-1
為輸入的維
。 
     
同的 hyper
 
段，一個是
對系統架
演算法-支
別，支援
得，資料中
擷取資料階
分類器，
料後，由
內容
D 維度
以取得
的字
 
式
識本
擷取
主題
聯度 
建
取
5] 
料透過分類
/+1，而我
遞歸(Regre
後的模型
料區分成
準備事先分
超平面(虛
較大的邊界
出的是比較
,+1}, 其中
度空間大
‐plane的分
訓練階段
構作運作流
援向量機
向量機的運
包含的內
段主要是
將未知的網
資料集中挑
運用 Laten
算本文中
HITS 鏈結
網頁爬行
構特定領
語間的語意
，並使用
演算法來
們使用的分
ssion)的方
，能夠在 d
兩類(ie:相關
類好的資
線)，兩平
。而由於
好的超平
{xi,yi}為訓
小，因此我
類效果 
(Training p
程說明，
(SVM)，透
作工具為
容有 ODP
大量擷取網
頁資料經
選主題來
t Semantic 
詞語間語意
分析的演算
器往相關性
取。 
域的知識本
關聯度較
類神經網路
相關性
對大量的網
類演算法
法，透過支
個維度的空
、不相關
料作訓練
行且與兩組
我們希望可
面。因此
練資料集(
們希望利
 
hase)，另一
說明如下
過訓練資料
LIBSVM[
所有主題的
路上的網
由網頁內容
作為分析及
Index(LSI)
的關聯度
法來引導
高的資料
體論來針
高的特徵作
來判斷網
。 
頁資料做
為支援向
援向量機
間之中找
)；由於支
，因而支援
資料點相
以找出將
，將問題簡
xi為特徵向
用 Training
個為網頁
： 
集作學習
2]。而訓練
列表及每
頁資料，透
的特徵來
測試網頁
來計
及
主題
作擷
對詞
擷
頁的
預測，
量機
可以
出一
援向
向量
切的
兩群
述如
量，
 Data
爬行
訓練
資料
一個
過訓
判斷
爬行
的特徵向量與訓練資料集的每個網頁的特徵向量，並且以相似度最高的來判斷目標網頁的類別(相關/
不相關)。 
步驟4:將網頁特徵向量資料集給定完每一個網頁特徵向量的類別後，藉由訓練階段使用支援向量機
(SVM)建構的分類器來作網頁資料分類，並將分類為相關的網頁資料作儲存；儲存完成後，網址優先
權分配機制(URL frontier)會依據網頁重要性權重值的高低作為優先擷取的標準，並且在網頁爬行器循
環擷取網頁時，網址優先權分配機制會依據網頁重要性的權重值作重新排序(Reranking)的動作，而網
頁爬行器的運作會循環擷取網頁資料直至達到我們預設的目標網頁數量或網址優先權分配機制為空佇
列為止。 
 
5. 結果與討論 
 
基於 ODP 網站中涵蓋網站的數量有 4,726,955 個及網站中的目錄超過 998,672 個，由此可知 ODP 網
站中的目錄分類的網頁資料量是相當龐大且資料可信度是相當高的，並且擁有 86,289 個人共同編輯而
成的目錄分類的網頁資料，因此經由人工編輯與篩選的目錄相關網址集合與主題的關聯性相當高；因
此，我們主要是挑選出 10 個實驗評估的主題及相關的網址(URL)集合，其中相關的網址集合包含訓練
資料集的正例網址資料集和評估特徵模組效能的評估資料集(如表 2)；由於訓練資料集中包含反例網址
資料集，因此反例網址資料集主要是由實驗評估主題外的主題相關網址集合中挑選。 
在本研究中我們主要是比較三種不同的網頁爬行器，分別為 Keyword-based crawling 、SVM-based 
focused crawling、SVM-Link analysis focused crawling，運用不同的網頁爬行器分別針對不同的實驗評
估主題擷取 1000 篇的網頁資料，用以評估不同網頁爬行器於擷取網頁資料上資料的精確率。以下列出
不同的網頁爬行器並說明在抓取網頁時，如何判斷抓取網頁相關性的方法: 
 Keyword-Based crawling: 以關鍵字作搜尋，以抓取網頁中有出現關鍵字的網頁。 
 SVM-based crawling: 擷取網頁中的 title、anchor text、keyword 作為特徵值，並運用 SVM 作分類，
來判斷網頁是否相關。 
 SVM-Link analysis crawling: 擷取網頁中的 title、anchor text、keyword 作為特徵值，並使用鏈結
分析作特徵值的擴張，以增加 SVM 分類的效果。 
 
5.1 實驗結果 
 
在本研究中，我們在實驗的過程主要是藉由 Harvest rate(公式 1)來分析網頁爬行器在爬行過程中的
檢索效能，並透過 10 個不同的主題來驗證網頁爬行器在擷取 Top-100~1000 篇的資料時的效能及分析
三種不同的網頁爬行器在結合不同的方法下能否有效的擷取到符合主題的網頁資料。 
 
(1)     
farsocrawleredURLsofnumbertotalthe
farsocrawleredURLsrelevantofnumberrate Harvest =  
 
圖 3~圖 5為分別針對 10個不同主題並透過不同模式的網頁爬行器來針對不同主題來進行擷取主題
相關網頁的實驗結果，並從中挑選出平均、最佳、最差的 3 組實驗結果，以分析不同模式的網頁爬行
器於擷取不同主題相關頁面的檢索效能；經由實驗結果可以看出，不同的主題於網頁爬行器上擷取相
關網頁的效能其差異性相當大，其原因在於有些主題所涵蓋定義較廣闊，像是主題 2-Education 或主題
4- Ice Cream，因此網路上相關的網頁資料便相對的增加許多，對於網頁爬行器於擷取網頁時更容易取
得主題相關的網頁資料；但是除了主題涵蓋定義廣闊或狹隘外，一開始給予網頁爬行器的種子網址
(Seed url)也會影響網頁爬行器擷取主題相關頁面的效能，倘若給予的種子網址是首頁或頁面資訊不多
的頁面，因此其可以比對的特徵相對地會減少許多，其網頁爬行器擷取網路的效能便會降低。 
表 2 訓練資料集主題及數量 
主題 正例數量 反例數量 
1. Top/Arts/Animation/Anime/ 
Fandom/Cosplay 49 100 
關的網頁資料量相當龐大，因此使用來判斷網頁相關性的特徵量較為龐大，因而其網頁爬行器的擷取
資料收獲率上是有顯著差異的；而主題 10 (Protein Analysis)於網路上網頁資料量較為稀少，但是擷取的
網頁資料中其網頁特徵較為顯著，因此運用 SVM‐Link analysis的方法其網頁收獲率仍優於其他兩種模式
的網頁爬行器。 
在本篇論文中，主要運用擴展特徵的概念來補足頁面資訊可能不足的部分，以避免訓練資料集或
網頁爬行器擷取網頁時頁面資訊不足，因而導致分類器的分類效能降低的情形發生；由實驗結果來看，
透過 SVM 結合鏈結分析來擴展網頁特徵數量，在擷取 10 個不同主題的網頁資料上是有幫助的，不僅
能夠提昇分類器的分類效能，對於整體的檢索效能上也能夠輔助網頁爬行器於擷取主題相關資料上有
顯著的提昇。 
 
 
圖 4 得到最佳收穫率的主題 5 於不同網頁爬行器擷取資料的收穫率 
 
圖 5 得到最差收穫率的主題 10 於不同網頁爬行器擷取資料的收穫率 
 
6. 結論與未來方向 
 
本研究中主要提出結合支援向量機的分類器與運用鏈結分析來擴張網頁特徵數量以提昇網頁爬行
器擷取相關主題網頁的檢索效能。經由透過 10 個不同的主題實驗的結果驗證主題式網頁爬行器在於擷
取不同主題的資料其檢索效能是有明顯的差異，而使用 SVM-Link analysis crawling 來擷取主題相關的
資料在於 Harvest rate 上，隨著擷取的資料量增加，其 Harvest rate 是優於其他兩種網頁爬行器。經由此
實驗可證明經由使用鏈結分析來擴張主題相關的網頁特徵量以提升 SVM 的分類效能，在於檢索效能上
是有改善的。 
在實驗過程中，使用三種不同的網頁爬行器在針對不同主題作資料抓取於檢索效能上的比較是有
明顯的差距，其原因可能在於訓練資料量及評估的網頁數量多寡會影響到檢索上的效能；雖然運用鏈
結分析來擴張網頁的特徵數量在於實驗結果上有優於其他兩種不同的爬行方法，但其檢索效能上還尚
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100 200 300 400 500 600 700 800 900 1000
H
ar
ve
st
 r
at
e(
%
)
Crawl pages
Keyword-Base
SVM-Base
SVM-Link analysis
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
100 200 300 400 500 600 700 800 900 1000
H
ar
ve
st
 r
at
e(
%
)
Crawl pages
Keyword-Base
SVM-Base
SVM-Link analysis
 
一、參加會議經過 
　 NTCIR 是日本國立情報學研究所 NII 主辦的資訊檢索公開評測研
討會，每十八個月舉辦一次，每次由許多不同國際團隊組成數個主
辦小組，舉行多個跨語言資訊檢索分享工作，主題包括問答系統，
專利文件探勘，時地資訊檢索，社群問答檢索，意見探勘等。每次
會有少許改變，評測項目會有變動。NTCIR 是亞洲最大的資訊檢索評
估會議，提供大量的資料以及人工標注的評估資料，讓資訊檢索研
究團隊瞭解研究的進步程度，有共通的標準。每一屆都有十幾個國
家的數十個團隊參加。經費主要由日本政府贊助。NTCIR 未來將由
NII 與 NiCT 共同主辦。NiCT 主要從事語音處理相關研究，同時也有
許多語言處理相關的研究計畫。 
　 今年的項目有先進跨語言資訊檢索 ACLIA ，地理與時間資訊檢索
- GeoTime，多語言意見探勘 MOAT，專利探勘-Patent Mining，專
利翻譯 - Patent Translation 及一個嘗試性的議題社群問答
-Community QA。我們的團隊此次參與了兩個評測的子項目一、先進
跨語言資訊檢索 ACLIA 的 IR4QA，以及多語言意見探勘 MOAT 的意見
句擷取及意見分類。並且發表論文，其中 IR4QA 為 oral present，
MOAT 是 poster。 
今年的第八屆 NTCIR 有約 170人報名參加。正式會議開始由主辦
單位主席 Noriko Kando 說明 NTCIR 的架構，包括歷史與分項工作。
今年的主題包括問答系統，專利文件探勘，時地資訊檢索，社群問
答檢索，意見探勘等。今年有六十五個團隊參加。 
第二天早上邀請演講的主題是 DeepQA 深度問答，由 IBM Research
的 Koichi Takeda 主講，介紹他們的系統，以及研究主題。 
下午是 ACLIA 分場，包括兩個次主題：複雜跨語言問答系統
語言資訊檢索 
(英文) Query Expansion from Wikipedia and Topic Web Crawler 
on CLIR 
發表論
文題目 
(中文)自動產生樣版應用於意見句擷取及意見分類 
(英文) Opinion Sentences Extraction and Polarity Classification 
Using Automatically Generated Templates 
未來應該朝向整合自動學習而來的資源以及人工整理的資源為目
標，減少人力參與，提高整體正確率。 
本屆 NTCIR 國內參與者有台大陳信希教授、林川傑教授、蔡宗
翰教授、陳光華教授、李政緯博士等人。 
 
二、與會心得 
　 　  
　 　 過去文字為主的資訊檢索，逐漸跨向語音、圖片、影片等多媒體方
向。特別是 internet 上有許多可以取得的資訊，對使用者來說過多而難
以消化，有賴系統的整理呈現。跨語言與多語言的資訊檢索依然需要大
量的研究。NTCIR 的參與者之間有很高的互動。除了一般國際會議都有的
休息時間 coffee break，晚宴等，在每天中午 buffet 的場地中間，NTCIR
要求每一篇作者都要貼海報。海報增加了互動的可能性，午餐吸引人潮，
這是主辦國際會議增加交流有效的方法。此次與會與國內外學者交換意
見後發現，中國大陸的研究能量大幅度的提升，包括人力、財力、設備、
資料、語料支援等等，很快的超越了處理日文、繁中、韓文的水準，朝
向英文處理的水準邁進。 
 
三、建議 
　 NTCIR 是亞洲最大的資訊檢索評測研討會，僅次於美國舉辦的 TREC
與歐洲舉辦的 CLEF。焦點除了英文這個主流語言外，這幾年參與者對簡
體中文的熱衷程度遠大於對繁體中文的興趣。長此以往，繁體中文相關的
研究將會弱化，相關的研究單位應投入更多的關注在中文處理的技術上。 
　 此次研討會，剛好有兩位學生獲得交換學生的機會前往日本姊妹校就
讀，也可以參與國際研討會發表。他們把握了這次的機會，充分準備後有
良好的表現。補助學生出國對於學生的成長很有幫助。 
四、攜回資料名稱及內容 
論文集摘要及全文光碟。 
 
Proceedings of NTCIR-8 Workshop Meeting, June 15–18, 2010, Tokyo, Japan
― 102 ―
The translation results are treated as query terms. 
Wikipedia is a good source for finding translations of 
named entities, such as personal names, organizational 
names, place names, and terminologies in various 
professional areas. In March 2010, the amount of 
entries in the English, Chinese, and Japanese versions 
of Wikipedia was 3215333, 297207, and 662360, 
respectively. 
Google translation, on the other hand, provides 
translation of common terms. Our system sends the 
English questions to the Google translation engine and 
filters out the stop words in the translation result. The 
rest of the words are treated as query terms. These two 
methods can complement each other in case a term 
might have different translations.  
Since there are five more new question types (why, 
person, organization, location, date), in addition to the 
four types (definition, biography, relationship, event) 
in the last IR4QA, we believe that the results from the 
answer type analysis of CCLQA groups might help. 
Therefore, the query terms used in the question 
analysis are also used in our system. 
2.2 Data processing by different Index 
method 
The indexing tool of our system is the Lucene toolkit 
(http://lucene.apache.org/). Before it can be indexed by 
Lucene, corpora in different languages are segmented 
using different tools. 
2.2.1 Traditional Chinese Document Indexing
Our system uses a traditional Chinese word 
segmentation toolkit developed by the CKIP group 
(Chinese Knowledge and Information Processing) to 
segment the traditional Chinese corpus into indexing 
terms. The CKIP group is a research team formed by 
the Institute of Information Science and the Institute of 
Linguistics of Academia Sinica in 1986. The average 
accuracy of the toolkit is about 95%. 
(http://ckipsvr.iis.sinica.edu.tw/)
2.2.2 Simplified Chinese Document Indexing
Our system uses a simplified Chinese word 
segmentation toolkit developed by ICTCLAS (Institute 
Computing Technology, Chinese Lexical Analysis 
System) to segment the simplified Chinese corpus into 
indexing terms. The average accuracy of the toolkit is 
about 98%. (http://ictclas.org/index.html)
2.2.3 Japanese Document Indexing 
For Japanese word segmentation, our system uses a
free Japanese segmentation toolkit JUMAN (a
User-Extensible Morphological Analyzer for Japanese)
development by Matsumoto et al. [9].
(http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html)
3. QUERY EXPANSION 
Query expansion [4] usually is based on the thesaurus 
method and the Pseudo relevance feedback. QE can 
help to increase the recall of information retrieval. 
Okapi BM25 is the most widely-used pseudo relevance 
feedback algorithm, which uses the result of the first 
retrieval as a source to extract more query terms for the 
second retrieval. In this paper, we also try to use a
topic crawler as another source of pseudo relevance 
feedback. 
3.1 Okapi BM25 
We use the OKAPI BM25 algorithm as the basic 
pseudo relevance feedback [9, 11]. The OKAPI BM25 
formulas are as follows. The similarity between a 
query Q and a document Dn can be computed by using
 





QT
n qtfktfK
qtfktfkwDQSim
))((
)1()1(,
3
311       where 
)5.0/()5.0(
)5.0/()5.0(log1



rRnNrn
rRrw
))1((1 avdl
dlbbkK 
N: Number of items (documents) in the collection 
n: Collection frequency: number of items containing a 
specific term 
R: Number of items known to be relevant to a specific 
topic 
r: Number of these containing the term 
tf: Frequency of occurrences of the term within a 
specific document 
qtf: Frequency of occurrences of the term within a 
specific query 
dl: Document length (arbitrary units) 
avdl: Average document length 
ki,b: Constants used in various BM functions 
3.2 Topic web crawler 
A topic web crawler is a Web spider program that can 
retrieve only the documents related to a give topic. 
This kind of crawler is called a focused crawler or 
thematic crawler. The key difference between a 
focused crawler and a general crawler lies in the ability 
of the focused crawler to find more related documents 
among all available links. In a previous research, G. 
Pant and P. Srinivasan [11] proposed a focused crawler 
based on a classifier. G. Almpanidis, C. Kotropoulos, I. 
Pitas [1] used the lantern semantic of a webpage text 
and link relation to design a focused crawler. Z. Chun, 
J. Ma, J. Lei [6] proposed a focused crawler for both 
English and Chinese, which used hierarchical 
taxonomy to describe the topic, and integrated the 
Shark-Search algorithm with four different relevance 
prediction strategies to find related documents. 
In our system, we incorporate a topic web crawler to 
get more related documents from the web and use them 
Proceedings of NTCIR-8 Workshop Meeting, June 15–18, 2010, Tokyo, Japan
― 104 ―
In the second experiment, we compared the proportion 
of the expanded query terms from two sources: Okapi 
BM25 and a Topic web crawler. We tried a total of 30
or 50 terms in different proportions between 0 to 
100%. 
5.2.1  Experiment 1 
We tested this setting on all EN-JA, EN-CT, EN-CS
runs and reported the MAP. 
In Tables 4, 5, and 6, the representative results of the 
experiments in EN-JA, EN-CT, and EN-CS are given, 
respectively. The result shows that more query terms 
from OKAPI BM25 and less query terms from 
Wikipedia will get a better MAP. The best proportion 
is about 80:20. 
5.2.2  Experiment 2 
We built a topic web crawler in Chinese only, because 
of the limitation of the language resource. Therefore, 
we tested this setting only on EN-CT and EN-CS runs 
and reported the MAP.
The original query terms were used to get the seed 
URLs by sending them to the Google search engine. 
The topic web crawler then followed the seed URLs to
get more related documents. These documents were 
used as another kind of pseudo relevance feedback. 
Our system extracted the keyword in the titles and 
anchor texts from these documents as query expansion 
candidates. 
In Tables 7 and 8, the representative results of 
experiments in EN-CT and EN-CS are given, 
respectively. The results show that a topic web crawler 
can improve the search MAP. The proportion of query 
terms from OKAPI BM25 or a Topic web crawler is 
not clear. The best proportion can be 70:30 or 40:60.  
6. CONCLUSIONS 
This paper reports the results of combining query 
terms from different sources on query expansion in 
CLIR. We tested this idea on EN-JA, EN-CT, and 
EN-CS pairs. The method in official runs combines the 
translation results from Wikipedia and Google 
translation. We conducted several additional runs to 
show that the combined QE is better than QE from a
single source. In additional runs, we added a topic web 
crawler for further query expansion in EN-CT and 
EN-CS. The titles and anchor texts in related pages 
were treated as another source of QE. The experiment 
results show that this further expansion improved 
performance.  
6.1  Future work 
The question types of the IR4QA task increased from 4 
in NTCIR-7 to 9 in NTCIR-8. This change makes the 
task more difficult. In the future, the IR system must 
use more information on the question types, such as 
building classifiers to relate documents to particular 
question types. 
Table4. The performances of JA-runs; QE term=20; the different proportion in QE term from 
Okapi and Wikipedia.
Table5. The performances of CT-runs; QE term=20; the different proportion in QE term from 
Okapi and Wikipedia. 
Okapi QE : Wikipedia QE(QE term=50)
Run 100:0 90:10 80:20 70:30 60:40 50:50 40:60 30:70 20:80 10:90 0:100
CYUT-EN-JA-T-01 0.1628 0.1636 0.161 0.1603 0.1594 0.1561 0.154 0.1515 0.1428 0.1321 0.1034
CYUT-EN-JA-T-02 0.1617 0.1625 0.1601 0.1594 0.1583 0.155 0.1528 0.1503 0.1414 0.131 0.1024
CYUT-EN-JA-D 0.0881 0.0928 0.0929 0.0917 0.0907 0.0893 0.0877 0.0849 0.0822 0.079 0.058
CYUT-EN-JA-DN 0.0857 0.0904 0.0895 0.0904 0.0905 0.0875 0.0851 0.0822 0.0813 0.077 0.0569
Okapi QE : Wikipedia QE(QE term=20)
Run 100:0 90:10 80:20 70:30 60:40 50:50 40:60 30:70 20:80 10:90 0:100
CYUT-EN-CT-T-01 0.1738 0.1738 0.1746 0.1762 0.1782 0.1768 0.1752 0.1704 0.1667 0.1648 0.153
CYUT-EN-CT-T-02 0.1938 0.1935 0.1943 0.1948 0.1971 0.1959 0.1938 0.1911 0.1877 0.1842 0.1697
CYUT-EN-CT-D 0.1382 0.1406 0.141 0.1379 0.1395 0.1396 0.1381 0.1352 0.1313 0.123 0.1137
CYUT-EN-CT-DN 0.1559 0.1567 0.1571 0.1565 0.1567 0.1555 0.153 0.152 0.149 0.1427 0.1343
Proceedings of NTCIR-8 Workshop Meeting, June 15–18, 2010, Tokyo, Japan
― 106 ―
pp.125-131.
[6] Zhumin Chen, Jun Ma, Jingsheng Lei, Bo
Yuan, Li Lian, Ling Song, “A
cross-language focused crawling algorithm 
based on multiple relevance prediction 
strategies ”, Computers & Mathematics 
with Applications, Volume 57, Issue 6, 
March 2009, Pages 1057-1072.
[7] Rada Mihalcea, "Using Wikipedia for 
Automatic Word Sense Disambiguation",
Proceedings of NAACL HLT, 2007, pp. 
196–203.
[8] A. Mirana, “Using statistical term 
similarity for sense disambiguation in 
cross-language information retrieval”, 
Information Retrieval, Volume 2, Number 
1, 2000, pp. 67–68.
[9] Yuji Matsumoto, Sadao Kurohashi, Yutaka 
Nyoki, Hitoshi Shinho, and Makoto Nagao. 
"User's Guide for the Juman System, a 
User-Extensible Morphological Analyzer 
for Japanese. Version 0.5", Kyoto 
University. (in Japanese). 
[10] Tetsuji Nakagawa, and Mihoko Kitamura, 
“NTCIR-4 CLIR Experiments at Okapi”, 
Proc. of the Fourth NTCIR Workshop on 
Research in Information Access 
Technologies Information Retrieval, 
Question Answering and Summarization,
April 2003 – June 2004. 
[11] G. Pant, P. Srinivasan, “Learning to Crawl: 
Comparing Classification Schemes” , ACM 
Transactions on Information 
Systems(TOIS), Vol. 23, No. 4, October 
2005, Pages 430–462. 
[12] S. E. Robertson, S. Walker, S. Jones, M. 
Hancock-Beaulieu and M. Gatford, “Okapi 
at TREC-3”, In Proceedings of the Third 
Text Retrieval Conference (TREC-3), NIST, 
1995.
[13] Chen-Yu Su, Tien-Chien Lin, Shih-Hung 
Wu, “Using Wikipedia to Translate OOV 
Terms on MLIR”, Proceedings of NTCIR-6
Workshop Meeting, May 15-18, 2007, pp. 
109-115.
[14] Tetsuya Sakai, Hideki Shima, Noriko 
Kando, Ruihua Song, Chuan-Jie Lin,
Teruko Mitamura, Miho Sugimoto,
“Overview of NTCIR-8 ACLIA IR4QA ”, 
In Proceedings of the 8th NTCIR Workshop,
2010. 
[15] Fan, Weiguo, Luo, Ming, Wang, Li, Xi, 
Wensi and Fox, Edward A. , “Tuning 
Before Feedback: Combining Ranking 
Discovery and Blind Feedback for Robust 
Retrieval”, Proceedings of the 27th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, 2004, pp. 138-145.
[16] Ying Zhang, Phil Vines, and Justin Zobel, 
“Chinese OOV Translation and 
Post-translation Query Expansion in 
Chinese-English Cross-lingual Information 
Retrieval”, ACM Transaction on Asian 
Language Information Processing, Vol. 4, 
No. 2, June 2005, pp. 55-77.
[17] Ying Zhang, and Phil Vines, “Using the
Web for Automated Translation Extraction 
in Cross-Language Information Retrieval”,
Proc. of the 27th annual international 
ACM SIGIR conference on Research and 
development in information retrieval, 
Sheffield, United Kingdom, July 25 - 29,
2004, pp. 162-169.
Proceedings of NTCIR-8 Workshop Meeting, June 15–18, 2010, Tokyo, Japan
― 256 ―
ε೽ϩ Neqa Other
ޑ DE Other
Ӧ୔ Nc Other
फ़Н VA Other
ୃ D Other
ӭ VH Other
An n-gram template is a sequence of length n. The terms in the 
sequence can be text or POS/NE tag. For example, a tri-gram 
templates in Table 1 can be: 
“εഌ/ε೽ϩ/ޑ”, “location/ε೽ϩ/ޑ”, “εഌ/Neqa/ޑ”,
“location/Neqa/ޑ”, “εഌ/ε೽ϩ/DE”, or “location/ε೽ϩ
/DE”.
And the bi-gram templates can be:
“εഌ /ε೽ϩ ”, “location/ε೽ϩ ”, “εഌ /Neqa”,
“location/Neqa”, or “ֽ೽/Ӧ୔”.
Since the POS/NE tag is too general, they do not suite to be 
used as features for classifier, unigram features used in our 
system are the text.
After all the template candidates are generated, our system 
counts the frequency of each candidate in each of the three 
training corpus. Templates with low frequency will be deleted. 
The templates with relative high frequency will be used as 
features for classifiers. 
2.2 Feature sizes 
We submitted three official runs. At run 1, we adopt NTUSD 
and templates up to length 3 as features of SVM. The total 
dimension is 11. Two are from NTUSD positive and negative.
Nine from our N-gram templates: they are unigram, bigram 
and trigram templates for positive, negative, and neutral. 
Since the numbers of templates are very large, we divide the 
templates into several sub-set, and treat each sub-set (size 
6000 or 3000) as a feature. The number of feature thus 
increases to 57 at Run 2, and 114 at Run 3. And in Run 2 and 
Run 3, we omitted the NTUSD to see if we can use domain 
independent features only. The setting is summarized in Table 
2.
Table2. Setting of official runs of cyut 
NTUSD N-gram Feature size
Run 1 Yes 1,2,3 11
Run 2 NO 1,2,3 57
Run 3 NO 1,2,3 114
2.3 Setting of additional runs 
In additional runs, we compare three factors in our official 
runs. First is the number of N-gram. Second is with or without 
NTUSD. Third is the number of features. 
In the official runs, we test only the templates with length less 
than 3. In the additional runs we test the templates with length 
less than 4. Table 3 listed the experiment settings of 
comparable runs. The sizes of features increase with different 
number, since we divide the 4-gram templates into sub-sets 
with various sizes. We merge 3000 templates into one feature 
in Run 6, 7, 10, and 11.We merge 6000 templates into one 
feature in Run 4, 5, 8, and 9. Originally, in Run 1, 2, 3, 4, 6, 8, 
an 10, the feature size of NTUSD is 2. Since the number of 
words in NTUSD is also above ten thousand, we divide it into 
several sub-set (size 300), therefore, the feature size of 
NTUSD is 37 for Run 5, 7.  
Then we compare the features with or without NTUSD. Note 
that, without NTUSD, the feature size will decrease by 2.
Table 4 listed the experiment settings comparable runs.  
We also change the sizes of sub-sets in different settings, the 
feature sizes are also changed accordingly. Table 5 listed the 
experiment settings of comparable runs. 
Table3. Comparable runs with different N-gram
NTUSD N-gram Size
Run 4 Yes 1,2,3 59
Run 5 Yes 1,2,3 116
Run 6 Yes 1,2,3 94
Run 7 Yes 1,2,3 151
Run 8 Yes 1,2,3,4 106
Run 9 Yes 1,2,3,4 141
Run 10 Yes 1,2,3,4 163
Run 11 Yes 1,2,3,4 198
Table 4. Comparable runs with or without NTUSD 
NTUSD N-gram Size
Run 2 No 1,2,3 57
Run 3 No 1,2,3 114
Run 12 No 1,2,3,4 104
Run 15 No 1,2,3,4 161
Run 4 Yes 1,2,3 59
Run 6 Yes 1,2,3 116
Run 8 Yes 1,2,3,4 106
Run 10 Yes 1,2,3,4 163
Table 5. Comparable runs with more or less features 
NTUSD N-gram Size
Run 2 No 1,2,3 57
Run 12 No 1,2,3,4 104
Run 15 No 1,2,3,4 161
Run 13 No 1,2,3 29
Run 14 No 1,2,3,4 52
Run 16 No 1,2,3,4 81
3. EXPERIMENT RESULTS
Table 6 shows the opinion sentence extraction and polarity 
identification results of our system. Where Run 1 to 3![10] are 
official runs of NTCIR-8 MOAT subtask in traditional Chinese; 
and Run 4 to 16 are additional runs. The P, R, and F are the 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參與兩項日本 NII 舉辦之 NTCIR-8 公開評測。其中支援問答系統之資訊檢索系
統 IR4QA 項目，獲得上台報告之機會。此外，在意見探勘 MOAT 項目以海報論文
發表。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
