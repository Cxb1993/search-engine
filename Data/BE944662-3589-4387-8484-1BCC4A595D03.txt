2 
 
英文摘要 (Abstract) 
In the first year of this project, we use Support Vector Clustering and One-class 
Support Vector Machines to solve multi-class classification and clustering problems. 
In the second year of this project, we use multiple-kernel learning to solve how to 
choose suitable kernel functions and hyperparameters problems. 
Early SVM-based multi-class classification algorithms work by splitting the 
original problem into a set of two-class sub-problems. The time and space required by 
these algorithms are very much demanding. We present in this project a hybrid 
method that integrates several one-class SVMs with discriminant functions to solve 
the multi-class classification problem. Several discriminant functions, including 
similarity measure, distance measure, and Z-score measure, have been applied in this 
research. The proposed method has low time and space complexities. Experimental 
results show that our method compares favorably with other multi-class classification 
algorithms on several real datasets from UCI and Statlog. 
One-class SVM (Support Vector Clustering) is a kernel-based method that 
utilizes the kernel trick for data clustering. However it is only able to detect one 
cluster of non-convex shape in the feature space. In this project, we propose an 
iterative two-stage one-class SVM (Support Vector Clustering) to cluster data into 
several groups. In the first stage, one-class SVM (Support Vector Clustering) is used 
to find non support vectors for each cluster in the feature space, while in the second 
stage the non support vectors are used to train one-class SVM (Support Vector 
Clustering) again and the new support vectors are used to refine the clustering result. 
Experimental results have shown that our method compares favorably with other 
kernel based clustering algorithms, such as KKM and KFCM on several synthetic 
data sets and UCI real data sets. 
Support vector machines (SVMs) have been successfully applied to classification 
problems. Practical issues involve how to determine the right type and suitable 
hyperparameters of kernel functions. Recently, multiple-kernel learning (MKL) 
algorithms have been developed to handle these issues by combining different kernels 
together. The weight with each kernel in the combination is obtained through learning. 
One of the most popular methods is to learn the weights with semidefinite 
programming (SDP). However, the amount of time and space required by this method 
is demanding. In this paper, we introduce two strategies, together with an induction 
setting, to reduce the search complexity of the multiple-kernel learning. The primal 
and the dual forms of SDP are derived. Experimental results obtained from running on 
Boosting One-Class Support Vector Machines
for Multi-Class Classification
Shie-Jue Lee, Chi-Yuan Yeh, Zhi-Ying Lee
Department of Electrical Engineering, National Sun Yat-Sen University,
Kaohsiung 804, Taiwan
1 Introduction
In a multi-class classification problem, we are given ` labeled training patterns
Str = {(x1, y1), ..., (x`, y`)}, where xi ∈ Rn, i = 1, ..., `, and yi ∈ {1, ..., k},
meaning that xi belongs to class yi. A model is learned from these patterns
and used to predict the class label of unseen data. Several solutions have been
proposed for solving multi-class classification problems, and support vector
machines (SVMs) is one of those with high performance [1],[2],[3]. In addition,
many studies have indicated that ensemble methods, such as boosting, can
improve the accuracy of any given learning algorithm [4],[5],[6],[7],[8],[9]. In
this study, a hybrid method using an improved multi-class boosting algorithm
and one-class SVMs with a well-designed discriminant function is proposed to
solve a multi-class classification problem.
AdaBoost, the most popular boosting method, was proposed by Freund and
Schapire in 1997 [5]. It requires the performance of each base classifier be bet-
ter than random guessing. Many studies using na¨ıve Bayes [10],[11], decision
trees [12],[13],[14],[9], artificial neural networks (ANN) [15],[16], or SVMs [6] as
the base classifiers for AdaBoost have been shown to obtain good performance
due to the generalization ability of AdaBoost. Since conventional AdaBoost
4
each classifier. On the other hand, binary SVMs constructed by OVO or DAG
use fewer training patterns taken from classes i and j where i, j ∈ {1, ..., k},
and i 6= j. But, the number of binary classifiers is large when k is large. For
example, if there are 20 classes, 190 classifiers must be trained.
Recently, KSVDD has been proposed for multi-class classification by con-
structing k support vector data descriptions (SVDD) [27],[3]. A SVDD is op-
timized only for the training patterns of class m, where m ∈ {1, ..., k}. Thus
the size of a kernel matrix and the training time of this method are less than
those of OVA and OVO. In the testing period it requires decisions based on
k SVDD classifiers and a discriminant function. One-class SVMs have also
been proposed for document classification [28]. Since the discriminant func-
tion used is simple, it may lead to poor performance. In this study, we pro-
pose a hybrid method integrating AdaBoost.MK and one-class SVMs to solve
multi-class classification problems. AdaBoost.MK alleviates the strict error
bound requirement imposed in AdaBoost.M1 and boosts the overall perfor-
mance. Using one-class SVMs as base classifiers in AdaBoost.MK reduces the
time and space complexities required by conventional SVMs. A well designed
discriminant function is used to improve the discrimination capability of one-
class SVMs. Experimental results on data sets taken from UCI and Statlog
show that the proposed method outperforms other multi-class classification
algorithms with statistical significance.
The rest of this paper is organized as follows. Section 2 presents related work
on boosting and SVDD. Section 3 describes the proposed multi-class classifier
based on one-class SVMs and an improved discriminant function. Section 4
describes the proposed AdaBoost.MK algorithm and the integration with the
proposed multi-class classifiers. In Section 5, experimental results and com-
parisons with other methods are presented. Finally, we conclude this study in
Section 6.
6
tor clustering (SVC) to compute the smallest sphere in the feature space en-
closing the image of the input data. This method can be adopted to solve the
multi-class classification problem [34],[27]. In order to determine the similar-
ity between a testing pattern xp and the training patterns of class m, various
discriminant functions have been proposed and are briefly described below.
(1) Lies-on or lies-inside the sphere [34]: The discriminant function is defined
to be:
fm(xp,am, Rm) = I[‖φ(xp)− am‖2 ≤ R2m],∀m ∈ {1, ..., k} (1)
with
I[A] =

1 if A is true;
0 else
(2)
where am and Rm are the center and the radius, respectively, of the
mth SVDD in the feature space. This discriminant function may lead to
undesired situations where a testing pattern xp is assigned to more than
one class or no class at all.
(2) Nearest-center (NC) [34],[27]: This discriminant function assigns a test-
ing pattern xp to class m with the center am being closest to xp. The
discriminant function is defined as follows:
f(xp,a)= arg min
m=1,...,k
‖φ(xp)− am‖2
=arg min
m=1,...,k
(
K(xp,xp)
)
−2
|Cm|∑
j=1
αj,mK(xp,xj,m)
(
+
|Cm|∑
i,j=1
αm,iαm,jK(xi,m,xj,m)
)
(3)
where a = {a1, ...,ak}, αj,m’s are Lagrange multipliers, Cm is the set of
training patterns belonging to class m, and |Cm| is the cardinality of Cm.
The drawback of this approach is that it does not consider the radius of
8
C1
C2
Ck
.
.
.
OCSVM1
OCSVM2
OCSVMk
.
.
.
training
training
Training Data
Testing Data xp
testing
testing
training
testing
Discriminant 
Function
ˆpy
,1pD
,2pD
,p kD
trS
1,sD
2,sD
ksD ,
Fig. 1. The architecture of MOCSVMC
classifier.
Algorithm 1 MOCSVMC.Training(Str)
Input: The set of labeled training patterns Str with k classes.
1: Set the paprmeter q for RBF kernel, and ν for the proportion of support
vectors.
2: Split Str into C1, C2, . . . , Ck where Cm contains the training patterns of
class m.
3: for m = 1 to k do
4: Train the one-class SVM OCSVMm for the training subset Cm, and
obtain a set of coefficients (αj,m, ρm) from this model.
5: for s = 1 to |Cm| do
6: Ds,m ←
|Cm|∑
j=1
αj,mK(xj,m,xs,m), where xj,m,xs,m ∈ Cm.
7: end for
8: σ2m ← according to Eq. (17).
9: end for
Return: OCSVMm and associated parameter values, m = 1, . . . , k.
10
mwmH
1 
m
P
mU
,p mD
,
,
1
p m
p m
d
D
 
O
mU1
,
,
1
p m
p m
d
D
 
support vector
unit ball
Fig. 2. The mth OCSVM, OCSVMm
large. In practice, the size of ν is an upper bound on the fraction of outliers
and a lower bound on the fraction of support vectors [35]. Throughout this
paper, we set ν = 0.05 meaning that there are at most 5% of outliers in the
training patterns.
In order to solve the constrained optimization problem, a Lagrangian is intro-
duced as follows:
L=
1
2
‖wm‖2 − ρm + 1
ν|Cm|
|Cm|∑
i=1
ξi,m
−
|Cm|∑
i=1
αi,m[(wm · φ(xi))− ρm + ξi,m]
−
|Cm|∑
i=1
βi,mξi,m (6)
where αi,m ≥ 0 and βi,m ≥ 0 are Lagrange multipliers. Setting partial deriva-
12
where q is the width parameter. This equation implies that each input point is
mapped on the octant surface of the unit ball in the high-dimensional feature
space, as shown in Figure 2. Notice that, OCSVM is completely equivalent to
SVDD when the RBF kernel is used [35],[32].
3.2 Discriminant Function
A discriminant function is designed so that a testing pattern xp is assigned to
the classm having the shortest distance between xp and µm which is the center
of the mth OCSVM. This distance measure considers not only the reciprocal
projection value difference between xp and µm but also the variance of the
training subset Cm. The discriminant function is defined as follows:
yˆp = f(Dp) = arg min
m=1,...,k
Map,m (14)
where
Map,m =
‖dp,m − µm‖2
2σ2m
(15)
with µm = 1 and dp,m = 1/Dp,m being the reciprocal projection value of xp
onto wm as shown in Figure 2. Dp,m is defined to be:
Dp,m =
∑
xj,m∈SVm
αj,mK(xj,m,xp) (16)
and σ2m is the variance of the training subset Cm which can be estimated by
using a maximum likelihood estimation technique. The unbiased estimate of
the variance is defined as follows:
σ2m =
|Cm|∑
s=1
(ds,m − µm)2
|Cm| − 1 (17)
14
4 An improved multi-class AdaBoost algorithm (AdaBoost.MK)
AdaBoost.M1 is an extension of Adaboost to solve the multi-class classifica-
tion problem. However, it tends to stop the iterative procedure pre-maturely
due to its strict restriction on the performance of base classifiers. We improve
this algorithm by introducing AdaBoost.MK that only requires a better per-
formance than random guessing for base classifiers. The algorithm is shown
in Algorithm 3 for a training set Str with k classes. Note that the base clas-
sifier is MOCSVMC described in the previous section. Firstly, the number of
iterations, T , to be done is set. Then the weight associated with each training
pattern is initialized. In each iteration, ` patterns are drawn from Str accord-
ing to the weights of the patterns. Then a distinct MOCSVMC is built and
the error induced by the built MOCSVMC is computed. Then the weights of
the patterns are updated according to the value of the ensemble coefficient
γ. The process is iterated until either the error induced by the MOCSVMC
currently built is too big or the number of iterations exceeds the limit T .
The major differences between AdaBoost.M1 and AdaBoost.MK are the state-
ments of lines 10, 14, 15, and 17. In AdaBoost.M1, the error of each base
classifier is required to be less than 1/2, i.e., ²t < 1/2. This requirement is
too strict to satisfy for a multi-class problem. In stead, AdaBoost.MK re-
quires ²t < (k − 1)/k. In AdaBoost.M1, the ensemble coefficient is set to be
γt = ln(
1−²t
²t
). The weights of the misclassified patterns are unchanged and
the weights of correctly classified are decreased by the factor e−γt . In Ad-
aBoost.MK, γt has the value as shown on line 15, and the setting of this value
will be derived later. Furthermore , the weights of misclassified patterns are
increased by an exponential factor eγt/(k−1) while the weights of correctly clas-
sified patterns are decreased by another exponential factor e−γt , as shown on
line 17. In this way, the chances of the misclassified patterns being selected in
the next iteration are effectively increased.
16
The setting of the ensemble coefficient γt is deriveded as follows. Similar to
AdaBoost, AdaBoost.MK tries to minimize the following objective function:
LAdaBoost.MK = e
−γt × (1− ²t) + e
γt
k−1 × ²t. (18)
Setting partial derivatives of the objective function equal to 0 yields the value
of the ensemble coefficient γt:
∂LAdaBoost.MK
∂γt
= 0
→−(1− ²t)× e−γt + 1
k − 1 × ²t × e
γt
k−1 = 0
→ (1− ²t)× e−γt = ²t
k − 1 × e
γt
k−1
→ ln((1− ²t)× e−γt) = ln
( ²t
k − 1 × e
γt
k−1
)
→ γt
k − 1 + γt = ln(1− ²t)− ln
( ²t
k − 1
)
→ γt = k − 1
k
(
ln
((1− ²t)× (k − 1)
²t
))
. (19)
When AdaBoost.MK stops, a number of MOCSVMCs are obtained. We can
then use them to determine to which class a test pattern xp belongs. Let
MOCSVMC1, . . . , MOCSVMCN beN base classifiers returned by AdaBoost.MK.
By applying MOCSVMC.Testing(xp) on these base classifiers, we obtain pre-
dictions yˆp,t for t = 1, . . . , N . The class C that xp belongs to is then determined
by
C = arg max
y∈{1,...,k}
N∑
t=1
γtI[yˆp,t = y]. (20)
Note that γt, t = 1, . . . , N , are ensemble coefficients of the base classifiers
returned by AdaBoost.MK.
18
Table 3
The accuracy for six datasets from UCI and Statlog
Method
Dataset MinD NC NSV
iris 95.000 ± 0.566 94.600 ± 0.734 94.400 ± 0.644
glass 71.549 ± 1.818 49.394 ± 0.867 64.577 ± 1.698
vowel 98.899 ± 0.279 98.586 ± 0.393 98.960 ± 0.274
segment 97.416 ± 0.119 91.255 ± 0.126 96.965 ± 0.133
satimage 91.806 ± 0.117 82.832 ± 0.077 87.157 ± 0.186
letter 96.131 ± 0.067 95.407 ± 0.055 93.715 ± 0.083
5.2 Experiment I
Firstly, we compare the performance of the proposed MOCSVMC with other
KSVDD-based methods. The results are summarized in Table 3. The “MinD”
column stands for the method by MOCSVMC, the “NC” column means the
method by KSVDD with the nearest-center measure, and the “NSV” column
means the method by KSVDD with the nearest-support-vector measure. Each
entry in the table represents the average of the accuracy values and standard
deviations from 10 runs of 10-fold cross validations. The highest performance
in each dataset is bold-faced. One can see that MOCSVMC outperforms the
other two KSVDD-based methods in all datasets, except for the vowel dataset.
In order to rule out random effects in this comparison, paired samples t-tests
were conducted for each pair of the classification methods on each dataset.
With a sample size of 10 accuracy values (there are 10 runs of 10-fold cross
validations) and a cut-off value of 0.05 for p, the critical t-value becomes
2.262. Table 4 lists the t-value from the paired samples t-tests for these three
classifiers on all datasets. The “MinD vs. NC” column compares MOCSVMC
with the MinD measure to KSVDD with the NC measure; the “MinD vs. NSV”
column compares MOCSVMC with the MinD measure to KSVDD with the
NSV measure; and the “NC vs. NSV” column compares KSVDD with the NC
measure to KSVDD with the NSV measure.
20
Table 5
The accuracy of multi-class AdaBoost.M1 for six datasets from UCI and Statlog
Method
Dataset MinD NC NSV
iris 98.467 ± 0.706 98.333 ± 0.956 98.267 ± 0.562
glass 90.061 ± 1.041 74.970 ± 1.448 89.482 ± 1.862
vowel 99.980 ± 0.064 99.909 ± 0.193 99.980 ± 0.064
segment 98.970 ± 0.104 97.905 ± 0.338 98.978 ± 0.128
satimage 97.191 ± 0.192 95.623 ± 0.527 97.802 ± 0.419
letter 99.408 ± 0.115 99.315 ± 0.080 99.231 ± 0.090
significant for most test datasets.
5.3 Experiment II
Secondly, we compare the performance of AdaBoost.MK with that of Ad-
aBoost.M1. The three multi-class classifiers (MOCSVMC with MinD, KSVDD
with NC, KSVDD with NSV) in Experiment I are used as the base classifiers
in this study. Tables 5 and 6 list the experimental results for each combination
of boosting algorithms (AdaBoostM1 and AdaBoost.MK) and base classifiers
on UCI and Statlog datasets respectively. Again, each entry represents the
average accuracy and standard deviation from 10 runs of 10-fold cross vali-
dations. The highest performance in each column is bold-faced. One can see
that in almost all cases, except for the segment and satimage datasets with
AdaBoost.M1, the combination of MOCSVMC with MinD and AdaBoost is
the best performer. This shows that MOCSVMC with MinD works effectively
when coupled with a meta-learner like AdaBoost.M1 or AdaBoost.MK.
In order to compare the effects of boosting, we also ran paired samples t-
tests to compare results from the three base classifiers with and without
AdaBoost.M1 or AdaBoost.MK. In Table 7, a column like “MinD M1 vs.
MinD” indicates the t-values from the comparison of AdaBoost.M1-boosted
22
Table 7
Paired Samples T-Test for six datasets from UCI and Statlog
Pair
Dataset MinD M1 vs. MinD NC M1 vs. NC NSV M1 vs. NSV
iris 10.615* 15.089* 13.931*
glass 33.421* 66.031* 34.087*
vowel 14.636* 15.670* 12.485*
segment 37.006* 56.172* 44.923*
satimage 72.068* 81.411* 68.913*
letter 81.547* 124.040* 123.113*
Pair
Dataset MinD MK vs. MinD NC MK vs. NC NSV MK vs. NSV
iris 11.551* 14.595* 17.448*
glass 66.383* 68.354* 30.345*
vowel 13.500* 13.035* 12.485*
segment 80.463* 95.293* 57.326*
satimage 201.283* 136.942* 138.581*
letter 162.359* 140.080* 189.497*
Pair
Dataset MinD MK vs. MinD M1 NC MK vs. NC M1 NSV MK vs. NSV M1
iris 6.091* 1.627 2.326*
glass 12.228* 23.453* 5.380*
vowel 1.000 0.000 0.000
segment 21.062* 17.008* 4.393*
satimage 40.039* 16.280* 12.200*
letter 10.527* 3.633* 11.222*
* : significant with p < 0.05
6 Conclusion
In this study, we propose a novel multi-class AdaBoost.MK algorithm with
MOCSVMC and an improved discriminant function as the base classifier to
classify a k-class dataset. Experimental results allow us to make the following
remarks regarding the proposed method:
24
1–15.
[5] Y. Freund, R. E. Schapire, A Decision-Theoretic Generalization of On-
Line Learning and an Application to Boosting, Journal of Computer and
System Sciences 55 (1) (1997) 119–139.
[6] X. Li, L. Wang, E. Sung, A Study of AdaBoost with SVM Based Weak
Learners, in: IEEE International Joint Conference on Neural Networks
(IJCNN), Vol. 1, 2005, pp. 196–201.
[7] G. Mart´ınez-Mun˜oz, A. Sua´rez, Using boosting to prune bagging ensem-
bles, Pattern Recognition Letters 28 (1) (2007) 156–165.
[8] R. E. Schapire, Y. Singer, Improved Boosting Algorithms Using
Confidence-rated Predictions, Machine Learning 37 (3) (1999) 297–336.
[9] Y. Sun, S. Todorovic, J. Li, Unifying multi-class AdaBoost algorithms
with binary base learners under the margin framework, Pattern Recogni-
tion Letters 28 (5) (2007) 631–643.
[10] H.-J. Kim, J.-U. Kim, Y.-G. Ra, Boosting Na¨ıve Bayes text classification
using uncertainty-based selective sampling, Neurocomputing 67 (2005)
403–410.
[11] R. Nock, M. Sebban, A Bayesian boosting theorem, Pattern Recognition
Letters 22 (3-4) (2001) 413–419.
[12] T. G. Dietterich, An Experimental Comparison of Three Methods for
Constructing Ensembles of Decision Trees: Bagging, Boosting, and Ran-
domization, Machine Learning 40 (2) (2000) 139–157.
[13] J. H. Friedman, T. Hastie, R. Tibshirani, Additive Logistic Regression:
a Statistical View of Boosting, The Annals of Statistics 38 (2) (2000)
337–374.
[14] J. J. Rodr´ıguez, J. Maudes, Boosting Recombined Weak Classifiers, Pat-
tern Recognition Letters.
[15] G. Ra¨tsch, T. Onoda, K. R. Mu¨ller, Soft Margins for AdaBoost, Machine
Learning 42 (3) (2001) 287–320.
[16] H. Schwenk, Y. Benggio, Boosting Neural Networks, Neural Computation
26
fication problems, in: The Congress on Evolutionary Computation, 2003,
pp. 1480–1485.
[28] L. M. Manevitz, M. Yousef, One-Class SVMs for Document Classification,
Journal of Machine Learning Research 2 (2001) 139–154.
[29] J. Huang, S. Ertekin, Y. Song, H. Zha, C. L. Giles, Efficient Multiclass
Boosting Classification with Active Learning, in: The 7th SIAM Interna-
tional Conference on Data Mining (SDM 2007), 2007.
[30] E. L. Allwein, R. E. Schapire, Y. Singer, Reducing Multiclass to Binary:
A Unifying Approach for Margin Classifiers, Journal of Machine Learning
Research 1 (2000) 113–141.
[31] D. M. J. Tax, R. P. W. Duin, Support Vector Domain Description, Pat-
tern Recognition Letters 20 (11-13) (1999) 1191–1199.
[32] D. M. J. Tax, R. P. W. Duin, Support Vector Data Description, Machine
Learning 54 (1) (2004) 45–66.
[33] A. Ben-Hur, D. Horn, H. T. Siegelmann, V. Vapnik, Support Vector Clus-
tering, Journal of Machine Learning Research 2 (2) (2002) 125–137.
[34] A. Sachs, C. Thiel, F. Schwenker, One-Class Support-Vector Machines
for the Classification of Bioacoustic Time Series, International Journal
on Artificial Intelligence and Machine Learning 6 (4) (2006) 29–34.
[35] B. Scho¨lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, R. C. Williamson,
Estimating the Support of a High-Dimensional Distribution, Neural Com-
putation 13 (7) (2001) 1443–1472.
[36] A. Asuncion, D. Newman, UCI Machine Learning Repository (2007).
[37] D. Michie, D. J. Spiegelhalter, C. C. Taylor, Machine Learning, Neural
and Statistical Classification, Prentice Hall, Englewood Cliffs, N.J., 1994.
28
researchers have adopted a set of kernels to construct a combined kernel and
solve these problems successfully [12],[13],[14],[15],[16],[17],[18],[19].
The simplest way to combine multiple kernels is by averaging them. But each
kernel having the same weight may not be appropriate for the decision pro-
cess, and therefore the main issues of the multiple-kernel combination is to
determine the optimal weight for each kernel. Lanckriet et al. [15] used a
linear combination of matrices to combine multiple kernels; they integrated
the multiple-kernel learning with SVMs, and transformed the problem into
a semidefinite programming (SDP) problem, which, as a convex optimiza-
tion problem, has a global optimum. SDP can be solved efficiently by the
interior-point method. They also transformed the problem to a convex opti-
mization problem with the quadratically constrained quadratic programming
(QCQP) form which is slightly inferior to SDP in learning ability. Other effi-
cient multiple-kernel learning algorithms include Bach et al. [12], Sonnenburg
et al. [18] and Rakotomamonjy et al. [17]. These approaches deal with large-
scale problems by iteratively using the SMO algorithm [20] to update Lagrange
multipliers and kernel weights in turn, i.e., Lagrange multipliers are updated
with fixed kernel weights and kernel weights are updated with fixed Lagrange
multipliers alternatively. Although these methods are faster than SDP, they
tend to suffer from the local minimum traps. Multiple-kernel learning based
on hyperkernels has also been studied [16],[21]. Ong et al. [16] introduced the
method of hyperkernels with SDP, while Tsang and Kwok [21] reformulated
the problem as a second-order cone programming (SOCP) form which can
be solved more efficiently than SDP. On the other hand, Crammer et al. [14]
and Bennett et al. [13] use boosting methods to combine heterogeneous kernel
matrices.
30
2.1 Soft margin support vector machines
SVM is a kernel method which finds the maximum margin hyperplane in
feature space linearly separating the image of the input data into two groups
[1],[24],[25],[26]. To allow for the possibility of outliers in the dataset, and
to make the method more robust, some patterns need not be strictly and
correctly classified by the hyperplane; but the misclassified patterns should
be penalized. Therefore, slack variables ξi are introduced to account for the
misclassified patterns. Suppose we are given l labeled training patterns S =
{(x1, y1), ..., (xl, yl)}, where xi ∈ Rn and yi ∈ {+1,−1}, i = 1, ..., l. The
objective function and constraints of the problem can therefore be formulated
as:
min
w,b
1
2
wTw+ C
l∑
i=1
ξi
s.t. yi
(
wTφ(xi) + b
)
≥ 1− ξi, ξi ≥ 0, i = 1, 2, ..., l
(1)
where C is a user-defined parameter which gives a tradeoff between maximum
margin and classification error. In the above setup, φ : X → F is a mapping
from the input space to the feature space F where patterns are more easily
separated, w is a weight vector in the feature space, and b is an offset of the
hyperplane.
In order to solve the constrained optimization problem, the Lagrangian is
introduced as follows:
L(w, ξ, b,α,β) =
1
2
wTw+ C
l∑
i=1
ξi −
l∑
i=1
βiξi
−
l∑
i=1
αi
[
yi
(
wTφ(xi) + b
)
− 1 + ξi
] (2)
where αi ≥ 0 and βi ≥ 0, i = 1, ..., l are the Lagrange multipliers. L has to be
minimized with respect to the primal variables given the Lagrange multipliers,
32
2.2 Composition of kernel matrices
Early SVM-based methods used a single-kernel function, k(x,y) ≡ 〈φ(x), φ(y)〉
to calculate the inner product between two images in the feature space F . In
practice, the kernel function is characterized by a kernel matrix computed
from the training patterns. A kernel matrix is a symmetric matrix K ∈ Rl×l
where each matrix entry Kij = k(xi,xj) measures the similarity of two pat-
terns in the feature space. If a dataset has varying local distributions, using
single kernel may lose some information to cope with this varying distribution.
Usually, the decision boundary obtained is closer to the examples of a specific
class. This implies that the decision boundary may have a poor generalization
ability. Most researchers use trial and error heuristic or grid search to choose
the best hyperparameters; this obviously takes a lot of efforts. Fortunately,
kernel fusion can solve this problem. A simple direct sum fusion can be de-
fined as k(xi,xj) ≡ 〈Φ(xi),Φ(xj)〉, where Φ is a new feature mapping defined
as Φ(x) = [φ1(x) φ2(x) ...]
T . This feature mapping may handle issues of vary-
ing pattern distributions by using multiple-kernel functions. The kernel matrix
can be easily written as K = K1+K2+ ... in this case. We can generalize this
simple fusion to a weighted combination of kernel matrices as follows:
K =
M∑
s=1
µsKs. (6)
where M is the total number of kernel matrices, and µs is the weight of the
sth kernel matrix. The goal of kernel fusion is to find the optimal weight for
each kernel matrix.
A linear combination of kernel matrices may suffer from one problem. When
different kernel functions are adopted, some function may have a dominating
34
where K =
∑M
s=1 µsKs ∈ Rl×l obtained from the training patterns is the prin-
cipal submatrix of K, K º 0 means that K is a positive semidefinite matrix,
and c is a user-defined parameter. The fourth constraint of Eq. (8) implies
that the multiple-kernel learning has a better generalization ability on testing
data, but it needs to check whether the combined matrix is positive semidef-
inite in each iteration. The last constraint of Eq. (8) control the capacity of
the search space to prevent overfitting. Note that if K is positive semidefinite,
K is positive semidefinite as well [27]. In other words, a feasible solution can
simultaneously make K º 0 and K º 0.
Lanckriet et al. [15] transformed Eq. (8) into the SDP standard form as follows:
min
t,K,λ,ν,δ
t
s.t.
M∑
s=1
µsKs º 0,
trace
(
M∑
s=1
µsKs
)
= c,
G
(∑M
s=1 µsKs
)
(e+ ν − δ + λy)
(e+ ν − δ + λy)T t− 2CδTe
 º 0,
ν ≥ 0,
δ ≥ 0
(9)
where t ∈ R, ν ∈ Rl, δ ∈ Rl, and λ ∈ R. The second constraint of Eq. (9)
implies that the weight, µs, s = 1, ...,M can be negative. The optimal weights
of the linear combination can be solved by the primal-dual interior point
method [28],[29],[30].
36
be formulated as follows:
min
µ
max
α
αTe− 1
2
αTG(K)α
s.t. α ≥ 0,
Ce−α ≥ 0,
µ ≥ 0,
αTy = 0,
eTµ = 1
(10)
where K =
∑M
s=1 µsKs ∈ Rl×l is obtained from the training patterns and
µ ∈ RM is the weight vector of the kernel matrices. A simple calculation shows
that a non-negative linear combination of kernels yields another valid kernel
(Cristianini et al. [24]). However, the converse is not necessarily true, namely,
valid kernels may be constructed from Eq. (6) by using some negative weights.
Since matrix K in Eq. (6) must be symmetric, all eigenvalues of it are real.
The positive semi-definiteness requirement for a kernel matrix becomes the
non-negativity requirement of all its eigenvalues, which are implicitly defined
by the weights µs with complex relationships. Thus, the feasible region for the
weights in Lanckriet et al. [15] can be very complicated.
A SDP problem is an optimization problem with a linear objective function,
linear matrix inequalities (LMI), affine inequality, and affine equality con-
straints. It can be seen from Eq. (10) that the objective function is a quadratic
function in αi’s. In order to transform Eq. (10) into the standard SDP form,
we introduce an auxiliary variable t ∈ R that serves as an upper bound on
38
This implies that for any t > 0, the inequality constraint t ≥ ω(K) holds if
and only if there exist ν ≥ 0, δ ≥ 0, and λ such that
t ≥ (e+ ν − δ + λy)T (G(K))−1(e+ ν − δ + λy)
+ 2CδTe.
(15)
If K is a positive semidefinite kernel matrix, it can be shown that G(K) =
diag(y)Kdiag(y) is also a positive semidefinite matrix as follows.
Proof :
Let ν be any vector and diag(y) = Y.
∵ Y T = Y and Y TY is an identity matrix
⇒ 〈ν, Y KY ν〉 = 〈Y Tν,KY ν〉
= 〈Y ν,KY ν〉
∵ K º 0, applied to Y ν
⇒ 〈Y ν,KY ν〉 ≥ 0
⇒ 〈ν, Y KY ν〉 ≥ 0
⇒ Y KY º 0
⇒ G(K) º 0
(16)
Because G(K) º 0 and t− ω(K) ≥ 0, the inequality constraint is now equiv-
alent to the following LMI, using the Schur complement lemma [27],[29]:

G(K) (e+ ν − δ + λy)
(e+ ν − δ + λy)T t− 2CδTe
 º 0. (17)
40
3.2 Dual form
First, we depend on the primal variables to decompose the linear matrix in-
equality of Eq. (18). The primal form of SDP can be rewritten as follows:
min
t,µ,λ,ν,δ
t
s.t. F0 + tFt + λFλ
+
M∑
i=1
µiFµi +
l∑
j=1
νjFνj +
l∑
k=1
δkFδk º 0,
µi ≥ 0, i = 1, 2, ...,M,
νj ≥ 0, j = 1, 2, ..., l,
δk ≥ 0, k = 1, 2, ..., l,
M∑
i=1
µi − 1 = 0
(19)
where F0, Ft, Fλ, Fµi , Fνj , and Fδk ∈ R(l+1)×(l+1) are symmetric matrices. To
express F0, Ft, Fλ, Fµi , Fνj , and Fδk , we first denote Omn as an m by n matrix
whose elements are all zero, and 0m as a m-component vector whose com-
ponents are all zero, and e as l-component vector whose components are all
one.
F0 =

Oll e
eT 0
 , (20)
Ft =

Oll 0l
0Tl 1
 , (21)
42
Lagrange technique. The Lagrangian is
L(t,µ, λ,ν, δ) = t−
F0 + tFt + λFλ + M∑
i=1
µiFµi
+
l∑
j=1
νjFνj +
l∑
k=1
δkFδk
 • Z
−
M∑
i=1
piµi −
l∑
j=1
qjνj −
l∑
k=1
rkδk
+ s
(
M∑
i=1
µi − 1
)
= −F0 • Z − s
− t(Ft • Z − 1)
− λ (Fλ • Z)
−
M∑
i=1
µi(Fµi • Z + pi − s)
−
l∑
j=1
νj(Fνj • Z + qj)
−
l∑
k=1
δk(Fδk • Z + rk)
(26)
where symmetric matrix Z ∈ R(l+1)×(l+1) º 0, pi ≥ 0, i = 1, ...,M , qj ≥
0, j = 1, ..., l, rk ≥ 0, k = 1, ..., l, and s ∈ R are Lagrange multipliers.
A • B denotes the inner product between two square matrices, defined by
trace(ATB). Setting partial derivatives of L with respect to primal variables
equal to 0 yields the following results:
∂L
∂t
= Ft • Z − 1 = 0, (27a)
∂L
∂λ
= Fλ • Z = 0, (27b)
∂L
∂µi
= Fµi • Z + pi − s = 0, i = 1, ...M, (27c)
∂L
∂νj
= Fνj • Z + qj = 0, j = 1, ...l, (27d)
∂L
∂δk
= Fδk • Z + rk = 0, k = 1, ...l. (27e)
44
3.3 Worst-case time complexity analysis
In solving SDP problem, it is clear that it takes a lot of efforts to check LMI
constraint than to check the affine inequality constraint. Ignoring the affine
equality and inequality constraints, we consider a simple case of SDP problem
with multiple LMI constraints :
min
u
cTu
s.t. F j(u) º 0, j = 1, ...L
(29)
where u ∈ RN is the vector of the primal variables, c ∈ RN is the objective
vector, symmetric matrices F j ∈ RNj×Nj , j = 1, ..., L are given, and L is the
number of LMI constraints. The worst-case time complexity of Eq. (29) for
each iteration is O(N2
∑L
j=1N
2
j ). Using the primal-dual interior point method
for solving Eq. (29), it need O((
∑L
j=1Nj)
0.5) iterations [29]. Hence, the total
worst-case time complexity is O(N2(
∑L
j=1N
2
j )(
∑L
j=1Nj)
0.5).
Regarding the total worst-case time complexity of the original SDP formula-
tion, comparing Eq. (9) with the form of Eq. (29), we have N =M+l+l+2 ≈
M+2l, L = 2,N1 = l, andN2 = l+1 ≈ l. The total worst-case time complexity
is O((M +2l)2(l2+ l2)(l+ l)0.5). Regarding the total worst-case time complex-
ity of the proposed SDP formulation, comparing Eq. (18) with the form of
Eq. (29), we have N = M + l + l + 2 ≈ M + 2l, L = 1, and N1 = l + 1 ≈ l.
The total worst-case time complexity is O((M + 2l)2(l2)(l)0.5).
Note that we adopted two strategies to improve the time performance of SDP
in Lanckriet et al. [15]. The first strategy is assuming the non-negativity of
the kernel weights instead of checking the positive semidefiniteness of an en-
semble kernel in each iteration. The second strategy is setting the sum of the
46
variance = 1 to the pattern, we get a pattern of class +1. A pattern of class -1
is randomly generated with mean vector = [0 0]T , variance = 1, and covariance
= 0.5. In the figure, a pattern of class +1 is marked with ‘+’ and a pattern
of class -1 is marked with ‘x’. Note that class -1 has a dense distribution
and class +1 has a sparse distribution. The decision boundaries for separating
the two classes obtained by different single RBF kernel SVMs with different
values of γ, e.g., γ ∈ {0.01, 5, 10}, are shown in Fig. 1. Obviously, the decision
boundaries for γ = 10 and γ = 5 are too specific to the data patterns of class
-1. On the other hand, the decision boundary for γ = 0.01 is too specific to
the data patterns of class +1. Obviously, these boundaries are not good and
cannot have good generalization ability.
−8 −6 −4 −2 0 2 4 6 8
−5
0
5
10
15
 
 
class +1
class −1
Single RBF kernel γ = 10
Single RBF kernel γ = 5
Single RBF kernel γ = 0.01
Multiple RBF kernel γ = [10 0.01]T
Fig. 1. Decision boundaries obtained by single kernel SVMs and SDPI.
Instead, we apply SDPI on the dataset. A two-kernel learning is adopted for
this case, i.e., K = µ1 × K1 + µ2 × K2, where K1 and K2 are RBFs with
γ1 = 10 and γ2 = 0.01, respectively. The optimal weights µ1 and µ2 obtained
by our SDPI are 0.726 and 0.274. As shown in Fig. 1, the decision boundary
obtained is close to the middle of the separating zone and hence can increase
the generalization ability.
48
−15 −10 −5 0 5
−15
−10
−5
0
5
10
15
20
 
 
class +1 (training data)
class −1 (training data)
class +1 (testing data)
class −1 (testing data)
SDPI
SDPT
Fig. 3. Decision boundaries obtained by SDPI and SDPT on dataset DS-II
this dataset, positive patterns and negative patterns are generated with dif-
ferent mean vectors, and are required to be linearly separable with margin
equal to 1 in the input space. From the figure, we can see that SDPI can find
a decision boundary similar to that obtained by SDPT. Regarding the time
performance, SDPI runs substantially faster than SDPT. SDPI spent 2.703
seconds on training, while SDPT spent 7.171 seconds on training.
The data patterns for the datasets, DS-III to DS-VI, are drawn from the
2-variate normal distribution with different mean vectors and covariance ma-
trices, as specified in Table 1. Note that DS-VI has more overlapped patterns
than the other datasets. Therefore, DS-VI is the most difficult one to be sepa-
rated into two classes. The decision boundaries obtained by SDPI and SDPT
on the four synthetic datasets are shown in Fig. 4 - Fig. 7. Obviously, SDPI
can obtain decision boundaries similar to those obtained by SDPT. However,
it seems that SDPT is more likely to cause overfitting. For example, there is
an outlier in DS-III and DS-IV. SDPT gets annoyed by the outlier, but SDPI
doesn’t, as shown in Fig. 4 and Fig. 6. Therefore, SDPI may have a better
generalization ability than SDPT. Table 2 shows performance comparisons
50
−4 −2 0 2 4 6 8 10
−4
−2
0
2
4
6
8
10
12
14
16
18
 
 
class +1 (training data)
class −1 (training data)
class +1 (testing data)
class −1 (testing data)
SDPT
SDPI
Fig. 4. Decision boundaries obtained by SDPI and SDPT on DS-III
−4 −2 0 2 4 6 8 10
−4
−2
0
2
4
6
8
10
12
14
 
 
class +1 (training data)
class −1 (training data)
class +1 (testing data)
class −1 (testing data)
SDPI
SDPT
Fig. 5. Decision boundaries obtained by SDPI and SDPT on DS-IV
between SDPI and SDPT on the four datasets. We can see that SDPI works
almost as accurately as SDPT. For the DS-VI dataset, SDPI even gets a test-
ing accuracy rate significantly higher than SDPT, i.e., 86% vs. 78%. Note that
the sum of the kernel weights is unity for SDPI, as stated in Section 3. Also,
note that SDPI runs much faster than SDPT for each case.
52
−4 −2 0 2 4 6 8 10
−4
−2
0
2
4
6
8
10
12
14
 
 
class +1 (training data)
class −1 (training data)
class +1 (testing data)
class −1 (testing data)
SDPI
SDPT
Fig. 6. Decision boundaries obtained by SDPI and SDPT on DS-V
−4 −2 0 2 4 6 8 10
−4
−2
0
2
4
6
8
10
12
14
 
 
class +1 (training data)
class −1 (training data)
class +1 (testing data)
class −1 (testing data)
SDPI
SDPT
Fig. 7. Decision boundaries obtained by SDPI and SDPT on DS-VI
4.3 Experiment III
In this experiment, we compare single kernel SVMs, SDPI, and SDPT on
some real datasets. We chose the ‘breast cancer’, ‘ionosphere’, and ‘sonar’
datasets from the UCI Repository [22] and the ‘heart’ dataset from the Statlog
collection [23]. A brief description of these datasets is given in Table 3. For
the sake of easy comparison, we scale the values of all the training patterns
within the range of [-1, 1]. Also, a 10-fold cross validation is used to evaluate
54
Table 4
Accuracy rates of single kernel SVMs with different types of kernels.
datasets
kernel breast cancer ionosphere
Linear 91.661 79.206
polynomial d = 2 91.214 84.905
RBF γ = 1 91.661 90.879
datasets
kernel heart sonar
Linear 83.333 53.786
polynomial d = 2 82.222 53.786
RBF γ = 1 75.926 86.929
ferent RBF kernels, i.e., γ ∈ {50, 5, 0.5, 0.05, 0.005}. Table 8 shows the results
obtained by SDPT and SDPI, with the combination of the above five differ-
ent RBF kernels. From Table 8, we can see that SDPI has a higher accuracy
rate than SDPT for the ‘heart’ dataset, but has lower accuracy rates than
SDPT for the ‘breast cancer’, ‘ionosphere’ and ‘sonar’ datasets. This may be
attributed to the narrower search region used in SDPI. Nevertheless, the accu-
racy difference is very small. Table 9 shows the kernel weights obtained from
SDPI and SDPT for each dataset. Regarding the time performance, SDPI runs
substantially faster, e.g., over five times, than SDPT. Also, SDPI has higher
accuracy rates than all single RBF kernel SVMs. Notice that, from Table 7,
for a given dataset the optimal hyperparamters are different for different sin-
gle RBF kernel SVMs. For example, for the ‘breast cancer’ dataset, the best
56
Table 6
Kernel weights of different kernel types for real datasets.
breast cancer
method µpolynomial µRBF µlinear
SDPT 0.379 2.346 0.275
SDPI 0.126 0.782 0.092
ionosphere
method µpolynomial µRBF µlinear
SDPT 2.037 1.487 -0.524
SDPI 0.475 0.525 0.000
heart
method µpolynomial µRBF µlinear
SDPT 0.000 2.366 0.634
SDPI 0.000 0.789 0.211
sonar
method µpolynomial µRBF µlinear
SDPT 0.023 2.977 0.000
SDPI 0.005 0.995 0.000
5 Conclusions
We have reformulated the SDP problem for multiple-kernel learning with an
induction setting. The kernel matrix involved is created from using the training
testing patterns only, thus the amount of time requirements is reduced. Two
58
Table 8
Performance comparisons of SDPI and SDPT on real datasets, combining different
RBF kernels.
datasets
method breast cancer ionosphere
SDPT
accuracy (%) 97.080 94.310
time(sec.) 4798.200 183.116
SDPI
accuracy (%) 96.643 93.744
time(sec.) 890.322 85.392
datasets
method heart sonar
SDPT
accuracy (%) 82.590 88.071
time(sec.) 70.284 34.462
SDPI
accuracy (%) 83.333 87.214
time(sec.) 40.545 18.488
culty in dealing with the cases with hundreds of thousands of patterns prac-
tically. We’ll keep working on how to further narrow down the feasible region
for kernel weights without significantly affecting the accuracy.
References
[1] C. Cortes, V. Vapnik, Support-vector network, Machine Learning 20 (3)
(1995) 273–297.
[2] M. Pontil, A. Verri, Support vector machines for 3D object recognition,
60
in: Proceedings of the 4th IEEE International Conference on Automatic
Face and Gesture Recognition, 2000, pp. 196–201.
[5] K. I. Kim, K. Jung, S. H. Park, H. J. Kim, Support vector machines
for texture classification, IEEE Transactions on Pattern Analysis and
Machine Intelligence 24 (11) (2002) 1542–1550.
[6] J. J. Ward, L. J. McGuffin, B. F. Buxton, D. T. Jones, Secondary struc-
ture prediction with support vector machines, Bioinformatics 19 (13)
(2003) 1650–1655.
[7] Y. Hou, W. Hsu, M. L. Lee, C. Bystroff, Efficient remote homology de-
tection using local structure, Bioinformatics 19 (17) (2003) 2294–2301.
[8] O. Chapelle, V. Vapnik, O. Bousquet, S. Mukherjee, Choosing multi-
ple parameters for support vector machines, Machine Learning 46 (1-3)
(2002) 131–159.
[9] K. Duan, S. Keerthi, A. N. Poo, Evaluation of simple performance mea-
sures for tuning SVM hyperparameters, Neurocomputing 51 (4) (2003)
41–59.
[10] F. Friedrichs, C. Igel, Evolutionary tuning of multiple svm parameters,
Neurocomputing 64 (Complete) (2005) 107–117.
[11] J. T.-Y. Kwok, The evidence framework applied to support vector ma-
chines, IEEE Transactions on Neural Networks 11 (5) (2000) 1162–1173.
[12] F. R. Bach, G. R. G. Lanckriet, M. I. Jordan, Multiple kernel learning,
conic duality, and the SMO algorithm, in: Proceedings of the 21st Inter-
national Conference on Machine Learning, 2004, pp. 6–13.
[13] K. P. Bennett, M. Momma, M. J. Embrechts, MARK: a boosting algo-
rithm for heterogeneous kernel models, in: Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2002, pp. 24–31.
62
[23] D. Michie, D. J. Spiegelhalter, C. C. Taylor, Machine Learning, Neural
and Statistical Classification, Ellis Horwood, New York, USA, 1994.
[24] N. Cristianini, J. Shawe-Taylor, An Introduction to Support Vector Ma-
chines and Other Kernel-based Learning Methods, Cambridge University
Press, Cambridge, UK, 2000.
[25] B. Scho¨lkopf, A. J. Smola, Learning with Kernels: Support Vector Ma-
chines, Regularization, Optimization, and Beyond, MIT Press, Cam-
bridge, MA, USA, 2001.
[26] J. Shawe-Taylor, N. Cristianini, Kernel Methods for Pattern Analysis,
Cambridge University Press, Cambridge, UK, 2004.
[27] K. M. Abadir, J. R. Magnus, Matrix Algebra, Cambridge University
Press, Cambridge, UK, 2005.
[28] Y. E. Nesterov, A. S. Nemirovsky, Interior Point Polynomial Methods in
Convex Programming: Theory and Applications, Society for Industrial
and Applied Mathematics (Siam Studies in Applied Mathematics, vol.
13), Philadelphia, USA, 1994.
[29] L. Vandenberghe, S. Boyd, Semidefinite programming, SIAM Review
38 (1) (1996) 49–95.
[30] M. J. Todd, K. C. Toh, R. H. Tu¨tu¨ncu¨, On the nesterov-todd direction in
semidefinite programming, SIAM Journal on Optimization 8 (3) (1998)
769–796.
[31] J. F. Sturm, Using SeDuMi 1.02, a MATLAB toolbox for optimization
over symmetric cones, Optimization Methods and Software 11 (1) (1999)
625–653.
[32] J. Lo¨fberg, YALMIP: A toolbox for modeling and optimization in MAT-
LAB, in: Proceedings of the 13th IEEE International Symposium on the
Computer Aided Control Systems Design, 2004, pp. 284–289.
64
