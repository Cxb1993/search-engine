（environment）的互動及衡量，由代理人做出最有益整體的選
擇，換句話說，即使面對相同的問題，代理人每次做的選擇不
一定會相同，但一定是當時環境下最有益的選擇；其優良的彈
性及適應能力，將大大增進動態電源管理的效能。同時，我們
也提出一套將多核心處理器的功率狀態減化並優化功耗的技
術，讓動態電源管理時間及空間的複雜度降低。 
在本計畫中，我們提出了一套動態電源管理的工具，其貢獻如
下： 
1. 在多核心處理器上，整合了動態電源管理與動態電壓與頻率
調節。 
2. 運用增強式學習法的彈性及適應能力來做動態電源管理。 
3. 將多核心處理器的功率狀態減化並優化功耗，讓動態電源管
理的複雜度降低。 
英文摘要： Power consumption has become a critical issue in modern VLSI 
design. Dynamic voltage and frequency scaling (DVFS) and 
dynamic power management (DPM) are two attractive solutions at 
system level. Unlike prior works that adopt offline techniques or are 
restricted to a single core processor, in this work, we combine 
DVFS with DPM together and propose an online DPM 
methodology to multicore processors. Our method is based on 
reinforcement learning and Markov decision process. With state 
reduction, we avoid state explosion and simplify the learning 
mechanism. Experimental results show that the number of states is 
greatly reduced, but the effectiveness of our DPM is still 
maintained. We apply our DPM on a quad core processor； our 
DPM method can achieve 20% power reduction on average without 
delay penalty. 
 
2 
 
一、中文摘要 
近年來，耗電量已成為必需正視的課題。產品的趨勢方面，時脈頻率不斷攀升、功能需求
不停添加以及體積更加輕巧；製程的演進趨勢方面，供應電壓持續下降，直接而有效的降低動
態耗電，但是從這部分所能得到的好處卻趨於緩和，甚至漸漸出現了負面效應（漏電流的效應
成指數型式成長）；兩相加乘的結果，使得耗電密度（power density，power-per-area）與日俱
增。根據 2006 年更新的 ITRS 所預估：耗電的問題從 2007 年開始變成不可不解的困擾，到了
2017 年會陷入更深的困境。低耗電管理的技術的發展，時至今日已有一段時間，就系統層級
（system-level）而言，動態電壓與頻率調節（dynamic voltage and frequency scaling，DVFS）
與動態電源管理（dynamic power management，DPM）為兩種最為廣泛應用的技術。 
動態電壓與頻率調節技術為多重供應電壓的延伸，不僅是空間軸上（block domain）甚至
是時間軸上（time domain）都可以應用，每個區域因工作模式的不同可以有不同的時序要求。
所以在 critical 的時候／區域使用高電壓、高頻時脈；在 non-critical 的時候／區域使用低電壓、
低頻時脈；而動態電源管理技術會根據時間軸上目前的工作多寡來調整晶片的工作能力。當工
作多時，晶片會處在高計算能力╱高功耗的狀態，當工作量少時，晶片會處在低計算能力╱低
功耗的狀態，而當無工作時，晶片將會進入閒置狀態，以此種動態調整來降低處理完一定工作
量之功耗。 
而近來，因對系統的效能要求，多核心處理器（multicore proccessor）也日漸受到重視，
藉由將數個相同的核心（core）整合，能在不大幅增加功耗的情況下，達到一定水準的效能。
在多核心處理器的架構下，可以將動態電壓與頻率調節與動態電源管理來做結合：每個核心擁
有自己的電壓島（voltage island）及頻率島（frequency island），因此在同一時間點下，每個核
心能處在不同的電壓╱頻率；由每個核心的電壓╱頻率，可以得到目前處理器的功率狀態
（power state）。動態電源管理將視當下的工作量來決定處理器的功率狀態，而動態電壓與頻率
調節會依目前處理器的功率狀態，來配置每個核心的工作電壓及頻率。 
動態電源管理的研究雖已行之有年，但是大部份的解決方法都只對特定的工作行為有效
用，但實際上，工作的行為特徵並無法在事前預知，因此，動態電源管理技術的彈性及其對不
4 
 
DVFS is a process to dynamically modify the supply voltage and the operating frequency. We 
boost the timing critical part of design with a higher voltage-frequency (V-F) setting to meet the 
performance requirement while slow down the non-critical part with a lower V-F setting to reduce 
the power consumption (referred to dynamic power in this work). Since power consumption is 
directly proportional to the operating frequency and quadratically proportional to the supply voltage, 
DVFS achieves a cubic reduction in power consumption. 
On the other hand, DPM is a procedure to dynamically adjust the computational ability of a 
device according to the workload applied on it. DPM slows down or turns off the device when no 
work is (predicted to be) coming. In other words, the computational ability of a device is reduced as 
it is idle. As a result, if DPM manages well, the power consumption of the idle period can be saved 
while the performance is maintained.  
In order to provide high performance with acceptable power consumption, a multicore processor 
(a.k.a. chip multiprocessor, CMP) is developed, which integrates numbers of monotonic cores into 
one chip. Figure 1(a) depicts a multicore processor with four identical cores. With DVFS, each core 
on this multiprocessor can be assigned with a specific V-F setting according to its workload. Four 
different V-F settings {V-F-1, V-F-2, V-F-3, V-F-4} are applied on each core in Figure 1(b), while 
three V-F settings are applied in Figure 1(c). (V-F-2 is applied on two cores, while V-F-1 and V-F-3 is 
applied on one core, respectively.) With different V-F settings, the processor shows different statuses 
in computational ability and power consumption. Furthermore, if we consider each circumstance as a 
 
V-F-4
V-F-1
V-F-3
V-F-2
 
V-F-2
V-F-1
V-F-3
V-F-2
 
(a) (b) (c) 
Figure 1.  (a) Multicore processor. (b) 4 cores with 4 different DVFS settings {V-F-1, V-F-2, V-F-3, V-F-4}. (c) 4 
cores with 3 different DVFS settings. 
 
 
6 
 
Unlike previous works, we propose an online DPM method to dynamically alter V-F settings of 
a multicore processor by adopting reinforcement learning in this work. Our method is based on 
Markov decision process while the number of states is greatly reduced. Our DPM does not include 
service requestor, service queue and service provider in the state model and thus simplifies the 
learning mechanism. In addition, our work guarantees the minimum transitions between states. This 
feature allows us to reduce the number of states as well as power consumption. Figure 2 illustrates an 
example of state reduction on a duo core processer, in which each has 3 V-F settings: {high (H), low 
(L), sleep (0)}. The states in Figure 2(a) are constructed by the permutation of V-F settings. Thus, 
there are 9 (=3*3) states: {{0, 0}, {0, L}, {0, H}, {L, 0}, {H, 0}, {L, L}, {L, H}, {H, L}, {H, H}}. 
As shown in Figure 2(b), with the proposed state reduction, each state is encoded by the number of 
cores under each V-F setting. Therefore, as shown in Figure 2(b), we have only 6 states: {{0, 0}, {0, 
L,}, {0, H}, {L, L}, {L, H}, {H, H}}. With the proposed state reduction, the number of states can be 
greatly reduced from 9 (see Figure 2(a)) to 6 (see Figure 2(b)), and the transitions between states are 
also minimized. Please note that, as the number of cores and available V-F settings increases, the 
state model like Figure 2(a) will face the state explosion issue. Moreover, our state reduction method 
can largely simplify our learning mechanism. Hence, our method is more suitable for multicore 
6
7
8
0
1
4
3
2
5
……
……… …
…
…
state 6: {L,H}; state 7: {H,L}  
0
1
4
3
2
5
……
………
state 4: {L,H}  
(a) (b) 
Figure 2. State reduction. Consider a duo core processer; each core has 3 possible V-F settings: {high (H), low (L), 
sleep (0)}. (a) Without state reduction. Totally 9 states. (b) With state reduction. Totally 6 states. It can be seen that 
two or more states can be merged into one, e.g., {L, H} and {H, L} has the same computational ability and power 
consumption. 
 
 
 
8 
 
B. Markov Decision Process 
A Markov decision process is a 5-tuple element (S, A, {Psas’}, , R), where S is the set of states, A is 
the set of actions, {Psas’} are the state transition probabilities,  is the discount factor, and R is the 
reward. psas' denotes state sS goes to state s'S by taking action aA. Moreover, the summation of 
the state transition probability of a given original state through an action will be one. For example, in 
Figure 4, we have S = {s0, s1, s2, s3} and A = {a0}. The state transition probability 
100 sas
p  gives the 
possibility of s0 transfers to s1 through a0, while 200 sasp , 300 sasp  represents that to s2 and s3, 
respectively, and 132 0000100  sassassas ppp . 
The discount factor, [0, 1), is used to discount the future reward R: SA  , where R(s, a) is 
the reward we can get after taking action a from state s. After n periods (time slots between two 
consecutive learning), the expected reward can be write down as: 
)].,(...),(),([ 1100 nn
ttntttt asRasRasR   
A policy  maps from states to actions (e.g.,  : SA). If action a = (s) is performed in state s, we 
may say that a policy  is executed. The value function of a policy V(s) is the expected sum of 
discount rewards of taking policy  in an initial state s:  
 ],|)(...)()([)( 0
10  sssRsRsRsV ntntt  
s0 s2a0
s1
s3
 
Figure 4.  States, action, and transitions of Markov decision process. The summation of the state transition probability of a 
given original state through an action will be one. 
10 
 
more suitable for multicore processors. Experimental results in Section IV.B show that the proposed 
DPM performs better than the DPMs without state reduction. 
A. State Reduction 
Unlike previous works, we do not model the service requestor, the service queue and the service 
provider into a state. On the contrary, we treat V-F settings of cores as our state to avoid state 
explosion. Each state indicates the number of cores under each V-F setting. Different settings which 
result in the same computational ability and power consumption are viewed as one state. To validate 
our state reduction, the state transition power is desired to be minimized. In consequence, with state 
reduction, our work adopts minimum state transition power between states.  
Assume that we conduct our DPM on a quad core processor, and each has three possible V-F 
settings: high (H), low (L) and sleep (0). Instead of 81 (=3
4
) states, TABLE I lists the 15 states used 
for this quad core processor, as well as the V-F settings, computational abilities and power 
consumption. In our work, the state transition power between two states is obtained by the following 
equations: 
,)',( HtxLtxLHtxtx tptptpssp HLLH 00 00  
where  
),,min( HLLH ddt  
),,min( LHLL tddt  00 
).,min( LHHLH tdtdt  000 
The notation 
LHtx
p  means the power consumption for a core transferring from L to H, or from H 
to L, while 
Ltx
p
0
stands for the power consumption from 0 to L or L to 0, 
Htx
p
0
represents that from 0 
to H (H to 0). The number of cores transfer from L to H is denoted as tLH and the numbers of cores 
transferring from 0 to L and 0 to H are t0L and t0H. Notation d0, dL, dH represents the number difference 
12 
 
manage the transitions between states: leave and stay. The action leave transports the original state to 
all the other states, while the action stay remains the next state as the original one. Figure 5 illustrates 
an example of our model, where the state s transfers to the state s' through the action a0 (leave), while 
s remains still when the action a1 (stay) is taken. The state transition probability 'sasp  gives the 
possibility of s transfers to s' through a. 
Our cost function from the state s to the state s' is defined as follows: 
 ),',()'()',( ssPenaltysVGssC a  
where G is a factor used to decide how important the historical information of the next state s' is with 
respect to the penalty can be received right afterwards (same function as discount factor mentioned in 
Section II.B). The penalty function, Penalty(s, s') is defined as 
 ),'()',()',( sDelayMssPowerssPenalty   
 ),'()',()',( spowersspssPower tx  
 ).'()'( scompQQsDelay inr  
Power(s, s') contains state transition power ptx(s, s') and power consumption power(s') of the next 
period (the time slot between two consecutive learning) when next state is s'. Delay(s') represents how 
many tasks will remain in queue if the processor is in state s'
 
with computational ability comp(s') 
when queue already has Qr tasks remained and the number of incoming tasks Qin of next period is the 
DPM
Learning
Adjusting
ASTP
AG
AM
 
Figure 6.  Overflow of the proposed DPM. 
 
 
14 
 
Adjusting: three parameters are adjusted online: the state transition probability (ASTP), the G value 
used in equation (6) (AG), and the M in equation (7) (AM). ASTP adjusting the transition 
probabilities every T1 periods by 
 ,
_
'_
'
acount
scount
psas  
where count_a represents how many times action a is taken in s during T1, while count_s' describes 
how many times s' is visited when action a taken in s during T1. AG increases G gradually during 
learning process. This is because that DPM knows very poor about the interactions with the 
environment; the penalty function takes the most responsibility for learning. As increasing of 
learning periods, DPM compiles more and more knowledge of the environment, thus we amplify G 
and let the experience, i.e., V
a
(s'), to guide DPM to choose the next state. In addition, AM balances 
the importance between power and delay. AM reduces M by m1 if no tasks remains in queue, while 
increase it by m2 vice versa. AM guides DPM chooses a state with lower power consumption as task 
TABLE I. THE V-F SETTINGS, COMPUTATIONAL ABILITY AND POWER CONSUMPTION OF EACH STATE 
State V-F settings comp( ) power( ) 
0 {0, 0, 0, 0} 0.00 4.00 
1 {0, 0, 0, L} 0.60 5.20 
2 {0, 0, 0, H} 1.00 8.55 
3 {0, 0, L, L} 1.20 6.40 
4 {0, 0, L, H} 1.60 9.75 
5 {0, 0, H, H} 2.00 13.10 
6 {0, L, L, L} 1.80 7.60 
7 {0, L, L, H} 2.20 10.95 
8 {0, L, H, H} 2.60 14.30 
9 {0, H, H, H} 3.00 17.65 
10 {L, L, L, L} 2.40 8.80 
11 {L, L, L, H} 2.80 12.15 
12 {L, L, H, H} 3.20 15.50 
13 {L, H, H, H} 3.60 18.85 
14 {H, H, H, H} 4.00 22.20 
 
16 
 
model, while 9 in Per and 18 in Que. We categorize the queue into three statuses: low occupancy, 
medium occupancy, and high occupancy in Que. Besides the state models used, Per and Que have 
basically the same actions and the same cost functions as ours. 
TABLE III shows the comparisons between ours, Per and Que in power and delay under initial M 
= 9.5. The results are averaged over 8 testcases, as well as normalized by power and delay of Ours, 
respectively. Since the multicore processor in this experiment has only two cores, we apply half tasks 
in each cycle for each testcase. It can be seen that our DPM outperforms Per in both power and delay. 
As expected, Per wastes power on unnecessary transitions. Que has lower power consumption than 
ours, but degrades performance dramatically.  It is because that staying at a wrong state will affect 
the evaluation of DPM on environment thus making non-optimal decisions. In conclusion, our state 
reduction reduces complexity of learning while maintains good performance. 
C. Performance Evaluation of Proposed DPM Methedology 
In the second experiment, we apply our DPM on a quad core processor, e.g., QX9000 series of 
Intel

. We assume that each core has 3 V-F settings including turn-off, i.e., 3.00GHz-1.36V (high 
performance, H), 1.66GHz-1.13V (low performance, L), and 0GHZ-0.85V (sleep, 0). TABLE I lists 
the processor states constructed by these four cores and three V-F settings, as well as the 
computational ability and power consumption of each state. The computational ability comp() is 
TABLE IV. POWER AND DELAY UNDER DIFFERENT INITIAL M OF OUR DPM 
Initial  Testcase P-1 P-2 P-3 P-4 P-5 P-6 P-7 P-8 
M =7.5 
Power 0.82  0.69  0.76  0.71  0.73  0.95  0.98  0.93  
Delay 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  
M =9.5 
Power 0.63  0.71  0.68  0.65  0.86  0.96  0.94  0.85  
Delay 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  
M=11.5 
Power 0.64  0.70  0.74  0.96  0.64  0.96  0.94  0.84  
Delay 0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  
Remarks: 
1. Power ratio and delay overhead are normalized to the results obtained by running at state 14. 
2. 1 period = 10 cycles. T1 = 184, T2 = 220, T3 = 50 periods. 
3. Initially G = 0.1 and increased by 0.1 every 220 periods until G = 1.0. 
4. m1 = 0.5; m2 = 0.5. M ranges from 0.5 to 50. 
18 
 
 
 
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
P-1 P-2 P-3 P-4 P-5 P-6 P-7 P-8
0={0,0,0,0}
1={0,0,0,L}
2={0,0,0,H}
3={0,0,L,L}
4={0,0,L,H}
5={0,0,H,H}
6={0,L,L,L}
7={0,L,L,H}
8={0,L,H,H}
9={0,H,H,H}
10={L,L,L,L}
11={L,L,L,H}
12={L,L,H,H}
13={L,H,H,H}
14={H,H,H,H}
 
(a) 
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
P-1 P-2 P-3 P-4 P-5 P-6 P-7 P-8
0={0,0,0,0}
1={0,0,0,L}
2={0,0,0,H}
3={0,0,L,L}
4={0,0,L,H}
5={0,0,H,H}
6={0,L,L,L}
7={0,L,L,H}
8={0,L,H,H}
9={0,H,H,H}
10={L,L,L,L}
11={L,L,L,H}
12={L,L,H,H}
13={L,H,H,H}
14={H,H,H,H}
 
(b) 
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
P-1 P-2 P-3 P-4 P-5 P-6 P-7 P-8
0={0,0,0,0}
1={0,0,0,L}
2={0,0,0,H}
3={0,0,L,L}
4={0,0,L,H}
5={0,0,H,H}
6={0,L,L,L}
7={0,L,L,H}
8={0,L,H,H}
9={0,H,H,H}
10={L,L,L,L}
11={L,L,L,H}
12={L,L,H,H}
13={L,H,H,H}
14={H,H,H,H}
 
(c) 
Figure 7. The state distribution of our DPM under (a) M = 7.5, (b) M = 9.5 and (c) M = 11.5. The x-axis shows each 
testcase, the y-axis shows the number that each state is applied on multicore processor, and the bars with different 
colors represent the states. 
 
20 
 
[9] H. Jung and M. Pedram, “Supervised learning based power management for multicore 
processors,” IEEE Trans. on CAD, vol. 29, no. 3, pp. 1395-1408, 2010. 
[10] Y. Wang, Q. Xie, A. Ammari and M. Pedram, “Deriving a near-optimal power management 
policy using model-free reinforcement learning and Bayesian classification,” in Proc. of DAC, pp. 
41-46, 2011. 
[11] T. Kolpe, A. Zhai and S. S. Sapatnekar, “Enabling improved power management in multicore 
processors through clustered DVFS,” in Proc. of DATE, pp. 1-6, 2011. 
[12] S. Marsland, Machine Learning: An Algorithmic Perspective. CRC Press, 2009. 
[13] R. Sutton and A. Barto, Reinforcement Learning: An introduction. MIT Press, 1998. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：江蕙如 計畫編號：99-2220-E-009-009- 
計畫名稱：後次微米時代新興電子設計自動化技術之研究--子計畫二：整合性低耗電管理之技術開發
(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 3 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
