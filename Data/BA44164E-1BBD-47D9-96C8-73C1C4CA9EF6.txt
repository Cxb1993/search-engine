TABLE I
RESEARCH WORKS ON THE ANALYSIS OF A FAULT TOLERANT NN.
Ref. Fault NN Work
[48] Any weight noise Madaline Probility of output error
[15] Any noise Any Output sensitivity measure
[43] Any noise Madaline Precision requirement
[10] Mul. weight noise RBF Generalizaton ability
[54] Any noise RBF Output sensitivity matrix
[2] Any weight noise MLP Output sensitivity measure
[41] - - Relationship between FT,
generalization and VC dim.
[4] Any weight noise MLP Generalization ability
[19] Any weight noise FN푎 Error sensitivity measure
푎 Functional net
followed an approach from signal to noise ratio (SNR) and
came up with a set of measures for the output sensitivity
of a network with respect to different noises. Using such
SNR, a weight accuracy selection algorithm is developed and
applied to determine the precision requirement in hardware
implementation. Townsend and Tarassenko [54] considered a
radial basis function (RBF) network with multiple outputs and
derived the output sensitivity in matrix form for an RBF that is
suffered from perturbations in input data, radial basis function
centers and output weights.
As output sensitivity is just an indirect view point to
understand the effect of NN due to noise, the actual effect
of noise to the performance of a NN cannot be identified
easily. A more practical view point to the problem is from its
actual performance — the generalization ability. Catala and
Parra proposed a fault tolerance parameter model and studied
the performance degradation of a RBF network if the RBF
centers, widths and the corresponding weights are corrupted
by multiplicative noise [10]. Bernier et al extended from Choi
& Choi statistical sensitivity approach [15] and derived the
error sensitivity measure for MLP [2], [4], RBF network [6]
Similarly, Fontenla-Romero et al derived the error sensitivity
measure for functional net [19].
Noise can be harmful to a NN. But sometimes, it can be
beneficial. Murray & Edwards [36] investigated and found
that adding multiplicative weight noise (and other kinds of
noise) during training can improve the generalization ability
of a MLP. While noise during training can improve the
generalization ability of a NN, Bishop [7] showed that adding
small additive white noise to a NN during training is equiv-
alent to Tikhnov regularization. Jim et al [24] noticed that
adding multiplicative weight noise not just can improve the
generalization ability, but also can improve the convergence
ability in training a recurrent NN.
B. Algorithms for dealing with multiplicative weight noise
While lot of works have been done to understand the effect
of noise to the network performance, various training methods
aiming to improve the fault tolerant ability of a NN have been
developed. Since the effect of a multiplicative weight noise
is proportional to the magnitude of the associated weight,
one intuitive approach is to control the magnitude of the
weights to small values. Cavalieri & Mirabella in [11] have
proposed a modified backpropagation learning algorithm for
multilayer perceptron. In their algorithm, a weight magnitude
control step has been added in each training epoch. Whenever
the magnitude of a weight has reached a predefined upper
limit, it will not be updated unless the update can bring its
magnitude down. Consider that the noise effect can eventually
be cancelled out at the output node if all the weight values
are equal, Simon in [47] suggested a distributed fault tolerance
learning approach for optimal interpolation net and formulated
the learning as a nonlinear programming problem, in which
training error is minimized subjected to an equality constraint
on weight magnitude. Extended from the work done in [10],
Parra and Catala in [38] demonstrated how a fault tolerant RBF
network can be obtained by using a weight decay regularizer
[33]. From model sensitivity point of view, Bernier et al
developed a method called explicit regularization to attain
a MLP [3], [5] or RBF network [6] that is able to tolerate
multiplicative weight noise.
C. Algorithms for dealing with node fault
To deal with node fault, those learning algorithms developed
can be classified into two approaches : (1) adding heuristics
(random fault or network redundancy) during training and (2)
formulating the training as a nonlinear optimization problem.
Adding heuristic in the training algorithm is essentially to
enforce the internal representation ability of a NN distributed
widely within the hidden nodes or weights. So that, no single
node or single weight is particularly important and then
random removal of a node or a weight will only gracefully
degrade the performance of the network. For this approach,
injecting random node fault alone [45], [9] or together with
random node deletion and addition [14] during training are
two techniques that have demonstrated succeed in attaining
fault tolerance. Adding network redundancy by replicating
multiple hidden layers after a NN has been well trained [18],
[40] is another one. Under the same scenario, limit weight
magnitude either by adding weight decay regularizer [14] or
hard bounding the weight magnitude to a small value during
training [11] are another two techniques that can succeed in
obtaining a fault tolerant NN.
Another approach is to formulate the learning directly as
a constraint optimization problem. Neti et al [37] defined
the problem as a minimax problem, in which the objective
function to be minimized is the maximum of the mean square
errors over all fault models. Deodhare et al took a similar idea
in [16] by defining the objective function to be minimized
as the maximum square error, over all fault models and all
training samples. As the computational cost in solving these
minmax problem could be severe for large number of hidden
units, Simon & El-Sherief in [46] and Phatak & Tcherner in
[42] formulated the learning problem to a simpler unconstraint
optimization problem, in which the objective function consists
of two terms namely the mean square errors of the fault-
free model and the ensemble average of the mean square
errors over all fault models. Although solving unconstraint
optimization problem is a lot more easy compared with a min-
imax problem, these formulations are still suffered from sever
computational burden when their formulations are extended
to handling multiple nodes fault. In view of the lacking of
a theoretical framework and the difficulty in extending the
existing approaches to multiple nodes fault, Leung et al in
[30] and Sum in [51] have attempted to these problems by
devising an objective function for fault tolerant learning.
output noise 푒푘. To model the unknown system, we assume
that 푓(푥) can be realized by an RBF network, i.e.
ℳ : 푦푘 =
푀∑
푖=1
휃푖휙푖(푥푘) + 푒푘 (6)
푒푘 ∼ 풩 (0, 푆푒), (7)
for all 푘 = 1, 2, ⋅ ⋅ ⋅ , 푁 . 푆푒 is known in advance, a model ℳ
in Ω can indeed be represented by an 푀 -vector,
휃 = (휃1, 휃2, ⋅ ⋅ ⋅ , 휃푀 )푇 .
The model set Ω is isomorphic to an 푀 -dimension Euclidean
space, 푅푀 .
The best estimated model ℳˆ is thus represented 휃ˆ. Equa-
tion (1) is rewritten as follows :
휃ˆ = arg min
휃∈푅푀
{퐽(휃∣풟)} . (8)
Here 퐽(휃∣풟) can be defined in one of the following forms.
1) Sum Square Errors (SSE) :
퐽(휃∣풟) = 1
푁
푁∑
푘=1
(푦푘 − 푓(푥푘, 휃))2. (9)
2) SSE with Regularizer (Weight Decay) :
퐽(휃∣풟) = 1
푁
푁∑
푘=1
(푦푘 − 푓(푥푘, 휃))2 + 휆휃푇 휃, (휆 > 0).
(10)
3) Likelihood Probability :
퐽(휃∣풟) = −푃 (풟∣휃). (11)
4) Log Likelihood :
퐽(휃∣풟) = − log푃 (풟∣휃). (12)
5) A Posterior Probability :
퐽(휃∣풟) = −푃 (풟∣휃)푃 (휃)
푃 (풟) . (13)
6) Log A Posterior Probability :
퐽(휃∣풟) = − log푃 (풟∣휃)− log푃 (휃). (14)
The probability 푃 (휃) which appears in Equation (13) and
Equation (14) is the A Prior distribution of 휃.
The best implemented model ℳˆ퐼 is thus represented 휃ˆ퐼 .
Equation (2) can be rewritten as follows :
ℒ(휃∣풟) =
∫
휃˜∈Ω˜휃
퐽(휃˜∣풟)푃 (휃˜∣휃)푑휃˜ (15)
휃ˆ퐼 = arg min
휃∈푅푀
{ℒ(휃∣풟)} . (16)
The integration is taken over the 푅푀 space. The probability
푃 (휃˜∣휃) is depended on the fault model concerned. Note that
this probability is not the same as the A Prior probability 푃 (휃).
If there are only finite number of possible faulty models, the
objective function defined in Equation (15) would be given by
ℒ(휃∣풟) =
∑
휃˜∈Ω˜휃
퐽(휃˜∣풟)푃 (휃˜∣휃)푑휃˜. (17)
The set of faulty models is depended on the estimated model
휃.
One should note that the best estimated model (i.e. the fault-
free model) obtained either by Equation (11) or Equation (12)
are the same because
arg min
휃∈푅푀
{−푃 (풟∣휃)} = arg min
휃∈푅푀
{− log푃 (풟∣휃)} .
However, for fault tolerant cases, there will have no such
guarantee that
arg min
휃∈푅푀
{
−
∫
푃 (풟∣휃˜)푃 (휃˜∣휃)푑휃˜
}
is the same as
arg min
휃∈푅푀
{∫ (
− log푃 (풟∣휃˜)
)
푃 (휃˜∣휃)푑휃˜
}
.
The same reason applies to Equation (13) and Equation (14).
Apart from defining an RBF network as in Equation (7),
one can also define the estimated model in other forms. For
instance,
푦푘 = 휃0 +
푀∑
푖=1
휃푖휙푖(푥푘) + 푒푘, (18)
푒푘 ∼ 풩 (0, 푆푒), (19)
for 푘 = 1, 2, ⋅ ⋅ ⋅ , 푁 . For a given 푆푒, the estimated model set
will be isomorphic to the 푅푀+1 space.
If we assume that the values of 푐푖s and 휎 in the 푀
basis functions are not predefined, an RBF model will be
parameterized by an (2푀 + 2)-vector,
(휃0, 휃1, ⋅ ⋅ ⋅ , 휃푀 , 푐1, 푐2, ⋅ ⋅ ⋅ , 푐푀 , 휎)
The estimated model set Ω will thus be isomorphic to the
푅2푀+2 space.
V. IMPLEMENTED MODELS Ω˜ℳ
Recall that an implemented model of ℳ is a model, in
which part of its structure is faulty. In this section, three typical
fault models will be introduced including (1) the multiplicative
weight noise (2) single-node fault and (3) multiple-nodes fault.
Similarly, we use RBF network as an example for illustration.
A. Multiplicative weight noise with 퐽(휃∣풟) = SSE
Mulplicative weight noise exists whenever a weight is
encoded in a low precision binary form. In order not to divert
the focus of this section, the exaplination of this effect is
presented in Appendix A.
Using the model described in Equation (55), an implemen-
tation of a model 휃 (denoted by 휃˜) can be defined as follows :
휃˜푖 = 휃푖 + 훽푖 휃푖, (20)
훽푖 ∼ 풩 (0, 푆훽), (21)
for all 푖 = 1, 2, ⋅ ⋅ ⋅ ,푀 . In other word,
푃 (훽푖) =
1√
2휋푆훽
exp
(
− 훽
2
푖
2푆훽
)
∀ 푖 = 1, ⋅ ⋅ ⋅ ,푀. (22)
Let 휃 = (휃1, 휃2, ⋅ ⋅ ⋅ , 휃푀 )푇 and 훽 = (훽1, 훽2, ⋅ ⋅ ⋅ , 훽푀 )푇 ,
휃˜ = 휃 +퐴(휃)훽,
퐴(휃) = diag {휃1, 휃2, ⋅ ⋅ ⋅ , 휃푀} .
Faulty Node
  
  
  



  
  
  



  
  


 
 

Normal Node
Fig. 3. Single-node fault NN models. For a network of 푀 hidden nodes,
there are 푀 possible single-node fault models.
By using the idea of gradient descent, a training algorithm
can thus be derived. Taking the gradients of the 2nd and the
3rd terms in Equation (37), it is readily obtained
∂
∂휃
log푆(푥푘, 휃) =
2푆훽
푆(푥푘, 휃)
G(푥푘)휃, (39)
∂
∂휃
(푦푘 − 휙푇 (푥푘)휃)2
푆(푥푘, 휃)
= −2푆훽(푦푘 − 휙
푇 (푥푘)휃)
2
푆2(푥푘, 휃)
G(푥푘)휃
−2(푦푘 − 휙
푇 (푥푘)휃)
푆(푥푘, 휃)
휙(푥푘), (40)
where G(푥푘) is a diagonal matrix defined as in Equation (26).
A fault tolerant RBF network can thus be obtained by the
following gradient descent algorithm :
휃(푡+ 1) = 휃(푡)− 휇 ∂
∂휃
ℒ(휃(푡)∣풟), (41)
where 휇 is a small positive value corresponding to the step
size and
∂ℒ(휃∣풟)
∂휃
=
푆훽
푁
푁∑
푘=1
(
1
푆(푥푘, 휃)
− (푦푘 − 휙
푇 (푥푘)휃)
2
푆2(푥푘, 휃)
)
G(푥푘)휃
− 1
푁
푁∑
푘=1
(푦푘 − 휙푇 (푥푘)휃)
푆(푥푘, 휃)
휙(푥푘). (42)
The initial condition 휃(0) is set to be a small random vector
close to null.
C. Single node fault with 퐽(휃∣풟) = SSE
Once a node has been faulty, we assume that its output will
be stuck at zero. Therefore, an RBF network with its 푖푡ℎ node
being faulty will be denoted by an 푀 -vector 휃−푖, which is
identical to 휃 except that the 푖푡ℎ element is zero.
휃−푖 = (휃1, 휃2, ⋅ ⋅ ⋅ , 휃푖−1, 0, 휃푖+1, ⋅ ⋅ ⋅ , 휃푀 )푇
Assume that there is at most one node will be removed
randomly. The probability that a network will be faulty is 푞.
Once a network is faulty, there is uniformly random for any
one of the node is fault, Figure 3. Under such circumstance,
Ω = 푅푀 , (43)
Ω˜휃 = {휃, 휃−1, 휃−2, ⋅ ⋅ ⋅ , 휃−푀}. (44)
A node will be fault is about 푞/푀 probability.
푃 (휃˜∣휃) =
⎧⎨⎩
1− 푞 if 휃˜ = 휃
푞/푀 if 휃˜ = 휃−1
...
...
푞/푀 if 휃˜ = 휃−푀 .
(45)
For 퐽(휃∣풟) is defined as the sum square errors,
ℒ(휃∣풟) = (1− 푞)퐽(휃∣풟) + 푞
푀
푀∑
푖=1
퐽(휃−푖∣풟). (46)
In which,
퐽(휃−푖∣풟) = 퐽(휃∣풟) + 휃2푖 푔푖
+ 2휃푖
1
푁
푁∑
푘=1
(푦푘 − 휙푇 (푥푘)휃)휙푖(푥푘) (47)
where 푔푖 is the 푖푡ℎ diagonal element of 푄푔 . Hence, the
objective function for attaining a RBF network to tolerate
single node fault can be written as follows :
ℒ(휃∣풟) = 퐽(휃∣풟) + 2푞
푀
1
푁
푁∑
푘=1
푦푘휙
푇 (푥푘)휃
+
푞
푀
휃푇 [푄푔 − 2퐻휙]휃. (48)
Taking the derivative of ℒ(휃∣풟) and setting it to zero, 휃ˆ퐼 can
be obtained as follows :
휃ˆ퐼 =
(
퐻휙 +
푞/푀
1− 푞/푀 푄푔
)−1
1
푁
푁∑
푘=1
푦푘휙(푥푘). (49)
The matrix 푞/푀1−푞/푀푄푔 which appears in the last equation plays
a role similar to a regularizer.
D. Multiple nodes fault with 퐽(휃∣풟) = SSE
We assume that a node fault is equivalent to permanently set
the output of the node zero. Therefore, a faulty RBF 푓ˆ(푥, 휃˜),
where 휃˜ = (휃˜1, 휃˜2, ⋅ ⋅ ⋅ , 휃˜푀 )푇 and
휃˜푖 = 훽푖휃푖, (50)
could be defined by multiplying each 휙푖(푥) by a random
binary variable 훽푖 :
푓(푥, 휃, 훽) =
푀∑
푖=1
훽푖휃푖휙푖(푥). (51)
When 훽푖 = 1, the 푖푡ℎ node is normal. When 훽푖 = 0, the 푖푡ℎ
node is fault. We assume that all nodes are of equal fault rate
푝, i.e.
푃 (훽푖) =
{
푝 if 훽푖 = 0
1− 푝 if 훽푖 = 1. (52)
for 푖 = 1, 2, ⋅ ⋅ ⋅ ,푀 and 훽1, ⋅ ⋅ ⋅ , 훽푀 are independent random
variables.
Information Processing: Proceedings of the 15th Inter-
national Conference on Neuro-Information Processing,
Springer LNCS 5507, pp. 324-331, 2009 (EI)
REFERENCES
[1] Shun’ichi Amari, Differential-geometrical methods in statistics, Lecture
notes in statistics, Springer-Verlag, Berlin, 1985.
[2] Bernier J.L. et al, An accurate measure for multiplayer perceptron
tolerance to weight deviations, Neural Processing Letters, Vol.10(2),
121-130, 1999.
[3] Bernier J.L. et al, Obtaining fault tolerance multilayer perceptrons using
an explicit regularization, Neural Processing Letters, Vol.12, 107-113,
2000.
[4] Bernier J.L. et al, A quantitative study of fault tolerance, noise immunity
and generalization ability of MLPs, Neural Computation, Vol.12, 2941-
2964, 2000.
[5] Bernier J.L. et al Improving the tolerance of multilayer perceptrons by
minimizing the statistical sensitivity to weight deviations, Neurocomput-
ing, Vol.31, 87-103, 2000.
[6] Bernier J.L. et al, Assessing the noise immunity and generalization of
radial basis function networks, Neural Processing Letter, Vol.18(1), 35-
48, 2003.
[7] Bishop C.M., Training with noise is equivalent to Tikhnov regularization,
Neural Computation, Vol.7, 108-116, 1995.
[8] Bishop C.M., Neural Networks for Pattern Recognition, Oxford Univer-
sity Press, 1995.
[9] Bolt G., Fault tolerant in multi-layer Perceptrons. PhD Thesis, Univer-
sity of York, UK, 1992.
[10] Catala M. A. and X.L. Parra, Fault tolerance parameter model of radial
basis function networks, IEEE ICNN’96, Vol.2, 1384-1389, 1996.
[11] Cavalieri S. and O. Mirabella, A novel learning algorithm which
improves the partial fault tolerance of multilayer NNs, Neural Networks,
Vol.12, 91-106, 1999.
[12] Chandra P. and Y. Singh, Fault tolerance of feedforward artificial neural
networks – A framework of study, Proceedings of IJCNN’03 Vol.1 489-
494, 2003.
[13] Chandra P. and Y. Singh, Feedforward sigmoidal networks - equicon-
tinuity and fault-tolerance properties, IEEE Transactions on Neural
Networks, Vol.15(6), 1350-1366, 2004.
[14] Chiu C.T. et al., Modifying training algorithms for improved fault
tolerance, ICNN’94 Vol.I, 333-338, 1994.
[15] Choi J.Y. and C.H. Choi, Sensitivity of multilayer perceptrons with dif-
ferentiable activation functions, IEEE Transactions on Neural Networks,
Vol. 3, 101-107, 1992.
[16] Deodhare D., M. Vidyasagar and S. Sathiya Keerthi, Sythesis of fault-
tolerant feedforward neural networks using minimax optimization, IEEE
Transactions on Neural Networks, Vol.9(5), 891-900, 1998.
[17] Eickhoff R. and U. Riickett, Tolerance of radial basis functions against
stuck-at-faults, Proc. ICANN 2005, LNCS 3697 Springer, 1003-1008,
2005.
[18] Emmerson M.D. and R.I. Damper, Determining and improving the fault
tolerance of multilayer perceptrons in a pattern-recognition application,
IEEE Transactions on Neural Networks, Vol.4, 788-793, 1993.
[19] Fontenla-Romero O. et al, A measure of fault tolerance for functional
networks, Neurocomputing, Vol.62, 327-347, 2004.
[20] Hassibi B and D.G. Stork (1993). Second order derivatives for network
pruning: Optimal brain surgeon. In Hanson et al. (eds) Advances in
Neural Information Processing Systems, 164-171.
[21] T. Hastie, R. Tibshirani, J. H. Friedman The Elements of Statistical
Learning, Springer, 2001.
[22] S. Haykin, Neural networks: A comprehensive foundation, Macmillan
College Publishing Company, Inc. 1994.
[23] Holt J. and J. Hwang, Finite precision error analysis of neural networks
hardware implementation, IEEE Transactions on Computers, Vol.42,
p.281-290, 1993.
[24] Jim K.C., C.L. Giles and B.G. Horne, An analysis of noise in recurrent
neural networks: Convergence and generalization, IEEE Transactions on
Neural Networks, Vol.7, 1424-1438, 1996.
[25] LeCun Y. et al. (1990). Optimal brain damage, Advances in Neural
Information Processing Systems 2 (D.S. Touretsky, ed.) 396-404.
[26] Kullback S., Information Theory and Statistics, Wiley, 1959.
[27] Chi-sing Leung, Kwok-wo Wong, Pui-fai Sum and Lai-wan Chan, A
pruning method for recursive least squared algorithm, Neural Networks,
14:147-174, 2001.
[28] Leung C.S., G.H. Young, J. Sum and W.K. Kan, On the regularization of
forgetting recursive least square, IEEE Transactions on Neural Networks,
Vol.10, 1842-1846, 1999.
[29] C.S.Leung, K.W.Wong, John Sum, and L.W.Chan, On-line training and
pruning for RLS algorithms, Electronics Letters, Vol.32, No.23, 2152-
2153, 1996.
[30] Chi-sing Leung and John Sum, Fault tolerant regularizer for multiple
nodes fault RBF, accepted for publication in IEEE Transactions on
Neural Networks.
[31] Mackay D.J.C. (1992), A Practical Bayesian Framework for Backprop
Networks, Neural Computation, Vol.4(3) 448-472.
[32] Merchant S, G.D. Peterson, S.K. Park and S.G. Kong, FPGA implemen-
tation of evolvable block-based neural networks, Proc. IEEE Congress
on Evolutionary Computation, Vancouver Canada, p.3129-3136, 2006.
[33] Moody J.E., Note on generalization, regularization, and architecture
selection in nonlinear learning systems, First IEEE-SP Workshop on
Neural Networks for Signal Processing, 1991.
[34] Murata N., S. Yoshizawa and S. Amari. Network information criterion–
Determining the number of hidden units for an artificial neural network
model, IEEE Transactions on Neural Networks, Vol.5(6), pp.865-872,
1994.
[35] Murray A.F. and P.J. Edwards, Synaptic weight noise during multilayer
perceptron training: fault tolerance and training improvements, IEEE
Transactions on Neural Networks, Vol.4(4), 722-725, 1993.
[36] Murray A.F. and P.J. Edwards, Enhanced MLP performance and fault
tolerance resulting from synaptic weight noise during training, IEEE
Transactions on Neural Networks, Vol.5(5), 792-802, 1994.
[37] Neti C. M.H. Schneider and E.D. Young, Maximally fault tolerance
neural networks, IEEE Transactions on Neural Networks, Vol.3(1), 14-
23, 1992.
[38] Parra X. and A. Catala, Fault tolerance in the learning algorithm of radial
basis function networks, Proc. IJCNN 2000, Vol.3, 527-532, 2000.
[39] Pedersen M.W., L.K. Hansen and J. Larsen. Pruning with generaliza-
tion based weight saliencies: 훾OBD, 훾OBS. Advances in Information
Processing Systems 8 521-528, 1996.
[40] Phatak D.S. and I. Koren, Complete and partial fault tolerance of
feedforward neural nets., IEEE Transactions on Neural Networks, Vol.6,
446-456, 1995.
[41] Phatak D.S., Relationship between fault tolerance, generalization and the
Vapnik-Cervonenkis (VC) dimension of feedforward ANNs, IJCNN’99,
Vol.1, 705-709, 1999.
[42] Phatak D.S. and E. Tcherner, Synthesis of fault tolerance neural net-
works, Proc. IJCNN’02, 1475-1480, 2002.
[43] Piche S.W., The selection of weight accuracies for Madalines, IEEE
Transactions on Neural Networks, Vol.6, 432-445, 1995.
[44] Reed R., Pruning algorithms – A survey, IEEE Transactions on Neural
Networks, Vol.4(5), 740-747, 1993.
[45] Sequin C.H. and R.D. Clay, Fault tolerance in feedforward artificial
neural networks, Neural Networks, Vol.4, 111-141, 1991.
[46] Simon D. and H. El-Sherief, Fault-tolerance training for optimal inter-
polative nets, IEEE Transactions on Neural Networks, Vol.6, 1531-1535,
1995.
[47] Simon D., Distributed fault tolerance in optimal interpolative nets, IEEE
Transactions on Neural Networks, Vol.12(6), 1348-1357, 2001.
[48] Stevenson M., R. Winter and B. Widrow, Sensitivity of feedfoward neu-
ral networks to weight errors, IEEE Transactions on Neural Networks,
Vol.1, 71-80, 1990.
[49] John Sum, Chi-sing Leung and Kevin Ho, On objective function,
regularizer and prediction error of a learning algorithm for dealing
with multiplicative weight noise, in submission to IEEE Transactions
on Neural Networks.
[50] John Sum and Chi-sing Leung, Derivation of explicit regularization from
KL divergence, Neural Processing Letters (In revision).
[51] John Sum, On a multiple nodes fault tolerant training for RBF: Objective
function, sensitivity analysis and relation to generalization, Proceedings
of TAAI’05, Tainan, ROC, 2005.
[52] Elko B. Tchernev, Rory G. Mulvaney, and Dhananjay S. Phatak, Inves-
tigating the Fault Tolerance of Neural Networks, Neural Computation,
Vol.17, 1646-1664, 2005.
[53] Elko B. Tchernev, Rory G. Mulvaney, and Dhananjay S. Phatak, Perfect
fault tolerance of the n-k-n network, Neural Computation, Vol.17, 1911-
1920, 2005.
[54] Townsend N.W. and L. Tarassenko, Estimations of error bounds for
neural network function approximators, IEEE Transactions on Neural
Networks, Vol.10(2), 217-230, 1999.
[55] Vollmer U. and A. Strey, Experimental study on the precision require-
ments of RBF, RPROP and BPTT training, Proc. IEE Ninth International
Conference on Artificial Neural Networks, p.239-244, 1999.
