中文摘要 
近年來，在科學應用上，對一個未知的系統行為函數建模(modeling)一直是非常重要的研
究課題，而模糊理論(fuzzy theory)常被用在取得的系統知識不完整、口語化的知識表達
或是以專家知識描述系統的操作行為等等的建模上，並逐漸發展成為系統建模的有效工
具。但是，在實際應用系統的取樣資料中，由於各種因素影響，常常使得取樣資料中存有
一般雜訊(noise)或大誤差(outlier)的資料。若將這些雜訊或大誤差資料直接使用在系統
行為函數的建模上，往往使得所建立的系統模式與實際的系統行為產生很大的誤差，這種
現象稱為，這種現象稱為「過度適應」為了解決這個問題，本文提出一個非監督模糊建模
方式，可直接由具有雜訊及大誤差的輸入-輸出取樣資料中擷取模糊規則，以建立系統行為
的近似函數。本文中利用多個實驗證明所提方法可以用在不同的資料領域上，且在效能上，
比其他方法好。 
關鍵詞：強健式、模糊建模、非監督模式、大誤差 
 
本論文以投稿並刊登在“Expert Systems with Applications＂第 36
期,2009，Impact factor: 2.59 。全文如下: 
 
Takagi–Sugeno–Kans’s model (TSK model). The Mamdani’s model
describes the relationship between inputs and outputs of a control
system through a set of linguistic control rules and membership
functions (Sugeno & Yasukawa, 1993; Takagi & Sugeno, 1985;
Yang, Shieh, & Lee, 2004). On the other hand, the TSK model parti-
tions an input space into several subspaces that are treated as clus-
ters to describe either a static or a dynamic nonlinear system. One
fuzzy rule is then created for each of these clusters. Below is a rep-
resentation of rules in a TSK model:
Ri :
IF x1 is A
i
1 and x2 is A
i
2 . . . and xk is A
i
n:
THEN yi ¼ ai0 þ ai1x1 þ ai2x2 þ . . . þ aikxk; ð1Þ
where
y^ ¼
Pc
i¼1 w
iyiPc
i¼1 wi
;
wi ¼
Yk
j¼1
lAij ðxjÞ; ð2Þ
Riði ¼ 1;2; . . . ; cÞ represents the ith rule, aibði ¼ 1;2; . . . ; c;
b ¼ 0;1; . . . ; kÞ is a constant called consequence parameter, yi is
the output of the ith rule ði ¼ 1;2; . . . ; cÞ;Aijði ¼ 1;2; . . . ;
c; j ¼ 0;1; . . . ; kÞ represents a linguistic term characterizing the
membership function of the jth input variable of the ith rule and
is called premise parameter, lAij ðxjÞ is the membership grade of lin-
guistic term Aij for jth input variable in ith rule, w
i is the product of
membership grades for all input variables xjðj ¼ 1;2; . . . ; kÞ of the ith
rule ði ¼ 1;2; . . . ; cÞ, and y^ represents the ﬁnal output of the TSK
model using wiði ¼ 1;2; . . . ; cÞ as the weight of rule Ri.
The above rule shows the output yi is a linear combination of in-
put variables xjðj ¼ 1;2; . . . ; kÞ. That is, the TSK fuzzy model de-
scribes a nonlinear system’s domain by several subspaces having
linear relations. One fuzzy rule is then created to model the in-
put–output relationship for each of these subspaces.
In the original TSK fuzzymodelingmethod, usermust deﬁne fuz-
zy subspaces in advance and the consequence parameters are ob-
tained by Kalman ﬁlter estimator. But there is no learning process
for premise parameters for the original TSK fuzzy model and result
in the accuracy of system model may be limited. Various learning
approaches for the TSK fuzzy model have been proposed in the lit-
erature (Chuang, Su, & Chen, 2001; Jang, Sun, & Mizutani, 1997;
Kim, Park, & Ji, 1997; Shieh et al., 2006; Wang, 1994). The gradient
descent algorithm and the BP algorithm have become most widely
used method to ﬁne-tune the premise and the consequent parame-
ters in the fuzzy modeling and neural networks, respectively.
In this paper, in order to overcome the problems of function
approximation for a nonlinear system with noise and outliers, a
fuzzy clustering method is proposed to greatly mitigate the inﬂu-
ence of noise and outliers and then a fuzzy-based data sifter is pro-
posed to partition the nonlinear system’s domain into several
piecewise linear subspaces to be represented by the TSK fuzzy neu-
ral networks. Firstly, the input data are clustered into different
clusters based on the distance relation. Each cluster is represented
by its cluster center. This step is to locate good cluster centers
without considering data noise and outliers. Secondly, the input
data are re-clustered by the robust fuzzy c-means (RFCM) algo-
rithm (Shieh et al., 2006) based on the number of clusters and
the cluster centers obtained in the ﬁrst step to search the ﬁnal clus-
ter centers of the data set by greatly mitigating the inﬂuence of any
noise and outliers. Thirdly, a proposed fuzzy data sifter (FDS) is run
against the resultant clusters from RFCM to best partition these
clusters into piecewise subclusters. The fuzzy rules and member-
ship functions are then extracted from these subclusters to build
the TSK fuzzy neural networks, then tune the parameters by the
BP algorithm.
2. Data fuzzy clustering using distance relation
2.1. Fuzzy c-means algorithm
Fuzzy c-means algorithm (FCM) (Chuang et al., 2001; Jang et al.,
1997; Sugeno & Yasukawa, 1993; Wang, 1994) has become the
most popular fuzzy clustering approach in both theory and practi-
cal applications for decades (Kim et al., 1997). Unfortunately, be-
sides the need of pre-setting the number of clusters, the result of
performing the FCM greatly depends on the condition of assigned
initial cluster centers.
2.2. Data fuzzy clustering based on distance relation
A disadvantage of some of popular fuzzy clustering approaches
is that the number of clusters must be predetermined. Even if the
number of clusters is given, the clustering results of these algo-
rithms are inﬂuenced by the choice of initial cluster centers (Wong
& Chen, 1999). Another problem occurs when some of the training
data incur large errors due to the outliers in real application sys-
tems. When there are outliers in training data, the BP algorithm
usually cannot come up with acceptable performance. Chung et
al. (2000) proposed a robust backpropagation (ARBP) learning algo-
rithm to overcome this problem. The ARBP derived a robust BP
learning algorithm that is resistant to the outlier effect during
the approximation process. Chuang et al. (2001) proposed a novel
approach termed as the RFRA clustering algorithm for function
approximation with outliers. The RFRA clustering algorithm is
modiﬁed from the robust competitive algorithm agglomeration
(RCA) clustering algorithm. Since the RCA algorithm is only a clus-
tering algorithm, the RFRA clustering algorithm modiﬁed the RCA
to ﬁnd fuzzy regression from a given set of training data. Lee,
Chung, Tsia, and Chang (1999) proposed a new activation function
composite of a set of sigmoidal functions and aims at approximat-
ing a giving function with nearly constant value. Chen and Jain
(1994) proposed an algorithm extended the BP algorithm by
employing a new objective function taking the Hampel’s hyper-
bolic tangent estimator in the theory ofM-estimators. In the learn-
ing phase of this paper, the shape of the objective function changes
with the iteration time. These robust algorithms above adopt ro-
bust objective function to distinguish outliers from real data by
using cutoff point. The cutoff point serves as an index for discrim-
inating against outliers. But in some real application systems, how
to set the cutoff point is a critical difﬁculty.
Shieh et al. (2006) proposed an unsupervised clustering algo-
rithm by which the reasonable clusters are automatically gener-
ated based on the data distribution of input data set without the
need of pre-specifying the number of resultant clusters. Instead
of randomly choosing initial cluster centers, the centers of the
resultant clusters generated by this algorithm are then used as ini-
tial cluster centers by the RFCM to search out good resultant clus-
ter centers by mitigating the inﬂuence of noise and outlier data.
Let vi and vj represent the centers of cluster i and j, respectively,
the distance relation between centers of two clusters is deﬁned by
the following equation
rij ¼ exp kvi  vjk
2
2r2
 !
; i ¼ 1;2; . . . ; n; j ¼ 1;2; . . . ;n; ð3Þ
where jjvi  vj jj represents the Euclidean distance between vi and vj,
and r is the width of the Gaussian function. Initially, each data
point is formed one cluster containing the data point itself. Clusters
are then to be merged into larger clusters based on how separated
6904 H.-L. Shieh et al. / Expert Systems with Applications 36 (2009) 6903–6913
function. Fig. 2 shows the 2D diagram of Eq. (10). The horizontal
coordinate in Fig. 2 represents the distance between xi and vk, and
the vertical coordinate represents the value oflik. The RFCM is sta-
ted as in Algorithm 1.
Algorithm 1. RFCM algorithm
Step 1: Set value of m satisfying 1 < m <1 and allowed maximum
number of iteration.
Step 2: Calculate initial cluster centers vk, k=1, 2 ,. . .,c by Eqs. (3)
and (4) in Section 2.2.
Step 3: Calculate the membership matrix of l by Eq. (10).
Step 4: Update fuzzy cluster centers using Eq. (9).
Step 5: Calculate objective function value by Eq. (7). Stop if either
the value is below a certain tolerance value or its improve-
ment over previous iteration is below a certain threshold
or the number of iteration is over the maximum value.
Step 6: Compute a new l using Eq. (10). Go to Step 4.
3. Fuzzy neural networks for function approximation
For the TSK fuzzy neural networks of function approximation,
the data domain is divided into subspaces and each subspace is
represented by a linear model. The locations where the data do-
main is divided are named turning-points. This section uses a slip
window based recognition system, namely the FDS, to search the
best turning-points of a nonlinear function. The obtained turn-
ing-points are used to divide a given set of data domain into clus-
ters. The piecewise linear regression algorithm is then applied to
calculate the regression parameters for each of clusters. The resul-
tant regression parameters for each cluster are the premise param-
eters of the TSK fuzzy model in Eq. (1).
The FDS is an iterated sifting system that uses a turning-point
sift network (TPSN) to search out the local maximum and mini-
mum data points in the data domain. For simplicity of explanation,
the discussion is detailed in a 2-dimension ﬁgure as Fig. 3. The
turning-points can be basically classiﬁed into following patterns:
peak in Fig. 3(a), valley in Fig. 3(b). The core idea of the FDS is to
use the two patterns in Fig. 3 as templates to search out all of
the turning-points in a data domain. Because the FDS is using fuzzy
membership degree to indicate the bending level (angle) of a
located turning-point, only the peak and valley patterns are used
by the FDS. For example, a turning-point with the pattern of
Fig. 4(a) will result in full fuzzy membership degree and a turn-
ing-point with the pattern of Fig. 4(b) will result in much less fuzzy
membership degree when searched by FDS. The higher member-
ship degree a turning-point has, the higher priority the turning-
point has to be used as a dividing point in a data domain.
3.1. The architecture of the FDS
The architecture of the FDS is shown in Fig. 5. The FDS is a com-
binational network and can be divided into four blocks: slip win-
dow, turning-point sift network (TPSN), fuzzy turning-point
matching degree (FTPD) matrix and indexes pool.
The slip window takes input into the FDS for processing like a
water ﬂow and is therefore called data stream. In Fig. 5, d1 and
dN, respectively mean the ﬁrst and last data points of a data stream
that the slip window covers to be currently being processed
(matched) against turning-point patterns in Fig. 5. The m stands
the width of the slip window, namely the data point numbers in
the slip window. The slip window advances one data point in a
data stream for each time of processing.
The TPSN set is to perform the task of searching out all turning-
points and generates their fuzzy matching membership degrees
Fig. 2. The diagram of the membership degree of xi belonging to class k.
a b
Fig. 3. Patterns of turning-points: (a) peak; (b) valley.
Fig. 4. Two sampling data patterns. (a) A pattern mach the peak pattern. (b) A
pattern dose not fully mach the peak pattern.
6906 H.-L. Shieh et al. / Expert Systems with Applications 36 (2009) 6903–6913
lj1ðyjÞ ¼ exp 
1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPk
i¼1
yj  ycpij
 2s
yrange
0
BBBB@
1
CCCCA: ð13Þ
Similarly, the matching degree between value yj and value ycvij in
a valley pattern can be measured by a fuzzy membership function
deﬁned in
lj2ðyjÞ ¼ exp 
1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPk
i¼1ðyj  ycvij Þ2
q
yrange
0
@
1
A; ð14Þ
where yrange = ymax  ymin.
Since there are m data points in the slip window and each data
point has its own matching degrees derived from Eqs. (13) and
(14), the overall matching degree between the pattern formed by
the m data points in the slip window and a designated pattern
can be deﬁned as
jqs ¼
Pm
j¼1 wjljqðyjÞPm
j¼1 wj
¼ w1l1qðy1Þ þw2l2qðy2Þ þ    þwmlmqðymÞ
w1 þw2 þ    þwm ð15Þ
where q = 1,2, s = 1, 2,. . .,n and 1Pm
1
wj
is normalization factor.
The s indicates the slip window is centered at the sth data point,
q = 1 stands for the overall matching degree against peak pattern
and q = 2 stands for the overall matching degree against valley pat-
tern. The wj(j = 1, 2,. . .,m) are the weight values on the inﬂuence of
deciding matching degree for the turning angle in the slip window
against peak or valley patterns. The weightw1 is assigned for ps and
w2, w3,. . .,wm are respectively assigned for the rest of m  1 points
in the slip window. It is obvious that w1 should have the highest
weight value since it is where the turning angle takes place, as
shown in Fig. 6. The second highest weight w2 is assigned for ps1
and the third highest weight w3 is assigned for ps+1 and so on. The
relationship of these weights is w1 >w2 > w3 >    > wm indicating
the fact that the farther from point ps, the less inﬂuence on decid-
ing matching degree of a turning angle at point ps against a desig-
nated pattern.
Eqs. (13)–(15), in conjunction with action of slip window, calcu-
late the fuzzy matching degrees against designated patterns for
every data point of {(x1j, x2j,. . .,xkj, yj)j 1 6 j 6 n}. A large value of
jqs indicates a certain data point ps has a turning angle having high
matching degree against a designated pattern. Consequently, the
task of partitioning a data domain into clusters can be done in
accordance of the descending list of jqs value.
After a system is divided into clusters, each of clusters can be
described by a fuzzy rule of Eq. (1). Rule Ri represents a fuzzy rule
model for ith cluster. All of these rules form a TSK fuzzy model. In
Eq. (1), the Aij of xj, j = 1, 2,. . .,k, is a premise parameter in a TSK
fuzzy model and stands for the membership function that maps
the jth parameter to ith cluster. Such membership function is
deﬁned by a Two Side Gaussian Function expressed as Eq. (16)
(Jang et al., 1997):
TSGFðx;u1;r1;u2;r2Þ ¼
exp  12 xu1r1
 2 
for x 6 u1
1 foru1 6 x 6 u2
exp  12 xu2r2
 2 
for u2 6 x
8>>><
>>>:
9>>>=
>>>;
;
ð16Þ
where ui and ri with i = 1,2 stand for the mean and deviation of the
Two Side Gaussian Function, respectively. In this paper, the lijðxjÞ
representing the ﬁring of input variable xj(j = 1, 2,. . .,k) towards
the linguistic membership function Aij is shown as:
lijðxjÞ ¼
exp  12
xjui1j
ri
1j
 2 !
for xj 6 ui1j
1 for ui1j 6 xj 6 ui2j
exp  12
xjui2j
ri
2j
 2 !
for ui2j 6 xj
8>>>><
>>>>:
9>>>>=
>>>>;
; ð17Þ
where ui1j, ui2j and ri1j, ri2j (i = 1,2,. . .,c, j = 1,2,. . .,k) refer to the mean
and standard deviation of the left and right Gaussian functions of
the jth input variable in the ith rule.
3.3. Consequence parameters identiﬁcation
In the TSK fuzzy model, the consequence represents the input–
output relationship by the linear combination of input variables.
That is, after a data domain is divided into several clusters, the in-
put–out relationship in each cluster is derived by the method of
multiple liner regression that allows the output to be represented
by the linear combination of input variables.
Suppose there are k independent input variables x1, x2,. . .,xk,
where {xij xi 2 Xi, i = 1, 2,. . .,k} and Xi represents the deﬁned domain
of the ith variable. For k independent input variables, Yj (x1,
x2,. . .,xk) represents a random variable Y from a set of input value
xi(i = 1, 2,. . .,k). The mean is denoted as MY jðx1 ;x2 ;...;xkÞ and can be ex-
pressed as the equation below:
MY jðx1 ;x2 ;...xkÞ ¼ aþ b1x1 þ    þ bjxk; ð18Þ
Fig. 8. Sample m data points and peak pattern comparison. (a) 3D slip window. (b)
Sampling data and peak pattern comparison.
6908 H.-L. Shieh et al. / Expert Systems with Applications 36 (2009) 6903–6913
From the proof by Kim et al. (1997):
o 12 e
2
t
ouipj
¼ ðyt  y^tÞðyi  y^tÞ
1Pc
i¼1 wi
owi
ouipj
; ð34Þ
o 12 e
2
t
oripj
¼ ðyt  y^tÞðyi  y^tÞ
1Pc
i¼1 wi
owi
oripj
; ð35Þ
o 12 e
2
t
oaij
¼ ðyt  y^tÞ
1Pc
i¼1wi
wixjt; ð36Þ
where
owi
ouipj
¼ 1
ripj
xjt uipj
ripj
exp 1
2
xjt uipj
ripj
 !28<
:
9=
;; ð37Þ
owi
oripj
¼ 1
ripj
ðxjt u
i
pj
ripj
Þ2 exp 1
2
xjt uipj
ripj
 !28<
:
9=
;
¼ xjt u
i
pj
ripj
owi
ouipj
: ð38Þ
The architecture of proposed method is shown in Fig. 10. The
neural network is used to build the TSK fuzzy model. Firstly, the
input data are clustered into different initial clusters based on
unsupervised clustering algorithm. Each cluster is represented
by its cluster center in Section 2.2. Secondly, the input data are
re-clustered by the RFCM algorithm based on the number of clus-
ters and the cluster centers obtained in the ﬁrst step to search the
ﬁnal cluster centers of the data set by greatly mitigating the inﬂu-
ence of any noise and outliers. The ﬁnal cluster centers of the
data are used as input of the neural networks. Because of the
RFCM mitigate inﬂuence of any noise and outliers, the model ob-
tained by the neural networks don’t have the phenomenon of
overﬁtting.
Thirdly, the FDS is run against the resultant clusters from the
RFCM to best partition these clusters into piecewise subclusters.
In this step, each cluster is represented a node in layer 2 of neural
network in Fig. 9. Each node in layer 2 of the networks represents
premise parameter Aij in the TSK fuzzy model. Layer 3 is the prod-
uct wi in Eq. (2). Each node in layer 4 calculate the weighting out-
put of rule i, wiyi. Where yi is the output of theith rule in Eq. (2).
The parameters Aij of y
i are obtained by the linear regression algo-
rithms described in Sections 3 and 4. Layer 5 is output layer
which represents the output of the TSK fuzzy neural network in
Eq. (2). While the parameters are obtained of above structure,
Eqs. (31)–(38) are used to tuning parameters of the TSK fuzzy
neural model.
5. Experimental results
In this section, two examples are illustrated to show the perfor-
mance of the proposed method. The root mean square error
(RMSE) is used to measure the performance of the method in each
experiment. The RMSE is deﬁned as:
RMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPN
i¼1ðy^i  yiÞ2
N
s
; ð39Þ
where yi is the desired value at xi, y^i is the output of the constructed
model, and N is the number of sample input data points.
Example 1. In this example, a function used by Chuang, Jeng, and
Lin (2004) is deﬁned as
y ¼ x23 with x 2 ½2;2: ð40Þ
There are 51 sample data points generated from Eq. (40) and three
artiﬁcial outliers are added into the sample data pool, as shown in
Fig. 11(a) in which outliers are denoted by circles.
Fig. 11(b) shows the result after running the unsupervised clus-
tering algorithm that generates cluster centers without consider-
ing data noise and outliers. The RFCM algorithm is then executed
against the cluster centers in Fig. 11(b) to obtain the result shown
in Fig. 11(c) in which the inﬂuence of data noise and outliers has
Use
unsupervised
clustering
algorithm to
obtain initial
clusters
Input
data
Use RFCM
to greatly
mitigate the
influence of
data noise
and outliers
 Use FDS to
locate good
turning-
points to
partition a
given
nonlinear
data domain
into
piecewise
clusters
Use the
linear
regression to
obtain the
parameters
of  each
cluster
x1
xk
.
.
.
Π
Π
Π
Π
i
ka
w i1
1A
1
kA
m
kA
fA1
.
.
.
^
y
Layer
1
Layer
2
Layer
3
Layer
4
Layer
5
kx
11 yw
cc yw
Σ ∑ iw
1
y1
yc
Fig. 10. The architecture for the TSK fuzzy neural networks.
6910 H.-L. Shieh et al. / Expert Systems with Applications 36 (2009) 6903–6913
The RSME has stabilized and reached the level of 0.0178 after
200 training. For comparison, the same data were simulated by
the ARRBFN (Chuang et al., 2004) resulting in 14 fuzzy rules (hid-
den nodes) and 0.04025 of RMSE.
-10 -8 -6 -4 -2 0 2 4 6 8 10
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
X
-10 -8 -6 -4 -2 0 2 4 6 8 10
X
-10 -8 -6 -4 -2 0 2 4 6 8 10
X
Y
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y
-10 -8 -6 -4 -2 0 2 4 6 8 10
X
-10 -8 -6 -4 -2 0 2 4 6 8 10
X
0 50 100 150 200 250 300 350 400
0.05
0.06
0.07
0.08
0.09
0.1
0.11
0.12
iteration
R
M
SE
a d
b e
c f
Fig. 12. (a) Equation y ¼ sin xx added noise Nð0; 0:03Þ. (b) Cluster centers obtained by the unsupervised clustering algorithm. (c) RFCM result greatly reducing the inﬂuence of
data noise. (d) The 7 turning-points searched out by FDS. (e) The result of the proposed method. Dotted lines denote the constructed TS fuzzy model. (f) RMSE convergence
curve of the constructed TS fuzzy model.
6912 H.-L. Shieh et al. / Expert Systems with Applications 36 (2009) 6903–6913
國科會  出席國際會議報告 
 
Extraction for Urban Planning、加拿大Weichang Du 所發
表Toward Community-Based Personal Cloud Computing論
文，台灣元智大學Chien-Hua Wang 發表論文Finding Fuzzy 
Association Rules Using FWFP-Growth with Linguistic 
Supports and Confidences. 
 
5 月 29 日：今日繼續參加 ICCIT2009 會議。 
 早上9:00到達會場，聆聽蘇丹 Anders Berglund的The effect 
of interlamellar distance in pearlite on CGI machining 
論文；加拿大 Rajib Ul Alam Uzzal, Ion Stiharu, Waiz Ahmed
的 Design and Analysis of MEMS based Accelerometer for 
Automatic Detection of Railway Wheel Flat 及芬蘭 AHM 
Shamsuzzoha, Tauno Kekale, Petri Helo 的 Towards 
External Varieties to Internal Varieties − Modular 
Perspective; 澳 洲 I.V.N.Rathnayake,Mallavarapu 
Megharaj, Nanthi Bolan,Ravi Naidu 的 Tolerance of Heavy 
Metals by Gram Positive Soil Bacteria 等論文發表，並參
與問題討論。 
下午 2:00 開始，聆聽馬來西亞 Nadia M. 的 IFS on the Multi-Fuzzy 
Fractal Space論文; 日本 Yan Xu and Toshihiro Nishimura 
Segmentation of Breast Lesions in Ultrasound Images 
Using Spatial Fuzzy Clustering and Structure Tensors
論文; 波蘭 Janusz Marchwinski, 的Architectural Concept 
of Sustainable Building in Warsaw論文; 中國的 Ming-Hui 
Lee, The study of the intelligent fuzzy weighted input 
estimation method combined with the experiment 
verification for the multilayer materials 等論文發表，
並參與問題討論。 
參加心得： 
    本次會議有來自世界 20 幾個國家地區共 210 篇論文發表，論文涵
蓋電腦科學、網路理論及應用、智慧型應用及資訊系統等範圍。會議中
所發表之論文均為各領域學者最新之研究，可作為未來研究的參考。例
如，在知識取得的應用可以用在知識分析的前置步驟，與我目前所研究
的模糊理論在工業運用上可相互結合。又如，在網路運用上，加拿大
Weichang Du 所發表的新的網路協同技術，如雲端計算，為目前在網路
上最為火紅的計算技術，該技術將網路上的眾多主機合眾連橫，共同為
目前網路龐大的計算提供快速的服務。在會議中，也有幾篇 Fuzzy理論
的應用論文發表，提供我在智慧型控制領域上新的看法及新的研究主
