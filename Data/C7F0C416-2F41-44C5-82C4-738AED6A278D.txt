 
中文摘要： 隨著超大型積體電路技術的高速發展，許多先進的微處理器已
經可以有數個不同的操作電壓/頻率 
(如 Intel StrongARM SA1100 及 Intel XScale 等微處理器)。知名
技術如 Intel SpeedSteptm and AMD PowerNOWtm 等也廣泛使
用在筆記型電腦上來提供動態電壓調整，以延長電池使用時
間。不同操作電壓在(動態電壓調整)微處理器上提供不同程式
執行速度，而操作電壓越低，則執行速度越慢，動態電壓調整
的電源消耗也越低。在此同時，省電系統設計也隨著先進的系
統結構而漸趨複雜化，目前兩個重要的趨勢是多核心系統的浮
現與快閃記憶體的廣泛採用。 
 
本研究計劃為期三年，第一年的主軸是發展具變動執行週期
(Variable Execution Cycles) 之程序(Tasks) 的即時省電排程技
術（例如多媒體程序等），我們亦將發展實作於 AMD Phenom 
II Quad-Core Processor X940 的平台上，提出了在多核心平台上
能源效率的多媒體工作的即時排程。計劃第二年的主軸是更進
一步考量更全面的系統架構，特別是加入多核心系統特性於即
時省電排程技術上，並延申成果於較先進之記憶體結構
(Memory Hierarchy) 考量，NAND 快閃記憶體固態儲存碟(Solid-
State Disks) 
與在地執行(XIP) NOR 快閃記憶體與記憶體的綜合將是一個重
心，快速開關機與效能暨節能考量將是軸線。第三年的主軸是
分析即時省電排程異象(Anomaly)，著重發展理論與工具程式，
以減少未來平台轉換或提升時的變數，系統實做上亦在記憶體
結構(Memory Hierarchy) 上更加動態 NOR 資源最佳化，以提升
效能與可靠度。本研究著重於在多核心系統上，其記憶體模組
不同的環境下，在工作執行時間會有顯著的影響。不同的記憶
體模組會有不同的延遲時間。 
英文摘要： With the advance technology of VLSI circuit designs, many modern 
processors can operate at different supply voltages/frequencies. 
Technologies, such as Intel SpeedSteptm and AMD PowerNOWtm, 
also provide dynamic voltage scaling for laptops to prolong their 
battery lifetime. Different supply voltages lead to different 
execution speeds on a dynamic voltage scaling processor. The lower 
the supply voltage is, the lower is the speed, and the less is the 
power consumption of the system in dynamic voltage scaling.  
 
This project aims at the development of critical technology of real-
time task scheduling in embedded systems with dynamic voltage-
scaling cores and flash memory considered in the memory 
hierarchy. The main theme of the first year is to explore realtime 
energy-efficient scheduling of tasks with variable execution cycles, 
such as those of multimedia workloads. In this research, explores 
行政院國家科學委員會專題研究計畫成果報告
計畫名稱：多核心嵌入式系統之即時省電程序排程
計畫編號： 97-2221-E-002-206-MY3
執行期限：計畫自民國97年08月01日起至民國100年07月31日止
主持人：郭大維
計畫參與人員：楊川岳、盧永豐、黃柏鈞、張哲維、賴昭榮、魏逸鴻、廖容章、林
育佳、柯佳玲、張繼儒、李德年、陳煒栩、張弘昇、何建忠、楊明昌、鄭聖威
執行單位 : 國立台灣大學資訊工程學系
F
1 摘要
1.1 中文摘要
隨著超大型積體電路技術的高速發展，許多先進
的微處理器已經可以有數個不同的操作電壓/頻率
(如Intel StrongARM SA1100 及Intel XScale等微
處理器)。知名技術如Intel SpeedSteptm and AMD
PowerNOWtm等也廣泛使用在筆記型電腦上來提供
動態電壓調整，以延長電池使用時間。不同操作
電壓在(動態電壓調整)微處理器上提供不同程式
執行速度，而操作電壓越低，則執行速度越慢，
動態電壓調整的電源消耗也越低。在此同時，省
電系統設計也隨著先進的系統結構而漸趨複雜
化，目前兩個重要的趨勢是多核心系統的浮現與
快閃記憶體的廣泛採用。
本研究計劃為期三年，主要目標是研發具有動態
電壓調整與快閃記憶體結構之多核心嵌入式系統
上的即時排程技術，而動態電壓調整多核心與快
閃記憶體結構也逐漸成為許多產品的特徵。計劃
第一年的主軸是發展具變動執行週期 (Variable
Execution Cycles) 之程序 (Tasks) 的即時省電
排程技術 （例如多媒體程序等），我們亦將發展
實作於AMD Phenom II Quad-Core Processor X940
的平台上，提出了在多核心平台上能源效率的多
媒體工作的即時排程。計劃第二年的主軸是更進
一步考量更全面的系統架構，特別是加入多核心
系統特性於即時省電排程技術上，並延申成果於
較先進之記憶體結構 (Memory Hierarchy) 考量，
NAND快閃記憶體固態儲存碟 (Solid-State Disks)
與在地執行 (XIP) NOR 快閃記憶體與記憶體的綜
合將是一個重心，快速開關機與效能暨節能考量
將是軸線。第三年的主軸是分析即時省電排程異
象 (Anomaly)，著重發展理論與工具程式，以減
少未來平台轉換或提升時的變數，系統實做上亦
將在記憶體結構 (Memory Hierarchy) 上更加動態
NOR資源最佳化，以提升效能與可靠度。本研究著
重於在多核心系統上，其記憶體模組不同的環境
下，在工作執行時間會有顯著的影響。不同的記
憶體模組會有不同的延遲時間。本研究將一起考
慮兩種不同記憶體模組上的分頁配置和即時系統
的排程，進而最小化效能滿載的核心。對於非周
期性的工作，本研究發展了一個兩階段演算法，
其中第一階段將利用記憶體配置找出系統最大效
能的下限，第二階段採用最壞適合(worst-fit)分
區分配分配。實驗結果顯示將在最大效能優先順
序和任意順序上會有(2 − 2M+1 )和 (2 − 1M )的近似界
限，其中M為核心的數量。本研究利用SPEC 2000
和模擬器來分析演算法。
1.2 英文摘要
With the advance technology of VLSI circuit
designs, many modern processors can oper-
ate at different supply voltages/frequencies
(e.g., Intel StrongARM SA1100 processor and
Intel XScale). Technologies, such as Intel
SpeedSteptm and AMD PowerNOWtm, also provide
dynamic voltage scaling for laptops to pro-
long their battery lifetime. Different supply
voltages lead to different execution speeds
on a dynamic voltage scaling processor. The
lower the supply voltage is, the lower is the
speed, and the less is the power consumption
of the system in dynamic voltage scaling.
1
energy and thermal problems. Examples are
the introductions of the Intel 48-core SCC
[24], the 80-core TeraFLOPS Processor [8],
and the 100-core Tilera Tile-GX [27]. It is
of paramount importance that a multiprocessor
system must be designed with effective hard-
ware architecture to optimize its performance
and utilization, such as the hierarchical
memory designs for Intel i7 and AMD Phenom II.
Moreover, real-time task scheduling should
consider not only task scheduling over pro-
cessors but also related resource allocation
problems. Such an observation motivates the
joint considerations of the task scheduling
over processors and the memory allocation of
task-execution images.
Real-time multiprocessor task scheduling
could be roughly classified into three types:
partitioned, global, and semi-partitioned scheduling
algorithms. Partitioned scheduling stati-
cally assigns each task to a processor such
that any arrival of a task instance only ex-
ecutes on its assigned processor [2]. Global
scheduling has a global queue to dynamically
dispatch task instances in the queue to
an available processor according to some
criterion, such as their priorities [12].
Semi-partitioned scheduling is a compromise
between the above two. For example, we can di-
vide a sporadic task into subtasks and assign
these subtasks to processors statically under
the guarantee that the subtasks of a task will
not execute on two processors simultaneously
[11]. We refer readers to a comprehensive
survey by Davis and Burns [7].
Beside the topics mentioned above, we also
explore the partitioned scheduling over mul-
tiple processors with heterogeneous memory
modules. Unfortunately, real-time multipro-
cessor partitioned scheduling has been proven
to be NP-complete even for implicit-deadline
sporadic tasks [17], in which the relative
deadline of a task is equal to its minimum
inter-arrival time. The closest related re-
sults are partitioned scheduling algorithms
with a ( 43 − 13M )-approximation bound and a(
2− 1M
)-approximation bound with the largest-
utilization-first (LUF) task order and any
arbitrary task order, respectively [9], where
M is the number of processors. A polynomial-
time approximation scheme (PTAS) is also
developed for partitioned scheduling [4].
In this work, we are interested in real-time
task scheduling over multiple processors with
different access latencies for different mem-
ory modules. It is motivated by the popularity
of non-uniform memory access architecture
[8], [24], [27], for which the worst-case
execution time of a task depends on its memory
allocation. Because of the considerations
in non-uniform memory access architecture,
the state-of-the-art results in partitioned
scheduling could not be directly applied to
the target problem of this work. Recent re-
lated work is the exploring of task scheduling
over heterogeneous processors with different
memory footprints for tasks on different
processors, where a global shared memory pool
is considered [18]. Another work studies the
task memory allocation with multiple memory
pools for real-time environments [20], but
only single processor is considered.
In this work, we consider the task schedul-
ing over homogeneous processors with a shared
fast memory pool and a shared slow one. Given
a set of implicit-deadline sporadic tasks
with worst-case CPU execution times and mem-
ory page access frequencies, the problem is to
decide the memory page allocation (in the fast
or slow memory pool) and the task assignment
(to the processors) so that the maximum
utilization of the processors is minimized.
If the maximum utilization of the derived
solution is no more than 100%, the system is
guaranteed to meet the timing constraints.
This work develops an algorithm to solve
this problem by two phases: (1) In the first
phase, we decide memory allocation to derive
a lower bound of the maximum utilization,
regardless of task allocation; (2) in the
second phase, we use worst-fit partitioning
under the LUF order and any arbitrary policy
to assign tasks based on the derived memory
allocation. It is shown that our algorithm
with the LUF ordering has a tight (2 − 2M+1 )-approximation bound, and our algorithm with
any arbitrary ordering has a tight (2 − 1M )-approximation bound. Moreover, we introduce
the mechanism of resource augmentation [10]
to quantify the failure when our algorithm
does not return a feasible solution. A case
study with the SPEC2000 benchmarks and a set
of extensive simulations are also conducted
to evaluate the performance and limitation of
the proposed algorithm.
3 多核心省電多媒體排程(EHSHM)
3.1 系統模型
In this work, we are interested in energy-
efficient real-time scheduling of a multime-
dia task, such as that of H.264 decoding,
over multiple homogeneous cores with the
Dynamic Voltage and Frequency Scaling (DVFS)
capability. The power consumption function
P (s) of a DVFS core has two parts Pd(s) and
3
nature, we adopt an even distribution of
workloads over cores to estimate the energy
consumption of a job so that the energy
consumption might be minimized. In such a way,
wk becomes α
√∑M
j=1(
Ck
M )
α = Ck
α
√
M1−α (instead of
the optimal case), where Ck is the predicted
total number of execution cycles of Jk. As a
result, θi could be set as Ci∑i+b−1
k=i Ck
(di+b−1 − t)
for the execution of Ji when its deadline
is not considered. When the deadline of Ji
is considered, the duration θi allocated to
execute Ji, denoted as Ti, should be set as
min{ Ci∑i+b−1
k=i Ck
(di+b−1 − t), (di − t)}.
After the determination of the duration Ti
for the execution of the target job Ji, the
second phase focuses on the derivation of
the subjob assignment and speed assignment
of Ji over M cores: Let Ji be the target
job to execute at time t. The Largest-Task-First
strategy is adopted to partition subjobs into
cores since the Largest-Task-First strategy
provides a 1.13-approximation algorithm for
frame-based task scheduling over a multipro-
cessor environment (when α is 3, and required
speeds are available)1. With the Largest-
Task-First strategy, subjobs are first sorted
according to their required execution cycles
in a non-increasing order. Subjobs are then
assigned to cores one after another, where
a subjob is assigned to the core with the
minimum total execution cycles so far. Once
the subjob assignment is done, the speed of
the jth core is set as the closest available
speed which is no less than wi,jTi . The timecomplexity of the two-phase algorithm, as
summarized in Algorithm 1, is O(B + ni log ni),
where ni is the number of subjobs of Ji.
Lemma 1: Given b jobs {Ji, Ji+1, . . . , Ji+b−1} which
should be done by a relative deadline d, the energy consump-
tion of the schedule derived by Algorithm 1 is no more than
1.13 times of that of an optimal schedule when any required
speeds are available.
Proof: Since the difference between the
schedule derived by Algorithm 1 and the
optimal one results from the convexity of
the power consumption function P (), the worst
case occurs when α is 3. Therefore, we only
need to consider the case where α is 3 in our
proof.
Let EMLTF and EOPT denote the energy
consumption of the schedule derived by Al-
gorithm 1 and that of an optimal schedule,
respectively. Since EMLTF is no less than
1. This property has been proven in Theorem 5 in [5]
Algorithm 1: MLTF
Input: The current time t, and the pending jobs in the
buffer Ji, Ji+1, . . . , Ji+b−1.
Output: The subjob assignment xi and the speed
assignment si.
1 Ti ← min{ Ci∑i+b−1
k=i Ck
(di+b−1 − t), di − t};
2 xˆi,j ← ∅ and wi,j ← 0 for j = 1, 2, . . . ,M ;
3 Sort subjobs of Ji such that c(i,p) ≥ c(i,q) if p < q;
4 for k = 1 to ni do
5 find wi,m where wi,m ≤ wi,j for any j ̸= m;
6 xˆi,m ← xˆi,m ∪ {J(i,k)} and wi,m ← wi,m + c(i,k);
7 Set sˆi,j as the closest available speed which is no less
than wi,jTi for j = 1, 2, . . . ,M ;
8 return xi, si;
EOPT, we have the following inequality:
EMLTF
EOPT
=
∑i+b−1
k=i
∑M
j=1 P
(wMLTFk,j
TMLTF
k
)
TXMLFk∑i+b−1
k=i
∑M
j=1 P
(wOPT
k,j
TOPT
k
)
TOPTk
≤
~∑i+b−1k=i ∑Mj=1 (wMLTFk,j )3 1(θMLTF
k
)
2+Mβ·d
~∑i+b−1k=i ∑Mj=1 (wOPTk,j )3 1(θOPT
k
)
2+Mβ·d
≤
∑i+b−1
k=i (w
MLTF
k )
3(θMLTFk )
−2∑i+b−1
k=i (w
OPT
k )
3(θOPTk )
−2 = γ1,
where wMLTFk,j and wOPTk,j are the number of
execution cycles on the jth core during the
execution of Jk in the schedule derived
by Algorithm 1 and the optimal schedule,
respectively. The worst case occurs when
Algorithm 1 allocates the duration TMLTFk forthe execution of Jk as θMLTFk and according tothe subjob assignment where wLBk,j is CkM . Thus,
γ1 can be rephrased as follows.
γ1 =
∑i+b−1
k=i (w
MLTF
k )
3
(
wLBk∑i+b−1
l=i
wLB
l
d
)−2
∑i+b−1
k=i (w
OPT
k )
3
(
wOPT
k∑i+b−1
l=i
wOPT
l
d
)−2
=
(
∑i+b−1
l=i w
LB
l )
2∑i+b−1
k=i
(wMLTFk )
3
(wLB
k
)2
(
∑i+b−1
l=i w
OPT
l )
2
∑i+b−1
k=i
(wOPT
k
)3
(wOPT
k
)2
= γ2
According to Theorem 5 in [5], we know that
(wMLTFk )
3 ≤ 1.13(wLBk )3 ≤ 1.13(wOPTk )3 for any job
Jk. Therefore, the proof can be completed
according to the following inequality.
γ2 ≤
(
∑i+b−1
l=i w
LB
l )
2∑i+b−1
k=i 1.13
(wLBk )
3
(wLB
k
)2
(
∑i+b−1
l=i w
OPT
l )
2
∑i+b−1
k=i w
OPT
k
= 1.13
(
∑i+b−1
k=i w
LB
k )
3
(
∑i+b−1
k=i w
OPT
k )
3
≤ 1.13
Lemma 2: The total expected duration to execute the pend-
ing jobs in the buffer at any time t under Algorithm 1 is no
less than 50% of that of an optimal schedule.
Proof: If b is less than B, the available
total duration for the executions of the b
5
The relative deadline of each instance of
task τi is equal to pi (i.e., its period) if
it is a periodic real-time task; otherwise,
the deadline is the minimum separation time
between its consecutive requests. The technical
problem is on how to minimize the entire booting time of a
given task set without violating both the run-time performance
requirement and the given capacity limitation of NOR flash.
Here the booting time of a task set is defined as
the total loading time of tasks from secondary
storage devices into the DRAM. For the clear-
ness of the following algorithm illustrations
and property proofs, we only consider real-
time tasks in our problem definition, and
how to include non-real-time and aperiodic
tasks in our fast booting framework without
violating any constraint and definition. The
Maximization of the Booting-time Saving (MBS)
Problem is defined as follows:
Definition 1: The Maximization of the Booting-time Saving
(MBS) Problem:
Consider a set T of real-time tasks running on a platform with
S bytes of NOR flash. The problem is to pick up a subset TF
of tasks from T to run directly on NOR flash such that the
reduced booting time is maximized, the total space required
by tasks in TF is no more than S, and all the tasks meet their
respective deadlines.
The MBS problem can also be formulated
with the following integer linear programming
problem, where the Earliest-Deadline-First
(EDF) scheduling algorithm [13] is adopted
for the task scheduling:
maximize
∑
τi∈T
li · xi, (3a)
subject to
∑
τi∈T
si · xi ≤ S, (3b)
∑
τi∈T
(eDi
pi
+
eFi − eDi
pi
· xi
) ≤ C, (3c)
xi ∈ {0, 1}, ∀τi ∈ T, (3d)
where xi = 1 means that τi is selected to run
directly on the NOR flash, and vice versa.
In the problem definition, Equation (3b) is
for the flash-size constraint, and Equation
(3c) shows the schedulability constraint.
Note that the least upper bound of the
utilization factor of the EDF algorithm is
100% [13]. Thus, when T consists of all tasks
in the system, the available CPU utilization
C can be set as 100%. For the rest of this
paper, Ω = (T, S, C) is used to denote an MBS
problem instance. A solution to the problem
instance is denoted as Ψ(Ω) = {xi | τi ∈ T},
and B(Ψ(Ω)) =∑τi∈T li ·xi denotes the saving ofthe booting time of a solution Ψ(Ω). For the
rest of this paper, we shall only consider
feasible solutions, where each task meets
its deadline, and the required NOR size of
a solution does not exceed S. A solution Ψ(Ω)
is optimal if it is feasible and has the
maximum saving in the booting time of all the
feasible solutions.
Theorem 2: The MBS problem is NP-hard.
Proof: The NP-hardness can be proved by
a reduction from the knapsack problem [16].
For an instance of knapsack problem, we
can construct a task τi for each item in
the knapsack problem instance such that its
required flash memory size si and reduced
booting time li are set as the weight and the
profit of the item, respectively. Moreover,
by setting pi and eFi properly such that∑
τi∈T
eFi
pi
≤ 1, the real-time constraint of the
constructed MBS problem is always satisfied
(it means that the schedulability constraint
of Equation (3c) can be ignored). Thus, a
solution that optimizes the special case of
the MBS problem is also an optimal solution
to the knapsack problem instance.
4.1 An Optimal Algorithm - A Dynamic Programming
Approach
In this section, we propose an optimal
algorithm with dynamic programming for the
MBS problem. We shall first show that the MBS
problem is of the optimal substructure property
and then present an optimal algorithm based
on the property. A problem is of the property
if the removing of a solution component, i.e.,
the task selection xi of any task τi in an MBS
problem instance, from an optimal solution
to a problem instance Ω would still result
in an optimal solution to the subproblem
by removing the corresponding items, i.e.,
τi, from Ω. Such an observation over the
property provides the rationale behind the
to-be-proposed algorithm.
Lemma 3: The MBS problem is of the optimal substructure
property.
Proof: Given an MBS instance Ω = (T, S, C),
suppose that Ψ∗(Ω) is an optimal solution to
the instance. Let x∗i be the task selection
for some arbitrary task τ∗i in Ψ∗(Ω), and
Ω′ = (T′, S′, C ′) be one of its subproblems by
removing items related to τ∗i . That is, T′ =
T\{τ∗i }, S′ = S−si ·x∗i , and C ′ = C−( e
D
i
pi
+
eFi −eDi
pi
·x∗i ).
We shall show that Ψ∗(Ω) \ {x∗i } (that denotes
7
of accessed memory pages, where |Pi| is the
cardinality of the set Pi, and (3) the worst-
case number Ai,h of the memory accesses to
page pi,h in Pi.
Without loss of generality, we order the
pages of each task τi such that Ai,h ≥ Ai,g when
h ≤ g. If a page pi,h is allocated in the fast
memory pool, the worst-case latency to access
this page is Lf · Ai,h; otherwise, the worst-
case latency is Ls · Ai,h. In order to derive
the parameters Ci, Pi, and Ai,h for each task
τi, the static analysis for the worst cases
has to consider two phases. The first phase
explores only the worst-case CPU execution
time (by ignoring memory access latencies) to
derive Ci, in which the worst-case execution
time analysis, such as the results in [23],
can be applied. In the second phase, we
have to identify the worst-case number of
the accesses to a memory page. However, the
second phase has to explore all the paths
for deriving the worst-case number of the
accesses to an individual page.
5.2 問題定義
For such a platform, to meet the timing
constraints, there are two major challenges:
(1) allocating memory pages for the task
execution and (2) partitioning tasks onto
the available processors. We assume that a
processor stalls when accessing memory, e.g.,
the system does not apply mechanisms such as
Hyper-Threading [15]. Such an assumption is
widely adopted, e.g., [19], to improve the
timing predictability of tasks.
This work considers partitioned schedul-
ing, in which a task is assigned on a
processor. Therefore, to decide whether the
tasks assigned on a processor can meet their
timing constraints, it has been shown that
the necessary and sufficient condition is
to test whether the total utilization of these
tasks is no more than 100% by applying the
earliest-deadline-first (EDF) strategy [14],
where the utilization of a task is defined
as its worst-case execution time divided
by its period (minimum inter-arrival time).
Therefore, if a subset P′i of the memory page
set Pi of task τi is assigned to the fast
memory pool, the utilization ui of task τi
is Ci+
∑
pi,h∈Pi′ Ai,hLf+
∑
pi,h∈Pi\Pi′ Ai,hLs
Di
. We do not
discuss this scheduling in details, as the
standard EDF policy is applied to give the
task instance with the earliest absolute
deadline the highest priority [14].
This work studies how to assign memory pages
onto slow/fast memory pools and partition
tasks onto processors such that the maximum
utilization among the processors is mini-
mized. Therefore, if the maximum utilization
is no more than 100%, the solution is feasible
to meet the real-time constraints. We define
the problem as follows:
Definition 2: The Minimization of the Maximum Utiliza-
tion (MMU) Problem:
Consider a set T of implicit-deadline sporadic tasks running
on a platform withM homogeneous processors, a fast memory
pool with Ff frames, and a slow memory pool with Fs
frames. The problem is to allocate a memory frame for each
page for the task execution and to partition the task set
onto the processors such that the maximum utilization of the
processors is minimized, the required amount of frames of
each memory pool does not exceed the size, and all tasks
meet their deadlines.
For the rest of this work, we assume that
the total space of the two memory pools can
accommodate all the memory pages; otherwise,
no feasible solution exists. The following
theorem shows the complexity of this problem.
Theorem 3: The MMU problem is NP-hard in the strong
sense.
Proof: The special case in which no memory
accesses are required is the traditional
schedulability problem for periodic real-time
tasks with implicit deadlines in partitioned
scheduling over multiprocessor systems, which
is NP-hard in the strong sense [17].
As the MMU problem is NP-hard, we look for
polynomial-time approximation algorithms in
this work, in which an algorithm is said
to be with an α-approximation bound if it
guarantees to derive a solution that is at
most α times of the maximum utilization in
the optimal solution.
6 工作分群及分頁配置機制
This section presents our algorithm for
the MMU problem. The algorithm includes
two phases. In the first phase, presented
in Section 6.1, we design a polynomial-
time algorithm to allocate pages onto the
memory pools and derive a lower bound of
the optimal solution of the MMU problem. In
the second phase, presented in Section 6.2,
we use an algorithm to partition tasks onto
the processors based on the page allocation
in the first phase. With the above simple
strategies, we will show that the proposed
algorithm has a 2-approximation bound for the
MMU problem.
9
Steps 4 to 13.
To reduce the time complexity, we can first
sort the pages in Pall with respect to values
of Aj,hDj . For each task τi under considerationsin the forall loop, we can duplicate the sorted
list and remove the pages associated with task
τi. In the loop from Step 4 to Step 13, it is
not difficult to see that an iteration for a
given ℓ takes amortized O(N) time complexity
by implementing Step 4 and Step 9 in an
incremental manner. That is, for a given ℓ,
the values ℓj and kj are recorded so that
they can be used in the next iteration when
ℓ is set to ℓ + 1. Therefore, the overall
time complexity with such an implementation
is O (N2Ff + PallN + Pall logPall).The following
theorem shows the optimality of the derived
solution from the LBM algorithm.
Theorem 4: The LBM algorithm derives the optimal solu-
tion for page allocation.
Proof: For a given tuple ℓ and τi, under the
assumption that task τi has exactly ℓ pages in
the fast memory pool and is of the maximum
utilization among all the tasks in T, we
will prove that the iteration of the loop
in Algorithm 3: either (1) shows that the
predicate is incorrect or (2) derives the
page allocation to minimize the lower bound
under the assumption.
For the iteration of the LBM algorithm, the
ℓ most frequently used pages of τi are selected
to the fast memory pool in Step 4. The
LBM algorithm also reduces the utilization
of all the other tasks by selecting the most
frequently used pages of each task to the
fast memory pool until τi has the highest
utilization among all the tasks. According to
Lemma 5, the LBM algorithm uses the minimum
number of the frames in the fast memory
pool to achieve the assumption that task τi
exactly has ℓ most frequently used pages in
the fast memory pool and is of the maximum
utilization among all the tasks. Therefore,
if the LBM algorithm fails the test in Step 6,
the assumption of this iteration must be
incorrect.
Otherwise, the remaining frames of the
fast memory pool, i.e., Frm frames,
are used to accommodate the pages in
{pj,h | τj ∈ T \ {τi} , ℓj < h ≤ |Pj |} to reduce the
average utilization, i.e.,
∑
τj∈T uj
M . There-fore, to minimize the average utiliza-
tion by selecting only Frm pages in
{pj,h | τj ∈ T \ {τi} , ℓj < h ≤ |Pj |}, it is not dif-
ficult to see that the solution by selecting
the Frm pages with the largest Aj,hDj is optimal.As the LBM algorithm iterates all possible
combinations of ℓ and τi, it is clear that the
minimum solution among all these combinations
is the optimal solution for page allocation.
6.2 工作分配
This subsection proposes a worst-fit ap-
proach to partition an implicit-deadline spo-
radic task set onto identical processors. The
Worst-Fit-Partition (WFP) algorithm first
sorts all tasks in a non-increasing order of
their utilization based on the result of the
LBM algorithm. Tasks are then sequentially
assigned to the processor with the currently
lowest utilization. Algorithm 4 illustrates
the pseudo-code of the WFP algorithm. For the
sorting in Step 2, it takes time complexity
O(N logN), by assuming U ℓii is also calculatedin Step 1. For the loop from Steps 4 to
6, a heap can be used to get the processor
with the minimum utilization. As a result,
excluding Step 1, the time complexity of
the WFP algorithm is O(N(logN + logM)). The
following theorems derive the approximation
bound of the WFP algorithm and show that the
bound is tight.
Theorem 5: The WFP algorithm is a (2 − 2M+1 )-
approximation algorithm for the MMU problem.
Proof: Suppose that OPT is the maximum
utilization among the processors in the op-
timal solution for the MMU problem, where LB
is the optimal solution for page allocation
derived in the LBM algorithm. By Theorem 4,
LB ≤ OPT. (5)
Let ℓi be the number of pages for task
τi ∈ T in the fast memory pool by applying the
LBM algorithm. Let cj∗ be the processor with
the maximum utilization after the WFP algo-
rithm is invoked, where τk is the last task
assigned to processor cj∗. According to the
worst-fit and largest-task-utilization-first
strategies of the WFP algorithm, if k ≤ M,
LB is the maximum utilization among the
processors in the solution. As the LBM algo-
rithm provides the optimal solution for page
allocation (with the minimum lower bound for
the MMU problem), the WFP algorithm always
returns the optimal solution when k ≤M. For
the rest of this proof, we consider only the
case k ≥M + 1.
With the definition of task set Tj for j =
1, 2, . . . ,M, before assigning task τk, we know that∑
τi∈Tj∗ U
ℓi
i ≤
∑
τi∈Tj U
ℓi
i for any cj ∈ C, which
11
be assigned to a processor with other M − 1
tasks. As a result, the utilization of the
processor is M−1M + 1 = 2− 1M .
6.3 資源擴張分析
If the maximum utilization of the solution
derived from the WFP algorithm is no more
than 100%, it is clear that the solution is a
feasible solution for the given MMU problem
instance, as the schedulability condition for
EDF is to ensure no more than 100% utilization
on a processor. The major issue is to answer
what can be guaranteed when the WFP algorithm
fails to return a solution with the maximum
utilization no more than 100%.
We adopt the concept of resource augmenta-
tion bound [10] here. For an algorithm with a
ρ resource augmentation bound of the MMU problem,
the algorithm guarantees that the solution derived
from the algorithm is always feasible by speeding up the
system (including the processor speed and the memory access
latency) to ρ times as fast as the original platform, if there
exits a feasible solution for the original problem instance.
The following theorem shows the resource
augmentation bound of the WFP algorithm.
Theorem 7: The WFP algorithm has a (2− 2M+1 ) resource
augmentation factor.
Proof: For an MMU problem instance, if
there exits a feasible solution, the solution
must have the maximum utilization no more
than 100%. Moreover, we know that the maximum
utilization of the solution derived from the
WFP algorithm is no more than (2− 2M+1 ) timesof the optimal solution. By speeding up the
processor speed and the memory access latency
by a factor (2− 2M+1 ), the resulting solutionhas maximum utilization no more than 100%.
In other words, if the WFP algorithm does not derive
a feasible solution, the input instance cannot be feasibly
scheduled by slowing down to 1
2− 2M+1
of the original platform
speeds (including the processors and the memory pools).
7 多核心異質記憶體模組的工作分群排
程之實驗分析
Based on the Intel SCC platform [24], this
section conducts evaluations by considering
48 identical processors and four DRAMs with
total size 16 Giga-Bytes as the slow memory
pool. In the following experiments, the
processors/cores are with 800 MHz, and the
access latency to the slow memory pool (Ls) is
bounded by 13.1 µ-seconds.3 For the fast memory
pool, to evaluate the impact of different
sizes of the fast memory pool, we consider
3. The latency bound was measured from Intel SCC in our experiments.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
gcc ammp twolf art mfc mixed gzipN
o
rm
al
iz
ed
 m
ax
im
u
m
 u
ti
li
za
ti
o
n
Slow-Arbitrary
Slow-LUF
WFP-Small
WFP-Large
Fig. 1. The performance evaluation with benchmarks.
two different configurations: (1) 384 Kilo-
Bytes (the total size of the shared message
passing buffer of the Intel SCC testbed) and
(2) 6144 Kilo-Bytes (16 times of 384 Kilo-
Bytes). For the latency to access the fast
memory pool, if not specified, we assume that
it is 4 times faster than that to access the
slow memory pool, i.e., LsLf = 4. A page/framehere is configured as 4 Kilo-Bytes.
We consider four solutions as follows:
• Slow-Arbitrary: It only uses the slow memory
pool and uses the worst-fit algorithm
for task assignment. This gives a (2− 1M )approximation bound without considering
the feature of the fast memory pool.
• Slow-LUF: It only uses the slow memory
pool and uses the worst-fit algorithm with
LUF task ordering. This gives a ( 43 − 13M )approximation bound without considering
the feature of the fast memory pool.
• WFP-Small: It is the algorithm proposed in
Section 6 with a fast memory pool of 384
Kilo-Bytes.
• WFP-Large: It is the algorithm proposed in
Section 6 with a fast memory pool of 6144
Kilo-Bytes.
In the first set, we evaluate these four
solutions based on the task sets generated
from SPEC2000 benchmarks. To have variant
execution times and required pages of tasks,
we use trace-based experiments here so that
a task τi has known Ci, Ai,h,Pi based on its
execution trace with a specific input. For
each configuration, we generate 100 inde-
pendent tasks. Each of the configurations
with gcc, ammp, twolf, art, mfc, and gzip has
100 sporadic tasks of a specified applica-
tion. The benchmark mixed is constructed by
including 20 tasks from each of gcc, ammp,
twolf, art, and mfc benchmarks. For each task
τi, the CPU utilization is set as a uniform
distribution in [1%, 10%] so that the minimum
inter-arrival time is Ci divided by the CPU
utilization. Figure 1 reports the maximum
utilization results of the solutions, nor-
malized to the results of Slow-Arbitrary. Except
the gzip benchmark, our algorithm with joint
considerations of page allocation and task
13
[7] R. I. Davis and A. Burns. A survey of hard real-time scheduling for
multiprocessor systems. Accepted for publication in ACM Computing
Surveys, 2010.
[8] S. Dighe, S. R. Vangal, P. Aseron, S. Kumar, T. Jacob, K. A. Bowman,
J. Howard, J. Tschanz, V. Erraguntla, N. Borkar, V. K. De, and S. Borkar.
Within-die variation-aware dynamic-voltage-frequency-scaling with op-
timal core allocation and thread hopping for the 80-core TeraFLOPS
processor. IEEE Journal of Solid-State Circuits, 46:184–193, January
2011.
[9] R. L. Graham. Bounds on multiprocessing timing anomalies. SIAM
Journal on Applied Mathematics, 17(2):416–429, 1969.
[10] B. Kalyanasundaram and K. Pruhs. Speed is as powerful as clairvoyance.
Journal of the ACM, 47(4):617–643, 2000.
[11] S. Kato, N. Yamasaki, and Y. Ishikawa. Semi-partitioned scheduling of
sporadic task systems on multiprocessors. In ECRTS, pages 249–258,
2009.
[12] H. Leontyev and J. H. Anderson. Generalized tardiness bounds for
global multiprocessor scheduling. In RTSS, pages 413–422, 2007.
[13] C.-L. Liu and J. W. Layland. Scheduling algorithms for multiprogram-
ming in a hard-real-time environment. Journal of the ACM, 20:46–61,
1973.
[14] C. L. Liu and J. W. Layland. Scheduling algorithms for multiprogram-
ming in a hard-real-time environment. Journal of the ACM, 20(1):46–61,
1973.
[15] D. T. Marr, F. Binns, D. L. Hill, G. Hinton, D. A. Koufaty, J. A.
Miller, and M. Upton. Hyper-threading technology architecture and
microarchitecture. Intel Technology Journal, 6(1):1–12, 2002.
[16] S. Martello and P. Toth. John Wiley & Sons, Inc., 1990.
[17] A. K. Mok. Fundamental design problems of distributed systems for the
hard-real-time environment. Technical report, Cambridge, MA, USA,
1983.
[18] M. Niemeier, A. Wiese, and S. Baruah. Partitioned real-time scheduling
on heterogeneous shared-memory multiprocessors. In ECRTS, 2011.
[19] R. Pellizzoni, A. Schranzhofer, J.-J. Chen, M. Caccamo, , and L. Thiele.
Worst case delay analysis for memory interference in multicore systems.
In DATE, pages 741–746, 2010.
[20] H. Takase, H. Tomiyama, and H. Takada. Partitioning and allocation of
scratch-pad memory for priority-based preemptive multi-task systems.
In DATE, pages 1124–1129, 2010.
[21] Y. Tan, P. Malani, Q. Qiu, and Q. Wu. Workload prediction and dynamic
voltage scaling for mpeg decoding. In Proceedings of the Conference on
Asia South Pacific Design Automation (ASP-DAC’06), pages 911–916,
2006.
[22] Texas Instruments. Davinci-dm644x evaluation module technical refer-
ence, March 2007.
[23] H. Theiling, C. Ferdinand, and R. Wilhelm. Fast and precise wcet
prediction by separated cache andpath analyses. Real-Time Syst.,
18:157–179, May 2000.
[24] R. F. van der Wijngaar, T. G. Mattson, and W. Haas. Light-weight com-
munications on intel’s single-chip cloud computer processor. SIGOPS
Operating Systems Review, 45:73–83, February 2011.
[25] V. V. Vazirani. Approximation Algorithms. Springer, 2001.
[26] T. Wiegand, G. Sullivan, G. Bjontegaard, and A. Luthra. Overview of
the h.264/avc video coding standard. IEEE Transactions on Circuits and
Systems for Video Technology, 13(7):560–576, July 2003.
[27] C. Yan, F. Dai, and Y. Zhang. Parallel deblocking filter for h.264/avc
on the tilera many-core systems. Lecture Notes in Computer Science,
6523:51–61, 2011.
15
1. Summary in Conference Events 
 
16th Asia and South Pacific Design Automation Conference  
(ASP-DAC) was held between January 25 and 28 in Yokohama, Japan in 
2007. ASP-DAC 2011 was hosted by Dr. Kunihiro Asada, as the General 
Chair, and Prof. Hyunchul Shin, as the Program Chair. ASP-DAC is the 
best conference in the Asian and Pacific Region for computer-aided 
design and automation. Every year, there are hundreds of professors, 
researchers, engineers, and graduate students from around the world to 
attend the great event! In 2011, there are totally 13 tracks, and 30 paper 
sessions. Paper presentations are arranged mainly in four parallel tracks. 
In ASP-DAC 2011, I organized an invited session on designs issues for 
embedded systems and present a paper at the conference.  
 
I left for Yokohama, Japan, on January 27 and arrived on the same 
day. On January 27 afternoon. I attended sessions on high-level 
simulation and high-level and logic synthesis. I also attend the banquet at 
night. On January 28 morning, I chaired an invited session on designs 
issues on embedded systems. I invited the ACM SIGDA Chair Patrick 
Madden to talk about parallel computation for embedded systems, Prof. 
Tatsuo Nakajima to talk about virtualization, and Prof. Jen-Wei Hsueh to 
talk about power-aware scheduling for FPGA. I also presented 
energy-efficient strategies in data transmission over 3G modules for 
mobile devices. In the past decades, there are strong demands for mobile 
devices with networking and/or telecommunication capabilities, such as 
mobile Internet devices (MID's). An example market growth is 300% 
plus for MID's from 2008 to 2010. Moreover, it is believed that all 
smartphones will be equipped with mobile networking functionalities, 
critical design issues of multi-core operating systems and design tools of 
embedded systems. Methodologies in the minimization of energy 
consumption and the optimization of real-time performance were also 
presented. Approaches on the library design for the minimization of 
communication overheads and co-simulation tool designs for multi-core 
embedded systems were then presented. 
 
ASP-DAC is the most important and major forum in the hosting of 
researchers and engineers in computer-aided design and automation in 
the Asian and Pacific Region. It also attracts a lot of embedded systems 
researchers in recent years. It is an excellent platform in setting up 
forums for people with multiple disciplines in the academics and the 
industry. During the conference and the workshop after that, I did have a 
lot of chances to discuss with professors and experts for future 
collaboration. The opportunity in attending ASP-DAC provides me a 
great chance to see more research in CAD/IP designs and embedded 
system designs. The experience will help me in identifying future 
research trends in the field of embedded systems. 
 
2. Conference Proceeding and Related Documents 
 
The conference proceeding and the CD ROM of ASP-DAC 2011 was 
brought back for future reference. 
four parallel tracks. In DAC 2011, we have a regular paper on 
flash-memory management systems being accepted. The audience of this 
conference represents the decision-makers at all levels of the buying 
process from the leading semiconductor, computer, telecommunication, 
and consumer electronics companies.  No place like DAC could provide 
such a platform with the same combination of influential buyers, press 
coverage, exposure to further the sales and marketing objectives of 
companies, and researchers in the academia. 
 
The first speech is given by Steve Wozniak from Fusion-io on June 6. 
He and Steve Jobs founded Apple Computer, Inc. to market Wozniak's 
Apple I personal computer. He is now a Chief Scientist for Fusion-io. 
Steve provided an insight into the vision that started the largest and most 
successful technology company in the work (by now). The second speech 
was delivered by Lisa Su (A senior vice president and general manager of 
networking and multimedia for Freescale Semiconductor) on June 7. She 
joined  Freescale in June 2007 as chief technology officer to lead 
Freescale’s technology roadmap and global research and development 
(R&D) operations. Before that, she was the vice president of the 
Semiconductor Research and Development Center of IBM. She talked 
about the trends that drives embedded multicore innovation. In particular, 
she pointed out that the exponential growth of the mobile device market 
is requiring new innovation throughout embedded devices and the 
supporting infrastructure to keep pace with the growing demand. She 
covered topics ranged from the evolution of the network infrastructure, 
the growth of multi-purpose embedded devices, the increased trend 
towards heterogeneous system-on-chip (SOC) integration, and the 
balancing with the market realities of maintaining system development 
after that, I did have a lot of chances to discuss with professors and 
engineers from different fields. The opportunity in attending DAC 
provides me a great chance to see more research in CAD/IP designs and 
embedded system designs. The experience will help me in identifying 
future research trends in the field of embedded systems. 
 
2. Conference Proceeding and Related Documents 
 
The conference proceeding and the CD ROM of DAC 2012 was 
brought back for future reference. 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：郭大維 計畫編號：97-2221-E-002-206-MY3 
計畫名稱：多核心嵌入式系統之即時省電程序排程 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 11 0 100%  
博士生 5 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
