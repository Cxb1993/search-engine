I 
 
行政院國家科學委員會補助專題研究計畫 □期中進度報告5期末報告
 
整合雲端運算與行動裝置之適應性互動視覺追蹤系統 
 
 
計畫類別：5個別型計畫   □整合型計畫 
計畫編號：NSC 100－2221 －E －130 －024－ 
執行期間：100 年 08 月 01 日至 101 年 07 月 31 日 
 
執行機構及系所：銘傳大學資訊傳播工程學系 
 
計畫主持人：黃博俊 
共同主持人：謝朝和 
計畫參與人員：楊哲偉 
 
 
 
 
本計畫除繳交成果報告外，另含下列出國報告，共 _1_ 份： 
□移地研究心得報告 
5出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，5一年□二年後可公開查詢 
 
中   華   民   國 101  年 10  月 31  日
II 
 
表目錄 
表 1 臉部與眼部準確率................................................................................................................... 16 
表 2 投票法則 3x3 混淆矩陣........................................................................................................... 16 
表 3 投票法則 3x3 準確率............................................................................................................... 17 
表 4 比例原則 3x3 混淆矩陣........................................................................................................... 17 
表 5 比例原則 5x5 準確率............................................................................................................... 18 
 
 
 
IV 
 
中文摘要 
本計畫研究非接觸式視覺追蹤系統，研究  降低其複雜度，提出投票法則與比率原則兩種方法，
達成計畫中：非侵入式(non-intrusive)、安裝簡單(less-setup time)、限制較少如頭部可自由擺動(free head 
movement)、操作方便(easy operating)等目標。除驗證其整體正確率證明此兩種方法在一些應用上是可
行的，並將視覺追蹤應用在三個方面: 機器人控制、互動遊戲-打地鼠、閱讀者對電子式廣告之瀏覽路
徑分析。此外將投票法則移植佈署於智慧型行動裝置中，也藉由機器人控制、互動遊戲-打地鼠兩個應
用情境，驗證其可行性。此計畫之成果，將有助於利用視覺追蹤系統，進行降低使用者干擾之分析，
提昇分析之有效數據，此外實現於智慧型行動裝置，可以提供更多視覺追蹤系統之使用情境。 
 
關鍵詞：視覺追蹤系統、投票法則、比率原則、智慧型行動裝置、電子式廣告 
6 
 
報告內容 
前言： 
而近年來，隨著攝影機取像精細度的提升，互動科技亦愈多研究著墨於研發眼睛互動的軟硬體技
術，一般而言，是將攝影機架設於螢幕前或電子式廣告刊版上，以取得使用者觀看影像之眼球移動影
片，並利用影像處理技術來取得對映之使用者視覺焦點並進行追蹤及行為分析，此議題及應用正廣泛
的受到重視與討論，如網頁與視覺實用性的檢測、利用眼球偵測來進行的文字輸入法、用來偵測駕駛
人的開車狀況，利用眼睛的焦點移動或開闔眼，來做為操控輸入的依據[1-3]。 
目前視覺追蹤系統已應用在許多領域，用以達成更友善的使用環境，以期增進互動成效，例如應
用在學習上，不但是一個自然且不影響學習的有效方法，且其得到之科學數據，亦可輔以探究學生之
認知歷程；在廣告設計上，可用來研究消費者對廣告訊息注視、理解到購買決定之間的行為模式，亦
可將主商品受注視度利用數據或是圖形視覺化，提供給廣告商檢討其是否須對廣告內容陳列或位置做
調整，以獲得最大之廣告利益；而在傳播行為上，用以探討媒體內容(contents)與接收者(receivers)之間
的關連性，及訊息解讀的過程。眼睛是一個很靈敏的器官，如果能以眼睛為互動來源，對於一些無法
隨心所欲控制的人來說，如肢體障礙、植物人、中風患者…等，不僅更直覺也更加便利。 
目前已發表之應用大都須藉助於昂貴的眼球偵測與追蹤設備，並不普及，因此本計畫以使用簡易
網路攝影機輸入設備為互動裝置，提出高效率並具適應性之運算法則，並且提出適當複雜度的行動裝
置視覺追蹤系統，可以整合應用於學習、廣告設計、身障者控制與復健等，有利於擴展在其他應用領
域的互動整合視覺追蹤系統。 
研究目的： 
本計畫建置適應性互動視覺追蹤系統為目標。第一部分主要針對視覺追蹤系統之實現，提出對系
統比較有意義之特徵值，並考慮未來實現於行動裝置與雲端之可能架構，可以整合應用於學習、廣告
設計、身障者控制與復健等，並且有利於擴展在其他應用領域的互動整合視覺追蹤系統。第二部分主
要目標為，整合視覺追蹤系統於行動裝置上包括智慧型手機與平板電腦，考慮採用 Andorid 平台為研
發應用系統。一般而言，智慧型手機與平板電腦除在資訊處理能力差異外，螢幕及攝影機之解析度也
會有很大差異，本計畫提出將視覺追蹤系統，實現於運算能力及系統資源都有限之設備上。 
目前視覺追蹤系統相關技術中，已發表文獻的研究環境大都在計算能力較佳，或是使用像素較高
的攝影機環境下，目標為提高辨別準確度，因此演算法的計算複雜度通常不是主要考慮因素，但是一
般行動裝置通常設備較為簡易，也就是資訊處理能力及攝影機的條件通常比較差，因此相對於已經發
表的一些法則，本計畫要研究的視覺追蹤系統是更為一般性，條件更為嚴苛。提出在行動裝置上設計
一個視覺追蹤系統，滿足基本系統效能，是本計畫在此議題的主要目標，計畫執行中先透過分析視覺
追蹤系統中關鍵技術，再研究如何簡化法則複雜度，提出適當複雜度的行動裝置視覺追蹤系統，並且
以行動學習與廣告兩種應用來驗證系統效能。 
文獻探討： 
在人機互動介面的領域中，視覺追蹤系統逐漸成為探討議題，許多的視覺評估演算法已經被發展
8 
 
(ubiquitous learning)已成為現今最新一代的學習趨勢[9]。結合定位系統與地圖之位置相關服務
(location based service)，如導航，結合擴增實境提供學習、娛樂等服務，結合感知元件進行遠距看
護或是生理監測等。圖 1 為應用服務運作示意圖。 
 
圖 1 應用服務運作示意圖 
 
隨著互動科技的發展，使用者已越來愈習慣利用肢體動作或手勢與電腦直接進行互動，相應而生
的除了有互動遊戲市場外，隨之發展的還有互動式電子廣告應用。傳統平面廣告已逐漸無法滿足尋求
刺激、求新求變的消費者，為了吸引消費者目光，近期的網路廣告、街頭電子牆廣告、櫥窗展示在設
計上都趨於利用互動廣告設計，試圖把握使用者短暫停留或等待的時段，將廣告資訊傳達給消費者，
利用互動式裝置，增加廣告與消費者之間的溝通，以雙向溝通的方式，廣告商亦可確保其訴求已成功
傳達給消費者，但在商品訴求、成本、環境的考量下，並非每個廣告都能或是適合以互動的方式呈現，
這時，在僅有的廣告篇幅中，主商品的擺放位置就顯得重要許多。 
隨著生活型態的改變，平面商務廣告逐漸消失，取而代之的是朝向電子網路廣告發展，廣告的
媒介與多媒體行銷概念也不斷推陳出新。訊息傳播的方式也從廣告商的單向式傳輸，逐漸演變成互
動式的雙向訊息交換，除了增加廣告的趣味性外，廣告商亦可開發潛在顧客群，廣告是一種說服性
的傳播，以人性為出發點，利用各種表現形式，獲得消費者的注意(attention)及興趣(interest)，使消
費者被說服或灌輸企業形象，進而激起消費者產生商品需求性，提升消費者的購買慾望，因此，廣
告被視為是一種說服性的藝術，藉由有限的篇幅、簡短的文字、鮮明的商品圖片構成一種意象，喚
起消費者共鳴。商業廣告其傳播媒介由早期至今，其傳播媒介依序由手寫文字、報紙、郵票、電台
廣播、電視到現在的網路傳播等類型，如今，廣告已和我們的生活密不可分，隨處可見廣告在我們
的身邊，且伴隨著網路及平板電腦、智慧型手機的發展，只要能連上網路，廣告已無分國界。 
依照人類視覺慣性而言，注視時間一般是以使用者視線(gaze direction)近進入某興趣區(ROI)至
離開區域的時間總合，因此許多研究都在探討閱讀者視覺路徑及區域凝視時間，以分析排版、顏色
以及圖文比例對人類視覺閱讀習慣的差異性。在傳統上探討人類對於某類特定事項的喜好程度，為
透過「晤談」(interview)、「放聲思考」(think aloud)等方法加以推測[10]，此外亦有根據德國心理學
家 J. Cohn 使用之問卷調查的方式[11]進行。然而，使用問卷調查的方式太過主觀，不但容易受到
記憶力偏誤的影響，也可能無法真實反應個體未曾意識到之內在認知歷程，因此問卷調查的方式測
量的結果，會影響數據之正確性。 
10 
 
 
圖 3 系統架構圖 
A. 初始化 
在初始化階段，會將輸入影像色彩空間由 RGB 轉換為灰階色彩空間 ( Gray )，經色彩空間換為灰
階色彩空間後，可作為特徵追蹤影像輸入使用。 
B. 特徵追蹤 
(1) 臉部追蹤與眼部追蹤 
將接收到的灰階影像使用 Viola and Jones 提出的積分影像（Integral Image）作為臉部偵測的特徵
擷取[20]。先使用 OpenCV 所提供之 haarcascade_frontalface_alt.xml 進行臉部追蹤，再將所追蹤之臉部
影像使用所提供之 haarcascade_eye.xml 進行眼部追蹤。 
(2) YCbCr 膚色偵測 
以華人而言可以使用膚色濾波器將眼睛瞳孔濾出，因此接著將取得之眼部影像轉換色域，再將其
從 RGB 轉換成 YCbCr，以找出瞳孔位置。因為瞳孔顏色與膚色眼白在 YCbCr 中差異性大故可將膚色
的地方濾除，以留下眼球位置，包含虹膜（iris）跟瞳孔（pupil），如圖 4 所示。 
 
圖 4 去除膚色區域 
在本研究中所設定之膚色門檻值為 RCr=[133,173] 和 RCb=[77,127]，每張影像都需透過被挑選出
眼睛候選區，然後再進行膨脹與侵蝕、門檻值二值化等步驟，流程如圖 5 所示[17]，為減少臉部誤判機
率，臉部影像須大於原始影像 50%才會再進行眼部偵測，為減少眼部偵測誤判，將其搜尋範圍定為臉
部上 2 分之 1，而其眼部區域大小定為大小大於寬 80 像素、長 60 像素之區域，滿足上述條件後選取
出的眼部區域即為眼睛候選區域。 
12 
 
(4) 門檻值二值化 
在取得正確眼睛區域後，對此區域之灰階影像使用門檻值進行二值化，可將影像差異度大的角膜
與虹膜分離，使用門檻值參數為 20 至 30 之間，本計畫將門檻值參數設定為 24 結果如圖 7 所示。 
 
圖 7 門檻值二值化取得虹膜區域 
C.  視覺對映 
經過特徵追蹤後可以取得視覺對映所需輸入影像，在本計畫中共提出兩個視覺對映方法，其一為
投票法則，其二為比例原則。投票法則為利用眼睛影像灰階值的變化，進行水平與垂直方向的視線
位置預測；而比例原則是利用瞳孔中心移動範圍的平面與對映的視覺平面產生的一個比例對映關
係。 
二、投票法則 
在傳統的視覺追蹤方法都需要先經過一個複雜的程序才能進行視覺焦點的估算，本計畫研究簡化
以增進使用視覺追蹤系統的便利性，在本計畫中提出之投票法則是利用眼部影像的灰階度變化，以投
票的方法量化原始灰階值，藉以找出其凝視位置，故可以不需要校正方法就能進行視線位置的預測。 
對於凝視點的對應物件，通常相對於影像之像素大很多，因此只要在合理誤差範圍內，皆會對應
到相同物件，因此本計畫提出 MxN 物件與投票法則，以降低運算複雜度，此方法可以應用於電子廣告
媒體分析、控制器以及智慧型行動裝置，流程如圖 8 所示，可分為三個階段：初始化、水平方向視線
位置預測與垂直方向視線位置預測。因為眼睛在移動時在水平方向的移動量較大，所以先預測水平方
向的視線位置區域，再透過垂直方向視線位置預測，找出垂直方向的視線位置。 
 
圖 8 投票法則流程圖 
14 
 
Step 7. 投票方式為找出水平方向線段權重值最低者 
 
C. 垂直方向視線位置預測  
Step 1. 在該水平預測區域 Wti 畫出垂直中心線 VLj, for j=1,…M. 
Step 2. 每一區塊垂直方向線段切割為 N+2 個線段 VLj-k, k=1,…N+2 
Step 3. 計算出垂直區間平均值 VLj-k 
Step 4. 轉換為權重值計算方式同水平方向權重值取得方式(step 5~step 7) 
Step 5. 再計算垂直方向每一區塊 VLj權重值總和，其總和值最低區塊即為視線預測位置 
三、比例原則 
因投票法則方法只能針對較簡單的情境或目標使用，故本計畫另提出一較高準確率的比例原則方
法，以改善原投票法則準確率的不足，及在 4 個角點誤判率較高的情形。首先在取得虹膜二值化影像，
進行凝視點估算的動作。如圖 10 其方式為取得眼睛影像中所有黑色像素點的座標位置，將這些點的
座標位置進行平均的動作，進而估出眼球中心位置，如式（5）P（X, Y）定義為瞳孔位置，而 B 為
影像中所有黑色像素點座標之集合。 
∑ = =∈= Nb nNBbN YXbYXP 1 ; ),(),(  式（5） 
在本系統初始化階段，會先請使用者做三次的校正點測試，除用以確定使用者有進入到測試區域
之外，亦可利用這三點校正點作為瞳孔移動量及視覺焦點座標轉換比例原則，此三點校正點依序為 A、
B、C，如圖 10 所示，B、C 點當作瞳孔最遠距離移動量之參考依據。G ( x, y )為視覺焦點平面而 E ( x, 
y )為瞳孔中心移動平面。也就是利用看 B 點時的瞳孔中心座標和 C 點時的瞳孔中心座標之距離，再
將其對映到螢幕上 B 點與 C 點之相對距離。 
 
圖 10 視覺焦點示意圖 
16 
 
表 1 臉部與眼部準確率 
 輸入張數 成功判斷張數 準確率 
臉部辨識率 10340 9616 93% 
眼部辨識率 9616 9135 95% 
整體辨識率 10340 9135 88% 
二、投票法則實驗結果分析 
A. 投票法則 
在投票法則實驗中會請受測者觀看螢幕上出現的十字標記，如圖 12 所示，而十字標記的位置為隨
機產生。在本實驗中每位受測者皆須接受 3 次的測試，在本次的實驗中共有 10 位受測者平均年齡為
24 歲 8 位男性 2 位女性。 
 
圖 12 實驗情境 
 
在表 2 中利用混淆矩陣( Confusion Matrix ) 來分析實驗的數據值，從實驗結果得知誤判的點大多
為其鄰近的點，表 3 中為各個位置的準確率數據，準確率的計算方式為該位置正確預測總數／該位置
預測總數。在表 3 中可發現 1、3、7、9 這四個角點誤判率又明顯高於其他點，整體的準確率為 75.5%。
整體準確率為 66.4%，其計算方式為投票法則整體準確率 x 臉部與眼部準確率。 
表 2 投票法則 3x3 混淆矩陣 
 真實的值 
預測的值 1 2 3 4 5 6 7 8 9 
1 20 2 0 2 0 0 0 0 0 
2 4 23 6 0 2 0 0 0 0 
3 0 2 19 0 0 2 0 0 0 
4 5 0 0 25 1 0 4 0 0 
5 1 3 1 2 26 1 2 2 1 
6 0 0 4 0 1 26 0 0 4 
7 0 0 0 1 0 0 19 1 0 
8 0 0 0 0 0 0 5 26 5 
9 0 0 0 0 0 1 0 1 20 
 
18 
 
在表 5 中為比例原則 5x5 之實驗準確率，在此一實驗中總判斷成功次數為 588，總數為 750，判斷
準確率為 78.4%，再加上人臉部與眼部準確率計算後整體準確率為 68.9%。 
表 5 比例原則 5x5 準確率 
位置 1 2 3 4 5 
準確率 76% 80% 73% 83% 80% 
位置 6 7 8 9 10 
準確率 76% 80% 70% 80% 83% 
位置 11 12 13 14 15 
準確率 76% 80% 80% 70% 76% 
位置 16 17 18 19 20 
準確率 80% 80 73% 83% 80% 
位置 21 22 23 24 25 
準確率 80% 83% 73% 76% 83% 
 
三、視覺追蹤系統之應用 
本研究將視覺追蹤應用在三個方面，分別為: 機器人控制、互動遊戲-打地鼠、閱讀者對電子式廣
告之瀏覽路徑分析。 
A. 應用於 Android 平板操控樂高機器人 
利用投票法則取得視線焦點，做為控制樂高機器人之訊號，其架構如圖 14。首先透過平板電腦前
鏡頭擷取使用者影像，然後再將截取之影像透過特徵追蹤與投票法則，計算出視覺焦點所對應之控制
鈕，分別將投票法則回傳值 2、4、6 與 8 作為上左右下移動的控制訊號，而 5 做為停止控制訊號。 
 
圖 14 Android 平板操控樂高機器人示意圖 
B. 互動遊戲-打地鼠 
利用投票法則取得視線焦點，做為控制樂高機器人之訊號，其架構如圖 15。當智慧型行動裝置前
端接收到後段之程式所傳送的數據時，行動裝置前端將判讀出使用者眼球的注視點，計算出視覺焦點
所對應之區塊，再以打地鼠遊戲達到跟使用者互動的效果，最後由行動裝置前端判定有無擊中地鼠，
並給予回饋分數。使用者可藉由遊戲畫面上方的視窗檢視攝影機所截取的臉部畫面，確保臉部影像能
被鏡頭完整截取，以便系統能正確的判讀出眼球位置，進而對眼球注視點做追蹤和判讀，按下開始遊
戲鍵後，地鼠將從遊戲佈局中的地洞隨機出現，同時鏡頭將不斷截取影像，若判讀結果和地鼠出現相
對位置符合時，地鼠將從彩色圖片替換成黑白圖片，做為擊中的回饋反應，如圖 16。 
20 
 
其二為對於此則廣告注視時間較長的資訊為圖片或內容，本研究之整體實驗流程如圖 18 所示。 
 
圖 17 實驗測試用電子式廣告 
 
圖 18 實驗流程圖 
本次實驗所收集到的資料中，包含有總體觀看廣告時間、視覺凝視時間與問卷調查結果，以下將
利用這些資訊作探討及分析。受測者閱讀電子式廣告時間分析，每位受測者平均時間如圖 19 所示，可
以發現受測者在閱讀單一則電子式廣告時，平均所需要的時間大約是在 19.9 秒左右。 
 
圖 19 總觀看時間平均值 
22 
 
在問卷調查結果中，在圖文注視時間比例上，88%的受測者認為花費在閱讀文字內容的時間較長，
而有 12%的使用者認為閱讀圖片的時間較長，如圖 23 所示。其結果與所收集到之實驗數據有 12%的落
差，但大致而言受測者閱讀文章內容的時間是遠大於觀看圖片。 
  
圖 23 問卷調查之自覺閱讀起始位置 
結論 
本計畫研究非接觸式視覺追蹤系統，研究降低其複雜度，提出投票法則與比率原則兩種方法，達
成計畫中：非侵入式(non-intrusive)、安裝簡單(less-setup time)、限制較少，例如:頭部可自由擺動(free head 
movement)、操作方便(easy operating)等目標。 
除驗證其整體正確率外，並將視覺追蹤應用在三個方面: 機器人控制、互動遊戲-打地鼠、閱讀者
對電子式廣告之瀏覽路徑分析。未來將將以建置整合雲端運算與行動裝置之適應性互動視覺追蹤系統
為研究發展目標。 
 
 
 
24 
 
附錄 
 
 
 
 
coordinates of the eyes. From the biological point of view, 
the iris is a circular structure in the eye, responsible for 
controlling the diameter and size of the pupil thus the amount 
of light reaching the retina.  “Eye color” is the color of the 
iris that which can be green, brown or blue.  And the other 
parts structure visible is white sclera as shown in fFigure 1. 
Hence, that will beit is feasible easily to distinguish between 
iris and sclera in image process by using pixel gray value of 
pixel. 
Figure 1. Human eye cross-sectional view grayscale [16] 
When looking at a target, the eye must be rotate around a 
vertical axis so that the projection of image is in the center of 
the retina in both eyes.  In case of looking towards the left, 
the right eye’s iris movement larger than left eye’s iris 
movement. On the other hand, looking towards the right, the 
amount of movement in left eye is greater.  Therefore, we 
proposed a method to estimate the visual moving direction 
based on biological characteristic. Figure 2 shows the gaze-
tracking algorithm with PVS.  
Figure 2. Flow chart of the gaze-tracking algorithm 
In this paper, the eye detection is carried out by using the 
method based on haar-like feature [17].  After finding the 
eye image, take the horizontal AB from the center of image as 
in the vertical dimension shown in Figure 3.  
          
(a)                                             (b) 
Figure 3. Shows (a) left eye (b) right eye take the 
horizontal AB
This may be a problem using the center line of image in 
the vertical dimension to get AB . Based on Figure 4, the AB
may not pass through the center of iris. So AB  needs to be 
regulated in this case. If all values of MVi are more close to 
255(white) or skin color, the AB will move up/down Į
percentage. In this paper, Į is set to 10. The Figure 5 
represents the regulated lines (i.e. red line).  
Figure 4.  AB does not go through the iris center. 
Figure 5.  Calibrate AB  to red line 
The screen is divided into N*M areas, where N along the 
horizontal dimension and M along the vertical dimension. 
According to Figure 3, Let AB  be divided into K*N equal 
segments, where K is an odd number that can be used to set 
the number of polling district. And then the mean value (MVi)
of gray value in each part can be obtained, where i is from 1 
to KN. Considering a grayscale eye image, the pixel value 
range from 0 (black) to 255 (white). The each parts of mean 
value will also during 0 to 255.  So we used N-1 dynamic 
thresholds to quantify the mean values, as shown in (1). The 
Max and Min represent two extreme pixel values in AB . Q is 
set to be dimension N.  
ThQ-j = Min + j*[(Max-Min) / Q] , until Q-j =1  (1) 
For example, in case of N=3, if a mean value below the 
Th2 it is replaced with 1. Else a mean value above the Th1 it 
is replaced with 3, otherwise it is replaced with 2.  Each part 
is examined. 
Finally, we sun up the number of K successive value of 
MVi to be voting weight (i.e. Wn), where n = 1, 2, …N.  For 
instance: if K = 3, then MV1 + MV2 + MV3 = W1, MV4 + 
MV5 + MV6 = W2 ... MVKN-2 + MVKN-1+ MVKN = WN.
When the voting weight Wn is obtained, and then the 
smallest value of Wn indicates the iris location.  Hence, we 
can use this result to estimate the visual translation direction.  
Figure 6 shows the eye gaze estimation procedure.  The 
larger of variable K can improve accuracy, but increases the 
computation.  Based on Figure 6 (a) the AB  is got first 
including regulating procedure. And the pixel values of 
AB can be obtained as shown in Figure 6 (b). In order to 
simplify the voting process, the mean of KN parts is 
computed as shown in Figure 6 (c). Based on Figure 6 (d), 
the quantifying is executed to reduce the computation. 
Finally, the voting weight Wn can be obtained to determine 
the iris location easy. 
III. EXPERIMENTAL RESULT
Our gaze detection system was developed in Android 3.0 
with 1G Hz processor.  We used the Built-In web cam with a 
resolution of 2.0 M at a frame rate of 15fps.  The size of the 
eye image is about 100¯50 pixels.. In experiments 
environment, user hold the smart phone and let the camera 
338
[11] D. Torricelli, S. Conforto, M. Schmid and T. D’Alessio, ”A neural-
based remote eye gaze tracker under natural head motion”, Comput. 
Meth. Prog. Bio, pp. 66–78, 2008. 
[12] J.G. Wang, E. Sung, and R. Venkateswarlu, “Estimating the Eye 
Gaze from One Eye”, Computer Vision and Image Understanding, 
Vol. 98, No. 1, pp. 83-103, Apr. 2005. 
[13] A. Meyer, M. Bo¨hme, T. Martinetz, and E. Barth, “A Single-Camera 
Remote Eye Tracker”, Perception and Interactive Technologies, pp. 
208-211, 2006. 
[14] Hirotake Yamazoe, Akira Utsumi, Tomoko Yonezawa, Shinji 
Abe, ”Remote gaze estimation with a single camera based on facial-
feature tracking without special calibration actions”, Proceedings of 
the 2008 symposium on Eye tracking research & applications, pp. 26-
28, March. 2008. 
[15] Chiao-Wen Kao, Che-Wei Yang , Kuo-Chin Fan, Bor-Jiunn Hwang, 
Chin-Pan Huang,” AN Adaptive Eye Gaze Tracker System in the 
integrated Cloud Computing and Mobile Device,”ICMLC,July 2011 
[16] Human eye cross-sectional view grayscale Original source: 
http://www.nei.nih.gov/health/macularhole/index.asp 
[17] Takeshi Mita , Toshimitsu Kaneko , Osamu Hori ,” Joint Haar-like 
Features for Face Detection,” Proceedings of the Tenth IEEE 
International Conference on Computer Vision ,2005. 
Figure 6.  The procedure of estimating gaze direction 
   
AB
340
 Center features. 
 
 
Fig. 1. General overview of the components of eye and gaze tracker 
 
(A)
(B)
(C)
 
Fig. 2. Haar-like shape features sets  
(A) Line features (B) Edge features (C) Center features 
 
The eye features are similar to the facial structure, so we 
also used Haar-like features to detect eyes. The face and eyes 
detection results are shown in Fig. 3. 
 
 
Fig. 3. Face and eyes detection 
 
Nevertheless, the eyes detection results may have missed 
caused by marking mouth. The eye candidates’ positions is 
satisfied the facial structure. Therefore, the follow processes, 
namely correction process, are proposed to determine the eye 
candidates accurately. 
(i) According to the facial structure, the eyes are located at 
region of 2/3 along the vertical dimension usually.  
(ii) The procedures is converting eye candidates to YCbCr 
color space and then using the skin color filter to 
remove skin pixels. In other words, skin color threshold, 
RCr=[133,173] and RCb=[77,127], is used to redefine 
the region of eye candidates. Fig. 4 shows the 
correction process.  
(iii) Finally, two more fitting regions of eye candidate are 
founded. Fig.5 shows the rectified result of eye 
detection. 
B. Voting scheme 
After locating the positions of eyes, Voting scheme is 
executed to estimation a gaze position on the monitor. Fig. 6 
exhibits the flowchart of estimating gaze position comprising 
three macro function blocks, Initial Stage, Predict horizontal 
position of iris center and Predict vertical position of iris 
center. And which are described as following. 
 
 
Fig. 4. Correction process 
 
 
Fig. 5. The rectified eye detection 
 
 
Fig. 6. Flowchart of estimating gaze 
 
Initial Stage: 
Step 1. From the biological point of view, it will be 
feasible to distinguish between iris and sclera by 
using grayscale. Therefore, the detected eye color 
image is converted to grayscale to estimate iris 
center position. 
Step 2. The object in full-screen is divided into M*N 
Face 
Detection
Eye
 Tracking
Gaze 
Estimation
Application
Eye 
Detection
Input Image
Initial eye position
Gaze coordinates
Eye location
Eye features
Eye tracker 
Eye candidate  by 
using Haar-like feature
Non-Skin 
region
Skin region
Using skin color filter
eye image correction
Initial Stage
Color conversion
Divide image into 
M*N blocks
Input eye detected image
Horizontal Center 
Line
Line 
segments
Feature 
Extraction
Compute 
Vote 
Weighting
Estimate
Horizontal 
Position 
Horizontal position 
of iris center prediction
Vertical Center 
Line
Line 
segments
Feature 
Extraction
Compute 
Vote 
Weighting
Estimate
Gaze
 Position 
Vertical position 
of iris center prediction
  
Fig. 9. Vertical projection of each line segment (x-axis: pixels of line 
segment; y-axis: gray scale, range of values is [0,255]) 
 
 
Fig. 10. Mean of each line segment pixel values 
 
 
Fig. 11. Sum of voting weight of each line segment  
 
 
Fig. 12. Candidate of horizontal position of iris center 
 
 
Fig. 13. Estimation horizontal position of iris center 
 
 
Fig. 14. Compute vote weight of each line segment 
 
III. EXPERIMENTAL RESULTS 
In this section, the experimental tests are given to evaluate 
the performance of proposed Voting scheme. The 
functionalities of tests are implemented by OpenCv on a 
3.4GHz 4-GB PC environment.  
We have evaluated the proposed method by three cases, as 
shown in Fig. 15. Case 1: White background, black target 
object. Case 2: Black background, white target object Case 3: 
White background, random target object color. The distance 
between participant and camera is about 50~80 cm. And the 
test block is emerged randomly with using red cross in the 
block center to attract the subject.  
The experiment results are obtained by 15 participants to 
test each case in 3 times, and summarized in TABLE I. Based 
on TABLE I, the average accuracy is higher than 80% in case 
of 3*3 blocks. But when full-screen is divided into more than 
3*3 blocks the accuracy is reduced. 
 
 
Fig. 15. Three cases in evaluated proposed method 
 
TABLE I 
THE PERFORMANCE OF THE PROPOSED APPROACH IN EACH 
CASE 
MxN Case 1 Case 2 Case 3 Average 
3x3 85.33% 84.11% 83.33% 84.25% 
5x5 66.66% 61.33% 64.75% 64.25% 
 
 
 
表 Y04 
出席國際學術會議報告 
                                                            101 年 3 月 20 日 
報告人姓名  黃博俊 服務機構及職稱 
銘傳大學資訊傳播工程學系 
副教授 
     時間 
會議 
     地點 
自民國 101 年 3 月 14 日
至 1 0 1 年 3 月 1 6 日
  香港 
本會核定
補助文號
教務處電子公文 43168 號 
會議 
名稱 
 (中文) 2012 年 IMECS 國際研討會 
 (英文) International MultiConference of Engineers and Computer Scientists 2012 (IMECS 2012) 
發表 
論文 
題目 
 (英文) A Novel with Low Complexity Gaze Point Estimation Algorithm 
 (英文) Through a Web Camera Base of Eye Tracking Technology to Explore 
the Audience’s Attention Preferences in terms of the Positions of 
Information and the Layout Compositions             
報告內容應包括下列各項： 
一、參加會議經過 
    此次參加 IMECS 2012 國際學術研討會，除會中發表兩篇研究論文，並參加多
場專題演講。 
二、與會心得 
    這次非常感謝國科會補助參加 IMECS 2012 國際學術研討會，與會來賓來自世
界多個國家，國內亦有多所學校及業界專家學者參加；本人於會中除發表兩篇研究
論文外，並聆聽多場精采專題演講及學術論文發表，於發表論文中，跟多位與會學
者所提出意見進行討論，因此得到對問題的不同解決方法與觀點；在參加會場中得
以認識多位學者，跟他們交換研究心得。 
    此回參與國際學術研討會後，讓本人除了肯定研究成果外，也獲得其他學者不
同的觀點解決問題，並從聆聽他人研究成果中，得到一些新的研究想法，可說是獲
益良多。 
三、考察參觀活動(無是項活動者省略) 
 
四、建議 
 
五、攜回資料名稱及內容 
會議論文光碟一份。 
 
六、其他 
 
附件三
 
  
Abstract—In this paper, a novel with low complexity gaze 
point estimation algorithm in unaware gaze tracker is proposed 
which is suitable in normal environment. The experimental 
results demonstrate our proposed method is feasible and has 
acceptable accuracy. Besides, the proposed method has less 
complexity in terms of camera calibration process than 
traditional method. 
 
Index Terms—Gaze Point Estimation; Unaware Gaze 
Tracker; Voting Scheme 
 
I. INTRODUCTION 
nteractive Installation is the most popular issue in recent 
years. Such as using Hand Gesture, Human Posture, Eye 
Detection, Gaze Tracking, Speech recognition to control the 
computer, device or play games. The Gaze tracking can be 
used in many applications such as web usability, advertising, 
sponsorship or in communication systems for disable people. 
Numerous techniques of eye gaze trackers have been 
developed [1-13]. These eye gaze tracker found in literature 
can be divided into two groups, intrusive techniques and 
non-intrusive techniques, respectively. Intrusive methods 
usually use special devices to attach the eye skin or wear 
head-mounted to catch the user’s gaze in very close to the 
eyes [1].  
The most widely used current designs for eye trackers are 
using a non-contacting video camera to focus on eyes and 
records their movement. Compared with intrusive methods, 
the non-intrusive methods have the advantage of being 
comfortable during the process of gaze estimation [13]. 
Video-based eye trackers typically use the corneal reflection 
and the iris center as feature to track over time [2-12]. 
The gaze calibration procedure that identifies the mapping 
from pupil parameters to screen coordinates using neural 
network has become more popular for eye gaze tracker. 
Baluja and Pomerleau proposed a neural network method 
 
Manuscript received December 30, 2011; revised January 17, 2012.  
Chiao-Wen Kao is with the Department of Computer Science and 
Information Engineering, National Central University, Taoyuan, Taiwan. 
(e-mail: chiaowenk@gmail.com). 
Bor-Jiunn Hwang is with the Department of Computer and 
Communication Engineering, Ming Chuan University, Taoyuan, Taiwan. 
(e-mail: bjhwang@ mail.mcu.edu.tw). 
Kuo-chin Fan is with the Department of Computer Science and 
Information Engineering, National Central University, Taoyuan, Taiwan. 
(e-mail: kcfan@csie.ncu.edu.tw). 
Che-Wei Yang is with the Department of Computer and Communication 
Engineering, Ming Chuan University, Taoyuan, Taiwan. (e-mail: 
yang.bng86@gmail.com). 
Chin-Pan Huang is with the Department of Computer and 
Communication Engineering, Ming Chuan University, Taoyuan, Taiwan. 
(e-mail: hcptw@mail.mcu.edu.tw). 
without explicit features [3]. Each pixel of the image is 
considered as an input parameter of the mapping function. 
Once the eye is detected, the image of the eyes is cropped and 
then used as the input of ANN (Artificial Neural Network). In 
[9], authors proposed remote eye gaze tracker based on eye 
feature extraction and tracking by combining neural mapping 
(GRNN) to improve robustness, accuracy and usability under 
natural conditions. 
For 3D model-based approaches, gaze directions are 
estimated as a vector from the eyeball center to the iris 
centers [8]. A stereo camera system is constructed for 3D eye 
localization and the 3D center of the corneal curvature in 
world coordinates.  Points on the visual axis are not directly 
measurable from the image. By showing at least a single 
point on the screen, the offset to the visual can be estimated. 
The intersection of the screen and the visual axis yield the 
point of regard. 
The purpose of this paper is to propose a novel with low 
complexity gaze point estimation algorithm in unaware gaze 
tracker and which is suitable in normal environment.  
The remainder of the paper is organized as follows. In 
section II, the proposed Voting scheme algorithm is 
presented. The gaze evaluation model and results are carried 
out in section III. Finally, the paper ends with our conclusions 
with discussion and recommendations for future work in 
section IV. 
II. PROPOSED VOTING SCHEME ALGORITHM 
A gaze tracker is used to acquire eye movements. A 
general overview of the gaze tracker is shown in Fig. 1, 
comprising Face Detection, Eyes Detection, Eyes Tracking 
and Gage Estimation. Eyes Detection and Gaze Estimation 
are important functionality for many applications including 
driver’s physical condition analysis, helping disabled people 
operate computer, auto stereoscopic displays, facial 
expression recognition, and more. The eye positions should 
be calculated first to estimate the person’s gaze coordinates. 
This section describes an algorithm for tracking gaze 
direction on the screen. 
A. Preprocessing 
Several preprocessing steps must be done before 
performing gaze tracking, as shown in Fig. 1. Firstly, 
detecting face in image is a fundamental task for surveillance 
system. This paper use Haar-like Features which firstly 
proposed by Paul Viola and Jones to detect the face [14] [15]. 
Haar-like features are digital image features used in object 
detection and recognition. . Each classifier uses K rectangular 
areas to make decision which the region of the image likes 
predefined image or not. Fig. 2 exhibits the Haar-like shape 
features sets including Line features, Edge features and 
A Novel with Low Complexity Gaze Point 
Estimation Algorithm 
Chiao-Wen Kao, Bor-Jiunn Hwang, Che-Wei Yang, Kuo-Chin Fan, Chin-Pan Huang 
I 
 blocks, where N along the horizontal dimension and 
M along the vertical dimension. 
Step 3. Divide detected eye images into the same number 
of blocks. 
 
Predict horizontal position of iris center: 
Step 1. To get the center line of vertical dimension in each 
block, HBLij, for i=1,…N, j=1,…M. 
Step 2. Divide HBLij into N equal line segments, HBLij-k, 
k=1,…N. 
Step 3. Compute the vertical projection and mean of the 
HBLij-k, respectively. 
Step 4. Adaptive thresholds (Th) are obtained to quantify 
the mean values according to the method in 
[11-12].The quantified mean value Qij-k of each line 
segment is computed by (1). 
  
(1) 
Where ⌊x⌋ denotes the nearest integers less than or 
equal to x. y and ybase represent maximum and 
minimum mean values of HBLij-k, respectively. 
Step 5. Sum of the quantified mean value, Sik, is computed 
by (2) 
  
 (2)
 
S={Sik for i=1…N, k=1…N}
 
Step 6. Initial voting weight Wtik. The set SN is composed 
by the lowest of N values in S, where 
 
(3)
 
The block weights Wti are obtained by summing of the 
voting weight as (4). 
 (4)
 
Step 7. Finally, to find maximum value of Wti to determine 
the iris center of horizontal.  
 
Therefore, the candidate of horizontal position of iris 
center can be found by using the Voting scheme. 
 
Predict vertical position of iris center:  
Step 1. It’s a great similarity between getting the vertical 
and horizontal position. To get the center line of 
vertical dimension in Wti which computed by (3), 
VLj, for j=1,…M. 
Step 2. Divide VLj into N+2 line segments on average, 
VLj-k, k=1,…N+2. From the biological point of view, 
vertical eye movement is smaller. Therefore we 
divided segment into more detail in order to 
improve the accuracy.  
Step 3. Compute the horizontal projection and mean of the 
VLj-k, respectively. 
Step 4. Repeat the step 5~step 8 in Horizontal position of 
iris center prediction procedure. 
Step 5. Finally, select maximum value of VLj to represent 
the iris center of vertical in this block.  
 
Based on these procedures of Voting scheme, we can 
estimate the gaze position on the screen facilely. For example, 
assume the test object in full-screen is divided into 3*3 
blocks as shown in Fig.7. And the gray scale eye image is 
also divided into 3*3 blocks. Thus, we can get 9 center line 
segments in the blocks, as shown in Fig. 8. 
The results of computing the vertical projection and mean 
of each line segment of Fig. 8 are shown in Fig. 9 and Fig. 10, 
respectively. Based on Fig. 10, the quantified mean value and 
sum of the quantified mean value are performed by (1) and 
(2), respectively, the results are shown in Fig. 11. Initial 
voting weight is performed by (3) and then summing of the 
weights by (4), the results are shown in Fig. 12. The 
candidate of horizontal position of iris center is determined 
by selecting the lowest of three values as shown in Fig. 13. 
 
 
Fig. 7. Divide full-screen advertisement into 3*3 blocks 
 
 
Fig. 8. Example of divide grayscale eye image into 3*3 blocks 
 
The purpose of vertical position estimation is to determine 
the horizontal candidate. As experimenting, brightness spots 
on the iris that maybe influence the vote result. Hence, in the 
Voting scheme, more divided segments in vertical are 
performed to improve accuracy. Fig. 14 shows the estimation 
result, and the vertical position of iris center is determined in 
the block 5. 
   1Q  Thyy baseij-k
  
M
j kijik
Q
1
S





otherwiseWt
SSifWt
ik
Nikik
,0
 ,1
 
 
N
k iki
WtWt
1
 
 IV. CONCLUSION 
We have surveyed several categories of eye tracking 
systems from the different methods of detecting and tracking 
eye images to computational models of eyes for gaze 
estimation and gaze-based applications. However, most of 
systems setup increases have higher both the complexity and 
cost. Stated thus, we propose a novel unaware method, 
namely Voting scheme, to estimate gaze tracking based on 
appearance-manifolds. In this system, the user only sits in 
front of a computer and use the webcam on the monitor to 
capture the user’s image sequences. This method first 
calculates the histogram of grayscale eye image and use 
dynamic thresholds to quantify the pixel values. Then gaze 
direction on the screen can be predicted by using voting 
scheme. The experimental results demonstrate the 
effectiveness of proposed gaze tracking approach. Based on 
this, we have tried to find out how people look at content of 
website or advertisement. However, some problems still need 
to be solved. Firstly, the proposed method cannot deal with 
low resolution image sequences. In addition, the blurred or 
bad illuminated image sequences could affect the tracking 
result. Future work will be to deal with those problems and 
achieve more robust algorithm. 
 
ACKNOWLEDGMENT 
This work is supported by the National Science Council in 
Taiwan. The project contract number is NSC 
100-2221-E-130-024-. 
REFERENCES 
[1] Craig A. Chin, Armando Barreto et al, “Integrated electromyogram and 
eye-gaze tracking cursor control system for computer users with motor 
disabilities,” Journal of Rehabilitation Research & Development, 
Vol.45, Num.1 2008. 
[2] T. Cornsweet, H. Crane, “Accurate two-dimensional eye tracker using 
first and fourth Purkinje images”, Journal of the Optical Society of 
America, vol. 63, pp.921-928, 1973. 
[3] S. Baluja and D. Pomerleau, “Non-intrusive gaze tracking using 
artificial neural networks”,  School of Computer Science, CMU, CMU 
Pittsburgh, Pennsylvania 15213, Jan. 1994. 
[4] Shota Miyazaki1, Hironobu Takano1 and Kiyomi Nakamura, “Suitable 
Checkpoints of Features Surrounding the Eye for Eye Tracking Using 
Template Matching,” SICE Annual Conference, Step. 2007 
[5] Shinji Yamamoto and V.G. Moshnyaga, “Algorithm Optimizations for 
Low-Complexity Eye Tracking,” IEEE International Conference on 
Systems, Man, and Cybernetics, October 2009 
[6] Yali Li, Shengjin Wang, Xiaoqing Ding, “Eye/eyes tracking based on a 
unified deformable template and particle filtering,” Pattern 
Recognition Letters ,January 2010 
[7] Diego Torricelli, Michela Goffredo, Silvia Conforto, Maurizio 
Schmi,”An adaptive blink detector to initialize and update a 
view-based remote eye gaze tracking system in a natural 
scenario,”Pattern Recognition Letters,June 2009 
[8] Hirotake Yamazoe, Akira Utsumi, Tomoko Yonezawa, Shinji Abe, 
“Remote gaze estimation with a single camera based on facial-feature 
tracking without special calibration actions,” Proceedings of the 2008 
symposium on Eye tracking research & applications, pp. 26-28,March. 
2008 
[9] D. Torricelli, S. Conforto, M. Schmid and T. D’Alessio,”A 
neural-based remote eye gaze tracker under natural head 
motion”,Comput. Meth. Prog. Bio, pp. 66–78, 2008. 
[10] Hu-Chuan Lu, Guo-Liang Fang, Chao Wang and Yen-Wei Chen, “A 
novel method for gaze tracking by local pattern model and support 
vector regressor,” Signal Processing, vol. 90, issue 4, April 2010, 
pp.1290-1299. 
[11] Chiao-Wen Kao, Che-Wei Yang , Kuo-Chin Fan, Bor-Jiunn Hwang, 
Chin-Pan Huang,” AN Adaptive Eye Gaze Tracker System in the 
integrated Cloud Computing and Mobile Device,”ICMLC,July 2011 
[12] Chiao-Wen Kao, Che-Wei Yang, Kuo-Chin Fan, Bor-Jiunn Hwang, 
Chin-Pan Huang,” Eye gaze tracking based on pattern voting scheme 
for mobile device,” Instrumentation, Measurement, Computer, 
Communication and Control, Oct. 2011 
[13] Dan Witzner Hansen, Qiang Ji, “In the Eye of the Beholder: A Survey 
of Models for Eyes andGaze,” IEEE Transactions on Pattern Analysis 
and Machine Intelligence, vol. 32, No. 3, pp. 478-500, Jan. 2010. 
[14] Viola and Jones, "Rapid object detection using boosted cascade of 
simple features", Computer Vision and Pattern Recognition, 2001 
[15] Takeshi Mita, Toshimitsu Kaneko,Osamu Hori,”Joint Haar-like 
Features for Face Detection,”Proceedings of the Tenth IEEE 
International Conference on Computer Vision,2005. 
100年度專題研究計畫研究成果彙整表 
計畫主持人：黃博俊 計畫編號：100-2221-E-130-024- 
計畫名稱：整合雲端運算與行動裝置之適應性互動視覺追蹤系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
