3.4 Delay Sensitive Bandwidth Guarantee EPON DBA Scheme .............................. ５０ 
3.5 Self-coupling time Synchronization in the Sensor Network ................................ ５５ 
3.6 Routing Scheme the Sensor Network .................................................................. ５７ 
4 Performance Evaluation ............................................................................................... ６７ 
4.1 Evaluating the Proposed Multiple Access Protocol ............................................. ６７ 
4.2 Fast Sensor Identification Technology ................................................................ ７１ 
4.3 TCP-YAM: A Fast and Fair TCP ........................................................................ ７４ 
4.4 EPON DBA Scheme ............................................................................................ ７９ 
4.5 Self-coupling time Synchronization in the Sensor Network ................................ ８５ 
4.6 Routing Scheme the Sensor Network .................................................................. ９７ 
5 Conclusion and Future Work ......................................................................................... 105 
6 Reference ....................................................................................................................... 106 
7 Project Self-Evaluation .................................................................................................. 108 
 
 
The LEO store-and-forward data systems provide non-real-time communication services.  
In the sea surface salinity (SSS) measurement, the buoys are deployed globally and transmit 
the collected data to the LEO satellite as it passes through the buoys as shown in Figure 1.  
The data is stored on board till the satellite reaches the communication range to the data 
center.  The data is downloaded and the satellite moves on for the next batch of data retrieval.    
However, the performance of this system may suffer from the collision problem when 
multiple buoys try to send their data simultaneously.  The conventional protocols such as 
ALOHA and its variations are not optimized to handle long propagation delay characterizing 
the satellite network.  Hence, to improve time and throughput efficiency, a novel protocol is 
proposed.  In this protocol, the communications are divided into two periods, the rush and 
transmission periods.  Conceptually, contentions are resolved during the rush periods while 
uninterrupted transmissions are ensured in the transmission period.  Through simulation, we 
found that this protocol provides strong throughput performance with moderate cost increase.   
 
To further improve the performance of the buoy access scheme, we proposed another 
method similar to those applied in the RFID area where fast sensor identification is essential.  
It takes advantage of the possible prediction of the buoy locations learned through the 
Argo buoy data center 
Aquarius LEO satellite 
Figure 1. Store-and-forward satellite system 
 In this research subproject, we also consider the performance in the terrestrial network.  
In particular, our attention is focus on the access technology that connect the local host to the 
wide area network.  This research topic becomes increasingly important as many competing 
fiber based access schemes are rolled in the world market.  Ethernet passive optical network 
(EPON) is one of the potential solutions for the next generation access technology.  Its 
economical equipment and maintenance costs are particularly attractive to telecommunication 
service providers.  The EPON has a tree topology and the upstream bandwidth from optical 
network terminals to the optical line terminal is shared.  To utilize the bandwidth in a fair and 
efficient manner, numerous dynamic bandwidth allocation schemes are proposed in the 
literature.  We will introduce and review these schemes, list their strengths and weaknesses.   
One of our goals is to give an overview to the current status of research in this area.  We will 
also introduce our work on a new scheme that consider performance guarantee on both 
bandwidth and delay.  To our knowledge, this is the first attempt to provide absolute 
guarantee on both performance metrics in EPON.    In conclusion, we summarize future 
works and point out the pending challenges for future study. 
The sensor technologies applied in the Aquarius project adopted Argo buoys.  However, 
there are other sensor networks where interactions among sensors play the focal role in the 
functions and operation of the networks.  It is critical to reduce the energy incurring in these 
interactions to extend the life of the sensor.  Time synchronization is a prerequisite for some 
applications in sensor networks.  In this subproject, two methodologies facilitating this task 
will be introduced.  One approach relies on the creation of a hierarchical structure to carry out 
synchronization. Many existing protocols, e.g., timing-sync protocol for sensor networks 
(TPSN) and time-diffusion synchronization protocol (TDP) belong to this category.  Another 
approach advocates pulse coupling among sensor nodes.  Its basic idea is to achieve global 
2 Existing Works 
 
In the following subsection, we will review the existing works in satellite access, 
automatic identification, TCP technologies, dynamic bandwidth allocation for EPON 
upstream traffic, time synchronization and routing for sensor networks.  We will explain their 
functions and point out their problems to motivate our study.  After chapter 2, we will, then, 
introduce our solutions in Chapter 3. 
2.1 Review of Satellite Access Schemes 
 
Traditionally, ALOHA protocol has been widely used for satellite access due to its cost 
effectiveness and predictable performance.  However, it is common to know that ALOHA and 
its main variation, slotted ALOHA, do not work well when the load is heavy.  In the 
following, we give more detailed description on these two schemes. 
2.1.1 Pure ALOHA 
 
The ALOHA protocol, also called pure ALOHA or unslotted ALOHA, is a random (or 
contention) multiple access protocol developed at the University of Hawaii for sharing 
broadcast channel access among a number of users [1].  It is a straightforward protocol in 
which the sender sends whenever there is data.  It means that a user accesses uplink as soon 
as it is ready to transfer data through the satellite.  But if the number of users increases, frame 
collisions tend to grow rapidly to cause delay problem caused by frame retransmission.  In 
the satellite network, the performance is further degraded because of the long delay on 
contention detection.  In the packet radio environment where the ALOHA was originally 
designed for, a collision is detected during the frame transmission.  However, in the satellite 
environment the ground stations rely on the satellite to know if there is a collision during 
RReT 2−=                                                                              Eq (4) 
The maximum throughput of the pure ALOHA protocol is 18.4% of the system capacity.  
It is obtained by making the derivative of Eq (4) equal to 0.  The maximum exists when R = 
0.5.  The throughput-load performance curve is plotted in Figure 3.  
 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
Offered traffic R
Thr
oug
hpu
t T
ALOHA
slotted
ALOHA
 
Figure 3. The Throughput-Load Curves of the Pure and Slotted ALOHA  
2.1.2 Slotted ALOHA 
 
Slotted ALOHA is a powerful extension of the pure ALOHA protocol just described.  It 
greatly improves the performance of the pure ALOHA protocol.  In the slotted ALOHA 
protocol, transmission time on the channel is divided into equal time slots.  And the length of 
each time slot is slightly greater than frame duration τ.  It assures no interference between 
successive frames.  Each user has a synchronized clock and transmits a message only at the 
beginning of a new time slot.  Consequently we have a discrete distribution of frames [2]. 
This scheme can prevent partial collisions as in the ALOHA protocol.  However, as the 
number of users increases, the probability of collisions where a frame collides completely 
with another will increase.  Figure 4 presents how the slotted ALOHA operates.  
in the following. The random access based schemes include aloha, frame slotted aloha (FSA), 
dynamic framed slotted aloha (DFSA), estimation and binary selection frame slotted aloha 
(EB-FSA), and so on [5, 6, 7, 8]. The query tree based schemes include, but not limited to, 
binary tree, query tree, cut-through, bi-slotted query tree, along with other schemes [9, 10, 11, 
12].  Next, we will introduce two query tree based schemes, the binary tree algorithm (BTA) 
and the query tree algorithm (QTA) in greater detail.  These two schemes are both compared 
to the proposed algorithm in Chapter 4.    
In [9], a binary tree based algorithm is proposed. In this algorithm, the satellite follows 
the binary tree and polls sensors of the same prefix.  Each sensor of the same prefix specified 
in the polling message will respond by sending the next bit after the prefix to the satellite.  
The satellite will observe one of the three cases, a collision, single response, and no response.   
If there is a collision, the satellite will append a 0 or 1 to the previous prefix and form an 
extended prefix.  It is sent out in a new polling message.  If there is only one response, then 
the satellite will attach the received bit to the previous prefix and send out the new prefix in a 
polling message.  Finally, if there is no response, then the satellite ignores the branch.  This 
method is inefficient if the length of the sensor ID is long.  Even if there is only one sensor in 
a subtree, the algorithm needs several polling messages to identify the sensor.   
In [10], a query tree based algorithm is proposed to govern the order of transmissions 
among the sensors. In this algorithm, the satellite follows a binary query tree to poll the 
sensors. If the prefix of a sensor’s ID matches what is sent by the satellite, then the sensor 
responds by sending out all remaining bits after the prefix to the satellite. If there is only one 
such sensor, then the satellite recognizes the sensor immediately and no collision needs to be 
resolved. However, when there are two or more sensors with same prefix, all respond and the 
satellite sees a collision from the garbled message. The satellite can resolve the collision by 
transmissions. It is not clear how the algorithm adapts to the case where the number of 
sensors, n, is much greater than maximum frame size Nmax. 
So far, we reviewed several existing schemes that can be used to find out the buoys in the 
footprint of the satellite.  These schemes choose to forget the location information of the 
buoys.  If the satellite has a good idea on the locations of the buoys, it can be much efficient 
in assigning uplink bandwidth to them.  In the next chapter, we will describe how this 
efficiency can be achieved. 
2.3 Review of TCP Technologies 
 
TCP is a data transfer protocol and starts to transmit data after the connection is established 
between two end users.  TCP is specified in RFC 793[13] by IETF.  In TCP, each user 
transmits or receives data in segment, a TCP protocol data unit.   It is composed of a header 
and payload.  The sizes of headers range between 20 to 60 bytes.  Each header contains the 
following fields: source and destination ports, sequence and acknowledgement numbers, 
header length, flags, window offset, checksum, urgent pointer, unused field, optional field, 
and padding. 
The TCP connection is built with the three-way handshaking protocol between end users.  
First, the sender transmits a connection request to the receiver.  Then, the receiver sends the 
acknowledgment (ACK) to the sender in return.  The ACK message consists of the 
acknowledge number as well as the maximum number of segments that the receiver can take.  
After the sender receives the ACK, the sender responds a SYN-ACK and starts to transport 
data.  Next, we review some popular TCP variations.   
TCP Tahoe is one of the earliest TCP variations.  It is characterized by sudden and drastic 
bandwidth drop.  That is, the connection goes to slow start after a segment loss. And, the 
in the TCP. 
In TCP Vegas, the increasing rate of the congestion window in the slow start mechanism 
is reduced.  This modification tries to avoid segment losses caused by rapid ramp up in 
segment transmission.  Moreover, TCP Vegas relies on a different scheme to detect network 
congestion in the congestion avoidance mechanism. For example, TCP Reno observes 
network congestion if a segment loss is experienced.  TCP Vegas, on the other hand, uses the 
timestamp to record the system clock as a segment is sent.  Hence, it can obtain more 
accurate RTT and judge if there is congestion in the network.  Hence, its congestion 
avoidance mechanism can better prevent segment loss and network congestion.  The 
difference between the expected and actual data rates is used to adjust the congestion window 
size.  The formula is provided in the following.  α is the lower bound threshold, and β is the 
upper bound thresholds that are set by the user. 
 
)
RTT  measured
ted  transmitbytes-
BaseRTT
WindowSize(  Actual - Expected  Diff ==   Eq (8) 
Increases congestion window linearly during the next RTT, if Diff < α. 
Decreases congestion window linearly during the next RTT, if Diff > β. 
Keep congestion window unchanged during the next RTT, if α< Diff <β. 
 
Unfortunately, TCP Vegas has the fairness problem.  When there are already several 
connections in the network, a new connection joining into the network will have the RTTs of 
these connections differ from one another.   
Ian F. Akyildiz et. al. proposed TCP Peach designed to provide better throughput for 
satellite IP networks [15].  TCP Peach is consisted of sudden start, congestion avoidance, 
control, window control burstiness control, and estimation. 
FAST TCP aims to solve the oscillation problem observed in bandwidth and congestion 
window.  For example, TCP Reno uses the binary congestion signal to indicate packet loss.  
Another example is that the size of congestion window is rather unstable due to the properties 
of exponential increment and drastic half drop.  FAST TCP relies on a congestion estimation 
scheme to provide stable congestion window.  The congestion estimation scheme periodically 
updates the congestion window based on the measured average RTT and average queueing 
delay.  It collects network information and provides various input parameters, i.e., RTT and 
queueing delay to other collaborating schemes, i.e., window control scheme.  Although FAST 
TCP offers more stable and higher throughput performance, it is still dubious how it will act 
in a bursty network environment. 
2.4 Review of Ethernet Passive Optical Network Upstream Bandwidth 
Allocation Schemes 
 
In this subsection, we will review several popular solutions regarding the dynamic bandwidth 
allocation schemes for EPON.   
2.4.1 Interleaved Polling with Adaptive Cycle Time 
 
IPACT [17] is one of the early works in this area.  It suggests several proposals on how to 
assign upstream bandwidth among the terminal users in the PON environment. Since it is a 
pioneer work, its performance has been compared against by the latter proposals.  In fact, five 
methods are covered in this paper including fixed service, limited service, constant credit, 
linear credit, and elastic service.  Their respective allocation formulas are explained in the 
following.   
 
 The fixed service is a variation of time division multiple access (TDMA).  The bandwidth 
allocation is fixed and regardless of the requested window size.  The limited service attempts 
to meet the ONU request as long as it does not exceed the upper bound WMAX.  Once the limit 
is reached, no matter how large the requested window size is, the OLT will assign WMAX to the 
ONU.  This assignment can cause unused time slot in the transmission because no partial 
frame transmission is allowed.  The elastic service tries to relax this rather strict upper bound.  
Specifically, in the premise that the cycle time will not be lager than the maximum cycle time 
TMAX, the OLT will satisfy the request window size of ONUs as long as the aggregated 
window size of last N requests is smaller than N × WMAX bytes, where N is the number of 
ONUs.  It should be noted that new frames may arrive at the ONU during the report-gate 
process.  Figure 6 shows the ONUs can remain idle before and after it’s transmission in the 
report-gate process.  Hence, even as the limited service allocates the exact window size 
request by the ONU, the new arrival frames need to wait for almost two cycles before 
transmissions.  To solve this problem, the constant credit method adds a fixed credit to the 
request window size serving those new frames.  The linear credit approach further extends 
this idea to make the credit linear to the requested window size in belief of that traffic burst 
STAGE 1 
To OLT 
STAGE 2 
L. priority 
ONU 
Figure 7. Two-stage Queue Structure 
To Subscriber 
H. priority 
M. priority 
essence, the OLT estimates the high priority new arrivals in the report-gate process using the 
known CBR arrival rate.  This scheme solves the light penalty in general and gives better 
delay performance when compared with the two-stage buffer scheme.  However, the CBR 
information required to make this scheme work may be elusive. 
2.4.3 DBA with Queue Management and Priority Queuing 
 
Assi et. al. proposed another DBA mechanisms in [19] to address the CoS and the problems 
encountered in the two-stage buffer and CBR credit methods.  There are three key ideas in 
their scheme.  First, the bandwidth calculation requires the complete buffer information 
reported from all connected ONUs.  Secondly, the system uses the number of high priority 
new arrivals in the previous cycle to forecast that of the current cycle.  Or, the CBR 
information is not required in this scheme.  The bandwidth assignment formula is listed in the 
following. 
( )( ) iiWiii LMnEHR +++=   Eq (9) 
where Ri is the grant window size, Hi, Mi, and Li are the size of the high, median, and low 
priority queues, respectively.  EiW is the estimate of the high priority new arrivals. 
Thirdly, a so-called fast granting is suggested.  Since the OLT needs the complete buffer 
profile to compute the bandwidth distribution in the next cycle, an undesirable idle period 
between two transmission cycles will occur.  This idle period not only increases system delay 
but also degrades the throughput performance.  Therefore, the fast granting scheme is used to 
remove the idle period by expediting the gate messages to the ONUs with requested window 
sizes less than WMAX.  We believe this scheme will perform relatively well in most scenarios.  
However, we think its performance will be very poor when in the extreme high load situation 
Finally, there is another scheme in [22] suggesting the use of a different kind of PON 
architecture.  The full request contention multiple-access (FULL-RCMA) protocol required 
two fibers between the splitter and every ONU, one for upstream and downstream traffic and 
another for detecting request contention.  Although it is an interesting idea, it significantly 
deviates from the standard EPON architecture and is not really comparable to other DBA 
schemes.  It is included here to give reference to readers interested on different physical 
implementations of EPON.  
2.5 Review of Time Synchronization Schemes in the Sensor Network 
 
In wireless sensor network, it can divide into several important issues; for example: routing 
selection [23, 24, 25], time synchronization [26, 27, 28], energy conservation [29, 30, 31] etc.    
It is important to conserve power.  When some nodes consume their energy, the operations of 
their surrounding nodes may be affected.  If a message is sent cross a node that is out of 
battery power, the network must consider if there is another route for the message.  However, 
many researches in optimizing power consumption in many various activities have been 
motivated and conducted. 
In conserving power, Wendi B. Heinzelman et al. proposed a routing protocol called 
low-energy adaptive clustering hierarchy (LEACH) in the sensor network [29].  In the 
LEACH, clusters are organized dynamically and all nodes in the cluster take turn to be the 
cluster head.  Figure 8 shows the organization of LEACH.  However, the cluster head usually 
consumes more energy than all the rest of the nodes in the same cluster.  In other words, all 
nodes probably switch to the sleep mode when they are not the cluster head.  The LEACH 
aims to balance the average load for each node in term of energy consumption so that all 
nodes have similar life expectancy.  Moreover, the cluster head must aggregate the data 
2.5.1 Pulse-coupling 
 
Sergio Barbarossa proposed a new protocol with information propagation based on mutual 
coupling of dynamic systems [32].  The information spreads as the result of the couplings 
among adjacent nodes.  One example is that the coupled nodes mutually act to adapt their 
clocks.  In the protocol, we can reach time synchronization by using the mathematical Eq 
(10), 
[ ])()()(
1
ttF
c
Kt ij
N
j
ij
i
ii a θθωθ −+= ∑
=
&                                                                         Eq (10) 
where )(tiθ  is the state function of the i-th sensor; ija  are real variables that describe the 
coupling between sensors i  and j ; K  is a control loop gain; ic  is a coefficient that 
quantifies the attitude of the i-th sensor to adapt its values; iω  is an initial condition given by 
the initial pulsation. 
For the function, ( )⋅F , we must use an monotonically increasing nonlinear odd function.  
The possible choices for F(x) are Eq (11) or Eq (12). 
 
1
1)( +
−= x
x
e
exF λ
λ
                                                                                                             Eq (11) 
)2sin()(
T
xxF π=           Eq (12) 
 
In this coupled dynamic system, a malicious node may prevent the entire network from 
synchronization by refusing to follow the algorithm.  It is important to know the 
synchronization convergent time.  If it takes too long or if there are too many message 
exchanges involved, then this approach is not justified.  Through local coupling, we can 
2.5.2.1 Passive Cluster Based Clock Synchronization 
 
Choong Seon Hong et al proposed a new protocol called passive cluster based clock 
synchronization in sensor networks [33].  The protocol first creates clusters by the passive 
clustering method.  Then, it applies the asynchronous averaging algorithm to reach time 
synchronization.  The passive clustering method is carried out on demand clustering and the 
formation of clusters is dynamic.  The clustering is initiated when the first data message is to 
be flooded.  The protocol can reduce its flooding overhead during message exchanges and 
make the synchronous process energy efficient.  In the passive clustering method, nodes can 
have four states: (1) Initial, (2) Cluster head, (3) Gateway, and (4) Ordinary. 
Figure 9 shows the states of the passive cluster based clock synchronization. First, the 
node receiving a request from any node becomes an initial node.  Then, the initial nodes 
compete for being the cluster head.  When the initial node first announce itself the cluster 
head, it turn to the cluster head immediately without further checks.  If a node receives 
packets from two cluster heads, then it becomes a gateway.  And all other nodes in the 
passive clustering method are ordinary nodes in their respective clusters.  But the issue in the 
passive cluster is that any pair of nodes is two hops most.  Therefore, it is not very effective 
for wide sensing region.  And we might spend more cost for putting more nodes.  Although 
the protocol does not need complicated mathematic operations, it has more restriction on 
component.  In the same condition, like the transmission range, the protocol provided some 
improvement on lifetime and operation times. 
2.5.2.2 Time-diffusion Synchronization Protocol 
 
Ian F. Akyildiz and Weilian Su proposed a time-diffusion synchronization protocol (TDP) for 
wireless sensor networks [34].  In TDP, it has four procedures including peer evaluation 
The new protocol considers numerous realistic aspects.  It includes essential supporting 
procedures and inevitably incurs more messages exchanges when compared with other less 
thorough mechanisms.  If the nodes are few and far between in the sense field, it needs more 
master nodes or diffusion leader nodes.  In opposition the sensor network consume more 
energy, I think the method use one hop by hop to transmit timing information message is 
appropriate.  In the paper, it does not stress how to construct a tree-like structure.  The cost of 
building and maintaining the tree may be overwhelming if its sole purpose is synchronization.   
And when it decides the master nodes, if it can consider the nodes with high degree or 
location in the center, I think the effort is better than previous. 
 
Figure 10：LDP of TPSN 
2.5.2.3 Timing-Sync Protocol 
 
Ganeriwal et al. [35] proposed a time synchronization protocol, which is called the timing-
sync protocol for sensor networks (TPSN).  It aims at providing network-wide time 
synchronization in the sensor network. 
Sensor nodes
0
1
1
1
2
2
2 2
Root node
Sensing field
2
2.6 Review of Routing Schemes in the Sensor Network 
 
In the WSNs, the research of routing protocol developed from the restriction of sensor nodes 
can divide into several parts: the routing scheme considering the geographic, shortest path, 
energy consumption, data-centre and so on.  And the factors that each the direction of 
researches considers are much distinctly, some of them consider the network topology, some 
pays attention to how to find the shortest path, some attach important to the energy 
distribution, and some emphasize the ratio of data delivery. 
Because the consideration directions are different, therefore, developing the suitable 
routing protocols regarding the different conditions can fulfill the requirements more 
efficiently, and achieve the goal of data delivering successfully. 
Because of above characteristics of the sensor network, the development of routing 
protocols of the WSNs has to observe the rules as following [36]: 
1. The routing protocol must possess the scalability and flexibility.  Because of the 
dynamic sensor distribution and the nature collapse of sensors, the designed routing 
protocols have to keep normal operation and not be influenced by the change of network 
topology. 
2. It must have the ability of self configuration and algorithm processing.  The WSNs 
must organize by themselves, so the developed routing protocol should consider about 
this factor. 
3. Reducing resource consumption as possible.  The routing protocol has to consider the 
factor because of the resource constraint of sensors, such as energy and memory size. 
4. Extending lifetime of the network.  The sensors would not be retrieved and reused 
after collapsing, therefore the routing protocol must use the limited resource efficiently 
The hybrid routing protocols combines the characters above two protocols, such as Direct 
Diffusion [24].  However, the memory size of sensor node is restricted, it can not save too 
much information, and the proactive routing protocols need mass memory space to save the 
routing tables, therefore, the reactive and hybrid routing protocols are more suitable for the 
WSNs. 
[2] The Method of Data Delivery: 
According the ways of the packet or data delivery to classify the protocols, it can be divided 
in three types, flat, direct communication and cluster routing protocols in Figure 11.  In direct 
communication, the sensor nodes send the packet to the sink directly and don’t pass through 
other sensor nodes, such as SPINS [41].  In the flat, the source node sends the packets to the 
sink by multi-hop, passing through several sensor nodes, such as GPSR [40] and GSGR [42].  
In the clustering routing protocols, it divides the sensor nodes into two types, general sensor 
and cluster head.  And the network topology is divided into several cluster, the cluster head is 
responsible for collecting the information in each cluster.  So the general sensors send the 
packets to the cluster head, and the cluster head send packets to the sink after collecting the 
information in its own cluster, such as LEACH [30].  When the network topology is bigger, 
the direct communication method has not enough communication range to send packet to the 
sink directly.  So the flat and the cluster methods are more suitable for the WSNs.   
[3] Other Methods: 
By other considered factors, the routing protocols can be divided into several fields as follow.  
In considering the geographical locations, there are GPSR [40] and GSGR [42] protocols.  
And in considering the energy consumption, there are LEACH [29] and GEAR [43] protocols.  
Some of the protocols emphasize the factor of data fusion, such as Direct Diffusion [24], and 
some other routing schemes are data-centric, such as AODV [44], DSDV [25] and DSR [39].  
routing protocol.  It uses two stages communications to deliver data that prevent too much 
duplicate packets sent to the sink.  In Figure 12(b), the cluster A collects the information of 
the area of its cluster, then it delivers the packet to the sink directly, this reduces the number 
of packets sent to the sink.  And LEACH also considers the remaining energy of each sensor 
that it refers to select the clusters in each round, and balances the energy consumption of the 
network.  But the  communication range of sensors is limited, if the cluster head is too far 
away from the sink, it is out of the range, so the LEACH is not suitable for large topology, it 
can not cover the whole area of network. 
 
Figure 13: The Steps of Greedy Forwarding Routing 
2.6.2 Greedy Perimeter Stateless Routing (GPSR) 
 
The GPSR [40] is a geographical routing scheme, it has not to save any information about 
route, and the protocol just need to know about the sensor-self location to present the IP 
address, and sensors decide where the packet should be sent to according the neighbor’s 
location.  GPSR contains two routing schemes, the greedy routing mode and the perimeter 
routing mode.  In generally, the packet is in the greedy mode, the sensor which holds the 
packet will choose the neighbor who is nearest to the sink to send the packet.  When the 
(a) (b) (c) 
Destination Neighbor node Source 
detour.  In Figure 14, the blue node is the source, and the red node is the destination.  Figure 
14(a) is the completed connection graph, and (b) is the detour routing path of the perimeter 
routing, and (c) is the routing path after the GSGR algorithm. 
 
Figure 14: The Eliminating Detours of GSGR 
 
(a) 
(c)
(b) 
overhead than other routing protocols such as AODV, and each node in ABRP just maintains 
the table about it own cell, and the route to destination will be divided several sections, so it 
is easily to maintain the route, therefore this improve the packet delivery ratio and network 
overhead. But the main disadvantage of ABRP is that the numbers of anchor must be enough 
to cover the whole network topology.  If the numbers of anchor are not enough, there would 
be nodes outside the cell, and they cannot send packet to neighbor cells because these nodes 
have no routing table and no routes to neighbor cells. 
2.6.5 Flossiping 
 
Yuecheng Zhang et al. proposed Flossiping [46] which combines the flooding and gossiping 
routing schemes.  This protocol uses single branch gossip with low-probability random 
selective relaying (LPRSR) to achieve the job of packet delivery.  The three main 
considerations of the scheme are simplicity, reliability and resource awareness.  In simplicity, 
the protocol provides simple methods for routing packets.  And the mechanism also makes 
sure that packets in the network are loss free.  Final is the resources awareness, this routing 
scheme provides several parameters to control the processes.  In Flossiping, when source is 
ready to send a packet, it random selects a neighbor to deliver this packet in gossiping mode.  
At the same time, the rest of the sender’s neighbors listen to the message and then generate a 
random number.  If the number is smaller than the threshold which the source decides and 
save in the packet header, they will take this packet and flood it.  Figure 16 shows the 
Flossiping routing paths, the red line is the packet in gossiping mode, and the black line is the 
flooding routing paths. 
 
3 Research Methods 
 
In this chapter we will introduce the proposed solutions to the six research topics stated in the 
earlier chapters.  Specifically, we suggest 
• A multiple access protocol for buoys to send data to the LEO satellite, 
• A fast identification scheme for retrieving the data from the buoys 
• A dynamic bandwidth allocation scheme for the upstream traffic in the PON network, 
• A fast and fair TCP protocol, 
• A self coupled time synchronization scheme that depends on local information, and 
• An energy efficient and reliable routing scheme in the sensor network. 
Their detailed mechanism is introduced respectively in the following sections. 
3.1 Multiple Access Protocol for Satellites Store-and-Forward Systems  
 
One of the characteristics of the store-and-forward system is that the little LEO satellite 
passes periodically through the ground stations.  The visibility time of the ground station is 
about 15 minutes while the typical orbital cycle is at least 90 minutes.  Since most of the time 
the ground station does not see the satellite, it is reasonable that the majority of the messages 
arrive at the ground station in the invisible period.  The station can only access the uplink to 
the satellite in the visible period.  Therefore, messages usually wait in the memory of the 
ground station before they are transmitted to the satellite when it becomes visible to the 
ground station.  In fact, the important performance metrics in this particular application 
include throughput and fairness.  
In our protocol, the visible period is divided into two portions. One is called transmission 
period and the other is called rush period (Figure 17). These two periods occur alternatively.  
receives a suppressing message from the satellite before the estimated visible time, then it 
will quietly withhold its request till the current transmission is completed. If it does not hear 
from the satellite before the visible time, then a new request is sent. This request may collide 
with the existing transmission. In such a case, the ground station will receive a suppressing 
message and it will wait for the rush period. Or, the new request is sent during the rush period.  
In such a case, it will join with the other ground stations to compete for the uplink access.  
The state diagram for the ground station is illustrated in Figure 18. 
 
  
Figure 18. State Diagram for the Ground Stations 
It should be noted that the Ethernet may not be a reasonable choice in the store-and-
forward system.  It is because of its lack of support in retransmission and suppression 
functions in the data link layer.  Hence, the proposed novel protocol is incompatible with the 
its communication request to the satellite.  Once a request is sent, the ground station waits for 
a constant period before it gives up.  Then, it will double its transmission window and 
randomly select a slot in the window to send the next request.   The ground station will 
conclude a busy uplink if certain maximum is reached for the transmission window.   
Figure 18 and Figure 19 present two state diagrams for the ground station and the 
satellite states, respectively, in our novel protocol.  The state transitions in these pictures are 
mostly explained previously.  Please note that Figure 19 indicates the state of the uplink in 
the satellite.  Since the satellite will be the sole user for the downlink, we do not consider 
multiple access control for the downlink.  To ensure fairness, the ground station will cease to 
transmission attempt if it successfully sends M frames in a day.   
3.2 Fast Sensor Identification Technology  
 
In this section, we explain the architecture of the proposed scheme.  In the algorithm, we 
build a binary tree whose height is the same as the length of the sensor ID.  A node in the 
binary tree is classified as one of the three types depending on the number sensors in the 
subtree rooted at the node.  For every type 1 node, there is no sensor in the subtree rooted at 
the node.  Similarly, every type 2 node contains exact one sensor while every type 3 node 
includes more than one sensor in the subtree rooted at the node.  Given a set of sensors, then 
the algorithm can build the binary tree and classify all nodes in the tree.  In Figure 20, we 
show a full binary tree of 8 leaves.  Three sensors whose IDs are 011, 100, and 111 are 
present and shaded in this example.  Figure 21 shows the node classification based on the 
criteria just explained. 
It is evident that if the sensors in the footprint of the satellite must belong to the given set, 
then the optimal reading sequence contains the type 2 nodes with the type 3 parent nodes.  
adequate prefix will be sent out.  If there is a single response as expected, then the algorithm 
identifies the sensor and discards the branch.  Otherwise, the algorithm invokes the query tree 
method to resolve the collision.  The algorithm will simply move down the branch when it 
sees a type 3 node because it is no need to waste a polling cycle on an expected collision.  
These three different cases are outlined in Figure 22. 
Subroutine 1– Initialization  
   ‧Given a set of sensor IDs： 
- Build a binary tree and mark the sensors. 
- Classify all nodes in the tree based on the number of 
sensors in the subtree rooted in the node. 
Subroutine 2 – Type 1 node handling 
   ‧Send out a polling message with the corresponding prefix 
- collision：go down the branch in the tree 
- no collision：either identify a sensor or ignore the 
branch 
Subroutine 3 – Type 2 node handling 
  ‧Send out a polling message with the corresponding prefix 
- collision：go down the branch in the tree 
- no collision：either identify a sensor or ignore the 
branch 
Subroutine 4 – Type 3 node handling 
  ‧Go down the branch without sending any polling message 
for the node 
Figure 22. Pseudocode for the associated subroutines of the proposed algorithm 
With the proposed algorithm, four readings are required for the example illustrated in 
Figure 21 instead of the three readings required when all sensors must come from the 
predefined set.  Hence, the degradation in performance is the cost paid for the robustness of 
the algorithm.  In the next chapter, we will show some performance comparison among the 
initial value is zero.   
2. In congestion avoidance phase, if cwnd < ssthresh, then cwnd = cwnd + 1. 
3. If the sender receives the positive ACKs , then 
(i)  if cwnd < awnd, then cwnd = cwnd + 1 / cwnd 
       (ii) if cwnd > awnd, then cwnd = cwnd - 1 / cwnd 
 
In TCP Yam, the average congestion window reflects the current trend of network 
congestion.  The congestion window is adapted accordingly to prevent segment loss.  The 
recovery mechanism cooperates with other mechanisms to accomplish higher throughput.  In 
this paper, the congestion avoidance mechanism will adjust the congestion window based on 
the observed congestion level of the network.  Similarly, we modify the initial congestion 
window in the recovery phase.  After a segment loss, both ssthresh are set to three quarters of 
the cwnd.   
The recovery mechanism is provided in the following. 
• When there are three duplicate ACKs or a timeout occurs, set ssthresh = 3*cwnd / 4. 
• Retransmit the missing segments.  During the retransmissions, the value of cwnd is 
frozen to withstand temporary network congestion. 
• When the next ACK of new segment number arrives, set cwnd = ssthresh.  Then, the 
protocol enters into the congestion avoidance phase. 
3.4 Delay Sensitive Bandwidth Guarantee EPON DBA Scheme 
 
In this section, we explain a delay sensitive bandwidth guarantee dynamic bandwidth 
allocation mechanism (DSBG DBA).  The design objectives are listed in the following.   
• No extra delay caused by calculating the bandwidth assignment.  
300k 600 76.8 ms 
           
We immediately find out that in order to obtain a reasonable delay of a few milliseconds 
in the EPON, we may need to adopt a very small WMAX.  As shown in Table 2, we assume that 
every ONU is loaded with 62.5 Mbps high priority traffic and the average size of frames is 
about 500 bytes.  In the following analysis, we ignore the guard time without loss of 
generality.  It seems the maximum bandwidth window contains about 78 frames to achieve 
the maximum delay of 10 ms.  In this case, the OLT needs to poll 3200 times per second.  
However, if we increase WMAX to 150k bytes as suggested in several previous works, we find 
the delay performance degrades linearly in Table 3.  In other words, all delay sensitive 
applications such as IP telephony cannot be supported in the EPON.  We admit our argument 
emphasizes on the worst case analysis where all 16 ONUs are heavily loaded.  It is worth 
noting that DMAX is somehow reduced by the OLT granting extra bandwidth based on its 
estimation of new frame arrivals in each ONU.  
If we wish to remove the WMAX limitation to allow more flexible bandwidth assignment, 
then we must, instead, control the polling cycle, Ti, to maintain so that the maximum delay 
can be guaranteed.  In fact, the relationship between the maximum delay and the polling 
cycle time is a slight modification of Eq. (13).  Equation (14) states that the sum of the two 
consecutive polling cycles should not exceed the maximum delay.  But, we should note that 
the underlying assumption is that the OLT does not grant extra bandwidth. 
1++= iiMAX TTD   Eq (14) 
Next, we consider the timing of sending out gate messages.  In the existing works, we 
briefly explain that two strategies are proposed.  The intuitive strategy calls for bandwidth 
delay or utilization under various load scenarios is still unclear.  
We propose a delay sensitive bandwidth guaranteed DBA.  In an EPON, we specify the 
target maximum delay requirement for traffic.  This is a system parameter and can be 
provisioned in the OLT.  The OLT also maintains a profile for the connected ONUs.  Each 
entry in the time profile is associated with the corresponding ONU and contains four items.  
The first item records the ID of the associated ONU.  The second item records the guaranteed 
bandwidth according to the service level agreement (SLA) with the subscriber.  The third 
item indicates the previous polling cycle time for the ONU, i.e., the period between two 
consecutive ONU bandwidth windows.  The last item stores the time when the last report 
from the ONU is received by the OLT.  Before we move forward, we would like to point out 
that the OLT is responsible for multiple EPON interfaces or several thousand queues in total.  
However, the memory space required for this profile should not be a major concern of cost.   
The proposed DSBG DBA allows each ONU to serve traffic with different classes of 
services.  However, for the sake of clarity we assume the bandwidth and delay guarantees are 
for the traffic of the highest priority.  If an ONU has only lower priority traffic, then the 
second item in the associated entry is 0 to reflect this situation.  Other than the second item, 
the other items of the entry associated with the ONU exclusively with the low priority traffic 
are handled in the same fashion as those of the ONU with the highest priority traffic.  
  With the time profile, the OLT can compute the target length of the next polling cycle 
using Eq (14).   
 
MAXN
k
k
i
i D
CC
CB
∑
=
⋅
=
1
target,
2
       Eq (14)  
communication range, and other constants.  Next we have to decide how to deploy all sensor 
nodes.  In generally sensor networks sensor nodes are randomly deployed in our interesting 
areas.  But in some cases they are regularly deployed in the street just like a chessboard.  So 
we consider the two cases in our simulation.  In our protocol, we randomly assign the clock 
to each sensor node.  The clock rate is related to each node’s time trend.  If first the 
difference of the two nodes’ clock rate was small, the difference of the two nodes’ time will 
be small.  It was the same on the contrary.  Then we need to judge the connectivity of each 
node.  The connectivity is how many neighbors of one’s sensor node in its communication 
range.  It is a critical point in our synchronization procedure.  Finally, it is important step to 
adjust the clock rate and the time difference of the sensor node.   
 
Figure 24：Flowchart of the Self-Coupled Algorithm 
hop packet delivery will cause several problems.  The flooding is the traditional routing 
scheme, it uses broadcast with multi-hop method to extend packets to the whole network 
topology.  Flooding routing protocol is an easy to implement scheme, and it also fits various 
network type, distribution and requirement.  But the unlimited broadcasting the packet will 
cause the problem of broadcast storm.  The duplicated packets are transmitted constantly in 
the network topology.  And the transmitting times of packets increase, the energy of the 
network consumes faster. 
The flooding routing protocol has three main disadvantages, (1) implosion, (2) overlap 
and (3) resource blindness. 
 
Figure 25. The Problems of the Flooding Scheme 
(1)Implosion: Because flooding deliver data by broadcasting, the packet would reach the 
same node via different routes.  When a sensor node receives a packet, it will not to see if it 
has received it before.  And this makes that the duplicated packets are sent to the same place.  
In figure 3.1(a), the source broadcast packet to relay nodes, and then the relay nodes send the 
(a) (b)
Source 
Destinatio
Relay 
Event
Sensor node 
Sensing field 
selected sensors might be farther than the sender to sink.  Thus this may extend the packet 
delay time. 
And the Flossiping combines above two routing schemes.  Although the Flossiping 
improves several problems of them, it also inherits their disadvantages.  It improves the 
power consumption of flooding, and the packet delay problem of gossiping.  So the power 
consumption and packet delay time of Flossiping are between flooding and gossiping. 
Next, we will propose our scheme to solve the problems.  We control the packet delay 
time closed to the shortest path and the number of delivery packets.  We propose the Single 
Gossiping with Directional Flooding routing protocol (SGDF) to solve the problems of 
flooding and Flossiping.  The SGDF can divide into two stages.  In the first stage, it 
initializes the network to make each sensor produce the gradient to the sink.  And In second 
stage, routing stage, the SGDF uses single gossiping and directional flooding routing 
schemes to deliver packets.  By using above schemes, the SGDF makes high packet delivery 
ratio, low traffic load and short packet delay, and improve the performance of the network. 
After the sensor nodes are randomly distributed in the specific area, the initialization 
scheme will be started.  At first, the sink broadcasts a hello message to its neighbors whose 
communication rang covers the sink.  The hello message includes the information about the 
address of the sink, hop count from the sink and the threshold TH.  The address of the sink is 
fixed but the hop count is not.  Each node will know the hop count from the sink after initial 
stage, and store the information in the memory.  The threshold TH is a number between 0 and 
1, and it is used to start the directional flooding or not.  When the sink broadcasts the message, 
its neighbor will receive the message and get the information of sink address and how many 
hops the message getting past.  When the sensor received the message, it looks if there is any 
information about hop count.  If the field used to store the hop count is null, the sensor will 
of the message is not smaller than the hop count it stores, it drops the message.  After the 
repeating steps above, the network completes the initialization like Figure 27(c). 
Due to the resource constraining of the sensors and cost restricting, we design a low 
complexity routing scheme to reduce the equipment cost and resource expense.  Although we 
reduce the cost of the equipment and the resource, we still care about the performance of the 
routing scheme.  In SGDF, we don’t need the expensive equipment like GPS, and we also 
don’t need the processor which has high and complex computation ability.  So we can reduce 
the cost of the sensors’ hardware.  In the conditions of restricted resources, the SGDF still has 
better performance than the Flossiping and Flooding routing schemes. 
The SGDF combines single gossiping and directional routing schemes.  When a packet 
is generated by source and ready to deliver, it will broadcast a request message to the 
neighbors, and all the neighbors receive the message and send the reply message including 
the information of the node’s gradient and its address to the source.  The source receives the 
reply messages, and if there is any node’s gradient smaller than its, it randomly selects one of 
them to be the next hop of the packet.  If there is no node’s gradient smaller than source but 
equal to source, it also randomly selects one of them to be the next hop.  If there is not any 
node’s gradient smaller than or equal to the source, the source randomly selects one of the 
neighbors to be the next hop.  After the source decides the next hop, it just transmits the 
packet.  When the source broadcasts the packet, its neighbors receive the packet and check 
that if they are the next hop or not.  And then they decide relay the packet or not. 
After the network initialization completes, if there is any packet to deliver, it will start 
the routing schemes.  When a node is ready to send data or information to the sink, it will 
generate a packet.  The header of the packet includes 8 fields (source, destination, present, 
next, SN, TTL, mode, hop).  These fields represent the source address, the sink address, the 
packets, the larger TH also makes higher deliver ratio. 
 
Figure 28. The Single Gossiping Algorithm 
Figure 28 and 29 show the routing algorithm of Single Gossiping and Directional 
Flooding.  We can see that when node n receives a packet, how the node works.  In these two 
figures, Adr(n) represents the address of n, S{} is the set that the nodes’ gradient are less than 
n, E{} is the set that the nodes’ gradient are equal to n, TH is the threshold that decides 
starting flooding or not, the gradient(n) is the distance from node n to the sink, and D{} is the 
set of packets which node n has received before. 
Single Gossiping  
When node n receives a packet p, 
if Adr(n) = destination, 
     n confirms all the nodes, 
else 
   if the mode=1 & the next=Adr(n), 
     n sets the present=Adr(n), 
     n broadcasts request message and gets the reply, 
        if there are existing neighbors S{}, the gradient of S is smaller 
than n, 
               Sets the next=random select one node in S, 
        else if there are existing neighbors E{}, the gradient of E is 
equal to n, 
               Sets the next=random select one node in E, 
        else 
               Sets the next=random select one node in neighbors, 
     n broadcasts p, 
   else if the mode=1 & the next!=Adr(n), 
     n generates a random number x, 
       if x < TH 
         n sets mode=0, 
         n sets TTL=10, 
         n starts Directional Flooding, and broadcasts p, 
   else 
     Directional Flooding 
 Figure 30 shows how a packet delivers in SGDF.  Node(0) is the source, and node(16) is 
the sink.  At the beginning, the source node(0) selects node(2) to the next hop of Single 
Gossiping and broadcasts the packet.  Then node(2) receives the packet, and other neighbors 
of node(0) whose gradient is less than node(0) will generate random number.  We assume that 
node(3) generates the random number less than the threshold and starts directional flooding 
in figure 3.6.  Then the node(2) repeats the above steps, at the same time the node(3) 
broadcasts the packet which is in the Directional Flooding mode, and the gradient of node(8) 
and node(14) is less than the gradient of node(3).  So node(8) and node(14) will receive the 
flooding packet and relay it.  And the node(2) selects node(6) to the next hop, and node(6) 
selects node(9) to be next.  And at the same time, node(14) may have chance to receive the 
packet in flooding mode from node(6), but node(14) has already received the packet from 
node(3), so node(14) doesn’t receive the Directional Flooding packet from node(6).  This 
solves the implosion problem which is introduced in section 3.1.  After repeating above steps, 
the packet is delivered successfully to the sink, node(16). 
 
computed in Eq (17.a) and (17.b) with respect to the number of fully served stations and the 
number of successfully transmitted packets.  The best throughput performance in our protocol 
is about 56% for CS and 58% for CP.  That is, our protocol performs three times as better as 
the pure ALOHA does and is 55% better than the slotted ALOHA. 
 Stationper srameF of NumberSize rameF
Capacity LinkTime SimulationC S *
*=
        Eq (17.a) 
Size rameF
Capacity LinkTime SimulationC P
*=
                             Eq (17.b) 
 
Figure 31(a). The Number of the Served Stations 
 
Figure 31(b). System Throughput 
suppressing messages rises as the system load intensifies. 
The average number of stations in the rush period is illustrated in Figure 32. As we 
expect, when the load increases, the rush period will become crowded. However, the 
frequency of the suppressing message does not seem to play any significant role. This result 
is consistent with our expectation because the suppressing message is not broadcast in the 
rush period. Figure 32 also tells us that if there were no suppressing scheme, then the system 
will have a great number of ground stations competing for the uplink bandwidth all the time. 
 
 
Figure 32. The Average Number of Stations in a Rush Period 
 
In Figure 33, we see the ratio of the rush periods to the entire simulation time decreases 
when the interval of the suppressing message is set to the larger value.  It may not be 
straightforward.  However, it is caused by additional retransmissions in the transmission 
period when the frequency of suppressing messages is small.  In other words, while the same 
amount of time in the rush period is necessary to find out the next transmitting ground station, 
a longer transmission period is resulted from a small suppressing frequency.  If the ratio is, 
say, 0.2, then the maximum throughput is upper bounded by 0.8.  
number from satellite in these schemes. The number of reading messages requirement can be 
reduced because of the require numbers for the satellite. In Figure 34, the sensor numbers are 
2 ~ 1024. The BTA is not better than QTA and our scheme. The performance of QTA and our 
scheme are similar in sensor numbers 2 ~ 32. Because the sensor numbers is small, the fast 
sensor identification problem is not very serious for these tree based schemes. When there are 
more and more sensors, QTA and our scheme performance statistics have more and more 
distinction as well.  
 
 
Figure 34. Comparing QTA, BTA, and the proposed scheme 
 
In addition, we show the number of reading requirement to use bit number from satellite 
in these schemes. From Figure 35 to Figure 38, we set percentage size to 25%, 50%, 75%, 
100% and let each sensor have 5, 10, 15 bit ID. We can observe the results. We see the 
number of satellite messages required in the QTA, BTA, and our scheme increases linearly as 
the number of percentages increases. Hence, we can prove our scheme to efficiently reduce 
the number of reading requirement for sensor identification. 
 Figure 38. Compare with QTA, BTA, and our scheme (full address space) 
 
4.3 TCP-YAM: A Fast and Fair TCP 
 
This session reports the simulation results of TCP Yam and compares it with other TCP 
protocols.  We use Network Simulation, version 2 (NS2), a simulation tool developed by 
University of California, Berkeley. 
 
A TCP connection is illustrated in Figure 39.  Each router has a FIFO queue that drops 
the packets in the tail during overflow.  The bandwidth is 10 Mbps and the delay is 1ms for 
the two links connecting the routers to the end hosts.  The bottleneck link located between the 
Source Sink R0 R1 
Buffer size = 18 
rwnd =20 
Capacity: 
1Mbps  
Capacity: 10Mbps 
Delay: 1ms 
Figure 39. Single TCP Connection  
 We discuss how two TCP connections interfere with each other in Figure 43 and Figure 
44.  The network configuration is shown in Figure 42 along with related parameters.  We 
observe the change of congestion window and throughput in our simulation.  As depicted in 
Figure 43 and Figure 44, TCP YAM achieves fairness between two similar YAM connections.   
 
Figure 43. The Congestion Window of Two TCP-YAM Connections 
 
Source 0 
Sink 0 
R0 R1 
Source 1 Sink 1 
Capacity: 1Mbps  
Delay: 4ms 
Capacity: 10Mbps 
Delay: 1ms 
Buffer size = 18 
rwnd =24 
Figure 42. Two TCP Connections
 Figure 46. Comparing the Throughput of YAM and Reno Connections 
Next, we show the results for three YAM connections. The network configuration and the 
traffic scenario are plotted in Figure 47 and Figure 48. It is shown in Figure 49 and 50 that 
the three connections compete for bandwidth and seems grabbing a similar share.  Their 
cwnds also move in synchrony. 
 
Source 0 Sink 0 
R0 R1 Source 1 Sink 1 
Source 2 Sink 2 
Capacity: 1Mbps  
Delay: 4ms 
Capacity: 10Mbps 
Delay: 1ms 
Buffer size = 18 
rwnd =24 
Figure 47. Three YAM Connections 
is compensated by the ranging process and served as a benchmark in many existing works.  
Thus, we make every ONUs have the same distance from the OLT. 
          In our simulation, we compare our scheme with the limited service.  Limited service 
has fair performance in bandwidth allocation and allows each ONU roughly equal access. We 
build a simulation testbed and set associated parameters to get performance statistics in our 
experiments.  Figure 50 shows our simulation model and Table 4 shows essential parameters 
used in our simulation experiments.  We develop our model in C++ language. 
          Regarding the traffic model, we consider that most network traffic such as http、ftp、
P2P applications can be characterized by self-similarity and long-range dependence (LRD).  
Therefore, we use synthetic self-similar traffic in our experiment.  The adopted packet 
generation program outputs a text file with packet arrival times to the ONUs and packet sizes.  
The packet generator will aggregate traffic from 40 sources (users) and generate the load of 
specified RU Mbit/s. This is bed into our program to simulate realistic traffic pattern. 
 
Figure 51: Average Packet Delay Performance 
We present the average packet delay in Figure 51.  As seen in the figure, DSBG Service 
has almost identical performance with limited service in light load.  But in heavy load, DSBG 
is 46% better than the limited service.  This is because in the limited service, the requested 
bandwidth of ONUs always exceeds the maximum window size in heavy load. The OLT can 
only assign WMAx bandwidth to the ONUs.  In addition, the number of packets in the ONU is 
much greater than WMAX.  The ONU cannot deliver all buffered packets in one cycle.  
Therefore, these packets have to stay in the buffer and suffer long delay time.  However, in 
the DSBG service, as long as the request is not greater than Bi,k calculated in Eq. (15), the 
OLT can satisfy the bandwidth even though it may be greater than the maximum window size.  
Thus, in heavy load, ONUs can deliver more packets in one cycle using DSBG service than 
the limited service. 
Maximum Delay Performance
0.001
0.01
0.1
1
10
0.1 0.2 0.3 0.4 0 .5 0 .6 0.7 0.8 0.9 1
Offer Load
De
lay
(s)
L im it Serv ice DSBG Serv ice
 
Figure 52. Maximum Packet Delay Performance 
We show the performance of the maximum packet delay in Fig. 52.  We can easily 
discover the difference between light load and heavy load.  The performance of the DSBG is 
bandwidth.  When it happens, the bandwidth will be enough to send all packets queued in the 
ONU including arriving during the waiting time. The entirely delay time is just a few short 
cycles.  Hence, please note that the performance of the DSBG service is better in the average 
delay but worse in the maximum delay than the limited service in heavy load. 
           In the Figure 52, we can note the opposite result in light load from heavy load.  This is 
because there are no drought spots in light load and consequently does not have any packets 
buffered in ONUs.  Besides, the DSBG service dose not restricted by WMAX, hence DSBG 
service can deliver more packet one time than limited service if the network burst suddenly. 
Throughput
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Offer Load
By
tes
L im it  Service DSBG Service
 
Figure 54. Throughput of DSBG and Limited Service 
Figure 54 shows the limited service and DSBG service has almost the same throughput. 
The throughput of the limited service is very good essentially.  It is difficult to get better 
throughput than the limited service. DSBG service is just little lower than the limited service 
in heavy load.  It is because the drought spots we talk about in the above section.  If the 
buffer is full before the drought spots, the packets arrived in this period would be drop.  It is 
synchronization simulation in degree 2.  We only consider the nodes that have two neighbors.  
Each curve represents the timing dynamics of each sensor node.  In this simulation, the 
degree is set to 2 and its topology is linear which yields one cluster.  It doesn’t have isolated 
sensor nodes.  All sensor nodes can synchronize in 3000 time units. 
 
Figure 55：Degree = 2 Time synchronization 
 
Figure 58：Degree = 10 Time synchronization 
Figure 57 represents the time synchronization simulation in degree 8 while Figure 58 
represents the time synchronization simulation in degree 10.  Degree 8 and 10 topologies 
require even less convergence time which is less than 1000 time units.  But the two figures’ 
rates of convergence characterize a salient difference.  In Figure 57 the local clock range is 
between 0.9 and 1.2 after 100 time units.  This range is drastically reduced to between 1.08 
and 1.15 in Figure 58 after 100 time units.   
Next we consider randomly deployed sensor nodes.  In our simulation, we randomly 
select the locations of the sensor nodes in the fixed sensing field.  We want to understand 
what the relevant factors are in the protocol.  They may contain factors such as sensor node’s 
sensing range and density.  If the degree of a node is equal to zero, then it is an isolated 
sensor node. Isolated sensor nodes are not considered clusters.  And we give a synchronized 
definition that the difference of all nodes is below 5% after 1000 time units.  We calculate the 
difference of all nodes’ clocks per 100 time units.  If it is below 0.1% before the 1000 time 
units, we use the new time units as the convergence time. 
 
Table 5(a)：Sensing Range 
 
 
Table 5 represents the difference sensing range simulations.  Table 5(a) shows the ten 
samples of 0.05 sensing range.  Table 5(b) and Table 5(c) are of 0.1 and 0.2 sensing ranges. 
Table 5 reveals that the self-coupled protocol in randomly deployment is suitable for 
high density sensor networks.  In Table 5(a) and Table 5(b) there are some isolated sensor 
nodes.  Most cases in these two tables have multiple clusters.  All nodes being in one cluster 
is the most important factor for time synchronization.  If there are multiple clusters, the time 
cannot converge.  On the other hand, Table 5(c) shows early synchronized state in high 
density and one cluster case. 
It is recognized that the performance of the hierarchical protocol degrades when the 
topology expands in size.  We set the sensing field in a +-0.6 X +-0.6 square.  We 
proportionally increase the node density in the simulation.  Table 6 shows the result.  
Although there are more sensor nodes to be synchronize, its performance still meets our 
expectation.  That means, unlike its hierarchical counterpart, the self-coupling protocol is 
insensitive to the size of the sensing field. 
4001.020Yes142414.36
3000.880Yes132112.76
2001.000Yes162313.96
2001.040Yes152013.96
2000.840Yes141813.08
3000.920Yes152112.96
2000.980Yes162214.04
4000.840Yes161813.16
3001.160Yes162314.16
4001.220Yes142113Nodes:50
Time range:
0~2
Sensing range:0.2
Sensing field:
+-0.3 X +-0.3
Calculation
Times
(K=1000)
Converged
clock
speed
SynchronizedCluster
(d>=1)
Min
degree
Max.
degree
Average
degree
Influence
Environment
 
 
 
Table 7(b)：Number of sensor nodes 
 
 
Table 7(c)：Number of sensor nodes 
10001.08~1.09Yes132112.53
10001.04~1.06Yes162212.87
10000.89~0.90Yes142613.89
10001.01~1.06Yes152212.39
10001.16~1.20Yes152413.27
10000.91~0.93Yes122913.04
10001.12~1.13Yes162313.41
10001.07~1.10Yes132614
10000.85~0.87Yes122613.52
10000.94~0.98Yes122414.17Nodes:150
Time range:
0~2
Sensing range:0.1
Sensing field:
+-0.3 X +-0.3
Calculation
Times
(K=1000)
Converged
clock
speed
SynchronizedCluster
(d>=1)
Min
degree
Max.
degree
Average
degree
Influence
Environment
  Figure 59：Degree & Cluster Trend 
 
When the topology contains a single cluster, it can achieve synchrony in certain amount 
of time.  Eq (18) is used to predict given an average degree, how many cluster there are in the 
sensing field.  When the average degree is 14.85243, one cluster is estimated.   
We next cope with the idea of random participation.  In other words, we would like to 
study the impact on the performance if the nodes are turned on and off in the synchronization 
process.  That is, a sensor does not need to always exchange time information with its 
neighbors.  In the simulation, we randomly selected one fifth of nodes to operate the self-
coupled scheme at any given iteration.  The one fifth nodes reference their neighbors to reach 
time synchronization and other nodes only reference the one fifth nodes in the sensing range.   
Evidently, random participation can reduce the energy consumption.  The level of saving 
depends on the portion of active sensors turned on for transmitting timing information in 
iteration.  It is interesting to see if random participation allows synchronization.  If so, how 
fast it can synchronize the nodes compared with the original method. 
0
2
4
6
8
10
12
14
16
18
0 2 4 6 8 10 12 14 16 18 20
Average degree
N
um
be
r 
of
 c
lu
st
er
Degree & Cluster Relation Trend
range 0.1, 0.2 and 0.3.  In Table 8(a) it is not synchronized because the node density is too 
lower and the reference nodes are only twenty.  Table 8(b) and table 8(c) show the 
synchronized cases. 
Table 9(a)：Random Participation 
 
 
Table 9(b)：Random Participation 
 
 
Table 9(c)：Random Participation 
 
 
3000.825Yes228454.46
2000.950Yes248957.71
3000.875Yes198055.81Nodes:200
Time range:
0~2
Sensing range:0.2
Sensing field:
+-0.3 X +-0.3
Calculation
Times
(K=1000)
Converged
clock
speed
SynchronizedMin
degree
Max.
degree
Average
degree
Influence
Environment
1001.000Yes4117198.67
1000.975Yes4016498.63
1001.350Yes4816497.99Nodes:200
Time range:
0~2
Sensing range:0.3
Sensing field:
+-0.3 X +-0.3
Calculation
Times
(K=1000)
Converged
clock
speed
SynchronizedMin
degree
Max.
degree
Average
degree
Influence
Environment
No23517.63
No43118.07
No33018.55Nodes:200
Time range:
0~2
Sensing range:0.1
Sensing field:
+-0.3 X +-0.3
SynchronizedMin
degree
Max.
degree
Average
degree
Influence
Environment
Sensing Range 250m 
Distribution Way Random Distributed 
Traffic Flow CBR of 1 packets/second 
Simulation Time 600seconds 
Node Density 100 to 200 nodes 
Packet Lost Rate 5% 
 
 
0
200
400
600
800
1000
0 200 400 600 800 1000
 
Figure 60: Sensor Node Distribution 
 
In the following, we show the simulation result and make discussion.  Figure 61 shows 
the variation of packet amount in different time.  The Flooding increases more quickly than 
Flossiping and SGDF because each node in the Flooding transmits packets to all the 
020000
40000
60000
80000
100000
100 110 120 130 140 150 160 170 180 190 200
number of nodes
nu
m
be
r 
of
 p
ac
ke
ts
Flooding
Flossiping
SGDF
 
Figure 62: Packet Overhead 
We evaluate the packet overhead to measure the power consumption.  In the WSNs, the 
power consumption of packet transmitting is most portion of the network.  So if we reduce 
the packet overhead, we also reduce the power consumption of the network.  In Figure 62, the 
packet amount of the Flooding is more than that of Flossiping and SGDF because the packet 
delivery in Flooding is in all directions.  Even in low node density, the Flooding still has high 
packet overhead.  As the node density doubles, the packet amount of Flooding also doubles.  
The packet overhead of the Flossiping is unlike Flooding.  When the node density is low, the 
overhead of Flossiping is low.  But when the node density increases, the probability of 
flooding also increases, and the packet amount increases quickly.  The nodes in SGDF just 
floods the packet to the neighbors closed to sink so the packet overhead of SGDF isn’t 
noticeably affected by the node density.  When the density increases to double, the packet 
overhead of the SGDF increases slowly.  So the SGDF has better performance than Flooding 
and Flossiping in the packet overhead. 
Packet average delay means the time between packets sent by source and received by the 
sink, it includes the time packets wait in the queue.  The delay time in the Flossping is longer 
than SGDF and Flooding.  Because the gossiping scheme is nondirectional, and most the 
packets in Flossiping received by the sink are in flooding mode, the Flossiping has low 
probability to starting flooding mode and it may have low chance to send packet in the 
shortest path.  So the packets may exist in the network in long time.  The Flooding broadcasts 
the packets, all the routes of packets are included.  So the shortest path is in the Flooding 
routing and the packet delay is low.  In SGDF, we use the information of gradient to deliver 
packets, the packets are sent to the nodes with lower gradient, and the scheme may cause 
several routes.  The shortest path is also included in the SGDF routes.  So the SGDF also has 
low average delay about 1ms.  Figure 64 shows the performances of the SGDF and Flooding 
are similar. 
 
0
1
2
3
4
100 110 120 130 140 150 160 170 180 190 200
number of nodes
tim
e(
m
s)
Flooding
SGDF
Flossiping
 
Figure 64: The Packet Average Delay 
Next, we simulate our protocol in different thresholds and discuss the performance.  In 
the simulation, 100 nodes are random distributed in 1000m x 1000m.  In Figure 65, the x-axis 
We consider the both performances, and use Eq(19) to combine them.   
)
)0()0.1(
)0()()(1()
)0()0.1(
)()0.1((
PP
PiPx
RR
iRRx −
−−+−
−                     Eq (19) 
In Eq (19), the x is the weight that we care about the packet overhead and the delivery 
ratio.  R(i) means the packet delivery ratio when the threshold = i, for example, R(1.0) means 
the value of packet delivery ratio when the threshold=1.0, and it is 0.98.  P(i) means the 
packet overhead when the threshold = i, for example, P(0) means the value of packet 
overhead when threshold=0, and it is about 2205.  We normalize these two parameters and 
give the weight to them.  If we emphasize the delivery ratio more, we set x bigger.  If we 
emphasize the packet overhead more, we set the smaller x.  After we combine the delivery 
ratio and packet overhead through Eq (19), we obtain the result in Figure 66.  In the Figure, 
we set x = 0.4 , 0.5 and 0.6.  When x = 0.4, we emphasize the packet overhead little more than 
the delivery ratio.  When x is bigger, the bigger threshold has better performance.  In figure 
4.8, we can see when the threshold = 0.4, the SGDF has the balance performance. 
0
0.2
0.4
0.6
0.8
0 0.2 0.4 0.6 0.8 1
x=0.4
x=0.5
x=0.6
 
Figure 67: Packet Overhead & Delivery Ratio 
 106
6 Reference 
[1] MLDesign Technologies, Inc., “ALOHA Protocol,” URL: www.mldesigner.com  
[2] Theodore S. Rappaport, “WIRELESS COMMUNICATIONS – PRINCIPLES AND 
PRACTICE 2nd EDITION,” PRENTICE HALL, pp. 462-466, 2002  
[3] Branko Bjelajac, “ADAPTIVE-ALOHA MULTIPLE ACCESS PROTOCOL FOR 
MOBILE SATELLITE SYSTEMS,” In the Proc. of Vehicular Technology Conference, 
1996, Mobile Technology for the Human Race, IEEE 46th, Vol. 3, pp. 1756-1760, 28 
Apr. – 1 May 1996  
[4] “Identification technology automatic identification and data capture tech-niques – 
radio frequency identifiation for item management air interface – part 6: parameters for 
air interface communications at 860 – 960MHz,” ISO/IEC FDIS 18000-6, Nov. 2003. 
[5] S. Lee, S. Joo, and C. Lee, “An enhanced dynamic framed slotted ALOHA algorithm 
for RFID tag identification,” in Proc. MobiQuitous 2, pp. 166-172, Jul. 2005. 
[6] J. Cha and J. Kim, “Novel anti-collision algorithms for fast object identification in 
RFID system,” in Proc. Parallel and Distibuted System, vol. 2, pp. 63-67, Jul. 2005. 
[7] J. Park, M. Y. Chung and T. J. Lee, “Identification of RFID Tags in Framed-Slotted 
ALOHA with Robust Estimation and Binary Selection,” IEEE Communication Letters, 
vol. 11, issue 5, pp. 452-454, Jan. 2007. 
[8] H. Vogt, “Multiple Object Identification with Passive RFID Tags,” in the Proc. of the 
IEEE International Conference, Tunisia, Vol. 3, pp. 651-656, Oct. 2002.  
[9] K. H. Rosen, “Discrete Mathematics and its Application”, 5th Ed.. Mc-Graw Hill, 
2003. 
[10] C. Law, K. Lee, and K. Y. Siu, “Efficient Memoryless Protocol for Tag 
Identification,” in the Proc. of the 4th International Workshop on Discrete Algorithms 
and Methods for Mobile Computing and Communications, Boston, MA, United States, 
pp. 75-84, Aug. 2000. 
[11] T. P. Wang, “Enhanced Binary Search with Cut-Through Operation for Anti-
collision in RFID Systems,” IEEE Communication Letters, vol. 10, issue 4, pp. 236-238, 
Apr. 2006. 
[12] J. H. Choi, D. Lee and H. Lee, “Bi-Slotted Tree Based Anti-Collision Protocols for 
Fast Tag Identification in RFID Systems,” IEEE Communication Letters, vol. 10, issue 
12, pp. 861-863, Dec. 2006. 
[13]  J. Postel, “Transmission Control Protocol”, Internet RFC 793, Sep 1981. 
[14] H. Balakrishnan, S. Seshan, E. Amir and R.H. Katz, “Improving TCP/IP 
performance over wireless networks”, in the Proc. of MOBICOM’95, Nov 1995. 
[15] I. F. Akyildiz, G. Morabito, and S. Palazzo, “TCP-peach: A new congestion control 
scheme for satellite IP networks,” IEEE/ACM Trans. Networking, vol. 9, June. 2001. 
[16] NS2 Learning Guide. http://140.116.72.80/~smallko/ns2/ns2.htm 
[17] G. Kramer, B. Mukherjee, and G. Pesavento, “IPACT: A Dynamic Protocol for an 
Ethernet PON (EPON),” IEEE Communications Magazine, Vol. 40, No. 2, pp. 74-80, 
Feb. 2002. 
[18] G. Kramer, B. Mukherjee, S. Dixit, Y. Ye, and R. Hirth, “Supporting Differentiated 
Classes of Service in Ethernet Passive Optical Networks,” Journal of Optical Networking, 
Vol. 1, Nos. 8&9, pp. 280-298, Aug. 2002. 
 108
on Telecommunications/Service Assurance with Partial and Intermittent Resources 
Conference/ E-Learning on Telecommunications Workshop, pp. 340–345, Jul. 2005. 
[34] W. Su and I. F. Akyildiz, “Time-Diffusion Synchronization Protocol for Wireless 
Sensor Networks,”IEEE/ACM Transactions on Networking, vol. 13, pp. 384-397, Apr. 
2005. 
[35] S. Ganeriwal, R. Kumar, and M. B. Srivastava,“Timing-sync Protocol for Sensor 
Networks,” in the Proc. of the 1st international conference on Embedded networked 
sensor systems, Nov. 2003, Los Angels, California, USA. 
[36] A. Ahmed, H. Shi, and Y. Shang, “A Survey on Network Protocols for Wireless 
Sensor Networks,” in the Proc. of International Conference on Information Technology: 
Research and Education, pp. 301-305, Aug. 2003. 
[37] Q. Jiang, and D. Manivannan, ”Routing Protocols for Sensor Networks,” in the Proc. 
of the 1st IEEE Consumer Communications and Networking Conference, 2004. pp. 93-98, 
Jan. 2004. 
[38] I. F. Akyildiz, W. Su; Y. Sankarasubramaniam, and E. Cayirci, ”A Survey on Sensor 
Networks,” IEEE Communication Magazine, vol. 40, pp. 102-124, Aug. 2002. 
[39] D. B. Johnson, and D. A. Maltz, “Dynamic Source Routing in Ad Hoc Wireless 
Networks,” Mobile Computing, T. Imielinski and H. Korth, Eds. Kluwer Academic 
Publishers, ch. 5, pp. 153-181, 1996. 
[40] B. Karp, and H. T. Kung, “GPSR: Greedy Perimeter Stateless Routing for Wireless 
Networks,” in the Proc. of the 6th Annual International Conference on Mobile 
Computing and Networking, Aug. 2000. 
[41] J. Kulik, W. Heinzelman, and H. Balakrishnan, “Negotiation-based Protocols for 
Disseminating Information in Wireless Sensor Networks,” ACM Wireless Networks, vol. 
8, issue 2/3, Mar. 2002. 
[42] M. T. Sun, X. Ma, J. Li, and, X. Liu, ”A Greedy Smart Path Pruning Strategy for 
Geographical Routing in Wireless Networks,” IEEE Military Communications 
Conference, Oct. 2005. 
[43] Y. Yu, R. Govindan, and D. Estrin, “Geographical and Energy-Aware Routing: A 
Recursive Data Dissemination Protocol for Wireless Sensor Networks,” in Proc. of the 
7th Annual ACM/IEEE International Conference on Mobile Computing and Networking, 
Jul. 2001. 
[44] C. E. Perkins, and E. M. Royer, ”Ad-hoc On-Demand Distance Vector Routing,“ in 
the Proc. of the 2nd IEEE Workshop on Mobile Computing Systems and Applications, 
1999. pp. 90-100, Feb. 1999.  
[45] H. Li, and M. Singhal, “A Scalable Routing Protocol for Ad Hoc Networks,” IEEE 
61st Vehicular Technology Conference, vol. 4, pp. 2498-2503, 2005. 
[46] Y. Zhang; and L. Cheng, “Flossiping：A New Routing Protocol for Wireless Sensor 
Networks,” in the Proc. of IEEE International Conference on Networking, Sensing and 
Control, 2004. vol. 2, pp. 1218-1223, 2004. 
 
 
 
 
 110
7 Project Self-Evaluation 
 
In the following, we evaluate the subproject from four perspectives including the 
completeness of the scope of the research work, publications of the results, training and 
education, budge control, and possible practical applications.   
 
Completeness of the Scope of the Research Work 
Excellent.  In this report, we provide results promised to be delivered in our research 
proposal.  Six protocols are designed to solve the problems in three networking problem 
areas in the sea surface salinity data collection system.  As a matter of fact, additional 
goals were identified along our research effort and completed in this project time frame.   
 
Publications and Presentations: 
Good.  Manuscripts are prepared based on the results of this subproject.  They are 
accepted and presented in seven international conferences listed in the following.     
1. Wei Yen, Ching-Wei Chen and Cheng-Hsiang Yang, “Single Gossiping with 
Directional Flooding Routing Protocol in Wireless Sensor Networks,” in the Proc. of 
the 3rd IEEE Conference on Industrial Electronics and Applications (ICIEA 2008), 
Singapore, June 3-5, 2008. 
2. Wei Yen, T. W. Yue, and Yu-Cheng Wang, “Fast Sensor Identification Technology 
for Sea Surface Salinity Measurement,” in the Prod. Of 2008 IEEE International 
Conference on Systems, Man and Cybernetics, Singapore, Oct. 12-15, 2008. 
3. Wei Yen and Hsueh-Sheng Huang, “On Pulse Coupling for Time Synchronization in 
Wireless Sensor Networks,” to appear in the Proc. of ICICIC 2007, Kumamoto, Japan, 
Sept. 2007.    
4. Wei Yen and Hsin-Kai Wang, “TCP YAM: A High Performance TCP,” in the Proc. 
of CITSA 2007, Orlando, Florida, U.S.A., Jul. 2007.  
5. Wei Yen and Shih-Wei Tseng, “Delay Sensitive EPON Dynamic Bandwidth 
Allocation Mechanism,” in the Proc. of 2006 IEEE International Conference on 
Systems, Man, and Cybernetics, Taipei, Taiwan Oct. 2006.  
 112
slight modification to the current system.  And, according to the simulation, they will all 
yield better performance compared with the existing systems.  Some technology we 
created in this subproject seems to have great potential in the RFID area.  The effort of 
finding industrial partners and sponsors is underway.    
 
