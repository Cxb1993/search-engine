distributed  
Sensor fusion based distributed and robust control 
mechanism； the number of local filters is equal to 
the number of controllers involved in the fusion. 
Thus the system is completely distributed. The set of 
controllers used in the control architecture depends 
on the specific application. In mobile robot 
navigation, at least one controller responsible for 
guiding the robot to the destination point, which 
receives information from the internal sensors of the 
robot, should be present. The individual controllers 
receive the sensory information they are designed to 
deal with, either directly or through a fusion 
engine. Such data fusion is used either for a better 
environmental representation or for noise reduction. 
英文關鍵詞： Visual SLAM, Detection and Recognition, Multi-sensor 
Fusion Algorithms, Covariance Intersection (CI) 
 
 2 
行政院國家科學委員會專題研究計畫成果報告 
國科會專題研究計畫成果報告撰寫格式說明 
Preparation of NSC Project Reports 
     計畫編號：NSC 100-2221-E-002-073- 
執行期限：100年 8月 1日至 101年 7月 31日 
主持人：羅仁權   國立臺灣大學電機工程學系暨研究所 
計畫參與人員：徐瑋隆、林献章、吳彥彰、林佩嫺、張瓈文、葉耕成 
 
一、 摘要 
為了能夠在日常生活的環境下使用，發展出一個行為基於感測器的多功能自主性機器人
是一個相當重要且具有挑戰性的研究。而要能夠在真實世界達成服務應用的機器人必須要有
物體辨識跟運動規劃的系統。這樣的要求包含了能夠定位出機器人位置來導航，辨識物體來
操作機器手臂，同時跟運動規劃的子系統溝通的視覺系統。多感測器融合的控制能夠增加機
器人在自主移動及避障時的敏捷性。本計畫會發展出基於多感測器融合的分散式運動控制系
統，以及基於多重視覺的即時同步定位及地圖建立技術。 
    多感測器融合的分散式運動控制系統能夠讓各感測器互相合作，並將估測誤差最小
化。採用分散式則可以達成規模上的彈性及減少運算的負擔。多重視覺的同步定位及地圖建
立技術在機器人學中是基礎且複雜的課題。本計畫採用的方法預測了環境的結構，在機器人
仍未勘測前便產生出對未勘測區域的假設，尤其是針對鄰近已勘測區域的未勘測區域，因為
這些區域是下一個勘測的目標，所以機器人能夠馬上運用這些資訊。 
   整合即時的運動規劃系統、智慧化的同步定位及地圖建立技術及分散式控制系統能夠
達成許多不同的任務，像是導引服務、大樓保全、老人及小孩照顧、娛樂及教育等。 
關鍵詞：視覺同時建地圖與定位導航、影像偵測與追蹤、多感測器融合理論 
 
二、 英文摘要 
In order to achieve the tasks for daily environment usages in real world, sensory information is 
essential. Thus, the development of general-purpose autonomous sensory based behavior humanoid 
robot system is an essential and challenging research issue. The humanoid robot system for real 
world service applications must integrate with object recognition and motion planning systems. 
These requirements involve the vision system capable of self-localization for navigation utility and 
object recognition for manipulation tasks, while communicating with the motion planning 
subsystem. The multi-sensor fusion control increases the ability of autonomous driving and obstacle 
avoidance of the robot. This project will develop multi sensor fusion based distributed motion 
control system, real time multi vision based simultaneous localization and mapping(SLAM) 
techniques and sensor fusion based distributed  
Multisensor fusion based distributed motion control system will develop cooperative sensors 
provide relative posture estimates between neighboring axles, and minimize posture estimate error. 
These sensor fusion methods are distributed for scalability and reduced axle level computational 
burden. 
 4 
 
圖 4.1.  立體視覺 
三維物體呈現在平面的圖片上，會有視差的情況發生，利用這個視差，以三角的幾何學
去運算，理論上便可以測量出物體的深度。在以上的立體視覺系統中只要在左右的攝影機中
找到對應的點，可由簡單的三角比例關係得到空間中座標點的深度，如圖 4.1.。相關推導如
下。 
z z f bz f
b b d d
−
= ⇒ =
−
   (1) 
其中 r ld x x= −  
其中，d稱之為視差(Disparity) ，Z則是三物體與基線的距離，也就是我們想求得的數據。視
差法的根據，便是基於兩個不同深度的點，在平面上的視差會有所不同。深度較淺的點會有
較大的視差，反之，深度較大的點則會有較小的視差，且基準線(b:BaseLine)可經由量測出和
焦距經由內部參數校正所求得，即可算出空間中點的深度座標值 z 。 
找出雙眼視覺中的特徵點在左右影像的對應關係是非常重要，有一重要的步驟就是立體
匹配( Stereo matching )，匹配的目的就是要尋找右影像中某一元素最相似於已知的左影像中
之元素(或左影像中某一元素最相似於已知的右影像中之元素)。在無限制條件的情形之下，匹
配的程序顯然會花費相當多的時間，因為只要影像對應點不正確，所計算出的三維景深資訊
就會有偏差，在實際的應用上必需由影像所提供的資訊去判斷是否是相同的點所投影的座
標，所以，通常在很多的立體匹配方法中皆假設左右影像是共線的( Collinear )，使得在尋找
對應點( Corresponding points )能在一維的範圍內進行。而共線的條件即是左右相機的光軸
( Optical axes )必須是平行且垂直於基準線 ( Baseline，兩透鏡中心的連線 )。搜尋的影像元素
可以是一個點、一個區域或某種特徵。無論對應的元素為點、區域或特徵，在其左右圖像搜
尋對應點( Corresponding points)的方法大致可以分為兩類： Correlation-based algorithms及
Feature-Based algorithms。 
4.1.1.  Correlation-based Methods 
 6 
4.1.3. 立體視覺之估測誤差模型： 
Left
Camera
(reference)
Right
Camera
b c
f
r
z
xy
z’
x’y’ r = r’
d
c’
),( cr σσ
Uncertainty
Covariance ellipsoid
X
Y
Z
World
Coordinate
 
圖 4.3.  立體視覺之估測誤差模型 
上圖為雙鏡頭立體視覺估測模型，利用相似三角形的幾何關係，可得 
d
fbZ
d
brrY
d
bccX =−=−= )()( 00                    (4) 
其中 
b: baseline (兩鏡頭中心之距離) 
f: focus length (鏡頭焦距) 
c0: 影像行中心 (mm/pixel)單位  
r0: 影像列中心 (mm/pixel)單位  
c: 影像特徵點行座標 (mm/pixel)單位  
r: 影像特徵點列座標 (mm/pixel)單位  
d: 為左右鏡頭特徵點行座標視差  
X: 全域座標 X 軸 
Y: 全域座標 Y 軸 
Z: 全域座標 Z 軸 
x: 影像座標 x 軸 
y: 影像座標 y 軸 
z: 影像座標 z 軸 
 
然而考慮由於鏡頭量化過程中的雜訊誤差和測量特徵點不確定性。 ),,( dcr σσσ 通常建模為
不相關的零均值高斯隨機變量的方差。利用一階近似誤差傳遞變量分析針對多元高斯分佈做
推衍，可以得到下面的協方差矩陣: 
 8 
 
圖 4.7. 超音波感測機率分佈 
4.3.  超音波建構環境地圖 
4.3.1.  環境探索演算法： 
1. 利用超音波陣列深度資料掃描未知環境。 
2. 遵循演算法開啟 Grid Map。 
3. 建立 Grid Map得知環境障礙物位置。 
4. 使用 256階層 Grid Map描述超音波不確定性。 
4.3.2.  Grid map演算法： 
1. 參考圖 3.8. 
2. 超音波 open angle 為 50度。 
3. Grid map上各個 grid的初始值為 255。 
4. 求得超音波的深度資訊。 
5. 再求出 50度 open angle 的點(紅點)。 
6. 利用超音波位置(藍點)與紅點位置求出三條直線方程式。  
7. 代入 grid map上的每一個座標點，符合範圍內 grid的減 5 
8. 當 grid的值小於 threshold value T時，將 grid的值設為 0，即可視為此 grid為 free 
space。 
 
 10 
這裡所使用的追踪演算法是屬於差異模型，其架構可以圖二表示。 
 
圖 4.10.  視覺追踪架構 
 
由圖二可看出，這個演算法包括兩個分類器，一個是在初始化時所訓練的，在追踪的過
程中都保持不變，另一個分類器則是在追踪的過程中，隨著目標外觀的改變會不斷的更新。
用兩個分類器的目的是為了兼顧追踪的穩定性和適應性。如果不更新分類器，則無法適應目
標外觀的變化。但每次更新分類器會引入一點誤差，累積的誤差會導致追踪失敗，而有一個
初始化時所訓練，並且保持固定的分類器會讓整個追踪更穩定。分類器是用一種十分出名的
整體學習法(ensemble learning)，稱為 AdaBoost 所訓練而成的。但是這裡所用的學習法和
傳統的 AdaBoost又略有不同。傳統的 AdaBoost會不斷加重判斷誤錯的取樣的權重，但我們
的演算法在權重的大小有一個上限，這個機制會讓分類器比較不容易受到雜訊的影響，進而
使追踪更加穩定。再來，我們使用了兩種不同的 tracker。一種是 Patch-level tracker 另
一種是 Pixel-level tracker。利用信心融合的方式(Confidence Fusion)將兩種 tracker 結
合在一起。兩種不同的 tracker 可以互相彌補不足的部分，我們從圖五就可以看出來 
Patch-level tracker 所預測的物體位置可能有一些並不是物體 所以由 Pixel-level 
tracker 所建立的信心地圖，彌補預測錯誤的位置 。而我們所用的信心融合的方式
(Confidence Fusion) 利用 算是(1)和(2) 決定 兩種 tracker的權重  
                     (1) 
         (2)  
 
               
圖 4.11.  Patch-level tracker               圖 4.12.  Pixel-level tracker 
 12 
 
 
 
光流法的實做效果，右圖顯示左邊兩張 frame間偵測到的物體相對移動。 
 
五、 結果及討論 
5.1.  立體視覺結果 
 
Left-image                        Right-image 
 
 
 
Rectify-left                                  Rectify-right 
 
 14 
  
  
圖 4.5.  超音波建立環境地圖 
  
 16 
圖 5.6.上方是影片 Board 的第 7569 和 661 的 frame 下方是影片 Lemming 的第 16540 和
1000的 frame圖中的黃色框框就是我們的演算法  
 
5.5手勢辨識 
隨著科技的進步，機器人在生活中扮演著極為重要的角色。在現今社會中，人們有更多的機
會接觸到機器人的功能，像是生活中常見的智慧型服務機器人以及救援機器人。因此人機互
動在當今成為一個重要的研究課題。在這個主題下，我們研究出人機互動最直接的方式。因
此我們介紹一個結合式的手勢辨識方法。此方法是利用電腦視覺以及數位影像處理的方式來
達成並且介紹了各式各樣人與機器人的互動方式。 
手勢辨識對人機互動來說，是非常重要且快速的;符號語言對於有身心障礙的患者來說，是最
常見也是最直接的工具。身心障礙者可以藉由手勢向看護或機器人進行溝通與傳達。因此，
我們介紹一個結合式的手勢辨識方式，這方式運用兩個不同的辨識器來完成手勢辨識演算
法；換言之，辨識器的目的就是要展現良好的識別力。為了達到良好辨識力，我們所結合的
兩個辨識器能互補相互得優缺點。其中一個辨識器藉由骨架分析(HSR)來辨識手勢，另一個則
是為了要藉由 SVM 機器學習演算法，來辨識手勢。此外要我們利用多樣化的特徵來表示影
像，並交給機器學習演算法，進行手勢辨識。最後這兩個辨識器經由 CAR 公式 的整合來決
定手勢。在 CAR 公式中包含了一系列的規則;例如轉換辨識器和利用結合辨識器的方式，此
外我們在訓練過程中 所利用的手勢影像有兩種，一部分是利用 Bosphorus的手勢資料庫，其
他一部分是我們自己所蒐集的手勢影像。總的來說，我們經過完整的概念和成功的實驗證明
我們手勢辨識的方法，可以實用於自行開發的智慧型機器人上並且實現人機互動。 
手指骨骼辨識器 
利用影像處理，透過手指骨骼的計算，來辨識手勢。在影像處理中，利用濾膚色、距離轉換
和八邊形近似法，來繪製出手指的骨骼，並且計算且辨識。下面顯示的圖一為手指骨骼辨識
器的流程圖，圖二為辨識結果。 
 18 
 
人機互動 
將上述的辨識器結合並且應用於機器人上，如此一來使用者可以快速、直接 且方便的，傳達
指令給機器人，讓機器人可以快速的回應，並和使用者互動。圖六為 使用者和本實驗室所研
發的機器平台所做的互動。透過簡單的手勢辨識 老人可以快速簡潔的讓機器人幫助以及提供
他的需求。 
 
5.6表情辨識 
多人臉部的表情及由此多人表情感受此時的環境氛圍之辨識系統，智慧型機器人利用視
覺影像系統取得環境中的影像瞭解氛圍並與人們產生互動。利用主動外觀模型(Active 
Appearance Models, AAM)來追蹤使用者的臉部特徵，並利用使用者的臉部特徵資訊去判斷臉
部表情，在表情辨識系統中，我們主要結合兩方法來萃取臉部特徵，分別是特徵向量與主動
外觀模型的差異量的方法，利用這些特徵與各表情間的相對關係來辨識表情。此外，我們將
單人的表情辨識發展成在一個環境中能夠同時辨識多人情緒的氛圍偵測系統，藉此來得知此
 20 
表情辨識 Algorithm of Facial Expression Recognition 
 
AAM則是利用分離資料圖庫中人臉的 texture及 shape，分別當成向量，由 PCA演算法得
出 eigenvectors，然後再藉由一連串的 iteration來調整重組這些 eigenvectors的比重。 
 
左圖為 texture 向量的 mean texture與其餘 eigenvectors，右圖為 shape向量的 mean 
shape與其餘 eigenvectors。 
 
 
 
因此人臉可藉由將 eigen shape 與 eigen texture利用線性組合表示，如上圖。 
 
表情辨識系統主要結合兩方法 : Feature Vectors based Approach (FVA)與
Differential-AAM Features based Approach (DAFA)。FVA分析臉部特徵點的幾何距離與比
例，DAFA在 manifold space 中分析 differential-AAM features而此特徵包含光影與臉部
紋路等等。經由 fusion後，可以得到較高的辨識率。下圖為主要的流程圖及實驗結果。 
 22 
 24 
 
六、 計畫成果自評 
本研究計畫的年度進度與計畫書所規畫的進度符合，視覺系統已經建立完成，並有相當
多的視覺偵測與追蹤成果，超音波定位系統完成，機器人實驗平台架設完畢，進行最後整合
測試。 
 
七、 發表論文 
期刊論文 
1. Ren C. Luo, Ogst Chen, “Wireless and Pyroelectric Sensory Fusion System for Indoor Human/Robot 
Localization and Monitoring” , IEEE/ASME Transactions on Mechatronics, Accepted 2011 (SCI)(EI) 
2. Ren C. Luo and Chun Chi Lai, “Consistent Info-Map Building Using Multi-Sensor Fusion for Indoor 
Service Robot” IEEE Transactions on Industrial Electronics, Accepted, 2011   (SCI) (EI) 
3. Ren C. Luo, Ogst Chen, “Wireless Pyroelectric Sensory Fusion System for Indoor Multiple Human 
Tracking and Localization” IEEE Transactions on Industrial Electronics, Accepted, 2011(SCI) (EI) 
4. Ren C.Luo and Yi Ven. Peng, "Advances of Mechatronics and Robotics: Challenges and 
Perspectives", IEEE Industrial Electronics Magazine Vol.5, No.3 , pp. 27-34, 2011  (SCI) (EI) 
5. Ren C. Luo, Chih C. Chang, and Chun Chi Lai, “Multisensor Fusion and Integration: Theories, 
Applications, and its Perspectives”,  IEEE Sensors Journal , Vol.11, No.12 , pp. 3122-3138, 2011  
(SCI) (EI) 
 
研討會論文 
1. Ren Luo, Pei-Hsien Lin, Li Wen Chang, “Confidence Fusion Based Emotion Recognition of 
Multiple Persons for Human-Robot Interaction”, 2012 IEEE/RSJ International Conference on 
Intelligent Robots and Systems (IROS 2012), October 7-12, 2012, Vilamoura, Algarve, Portugal.   
(EI) 
2. Ren Luo, Kuan-Yu Chen, Ming Hsiao, “Visual Simultaneous Localization and Mapping Using 
Stereo Vision with Human Body Elimination for Service Robotics”, 2012 IEEE/ASME 
International Conference on Advanced Intelligent Mechatronics (AIM 2012), July 11-14, 2012,  
Kaohsiung, Taiwan. (EI) 
3. Ren C. Luo and Wei-Lung Hsu, “Autonomous mobile robot localization based on multisensor 
fusion approach”, 2012 IEEE International Symposium on Industrial Electronics (ISIE 2012), 
28-31 May 2012, Hangzhou, China. (EI) 
4. Ren C. Luo and Ogst Chen, “Indoor Robot/human Localization Using Dynamic Triangulation 
and Wireless Pyroelectric Infrared Sensory Fusion Approaches”, 2012 IEEE International 
Conference on Robotics and Automation (ICRA 2012), May 14-18, 2012, Minnesota, USA.   
(EI) 
5. Ren C. Luo and C.C. Lai, “Concurrent Indoor Map Construction and Patterns of Interests 
Recognition Using Sensory Fusion Approach for Service Robotics”,  2012 IEEE International 
Conference on Robotics and Automation (ICRA 2012), May 14-18, 2012, Minnesota, USA.   
(EI) 
 26 
八、 參考文獻 
1. H. R. Hashemipour, S. Roy, and A. J. Laub, “Decentralized structures for parallel Kalman 
filtering,” IEEE Trans. Automat. Control, vol. 33,pp. 88–93, Jan. 1988. 
2. Dietrich. Fraenken, Andreas . Huepper, “Improved Fast Covariance Intersection for Distributed 
Data Fusion”, IEEE Trans. International Conference on Information Fusion, July, 2005 
3. Ren C. Luo, Ogst Chen, Liang Chao Tu, "Nodes Localization through Data Fusion in Sensor 
Network," AINA 2005. (accepted) 
4. Xin Ma, Wei Liu, Yibin Li, Rui Song, “LVQ Neural Network Based Target Differentiation 
Method for Mobile Robot”, IEEE ICRA, July, 2005 
5.  Kwang Ho An; Dong Hyun Yoo; Sung Uk Jung; Myung Jin Chung, “Real-Time Multi-View 
Face Tracking for Human-Robot Interaction”, IEEE International Conference on Development 
and Learning, July, 2005 
6.  Zhou, H.; Sakane, S, “Sensor planning for mobile robot localization using Bayesian network 
representation and inference”, IEEE Intellignet Robot and Systems, Oct, 2002 
7. Murphy, R.R, “Dempster-Shafer theory for sensor fusion in autonomous mobile robots”, IEEE 
Transactions on Robotics and Automaiton VOL. 14, NO.2, April, 1998 
8.  Torgny Brogardh, “Present and future robot control development-an industrial perspective,” 
Annual Reviews in Control, 31 (2007), 69–79 
9.  Veljko Potkonjak, Spyros Tzafestas, Dragan Kostic, Goran Djordjevic, “Human-like behavior 
of robot arms: general considerations and the handwriting task,” Robotics and Computer 
Integrated Manufacturing, 17 (2001), 305-315 
10. Seungsu Kim, ChangHwan Kim, Jong Hyeon Park, “Human-like Arm Motion Generation for 
Humanoid Robots Using Motion Capture Database,” Proceedings of the 2006 IEEE/RSJ, 
October 9 - 15, 2006, Beijing, China 
11.  Aaron Edsinger, Charles C. Kemp, “Human-Robot Interaction for Cooperative Manipulation: 
Handing Objects to One Another,” 16th IEEE International Conference on Robot & Human 
Interactive communication, August 26 - 29, 2007, Jeju, Korea  
12.  Ashutosh Saxena, Justin Driemeyer, Justin Kearns, Chioma Osondu, and Andrew Y. Ng 
“Learning to Grasp Novel Objects Using Vision.” Experimental Robotics, STAR 39, pp. 33–42. 
Springer-Verlag Berlin Heidelberg 2008 
13. Ashutosh Saxena, Justin Driemeyer, Justin Kearns, Chioma Osondu, and Andrew Y. Ng 
“Learning to Grasp Novel Objects Using Vision.” Experimental Robotics, STAR 39, pp. 33–42. 
Springer-Verlag Berlin Heidelberg 2008 [7] Pedram Azad, Tamim Asfour, Ruediger Dillmann, 
“Stereo-based 6D object Localization for Grasping with Humanoid Robot Systems,” 
Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, 
Oct 29 - Nov 2, 2007, San Diego, CA, USA  
14.  Mikolajczyk, K., Schmid, C.: A performance evaluation of local descriptors. In:CVPR. 
Volume 2. (2003) 257 – 263 
15. A. Yilmaz, O. Javed, and M. Shah, "Object tracking: A survey," ACM Computing Surveys 
(CSUR), vol. 38, 2006. 
16.  K. Shafique and M. Shah, "A non-iterative greedy algorithm for multi-frame point 
correspondence," IEEE International Conference on Computer Vision, pp. 110-115, 2003. 
17. A. Yilmaz, X. Li, and M. Shah, "Contour-Based Object Tracking with Occlusion Handling in 
Video Acquired Using Mobile Cameras," IEEE Transactions on Pattern Analysis and Machine 
Intelligence, pp. 1531-1536, 2004. 
18.  H. Bang, D. Yu, S. Lee, I. H. Suh, " An object recognition system based on color 
co-occurrence histogram and geometric relations of pyramidal image patches," Intelligent 
Robots and Systems, 2008.(IROS 2008). Proceedings. 2008 IEEE/RSJ International 
Conference on,, September 22-26, 2008 Page(s):3591 – 3596. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/08
國科會補助計畫
計畫名稱: 多感測器融合理論應用於視覺影像同步定位地圖建置及運動規劃
計畫主持人: 羅仁權
計畫編號: 100-2221-E-002-073- 學門領域: 自動化系統整合技術
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本研究計畫的年度進度與計畫書所規畫的進度符合，視覺系統已經建立完成，
並有相當多的視覺偵測與追蹤成果，超音波定位系統完成，機器人實驗平台架
設完畢，進行最後整合測試。 
2011 IRHOCS 國際機器人實作競賽 冠軍 
2011 全國機器人競賽夢想實現組 亞軍 
2011 新光保全機器人競賽 佳作 
2010 IRHOCS 國際機器人實作競賽 冠軍 
2010 全國機器人競賽夢想實現組 季軍 
2010 新光保全機器人競賽 亞軍 
2010 東元科技創意競賽 亞軍 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
