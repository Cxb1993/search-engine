develop more applicable, we setup a multiline emotion 
speech recognition system that can be run in 
parallel. Whenever an anger emotion is being 
confirmed, a warning will be issued to the desktop of 
a manager. The manager can then determine how to 
react such that the enterprise image can be retained.
英文關鍵詞： Speech emotion recognition, Call center system, 
Emotion corpus database 
 
用量化情緒種類語音模型的方式，對測試語句產生視覺化直觀的評分數據，對於擴大了
對其他領域的應用。但在實際應用方面，短語句的中文表達是不盡合理的，連續語音交
談才是人類自然之言語表達方式。伴隨環境的改變及周遭事物之互動，人類的情緒會隨
著時間軸而起伏，藉由非侵入式之語音信號，持續不斷偵測語者的情緒起伏，才能真正
應用在電話客服中心以及顧客的行為分析上。但是如何在連續語音中標記情緒類別，是
一件相當困難的工作，為能建構一個更為完整及正確的連續語句與料庫，我們藉由 Luis 
von Ahn 所提出的人腦運算(Human Computing)的架構，讓標記者能協助標記我們所錄製
的長語句，建置更完整的情緒語音資料庫，藉以發展出更具實用價值的連續中文語音情
緒辨識技術，持續改善其在 VoIP 客服系統上應用的有效性。 
本計畫的第二個目標是希望找出最具有識別能力的語音特徵參數，傳統上，研究者
擷取數量很大的語音特徵參數建構成特徵向量，做為分類氣的輸入參數，但過多的特徵
參數提高了運算所需要的時間，讓多線即時分析面臨挑戰，因此我們藉由分析各種語音
特徵參數對辨識率的影響，希望找出最具有鑑別性的參數，藉由減少特徵參數的個數，
提升處理效能。 
本計畫第三個目標是希望建構一個能夠處理多線客服電話之語音情緒辨識系統，並
對將這個系統發展成雲端服務(SaaS)的可行性進行研究，利用本地端電腦或裝置計算語
音特徵參數，並將這些特徵資料透過網路傳送到雲端伺服器，藉由雲端伺服器的服務進
行情緒分析並將結果回傳給用戶端，這樣可以提供給更多的 VoIP 客服系統使用，同時
也可繼續發展應用到其他智慧型裝置上使用。 
文獻探討 
近年在語音情緒辨識方面相關研究的論文相當多，以下簡單討論一些較新或是與我們研
究方向較為接近的論文： 
 Wu and Liang, 2011 [1]: 作者提出一種多重分類器去辨識語音情緒，針對聲學韻律
訊息以及語意標記以多重分類器進行四種情緒(中性、高興、生氣及憂傷)的情緒辨識。
所用的聲學韻律包含頻譜、共振峰、音高等特徵，GMM、SVM 即 MLP 分類器被用來
做聲學韻律分類之分類器，至於語意標記則使用既存的 HowNet 知識庫，透過加權方式
融合聲學韻律的辨識結果與語意標記的結果，辨識率超越單用語音訊號或是語意標記的
結果。 
 Luengo, Navas, and Hernaze, 2010 [2]: 作者試圖比較何種語音特徵參數比較適合用
來分辨語音情緒，他們針對韻律、頻譜及語音品質之參數進行分析，以多參數合併分析
方法計算分類準確率，實驗結果證實頻譜相關的參數可以提供較佳的分類鑑別度。這個
發現與我們團隊先前研究的結果吻合。 
 Busso, Lee ,and Narayanan, 2009 [3]: 作者用與基頻相關之特徵參數，包括 F0 的平均
值、標準差、最大值、最小值、中值、斜率等，利用 KULLBACK–LEIBLER 距離來
分辨負面情緒，辨識率為 77%，這篇論文與其他論文比較大的不同點是他用了五種不同
的語料庫，其中一個是中性語料庫，其他為情緒語料庫，其中之一為德文，其他三種為
英文，作者稱以基頻作為參數，可以不受語文的影響。 
 Pao, Yeh and Chen, 2008 [4]： 發展了一套中文語音連續情緒辨識的方法，並結合語
音的切割方式、語音訊號處理和訓練短句情緒語句的資料庫，使用與情緒在情緒空間分
佈的關係，以視覺化的方式回饋語句中各種情緒種類的感受相對強度。 
 Shafran and Mohri, 2005 [5]： 比較 3 種不同的分類器在語音情緒辨識率上的表現，
3 個分類器分別是 kernel-based 分類器（作者所提出的）、內插語言模型分類器及互訊息
（mutual information）分類器，實驗結果顯示作者所提出的分類器表現優於其他兩個分
目前是採用紙本方式讓標記者一邊聽所撥放的語音，一邊在紙本印出的語句上標記所感
受到的情緒類別，這樣的做法相當費力且耗費天然資源，本計畫中我們將嘗試利用由
Luis von Ahn 所提出的人腦運算(Human Computing)的方式來進行，藉由設計一個簡單的
介面，讓標記者透過網路來協助標記。 
 
 
 
 
 
 
 
 
 
 
 
 
圖一：語音情緒辨識系統架構圖 
 
中文語音情緒辨識系統架構如圖一所示，主要步驟說明如下： 
 語音信號的前處理：由於語音信號為時變性的信號，信號隨著時間快速變化，因此，
必須對語音做短時距分析(short-term analysis)，也就是將語音切成短音框(frame)來處理，
每個音框取 256 樣本（約 20~30 ms）。語音訊號切割成音框後會乘上一個漢明視窗
(Hamming window)函數，以消彌在做後續的 FFT 處理因為音框兩端不連續而造成之高
頻雜訊。 
 語音特徵參數抽取：這是語音情緒辨識最主要的步驟。語音特徵參數抽取的目的是
將語音波形轉換成某種形式的參數表示法，這些求得的參數需具有相當程度的代表性及
鑑別性，才可以在之後的分類階段讓分類器做出正確的判斷。由於客服系統的語音情緒
辨識需要即時完成，加上未來要擴充到多線處理，辨識引擎的效能將極為重要，為加速
語音情緒辨識處理，經由過去研究發現，選用 20 個 MFCC (Mel-Frequency Cepstral 
Coefficients)係數做為特徵參數可以達到可接受之準確度及合理之運算速度的要求。 
 語音情緒分類：將測試語音所抽取出來的特徵樣本與訓練階段已求得的參考樣本做
比對的工作，我們採用距離量測分類法，從各個參考樣本中之最小距離決定分類的類別。
常用在語音處理的分類法有 Hidden Markov Model (HMM)、Neural Network (NN)、Linear 
Discriminant Analysis (LDA)、Support Vector Machine (SVM)、k-Nearest Neighbor (kNN)
等，其中 kNN 為距離向量，有計算複雜度低的優勢，在需要即時音訊處理時，可以比
較有優勢。 
到目前為止的研究，在非特定本文及非特定語者的中文連續語音情緒辨識中，整體
的辨識率約可達到 76%。我們將持續改善目前所建構的連續中文語音之情緒辨識系統。
綜合過去對於語音情緒辨識參數有效性之研究，我們選擇了梅爾頻率倒頻譜係數 
(MFCC) 和線性預測倒頻譜係數 (LPCC) 做為實驗時的特徵參數值。情緒資料庫包含了
六種基本情緒，分別是生氣(Anger)、快樂(Happiness)、悲傷(Sadness)、厭煩(Boredom)、
害怕(Fear)和一般(Neutral)，以及焦慮(Anxiety)、失望(Despair)、厭惡(Disgust)、感激
(Gratefulness)、愛(Love)、稱讚(Praise)、羞愧(Shame)及驚訝(Surprise)等八種複合情緒。
另外，對於所錄製超過 40 分鐘之對話式連續語音長語句內容，我們將利用整句切割技
術來對連續語音進行切割，並結合狀態機模型來進行情緒判定，避免因一小段辨識錯誤
而發出錯誤警訊。分類器則是使用權重式離散型最近鄰居演算法(WD-KNN)，並以 SVM
Preprocessing 
Feature 
Extraction Classification 
 Feature 
Database 
Classification 
Result 
Input signal 
x(n) 
必須使用分散式處理來進行，因此在語音辨識伺服器需要以分散式架構來從新規劃，我
們的做法是將封包擷取模組獨立出來，進行 Session 管理及語音封包重組工作，並將重
組後的語音資料以每兩秒組成一個檔案的方式存入相對於該 Session 之目錄下。在語音
情緒辨識模組方面，由於計算複雜度高，需要用多台主機來處理，所需主機數量會依同
時通話線數而定。Session 控制並依各語音情緒辨識模組的系統負載分配工作，各辨識
模組主機將每兩秒為單位之情緒辨識結果將存入中央資料庫中，警示模組則由資料庫中
讀取目前辨識的結果，經由狀態機模型將分析結果以圖示方式呈現在管理人員的電腦
上。 
因應雲端運算風潮的來臨，我們也將探討把情緒辨識模組提升到雲端服務模式，也
就是將比較複雜的運算建置在雲端系統，如圖三所示。本地端只需將由語音中抽出的特
徵參數透過網路傳送到雲端系統，由雲端系統負責語音情緒辨識的工作，並將結果回傳
到本地端的資料庫中，再由本地端之警示系統進行後續處理，這種架構的好處是我們可
以運用更複雜且準確的情緒辨識演算法，同時也可以擴充這個辨識模組到客服系統以外
的各種可能之應用。 
 
圖三：雲端語音情緒辨識系統架構圖 
結果與討論 
 本計畫持續對中文連續語音情緒辨識進行深入研究，尤其在辨識情緒最有效的特徵
參數方面，進行了深入的研究，同時也把我們的語音情緒辨識系統應用到實驗室架設
的 VoIP電話系統上。為了能獲得更多人來對我們所收錄的語料庫進行標記，我們將標
記的語句數量降低，避免標記者因為需要標記上萬句的語句而致隨便標記，藉著網路
社群的協助，持續進行情緒與料庫人工標記的工作。在 VoIP客服系統的建置方面，我
們利用 Asterisk IP-PBX 系統建置了一個 VoIP 的環境，並利用 IP Phone 進行實機驗
證，並利用多部雲端虛擬主機，建構負載平衡伺服器系統，將需要大量運算的情緒辨
識運算分散到多台伺服器，如此來實現多線通話即時監控並於負面情緒出現時發出警
示，以利管理人員能隨時掌握第一線客服人員的情緒狀態，並於必要時介入監聽或取
而代之，以維護企業形象。 
 
 2 
實驗性的在線上教授程式設計，程式開發平台為 Ruby on Rail，選課人數相當多，但
退選人數約達九成，主要是程度不夠或是時間不允許修完，Patterson 教授提供了甚
多大量學生選修，如何與學生或是讓學生間互動的經驗，值得未來教學時參考。大
會在每天晚上皆安排了幾場 Tutorial，第一天晚上的一場 Tutorial 是由 Prof. George 
Markowsky 所講的 Hand-on Demonstrations of Some Basic Cybersecurity Tools，因為題
目與自己的教學課程有關，因此也從這個 Tutorial 中獲得不少教學工具的經驗。 
二、與會心得 
本次會議出席人數相當多，也新認識了不少同領域的學者，並聆聽了許多最新的研
究成果，對於未來的研究有不少的啟發作用。因為發表論文數很多，相對的會議場
次也相當多，因此場地分佈在旅館不同的區域，不同場地間換場需要花不少時間，
場地的安排似乎不甚理想，在未發表論文的時間，也去聽了幾場與個人研究相關的
Session，各 Session的參與度看起來還不錯。 
三、考察參觀活動(無是項活動者略) 
四、建議 
國內也有不少相當具規模的會議場所，應該可以爭取辦理國際型的會議，觀察此次
主辦單位的運作，似乎有很多值得學習的地方，未來應該多多鼓勵學者加入各類國
際會議的籌辦單位，累積經驗及人脈，然後再進一步爭取主辦權。 
五、攜回資料名稱及內容 
本次共攜回論文集兩冊及光碟片一片，論文集只包含 ICAI之論文，光碟片則含有所
有子 Conference的論文。 
六、其他 
 
 
 
 
2 
‐ Presentation Formats / Accepted Paper Categories 
‐ Typing Instructions 
‐ Deadline (registration and camera‐ready papers due: May 16, 2012); 
    We strongly encourage authors to register as soon as possible. 
‐ Conference Registration 
‐ Hotel Reservation 
‐ Conference Program/Schedule 
Please visit: 
http://www.worldacademyofscience.org/worldcomp12/ws/authors 
 
For Submission (uploading) of Final Camera‐Ready Papers for Publication in the Conference 
Proceedings/book, please visit: 
http://www.ucmss.com/cr/main/papersNew/worldcomp12_first_html 
 
To Request Invitation Letters for US Visa Purposes, please visit: 
http://www.ucmss.com/cr/main/invitations/worldcomp_index_html 
 
General information can be found at: 
http://www.world‐academy‐of‐science.org/ 
 
Congratulations, and thank you for your contribution to the Conference. 
We look forward to seeing you at the conference in July. 
 
Kind regards, 
Steering Committee, ICAI 2012 
ps. 
‐> ICAI'12 paper acceptance rate has been 26% (as of April 25, 2012);   
‐> this does 
      not yet reflect the data for all individual sessions (ie, the acceptance rate 
      is likely to change). 
‐> See the appended text for useful information about the conference. 
‐> If at least one author of an accepted paper registers for the   
‐> conference 
      but for some reason he/she is unable to attend the conference, then 
      his/her paper will be published and arrangements would be made to ship 
      the proceedings to the registered author after the conference. However, 
      the registered author MUST inform us by no later than June 1 that he/she 
      will not be able to attend. 
‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐cut here‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐ 
 
PUBLICATION: 
 
Proceedings of ICAI will be published in printed (ISBN) conference books/proceedings and will also be 
made available online. The proceedings will be indexed in science citation databases that track citation 
frequency/data for each paper; such databases include: Inspec / IET / The Institute for Engineering and 
Technology; The French National Center for Scientific Research, CNRS, INIST databases, PASCAL (covers the 
core scientific literature in Science; about 90% of inclusions are journals; only about 9% are proceedings; 
this congress tracks are selected to be among the 9% ‐ accessable from INIST, Datastar, Dialog, EBSCO, OVID
Questel.Orbit, Qwam, and STN International); and others. Though, there is no guarantee that the 
proceedings will also be included in EI Compendex/Elsevier indexings, in the past various tracks of ICAI 
4116 to CDES papers; 65 to CGVR papers; 55 to CSC papers; 271 to DMIN papers; 69 to EEE papers; 1,286 to 
ERSA papers; 504 to ESA papers; 
176 to FCS papers; 9,408 to FECS papers; 157 to GCA papers; 75 to GEM papers; 2,172 to ICAI papers; 1,529 
to ICOMP papers; 1,247 to ICWN papers; 
571 to IKE papers; 255 to IPCV papers; 137 to MSV papers; 5,239 to PDPTA papers; 496 to SAM papers; 
1,390 to SERP papers; and 177 to SWWS papers. 
In total, over 25,600 citations have been made (so far) to papers published in the proceedings of this 
federated joint conferences. Refer to the URLs below to see the actual citation numbers (each is a link to a 
live search and so it may take a few seconds for the data to pull up): 
 
http://academic.research.microsoft.com/Conference/1816/biocomp‐bioinformatics‐&‐computational‐biol
ogy 
http://academic.research.microsoft.com/Conference/1845/cdes‐international‐conference‐on‐computer‐de
sign/ 
http://academic.research.microsoft.com/Conference/1852/cgvr‐international‐conference‐on‐computer‐gr
aphics‐and‐virtual‐reality/ 
http://academic.research.microsoft.com/Conference/1887/csc‐international‐conference‐on‐scientific‐com
puting/ 
http://academic.research.microsoft.com/Conference/1920/dmin‐int‐conf‐on‐data‐mining/ 
http://academic.research.microsoft.com/Conference/1891/csreaeee‐international‐conference‐on‐e‐busine
ss‐enterprise‐information‐systems‐e‐government/ 
http://academic.research.microsoft.com/Conference/679/ersa‐engineering‐of‐reconfigurable‐systems‐and
‐algorithms/ 
http://academic.research.microsoft.com/Conference/681/esa‐embedded‐systems‐and‐applications/ 
http://academic.research.microsoft.com/Conference/1981/fcs‐international‐conference‐on‐foundations‐of
‐computer‐science/ 
http://academic.research.microsoft.com/Conference/1983/fecs‐conference‐on‐frontiers‐in‐education/ 
http://academic.research.microsoft.com/Conference/1998/gca‐international‐conference‐on‐grid‐computin
g‐and‐applications/ 
http://academic.research.microsoft.com/Conference/2467/gem‐international‐conference‐on‐genetic‐and‐
evolutionary‐methods/ 
http://academic.research.microsoft.com/Conference/7/ic‐ai‐international‐conference‐on‐artificial‐intellige
nce/ 
http://academic.research.microsoft.com/Conference/5/icomp‐international‐conference‐on‐internet‐comp
uting/ 
http://academic.research.microsoft.com/Conference/48/icwn‐international‐conference‐on‐wireless‐netwo
rks/ 
http://academic.research.microsoft.com/Conference/65/ike‐information‐and‐knowledge‐engineering/ 
http://academic.research.microsoft.com/Conference/2114/ipcv‐international‐conference‐on‐image‐proces
sing‐computer‐vision‐and‐pattern‐recognition/ 
http://academic.research.microsoft.com/Conference/2209/msv‐int‐conf‐on‐modeling‐simulation‐&‐visuali
zation‐methods/ 
http://academic.research.microsoft.com/Conference/214/pdpta‐parallel‐and‐distributed‐processing‐techni
ques‐and‐applications/ 
http://academic.research.microsoft.com/Conference/332/sam‐security‐and‐management/ 
http://academic.research.microsoft.com/Conference/361/serp‐software‐engineering‐research‐and‐practic
e/ 
http://academic.research.microsoft.com/Conference/2308/swws‐conference‐on‐semantic‐web‐and‐web‐s
ervices/ 
 
 
Integration of Negative Emotion Detection into a  
VoIP Call Center System 
 
Tsang-Long Pao, Chia-Feng Chang, and Ren-Chi Tsao 
Department of Computer Science and Engineering 
Tatung University, Taipei, Taiwan 
 
Abstract - The speech signal itself contains not only the 
semantics of the spoken words but also the emotion state of 
the speaker. By analyzing the voice signal to recognize the 
emotion hidden in the speech signal, it is possible to identify 
the emotion state of the speaker. With the integration of a 
speech emotion recognition system into a VoIP call center 
system, we can continuously monitor the emotion state of 
the service representatives and the customers. In this paper, 
we proposed a framework that integrates the speech 
emotion recognition system into a VoIP call center system 
Using this setup, we can detect in real time the speech 
emotion from the conversation between service 
representatives and customers. It can display the emotion 
states of the conversation in a monitoring console and, in 
the event of a negative emotion being detected, issue alert 
signal to the service manager who can then promptly react 
to the situation. 
Keywords: Speech Emotion Recognition, Call Center, 
Negative Emotion Detection, WD-KNN Classifier 
 
1 Introduction 
 The voice signal in the conversation represents the 
semantics of the spoken words and also the emotion state of 
the speaker. If a dispute happened between the service 
representative and the customer, there is no way to notify 
the service manager to take action immediately in current 
call center system. Since the customer service is playing an 
important role for an enterprise, the customer satisfaction is 
very important. So, the management of customer service 
department to improve the customer satisfaction is an 
important issue for the enterprise. 
 Traditional customer service lacks of the ability in 
issuing alerts for conversation with negative emotion in real-
time. For example, when the customer disagrees with the 
service representative, a dispute may arise. The traditional 
call center handles a large amount of calls every day and 
will usually make recording of all the calls for later analysis 
to see whether there is any improper conversation or not. 
However, these setups cannot handle the dispute situation in 
a timely manner. 
 A considerable number of studies have been made on 
speech emotion recognition over the past decades [1-16]. By 
integrating the speech emotion recognition system into the 
VoIP call center system, we can continuously monitor the 
emotional state of the service representatives and the 
customers. In this paper, we propose the mechanism to 
integrate the negative emotion detection engine into the 
VoIP call center system. A parallel processing architecture 
is implemented to meet the performance requirements for 
the system. We also record the emotion states for all of the 
calls into the database. Alerts will be issued to the service 
manager whenever a negative emotion such as anger is 
being detected. The service manager has a chance to 
intervene into the two quarreling parties to pacify the 
customer and resolve the problem immediately. With this 
mechanism, it can enhance the level of customer satisfaction. 
 The organization of this paper is as follows. In Section 
2, the background and the related researches of speech 
emotion recognition and voice over Internet Protocol (VoIP) 
are reviewed. In Section 3, the system architecture of a 
multi-line negative emotion detection in VoIP Call Center is 
described. In Section 4, the experimental setup is presented 
and the results are discussed. Conclusions are presented in 
Section 5.  
2 Backgrounds 
2.1 Speech Emotion Recognition 
 In the past, quite a lot of researchers studied the human 
emotion and try to define what an emotion is. But it is hard 
to define emotional category because of there is no a single 
universally agreed definition. The emotion category defined 
by Ortony and Turner is a commonly accepted definition 
[10]. In recent years, the study of psychological tends to 
divide emotion category into basic emotion and complex 
emotion. The complex emotion is a derived version from the 
basic emotion in their definition. 
 In addition to the semantics of spoken word, the speech 
signal also carries information of the emotion state of the 
speaker. That is, inside the speech signal, there are features 
that are related to the emotion state at the time of making 
that speech. By analyzing these features, it is possible to 
classify the emotion categories with a suitable classifier. 
selected features were extracted and sent to the classifier to 
determine the most probable emotion of that segment. 
 
 
 
 
 
 
 
 
Figure 1. Block diagram of emotion recognition system 
2.2 VoIP Telephony and Packet Capture 
Techniques 
 VoIP or known as IP phone is a technology that uses 
internet to accomplish telephone communication. The IP 
phone was used internally in the enterprise in the past. 
However, owning to the rapid grown of internet, the IP 
phone is now widely adopted and is gradually replacing the 
traditional telephone communication system. Currently, the 
commonly used VoIP communication protocol is the 
Session Initiation Protocol (SIP). The SIP is a protocol 
developed by IETF MMUSIC which is used in the 
establishment, modification and termination of an 
interactive call session. Possible applications include voice 
and video communication, instant messaging, online game, 
virtual reality and other multimedia applications. The 
purposes of SIP are to define the format and the control of 
the packets transmitted over the internet. 
 The SIP is a point to point protocol. Using a distributed 
architecture, the SIP transmits the text-based information 
and names the address by URL. The SIP use similar syntax 
as some protocols used in the internet such as HTTP and 
SMTP which consist of headers and message body. 
 There are two types of SIP operations for the client 
connection, one is to connect the two clients in a point to 
point manner, and the other is to connect the party though a 
proxy server. In this research, we adopt proxy server 
configuration to simplify the packet capture process. The IP 
PBX (IP Private Branch exchange) works as the proxy 
server and plays the role of packet relaying. We capture the 
packets in and out from the IP PBX by port mirroring from 
the switch the server connected to. 
 The packet capture tool used in this study is WinPcap. 
WinPcap is a tool for link-layer network access in Windows 
environments. It includes kernel-level packet filter, a 
dynamic link library (packet.dll) and a high-level and 
system-independent library. It can be used in win32 
platform to capture network packets. WinPcap consists of a 
driver that facilitates the operating system to access the low-
level networks. WinPcap also provides a library that can be 
used easily to access the low-level network layers by the 
application programs. In addition, the kernel of WinPcap 
also provides packet filtering. Through the filter setting, the 
driver will directly discard unwanted packet in driver layer. 
The performance of the packet filter is good. Hence it is 
now widely used in a lot of software such as WireShark. 
3 System Architecture 
 In this paper, we proposed a framework that integrates 
the speech emotion recognition system into a VoIP call 
center system. The components of the system are shown in 
Figure 2. The customer and the service representative 
communicate with each other through the VoIP phones. We 
defined session as the conversation between the pair of the 
customer and the service representative. We use a layer 2 
switch to mirror packets going in and out from the IP PBX 
into our packet capture agent. Then, we identify the session 
and assign the session with a Session ID (SID) if it is new. 
After the session is established, we extract the voice signal 
from the RTP packets captured and regroup it into proper 
segments. The speech emotion recognition system will be 
activated to classify the emotion of each segment. Finally, 
the recognition result will be stored into a database. The 
system will issue an alert whenever a negative emotion, 
mainly anger, is detected. 
 
Figure 2. Components of the proposed system 
 The operation steps of system are shown in Figure 3. 
First, the system will capture the packets and filter out the 
SIP and RTP packets. The Speech Emotion Recognition 
System (SERS) will assign a Session ID (SID) according to 
the phone number of the service representative and 
Emotional 
Speech 
Evaluation 
Result 
Feature 
Database 
Preprocessing 
Feature 
Extraction 
Classification 
  
 
 
 
 
Figure 4. Framework of the proposed system 
 The Asterisk IP PBX server is installed in a Linux 
platform. In this research, we assign the SIP ID 1xx as the 
phone number for customers, SIP ID 4xx for the service 
manager, and SIP ID 6xx for the service representatives. 
The SIP phone is configured in the proxy mode, such that 
all the SIP and RTP related communication packets between 
the customer and the service representative will pass 
through the IP PBX. With this configuration, we can capture 
all the SIP and RTP packets by mirroring the traffic going 
into the server from the Layer 2 switch where the server is 
attached. 
 The session information is stored in a table structure. 
When a new SIP or RTP packet is received, the packet 
capture module will capture the packet, analyze the content 
of header and compare it with the contents in the session 
table. If it is a new session, the system will create a new 
session identifier and assign a new session ID to it; 
otherwise the existing session ID will be retrieved. 
4.2 Emotion Recognition 
 We capture the voice signal from the conversation 
between the customer and service representative. The voice 
we captured will be regrouped into sound file in WAV 
format with 1 second in duration each. We will send the 
segmented voice file into the speech recognition engine, and 
the engine will output the recognition result. In order to 
increase the performance of the emotion recognition engine, 
we use a parallel architecture to build our recognition 
system. We setup several machines and install the 
recognition engine written in MATLAB into each machine. 
By using this structure, the bottleneck problem of the speech 
emotion recognition system can be avoided. The output 
from each recognition engine is stored into a database. The 
results are analyzed by using the score board algorithm 
stated above to determine whether the anger emotion exists 
in the conversation or not.  
4.3 Testing Samples 
 A six scripts corpus was recorded by inviting 
volunteers to speak out the scripts with emotion as they like. 
Each corpus is tagged subjectively by human judge. Only 
the results with more than 80% agreement in this process are 
kept. The emotion of human seems to change gradually. So 
we use the score board concept to determine whether or not 
an anger emotion was occurred in the conversation between 
the customer and the service representative.  
 In our environment, we set the score to 0 at the 
beginning. If a negative emotion is being detected, the 
system will add 4 points to the corresponding session, or 
otherwise the system will subtract 1 point from that session 
if the score is positive. In order to reduce the false negative 
emotion recognition rate, a threshold should be set. We test 
several threshold values, including 4, 8, 12, and 16, to check 
the recognition accuracy. We compare the emotion 
recognition results with human judge for the chosen 
threshold. The result of the negative emotion detection is 
listed in Table 1. 
Table 1. Number of detected negative emotion with 
different threshold T. 
 
Human 
Judge T=4 T=8 T=12 T=16 T=20 
Script1 2 59 36 18 1 0 
Script2 0 2 0 0 0 0 
Script3 0 48 33 16 5 0 
Script4 1 21 1 1 1 0 
Script5 7 97 85 61 29 10 
Script6 10 54 39 23 12 7 
 
 In Table 1, the negative emotion detection results are 
close to the human judge when the threshold is 16 except 
the script 3 and 5. For script 3, the content of the script 
consist of a lot of happy sentence. It is hard to differentiate 
between happiness and anger in a speech emotion 
recognition system since they are both in the activation 
category. For script 5, the content of the script consists of 
continuous negative emotion. Due to the score board 
architecture, it is hard to count the dispute correctly. The 
advantage of the score board framework is that it can reduce 
the judgment error. But it cannot count the dispute exactly. 
For the negative emotion detection as mentioned above, the 
most important thing is to issue an alert when the dispute 
happened. The frequency of the dispute is not an important 
issue in this research. 
5 Conclusions 
 The call center plays an important role for an enterprise. 
It is obviously that collects the opinion from the customer 
Packet capture 
& Session 
identification 
Emotion 
recognition 
engine 1 
Emotion 
recognition 
engine 2 
Emotion 
recognition 
engine N 
File system Database 
Negative 
emotion 
detection 
Alert 
. . . 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/07/26
國科會補助計畫
計畫名稱: 多線VoIP電話客服系統情緒異常警示機制及發展成雲端服務之可行性分析研究
計畫主持人: 包蒼龍
計畫編號: 100-2221-E-036-043- 學門領域: 自然語言處理與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
