in the data makes the data pattern more visible. Near 
sets is introduced by J. Peter in 2007, it is a 
recent generalization of rough set theory. It leads 
to descriptions of objects that are minimally near 
each other and it has been shown that can be used in 
a perception-based approach to discover 
correspondences between images. In this project, we 
will propose a Rough Margin SVR network and a robust 
learning algorithm based on near set those are 
combined to discover the problems of function 
approximation with outlier. 
英文關鍵詞： rough set, near set, Support vector regression, 
outlier, robust learning 
 
 2
information table can be reduced by RST 
approach, is utilized as a preprocessor to improve 
business failure prediction capability by SVM. 
Asharaf [22] incorporates rough set theoretic 
flavour in support vector clustering paradigm to 
achieve soft clustering. The user-defined 
parameters allows to control the extent of softness 
by controlling the width of boundary region and 
number of outliers. Chen [25] proposed fuzzy 
transitive kernels based fuzzy rough sets, and 
then reformulated hard margin support vector 
machines into fuzzy rough set based SVMs 
(FRSVMs) with new constraints in which the 
memberships are taken into account. It have 
certain capacity to deal with noise. In [26], a 
called rough support vector regression (RSVR), is 
proposed to improve the modeling of rough 
patterns. rough insensitive loss function by 
applying separate SVR for patterns of lower and 
upper values. The main advantage of rough-SVR 
was that it almost never over-predicted the upper 
values. Based on v-SVM(SVR)[29], Zhang [27] 
gave a rough margin based support vector 
machine for classification to escape from the 
overfitting problem due to outliers. In [28] focus 
on the rough margin based v-SVR which 
constructs the rough margin with double 
tolerance bands. 
A recent generalization of rough set theory has 
led to the introduction of near sets [30-32,38]. In 
a near set approach to object classification, an 
object description is modeled as a vector function 
that represent object features [31]. Near set theory 
begins with the selection of probe functions that 
provide a basis for describing and discerning 
affinities between objects in distinct perceptual 
granules [32]. Recently, it has been shown that 
near sets have proved to be quite useful in solving 
a number of problems such as image 
segmentation quality [33], image correspondence 
[34], image retrieval [35], medical image analysis 
[36], face recognition [37], measurement of 
image similarity [35,36] and adaptive learning 
[39,40]. 
In this project, we present an application of 
rough sets and near sets in solving the function 
approximation with outlier problems. It separates 
as two topics: (1). It introduces rough concept in 
Support Vector Regression to reject the outlier for 
function approximation and terms as Rough 
Margin Support Vector Regression (RMSVR) 
Algorithm. (2). A robust learning algorithm is 
proposed, the near set approach is provided for 
back propagation fashion. 
2. Related Technologies 
2.1 Support Vector Regression  
  A function approximation problem can be 
formulated as to obtain a function from a set of 
observations  1 1 2 2( , ), ( , ),..., ( , )  N Nx y x y x y  with 
 mix R  and iy R , where N is the number of 
training data, ix
 is the i-th input vector, and iy is 
the desired output for the input ix
  . Based on the 
SVM theory, SVR is to approximate the given 
observations in an m-dimensional space by a 
linear function in another feature space F. The 
function in SVR is of the form 
( , ) , ( )          f x w w x b      (1) 
where ,   is  an inner product defined on 
F, ( )  is a nonlinear mapping function from mR  
to F, w F  is a weight vector to be identified in 
the function, and b is a threshold. Usually, the 
considered cost function is  
2[ ] [ ]   SV empR f R f C w        (2) 
 4
(3) If kx  is not a part of any lower 
approximation, then it belongs to two or 
more upper approximations. 
Near sets and rough sets are very much like 
two sides of the same coin. From a rough set 
point-of-view, the focus is on the approximation 
of sets with nonempty boundaries. By contrast, in 
a near set approach, the focus is on the discovery 
of sets having matching descriptions that does not 
require a consideration of approximation 
boundaries [36]. Near sets are considered a 
generalization of rough sets, since it has been 
shown that every rough set is a near set but not 
every near set is a rough set. Near sets grew out 
of the idea that two or more rough sets can share 
objects with matching descriptions if they both 
contain objects belonging to the same class from 
the partition. When this occurs, the sets are 
considered near each other with respect to the 
classes contained in the partition. There are some   
basic definitions are described as follows. 
Definition 1: Probe Function 
 A probe function is a real-valued function, 
:  O  representing a feature of an object. 
Where O  is the set of perceptual objects and   
is the set of real numbers. The set of all probe 
functions is denoted by  , and a set of specific 
probe functions for a given application is denoted 
by  B . 
Definition 2: Perceptual System 
A perceptual system ,O consists of a 
non-empty set O  together with a set   of 
real-valued functions. 
Definition 3: Object Description 
Consider a perceptual system ,O . The 
description of an object , ix O  is given 
by the vector  
 1 2( ) ( ), ( ), , ( )     lx x x x       (7) 
The intuition underlying a description ( )i x  is a 
recording of measurements from sensors, where 
each sensor is modeled by a function. 
Definition 4: Indiscernibility Relation 
Let ,O  be a perceptual system. For 
every  B the indiscernibility relation ~B is 
defined as follows: 
   2~ ( , ) : ( ) ( ) 0     B B Bx y O O x y  (8) 
where 
2
 represents the L2 norm. 
Definition 5: Tolerance Relation 
Let ,O  be a perceptual system and let 
  . For every  B the tolerance relation  
B is defined as follows: 
  2( , ) : ( ) ( )       B B Bx y O O x y  (9) 
where   is the tolerance. 
The basic idea in the near set approach to 
object recognition is to compare object 
descriptions. Sets of objects X , X’ are considered 
near each other if the sets contain objects with at 
least partial matching descriptions. 
Definition 6: Near Sets  
Let , ' X X O , ,  B . Set X is near X’  
if and only if there exists , ' ' x X x X , i B  
such that { }~ 'ix x . 
A perceptual granule is a set of perceptual 
objects originating from observations of the 
objects in the physical world. Near set theory 
begins with the selection of probe functions that 
provide a basis for describing and discerning 
affinities between objects in distinct perceptual 
granules [32]. A probe function is a real-valued 
 6
is employed and   can be updated as 
( 1) ( ) ( )       t t t          (16) 
with 
( )( )  
  
 NRBPE tt          (17) 
where   is the learning constant. 1[ ,..., ]  

l  
is the weight vector of the network. 
3.2 Rough Margin Support Vector Regression 
Algorithm (RMSVR) [43]  
A novel approach, termed as Rough Margin 
Support Vector Regression (RMSVR) Algorithm, 
is proposed. Support vector regression (SVR) [4] 
which is constructed by minimizing Vapnik’s 
 -insensitive loss function residuals between the 
outputs of SVR and the target values and the 
model complexity. Instead of selecting an 
appropriate  , Schölkopf et al [29] proposed a 
v-support vector regression (v-SVR), which 
introduces a new parameter v which can control 
the number of support vectors and training errors 
without defining   a prior. They proved that v is 
an upper bound on the fraction of margin errors 
and lower bound of the fraction of support 
vectors.  
In [28] focus on the rough margin based 
v-SVR, which constructs the rough margin with 
double   instead of single   for v-SVR such 
as Fig.1. Following the rough theory, l  and u  is 
the lower margins and the upper margins, 
respectively. The regions between those two 
margins are equivalent to the boundary regions; 
the region inside the lower margins is equivalent 
to the positive region; and the regions outside the 
upper margins are equivalent to the negative 
regions. When the datum lies in the positive 
region, it is regarded as non-support vector. The 
datum lying in the boundary and negative regions 
are all support vectors. The farther the datum 
deviates from the target function, the more 
important it is, so it emphasizes the residuals 
introduced by the data lying in the negative 
regions. Although the experimental results on 
benchmark datasets have been conformed the 
effectiveness of this idea, but if the outliers exist 
that will result in overfitting phenomena. In our 
approach, the datum lying in the boundary 
regions are considered as support vectors, i.e. l  
and u  is the lower bound and upper bound of the 
fraction of support vectors, respectively. The 
datum lying in negative regions are viewed as 
outliers. Similar to the classical v-SVR and rough 
v-SVR, the primal problem can be described as: 
     * * 2 * *, , , , , , 1 1
1 1min   
2     
      
 
         l u i i i i
nl l
b
l u i i i i
i i
C v
l lw
w (18) 
subject to  
  
   * *
*
*
,
,
0 ,  0 ,
0,   0,   0,   0,
   
   
     
   
     
     
     
   
i i l i i
i i l i i
i u l i u l
i i l u
b y
y b
w x
w x
 
The Lagrange function is defined as 
     
   
   
   
2 * *
1 1
1
* * *
1
* *
1 1
* * * *
1 1 1 1
1 1
2
      
    
    
       
         
 


 
   
         
      
      
     
     
 


 
   
nl l
b
l u i i i i
i i
l
i l i i i i
i
l
i l i i i i
i
l l
i u l i i u l i
i i
l l l l
i i i i i i i i l l u u
i i i i
L C v
l l
y b
y b
w
w x
w x
(19) 
where
* * * *0, 0, 0, 0, 0, 0, 0, 0, 0, 0                  i i i i i i i i l u
are Lagrangian multipliers. According to KKT 
conditions and then the dual problem can be 
written as   
 8
probability 1   and  , respectively. In this 
example, the training data are corrupted by a 
gross error model with 0.05  , 
~ (0,0.05)G N and ~ (0,1)H N . The training 
data are shown as Fig.2. The proposed NRBP is 
compared to the robust BP learning algorithm [10] 
and ARBP [41]. The length of each subset in 
training data is selected as 4 in this example. The 
approximated results of each approach are 
presented in Fig. 3. It can show that the NRBP 
has the better simulated results. 
Example 2. A sinc function is considered and 
defined as 
sin( ) xy
x
, 10 10  x .     (24) 
80 input-output data are used. In this example, 
outliers are added artificially by moving some 
points away from designated locations. The 
parameters selected as 3, 0.5  C . Based on 
the above procedure, the approximated results of 
RMSVR are presented in Fig. 4 and RMSVR 
with robust learning(NRBP) are shown as Fig.2. 
It can be found that the proposed RMSVR can 
reduce the overfitting phenomena. 
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
x
y
 
 
Desired
Trauning Data
 
Fig.2  The training data 
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
x
y
 
 
Desired
NRBP
RBP
ARBP
 
Fig. 3. The approximated results with robust learning 
algorithms. 
-10 -5 0 5 10
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
y
sinc function
 
 
Desired
RMSVR
Training data
 
Fig.4  The approximated results  
-10 -5 0 5 10
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
sinc function
x
y
 
 
Desired
RMSVR
Training data
 
Fig.5 The approximated results with robust learning. 
 10
“Rough Fuzzy MLP: Modular evolution, rule 
generation and evaluation”, IEEE Trans. Knowl. 
And Data Eng., pp.142-157, 2003. 
[17]. Q. Shen, A. Chouchoulas, “A rough-fuzzy 
approach for generating classification rules”, 
Pattern Recognition, vol.35, pp.2425 – 2438, 
2002. 
[18]. X. Wang, Eric C.C. Tsang, S. Zhao, D. Chen 
and Daniel S. Yeung, “Learning fuzzy rules 
from fuzzy samples based on rough set 
technique”, Information Sciences , vol.177, 
pp.4493–4514, 2007. 
[19]. P. Lingras and C. West, “Interval set clustering 
of web users with rough K-means,” J. Intell. Inf. 
Syst., vol. 23, no. 1, pp. 5–16, 2004. 
[20]. Sushmita Mitra, Haider Banka, and Witold 
Pedrycz, “Rough–Fuzzy Collaborative 
Clustering”, IEEE Trans. Syst., Man, 
Cybern-Part B, vol. 36, no. 4, pp.795-805, 
2006. 
[21]. Pradipta Maji, Sankar K. Pal, “Rough Set 
Based Generalized Fuzzy C-Means Algorithm 
and Quantitative Indices”, IEEE Trans. Syst., 
Man, Cybern-Part B, vol. 37, no. 6, 
pp.1529-1540, 2007. 
[22]. S. Asharaf, S.K. Shevade,M. and Narasimha 
Murty, “Rough support vector clustering”, 
Pattern Recognition, vol.38 , pp.1779 – 1783, 
2005. 
[23]. P. Lingras, C.J. Butz, “Rough set based 1-v-1 
and 1-v-r approaches to supportvector machine 
multi-classification,” Information Sciences, 
vol.177, pp. 3782–3798, 2007.  
[24]. C.C. Yeh , D.J. Chi and M.F. Hsu, “A hybrid 
approach of DEA, rough set and support vector 
machines for business failure prediction,” 
Expert Systems with Applications, vol. 37, 
pp.1535–1541, 2010. 
[25]. D. Chen, Q. He and X. Wang, “FRSVMs: 
Fuzzy rough set based support vector 
machines,” Fuzzy Sets and Systems , vol.161, 
pp.596–607, 2010. 
[26]. P. Lingras, C.J. Butz, “Rough support vector 
regression,” European Journal of Operational 
Research, 2009, article in press. 
[27]. Zhang, J., & Wang, Y., “A rough margin based 
support vector machine,” Information Sciences, 
vol.178, pp.2204–2214, 2008. 
[28]. Y. Zhao and J. Sun, “Rough v-support vector 
regression,” Expert Systems with Applications, 
vol.36, pp.9793–9798, 2009. 
[29]. Schölkopf, B., Smola, A., Williamson, R., and 
Bartlett, P., “New support vector algorithms,” 
Neural Computation, vol.12, pp.1207–1245, 
2000. 
[30]. J. Peters, “Near sets. Special theory about 
nearness of objects,” Fundam. Inform., vol. 75, 
no. 1–4, pp. 407–433, 2007. 
[31]. J. Peters, “Near sets. General theory about 
nearness of objects,” Appl. Math. Sci., vol. 1, 
no. 53, pp. 2609–2029, 2007. 
[32]. J. Peters, “Classification of perceptual objects 
by means of features,” Int. J. Info. Technol. 
Intell. Comput. Vol.3,no.2,pp.1-35, 2008. 
[33]. C. Henry, J. Peters, “Near set image 
segmentation quality index,” GEOBIA 2008 
Pixels, Objects, Intelligence., pp. 1–6, 2008. 
[34]. J. Peters, “Tolerance near sets and image 
correspondence,” Int. J. Bio-Inspired Comput. , 
vol.1,no.4, pp.239-245,2008. 
[35]. H. Fashandi, J. Peters, S. Ramanna, “l2 norm 
length-based image similarity measures: 
Concrescence of image feature histogram 
distances,” Proc. 11th IASTED Int. Conf. Signal 
and Image Processing, 2009. 
[36]. A. Hassanien, A. Abraham, J. Peters, G. 
Schaefer, C. Henry, “Rough sets and near sets 
in medical imaging: A review,” IEEE Trans. 
Info. Tech. Biomed.,vol.13,no.6, pp.955-968, 
2009. 
[37]. S. Gupta, K. Patnaik, “Enhancing performance 
of face recognition systems by using near set 
approach for selecting facial features,” J. Theor. 
Appl. Inform. Technol. Vol.4,no.5, 
pp.433-441,2008. 
[38]. J. Peters, & P. Wasilewski, “Foundations of 
near sets,” Information Sciences, vol.179, 
pp.3091–3109, 2009. 
[39]. J. Peter, S. Shahfar, S. Ramanna and T. Szturm, 
“Biologically-Inspired adaptive learning:a near 
set approach,” Frontiers in the Convergence of 
Bio. And Info. Tech. IEEE, pp.403-408,2007. 
[40]. D. Lockery and J. Peters, “Adaptive learning 
by target-tracking system,” Int. J. Intel. Compu. 
And Cyber., vol. no.1, pp.46-68, 2008. 
[41]. C.C. Chuang, S.F. Su, and C.C. Hsiao, “The 
annealing robust backpropagation (ARBP) 
learning algorithm,” IEEE Trans. Neural 
Networks, vol. 11, pp. 1067–1077, Sept. 2000. 
[42]. C. C. Hsiao , C.C. Chuang and J. T. Jeng, 
“Robust Back Propagation Learning Algorithm 
Based on Near Sets,” 2012 International 
Conference of System Science and 
Engineering(ICSSE2012), pp.19-23, 2012.  
[43]. C. C. Hsiao , S. F. Su and C.C. Chuang, “A 
Rough-based Robust Support Vector Regression 
Network for Function Approximation,” 2011 
IEEE International Conference on Fuzzy 
Systems, pp.2814-2818, 2011.  
要講述如何實現時間序列的粒化模型之合理的資訊粒化 (information granularity)原理，其
中建構資訊粒化可視為是一個最佳化問題。第三場由上海寶信軟體公司, Li-Quan Cong1
博士主講，講述題目為“The Green Manufacturing Techniques and Applications under the 
Consideration of Energy Supply Constriction”，主要講述製造執行系統（MES，Manufacturing 
Execution System）已經證實可以提高生產效率和改善產品品質，未來將進一步考慮在生
產調度之能源供給因素及低能源消耗。下午則同時舉行八場 Oral Sessions，本人被安排
於 Session I-R1: Applications of Intelligent Mechanisms，由台灣大學電機系連豊力教授主
持，共有 5 篇論文發表，本人報告之論文題目為“Robust Back Propagation Learning 
Algorithm Based on Near Set＂；此篇是國科會計畫 “相近集合與粗略集合在函數近似之
應用研究＂之研究成果，主要是講述在強健學習法則中引入相近集合，以改善離異點之
效應。 
第二天（7/1）：今天同時舉行十二場 Oral Sessions。本人選擇參加了Session II-R2: 
Pattern Recognition and Applications，由Yan Zhuang 教授主持，共有7篇論文發表，主要
講述包含了Iris資料、語音與3D視覺等之辨識演算法及其應用； 另外下午亦參加了Session 
III-R2: Intelligent Control and its Applications II，由Quanli Liu 教授主持，共有7篇論文發
表，主要講述包含了模糊理論、粒子群法在最佳化、預估、分類及控制上的應用。晚上
並參加晚宴，認識了一些國內、外之學者，增廣不少見聞。 
第三天（7/2）：今天是會議的最後一天，大會安排了 City tour。 
二、與會心得 
今 （ 2012 ） 年 第 三 屆 International Conference on System Science and Engineering 
(ICSSE2012)，於2012年6月30日至7月2日在大連理工大學舉行。根據大會統計，本年度投稿
論文共有220篇論文投稿，經過審查通過在會議中發表之論文共有120篇論文；論文全文收錄
於論文集光碟，並由IEEE發行，其亦收錄於EI索引。 
本次研討會的除了論文發表及海報展示外，大會在第一天全部安排3場的「大會專家論譠」
(Plenary Lecture) ，邀請來自不同國家之知名學者分別針對特定主題進行演講，發表其最新研
究成果，並總共有20場Oral Sessions，對本人的研究方向，有很大的啟示。 
(1). ACP架構特別是應用於複雜的工業生產系統和並行管理，人力資源管理在企業管理中
Robust Back Propagation Learning Algorithm Based 
on Near Sets
Chih-Ching Hsiao 
Department of Information Technology  
Kao Yuan University 
Kaohsiung City, Taiwan 
t20051@cc.kyu.edu.tw
Chen-Chia Chuang 
Department of Electrical Engineering, 
National Ilan University 
I-Lan County, Taiwan 
ccchuang@niu.edu.tw
Jin-Tsong Jeng 
Department of Computer Science & 
Information Engineering  
National Formosa University 
Yunlin County, Taiwan 
tsong@nfu.edu.tw
Abstract—The traditional robust learning algorithms are based 
on the estimated errors, which is not correct in the early stage of 
the training process. Therefore, the use of those approaches still 
cannot provide very decent learning performance in face of 
outliers unless a set of good initial weights is used. In this paper, a 
novel approach, termed as NRBP (Near set based Robust Back 
Propagation learning algorithm) is proposed. In this learning 
algorithm, the training (\estimated) data sets are separated into 
overlapping (or nonoverlapping) subsets of those data. It uses the 
set error measure instead of one-step error in robust back 
propagation based on near set. The set error measure is an 
estimated error measure between a subset of training data set 
and corresponding subset of estimated data set. Its benefit is it 
includes error messages and also reduces the outlier effect. 
Keywords- robust;  learning; Near set; outlier 
I. INTRODUCTION 
Multilayer feedforward neural networks are often used for 
modeling systems. In those approaches, the task is to obtain 
networks that can act as closely to the system to be modeled as 
possible. Since neural networks estimate functions without 
requiring mathematical description of how the outputs 
functionally depend on the inputs, they are often referred to as 
model-free estimators [1]. The basic modeling philosophy of 
model-free estimators is that they build systems from input–
output patterns directly, or in more abstract, they learn from 
examples without any knowledge of the model type. This kind 
of learning schemes used for neural networks can also be 
called data learning. Such learning schemes are to find 
functions that can match all training data as close as possible, 
no matter whether these data are trustable or not. In fact, usual 
three layer networks with sufficiently many neurons in the 
hidden layer have been proven to be capable of approximating 
any Borel measurable function in any accuracy [2]. Thus, they 
are also referred to as universal approximators [2]. However, 
if the training data are corrupted by noise, those data learning 
schemes may not always come up with acceptable 
performance. 
For any real-world applications, the obtained training 
data are always subject to noise or maybe outliers. The 
intuitive definition of an outlier [3] is “an observation which 
deviates so much from other observations as to arouse 
suspicions that it was generated by a different mechanism.”
Outliers may occur due to various reasons, such as erroneous 
measurements or noisy data from the tail of noise distribution 
functions. For the existence of noise in the training data, when 
it is small, noise may not always cause bad influence, 
especially small noise injected in the input variables [4]. But, 
when noise becomes large or outliers exist, the networks may 
try to fit those improper data and thus, the learned systems are 
said to be corrupted. In other words, if there exist outliers and 
the training processes last long enough, the obtained systems 
may have the phenomenon of overfitting [5]. A system that is 
not sufficiently complex may fail to capture fully the signal 
when a set of complicated training data set is used for training, 
leading to the phenomenon of underfitting. On the other hand, 
a system that is too complex may perfectly fit the given data 
including noise, not just the signal, leading to the phenomenon 
of overfitting [5]. In overfitting situations, the learning process 
may be able to achieve lower training error levels because the 
learned function has tried to fit all data as close as possible [6]. 
However, in the testing phase, another set of data, which is not 
used in any way in the training process, is used to evaluate 
validation of the learned system. Thus, the testing errors may 
significantly be increased because the noise in the training 
patterns is different from that in the testing patterns.  
The rough set theory proposed by Pawlak [7-8] is a 
mathematical theory dealing with uncertainty in data. Rough 
sets rely on the notion of lower and upper approximations of a 
set and are applied to rough fuzzy control[9-10], modeling, 
system identification[11-12] , discovery of discussion rules 
from experimental data and clustering[13-14]. A recent 
generalization of rough set theory has led to the introduction 
of near sets [15-17]. Near sets grew out of a generalization of 
the approach to the classification of objects proposed by 
Pawlak’[8] and Oráowska’s suggestion that approximation 
spaces are the formal counterpart of perception or observation 
[18]. Near set theory provides a formal basis for observation, 
comparison and classification of perceptual granules. In 
addition, near sets have been introduced to cater to the needs 
of science and engineering, where the focus is on the 
discovery of affinities between observed. A natural 
consequence of the near set approach is the realization that 
affinities between speciments are discovered by comparing the 
descriptions of pairs of objects in perceptual granules such as 
images. In other words, the focus in near sets is on discovering 
19
2012 International Conference on System Science and Engineering
June 30-July 2, 2012, Dalian, China
978-1-4673-0945-5/12/$31.00 ©2012 IEEE
B. Near sets 
Near sets and rough sets are very much like two sides of the 
same coin. From a rough set point-of-view, the focus is on 
the approximation of sets with nonempty boundaries. By 
contrast, in a near set approach, the focus is on the 
discovery of sets having matching descriptions that does 
not require a consideration of approximation boundaries 
[23]. Near sets are considered a generalization of rough 
sets, since it has been shown that every rough set is a near 
set but not every near set is a rough set. Near sets grew out 
of the idea that two or more rough sets can share objects 
with matching descriptions if they both contain objects 
belonging to the same class from the partition. When this 
occurs, the sets are considered near each other with respect 
to the classes contained in the partition. There are some 
basic definitions are described as follows [17]: 
Definition 1: Probe Function 
A probe function is a real-valued function, : Oϕ → \  , 
representing a feature of an object. 
Where O  is the set of perceptual objects and \  is the set of 
real numbers. The set of all probe functions is denoted by Ψ ,
and a set of specific probe functions for a given application is 
denoted by B ⊆ Ψ .
Definition 2: Perceptual System 
A perceptual system ,O Ψ consists of a non-empty set O
together with a set Ψ  of real-valued functions. 
Definition 3: Object Description 
 Consider a perceptual system ,O Ψ  . The description of 
an object , ix O φ∈ ∈ Ψ  is given by the vector  
( )1 2( ) ( ), ( ), , ( )lx x x xφ φ φΦ = "                 (5)
The intuition underlying a description ( )i xφ  is a recording of 
measurements from sensors, where each sensor is modeled by a 
function. 
Definition 4: Indiscernibility Relation 
Let ,O Ψ  be a perceptual system. For every B ⊆ Ψ the 
indiscernibility relation ~B is defined as follows: 
{ }2~ ( , ) : ( ) ( ) 0B B Bx y O O x yϕ ϕ= ∈ × − =          (6) 
where 2< represents the L2 norm. 
Definition 5: Tolerance Relation 
Let ,O Ψ  be a perceptual system and let ε ∈\ . For 
every B ⊆ Ψ the tolerance relation  B≅ is defined as follows: 
{ }2( , ) : ( ) ( )B B Bx y O O x yϕ ϕ ε≅ = ∈ × − ≤          (7) 
where ε  is the tolerance. 
The basic idea in the near set approach to object 
recognition is to compare object descriptions. Sets of objects 
X , X’ are considered near each other if the sets contain objects 
with at least partial matching descriptions. 
Definition 6: Near Sets 
Let , 'X X O⊆ , , B ⊆ Ψ . Set X is near X’  if and only if 
there exists , ' 'x X x X∈ ∈ , i Bφ ∈  such that { }~ 'ix xφ
A perceptual granule is a set of perceptual objects 
originating from observations of the objects in the physical 
world. Near set theory begins with the selection of probe 
functions that provide a basis for describing and discerning 
affinities between objects in distinct perceptual granules [19]. 
A probe function is a real-valued function representing a 
feature of physical objects such as images or behaviors of 
individual biological organisms or swarms of organisms or 
collections of artificial organisms such as robot societies 
[16,26]. 
III. NEAR-BASED ROBUST BACK PROPAGATION (NRBP)
LEARNING ALGORITHM
The cost function of a robust learning algorithm is defined 
as Equ.(4). Even though the previous selections of somehow 
are meaningful mechanisms, the problem is still there; that is, 
those approaches are all based on the estimated errors, which 
is not correct in the early stage of the training process. 
Therefore, the use of those approaches still cannot provide 
very decent learning performance in face of outliers unless a 
set of good initial weights is used. Thus, we replace the one-
step error ke  as a set error measure [ ]ke  that is an estimated 
error measure between a subset of training data set and 
corresponding subset of estimated data set. 
In [17,22,23] proposed a nearness measure (NM) to 
determine the degree of resemblance between two sets. In this 
paper, a 2L  norm-based nearness measure [22] is used. 
/ /
/ /
/ /
/
/ /
/
min ,
max ,
( , )
B B
T E
B
B B B B
T E
B
B
B B
D D
d D
D D
T E
d D
d d
d
d d
NM D D
d
≅ ≅
≅ ≅
≅ ≅
≅
∈
≅ ≅
≅
≅
∈
§ ·ª º ª º¨ ¸¬ ¼ ¬ ¼© ¹
§ ·ª º ª º¨ ¸¬ ¼ ¬ ¼© ¹
=
¦
¦
                  (8) 
where ,T ED D  is training subset and estimated subset, 
respectively. ( , )
B T ENM D D≅ is a Nearness Measure between 
two sets. T ED D D= ∪  and a tolerance relation between two 
sets, defined as 
{ }2, : ( ) ( )B I I E E I E Nd D d D d dϕ ϕ ε≅ = ∈ ∈ − ≤          (9) 
21
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
x
y
Desired
NRBP
RBP
ARBP
Fig.2 The approximated results with robust learning algorithms
REFERENCES
[1] B. Kosko, Neural Networks and Fuzzy Systems, A Dynamical Systems 
Approach to Machine Intelligence. Englewood Cliffs, NJ: Prentice-Hall, 
1992. 
[2]  K. Hornik, M. Stinchconbe, and H. White, “Multilayer feedforward 
network are universal approximators,” Neural Networks, vol. 2, pp.359–
366, 1989. 
[3]  D. M. Hawkins, Identification of Outliers. London, U.K.: Chapman 
&Hall, 1980. 
[4] L. Holmström and P. Koistinen, “Using additive noise in back-
propagation training,” IEEE Trans. Neural Networks, vol. 3, pp. 24–38, 
Jan. 1992.  
[5] M. Smith, Neural Networks for Statistical Modeling. New York: Van 
Nostrand Reinhold, 1993. 
[6] G.-Y. Chen, “On the study of the learning performance for neural 
networks and neural fuzzy networks,” M.S. Thesis, Dept. Elect. Eng., 
Nat. Taiwan Univ. Sci. Technol., Taipei, Taiwan, R.O.C., 1998. 
[7] Pawlak. Z, “Why rough set?,”  Proceedings of IEEE international 
conference on Fuzzy systems, New Orleans, Louisiana. USA, 1996, pp. 
738-743.
[8] Pawlak. Z, “Rough sets,”  Int. J. Coinput. Inf. Sci., 19821, vol.11,pp. 
341-356 
[9] T. Y. Lin, “Fuzzy Controllers-An Integrated Approach Based on Fuzzy 
Logic, Rough sets, and Evolutionary Computing”, CSC’95 Workshop on 
Rough Sets and Database Mining , 1995. 
[10] E. Czogala, A. Mrozek and Z. Pawlak, “The idea of a rough fuzzy 
controller and its application to the stabilization of a pendulum-car 
system”, Fuzzy Sets and Systems, pp.31-73,1995.
[11] Y. Cho, K. Lee, J. YOO and M. Park, “Autogeneration of fuzzy rules 
and membership functions for fuzzy modeling using rough set theory,” 
IEE Proc. Control Theory Appl., vol.145, pp.437-442, 1998. 
[12] C. C. Hsiao, “A Rough-Set-Based for fuzzy modeling with outlier,”
2008 International Conference on Instrumentation, Control and 
Information Technology(SICE 2008), pp.364-368, August, 2008.
[13] Sushmita Mitra, Haider Banka, and Witold Pedrycz, “Rough–Fuzzy 
Collaborative Clustering”, IEEE Trans. Syst., Man, Cybern-Part B, vol. 
36, no. 4, pp.795-805, 2006. 
[14] Pradipta Maji, Sankar K. Pal, “Rough Set Based Generalized Fuzzy C-
Means Algorithm and Quantitative Indices”, IEEE Trans. Syst., Man, 
Cybern-Part B, vol. 37, no. 6, pp.1529-1540, 2007. 
[15] J. Peters, “Near sets. Special theory about nearness of objects,” Fundam. 
Inform., vol. 75, no. 1–4, pp. 407–433, 2007.
[16] J. Peters, “Near sets. General theory about nearness of objects,” Appl. 
Math. Sci., vol. 1, no. 53, pp. 2609–2029, 2007. 
[17] J. Peters, & P. Wasilewski, “Foundations of near sets,” Information 
Sciences, vol.179, pp.3091–3109, 2009. 
[18] E. Oráowska, Semantics of vague concepts. applications of rough sets, 
Polish Academy of Sciences 469, in: G. Dorn, P. Weingartner (Eds.), 
Foundations of Logic and Linguistics. Problems and Solutions, Plenum 
Press, London/New York, 1985, pp.465–482. 
[19] J. Peters, “Classification of perceptual objects by means of features,” Int. 
J. Info. Technol. Intell. Comput. Vol.3,no.2,pp.1-35, 2008. 
[20] C. Henry, J. Peters, “Near set image segmentation quality index,” 
GEOBIA 2008 Pixels, Objects, Intelligence., pp. 1–6, 2008. 
[21] J. Peters, “Tolerance near sets and image correspondence,” Int. J. Bio-
Inspired Comput. , vol.1,no.4, pp.239-245,2008. 
[22] H. Fashandi, J. Peters, S. Ramanna, “l2 norm length-based image 
similarity measures: Concrescence of image feature histogram 
distances,” Proc. 11th IASTED Int. Conf. Signal and Image Processing,
2009. 
[23] A. Hassanien, A. Abraham, J. Peters, G. Schaefer, C. Henry, “Rough 
sets and near sets in medical imaging: A review,” IEEE Trans. Info. 
Tech. Biomed.,vol.13,no.6, pp.955-968, 2009, 
[24] S. Gupta, K. Patnaik, “Enhancing performance of face recognition 
systems by using near set approach for selecting facial features,” J. 
Theor. Appl. Inform. Technol. Vol.4,no.5, pp.433-441,2008. 
[25] C. Henry, J. Peters, “Perception-based image analysis,” Int. J. Bio-
Inspired Computation, vol.2, no.2, 2009. 
[26] J. Peter, S. Shahfar, S. Ramanna and T. Szturm, “Biologically-Inspired 
adaptive learning:a near set approach,” Frontiers in the Convergence of 
Bio. And Info. Tech. IEEE, pp.403-408,2007. 
[27] D. S. Chen and R. C. Jain, “A robust backpropagation learning 
algorithm for function approximation,” IEEE Trans. Neural Networks,
vol. 5, pp. 467–479, May 1994. 
[28] A. Cichocki and R. Unbehauen, Neural Networks for Optimization and 
Signal Processing. New York: Wiley, 1993. 
[29] L. Huang, B. L. Zhang, and Q. Huang, “Robust interval regression 
analysis using neural network,” Fuzzy Sets Syst., pp. 337–347, 1998. 
[30] C.C. Chuang, S.F. Su, and C.C. Hsiao, “The annealing robust 
backpropagation (ARBP) learning algorithm,” IEEE Trans. Neural 
Networks, vol. 11, pp. 1067–1077, Sept. 2000. 
23
100 年度專題研究計畫研究成果彙整表 
計畫主持人：蕭志清 計畫編號：100-2221-E-244-003- 
計畫名稱：相近集合與粗略集合在函數近似之應用研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
