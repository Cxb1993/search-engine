management (CTM) and cloud service cluster (CSC) is 
proposed in the second year. The CTM uses the queues 
and dispatching algorithms to reduce the lost tasks 
and achieve fairness for resources. Furthermore, the 
weight-based dispatching (WBD) is proposed to provide 
high throughput under a variety of distribution 
patterns and more stable than others on the 
performance of queue waiting time. Finally, the FCA 
(Fast Cloud Agreement) protocol is proposed to 
enhance the reliability of the cloud computing 
environment in the third year. The FCA can reach an 
agreement and detect faulty processors by using a 
minimum number of messages simultaneously and 
efficiently. Besides, the maximum number of faulty 
processors can be detected by the FCA in a cloud 
computing environment. 
英文關鍵詞： Distributed System； Fault-tolerant； Cloud 
Computing； dispatching algorithm； Load Balance； 
and Data Replication 
 
 II 
摘 要 
 
由於網際網路的蓬勃發展及電腦硬體的快速成長，使得現行的網路應用服務需求日漸增加，
傳統的網路架構都將漸漸變成分散式雲端運算網路環境。而雲端運算環境使用大量的低效能處理
單元(節點)來提供使用者各種網路應用服務。然而，有很多重要的議題需要被探討，如效能、任
務配置及可靠度。 
第一年，本計畫提出一個使用 MDR(Multi-Dimensional Ranking)為基礎的雲端服務節點選擇
機制 CSNEM(Cloud Service Node Election Mechanism)，除了選出區塊中適合提供工作分配的叢
集頭節點外，並藉由 MDR的方法讓系統在選擇服務節點進行工作時可以降低總執行時間，進而提
昇服務效率。隨後，我們也於第二年提出一個全新的 Cloud Task Management (CTM)及 Cloud 
Service Cluster (CSC)架構整體雲端環境，CTM主要是透過佇列及分配演算法來降低工作遺失來
達到公平的資源配置。此外，本計畫也於本年度提出 WBD演算法來提昇整體服務效能及達到負載
平衡。最後，Fast Cloud Agreement (FCA)協定於第三年被提出來提昇雲端運算環境的可靠度。
FCA可同時使用最小的訊息交換數來有效地偵測錯誤元件並達成分散式協議。此外，FCA也可於雲
端運算環境偵測出最大的錯誤元件。 
 
關鍵字：分散式系統、容錯、雲端運算、配置演算法、負載平衡及複本。 
 
 
 
 
 
  
1 
1 
 
1 Introduction 
Currently, the cloud computing [3] which is evolved from the grid computing is a popular 
IT technology. The major difference is that each task of a grid computing contains a large 
quantity of data and usually to be used in academic research. Contrarily, the tasks in cloud 
computing environment contain a small quantity of data and vary with services. Therefore, the 
cloud computing has capability of extensibility to support the different service requirements. For 
example, a company can rent the ability from the cloud when it needs short-term resources, such 
as Amazon EC2 [28]. Most emerging industries prefer to use cloud computing to save costs and 
improve service ability on demand. In general, the cloud computing environment can be divided 
into three major parts, Software as a Service (SaaS), Platform as a Service (PaaS), and 
Infrastructure as a Server (IaaS) [2][10]. There are some services on web 2.0 to be provided in 
SaaS such as Google docs [30], Youtube [32], Citrix [29], and so on. In PaaS, a development 
platform can be used to develop the program, such as the Windows Azure Platform [31]. 
Subsequently, the resource infrastructure can be provided in IaaS, such as Amazon EC2, Zeus 
[28][33]. Therefore, the construction costs of physical equipment can be reduced. Based on 
service architecture, the cloud computing can also be classified into three types, private cloud, 
public cloud, and hybrid cloud [7]. A private cloud is used only within a company, such as 
members of the enterprise. A public cloud refers to enterprises renting required resources, thus is 
suitable for enterprises that intend to construct a cloud environment with limited construction 
equipment. A hybrid cloud [7] is a combination of private and public cloud. The hybrid cloud is 
used a company private cloud copartnership with public cloud provider. Thus, it’s solved 
resource insufficient of private cloud by lease increase company service resource, and save the 
cost of hardware devices  
However, cloud computing is a service-oriented architecture, thus, how to allocate resources 
and dispatch of tasks according to the required capacity are a most important issue. Therefore, we 
proposed an efficient dispatching algorithm to enhance the throughput of tasks and stable queue 
waiting time in this paper. 
Besides, the service processors in the cloud computing environment are allocated to 
  
3 
3 
As a result, an efficient and suitable protocol FCA (Fast Cloud Agreement) is proposed to 
enhance the reliability of the cloud computing environment. The protocol FCA can still tolerate 
and detect/locate a maximal number of faulty processors in a minimal number of message 
exchanges under a dual failure mode. 
The rest of this paper is organized as follows: Section 2 illustrates the previous work and 
underlying assumptions. The detail of this project is shown in Section 3 and 4. Section 5 shows 
the performance evaluation in this study. Section 6 illustrates example of executing FCA. Finally, 
the conclusion is presented in Section 7.  
 
2.  Previous Work and Underlying Assumptions 
In general, users can send the service request through the internet in a cloud computing 
architecture which is shown in Fig. 1. A cloud computing environment consists of numerous 
physical computers that can carry out large amounts of calculations. It was initially evolved from 
distributed systems. However, the traditional type of servers can not satisfy the rapid 
development and diversification of network services. Therefore, a novel IT technology, the cloud 
computing is proposed to combine existing physical architectures with virtualization technology 
to enhance massive computing capacity. In order to improve task allocation efficiency, a scholar 
have proposed multiple task configuration methods and three-level hierarchical architectures [8] 
to manage the numerous resource allocation problems of a cloud. Besides, some studies proposed 
the task allocation of cloud computing, which are calculated the required completion time to 
organize the appropriate task assignment, such as OLB, MET and Min-min [1][6][8].  
However, there still has task lost in previous method when a large number of tasks incoming. 
In order to allocate the tasks to service nodes correctly, the tasks are allocated to appropriate 
queues in advance.  
  
5 
5 
can receive a set of service requests (1, 0, 0, 1, …; each vector represents the different service 
requests) and stores it in its service queue. For avoiding the influence of failure processors, each 
processor of the upper layer uses the service requests as an initial value to exchange with other 
processors in order to obtain a common value of service requests. Besides, a maximum number 
of faulty service processors can be detected/located in a minimal number of message exchanges. 
As a result, the reliability of cloud computing can be enhanced. 
 
Figure 2. The cloud computing environment 
 
Subsequently, the basic assumptions of an agreement protocol are described as follows. In 
general, the BA problem cannot achieve agreement in an asynchronous network even if only one 
processor has failed and that failure is a crash failure [14]. Therefore, we assume that the 
processing and the communication delays of healthy processors are finite [14] in a synchronous 
  
7 
7 
even though some of them are dormant faults. This treatment ignores the fact that the faulty 
behaviors of dormant faults are not as severe as those of malicious faults. When a dormant fault 
exhibits its specific behavior, it can be detected and thus ignored by all healthy processors. 
Therefore, their protocol cannot detect/locate the maximum number of faulty processors. 
To detect/locate the maximum number of faulty processors, the proposed protocol must 
consider the dual failure modes (including malicious and dormant faults) of the processors and 
instruct all healthy processors to detect/locate all faulty components. Subsequently, the following 
FDA requirements [11][12] need to be satisfied to ensure the performance and integrity of a 
distributed network. 
 (FDA1) Agreement: All healthy processors identify the common set of faulty processors. 
 (FDA2) Fairness: No fault-free component is falsely identified as faulty by any healthy 
processor. 
To achieve the requirements above, Hsiao et al., [15] proposed an evidence-based protocol 
FDAMIX with dual failure modes to solve the FDA problem. The FDAMIX protocol first 
collects received messages according to the GPBA [25] protocol that is designed to determine a 
common value in a distributed system under a dual failure mode. Subsequently, the collected 
messages are used as evidence to detect/locate faulty processors. In the FDAMIX protocol, all 
healthy processors can identify the maximum number of faulty processors under dual failure 
modes using fm+2 rounds of message exchanges because the GPBA requires fm+1 rounds of 
message exchanges to achieve an agreement. As a result, the fm+1 and fm+2 rounds of message 
exchanges are necessary in previous protocols [11][21][24] to solve BA and FDA problems for 
achieving reliability. However, message passing is a time consuming process and the number of 
  
9 
9 
Cloud Task Management (CTM) 
The first layer in this architecture is the Cloud Task Management (CTM), which is 
composed of many nodes, as shown in Fig. 4. The users can temporarily store tasks in the Task 
Management Input Queue (TMIQ) through the Cloud User Interface (CUI). Subsequently, the 
current status of each CSC in the second layer can be stored into the Cluster Server Queue Status 
(CSQS). All tasks can be allocated according to the weight value which is set in WBD algorithm 
to each CSC in the next round. 
The execution flow of WBD is as shown in Fig. 5. It is assumed the tasks can be assigned to 
CSC 1 to N from CTM in first round by Round-Robin algorithm. Subsequently, the utilization 
weight of CSC can be updated in CSQS. In following round, the tasks can be dispatched 
according to the utilization weight of CSQS by WBD algorithm. For example, as shown in Fig. 6, 
Round-Robin algorithm can be used to dispatch task to each CSC in first round. In second round, 
the task can be allocated in step 1 when a CSC A has free queue space are shown in Fig. 6. 
Subsequently, the CSQ utilization weight can be updated to CSQS of CTM in step 2. In 
following step, the CSC A can be assigned the task again according to the weight value of CSCQ 
of CTM in step 3. 
 
Figure 4. Cloud task management architecture (CTM).  
  
11 
11 
Cluster Server Queue 
 
Figure 7. Cloud service cluster architecture.  
Based on the description above, the weight-based two-layer architecture in this paper can 
appropriate allocate tasks according to weight to avoid the CSC overloading. Therefore, the 
proposed algorithm, WBD, will be enhance throughput and the reduce queue waiting time. 
Subsequently, the proposed method in this study is simulated in the next section. 
 
4 The FCA protocol 
 
The Fast Cloud Agreement (FCA) protocol we propose includes four phases: the request 
collect phase, message exchange phase, fault diagnosis phase, and decision-making phase. The 
FCA protocol is shown as Figure 10. In our protocol, the service request is collected in the 
request collect phase. Subsequently, the message exchange phase requires three rounds to 
exchange service requests, as does the GPBA [25]. The fault diagnosis phase is used to detect 
the faulty service processors. In last phase, the decision-making phase, the agreement of service 
requests is obtained by each service processor. Subsequently, the parameters are assumed to be 
the following table:  
TABLE 1. THE DESCRIPTION OF PARAMETER IN FCA PROTOCOL 
parameter description 
n The total number of service processors in the cloud computing environment. 
  
13 
13 
service processor name list contains the names of the service processors through which the stored 
messages have been transferred. For example, the statement v(sbc) represents the service 
processor having received the value sb from service processor SPc, which was sent from source 
service processor SPs to service processor SPb. The vertices, having repeated service processor 
names of ic-trees are removed in order to avoid cyclical influences from the faulty service 
processors. The cyclical influences originate from messages sent by faulty service processors that 
may be stored repeatedly in the ic-tree, and thus, could result in an incorrect common value being 
obtained by taking a simple majority. Therefore, the ic-tree (Ti) can be used to store the received 
messages and eliminate the influence of faulty components, as shown as Figure 8. 
After constructing the ic-trees, our protocol can remove dormant faulty processors if the 
Manchester code [20] is used in encoding before transmission. This is because Manchester 
encoding is a synchronous clock encoding technique and the values sent by dormant faulty 
components are replaced by λ; thus, the recipient can easily distinguish the dormant faulty 
components by function MAJ(α), shown as Figure 9. In addition, the collected messages of this 
phase can be used as evidence to eliminate the influence of faulty processors. 
 
 
 
 
 
 
Figure 8. The ic-tree (Ti)                    Figure 9. The function MAJ 
Level 1 
root 
Level 2 Level 3 
 v (s) v (sa) v (saa) v (sab) 
 v (sac) 
v (sb) v (sba) 
v (sbb) 
v (sbc) 
v (sc) v (sba) 
v (scb) 
v (scc) 
The function MAJ(α) 
MAJ(α)= 
1. The majority value in the set 
of {v(αj)|1≤j≤n}, if such a 
majority value exists. 
2. The complement value of 
v(α), denoted as ¬v(α), is 
chosen, otherwise. 
  
15 
15 
processors gets larger, more and more message exchanges become necessary. It is unreasonable 
and inefficient in cloud computing environment. In our protocol, the message complexity of our 
proposed protocol FCA is O(cn2) by using only three rounds of message exchanges to get an 
agreement. Besides, the round of message exchanges is less than another famous Byzantine 
problem, Eventual Byzantine Agreement (EBA) [26][27]. EBA allows the healthy processors to 
achieve an agreement during different rounds when they receive enough information. The 
comparisons between FCA and previous BA protocols [15][17][18][26][27] are shown as Table 
2. 
Furthermore, an additional and important contribution of our protocol is to reduce the 
complexity of the diagnostic procedure. The FDA problem is solved early by our proposed FCA 
protocol that uses the minimum number of rounds characterized by the dual failure of the 
processors. The comparisons between FCA and previous diagnosis protocols are shown as Table 
3.  
 
Request Collect Phase  
The service request is sent to each service processor. 
Message Exchange Phase 
Collect σ (σ = 3) rounds of messages as evidences. 
σ = 1, do: 
1) Each service processor broadcasts its initial vector value v(s,s,s,s,s) to other service 
processors and to itself. Subsequently, the corresponding ic-tree is constructed by 
each service processor. 
σ = 2, do: 
2) Each service processor broadcasts its ic-tree to other service processors and 
constructs its corresponding ic-tree for each round.  
σ = 3, do: 
3) Each service processor broadcasts its ic-tree to others, and receives the other 
ic-trees sent by other service processors to construct the corresponding ic-tree for 
  
17 
17 
Yang, W. 
P.[15] 
FCA 3 O(cn3) Dual faults n > 3SPfm+SPfd 
 
5 Performance Evaluation  
We modeled three dispatching schemes for simulation: WBD, RRD, and RD to show the 
performance of weight-based and weightless-based schemes. We considered TMIQ=16 to show 
the performance of these schemes, and CSQ = {2, 4, 16} for a fair comparison of WBD and the 
other two schemes. The results of simulation are shown as follows figures. 
0 20000 40000 60000 80000 100000
0.5
0.6
0.7
0.8
0.9
1
Simluation time
T
hr
ou
gh
pu
t
CSQ=2
RD
RRD
WBD
 
Figure 11. Throughput performance of RD, RRD, and WBD with CSQ=2 under identical 
distribution of tasks. 
0 20000 40000 60000 80000 100000
0.5
0.6
0.7
0.8
0.9
1
Simulation time
T
hr
ou
gh
pu
t
CSQ=4
RD
RRD
WBD
 
Figure 12. Throughput performance of RD, RRD, and WBD with CSQ=4 under identical 
distribution of tasks.  
  
19 
19 
Figure 15. Queue waiting time by RD, RRD, and WBD with CSQ=4 under identical 
distribution of tasks.  
0 20000 40000 60000 80000 100000
0
100
200
300
400
500
600
Number of Tasks
Q
ue
ue
 w
ai
ti
ng
 t
im
e
CSQ=16
RD
RRD
WBD
 
Figure 16. Queue waiting time by RD, RRD, and WBD with CSQ=16 under identical 
distribution of tasks.  
0 20000 40000 60000 80000 100000
0.5
0.6
0.7
0.8
0.9
1
Simulation time
T
hr
ou
gh
pu
t 
CSQ=2
RD
RRD
WBD
 
Figure 17. Throughput performance of RD, RRD, and WBD with CSQ=2 under 
non-identical distribution of tasks.  
  
21 
21 
0 20000 40000 60000 80000 100000
10
100
1000
10000
100000
Number of Tasks
Q
u
eu
e 
w
ai
ti
n
g 
ti
m
e
CSQ=4
RD
RRD
WBD
 
Figure 21. Queue waiting time by RD, RRD, and WBD with CSQ=4 under non-identical 
distribution of tasks.  
0 20000 40000 60000 80000 100000
10
100
1000
10000
100000
Number of Tasks
Q
ue
ue
 w
ai
ti
ng
 t
im
e
CSQ=16
RD
RRD
WBD
 
Figure 22. Queue waiting time by RD, RRD, and WBD with CSQ=16 under non-identical 
distribution of tasks.  
 
6 Example of Executing FCA 
In this section, an example of 14-service processors is provided to illustrate the FCA 
protocol in Figure 23. Besides, the service processors SPa, SPb, SPf, and SPj are assumed as 
malicious service processors and service processor SPn as the dormant faulty service processor 
shown in Figure 23(a). In general, the behavior of malicious service processors is unpredictable 
and damaged, making the agreement fail. Therefore, the positive and negative messages are 
  
23 
23 
of message exchanges under the e-mail service requests. The result of dcT  is shown in Figure 
23(e) and is the same in other healthy service processors. 
In the following phase, the fault diagnosis phase, the dormant faulty service processor can 
be detected by the dormant removing rule. This is because a transmitted message from the 
dormant faulty service processor is encoded appropriately by either the Non-Return-to-Zero code 
or the Manchester code. Subsequently, the SPn is detected as dormant faulty service processor 
and stored in the DFset when the number of λ is greater than 4 (13 > (n-1-SPfd)/3 =4). In the 
malicious removing rule (a) of the fault diagnosis phase, the fault-free service processor is 
recognized so it can to remove the malicious faulty service processor as shown in Figure 23(f). 
Subsequently, the example of malicious removing rules (a) and (b) are shown as follows. 
.
).(),1(_
.12:_,11:_
,6:_,5:_,4:_,0:_
._)(
),8411141(8)(_
)1(.
)()(1()(_),()()(
,,,,,,,,
,,,,,,,
,
,,,,,,,,3
333
d
SPcmlkihgedc
d
SPcxfdfmx
ihgedcmlk
bajfn
d
SPcx
mlkihgedc
d
SPcfdfm
d
SPcx
fdfm
FFSPinstoredandprocessorsservicefreefaultisSP
FFSPprocessorservicefreefaultisSPthenSPSPnSPnumif
SPnumSPnum
SPnumSPnumSPnumSPnum
FFLSPallincountedisSPnumtheb
SPFFLSPSPSPndcMAJnum
nxFFLSPinstoredbecanSPthethen
dcxvdcMAJandSPSPndcMAJnumdcMAJdcvifa
−⇒
−−−−>
==
====⇒
==−−−=−−−≥=⇒
≤≤
=−−−≥=
 
Based on the description above, the v(dc) = MAJ3(dc) = 1 and the num_MAJ3(dc) = 8 ≥ 
[(n-1-SPfm-SPfd) = 8] are satisfied. Then, the SPx can be added to the dSPcFFLSP  set of ic-tree 
d
cT  and recognized as a fault-free like service processor while the v(dcx) = MAJ3(dc). 
Subsequently, the num_SPx will be counted to represent the number of SPx that appears in all 
d
SPcFFLSP s. Finally, the SPc, d, e, g, h, i, k, l, m are recognized as fault-free service processors and 
  
25 
25 
 
 
Figure 23(a). The proposed example in a cloud computing environment 
 
 
 E-mail FTP On-line Others 
SP 1 0 1 1 
 
Figure 23(b). The received service requests in request collect phase. 
 
 E-mail FTP On-line Others 
SPa 0 1 0 0 
SPb 1 0 0 0 
SPc 1 0 1 1 
SPd 1 0 1 1 
SPe 1 0 1 1 
SPf 0 1 1 1 
  
27 
27 
 
l evel 
0 
Root  
 level 
0 
Root  
 level 
0 
Root  
 level 
0 
Root 
 
 
 level 
0 
Root 
 
SPi v(a) 0  SPk v(a) 0  SPl v(a) 0  SPm v(a) 0  SPn v(a) 0 
1 v(b) 1  0 v(b) 1  0 v(b) 1  0 v(b) 1  0 v(b) 1 
 v(c) 1   v(c) 1   v(c) 1   v(c) 1   v(c) 1 
 v(d) 1   v(d) 1   v(d) 1   v(d) 1   v(d) 1 
 v(e) 1   v(e) 1   v(e) 1   v(e) 1   v(e) 1 
 v(f) 0   v(f) 1   v(f) 0   v(f) 1   v(f) 0 
 v(g) 1   v(g) 1   v(g) 1   v(g) 1   v(g) 1 
 v(h) 1   v(h) 1   v(h) 1   v(h) 1   v(h) 1 
 v(i) 1   v(i) 1   v(i) 1   v(i) 1   v(i) 1 
 v(j) 1   v(j) 0   v(j) 1   v(j) 0   v(j) 1 
 v(k) 1   v(k) 1   v(k) 1   v(k) 1   v(k) 1 
 v(l) 1   v(l) 1   v(l) 1   v(l) 1   v(l) 1 
 v(m) 1   v(m) 1   v(m) 1   v(m) 1   v(m) 1 
 v(n) λ   v(n) λ   v(n) λ   v(n) λ   v(n) λ 
 
Figure 23(d). The ic-tree of e-mail service request received from each processor in first round of 
message exchange phase. 
  
29 
29 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 23(e). The ic-tree of SPc in second round of message exchange phase under E-mail service 
request 
level 0 
Root 
 
level 1 level 2 
  
level 1 level 2 
  
level 2 level 2 
 
SPc v(j)= 0 v(ja) 1  v(k)=1 v(ka) 1  v(l)=1 v(la) 1 
1  v(jb) 1   v(kb) 1   v(lb) 1 
  v(jc) 1   v(kc) 1   v(lc) 1 
  v(jd) 1   v(kd) 1   v(ld) 1 
  v(je) 1   v(ke) 1   v(le) 1 
  v(jf) 0   v(kf) 0   v(lf) 0 
  v(jg) 1   v(kg) 1   v(lg) 1 
  v(jh) 1   v(kh) 1   v(lh) 1 
  v(ji) 1   v(ki) 1   v(li) 1 
  v(jj) 0   v(kj) 0   v(lj) 0 
  v(jk) 1   v(kk) 1   v(lk) 1 
  v(jl) 1   V(kl) 1   v(ll) 1 
  v(jm) 1   v(km) 1   v(lm) 1 
  v(jn) λ   v(kn) λ   v(ln) λ 
            
 v(m)=1 v(ma) 1  v(n)=1 v(na) 1  
  v(mb) 1   v(nb) 1  
  v(mc) 1   v(nc) 1  
  v(md) 1   v(nd) 1  
  v(me) 1   v(ne) 1  
  v(mf) 0   v(nf) 0  
  v(mg) 1   v(ng) 1  
  v(mh) 1   v(nh) 1  
  v(mi) 1   v(ni) 1  
  v(mj) 0   v(nj) 0  
  v(mk) 1   v(nk) 1  
  v(ml) 1   v(nl) 1  
  v(mm) 1   v(nm) 1  
  v(mn) λ   v(nn) λ  
         
  
31 
31 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 23(g). The example of vertex v(d) in SPc during malicious removing rule (c) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
level 0 
Root 
 
level 1 level 2 level 3 
   
SPc v(d)=1 v(dc)=1 v(dca) 0  1 
1   v(dcb) 0  1 
   v(dcc) 1   
   v(dcd) 1   
   v(dce) 1   
   v(dcf) 0  1 
   v(dcg) 1   
   v(dch) 1   
   v(dci) 1   
   v(dcj) 0  1 
   v(dck) 1   
   v(dcl) 1   
   v(dcm) 1   
   v(dcn) λ   
       
  
33 
33 
 
 
7 Conclusion 
 
In this project, we introduced a Cloud Task Management (CTM) to determine task eligibility in the 
dispatching process for the cloud computing environment. Also, we proposed the queued framework and 
the weight-based dispatching (WBD) algorithm, which are used in the CTM. The loss and overloading 
for cloud computing environments can be reduced by the CTM. This is because the tasks are dispatched 
according to the conditions of clusters (servers) by counting the occupancy of server queues while other 
mechanisms don’t consider them. According the results of experiment, we showed that WBD delivers 
higher performance than the other schemes under different distributional models. The presented WBD 
scheme shows above 99% throughput under two patterns of task distributions. 
Besides, the proposed algorithm, FCA, can use to detect/locate the faulty processors to enhance the 
reliability of cloud computing environment. The FCA can reduce a large number of message exchanges 
to reach an agreement and detect a maximum number of faulty service processors with dual failure 
modes by using three rounds of message exchanges. The proposed protocol cannot only enhance the 
reliability of the cloud computing environment, but it also can reduce the number of messages to O(cn3). 
As a result, the FCA protocol is more efficient and reasonable in the cloud computing environment than 
previous protocols where n>(n-1)/3+2SPfm+SPfd and c>2SPfm + SPfd. 
According the descriptions above, a novel cloud computing architecture including cloud task 
management (CTM) and cloud service cluster (CSC) is proposed to reduce the lost tasks and achieve 
fairness for resources in this project. Furthermore, the weight-based dispatching (WBD) is proposed to 
provide high throughput under a variety of distribution patterns and more stable than others on the 
performance of queue waiting time. In last year, the FCA (Fast Cloud Agreement) protocol is proposed 
to enhance the reliability of the cloud computing environment. The FCA can reach an agreement and 
detect faulty processors by using a minimum number of messages simultaneously and efficiently. 
Besides, the maximum number of faulty processors can be detected by the FCA in a cloud computing 
environment in this project.  
ACKNOWLEDGMENT 
This work was supported in part by the Taiwan National Science Council under Grant 
  
35 
35 
Algorithm for Open-Source Cloud Systems," Fifth Annual ChinaGrid Conference 2010, pp. 
124-129, July 2010. 
[11] Halsall, F.: Data Communications, Computer Networks and Open Systems. 4th ed., 
Addison-Wesley Publishers, Massachusetts, USA (1995). 
[12] Gong, C., Liu, J., Zhang, Q., Chen, H., Gong, Z.: The characteristics of cloud computing. Proc. of 
the International Conference on Parallel Processing, 275-279 (2010) 
[13] Bar-Noy A., Dolev, D., Dwork, C., Raymond Strong, H.: Shifting gears: changing algorithms on the 
fly to expedite byzantine agreement. Information and Computation. 97, 205-233 (1992) 
[14] Dolev, D., Reischuk, R.: Bounds on information exchange for byzantine agreement. Journal of 
ACM 32, 191-204 (1985) 
[15] Hsiao, H. S., Chin, Y. H., Yang, W. P.: Reaching fault diagnosis agreement under a hybrid fault 
model. IEEE Transactions on Computers 49, 980-986 (2000) 
[16] Meyer, F. J., Pradhan, D. K.: Consensus with dual failure modes. IEEE Transactions on Parallel 
Distribute System 2, 214-222 (1991) 
[17] Wang, S. S., Wang, S. C., Yan, K. Q.; An optimal solution for byzantine agreement under a 
hierarchical cluster-oriented mobile ad-hoc network. Computers and Electrical Engineering 36, 
100-113 (2010) 
[18] Wang, S. C., Yan, K. Q., Wang, S. S., Zheng, G. Y.: Reaching agreement among virtual subnets in 
hybrid failure mode. IEEE Transactions on Parallel and Distributed Systems 19, 1-11 (2008) 
[19] Yan, K. Q., Wang, S. S., Wang, S. C.: Reaching an agreement under wormhole networks within 
dual failure component. International Journal of Innovative Computing, Information and Control 6, 
1151-1164 (2010) 
[20] Fischer, M.J., Lynch, N.A.: A lower bound for the time to assure interactive consistency. 
Information Processing Letters 14, 183-186 (1982) 
[21] Yan, K. Q., Wang, S. C.: grouping byzantine agreement. Computer Standard & Interfaces 28, 75-92 
(2005) 
[22] Yan, K. Q., Wang, S. C.: Reaching fault diagnosis agreement on an unreliable general network. 
Information Sciences 170, 397-407 (2005) 
[23] Lamport, L., Shostak, R., Pease, M.: The byzantine generals problem, ACM Transactions on 
Programming Languages and Systems 4, 382-401 (1982) 
[24] Shin, K., Ramanathan, P.: Diagnosis of processors with byzantine faults in a distributed computing 
system. Proc. of the Symp. Fault-tolerant Computing, 55-60 (1987) 
  1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                日期：2013 年 07月 14 
  
一、參加會議經過 
此次研討會的會期共計三天，就在印尼/巴里島 舉行。大會正式展開的
時間為 7月 7日至 7月 9日。 
而為了促進學術界與產業界在資訊相關領域的研究與發展能力，並培
養在本領域之研發人才，此次研討會的主題分列如下： 
• Accounting  
• Business Administration  
• Business Policy and Strategy  
• Economics  
• Electronic Commerce  
• Entrepreneurship  
計畫
編號 
NSC99－2221－E－324-041-MY3 
計畫
名稱 
雲端運算環境下最佳協議與損毀診斷之探討 
出國
人員
姓名 
江茂綸 
服務
機構
及職
稱 
資訊與通訊系 助理教授 
會議
時間 
2013年 7月 7日至 
2013年 7月 9日 
會議
地點 
印尼 巴里 
會議
名稱 
(中文) 2013商務與資訊國際研討會 
(英文) The 2013 International Conference on Business and 
Information (BAI2013) 
發表
論文
題目 
(中文) 在雲端運算環境中的動態資料複製存取機制 
(英文) DYNAMIC DATA REPLICATION ACCESS SCHEMES IN 
CLOUD COMPUTING 
附件四 
  3 
多元化的服務來滿足不同需求的使用者。然而，雲端運算使用數十萬台運
算能力與個人電腦的主機效能相當的電腦來組成一個龐大的分散式系統，
將可能有不同的需求在不同的地點被產生，而為了增加整體服務的效能，
通常會將常用及有需要的資料進行複製，以方便本地端的使用者可以快速
取得資料。而這些複本也可能因為過多的複製而影響資料存取的效能，並
浪費儲存空間。此外，過久且閒置的資源也需要被更新及置換。因此，如
何在雲端運算環境中處理巨量的資料，讓使用者可以快速適當的取得複本
資料來增加系統整體的效能，且更新及置換閒置資源，都將是我們需要去
解決的議題。本研究提出 DDRA將可以有效提昇雲端運算上複本的存取效
能，更可透過服務節點的調整來達到節點間的負載平衡。 
 
三、考察參觀活動(無是項活動者略) 
無。 
四、建議 
希望多鼓勵參與國際學術研討會，同時也可以增加參加產學研討會的項
目。 
 
五、攜回資料名稱及內容 
Proceedings of The 2013 International Conference on Business and Information 
(BAI2013)。 
  5 
Therefore, this paper proposes a novel access scheme of data replication to provide high 
performances of storage and load balance for the cloud system, called dynamic data replication 
access (DDRA) scheme. The proposed scheme can provide users to access to a replica from the 
adapted service nodes according to the loads of nodes. Furthermore, this approach also can 
distributed workloads to enhance performances of access ability, availability, and reliability of data. 
The result via analysis shows that the DDRA can provide the high performance of replication access 
and achieve load balance of service nodes.   
 
Keywords: cloud computing; cloud storage; data replication; load balance. 
 
I. INTRODUCTION 
Cloud computing (Buyya et al., 2009; Gunarathne et al., 2010) is developed and evolved from 
grid computing (Ranganathan and Foster, 2001); the word "grid" means to integrate heterogeneous 
servers located in different domains to perform mass data computing. Therefore, cloud computing is 
different from the traditional distributed computing (Kamali et al., 2011). 
The goal of the cloud computing is to enhance the next generation of data centers and provide 
lease service based on quality of service for users(QoS) (Mansouri and Monsefi, 2008). Then, the 
service is divided into three categories: Infrastructure as a Service (IaaS), Platform as a Service 
(PaaS) and Software as a Service (SaaS) (Qaisar, 2012). 
The IaaS includes servers, storage, virtual devices and other core infrastructures, so users are 
able to store, execute and compute on hardware resources provided by the vender via virtual 
technologies. For PaaS, it includes databases, network servers and development tools, and allows 
users to develop a number of services that they are in want of through this platform. The last service, 
SaaS, the software is developed by the cloud provider and uploaded to the Internet platform, so 
users can utilize services without installing any programs. 
In addition, in order to provide virtual servers, computing and storage services, current cloud 
provider set up data centers in different locations, as the Cloud Exchange (CEx) shown in Figure 1 
(Buyya et al., 2009). In other words, the cloud service providers integrate available resources to the 
cloud; the cloud coordinators allocate the resources on the CEx through categories and 
arrangements; the customer negotiates with cloud coordinators to acquire various services on 
demand. 
  7 
enhances the overall performance of system by allocating replications in more popular nodes. 
However, the method causes two problems: First, it creates the block on the node and is not suitable 
for cloud computing, which requires an environment of a large number of services. Second, it also 
causes the longer response time of system.   
Therefore, Wei presents an algorithm, CDRM, which is suitable for the cloud environment. By 
allocating replicas on nodes with lower blocking probabilities and moving replicas according to the 
popularity of the node, the load balance and the performance of system are much better (Wei et al., 
2010). However, the CDRM uses "  tree" to manage and update the system; the “  tree” 
selects the same node over and over again due to the high capability of nodes, hence causes 
unbalances between nodes and degrades the performance of system. 
In order to solve the above problem, this paper provides a dynamic replication mechanism to 
access replicas according to the workload of nodes. The proposed scheme, Dynamic Data 
Replication Access (DDRA), can improve the workload balance of nodes, and control the number 
of replicas according to the popularity of nodes inside clusters. Therefore, the load of system can be 
balanced and the performance of system can be enhanced. 
 
III. RESEARCH METHODS 
In this Section, we propose a storage architecture of replication in cloud environments, as 
shown in Figure 2. This architecture can be divided into three parts respectively: Cloud manager, 
Name node, and Cloud user. 
 
Figure 2. Cloud Computing Architecture of Replication. 
 
The cloud manager is mainly responsible for the communication and management of the main 
nodes and users. Managers sent requests to the main nodes according to the metadata and updated 
metadata to make users to access the correct information. The Name node manages nodes in its own 
group and adjusts replicas dynamically according to popularity of replicas. It moves high-demand 
  9 
order to reduce the request losses, the access times of replicas are adjusted  in a later phase to 
sharing the loads in those sub-nodes which has BP higher than the BP threshold value ( ). In 
other words, overloaded nodes degrade work performances and lack-loaded nodes waste their own 
capacities. Therefore, through the adjustment of this phase, the entire system is able to reach the 
initial load balance. Besides, the performance and the stability can be enhanced. The detailed 
procedure is shown in Figure 3: 
 
: the number of sessions (c) in the nodes i  
taski: the request task of user i 
: the BP value of node i 
avg (BP): the average BP of all nodes  
low_ BPi: node i with BP lower than the avg(BP)  
high_ BPi: node i with BP higher than the avg(BP) 
minCS: the set of nodes with BP lower than the avg(BP) 
Input: 、 、taski、session 
Output: minCS 
1. for each taski 
2.   If replica exists in node 
3.    calculate the BPi by eq.(2) 
4.    calculate the threshold ( ) by avg(BP) 
5.   If BPi< avg(BP) 
6.    node i ∪ minCS 
7.   end if 
8. end for 
9. return taski 
 
Figure 3. Anti-Blocking Probability Selection Phase 
 
B. Node Load Balance phase; NLB 
After selecting replica-nodes with lower BP, this phase balances the loading rate of nodes. In 
this phase, the concept of the atomic Half-life is applied to calculate and update the weight values 
of AF; this means to setup an updating time ( ) and based on the number of time intervals passed, 
to set different weighted values according to the access frequency of nodes. This process enables 
users to access on nodes with better weight values to balance the workload of nodes. The weighted 
formula is shown as formula (3); and a detailed procedure is shown as Figure 4: 
=   (3) 
 
min(AF): the lowest AF value among all nodes  
Input: AF(access frequency) 
Output: min(AF  
1. for each node i from minCS set 
2.  calculate the weight of each node i by eq.(3) 
3.  If  is min(AF  of all of nodes the replica data of node i can be provided  
4.  end if 
  11 
IV. Performance Evaluation 
In this section, examples are presented to illustrate the steps of "DDRA". In the following 
instance, the Name node is assumed to provide different replicas of files through five Data nodes: 
N1, N2, N3, N4, N5, N6 and N7; the files A, B and C are accessed at the time point t respectively. 
The access times for A, B, and C are shown below: 
 
Table 1: The Access Number of Times of File in Each Node 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Now, we can define each parameters of formula (1) in more details: 
 : We assumed that the total number of requests is 300, and only 235 delivered according to the 
concept of ”poisson” (Berger, 1991). Other requests may be failed due to pack losses or long 
waiting time during the delivery from the customer. Therefore,  is assumed to be 0.783. 
 
 : During the delivery of requests from Name node to Data node, when the targeted replicas is 
being accessed by the request, we define this period as the delay time . In this case, we assume 
the time for the requests to all nodes as 3s. 
 
 : The memory of every sub-node can be divided into 3 sessions to store different replicas. 
 
Assuming the customer requests for file A, we have to calculate from all nodes containing file A. 
An example is shown as Figure 6. 
FileID NodeID Access times 
A N1 15 
B N6 5 
B N7 6 
A N4 5 
C N1 5 
A N5 6 
B N7 10 
C N2 9 
C N4 6 
B N2 12 
A N2 8 
A N3 10 
C N3 5 
C N6 8 
B N5 7 
  13 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
After the calculation, we select nodes based on the average BP value as shown in Table 4. 
Table 4: The Average of Blocking Probabilities (BP) 
 
 
 
 
 
By setting up the threshold of the average BP, nodes can be selected if its BP is below the 
threshold. For the above example, Node 2, 4 and 5 are compared. Afterwards, we use the value of 
AF from phase 2 and couple those with the atomic Half-life to set our weight values. The node with 
the smallest weight value is picked as the appropriate node for users to access. See Table 5. 
 
Table 5: Weighted Value of Nodes 1 and 2 
Node ID Access times 
Node 1 15*20=7 
Node 2 8*20=10 
 
 
In order to compare DDRA and CDRM, we first set t as a time period, and the interval 
between each period is 10s. We then illustrate how the nodes are selected by simulating the process 
through 6 time intervals; and between the intervals, each node accepts requests randomly. In a 
traditional CDRM method, we obtain requesting amount through each period (t), and calculate 
NodeID 
 
Node1 
 
=0.36 
Node2 
 
=0.144 
Node3 
 
=0.19 
Node4 
 
=0.07 
Node5 
 
=0.114 
avg(BP) (0.36+0.144+0.19+0.07+0.114)/5
=0.1756 
  15 
(t=2) (t=5) 
 
0.138 0.138 0.414 0.829 0.829 
 
0.335 0.447 0.559 0.783 0.223 
 
0.019 0.019 0.136 0.314 0.314 
 
0.1 0.151 0.203 0.295 0.052 
 
Now we use the formula of phase 1 of DDRA, = , to calculate the BP 
and threshold value. Besides, the node which has BP smaller than the threshold can be elected. We 
then compare those nodes in phase 2, based on their total AF; and pick the node with the lowest AF 
as the service node for the current period. In this case, we pick Nodes 2, 4 and 5 based on the BP 
threshold:  
 (0.3+0.144+0.19+0.07+0.114)/5=0.169 
Then in phase 2, at t = 0, the Node 4 is selected due to it has the smallest total AF which is 
shown below: Node 2 = 8, Node 4 = 5 and Node 5 = 6. 
The same procedures are done for t = 1. In this case, BP threshold = 
(0.008+0.265+0.265+0.16+0.008)/5=0.1412. Thus, Node 1 and 5 are selected in phase 1 for both 
having the BP below average BP threshold (0.008). In phase 2, the total AF can be calculated as 
following: Node 1 = 15 (t=0) + 1 (t=1) =16 and Node 5 = 6 (t=0) +3 (t=1) =9. 
Based on the calculation above, Node 5 is selected as it has the smallest AF. We use this method to 
pick nodes from different periods: Node 5 (t=1)，Node 2 (t=2)，Node 3 (t=3)，Node 4 (t=4) and 
Node 1 (t=5), as shown in Table 7. 
 
Table 7: An Example of DDRA at Six-time Intervals 
 Node1 Node 
2 
Node3 Node4 Node5   Node1 Node 
2 
Node3 Node4 Node5 
Incoming task 
(t=0) 
15 8 10 5 6 Incoming task 
 (t=3) 
2 5 0 4 3 
 
0.8 0.43 0.54 0.27 0.323 
 
0.335 0.838 0 0.671 0.503 
 
0.3 0.144 0.19 0.07 0.114 
 
0.1 0.318 0 0.251 0.177 
total  
AF 
15 8 10 5 6 total 
AF 
19 17 16 17 16 
Incoming task 
 (t=1) 
1 3 3 2 1 Incoming task 
 (t=4) 
1 5 3 1 6 
 
0.078 0.704 0.704 0.469 0.078 
 
0.146 0.734 0.44 0.146 0.88 
 
0.008 0.265 0.265 0.16 0.008 
 
0.025 0.277 0.148 0.025 0.333 
total  
AF 
6 11 13 7 7 total 
AF 
20 22 19 18 22 
Incoming task 
 (t=2) 
1 1 3 6 6 Incoming task 
 (t=5) 
3 4 5 7 2 
 
0.138 0.138 0.414 0.829 0.829 
 
0.335 0.447 0.559 0.783 0.223 
 
0.019 0.019 0.136 0.314 0.314 
 
0.1 0.151 0.203 0.295 0.052 
total  
AF 
17 12 16 13 13 total 
AF 
23 26 24 15 24 
 
As shown in Figure 7, the x-axis represents service nodes from Node 1 to 5; the y-axis 
represents the utilization of nodes; and the AVERAGE represents the best utilization of each nodes. 
The figure shows that the closer the curve of the node approaches the AVERAGE, the better load 
  17 
distributed switching systems and telecommunications networks. IEEE Transactions on 
Communications, 39(4), 574-580. 
Björkqvist, M., Chen, L.Y. & Binder, W. 2010. Load-Balancing Dynamic Service Binding in 
Composition Execution Engines. IEEE Asia-Pacific Services Computing Conference 
(APSCC), Hangzhou, 67-74. 
Buyya, R., Ranjan, R. & Calheiros, R.N. 2009. Modeling and Simulation of Scalable Cloud 
Computing Environments and the CloudSim Toolkit: Challenges and Opportunities. 
International Conference on High Performance Computing & Simulation, Leipzig, 1-11.  
Chang, R.S., Chang, H.P. & Wang, Y.T. 2008. A dynamic weighted data replication strategy in data 
grids. International Conference on Computer Systems and Applications (AICCSA), Doha, 
414-421. 
Gunarathne, T., Wu, T.L., Qiu, J. & Fox, G.C. 2010. MapReduce in the Clouds for Science. IEEE 
Second International Conference on Cloud Computing Technology and Science (CloudCom), 
Indiana: Indianapolis, 565-572.  
Kamali, S., Ghodsnia, P. & Daudjee, K. 2011. Dynamic data allocation with replication in 
distributed systems. IEEE 30th International Performance Computing and Communications 
Conference (IPCCC), Florida: Orlando, 1-8. 
Li, W.H., Yang, Y. & Yuan, D. 2011. A Novel Cost-Effective Dynamic Data Replication Strategy for 
Reliability in Cloud Data Centres. IEEE Ninth International Conference on Dependable, 
Autonomic and Secure Computing (DASC), New South Wales: Sydney, 496-502. 
Mansouri, Y. & Monsefi, R. 2008. Optimal Number of Replicas with QoS Assurance in Data Grid 
Environment. Second Asia International Conference on Modeling & Simulation(AICMS), 
Kuala Lumpur, 168-173.   
Myint, J. & Naing, T.T. 2011. A data placement algorithm with binary weighted tree on PC 
cluster-based cloud storage system. International Conference on Cloud and Service 
Computing (CSC), Hong Kong, 315-320. 
Qaisar, E.J. 2012. Introduction to cloud computing for developers: Key concepts, the players and 
their offerings. Information Technology Professional Conference (TCF Pro IT), New Jersey: 
Ewing, 1-6. 
Ranganathan, K. & Foster, I. 2001. Identifying Dynamic Replication Strategies for a 
High-Performance Data Grid. Proceedings of the Second International Workshop on Grid 
Computing, 75-86. 
Wei, Q.S., Veeravalli, B., Gong, B., Zeng, L.F. & Feng, D. 2010. CDRM: A Cost-Effective Dynamic 
Replication Management Scheme for Cloud Storage Cluster. IEEE International Conference 
on Cluster Computing (CLUSTER), Crete: Heraklion, 188-196. 
 
 
 
99年度專題研究計畫研究成果彙整表 
計畫主持人：江茂綸 計畫編號：99-2221-E-324-041-MY3 
計畫名稱：雲端運算環境下最佳協議與損毀診斷之探討 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 8 3 80% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 20%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 5 2 80% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
