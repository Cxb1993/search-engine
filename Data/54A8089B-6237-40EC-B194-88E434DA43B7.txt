Structure Theory（RST）理論，考慮語句間的從屬關係，建
立階層式韻律訊息模型，使合成語音可以更自然流利。 
中文關鍵詞： 數位有聲書播放機，語音合成，嵌入式系統 
英 文 摘 要 ： Digital Accessible Information System, or DAISY, is a 
means of creating digital talking books for people 
who wish to hear—and navigate—written material 
presented in an audible format； many such listeners 
have ＇print disabilities,＇ including blindness, 
impaired vision, dyslexia or other issues. However, 
DAISY players are usually too expensive in Taiwan and 
don｀t support Chinese text-to-speech function very 
well.  
 
Therefore, in this project, we had developed a 
portable Chinese DAISY player prototype. The result 
is a small, low-cost device with long battery life. 
Moreover, a mixed Chinese and English audiobook 
corpus had been collected and two DAISY audiobooks 
had been published. Finally, the audiobook corpus was 
used to establish a high-quality mixed Chinese and 
English text-to-speech system for DAISY audiobook 
application and will be used to further upgrade our 
embedded device. 
英文關鍵詞： DAISY Player, text-to-speech. audiobook, embedded 
system 
 
  需邀請工業工程設計老師，學生與視障同胞直接參與設計，以確保最終成品
具有符合視障同胞需要之人性化設計。 
 需以低功耗之微控制器(MCU)為主體，並自行設計電路板盡量減少 IC 數量，
成本與消耗電量。且直接以硬體語言開發專屬之電子書撥放軟體，以避免使
用較龐大的通用作業系統（如 android，通常較佔資源與耗電）。 
 需支援可抽換之記憶卡，以方便視障同胞擴充藏書與互相交換流通。 
 
3.1.2 DAISY player 執行成果 
 
執行成果主要包含（1）人因工程實驗，（2）硬體設計與實作，（3）嵌入式 TTS
系統實現與（4）嵌入式人機介面實作。 
 
3.1.2.1 人因工程實驗 
 
我們使用 GOMS 方法以下圖之流程，進行人因實驗，制定出一適合視障同胞使用
之使用者界面與硬體規格基準，供工程人員遵循。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    圖 人因工程研究流程圖 
定義操作任務及操作流程 
GOMS 分析評估現有之 DAISY 播放機 
預試 
根據GOMS評估結果定義Prototype 操作流程 
實驗設計 
實驗結果分析 
正式實驗 
內部電路部分: 
 
 
 
3.1.2.3 嵌入式 TTS 系統實現 
 
在 DAISY Player 所必須之語音合成與 MP3 播放功能，已在自行開發的 microchip 
pic32 板上開發完成。其 TTS 架構如下 
 
TTS_Init()
初始化
TTS_Play()
讀取文字
Word1~Word5
中文詞庫
cut_sen()
剖詞與標詞類
get_weight()
詞料庫搜尋
Rule()
規則構詞
TextNormalized()
文字正規化
Fea_Ext()
特徵參數轉換
Synth()
韻律產生器
synthesis()
PSOLA語音合成器
WM8731CodecWrite(
)音頻編碼器播放
Psola_ulaw.8k
411音節語料庫
Ulaw2PCM()
G.711解碼轉換
文句分析
韻律訊息
產生器
頻譜訊息
產生器
語音合成器
 
3.1.2.4 嵌入式人機介面實現 
 
主控之 DAISY 電子書閱讀控制軟體，則已完成以硬體插斷驅動下圖之有限狀態
機方式，實現到我們自行設計的 microchip pic32 板上。 
 
 
圖 軟體設計流程圖 
  
 
 
 
 
連接 
文字檔 MP3 
無裝置連接 
決定取
樣頻率  
8K 
檔案
開啟 
開
啟
失
敗 
TTS 
初始化 檔案
讀取 
開啟成功 
取得下
個 MP3 
取得下
段字 
決定取
樣頻率  
檔案
關閉 
開啟下
一個檔 
開啟下
一個檔 
錯誤或結尾 
Mp3解碼 
錯誤或結尾 
44.1K 
檔案
開啟 
開
啟
失
敗 
開啟成功 
檔案
讀取 
播放 
裝置
連接 
閒置 
選擇 mp3 
或文字檔 
連接失敗 
系統初始化 
 3.1.3 DAISY player 實現結論 
 
總之，經過軟硬體設計與實現，我們已完成計畫目標，製作出可實際運作的 DAISY 
player，將來將以實際機器，邀請視障朋友試用，進行第二次人因工程實驗，然
後再進行第二次修正。 
 
3.2 中英夾雜有聲書語音語料庫錄製 
 
為製作出一個適用於 Daisy 數位有聲書的中英夾雜語音合成系統，我們首先針對
有聲書設計並錄製一雙語語料庫。為符合 DAISY 有聲書的需求，我們在設計語
料時，先跟有聲書學會討論錄音內容與方式。最後達成協議，由有聲書學會挑選
他們想要錄製的兩本書籍，並獲得作者授權，再由有聲書學會挑選合適的錄音員
（一男一女），在他們的錄音室中進行兩本電子書的錄製。結果就是有聲書學會
可以增加發行兩本有聲書，我們也可獲得有授權的有聲書語料。以下為所錄製的
兩本有聲書之文本，與有聲書學會轉成 DAISY 格式後發行的 DAISY 有聲書，皆
已可在市面上購買。 
 
 
3.3 中英夾雜有聲書語音合成系統 
 
此部分主要是針對未來中英夾雜有聲書需求，進行長篇語句中英夾雜語音合成系
統研究，為將來升級 DAISY player 的合成系統做準備。研究內容包括（1）中英
夾雜語音語料庫音素系統（2）超語句文脈訊息標記與（3）中英夾雜語音合成
器開發。此外我們並對所完成的系統，做主觀效能評估實驗，以下逐一說明執行
成果 
 
3.3.1 長篇語句中英夾雜語音語音合成器 
 
3.3.1.1 中英混和語言 X-SAMPA 音素系統 
 
為製作中英夾雜語音合成系統，我們先用 Extended SAM Phonetic 
Alphabet(X-SAMPA)來統一中英雙語音素集(Phone Set)。這部分主要是參考 IPA，
英文 SAMPA 與中文 SAMPA 後，整理出來的結果。 
 
下表為母音部分： 
 
Vowels  
X- 
SAMPA 
IPA for  
Mandarin 
IPA for  
English 
X- 
SAMPA 
IPA for  
Mandarin 
IPA for  
English 
A: a ɑː A:+N a+ŋ  
O: ɔ ɔː ax+N ə+ŋ  
@ ə ə @` ɚ ər 
E ɛ ɛ i: i iː 
aI aɪ aɪ u: u uː 
eI eɪ eɪ y y  
aU ɑʊ aʊ {  æ  
oU oʊ c I  ɪ 
A:+n a+n  OI  ɔɪ 
ax+n ə+n  U  ʊ 
 
中英雙語母音音素集 
 
 Utterance 
the semantic topics of current and surrounding utterance 
the PMs of current and surrounding phrase 
the language of current and surrounding utterance is 
chinese, english or mixed 
Paragraph 
the number and forward and backward position of a 
utterance in a paragraph 
 
句子以上層次文脈相關 
 
3.3.1.3 中英夾雜有聲書語音合成器系統 
 
在文字剖析方面，我們先採用 Stanford 的中文與英文斷詞器與文法剖析器，更
進一步使用語意分析器，分析各句的語意與語句間的語意轉折，最後以以下圖之
系統架構做中英夾雜語音合成。 
 
 
 
表   測試語料評估設定 
 
最後的評估結果如下表所示： 
 
 
表  加入語意文脈之評分結果 
 
由整體評估結果看來，我們的系統還不錯。整體相似度平均為 3.23，表示該合
成結果的聲音跟原始語者相比，其聲音相似性處於中間；而整體自然度為 3.19，
評估者認為其自然度還可以。中英轉換的評估分數為 3.03，數據顯示我們中英
轉換的地方也還好，最後是文章閱讀的感覺，大部分的人覺得還可以接受此合成
音檔。 
 
另外在相似度的部分，評估者認為中英夾雜的合成音檔其相似度比較高，而純英
文的合成音檔較低。在自然度的部分，也是同樣的情況。細分自然度評估來看，
中文自然度評估為 3.47 分、英文自然度評估為 2.93 分、中英夾雜自然度評估為
3.17 分。由於我們所蒐集的訓練語料，純中文比例佔大部分，所以在相似度及
自然度的評估上，中文的分數都會略高於英文一些。 
 
 
2012 
 
6 其他成果 
 
 DAISY player 軟硬體成品（包含硬體線路佈局說明書）。 
 由有聲書學會發行之兩本 DAISY 有聲書，包括 
 
 
 一男一女中英夾雜語料庫，每人約有十小時音檔。將由計算語言學會發行 
 中英夾雜有聲書語音合成系統 
 
  
 
2 
一、參加會議經過 
InerSpeech 是語音研究之重要國際會議，而 BlizzardChallenge 2011 則是語音合成
最重要之評比會議（為 InterSpeeh 之衛星會議）。因此此次雖然 Blizzard Challenge
部分，依現行規定並無法補助出國經費，但因機會難得，此次仍先參加 InterSpeech 
2011，再自費參加 Blizzard Challenge 會議。行程則事先搭機從 Florence入境義
大利參加 InterSpeech 2011，再搭車北上 Turin（三小時車程），參加 Blizzard 
Challenge 2011，會議結束後直接從 Turin搭機回國。 
 
二、與會心得 
 
InterSpeech 2011部分； 
此次發表三篇論文，兩篇為 oral presentation，一篇為 poster，其中我負責上台
報告“Minimum Classification Error Based Spectro-Temporal Feature 
Extraction For Robust Audio Event Classification”，工研院廖憲政口頭報告
“Maximum Confidence Measure Based Interaural Phase Difference Estimation 
for Noise Masking in Dual-Microphone Robust Speech Recognition”，交大江
振宇以海報報告“A New Model-based Mandarin-speech Coding System”。 
 
此次會議投稿總數為 1439，率取 846篇，錄取率則只有 59%，見下圖： 
 
  
 
4 
 
 
 
 
參加者則總共有 1305 人參加，還是以歐美研究單位居多： 
  
 
6 
此外，亦有人開發一套完整的開源韻律訊息處理工具，包含可下載之原始碼與範例程
式，應善加利用： 
 
 
在論文發表方面，則以 Hynek Hermansky 的 multi-stream 語音特徵參數，最為令人
印象深刻，主要是開發新的 temporal-frequency 語音參數，與大腦聽神經運作有相
當關連： 
 
 
  
 
8 
 Institute for Infocomm Research (I2R), Singapore  
 Institute for Language and Speech Processing / Research Center "Athena" 
 Lessac Technologies, Inc., USA 
 Nagoya Institute of Technology 
 iFLYTEK Speech Lab, 
 University College Dublin, Ireland 
 Politehnica University of Bucharest, Bucharest, Romania 
 University of the Basque Country, Bilbao, Spain 
 TU Berlin 
 
從評比結果來看（如下圖所示），主流合成技術則已經變成使用 hybrid unit 
selection 與 HTS方式，幾乎評比成績較好的單位皆是使用 hybrid 方法。 
 
 
  
 
10 
 
四、建議 
 
建議即使在主會議的衛星會議中並無論文發表。仍考慮放寬規定，合理補助參加衛
星會議，因為機票已由參加主會議部分吸收掉了，所以應可藉此機會一石兩鳥。 
 
五、攜回資料名稱及內容 
 
包括 InterSpeech2011 會議論文集與 Blizzard Challenge 評比結果（報告與成績）。 
 
六﹑論文發表 
 
 
to1t wo  2t wo   1t wo   t wo 
tx
ty
1tx  X
Y
O
1ty 
 
Fig. 1: A general block diagrams of the spectro-temporal 
feature extraction framework for audio classification. 
 
3. The Proposed MCE-Based Approach 
 
Fig. 2 shows a block diagram of the proposed MCE-based 
framework.  As shown in Fig. 2, the MCE criterion could be 
used to optimize both (1) the frontend feature extraction matrix 
and (2) the backend hidden Markov models (HMMs). 
However, in the following subsections, only optimization 
of the feature extraction matrix, E , will be discussed. The 
detail MCE-based algorithm for adjusting HMMs could be 
found in [10]. 
 
 
Fig. 2: Block diagram of the proposed MCE-based feature 
extraction framework for robust audio classification. 
 
3.1. MCE criterion 
 
Given an input test feature vector sequence Y , a score function 
( ; )kg Y   is first defined for each audio class ( K  classes in 
total) using the log-likelihood functions ( ; )kP Y   of their 
corresponding HMMs  , i.e., 
 
 ( ; ) log ( ; ) , 1 ~k kg Y P Y k K      (2) 
 
Then, the decision rule of the recognizer is to choose the one 
with maximum score: 
ˆ argmax ( ; )kkk g Y     (3) 
 
Next, if the input test sequence in fact belongs to the k –th 
class, a miss-classification function  d Y  is defined for the 
decision making: 
 
     
1/1; exp( ; )
1k ll k
d Y g Y g Y
K



          (4) 
 
where ( ; )lg Y   are the scores of competitive candidates other 
than k  and   is a constant to control the non-linearity of the 
miss-classification function . 
If only the most competitive candidate, i.e., the *k -th 
class, is considered,   is usually set to  , and the miss-
classification function  could be further simplified into Eq. (5). 
 
     *; ;k kd Y g Y g Y       (5) 
 
Finally, a loss function  ( )L d Y is defined using a smooth 
“zero-one” Sigmod function: 
 
  -( ( )+ )1( ) =1 d YL d Y e      (6) 
 
where   and   are two constants that control the slope and 
bias of the Sigmod function. 
It is worth noting that the summation of the loss functions 
over a set of test data approximates the empirical error counts 
of the test set. 
 
3.2. GPD-based extraction matrix optimization 
 
To properly adjust each element ,i je  of the feature extraction 
matrix E , an iterative training algorithm using generalized 
probabilistic descent (GPD) [10] is applied here, i.e., 
 
, ,
,
( ( ))( 1) ( )i j i j n
i j
L d Ye n e n
e
        (7) 
 
where n  is a constant that control the learning step in the n -th 
iteration. 
The last term of Eq. (7) could be further derived as 
follows: 
 
, ,
( ( )) ( )( ( )) (1 ( ( )))
i j i j
L d Y d YL d Y L d Y
e e
      , (8) 
 
where 
 
*
, , ,
( ; )( ) ( ; )k k
i j i j i j
g Yd Y g Y
e e e
           (9) 
 
If partition Gaussian mixture is used in the HMMs,  , 
the score function ( ; )kg Y   in Eq. (9) could be simplified as 
follows: 
 1 , , ,
0
( ; ) log ( ; , )
t t t t t t
L
k k k
k q m t q m q m
t
g Y N y     

   (10) 
that LDA features are more sensitive to SNR and, especially, 
channel mismatch. Fig. 5 (a) also shows that Cabor+PCA 
features are better than MFCC+MVA in high SNR conditions 
but it degrades more quickly in low SNR cases. 
Therefore, the results in Fig. 4 and 5 confirm the 
superiority and robustness of the proposed MCE-based spectro-
temporal feature extraction approach in noisy environment. 
 
 
 
Fig. 4: Performance comparison of 5 different features on the 
noisy RWCP non-speech database. 
 
 
(a) 
 
(b) 
 
Fig. 5: Average recognition error rates (%) of 5 features over 
different (a) SNRs and (b) test sets on the noisy RWCP non-
speech database. 
 
4.4. Analysis 
 
Fig. 6 shows the first 10 spectro-temporal filters found by the 
proposed MCE-based approach. It can be seen from Fig. 6 that 
some filters are similar to conventional Gabor filters or edge 
detectors. But others (the last three ones in the right-bottom 
panels) indeed have more complex spectro-temporal patterns. 
This may explain why the proposed new features are superior 
to Gabor+PCA features. In other words, the proposed method 
could capture more complex spectro-temporal cues. 
 
 
Fig. 6: The first 10 spectro-temporal filters found by the 
proposed MCE-based approach. 
 
5. Conclusion 
 
In this paper, a MCE-based approach has been proposed to 
extract new spectro-temporal features as alternatives to MFCCs 
in order to meet the goal of audio classification. Experimental 
results on noisy RWCP non-speech database show that the 
proposed MCE-based features are superior to several state-of-
the-art features. Besides, experiment analysis shows the 
proposed method could capture more complex spectro-
temporal cues. We thus confirm the robustness of the proposed 
MCE-based spectro-temporal feature extraction method. 
 
6. Acknowledgment 
 
This paper is partially supported by National Science Council, 
Taiwan with project 97-2628-E-027-003-MY3 and 98-2221-E-
027-081-MY3. 
 
7. References 
 
[1] Chia-Ping Chen and J. Bilmes, “MVA Processing of Speech Features,” 
IEEE Transactions on Audio, Speech, and Language Processing, vol. 
15, no. 1, pp. 257-270, January 2007. 
[2] Yuan-Fu Liao, Hung-Hsiang, Fang and Chi-Hui Hsu, “Eigen-MLLR 
environment/speaker compensation for robust speech recognition,” in 
Proc. InterSpeech, 2008. 
[3] Hermansky, H., Ellis, D. and Sharma, S., “Tandem connectionist 
feature extraction for conventional HMM system,” in Proc. ICASSP, 
2000. 
[4] Depireux, D.A., Simon, J.Z., Klein, D.J. and Shamma, S.A., “Spectro-
temporal response field characterization with dynamic ripples in ferret 
primary auditory cortex,” J. Neurophysiology, vol. 85, pp. 1220–1234, 
2001. 
[5] Zhao, S., Ravuri, S. and Morgan, N., “Multi-Stream to Many-Stream: 
Using Spectro-Temporal Features for ASR,” in Proc. ICASSP, 2009. 
[6] Michael Kleinschmidt, “Robust speech recognition based on    
spectro-temporal processing,” PhD thesis, 2002, Universitaet 
Oldenburg. 
[7] K. Schutte and J. Glass, “Speech Recognition with Localized Time-
Frequency Pattern Detectors,” in Proc. ASRU, Kyoto, Japan, 
December 2007. 
[8] Mesgarani, N.,Slaney, M. and Shamma, S., “Discrimination of speech 
from non-speech based on multiscale spectro-temporal modulations,” 
IEEE Transactions on Audio, Speech and Language Processing, 
Volume 14, Issue 6, p.920–930., 2006. 
[9] Ruvolo, P., et al. A learning approach to hierarchical feature selection 
and aggregation for audio classification. Pattern Recognition Letter, 
2010 
[10] B. H. Juang, W. Chou, and C. H. Lee, “Minimum Classification Error 
Rate Methods for Speech Recognition,” IEEE Trans. on Speech and 
Audio Processing. Volume 5, No. 3, May 1997. 
[11] S. Nakamura, K. Hiyane, F. Asano, T. Nishiura, and T. Yamada, 
“Acoustical Sound Scene Database in Real Environments for Sound 
Scene Understanding and Hands-Free Speech Recognition,” in Proc. 
Int. Conf. Lang. Resources Evaluation, 2000. 
[12] H.G. Hirsch, D. Pearce, “The Aurora Experimental Framework for the 
Performance Evaluation of Speech Recognition Systems under Noisy 
Conditions,” in Proc. 6th International Conference on Spoken 
Language Processing - ICSLP'00, 2000. 
[13] H.G. Hirsch, “FaNT - Filtering and Noise Adding Tool,” 
http://dnt.kr.hs-niederrhein.de/download/fant.tar.gz , 2010. 
0
1
2
3
4
5
6
MFCC MFCC+MVA Gabor+PCA LDA MCE
Average Recognition Error Rate (%)
0
5
10
15
20
20 15 10 5 0 ‐5
MFCC
MFCC+MVA
Gabor+PCA
LDA
MCE
0
1
2
3
4
5
6
SetA SetB SetC
MFCC
MFCC+MVA
Gabor+PCA
LDA
MCE
Fourier transform (STFT). Then the spectrals of the left and 
right signals can be represented as 
( , )
0 0
( , ) ( , ),    ( , ) ( , )k i
I I
j d t k
L i R i
i i
X t k X t k X t k e X t k
 
    (2) 
where Xi(t,k) represents the STFT of xi[n] at frame t and 
frequency bin k, and di(t,k) indicates the frequency-dependent 
ITD of the i-th source. 
Assume that each time-frequency bin is dominated by a 
single source i*, the two STFTs on a specific time-frequency 
bin (t0,k0) could be further simplified as: 
0 00 ( , )
0 0 0 0 0 0 0 0( , ) ( , ),    ( , ) ( , )
kj d t k
L Ri i
X t k X t k X t k e X t k 
   (3) 
Accordingly, the ITD for a particular time-frequency bin (t0,k0) 
is determined by its corresponding IPD value: 
0
0 0 0 0 0 0
1( , ) min ( , ) ( , ) 2R Lr
k
d t k X t k X t k r     (4) 
By comparing the ITD of each time-frequency bin with a 
predefine ITD threshold τ, a binary noise mask μ(τ,t,k) in the 
frequency domain can be derived:  
 
 
       otherwise   ,
),( if ,1
),,( 
 ktdkt  (5) 
where ε is a floor constant with a value of 0.01. 
By applying the mask μ(τ,t,k) to the averaged spectral  of 
the two signals, only the time-frequency bins which have 
small ITDs are considered to belong to the target and therefore 
an enhanced spectra ),,(~ ktX   can be obtained as follows: 
  ( , , ) ( , , ) ( , ) ( , ) / 2L RX t k t k X t k X t k     (6)  
It is worth noting that the quality of the noise mask, μ(τ,t,k), 
and therefore the output speech are highly dependent on the 
threshold τ. More details can be referred to [7]. 
3. The Proposed MCM-Based Method 
Generally speaking, if prior knowledge of the speech and 
interference noises is available, one can build a speech and an 
interference noise model to exam how good a threshold is 
capable to separate the two signals. However, it is often 
difficult, if not impossible, to obtain such information, 
especially for the noise model. 
To alleviate this issue, a speech recognizer purely trained 
from clean speech data (which is usually available) is used to 
compute CMs in order to transform the problem into a 
verification-based ITD threshold estimation task. The CM 
scores are defined as a function of the threshold τ: 
    SP F( )  log ( ) | log ( ) |  CM P P       C C   (7) 
where )(~ C  is the filtered mel-frequency cepstral coefficient 
(MFCC) feature vector obtained from )(~ X  , ΛSP and ΛF 
represent the speech and filler models of the speech recognizer, 
respectively. 
By this way, if a suitable threshold τ is chosen, most of 
the noise interferences will be removed from a test utterance 
and the recognizer will report a high CM score. On the other 
hand, a low CM score will be observed, if an improper τ is 
adopted. Therefore, the estimation of best threshold, τCM, can 
be formulated as the following MCM optimization problem: 
    CM SP Farg max  log ( ) | log ( ) |  P P       C C   (8) 
3.1. Preliminary Experiments on the Relationship 
between CM and Recognition Accuracy 
To verify the feasibility of the proposed approach, a 
preliminary voice command experiment was first executed. 
The direction of the target speaker is positioned at a 90-degree 
angle, input SNR is 0dB and the testing utterances were 
interfered by a babble noise placed at 30 or 60 degrees (details 
will be described in the next section). 
Fig. 1 shows the averaged recognition rates and the CM 
scores as a function of threshold τ and interference noise 
directions. From the results, a strong correlation between 
recognition accuracy and CM scores could be observed. 
Especially, the highest recognition accuracy and  CM scores 
for 30 (or 60) degree case all happen in the same τ value. This 
confirms the feasibility of the proposed approach. 
 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1
0
50
100

ac
cu
ra
cy
 (%
)
 
 
30 degrees
60 degrees
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1
-1
-0.5
0
0.5
1
C
M
 s
co
re
 
 
30 degrees
60 degrees
 
 
Figure 1: The correlation between recognition accuracy and 
CM score as a function of τ observed on a set of filtered test 
utterances interfered by a babble noise placed at 30 or 60 
degree and 0dB SNR: (a) recognition accuracy and (b) CM 
score. 
3.2. MCM-Based EM Algorithm 
To automatically find an optimal τ, an EM algorithm is 
developed in this section. In the E step, we formulate a Q 
function as follows: 
      
   
   
( ) ( )
SP F SP F
( )
SP SP
( )
F F
| log ( ) | log ( ) | , , ,
log ( ), , | , | , ,
   log ( ), , | , | , ,
k k
k
t t
t s m
k
t t
t s m
Q E P P
p s m p s m
p s m p s m
    
 
 
        
  
  


C C C
c c
c c
 


 (9) 
here C is the noisy MFCC feature set, ct is a feature vector in 
C, )(~ tc  is a feature vector in )(~ C , t is a frame index, s and m 
represent the state and mixture component, respectively. In the 
M step, the value of τ that maximizes Q(τ|τ(k)) is chosen:  (k)k τQ 

 maxarg)1(                    (10) 
Taking the underline speech recognizer into 
consideration, the auxiliary function can be further simplified 
as: 
(a) 
(b) 
30 degree cases.  This may come from the fact that the setting 
of the initial value of the threshold, since 0.2 is a good guess 
for the 60 degree case but not for the 30 degree (it should be 
around 0.5). This may also explain why MCM performed 
better in 60 than in 30 degree case as shown in Table 1. 
4.3.2. Relationship between SNR and Threshold 
The means of the estimated τ at 4 different SNRs with noise 
comes from 60 or 30 degree are shown in Fig. 3. It is 
interesting to find there is a strong and linear relationship 
between τ ,SNRs and noise direction. In other words, a rough 
estimation of SNR may help to guess a proper initial value of τ 
and to speed up the whole training procedure. On the other 
hand, the estimated τ could also be used to guess the SNR 
conditions and the directions of interference noises. 
 
Table 1. Performance comparison of three different 
methods for (a) babble noise located at 30 degree and (b) 
babble noise located at 60 degree. 
30 degree baseline MCC MCM 
0dB 8.96  74.59  89.76  
6dB 61.79  83.55  93.60  
12dB 90.31  88.12  96.34  
18dB 95.80  93.05  95.98  
(a) 
60 degree baseline MCC MCM 
0dB 20.29  38.57  92.32  
6dB 71.85  76.23  95.80  
12dB 92.14  89.76  95.61  
18dB 96.53  93.97  97.26  
(b) 
 
2 4 6 8 10 12 14
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
Number of iterations
C
M
 s
co
re
 
 
30 degrees
60 degrees
 
Figure 2: Two typical convergence curves while the 
interference noises are located at 30 or 60 degree, respectively. 
 
0 6 12 18
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SNR (dB)

 
 
30 degrees
60 degrees
 
Figure 3: The mean of the estimated τ versus different SNRs 
for two different noise directions. 
5. Conclusion 
A new one-stage maximum confidence measure (MCM)-based 
interaural phase difference estimation framework for robust 
speech recognition have been shown to be able to significantly 
improves the speech recognition rates and degrade more 
gracefully in worst conditions. It is also interesting to find 
there is a strong linear relationship between the best τ and 
SNRs and may help to speed up the whole training procedure. 
All experimental findings confirm the feasibility of the 
proposed MCM approach. 
 
6. Acknowledgment 
This paper is a partial result of Project A353C41221 
conducted by ITRI under sponsorship of the Ministry of 
Economic Affairs, Taiwan, R.O.C. 
7. References 
[1] Park, H. and Stern, R., “Spatial separation of speech signals 
using amplitude estimation based on interaural comparisons 
of zero-crossings”, Speech Communication, 51:15-25, 2009. 
[2] Cobos, M. and Lopez, J.J., “Two-microphone separation of 
speech mixtures based on interclass variance maximization. J. 
Acoust”, Soc. Am., 127:1661-1672, 2010. 
[3] Harding, S., Barker, J. and Brown, G., “Mask estimation for 
missing data speech recognition based on statistics of 
binaural interaction”, IEEE Trans. Audio Speech Lang. 
Process., 14:58-67, 2006. 
[4] Srinivasan, S., Roman, N. and Wang, D., “Binary and ratio 
time-frequency masks for robust speech recognition”, 
Speech Communication, 48:1486-1501, 2006. 
[5] Woodworth, R. S., “Experimental Psychology”, New York: 
Holt, Rinehart, Winston, 1938. 
[6] Feddersen, W.E., Sandel, T.T., Teas, D.C., and Jeffress, L.A., 
“Localization of high frequency tones”, Journal of the 
Acoustical Society of America, 5:82-108, 1957. 
[7] Kim, C., Kumar, K., Raj, B. and Stern, R.M., “Signal 
separation for robust speech recognition based on phase 
difference information obtained in the frequency domain”, In 
INTERSPEECH-2009, pp. 2495-2498, 2009. 
[8] Kim, C., Stern, R.M., Eom, K. and Lee, J., “Automatic 
selection of thresholds for signal separation algorithms based 
on interaural delay”, In INTERSPEECH-2010, pp. 729-732, 
2010. 
[9] Shi, G., Aarabi, P. and Jiang, H., “Phase-Based Dual-
Microphone Speech Enhancement Using A Prior Speech 
Model”, IEEE Trans. Audio Speech Lang. Process., 15:109-
118, 2007. 
[10] Sukkar, R.A. and Lee, C.H., “Vocabulary independent 
discriminative utterance verification for nonkeyword 
rejection in subword based speech recognition”, IEEE Trans. 
Speech Audio Process., 4:420-429, 1996. 
[11] Chien J.T. and Liao C.P., “Maximum Confidence Hidden 
Markov Modeling for Face Recognition”, IEEE Trans. 
Pattern Anal. Mach. Intell. 30:606-616, 2008. 
[12] Wang, H.C., Seide, F., Tseng, C.Y., and Lee, L.S., “MAT-
2000 - design, collection, and validation of a Mandarin 2000-
speaker telephone speech database”, In ICSLP-2000, 4:460-
463, 2000. 
[13] Young, S., Kershaw, D., Odell, J., Ollason, D., Valtchev, V. 
and Woodland, P., “The HTK Book Version 3.0”, 
Cambridge University Press, 2000. 
normalization factor (UPNF) encoder is required for encoding 
these prosody normalization factors. By using the HMM-state 
segmentation information, we can extract state-based spectral 
features and encode them by vector quantization (VQ). 
In the decoder, we first use the decoded LP features to 
reconstruct the four prosodic-acoustic features by HPM whose 
parameters are sent to the decoder in advance as side 
information. We then use base-syllable type and syllable 
duration to predict state durations by a state duration model. 
Lastly, by using the decoded state spectral features, the 
reconstructed prosodic-acoustic features, and the predicted 
state durations, an HMM-based speech synthesizer generates 
the output speech. 
In the following subsections, we discuss the encoder and 
the decoder in more detail. 
2.1. The Speech Encoder 
As shown in Fig. 1, the speech encoder is composed of four 
parts including a PE-ASR [6,7], an LP encoder, a UPNF 
encoder, and a spectrum encoder. The PE-ASR system is a 
sophisticated speech recognizer developed previously [6,7]. 
Fig. 2 displays its functional block diagram. It is a two-stage 
system to firstly use an AM and a bigram LM to generate a 
word lattice in the first stage decoding, and to then use an 
FLM [8] and an HPM [9] to finely decode from the word 
lattice the best linguistic sequences (i.e. base-syllable, tone, 
word, POS and PM) and their corresponding segmentation 
information, as well as prosody tag sequences (i.e. prosodic 
states and break types) that represent a hierarchical prosody 
structure of the input utterance. The AM is a syllable-based 
HMM model. It models each of 411 base-syllables as an 8-
state left-to-right HMM. The FLM is an extension of the 
conventional trigram model to additionally consider POS and 
PM aside from word. The HPM consists of various prosodic 
sub-models to describe the relationship of prosodic tags, 
prosodic-acoustic features, and linguistic features. 
rescoring
FLM
Viterbi 
search
Prosodic-
acoustic 
features 
extraction
Second stage
AM
Word 
bigram LM
Syllable
prosodic-
acoustic 
model
Syllable-
juncture 
prosodic-
acoustic model
Frame-based 
features 
extraction
First stage
F0 & energy sequences
Syllable
Segmentation
information
Input speech
Prosodic 
state model
Break-
syntax 
model
Word lattice
Utterance normalization 
factors
HPM
Words, POSs, PMs, 
base-syllables, tones, 
, break types, 
prosodic states
 
Fig. 2: The prosody-enriched ASR system [7] 
Four sub-models of the HPM are involved in the coding 
process. They include three syllable prosodic-acoustic models, 
which are used to describe the variations of syllable pitch 
contour, duration and energy level, and one prosodic-acoustic 
model which describes the variation of syllable-juncture pause 
duration influenced by some linguistic features. For syllable 
pitch contour, it is formulated as an additive model: 
1
1 1, ,
n n
n n n n n n
r f b
n n t p spB t B t
sp sp     
 
        (1) 
where 
nsp  is a vector of four orthogonally-transformed 
parameters representing the observed log-F0 contour of 
syllable n [10]; rnsp  is the residual of modeling nsp ; nt  and 
np
  are the affecting patterns (APs) for tone 
nt  and prosodic 
state tag np , respectively; 
1 1,
n
n n
f
B t

 
 and 1, nn n
b
B t
   are the forward 
and backward coarticulation APs contributed from syllable 
1n   and syllable 1n  , respectively; and sp  is the global 
mean of pitch vector. Here, 
nB  is the break tag after syllable n. 
Similarly, syllable duration and energy level are modeled as 
n n n
r
n n t s q sdsd sd            (2) 
n n n
r
n n t f r sese se            (3) 
where 
nt
 /
nt
 , 
ns
 , 
nf
  and 
nq
 /
nr
  are the APs of tone 
nt , 
base-syllable 
ns , final type nf , and prosodic state tags nq / nr ; 
and 
sd  and se  are global means. To reconstruct these three 
prosodic-acoustic features using the three sub-models in the 
decoder, we need to encode and transmit low-level linguistic 
features of tone, base-syllable and final types as well as 
prosodic features of break type and prosodic state tags. 
Besides, all affecting patterns are sent as side information. It is 
noted that we neglect the coding of the residuals because they 
all have small variances. 
The fourth sub-model describes the variation of inter-
syllable pause duration by break-dependent decision trees 
(BDTs). For each break type, a decision tree is used to 
determine the pdf of pause duration according to linguistic 
features. For reconstructing the pause duration, we needs to 
send the information of the break tag and the residing leaf 
node of the associated decision tree for each inter-syllable 
juncture to the decoder. All pdfs of leaf nodes in these seven 
decision trees are also sent to the decoder as side information. 
Table 1 shows the bit assignment of the encodings of these 
low-level linguistic features of tone, base-syllable and final 
types, prosodic tags of prosodic state and break type, and leaf 
nodes of BDTs. Notice that the BDT is constructed for each 
break type, and each BDT has different number of leaf nodes. 
Therefore, the bit length is variable for each given known 
break type. 
 Table 1: Bit assignment for encoding linguistic features and 
prosody tags. 
Symbol # of symbol bit 
Lexical tone tn 5 3 
Base-syllable type  sn 411 9 
Pitch prosodic state  pn 
 16 4 
Duration prosodic state qn 
 16 4 
Energy prosodic state rn 16 4 
Break type Bn 
 7 3 
BDT leaf node index Tn  
for 
 
B0, B1, B2-1, B2-2, B2-3, B3, B4
 5/7/3/2/4/3/1 3/3/2/1/2/2/0 
Total bits per syllable (maximum) 30 
 
For avoiding taking care of the speaker/utterance 
variability of prosodic-acoustic features in HPM, they are pre-
normalized. For syllable pitch contour, a scheme of frame-
based F0 value normalized by speaker-level (training phase) or 
utterance-level (test phase) mean and variance is adopted; 
while for both syllable duration and syllable energy level, they 
are simply normalized by their corresponding speaker-
/utterance-level means and variances. These normalization 
factors are needed to be encoded and sent to the decoder. In 
this study, they are scalar-quantized independently by the 
UPNF encoder. Their codebooks are also sent to the decoder 
as side information. 
Since we want to use the HMM-based speech synthesizer 
in the decoder to generate the output speech, we extract 25-
dimensional mel-generalized cepstral (MGC) [11] vector 
including the zero-th coefficient for each 25ms frame with 
5ms shift. Blackman window is used in the feature extraction. 
Besides, delta and delta-delta MGCs are also extracted. In the 
training phase, we calculate the pdf parameters (i.e., mean and 
low. Fig.3 shows an example of the reconstructed prosodic 
features of an utterance of the outside test. As shown in the 
figure, most reconstructed prosodic features were close to their 
reference values. 
Table 4: The RMSE of the reconstructed prosodic features 
 F0 (Hz) Syllable 
duration (ms) 
Syllable energy 
level (dB) 
Pause duration 
(ms) 
Inside test  11.4 18.4 0.52 73.8 
Outside test 14.7 16.8 0.20 75.6 
Table 5: The RMSE (ms) performance of the reconstructed 
pause duration with respect to different break types. 
 B0 B1 B2-1 B2-2 B2-3 B3 B4 
Inside 19.3 26.5 75.6 149.2 35.0 177.9 312.9 
Outside 12.4 17.1 88.3 178.4 39.6 176.9 292.7 
 
Table 6: Bit rates for inside and outside tests 
 Average Max Min 
Inside    prosody 104.56 163.79 42.23 
spectral 423.73 661.90 178.07 
outside   prosody 107.55 147.20 78.00 
spectral 435.06 594.44 318.05 
 
 
Fig. 3: An example of the reconstructed prosodic features of 
an utterance. From top to bottom: syllable pitch mean, syllable 
duration, syllable energy level, and pause duration. (open 
circle: reference, dot: recognition result, solid line: deletion, 
dash dot line: insertion) 
 
Lastly, an informal listening test was performed. Generally, 
all reconstructed speeches sounded good. The effects of 
recognition errors were not serious. Most substitution, deletion, 
and insertion errors were slightly perceptible. This mainly 
resulted from encoding and sending the spectral features to the 
decoder.  
4. Discussions and Conclusions 
A model-based Mandarin-speech coding system has been 
discussed in this paper. It differs from the conventional speech 
coding system on using a prosody-enriched ASR in the 
encoder to extract high-level linguistic and prosodic features 
to assist in improving the coding efficiency. Experimental 
results showed that high-quality reconstructed speech can be 
obtained at a low data rate of 543 bits/s. 
Another advantage of the proposed coding system can be 
found. By properly adjusting the prosodic features, we may 
modify the prosody of the reconstructed speech, e.g. changing 
the speech rate. 
The proposed coding system can also operate on another 
two modes. One is the case of knowing both the speech signal 
and the associated text. This case has been examined as the 
inside test discussed in Section 3. An application of the mode 
is the speech coding of story readings in an electronic book. 
Prosody modification will be the most attractive feature of the 
application. The other mode is the case of low-rate speech 
coding without transmitting the spectral parameters. A text-to-
speech system, such as the HTS [16] can be used to generate 
spectral parameters of a standard voice for their substitutions 
by using the recognized text sent from the encoder. In this case, 
we can keep the prosody of the input speech but losing the 
speaker identity. 
5. Acknowledgements 
The work was supported by the NSC, Taiwan, under the 
project with contracts  NSC 99-2221-E-009-009-MY3 and . 
98-2221-E-009-075-MY3. The authors would like to thank the 
ACLCLP for providing the TCC300 Corpus, NTCIR, and 
Sinca Corpus. 
6. References 
[1] Jayant, N. S., "Digital coding of speech waveforms: PCM, 
DPCM, and DM quantizers," Proceedings of the IEEE , vol.62, 
no.5, pp. 611- 632, May 1974. 
[2] Schroeder, M.,  Atal, B., "Code-excited linear prediction(CELP): 
High-quality speech at very low bit rates," Acoustics, Speech, 
and Signal Processing, IEEE International Conference on 
ICASSP '85. , vol.10, no., pp. 937- 940, Apr 1985. 
[3] Yoshimura, T., Tokuda, K., Masuko, T., Kobayashi, T., 
Kitamura T., "Simultaneous modeling of spectrum, pitch and 
duration in HMM-based speech synthesis", Proc. of Eurospeech, 
pp.2347-2350, Sept. 1999. 
[4] Hoshiya, T., Sako, S., Zen, H., Tokuda, K., Masuko, T., 
Kobayashi, T., and Kitamura, T., “Improving the performance of 
HMM-based very low bitrate speech coding”, Proc. ICASSP, 
pp.800–803, 2003. 
[5] Tokuda, K., Masuko, T., Hiroi, J., Kobayashi, T. and Kitamura, 
T., "A very low bit rate speech coder using HMM-based speech 
recognition/synthesis technique," in Proc. ICASSP, 1998. 
[6] Yang, J. H. Liu, M. J., Chang H. H. Chiang, C. Y., Wang, Y. R. 
and Chen, S. H., “Enriching Mandarin speech recognition by 
incorporating a hierarchical prosody model”, accepted and to 
appear in ICASSP 2011. 
[7] Chen, S. H., Yang, J. H., Chiang, C. Y., Liu, M. C., and Wang, 
Y. R., "A New Prosody-Assisted Mandarin ASR System", 
submit to Trans. on IEEE Audio, Speech, & Language 
Processing. 
[8] Bilmes, J. A. and Kirchhoff, K., “Factor language models and 
generalized parallel backoff,” in Proc. of HLT/NACCL, 2003, pp. 
4-6. 
[9] Chiang, C. Y., Chen, S. H., Yu, H. M. and Wang, Y. R., 
“Unsupervised joint prosody labeling and modeling for 
Mandarin speech,” Journal of the Acoustic Society of America, 
125, No. 2, pp.1164-1183, Feb. 2009. 
[10] Chen, S. H. and Wang, Y. R., “Vector quantization of pitch 
information in Mandarin speech,” IEEE Transactions on 
Communications, vol. 38, no. 9, pp. 1317-1320, September 1990. 
[11] Tokuda, K., Masuko, T., Kobayashi, T. and Imai, S., “Mel-
generalized cepstral analysis-a unified approach to speech 
spectral estimation,” in Proceedings of the International 
Conference on Spoken Language Processing (ICSLP '94), pp. 
1043–1046, Yokohama, Japan, September 1994. 
[12] Tokuda, K., Yoshimura, T., Masuko, T., Kobayashi, T., 
Kitamura, T., “Speech parameter generation algorithms for 
HMM-based speech synthesis”, Proc. of ICASSP, pp.1315-1318, 
June 2000. 
[13] Mandarin microphone speech corpus – TCC300, 
http://www.aclclp.org.tw/use_mat.php#tcc300edu. 
[14]  “HTK Web-Site”, http://htk.eng.cam.ac.uk. Accessed 2009. 
[15] Bahl, L.R., Brown, R. F., de Souza, P. V., and Mercer, R.L., 
“Maximum mutual information estimation of hidden markov 
model parameters for speech recognition,” in Proc. ICASSP 
1986, pp. 49-52. 
[16] Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T., Black, 
A.W., Tokuda, K., The HMM-based speech synthesis system 
version 2.0, Proc. of ISCA SSW6, Bonn, Germany, Aug. 2007. 
Outline 
n Introduction 
n Conventional Approaches 
n MCE-based Spatial-Temporal Feature Extraction 
n Noisy RWCP Sound Scene Experimental Results 
n Conclusions 
Department of Electronic Engineering, National Taipei University of Technology 
Spectro-Temporal Cues Extraction 
n Convolution of 2-D filters and mel-spectral	 
Introduction  Conventional Proposed Experiment Conclusion 
From Ken Schutte,  “Parts-based Models and Local Features for Automatic Speech 
Recognition ”, MIT PhD Thesis, 2009 
Department of Electronic Engineering, National Taipei University of Technology 
Conventional Approach
n Independent feature extraction frontend and
 recognizer backend 
Feature 
extraction
Audio event 
recognizer
Recognized 
results
Mel-filterbank 
features
Introduction  Conventional Proposed Experiment Conclusion 
Department of Electronic Engineering, National Taipei University of Technology 
n Conventional approach may be sub-optimal 
–  subject to accuracy/error rate criterion 
n Let’s bridge the gap between feature extraction
 frontend and backend recognizer 
Proposed Integrated Approach
Feature 
extraction
Audio 
recognizer
Recognized 
results
Mel-filterbank 
features
Feature 
extraction
Audio 
recognizer Recognized 
results
Mel-filterbank 
features
Introduction  Conventional Proposed Experiment Conclusion 
Department of Electronic Engineering, National Taipei University of Technology 
MCE/GPD Formulation (1/2)	 
n Minimum Classification Error (MCE) 
–  Score function for each HMM, 
–  Mis-classification measure for the correct       and competing
 hypotheses  
 
 
–  Decision cost (sigmod with parameters         ) 
Introduction  Conventional Proposed Experiment Conclusion 
gk (Y ;Λ) = log Pk (Y ;Λ){ },k =1~ K
 
d Y( ) = −g ′k Y ;Λ( ) + 1K −1 exp(η ⋅ gl Y ;Λ( ))l≠ ′k∑
⎛
⎝⎜
⎞
⎠⎟
1/η
L d (Y )( )= 11+ e-(γd (Y )+θ )
Department of Electronic Engineering, National Taipei University of Technology 
 ′k
 l ≠ ′k
 
Λ = ω q , j
k ,m ,µq , j
k ,m ,σ q , j
k ,m{ }
 
γ ,θ( )
Experimental Settings	 
n Aurora 2 multi-condition training task-like protocol 
–  Noisy RWCP sound scene database 
–  23-dim. Mel-filterbank (3ms frame shift) * 10 neighboring frames (in
 total 230 dimensions) 
–  HMMs, 7 states, 4 mixtures per state 
n 5 Benchmark systems 
–  MFCC (39 dimensions) 
–  MFCC+MVA (mean, variance normalization and ARMA
 filtering) 
–  Gabor+PCA (uniform sampling, 390 à 150 dimensions) 
–  LDA (230 à 150) 
–  MCE (230 à 150, initialized by PCA) 
Introduction  Conventional Proposed Experiment Conclusion 
Department of Electronic Engineering, National Taipei University of Technology 
Experimental Results – Summary
0 
1 
2 
3 
4 
5 
6 
MFCC MFCC+MVA Gabor+PCA LDA MCE 
Average Recognition Error Rate (%) 
Introduction  Conventional Proposed Experiment Conclusion 
Department of Electronic Engineering, National Taipei University of Technology 
Experimental Results – different SNR
0 
2 
4 
6 
8 
10 
12 
14 
20 15 10 5 0 
MFCC 
MFCC+MVA 
Gabor+PCA 
LDA 
MCE 
Average Recognition Error Rate (%) 
Introduction  Conventional Proposed Experiment Conclusion 
Department of Electronic Engineering, National Taipei University of Technology 
n  23.2% relative error rate reduction 
–  4.13％ (MFCC+MVA) à	 3.17％ (MCE) 
 
n  RWCP sound scene database is free available and
 free shipping! 
–  Thanks                    a lot! 
–  http://research.nii.ac.jp/src/eng/list/detail.html#RWCP	 
	 
	 
Conclusions	 
Introduction  Conventional Proposed Experiment Conclusion 
0 
1 
2 
3 
4 
5 
6 
Average Recognition Error Rate (%) 
Department of Electronic Engineering, National Taipei University of Technology 
Standard 39 dimensional MFCC-based representation viewed as 39 T-F 
patches. Each patch gives a scalar feature value at each point in time, by 
centering the patch at that time and computing a dot-product with the 
Mel-spectrogram (here, assuming 100 Hz frame rate and 40 Mel-filters).
Department of Electronic Engineering, National Taipei University of Technology 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/11/27
國科會補助計畫
計畫名稱: 視障者資訊輔具計畫:中文DAISY數位有聲書播放機軟硬體開發
計畫主持人: 廖元甫
計畫編號: 100-2221-E-027-006- 學門領域: 殘障輔具研究 
研發成果名稱
(中文) 可攜式DAISY數位有聲書播放機
(英文) Portable Chinese Mandarin DAISY player
成果歸屬機構
國立臺北科技大學 發明人
(創作人)
廖元甫
技術說明
(中文) 完整的DAISY Player系統，其具有DAISY parser，語音合成與MP3解碼等功能模
組。此外並具攜帶方便（約智慧型手機大小)，目標價錢便宜（預估3000元以
下），可長時間（10小時可充電鋰電池）使用，與可以以SD記憶卡交換有聲書等
特點。 
 
可供視障資訊輔具，或是數位有聲電子書使用。 
 
技術內容包含多項說明書，硬體線路佈局，人機介面按鍵設計，外殼設計與實際
樣品。語音合成器，DAISY parser與MP3程式原始碼。 
(英文) A portable DAISY player device with DAISY parser, TTS and MP3 decoder including 
document, PCB layout, mockup and source code
產業別 資訊服務業
技術/產品應用範圍 殘障資訊輔具，有聲書
技術移轉可行性及
預期效益
已發展完成實際硬體線路，外型設計與相關程式碼。因此技術移轉應該相當方便，可推
廣至供國內每一位視障學生隨身使用，或是一般人可隨身使用之數位有聲電子書
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
已投稿一篇期刊論文,發表兩篇國際會議論文，一篇國內會議與被接受一篇國內
會議論文,包括: 
 JASA Express Letters: Yuan-Fi Liao, Ming-Long Wu and Jia-Chi Lin, 
Maximum intelligibility-based speech synthesis framework for noisy 
environments 
 IS3C: Po-Hsin Huang, Sheue-Ling Hwang,  Jan-Li Wang and Yuan-Fu 
Liao, ＇＇The Interface Development of Chinese DAISY Player Operation 
Procedure,＇＇ Computer, Consumer and Control (IS3C), 2012 
International Symposium, 4-6 June 2012, pp.313-316. 
 Yuan-Fu Liao, Chia-Chi Lin and Jiun-Yan Pan, ＇＇The NTUT Blizzard 
Challenge 2012 Entry＇＇, Blizzard Challenge 2012 
 NST2011: 吳明龍、林佳琪、廖元甫，以理解度為基礎之雜訊環境語音合成
 NST2012: 林佳琪,潘俊言,廖元甫,適用於 DAISY 數位有聲書之中英夾雜語
音合成系統初步實作 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
