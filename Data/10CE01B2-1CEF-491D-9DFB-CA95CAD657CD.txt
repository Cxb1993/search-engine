輸入環境，且使用者無頇穿戴任何感應裝下
前提下，即可進行人體三維動作間的截取系
統。並將其與擴增實境(Augmented Reality)
系統結合。不僅讓輸入介面創新且增加實用
性，更能與整體現實空間結合，讓使用者真
有身歷其境的感覺，增添使用樂趣。 
 
三、 研究方法 
  在此計畫中，單一架攝影機為我們整個
研究中所用到為一輸入設備，有鑒於一開始
設定即為在最一般情況下任何使者用皆可
容易取得設備與輸入環境，因此採用網路攝
影機，在解析度 640 x 480 下的影像資訊擷
取。 
 
（一）人體三維資訊擷取 
 
上圖為人體三維資訊擷取流程圖 
1. 剪影萃取 
為了有利於接下來的人體三維動作擷取
計算，我們必頇先藉由剪影萃取這動作
將人體動作剪影盡可能完整的切割出
來。前景與背景的切割在影像處理領域
一直是不斷探討之研究，縱使到現在仍
然沒有一個完美的演算法能符合所有情
境下完美的切割。而在我們的研究中，
所考量到的是及時互動處理的速度，相
對來說得到的剪影品質不需要很精細即
可處理，因此我們選用了較易計算的背
景相減演算法，以已知單純一顏色布幕
為背景，由於單純背景，並不需要建立
太複雜的背景模型，僅需要截取Ｎ張不
含前景的影像取每像素平均做為背景模
型，接著利用色彩與亮度的相依性，將
前 景 資 訊 切 割 出 來 。
 
以下為背景影像，原始影像與剪影成果 
 
 
 
2. 膚色偵測 
為了準確估計計算出人體動作於三維空
間中資訊，我們在研究中一起加入了膚
色偵測。利用將輸入影像色彩從 RGB 轉
 
如上圖，藍色區塊為上一次比對結果，
一開始我們統計此區塊內像素對應到剪
影中相同位置為背景的數量，接著將角
度各加減一個單位對硬到紅色與黃色區
域，並計算兩區域中為背景的數量，相
之比較，可知紅色區塊背景數較少，所
以歸類出實際人體活動為往紅色區域方
向走，並繼續做以上統計動作，如此反
覆疊代比對最後將可收斂出最佳結果。
得到所要的角度。 
 
如上圖，一樣利用剪影比對疊代修正得
到另一方向角度。不過在此遇到比較大
的困難點是人體部位於前傾跟後仰剪影
是相同的，因此我們加入了先前所做的
膚色偵測與人體運動限制，來彌補資訊
的不足，同時也可加快演算法速度，縮
小比對方向。 
 
（二）運算加速 
1. GPU加速 
由於上述人體三維資訊截取使用了大量
繁複的迴圈計算，在為了因應與擴增實
境及時應用下，必頇力求處理速度達到
更快的效果。 
CUDA為NVIAIA所開發的通用型平行運算
架構，能讓繪圖處理器擁有足夠處理複
雜的運算問題，其包含 CUDA指令集架構
(ISA)以及繪圖處理器中的平行運算引
擎。現今的開發者可以利用最普及的高
階語言-C 語言寫平行加速運算程式，並
以極佳效能運算於支援 CUDA的處理器上
運作，未來將更支援其他程式語言如
C++。 
以下為人體三維資訊截取對人體各個部
位追蹤演算法流程： 
 
當我們更細部拆入進去看核心演算法會
發現幾乎每個流程都是做大量重複的工
作。 
 
而在其中最為需要大量運算處理的就是 
＜與當下剪影比對＞ 
區塊。 
 
因此我們將之引導入 CUDA 運算，利用繪
圖處理器來嘗試多工平行處理來達到加
速效果。以繪圖處理器多 thread應用來
讓所需要運算同一時間處理完畢。 
人體三維資訊截取連接一台已固定視
角的攝影機，透過截取到人體全身的影
像。而擴增實境應用連接到的是頭戴式
顯示器及小型網路攝影機，透過攝影機
輸入，利用頭戴式顯示器做為輸出，這
裡我們將攝影機固定於頭戴式顯示器
的前方，如此能利用小型攝影機截取到
使用者前方的影像，再由頭戴式顯示器
整合成像於使用者的視野中。讓使用者
可以看到眼前的實際景物，再藉由小型
網 路 攝 影 機 偵 測 前 方 的 樣 板
(Marker)，透過樣板圖形在系統中定義
的虛擬物件，並利用三維座標的轉換成
像於影像中，讓使用者能感覺到現實場
景中融入了不一樣的感官體認。最後再
經由判斷人體三維資訊截取得到的資
訊結合，讓使用者不僅能看到虛擬成像
的物件，並能與之互動。 
下圖為頭戴式顯示器與小型攝影機 
 
 3. 樣板辨識與虛擬成像原理 
為了要在影像中顯示出虛擬物件，首先
必頇偵測出樣板所在位置，在我們系統
中定義的樣板為已知大小的正方形，因
此在影像中存在四條正方形的邊線。確
認了四條線所在的位置，再來我們必頇
確認這偵測到的範圍是否為我們所定
義的樣版圖案，由於樣板在影像中因為
透視投影(Perspective Projection)的
關係，看起來會有不同程度的歪斜，不
一定為標準的資訊，因此在這我們必頇
先將其作一個正規化的處理，利用四點
頂點位置代入投影矩陣求得轉換關
係，找出其中關係。 
 
正規化完的圖形接著再與系統中所儲
存的樣板圖形做一樣板比對處理，若確
定為系統定義圖形，則我們可得知樣板
所在位置與圖形方向。 
得知了影像中所處的位置後，接著再藉
由座標系之間的轉換關係如下圖 
 
我們便可順利的將虛擬物件成像於想
要的位置上。 
 
4. 擴增實境應用 
在實際應用方面我們用下面這個樣板
(marker) 
 
運用 openGL 繪圖繪畫一個虛擬物件
桌球桌，並賦予球模擬實際空間的物理
行動參數，接著利用前面章節所說原理
成像於上面如下圖 
Techniques for 3-D Computer Vision”, chapter 
10, Prentice Hall, Upper Saddle River, N.J., 
1998 
[6] R. Hartley and A. Zisserman, “Multiple View 
Geometry in Computer Vision, 2nd Ed.”, 
Cambridge University Press, 2003 
[7] German K.M. Cheung, Simon Baker, and  
Takeo Kanade, “Visual Hull Alignment and 
Reﬁnement Across Time: A 3D Reconstruction 
Algorithm Combining Shape-From-Silhouette 
with Stereo”, Computer Vision and Pattern 
Recognition, 2003 
[8] Nina Amenta, Sunghee Choi, Ravi Krishna 
Kolluri, “The Power Crust”, ACM Symposium 
on Solid and Physical Modeling, 2001 
[9] T Matsuyama, XJ Wu, T Takai, T Wada, 
“Real-Time Dynamic 3-D Object Shape 
Reconstruction and High-Fidelity Texture 
Mapping for 3-D Video”, IEEE Transactions on 
Circuits and Systems for Video Technology, 
2004 
[10] Pooja Verlani, Aditi Goswami, P. J. Narayanan, 
Shekhar Dwivedi, Sashi Kumar Penta, “Depth 
Image: Representations and Real-time 
Rendering”, 3D Data Processing, Visualization, 
and Transmission, 2006. 
道參觀埃及散落各地的古文明，並於十一月十四日返國。 
 
二、與會心得 
 
ICIP 為 IEEE 所直接贊助每年一度的大型國際研討會，其論文的品質一向非常整齊且優
良，在影像處理領域有著相當的影響力，而眾多的大師級學者也必親自與會。本屆會議由國
內前往參加的包括了台大、清大、交大、成大、中央、中正、海洋、台科大、東方、中研院、
工研院等單位的教授，因此除了可以在短暫的研討會期間接觸到世界各地的研究近況，亦得
以與國內學者詳談並了解國內的研究趨勢與優勢。在會場上與會人員直接與論文作者的討論
與互動，以及積極參與的態度相當值得我們學習。 
本次研討會最大的收穫可以算是兩個 plenary speeches 的部分，除了 Dr. Hawass 說明了
影像處理在考古研究的貢獻之外，Kanade 教授也提出了在顯為影像上對於大量細胞追蹤的技
術。在這一方面的研究上他認為已有突破性的進展，並得以為人類在醫學上的研究作出進一
步的貢獻。相較於他早期在電腦視覺上與生活與娛樂相關的研究，此一改變或可察覺到未來
在影像方面的研究趨勢。整體而言，除了四天的行程無法涵蓋研討會全程之外，其收穫可以
說是相當豐富。 
with a single exposure using standard cameras. One simple
way to create an HDR image is combining several LDR ex-
posures based on the knowledge of the camera response func-
tion. In general, the camera response characteristics are not
provided by the manufacturers. Thus, we adopt the classic
method proposed by Debevec and Malik [2] to derive the in-
tensity response curve, with some modifications on sample
point selection as described in the following section. The un-
known scene radiance can then be recovered using the inten-
sities of the acquired images and the associated camera expo-
sure settings.
The HDR image generated from multiple LDR exposures
usually suffers from noise, blurring, and ghost phenomenon,
etc. In the previous work, Grosch proposed a ghost removal
algorithm by eliminating the moving object in the scene based
on the difference of median threshold bitmaps of two con-
secutive exposures [6]. Since the noise can easily affect the
threshold result, especially for the images with low exposure,
the method usually requires to set the threshold manually.
Akyuz and Reinhard presented a noise reduction method by
replacing the low exposure images with multiple weighted
high exposure ones [7]. Different from the previous ap-
proaches, except for the issue on noisy image captures under
low illumination conditions, we will also have to deal with
the ghost image problem of SHDRI due to stereo mismatches.
Although the HDR radiance map can be used to represent
an HDR image, most of the existing display devices and me-
dia are not capable of covering the full dynamic range. Com-
pressing the HDR images for a LDR display is known as the
tone-reproduction or tone-mapping problem, and there is a
number of techniques with remarkable performance. In this
work, the algorithm proposed by Drago [8] is used to gener-
ate the tone-mapped images for display and correlation-based
stereo matching.
3. THE ALGORITHM
This section presents the proposed technique for stereoscopic
high dynamic ranging imaging. Two cameras placed side-by-
side as a conventional stereo configuration are used for image
acquisition. Each stereo image pair acquired with different
shutter speed is used for HDR image synthesis and stereo dis-
parity computation.
3.1. Deriving the Camera Response Function
To derive the camera response function from a single view-
point with multiple exposures, it is necessary to obtain the in-
tensities associated with the same image pixel. In the stereo-
scopic high dynamic range imaging, however, a scene point
is not necessarily projected to the same pixel for two images.
Thus, it is mandatory to find the feature correspondences be-
tween the images and then used as the sample points for cam-
era response function derivation. In this work, the SIFT de-
Fig. 1. Correspondence feature extraction using SIFT.
scriptor [9] is used to select the sample points for camera re-
sponse curve modeling. An example of the correspondence
feature extraction is shown in Fig. 1.
Ideally the camera response curve can be derived using
the obtained feature correspondences and the associated im-
age intensities. However, there might exist incorrect corre-
spondences due to noise or stereo mismatch. Based on the
SHDRI setting with a conventional stereo configuration, the
epipolar, ordering and exposure constraints are used to in-
crease the robustness of correspondence matching. Moreover,
the feature points with less intensity variation in the neigh-
borhood are selected as sample points for deriving the camera
response curve. A window size of 11 × 11 pixels is used to
calculate the variance associated with each feature correspon-
dence. Only those feature points with variance smaller than a
given threshold are qualified as sample points.
To ensure that the camera response curve is well sampled
for the full brightness range, the above feature points are fur-
ther divided into 10 equally-spaced groups based on their in-
tensities, and a fixed number of samples is selected from each
group.
3.2. Image Normalization and Correspondence Matching
In this work we adopt the stereo matching algorithm based on
belief propagation to derive the disparity map [10]. Since the
correlation should be evaluated on the images captured with
similar illumination conditions, the stereo image pair cap-
tured under different exposures should be normalized prior
to the correspondence matching process. Using the camera
response function derived from the previous section, one im-
age can be transformed to another with a different exposure
setting. Thus, by transforming the image pair to the ones with
the same exposure, the stereo matching algorithm can be car-
ried out straightforwardly.
Fig. 2 (left two images) shows the normalized stereo im-
age pairs corresponding to the low, median and high expo-
sures (top, middle and bottom figures). The disparity images
obtained from stereo matching using belief propagation are
shown in Fig. 2 (right figures) for all three cases. It can be
seen that there are no significant differences among the dis-
parity images. This also verifies the correctness of the derived
camera response function. Since the disparity of each pixel
4306
(a) Left image. (b) Right image.
0 50 100 150 200 250 300
−5
−4
−3
−2
−1
0
1
2
3
4
5
pixel value Z
lo
g 
ex
po
su
re
 X
response curve
(c) Camera response curve.
Fig. 4. Experiment using the “art” images.
(a) Synthesized HDR image. (b) Disparity image.
Fig. 5. The results obtained by the proposed SHDRI. Two im-
ages with different viewpoints, different exposures are used.
ber of literature related to HDRI research, no previous work
on stereoscopic HDRI has been addressed. The camera re-
sponse function is derived based on the stereo image pair,
which is then used to normalize the images for correspon-
dence matching. We have demonstrated the feasibility of the
proposed technique using standard stereo image datasets.
References
[1] S.K. Nayar and T. Mitsunaga, “High dynamic range
imaging: Spatially varying pixel exposures,” in IEEE
Computer Vision and Pattern Recognition, 2000, pp. I:
472–479.
[2] Paul E. Debevec and Jitendra Malik, “Recovering high
dynamic range radiance maps from photographs,” in
SIGGRAPH ’97: Proceedings of the 24th annual confer-
(a) Synthesized HDR image. (b) Disparity image.
Fig. 6. The results obtained by conventional methods. (a)
From two images of the same viewpoint with different expo-
sures. (b) From a stereo image pair with the same exposure.
ence on Computer graphics and interactive techniques.
1997, pp. 369–378, ACM Press.
[3] Sing Bing Kang, Matthew Uyttendaele, Simon Winder,
and Richard Szeliski, “High dynamic range video,” in
SIGGRAPH ’03: ACM SIGGRAPH 2003 Papers, New
York, NY, USA, 2003, pp. 319–325, ACM.
[4] Katrien Jacobs, Ce´line Loscos, and Greg Ward, “Au-
tomatic high-dynamic range image generation for dy-
namic scenes,” IEEE Computer Graphics and Applica-
tions, vol. 28, no. 2, pp. 84–93, 2008.
[5] D. Scharstein and R. Szeliski, “Middlebury stereo vision
page,” http://www.middlebury.edu/stereo, 2002.
[6] T. Grosch, “Fast and robust high dynamic range image
generation with camera and object movement,” in Vi-
sion, Modeling and Visualization, RWTH Aachen, 2006,
pp. 277–284.
[7] A.O. Akyuz and E. Reinhard, “Noise reduction in high
dynamic range imaging,” Journal of Visual Communica-
tion and Image Representation, vol. 18, no. 5, pp. 366–
376, October 2007.
[8] F. Drago, W.L. Martens, K. Myszkowski, and N. Chiba,
“Design of a tone mapping operator for high dynamic
range images based upon psychophysical evaluation and
preference mapping,” IS&T/SPIE Electronic Imaging,
2003.
[9] David G. Lowe, “Distinctive image features from scale-
invariant keypoints,” Int. J. Comput. Vision, vol. 60, no.
2, pp. 91–110, 2004.
[10] J. Sun, N.N. Zheng, and H.Y. Shum, “Stereo matching
using belief propagation,” IEEE Trans. Pattern Analysis
and Machine Intelligence, vol. 25, no. 7, pp. 787–800,
July 2003.
[11] K. Jacobs, C. Loscos, and G. Ward, “Automatic high-
dynamic range image generation for dynamic scenes,”
Computer Graphics and Applications, IEEE, vol. 28, no.
2, pp. 84–93, March-April 2008.
[12] E.A. Khan, A.O. Akyuz, and E. Reinhard, “Ghost re-
moval in high dynamic range images,” in Image Pro-
cessing, 2006 IEEE International Conference on, Oct.
2006, pp. 2005–2008.
4308
2Schedule at a Glance
Saturday, November 7
09:30 – 12:30 Tutorials
10:50 – 11:10 Break 
12:30 – 13:30 Lunch (On your own) 
13:30 – 16:30 Tutorials
14:50 – 15:10 Break 
18:00 Buses leave for reception 
Sunday, November 8
08:30 – 10:00 Opening Remarks and Plenary Talk: Dr. Zahi Hawass
10:15 – 11:35 Technical Sessions (First Half)
11:35 – 12:05 Break 
12:05 – 13:25 Technical Sessions (Second Half)
13:25 – 14:45 Lunch (On your own) 
13:35 – 14:35 Meeting: Image, Video and Multidimensional Signal 
Processing TC Luncheon
14:45 – 16:05 Technical Sessions (First Half)
16:05 – 16:35 Break 
16:35 – 17:55 Technical Sessions (Second Half)
18:00 – 19:00 Meeting: Publications Board Dinner
19:00 – 23:00 Meeting: Publications Board Meeting
Monday, November 9
08:30 – 09:50 Technical Sessions (First Half)
09:50 – 10:20 Break 
10:20 – 11:40 Technical Sessions (Second Half)
11:30 – 13:30 Meeting: Executive Committee Luncheon
11:40 – 13:00 Lunch (On your own) 
11:50 – 12:50 Meeting: Information Forensics and Security TC
13:00 – 14:20 Technical Sessions (First Half)
13:30 – 16:30 Meeting: ExCom Strategic Planning Retreat
14:20 – 14:50 Break 
14:50 – 16:10 Technical Sessions (Second Half)
16:45 Buses leave for banquet
Tuesday, November 10
08:30 – 09:30 Plenary Talk: Takeo Kanade
09:45 – 11:05 Technical Sessions (First Half)
11:05 – 11:35 Break 
11:35 – 12:55 Technical Sessions (Second Half)
13:05 – 14:05 Meeting: IEEE Transactions on Image Processing 
Editorial Board
13:05 – 14:05 Meeting: ICIP to ICIP
12:55 – 14:15 Lunch (On your own) 
14:15 – 16:05 Technical Sessions (First Half)
16:05 – 16:35 Break 
16:35 – 17:25 Technical Sessions (Second Half)
18:00 – 19:00 Student Reception
18:00 – 19:00 Meeting: Conference Board Dinner
19:00 – 23:00 Meeting: Conference Board Meeting
Wednesday, November 11
08:00 – 09:00 Meeting: Board of Governors Breakfast
09:00 – 17:00 Meeting: Board of Governors Meeting
12:00 – 13:00 Meeting: Board of Governors Luncheon
11
Prof. Takeo Kanade has a profound impact on humanity. He is 
directing two centers. The first is at Carnegie Mellon University 
(USA): Quality of Life Technology Engineering Research Center. 
The second center is in Tokyo: Digital Human Research Center. 
Prof. Kanade has been elected to the National Academy of 
Engineering and the American Academy of Arts and Sciences.
In addition to this stimulating and intellectual technical program, 
a colorful, cultural and entertaining social program is planned. 
Two historic landmarks—the Pyramids and Citadel—are the 
centerfolds of this culture program:
• A welcome reception at the Citadel, Saturday, Nov. 7th.
The Citadel is a major Islamic landmark. It is overlooking Cairo, 
it offers great city views and breeze. The program highlights 
of the impact of Islamic history on music and entertainment.
• Conference Banquet at the Pyramids, Monday, Nov. 9th.
It’s the view of a lifetime. The greatness of its history is lit with 
you in the picture. You will surf  all over Egypt through dancing 
and music. You will get a taste of each region in Egypt.
The conference hotel, the Grand Hyatt, is one of the finest hotels 
in Cairo, located on an island in the middle of the Nile. All 
rooms have a Nile view; it is beyond imagination. You will feel 
that you are “hugging Cairo on the Nile”. Aly Farag and Ayman 
El-Desouki are working with both U.S. and Egyptian teams to 
ensure a successful conference.
Many people, colleagues and friends have helped in making this 
conference a reality. Lina Karam and Rabab Ward came up with 
the idea; we believed in it and worked hard with all parties to 
bring it to life. Aly Farag and Ayman El-Desouki have laid down 
the foundation in the U.S. and Egypt. Samia Mashaly and Amal 
Zaki have been the front team to decide on the location for the 
conference 4 years ahead (that is not very common in Egypt). We 
have formed a real global team among CMS (USA), Wings Tours 
(Egypt), and the Grand Hyatt (Egypt); they are working for one 
unified cause: a successful ICIP. Now, Bryan Stewart is an expert 
in Cairo (especially crossing streets), Lance still keeping his cool 
after all these visas questions, and Ahmed Glal and Mohamed 
Aly know the American secrets of organization. Special thanks 
to John Mathews for his leadership and the SPS office led by 
Mercy for facilitating all the hurdles of having a conference in 
this region for the first time. Special thanks to Ed Delp, whom we 
asked for help and advice quite often. My deep appreciation to 
my students and dynamic assistant, Cathy Pomier.
We are very proud to be hosting ICIP 2009. We hope that it 
will be a rewarding, informative and stimulating experience. We 
look forward to welcoming you in Cairo, where images were 
invented 7000 years ago! Come and celebrate the vivid images 
across history with us. All pixels will welcome you.
Prof. Magdy Bayoumi
ICIP 2009 General Chair
13
The ICIP 2009 technical program includes two plenary lectures 
highlighting exciting and timely topics that complement the 
topics covered in the sessions. We are grateful to the plenary 
chair, Rabab Ward, and the general chair, Magdy Bayoumi, for 
selecting the distinguished plenary speakers: Dr. Zahi Hawass, 
Secretary General of the Supreme Council of Antiquities,  Egypt, 
and Takeo Kanade, Robotics Institute, Canegie Mellon University, 
USA.
The conference begins on Saturday, November 7, with 8 tutorials 
that were selected by the tutorial chairs, Eli Saber and David 
Taubman.  The tutorials were selected from 19 proposals that 
were submitted in response to the Call for Tutorials and were 
reviewed by a team of experts under the coordination of the 
tutorial chairs.  The tutorials offer overviews of the state of the art 
in several key areas of both theoretical and practical interest to 
researchers in the field. The tutorial topics and their presenters 
are: 1) Nonlocal Image Processing Techniques presented by 
Vladimir Katkovnik (Tampere University of Technology, Finland), 
2) Shape Representation and Registration using Different Implicit 
Spaces presented by Aly Farag (University of Louisville, USA) and 
Hossam Abdelmunim (Ain Shams University, Egypt), 3) Color in 
Image/Video Processing Applications presented by Theo Gevers 
(University of Amsterdam, The Netherlands) and Joost van de 
Weijer (Universitat Autònoma de Barcelona, Spain), 4) Video 
Processing Techniques for 3-D Television presented by Yo-Sung 
Ho (Gwangju Institute of Science and Technology, South Korea), 
5) Multiresolution Analysis for Imaging on Reconfigurable 
Hardware presented by Abbes Amira (Brunel University, UK), 
6) Image Fusion for Image and Video Quality Enhancement 
presented by Jan Flusser, Filip Šroubek and Barbara Zitová 
(Institute of Information Theory and Automation of the ASCR, 
Czech Republic), 7) Perceptual Metrics for Image Quality 
Evaluation presented by Sheila Hemami (Cornell University, 
USA) and Thrasos Pappas (Northwestern University, USA), 8) 
Introduction to Image Registration presented by William Wells 
(Massachusetts Institute of Technology, USA) and Lilla Zollei 
(Massachusetts General Hospital, USA).
The ICIP 2009 technical program also includes 7 special 
sessions covering a broad spectrum of topics that highlight 
emerging areas of research and new perspectives to existing 
topics.  Special session chairs Sheila Hemami and Moumen 
Elmelegy coordinated a thorough review of both the special 
session proposals and the individual papers submitted to each 
of the selected sessions.  This year’s special sessions and their 
organizers are: 1) Digital Imaging in Cultural Heritage organized 
by Patrick Ndjiki-Nya (FhG Heinrich-Hertz-Institut, Germany), 
Peter Schallauer (Joanneum Research Forschungsgesellschaft 
mbH, Germany), Bogdan Smolka (Silesian University of 
Technology, Poland), Konstantinos Plataniotis (University of 
203
(Continued from previous page.)
15:35 - 16:05 BREAK
16:05 - 17:25
TP.PI.7b STEREOSCOPIC VIDEO ERROR CONCEALMENT FOR 
MISSING FRAME RECOVERY USING DISPARITY-
BASED FRAME DIFFERENCE PROJECTION
Yibin Chen, Canhui Cai, Huaqiao University, China; Kai-
Kuang Ma, Nanyang Technological University, Singapore
TP.PI.8b SHAPE FROM FOCUS USING KERNEL REGRESSION
Muhammad Tariq Mahmood, Tae-Sun Choi, Gwangju Institute 
of Science and Technology, Republic of Korea
TP.PI.9b OVERLAP ELIMINATION FOR REGISTERED RANGE 
IMAGES
Xiaokun Li, DCM Research Resources LLC, United States; 
William G. Wee, University of Cincinnati, United States
TP.PI.10b PARALLEL HIGH RESOLUTION REAL-TIME VISUAL 
HULL ON GPU
Wolfgang Waizenegger, Ingo Feldmann, Peter Eisert, Peter 
Kauff, Heinrich-Hertz-Institute, Germany
TP.PI.11b HIGH DYNAMIC RANGE IMAGING FOR 
STEREOSCOPIC SCENE REPRESENTATION
Huei-Yung Lin, Wei-Zhe Chang, National Chung Cheng 
University, Taiwan
TP.PI.12b 3-D STRUCTURE RECOVERY FROM 2-D OBSERVATIONS
Huiyu Zhou, Queen’s University Belfast, United Kingdom; 
Gerald Schaefer, Loughborough University, United Kingdom; 
Tangwei Liu, Faquan Lin, Guangxi Medical University, China
道參觀埃及散落各地的古文明，並於十一月十四日返國。 
 
二、與會心得 
 
ICIP 為 IEEE 所直接贊助每年一度的大型國際研討會，其論文的品質一向非常整齊且優
良，在影像處理領域有著相當的影響力，而眾多的大師級學者也必親自與會。本屆會議由國
內前往參加的包括了台大、清大、交大、成大、中央、中正、海洋、台科大、東方、中研院、
工研院等單位的教授，因此除了可以在短暫的研討會期間接觸到世界各地的研究近況，亦得
以與國內學者詳談並了解國內的研究趨勢與優勢。在會場上與會人員直接與論文作者的討論
與互動，以及積極參與的態度相當值得我們學習。 
本次研討會最大的收穫可以算是兩個 plenary speeches 的部分，除了 Dr. Hawass 說明了
影像處理在考古研究的貢獻之外，Kanade 教授也提出了在顯為影像上對於大量細胞追蹤的技
術。在這一方面的研究上他認為已有突破性的進展，並得以為人類在醫學上的研究作出進一
步的貢獻。相較於他早期在電腦視覺上與生活與娛樂相關的研究，此一改變或可察覺到未來
在影像方面的研究趨勢。整體而言，除了四天的行程無法涵蓋研討會全程之外，其收穫可以
說是相當豐富。 
98年度專題研究計畫研究成果彙整表 
計畫主持人：林惠勇 計畫編號：98-2221-E-194-041- 
計畫名稱：以全視域影像系統進行即時 3D 視訊擷取及虛擬互動技術之開發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 1 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 1 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
