 2
掃描控制系統，使用單鍵開關輸入來取代以
往複雜的使用者介面，進而將使用者推廣至
具有運動神經疾病、多重硬化症的患者、老
年人以及因意外而無法自行動手進食的傷
患。目前，「Handy 1」的上半部為五個自由度
的機械手臂及一個餐盤，而下半部則是具有
五個輪子的可移動式平台，此移動平台是採
用人力來移動的方式，照顧者在用餐時間，
將 Handy 1 移動至行動不便的使用者旁邊，接
著再把菜餚一樣一樣放在不同的欄位上，開
啟電源，使用者在想吃的食物的 LED 燈亮起
時，按下按鈕，機械手臂便會將那欄的食物
舀起，移動至定位點，使用者即可將食物吃
下，完成餵食的動作。 
而日本的餵食用機器人「My Spoon」的
研發，則起始於 1991 年。SECOM 公司於 1995
年，所開發出的「My Spoon」[4]是一台五軸
的機械手臂，其製造理念是來自 SCARA 型的
機械手臂，將原本末端為線性移動的部份改
成旋轉，以便於給使用者進食，其操作介面
為雷射與雷射感應器，將雷射發送端設置在
使用者頭上，介面板上設置多個雷射感應
器，每個雷射感應器都有其對應使機械手臂
動作的命令，機械手臂所有的動作皆依靠使
用者的命令來進行。目前，市面上的「My 
Spoon」的使用者介面已改為操縱桿，且照顧
者可將操作模式分為：手動、半自動、自動
三種模式，其中食物盤為方形且分成四格，
無論是以哪一種模式進行餵食，抓取食物
後，機械手臂便會將食物送至固定位置，當
夾爪抓取完食物要送到使用者口中時，上夾
爪便會往回縮，只剩下夾爪盛著食物，上夾
爪的設計為叉子狀，下夾爪的設計為湯匙狀。 
綜觀以上兩種餵食用機器人，雖然
「Handy 1」後來將餐盤設計為可替換成其他
盤面，以擁有多功能用途，但就餵食用途而
言，只用單支湯匙能舀起的東西有限，且舀
起的成功率不會很高，當食物舀起後，只能
移動至固定的位置，無法依據使用者嘴巴的
位置來調整，容易造成使用者吃東西的不
便。而「My Spoon」尚未克服的是在自動模
式下，機械手臂有可能會往那一格中無食物
的地方夾取，且機械手臂將食物送到的位置
是固定的，這個問題與 Handy 1 一樣，容易造
成使用者吃東西的不便;除此之外，使用者在
手動的模式下吃東西時，需要不停地用下巴
來操控操縱桿，時間一長，使用者易感到疲
累，甚至心情煩躁，進而影響食慾。 
計畫目的 
為了提昇餵食用機器人的功能，本計畫
的目的在於研發可餵食手部不方便的長者或
殘障者之智慧型輪式人形機器人。為了使所
研發的餵食用機器人，能讓手部不方便的長
者或殘障者，安心且便利地進食，並使他們
覺得機器人是他們的看護，願意與其親近，
則必須考慮到機器人之自主性、機動性、互
動性與安全性。本計畫雖僅針對智慧型餵食
用人形機器人進行研發，但其為追求以上的
四個特質，所發展出之多項技術將可被移植
或應用到其他「與人共存型服務用機器人」
上。  
三、研究方法 
3.1 智慧型餵食用人形機器人之設計 
為了使餵食用機器人能融入人類生活環
境中，機器人若能以擬人形活動於人類生活
環境，將有助於增加機器人對人類的親和
力。然而，完全仿似人類外形的機器人，在
製作上存在著一定程度的複雜性，因此適當
簡化人形機器人外形，不但在製作上容易完
成，且可使機器人保有足夠的親和力。因此，
本計畫將設計一部簡易型的人形機器人(如圖
一所示)，能安全且智慧地餵食人類，主要強
調其安全性、自主性、機動性及與人類的互
動。機器人之上半身將擁有一個固定式的身
軀、一個具有全景式攝影機與二維旋轉變焦
式攝影機之固定於身軀的機械頭、一雙各為
七自由度的機械手臂(Mechanical Arm);左機
械手臂將於本計畫之第二年期完成)，而在下
半身方面，由於二足式的人型機器人所需的
研發時間相當長，而且，有些場合以輪式移
動平台取代雙足就足夠了，例如：家庭中，
故本計畫於第一年期已將其下半身設計為四
輪之全方向輪式移動平台，此一輪式移動平
台與本實驗室先前發展的三輪之全方向輪式
移動平台不同，除了所採用的輪子不同外，
輪子的排列方式亦不同，新的排列方式與一
 4
要直接與人臉資料庫做比對，便能達到
人臉偵測效果。此種方法的缺點除了資
料庫必須靠大量人臉樣本作訓練而產生
外，運算量也相對複雜許多。 
以上這兩種方法各有利弊，所以必須在
處理速度與偵測效果之間做一取捨，而本研
究的目標需要一個即時的人臉特徵偵測，所
以選用以特徵為基礎的方法為本系統優先考
量。由攝影機擷取含有人臉的影像，將此影
像同樣由 RGB 色彩空間轉換至 YCbCr色彩空
間，經由閥值轉換過濾出膚色的影像，利用
標號演算法搜尋出影像中最大的區域，即為
人臉影像區域。在此區域下半部，經由閥值
轉換找到嘴部的影像，計算出此嘴部影像的
形心位置；接著取 Y 色彩空間做閥值轉換，
取出深色部份的影像，在人臉區域的上半
部，利用標號演算法設定面積、形心位置來
搜尋眼部區塊，得到左、右眼的形心位置，
以嘴部、雙眼形心位置作為類神經的影像輸
入向量。完整的人臉影像處理流程圖如圖四
所示。  
 
圖四 人臉影像處理流程圖 
3.2.3 人臉替代圖樣影像處理 
由於使用者的臉部無法放置於機械手臂
末端上來進行類神經訓練，因此，設計一個
取代人臉的圖樣來代替使用者作類神經訓練
的工作，此圖樣上具有三個紅色區塊，分別
代表人的雙眼及嘴部，雙眼形心距離為
70mm，嘴部形心到兩眼形心連線的中心點距
離為 75mm，利用色彩空間轉換方式，由 RGB
色彩空間轉換至 YCbCr 色彩空間，透過閥值
轉換，濾除雜訊，擷取出紅色部分的三個區
塊，再將標號演算法套用至整張影像上，求
出此三個區塊的形心位置。最後，依照人臉
器官分佈位置，判別出替代圖樣左上角的區
塊形心位置為人臉影像中的左眼形心位置、
替代圖樣右上角的區塊形心位置為人臉影像
中的右眼形心位置、替代圖樣下方的區塊形
心位置為人臉影像中的嘴部形心位置。完整
的人臉替代圖樣影像處理流程圖如圖五。  
 
圖五 人臉替代圖樣影像處理流程圖 
3.3 類神經網路應用於手眼關係之學習 
本研究使用倒傳遞類神經網路應用於手
眼之映射關係。倒傳遞類神經網路的架構為
多層感知器(Multilayer Perceptron)，此種網路
具有一層輸入層、一層或一層以上的隱藏層
以及一層輸出層，如圖六，圖中為具有兩層
的感知器神經網路，箭頭方向從輸入層至隱
藏層最後再到輸出層，此為倒傳遞網路中前
饋網路(Feedforward Network)的部份，其中 xi
表示輸入向量，i = 1, 2, …, N；hj 表示隱藏層
的輸出向量，j = 1, 2, …, L，表示有 L個神經
元數；yk 表示輸出層的輸出向量，k = 1, 2, …, 
K；IWji、b1j和 LWkj、b2k 分別為輸入層至隱
藏層、隱藏層至輸出層的權重值與偏權值。
 6
內外部參數，也不用對攝影機與機器人之間
的幾何配置、參數等等進行精確量測。藉由
雙軸攝影機獲得所需要的影像特徵向量後，
利用已經訓練好的類神經網路，可以直接映
射出機械手臂的空間姿態來做抓取食物或是
做餵食的動作，而不用像以往餵食機器人的
餐盒及使用者必須固定在相同位置。  
3.4 餵食機器人之餵食性能測試 
利用倒傳遞類神經網路來訓練時，設定
隱藏層神經元個數為 50 個，訓練次數為 500
次，訓練資料為 5000 筆，分別擷取餐盒角落
影像位置、人臉替代圖樣之嘴部與雙眼形心
影像位置當作輸入向量，所對應的機械手臂
末端姿態則當作期望輸出向量，兩個階段的
誤差收斂情形分別如圖八、圖九所示，當收
歛次數到達 500 次時，誤差收歛至 0.0213、
0.0008。 
0 50 100 150 200 250 300 350 400 450 500
10
-2
10
-1
10
0
10
1
10
2
10
3
10
4
10
5
Epochs
M
ea
n-
S
qu
ar
e 
E
rr
or
 
圖八 抓取階段之誤差收斂情形 
0 50 100 150 200 250 300 350 400 450 500
10
-4
10
-2
10
0
10
2
10
4
10
6
Epochs
M
ea
n-
S
qu
ar
e 
E
rr
or
 
圖九 餵食階段之誤差收斂情形 
為了測試此訓練結果，在抓取的訓練空
間中訂定六個測試位置，將餐盒擺放上去，
並旋轉或傾斜不同角度來做測試；餵食階段
亦同，於餵食的訓練空間中訂定四個測試位
置，將人臉替代圖樣擺放於此，並旋轉不同
角度來做測試。在抓取階段的測試結果，位
置誤差平均為 2.6144mm、角度誤差平均為
0.6753°；而餵食階段的測試結果，位置誤差
平均為 2.6916mm、角度誤差平均為 0.8188°，
此誤差結果相當良好。 
實際應用時，需將圖二中的每個餐盒分
別訓練，再將訓練好的參數記錄下來，透過
攝影機擷取全部餐盒的影像，利用影像處理
將每個餐盒的角落位置記錄下來，分別代入
每個餐盒的參數，可得到每個餐盒所對應的
機械手臂末端姿態，使用者可依需求選取餐
盒號碼來夾取餐盒中的食物。在本計畫中，
只使用一號餐盒來做訓練，因此將利用先前
一號餐盒所訓練好的參數來進行實際應用。
由於先前抓取餐盒時，機械手臂末端姿態並
未對準餐盒中心，故在實際應用時，需先將
機械手臂末端姿態修正至對準餐盒中心。餵
食的對象也將由實際真人取代人臉替代圖
樣，並將待抓物放入餐盒內的中心位置。從
實際應用一號餐盒的實驗中，驗證本研究所
提出的餵食導引方法有很高的成功率。 
四、結論 
本計畫之原目標是以二年時間，設計與
建構一個能支援無法獨立進食的人們之智慧
型餵食用人形機器人，但計畫經審查後，僅
獲核定一年；目前，已如計畫書所規劃，完
成第一年的研究進度。除了已完成全方向輪
式移動平台之設計，並針對機器人之手眼系
統，提出待餵食者之嘴部定位法則，以及擁
有學習能力之機械手臂/手之視覺導引夾取/
餵食食物之控制策略，實驗結果顯示本計畫
所提出的餵食導引方法有很高的成功率。 
五、參考文獻 
[1]  A. J. Colmenarez, B. Frey and T. S. 
Huang, “Detection and Tracking of 
Faces and Facial Features,” Proceedings 
of the International Conference on 
Image Processing, Vol. 1, pp. 657-661, 
Oct. 1999. 
[2]  M. T. Hagan and M. B. Menhaj, 
"Training Feedforward Networks with 
表 Y04 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            98 年 06 月 23 日 
報告人姓名 
 
蔡清元 服務機構
及職稱 
 
成功大學  機械工程學系 
副教授 
     時間 
會議 
     地點 
 
自 97 年 10 月 15 日至 97 
年 10 月 17 日；韓國‧首爾
本會核定
補助文號
 
NSC 97-2221-E-006-180 
會議 
名稱 
 (中文) 第三十九屆(2008 年)國際機器人學研討會 
 (英文)  39th International Symposium on Robotics (ISR 2008) 
發表 
論文 
題目 
 (中文) 人形機器人之設計與手∕眼協調 
 (英文) Design and Hand-eye Coordination of a Humanoid Robot 
一、參加會議經過 
第三十九屆國際機器人學研討會(2008 International Symposium on Robotics; ISR
2008)係由國際機器人學聯盟(IFR)與韓國浦項智慧型機器人學院(Pohang Institute of 
Intelligent Robotics; PIRO)共同舉辦，于 2008 年 10 月 15 日至 10 月 17 日，在韓國
首爾的國際展示中心(COnvention EXhibition; COEX)舉行。由於本次會議利用韓國
首都的展示中心為會議場所，有相當好的交通運輸網以及會場安排等，參加此一會
議的學者專家均能享有十分融洽的場面。 
會議第一天(10 月 15 日)，上午排定一場大會演講(Plenary Speech)，由 iRobot
公司 Greiner 女士主講“由熱情到真實﹕iROBOT 公司的故事＂，介紹該公司將機器
人之應用領域由家庭擴展至國防之心路歷程，下午則全為論文發表，晚上準備有一
簡單的歡迎酒會，提供與會者交流之機會；第二天(10 月 16 日)，上午安排有一場
大會演講與一場邀請演講(Invited Talk)，其講題分別為“機器人技術在韓國核能電
廠之發展與應用＂與“以醫療機器人進行手術＂，下午全為論文發表，晚上在奧林
匹克公園安排了室外晚宴，晚宴進行過程，頒發了一些獎項，並有小型的音樂演奏
會，同桌的與會者皆能在輕鬆的氣氛中，交換研究心得；第三天(10 月 17 日)，除
了上午安排一場大會演講“機器人之太空探測、技術與任務＂與小組討論會“機器
人於國防、太空與核能電廠之應用＂外，全天皆為論文發表。筆者論文“Design and 
Hand-eye Coordination of a Humanoid Robot＂，被安排於第一天之『Intelligent Human 
Robot Interaction』場次中發表，出席該場次的專家學者均熱烈發言討論，收穫不少。
出席這次大會的學者、專家約有 350 餘人，共有一百五十五篇論文發表。 
 
39th International Symposium on Robotics 2008            
Seoul, Korea / October 15~17, 2008
೿WEA2-5ഀ
Proceedings of the 39th ISR(International Symposium on Robotics), 15~17 October 2008 
Design and Hand-eye Coordination of a Humanoid Robot 
T. I. James Tsay and C. H. Lai 
Department of Mechanical Engineering 
National Cheng Kung University 
Tainan, Taiwan (R.O.C.) 
E-mail : tijtsay@mail.ncku.edu.tw and n1892114@ccmail.ncku.edu.tw  
Abstract 
   This work presents a wheeled humanoid robot for 
preliminary research into humanoid robots that play 
ping-pong. This humanoid robot comprises mainly a 
wheeled mobile base, a torso with a 3-DOF waist mounted 
on a mobile base, two 7-DOF robot arms, two 7-DOF robot 
hands and one 7-DOF robotic binocular head. An embedded 
DSP-based control system is also designed and constructed 
for real-time control. An intelligent hand-eye coordination 
control structure is then developed for this humanoid robot. 
The proposed design framework for robotic hand-eye 
coordination control that tracks the ball is based on the 
proposed control structure and involves application of 
self-organizing invertible map (SOIM), grey theory, and 
Kohonen neural network. The SOIM neural network 
acquires a calibration-free spatial representation of 3D 
targets. The Kohonen neural network is applied to learn the 
mapping from incremental changes in the spatial 
representation to incremental changes in the joint 
configurations of a robot arm. 
1. Introduction 
Humanoid robots have emerged in recent decades due to 
factors such as anthropomorphism, friendly design and 
intelligence within human living environments. To satisfy 
consumer demands, several humanoid robots have been 
developed in Japan in recent years [4][7][8][9]. Some large 
companies have devoted considerable resources to 
humanoid robotics research. An outstanding example is the 
Honda humanoid robot—the current version is called 
Asimo—which took a dedicated Honda team over 13 years 
to develop [3]. This robot has very advanced walking 
abilities and fully functional arms and hands with very 
limited eye/arm coordination for grasping and manipulation. 
In the US, the Books group at the AI lab of MIT developed 
“Cog”, which has a head with four eyes (two for close up 
and two for distance), two arms and a torso and no legs [1]. 
The Cog project focuses on human-robot interaction and 
sociability, which aims at favoring the introduction of 
humanoid robots into human society. 
Although humans and many animals walk on legs, most 
mobile robots currently used by the military, consumers or 
businesses roll on wheels. Wheels, compared with legs, are a 
significantly more efficient means of locomotion. This study 
develops a wheeled humanoid robot as a research platform 
for perception-action integration such as that when a robot 
plays ping-pong or functions as a home assistant. Moreover, 
most humanoid robots developed by the academy, such as 
Hadaly, Wendy and Cog, are controlled by multiple PCs. 
The PC-based control system is convenient to use, but is too 
large and heavy. By developing a wheeled humanoid robot 
for preliminary research into humanoid robots, this work 
focuses on developing an embedded high-performance 
control system that facilitates hand-eye coordination. 
2. Design of the Humanoid Robot 
The developed humanoid robot consists of a wheeled 
mobile base, a torso mounted on a mobile base, one robotic 
binocular head, two robot arms, two robot hands and a 
control system (Fig. 1). 
Front view   Rear view 
Fig. 1  Overview of developed humanoid robot 
2.1 Design of the Robotic Binocular Head  
The robotic head mainly consists of an eye part and neck 
part. The eye part uses a binocular camera arrangement, 
which is essential because disparity and approximate depth 
are important factors when differentiating between objects. 
The eye mechanism should be sufficiently compact that fits 
in the robotic head, and should be active such that the eyes 
have a human-like speed and range of motion. Furthermore, 
to imitate human vision, the vision system must have a wide 
field of view and very high resolution. The neck mechanism 
Fig. 3  Assembly drawings of a 7-DOF arm and 7-DOF hand 
2.3 Design of the Robot Waist and Wheeled Mobile Base  
The waist mechanism is utilized for expanding the work 
envelope of the humanoid robot. A torso with a 3-DOF waist 
mounted on a wheeled mobile base was developed. The 
waist is a spherical joint with a roll-pitch-yaw equivalent. 
The first axis facilitates roll movement (-90 to 90°) of the 
whole upper torso around the vertical axis, and the other 
axes facilitate bending from side-to-side (-45° to 45°) and 
front-to-back (45° to -15°). All mechanical stops on the 
waist facilitate a human-like range of motion, and the waist 
has human-like speed of 100°/s.  
Fig. 4 presents an assembly drawing of the 3-DOF waist 
and wheeled mobile base.  
Fig. 4  Assembly drawing of the 3-DOF robot waist and 
wheeled mobile base 
3. Control System of the Humanoid Robot   
The strategy used in designing the control system of the 
humanoid robot was to develop an embedded control system 
platform and construct a stand-alone system that facilitates 
real-time control. The control system consists of three 
parts—the servo-control part of the robot actuators, control 
decision-maker part, and image-processing part. For each 
part, one circuit board is designed and the number of boards 
is based on the demands of each part in the control system. 
All boards can directly communicate with each other 
through the DMA circuit. 
3.1 Design of Motor Board for Robotic Servo Control 
The motor board design employs Texas Instrument (TI) 
F2812 32-bit fixed-point digital signal processor (DSP) as 
the main processor. The F2812 processor has an external 
memory interface (EMIF), which is used as an interface that 
acquires digital data from peripherals. In the motor board 
interface, this work developed five decoders for reading 
signals from the motor encoders, 16-bit digital input/output 
(DI/DO) for obtaining external digital information, and one 
128KB DP-SRAM port that can be used for communication 
with other circuit boards.  
The driver board has a power PWM module, which 
drives the DC servomotors and generates the motor current 
signal. The motor board facilitates torque control of each 
motor by generating PWM signals and receiving the current 
signal from the driver board.  
3.2 Design of Processor Board for the Decision Maker 
Part
For the application of the humanoid robot, a processor 
board was developed based on the principle of multi-board 
configuration and modularity. All data-processing and 
control algorithms, such as that for robot dynamics and the 
fuzzy sliding-mode controller, can be realized on this board. 
The processor board uses one TI C6713 32-bit floated-point 
DSP as the main kernel. The TI C6713 DSP has a 200-MHz 
clock rate, 192 KB L2 SRAM and 64 KB Cache. The C6713 
DSP offers an EMIF that supports a glueless memory 
architecture interface, such as FIFO or SDRAM, and an 
enhanced DMA (EDMA) controller, which can be used to 
move a block of memory from its current address to desired 
locations.  
The peripheral interface of the processor board has a 
16-bit DI/DO for obtaining external digital information and 
one COM port RS232 interface that is utilized for 
communication with the UART standard device. To achieve 
real-time feedback control of the entire robot, a large 
amount of data must be transmitted between the boards. 
Thus, one 128KB DP-SRAM port is used for 
communication between other circuit boards.  
control structure of the humanoid robot. The control 
structure involves the application of self-organizing 
invertible map (SOIM) [10], grey prediction [5], and 
Kohonen neural network [6]. The SOIM neural network is 
utilized to acquire a calibration-free spatial representation of 
3D targets. The Kohonen neural network is applied to obtain 
the mapping from incremental changes in spatial 
representation to incremental changes in space 
configurations of wrist on the robot arm. To predict the 
trajectory of a moving target, grey prediction is utilized.  
Fig. 8  Control structure of hand-eye coordination of the 
humanoid robot 
In hand-eye coordination, the images from cameras must 
be processed to obtain visual inputs. Unnecessary 
background must be removed to reduce image-processing 
load. Sophisticated image processing is applied to remove 
unnecessary background for rapid preprocessing an entire 
image and determine the location of a target object in the 
image plane. First, an image sized 2360 240( )pixelsu  is 
obtained from cameras and down-sampled to half of the size 
of the original ( 2180 120( )pixelsu ) image. Then, the whole 
image is thresholded using the keycolor for the binary image. 
The noise is eliminated by a median filter. The blob of the 
image is then analyzed and the center coordinates of the 
target blob are calculated. Because these center coordinates 
are roughly obtained, they are recalculated to achieve 
accurate center coordinates for the original image based on 
the previous rough coordinates. Finally, the center 
coordinates of the target object can be obtained in the image 
plane with 60Hz.  
6. Experimentation  
Since this robot is to play ping-pong with a human, the 
centers of the ping-pong paddle and ball are chosen as 
tracking targets in the hand-eye coordination to simplify 
image processing. Fig. 9 shows the experimental setup. 
The SOIM network and Kohonen neural network must 
be trained before the hand-eye coordination control is used 
in this experiment. In the training stage for the SOIM 
network, an orange ping-pong ball hangs on a line in front 
of the binocular head. The images of the target are captured 
from two CCD cameras with a wide view and processed in 
the processing subsystem to acquire the center coordinates 
of the target in the image planes. The errors for all three 
components of the spatial representation converge to within 
0.03. Furthermore, the Kohonen neural network must also 
be trained before being implemented in the control structure. 
The size of working space for the Kohonen neural network 
was determined based on the reachable range of robot arm 
and observable range of the binocular head. Once the 
working space of network was determined, the network 
randomly generated the target for training conditions. The 
working space in this study is the region with X (400mm) u
Y (300mm) u Z  (300mm). In this region, the robot can 
drive the arm into the observable range of the two cameras 
on the binocular head. Variation in learning trajectory is 
extreme initially, but converges to a value after 4000 
learning steps.  
Left camera with 
wide view 
Right camera 
with wide view
Left camera with 
narrow view 
Right camera 
with narrow 
view 
Fig. 9  Views from the four cameras (top) and 
experimental setup (bottom) 
In the tracking experiment, the ping-pong ball moved as 
a pendulum. The binocular head and robot arm started 
tracking the target at its initial location. The paddle, which is 
driven by the robot arm, must be kept within the observable 
range of the binocular head and follow the ball when the 
eyes perform active visual tracking of the target.  
7. Conclusion and Future Work 
This work developed a wheeled humanoid robot 
consisting of a wheeled mobile base, a torso with a 3-DOF 
waist mounted on a mobile base, two 7-DOF robot arms, 
two 7-DOF robot hands and a 7-DOF robotic binocular head. 
A stand-alone real-time embedded control system was also 
designed as the control system for the humanoid robot. The 
servo controller for the robot and hand-eye coordination 
were used to verify the performance of the robot. 
Experimental data indicate that the developed humanoid 
robot successfully tracked the target via the proposed control 
strategy.  
