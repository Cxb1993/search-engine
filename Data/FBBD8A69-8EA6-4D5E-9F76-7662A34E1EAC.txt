本計畫共發表兩篇期刊論文以及九篇會議論文，如下所列： 
 
期刊論文 
 Y.-H. Yang and H.-H. Chen, “Machine recognition of music emotion: a review,” 
ACM Transactions on Intelligent Systems and Technology, vol. 3, no. 3, May 
2012.  
 Y.-C. Lin, Y.-H. Yang, and H.-H. Chen*, “Exploiting online tags for music 
emotion classification,” ACM Transactions on Multimedia Computing, 
Communications, and Applications, vol. 7s, no. 1, Oct. 2011. 
 
會議論文 
 J.-C. Wang, Y.-H. Yang, H.-M. Wang, and S.-K. Jeng, "The Acoustic Emotion 
Gaussians model for emotion-based music annotation and retrieval," Proc. ACM 
Multimedia (MM), full paper (accept rate=20%), 2012. 
 J.-Y. Liu, C.-C. Yeh, Y.-C. Teng, and Y.-H. Yang, “Bilingual analysis of song 
lyrics and audio words,” Proc. ACM Multimedia (MM), short paper (accept 
rate=31%), 2012. 
 Y.-H. Yang, “On sparse and low-rank matrix decomposition for singing voice 
separation,” Proc. ACM Multimedia (MM), short paper (accept rate=31%), 2012. 
 J.-C. Wang, Y.-H. Yang, H.-M. Wang, “The audiovisual emotion Guassians 
model for automatic generation of music video,” Proc. ACM Multimedia (MM), 
Grand Challenge solution paper, 2012. 
 J.-Y. Liu and Y.-H. Yang, “Inferring personal traits from music listening history,” 
Proc. Int. Workshop on Music Information Retrieval with User-Centered and 
Multimodal Strategies (MIRUM), 2012. 
 J.-C. Wang, Y.-H. Yang, K.-C. Chang, H.-M. Wang, and S.-K. Jeng, “Exploring 
the relationship between categorical and dimensional emotion semantics of 
music,” Proc. Int. Workshop on Music Information Retrieval with User-Centered 
and Multimodal Strategies (MIRUM), 2012. 
 Y.-H. Yang and X. Hu, “Cross-cultural music mood classification: A comparison 
of English and Chinese songs,” Proc. Int. Conf. Music Information Retrieval 
(ISMIR) (accept rate=44%), 2012. 
 C.-C. Yeh and Y.-H. Yang, “Supervised dictionary learning for music genre 
classification,” Proc. ACM Int. Conf. Multimedia Retrieval (ICMR), 2012. 
 Y.-H. Yang, D. Bogdanov, P. Herrera, and M. Sordo, "Music retagging using 
label propagation and robust principal component analysis," Proc. Int. Workshop 
on Advances in Music Information Research (AdMIRe), in conjunction with Int. 
World Wide Web Conference (WWW), 2012. 
26:2 • Y.-C. Lin et al.
services [Cheng et al. 2008]. Since songs are often created to express feelings, it is a sensible way to
organize and retrieve music by emotion [Meyer 1956; Picard 1997; Huron 2000; Lee and Downie 2004;
Lewis et al. 2008; Yang and Chen 2011a], for which emotion classification is a critical element.
A typical approach to music emotion classification (MEC) gathers the ground truth of music emotion
by subjective test and applies machine learning to train a classifier for automatically recognizing the
emotion of a music piece by audio features, such as Mel-frequency cepstral coefficient (MFCC). How-
ever, the required subjective test is labor-intensive and time-consuming. A subject needs about one
minute to annotate a song on the average [Yang et al. 2006]. This makes large-scale ground truth col-
lection practically difficult. Therefore, most existing work only uses a small dataset for training [Feng
et al. 2003; Li and Ogihara 2003; Lu et al. 2006; Yang et al. 2008; Trohidis et al 2008].
In recent years, the online music services and social tagging have created abundant music tags that
describe a music piece with semantically meaningful words and allow music pieces to be found by
keyword search. For example, more than 1.5 million tags are found in the professional review website
All Music Guide (AMG)1 and over 40 million tags in the social tagging website Last.fm2 [Lamere 2008].
These freely available tags contain useful information such as emotion, genre, theme, instruments, and
artists of music. It is therefore our goal to use such a rich source of semantic descriptions of songs as
the training data for automatic music classification.
A large number of emotion descriptions exist in the online repository of music tags. However, it is
noted that the taxonomy of emotion descriptions used by the online taggers is imbalanced. For example,
the negative examples of an emotion class may outnumber the positive ones, or one class (the majority
class) may be represented by a large number of examples while the others are represented by only
a few. Such a class imbalance causes machine learning to bias the majority examples and affects the
final classification performance. Therefore, we need an effective scheme to counteract the bias.
A popular approach to the class-imbalance problem is data sampling, which changes the training
sets by sampling a smaller majority training set or by repeating instances in the minority training set
[Drummond and Holte 2003; He and Garcia 2009; Liu et al. 2009]. Though data sampling has shown
success in several applications, this approach ignores the fact that an unknown sample is more likely
to be labeled with the majority class. In this article we develop a novel method that integrates such
information into the original data-sampling scheme.
In addition to the size of the training dataset (which is related to the generality of the training
instances), another factor that impedes the progress of MEC is the so-called semantic gap between the
object feature level and the human cognitive level of emotion perception [Laurier et al. 2008; Huq et al.
2010; Yang and Chen 2011a]. Existing audio features can only characterize the power spectrum of a
music piece but fail to represent the emotion perceived by humans, which makes emotion classification
even more challenging.
Genre, by which a song is classified into classical, jazz, hip-hop, etc., is closely related to emotion.
From a musicological perspective, genre is a constituent of conventions and expectations that influence
the way music is composed and perceived [Hanks 1987; Beard and Gloag 2005]. That is, although there
are various performing techniques to express an emotion, songs of the same genre are usually sung
by the same performing technique. For example, country singers often sing with a smooth and tender
voice to express a sad emotion, while rock bands may shout loudly when expressing the sad emotion.
Because this genre-specific characteristic of songs serves as a good precursor of emotion classification
and because most music websites provide the genre metadata of music pieces, it is plausible to exploit
1Last.fm. [Online] http://www.last.fm/.
2All Music Guide. [Online] http://www.allmusic.com/.
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:4 • Y.-C. Lin et al.
Table I. Five Emotion Clusters Adopted in MIREX
Cluster Description
1 Rowdy, Rousing, Confident, Boisterous, Passionate
2 Amiable/Good natured, Sweet, Fun, Rollicking, Cheerful
3 Literate, Wistful, Bittersweet, Autumnal, Brooding, Poignant
4 Witty, Humorous, Whimsical, Wry, Campy, Quirky, Silly
5 Volatile, Fiery, Visceral, Aggressive, Tense/Anxious, Intense
Table II. Statistics of the Dataset
All songs Blues Country Jazz R&B Rap Rock
# albums 1036 168 179 160 178 176 175
# songs 7922 972 1660 710 1307 1613 1660
Avg. similarity 0.32 0.62 0.56 0.57 0.49 0.47 0.41
However, as a retrieval system with limited emotion vocabulary may not satisfy the user requirement
in real-world music retrieval applications [Juslin and Laukka 2004], we choose to use all the 183 emo-
tion classes of AMG instead of grouping or removing the emotion classes.
Note that the use of genre-first classification for music emotion classification has also been shown
effective (independently) in Kim and Larsen [2010]. Our work differs from this work in that we in-
vestigate the classification of 183 emotion classes using the online repository of music tags, whereas
only 16 emotion classes are considered in Kim and Larsen [2010]. In addition, our work deals with the
challenging multilabel classification problem, as opposed to the single-label multi-class classification
in Kim and Larsen [2010].
3. ONLINE MUSIC TAGS
In this section, we build a dataset of online music tags and construct the emotion taxonomy from
the music tags. We then use the dataset to validate the genre-specific characteristic of emotion and
demonstrate the imbalance of the taxonomy.
3.1 Data Collection
Music tags are available from social tagging Web sites or professional editing websites. Because the
tags from social tagging Web sites are noisy, we use the tags edited by the experts of AMG to build a
dataset composed of the genre and emotion tags. Note that because AMG assigns tags to each album
instead of each song, we propagate the album-level tags to all the songs of the album and use the
approximated song-level tags as ground truth to train the MEC system.
The dataset contains 183 emotions and six mainstream genres: blues, country, jazz, R&B, rap, and
rock. Because an albummay contain songs of various emotions, an album is labeled with 12.62 emotion
classes on the average. The audio files of each album are obtained by using a YouTube crawler [Shah
2008].4 After querying YouTube with the title and artist name of each song, we obtain a dataset of
1,036 albums, which contain 7,922 songs in total. See Table II for a summary of the dataset.
Figure 1 shows the percentage of positive songs of each emotion class in the dataset. We can see that
about three-fourths of the classes are represented as positive ones by fewer than ten percent of the
songs. Clearly, the taxonomy of emotion classes generated by the online tagging system is imbalanced.
A data-sampling method to counteract such bias is described in Section 4.
4Youtube. [Online] http://www.youtube.com/.
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:6 • Y.-C. Lin et al.
Fig. 2. System diagram.
whether ds is annotated with the emotion class j, and E is the number of emotion classes. A multilabel
classifier H(·) is created by minimizing the mismatch ε between the ground truth and the estimate
[Joachims 2004],
ε = 1|D|
|D|∑
s=1
(ys, H(xs)), (2)
where the hamming distance (·) counts the number of emotion classes of the ground truth that are
different from those of the estimate. Motivated by the fact that ε can also be computed by summing
the error of each emotion class,
ε = 1|D|
|D|∑
s=1
E∑
j=1
(ysj, hj (xs)), (3)
a widely used approach to train a multilabel classifier H(·) is to split it into E binary classifiers
{h1(·), h2(·), . . . , hj(·), . . . , hE(·)}. The emotion of an input song d∗ is then predicted as yˆ∗ = [yˆ∗1, yˆ∗2, . . . ,
yˆ∗ E]T , where yˆ∗ j = hj(x∗ ).
In this study, we use support vector machines (SVMs) to train the binary classifier hj(·) for its su-
perior performance shown in many emotion classification systems [Feng et al. 2003; Li and Ogihara
2003; Yang et al. 2008]. SVMs nonlinearly map input feature vectors xs to a higher dimensional space
by the so-called kernel trick ϕ(xs), and find a decision boundary hj(xs) = wT ϕ(xs) + θ that separates
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:8 • Y.-C. Lin et al.
Fig. 3. Illustration of the proposed threshold-shifting method. The dotted line is the decision boundary of the classifier trained
by the sampled negative examples and the positive examples, whereas the solid line is the decision boundary after threshold-
shifting.
the minority examples are smaller in quantity and shifts the decision boundary closer to them, this
issue can be resolved.
Motivated by the above observation, a novel threshold-shifting method is proposed. We first record
the number of negative examples before under-sampling is applied. After the decision boundary hj(xs) =
sgn(
∑T
t=1 ft(xs)) is learned, the location of the decision boundary is shifted by δ such that a percentage
of |P|/(|P| + |N|) examples in a validation set would be classified as the minority class. That is, the
new decision boundary becomes,
hj(xs) = sgn
(
T∑
t=1
ft(xs) + δ
)
. (4)
Note that because the proposed threshold-shifting method does not change any procedure of the learn-
ing algorithm and the data-sampling process, it can be easily incorporated into any existing method
for class-imbalance learning.
5. PERFORMANCE EVALUATION
An extensive experiment is conducted to evaluate the performance of the proposed system using the
dataset described in Section 3. We first describe the setup of the experiment and then discuss the
results.
5.1 Experimental Setup
For fair comparison, each song is converted to a uniform format (22,050Hz, 16 bits, and mono channel
PCM WAV) and normalized to the same sound level. In addition, we extract the 30-second segment
starting from the initial 30th second of the song and regard the classification result of the segment as
that of the entire song. This is a common practice in music classification [Scaringella et al. 2006].
After preprocessing, we then perform feature extraction by using the music processing system
Marsyas [Tzanetakis and Cook 2000], which generates 68 timbral textual features, 48 pitch content
features, and 8 rhythmic content features.5
5We have employed principal component analysis [Duda et al. 2000] for feature selection but found the result not improved
much.
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:10 • Y.-C. Lin et al.
Table IV. Experimental Result for Classifying 183 Emotion Classes
Classifier # layer Precision Recall F-score # tags per album
Random guess 0.07 0.59 0.12 106.79
SVM 0.09 0.02 0.03 0.28
SVM+EE One 0.12 0.63 0.20 69.24
SVM+EE+TS 0.30 0.19 0.23 7.57
SVM 0.51 0.18 0.27 3.30
SVM+EE Two 0.14 0.59 0.23 54.75
SVM+EE+TS 0.38 0.35 0.36 11.21
EE and TS represent EasyEnsemble and threshold-shifting, respectively.
5.2 Data Sampling and Threshold Shifting
We first compare the performance of the MEC system using three different learning strategies: SVM,
SVM + EasyEnsemble (SVM + EE), and SVM + EasyEnsemble + threshold-shifting (SVM + EE +
TS). The MEC system with the first learning strategy serves as the baseline system that trains a multi-
label classifier without data sampling. With the second learning strategy, the MEC system trains the
classifier with a balanced dataset. The third learning strategy enables the MEC system to further
use the proposed threshold-shifting method to modify the decision boundary according to the prior
probability of each emotion class (positive or negative). The implementation of SVM is based on the
LIBSVM library [Chang and Lin 2001], along with a grid parameter search to find the best parameters.
Empirically, we set C to 100 and γ to 0.1 and use the default values for other parameters. Finally, we
add a random guess system for performance comparison purpose; for each emotion class the system
tosses a fair coin to determine whether its label is positive.
The top four rows in Table IV show the evaluation results. Due to the severe imbalance of online
taxonomy, the SVM predicts almost every album being associated with no classes, and all evaluation
measures are nearly zero. The performance of SVM is even worse than the random guess system,
which attains a recall rate of about 0.5 and thereby a higher f-score 0.12 because of the random guess
mechanism. By using the under-sampling method, the f-score is greatly improved from 0.03 to 0.20.
However, we observe that although the recall is high, the precision is fairly low and the too many tags
are associated with each album. As described in Section 3.1, in our dataset each album is associated
with 12.62 tags on the average, but using SVM+EasyEnsemble the average number of tags per album
becomes 69.24. Therefore, we consider such a system not useful in practice.
As the fourth row of Table IV shows, the proposed threshold-shifting method serves as a good solution
to avoid the error made by EasyEnsemble; the average number of tags applied to each album is reduced
to a reasonable number of 7.57, and the precision is greatly improved from 0.12 to 0.30. We can see
that the combination of EasyEnsemble and threshold-shifting leads to a better emotion model out of
an imbalanced dataset.
5.3 Two-Layer Structure of Emotion Classification
Two structures of emotion classification are compared here. The “single-layer” structure refers to the
structure of conventional systems that train the emotion classifier with all songs, while the “two-layer”
structure refers to the structure of the proposed system that performs genre grouping prior to emotion
classification. Both structures are implemented along with different learning strategies described in
Section 5.2 for performance evaluation.
The bottom three rows in Table IV show the evaluation results. As the class imbalance is not a prob-
lem for the subsets of genres, the SVM under the two-layer emotion classification structure achieves a
medium high f-score 0.27. Similarly, we observe that the use of EasyEnsemble alone would result in too
many tags per album and a significant reduction of the precision rate. By incorporating the proposed
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:12 • Y.-C. Lin et al.
Table V. Classification Result for the
Five-Class MIREX Emotion Taxonomy
System Precision Recall F-score
Random guess 0.39 0.54 0.45
One-layer 0.58 0.61 0.59
Two-layer 0.61 0.65 0.63
Fig. 5. F-score of emotion classification for songs of different genres.
outperforms the single-layer structure for most emotion classes. Note that the performance of the two-
layer system is reasonably well even for the rare emotion classes (e.g., emotion classes 136–183).
Figure 5 shows the f-score of the two structures equipped with data-sampling and threshold-shifting
for emotion classification of the songs grouped by genre. It can be found the proposed two-layer struc-
ture greatly outperforms the conventional one-layer structure. In addition, we observe a positive cor-
relation between the performance of the two-layer structure and the average similarity of songs in
each genre expressing the same emotion (cf. Table II). For example, the f-score of emotion classifica-
tion for rock songs is the lowest among the six genres, possibly because the relatively lower acoustic
homogeneity of rock songs.
Finally, we evaluate the performance of the proposed system for classifying the five emotion classes
adopted by MIREX (cf. Table I) using our dataset. The emotion annotation is mapped as follows: An
album is assigned to an emotion cluster if its labeled emotion class belongs to the emotion cluster
[Hu et al. 2008]. Because the class imbalance problem is not severe for the MIREX taxonomy (the
number of positive samples for the five classes are 5299, 3988, 3794, 2337, and 1955, respectively),
we do not employ data-sampling techniques in this experiment. As Table V shows, both the one-layer
and two-layer structures outperform the random guess baseline, and the two-layer structure achieves
the highest performance of 0.63 in f-score. These results show that the two-layer structure is indeed
effective.6
Note that because MIREX does not consider multilabel classification and because our dataset is dif-
ferent from the one used in MIREX, it is difficult to directly compare our results with those reported in
the MIREX contest. However, it should be fair to say that our performance is comparable to that of the
6Note because we are dealing with multi-label classification problem instead of multiclass classification, the baseline perfor-
mances of 183-class classification and 5-class classification are not 1/183 and 1/5. Our result shows that the performance of the
MIREX emotion classes is similar to that of the classes with more positive examples among the 183-class taxonomy (cf. the
upper subfigure of Figure 4). The f-score of the five MIREX classes are 0.75, 0.59, 0.66, 0.57, and 0.43, respectively.
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:14 • Y.-C. Lin et al.
7. CONCLUSION
In this article, we have presented a system that exploits online music tags for emotion classification.
We first use the online emotion tags to construct a large-scale ground truth dataset with a fine granu-
larity of emotion classes (183 total). We show that the class-imbalance problem inherent to the online
taxonomy can be effectively resolved by the proposed data-sampling method. We then use the on-
line genre tags as a precursor of emotion classification and create a two-layer classification structure,
which enables us to reduce the diversity of songs of the same emotion and improve the average f-score
of emotion classification from 0.23 to 0.36.
APPENDIX
The 183 emotion classes studied in this article are listed in Table VI in descending order of the number
of positive songs.
Table VI. Emotion Classes
1 Confident 38 Intensive 75 Irreverent 112 Sleazy 148 Brassy
2 Amiable 39 Wistful 76 Rebellious 113 Angst-Ridden 149 Paranoid
3 Playful 40 Refined 77 Rambunctious 114 Reserved 150 Spring-like
4 Earnest 41 Lively 78 Urgent 115 Weary 151 Malevolent
5 Reflective 42 Confrontational 79 Joyous 116 Rustic 152 Unsettling
6 Passionate 43 Soothing 80 Summery 117 Gloomy 153 Knotty
7 Rousing 44 Rowdy 81 Wry 118 Precious 154 Wintry
8 Intimate 45 Slick 82 Ambitious 119 Hedonistic 155 Spacey
9 Bittersweet 46 Raucous 83 Enigmatic 120 Delicate 156 Nihilistic
10 Energetic 47 Elegant 84 Whimsical 121 Bleak 157 Difficult
11 Stylish 48 Witty 85 Restrained 122 Snide 158 Fierce
12 Freewheeling 49 Cerebral 86 Exciting 123 Cold 159 Freakish
13 Sentimental 50 Autumnal 87 Detached 124 Sprawling 160 Meandering
14 Sophisticated 51 Searching 88 Spiritual 125 Self-Conscious 161 Lazy
15 Poignant 52 Brooding 89 Humorous 126 Innocent 162 Naı¨ve
16 Earthy 53 Quirky 90 Ominous 127 Reckless 163 Spicy
17 Laid-back 54 Theatrical 91 Cynical 128 Gleeful 164 Spooky
18 Organic 55 Sweet 92 Hostile 129 Greasy 165 Hungry
19 Street-smart 56 Cheerful 93 Gusty 130 Indulgent 166 Uplifting
20 Plaintive 57 Fiery 94 Soft 131 Ironic 167 Fractured
21 Brash 58 Gritty 95 Atmospheric 132 Outrageous 168 Brittle
22 Literate 59 Thuggish 96 Light 133 Silly 169 Suffocating
23 Yearning 60 Visceral 97 Druggy 134 Campy 170 Pastoral
24 Melancholy 61 Cathartic 98 Eerie 135 Manic 171 Nostalgic
25 Gentle 62 Bright 99 Sad 136 Ramshackle 172 Tender
26 Fun 63 Nocturnal 100 Elaborate 137 Outraged 173 Clinical
27 Aggressive 64 Volatile 101 Bitter 138 Sparkling 174 Hopeful
28 Party 65 Carefree 102 Trippy 139 Messy 175 Austere
29 Rollicking 66 Relaxed 103 Lush 140 Sparse 176 Trashy
30 Swaggering 67 Menacing 104 Angry 141 Happy 177 Stately
31 Exuberant 68 Uncompromising 105 Dreamy 142 Distraught 178 Circular
32 Smooth 69 Tense 106 Acerbic 143 Eccentric 179 Thoughtful
33 Boisterous 70 Provocative 107 Sexual 144 Epic 180 Effervescent
34 Warm 71 Bravado 108 Calm 145 Ethereal 181 Giddy
35 Romantic 72 Harsh 109 Reverent 146 Sardonic 182 Crunchy
36 Sensual 73 Complex 110 Somber 147 Insular 183 Sugary
37 Dramatic 74 Sexy 111 Hypnotic
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
26:16 • Y.-C. Lin et al.
LU, L., LIU, D., AND ZHANG, H. 2006. Automatic mood detection and tracking of music audio signals. IEEE Trans. Audio, Speech
Lang. Proces. 14, 1, 5–18.
MANDEL, M. AND ELLIS, D. P. W. 2008a. A web-based game for collecting music metadata. J. New Music Res. 37, 2, 151–165.
MANDEL, M. AND ELLIS, D. P. W. 2008b. Multiple instance learning for music information retrieval. In Proceedings of the Interna-
tional Conference on Music Information Retrieval. 577–5.
MEYER, L. B. 1956. Emotion and Meaning in Music. University of Chicago Press.
PICARD, R. W. 1997. Affective Computing. The MIT Press.
RENTFROW, P. J. AND GOSLING, S. D. 2003. The Do Re Mi’s of everyday life: The structure and personality correlates of music
preferences. J. Person. Soc. Psych. 84, 1236–1256.
SCARINGELLA, N., ZOIA, G., AND MLYNEK, D. 2006. Automatic genre classification of music content: a survey. IEEE Sign. Process.
Mag. 23, 2, 133–141.
SHAH, C. 2008. TubeKit: A query-based Youtube crawling toolkit. In Proceedings of the IEEE/ACM Joint Conference on Digital
Library.
SLANEY, M., ELLIS, D. P. W., SANDLER, M., GOTO, M., AND GOODWIN, M. M. 2008. Introduction to the special issue on music
information retrieval. IEEE Trans. Audio Speech Lang. Proces. 16, 2, 253–254.
SCHULLER, B., HAGE, C., SCHULLER, D., AND RIGOLL, G. 2010. Mister, D.J., Cheer Me Up!’: Musical and textual features for
automatic mood classification. J. New Music Res. 39, 1, 13–34.
TROHIDIS, K., TSOUMAKAS, G., KALLIRIS, G., AND VLAHAVAS, I. 2008. Multi-label classification of music into emotions. In Proceedings
of the International Conference on Music Information Retrieval. 325–330.
TSOUMAKAS, G. AND VLAHAVAS, I. 2007. Random k-labelsets: an ensemble method for multilabel classification. In Proceedings of
the European Conference on Machine Learning. 406–417.
TURNBULL, D., LIU, R., BARRINGTON, L., AND LANCKRIET, G. 2007. A game-based approach for collecting semantic annotations of
music. In Proceedings of International Conference on Music Information Retrieval. 535–538.
TZANETAKIS, G. AND COOK, P. 2000. Marsyas: A framework for audio analysis. Orga. Sound 4, 3, 169–175.
TZANETAKIS, G. AND COOK, P. 2002. Musical genre classification of audio signals. IEEE Trans. Speech Audio Process. 10, 5, 293–
302.
VON AHN, L. AND DABBISH, L. 2004. Labeling images with a computer game. In Proceedings of the ACM SIGCHI Conference on
Human Factors in Computing Systems. 319–326.
WIECZORKOWSKA, A., SYNAK, P., AND RAS´, Z. W. 2006. Multi-label classification of emotions in music. In Proceedings of the Intelli-
gent Information Processing and Web Mining. 307–315.
YANG, Y.-H., LIU, C.-C., AND CHEN, H. H. 2006. Music emotion classification: a fuzzy approach. In Proceedings of the ACM
International Conference on Multimedia. 81–84.
YANG, Y.-H., LIN, Y.-C., SU, Y.-F., AND CHEN, H. H. 2008. A regression approach to music emotion recognition. IEEE Trans. Audio
Speech Lang. Proces. 16, 2, 448–457.
YANG, Y.-H., LIN, Y.-C., LEE, A., AND CHEN, H. H. 2009. Improving musical concept detection by ordinal regression and context
fusion. In Proceedings of the International Conference on Music Information Retrieval. 147–152.
YANG, Y.-H. AND CHEN, H. H. 2011a. Music Emotion Recognition, CRC Taylor & Francis Press.
YANG, Y.-H. AND CHEN, H. H. 2011b. Ranking-based emotion recognition for music organization and retrieval. IEEE Trans. Audio
Speech Lang. Process. 19, 4, 762–774.
Received September 2010; revised March 2011; accepted July 2011
ACM Transactions on Multimedia Computing, Communications and Applications, Vol. 7S, No. 1, Article 26, Publication date: October 2011.
 2
6/5 中午是和一名香港科技大學的博士生，叫做 Bin Wu，一起
吃飯。他對於我博士班期間所做的音樂情緒辨識很感興趣，因此
我們先前在網路上就認識並有討論過，這次來剛好遇他見面，並
有更深入的討論。我向他提到我在今年十月的 ISMIR(數位音樂檢
索領域年度最重要的國際會議)講演有關音樂情緒的 tutorial，可
以加入他做的東西到講題裡面。另外我們也對他的博士研究題目
交換了許多看法。 
6/5 下午也有兩場並行的演講，一場是講 3D 的、另一場是臺
大資工系的林智仁教授(http://www.csie.ntu.edu.tw/~cjlin/)主講的
Distributed Computing，我選擇參加林教授的。林教授是該領域非
常有名的學者，其 libsvm 廣受使用，我們之前即有認識。目前林
教授正在 eBay 訪問，所以能夠接觸到許多實務上的問題，其演講自然十分精彩。會後我邀請林教
授一起用晚餐，聽他分享他對於機器學習領域與雲端計算的看法。 
6/6 到 6/8 三天就是各個論文的發表。ICMR 的會議
發表是單一 session，所以講者與聽眾能夠有許多的交
流。在第一天發表的論文當中，許多都是基於影像處
理領域 bag-of-word model 的改進，正好與我和晉嘉近
來在做的研究相關。雖然我們是專注在音樂的領域發
展，但聽取這些影像領域的進展對我們來說也常常能
有許多啟發。會中遇到了去年去西班牙訪問的時候認
識的 Gerard Roma，他目前在 Universitat Pompeu Fabra
攻讀博士，做的也是音樂相關的研究。這次的會談確
定了一些暑假期間可以一起合作的目標。 
會中也結識了許多人，包含浙江大學的 Yu-Gang Jiang 教授、MSRA 的 Yang Cai 博士、Princeton
的 Zhe Wang 博士等等，不一一贅述。比較有意思的是與 Zhe Wang 深談了許久。Zhe Wang 是大陸
人，目前在 Princeton 做博士後。他談到他現正與大陸的一家廣播業者的合作，規模非常的大，題
目也很有趣。Zhe Wang 很關心音樂技術在業界的運用和商機，而且他人就在創業風氣興盛的美國，
所以他不但有許多人脈，對於什麼技術能夠應用的看法相對來說也比較準。跟他交談十分有趣。
當晚 reception 結束之後，與中正大學的朱威達教授已經同在資創服務的鄭文皇博士一起晚餐。 
6/7 早上的 keynote 十分精彩，INRIA 的 Cordelia 
Schmid 講演 Aggregating Local Image Descriptors for 
Large-scale Image Retrieval and Classification，演講中 
展示了 Fisher kernel 的效果。我在午餐後與主講音樂
tutorial 的 Masataka Goto (http://staff.aist.go.jp/m.goto/)
和 Markus Schedl(http://www.cp.jku.at/people/schedl/)教
授都有機會聊了很久，增進了彼此的認識。Goto 博士
可以說是在亞州音樂研究這一塊領域做得最好的，我
也很高興地邀請他明年初來台灣訪問，我將會向先請
邀請 Xaiver 教授一樣地為他舉辦 workshop。另外與 Markus Schedl 教授聊到最近我和他都有在做的
context-aware music recommendation 方面的研究，很高興越來越多人在研究這塊領域。 
 4
四、建議 
華人在多媒體領域其實滿活躍的，台灣的學者能見度也不低。此次行程當中發現許多結合音訊、
影訊分析有趣的題目，在國內外都較少被發展，或許這也是我們能做出巨大貢獻的一個方向。 
五、攜回資料名稱及內容 
大會議程電子檔與兩場 tutorial 之紙本講義。  
六、其他 
無。 
 
100 年度專題研究計畫研究成果彙整表 
計畫主持人：楊奕軒 計畫編號：100-2218-E-001-009- 
計畫名稱：運用社群媒體實現大規模音樂情緒辨識系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 9 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 2 100%  
博士生 0 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 2 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本計畫申請時為一兩年期之計畫。第一年之計畫內容主要為利用線上社群網站之資料所建
立之大規模情緒辨識系統，預計分辨一百八十類以上的情緒，並運用在上萬首歌曲之資料
庫上。第二年之計畫內容為研究聲紋辨識與雲端計算等技術，讓音樂情緒辨識可以在數秒
之內完成。由於後來計畫只有通過為一年期之計畫，故規劃於第二年度進行之研究並沒有
執行。原定於第一年執行之研究皆已執行完畢，並且產出超過預期的目標。共發表兩篇期
刊論文以及九篇會議論文，其中發表於 ACM Transactions on Multimedia Computing, 
Communications, and Applications 之’Exploiting online tags for music emotion 
classification,’即具體描述了本計畫之成果，其英文摘要如下。 此論文不僅成功使用
線上社群標籤來做為訓練資料，更提出新的演算法解決社群標籤資料不對稱之問題、以及
一以音樂曲風輔助之兩層式情緒辨識架構，在高達 183 類情緒類別、7,922 首歌曲的分類
問題當中，平均能夠有 0.36 的準確率(以 f-score 衡量)。 
 
英文摘要： 
The online repository of music tags provides a rich source of semantic descriptions 
useful for training emotion-based music classifier. However, the imbalance of the 
online tags affects the performance of emotion classification. In this paper, we 
present a novel data-sampling method that eliminates the imbalance but still takes 
the prior probability of each emotion class into account. In addition, a two-layer 
