recognition is a recent re-emerging speech 
recognition research. The latest research shows that, 
by constructing a universal phone set based on the 
mapping between phones of different languages and the 
manners or places of articulation, we can establish a 
universal phone recognizer, which is trained by 
existing phone labeled speech corpora of various 
languages. The universal phone recognizer can be used 
to recognize speech of languages which lack labeled 
training data. In this way, we can develop Min-nan 
and Hakka speech recognition systems or Mandarin/Min-
nan/Hakka/English multilingual speech recognition 
systems even though the labeled training data for 
Min-nan and Hakka speech are not available. In view 
of this, the objective of this sub-project is to 
develop multilingual speech recognition systems and 
spoken language recognition systems based on our 
experience on developing detection-based English 
phone recognition on the TIMIT corpus. 
 
英文關鍵詞： speech attributes, speech recognition, spoken 
language recognition, multilingual speech recognition
 
include multilingual speech recognition and 
speech attribute detection-based speech 
recognition and spoken language recognition. 
Speech attribute detection-based speech 
recognition is a recent re-emerging speech 
recognition research. The latest research 
shows that, by constructing a universal phone 
set based on the mapping between phones of 
different languages and the manners or 
places of articulation, we can establish a 
universal phone recognizer, which is trained 
by existing phone labeled speech corpora of 
various languages. The universal phone 
recognizer can be used to recognize speech 
of languages which lack labeled training data. 
In this way, we can develop Min-nan and 
Hakka speech recognition systems or 
Mandarin/Min-nan/Hakka/English 
multilingual speech recognition systems even 
though the labeled training data for Min-nan 
and Hakka speech are not available. In view 
of this, the objective of this sub-project is to 
develop multilingual speech recognition 
systems and spoken language recognition 
systems based on our experience on 
developing detection-based English phone 
recognition on the TIMIT corpus. 
 
Keywords: speech attributes, speech 
recognition, spoken language recognition, 
multilingual speech recognition 
 
二、計畫緣由與目的 
 
本計畫的研究課題橫跨自動語音辨識
(automatic speech recognition, ASR)研究的
三 個 重 要 主 題 ： 多 語 言 語 音 辨 識
(multilingual ASR)、基於語音屬性偵測之語
音 辨 識 (speech attribute detection-based 
ASR)及口語語言辨識 (spoken language 
recognition)。 
自動語音辨識技術發展了半個世紀，
中、英、日、德、法、西班牙及阿拉伯(911
恐怖攻擊事件之後美國積極投入)等主要
語言自動語音辨識技術已相當成熟，只要
有足夠的訓練語料就能建立高辨識率的語
音辨識系統。隨著全球化的風潮，不同語
言使用者間的口語溝通場合日益增加，引
發口語語言辨識和多語言語音辨識的需求，
進而衍生口語語言翻譯(spoken language 
translation)的發展，最著名的例子是 90 年
代初期開始的 C-STAR (The Consortium for 
Speech Translation Advanced Research)，結
合美、日、德、法、韓、義、中等國的研
究團隊，協力開發口語語言翻譯在觀光旅
遊方面的應用，包括行程安排、資訊查詢、
預訂服務及會談協商等。在口語語言辨識
方面，美國 NIST (National Institute of 
Standards and Technology)自 1996年起，積
極 推 動 語 言 辨 識 評 比 (Language 
Recognition Evaluation, LRE)，藉由提供標
準語料庫和相關基準(benchmarks)，帶動了
這個領域的發展。另一方面，單一地區內
多種語言，例如當地語言與國際通用語言
英語的交錯使用已相當普遍，但目前雙語
語音辨識系統的發展程度卻遠不及單語系
統；而方言因為本身資源不足，單語語音
辨識的發展已經極為有限，更遑論混合主
要語言與方言的雙語或多語語音辨識。針
對資源匱乏語言的處理的相關議題在國際
學術界已逐漸受到重視，相關的研討會繼
2006 年的 Workshop on Resource-Scarce 
Language Engineering，後續有Workshop on 
Spoken Languages Technologies for 
Under-Resourced Languages (SLTU2008, 
SLTU2010)，如何將過去在資源相對豐富
語言處理累積的經驗用到資源相對匱乏語
言的處理，達到資源共用，降低開發成本，
已成為重要的研究課題。 
基於語音屬性偵測之語音辨識是近年
重新興起的語音辨識研究方向。自動語音
辨識技術發展的歷史中，早期即是依據語
言學家的聲音分類知識，利用聲學與語言
學的規則，建立語音辨識的機制，採用知
識驅動(knowledge-driven)的解決方法。這
類 方 法 所 建 立 之 以 規 則 為 基 礎 的
(rule-based)語音辨識系統難以應付複雜的
變化，欠缺強健性(robustness)。在隱藏式
馬可夫模型(hidden Markov model, HMM)
被提出之後，以知識驅動的語音辨識方法
幾 乎 完 全 由 以 語 料 庫 為 基 礎 的
(corpus-based)語音辨識方法所取代。到了
90年代，許多由資料驅動(data-driven)的演
算方法被提出來，自動語音辨識技術有了
快速的進步，大詞彙連續語音辨識(large 
vocabulary continuous speech recognition, 
 2 
的使用，也能利用其與 IPA 一對一之對應
關係，以及簡單的 ASCII 符號表示法，來
簡化電腦系統的編碼方式。 
  現階段我們的 X-SAMPA 與其語音屬
性對應表已完成國語和英語部分，其音素
對應基本原則如下： 
  一、國語的部分，由於漢語拼音已是
華人世界較為普遍的國語拼音系統，我們
便參照北京大學中文系現代漢語教研室所
編著之《現代漢語》作為漢語拼音中各個
音素與 IPA 對應之主要根據。如此一來，
原本在國語注音符號中常見的雙元音，就
不需另建立音素，而是將它們拆成兩個單
一元音音素來表示。這樣不僅之後的語音
屬性能夠表達得更明確，也簡化了日後多
連音(multi-phone)辨識系統實作上的複雜
性。 
  二、英語的部分，我們主要是基於
DARPA 出版的 TIMIT 語料庫中所定義的
61個音素[2]，並參照[3, 4]來尋找其與 IPA
的對應關係。由於 TIMIT中具有 5個雙元
音（/aw/、/ay/、/ey/、/ow/、/oy/），為了
方便使用具人工音素邊界標記的 TIMIT語
料庫作為辨識系統的訓練語料，儘管這五
個雙元音都是由兩個單元音所構成，我們
仍不將其拆開，反而視其為單一元音音
素。 
  在找出了國、英語中所有音素的
X-SAMPA表示法後，我們便以[5, 6]為主，
[1]為輔，找出這些音素的發音方式與部位，
原則如下： 
  一、輔音音素的發音方式可分為塞音、
塞擦音、擦音、鼻音、近音、邊近音共六
種，發音部位可分為雙唇、唇齒、齒、齒
齦、齒齦後、齦顎、捲舌、硬顎、唇顎、
軟顎、聲門共十一種；而元音音素的發音
則主要依照舌位與唇形，可分為舌面前、
舌面次前、舌面央、舌面次後、舌面後、
閉、半閉、中、半開、開、圓唇共十一種。 
  二、為了區別許多具有相同發音方式
與部位的音素，我們也參照 IPA 的特殊符
號標記另加上有聲(voice)、送氣(aspiration)
與否的兩項屬性。 
  三、國語的部分，值得一提的是兩種
舌尖元音(Apicale)的處理，也就是我們在發
音『之』、『吃』、『施』、『日』、『資』、
『疵』、『斯』時所加上的兩種空韻母，
在X-SAMPA中的表示法分別為/z=/與/z`=/。
儘管它們具有諸如捲舌與否的輔音發音屬
性，我們仍參照《現代漢語》，將它們視
為元音，並找出所在的舌位與唇形。 
  四、英語的部分，對於 TIMIT 中的 4
個音節主音(syllabics)（/em/、/en/、/eng/、
/el/），由於主要參考資料[5, 6]並未特別針
對它們說明發音方式與部位，因此我們根
據論文[7]中關於TIMIT與SPE的對應表格，
將它們的語音屬性分別設定為與/m/、/n/、
/ng/、/l/相同。此外，對於 TIMIT中的 6個
聲門閉鎖音(glottal closures) （/bcl/、/dcl/、
/gcl/、/kcl/、/pcl/、/tcl/），我們的處理亦
同上，即將其發音屬性分別關聯於/b/、/d/、
/g/、/k/、/p/、/t/。最後，在視為單一音素
的雙元音部分，我們也是參照[7]為它們訂
出舌位與唇形。 
  元音與輔音的對應表格如表一和表二，
其中每一欄位均代表同一個音素，由上而
下的表示法依序為 X-SAMPA、IPA、注音
符號、TIMIT，而『*』則表示沒有相對應
的表示法。表一中的『V』代表有聲，則『Vl』
代表無聲，而X-SAMPA符號中若有『_h』，
則代表具送氣屬性。儘管我們發現，許多
音素（特別是元音）都具有相同的發音方
式與部位，之後將待辨識器完成後，才能
根據辨識率修正這些發音屬性的重要性或
是加上其他的發音屬性。 
  最後，在通用音素集(universal phone 
set)的整併方面，由於具有人工音素及音素
邊界標記的語料非常有限（我們目前只能
取得 TIMIT與 OGI多語言口語語料庫），
通用音素集必須以語料庫中出現過的音素
為限，而其他未被使用或罕用的音素，則
依據其發音方式與發音部位、或是統計模
型的機率距離予以合併。合併後的通用音
素集大小為 69 個，其中包含元音 22 個與
輔音 47個。對應表格如表三和表四，其中
國語的音素系統參考漢語拼音，台語和客
語是通用拼音，而英語的音素系統則採用
TIMIT語料庫的標音系統(TIMITbet)。 
 
(3) 建置基於語音屬性偵測之通用音素辨
識系統及單語音素辨識系統 
 
 4 
下幾點： 
  一、如何更精簡化的表示 G？我們發
現，整個 WFST 的大小主要決定在 G，而
G的表示法攸關整個WFST的運作效能。 
  二、在 WFST 的運算方面，由於我們
發現建立離線的 WFST 圖形的運算時間主
要在於 Det()與 Push()，因此這兩個函數的
演算法亦值得更有效率的改進。 
  三、未來我們會進一步將整個多連音
(multi-phone)的資訊實作於WFST中。 
  此外，我們發現最近微軟公司
(Microsoft)建置了一個名為 Kaldi [8]，以
WFST 為基礎的語音辨識工具箱。此工具
箱整合了目前在前端與後端語音處理上最
先進的方法，方便語音處理研究者使用。
我們已開始著手評估將目前的建置成果與
新工具箱整合的可行性。 
 
(5) 建置基於子空間特徵表示與學習的口
語語言辨識系統 
 
在眾多自動語言辨識的方法中，較直覺且
具有語言意涵的要算是語音組合法
(phonotactics)。語音組合法是語音學的其中
一個分支，研究在某一種語言中的語音組
合法則與限制(phonotactic constraints)。舉
例說，在國語中，/j/、/q/、及/x/之後只能
跟 /i/和/ü/，而不能跟 /a/、/o/或/e/等元音；
而在英語裡，輔音叢集最多只能由兩個輔
音組成，除非第一個輔音是/s/音，則其後
可容許符合特殊組合的輔音群。在相近的
語言，相同的語音組合法則未必相通，例
如：在現代英語中，/kn/及/ɡn/在字詞的開
首是不容許的，但在同系的德語及荷語中
卻是允許的。 
  目前在語音處理社群中，最常見的基
於語音組合法的自動語言辨識方法是利用
向量空間模型(vector space model, VSM)[9]，
將每個語句的語音組合情形表示為根據所
有單連詞與二連詞出現的頻率所組成的向
量，再利用支持向量機 (support vector 
machine, SVM)進行分類。由於 N連詞的表
示方法需要利用大量的儲存空間，因此這
種方法最大的缺點在於只能在每個語句中
捕捉最多二連詞的語音組合情形。 
  因此，我們提出了基於子空間的語句
表示法，其優點有二：一、它可以充分使
用音素辨識器所提供的資訊，包括每個音
素在每個音素段落中的事後機率，而非單
單的出現次數與頻率；二、它利用向量串
接的方式，能捕捉到更長距離的語音組合
情形以及前後文資訊，而需要的空間比
VSM少了許多。 
  在語音組合特徵的抽取與串接上，如
圖五，音素辨識器會先產生一條最佳的音
素串列與邊界（如𝑢1,𝑢2,𝑢1, … , 𝑢4），據此
邊界，我們可以求得所有音素在每個音素
段落(segment)的事後機率，形成向量串列
𝐬1, … , 𝐬𝐾。因此，每個向量均可視為一段『聲
音』的表示法，而其中的數值代表了各個
音素發生的可能性。接著，藉由向量的前
後串接，形成了超級向量(supervector)，此
向量涵蓋了此段『聲音』與其前後『聲音』
的關係資訊，進而具有描述語音組合的特
性。以前後各串接一個向量的超級向量為
例，其涵蓋的前後文資訊相當於三連詞的
表示法，而所占有的空間大小卻只要原來
空間大小的三倍，而 VSM的三連詞卻需要
原來空間（單連詞）大小的三次方。 
  由於每段語句的音素串列長度並不一
致，不同語句的矩陣（其列數與行數分別
為前述超級向量的維度(亦即通用音素個
數乘上前後串接向量數)與音素串列長度）
表示的行數將不同，需轉換成固定大小的
矩陣。我們使用 SVD (singular value 
decomposition)來求得一組子空間，此子空
間能夠近似的生成(span)出原來的矩陣。和
許多使用 SVD來降維的方法所不同的是，
我們並非尋找一個投影或降維後的矩陣，
而 是 用 一 組 投 影 空 間 或 特 徵 空 間
(eigen-space)來重新表示這段語句。如圖六
所示，原本的矩陣𝐗，經由 SVD 後，所得
出的矩陣𝐔才是這個語句的新表示法。我們
稱這種表示法為 Grassmann Manifold，是一
組散佈在ℝ𝑁空間的子空間。這種表示法具
有以下好處：一、若原本的音素串列含有
少數不正確的結果，對於所求得的子空間
不至於造成太大的影響；二、我們可以使
用許多在數學上已經發展近百年的子空間
性質來進行有意義的子空間之間的相似度
估量。 
  我們參考論文[10]的做法，使用與子空
 6 
一 種 方 法 ， 稱 之 為 相 異 性 學 習 法
(dissimilarity-based learning)。與[11]中的核
方法不同的是，這種學習法不需要將每個
語句𝐒投影至高維的核空間，而是直接使用
子空間之間的距離函數來進一步將每個語
句表示為它與所有訓練語料中的語句的距
離向量，最後再利用傳統的線性鑑別分析
(linear discriminant analysis, LDA)來進行分
類。 
同樣地，我們以 OGI多語言口語語料
庫做為自動語言辨識的實驗語料。實驗結
果如圖十，我們的方法無論在包含 1至 50
秒不同長度語句的完整測試集或是只包含
約 3 秒長度短句的子測試集上均優於現有
的 VSM方法。這部分的研究成果與實驗討
論已經發表在國際學術會議 ICASSP 2013 
[12]。 
 
四、計畫成果自評 
 
本計畫在國台客英四種語言的測試語料收
集標記、語音屬性空間建立、通用音素發
音詞典建立等基礎工作已經大致完成，將
有利於實驗室繼續發展多語言語音辨識技
術，未來並將進一步擴展到多語言語音合
成技術。在基於加權式有限狀態轉換機的
大詞彙連續語音辨識系統的建置方面也已
經獲得相當不錯的初步成果，雖然這個系
統的複雜度很高，仍有一些問題尚待克服。
在口語語言辨識方面的研究成果已經發表
在 InterSpeech2012 [11]與 ICASSP2013 [12]
國際會議，目前正在著手整理與撰寫期刊
論文，近期內將投稿至 IEEE Transactions 
on Audio, Speech and Language 
Processing。 
 
五、參考文獻 
 
[1]  N. Chomsky, and M. Halle, The Sound 
Pattern of English, MIT Press, 
Cambridge, MA, U.S.A, 1968. 
[2]  W. M. Fisher et al., "The DARPA speech 
recognition research database: 
specifications and status," in Proceedings 
of DARPA Workshop on Speech 
Recognition, pp. 93–99, 1986. 
[3]  J. L. Hieronymus, "ASCII phonetic 
symbols for the world's languages: 
Worldbet," 1994. 
[4]  T. Lander, "The CSLU labeling guide," 
1997. 
[5]  Handbook of the International Phonetic 
Association: A Guide to the Use of the 
International Phonetic Alphabet, 1999. 
[6]  P. Ladefoged and I. Maddieson, The 
Sounds of the World's Languages, 1996. 
[7]  S. King and P. Taylor, "Detection of 
phonological features in continuous 
speech using neural networks," Computer 
Speech and Language, vol. 14, pp. 
333-353, 2000. 
[8]  D. Povey, M. Hannemann et. al, 
“Generating exact lattices in the WFST 
framework,” in Proc. ICASSP2012. 
[9]  H. Li et al., "A vector space modeling 
approach to spoken language 
identification," IEEE Trans. Audio, 
Speech, Lang. Process, vol. 15, no. 1, pp. 
271-284, 2007. 
[10] J. Hamm, and D. D. Lee, "Grassmann 
discriminant analysis: a unifying view on 
subspace-based learning," in Proc. 
ICML2008. 
[11] Y. C. Shih, H. S. Lee, H. M. Wang, and 
S. K. Jeng, "Subspace-based feature 
representation and learning for language 
recognition," in Proc. Interspeech2012. 
[12] H. S. Lee, Y. C. Shih, H. M. Wang, and 
S. K. Jeng, "Subspace-based phonotactic 
language recognition using multivariate 
dynamic linear models," in Proc. 
ICASSP2013. 
[13] K. D. Cock and B. D. Moor, "Subspace 
angles between ARMA models," Systems 
Control Lett., vol. 46, no. 4, 2002. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 8 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
表三、國、台、客、英語輔音的音素系統對應表 
表四、國、台、客、英語元音的音素系統對應表 
 10 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖五、語音組合特徵的抽取與串接 
圖六、SVD所生成的子空間 
 12 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖九、語音組合特徵的抽取與音素相似度向量的形成 
圖十、各種語音組合法的口語語言辨識 DET結果，（左）3秒語料；（右）
1-50秒語料 
 14 
The rest of the paper is organized as follows. In Section 2, 
we introduce the new representation of a spoken utterance in the 
sense of subspaces. In Section 3, we present the learning mecha-
nism for subspaces with Grassmann kernels. Section 4 gives the 
evaluation results and some discussions. Finally, conclusions and 
future work are outlined in Section 5.  
2. Data representations and subspaces 
2.1. Phonotactic feature extraction 
Given the observed sequence of acoustic vectors         
derived from a spoken utterance, phonetic decoders single out 
the best phone sequence         based on Viterbi approxima-
tion, which ﬁnds the most likely time alignment path through a 
huge probabilistic network. The main task of phonotactic-based 
language classifiers is to take advantage of the phone sequence 
        as a basic unit for language identification or verifica-
tion. The basic idea behind our proposed phonotactic data 
representation is to take the single-best phone sequence produced 
by phonetic decoders as a kind of clusters toward the acoustic 
feature vectors in a phonotactic fashion. From the example in 
Figure 1, we can see that after phone decoding and time align-
ment, feature vectors {        } belong to top-1phone    while 
feature vectors {           } belong to top-1 phone   . Along 
this vein, given phone boundaries, each set of acoustic vectors in 
its corresponding phone segment is further used to derive a more 
meaningful phonotactic feature vector built up with the average 
posterior probability for each phone, which is characterized by a 
hidden Markov model (HMM), through the Viterbi search 
algorithm. Consequently, each phone segment (or phone frame) 
   is expressed by a vector, whose dimensionality is the size of 
the universal phone set {  }. Figure 1 also shows that the phone 
frame   , which is labeled as   , indeed has the highest posterior 
probability of 0.6 with respect to    due to the nature of dynamic 
programming contributed by the first three feature vectors 
{        }. As for   ,   , and    in this phone frame, although 
the posterior probabilities might be much smaller than the single-
best   , and even    never appears in the phone sequence, they 
are not cast off but included into the new phonotactic feature 
vector    to bring more uncertainty or information that might be 
helpful for classification. 
2.2. Frame concatenation 
In order to capture the contextual information as well as to learn 
patterns of phonotactic constraints, like the role that n-gram 
terms play in the VSM scheme, we concatenate the phone frames 
belonging to a given contextual window centered on a current 
phone frame   . Let        denotes the width of the 
contextual window, meaning that the larger the value n, the more 
phonotactic patterns can be modeled. In Figure 1, if we set n to 1, 
the 3 consecutive phone frames,   ,   , and   , are spliced 
together forming a context-dependent supervector. Moreover, it 
is worth noting that the size of the phonotactic representation no 
longer increases exponentially with respect to n; on the contrary, 
the size has a linear growth that guarantees to avoid memory 
deficiency and unnecessary reduction in practice. 
2.3. Subspace generation 
After frame concatenation, each utterance can be expressed by a 
matrix   consisting of       rows and   columns, where   
and   denote the size of the phone set and the number of phone 
frames in an utterance, respectively. Since   varies in utterances, 
we employ singular value decomposition (SVD) to map the 
varying-size phonotactic matrix to a fixed-size pattern as depict-
ed in Figure 2 [10]. The SVD of   can be written as a reduced 
form,       , where   is a     diagonal matrix containing 
the largest d singular values        ,   and   are matrices, 
whose orthonormal columns {  }  and {  } approximately span 
the column and row spaces of  , respectively. Our goal here is 
not to find a lower-dimensional projection of  , but to find a 
linear subspace that can approximately span the whole column 
spaces of  . In other words, what we need is the columns {  } of 
 , called eigenframes, that can characterize each phone frame in 
 . Therefore, each utterance j is represented as             , 
a collection of orthonormal bases belonging to a fixed-
dimensional linear subspace of a Euclidean space. The main 
difference between our representation and other subspace-based 
methods for an utterance lies in that, we represent an utterance as 
Figure 1: Phonotactic feature extraction and frame concatenation 
for a four-phone {𝑢  𝑢  𝑢  𝑢 } ASLR system. 
Figure 2: Subspace generation by SVD. 
used in the experiments. Each phone model models a phone in 
the universal phone inventory of size 69, which is a union of the 
phones appearing in the 619 transcription files, with the phones 
sharing the same manner and place merged into one. Each phone 
model is a 3-state left-to-right CD-HMM with 32 Gaussian 
mixture components per state. The acoustic feature vector has 39 
attributes comprised of 13 MFCCs including C0, along with their 
first and second order derivatives. All phone models were trained 
and refined from the 619 phone-transcribed utterances and the 
4650 training utterances according to the maximum likelihood 
criterion, respectively. 
We compared the proposed method with two well-known 
phonotactic language recognition methods, namely UPR-LM and 
UPR-VSM [7]. UPR-LM is an extension of PRLM by using a 
universal phone recognizer instead of a phone recognizer of only 
one specific language. We applied the bigram back-off language 
model with Good-Turing smoothing in the implementation. For 
UPR-VSM, a phone sequence is represented by a          
dimensional vector consisting of the TF-IDF of unigram and 
bigram phonotactic patterns. Latent semantic indexing (LSI) was 
further used for extracting key features needed for discriminating 
spoken utterances from the statistics of some salient units and 
their co-occurrence. In the back-end classification, UPR-VSM 
used the SVM system with an RBF kernel trained for each target 
language individually. The decision for each utterance was made 
based on the posterior probability given by applying the SVM 
output to a sigmoid function. Since the proposed method is also 
based on a universal phone recognizer front-end, it is denoted as 
UPR-Subspace.  
From Figure 4, we can see that UPR-subspace outperforms 
UPR-LM and UPR-VSM. UPR-subspace achieves relative 
reductions of 38.90% and 27.13% over UPR-VSM in equal error 
rate (ERR) on the 3-s and 1-to-50-s data sets, respectively. 
Figure 5 (a) shows EER with respect to the number of eigen-
frames (d). The results demonstrate that, with a moderate value 
of d, the EER is dramatically degraded. From Figure 5 (b), we 
observe that larger widths           of the contextual 
window not necessarily guarantee lower EERs. We can also see 
that the ERR is minimal when     (   ). The results seem 
to somewhat explain the reason why the maximum size of 
phonotactic constraints are set to 3 (trigram) in some phonotactic 
learning simulation systems [15] from another viewpoint. 
5. Conclusion 
This paper presents a new phonotactic feature representation 
based on subspace formulation for automatic spoken language 
recognition. On the basis of the representation, the combination 
of KLDA and the simple nearest neighbor classifier has been 
shown to perform excellently. It is expected that the integration 
of a more discriminative kernel-based classier, such as SVM, 
with a subspace-based kernel could perform even better. In our 
future work, we plan to evaluate the proposed framework on the 
more standard NIST LRE corpora, and other subspace-based 
methods will be implemented and compared in experiments.  
6. References 
[1] E. Ambikairajah et al., “Language identification: a tutorial,” IEEE Circuits 
and Systems Magazine, vol. 11, no. 2, pp. 82-108, 2011. 
[2] P. A. Torres-Carassquilo et al., “Approaches to language identification using 
Gaussian mixture models and shifted delta cepstral features,” in Proc. ICSLP, 
2002. 
[3] A. S. House and E. P. Neuburg, “Toward automatic identification of the 
language of an utterance. I. Preliminary methodological considerations,” J. 
Acoust. Soc. Amer., vol. 62, no. 3, pp. 708-713, 1977. 
[4] Y. K. Muthusamy et al. “A comparison of approaches to automatic language 
identification using telephone speech,” in Proc. Eurospeech, 1993. 
[5] T. J. Hazen and V. W. Zue, “Segment-based automatic language identifica-
tion,” J. Acoust. Soc. Amer., vol. 101, no. 4, pp. 2323-2331, 1997. 
[6] M. A. Zissman and E. Singer, “Automatic language identification of telephone 
speech messages using phoneme recognition and N-gram modeling,” in Proc. 
ICASSP, 1994. 
[7] H. Li et al., “A vector space modeling approach to spoken language identifica-
tion,” IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 1, 2007. 
[8] M. Penagarikano et al., “Improved modeling of cross-decoder phone co-
occurrences in SVM-based phonotactic language recognition,” IEEE Trans. 
Audio, Speech, Lang. Process., vol. 19, no. 8, pp. 2348-2363, 2011. 
[9] J. L. Gauvain et al., “Language recognition using phone lattices,” in Proc. 
ICSLP, 2004. 
[10] J. Hamm, and D. D. Lee, “Grassmann discriminant analysis: a unifying view 
on subspace-based learning,” in Proc. ICML, 2008. 
[11] A. Björck and G. H. Golub, “Numerical methods for computing angles 
between linear subspaces,” Math. Computation, vol. 27, no. 123, 1973.  
[12] J. Hamm. Subspace-Based Learning with Grassmann Kernels, PhD Thesis, 
2008. 
[13] B. W. Silverman, Density Estimation for Statistics and Data Analysis, 
Chapman and Hall, New York, 1986. 
[14] A. Y. K. Muthusamy et al., “The OGI multi-language telephone speech 
corpus,” in Proc. ICSLP, 1992. 
[15] B. Hayes and C. Wilson, “A maximum entropy model of phonotactics and 
phonotactic learning,” Linguistic Inquiry, vol. 39, no. 3, pp. 379-440, 2008. 
[16] M. Soufifar et al., “iVector approach to phonotactic language recognition,” 
Proc. Interspeech, pp. 2913-2916, 2011. 
[17] T. Mikolov et al., “PCA-based feature extraction for phonotactic language 
recognition,” Proc. Odyssey, pp. 251-255, 2010. 
Figure 4: DET plots for several phonotactic approaches on (a) the 3-s data 
set and (b) the 1-to-50-s data set.  
Figure 5: EER with respect to (a) d, the number of eigenframes, 
and (b) n, the width of the contextual window  𝑤   𝑛   . 
superior performance in the speaker recognition field [12], some 
researchers have used probabilistic PCA (PPCA) to transform each 
high-dimensional vector filled with discrete features into a small-
size set of latent variables corresponding to a high variability 
subspace [13-15, 30]. For example, in [13] and [14], Soufifar et al. 
used the subspace multinomial model, along with the maximum 
likelihood criterion, to effectively represent the information 
contained in the n-grams.  
        In this paper, as an extension of our previous subspace-based 
work in [16], we propose a new approach for data representation, 
in which the phonetic information as well as the contextual 
relationship can be more abundantly retrieved by likelihood 
computation and dynamic linear models, given a universal phone 
recognizer. In the VSM framework, the count or frequency of a 
phonotactic term is the only attribute that is concerned. In contrast, 
our approach enables us to look much farther and deeper through 
the decoder’s eyes without much more memory. That is, not only 
can more information, such as likelihood scores of any phone 
segments, be captured, but also all possible phones can be taken 
into account instead of the single most likely one. The spirit is 
somewhat similar to the employment of phone lattices [17] or 
posteriogram-based n-gram counts [15]. Moreover, our 
representation also fits for the back-end classification. Under the 
assumption that the utterance representation can be approximately 
described by a collection of lower dimensional linear subspaces, a 
suitable dissimilarity-based learning algorithm along with the well-
surveyed Projection metric are introduced for classification. 
        The remaining of this paper is organized as follows. In 
Section 2, we introduce the new representation of a speech 
utterance in the sense of dynamic linear models. In Section 3, we 
present the learning and scoring mechanisms for subspaces with 
the Projection metric. Section 4 gives the evaluation results and 
some discussions. Finally, conclusions and future work are 
outlined in Section 5. 
 
2. SUBSPACE-BASED REPRESENTATION 
 
According to the definition in [19, p. 3, 20], subspace-based 
learning is based on the extraction of the most conspicuous 
properties of each class separately, as represented or spanned by 
vector series expansions constructed from the feature vectors of 
each class. The learning mechanism focuses on how to measure the 
similarity between each subspace and a given data point. However, 
most widely-alleged subspace-based methods for the SLR tasks, 
such as i-vectors and PCA, do not fall into this definition. On the 
contrary, their goal is to derive a coordinate representation (or a 
vector-like point) for an utterance in a lower-dimensional linear 
space where all projected utterances are supposed to share the 
same set of bases. In contrast, we will introduce a new approach to 
represent each utterance as a subspace of the original feature space, 
where the salient structure of each utterance can be preserved. 
 
2.1. Phone-likelihood vectors 
 
Given the observed sequence of acoustic feature vectors         
derived from a speech utterance, a phone decoder singles out the 
best phone sequence         based on Viterbi decoding which 
ﬁnds the most likely time alignment path through a huge 
probabilistic network. The main task of phonotactic-based 
language classifiers is to take advantage of the phone sequence 
        as a basic unit for SLR. The basic idea behind our 
proposed phonotactic data representation is to take the single-best 
phone sequence given by the phone decoder as a kind of clusters 
toward the acoustic feature vectors in a phonotactic fashion. From 
the example in Figure 1, we can see that after phone decoding and 
time alignment, feature vectors {        } belong to top-1 phone 
   while feature vectors {           } belong to top-1 phone   . 
Along this vein, given phone boundaries, each set of acoustic 
vectors in its corresponding phone segment is further used to 
derive a more meaningful phonotactic feature vector built up with 
the average log-likelihood score for each phone, which is 
characterized by a hidden Markov model (HMM), through the 
Viterbi search algorithm. Consequently, each phone segment (or 
phone frame)    is expressed by a phone-likelihood vector   , 
whose dimensionality is the size of the universal phone set {  }. 
Figure 1 also shows that the first phone   , which is most likely 
labeled as   , indeed has the highest log-likelihood score of -14 
with respect to the first attribute    due to the nature of dynamic 
programming contributed by the first three acoustic feature vectors 
{        }. As for other attributes   ,   , and    in   , although 
their scores might be much smaller than the single-best   , and 
even    might never appear in the single best phone sequence, they 
are not cast off but included into the phone-likelihood vector    to 
bring more uncertainty or information that might be helpful for 
classification. 
 
2.2. Dynamic linear models 
 
After the aforementioned procedure, each utterance is expressed by 
a sequence of phone-likelihood vectors, which is more than just a 
set of vectors due to the temporal information, especially 
phonotactic constraints, contained in the sequence. To capture the 
temporal dynamics, we make a conjecture that each sequence was 
generated by a causal linear time-invariant (LTI) system, which 
might be a sub-system pertaining to some language production 
system. This conjecture is similar to the acoustic theory of speech 
production that assumes the speech production process to be a 
linear system, consisting of a source and ﬁlter [29]. A multivariate 
dynamic linear model (DLM), also known as a state space model, 
is one of causal LTI systems, which has been used to model 
moving human bodies or textures in computer vision [21] and 
signal analysis [22]. A simpler DLM to model the phone-
likelihood sequence is described as follows. 
Figure 1. Phone-likelihood vector extraction for an SLR system 
with four universal phones {           }. 
6871
 where  (     )  (     )  
  (     ), and    denotes the 
within-class covariance matrix derived in the projective space. 
 
4. EXPERIMENTS 
 
We conducted the language verification task on the Oregon 
Graduate Institute Multi-language Telephone Speech (OGI-TS) 
Corpus [27], which contains the speech of 10 languages: English, 
Farsi, French, German, Japanese, Korean, Mandarin, Spanish, 
Tamil, and Vietnamese. The corpus is divided into three parts: 
4650 utterances for training, 1899 utterances for development, and 
1848 utterances for test. Some test utterances with lengths ranging 
from 2 to 4 seconds are culled to form the 3-s set to evaluate the 
system performance for short utterances, while all test utterances 
form the 1-to-50-s set. Besides, the corpus also includes 619 
“story-before-tone” utterances, which have manually generated 
fine-phonetic transcriptions that can be used for supervised phone 
modeling for six languages.  
        A universal phone recognizer (UPR), which is composed of a 
set of language-independent context-independent phone models, is 
used in the experiments. Each phone model models a phone in the 
universal phone inventory of size 69, which is a union of the 
phones appearing in the 619 transcription files, with the phones 
sharing the same manner and place merged into one. Each phone 
model is a 3-state left-to-right CD-HMM with 32 Gaussian mixture 
components per state. The acoustic feature vector has 39 attributes 
comprised of 13 MFCCs including C0, along with their first and 
second order derivatives. According to the procedure shown in [8], 
all phone models were trained and refined from the 619 phone-
transcribed utterances and the 4650 training utterances according 
to the maximum likelihood criterion, respectively. 
        Given the same UPR, we compared the proposed method with 
two well-known VSM-based methods, namely UPR-VSM [8] and 
UPR-IVCT [13], and one subspace-based method, UPR-CONCAT 
[16]. For UPR-VSM, a phone sequence is represented by a 
(          )  dimensional vector consisting of the TF-IDF 
values of unigram, bigram, and trigram phonotactic patterns. 
Latent semantic indexing (LSI) was further used for extracting 
2000-dimensional key features needed for discriminating 
utterances from the statistics of some salient units and their co-
occurrences. However, in UPR-IVCT, only unigram and bigram 
phonotactic patterns were used in the multinomial subspace model 
to train the 700-dimensional i-vector of each utterance [28]. 
Instead of the DLM mentioned in Section 2.2, UPR-CONCAT 
models the phonotactic information within an utterance by simply 
concatenating the phone-likelihood vectors belonging to a fixed-
length sliding window centered on a current vector, whose size was 
set to be 3 in our experiment. All of the above parameter settings 
were determined by the experiments on the developing data set. 
        To make fair comparisons, in the back-end classification, we 
used LDA and its corresponding scoring technique mentioned in 
the end of Section 3 in all of the four methods. From Figure 2 and 
Table 1, we can see that when    , UPR-DLM outperforms two 
VSM-based methods, UPR-VSM and UPR-IVCT, in terms of the 
equal error rate (EER) nearly on both data sets, and achieves 
comparable performance to UPR-CONCAT on the 1-to-50-s data 
set. Some possible reasons why UPR-DLM performs worse than 
UPR-CONCAT lie in that, 1) the language production process we 
attempt to model may have some nonlinearity effects that linear 
systems cannot fully describe; 2) the compact solutions of model 
parameters shown in (3) and (6) may not be close enough to the 
true solutions although the computation is efficient; 3) in UPR-
CONCAT, SVD acts as a robust subspace generator that allows for 
high discrimination against noise contamination even when the 
phone decoder is not reliable, but UPR-DLM lacks this kind of 
operations. 
        Figure 3 shows the EER with respect to the state order ( ). 
We can see that the minimal ERR is achieved when    , which 
means that the phonotactic information contained in 4 consecutive 
phone-likelihood vectors are considered. Compared with the 
results of UPR-CONCAT, it seems to imply that the maximum size 
of phonotactic constraints can be set to 3 (trigram) or 4 (4-gram) in 
most of the phonotactic SLR tasks. 
 
5. CONCLUSIONS 
 
This paper presents a new phonotactic feature representation based 
on dynamic linear models and subspace formulation for automatic 
spoken language recognition. On the basis of the representation, 
the combination of the dissimilarity-based learning algorithm and 
LDA has been shown to perform well. In our future work, we plan 
to remedy the flaws found in the proposed framework and evaluate 
it on the NIST LRE corpora. Other nonlinear subspace-based 
methods will also be investigated, implemented, and compared in 
the experiments. 
Methods 1-to-50-s data set 3-s data set 
UPR-VSM 15.12 19.81 
UPR-IVCT 18.62 23.48 
UPR-CONCAT 10.68 16.78 
UPR-DLM 12.09 20.58 
Figure 3. EER with respect to d, the state order, in 
UPR-DLM on the 3-s and 1-to-50-s OGI-TS data 
sets. 
Figure 2. DET plots for two VSM-based and two subspace-based 
approaches on (a) the 3-s and (b) the 1-to-50-s OGI-TS data sets.  
Table 1. EER (%) for various phonotactic approaches on the 3-s 
and 1-to-50-s OGI-TS data sets. 
6873
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                                        日期：101 年 9 月 17 日 
計畫編號 NSC99-2221-E-001-009-MY3 
計畫名稱 
應用發音知識源於多樣化語言之語音處理-應用發音知識源於
多語言自動語音辨識之研究 
出國人員
姓名 
王新民 服務機構
及職稱 
中研院資訊所 
研究員 
會議時間 
101年 9月 9日至 
101年 9月 13日 會議地點 Portland, Oregon, USA 
會議名稱 
 (中文)  
 (英文) The 13th Annual Conference of the International Speech 
Communication Association (Interspeech2012) 
發表論文
題目 
1. Word Relevance Modeling for Speech Recognition 
2. A Study of Mutual Information for GMM-Based Spectral Conversion 
3. Subspace-Based Feature Representation and Learning for Language 
Recognition 
 
一. 參加會議經過 
 
此行於 9月 9日早晨搭乘達美航空班機，經由東京轉機，於當地時間 9日早上抵達
波特蘭機場，再搭乘輕軌電車機場線前往下榻飯店，行李安置妥當之後，便前往會場報
到，取回會議資料。同行的有本院資創中心的曹昱博士、我的三位學生及研究助理、及
師大陳柏琳教授的三位學生。 
Interspeech是由International Speech Communication Association(ISCA)主辦之國際研
討會，為口語處理相關研究領域除IEEE國際聲學語音信號處理研討會(ICASSP)外，規模
最大、最重要、最專業的研討會。Interspeech2012共收到1千3百多篇論文投稿，經過嚴
格的審查，錄取6百多篇論文，論文接受率約50%。這些論文被安排在39個口頭報告議程、
37個壁報報告議程及8個特別議程發表，同一時間有6個口頭報告議程和8個壁報報告議
程平行進行，另外，此次會議安排4個專題演講、8個tutorials、5個workshops。 
此次會議的4個專題演講都很有趣，第一位講員是美國Georgia Institute of Technology的李
錦輝教授，也是今年的ISCA Medalist得主，演講內容是有關音訊和聲學信號處理的未來
發展；第二位講員是美國Carnegie Mellon University的Roger B. Dannenberg教授，演講內
容是自動語音辨識的新觀點，從語音信號處理到語音資訊擷取，第三位講員是Google的
Michael Riley博士，演講內容是Weighted Transduser and Push Transduser，第四位講員是
美國Oregon Health & Science University的Garet Lahvis教授，演講內容是有關計算神經學
與腦波解碼的發展與應用。 
我們的第一篇論文被安排在Music: Classification and Recognition議程，採口頭報告
方式，報告時間是 13:30 - 13:50；第二篇論文被安排在 Classification and Clustering議程，
採壁報解說方式；第三篇論文。 
此行於 11日中午受邀代表 ISCA SIG-CSLP (Special Interest Group on Chinese Spoken 
Language Processing) 參加 The Students Meet Experts/Student Lunch 活動，與新加坡
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                                        日期：101 年 11 月 6 日 
計畫編號 NSC99-2221-E-001-009-MY3 
計畫名稱 應用發音知識源於多樣化語言之語音處理-應用發音知識源於多語言自動語音辨識之研究 
出國人員
姓名 王新民 
服務機構
及職稱 
中研院資訊所 
研究員 
會議時間 
101 年 10 月 29 日至 
101 年 11 月 2 日 會議地點 Nara, Japan 
會議名稱 
 (中文)  
 (英文) ACM Multimedia 2012 
發表論文
題目 
1. The Acoustic Emotion Gaussians Model for Emotion-based Music 
Annotation and Retrieval 
2. The Acousticvisual Emotion Gaussians Model for Automatic Generation of 
Music Video 
3. Exploring the Relationship between Categorical and Dimensional Emotion 
Semantics of Music 
 
一. 參加會議經過 
 
此行於 10 月 28 日早晨搭乘華航班機，於中午時間抵達日本關西國際機場，入關之
後先搭乘南海電鐵電車至大阪難波，再轉乘近鐵電車前往奈良，約於下午三點抵達下榻
飯店，行李安置妥當之後，便先前往會場熟悉環境。 
ACM Multimedia會議是多媒體研究領域最頂尖的研討會。本次會議共收到331篇長
篇論文(long paper)及407篇短篇論文(short paper)投稿，經過嚴格的審查，只接受67篇長
篇論文(接受率為20.2%)，短篇論文的接受率則是31.2%。 
10 月 29 日會議第一天的議程為 Workshop 及 Tutorial，上午我參加了 International 
Workshop on Crowdsourcing for Multimedia，日本 AIST 的 Masataka Goto 教授以
「PodCastle and Songle: Crowdsourcing-Based Web Services for Spoken Content Retrieval 
and Active Music Listening」為題，做了一個精采的專題演講。下午則參加 Hatice Gunes
和 Björn Schuller 兩位教授合講的 Tutorial 「Continuous Analysis of Emotions for 
Multimedia Applications」。 
10 月 30 日是 Main Conference 的第一天，開幕式之後緊接者是 Best Paper Session，
共有四篇論文競逐，其中一篇是來自台灣大學洪一平教授團隊的研究成果，這篇論文雖
然沒有奪得最佳論文獎，但也贏得最佳學生論文獎，讓我們覺得與有榮焉。我們的長篇
論文「The Acoustic Emotion Gaussians Model for Emotion-based Music Annotation and 
Retrieval」被安排在下午的「Audio and Music」議程，另外，也在中午的時間先以海報
解說的方式發表。晚上則出席 The 20th Anniversary Keynote Talk 「Future Direction of 
Digital Content」。 
10 月 31 日上午的議程包括「Technical Achievement Awards」及 The 20th Anniversary 
Panel「Coulda, Woulda, Shoulda: 20 Years of Multimedia Opportunities」，全體與會人員齊
聚一堂交換意見。下午則是一般的 oral 及 poster 議程。 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                                        日期：102 年 6 月 6 日 
計畫編號 NSC99-2221-E-001-009-MY3 
計畫名稱 
應用發音知識源於多樣化語言之語音處理-應用發音知識源於
多語言自動語音辨識之研究 
出國人員
姓名 
王新民 服務機構
及職稱 
中研院資訊所 
研究員 
會議時間 
102年 5月 26日至 
102年 5月 31日 會議地點 加拿大溫哥華 
會議名稱 
 (中文)  
 (英文) The 38th International Conference on Acoustics, Speech and Signal 
Processing (ICASSP2013) 
發表論文
題目 
1. Subspace-based Phonotactic Language Recognition Using Multivariate 
Dynamic Linear Models 
2. Weighted Matrix Factorization for Spoken Document Retrieval 
3. Effective Pseudo-Relevance Feedback for Spoken Document Retrieval 
 
一. 參加會議經過 
 
此行於 5月 26日深夜搭乘華航班機，於當地時間 5月 26日晚間 7點抵達溫哥華機
場，隨即趕往下榻飯店休息，第二天先到會場辦理報到手續，並參與晚間的 reception。 
由 IEEE Signal Processing Society所舉辦的 ICASSP是國際上關於信號處理及其相關
應用研究規模最大，最負盛名的學術研討會。語音與音訊處理研究領域的學者亦視其為
年度最重要的研討會，所有知名學者專家幾乎都會與會發表最新研究成果及互相討論、
交換研究心得。歷屆 ICASSP 邀請的演講學者都是相關領域世界級的一時之選，oral 及
poster 議程總數在 50 以上，大會也提供許多優質的 tutorial 課程供與會者選擇，展覽會
也非常盛大。本屆也不例外，不包括特別議程即有三千三百多篇論文投稿，經過嚴格審
查，約一千七百篇論文獲接受在大會發表。主題演講有五場，講題和講員分別是 1. Recent 
Developments in Deep Neural Networks (Geoffrey Hinton); 2. Inverse Problems Regularized 
by Sparsity (Martin Vetterli); 3. Information measures and estimation theory (Sergio Verdú); 
4. The Splendor of Nature: Lessons in Adaptation and Learning over Networks (Ali H. 
Sayed); 5. The Online Revolution: Education for Everyone (Daphne Koller)。課題涵蓋機器
學習、compressed sensing、資訊理論、信號處理與線上學習。 
    此次會議我們共發表三篇論文，分別是 1. Subspace-based Phonotactic Language 
Recognition Using Multivariate Dynamic Linear Models；2. Weighted Matrix Factorization 
for Spoken Document Retrieval；3. Effective Pseudo-Relevance Feedback for Spoken 
Document Retrieval。第一篇論文被安排在 5月 30日下午 17:00-17:20做 oral報告，另外
兩篇則被安排在 5月 31日上午 8:00-10:00間做 poster講解。三篇論文都獲得與會者不少
的討論與建議。 
    四天的主會議非常緊湊，會議在 5月 31日結束，旋即搭乘 6月 1日凌晨的班機返
台。 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/10/21
國科會補助計畫
計畫名稱: 應用發音知識源於多語言自動語音辨識之研究
計畫主持人: 王新民
計畫編號: 99-2221-E-001-009-MY3 學門領域: 自然語言與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
