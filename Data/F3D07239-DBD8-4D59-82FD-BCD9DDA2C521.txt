 2
適用於多核心之低功率隨選記憶體系統(3/3) 
 
計畫編號：NSC98-2220-E-009-002 執行期間：98年8月1日至99年7月31日 
主持人：黃威教授 國立交通大學電子工程研究所 
闕河鳴助理教授 國立交通大學電信工程學系 
一、中文摘要 
隨著多核心系統晶片的發展，多系統融合已成為必然的趨勢。因此，晶片上不
僅需要放置更多、更快、且低功耗的記憶體來提供儲存資料，此記憶體也必須具備
可以同時支援多個不同系統的能力，為此，我們提出一個隨選記憶體系統的構想。
此隨選記憶體系統建構在多核心平台上，藉由記憶體管理單元的協助與管理，在此
平台上執行之系統(多系統融合)將可以對記憶體快速地存取資料。隨選記憶體系統
將包含分散式記憶體、共享式記憶體、多執行緒暫存器和三層式記憶體管理單元。
我們結合電路與架構的設計，提出管理晶片上動態記憶體分配、頻寬、與資料傳輸
之機制，此外還負責管理晶片內外間資料傳輸之方法，且著重於低功耗之設計以解
決多系統融合時所將遇到之瓶頸。 
在第一年的執行年度中，我們發展低功率高頻寬記憶體設計技術、睡眠電晶體
控制技術、建構多核心融合系統發展平台及多階層式記憶體管理單元。在第二年的
執行年度中，我們完成有寫入輔助電路的穩健低功率靜態隨機存取記憶體、低功率
時間共享多線程暫存器及低功率高可靠度晶內傳輸線架構。在最後一年度，我們整
合前兩年關鍵技術，完成了隨選記憶體系統設計，並配合總計劃完成晶內資料傳輸
平台。隨選記憶體子系統負責有效地管理異質多核心系統晶片中多執行程序的記憶
體存取，讓系統達到最佳的記憶體使用率。在所提出的隨選記憶體子系統中，主要
包含了私有式記憶體管理單元以及集中式記憶體管理單元，負責管理快取及外部記
憶體之資料存取；其中包含了五項關鍵技術，分別為借取機制、適應性快取控制機
制、外部記憶體存取介面、跨層間預取資料機制和動態記憶體位址轉換器。 
關鍵字 
隨選記憶體、多核心、多執行緒暫存器、記憶體管理單元
 4
二、計畫的緣由與目的 
For multi-task wireless video entertainment systems, a heterogeneous multi-core SoC 
provides an integrated solution for supporting great amount of data computation [1]. With 
the increasing data computation, the increasing demand of the memory capacity and 
bandwidth is a bottleneck in multi-core SoCs since the overall performance of PEs is 
much faster than that of the memory. Additionally, multimedia technologies are usually 
utilized in multi-task systems for video processing that spawns brand new industries and 
services, such as digital video recording, video-on-demand services, high-definition TV, 
and digital home sever. Generally, a great amount of memory requirements are required 
for high quality or multiple scalable level video processing. Therefore, an efficient 
memory system is indispensable that provides enough memory space and high data 
bandwidth for satisfying the video real-time requirement. Furthermore, the strategy of the 
memory management has become one of the design challenges. Accordingly, the 
organization of memory system for a multi-task system will affect the system 
performance dramatically. 
 
Figure 1. Relative complexity of a video system. 
In multi-core SoC designs, the processes of data streaming can be divided into three 
parts, including data computation, data storage and data communication. With the 
increasing PEs in multi-core SoCs, the capability of data computation increases rapidly to 
satisfy the increasing demands of mobile multimedia services. Furthermore, memory 
accesses and on-chip data communication dominate the overall performance of 
multi-core SoCs. Therefore, the development of memory system in multi-cure systems 
will affect the overall performance dramatically. Moreover, the relative complexity of a 
 6
system. Heterogeneous PEs, such as microprocessors, application-specific stream 
processors and IP (intellectual property) cores, can be integrated in this platform. In eH-II 
platform, each PE owns a private memory management unit (p-MMU). The p-MMU 
includes local caches (D-cache and I-cache) and a cache controller which can efficiently 
handle all memory requests generated by the PEs. It can dynamically allocate unused 
space in cache for buffering the transmitting data. If the PE requires extra memory 
resources, the centralized memory resources, including centralized cache and off-chip 
DRAM, can be used and controlled by a centralized memory management unit (c-MMU). 
The c-MMU can dynamically allocate and manage the memory resources according to 
different memory requirements. For the data communication between PEs, 
message-passing technique is applied for this platform. The PEs transmit/receive the data 
to/from others through the memory-centric OCIN. Therefore, Network interfaces (NIs) 
are designed to packetize the transmitted data.  
 
Figure 3. Memory hierarchy in on-demand memory system. 
In the on-demand memory system, a three-level memory hierarchy is constructed as 
shown in Fig. 3. For the first hierarchy level, p-MMUs are applied to control the memory 
accesses. Furthermore, in order to improve the network efficiency of the OCIN, p-MMUs 
can dynamically allocate unused space in distributed caches to store blocked packets. For 
the second level hierarchy of the on-demand memory sbu-system, a c-MMU is 
 8
wireless video entertainment system is as shown in Fig. 4. In eH-II platform, on-demand 
memory system can support heterogeneous and real-time memory requirements for 
wireless video entertainment systems. Based on different memory requirements of PEs, 
the c-MMU can dynamically allocate memory resources to enhance the flow rate of the 
data stream. Therefore, with the adaptive memory resource arrangement for different PEs, 
the execution efficiency of the streaming processing in wireless video entertainment 
systems can be improved.  
The overall architecture of the on-demand system in eH-II platform is as shown in Fig 
5. The system components can be categorized into data computation, data communication 
and data storage. For data computation, WPU, MAC, LT coding and SVC are PEs in the 
wireless video entertainment system. Wrappers are utilized as core interfaces to satisfy 
the specification of the pre-defined protocol. Subsequently, the memory-centric OCIN are 
developed for data communication that includes network interfaces (NIs) and an 
interconnection network. In eH-II paltform, message-passing mechanism is adopted. 
With this mechanism, the transmitting data are packetized into packets by NIs, and 
through the interconnection network using a pre-defined message-passing protocol.  
For data storage, each PE owns a p-MMU, which is constructed a distributed cache 
(L1 cache) and a cache controller for accessing memory and managing the cache usage. 
When the packet queue in NI is insufficient, the p-MMU can borrow unused cache blocks 
for NI. Moreover, the c-MMU, consisting of a centralized cache (L2 cache), a cache 
controller and a DRAM controller, is designed to provide more memory resources for 
PEs. The cache controller can support dynamical cache re-organization for allocating 
different cache resources. Consequently, the DRAM controller is designed to efficiently 
access off-chip DRAM. In the DRAM controller, the address translator rearranges and 
translates address to have efficient memory allocation, and the external memory interface 
reschedules the commands to reduce memory access latency. 
The distributed memory resources may be insufficient for PEs. Therefore, lower level 
cache is utilized to provide larger on-chip memory resources in a c-MMU. According to 
distinct memory resource requirements from different PEs, the proposed c-MMU can 
allocate different cache resources for PEs via an adaptive cache controller. In addition, 
 10
cache size can be assigned for PEs dynamically associated with small area and timing 
overhead for the cache reconfiguration. In the proposed c-MMU organization, the 
associativity-based partitioning scheme is adopted for the cache partition.  Each SRAM 
sub-block represents a way and forms a bank for the cache organization. Therefore, N 
SRAM sub-blocks in the c-MMU represent N-way associativity capacity in the 
centralized cache. For different PEs, the SRAM sub-blocks can be grouped into several 
groups for PEs.  
 
Figure 7. Cache table checking by a bank assignment table. 
In order to dynamically allocate the memory resources for PEs during runtime, a bank 
assignment table (BAT) is designed to record the memory usage information in three 
time intervals. Fig. 7 illustrates the cache table checking method using the BAT. 
According to the corresponding ID of the PE, the adaptive cache controller searches the 
BAT and returns the assigned bank numbers indicating the associated bank tables. In Fig. 
7, four memory banks are assigned for PE3 in the first time interval. When a request from 
PE3 is served, Bank0, Bank1, Bank2 and Bank3 tables will be selected for hit checking. 
Based on this configuration, a 4-way associativity L2 cache memory resource can be 
accessed by PEs. 
 12
The overall architecture of the c-MMU is divided into four execution stages as shown 
in Fig. 9. The blue blocks indicate the adaptive cache controller, and the tables in red 
block are the selected bank tables for tag checking. In the first execution stage, the BAT 
search engine searches the bank assignment information according to the Node ID and 
selected time interval. In the second execution stage, the hit detector outputs the 
corresponding read/write address and bank destination to the MUX-based switch. If the 
data does not exist in the c-MMU for a read request, the address will be transferred to the 
corresponding bank pending buffer and the external memory read queue. When the read 
data is read from off-chip DRAM, the data can be directly stored into the corresponding 
bank and transmitted back to the corresponding p-MMUs via comparing the address in 
the pending buffer in third execution stage. Since a bank can be accessed by the cache 
controller, external memory and other banks, an arbiter at the third execution stage is 
adopted to determine the priority of access requests for a memory bank. The requests 
from external memory have the highest priority to minimize the miss penalty, and the 
priority of additional write requests generated by cache reorganization is set as the lowest 
one. At the final execution stage, the cache data are read out and forwarded to the PEs, 
external memory or another SRAM banks by a MUX-based switch. Additionally, for the 
external memory request arbitration, the read requests are served prior to the write 
requests.  
For realizing the wireless video entertainment system, power consumption has been 
the most critical issue. However, the property of intensive memory data accessing, 
especially for external memory accesses, contributes the significant part of the power 
consumption in the system. Therefore, if the external memory accesses can be reduced, 
both of the power saving and system performance can be improved. To reduce the 
overhead of external memory accesses, several works have been proposed to reduce the 
search range for motion estimation [5-7] in the video encoder. Moreover, the pre-fetch 
schemes were adopted to early acquire the data which will be used for reconstructing the 
video frames [9], [10]. In the conventional single layer video codec structures, such as 
H.264, the mechanism of pre-fetch schemes is usually utilized on motion estimation and 
compensation. However, the performance of pre-fetch is usually inefficient for the 
irregular moving of video content. The video decoder cannot guarantee where the object 
 14
When reconstructing a macroblock, the pre-fetch engine will pre-fetch the necessary 
residuals and MVs in base layer for the next macroblock reconstructing.  For inter-layer 
residual prediction, IPS loads an 8X8 block and additional residual signals which will 
need by bilinear interpolation. For inter-layer motion prediction, all MVs in 8X8 block 
are also pre-fetched by IPS. The purple block illustrates the pre-fetch data for IPS. Based 
on the proposed scheme, the cache hit rate can significantly been improved for inter-layer 
prediction. 
 
Figure 11. Inter-layer pre-fetch scheme (IPS). 
四、結論與討論 
In the wireless video entertainment system, SVC requires great amount of memory 
resource to store the frame data. However, decoding different scalable levels has different 
memory requirements for storing the reconstruct frames of different layers. The effective 
bandwidth of the wireless channel can be detected by MAC. According to the detection 
of the wireless channel, the transmitter can determine the scalable level of SVC bitstream 
to satisfy the effective bandwidth. Based on various bitstream, the memory requirement 
of different quality and resolution levels is also various and can be profiled off-line. With 
profiling the SVC memory requirements and dynamically updating the BAT in c-MMU 
by MAC, the appropriate bank assignments for SVC in different effective bandwidths can 
be achieved. The adaptive cache controller can reduce the miss rate of frame 
reconstructing. And thus, the number of DRAM accesses can also be decreased. In 
contrast, the adaptive cache controller can turn-off unused banks in the c-MMU when 
decoding low spatial and quality layer frames. Based on the bank assignment of different 
SVC quality levels, the total execution cycles and memory energy consumption are as 
shown in Fig. 12. Therefore, 7.13% execution time reduction and 10.53% memory 
 16
energy consumption including on-chip cache and off-chip DRAM with different L2 
cache sizes. Although the IPS induces 18.9% energy overhead in the p-MMU, the 
execution time and number of L2 cache and DRAM accesses can be reduced. Therefore, 
the total memory energy consumption can be reduced. Based on the combination of the 
proposed IPS and address translator, 37.53% energy reduction can be achieved on 
average. 
For heterogeneous multi-core SoC, data communication and data storage dominate 
the overall performance. In the proposed on-demand memory system, MMUs, including 
p-MMUs and a c-MMU, can efficiently control the memory access and memory resource 
allocation for PEs. The p-MMU performs as a high level cache for the dedicated PE in 
the on-demand memory system. Furthermore, for reducing the stall caused by heavy 
traffic in OCIN, a buffer borrowing mechanism is proposed. The p-MMU can 
dynamically allocate the memory resources for buffering the blocking network data. By 
considering the borrowed memory blocks and p-MMU, the size of the output queue in NI 
can be dynamically scheduled. According to the cycle-driven simulation results in 
SystemC, the proposed efficient NI can achieve performance improvement by 1.15x 
compared to the conventional one. Therefore, the proposed efficient NI can increase the 
performance of the memory-centric OCIN.  
The c-MMU is designed for managing and providing larger centralized memory 
resources for the system. Based on the adaptive cache controller, the proposed c-MMU 
can dynamically assign variable number of SRAM banks for PEs to optimize the 
utilization of centralized on-chip cache can be optimized. By profiling the memory 
requirements for different SVC spatial and quality layers, the appropriate bank 
assignment can be determined and controlled by MAC at runtime. From the simulation 
results, 7.13% execution time reduction and 10.53% memory energy consumption 
reduction can be realized using the adaptive cache control. Additionally, an EMI of the 
DRAM controller in the c-MMU is applied to access external memory efficiently. By 
re-scheduling the DRAM commands, the effective bandwidth of DRAM can be improved 
by 1.42x. 
For SVC in the wireless video entertainment system, the IPS and address translator 
 18
[6] T.-C. Chen, C.-Y. Tsai, Y.-W. Huang, and L.-G. Chen, “Single Reference Frame 
Multiple Current Macroblocks Scheme for Multiple Reference Frame Motion 
Estimation in H.264/AVC,” IEEE Transaction on Circuits and Systems for Video 
Technology, Vol. 17, No. 2, pp.242-247, Feb. 2007. 
[7] H. Shim, K. Kang, and C.-M. Kyung, “Search area selective reuse algorithm in 
motion estimation,” in proc. of IEEE International Conference on Multimedia and 
Expo, pp.1611-1614, July 2007. 
[8] M.-C. Lin and L.-R. Dung, “Two-step Windowing Technique for Wide Range 
Motion Estimation,” in proc. of IEEE Asia Pacific Conference on Circuits and 
Systems, pp.1478-1481, November 2008. 
[9] C.-Y. Tsai, T.-C. Chen, T.-W. Chen and L.-G. Chen, “Bandwidth Optimized 
Motion Compensation Hardware Design for H.264/AVC HDTV Decoder,” in Proc. 
of IEEE International Midwest Symposium on Circuit and Systems, pp. 1199-1202, 
Aug. 2005. 
[10] R.-G. Wang, J.-T. Li and C. Huang, “Motion Compensation Memory Access 
Optimization Strategies for H.264/AVC Decoder,” in Proceeding of IEEE 
International Conference Acoustics, Speech, and Signal Processing, pp. 97-100, 
Mar. 2005. 
 20
77-83, Apr. 2008.  
[6] P.-T. Huang, S.-W. Chang, W.-Y. Liu, and W. Hwang, “Green Micro-architecture 
and Circuit Co-Design for Ternary Content Addressable Memory,” Proc. of IEEE 
International Symposium on Circuits and Systems (ISCAS), pp. 3322-3325, May 
2008. 
[7] L.-P. Chuang, M.-H. Chang, P.-T. Huang, C.-H. Kan, and W. Hwang, “A 5.2 mW 
All-Digital Fast-Lock Self-Calibrated Multiphase Delay-locked Loop,” Proc. of 
IEEE International Symposium on Circuits and Systems (ISCAS), pp. 3342-3345, 
May 2008. 
[8] H.-I Yang, S.-Y. Lai and Wei Hwang, “Low-Power Non-Pre-Charged Bitline 8-T 
SRAM with Low-Voltage Swing Write Wordline,” Proc. of IEEE International SoC 
Conference, (SOCC), pp.239-242, Sept. 2008. 
[9] W.-C. Hsieh and Wei Hwang, “In-Situ Self-Aware Adaptive Power Control with 
Multi-Mode Power Gating Network,” Proc. of IEEE International SoC Conference, 
(SOCC), pp.175-178, Sept. 2008. 
[10] M.-H. Chang, L.-P. Chuang, Y.-M. Chang, and Wei Hwang, “A 300-mV 36 uW 
Multiphase Dual Digital Clock Output Generator with Self-Calibration,” Proc. of 
IEEE International SoC Conference, (SOCC), pp. 97-100, Sept. 2008. 
[11] M.-T. Chang, P.-T. Huang, and W. Hwang, “A Robust Ultra-low Power 
Asynchronous FIFO Memory With Self-Adaptive Power Control,” Proc. of IEEE 
International SoC Conference, (SOCC), pp.175-178, Sept. 2008. 
[12] M.-T. Chang and W. Hwang, “A Fully Differential Subthreshold SRAM Cell with 
Auto-Compensation,” Proc. of IEEE Asia Pacific Conference on Circuits and 
Systems (APCCAS), pp. 1771-1774, Dec. 2008. 
[13] S.-C. Yang, H.-I Yang, C.-T. Chuang, and Wei Hwang,, “ Timing Control 
Degradation and NBTI/PBTI Tolerant Design for Write –Replica Circuit in 
Nanoscale CMOS SRAM,” Proc. of IEEE International Symposium on VLSI 
Design, Automation and Test, pp. 162-165, Apr. 2009. 
[14] H.-I Yang, C.-T. Chuang, and Wei Hwang, “Impacts of NBTI on SRAM Array with 
Power Gating Structure,” Proc. of IEEE International Symposium on VLSI 
Technology, Systems and Applications, pp. 76-77, April 2009. 
[15] H.-I Yang, C.-T. Chuang, and Wei Hwang, “Impact of Gate-Oxide Breakdown on 
Power-Gated SRAM,” Proc. of IEEE International Conference on Integrated 
Circuit Design and Technology, May 2009. 
[16] H.-I Yang, C.-T. Chuang, and Wei Hwang, “Impacts of NBTI and PBTI on 
Power-Gated SRAM with High-k Metal-Gate Devices,” Proc. of IEEE International 
Symposium on Circuits and Systems (ISCAS), pp. 377-380, May 2009. 
[17] H.-I Yang, C.-T. Chuang, and Wei Hwang, “Impacts of Contact Resistance and 
NBTI/ PBTI on SRAM with High –k Metal-Gate Devices”, Proc. of IEEE 
International Workshop on Memory Technology, Design, and Testing (MTDT), pp. 
27-30, Sept. 2009. 
[18] S.-C. Yang, H.-I Yang and Wei Hwang, “A Micro-Watt Multi-port Register file 
with Wide Operation Voltage Range” Proceedings, Proc. of IEEE International 
Workshop on Memory Technology, Design, and Testing (MTDT), pp. 34-37, Sept. 
2009. 
[19] Y.-M. Chang, M.-H. Chang and Wei Hwang, “A 2.1uw 0.3V -1.0V Wide-Locking 
 22
[8] Y. Chang, P.-T. Huang and W. Hwang, “A Capacitive Boosted Buffer for 
Energy-Efficient and Variation–Tolerant Sub-threshold Interconnect,” Electronic 
Technology Symposium (ETS), Jun. 2009. 
[9] C.-W. Liu, W.-C. Hsieh and Wei Hwang, “Adaptive Voltage Scaling for Discrete 
Cosine Transform,” Electronic Technology Symposium (ETS), Jun. 2009. 
[10] Y.-M. Chang, M.-H. Chang and Wei Hwang, “An Ultra-Low Power Harmonic-Free 
Multiphase DLL Using a Frequency-Estimation Selector,” 20th VLSI Design/CAD 
Symposium, Aug. 2009. 
[11] J.-Y. Wu, W.-C. Hsieh, T.-H. Tsai and Wei Hwang, “High Efficiency Power 
Management System for Solar Energy Havesting Applications,” 20th VLSI 
Design/CAD Symposium, Aug. 2009. 
[12] S.-C. Yang, H.-I Yang and Wei Hwang, “A Sub-threshold Multi-Port Register File,” 
20th VLSI Design/CAD Symposium, Aug. 2009. 
 
專利 
 
專利名稱 發明人 獲證日期 專利證號 
蝴蝶式比較線結構 黃柏蒼、張書瑋、黃威 2010/5 I324346 
內儲存無關項之階層式搜尋線 黃柏蒼、張書瑋、張銘宏、黃威 2010/3 I321793 
Stored Don't-care Based 
Hierarchical Search-line 
S.-W. Chang, P.-T. 
Huang, W. Hwang 
and M.-H. Chang 
2009/4 No.7525827 
Super Leakage Current Cut-off 
Technique for Ternary Content 
Addressable Memory 
P.-T. Huang, W.-Y. 
Liu and W. Hwang 2009/11 No.7616469 
Leakage Current Cut-off Device 
for Ternary Content Addressable 
Memory 
P.-T. Huang, W.-Y. 
Liu and W. Hwang 2010/6 No.7738275 
低功率可快速寫入單端多埠靜態
可隨機存取記憶體 黃威、楊皓義 申請中  
使用多模式功率閘網路具自我感
知之適應性功率控制 黃威、謝維致 申請中  
三元內容可定址記憶體漏電流截
斷裝置 黃威、黃柏蒼 申請中  
全數位快速鎖定自我校正相位延
遲鎖定電路 
黃威、莊立溥、張
銘宏 
申請中  
 24
附錄 – 完整技術報告 
 
Chapter 1 
 Introduction 
1.1 Motivation 
For development of system on a chip (SoC) and multimedia technologies, amount of 
data and computing required to be processed increase quickly. Multi-task processing 
technique is more and more important for integrating various processor elements into a 
chip [1.1]-[1.3]. Generally, most of systems require the memories for storing. In 
multi-task environment, memory is center of storage system, and it is the most serious 
bottle neck because the performance of processor elements is much faster than the 
memory. Accordingly, the organization of memory system for a multi-task system will 
affect the system performance dramatically. 
In addition, multimedia technologies are usually applied in multi-task systems for 
video processing. These technologies have not only provided existing applications like 
desktop video/audio but also spawned brand new industries and services like digital video 
recording, video-on-demand services, high-definition TV, digital home sever, etc. It 
generally needs huge memory requirement for high quality or multiple scalable level 
video processing. The memory system needs to provide enough memory space and high 
data bandwidth for satisfying the video real-time requirement. 
In order to provide huge bandwidth requirement for multi-task system, a multilevel 
memory hierarchy is a well-known design methodology. A well-organized memory 
hierarchy system can have fast memory access time provided by highest hierarchy level 
memory and cheap cost per storage bit provided by the lowest hierarchy memory. In 
 26
Chapter 2 
Related Researches of Memory Systems 
In this chapter, the related research of memory system including cache and DRAM 
systems will be introduced. Furthermore, the previous work of reconfigurable cache and 
DRAM controllers will be introduced, too. Firstly, the concept of memory hierarchy will 
be described in section 2.1. After that, the overview of cache and DRAM systems will be 
described in section 2.2 and 2.3, respectively. 
2.1 Memory hierarchy 
 
Fig.2. 1 Memory hierarchy 
In computer or SoC systems, memory elements are necessary for data storage, and 
the most important development concept is memory hierarchy because a well-organized 
hierarchy enables the memory system to have both advantages simultaneously which are 
the fastest memory access time and the cheapest cost per storage bit. The memory 
hierarchy is base on a principle of locality including temporal and spatial locality. In 
general, the memory hierarchy is described as a pyramid which is shown in Fig.2. 1 [2.1]. 
The higher levels have better performance than the lower levels, but the cost per bit is on 
the contrary. In ideal, the processor element can access the data with the best memory 
access performance and have large memory space. Nowadays, the hierarchy is formed 
with Cache(SRAM), DRAM and Disk storage elements. The list of the performance and 
energy consumption is shown in Table.2.1. So far, there are no storage element can 
provide low cost, high bandwidth and low latency simultaneously. The memory hierarchy 
 28
 
Fig.2. 2 A simple cache memory. 
The mapped structure of the above example is called direct mapped because all the 
memory block address is directly mapped to a single location in the cache. Another 
extreme mapped method is called fully associative mapped which the memory block can 
be placed in any location in the cache. To find a wanted block in a fully associative cache, 
whole entries in the cache must be searched. The hardware cost significantly increases 
because it needs more number of parallel comparators. The middle mapped scheme 
between direct mapped and fully associative is called set associative. Fig.2. 3 shows the 
examples of different associativity structures for a four-block cache. 
 
Fig.2. 3 A four-block cache configured as direct mapped, two-way set associative, and fully associative. 
 30
set-associative cache organization. The second one is that the different requests which 
address to different partitions can be isolated from each other. However, the drawback of 
this organization is that the number and granularity of the partitions are limited by the 
associativity of the cache. 
Albonesi [2.5] proposed a selective cache ways method for on-demand cache resource 
allocation. The technique disables a subset of the ways in the set associative cache to 
have lower energy consumption. Parthasarathy [2.4] presented the reconfigurable caches 
for media processing applications, and the associativity-based partitioning mechanism 
was selected. In contrast to simply turning off some partitions in [2.5], it suggests using 
the partitions for alternate processor activities to enhance performance. Zhang [2.6] 
proposed the highly configurable cache architecture for embedded systems. The basic 
principle is also base on associativity-based partitioning. The cache used a way 
concatenation technique so that it can be configured by software to be direct-mapped, 
two-way or four-way set associative. 
Overlapped wide‐tag partitioning 
Another partitioning method is called overlapped wide-tag partitioning [2.4]. The 
different part to the conventional cache is indicated by the dark-shade regions shown in 
the Fig.2. 5. This partitioning increases the tag array bit size to support the maximum tag 
bit variation with various partition sizes. According to this organization, the size of 
partition can potentially be any size, but generally the size would be limited to be powers 
of two to have simpler implementation. The main drawback of this partitioning is that the 
data in all blocks requires be flushed when the resizing occur because the mapping of the 
address has been changed. 
 
Fig.2. 5 Overlapped wide-tag partitioning organization for reconfigurable caches 
Yang [2.7] proposed an i-cache design that the cache size can dynamically be changed, 
 32
Molecular caches support selective enablement of molecules according to different 
application requirements so that the dynamic power dissipation can be reduced. The 
physical organization of molecules is shown in Fig.2. 8. The ‘M’ is the symbol of a 
molecule. 4-8 tiles are grouped into a tile cluster, and every cluster is associated with a 
tile controller named Ulmo. It processes the coherence traffic and tile-misses between 
clusters. Fig.2. 9 shows the cache access method. Each molecule is configured with the 
Application Space Identifier (ASID) which uniquely identifies a running application. 
Before any cache operation is performed on the molecules, an ASID match is performed 
to see if the molecule is eligible to perform the operation. 
  
Fig.2. 8 Tiles - A physical organization of molecules. 
 
Fig.2. 9 Different steps in cache access in the molecular cache 
 34
 
Fig.2. 11 Basic structure of the reconfigurable Amorphous Cache for processors with large on-chip cache 
memories 
2.2.2.2 Data Consistency 
Another problems need to conquer is data consistency after resizing the cache. 
Reconfigurable caches need a mechanism to ensure that the data which belongs to a 
particular processor element resides only in the partition associated with that particular 
activity [2.4]. Generally there are two approaches for the data consistency which are 
cache scrubbing and Lazy transitioning. The concept will briefly be introduced as 
follows. 
Cache scrubbing 
Cache scrubbing scheme moves all valid data to the new partition parts or lower levels 
of memory when the reconfiguration happened. At the time of reconfiguration, this 
approach requires examining all the locations of the cache to check for their validity and 
performing suitable actions on valid data [2.4]. Cache-scrubbing would induce big 
overhead because of the huge data access. But it can be acceptable when the 
 36
Vectors(BBV)-based tuning technique to trace the loop characteristics of the program in 
the runtime, and it dynamically learned the configuration type by holding the previous 
CPI value. 
The related works of the reconfigurable caches are shown in the Table.2. 2. 
Work Partitioning mechanism 
Data 
consistency
Detection 
mechanism 
Reconfigurable 
cache level Application 
[2.2] Molecular-based Cache scrubbing 
Hardware 
controlled; 
Dynamic 
strategy 
L2 General purpose 
[2.4] Associativity-based Cache scrubbing 
Software 
controlled L1 Media processing
[2.5] Associativity-based Lazy transitioning
Software 
controlled; 
Dynamic 
strategy 
L1 General purpose 
[2.6] Associativity-based N/A 
Software 
controlled; 
Static strategy
L1 Embedded System 
[2.7] Overlapped wide-tag 
Cache 
scrubbing 
Software 
controlled, 
Static/Dynamic 
strategy 
L1 I-cache General purpose 
[2.8] Hybrid Cache scrubbing 
Software 
controlled 
Static/Dynamic 
strategy 
L1 General purpose 
[2.9] 
Overlapped 
wide-tag 
Molecular-based 
Cache 
scrubbing 
Software 
controlled 
dynamic 
strategy 
Shared cache 
Multi-core 
Network-intensive 
applications. 
[2.10] Molecular-based Cache scrubbing 
Software 
controlled; 
Dynamic 
strategy 
L2 General purpose multi-core 
[2.11] Molecular-based N/A 
Software 
controlled 
Dynamic 
strategy 
L2 General purpose multi-core 
Table.2. 2 Related work of adaptive caches 
 38
2.3.1.2 DRAM command and operation 
The normal commands and its operation used in DRAM will be introduced as follows. 
NO OPERATION (NOP): 
The NOP command can prevent unwanted commands from being registered during 
idle or wait states. Operations already in progress are not affected. 
ACTIVE: 
This command is used to open a row in a particular bank. The row remains open for 
accesses until a PRECHARGE command is issued to that bank. 
READ/WRITE: 
The read/write command is used to initiate a read/write access to an active row, if auto 
precharge is selected, the row being accessed will be closed at the end of read. 
PRECHARGE: 
The precharge command is used to deactivate the open row in a particular bamk. The 
bank will be available for a subsequent row access a specified time (tRP). 
REFRESH: 
The refresh command can be used to retain data in the DRAM. 
A memory access operation, which simplified state diagram is depicted in Fig.2. 13, 
contains three operation including row activation (ACTIVE), column access (read/ write), 
and precharge. 
 
Fig.2. 13 Bank state diagram. 
The active command opens a particular row in one of the bank, and copies the row 
data into the row buffer. The active command needs a latency period called tRCD to 
 40
data or motion information. But the memory access speed is much slower than the 
processor unit execution speed. Many researchers have shown the well memory 
management method according to the regular memory access behavior in video process 
can significantly improve the overall system performance. 
 Base on the different specific applications, there have several approaches been 
proposed to increase the efficiency of memory access for video coding applications. Kim 
memory interface architecture [2.13] reorganizes data arrangement in synchronous 
DRAM to increase the row-hit rate. Park proposed a memory node control approach 
[2.14] for HDTV video decoder. It uses history-based prediction to predict the next 
command is row-hit or row-miss. If it predicts the next command is row-miss, it will 
pre-charge the current bank. If row-hit, the current row will stay in the active state. The 
prediction is implemented by a finite state machine which shown in Fig.2. 16. 
 
Fig.2. 16 State machine for storing page hit history information. 
Chang proposed a two-layer external memory management unit [2.15] for H.264/AVC 
decoder. The memory management unit consists of two layers. The first layer is the 
address translation which provides an efficient pixel data arrangement to reduce the 
row-miss occurrence. The second layer is the external memory interface (EMI). In the 
address translation layer, the address translation machine uses a novel data arrangement 
which is suitable for H.264/AVC decoder to increase the memory bandwidth and reduce 
the power consumption. In order to minimize the number of active and pre-charge, 
chessboard-based arrangement memory mapping is presented as shown in Fig.2. 17. It is 
further compounded with the fact that Luma and Chroma are placed interleaved. The 
interlaced memory mapping method put the luminance block and chrominance block in 
 42
history-based memory mode controller, Zhu [2.16] and Hongqi [2.17] adjust the page 
size.  These designers are trying to reduce the total row-miss and minimize the DRAM 
access latency. In the advanced memory controller, rearrange data is necessary to reduce 
the access latency. In addition, the advance video coding standard, H.264/AVC, provides 
several new coding tools including sub-pixel inter-prediction, variable block size motion 
compensation. Although these techniques can reduce bit-rate and improve the video 
quality, they require huge memory bandwidth to fetch additional reference pixel for 
motion compensation(MC) and interpolation. Fortunately, designers can use data reuse 
scheme to reduce the sub-pixel MC data loading bandwidth from DRAM. Interpolation 
window reuse(IWR) scheme was [2.18] proposed to reduce data access for the 
overlapped data. Li [2.19] proposed a cache-based architecture to reuse intra-MB 
overlapped data, and Chuang [2.20] also proposed an IWR-liked with N-way associative 
cache architecture to reuse inter-MB and inter-MB overlapped data. 
In order to improve the bandwidth, Kang [2.21] and Heithecker [2.22] proposed 
multi-channel memory controller. The concept of the multi-channel can be applied to the 
general purpose memory controller. In the SoC system design, a variety of processor 
elements integrate into a chip. Different applications have different memory needs, 
finding a single topology that fits well with all applications is difficult, in order to adopt a 
variety of the functions, flexible and adaptable memory control is more and more 
important in SoC systems. Furthermore, in the multi core systems, the multi-channel 
memory controller will be needed to support high bandwidth and provide different 
application memory requirement. There are many researches develop many kind of 
efficiency memory systems. Lee [2.23] presents a multilayer, quality-aware memory 
controller to satisfy different memory access requirement. Fig.2. 19 shows the 
configurations of different layers of the proposed memory controller. Layer 0 is called 
memory interface socket (MIS), it is a configurable, programmable, and high-efficient 
SDRAM controller for designers to rapidly integrate SDRAM subsystem into their 
designs. Layer 1 is quality-aware scheduler (QAS), it is a memory controller layer which 
has the capability to provide quality-of-service guarantees including minimum access 
latencies and fine-grained bandwidth allocation for heterogeneous processor elements in 
SoC designs. Moreover, Layer 2 built-in address generator (BAG) designed for 
 44
bandwidth significantly increased in these years. For discussing the DRAM, the 
important issues are bandwidth, latency, and power. This section will introduce the 
development of DRAM that improve the performance and the future trend of DRAM. 
 
Fig.2. 20 DRAM roadmap 
2.3.3.1 Bandwidth 
The improvement of DRAM bandwidth has never satisfied the increasingly 
complicated application such as multimedia and 3D processing. To fulfill the demand for 
high bandwidth, various new DRAM specifications have been announced by DRAM 
manufacturers. The SDRAM standards supported by JEDEC [2.30] have become the 
mainstream of DRAM market. Several techniques have been applied on the latest 
standards announced by JEDED to provide users higher bandwidth. 
Component I/O bus clock 
(MHz) 
Data transfer rate 
(MT/s) 
Peak transfer 
rate(MB/s) 
SDR 133 133 532 
DDR 200 400 3200 
DDR2 533 1066 8533 
DDR3 800 1600 12800 
Table.2. 3 The maximum transfer rate for SDR, DDR, DDR2 and DDR3 
 46
banks can be increased. Table.2. 4 shows the number of banks of the DDR family. 
 SDR DDR DDR2 DDR3 
Number of banks 4 4 4,8 8 
Table.2. 4 Number of banks for SDR, DDR, DDR2 and DDR3 
 
Fig.2. 22 Accesses addressed to same bank 
 
Fig.2. 23 Accesses addressed to different bank 
2.3.3.3 Power 
In many application of portable wireless devices such as mobile and PDA, power 
consumption is the significant issue because of battery life is limited. With the 
application of multimedia becomes popular, the request of memory size is larger. 
Accordingly, the designers often select DRAM to be the body memory component. 
In order to reduce the power of DRAM, many products have been invented for low 
power such as BAT-RAM from micron [2.31] and Mobile-RAM from Infineon [2.32]. 
The low-power DRAM has some special features inside. 
Low Operating voltage 
 Compare with SDR SDRAM, the operating voltage of low-power DRAM is 
lowered from 3.3v to 1.8v. Thus, the power consumption can significantly be decreased. 
 48
2. Stopping the clock altogether. 
Both of these are specific to the application and its requirements and both allow power 
savings due to possible fewer transitions on the clock path. 
The clock can also be stopped altogether if there are no data accesses in progress, 
either WRITE or READ, that would be affected by this change; i.e., if a WRITE or a 
READ is in progress, the entire data burst must be through the pipeline prior to stopping 
the clock. 
For the full duration of the clock stop mode. One clock cycle and at least one NOP is 
required after the clock is restarted before a valid command can be issued. 
It is recommended that the DRAM be in a pre-charged state if any changes to the 
clock frequency are expected. This will eliminate timing violations that may otherwise 
occur during normal operations. 
Power-Down 
Power down can occurs when all banks idle, this mode is referred to as precharge 
power-down. If power down occurs when there is a row active in the bank, this mode is 
referred as active power-down. Entering power-down mode deactivates all input and 
output buffers, therefore the power is saved. 
Deep Power-Down 
 Deep power down is an operating mode used to achieve maximum power 
reduction by eliminating the power of the memory array. Data will not be retained when 
the device enters power-down mode. Since DRAM is often used as temporary data 
buffers, enter DPD mode while the device is in standby mode won’t cause any loss. 
 50
various digital communication products are developed in our life. These modern 
electronic products provide more convenient communication environment and media 
enjoyment for humans than those before. However, with different applications or 
standards, a variety of devices would be needed. Fig.3. 1 illustrates a heterogeneous 
network environment in our life. In recent years, merging different networks, electronic 
appliances and media devices into a heterogeneous integrated platform becomes an 
important issue that enables people enjoy their life in an more friendly and 
energy-efficient digital environment. 
 
Fig.3. 2 Homogeneous multi-core platform (a) Intel Polaris (b) Tilera TILEPro64TM Processor 
 
Fig.3. 3 Trend of the data transmitting bandwidth 
To integrate various applications into a system, a multi-task/multi-core concept 
provide a typical solution to build the system. The design of multi-core platform is a 
popular research area recently [3.1]-[3.7]. Fig.3. 2 shows two homogeneous multi-core 
platforms. Intel proposed an 80-core platform as shown in Fig.3. 2(a) [3.1] and Tilera 
[3.2] proposed a 64-core platform as shown in Fig.3. 2(b). These multi-core platforms 
 52
stream processors can be integrated in the platform. In this platform, each processor 
element owns distributed memory management unit (d-MMU). The d-MMU includes 
local cache (D-cache and I-cache) and cache controller which can efficiently handle all 
memory requests generated by the processor elements. It can dynamically allocate unused 
space in cache for buffering the transmitting data. If processor elements need additional 
memory resource requirements, the centralized memory resources including centralized 
cache and off-chip DRAM can be used. It is controlled by a centralized memory 
management unit (c-MMU). It can dynamically allocate and manage the memory 
resources according to different memory requirements. 
For the data communication between processor elements, message-passing technique 
is applied for this platform. The processor elements transmit/receive the data to/from 
others through an on-chip interconnection network. Network interface is applied to 
packetize the transmitted data to interconnection and de-packetizes the received data 
from interconnection. Furthermore, in order to have better energy utilization for green 
computing, the power management unit can be applied to dynamically control the supply 
voltage and operating frequency of each processor element for saving energy 
consumptions. 
 
Fig.3. 5 The architecture of memory-centric on-chip data communication platform 
In the heterogeneous multi-task platform, different processor elements would have 
 54
memory hierarchy level in the system. DRAM controller is needed to access the off-chip 
DRAM devices. It includes an external memory interface and address translator to 
improve the memory access efficiency. 
In the on-demand memory system, all processor elements own a private address space 
and can dynamically be allocated. For data switching between processor elements, 
message-passing mechanism is used. On-chip interconnection network in the platform is 
designed for data communication. Note that the thesis is focus on on-demand memory 
system. The design of interconnection network is not included in this thesis. 
In conclusion, adaptive memory resource allocation can be achieved and the memory 
utilization can be improved by the memory management units. The detail organizations 
and the design of these memory management units are described in chapter 4. 
 
Fig.3. 6 Illustration of the memory hierarchy in on-demand memory system 
3.3 Wireless Video Entertainment Systems 
With the ongoing advancement in digital and communication techniques, digital home 
service becomes a trend nowadays. In the daily life, home is the personal headquarters for 
living, keeping personal assets and information. If the digital home services are applied, 
 56
Multi-System
MAC
Streaming 
Units
Display 
Units 
Peripherals
Processor
Scalable Video
Coding
Analog
Front-end
Joint Channel/ 
Source Coding
MIMO-OFDM PHY
WUWB
3G HSOPA
WiMAX
Terrestrial
WLAN
On-Demand
Memory System
PAC DSP
 
Fig.3. 7 Multi-Task wireless video entertainment system 
 
Fig.3. 8 Transmitter and receiver block diagram 
In order to support various communication standards and have video entertainment for 
digital home service, a wireless video entertainment systems is developed. It includes 
four functional blocks. The block diagram of wireless video entertainment system 
transceiver is shown in Fig.3. 8. In this system, Scalable Video Coding (SVC), the 
extension of the H.264/AVC standard technique, is applied to provide spatial, temporal 
and quality scalability of the video sequences [3.9]. For the channel coding, Luby 
Transform (LT) coding, one kind of error correcting method, is applied to have high 
channel reliability. Media Access Control (MAC) module is the interface between 
 58
z Frequency-domain (FD) synchronization 
1. FD Adaptive Sampling  
2. FD Boundary Decision 
3. FD Anti-I/Q Phase Recovery 
z Single carrier frequency domain equalizer (SC-FDE) 
1. Frequency-domain channel estimation (FD-CE) 
2. Frequency-domain ISI cancellation for DSSS non-CP SCBT 
3. Frequency-domain data decision 
z FD adaptive sampling 
1. 6-symbol Lock 
2. 32 multiphase clocking 
3. Boundaryless 
4. Tolerance of -30,000~40,000 ppm SCO as shown in Fig.3. 10. 
 
Fig.3. 10 Single-FFT Architecture for MIMO Modem 
...
Bo
un
da
ry
 o
ffs
et
In
se
rte
r
S
ta
tis
tic
al
 o
pe
ra
tio
n
(m
od
e)
...
…Sign bit m
ask
...
... ...
C
on
fid
en
ce
 fi
lte
r
Tw
o 
Pa
ra
lle
l 
co
m
pa
ra
to
r b
an
ks
 
( ),t kΦ R Pk ( ),t kΦ R Pk
 
Fig.3. 11 Single-FFT Architecture for MIMO Modem 
Moreover, For the FD boundary decision, it contains the following features, only 1% 
detection error with low SNR (<5 dB) and gigh CFO tolerance. It is a trellis-based 
detector, and can be used both for DSSS and OFDM different systems. Fig.3. 11 displays 
 60
As shown in Fig.3. 12, the MAC protocol layer, in terms of implementation, could be 
separated in two parts: the Data Plane and the Control Plane. The main function of the 
Data Plane is production of MAC layer’s protocol data units (PDUs). It could either be 
analyzed with electronic system level (ESL) methodologies, or realized by FPGA 
hardware solutions. The Control Plane takes control of the Data Plane according to 
various signal feedbacks. These feedbacks include PHY-to-MAC, Network-to-MAC and 
inter-BS or BS-to-MS signaling.  
Besides data processing performance that directly relates to software/hardware 
co-design, there are other factors that have great impact or overall system performance. 
For example, the Request/Grant mechanism – the content of MS request shall be properly 
received and recognized by BS, and then properly responded, vice versa. Some MAC 
transmission mechanisms including auto retransmission request (ARQ), handover, uplink 
scheduling, external environmental mechanisms such as BS-end or MS-end channel 
condition, could deeply influence system performance. Unfortunately, it is difficult to 
analyze and verify the interaction of MAC functional interactions. The inter-node 
concepts cover a range even broader than system-level design flows, and traditionally the 
verification of Control Plane begins at a later stage of design flow. 
3.3.3 LT Coding 
LT code is a class of rateless codes. Its performance is approximately close to channel 
capacities of arbitrary erasure channels. In theory, LT encoder generates infinite 
codewords. Each receiver starts decoding when sufficient codewords are collected. In 
spite of which codeword set is collected, the high recovery probability of source symbols 
is guaranteed. Consequently LT codes are channel independent and require no 
retransmission. For block codes, when there are too many codewords erased within a 
block, codewords in this block are undecodable and retransmission is needed. However, 
retransmission can jam the transmission and paralyze multicasting servers in multicasting. 
In comparison with block codes, LT codes are more suitable for multicasting. Recently, 
pre-codes concatenated with LT codes are standardized in 3GPP MBMS. 
 62
exploited by BP decoding. After BP decoding, codeword 2, 4, and 5 are left. Notice that, 
the source symbol 1 can be recovered by performing exclusive-or on codeword 2 and 
codeword 5. Similarly, source symbol 4 can be recovered by performing exclusive-or on 
codeword 2 and codeword 4. Finally, source symbol 5 is recovered by performing 
exclusive-or on codeword 2, codeword 4, and codeword 5. For rateless codes, decoding 
complexity is proportional to the total number of codeword degrees. After BP decoding, 
most of the codewords are removed. Besides, the average degree of remaining codewords 
is decreased. For example, with K=1000 and N=1120, the average degree of the received 
codewords is 43.6. After BP decoding, the average degree of remaining codewords is 8.3 
and the corresponding degree distribution is shown in Fig.3. 13. In addition, the average 
number of remaining codewords is 85.9. The total number of codeword degrees are 
(43.6×1120)/(8.3×85.9)=68.5 times less after BP decoding. It is efficient to conduct more 
complicated decoding methods to recover the information in the remaining codewords. 
3.3.4 Scalable Video Coding (SVC) 
 
Fig.3. 14 Architecture of an SVC encoder 
Recently, with the prosperity of the Internet video, digital television, and portable 
devices, the demand of digital video becomes more and more diversified. To deal with 
those diversified video applications, Scalable Video Coding, the latest video coding 
 64
memory and processor unit is large in the SoC system. Many researches are trying to 
minimize the speed gap. A well-organized memory management can significantly reduce 
the memory access latency. According to the data features of these applications, designer 
can find a well memory allocation method to reduce the number of memory access time 
and average access latency. Accordingly, for wireless video entertainment systems, 
memory-centric on-chip data communication platform is applied to provide a high 
bandwidth and satisfy enough memory requirements. 
According to the receiver system as mentioned in section 3.3, the processing sequence 
of these multiple tasks is generally step by step. the data stream of wireless video 
entertainment systems is shown in Fig.3. 15. In memory-centric on-chip data 
communication platform, on-demand memory system can support heterogeneous and 
real-time memory requirement for wireless video entertainment systems. MMUs in 
on-demand memory system enable the processor elements to have adaptive memory 
resources. Base on different memory requirement of these processor elements, centralized 
MMU can dynamically allocate memory resources for processor elements. With suitable 
memory resource arrangement for different processor elements, the execution efficiency 
of the streaming processing in wireless video entertainment systems can be improved. 
 
Fig.3. 15 Data stream of wireless video entertainment systems 
Overall architecture of the system is shown in Fig.3. 16. The system components can 
be categorized into data computation, data communication and data storage. For data 
computation, it includes WPU, MAC, LT coding and SVC processor elements. Wrappers 
are applied to satisfy the specification of the pre-defined protocol. Subsequently, the 
other components will be introduced as follows. 
 66
allocating different cache resources for different processor elements. In c-MMU, a 
DRAM controller is constructed to efficiently access off-chip DRAM. In DRAM 
controller, Address translator rearranges and translates address to have an efficient 
memory allocation, and the memory requests enter the memory interface with command 
scheduling to reduce memory access latency. The detail description of c-MMU will be 
described in chapter 4. 
 68
4.1.1 Design of d-MMU 
For the memory-centric on-chip data communication platform, d-MMUs are designed 
for PEs to store the temporal data of their tasks. Distributed cache performs as a high 
level cache for the dedicated PE in the on-demand memory system. In addition, a 
Wrapper is applied to be an interface between processor element and d-MMU. In 
on-demand memory system, PE uses the burst-based memory access protocol to access 
memory. By this protocol, read/write operation uses burst transmission mechanism so 
that it can access continuous data easily. The detail memory access operation will be 
introduced as follows. 
4.1.1.2 Memory access operation 
By applied burst-based memory access protocol, the read and write operations are 
shown in Fig.4. 2 and Fig.4. 3, respectively. With providing start address and burst 
length(BL) information, processor elements can efficiently access the burst data in 
memory. Note that the data width is 32-bit (1word) and the addressing unit is in word by 
definition. Accordingly, the cache miss penalties can be hidden by burst-based memory 
access protocol. The cache miss would be discovered immediately when a memory burst 
request has been served. Fig.4. 4 provides the explanation of hiding miss penalties. In 
Fig.4. 4(a), a read request with miss follows by a read request with hit. The miss penalty 
can be hidden because the data transmit of the first read haven`t been finished. For the 
memory write request as shown in Fig.4. 4(b), all the miss in the burst can be found 
immediately whenever write request comes, so it also can hide the miss processing 
latency of memory write. 
 70
 
Fig.4. 4 Illustration of hiding miss penalty 
Additionally, NI is designed as a bridge between the PEs and the OCIN [4.1]-[4.4]. NI 
contains the input queue and output queue for buffering packets. However, the sizes of 
the queues dominate the area and the performance. If the buffer is insufficient, the PE 
will be stall until the head-of-line blocking releases. Therefore, if the utilization of the 
distributed memory is low, the d-MMU can borrow the memory resources for buffering 
the blocking packets from the PEs, and the PEs can keep computing for their tasks. 
Below the d-MMU with buffer borrowing mechanism will be introduced in detail. 
4.1.1.2 Buffer Borrowing Mechanism 
The architecture of proposed d-MMU and efficient Network Interface with buffer 
borrowing mechanism is shown in Fig.4. 5. The NI uses a buffering control to generate a 
borrowing request to the d-MMU for borrowing memory resources. And thus, the 
d-MMU checks the valid table and generates the borrowing address for the NI. Fig.4. 6 
presents the buffer borrowing interface between the NI and d-MMU. The operations of 
the buffer borrowing include write, read and release. For the write operation, the 
buffering control should send a buffer request to the d-MMU first, and send the blocking 
data until receiving a grant signal. However, the head-of-line blocking may release while 
waiting the grant from d-MMU or setting the data. Therefore, a release operation can 
release the extension memory resources. The details of the borrowing address generator 
and buffering control will be described as follows. 
 72
contains 4x8 words. Therefore, the maximum payload of a packet can be stored in a 
memory block (8 words) in one cycle. If a memory block is borrowed, the d-MMU 
asserts the status bit that represents the borrowing data. Depending on the status bit, the 
cache control can mask the searching of this table in a searching operation. 
 
Fig.4. 7 Borrowing mechanism in d-MMU 
 
Fig.4. 8 Architecture of the empty memory block searching 
After the NI send a borrowing request to the d-MMU, the NI should take 2-8 cycles 
for collecting the payload. Most packets contain 8 flits in their payloads, and the average 
size of payload is about 4 words. Therefore, the d-MMU has to search the empty memory 
block in 4 cycles. Additionally, the last associated tables in bank 0 and bank 1 contains 
512 valid bits. To search the empty memory block, a 128-bit searching window is 
adopted. Fig.4. 8 shows the architecture of the empty memory block searching. The 
searching window is controlled by a search counter. The empty detector detects an empty 
 74
4.1.1.2.2 Buffering Control 
The buffering control in NI detects the empty size of the output queue and sends the 
borrowing operations to d-MMU. Fig.4. 10 shows the block diagrams of borrowing 
mechanism in the buffering control. The buffering control sends the write, read, and 
release operations depending on an empty pointer of the output queue and a borrowing 
pointer of the borrowing header queue. The empty pointer and borrowing pointer indicate 
the number of the occupied buffers in the output queue and borrowing header queue, 
respectively. In addition, the write control contains a payload queue for collecting the 
payload, and then writing this payload to the borrowed memory block. The borrowing 
control policy of the buffering control is presented as shown in Fig.4. 11. The borrowing 
mode indicates whether the blocking data stored in the d-MMU or not. Therefore, after 
receiving data from the PE, the data should be stored in the d-MMU in the borrowing 
mode. Otherwise, the data can be stored in the output queue when the size of the empty 
slots is larger than the payload. While waiting the borrowing grant from d-MMU and 
collecting the payload, the head-of-line blocking may be released. Therefore, the 
borrowing mechanism can also be released if the borrowing mode equals to zero. The 
release signal will interrupt the search operation of d-MMU. 
 
Fig.4. 10 Block diagrams of borrowing mechanism in network interface 
 76
E
xe
cu
tio
n 
tim
e 
(x
10
6
C
yc
le
s)
N
um
be
r o
f T
ra
ns
fe
rr
ed
 P
ac
ke
t  
(x
10
4 )
 
Fig.4. 12 (a) Execution time under various injection loads and queue sizes (b) Transferred packets under 
various injection loads and queue sizes. 
4.2 Centralized Memory Management Unit Organization 
The distributed memory resources may be insufficient for PEs. Lower level cache is 
applied to provide larger on-chip memory resources. Centralized cache and cache 
controller is included in centralized memory management unit(c-MMU). According to 
distinct memory resource requirements from different PEs, the proposed c-MMU can 
allocate different cache resources for each PE. In addition, the external memory is 
required for storing the huge data such as video frames in video processing. A DRAM 
controller is constructed in c-MMU to access DRAM device. The overall c-MMU 
architecture with adaptive cache control and DRAM controller will be introduced in the 
following sections. 
4.2.1 Design of c-MMU 
The simple block diagram of the c-MMU is shown in Fig.4. 13. It is organized by an 
adaptive cache controller, switches, several SRAM sub-blocks and DRAM controller. 
Adaptive cache controller accepts the memory requests from d-MMUs. The requests 
issued by different d-MMU can simultaneously be executed if the used memory resources 
have no conflict. Cache controller will check the selected cache tables to determine 
whether the data is in the cache or not. According to the check result, the corresponding 
data and addresses are forwarded to the SRAM sub-block or DRAM controller by switch. 
For read requests, the read data forward to the output switch and send back to d-MMUs. 
 78
which had been proposed in [4.5]. With selecting different number of ways, the different 
cache size can be assigned for processor element. It is a simple method with less area and 
timing overhead for cache reconfiguration. In proposed c-MMU organization, 
associativity-based partitioning scheme is applied for the cache partition.  Each SRAM 
sub-block represents a way and form a bank for the cache organization. Assume there are 
number of N SRAM sub-blocks in c-MMU, it represents there have N-way associativity 
capacity in centralized cache. For different processor elements, the SRAM blocks can be 
grouped into several groups for processor elements. Fig.4. 15 shows the example of 
SRAM bank partition. Assume the system have X processor elements and c-MMU has N 
SRAM banks. The memory partition can be achieved as illustrated in Fig.4. 15. 
 
Fig.4. 15 Illustration of the memory partition 
In order to dynamically allocate the memory resources for different processor 
elements at runtime, a Bank Assignment Table (BAT) is applied for recoding the memory 
usage information of three time intervals. Fig.4. 16 illustrates the cache table checking 
method when a request is served. According to the corresponding processor element node 
ID, the cache controller searches the BAT and returns the assigned bank numbers. These 
bank numbers indicate which bank tables need to be checked for the request. Fig.4. 16 
shows the example that four banks are applied for node 3 in the first time interval. When 
a request from node 3 is served, Bank0, Bank1, Bank2 and Bank3 tables will be selected 
for hit checking. By this configuration, node 3 can own a 4-way associativity L2 cache 
memory resource for processing. 
 80
controlled via the detection of MAC and the profiled memory requirements. 
Node 3Node 2Node 1
node TAG node TAG node TAG
Target Bank tables IndexNode 0 TAG offset
node TAG
Hit detector
Hit SRAM number
Hit
Selected by checking BAT
 
Fig.4. 17 Illustration of checking multiple requests 
With changing time intervals, the bank assignment for each processor element will be 
reorganized. The organization may be different from previous configurations. The 
Lazy-based transitioning scheme [4.6] is applied for maintaining data consistency. The 
basic concept of Lazy transitioning has been mentioned in the chapter 2. If a miss occurs, 
the data may remain in the other banks. The bank tables which are assigned in previous 
time interval need to be checked again. The flow chart of the c-MMU adaptive cache 
control is shown in Fig.4. 18. In Box 1, the read or write request will be chose from the 
request queues. The priority of read request is higher than the write request unless the 
data dependency is detected or the write queue is full. Follow that, the corresponding 
tables are chose by BAT information (Box 2). If miss occur, the other tables would be 
checked because the corresponding data may still lie on the centralized memory. The 
situation will be happened when the memory resource for a particular processor element 
have been reduced. In Box 3, the BAT will be check again. If there have the other tables 
need to be checked, the checking operation will be lunched again. Otherwise, the miss 
operation will be executed. After finishing the second hit detection, additional data 
movement will be executed if hit occur (Box 5). The corresponding data will be moved 
 82
search engine searches bank assignment information according to the Node ID and 
selected time interval, and it determine what tables need to be check. At the following 
execution stage, the information in the bank tables has been read out, and then the hit 
detector outputs the corresponding read/write address and bank destination to the 
mux-based switch. If it wants to read data from external memory, the address will enter 
the corresponding bank pending buffer and external memory read queue. When the read 
data returns, the data can be directly write to the corresponding bank and transmit back to 
the corresponding d-MMUs by compare the address in the pending buffer(at third 
execution stage). The bank access input could come from cache controller, external 
memory and the other bank outputs generated by cache reorganization. An arbiter at the 
third execution stage is applied to determine the priority of these access requests for each 
memory banks. The requests from external memory have the highest priority to minimize 
the miss penalty, and the priority of additional write requests generated by cache 
reorganization is set to the lowest. At final execution stage, the cache data are read out 
and forward to the output, external memory or another SRAM banks by switch. For 
external memory request arbitration, read requests are served prior to the write requests.  
In the DRAM controller, the address translator is applied to re-generate friendly 
DRAM address for improving memory bandwidth efficiency and reducing the DRAM 
energy consumption. The design strategy strongly depends on the memory access 
behavior of the applications. The detail design of the address translator will be described 
in chapter 5. Furthermore, the external memory interface is constructed for accessing the 
DRAM data. The design of external memory interface will be intruded in the section 
4.3.2. 
4.2.2 External Memory Interface in DRAM controller 
In the design of memory hierarchy system, it usually needs an off-chip memory to be a 
hierarchy level for storage the large amount of data. The external memory interface is 
used to communicate with the external memory for the system. To deal with tremendous 
data transfer and storage in video processing, the external memory must provide high 
data bandwidth to achieve the real time request. The bandwidth of the external memory is 
 84
the DRAM can operate in parallel, the commands with different banks would enable 
issued without timing constrain. According to this concept, rescheduling DRAM 
commands enables higher bandwidth utilization than in-order issuing. The detail 
architecture of proposed EMI will be described in the next section. 
 
Fig.4. 20 Connection of EMI 
In this work, 1Gb DDR3 SDRAM model provided by Micron Inc. [4.8] is used. 
Several speed grades and configurations can be chosen as shown in Table.4. 2. 15E speed 
grade and 64Megx16 configuration is chosen. There are 8 independent banks in a DDR3 
device. The EMI would recode the bank status and generate appropriate commands 
according to the corresponding bank states. Different speed grades and configurations 
may have different timing constrain, the designer must follow these timing rules to build 
the memory interface. The detail timing issues will also be described in the following 
sections. 
 
Table.4. 2 Micron`s DDR3 configurations 
4.2.2.2 Architecture of EMI 
The architecture of EMI is shown in Fig.4. 21. It consists of three finite state machines, 
FIFOs, command scheduler, Timing counters and I/O control circuit. The operation of 
 86
commands in the right time without any timing violations. It is controlled by several 
timing counters which recode the cycle margins of different timing constraints. When a 
command is issued, the relative timing counters will be set to a certain value and start to 
decrease until the counter is return to zero. The timing counters will be checked when 
issuing new commands from issue FIFO. If there is no timing violation, the command can 
be issued to DRAM. Otherwise, additional stalls will occur. During the time of waiting, 
EMI will issue NOP commands to external memory. The common timing parameters are 
shown in Table.4. 3. In addition, The DRAM needs a long latency to power up and 
initialization including ZQ calibration and mode register loading. Fig.4. 22(b) shows 
command FSM state diagram. It includes initialization states, issue states and several 
waiting states. Initialization states handle the DRAM initializations. Issue states generate 
the appropriate DRAM commands to I/O control block. Additional waiting states would 
stall the command issuing until the following command can be issued legally. 
 
Fig.4. 22 State diagram of EMI Finite State Machines 
The third part is I/O control. When a write command is issued, the write data must be 
sent after column write latency. Also, the read data would appear in the data bus after 
 88
that interleaving ACTIVATE and column access commands as shown in Fig.4. 23(c). 
The proposed Command Scheduler can schedule the ACTIVATE and column access 
command with different banks to the optimal sequence. Note that the calculation of 
cycles in Fig.4. 23 is base on the minimum clock cycle time defined in Table.4. 3. 
 
Fig.4. 23 bank-miss scheduling 
For accessing DDR3 devices, data for any write burst may be followed by a 
subsequent read command after tWTR has been met. It may cause worse bandwidth 
efficiency when the read and write commands are interleaved frequently. Fig.4. 24(a) 
illustrates the example of issuing the read bursts after write bursts. If the successive read 
and write commands have no data dependency, the issue sequence can be exchanged so 
that the bandwidth efficiency can be improved. The example with scheduling is shown in 
Fig.4. 24(b). 
(a). Without schedule
(b). With schedule
WriteReadWriteRead
Write
ReadWriteRead
 
Fig.4. 24 read / write scheduling 
When row-conflict occurs, the PRECHARGE and ACTIVATE commands must be 
 90
efficiency, we define the bandwidth utilization as shown in the following equation to 
calculate the DRAM bandwidth utilization. 
%100
commandsaccessprocessingofcyclesTotal
DRAMbetweendatainputtingandoutputtingofcyclesTotalnUtilizatioBandwidthDRAM ×=  
For simulation, the random access pattern is applied for measuring the DRAM bandwidth 
utilization in the worst case. The successive access requests are random, so the DRAM 
row and bank conflict would happen frequently. For the other simulation information, the 
summary of simulation configurations is shown in Table.4. 4. 
the bandwidth utilization can be estimated, and the simulation result is shown in 
Table.4. 5. With Command Scheduler, it can improve 42.8% bandwidth utilization. 
 Without scheduler With Scheduler Improvement 
Bandwidth Utilization 18.58% 26.53% 42.8% 
Table.4. 5 Simulation of the bandwidth utilization 
4.2.3 Simulation Results of the Adaptive Cache 
In this section, the simulation results of the adaptive cache will be introduced. In the 
beginning of this section, the access latency and energy estimation method of memories 
will be introduced. Base on the measurement method, the simulations for static bank 
assignment and dynamic bank assignment will be described in section 4.2.3.2. 
4.2.3.1 Latency & Energy Estimation method 
For measuring the execution latency and energy consumptions including on-chip 
cache and off-chip DRAM, the estimation methods will be introduced in the following 
sub-sections. 
4.2.3.1.1 Cache Latency Estimation 
For verification and simulation, a cycle-driven model is development by SystemC. 
With constructed systemC models of the memory management units, the cache access 
latency can be considered in simulation. 
 92
 
Fig.4. 26 DRAM latency estimation for different situations 
4.2.2.2.4 DRAM Energy Estimation 
In order to measure the DRAM energy consumption, the system power calculators are 
provided by Micron Technology Inc. [4.9]. These models can estimate the power 
requirement of SDRAM devices in a system environment. These tools provide a friendly 
interface for estimating the memory power requirements needed in making important 
system architecture and design decisions. With an accurate estimation of power 
consumption, the system designer can quickly handle complex system trade-offs to 
optimize the system performance [4.9]. 
According to the selected DRAM and system configurations, the DRAM power 
consumption can be automatically calculated. The configuration summary in our 
simulation environment is shown in Table.4. 6. The example of the DDR3 configuration 
interface in the System Power Calculator is shown in Fig.4. 27 and the system 
configuration interface is shown in Fig.4. 28. To simplify the simulation, only one 
 94
 
Fig.4. 27 DDR3 Configuration interface of the System Power Calculator 
 
Fig.4. 28 System configuration interface of the System Power Calculator 
 96
 
Fig.4. 30 Organization of simulation 
For adaptive cache simulation, the tasks with random memory access are applied in 
each node. A task is composed of 100 number of random memory accesses with a 
particular range. For pipeline behavior, the task in pipe N can be lunched when the task in 
pipe N-1 is done. In the simulation, it is assumed that the memory requirement of each 
node in different intervals of time can be profiled by system. By updating BAT in 
proposed c-MMU with profiled information, adaptive memory resources partition can be 
achieved. Suitable adaptive bank assignment can improve the execution time and energy 
consumption compare to the fixed bank assignment (every node owns equal number of 
banks statically).  
Table.4. 7 lists the simulation information of the memory configurations. Here a 
pattern with memory requirement assumption is simulated in my simulation. Table.4. 8 
lists the information of this pattern. By the assumption of memory requirements, BAT in 
c-MMU can be updated by system for bank assignment. Additionally, assuming the 
memory requirements would be different at runtime, the assumption for three intervals of 
time are listed in Table.4. 8. For simulation, 500 tasks would be finished in a time 
interval. With profiling the memory requirement and re-allocating the bank assignment, 
the memory resources in c-MMU can be utilized effectively. When finishing 1500 tasks 
(three time intervals), 40.41% execution cycles and 48.54% memory energy reductions 
can be achieved compared to the fixed bank assignment method. 
 
 98
0
1
2
3
4
5
Fixed Adaptive
N
um
be
r o
f c
yc
le
s 
(1
06
)
Execution Cycles
40.41% Reduction     
 
Fig.4. 31 Total execution cycles 
Fixed Adaptive
Off‐chip DRAM 3.51  1.76 
On‐chip Cache 0.25  0.17 
0
1
2
3
4
En
er
gy
 C
on
su
m
pt
io
n 
(m
J)
Memory Energy Consumption
48.54% Reduction    
 
Fig.4. 32 Total memory energy consumption 
4.3 Summary 
For on-demand memory system presented in chapter 3, it needs efficient memory 
management units to process the memory access and control the memory resource 
allocation. In this chapter, the design of distributed memory management unit (d-MMU) 
and centralized memory management unit (c-MMU) have been described. 
In on-demand memory system, d-MMU performs as a high level cache for the 
 100
Chapter 5 
On-Demand Memory System for Wireless Video 
Entertainment Systems 
In this chapter, on-demand memory system is applied for wireless video entertainment 
systems. A pre-fetch and address translation mechanism will be proposed to improve the 
performance for Scalable Video Coding (SVC) processor element in this chapter. SVC 
processor element is the largest memory user in the system because there have multiple 
quality and spatial layers of the video frames need be stored and accessed. Thus, a 
pre-fetch approach for SVC is proposed to improve the memory access performance. 
Proposed pre-fetch mechanism can pre-fetch the residual data and motion vectors which 
will probably be read for inter-layer prediction decoding presently. Furthermore, a 
suitable DRAM data arrangement for SVC data is applied to reduce the DRAM access 
latency and power consumption. The address translation machine can increase the 
probability of row-hit and bank-hit status in the memory controller so that the additional 
activated power and the pre-charge time can be saved. The pre-fetch mechanism is 
presented in section 5.1 and the address translation is presented in section 5.2. In addition, 
SVC has different memory requirements for decoding different layers. With Proposed 
adaptive cache control in c-MMU as mentioned in chapter 4, optimizing on-chip memory 
utilization can be achieved for SVC. The analysis and simulation results will be described 
in section 5.3. 
5.1 Data Pre-fetch for SVC 
5.1.1 Introduction 
In realizing the video coding hardware system, the most critical issue not only focuses 
on the hardware costs but the power consumptions. However, the property of intensive 
memory data accessing, especially for external memory access, contributes significant 
part of entire video coding system power consumptions. Therefore, if the external 
 102
prediction mode, the motion information of base layer can be used as reference for 
prediction in enhancement layer as shown in Fig.5. 1. In this manner, the macroblock 
partition of the enhancement layer is acquired from corresponding 8x8 block of the base 
layer associated with a scaling operation. In addition to the block size, the motion vectors 
of the enhancement layer are obtained by multiplying the motion vectors of 
corresponding 8x8 block size in base layer by 2. Furthermore, the up-sampled motion 
information is used to refine the search results. 
 
Fig.5. 1 Illustration of inter-layer motion prediction [5.12] 
B. Inter-layer residual prediction 
Fig.5. 2 shows the concept of inter-layer residual prediction mode. When inter-layer 
residual prediction is performed, the residual data is up-sampled from corresponding 8x8 
block of the base layer by bilinear interpolation. Afterward, the up-sampled residuals are 
used for predicting the residuals of the current macroblock in the enhancement layer. 
 
Fig.5. 2 Illustration of inter-layer residual prediction [5.12] 
 104
enable the usage of as much lower layer information as possible for improving 
rate-distortion efficiency of the enhancement layers [5.13]. Fig.5. 4 shows the data 
relations for inter-layer prediction. When decoding advanced spatial layer of video 
frames, low layer frames will be referenced frequently. In order to increase processing 
time of the inter-layer prediction by reducing memory access latency, IPS is designed to 
load lower layer signals to the cache in advance. 
Spatial layer 0
Spatial layer 1
Spatial layer 2
 
Fig.5. 4 Data relations of three spatial layers for inter-layer prediction 
For the inter-layer prediction, the prediction signals are usually formed by 
motion-compensated prediction inside the enhancement layer or by upsampling the 
reconstructed lower layer signal. When decoding the advanced layer by inter-layer 
motion prediction and residual prediction, SVC processor element read the residual and 
motion vector(MV) signals of lower layer from memory in regular. Accordingly, IPS 
pre-fetches the required residual and MV data which will be referenced for decoding the 
following macroblock by inter-layer prediction. Fig.5. 5 gives an explanation of the 
proposed IPS. In this figure, the green frame represents base layer frame and the blue 
frame represents enhancement layer frame. When reconstructing a macroblock(as 
illustrated by red block in Fig.5. 5), the pre-fetch engine will pre-fetch the necessary 
residuals and MVs in base layer for the next macroblock reconstructing.  For inter-layer 
residual prediction, IPS loads an 8X8 block and additional residual signals which will 
need by bilinear interpolation. For inter-layer motion prediction, all MVs in 8X8 block 
 106
 
Fig.5. 6 d-MMU architecture with Pre-fetch Command Generator 
5.2 Address Translator for SVC 
5.2.1 Introduction 
To improve memory bandwidth and power consumption in video applications, a new 
address translation machine is proposed. This address translation machine is used for 
SVC decoder. The advantage of the address translation machine is the accessing to 
external memory can become more regular. Since the translation can minimize the 
number of overhead cycles needed for row-activations in synchronous DRAM (SDRAM), 
the memory bandwidth and energy consumption can be improved significantly. The 
features of SDRAM and memory-access patterns of video-processing applications are 
considered to find a suitable address translation which can improve the performance of 
whole system. As the resolution of video-processing applications becomes high and 
H.264 supports the high compressing efficiency, video signal processors should deal with 
a large amount of data within a tightly bounded time. Due to the large amount of data 
transfer, video data are stored in the external memory that are usually slow, and thus the 
system performance strongly depends on the memory bandwidth between processors and 
external memories. The data transfer in the video decoder is especially huge in order to 
support different level and complex mode. To meet the requirement, we must exploit the 
 108
physical address, the external memory interface(EMI) in DRAM controller can 
successfully access the DRAM data by generating corresponding DRAM commands. 
Generally, the EMI use the characteristic of external memory and the AT use the 
characteristic of video processing to improve the DRM access performance. 
In this work, DDR3 SDRAM provided by Micron Inc. [5.14] is applied. The 
configuration parameters of this DDR3 model are shown in Table.4. 2. In order to have 
high DRAM bandwidth, the 64MegX16 configuration is selected because the DRAM 
data bus width is the widest in these models. For the wireless video entertainment 
systems, two DDR3 devices are applied and share the same bus. The detailed DRAM 
organization and data arrangement will be described in the next section. 
5.2.3 Data Arrangement 
The DRAM organization is shown in Fig.5. 8. There are two DDR3 devices in our 
system, In order to reduce the chip I/O port, these two DRAMs share the same address, 
data and command bus controlled by chip select signal. Because the SVC has huge 
memory requirement and the regular data access behavior, we particularly arrange a 
DRAM for SVC processor element, and the other DRAM is arranged for another 
processor elements. 
 
Fig.5. 8 Architecture of the DRAM organization 
 110
decoding relations of video frames in a group of pictures(GOP), which is supported by 
the SVC in our system. In order to reduce the DRAM row miss rate, the video frames are 
allocated to different banks according to the decoding references. For instance, F4 will 
probably reference F0 or F8 to reconstruct the frame, so these three frames are allocated 
to different banks. Hence, the video decoder writes the reconstruct data to the new 
DRAM bank in regular, and would not be disarranged by read. It enables high row-hit 
rate for data write because of the regular write behavior for reconstructing a frame. 
Compared to the modern memory mapping methods which have been used in many 
works for video frames [5.15]-[5.17], the proposed memory mapping method is more 
suitable for the traditional cache-based hierarchy memory system because the row-hit rate 
of the DRAM write can be improved. Although the row-hit rate of the DRAM read would 
be higher than the modern method, the performance would not be degraded because the 
DRAM data can be cached and reused in the cache system. Furthermore, for motion 
compensation, the locality of reference data would strongly depend on the range of search 
window defined in the encoder. The write is more regular than the read for reconstructing 
frames because the write data is in raster-scan sequence but the read data may in random 
sequence. Accordingly, the proposed method can have low row-miss rate with reducing 
the DRAM row-conflict caused by writing reconstruct frame data. 
F1 F2 F3 F4 F5 F6 F7 F8F0
I PB B B BBBB
T0 T1T8T4T7T2T6T3T5  
Fig.5. 10 Video frame arrangement of a GOP 
The detail memory mapping method is shown in Fig.5. 11. Take the QCIF resolution 
as an example, the reconstruct Luma and Chroma data are stored into the bank which is 
assigned by the bank interleaved scheme. Luma data will assigned to bank 0, bank 1 or 
bank 2 and the Chroma data will assigned to bank3, bank4 or bank 5 according to the 
temporal relations of the decoding video frames. In order to increase the row-hit rate of 
the DRAM write, the frame data is allocated in raster-scan with MB unit because the 
 112
Number of Quality layer 2 
Quality layers QPBL : 32 - QPEL : 16 
Frame rate 30fps
GOP 8 (I-B-B-B-B-B-B-B-P) 
Sequence Stephen 
Table.5. 2 Summary of SVC information 
5.3.1 Improvement of adding IPS 
With proposed IPS as mentioned in section 5.1, the miss rate of distributed memory 
(L1 cache) can be reduced for the SVC application. Fig.5. 12 shows the simulation result 
of the miss rate. By adding the pre-fetch scheme (shown in Green line), it can reduce 
30.01% on average. Note that 4-way associativity cache configuration and Least Recently 
Used (LRU) replacement policy are applied in the simulation. The simulation for other 
associativity is done for 64KB cache size, and it is shown in Fig.5. 13. As the observation, 
higher way-associativity may have no obvious miss-rate reduction when the number of 
way is over 4 for this test pattern, so we choose a 4-way associativity configuration for 
L1 cache. 
Furthermore, IPS can reduce unnecessary cache misses in L1 cache by keeping useful 
data, which will be accessed recently, in the cache, so the number of L2 memory access 
can also be reduced. Moreover, the number of DRAM access caused by L2 cache data 
replacement also can be reduced. As shown in Fig.5. 14 and Fig.5. 15, the number of 
memory access in the centralized memory(L2 cache) can be reduced by 24.6% and the 
number of DRAM access can be reduced by 34% on average. 
 114
600
800
1000
1200
1400
1600
1800
0 1 2 3 4 5
N
um
be
r o
f m
em
or
y 
ac
ce
ss
 (1
03
)
L2 Cache Size(MB)
Memory Access Count in L2 Cache
original
original+IPS
L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
Line size : 64‐byte
24.6% reduction on average
 
Fig.5. 14 Memory access count of L2 Cache 
0
1
2
3
4
5
0 1 2 3 4 5N
um
be
r o
f D
R
A
M
 a
cc
es
s 
(1
06
)
L2 cache size(MB)
DRAM Access Count
Original
Original+IPS L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
Line size : 64‐byte
34% reduction on average
 
Fig.5. 15 DRAM access count 
Pre-fetch mechanism may induce additional energy overhead because there have extra 
cache access generated by pre-fetch requests. By using CACTI model, the energy 
measurement of L1 cache can be achieved and the result is shown in Fig.5. 16. The green 
part is the pre-fetch energy overhead produced by the miss pre-fetch requests. The red 
part is standby leakage energy consumption, and the blue part is the access energy. As 
shown in this figure, the access energy would increase with pre-fetch mechanism because 
of the additional pre-fetch requests. With pre-fetching, the standby leakage energy can be 
saved because it reduces the cache miss rate and execution time. By the cache 
configuration as illustrated in Fig.5. 16, pre-fetch have additional 18.9% energy overhead 
compared to the original design. Although the IPS may induce larger energy 
 116
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
0 1 2 3 4 5
R
ow
 m
is
s r
at
e
L2 cache size(MB)
DRAM Row Miss Rate
Original
Original+IPS
Original+IPS+AT
60.64% reduction on average
L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
Line size : 64‐byte
 
Fig.5. 17 DRAM row-miss rate 
0
200
400
600
800
1000
0 1 2 3 4 5
N
um
be
r o
f R
ow
 co
nf
lic
t (
10
3)
L2 cache size(KB)
DRAM Row Conflict
Original
Original+IPS
Original+IPS+AT
74.35% reduction on average
L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
Line size : 64‐byte
 
Fig.5. 18 Number of DRAM row-conflict 
Reducing row-miss rate can decrease the DRAM activate power and improve the 
DRAM access bandwidth utilization. Fig.5. 19 shows the measurement result of the 
DRAM activate power. As expected, the proposed pre-fetch and data allocation method 
can reduce the activate power about 57.19% on average. For measuring DRAM memory 
access efficiency, we define the bandwidth utilization as shown in the following equation 
to calculate the DRAM bandwidth efficiency. 
%100
commandsaccessprocessingofcyclesTotal
DRAMbetweendatainputtingandoutputtingofcyclesTotalnUtilizatioBandwidthDRAM ×=  
 118
access with lowing cache miss probability. Furthermore, activate power can be saved by 
address re-allocation mechanism so that DRAM energy can be reduced again. For 
execution time, pre-fetch mechanism significantly reduces the cache miss rate, so it has 
lower average memory access time than the original. By DRAM data re-allocation, the 
average miss penalty can be reduced. Accordingly, the proposed mechanisms reduce the 
execution time successfully. 
In addition, we measure total on-chip cache energy consumption with different L2 
cache size as shown in Fig.5. 23. Larger L2 cache size has more cache energy 
consumptions, but lower DRAM energy consumption can be achieved because of the low 
cache miss-rate. Fig.5. 24 illustrates the total memory energy consumption including 
on-chip cache and off-chip DRAM with different L2 cache size. As shown in this figure, 
larger L2 cache size has lower total memory consumption in the range from 256KB to 
4MB. On average, 37.53% energy reduction can be achieved when adding proposed 
mechanisms. 
0
5
10
15
20
25
30
0 1 2 3 4 5
D
R
A
M
 e
ne
rg
y 
(m
J)
L2 cache size(KB)
DRAM Energy Consumption
Original
Original+IPS
Original+IPS+AT
38.33% reduction on average
L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
Line size : 64‐byte
 
Fig.5. 21 DRAM energy consumption 
 120
‐10
‐5
0
5
10
15
20
25
30
A B C A B C A B C A B C A B C
en
er
gy
 co
ns
um
pt
io
n(
m
J)
Memory Energy Consumption
d‐MMU cache energy
c‐MMU cache energy
DRAM energy
L1 configuration
Size : 64KB
Replacement : LRU
Associativity : 4‐way
Line size : 32‐byte
L2 configuration
Replacement : LRU
Associativity : 8‐way
256KB     512KB    1024KB    2048KB    4096KB   
45.23%   
42.09%   
33.40%   
33.38%   
33.54%   
L2 Cache 
A : Original  ;  B : Original+IPS  ;  C : Original+IPS+AT
37.53% reduction on average
 
Fig.5. 24 Total memory energy consumption 
5.3.3 Analysis and Simulation Results of Adaptive Cache 
Control for Wireless Video Entertainment Systems 
For wireless video entertainment systems, SVC can optimize the video quality over a 
given bit rate range. Generally, a non-scalable video encoder generates the compressed 
bitstream with a fixed resolution and quality. In contrast, a scalable video encoder 
compresses a raw video sequence into multiple layers [5.18]. One of the compressed 
layers is the base layer, which can be independently decoded and provide coarse visual 
quality. Other compressed layers are enhancement layers, which can only be decoded 
together with the base layer and can provide better visual quality. The complete bitstream 
(i.e., combination of all the layers) provides the highest quality. In receiver, according to 
different channel situation or different application in end-user device, the most suitable 
quality and resolution of the video can be reconstructed by SVC. Fig.5. 25 illustrates the 
video performance for different channel bit rate. In this figure, the distortion-rate curve 
represents the upper bound in quality for any coding technique at the given bit rate. With 
SVC technique, the non-scalable single staircase curve is changed to a curve with several 
stairs. 
 122
0
2
4
6
8
10
12
(0,0) (0,1) (1,0) (1,1) (2,0) (2,1)
M
em
or
y 
Re
qu
ir
em
en
t (
M
B)
( Spatial layer , Quality layer )
SVC Memory Requirement (a GOP)
 
Fig.5. 26 SVC memory requirements of different scalable layers for a GOP 
For wireless video entertainment systems, the summary of memory configuration is 
listed in Table.5. 3. The bank assignment in c-MMU is also been profiled for different 
SVC decoding levels as shown in Table.5. 4. When SVC needs to decode high spatial and 
quality layer frames, adaptive bank assignment enables c-MMU to assign more banks for 
SVC. It can reduce the miss rate of frame reconstructing so that number of DRAM access 
can be decreased. In contrast, adaptive bank assignment can turn-off some banks in 
c-MMU when decoding low spatial and quality layer frames. By this configuration, Fig.5. 
27 shows the execution time for decoding a GOP with different SVC level. Compared to 
the fixed bank assignment (every processor elements assign equal banks), adaptive bank 
assignment can reduce the execution time for decoding enhancement layers in SVC. In 
addition, the memory energy comparison is done, and the result is shown in Fig.5. 28. 
Note that IPS and data allocation mechanism which are proposed in section 5.1 and 5.2 
are applied in the simulation. 
L1 cache (d-MMU) configuration 
Cache Size 64KB 
Number of banks 2 
Associativity 4-way 
Block size 32-byte 
Replacement policy LRU 
Write policy Write back 
 124
 
0
5
10
15
20
25
(1,0) (1,1) (2,0) (2,1)
Cy
cl
es
 (1
06
)
(Spatial layer , Quality layer)
Execution Cycles
Fixed
Adaptive
9.19%  11.68% 
10.57% 
8.48% 
 
Fig.5. 27 Execution cycles for different SVC levels 
A B A B A B A B
off‐chip memory energy 1.78  1.39  2.19  1.67  13.70  11.33  15.57  13.49 
on‐chip cache energy 0.39  0.27  0.46  0.33  2.62  2.35  2.98  2.74 
0.00 
2.00 
4.00 
6.00 
8.00 
10.00 
12.00 
14.00 
16.00 
18.00 
20.00 
M
em
or
y 
e 
en
er
gy
 c
on
su
m
pt
io
n(
m
J)
Memory Energy Consumption
(2,1)   (Spatial layer, Quality layer)
A : Fixed  ;  B : Adaptive
(2,0)    (1,1)    (1,0)    
23.80%    24.78%   
16.15%   
12.52%   
 
Fig.5. 28 Memory energy consumption for different SVC levels 
 126
Fixed Adaptive
Off‐chip DRAM 34.55  30.86 
On‐chip Cache 6.83  6.16 
0
10
20
30
40
50
En
er
gy
 C
on
su
m
pt
io
n 
(m
J)
Memory Energy Consumption
10.53% Reduction    
 
Fig.5. 31 Simulation result of memory energy consumption 
 5.4 Summary 
The memory management units (MMUs) have been constructed for the on-demand 
memory system in the chapter 4. In this chapter, additional inter-layer pre-fetch scheme 
(IPS) and address translation mechanism are proposed and integrated in MMUs to 
improve the performance for scalable video coding (SVC) processor element. These 
proposed methods not only reduce cache miss rate but also reduce total memory energy 
consumptions. 
For proposed IPS, the required information for inter-layer prediction in SVC technique 
will be pre-fetched ahead when reconstructing the frames, so the cache miss can be 
reduced significantly. Furthermore, IPS can reduce unnecessary cache misses in L1 cache 
and the number of DRAM access caused by cache data replacement. Accordingly, the 
execution time and memory energy consumptions can be reduced by IPS. In d-MMU of 
on-demand memory system, pre-fetch command generator (PCG) is constructed for 
generating the pre-fetch commands. Even though proposed IPS may have additional 
power overhead in d-MMU, the overall memory energy consumptions including total 
on-chip cache and off-chip DRAM can significantly be reduced. 
Additionally, to improve DRAM memory bandwidth efficiency and reduce DRAM 
power consumption, a new address translation mechanism for video applications is 
 128
Chapter 6 
Conclusions and Future Work 
6.1 Conclusions 
For constructing a multi-task platform, how to manage and utilize the memory is an 
important issue. This thesis proposes a message-passing based memory-centric on-chip 
data communication platform with on-demand memory system, and it can be applied for 
wireless video entertainment systems. In on-demand memory system, memory 
management units (MMUs) can efficiently control the memory access and memory 
resource allocation for processor elements (PEs). 
Proposed distributed memory management unit (d-MMU) performs as a high level 
cache for the dedicated PE in the on-demand memory system. Burst-based memory 
access protocol is applied to access continuous data easily, and the cache miss penalty 
also can be hidden. Furthermore, in order to reduce the stall caused by high traffic in 
network interconnection and small packet buffer size in network interface, a novel buffer 
borrowing mechanism is proposed. It enables d-MMU to borrow the cache blocks for 
buffering the blocking packets from PEs. The simulation result shows that number of 
transferred packet can be increased when the packet buffer size is small, and the 
execution time of PE can be reduced because the stall has been reduced. 
Centralized memory management unit (c-MMU) is designed for managing and 
providing larger centralized memory resources for system. PEs may have different 
memory requirements at runtime. With adaptive cache control, proposed c-MMU can 
support cache resource re-allocation for different PEs. By assigning suitable number of 
SRAM banks to PEs, the utilization of centralized on-chip cache can be optimized. 
Additionally, an external memory interface (EMI) in DRAM controller is applied to 
access external memory efficiently. By re-scheduling DRAM commands, the effective 
bandwidth of DRAM can be improved. 
For SVC in wireless video entertainment systems, inter-layer pre-fetch scheme (IPS) 
and address translation mechanism are proposed and integrated in MMUs to improve the 
 130
flexibility such as the works in [6.1]-[6.4]. Moreover, the profiling of memory behaviors 
is done off-line in our work. Ideally, it would be achieved by a powerful profiling engine 
in system. We assume MAC in wireless video entertainment systems can handle the 
profiling task. In the future, more complete profiling mechanism will need to be 
constructed. 
Additionally, the standby power of DRAM would induce large static power. Modern 
DRAM devices can support sleep mode for reducing the standby power significantly. 
When the system access DRAM infrequently, sleep control mechanism can be applied to 
power-down the banks in DRAM dynamically. The mechanism can control by the 
powerful profiling engine which can detect the memory behavior at runtime. Therefore, 
the overall memory energy consumptions can be reduced. 
The eHome project is still going on. For eH-III project, a femtocell home multimedia 
center will be developed for supporting multi-view 3D video, high-speed MIMO OFDM 
and gigabit cross-layer RRM in a heterogeneous platform. The architecture is shown in 
Fig.6. 1. In the future, in order to support huge memory bandwidth and data transmitting 
requirements, it will be necessary that constructing a heterogeneous memory-centric 
multi-core platform for multimedia center. 
 
Fig.6. 1 Architecture of femtocell home multimedia center 
 132
Conference Papers (ISSCC), pp.18-24, 7-11 Feb. 2010. 
References of Chapter 2 
[2.1]  Bruce Jacob, Spencer W. Ng, David T. Wang, “Memory Systems : Cache, 
DRAM, Disk”, Morgan Kaufmann, 2007. 
[2.2]  Domingo Benitez, Juan C. Moure, Dolores Rexachs, Emilio Luque, "A 
reconfigurable cache memory with heterogeneous banks“, Design, 
Automation and Test in Europe Conference and Exhibition (DATE), 8-12, 
pp.825-830. March, 2010. 
[2.3]  N. Hardavellas, I. Pandis, R. Johnson, N. Mancheril, A. Ailamaki, B. Falsafi, 
“ Database Servers on Chip Multiprocessors: Limitations and Opportunities”, 
in 3rd Conference on Innovative Data System Research, Asilomar, CA, USA, 
pp.79-87, 2007. 
[2.4]  P. Ranganathan, S. Adve, N. P. Jouppi, “Reconfigurable Caches and their 
Application to Media Processing”, in Proc. of the 27th Symposium on 
Computer Architecture, ACM Press, pp.214-224, 2000. 
[2.5]  D.H. Albonesi, “Selective cache ways: on-demand cache resource 
allocation”, in Proceedings of the 32nd Symposium on Microarchitecture, 
IEEE Computer Society, pp.248-259, 1999. 
[2.6]  C. Zhang, F. Vahid, W. Najjar, “A Highly Configurable Cache Architecture 
for Embedded Systems”, in Proc. 30th Symp. ISCA, 136-146, 2003. 
[2.7]  S.-H. Yang, M. D. Powell, B. Falsafi, K. Roy, and T. N. Vijaykumar, “An 
integrated circuit/architecture approach to reducing leakage in 
deep-submicron high-performance i-caches”. In Proceedings of the Seventh 
IEEE Symposium on High-Performance Computer Architecture, Jan. 2001. 
[2.8]  S. Yang, M. Powell, B. Falsafi, T.N. Vijaykumar, “Exploiting Choice in 
Resizable Cache Design to Optimize Deep-Submicron Processor 
Energy-Delay”, Proc. 8th Symp. HPCA, IEEE Comp. Soc., pp.151-161, 
 134
Electronics and Applications, 2009. 25-27 Page(s):2132 – 2136, May 2009. 
[2.18] C.-Y. Tsai, T.-C. Chen, T-W. Chen, and L.-G. Chen, “Bandwidth optimized 
motion compensation hardware design for H.264/AVC HDTV decoder”, 
ISCAS pp. 273-276, August 2005. 
[2.19] Y. Li, Y. Qu, and Y. He, “Memory Cache Based Motion Compensation 
Architecture for HDTV H.264/AVC Decoder”, ISCAS 2007, pp. 2906 - 
2909, May 2007. 
[2.20] Tzu-Der Chuang, Lo-Mei Chang, Tsai-Wei Chiu,Yi-Hau Chen, and 
Liang-Gee Chen, “BANDWIDTH-EFFICIENT CACHE-BASED MOTION 
COMPENSATION ARCHITECTURE WITH DRAM-FRIENDLY DATA 
ACCESS CONTROL”, ICASSP, pp. 2009 – 2012, 2009. 
[2.21] H.Y. Kang, K. A. Jeong, J. Y. Bae, Y. S. Lee, S. H. Lee, “MPEG4 
AVC/H.264 decoder with scalable bus architecture and dual memory 
controller”, ISCAS Circuits and Systems Volume 2, 23-26, pp. II–145-8, 
May 2004. 
[2.22] S. Heithecker, A Carmo Lucas, R. Ernst, “A mixed QoS SDRAM controller 
for FPGA-based high-end image processing”, IEEE workshop signal 
processing System, pp. 322-327, 2003. 
[2.23] K. B. Lee, T. C. Lin, and C. W Jen, “An Efficient Quality-Aware Memory 
Controller for Multimedia Platform SoC, ”IEEE Transactions on Circuits and 
Systems for Video Technology, vol. 15, no. 5, pp. 620 – 633, May 2005. 
[2.24] Hristo Nikolov, Todor Stefanov, Ed Deprettere, “Efficient External Memory 
Interface For Multi-Processor Platforms Realized On FPGA Chips”, In Proc. 
International Conference on Field Programmable Logic and Applications, 
2007, 27-29, pp. 580 – 584, Aug. 2007. 
[2.25] Juha-Pekka Soininen, Antti Pelkonen and Jussi Roivainen, ”Configurable 
memory organisation for communication applications”, Euromicro 
Symposium on Digital System Design, Proceedings, 4-6, pp. 86 – 93, Sept. 
2002. 
 136
J. Stickney, J. Zook, "TILE64 - Processor: A 64-Core SoC with Mesh 
Interconnect", IEEE International Solid-State Circuits Conference, 2008, 
ISSCC 2008, pp.88-598, 3-7 Feb. 2008. 
[3.3]  S. Nomura, F. Tachibana, T. Fujita, Chen Kong Teh, H. Usui, F. Yamane, Y. 
Miyamoto, T. Yamashita, H. Hara, M. Hamada, Y. Tsuboi, "A low-power 
multi-core media co-processor for mobile application processors," IEEE 
International Conference on IC Design and Technology, 2009. ICICDT '09., 
pp.129-134, 18-20 May 2009. 
[3.4]  D. Kim, K. Kim, J.-Y. Kim, S. Lee, H.-J. Yoo, “Memory-centric 
network-on-chip for power efficient execution of task-level pipeline on a 
multi-core processor”, IET Computers & Digital Techniques, Volume 
3,  Issue 5,  pp. 513 – 524, September 2009. 
[3.5]  Joo-Young Kim; Junyoung Park; Seungjin Lee; Minsu Kim; Jinwook Oh; 
Hoi-Jun Yoo, "A 118.4 GB/s Multi-Casting Network-on-Chip With 
Hierarchical Star-Ring Combined Topology for Real-Time Object 
Recognition," IEEE Journal of Solid-State Circuits, vol.45, no.7, 
pp.1399-1409, July 2010. 
[3.6]  S. Vakili, S.M. Fakhraie, S. Mohammadi, "Evolvable multi-processor: A 
novel MPSoC architecture with evolvable task decomposition and 
scheduling", Computers & Digital Techniques, IET, vol.4, no.2, pp.143-156, 
March 2010. 
[3.7]  Donghyun Kim; Kwanho Kim; Joo-Young Kim; Seungjin Lee; Hoi-Jun Yoo, 
"Implementation of Memory-Centric NoC for 81.6 GOPS object recognition 
processor", IEEE Asian Solid-State Circuits Conference, 2007. ASSCC '07., 
pp.47-50, 12-14 Nov. 2007. 
[3.8]  Yunxin Li, “Cognitive and Integrated Digital Home via Dynamic Media 
Access”, IEEE International Symposium on , pp. 1 – 6, May 2009. 
[3.9]  H. Schwarz, D. Marpe, T. Wiegand, "Overview of the Scalable Video Coding 
Extension of the H.264/AVC Standard", IEEE Transactions on Circuits and 
 138
Computer Architecture, ACM Press, pp.214-224, 2000. 
[4.7]  HP Labs : CACTI model, available on http://www.hpl.hp.com/research/cacti/ 
[4.8]  Micron Technology, Inc., Website : http://www.micron.com/ 
[4.9]  Micron System Power Calculators, available on 
http://www.micron.com/support/dram/power_calc.html. 
[4.10] Po-Chun Wang, “Layer-Adaptive Mode Decision based on Rate Distortion 
Cost Correlation Coefficients for Scalable Video Coding”, master thesis, 
Department of Electrical Engineering, National Dong Hwa University, 2009 
[4.11] R. Iris Bahar, Dan Hammerstrom, Justin Harlow, William H. Joyner Jr., 
Clifford Lau, Diana Marculescu, Alex Orailoglu, Massoud Pedram, 
“Architectures for Silicon Nanoelectronics and Beyond”, IEEE Comput., vol. 
40, no. 1, pp. 25–33, Jan. 2007. 
[4.12] L. Benini and G. De Micheli, “Network on Chips: Technology and Tools”, 
Morgan Kaufmann, 2006 
[4.13] W. J. Dally and B. Towles, “Principles and Practices of Interconnection 
Networks”, Morgan Kaufmann, 2004. 
[4.14] J. Howard, S. Dighe, Y. Hoskote, S. Vangal, D. Finan, G. Ruhl, D. Jenkins, 
H. Wilson, N. Borkar, G. Schrom, F. Pailet, S. Jain, T. Jacob, S. Yada, S. 
Marella, P. Salihundam, V. Erraguntla, M. Konow, M. Riepen, G. Droege, J. 
Lindemann, M. Gries, T. Apel, K. Henriss, T. Lund-Larsen, S. Steibl, S. 
Borkar, V. De, R. Van Der Wijngaart, T. Mattson, “A 48-Core IA-32 
Message-Passing Processor with DVFS in 45nm CMOS,” in Proc. IEEE Int. 
Solid-State Circuits Conf., pp. 108–110, Feb. 2010. 
References of Chapter 5 
[5.1]  C.-Y. Chen, C.-T. Huang, Y.-H. Chen, and L.-G. Chen, “Level C+ Data 
Reuse Scheme for Motion Estimation with Corresponding Coding Orders,” 
IEEE Transactions on Circuits and Systems for Video Technology, vol.16, 
 140
250, April 2006. 
[5.10] P. Chao and Y.-L. Lin, “A Motion Compensation System with a High 
Efficiency Reference Frame Pre-Fetch Scheme for QFHD H.264/AVC 
Decoding,” in Proceedings of IEEE International Conference on Circuits and 
Systems, pp. 256-259, May 2008. 
[5.11] C.-H. Li, C.-H. Chang, W.-H. Peng, W. Huang, and T. Chiang, “Design of 
Memory Sub-System in H.264/AVC Decoder,” in proceeding of IEEE 
International Conference on Consumer Electronics, pp.1-2, January 2007. 
[5.12] Po-Chun Wang, “Layer-Adaptive Mode Decision based on Rate Distortion 
Cost Correlation Coefficients for Scalable Video Coding”, master thesis, 
Department of Electrical Engineering, National Dong Hwa University, 2009. 
[5.13] H. Schwarz, D. Marpe, T. Wiegand, "Overview of the Scalable Video Coding 
Extension of the H.264/AVC Standard", IEEE Transactions on Circuits and 
Systems for Video Technology, Sept. 2007, vol.17, no.9, pp.1103-1120. 
[5.14] Micron Technology, Inc., http://www.micron.com/ 
[5.15] C. Chang, M. Chang, and W. Hwang, “A Flexible Two-Layer External 
Memory Management for H.264/AVC Decoder”, SOC Conference, 2007 
IEEE International 26-29 , Page(s):219 – 222, Sept. 2007. 
[5.16] J. Zhu, L. Hou, R. Wang, C. Huang, and J. Li, “High Performance 
Synchronous DRAMs Controller in H.264 HDTV decoder” in Proc. IEEE 
Int. Conf. Solid-state and Integrated Circuits Technol., vol. 3, pp. 1621-1624, 
2004. 
[5.17] Tzu-Der Chuang, Lo-Mei Chang, Tsai-Wei Chiu,Yi-Hau Chen, and 
Liang-Gee Chen, “BANDWIDTH-EFFICIENT CACHE-BASED MOTION 
COMPENSATION ARCHITECTURE WITH DRAM-FRIENDLY DATA 
ACCESS CONTROL”, ICASSP, pp. 2009 – 2012, 2009. 
[5.18] M. Mrak, M. Grgic, S. Grgic, "Scalable video coding in network 
applications", International Symposium on Video/Image Processing and 
 142
[6.4]  S. Yang, M. Powell, B. Falsafi, T.N. Vijaykumar, “Exploiting Choice in 
Resizable Cache Design to Optimize Deep-Submicron Processor 
Energy-Delay”, Proc. 8th Symp. HPCA, IEEE Comp. Soc., pp.151-161, 
2002. 
 
 2
???(2? 8?)? ISSCC?????(Plenary Session)????????????? Bosch, Senior 
Vice President, Jirl Marek?Texas Instruments, Senior Vice-President, Greg Delagl, Sony, Senior 
Vice-President, Tomoyuki Suzuki? George Institute of Technology, Professor James Meindl?
???ME MS ??????????????????????????????????????
?????????? 
????????????????????? IBM,Intel, Sun???????????????
???(1). IBM Power7 (2)Intel, 48 core IA Processor (3)Reneasas 45nm multi-core Procesor (4) AMD, 
x86-64 core CPU ?????????????????????????? 
??????????????? Samsung ??? 7Gb/s/pin GDDR5 SDRAM ?? 3D graphic 
systems?? Toshiba 3D DRAM using ?? TSV ???Micron ??? 32Gb (3b/cell)? NAND Flash 
Memory??????????????????????????? QDR DRAM ? GPU????
?????? 8Tb/s????????????????? 
?????? 
 ?? ISSCC 2010??Sensing the Future?????????????????????????
????????????????????????????????????????????
???????????????? 10???????????????????????????
???????????????????????????????????????  
 ISSCC ???????????????????????????????????????
?? MIT/Stanford ????????????????????????????????????
????????????????????????????????????????????
????????????????????????????????????? 
???? 
?????????????????????????????????????????
???????????????????????????????????????????
????????(? 45nm, 32nm SRAM)????????????? ISSCC??絶?????
?? 
??????????? 
??????????????????:1.2010 Conference CD?2. 2010 Digest of Technical 
Papers ???3. Silicon 3D-Integration Technology and Systems ?????4. High-Speed Memory 
Interfaces (Tutorial) ????? 
????????????????????????????????????????????
????????????? e-Home (II)?????? 
98年度專題研究計畫研究成果彙整表 
計畫主持人：黃威 計畫編號：98-2220-E-009-002- 
計畫名稱：適用於無線視訊娛樂之多系統融合及節能技術--子計畫一：適用於多核心之低功率隨選記
憶體系統(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 3 3 100%  
研討會論文 12 9 133% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 11 9 122%  專利 已獲得件數 2 3 66% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 7 7 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 6 3 200%  
研究報告/技術報告 0 0 100%  
研討會論文 26 18 144% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 1 3 33%  專利 已獲得件數 3 3 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
