 2
In this research, we deal with the constrained parallel machine problem, where the constraints 
include the limit of number of jobs assigned on each machine, the availability constraint of 
machines, and the machine eligibility restriction. In particular, we focus on three constrained 
parallel machine problems. First, we consider a two parallel machine problem where there is a limit 
on the number of jobs that can be assigned on any machine. This problem has practical interest in 
the optimization of assembly lines for printed circuit boards (PCB). Second, we deal with a problem 
consisting of two parallel machines where one is available only over a time period after which it can 
no longer process any job. This happens for the situations of preventive maintenance and periodical 
repair. Third, we deal with a parallel machine problem in the presence of machine eligibility 
restrictions, where machines and jobs can be classified into two levels: high and low levels. Each 
job is allowed to be processed by a particular machine only when the level of the job is no lower 
than the level of the machine. This problem arises in both manufacturing (e.g., the Probe 
workcenter in wafer testing) and service (e.g., credit card management) industries. For all the three 
addressed problems, optimal solution methods will be developed to solve the problems. 
 
Keywords: Scheduling; Parallel machines; Availability constraints; Machine eligibility restriction 
 
 4
that the machines can be maintained simultaneously; the other is that maintenances cannot be 
overlapped among the machines. They propose branch and bound algorithms based on the column 
generation approach for solving medium-sized cases. 
On the other hand, Lee (1996) assumes that there is only one unavailability period during the 
scheduling horizon and analyzes the bounds of heuristics under various performance measures and 
different machine environments, including single machine and parallel machines. Both 
nonresumable and resumable cases are under consideration.  
The problem considered in the first part of this research consists of two parallel machines 
where one has an unavailability period. This problem has been considered by Lee (1996) where a 
heuristic is proposed. In this research, we provide optimality algorithms for the problem. 
 
2. Preliminaries   
 
It has been shown that the two parallel machine makespan-minimization problem without the 
restriction of availability is NP-hard (Lenstra and Rinnooy, 1977). Because the inclusion of 
availability complicates the problem, the problem with availability constraint is also NP-hard. 
The following assumptions are made for the considered problem: 
1. The number of jobs is fixed and known in advance. 
2. There are two parallel identical machines, i.e., the processing time of each job is the same on the 
two machines. 
3. One of the machines is not available during the time period from s  to t  ,0( ts ≤≤ ),0>t  and 
the other machine is always available. 
4. The ready times of all jobs are zero. 
5. No machine may process more than one job at a time. 
6. There is no priority for any job. 
The following notation is used throughout this research: 
1M  The machine with restricted availability. 
2M  The machine without any restriction of availability. 
iJ  Job ,i  .,,1 ni L=  
iP  The processing time of job ,i  .,,1 ni L=  
Sum  The total of all processing times, i.e., .
1∑== ni iPSum  
maxC  The makespan of the system, i.e., the larger of the job completion times on the two 
machines. 
jCM  The completion time of the last-processed job on .jM  
s , t  Machine 1M  is not available during the time period from s  to t  for all jobs, where 
,0 ts ≤≤  .0>t  
B  The total processing time of jobs on 1M  before time .s  
A  The total processing time of jobs on 1M  after time .t  
W  The total processing time of jobs on 2M  after time .t  
Before presenting our algorithms, it is necessary to introduce the TMO algorithm 
(Two-Machine Optimal Scheduling), proposed by Ho and Wong (1995). Except for the third case, 
TMO is either directly applied or forms the basis of the developed algorithms. The TMO algorithm 
is used to minimize the makespan for two parallel machines without availability constraint. By 
connecting with the LPT sequence, TMO uses lexicographic search to derive an optimal schedule. 
Basically, TMO uses exhaustive search but eliminates lots of unnecessary situations based on some 
 6
3.3. Case 3. ,2 tsSums +≤<  0>s  
 
Theorem 1. When ,2 tsSums +≤<  the optimal solution can be obtained by maximizing .B  
Proof. We will first show that the optimal schedule must have no jobs after t  on 1M  when B is 
maximized. Consider a schedule as depicted in Figure 1, where B is the workload (total job 
processing time) before ,s  AtCM +=1  and .2 WtCM +=  By definition, 
.WtABSum +++=  Thus, 
 tsSum +≤  
⇒      tsWtAB +≤+++  
⇒      sWAB ≤++  
⇒      BWsA −−≤                                                 )1(  
 
When B is maximized, each job processed in ) ,( Att +  must have a processing time larger than the 
gap )( Bs −  since, otherwise, the job can be processed in )( Bs − . Hence, we have ,BsA −>  
which is inconsistent with (1). Thus, we can conclude that A  doesn’t exist when B is maximized. 
So we obtain WtCMC +== 2max  and Sum = .WtB ++  Therefore, minimizing maxC  is 
equivalent to minimizing ,W  which in turn is equivalent to maximizing B  for t  and Sum  are 
constant.   □ 
The basic idea of the algorithm is to assign jobs into the interval ) ,0( s  as much workload as 
is possible. If the workload B  is equal to ,s  an optimal solution is found. Otherwise, an iterative 
procedure is continued until B  is maximized. The following additional notation is needed in the 
algorithm:  
a   The job index currently considered to be assigned into ).,0( s  
L   The number of jobs being assigned in the interval ).,0( s  
LB   The current value of ,B  i.e., the total processing time of the L assigned jobs. 
∧
B   The best value of .B  
LS   The sequence of jobs for .LB  
∧
S   The sequence of jobs for .
∧
B  
)(1 LSl    The job index of the last assigned job in .LS  
)(2 LSl    The job index of the second-last assigned job in .LS  
Algorithm 3: 
Step 0.  Sort all jobs in non-increasing order of their processing times and renumber them such 
that .1+≥ ii PP  Find the first job, ,kJ  whose processing time is smaller than or equal to 
.s  
Step 1.  If ,0=k  then φ=∧S ; go to Step 5.  
 
 
B  s  
t  
t  t+ A
t+ W
M 1
M 2
 
Figure 1. The graphical representation for Case 3 
 8
Step 0.  Sort all jobs and obtain ,201 =P  ,182 =P  ,93 =P  ,74 =P  .35 =P  We find kJ = .3J  
Step 1.  0≠k ; go to Step 2. 
Step 2.  Set ,0=L  ,3=a  ,00 =B  .0=
∧
B  
Step 3.  ,99030 sPB <=+=+  so ,11 =+= LL  ,990301 =+=+= PBB  and ).( 31 JS =  
Because ,1
∧> BB  ,9=∧B  and ).( 3JSS L ==
∧
 Set ,1=L  ;4=a  restart Step 3. 
Step 3.  ,167941 sPB >=+=+  ;5=a  restart Step 3. 
Step 3.  ,123951 sPB >=+=+  ;6=a  restart Step 3. 
Step 3.  5>a  and go to Step 4. 
Step 4.  ,3)(1 nSl L ≠=  so ,413 =+=a  ,0310 =−= PBB  ,)( 310 φ=−= JSS  ,011 =−=L  
and go to Step 3. 
Step 3.  ,77040 sPB <=+=+  so ,11 =+= LL  ,770401 =+=+= PBB  and ).( 41 JS =  
Because ,1
∧< BB  5=a  and restart Step 3. 
Step 3.  ,103751 sPB ==+=+  so ,21 =+= LL  ,1037512 sPBB ==+=+=  and 
).,()( 54512 JJJSS =+=  Because ,2
∧> BB  10=∧B  and ),( 54 JJSS L ==
∧
. Go to Step 
5. 
Step 5.  1M  processes jobs ( 54 , JJ ) and 2M  processes jobs ( 321 ,, JJJ ). 
21max PPC += .473 =+ P  
 
3.4. Case 4. ,tsSum +>  0>s  
 
This case is more complicated than other cases. We first propose two theorems, and then apply 
the TMO algorithm to solve the case optimally. 
According to Case 3, we might think that maximizing B  in Case 4 will get an optimal 
solution. This intuition, however, is incorrect as evidenced by the following counter-example. 
Suppose that there are five jobs, 521 ,,, JJJ L , with processing times 11, 9, 8, 6, 4, and that ,10=s  
,20=t  and .38=Sum  Since ,tsSum +>  the problem belongs to Case 4. To maximize ,B  we 
process 4J  and 5J  in )10 ,0(  on 1M  and obtain 10== sB  with .28max =C  If we process 
2J  in )10 ,0(  on ,1M  we get a smaller B )9( =B  with a smaller maxC  ( 25max =C ; 5J  is 
processed after t  on 1M ). 
We now present two theorems to be used in the algorithm. In Theorem 2, a lower bound of B 
will be derived, which allows us to eliminate lots of unnecessary computations. In Theorem 3, we 
will prove that if there is still enough space in ),0( s  on ,1M  we should allocate the workload as 
much as possible. 
Theorem 2. Let 1S  be the schedule with 1BB =  and )( 1max SC , and 2S  be another schedule 
with 2BB =  and ).( 2max SC  If )()( 1max2max SCSC < , then a lower bound of 2B  is 
tSum + ).(2 1max SC−  
Proof. Note that tBSum +− 2  is the total time that jobs not in B can be allocated to the two 
machines. Thus, a smallest possible value of )( 2max SC  can be expressed as .2/)( 2 tBSum +−  If 
),()( 1max2max SCSC <  then we have 
 10
             If na <  and ),( Ln BsP −≤  then 1+= aa  and restart Step 3. 
             Else if ,lbBL ≤  then 1+= aa  and restart Step 3. 
                  If ,'
LL SjSi PP ∈∈ =  then 1+= aa  and restart Step 3. 
                  If ),(1)(1 LSf BsP L −≤−  then 1+= aa  and restart Step 3. 
                  Add ,dJ  remove all jobs in ,LS  and perform TMO. 
                  Set 1+= aa  and go to Step 5. 
       Else if ,sPB aL =+  then ,1+= LL  aLL PBB += −1  and ).(1 aLL JSS += −  
              If ,'
LL SjSi PP ∈∈ =  then 1+= aa  and restart Step 3. 
              Add ,dJ  remove all jobs in ,LS  and perform TMO. 
              Set 1+= aa  and go to Step 5. 
Step 4.  If ,)(1 nSl L = then ,1)(2 += LSla ,)()(2 12 LL SlSlLL PPBB −−=−  )( )(2 2 LSlLL JSS −=−  
),( )(1 LSlJ− and .2−= LL  
       Else ,1)(1 += LSla  ,)(1 1 LSlLL PBB −=−  ),( )(1 1 LSlLL JSS −=−  and .1−= LL  
        Go to Step 3. 
Step 5.  If [ ], 2/)(max tsSumC +−=∧  then go to Step 6, else go to Step 3. 
Step 6.  Let the machine processing dJ  be .1M  
Step 7.  Move dJ  to the first position of 1M  and move all the jobs preceding dJ  backward 
one position. Assign the jobs in LS  into ),0( s  on .1M  
The complexity of Algorithm 4 is ),4(O n  because in the worst case we must try n2  job 
combinations in ),0( s  on ,1M  in which each needs to perform TMO once. 
 
Example 
As an illustration, consider a five-job problem with processing times 3, 18, 9, 7, 20, ,10=s  
and .44=t  Because ,5457 =+>= tsSum  the problem belongs to Case 4. Applying the above 
algorithm yields the following steps:  
Step 0.  Sort all jobs and obtain ,201 =P  ,182 =P  ,93 =P  ,74 =P  .35 =P  We find .3JJ k =  
Create dJ  with .44=dP  Compute [ ]2/)( tsSum +−  [ ] ]5.45[2/)441057( =+−=  
.46=  
Step 1.  0≠k ; go to Step 2. 
Step 2.  Set ,0=L  ,3=a  ,0=∧lb .max ∞=∧C   
Step 3.  ,99030 sPB <=+=+  so ,11 =+= LL  ,990301 =+=+= PBB  and ).( 31 JS =  
Because ,01 =>
∧
lbB  add ,dJ  remove ,3J  and perform TMO. We obtain 47max =C  
and .74724457 =×−+=lb  Update 47max =∧C  and .7=∧lb  Then ,1=L  ,4=a  
and go to Step 5. 
Step 5.  4647max ≠=∧C ; go to Step 3. 
Step 3.  ,167941 sPB >=+=+  5=a ; restart Step 3. 
Step 3.  ,123951 sPB >=+=+  6=a ; restart Step 3. 
Step 3.  5>a  and go to Step 4. 
 12
2M .  
Based on the above discussion, we need only consider Cases 2 and 3 in the resumable case. 
 
4.1. Case 2. ,2sSum ≤  0>s  
 
Theorem 4. When ,2sSum ≤  Algorithm 2 can solve the problem optimally. 
Proof. Suppose there exists no availability constraint. We can obtain an optimal solution by TMO 
with BCM =1  and .2max BSumCMC −==  We move a job kJ  with kP  on 2M  to 1M  and 
obtain a new ,1max kPBCMC +==  which must be larger than or equal to the original 
.max BSumC −=  That is, ,BSumPB k −≥+  or   
.2BSumPk −≥                               (2) 
Under the availability constraint, we apply Algorithm 2 (a modification of TMO) to the problem by 
treating each job as a nonresumable job and obtain BCM =1  and .2max BSumCMC −==  If 
there exists a better schedule, its maxC  must be smaller than the original .maxC  In such a case, we 
need to move a job kJ  on 2M  to .1M  If kJ  is not interrupted, the situation can be treated as 
non-existence of availability constraint. The new maxC  must be larger than or equal to the original 
one because Algorithm 2 has found the optimal solution to the nonresumable case. Thus, there 
exists no better schedule. If kJ  is interrupted, the new ).(1max BsPtCMC k −−+==  Then, 
       new −maxC  original maxC  
= )()( BSumBsPt k −−−−+  
kPSumsBt +−−+= 2   
)2(2 BSumSumsBt −+−−+≥     (from (2)) 
0≥−= st  
Thus, there exists no better schedule. Therefore, Algorithm 2 can solve the resumable case optimally.  
□ 
 
4.2. Case 3. ,2 tsSums +≤<  0>s  
 
In this case, we first use Algorithm 3 to solve the nonresumable case. If ,sB =  then an 
optimal solution has been found. Otherwise, we move the job with the smallest processing time kP  
from 2M  to ,1M  which leads to kJ  being interrupted and tCM ≤2  (since tsSum +≤ ). The 
new ).(1max BsPtCMC k −−+==  We compare the new maxC  with the original maxC  and select 
the one with smaller value as the solution. 
Algorithm 5: 
Step 1.  Apply Algorithm 3 to solve the nonresumable case and obtain B  and .1maxC  
Step 2.  If ,sB =  then the obtained schedule is optimal for the resumable case. 
Step 3.  Find kJ  which has the smallest processing time kP  on .2M  Move kJ  from 2M  to 
1M  and obtain .
2
max BstPC k −++=  
Step 4.  ).,min( 2max1maxmax CCC =  The optimal schedule is the one with .maxC  
The complexity of Algorithm 5 is ),2(O n  the same as that of Algorithm 3. 
 14
Table 1 
The computational result for Part A 
10=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s     s   
t  0 200 175 t  0 200 300 
250  a0005.0  
 b)100 ,1(  
0.0750 
(2, 95) 
0.0789 
(3, 5) 
0.0342 
(2, 48) 
0.0278 
(3, 52) 
350 0.0005 
(1, 100)
0.0033 
(3, 23) 
--- 
(4, 57) 
0.1840 
(2, 53) 
0.1924 
(3, 24) 
--- 
(4, 23) 
300 0.0005 
(1, 100) 
0.0409 
(2, 96) 
0.0292 
(3, 4) 
0.0331 
(2, 46) 
0.0369 
(3, 54) 
 
400 0.0005 
(1, 100)
0.1921 
(3, 49) 
--- 
(4, 51) 
0.0948 
(2, 42) 
0.1076 
(3, 53) 
--- 
(4, 5) 
350 0.0005 
(1, 100) 
0.0318 
(2, 95) 
0.0352 
(3, 5) 
0.00417
(2,51) 
0.0408 
(3, 49) 
450 0.0006 
(1, 100)
0.1057 
(3, 72) 
--- 
(4, 28) 
0.0781 
(2, 55) 
0.0755 
(3,41) 
--- 
(4, 4) 
        
50=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  0 1000 875 t  0 1000 1500 
1250 0.0044 
(1, 100) 
0.2025 
(2, 100) 
0.2157 
(2, 55) 
0.2214 
(2, 45) 
1750 0.0039 
(1, 100)
0.0318 
(3, 8) 
--- 
(4, 92) 
0.0022 
(2, 71) 
0.2069 
(3,25) 
--- 
(4, 4) 
1500 0.0050 
(1, 100) 
0.2314 
(2, 100) 
0.1891 
(2, 54) 
0.2113 
(3, 46) 
2000 0.0044 
(1, 100)
0.1230 
(3, 45) 
--- 
(4, 54) 
0.1709 
(2, 49) 
0.2059 
(3, 50) 
--- 
(4, 1) 
1750 0.0027 
(1, 100) 
0.2467 
(2, 100) 
0.2080 
(2, 50) 
0.2202 
(3, 50) 
2250 0.0043 
(1, 100)
0.2056 
(3, 93) 
--- 
(4, 7) 
0.2057 
(2, 47) 
0.2025 
(3, 53) 
        
 16
Table 2 
The computational result for Part B 
10=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  55 85 115 t  100 150 200 
70 0.0022   120 0.0033   
115 0.0016 0.0038  200 0.0022 0.0044  
175 0.0022 0.0039 0.0067 300 0.0023 0.0044 0.0063 
        
20=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  115 175 230 t  200 300 400 
140 0.0033   240 0.0033   
230 0.0059 0.0059  400 0.0039 0.0059  
350 0.0051 0.0082 1.4063 600 0.0109 0.1094 1.7789 
        
30=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  175 260 350 t 300 450 600 
210 0.0022   360 0.0039   
350 0.0033 0.0045  600 0.0033 0.0063  
525 0.0033 0.0044 0.0065 a 900 0.0044 149.4359 272.5070
             
50=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  290 440 580 t  500 750 1000 
350 0.0328   600 0.0169   
580 0.0211 0.0285  1000 0.0159 0.0161  
875 0.0281 0.0291 0.0196 1500 0.0143 0.0164 0.0160 a  
        
100=n  
  ~iP (20, 50)   ~iP (20, 100) 
    s    s   
t  580 875 1150 t  1000 1500 2000 
700 0.0769   1200 0.0438   
1150 0.0832 0.0820  2000 0.0328 0.0707  
1750 0.0594 0.0539 0.0934 3000 0.0285 0.0492 0.0379 
        
a  The average computation time of 9 replications (in seconds); one instance took more than one hour. 
 18
PART II 
 
The second part of this research considers the problem of scheduling on two identical parallel 
machines to minimize total completion time when a machine is available only in a specified time 
period. The research proposes an optimal branch-and-bound algorithm which employs three 
powerful elements, including an algorithm for computing upper bound, an improved lower bound, 
and a fathoming condition. The branch and bound algorithm was tested on problems of various 
sizes and parameters. The results show that the algorithm is quite efficient to solve all the test 
problems. In particular, the total computation time for the hardest problem is less than 0.1 second 
for a set of 100 problem instances. An important finding of the tests is that the upper bound 
algorithm can actually find optimal solutions to a quite large number of problems.  
 
1. Introduction 
 
Most literature on scheduling assumes that machines are continuously available over the 
planning time horizon. In practice, a machine may not always be available, for example, due to 
preventive maintenance or machine breaks. Therefore, a more realistic scheduling model should 
take the limited machine availability into account. In this research, we discuss a scheduling problem 
of two identical parallel machines where one is available during a time period after which it can no 
longer process any job. This happens for the situations of preventive maintenance and periodical 
repair. To avoid disrupting the overall production run, the preventive maintenance or periodical 
repair is usually done on a rotation basis instead of maintaining or repairing all the machines 
simultaneously (Lee and Liman, 1993). So it is reasonable to assume that one machine is always 
available. The total completion time is the objective to be minimized for the problem because it 
gives an indication of the total holding or inventory costs (Pinedo, 2002). 
Scheduling problems with limited machine availability have been studied to a less extent 
(Schmidt, 2000). In what follows, we briefly review the related research in an identical parallel 
machine environment. The makespan is the objective considered by most researchers for the 
problem. Schmidt (1984) considers the problem on parallel machines where all machines are 
available only in an arbitrary number of time periods. He proposes a preemptive scheduling 
algorithm with polynomial time complexity for the problem. In Schmidt (1988), the problem is 
further generalized making allowance for different job release times or deadlines. For 
non-preemptive scheduling, Lee (1991) considers the problem with non-zero available times of 
machines and studies the worst-case performance of the LPT (Longest Processing Time) rule. He 
then presents a modified algorithm to provide a makespan with tighter bound. 
There also exists some related research considering the objective of total job completion time. 
Lee and Liman (1993) study a two parallel machine problem where only one machine is available 
for a specified period after which it can no longer process any job. They first prove the NP-hardness 
of the problem and then provide a pseudo-polynomial dynamic programming algorithm to solve the 
problem. They also propose a SPT-based heuristic that has a worst case error bound of 50%. For the 
same problem, Mosheiov (1994) proposes a modified SPT-based heuristic and a lower bound on the 
optimal solution. Moreover, he extends the results to the multi-machine case for which all machines 
are available only in a specified time periods. To minimize total weighted completion time, Lee and 
Chen (2000) consider a parallel machine problem under two circumstances: maintenances can or 
cannot be overlapped among the machines. Based on column generation, branch and bound 
algorithms are proposed for solving medium-sized problems. 
The problem addressed in this research consists of two parallel machines where one is 
 20
bounds at other branching nodes is presented at the end of this subsection. 
Algorithm 1.  
Step 1: Arrange the jobs in SPT rule.  
Step 2:  Sum up the odd jobs (i.e., jobs placed in the odd positions) until the sum oddA  attains its 
largest value less than or equal to .s  Similarly, sum up the even jobs until the sum evenA  
attains its largest value less than or equal to .s  
Step 3: Assign the jobs in the set (odd or even set) with odd evenmax{ , } ( )A A A=  to 1M . Assign the 
jobs in the other set as well as the remaining jobs to 2M . 
Step 4: If A S= , the obtained schedule is optimal; stop. Continue, otherwise. 
Step 5: If the even jobs are assigned to 1M , place a dummy job (job 0) with zero processing time 
in the first position of 1M . Let 1n  be the number of jobs assigned to 1M . 
Step 6: For each k  ( 11, , )k n= K , subtract the processing time of the thk  job on 1M  from the 
processing time of the thk  job on 2M . Let kq  be the difference. 
Step 7: Solve the Subset Sum problem with sum s A= −  and integers kq . Perform the 
interchanges according to the solution of the Subset Sum problem. 
Step 8: If A s= , the obtained schedule is optimal. If A s< , it is an upper bound on the minimum 
total completion time. 
We now explain the algorithm. The first two steps are self explanatory. In Step 3, we assign the 
jobs in the odd or even set, depending on which has a total processing time closer to s , to 1M . In 
Step 4, we check if the total amount of processing on 1M  is just equal to s . If yes, the resulting 
schedule is optimal according to Theorem 1 (to be presented). The purpose of adding a dummy job 
in Step 5 is to assure that a job on 1M  has a smaller processing time than the corresponding job in 
the same position on 2M . Under this circumstance, we can increase the value of A  (the total 
amount of processing on 1M ) by performing the necessary interchanges between the jobs in the 
same position on the two machines. To determine the best interchanges, we transform the problem 
into the so called Subset Sum problem (Cormen et al., 2001) in Steps 6 and 7. The problem is this: 
given a set of integers and an integer s, does any subset sum to s? The Subset Sum problem is the 
same as the P( )s  problem (Liao and Lin, 2003), in which its objective is to minimize the 
makespan on 2M  (or equivalently, maximize the total amount of processing on 1M ). The P( )s  
problem can be solved optimally and efficiently by the algorithm of Liao and Lin (2003). In Step 8, 
we check if the total amount of processing on 1M  is exactly equal to s . If yes, the resulting 
schedule is optimal according to Theorem 2; otherwise, it is a good feasible schedule that can be 
used as an upper bound. 
Theorem 2.  If the schedule obtained by Algorithm 1 has a total amount of processing on 1M  
exactly equal to s , then it is optimal. 
Proof.  We consider the problem in two parts (see Figure 1). Part (i) deals with the jobs on 1M  
along with the same number of jobs on 2M . Part (ii) considers the remaining jobs on 2M . 
Consider part (i) first. According to Theorem 1 (ii), the SPT rule (with jobs alternately assigned 
to the two machines) is optimal if the total amount of processing on 1M  (denoted by A ) is 
exactly equal to s . We now show that a schedule obtained, from SPT, by interchanging jobs in the 
same position on the two machines is also optimal if A s= . Let 1( )kp  and 2( )kp  be the processing  
 22
 
Figure 2. Gantt chart for Example 1 
Algorithm 2 
Step 1: Arrange the unassigned jobs in SPT order.  
Step 2: Assign the jobs to 2M  until their total processing time attains its largest value (denoted by 
2t ) less than or equal to 1t . 
Step 3:  For the remaining jobs, sum up the odd jobs until the sum oddA  is greater than or equal to 
1( )s t− . Similarly, sum up the even jobs until the sum evenA  is greater than or equal to 
1( )s t− . 
Step 4: Assign the jobs in the set (odd or even set) with odd evenmin{ , } ( )A A A=  to 1M . Assign the 
jobs in the other set as well as the remaining jobs to 2M . 
Step 5: If the minimum occurs at oddA , add a dummy job with zero processing time to 2M . 
Step 6: Let in  be the number of jobs on iM  ( 1 2i ,= ). Compute a lower bound as follows:  
1 2 1 1
1, ,  
( ) ( )j
j n
LB C n n t A s
=
= + − × + −∑
K
 
Step 7: Interchange the dummy job with the job placed in the same position on 1M . Compute 
another lower bound as follows: 
2 1 1 1
1, ,  
2
2 1 1 1
1, ,  
( ) ( ) if  
( 1) ( ) if  
j
j n
j
j n
C n n t A s t A s
LB
C n n s t A t A s
=
=
⎧ + − × + − + ≥⎪= ⎨ − − − × − − + <⎪⎩
∑
∑
K
K
 
The final lower bound is 
1 2min{ , }LB LB LB=  
Theorem 3.  Algorithm 2 produces a lower bound on the minimum total completion time. 
5(13)
17
1M 0(0) 
2M
2(3) 4(5)
1(1) 3(4) 5(13) 6(28) 7(33) 8(73)
17
1M
2M
(a)
(b)
0(0) 3(4) 4(5) 6(28) 7(33) 8(73)
1(1) 2(3) 
 24
 
Figure 3. Gantt chart for Example 2 
 
It should be noted that Algorithm 2 may also apply to the root node with no jobs being 
assigned. In this situation, the lower bound simply equals 1LB  because the two equations in 2LB  
result in the same value.  
As mentioned earlier, Mosheiov (1994) also develops a lower bound algorithm for the root 
node. A comparison of his algorithm with Algorithm 2 is given in the following numerical example 
taken directly from Mosheiov. 
 
Example 3.  Consider a 6-job problem with processing times 1, 2, 3, 4, 5, 6, and 3.s =  Applying 
Algorithm 2 yields the following steps:  
Step 1: The SPT sequence is (1, 4, 5, 6, 7, 8). 
Step 3: Compute odd 1 3 4A = + =  and even 2 4 6.A = + =  
Step 4: Assign jobs (1, 3) to 1M  and jobs (2, 4) to 2M . Set 4.A =  
Step 5: Since oddA A= , add a dummy job 0 with 0 0p =  to 2M . 
Step 6: Let 21 =n  and 52 =n . Compute 
2 1 1
1, ,  
( ) ( )
41 (5 2) (0 4 3) 44
j
j n
LB C n n t A s
=
= + − × + −
= + − × + − =
∑
K  
The resulting lower bound is 44, which is better than the lower bound 41 obtained by the algorithm 
of Mosheiov (1994). For this particular example, our lower bound value is the same as the optimal 
solution value, which can be obtained by Algorithm 1. It is noted that in this example only 1LB  is 
used. This is due to the fact that the lower bound is computed for the node with no jobs assigned 
yet.  
  
3.3. A fathoming condition 
 The next theorem introduces a condition that can be used to fathom nodes in the algorithm 
7(33)
48
1M 2(3) 
2M
3(4) 5(13)
1(1) 4(5) 0(0) 6(28) 8(73) 
(a)
(b)
7(33)
48
1M 2(3) 
2M
3(4) 0(0) 
1(1) 4(5) 5(13) 6(28) 8(73) 
 26
Most of the steps in the branch and bound algorithm are self explanatory except for the 
following steps. In the bounding step, we compute jC∑  for a node only when A s= , because the 
node has a better chance to update the incumbent solution. On the other hand, when no more jobs 
can be placed on 1M  for a given node, a specific schedule is actually determined by the node. 
Thus, there is no need to compute LB  and the node can be fathomed (condition (iii)). According 
to Theorem 4, the node can be deleted immediately if 1 0t A A+ ≤ . Otherwise, we compute the total 
completion time of the node and compare it with the incumbent solution (see the bounding step). 
 
5. Computational Results 
 
To determine the efficiency of the developed algorithms, they were tested by using a 
computer program coded in Java and run on a Pentium IV 3.4 GHz PC. The processing times of the 
problems were generated from three discrete uniform distributions [1, 100], [1, 500], and [1, 1000]. 
Different problem sizes of jobs =n 10, 20, 40, 60, 80, 100 were tested. Capacity s  was 
determined by 2js p /= ×∑α  with 0 4 0 6 0 8. , . , .=α . For each combination, 100 replications 
were run by both our branch and bound (BAB) algorithm and the dynamic programming (DP) 
algorithm (Lee and Liman, 1993). The computational results are summarized in Tables 1 and 2, 
which provides information on the maximum and total computation times in 310−  second.  
It is observed that the proposed BAB algorithm is more efficient than the DP algorithm in 
solving all the problems. It is noteworthy that the problem is comparatively difficult for BAB when 
20n =  and/or 40, after which the problem becomes easier. This is due to the fact that the job 
combination increases as the number of jobs increases, and hence it becomes easier to match the 
total amount of processing on 1M  with capacity s (refer to the third column of the table). As for 
the DP algorithm, the computation time increases as n  and s  increase. This is consistent with the 
pseudo-polynomial time complexity of the DP algorithm. Also observe that the computation time 
for each problem is varied since the DP solution time depends on s , which is a function of random 
generated processing times. 
 
6. Conclusions 
 
In the second part of this research an optimal branch and bound algorithm have been proposed 
for scheduling jobs on two identical parallel machines where one is available during a time period 
after which it can no longer process any job. The objective considered is minimizing the total 
completion time.  
The developed branch and bound algorithm differs from the dynamic programming algorithm 
in the literature in that three powerful theorems inherent in the problem are employed. Based on one 
of the theorems, an efficient algorithm, i.e., Algorithm 1, has been proposed for computing upper 
bound that can also be used to obtain the optimal solutions to more than 95% of the problems with 
40 or more jobs.  
The proposed algorithm has been tested on problems of various sizes in terms of the number of 
jobs, three distributions of processing times, and three parameters that determine the availability 
time period on the capacitated machine. The results of these tests have shown that the branch and 
bound algorithm can be used to solve various sized problems in a short time. For the most difficult 
problem, the total computation time is less than 0.1 second for a set of 100 problem instances. 
Extension of the developed branch and bound procedure to the problem with more complicated 
availability constrains, e.g., allowing the capacitated machine to process jobs after the unavailability 
 28
Table 2 
Computational results of the DP algorithm for a total of  
100 problem instances (in micro-seconds) 
~ [1,100]jp  
  0.4α =  0.6α =  0.8α =  
 n Max Total Max Total Max Total 
10 16 16 16 16 16 47 
20 16 78 16 124 16 172 
40 31 515 47 1,016 47 1,639 
60 32 998 47 3,046 63 3,932 
80 47 2.656 47 4,453 78 5,889 
100 78 2,170 109 4,805 125 8,610 
~ [1,500]jp  
  0.4α =  0.6α =  0.8α =  
 n Max Total Max Total Max Total 
10 16 125 16 170 16 203 
20 32 797 47 1,841 47 1,855 
40 78 3,543 109 6,015 109 7,379 
60 125 8,124 156 11,325 203 14,932 
80 187 13,161 250 20,425 328 25,757 
100 281 20,903 375 30,065 625 40,205 
~ [1,1000]jp  
  0.4α =  0.6α =  0.8α =  
 n Max Total Max Total Max Total 
10 16 251 31 438 31 594 
20 32 2,015 47 2,787 63 4,171 
40 109 7,547 172 10,549 250 15,046 
60 172 14,874 281 21,639 375 27,873 
80 328 27,671 500 38,126 735 52,671 
100 484 39,948 1016 58,352 953 76,251 
 
 
 
 
 30
they study an on-line makespan minimization problem with jobs having non-zero release times 
(Centeno and Armacost, 2004). As an application in service industry, Hwang et al. (2004) consider a 
parallel machine scheduling problem under different grades of service levels. They investigate the 
worst-case performance of the longest processing time (LPT) algorithm and provide a LG-LPT 
(lowest grade-longest processing times first) rule for the problem. The LG-LPT rule, which is 
slightly modified from LPT, is to first sequence jobs based on the priority of the grade of service 
levels and then sequence the jobs within the same level in the LPT order. 
To the best of our knowledge, the optimal algorithm has not been developed for the 
max/ /jPm M C  problem with non-unit-length jobs. In the third part of this research, we consider a 
two level system where high level machines can process all the jobs and low level machines can 
process only those jobs with low level. We will propose an algorithm to obtain the optimal solution 
to the problem.  
 
2. Proposed Algorithm 
 
We now formally define the addressed problem. Consider a two-level system with m 
( h lm m= + ) machines and n ( h ln n= + ) jobs, where the subscripts h  and l  stand for the high 
and low levels. Let jp  be the processing time of job j and ( )h lT T  be the total processing time 
of the  ( )h ln n  jobs. The hn  jobs can only be processed on the hm  machines and the ln  jobs 
can be processed by all the machines. The objective considered is minimizing the makespan.  
Since all the jobs can be processed by the high-level machine, the number of the high-level 
machines has a greater impact on the solution. Thus, we start to investigate the nature of the 
problem with the case 1hm = . 
The case 1hm =  
We denote the single high-level machine by hM  and the largest processing time among all 
the low-level jobs by lp . In Theorem 1, we consider a lower bound on makespan for the problem. 
 
Theorem 1.  For the case 1hm = , the lower bound on makespan can be expressed as 
{ }max max  , ,  ( ) /l h h lC p T T T m= +⎡ ⎤⎢ ⎥ , where 1 lm m= + . 
Proof.  The hT  has to be processed on hM , and hence it is a lower bound. Now consider the 
following two cases:  
(i) /h l lT T m≥ : In this case, all the lT  should be processed on the lm  machines. The 
problem is reduced to a parallel machine problem, which has a lower bound { }Max , / .l l lp T m⎡ ⎤⎢ ⎥   
(i) /h l lT T m< : In this situation, lT  cannot be completed at time hT . To balance the loading, 
hM  can process part of lT . So a lower bound can be expressed as ( ) /h lT T m+⎡ ⎤⎢ ⎥ .  
Combining all the results, we obtain { }max max  , ,  ( ) /l h h lC p T T T m= +⎡ ⎤⎢ ⎥ .     
 
We now introduce the underlying idea of the algorithm for 1hm = . Clearly, hM  must be busy 
for processing before time hT . The problem of assigning lT  is equivalent to a parallel machine 
problem with non-simultaneous machines where only one machine starts at time hT  (i.e., hM ) and 
other lm  machines start at time zero. When 1lm = , Liao et al. (2005) have proposed an optimal 
 32
combination on M3 directly. Because the unassigned machines reduce to two machines, that arises a 
max2 //P C  problem and we can directly apply the TMO algorithm to obtain the makespan max.C′  In 
Step 5, we can update the upper bound on makespam max max ,C C′= and record the job sets * ,m mS π=  
*
1 1,m mS S− −′=  * 2 2 ,m mS S− −′=   …, *2 2.S S′=  if maxmaxC C′ < . When maxC C′ > , we set maxC C=  and 
terminate the algorithm. If yet, we need to change the job assignment on 3M  and reapply TMO in 
step 6. When all the job combinations on 3M  have been tried (ie, k=1), we check the next 
completion time 1C C= +  and restart the entire search until C has been achieved. For 
convenience, the modified algorithm for three-machine problem is called 3-IMO’. Similarly, we can 
develop algorithms for more than three machines.  
Since the total workload on machines equals to the sum of all the job processing times, we can 
investigate the lower bound on the assigned workload, in ( 0,C ), on iM  in the m-IMO’ algorithm. 
Let ia  be actual assigned workload and ia  be the lower bound on the assigned workload on .iM  
When iia a< , we proceed immediately to change the job assignment in (0,C ) on iM  without 
performing (i-1)-IMO’, which can save much unnecessary computation time. 
 
Theorem 2  In the m-IMO’ algorithm, the lower bound on the assigned workload on machine i , 
ia , can be expressed as 
 ( 1) ,    ,ia T m C i m= − − × =  
 
1
( 1) ( ),      1, 2,...,3.mi kk ia T m C C a i m m= += − − × + − = − −∑  
Proof   
Assume that the completion time C can be achieved. It means that the total capacity m C×  can 
include the total processing time T  and the sum of ( iC a− ) equals .m C T× −  On the first 
selected machine mM , the gap of the workload (i.e. mC a− ) must be not larger than ( )m C T× − . 
We have 
      ( 1)ma T m C≥ − − × .  
Then we obtain the lower bound ( 1)ma T m C= − − × . On the next selected machine 1mM − , the 
sum of gaps (i.e. 1mC a −−  and mC a− ) keeps to be not larger than ( )m C T× − . We have  
       1 ( 1) ( )m ma T m C C a− ≥ − − × + − . 
Hence we obtain 1 ( 1) ( )m ma T m C C a− = − − × + − . Similarly, we can derive the other lower bounds 
1
( 1) ( )mi kk ia T m C C a= += − − × + −∑ , 1,...,3i m= − .     □ 
To analyze the time complexity of the m-IMO’ algorithm, we first study the three-machine 
case. In the worst case, the TMO algorithm needs to be performed for each of different job 
combinations on 3M  for the specific completion time C. This is the same as the 3-IMO algorithm 
(Lin and Liao, 2004), and hence the time complexity of 3-IMO’ algorithm is O( 3 )nK , where K is 
the number of the checked completions times. Therefore, the time complexity of m-IMO’ algorithm 
is O( )nKm . 
 
Example 1.  We illustrate 3-IMO’ algorithm by considering a three-machine problem with 1hm = . 
The processing times of the seven jobs are given in Table 1. First, we can calculate the following 
data: the total processing time 33T = , the lower bound on makespan max 11C =  and the 
composite job cJ  with processing time 9hT = . In 3-IMO’, we start to set the capacity of machine  
 34
assign hn  high-level jobs to two high-level machines. That is, we partition hT  into two sets. Then 
we treat the two sets of workloads as two composite jobs which schedule with ln  low-level jobs. 
We solve the maxPm C  problem by performing m-IMO’, starting from maxC C= , and obtain the 
makespan maxC′ . Note that any machine can process at most one composite job. If C has been 
achieved, we terminate the algorithm. Otherwise, we need to change the job combination of two 
high-level job sets and continue to perform m-IMO’. When all the job combinations of the 
high-level jobs have been tried, we set 1C C= +  and restart to perform m-IMO’.  
 
Two Implementation Problems 
Before introducing the solving algorithm, we need to further investigate two implementation 
problems. In our solution procedure, we need to perform m-IMO’ algorithm once when the two 
high-level job set has been change. Hence an efficient partition of hT  is required. For this purpose, 
we first consider the valid range of the assigned workload and then eliminate unnecessary job 
combinations to save computational time. For convenience, let 1CM  and 2CM  be the job 
completion times with high-level jobs on two high-level machines. The procedure of the m-IMO’ is 
to check whether the completion time C can be achieved. It is clear that 1CM C≤  and 2CM C≤ . 
Since 1 2 ,hCM CM T+ = we have 1 2h hCM T CM T C= − ≥ −  Hence, the valid range of the assigned 
workload is between hT C−  and .C   
Based on the above result, we can partition hT  into 1CM  and 1hT CM−  and treat them as 
two composite jobs. Further, the “workload” of the composite job instead of “job combination” will 
be used in the proposed algorithm. Although the number of job combinations is quite large, the 
values of workloads are limited. Hence we record the values of 1CM  and 1hT CM−  to avoid 
wasting the computation time for repeating the same procedure. Therefore, we only perform the 
m-IMO algorithm several times when the workload in 1CM  falls between hT C−  and .C  
 
Algorithm B 
Step 0. Renumber the high-level jobs 1 2( , ,..., )nhJ J J  in the LPT order, i.e., 
1,  1,2,..., .i i hp p i n+≥ = { }max max , , / 2 , ( ) / ,h l h h lC p p T T T m= +⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥  Set ,1=j  
,ˆmax ∞=C max ,s C=  0,s =  ,1=k  )),(  ...,  ),1(( kπππ =  ,1)1( =π  and .1 )()( ∑ == ki ik pP ππ  
Step 1. Set .1+= jj  If ,hj n>  then go to Step 4; else .)1( jk =+π  
Step 2. If ( 1) ,m kP Cπ + =  then 1+= kk  and go to Step 4. 
Step 3. If ( 1) ,m kP Cπ + <  then .1+= kk  Return to Step 1. 
Step 4. If ( )m mkP aπ <  or ( )( ) 1,m kA Pπ =  then go to Step 7, else set two composite jobs with 
processing time ( )m kPπ  and ( )mh kT Pπ−  respectively and set ( )( ) 1m kA Pπ =  and 
( )( ) 1.mh kA T Pπ− =  
Step 5. Schedule the two composite jobs and the ln  low-level jobs by performing m-IMO’ to 
obtain makespan max'C  and the job sets 1mS −′ , 2mS −′ , …, 2S′ . 
Step 6. If maxmaxC C′ < , update the incumbent makespan and the job sets by letting 
max max ,C C′= * ,m mS π= * 1 1,m mS S− −′= * 2 2 ,m mS S− −′=  …, *2 2.S S′=  If max ,C C≤  update 
maxC C=  and go to Step 8.  
 
36
     Ta
bl
e 
3 
  
Se
t C
PU
 ti
m
es
 o
f 1
00
 p
ro
bl
em
s f
or
 th
e 
pr
op
os
ed
 a
lg
or
ith
m
 
 
1
h
m
=
 
 
 
  
 
 
 
 
 
 
2
h
m
=
 
 
 
 
 
 
3
m
=
 
 
 
 
4
m
=
 
 
 
5
m
=
 
 
 
 
4
m
=
 
 
 
5
m
=
 
 
 
n 
(1
,2
5)
(1
,5
0)
(1
, 1
00
) 
 
(1
,2
5)
(1
,5
0)
(1
, 1
00
)
(1
,2
5)
(1
,5
0)
(1
, 1
00
) 
 
(1
,2
5)
(1
,5
0)
(1
, 1
00
)
(1
,2
5)
(1
,5
0)
(1
, 1
00
) 
10
 
0.
03
 
0.
02
 
0.
05
  
0.
08
 
0.
17
 
0.
56
 
0.
13
 
0.
20
 
0.
25
  
0.
08
 
0.
13
 
0.
48
 
0.
23
 
0.
20
 
0.
50
 
15
 
0.
02
 
0.
03
 
0.
03
  
0.
00
 
0.
38
 
0.
17
 
0.
20
 
1.
39
 
6.
38
  
0.
14
 
0.
53
 
0.
84
 
0.
53
 
0.
64
 
6.
92
 
20
 
0.
05
 
0.
05
 
0.
05
  
0.
06
 
0.
02
 
0.
05
0.
02
 
0.
06
 
0.
14
  
0.
06
 
0.
19
 
0.
81
 
0.
09
 
0.
73
 
3.
00
 
30
 
0.
06
 
0.
00
 
0.
02
  
0.
02
 
0.
03
 
0.
05
 
0.
05
 
0.
00
 
0.
06
  
0.
02
 
0.
03
 
0.
09
 
0.
05
 
0.
02
 
0.
06
 
50
 
0.
06
 
0.
05
 
0.
06
  
0.
05
 
0.
06
 
0.
08
 
0.
08
 
0.
08
 
0.
09
  
0.
06
 
0.
03
 
0.
03
 
0.
05
 
0.
11
 
0.
13
 
10
0 
0.
06
 
0.
03
 
0.
08
  
0.
13
 
0.
13
 
0.
06
 
0.
08
 
0.
08
 
0.
11
  
0.
05
 
0.
08
 
0.
09
 
0.
06
 
0.
13
 
0.
11
 
50
0 
0.
73
 
0.
75
 
0.
81
  
0.
81
 
0.
73
 
0.
81
 
0.
77
 
0.
81
 
0.
81
  
0.
83
 
0.
84
 
0.
81
 
0.
84
 
0.
83
 
0.
81
 
10
00
 
2.
80
 
2.
56
 
2.
66
  
2.
53
 
2.
56
 
2.
55
 
2.
44
 
2.
48
 
2.
44
  
2.
72
 
2.
70
 
2.
86
 
2.
58
 
2.
58
 
2.
44
 
 
 38
4. Conclusions 
 
In the third part of this research, we consider a makespan minimization parallel machine 
problem with nested machine eligibility restraints. In our problem, the machines and jobs are 
labeled two levels, low-level and high-level. The high-level jobs can be only processed by the 
high-level machines and the low-level jobs are available for processing on any machine. We 
propose an optimal algorithm with the developed lower bounds for the addressed problem.  
Due to the machine eligibility restriction, the high-level workload processed only on the 
high-level machines can be treated as the composite jobs. Then the addressed problem can be 
regarded a parallel machine problem for processing the low-level jobs and the composite jobs. We 
proposed an optimal algorithm, called m-IMO’, modified from m-IMO (Lin and Liao, 2004) to 
solve this parallel machine problem. Although the proposed algorithm has an exponential time 
complexity, the results show that it can find the optimal solution for various-size problems in a short 
time.
