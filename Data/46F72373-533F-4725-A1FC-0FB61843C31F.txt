projection by moving an axis around to scale or rotate the axis. Linear mapping
has an important property that it does not break clusters but may cause cluster
overlapping [5], which means it can preserve the cluster structure partially. Star
coordinates has several desirable features like: (1) the mapping is conceptually
and computationally simple, (2) data objects with similar attribute values always
map to nearby positions on the display, (3) the user can edit the axis and change
the projection very easily (the reverse, (4) the position of each data object can
be influenced by all its values or any combination of them according to the user’s
choice [11].
Star coordinates have been used in data mining for model verification. For
example, VISTA is a visual framework which capitalize on the power of vi-
sualization and interactive feedbacks to encourage users to participate in the
clustering revision and validation process [12]. VISTA utilizes α-mapping, the
scale-parameterized form of 2D star coordinates (initial setting of star coordi-
nates with adjustable dimension length) as the visual rendering mechanism. To
improve the cluster quality or distinguish the overlapped clusters, a set of in-
teractive operations are introduced to help users either in unsupervised or in
supervised cluster rendering.
StarClass is an visual classification system which allows users to interact with
the display to create the decision tree [11]. The idea is that parameter setting
for algorithmic classification/clustering methods can be verified or validated by
human via visual inspection. However, such an argument is questionable since
star coordinates itself is a parameterized visualization method determined by
the lengths and the angles of dimensions. In order for the visual verification to
work, a good dimension-axis mapping for the whole data set or a subset of the
data is need.
In this paper, we focus on the problem of automatical axes arrangement in
star coordinates for better visualization. The goal is to find the best projection
that can present clustering phenomenon in the data. Two objective functions,
distance based and correlation based, are proposed and optimized by gradient
approaches. We also provide “Auto Play” which shows a series of projection
results. The rest of the paper is organized as follows. Section 2 discusses related
works. Section 3 presents the dimension-axis mapping approach. Experimental
results are shown in Section 4. Section 5 concludes the paper and suggests the
future work.
2 Axis Arrangement
Star coordinates is a method for visualizing multi-dimensional cluster, trends
and outliers. The fundamental idea is to arrange the coordinate axes on a circle
of a two-dimensional plane with equal (initially) angles between the axes with
an origin at the center of the circle (Figure 1). Initially, all axes have the same
length (width of display area/2). The minimum data value on each dimension
is mapped to the origin, and the maximum value is mapped to the other end of
the coordinate axis. For convenience, we use the min-max normalized notation
Fig. 2. Axis adjustment for car specs.
we should project data such that the various classes are as scattered as possi-
ble. In the later case, we should project data such that the data objects are as
scattered as possible.
2.1 Supervised Mapping
Supervised mapping aims to provide a proper projection for labelled data sets.
This will helps users understand whether the data is well separated or poorly sep-
arated clusters, globular or arbitrary shaped (center-based or contiguity-based)
clusters, with noise and outliers or not. Supervised mapping can also be used
with clustering algorithms to display unlabelled data set.
Suppose we have a set of labelled data objects, X = {xt, yt}, t = 1, . . . , n,
where ytp = 1 if x
t ∈ Cp, and 0 otherwise, p = 1, . . . , k. The goal here is to find
the best axes arrangement such that data points of the same class are as close as
possible, while the data points from two separate classes are as far as possible.
The objective function can be separation defined by the between cluster sum of
squares (BSS):
F1(θ) =
1
2
k−1∑
p=1
k∑
q=p+1



1
d
d∑
j=1
(cpj − c
p
j ) cos θj


2
+

1
d
d∑
j=1
(cpj − c
p
j ) sin θj


2


(1)
where cp = (c
p
1, c
p
2, . . . , c
p
d) denotes the mean or centroid for class Cp.
cp =
∑
t y
t
pd
t∑
t y
t
p
, cpi =
∑
t y
t
pd
t
i∑
t y
t
p
To maximize the between cluster sum of squared distances (BSS), we can
apply numerical methods such as gradient ascent method which progressively
updates the variables as follows:
θ(t+1) ← θ(t) + λ∇F1(θ
(t)) (2)
computation of the pairwise data distance. Let m = (m1, m2, . . . , md) denotes
the centroid of the data set. SSD can be simplified as follows
F3(θ) =
1
2
n∑
t=1


(
1
d
d∑
i=1
(dti −mi) cos θi
)2
+
(
1
d
d∑
i=1
(dti −mi) sin θi
)2
 (8)
Again, gradient ascent method is applied to maximize the sum of squared
distances, where the partial differential
∂F3
∂θi
=
1
d2
n∑
t=1
(dti −mi)

cos θi d∑
j=1
(dtj −mj) sin θj − sin θi
d∑
j=1
(dtj −mj) cos θj


(9)
Another way to define the objective function is though the analysis of at-
tribute correlation. Let corrij denote the correlation coefficient between attribute
i and j, which measures the strength and direction of a relationship between two
variables.
corrij =
∑n
t=1(d
t
i −mi)(d
t
j −mj)√∑n
t=1(d
t
i −mi)
2
∑n
t=1(d
t
j −mj)
2
where mi =
∑n
t=1 d
t
i/n is as defined for centroid. Note that the correlation co-
efficient between two attributes is equivalent to the cosine of the angle between
the deviation vectors for the two attributes. The best axes arrangement is one
where the cosine of the angle between two attributes is as close to the its cor-
relation coefficient as possible. Therefore, we propose the following function to
substitute F1.
F4(θ) =
1
2
d−1∑
i=1
d∑
j=i+1
(corrij − cos(θi − θj))
2 (10)
The objective function F4 can be minimized by gradient descent method with
partial :
∂F4
∂θi
=
d∑
j=1
(corrij − cos(θi − θj)) sin(θi − θj) (11)
The computation time of this objective function, although proportional to
the square of dimensions, is much less than that for Eq. 8.
Dataset # of classes # of dimension Data Size
Satimage 6 36 4435
Segment 7 19 2310
Shuttle 7 9 43500
Iris 3 4 150
Diabetes 2 8 768
Table 1. Dataset Description
Fig. 4. Segment.
Fig. 5. Iris
