segmented to represent the body shapes. When an image 
was inputted, the interest points were searched and 
matched with the constructed models using the AAM 
searching algorithm. The discrete postures are 
identified by the proposed AAM matching algorithm. 
These are the corresponding motion strings in Hidden 
Markov Models (HMM). After the human motion models 
built, the input motion can be recognized by the 
motion string with the highest probability on each 
HMM motion model. 
英文關鍵詞： lip information, AAM, HMM, human motion 
 
 2
一、中文摘要 
現今社會中，人類比以前更加在乎人身安全及隱私問題。隨著日益進步的科技發展，硬
碟和攝影器材價格的下降，數位影音資訊變得更容易取得，自動化的身份辨識系統變成人類
重要的訴求。利用人類生物特徵如人臉、指紋、聲紋、簽名…等做為身份認證的依據，因具
有唯一性和便利性，所以成為最近很熱門的主題。近來有許多雙模組身份認證方法被提出，
利用兩種以上的生物特徵進行辨識，以增加認證系統之可靠度。本計畫提出兩種基於主動外
觀模型(AAM)的身份認證系統，第一種主要是結合音訊、人臉影像及唇形進行身份之認證。
首先利用音訊資料進行關鍵字認證，辨識語者是否說出正確的關鍵字，並利用嘴唇資訊輔助
辨認，當確認關鍵字正確後，則繼續進行語者身份辨識。身份辨識部份，抽取人臉五官靜態
特徵和嘴唇、下巴範圍的移動變化等動態特徵，分別進行身份辨識。最後結合靜態和動態特
徵的辨識結果進行身份判定，並進行身份之認證。第二種則是利用主動外觀模型(AAM)萃取
人體的輪廓作為特徵，輸入至隱藏式馬可夫模型中(HMM)進行姿勢判斷，並進一步進行身份
認證。 
關鍵詞：唇形辨識、隱私、主動外觀模形、馬可夫模形、身份認證 
二、英文摘要 
Currently, people cares more about the security and the privacy than ever before. Digital video 
information can be accessed more conveniently and easily, and hence the results in the boost of 
automatic identity verification or even recognition. Biometrics recognition by using biometrics 
features attracts the attention of researchers because of its uniqueness and convenience. Among 
them, multi-model identity authentication is an arising trend to accomplish the authentication goal. 
In this project, two bimodal identity authentication modules based on active appearance model 
(AAM) are proposed. In the first module, the audio information together with the lip information 
are used to check if the user says a correct password. Then, recognize the identity of the user if the 
password were verified as correct. In our work, the static features, like facial features, and the 
dynamic features, like the movement of the lips and the jaw, are used in the identity recognition step, 
respectively. In the second module, a shape and a texture features, were extracted from the 
segmented blobs. The interested points were defined and segmented to represent the body shapes. 
When an image was inputted, the interest points were searched and matched with the constructed 
models using the AAM searching algorithm. The discrete postures are identified by the proposed 
AAM matching algorithm. These are the corresponding motion strings in Hidden Markov Models 
(HMM). After the human motion models built, the input motion can be recognized by the motion 
string with the highest probability on each HMM motion model. 
Keywords: lip information, AAM, HMM, human motion 
 4
間不改變的特性，提出了一個結合指紋和人臉特徵的身份認證系統。Y. Wang 等[10]於 2003 
年發表了一篇雙模身份認證的論文，結合了人臉和虹膜特徵做身份認證，在這篇論文裡仍然
是以人臉和虹膜兩張不同的影像進行辨識，但若能自動化的由人臉影像中擷取出來，會提高
此種特徵組合之雙模身份認證系統的效率和便利性。Slobodan Ribaric 等[8]在 2005 年以一台
掃瞄器和一個網路攝影機分別獲得掌紋和人臉影像，以掌紋資訊和特徵臉進行辨識，最後結
合兩個辨識結果進行身份比對。 Ajay Kumar 等[1]於 2006 年於同一張影像中擷取手掌形狀資
訊和掌紋紋理資訊，並同時考慮這兩項特徵資訊進行認證。除了以上提到的雙模認證所使用
的特徵組合，結合人臉資訊和聲紋的身份認證研究也很常見。S. Ben-Yacub 等[7]於 1999 年
提出了一篇結合人臉影像和語音訊號的雙模語者辨識系統， 以線性預估頻譜係數(Linear 
Prediction Coefficients，LPCC)進行音訊配對和以 Elastic Graph Matching(EGM)的方法進行人
臉配對，最後混合兩個配對分數進行使用者身份認證。Claude C. Chibelushi 等[2]於 2002 年
發表了一篇對於結合音訊影像的雙模辨識方法之整理回顧。人類的語言包含了聽覺的(audio)
和視覺的(visual)的資訊，所謂視覺上的資訊包括人說話時嘴唇、牙齒、下巴和一部份舌頭等
發聲器官的外觀狀態。聽覺和視覺的說話資訊對於語者和語料辨識上有相輔相成的作用，
Patrick Lucey 等[5]在 2006 年提出使用正臉和側臉資訊的唇讀系統中，分別對只有音訊、音
訊加上正臉資訊、音訊加上側臉資訊以及音訊同時加上正臉和側臉資訊等四種情況進行語料
辨識，實驗結果說明同時使用聲音資訊和影像資訊，在有雜訊干擾下的辨識率都比單獨使用
聲音資訊的辨識率要高，證明影像資訊有助於以視訊為基礎之辨識系統的準確率。以視訊為
基礎的辨識方法在其架構中，可以分為特徵結合(Feature fusion)和決策結合(Decision fusion)
兩種方法，前者為抽取出音訊和影像特徵後，即結合兩組特徵值進行辨識，而後者是抽取出
音訊和影像特徵後，先個別進行辨識，最後再結合兩組辨識結果進行最後的決策。綜合以上
基於生物特徵之雙模認證系統的相關研究，考慮到使用視訊資訊進行辨識需要的設備較為簡
單，只需要一個網路攝影機便可以取得所需的資訊，而且以被動方式取得使用者資訊，較不
易讓使用者有侵入性等不舒服的感覺，加上人臉五官、說話時嘴唇的移動和聲紋不易模仿等
因素，因此本計畫採用以唇形辨識與姿勢辨識進行身份辨識的方法。 
 
四、成果 
本研究計劃目標是以主動式外觀模型（Active Appearance Models, AAM）的特性作為特
徵擷取的工具，並開發出唇形辨識與姿勢辨識兩個模組以進行身份認證。本研究計畫共分為
兩個模組：(1)唇形辨識，以及(2)姿勢辨識。各模組的詳細說明如下： 
 
(1) 唇形辨識模組： 
本模組的系統流程分為兩個主要架構，分別為模組訓練和身份認證。在模組訓練的部份，
首先將訓練用的影片資訊，分別抽取出音訊資訊和影像資訊。由音訊資訊中取得語音特徵，
利用此語音特徵資訊，建立一語音辨識模組。另一方面從影片中取得一串序列影像資訊後，
分別對每張影像做人臉區域偵測，擷取已偵測出臉部區域的影像，並對影像大小做正規化。
接著對其中一小部份的正規化人臉影像標記臉部特徵點，利用此已標記臉部特徵點的影像資
訊，建立一臉部特徵點模組，再利用此模組對所有正規化人臉影像做臉部特徵點搜尋，進而
 6
本模組系統架構是先以音訊和動態影像資料進行語音辨識，確認語音正確後，再同時利
用動態和靜態影像資料進行身份辨識的動作。因此在進行語音和身份辨識之前，首先必須要
由影片中抽取出音訊和影像資料，再使用這些資料得到用來做辨識的特徵值完成接下來的流
程。在音訊擷取部份，由於本研究中所採用的影片格式為音頻視頻交錯格式(Audio Video 
Interleaved, AVI)，因此必須先擷取出音訊資料WAV格式檔案，接著將音訊檔切成許多音框，
再對每個音框抽取出語音特徵。而在影像部份，於抽取辨識特徵值之前，必須先搜尋出人臉
特徵點的座標位置。由於本系統只使用到人臉區域裡的特徵點，所以在搜尋特徵點之前，必
須先進行人臉偵測，以偵測出人臉的區域範圍。由於後續的特徵抽取，需使用到這部分的特
徵點座標資訊，且為了達到良好的辨識目的，人臉與特徵點偵測的準確率變得十分重要。在
本模組中，雖然語音辨識的部份是同時使用到音訊和影像資訊，但由於系統是分別先以音訊
資訊和影像資訊做辨識，再將兩者之結果各乘予一權重值，總和此加權值為最後之結果，因
此音訊檔和影像檔資訊不需要做同步處理。 
在偵測出人臉並予以正規化後，我們以主動式外觀模型（Active Appearance Models, 
AAM）擷取特徵，外觀模型是結合了形狀模型和紋理模型建構而成的。欲建立外觀模型之前，
必須要有一些訓練用的人臉影像，並於影像上標記特徵點，如圖三所示： 
 
 
圖三：已標記的人臉影像 
找到每張影像片段中相對應的特徵點座標位置後，考慮人在說話時的臉部五官特徵動作，我
們定義了 120 個特徵點，如表一所示： 
 
 
 
 
 
 
 8
代替二至三個特徵點。以下即為本模組中靜態影像特徵值的取法：基於人臉五官有其相對應
位置關係，和一些不隨時間變動的特徵值，我們定義了 7 種特徵值，分別是： 
1. 右眉毛內側至右眼內眼角的垂直距離 
2. 左眉毛內側至左眼內眼角的垂直距離 
3. 右眉毛寬度 
4. 左眉毛寬度 
5. 右眼寬度 
6. 左眼寬度 
7. 雙眼之間的距離 
8. 鼻頭寬度 
9. 雙眼中心點至鼻頭下端中心點的垂直距離 
10. 臉的寬度 
為了使所有特徵值有比較的依據，以上 10 個特徵值皆需除以“眉心至耳下的垂直距離“。
為了方便表示這些特徵值，以下將特徵點和特徵值參數化，如圖五所示，表二為各符號於圖
五中所代表之位置，並以表三列出 10 個特徵值的求法。 
 
 
圖五：特徵點和特徵值參數化 
 
 
 
 
 
 
 
 10
表三:特徵表示法 
右眉毛內側到右眼內眼角的垂直距
離 
左眉毛內側到左眼內眼角的垂直距
離 
右眉毛寬度 
yPyPD .2.61 −=  
11
11 D
DF =  
yPyPD .3.72 −=  
11
22 D
DF =  
( )2,13 PPdD =  
11
33 D
DF =  
左眉毛寬度 右眼寬度 左眼寬度 
( )4,34 PPdD =  
11
44 D
DF =  
( )6,55 PPdD =  
11
55 D
DF =  
( )8,76 PPdD =
11
66 D
DF =  
兩眼距離 雙眼中心點至鼻頭下端中心點的垂
直距離 
鼻頭寬度 
( )7,67 PPdD =  
11
77 D
DF =  2
.7.6.1 xPxPxC +=  
2
.7.6.1 yPyPyC +=  
( )17,18 PCdD =  
11
88 D
DF =  
( )10,99 PPdD =
11
99 D
DF =  
臉寬度 眉心至耳下的垂直距離  
3
.13.12.11.2 xPxPxPxC ++=  
3
.13.12.11.2 yPyPyPyC ++=  
3
.16.15.14.3 xPxPxPxC ++=  
3
.16.15.14.2 yPyPyPyC ++=  
( )3,210 CCdD =  
11
1010 D
DF =  
2
.14.11.4 xPxPxC +=  
2
.14.11.4 yPyPyC +=  
( )4,111 CCdD =  
 
 
 
 
 
 
於表格中， ( )bad , 表示特徵點a和特徵點b之間的距離。 
本模組的重點，即在提出不同人講同樣字彙時，每個人在某些特徵點之位移變化上會有
所不同，另一方面在關鍵字的部份，由於英文字的發音會導致嘴形的不同，因此將可利用這
些特徵值去辨識關鍵字和身份，以致於不會有偽裝的情況發生。本模組對一段使用者說話的
影片，其所產生的一串連續影像片段去觀察其光流變化，發現人在說話時，在嘴唇和下巴的
地方會有較大的光流變化，如圖六所示。 
 12
接下來要進行確認關鍵字與辨識身份的步驟，以下為辨識部份的流程圖： 
 
關鍵字辨識
語音
SVM 模組
關鍵字是否
正確 ? 否決
身份是否
符合 ?
否決
接受身份辨識
身份
SVM 模組
39維MFCC
靜態影像
特徵
動態影像
特徵
是
否
是
否
唇型特徵
 
圖九:關鍵字與身份辨識流程圖 
 
首先利用39維的MFCC參數和動態影像特徵進行關鍵字確認，若關鍵字錯誤則發出錯誤
的警告通知並拒絕該使用者。若關鍵字正確則繼續進行身份辨識的動作，這個部份則是同時
參考了靜態和動態影像特徵。分別使用靜態和動態的影像特徵去得到身份辨識的結果，並對
兩個辨識結果設權重值作最後的身份判斷。並發出警告通知。利用這樣的辨識機制，在關鍵
字確認的部份，即可排除大部份事先錄音播放錄音帶內容的情況，因為事先錄音無法預測系
統隨機指定的關鍵字。而使用動態影像特徵進行身份辨識，並結合靜態影像辨識結果，可以
避免利用照片偽裝的可能性，因為每個人說同樣的字，嘴唇和下巴的位移變化不會相同，即
使利用照片偽裝，也很難模仿他人說話時嘴唇和下巴的移動情形。 
由於語音資訊有時間序列的概念，所以通常是以隱藏式馬可夫模型進行辨識，而本論文
基於支援向量機在高維空間有很好的分類效果，則選擇 使用此方法進行語音和唇型序列特
徵的辨識。下圖為利用語音特徵辨識關鍵字之流程： 
 
 
圖十:關鍵字辨識流程圖 
 14
 
圖十三:決策結合流程圖 
 
 首先結合語音特徵和唇型特徵進行關鍵字辨識，進行關鍵字確認，若正確才繼續進行身
份辨識。身份辨識決定結合的部份，對第一部份和第二部份的身份辨識結果設權重值，結合
兩者的權重機率後，即可得到最後的身份辨識結果，再檢查此身份是否符合。 
實驗所採用的資料庫當中，有 7 位男性和 2 位女性，其中 5 位有戴眼鏡，4 位沒有戴眼
鏡，圖十四為影片資料庫中每位實驗對象之影像片段，每位實驗對象有 30 組字彙的視訊資
料，如表四所示，每組字彙重複兩次，其中一組做為訓練資料，另一組為測驗資料。每段視
訊資料格式如下：影像大小 480*720，音訊大小 16k，框架速度為 29 框架/秒。 
 
 
圖十四:影片資料庫之影像片段 
 
 16
控系統中主要考量的自動化操作與即時性。傳統的監控系統通常是純粹使用大量的監控設備
與操作人員以求達到即時監控的效果，而為了節省成本則必須減少操作人員的支出，進而大
量的監控設備僅由少數的操作人員做即時監控，大大減少了操作人員之效率與監控系統之監
控能力。本計畫透過人體動態行為分析的技術即時分析並對分析結果判斷出人體的姿勢與行
為，並進一步進行身份辨識。 
傳統的人體動態行為分析通常是以輪廓為基礎，利用前景擷取技術（如背景相減法）擷
取出人體輪廓後，針對其進行行為分析；亦或透過輪廓取得身體各部位的資訊，進而建立出
人體骨架並與資料庫進行比對進而分析出行為。但是，隨著人不同的習慣動作，儘管是同一
種動作在不同的人身上也會有差異因而造成輪廓的多樣性大幅提升，進而使得行為辨識率大
幅降低。因此，如何描述人體輪廓並使其能夠適用於大多數的人在人體動態行為分析上扮演
了一個很重要的角色。因此，本模組將利用主動式外觀模型（Active Appearance Models）來
描述人體輪廓，利用定義好的身體輪廓特徵點透過建立好的主動式外觀模型對這些特徵點進
行追蹤以找出測試影像相對應之特徵點；最後，利用隱藏馬可夫模型的概念進行行為分析。 
本模組主要可分為三大部分：前景偵測、姿勢判定、行為辨識三大部分。首先，在前景
偵測部份先利用目標前景物尚未進入畫面之背景影像建立背景模組，之後再利用建立好之背
景模組對包含前景物之影像進行背景相減，透過影像與背景影像兩者之差異取得前景區域。
最後再利用形態學影像處理技巧使前景區域群聚成一塊完整區域。姿勢判定部分則是將上述
取得的前景區域做比例切割並給予虛擬色彩之後利用事先透過主動式外觀模型建立好的姿勢
模組分別對前景區域做搜尋，針對搜尋之結果利用區域比對與整體比對的方法選出最佳的搜
尋結果作為前景姿勢判定之結果，並以 HMM 判定行為。 
 
 
圖十五:姿勢辨識流程圖 
 
當背景模組建立好之後，表示背景影像與輸入影像之差異已可計算出來，且此差異可分解為
輸入影像 
背景相減 
AAM人體 
姿勢模組訓練 
前景影像
前景區域切割 
AAM樣板比對 AAM人體 
 姿勢模組 
背景模組 
姿勢判定 
連續姿勢字元
行為字串
HMM模組比對 
行為判定 
HMM人體 
行為模組訓練
HMM人體 
 行為模組 
 18
圖十六:正規化亮度失真直方統計圖 
圖十七:正規化色彩失真直方統計圖 
從（圖十六）與（圖十七）可以觀察出亮度失真與色彩失真的分布並非高斯分部。因此，
閥值的選擇將透過統計的方式來決定。首先，先根據一連串的背景模組訓練影像建立正規化
亮度失真直方統計圖（圖十六）與正規化色彩失真直方統計圖（圖十七），再根據預期的偵
測比率（detection rate，r）自動選定閥值。正規化後的
∧
iα 直方統圖（圖十六）中，兩個門檻
值 1ατ （3.0）和 2ατ （-3.0）定義了亮度的範圍，從 1ατ 為
∧
iα 的值於偵測比率 r 到 2ατ 為
∧
iα 的值
於偵測比率（1－r）；正規化後的彩色失真（
∧
iCD ）直方統計圖（圖 2.3），門檻值 CDτ （3.0）
Normalized Alpha Value 
∧
iα  
-5 1 2 3 4 5 0-1-2-3 -4 
2ατ  1ατ  
Frequency 
1000 
2000 
3000 
4000 
Normalized CD 
∧
iCD  
CDτ  
Frequency 
0 3 3.5 4 2.5 2 1.5 1 0.5 
2000 
4000 
6000 
8000 
 20
 根據主動式外觀模型（Active Appearance Models，AAM）的特性，每個姿勢模組分
別選擇五至六個訓練樣本作訓練。首先，先針對每張已進行過前處理之訓練樣本建立相對應
的特徵點檔案，分別記錄每張訓練樣本的特徵點位置及順序、特徵點路徑的順序及類型。每
個姿勢模組相對應之特徵點的分配如下： 
1. 姿勢ㄧ：代表雙腳靠攏。利用一條包含 6 個點的路徑代表頭部；分別利用兩條包含
4 個點的路徑代表身體的兩側；最後，利用兩條包含 6 個點的路徑分別代表前腳與後腳，故
此姿勢模組總共使用 5 條路徑且其中包含 26 個特徵點。 
2. 姿勢二：代表雙腳自然張開。同樣利用姿勢ㄧ的方式建立相對應之特徵點資訊檔案。 
3. 姿勢三：代表前腳伸直且抬起後腳。同樣利用一條包含 6 個點的路徑代表頭部；分
別利用兩條包含 4 個點的路徑代表身體的兩側；最後，利用一條包含 9 個點的點的路徑表示
下半身的特徵點，故此姿勢模組總共使用 4 條路徑且其中包含 23 個特徵點。 
4. 姿勢四：代表雙腳合併且彎曲。同樣利用一條包含 6 個點的路徑代表頭部；分別利
用兩條包含 4 個點的路徑代表身體的兩側；最後，利用一條包含 8 個點的點的路徑表示下半
身的特徵點，故此姿勢模組總共使用 4 條路徑且其中包含 22 個特徵點。 
5. 姿勢五：代表雙腳張開且前腳彎曲的弓箭步姿勢。同樣利用姿勢ㄧ的方式建立相對
應的特徵點資訊檔案。 
6. 姿勢六：代表蹲下的姿勢。同樣利用姿勢三的方式建立相對應的特徵點資訊檔案。 
 
   
模組ㄧ 模組二 模組三 
   
模組四 模組五 模組六 
圖十九:姿勢模組 
 將每個姿勢模組訓練影像分別建立好相對應的特徵點資訊檔案後即可分別建立出相
 22
3. 屬於姿勢模組三之前景影像於各個模組之搜尋結果： 
   
原始前景影
像 姿勢模組一 姿勢模組二 姿勢模組三
    
虛擬色彩影
像 姿勢模組四 姿勢模組五 姿勢模組六
圖二十二:搜尋結果三 
 
4. 屬於姿勢模組四之前景影像於各個模組之搜尋結果： 
    
原始前景影
像 姿勢模組一 姿勢模組二 姿勢模組三
    
虛擬色彩影
像 姿勢模組四 姿勢模組五 姿勢模組六
圖二十三: 搜尋結果四 
 
 24
接下來就要針對利用主動式外觀模型搜尋的結果選出最好的搜尋結果進行姿勢的判定。
本論文對於姿勢的判定主要使用兩種比對準則： （1）區域比對（Local Match）：利用 Chamfer 
Distance 進行區域比對；（2）整體比對（Global Match）：利用姿勢模組與搜尋結果兩者區域
重疊比例與搜尋結果區域超出姿勢模組區域的面積比例。下面將針對所使用的三種方法分別
做進一步的介紹： 
1. Chamfer Distance：計算前景物邊緣上的每個像素點與利用姿勢模組所得到的搜尋結
果兩者外型的相似性。 
Chamfer Distance 是指物體邊緣上每一個點至另一個物體邊緣的最短距離總和，其值越大
則表示兩者外形之差異越大；反之，值越小則兩者外形越相近。 
首先，利用 Distance Transform 建立每個姿勢模組所得到的搜尋結果之特徵點所圍成外形
相對應的 Distance Map，記錄每個像素點與邊緣的最短距離。可用下圖表示：  
            
    a. 原始輪廓影像               b. Distance Map 
圖二十六:Distance Map 
 
   
模組ㄧ 模組二 模組三 模組四 模組五 模組六 
圖二十七:Edge Map 
 
2. 姿勢模組與搜尋結果兩者區域重疊比例（Covered Region Rate，C.R.）：即是計算利
用姿勢模組所搜尋的結果中，特徵點所形成的區域中佔姿勢模組特徵點所形成區域的比例。 
 26
將上述取得的前景區域予以正規化並利用比例關係給予其虛擬色彩（Pseudo Color）再透
過建立好的六個姿勢模組做搜尋，並從六個姿勢模組搜尋結果中根據第四章所述的姿勢判定
方法選出每個目標前景物最適合的姿勢並給予相對應的符號代表之，作為姿勢判定之結果。
實驗結果如下圖所示： 
   
原始前景影像 虛擬色彩影像 正確判定結果 
圖三十:姿勢模組ㄧ判定結果 
   
原始前景影像 虛擬色彩影像 正確判定結果 
圖三十一:姿勢模組二判定結果 
   
原始前景影像 虛擬色彩影像 正確判定結果 
圖三十二:姿勢模組三判定結果 
 28
訓練樣本個數與AAM搜尋正確率之關係
85.00%
87.00%
89.00%
91.00%
93.00%
95.00%
97.00%
99.00%
2 4 6 8 10
訓練樣本個數
搜
尋
正
確
率
Model 1
Model 2
Model 3
Model 4
Model 5
Model 6
 
圖三十六:各姿勢模組訓練樣本與AAM搜尋正確率之關係圖 
 
從上圖中可觀察出，姿勢模組二與姿勢模組三搜尋正確率隨著訓練樣本數的增加而增加，但
是至訓練樣本數6個之後正確率的提升已不明顯；姿勢模組四與姿勢模組五至訓練樣本個數為
8個時達到最高之搜尋正確率，之後再增加訓練樣本個數反而使搜尋正確率降低；最後，姿勢
模組ㄧ與姿勢模組六至訓練樣本個數為6個時達到最高之搜尋正確率，之後再增加訓練樣本個
數反而使搜尋正確率降低。綜合上述觀察之結果，可得到下圖綜合表示訓練樣本個數與姿勢
模組搜尋正確率之關係： 
 
AAM 訓練樣本與搜尋正確率之關係
88.00%
90.00%
92.00%
94.00%
96.00%
98.00%
100.00%
2 4 6 8 10
訓練樣本個數
搜
尋
正
確
率
平均正確率
 
圖三十七:姿勢模組訓練樣本與AAM搜尋平均正確率之關係圖 
 
從上圖可觀察出姿勢模組在訓練樣本個數為6個時會有最好的平均搜尋正確率，故本姿勢模組
均使用6個訓練樣本。 
 30
表八:長度為 50 之測試樣本在不同長度訓練樣本下之行為辨識率  
訓練長度 
行為 
跳躍 跛行 蹲下 走路 辨識率
20 10 22 6 10 70.59%
30 10 28 10 11 86.76%
40 10 28 11 11 88.24%
50 10 27 11 11 86.76%
60 10 27 9 11 83.82%
ALL 10 27 9 11 83.82%
樣本
個數 10 29 13 16  
 
表九:長度為 40 之測試樣本在不同長度訓練樣本下之行為辨識率  
訓練長度 
行為 
跳躍 跛行 蹲下 走路 辨識率
20 16 29 7 16 78.16%
30 16 34 13 16 90.80%
40 16 34 12 17 90.80%
50 16 33 11 16 87.36%
60 16 33 7 15 81.61%
ALL 16 32 8 14 80.46%
樣本
個數 16 36 14 21  
 
綜合上述結果，本模組將訓練樣本的序列長度定為 40（frames），而測試樣本之序列長度
定為 30（frames），分辨對包含四種行為之 589 段測試樣本作實驗，可得到以下結果：根據下
表所示，四種行為共 589 段測試樣本一共有 550 段測試樣本的行為辨識正確，39 段測試樣本
辨識錯誤，因此，根據本實驗結果經計算可得辨識率為 93.37%。透過本模組，可將行為辨識
進一步與上一個模組搭配進行個人身份辨識使用，可提升身份辨識的正確率。 
 
表十: 589 段分割影片測試結果 
原始行為 
辨識結果 跳躍 跛行 蹲下 走路 辨識率 
跳躍 91 3 3 0 93.8% 
跛行 2 251 1 22 90.9% 
蹲下 5 0 87 2 92.5% 
走路 0 1 0 121 99.1% 
589 段分割影片測試結果（辨識率 93.37%） 
 32
[14] S. Ju, M. Black, Y. Yacoob “Cardboard People: A Parameterized Model of Articulated Image 
Motion” International Conference on Face and Gesture Analysis, 1996. 
[15] Fujiyoshi and A. J. Lipton. “Real-Time Human Motion Analysis by Image Skeletonization,” 
Proceedings of the Fourth IEEE workshop on Application of Computer Vision, pp.15 - 21, 
1998. 
[16] I. Haritaoglu, et al. “Ghost: A human body part labeling system using silhouettes,” in proc. of 
the 14th Intl. Conf. on Pattern Recognition, pp. 77 – 82, 1998. 
[17] Wren, Christopher R., Azarbayejani, Ali, Darrell, Trevor, Pentland, Alex, “Pfinder: Real-Time 
Tracking of the Human Body”, IEEE Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 19, No. 7, pp. 780-785, July 1997. 
[18] Chih-Chiang Chen, Jun-Wei Hsieh, Yung-Tai Hsu, Chuan-Yu Huang, “Segmentation of 
Human Body Parts Using Deformable Triangulation,” ICPR, vol.1, pp.355 – 358, 2006. 
[19] J. W. Davis, A. F. Bobick: “The Recognition of Human Movement Using Temporal 
Templates”. IEEE Transaction on Pattern Analysis and Machine Intelligence, vol. 23 no.3, pp. 
257 - 267, 2001. 
[20] D. Y. Chen, H. Y. Mark Liao, H. R. Tyan, and C. W. Lin, "Automatic Key Posture Selection 
for Human Behavior Analysis," IEEE International Workshop on Multimedia Signal Processing, 
Shanhai, China, November 2005. 
[21] Wren, Christopher R., Azarbayejani, Ali, Darrell, Trevor, Pentland, Alex, “Pfinder: Real-Time 
Tracking of the Human Body”, IEEE Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 19, No. 7, pp. 780-785, July 1997. 
[22] I. Haritaoglu, D. Harwood, L. S. Davis, ”W4：Who? When? Where? What? A Real-Time 
System for Detecting and Tracking People”, Proc. International Conference on Face and 
Gesture Recognition, April, 14-16, 1998. 
[23] J. Barron, D. Fleet, S. Beauchemin, “Performance of optical flow techniques”, International 
Journal of Computer Vision, vol. 12, no. 1, pp. 42-77, 1994. 
[24] C. Anderson, Peter Burt, G. van der Wal, “Change detection and tracking using pyramid 
transformation techniques”, In Proceedings of SPIE - Intelligent Robots and Computer Vision, 
volume 579, pages 72–78, 1985. 
[25] T. Horprasert, D. Harwood, and L.S. Davis, “A Statistical Approach for Real-time Robust 
Background Subtraction and Shadow Detection”, IEEE International Conference on Computer 
Vision, Frame-Rate Workshop, pp.1-19, 1999 
[26] M. B. Stegmann, B. K. Ersbøll, R. Larsen, "FAME - A Flexible Appearance Modelling 
Environment", IEEE Transactions on Medical Imaging, vol. 22(10), Institute of Electrical and 
Electronics Engineers (IEEE), 2003. 
[27] L. R. Rabiner. “A Tutorial on Hidden Markov Models and Applications in Speech 
Recognition,” Proceeding of the IEEE, pp. 257 – 286, 1989. 
[28] S. Eickeler, A. Kosmala, G. Rigoll: “Hidden Markov Model Based Continuous Online Gesture 
Recognition”. In: International Conference on Pattern Recognition (ICPR), Brisbane, vol. 2, 
pp.1206 – 1208, 1998. 
  
1 
 
出席國際學術會議報告 
                                                            100 年 10 月 27 日 
 
報告人姓名 王宇晨 
服務機構及職稱 中央大學資工系博士生 
會議時間 
自民國 100 年 10 月 18 日至 100 年 10 月 21
日 會議地點 
西班牙 巴塞隆納 
經費來源 NSC 99-2221-E-008-069-MY3 
會議名稱 
(中文) 2011 年 ICCST 國際研討會 
( 英文 ) 45th IEEE INTERNATIONAL CARNAHAN CONFERENCE ON 
SECURITY TECHNOLOGY(ICCST) 2011 
發表論文題目 (英文) The color identification of automobiles for video surveillance 
參與性質 Oral 
報告內容： 
一、參加會議經過 
此次參加 ICCST 2011 國際學術研討會，係於 2011 年 10 月 18 日至 10 月 18 日在西班牙巴塞
隆納舉行，會議地點是在巴塞隆納附近城鎮馬塔羅（Mataró）中的Tecnocampus Mataró Maresme 
University 舉行，學生除了在會議中發表一篇研究論文，並參加多場專題演講。 
2011/10/16：晚間 10:45 搭乘中華航空 CI 65 班機飛往阿姆斯特丹後轉搭荷蘭航空 KL 1673 班 
           機前往巴塞隆納機場。並於 10/17 號下午 15:00 抵達巴塞隆納機場，並前往預定 
           飯店登記住房。 
2011/10/18：下午至會場報到，並參與開幕晚會，由大會主席歡迎所有參與此次會議的長官及  
           學者們，過程中並介紹許多參與會議的各國相關國土安全防護的政府長官及西班 
           牙當地官員。 
2011/10/19：早上 9:00 大會正式開始，大會開幕吸引相當多學者的參與，首先有請大會主席 
           Gordon Thomas 簡單闡述本研討會議的歷史及主旨。今日有 6 場 Sessions，學生 
           參與 Biometrics 和 Biometrics II 兩場，主要描述相關生物認證機制應用於機場保 
           安及大城市安全監控上。 
2011/10/20：今日大會安排一場 Keynote speech 由 Fernando J. Sánchez Gómez 國家關鍵基礎設 
  
3 
 
2.建議與期許 
感謝國科會同意補助參加 ICCST 2011 國際學術研討會，有多國的與會學者，國內亦有多
所學校的學者專家參與會議，此次的研討會，讓學生不僅學習到影像處理領域相關的技
術及應用，也借由多場 sessions 的報告，學習到其它領域知識及安全監控在國土安全的
應用，如指紋辨識、掌紋辨識、機場 X 光機影像辨識和語音辨識等，藉由多面向的學習，
激發出新的應用領域及改進目前既有技術的方法，多場精采專業的演講，不僅讓學生學
習到新知外，更學習到報告時應有的台風、用詞、以及時間的掌控，在有限時間內做最
詳細的技術說明，並讓與會人員清楚了解所想傳達之意思，是學生需努力練習的。學生
此回參與國際學術研討會，很榮幸的獲得其他學者在研究面上的提點，讓學生可以修正
所提的方法，提高準確度及效能，並從他人的研究成果中，得到一些新的研究想法，可
說是獲益良多。 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片。 
     
四、其它 
附件一、論文接受通知函 
附件二、論文全文 
 The Color Identification of Automobiles for Video Surveillance 
 
Yu-Chen Wang1, Chen-Ta Hsieh1, and Chin-Chuan Han2, Kuo-Chin Fan1 
1Department of Computer Science and Information Engineering, 
National Central University, Taoyuan, Taiwan 
2Department of Computer Science and Information Engineering, 
National United University, Miaoli, Taiwan 
 
 
Abstract – Color identification of automobiles plays a significant in 
intelligent transportation systems (ITS). In this paper, a novel 
scheme for color identification of automobile is proposed using the 
taillight detection and a template matching module. The taillights of 
cars are detected to find the valid regions of interested (ROIs) for 
color identification. The color feature vectors generated by 3 by 3 
neighboring pixels are classified by a template matching strategy. 
Seven classes, red, yellow, blue, green, black, white, and gray, are 
identified in this work. Experimental results have been conducted to 
show the validity of the proposed method. The averaged accuracy 
rate 81.71% is achieved and the performance of this scheme is up to 
20 frames per second.  
 
 
Index Terms — Intelligent transportation system, region of 
interest, color identification of automobiles, taillight detection 
algorithm, color template matching algorithm. 
 
 
I.  INTRODUCTION 
 
Recently, the automobile color identification is increasingly 
demanded on urban roads for video surveillance. When an accident 
occurs, license plate (LP) numbers are the directly cues of escape 
cars. However, they are not the effective cues due to the missed and 
small LPs which are caused from the view angle and distance of 
witnesses. Witnesses only remember the automobile colors. The 
color identification from video data can assist police agencies in the 
crime prevention and the later investigation of crime events. 
Moreover, the government widely set up cameras on the road for 
traffic monitor or crime prevention. How to extract color of 
automobile effectively and also taking color as a metadata of other 
feature recognition is a hot issue of current research. 
Early, color feature descriptors have widely used in content 
based image retrieval (CBIR)[1-3]. High quality images with less 
illumination impact in their works. However, the color identification 
of automobiles in outdoor is susceptible to the camera design and 
environmental factors. First, the sequential images captured from the 
outdoor cameras are distorted by chromatic polarization and white 
balance. Second, the performance of identification is impacted by the 
illumination and weather in outdoor. Cameras capture the color 
features from the front, side, or back views of automobiles. 
Whichever views are captured, the over-exposure regions in images 
are always generated due to the sun light. As illustrated in Fig. 1, the 
classification errors always occur. Baek et al. [4] extracted the two-
dimensional histogram on the hue and saturation features. A multi-
class support vector machine (SVM)-based classifier was trained to 
identify color of automobiles. Tsia et al.[5] proposed a color model 
to reduce the illumination impact in outdoors. After the 
transformation on color spaces, a Bayesian classifier is used to 
identify the vehicle color and background. Chen et al. [6] applied the 
PCA-based technique to calculate the eigenvectors and the 
corresponding eigenvalues from the training samples represented in 
the RGB color space. The color pixels in space RGB are transformed 
to the new color space. The multiple instance learning is used to 
classify color. Brown[7] has evaluated four color feature descriptors, 
including standard color histograms, weighted color histograms, 
variable bin size color histograms and color correlograms, to identify 
vehicle colors for surveillance. Stokman et al.[8] proposed a fusion 
scheme to combine several color models. The selected and weighted 
color model is constructed for the detection of discriminatory and 
robust color features. Dule et al.[9] manually assigned the regions of 
interested (ROIs) on car hoods from the detected foreground objects. 
The color features in the half ROI are classified to determine the 
vehicle’s color. Practically, automatically finding the ROI is the 
critical issue in surveillance systems. Wu et al.[10] found the ROIs 
by integrating the results of background subtraction and those of 
color segmentation. An SVM-based classifier in a two-layer 
structure is then applied to classify the pixels in ROI. Li et al.[11] 
proposed a classification method using the template matching 
strategy. Before identification process, a color calibration procedure 
is executed to adjust the image colors [12]. The HSI color space is 
selected and the relative error distances are calculated to identify the 
automobile colors.  
 
 
(a) (b) 
 
(c) (d) 
Figure 1: The over-exposure regions (red regions). (a)(b): the frontal 
views, and (c)(d) the back views. 
 
In this paper, a novel scheme is proposed to identify the 
automobile colors in outdoors as shown in Fig. 2. The identification 
process consists of two modules: The location of a valid ROI and the 
classification using color features. Unlike the traditional background 
subtraction methods which are very sensitive to illumination 
changes, a taillight detection algorithm is proposed to obtain the 
  
(a) (b) 
 
(c) (d) 
Figure 4: (a) an original image, (b) the detected red regions, (c) a pair 
of taillights, and (d)the valid ROI. 
 
Many color spaces (e.g. RGB, HSV, YCrCb, CIELab...etc) are 
explored for object segmentation or classification using color 
features. In this study, color space CIELab model is adopted for 
representing car colors. From the previous studies, the colors in 
space CIELab are very close to human visual aspects. Due to the 
illumination of sun light, robust features should be extracted to 
reduce the noises and lighting impacts. The feature vectors of length 
27 are generated from the 3 by 3 neighboring pixels in space CIELab. 
They are classified with the trained prototypes by the template 
matching rule. Each feature vector is represented as { }
nkikikikiki
vvvvv ,........,,,
321
= , that subscript i denotes the ith 
template sample, subscript k represents the kth color index, and n is 
the vector dimension, 27 in this work. As to the parameters of 
template matching, k nearest neighbor (K-NN) strategy is performed 
to find the best matched prototype. In addition, the metric of 
Euclidean distance is adopted to assess the similarity of color 
features as defined as:  
 
( ) 2
1
1
2
, ∑
=
⎟⎠
⎞⎜⎝
⎛
−=
n
m
jkijki
ij
k mm
vvvvd
.
 (4) 
 
Here symbol kiv  denotes the color features of the kth trained 
samples, and symbol jv is the feature vectors in ROI. Using the 
results in section 3.A, the valid ROI is obtained by using taillight 
detection algorithm. The feature vector for each pixel in ROI is 
extracted and matching with the trained prototypes. The similarity is 
defined as the inversed Euclidean distance. The color of each pixel in 
ROI is classified by EQ. (5):  
 ( )jkiijkkk vvdC ,minarg= , (5) 
 
where Ck denotes the color prototype index. Finally, the distances 
from ROI to seven prototypes are calculated. The statistics data 
determine vehicle in which the color of the valid ROI area with 
highest percentage, and defines this color for the automobile. In this 
study, seven colors’ prototypes, e.g., black, gray, white, red, yellow, 
green and blue, are collected which frequently appear in city roads. 
 
 
III.  EXPERIMENTAL RESULTS 
 
In this section, some experiments are conducted to show the 
effectiveness of the proposed method. A stationary CCD camera was 
installed in outdoors. Six video clips are captured in 30 f/s in various 
sunny, cloudy, and rainy days. 8319 image frames of size 320 by 240 
were extracted from the video clips, and there are 3654 image frames 
possess the automobiles. Several illustrated images used in the 
experiments are shown in Fig. 5. The identification system is 
implemented in a PC-based platform and is developed by the 
Microsoft visual C++ 2008 and Opencv 2.1 tool kits.  
The taillights are first detected and a valid ROI is determined. The 
color features in ROI are extracted and matched with the trained 
prototypes by using the template matching strategy. The detection 
rates as shown in Table I are very high. In addition, 3579 taillight 
pairs are correctly paired from 3654 pair of taillights whose ground-
truths are manually assigned. The averaged detection rate for 
taillights is more than 98%. The proposed method could correctly 
detect the taillight regions from videos. The taillight pairs are 
verified by the geometric rules to generate the valid ROI, and the 
color features in ROIs are extracted to classify the vehicle colors. 
The automobile colors are determined by the dominative colors in a 
valid ROI area. The template matching algorithm considers the 
neighboring pixels to reduce the noise distortion. The window pixels 
of 3 by 3 are fused to generate a color feature of length 27 in CIELab 
color space. Since the cameras are installed in outdoors, the image 
quality of automobile colors is poor with much variation due to the 
illumination.  
In the experiments, the colors of ROIs are classified into seven 
classes using the template matching algorithm, e.g. color red, yellow, 
blue, green, black, white and gray. Several correct classified results 
are shown in Fig. 6. Each illustration correctly finds the valid ROI 
and identifies the correct color. The classification rates for six video 
clips are tabulated in Table II. The averaged accuracy rate is 81.71%. 
In addition, the identification performance in the proposed method is 
up to 20 frames per second. On the contrary, two mis-classification 
results are shown in Fig. 7. The errors are resulted from the invalid 
ROI. Invalid ROIs will generate the incorrect features. The locations 
of taillights for a van and a truck are different from those of a car. 
The truck taillights are located at the lower parts, and the windshield 
of a van is located between two taillights.  
 
TABLE I 
The detection rates of taillight regions. 
Video clips Match taillight pairs 
Number of total 
pair taillights detection rates 
1 450 455 0.989 
2 530 557 0.952 
3 432 457 0.945 
4 166 167 0.994 
5 981 996 0.985 
6 1020 1022 0.998 
Average 
Accuracy rate   0.98 
 
 
 consists of two modules: The location of a valid ROI and the 
classification using color features. The taillights are detected and a 
valid ROI is generated. The pixels in ROI are classified to determine 
the color of cars using color features. The neighboring pixels are 
considered to effectively reduce the illumination impacts and noises. 
From the conducted experimental results, the high performance of 
the proposed method is achieved. In the future works, the shapes of 
cars will be identified to correctly locate the valid ROI. The ROIs 
from the frontal view or the side view will be extracted for car color 
identification. In addition, more image processing algorithms will be 
designed to overcome the over-exposure problem. 
 
 
V.  ACKNOWLEDGEMENTS 
 
The work was supported by the National Science Council 
under grant no. NSC 99-2221-E-239 -030, and by the 
Technology Development Program for Academia of DOIT, 
MOEA, Taiwan under grant no. 99-EC-17-A-02-S1-032. 
 
 
VI.  REFERENCES  
 
[1]. I. K. Park, I. D. Yun and S.U. Lee “Color image retrieval 
using hybrid graph representation,” Image and Vision 
Computing, vol. 17, pp. 465-474, 1999. 
[2]. L. Cinque, G. Ciocca, S. Levialdi, A. Pellicanò and R. 
Schettini, “Color based image retrieval using spatial 
chromatic histograms,” Image and Vision Computing, vol. 19, 
pp. 979-986, 2001. 
[3]. G. Qiu, “Embedded colour image coding for content-based 
retrieval,” Journal of Visual Communication and Image 
Representation, vol. 15, pp. 507-521, 2004. 
[4]. N. Baek, S. M. Park, K. J. Kim, and S. B. Park, ‘‘Vehicle 
color classification based on the support vector machine 
method,’’ Communications in Computer and Information 
Science, vol. 2, pp.1133-1139, 2007. 
[5]. L. W. Tsai, J. W. Hsieh and K. C. Fan, ‘‘Vehicle detection 
using normalized color and edge map,’’ IEEE Transactions on 
Image Processing, vol. 16, pp.850-864, 2007. 
[6]. S. Y. Chen, J.W. Hsieh, J. C. Wu and Y. S. Chen, “Vehicle 
retrieval using eigen color and multiple instance learning,” 
International Conference on Intelligent Information Hiding 
and Multimedia Signal Processing, pp.657-660, 2009. 
[7]. L. M. Brown, “Example-based color vehicle retrieval for 
surveillance,” IEEE International Conference on Advanced 
Video and Signal Based Surveillance, pp.91-96, 2010. 
[8]. H. Stokman and T. Gevers, "Selection and fusion of color 
models for image feature detection," IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 29, pp. 371-
381, 2007. 
[9]. E. Dule, M. Gokmen and M.S. Beratoglu, “A convenient 
feature vector construction for vehicle color recognition,” 
Proceedings of the 11th WSEAS international conference on 
nural networks, evolutionary computing and Fuzzy systems, 
2010. 
[10]. Y. T. Wu, J. H. Kao, and M. Y. Shih, “A vehicle color 
classification method for video surveillance system 
concerning model-based background subtraction,” Pacific-
Rim Conference on Multimedia, vol. 6297, pp. 369-380, 2010. 
[11]. X. Li, G. Zhang, J. Fang, J. Wu and Z. Cui, “Vehicle color 
recognition using vector matching of template,” International 
Symposium on Electronic Commerce and Security, pp.189-
193, 2010. 
[12]. G. D. Finlayson, B. Schiele, J.L. Crowley, “Comprehensive 
color image normalization,” Proceedings of 5th European 
Conference on Computer Vision, vol. 1, pp. 475-490, June 
1998. 
[13]. Y. Y. Lu, C. C. Han, M. C. Lu and K. C. Fan, “A vision-
based system for the prevention of car collisions at night,” 
Machine Vision and Applications, vol. 22, pp. 117-127, 2011. 
 
 
VIII.  VITA 
 
Yu-Chen Wang was born in Taichung, Taiwan, R.O.C., in 1984. 
He received the B. S. degree in Department of Computer Science 
and Information Engineering from Min-Dao University, Changhua, 
Taiwan, in 2005. He received M. S. degree in the Department of 
Computer Science and Information Engine from Chung Hua 
University, Hsinchu, Taiwan, in 2008. He is currently pursuing the 
Ph.D degree in computer science at the National Central University. 
His research interests include pattern recognition, computer vision, 
and machine learning. 
Cheng-Ta Hsieh was born in Taipei, Taiwan, R.O.C., in 1983. 
He received the B. S. and M. S. degree in Department of Computer 
Science and Information Engineering from Chung Hua University, 
Hsinchu, Taiwan, Republic of China in 2005. He is currently 
pursuing the Ph. D degree in computer science at the National 
Central University. His research interests include pattern recognition, 
computer vision, and machine learning. 
Chin-Chuan Han received the BS degree in computer 
engineering from National Chiao-Tung University in 1989, and the 
MS and PhD degrees in computer science and electronic engineering 
from National Central University in 1991 and 1994, respectively. 
From 1995 to 1998, he was a postdoctoral fellow in the Institute of 
Information Science, Academia Sinica, Taipei, Taiwan. He was an 
assistant research fellow in the Telecommunication Laboratories, 
Chunghwa Telecom Co. in 1999. From 2000 to 2004, he worked 
with the Department of Computer Science and Information 
Engineering, Chung Hua University, Taiwan. In 2004, he joined the 
Department of Computer Science and Information Engineering, 
National United University, Taiwan, where he became a professor in 
2007. He is a member of the IEEE, the SPIE, and the IPPR in 
Taiwan. His research interests are in the areas of face recognition, 
biometrics authentication, video surveillance, image analysis, 
computer vision, and pattern recognition. 
Kuo-Chin Fan received the BS degree in electrical engineering 
from the National Tsing-Hua University, Hsinchu, in 1981, and the 
MS and PhD degrees from the University of Florida (UF), 
Gainesville, in 1985 and 1989, respectively. In 1983, he joined the 
Electronic Research and Service Organization (ERSO), Taiwan, as a 
computer engineer. From 1984 to 1989, he was a research assistant 
with the Center for Information Research at UF. In 1989, he joined 
the Institute of Computer Science and Information Engineering, 
National Central University, Chung-Li, Taiwan, where he became a 
professor in 1994, and from 1994 to 1997, he was the chairman of 
the department. Currently, he is the director of the Computer Center. 
His current research interests include image analysis, optical 
character recognition, and document analysis. He is a member of the 
SPIE. 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：范國清 計畫編號：98-2221-E-008-066-MY3 
計畫名稱：運動分析於生物認證之應用-真假臉分辨,動態唇型身份認證,與走路姿勢身份認證 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
