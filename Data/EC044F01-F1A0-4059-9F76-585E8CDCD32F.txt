 2
  
中文摘要 
 
 
隨著數位照相機與數位攝影機的普及，有愈來愈多人用這些數位產品來記錄生
活上所發生的點點滴滴。為了要找出拍攝影片時場景的位置所在，本計畫以影像視
訊內容分析與辨識的技術，開發「以視訊內容為基礎之視訊場景定位系統」。在完
成本系統的過程中，我們開發了建築物偵測、建築物辨識及場景定位與蒐集相關資
訊等技術，並建構完成了三個子系統：建築物偵測系統，建築物辨識系統，場景定
位系統。所開發之「以視訊內容為基礎之視訊場景定位系統」雛形能分析影像與視
訊內容，並且與已知拍攝地點的照片來比對，進而達到影像拍攝地點的定位。透過
本系統，影像與視訊的提供者將可容易的得到與視訊畫面相關的空間資訊與時間資
訊。 
 
關鍵詞：類神經網路，影像分析，場景定位 
 
  
 4
一、前言 
 
隨著數位照相機與數位攝影機的普
及，有愈來愈多人將其拿來記錄生活上與
週遭上所發生的點點滴滴：如紀錄旅遊行
程，紀錄特殊事件等。一般這些所拍攝的
影像或照片不外乎包括人、事、時、地、
物等五種構成要素。然而在觀看拍攝的作
品時往往發現，對於大部分的作品我們只
能就拍攝時的人、事、時、物可以很清楚
的描述；對於拍攝的地點卻無法很正確的
描述。 
為了解決這個問題，目前市面上已
有 GPS 紀錄器（GPS Data Logger）來輔
助使用者紀錄拍攝時的地點。然而拍攝者
仍需透過後製作，將拍攝的時間與 GPS
紀錄器所紀錄的時間相對應。如此才能得
到拍攝位置的正確經緯度。因此，市面上
也出現了高階的數位照相機將 GPS 紀錄
器包含於照相機中。當拍攝者按下快門的
同時，照相機立即將其拍攝地點的經緯度
紀錄在此照片的檔頭中。在瀏覽照片的時
候，觀賞者只需透過軟體便可以很方便且
非常清楚地知道拍攝時所處的位置。至於
數位攝影機結合 GPS 紀錄器，目前則仍
未看到相關的產品出現。 
雖然目前已經有 GPS 紀錄器可以來
輔助拍攝者紀錄拍攝當時的位置，但對於
沒有 GPS 紀錄資料的大量照片或影像，
如何找出拍攝時的確切位置仍是一個有待
研究的課題。 
本計畫以影像分析與辨識的技術，
開發「以視訊內容為基礎之視訊場景定位
系統」。觀察目前一般所拍攝的影片可以
發現，大部分的攝影者往往會將拍攝點的
地標或者有代表性的建築物拍攝到影片中。
因此，若能找出場景中地標或代表性建物
所在位置，就能知道拍攝時場景的位置進
而得到與此場景相關的資訊。 
本系統主要是由三個子系統所組成：
建築物偵測系統，建築物辨識系統，場景
定位系統。當使用者輸入一個尚未定位的
視訊後；首先，建築物偵測系統會分析視
訊中的畫面並偵測出含有建築物的畫面。
接著，建築物辨識系統會針對這些含有建
築物的畫面，判斷此畫面的建築物是什麼，
進而透過資料庫的查詢得到此建築物的資
訊：如名稱，經緯度等。有了這些資訊，
場景定位系統即可根據經緯度將其定位於
地圖上；同時也可以將其相關資訊顯示於
地圖上供使用者查詢觀看。 
目前在建築物偵測的研究上，有著
重於如何在衛星圖或是空照圖中偵測是
否有建築物[1-8]。由於衛星圖或是空照
圖都是由空中往地面拍攝的，所以建築
物呈現在這類影像中的大部分是天台或
屋頂，因此這些研究的主要工作是在偵
測建築物的屋頂[4] [5]。另一方面，一些
偵測在地面拍攝的影像中的建築物的研
究也已發表 [10-13]。 
在場景定位上，目前已有網站提供
使用者上傳影像或視訊[29][30]。不過，
這些上傳資料的拍攝地點的定位，仍需
上傳者手動於地圖上點選或者是上傳拍
攝時 GPS 所紀錄的經緯度。因此，若能
自動的分析影像與視訊來做拍攝地點的
定位並且自動的蒐集網路上的相關資訊，
將可讓影像與視訊的提供者更方便的分
享其作品。 
 
二、研究目的 
 
本計畫目的要完成一「以視訊內容
為基礎之視訊場景定位系統」。藉由此
計劃的執行，我們將開發建築物偵測、
建築物辨識及場景定位與蒐集相關資訊
等技術。根據所開發的技術，本計畫擬
完成的系統能自動的分析影像與視訊內
容來做拍攝地點的定位，並且自動的蒐
集網路上的相關資訊。 
 
三、研究方法 
 
本系統主要是由三個子系統所組成：
建築物偵測系統，建築物辨識系統，場
景定位系統。此三個子系統之研究方法
與步驟如下： 
1. 建築物偵測系統 
基本上，建築物偵測可視作一個影
 6
 
五、計畫成果自評 
 
本計畫開發了建築物偵測、建築物辨
識及場景定位與蒐集相關資訊等技術；並
完成了「以視訊內容為基礎之視訊場景定
位系統」來自動分析影像與視訊內容來做
拍攝地點的定位。透過本系統，影像與視
訊的提供者將可更方便的分享其作品；觀
賞者可讓更容易的得到與視訊畫面相關的
空間資訊與時間資訊。本研究成果除可自
動化的將所拍攝的影片之某個場景定位外，
更可於網路上搜尋與此場景相關之時間與
空間資訊，進而拓展拍攝影片本身之相關
知識。於是，觀賞者除可特過本系統知道
拍攝地點外，也可很容易的獲得與此地點
相關的歷史、地理，甚或旅遊資訊。因此，
本計劃除能於國家在資訊數位化所欲促進
之教育、學術研究有所助益外，更可促進
民生育樂上的利用。 
 
參考文獻 
 
[1] A.Huertas and R. Nevatia, “Detecting 
buildings in aerial images,” Computer Vision, 
Graphics, and Image Processing, vol. 41, pp. 
131-152, 1988. 
[2] R.B.Irvine, and D.M.McKeown, “Methods 
for Exploiting the Relationship Between 
Buildings and Their Shadows in Aerial 
Imagery,” IEEE Transactions on Systems, 
Man, and Cybernetics, vol.19, no.6, 
pp.1564-1575, 1989. 
[3] Jefferey A. Shufelt and David M. McKeown, 
Jr., “Fusion of monocular cues to detect 
man-made structures in aerial imagery”, 
CVGIP: Image Understanding, vol.57 no.3, 
p.307-330, May 1993. 
[4] S. Krishnamachari and R. Chellappa, 
“Delineating buildings by grouping lines 
with MRFs,” IEEE Trans. on Image 
Processing, vol. 5 no. 1, pp. 164-168, Jan 
1996. 
[5] Chungan Lin and Ramakant Nevatia, 
“Building detection and description from a 
single intensity image,” Computer Vision 
and Image Understanding, vol.72 no.2, 
p.101-121, Nov. 1998. 
[6] Sanjay Noronha and Ramakant Nevatia, 
"Detection and Modeling of Buildings from 
Multiple Aerial Images," IEEE Transactions 
on Pattern Analysis and Machine 
Intelligence, vol. 23, no 5, pp. 501-518, May 
2001. 
[7] Christopher Jaynes, Edward Riseman, and 
Allen Hanson, “Recognition and 
reconstruction of buildings from multiple 
aerial images,” Computer Vision and Image 
Understanding, vol.90, no.1, p.68-98, April 
2003 
[8] Z. Kim and R. Nevatia, “Automatic 
Description of Complex Buildings from 
Multiple Images,” Computer Vision and 
Image Understanding, vol. 96, no. 1, pp. 60-
95, 2004. 
[9] A. Vailaya, A. K. Jain, and H. J. Zhang. “On 
image classification: City images vs. 
landscapes,” Pattern Recognition, vol. 31, pp. 
1921–1936, 1998. 
[10] Qasim Iqbal and J.K. Aggarwal, “Applying 
Perceptual Grouping to Content-Based 
Image Retrieval: Building Images,” in 1999 
IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition 
(CVPR'99), vol. 1, pp.1042, 1999. 
[11] Yi Li and Linda G. Shapiro, “Consistent 
Line Clusters for Building Recognition in 
CBIR,” in Proceedings of the 16 th 
International Conference on Pattern 
Recognition (ICPR'02), vol. 3, pp.30952-
30956, 2002. 
[12] Sanjiv Kumar and Martial Hebert, “Man-
Made Structure Detection in Natural Images 
using a Causal Multiscale Random Field,” In 
Proc. IEEE Int. Conf. on Computer Vision 
and Pattern Recognition, pp. 119-126, 2003. 
[13] Google Maps API from Google on the World 
Wide Web: 
http://code.google.com/apis/maps/ 
[14] Yahoo! Maps API from Yahoo! on the World 
Wide Web: 
http://developer.yahoo.com/maps/ 
[15] Panoramio on the World Wide Web: 
http://www.panoramio.com/ 
[16] Cmoremap on the World Wide Web: 
http://www.cmoremap.com.tw/ 
11. Power electronics and motion control systems 
12. Intelligent computation in advanced measurement systems 
13. Control systems to improve quality of life 
14. Identification and compensation for the hysteresis nonlinearities 
15. Mechatronic systems MIC 
16. Mechatronic systems fault diagnosis and fault tolerance 
17. Artificial intelligence in mechatronic systems 
18. Swarm intelligence 
19. Intelligent modelling for social and economic applications 
20. Networked control system 
21. MIC of mechanical and electrical equipment and component 
22. Human adaptive mechatronics 
23. Discrete-time systems 
24. Nonlinear and linear system identification 
25. Discrete event and hybrid systems 
26. Robotics 
b]'* 
Saturday, July 17, 2010 08:50 - 09:00 Opening Remarks  
09:00 - 10:00 Plenary Speech 1 (PS01) Special lecture room  
Title: Multiple-model-based diagnosis and fault tolerant control  
Speaker: José Ragot, Nancy University, France  
10:00 - 10:20 Break  
10:20 - 12:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SatA01  
Novel and unconventional 
linear system modelling 
techniques  
SatA02  
Neural networks, 
fuzzy logic, and 
other heuristic 
techniques I  
SatA03  
Simulation, 
modelling and 
control on smart 
grids I  
SatA04  
Static and dynamic 
systems modelling I  
SatA05  
Control and 
optimization 
techniques I  
12:00 - 13:00 Lunch  
13:00 - 14:00 Plenary Speech 2 (PS02) Special lecture room  
14:20 - 16:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SunM01  
Artificial intelligence in 
mechatronic systems  
SunM02  
Swarm intelligence: 
Theory and 
application  
SunM03  
Simulation, 
modelling and 
control on smart 
grids III  
SunM04  
Intelligent 
modelling for social 
and economic 
applications  
SunM05  
Networked control 
system I  
16:00 - 16:20 Break  
16:20 - 18:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SunP01  
MIC of mechanical and 
electrical equipment and 
component  
SunP02  
Human adaptive 
mechatronics  
SunP03  
Simulation, 
modelling and 
control on smart 
grids IV  
SunP04  
Discrete-time 
systems  
SunP05  
Networked control 
system II  
19:00 - 21:00 Banquet (ANA Hotel Okayama)  
	
Monday, July 19, 2010 10:00 - 11:20 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  
MonA01  
Nonlinear and linear system 
identification  
MonA02  
Discrete event and 
hybrid systems  
MonA03  
Robotics  
MonA04  
Static and dynamic systems modelling II  
11:50 - 12:50 Closing Reception (Peach Union)  
	
Kp%Neural networks, fuzzy logic Intelligent computation and control system
Swarm intelligence Static and dynamic systems modelling() 7 18
 SunA05 Neural networks, fuzzy logic, and other heuristic techniques II sessionH
tuhuh:; ” Generalized Probabilistic Decision-Based Neural Networks for 
Texture Classification and Retrieval”c&'Kp
* 
	uh	
	
p	

dKp ¡¢£&p¤¥d
¦§¨£&©ªug«w¬G­®¯°±b²v±

³&´G­µ¶^©·¸Kuhg¹ºHI¡»¼½¼p¾
¿ÀÁÂÃÄÅgÆºÇI¡»ÈÉ±´ÊËpÊ¿
ÌÍ%¡»Î£&Ï	
	
Ðl	
ÄÑÒ£'Ó£ 
)Jj¡¢£&p¤¥dÔ±Ôª
uÕ£&ÄÖ@!×ØÙÚÄÛ¼G­ÜÝÌÞßàG­áo
ºHI¼G­½âpÊ¿
ãAäpSåæçèéê¡ë	
Generalized Probabilistic Decision-Based Neural Networks for Texture
Classification and Retrieval
Yeong-Yuh Xu, S.C. Chuang, C.-L. Tseng, and Hsin-Chia Fu
Abstract—For various applications, formulating texture fea-
tures in distributional forms can sometimes provide meaningful
representation than in numerical forms. In this paper, we first
proposed a novel methodology for measuring the difference
between two mixture Gaussian distributions. Based on the
derived formula, a Generalized Probabilistic Decision-Based
Neural Network (GPDNN) is then proposed and implemented
to realize the difference measurement method. By constructing
a two layered pyramid-type network structure, the proposed
GPDNN receives data in distributional form via 2-D grid
input nodes, and outputs the classification and/or retrieval
results from the top layer node. Forty texture images are
selected from the MIT Vision Texture (VisTex) database to
evaluate the proposed GPDNN for the texture classification
and retrieval. Experimental results show that (1) by using
the proposed difference measurement methods, the texture
pattern retrieval rates can be improved from 77% to 82%,
compared with some published leading methods, and (2) the
proposed GPDNN shows significant texture classification and
retrieval performance, which are about 90.1% and 88.6% of
the accuracy, and much better than the traditional methods,
i.e., 82.2% and 79.9%, respectively.
I. INTRODUCTION
In the past decades texture analysis has received lots of
attention and numerous methods have been proposed[1], [2],
[3] due to its wide applications in computer vision and pat-
tern recognition[4]. As the support of physiological studies
of human visual cortex[5], [6], wavelet-like approaches are
popular in extracting and formulating texture features in the
recent years[7], [8], [9], [10].
Based on those extracted features, it is still a challeng-
ing task and an ongoing research topic to find a human-
perception based similarity/dissimilarity measurement be-
tween textures. some methods are proposed for computing
the norm-based distances (e.g., Euclidean distance) as the
dissimilarity measurement [9] between two feature vectors.
In the case that the texture pattern features are formulated in
distributional forms, norm-based distances are not suitable
to measure the dissimilarity between textures. For some
pattern recognition applications, formulating features in dis-
tributional forms can sometimes provide more meaningful
representation than numerical forms can do[3], [9].
In order to measure the difference between two normal
distributions, the weighted Euclidean distance has been pro-
This work was supported by the National Science Council under Grant
NSC98-2218-E-241-001
Yeong-Yuh Xu is with the Department of Computer Science and Infor-
mation Engineering, Hungkuang University, Taichung 433, Taiwan, ROC
yyxu@sunrise.hk.edu.tw
S.C. Chuang, C.-L. Tseng, and Hsin-Chia Fu are with the Department of
Computer Science, National Chiao Tung University, Hsinchu 300, Taiwan,
ROC hcfu@cs.nctu.edu.tw
posed as the dissimilarity measuring function in [9], where
the weighting factors are the standard deviation of each
normal distribution. When a feature component needs to
be represented by a complex distribution, such as mixture
Gaussian distribution, weighted Euclidean distance is not
suitable for dissimilarity measurement.
In this paper, we proposed a novel methodology for the
measurement of the difference between two complex distri-
butions. Furthermore, based on the proposed methodology,
a Generalized Probabilistic Decision-Based Neural Network
(GPDNN) is proposed and implemented for the measure-
ment of two complex distributions. Forty texture images are
selected from the MIT Vision Texture (VisTex) database
to demonstrate and evaluate the proposed GPDNN for the
texture classification and retrieval. Experimental results show
that, by using the proposed difference measurement methods,
the retrieval rates is improved from 77% to 82%, compared
with some published leading methods [3]. The proposed
GPDNN shows significant texture classification and retrieval
performance, which are about 90.1% and 88.6% of the
accuracy, and much better than the traditional methods, i.e.,
82.2% and 79.9%, respectively.
The rest of this paper is organized as follows. In the
next section, the proposed difference measurement is in-
troduced, and the formula for difference measurement of
two distributions are presented. Then, in Section III, the
implementation of GPDNN and its learning scheme are
introduced. Experimental results are presented and discussed
in Section IV. Finally, Section V draws some concluding
remarks.
II. DIFFERENCE MEASUREMENT BETWEEN TWO
DISTRIBUTIONS
Given two distributions Pa and Pb, the difference between
Pa and Pb could be defined as
ϕ(Pa,Pb) =
∫
RD
(Pa − Pb)
2dz, (1)
where RD is a D-dimensional feature space. Suppose Pa and
Pb are two mixture Gaussian distributions,
Pa =
Ra∑
n=1
P anp(z | θ
a
n)
and
Pb =
Rb∑
n=1
P bnp(z | θ
b
n),
Proceedings of the 2010 International Conference on 
Modelling, Identification and Control, Okayama, Japan, July 17-19, 2010
©2010 ICMIC 500
i1q
t
1q
i
P1
the parameters of P
i the
para
met
ers
of
P t
F (P
i
,P
t
)
´
´
´
´
´
´
´
´
´
´
´
´
1,1G
2,1G
tR
G ,1
1,2G
2,2G
tR
G ,2
1,iR
G
2,iR
G
ti RR
G ,
i
P2
i
Ri
P
t
P1
t
P2
t
Rt
P
t
P1
t
P2
t
Rt
P
t
Rt
P
t
P2
t
P1
t
2q
t
Rt
q
i
2q
i
Ri
q
1h
2h
iR
h
Fig. 3. The internal architecture of a model in Fig. 2 for computing of
F(Pi,Pt).
the outputs of all the Gn,m nodes are weighted by P
t
m and
summed to the hidden node hn. Suppose that the output of
the hidden node hn is Tn, then
Tn =
Rt∑
m=1
P tm · G(θ
i
n, θ
t
m).
Finally, the output of each hidden node hn are weighted
by P in and then summed to the output node of the pyramid
subnetwork:
F(Pi,Pt) =
Ri∑
n=1
P in · Tn.
When the outputs of all the subnets reach the top layer of
the GPDNN, the MINNET is activated to select the minimum
of the values from the lower subnet and its corresponding
subnet ID. That is if the output value of subnet i is the
minimum among the outputs of all subnets in a GPDNN,
the input data x(t) is classified as the class wi.
B. Learning Phase
The GPDNN adopts the SPDNN learning scheme. While
the input data x(t) belonging to the class wi is misclassifi-
cated to the class wj , the reinforced and antireforced learning
rules are applied to the subnets of wi and wj , respectively;
Reinforced Learning rule:
w
(m+1)
i = w
(m)
i + η∇ϕ(x(t), wi) (4)
Antireinforced Learning rule:
w
(m+1)
j = w
(m)
j − η∇ϕ(x(t), wj) (5)
The gradient vectors in (4) and (5) are computed as follows:
∂ϕ(x(t), wi)
∂µi
n(d)
=
2P
i
n

 Ri∑
m=1
(
P i
m
G(θi
n
, θi
m
)(µi
n(d) − µ
i
m(d))
(σi
n(d)
)2 + (σi
m(d)
)2
)
−
Rt∑
m=1
(
P t
m
G(θt
m
, θi
n
)(µt
m(d) − µ
i
n(d))
(σt
m(d)
)2 + (σi
n(d)
)2
)]
, (6)
∂ϕ(x(t), wi)
∂(σi
n(d)
)2
=
P
i
n

 Ri∑
m=1
(
P i
m
G(θi
m
, θi
n
)
(σi
m(d)
)2 + (σi
n(d)
)2
)
·
(
(µi
m(d) − µ
i
n(d))
2
(σi
m(d)
)2 + (σi
n(d)
)2
− 1
)
−
Rt∑
m=1
(
P t
m
G(θt
m
, θi
n
)
(σt
m(d)
)2 + (σi
n(d)
)2
)
·
(
(µt
m(d) − µ
i
n(d))
2
(σt
m(d)
)2 + (σi
n(d)
)2
− 1
)]
, (7)
for each n ∈ {1, 2, . . . , Ri} and d ∈ {1, 2, . . . , D}.
IV. EXPERIMENTS
A total of 40 different texture images are selected from
the MIT Vision Texture (VisTex) database [12]. The original
size of the images is 512×512, and each image was divided
into sixteen disjoint subimages, i.e., 128×128 pixels. Hence,
the classification problem involved a total of 640 subimages,
16 subimages in each of the 40 texture categories[3], and
only the gray-scale images were used in the experiments.
The Gabor filter bank proposed by Manjunath and Ma [9]
is used to extract the texture features. The filter parameters
we used are four scales and six orientations, which produced
a bank of 24 filters. Mixture Gaussian distributions are used
to approximate the texture feature distributions of the 640
subimages. For each subimage, an EM algorithm is employed
for finding maximum likelihood estimates of parameters,
and the Bayesian information criterion (BIC) [13] is used
to determine the number of components for each mixture
Gaussian distribution. Fig. 4 shows the mean and standard
deviation (SD) of the number of components per texture
category, where the first row is the texture image, the second
row is the texture name, and the third row is the mean and
SD in the form of “mean(SD)”. As we can see, the more
complicated the texture is, the more mixture components are
needed to approximate the texture feature distribution.
To examine the performance of the proposed difference
measurement method, the retrieval results are compared with
the generalized Gaussian density (GGD) and kullback -
Leibler distance (KLD) method proposed in [3]. Similar to
the experiments described in [3], we used each of 640 images
in the databases as a simulated query image, and defined the
relevant images for each query as the other 15 subimages
from the same original VisTex image. The retrieval rate is
the average percentages of retrieving relevant images in the
top 15 matches. Experimental results show that the proposed
method improves the retrieval performance from 77% to
82%, which is 5% better than the GGD and KLD method.
The detail comparison on precision for each texture classis
shown in Fig. 5.
In order to evaluate the performance of the GPDNN, the
cross validation method [14] is used to randomly split the
502
TABLE I
MEANS AND STANDARD DEVIATIONS OF CLASSIFICATION AND
RETRIEVAL ACCURACIES FOR GPDNN, SPDNN, AND MDC, WHERE
THE STANDARD DEVIATION IS GIVEN IN THE PARENTHESES.
classification rate
training set testing set
GPDNN 0.989(0.006) 0.901(0.021)
SPDNN 0.947(0.025) 0.851(0.016)
MDC 0.873(0.022) 0.822(0.016)
retrieval rate
training set testing set t
GPDNN 0.988(0.008) 0.886(0.026)
SPDNN 0.940(0.026) 0.826(0.020)
MDC 0.858(0.025) 0.799(0.023)
are shown in Table I. It is clear to see that the proposed
GPDNN has significant improvements in classification accu-
racy from 82.2% to 90.1% and retrieval accuracy from 79.9%
to 88.6%.
V. CONCLUSION
In this paper, we proposed a novel methodology for mea-
suring the difference between two distributions. Based on the
proposed measuring methodology, a Generalized Probabilis-
tic Decision-Based Neural Network (GPDNN) is proposed.
Forty texture images are selected from the MIT Vision
Texture (VisTex) database to demonstrate and evaluate the
proposed GPDNN for the texture classification. Experimental
results show that (1) the proposed difference measurement
improves retrieval rates, e.g., from 77% to 82%, compared
with some published leading methods, and (2) the proposed
GPDNN shows significant texture classification and retrieval
performance, which are about 90.1% and 88.6% of the
accuracy, and much better than the traditional methods, i.e.,
82.2% and 79.9%, respectively. Although this paper presents
applications of the GPDNN for the texture classification and
retrieval, we believe that a lot of image/pattern recognition
and classification problems and applications can be solved
and/or applied by the proposed GPDNN. For instances,
we are currently working on the building recognition by
GPDNN.
REFERENCES
[1] T. R. Reed and J. M. H. Du Buf, “A review of recent texture
segmentation and feature extraction techniques,” in Computer Vision
Graphics Image Process. Image Understanding, 1993, vol. 57, pp.
359–372.
[2] T. Ojala, M. Pietikainen, and D. Harwood, “A comparative study of
texture measures with classification based on feature distributions,”
Pattern Recognition, vol. 29, no. 1, pp. 51–59, 1996.
[3] Minh N. DO and Martin Vetterli, “Wavelet-based texture retrieval
using generalized gaussian density and kullback-leibler distance,”
IEEE Transactions on Image Processing, vol. 11, no. 2, pp. 146–158,
2002.
[4] M. Tuceryan and A. Jain, Handbook Pattern Recognition and
Computer Vision, chapter Texture analysis, pp. 235–276, Singapore:
World Scientific, 1993.
[5] D.H. Hubel and T.N. Wiesel, “Receptive field, binocular interaction,
and functional architecture in the cats visual cortex,” in J. Physiol,
1962, vol. 160, pp. 106–154.
[6] JG. Daugman, “Two-dimensional spectral analysis of cortical receptive
field profiles,” Vision Research, vol. 20, pp. 847–856, 1980.
[7] A. Laine and J. Fan, “Texture classification by wavelet packet
signatures,” IEEE Transactions on Pattern Recognition and Machine
Intelligence, vol. 15, no. 11, pp. 1186–1190, 1993.
[8] M. Unser, “Texture classification and segmentation using wavelet
frames,” IEEE Transactions on Image Processing, vol. 4, no. 11,
pp. 1549–1560, 1995.
[9] B.S. Manjunath and W.Y. Ma, “Texture features for browsing and
retrieval of image data,” IEEE Transactions on Pattern Recognition
and Machine Intelligence, vol. 18, no. 8, pp. 837–842, 1996.
[10] T. Randen and J. H. Husoy, “Filtering for texture classification: A
comparative study,” IEEE Transactions on Pattern Recognition and
Machine Intelligence, vol. 21, no. 4, pp. 291–310, 1999.
[11] H. C. Fu and Y. Y. Xu, “Multilinguistic handwritten character recogni-
tion by bayesian decision-based neural networks,” IEEE Transactions
on Signal Processing, vol. 46, no. 10, pp. 2781–2789, 1998.
[12] MIT Vision and Modeling Group. Vision Texture.,
“http://vismod.www.media.mit.edu,” .
[13] G. Schwarz, “Estimation the dimension of a model,” Annals of
Statistics, vol. 6, pp. 461V464, 1978.
[14] Pierre A. Devijver and Josef Kittler, Pattern Recognition: A Statistical
Approach, Prentice-Hall, 1982.
504
98年度專題研究計畫研究成果彙整表 
計畫主持人：徐永煜 計畫編號：98-2218-E-241-001- 
計畫名稱：以視訊內容為基礎之視訊場景定位系統之研發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
