they hoped to get better prediction model. Competitors use many 
kinds 
of statistical models and machine learning tools to predict the user 
rates. The best performance is achieved by Hungarian Academy of 
Sciences. They get the first prize and the benchmark RMSE is 
0.256. In 
this project proposal, we use the same dataset, and we want to 
achieve 
the better performance. A novel web recommendation model, called 
LISA 
model, has been proposed. LISA model is based on Latent Dirichlet 
Allocation (LDA) and Social Network Analysis. In our first phase, 
LDA 
extracts topics from movie dataset. According to the rate content 
information, the RMSE we achieve can only ranked after top 5. In 
the 
second experiment, we construct a user social network to get the 
collaborative information from users. By calculating the Bonacich 
Power Centrality, we get the power nodes to adjust user rates. 
According to the LISA model, we combined two phase information 
and 
achieved the better performance than any other competitors in KDD 
Cup 
2007. 
Our proposal aims at providing a more enhanced hybrid 
recommendation 
model to get even better recommendation result than our current 
achievements, and according to the response time requirements in 
real-time environment, we propose to use cloud computing platform 
to 
solve this problem, trying to provide real-time recommendation with
high quality prediction. 
 
 1 
中文摘要 
本計畫目的是要以目前已經由本計畫主持人所開發出來的推薦技術為基
礎，進一步改良並提升推薦系統的正確性，同時更採用雲端計算架構(取代原本
的網格計算架構)，以增進推薦計算的效率。這樣的研究應用了目前主持人在相
關研究領域中所採用的資檢索相關技術，同時也研發並修改了近年來在隱性主
題發掘 (Latent Topic Discovery)的演算方式，使其適當地應用在資訊推薦 
(information recommendation)的領域中。 
 
本計畫利用在 2007 年的 ACM KDD Cup 的比賽中，由 Netflix 網站提供的電
影資料集，經過許多競爭者透過不同的資料探勘以及分群的方式來達到有效的
評分，該屆比賽評分效果最好達到 RMSE =0.256，而這個網站提供這樣的一個
比賽，目的是想要透過比賽發掘出好的資料探勘模型。許多競爭者，利用到 SVM
結合各種協同式推薦(Collaborative recommendation)的方式，以及統計模型的技
巧，來達到好的預測結果。本計畫主要的目的是在利用相同的實驗資料，以期
望求出更好的預測效能，本計畫設計了一個結合隱性主題擷取和社會網路分析
兩種方式的模型，稱為 LISA，來實現良好預測效果。透過兩次分群結果，找出
最佳的預測效能，本模型在一開始只使用隱性主題分析 (Latent Dirichlet 
Allocation)作分群預測，且 RMSE 只能達到當年比賽的前五名，而在加入了社
會網路分析的方式之後，透過 Bonacich Power Centrality 的概念，找出最佳的影
響力中心，達到適當的預測。目前我們達到的最佳效能 RMSE 為 0.237，一舉
超越了當年的冠軍成績。由此可見，目前的 LISA 模型，比之前比賽中的幾個
RMSE 效果更好。 
 
 本計畫後續提出使用更完善的混合式推薦模型進行效果更好的推薦計算，同
時因應即時推薦反應的需求，我們提出了使用雲端計算取代我們原先的網格運算
架構，期待能同時在推薦效果與反應速度上得到提升，達到更令人滿意的結果。 
 
 3 
單一個推薦預測問題的計算時間平均為 0.6 分鐘，這樣的計算時間仍
無法成為即時的演算方式。本計畫的另一項嘗試，就是提出使用雲
端計算平台，將資料儲存以及資料計算都進行適當的分散，以建立
更快速的預測計算方式，試圖建構有效的線上預測系統。 
 
因此本研究將重點置於以隱性主題發掘為基礎，研發以雲端架構計
算為基礎的混合式推薦模型。以下先針對基本的推薦技術進行介紹，接
著依序介紹本計畫先前的開發成果，由其值得一提的是，我們的方法在
以 ACM KDDCup 2007 為資料集(電影評分資料預測)的實驗中，勝過了第
一名的成績，同時在後續的研發改良中，持續提高了正確率(RMSE)，確
立了技術的靠性與潛力。最後我們還會針對我們目前研發成果的特性，
提出改進的方案。本計畫將會同時以提升正確性與效率為目標，建構出
一個以雲端計算為基礎架構的推薦平台，以增進即時推薦的可利用。 
 
2. 研究結果 
 
因此本研究將重點置於以隱性主題發掘為基礎，研發以雲端架構計算
為基礎的推薦模型。在以 MovieLens 電影資料集為測試資料，並以四部四
核心個人電腦(AMD 4X 2.5GHz, 4GB RAM)的組態下，MovieLens 100k 資
料集進行推薦計算測試分別使用網格計算與雲端計算的平均結果為: 
Java-based grid structure: 592 sec 
Hadoop cloud structure: 2184 sec 
以上實驗 Hadoop 部分以預設參數執行之。 
 
上述結果我們發現，網格效能較雲端計算平台要來得好，可能的原因
與討論如下: 
1. Hadoop 平台需先將資料寫入 HDFS，造成時間損耗。反觀 Grid
平台則動態利用網路傳輸資訊給 Grid worker，時間運用較為短
暫。 
2. Grid worker 使用[收工作-運算-結果回傳]的運作流程，反觀
Hadoop 平台需要[收工作-Map 程序發散工作-Reduce 程序會整資
料-結果寫出]的運作流程，似乎較網格結構複雜。 
3. Hadoop 在 mapper/reducer 的管理較為有彈性，網格平台的
deployment 則較不容易，相對來說在管理成本上 Hadoop 較具優
勢，但對於效能則較為弱勢。 
 5 
己的研究更趨完整,亦可激發許多研究靈感。本次發表的論文著重於分類演算法
的研發, 同時以 ACM KDD 於 2009 年舉辦的醫學資料分類為實驗主體, 由於實驗
成果不錯, 因此得以在此場合發表, 與各國研究者進行交流。 
 
三、建議  
 
感謝國科會與本校研發處補助出席國際會議,讓本次的論文發表得以順利成行。
藉由論文發表,可使本校研究者進行國際交流, 並藉以增進國際視野。  
 
四、攜回資料名稱及內容  
 
大會論文發表議程、論文摘要集一份及論文光碟一份。  
 
註:本會研究主題如下表所示, 子題中為粗體底線者與本論文相關 
資料管理 
(Data Management) 
Benchmarking and performance evaluation  
Data exchange and integration  
Data quality, cleaning and lineage  
Database monitoring and tuning  
Data privacy and security  
Data warehousing and mining  
Embedded, sensor, mobile databases and applications  
Managing uncertain, imprecise and inconsistent information 
Metadata management  
Multilingual data management  
Multimedia data management and mining  
Novel Data Types  
Parallel and distributed databases  
Peer-to-peer data management  
Personalized information systems  
Query processing and optimization  
Replication, caching, and publish-subscribe systems  
Text search and database querying  
Semi-structured data  
Social Networks  
Storage and transaction management  
Web services  
資料庫 
(Databases) 
Access methods and indexing  
Authorization, privacy and security  
 7 
Link analysis and community discovery  
Mining and representing data streams, web content, structure 
and usage data  
Mining and representing temporal and spatial data  
Mining and representing text and semi-structured data  
Mining and representing reviews, blogs & discussions  
Novel data mining algorithms  
Pre-processing, post-processing and visual data mining  
Security and privacy issues  
Semantic Integration  
Knowledge Management Processes  
Data Mining (store/discover/propagate)  
Technologies for Knowledge Sharing  
Learning Technologies  
Management and Measurement of Intangibles  
Optimization of Organisations  
Assessment Methods  
Innovation Methods  
Social Networks and Psychological Dimensions  
Case Studies on Collaboration  
Tools for Knowledge Management  
 
 9 
well-known breast cancer detection examinations is through breast X-ray images. In 
recent years, with the progress of computer technology, the X-ray images are often 
stored in digital formats. With these digital images, the cancer classification based on 
the digital information becomes easier than before. The scientists use extracted 
features from historical medical images to train a well-designed classifier, then the 
classifier might be able to correctly classify an unknown image data for screening 
cancerous patients. 
 
To determine whether a patient is cancerous is a typical one-class classification 
problem, since the classification focuses on catching the features of cancer 
images(called target set), and the others are treated as outliers[1]. In this paper, we use 
the data from ACM KDDCup 2008[2] to demonstrate our classification process based 
on latent topic discovery. In this data set, the target set and outliers are quite different 
in their nature: target set is only 0.6% size in total, while the outliers consist of 99.4% 
of the data set. We use this data set as an example to show how we dealt with this 
extremely biased data set with latent topic discovery and noise reduction techniques. 
 
2. Related Work 
 
 There are typically two ways to separate data in groups appropriately: grouping 
by their features without indication, called clustering, and forming groups by existing 
classes, called classification. Clustering is known as unsupervised learning and 
classification is known as supervised learning. In statistics domain, the supervised 
learning is often called discrimination which uses correctly classified data to build 
discrimination rules. There are several evaluation aspects for classification: accuracy, 
speed, comprehensibility, and time to learn. The KDDCup 2008 contest focuses on 
accuracy competition, that is, to compete for the best classification accuracy for given 
data set. As mentioned earlier, the outliers consist of the major part of the data set, 
which is about 99.4% of original data. So the outlier is a major problem to affect the 
classification accuracy in KDDCup 2008. There are two types of error caused by 
outlier: 
 
i. Type I error: the classifier classifies members of target set as outliers. 
ii. Type II error: the classifier classifies members of outlier as target set. 
 
These errors seriously affect the accuracy of classification in our problem. In this 
paper, we will propose a process flow to minimized these errors. 
 
 11 
KDDCup 2008. For the huge gap between target set and outliers, we design a feature 
preprocessing flow for this situation: 
 
i. According to the data distribution in each feature, applying standard outlier 
detection method[7,8] to detect and normalize them. 
ii. Simplify the data complexity by applying feature scaling methods[9]. 
iii. Reducing data noise by a well-known information retrieval technique called 
TF-IDF[10]. 
 
The first part of feature preprocessing is the outlier adjustment. Statistically, 
outliers are observed as an extremely biased data than the normal ones. Grubb's test 
defined outliers as the member outside the largest absolute deviation from the sample 
mean in units of the sample standard deviation[7]. The training data of KDDCup 2008 
shows a heavy-tailed condition. Without knowing the meanings of features in advance, 
we propose to adjust the outliers by using interquartile range: 
 
 Q1 as the 25th percentile data value of target feature 
 Q3 as the 75th percentile data value of target feature 
 Interquartile range IQR = |Q3 – Q1| 
 Adjustment upper bound AUB = Q3 + 3 * IQR …..(1) 
 Adjustment lower bound ALB = Q1 – 3 * IQR …..(2) 
 
 The second part of preprocessing is the scaling of feature values. Feature scaling 
not only can reduce the data complexity but is also possible to discover some 
characteristics of data. According to [9], there are several ways to scale features to 
exemplify the differences between target set and outliers: scaling by variance, scaling 
by domain, and scaling by min-max. The scaling by variance method simply divides 
each feature value by pre-calculated variance. The scaling by domain method scales 
each feature value to an assigned range. The scaling by min-max method assigns 
minimum maximal feature value as the radius R of a sphere, then scaling every value 
to [0, R]. In our experiment, we apply variance and domain scaling to fit to our latter 
steps of the classification process. 
 
The final step of feature preprocessing is noise reduction. In our previous 
experience[11], it is found that the noise reduction technique, TF-IDF, is able to 
improve classification correctness by removing highly frequent but meaningless 
features. The TF-IDF method is a weighted feature filtering mechanism in 
information retrieval domain. This statistical approach evaluates the importance of a 
 13 
 
Fig.2. The proposed processing flow 
 
4. Experiment 
 
 The KDDCup 2008 does not provide answers for the test set, so we will not be 
able to evaluate our classifier with test set data. We separate the original training data 
as two parts, each part contains 826 normal patient records and 59 cancerous ones. 
The reason to make training and test set this way is to keep the correct target set ratio. 
Our training data is applied outlier adjustment first, the upper bound and lower bound 
for each feature is calculated with formula (1) and (2) above. Every feature value 
above AUB will be set to AUB and every value below ALB will be set to ALB. Next 
step the adjusted data is applied with variance and domain scaling. Fig.3 shows the 
original data and preprocessed distribution. 
 
 15 
 
Fig.5. Latent topics generated by LDA 
 The number of latent topics is set to 40 as one-third of the number of total 
features. Each document generates a topic vector describe in the previous section. 
These vectors, together with cancerous labels "+1" and "-1", are fed into SVM for 
training classification model. Here we apply libSVM[13] to do the classification job. 
There are four kernel functions provided in libSVM: linear, polynomial, radial basis, 
and sigmoid. Among these functions, the polynomial function is selected because no 
cancerous mark is predicted by other kernel functions in our process. The 10 TF-IDF 
thresholds are applied in preprocessing step to calculate accuracy benchmarks, as 
shown in table 1. 
 
Table 1. The experiment accuracy 
Number Threshold Accuracy 
1 0 99.27% 
2 0.0000002996785829117312 99.32% 
3 0.0000008062351621362247 99.07% 
4 0.000004798197420076042 97.73% 
5 0.000009918308293792554 96.39% 
6 0.00001774619447307921 98.85% 
7 0.00003529061359145861 99.18% 
8 0.00008242479532284817 98.68% 
9 0.00031514232937074674 99.05% 
10 0.004568104199841167 99.38% 
 
 In order to do a better evaluation of our experiment, we calculate Receiver 
Operating Characteristic(ROC)[14] values and generate ROC curve to show our 
results. The ROC curve consists of TPR(sensitivity) as x-axis and FPR(1-specifity) as 
y-axis. The AUC is defined as the area under ROC curve. The AUC result under 
different TF-IDF thresholds is shown in table 2 and Fig.6. 
 17 
statistical methods, how to appropriately increase the positive sample ratio 
is an important way to improve the benchmark result of our approach. 
B. Using patient-wise instead of point-wise processing: the proposed method 
in this paper is candidate-based, that is, point-wise processing. Since the 
number of candidates provided by every patient may vary, using point-wise 
processing seems to be less better than patient-wise method. 
C. The use of TF-IDF approach: the TF-IDF method will be able to filter 
unnecessary noise contained in data set, but it is also possible to filter out 
important message contained in data. According to table 2, the peak 
performance appeared in 5th threshold. But with higher thresholds, the 
benchmark falls, which means positive messages contained in training data 
is also filtered out. How to maintain the best information for classifier will 
be an important issue. 
 
Acknowledgments: This work was supported in part by the National Science Council 
of Taiwan via the grant NSC 99-2221-E-156-007. 
 
References 
 
1. D.M.J. Tax, "One-class classification" , PhD Thesis, Delft University of 
Technology, http://www.ph.tn.tudelft.nl/˜davidt/thesis.pdf ISBN: 90-75691-05-x, 
2001. 
2. Claudia Perlich , Prem Melville , Yan Liu , Grzegorz Swirszcz , Richard 
Lawrence , Saharon Rosset, "Breast cancer identification: KDD CUP winner's 
report", ACM SIGKDD Explorations Newsletter, v.10 n.2, December 2008. 
3. M. Girolami and A. Kaban, "On an equivalence between PLSI and LDA", 
Proceedings of the 26th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp. 433-434, 2003. 
4. Thomas Landauer, P. W. Foltz, and D. Laham, Introduction to Latent Semantic 
Analysis, Discourse Processes 25: 259–284, 1998. 
5. T. Hofmann, "Unsupervised learning by probabilistic latent semantic analysis", 
Machine Learning, vol. 42, no. 1, pp. 177–196, 2001. 
6. D. M. Blei, A. Y. Ng and M. I. Jordan, "Latent Dirichlet Allocation", Journal of 
Machine Learning Research, vol. 3, no. 5, pp. 993-1022, 2003. 
7. Grubbs, F. E., "Procedures for detecting outlying observations in samples", 
Technometrics 11, 1–21, 1969. 
8. Rousseeuw, P. and Leroy, A., "Robust Regression and Outlier Detection", John 
出國會議報告 
100年 7月 5日
報告人 葉建華 (資訊工程系助理教授)
會議期間 2011/06/24-2011/06/26 
會議地點 法國巴黎
會議名稱 (中文) 資訊與知識管理國際會議
(英文) International Conference on Information and 
Knowledge Management
發表論文題目 (中文) 以隱性主題為基礎的醫學資料分類
(英文) Latent Topic Based Medical Data Classification
一、參加會議經過 
2011 年資訊與知識管理國際會議 (2011 International Conference on Information and 
Knowledge Management, ICIKM2011)於 2011年 6 月 24日至 26日舉行,會議地點為法國首都巴
黎 Holiday Inn 飯店(Holiday Inn Paris Montparnesse-Avenue du Maine)會議中心。該研討
會之主辦單位為全世界科學與工程技術學會(World Academy of Science and Engineering and 
Technology, WASET),本會議為每年舉辦一次, 其目的是在於將資訊與知識管理最新的研究進展
與成果, 透過這個會議進行交流與呈現。此會議為全世界從事資料與知識管理相關的工程人員
與研究者至為重要的科技會議,會議為期三天, 共分為以下幾大主題:資料管理(Data 
Management)、資料庫(Databases)、資訊擷取(Information Retrieval)、知識管理(Knowledge 
Management)等以及其下細分的 80個子題(註), 共計超過 840作者發表約 400篇論文。與會的
專家學者主要來自包括美國、法國、德國、加拿大、義大利、印度、日本、韓國、中國、泰國
等60個國家。下圖為議場一隅。
Social Networks 
Storage and transaction management 
Web services 
資料庫
(Databases)
Access methods and indexing 
Authorization, privacy and security 
Concurrency control and recovery 
Database languages and models 
Mobile databases and ubiquitous data management 
Peer to peer, parallel and distributed databases 
Query processing, optimization and performance 
Real-time and active database 
Scientific and biological databases 
Semi-structured data and XML databases 
Stream-based processing and sensor databases 
Temporal, spatial and multimedia databases 
資訊擷取
(Information Retrieval)
Cross-language and multi-lingual information retrieval 
Digital libraries 
Document and query representation 
Document summarization and question answering 
Document tracking, routing, and filtering 
Hypertext and hypermedia management 
Information retrieval applications 
Information retrieval evaluation, metrics 
Information retrieval models and theory 
Information visualization and exploration 
Interactive retrieval, user models and studies 
Interoperability and integration 
Metadata extraction and generation 
Multi-media and content-based information retrieval, 
music retrieval 
Performance issues in information retrieval 
Personalization and personal information management 
Question answering 
Test collections, experimental design 
Text classification and clustering, machine learning 
Web and distributed information retrieval 
知識管理
(Knowledge Management)
Classification and clustering 
Data/information extraction and integration 
Data mining applications 
Explanation and Knowledge Provenance 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/31
國科會補助計畫
計畫名稱: 以隱性主題向量為基礎的雲端式推薦技術之研究與實做
計畫主持人: 葉建華
計畫編號: 99-2221-E-156-007- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
