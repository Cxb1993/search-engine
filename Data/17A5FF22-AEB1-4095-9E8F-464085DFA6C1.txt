2 
 
et al. [7] 利用特製的手套追蹤手勢的改變情形，
進 而 設 計 出 一 套 無 接 觸 式 之 電 腦 介 面 
(Non-Contact Computer Interfaces) 供手術室中的
醫生們進行模擬使用。Davis 和 Vaks [8] 提出了
一套以頭部姿勢辨識為基礎的使用者感知介面
技術，透過即時分析，建構出人機之間的互動，
如點頭表示確定、搖頭表示否定等；Silva et al. [9] 
提出了以視覺為基礎的臉部追蹤系統，藉由頭部
移動進而操作螢幕指標的移動，達到人機之間的
溝通。Turk [10] 提出透過感知的互動機制介面，
藉由電腦影像的判斷進行鑑別和描述，使得電腦
的表達能夠更符合人們的需求，應對方式亦更加
自然。 
在立體視覺的運用方面，Lucas和Kanade [11] 
在傳統的影像對位的方法，提出了一套技術，比
傳統方法在速度上增進許多，並且可以將其運用
到立體視覺上。Raphael et al. [12] 運用立體視覺
的概念建置出一種新的圖像理念「v- disparity」，
能夠評估出道路的長度，進而偵測路面上的障礙
物；Matsumoto 和 Zelinsky [13] 提出的一套即時
運算的系統，一開始利用立體視覺在臉部上建置
出三維的座標系後，便能持續且準確的追蹤臉部
及視線所在。 
 
(三)  研究目的 
本論文的研究目的在於發展一套「運用立體
視覺之指尖偵測互動系統」，經由左右兩台不同
之網路攝影機，擷取影像中手勢的部分，利用攝
影機校正 (Camera Calibration) 後取得攝影機參
數，並根據立體影像擷取手部位置及對指尖定位，
能夠較精確的取得深度資訊，以期望能夠進一步
操控電腦，亦或是應用到其他立體視覺方面的應
用。 
 
二、研究方法 
圖 1 為「運用立體視覺之指尖偵測與互動系
統」之流程圖，主要分為兩大部分：(1) 攝影機
校正 (Camera Calibration)；(2) 指尖偵測及定位
模 組  (Fingertip Detection and Localization 
Module)。而指尖偵測及定位模組包括影像前置
處理 (Image Preprocessing)、指尖偵測 (Fingertip 
Detection)、定位及深度計算  (Localization & 
Depth Information) 等。 
 
 
圖 1  運用立體視覺之指尖偵測與互動系統流程
圖。 
 
(一)  攝影機校正 
首先，在執行系統程式之前，為了能夠精確
的從 2D 影像中取得目標物之 3D 資訊，攝影機
校正是必要的步驟。校正的目的，主要是經由校
正的過程，求取攝影機的內部參數  (Intrinsic 
Parameter) 和外部參數 (Extrinsic Parameter)。攝
影機的內部參數可以用來描述影像座標系統
(Image Coordinate System) 與攝影機座標系統
(Camera Coordinate System) 之間的轉換關係。而
攝影機外部參數則可以用來描述攝影機座標系
統與世界座標系統 (World Coordinate System) 
之間的轉換關係，以下就從攝影機校正的原理開
4 
 
P(Xc, Yc, Zc) 為空間中 P點在攝影機座標下的座
標，用齊次座標與矩陣來表示(4)： 
 
         𝑍𝑐 [
𝑥
𝑦
1
] = [
𝑓 0 0 0
0 𝑓 0 0
0 0 1 0
] [
𝑋𝑐
𝑌𝑐
𝑍𝑐
1
]       (4) 
 
將公式 (1) 及公式 (2) 代入公式 (4)，得到從世
界座標系轉換至影像平面座標系之關係： 
 
𝑍𝑐 [
𝑢
𝑣
1
]
= [
1
𝑑𝑥⁄ 0 𝑢0
0 1 𝑑𝑦⁄ 𝑣0
0 0 1
] [
𝑓 0 0 0
0 𝑓 0 0
0 0 1 0
] [
𝑹 𝒕
0𝑡 1
] [
𝑋𝑤
𝑌𝑤
𝑍𝑤
1
] 
= [
𝑓𝑥 0 𝑢0 0
0 𝑓𝑦 𝑣0 0
0 0 1 0
] [
𝑹 𝒕
0𝑡 1
] [
𝑋𝑤
𝑌𝑤
𝑍𝑤
1
] 
= 𝑴𝟏𝑴𝟐𝑿𝑤 = 𝑴𝑿𝒘                      (5) 
 
其中𝑴𝟏只和攝影機內部結構有關，稱之為內部
參數，而𝑴𝟐完全由攝影機相對至世界座標的方
位來決定，稱之為外部參數，M 為 34的矩陣，
稱作投影矩陣，而 Xw 為空間中 P 點在世界座標
系下的齊次座標。若已知攝影機的內外部參數，
就可得到投影矩陣 M。本系統在校正的過程中，
便是希望利用西洋棋盤校正板，在三維空間中，
作任意角度之擺放，然後利用其平面的特性，估
算攝影機內外部參數。 
在本研究中，為了求取攝影機的參數，我們
使用了 Matlab 的攝影機校正工具作為輔助，流
程圖如圖 5，大致上分為左右攝影機校正包含讀
入影像  (Load Image)、參數設定  (Parameter 
Setting)、提取影像角點 (Extract Grid Corner)、
校正 (Calibration) 及儲存結果 (Save Results)；
中間的立體視覺校正包含讀取左右攝影機校正
結果以及進行立體校正。 
 
 
圖 5  攝影機校正流程圖。 
 
為了進行校正的動作，由已經設置好的左右
攝影機，同時拍攝一連串含有西洋棋盤校正板的
影像序列，經 Matlab 的工具程式讀入，設置用
來找尋角點的方格大小，再利用手動點選的方式，
將序列中每張影像的角點標示出來，程式便可以
進行校正，求出攝影機的內外部參數。最後將其
個別儲存起來，利用立體視覺再作一次校正，程
式能夠分析左右攝影機個別參數，經由調整呈現
出各西洋棋盤校正板於空間內的相對位置及調
整過後攝影機的內外部參數。 
 
(二)  指尖偵測及定位模組 
在指尖偵測及定位模組中，其目的是在左右
兩張影像中，偵測指尖座標位置，再利用攝影機
之內部參數與視差 (Disparity) 估算指尖之深度
資訊。包含影像前置處理、指尖偵測、定位及深
度計算等。 
A. 影像前置處理 (Image Preprocessing) 
6 
 
的部分作為特徵點，以便計算深度。在此部分，
透過 k-Curvature 演算法來找出手部輪廓中指尖
的位置，主要概念是利用兩向量的内積，衡量兩
向量之間的夾角尖銳與否，藉此找出指尖座標 
[16]。 
藉由輪廓搜尋結果後，得到一個手勢輪廓，
由一序列的 x與 y座標，分別為 (𝑥1, 𝑦1), (𝑥2, 𝑦2), 
(𝑥3, 𝑦3),…, (𝑥𝑛−1, 𝑦𝑛−1), (𝑥𝑛, 𝑦𝑛) 所組成，而此序
列中的每個點都有可能為指尖，假設其中一點A，
距離 A點 k個座標點為 B與 C，如圖 9所示，則
利用 A點與 B、C 兩點可形成兩個具有方向性之
向量 𝐴𝐵⃗⃗⃗⃗  ⃗ 與 𝐴𝐶⃗⃗⃗⃗  ⃗，透過兩向量之內積，可用來判
斷兩向量之間的夾角，假設為 𝜃，計算方式如公
式 (7)：  
 
            𝜃 = cos−1 (
𝐴𝐵⃗⃗ ⃗⃗  ⃗∙𝐴𝐶⃗⃗⃗⃗  ⃗
‖𝐴𝐵⃗⃗ ⃗⃗  ⃗‖×‖𝐴𝐶⃗⃗⃗⃗  ⃗‖
)         (7) 
 
因此可以利用這個關係，找尋夾角 𝜃 最小的角
度，便可以辨識出為指尖的位置。 
圖 10為利用 k-Curvature 演算法取得之典型
的指尖偵測結果影像。 
 
 
圖 9  手指指尖夾角示意圖。 
 
   
(a)                (b) 
圖 10 影像透過 k-Curvature演算法偵測出指尖之
結果。(a) 左邊影像偵測結果；(b) 右邊影像偵測
結果。 
 
C. 定位及深度計算  (Localization & Depth 
Information ) 
從攝影機校正得知，只從一台攝影機是無法
獲得深度資訊。因此，本研究架設兩台攝影機，
假設當兩台攝影機都已經過校正，且得到其內部
參數，並且以其光軸互相平行的情況下架設，如
圖 11 所示，IL與 IR分別為左右攝影機所拍攝的
投影平面，而 P點在左右攝影機的投影平面上形
成的點分別為 xL 與 xR，其點與光軸的距離分別
為 dxL 和 dxR。利用相似三角形的幾何關係可以
得到公式 (8)，進一步推導成公式 (9)：  
 
               
𝐿−(𝑑𝑥𝐿−𝑑𝑥𝑅)
𝑍−𝑓
=
𝐿
𝑍
            (8) 
               𝑍 =
𝐿×𝑓
𝑑𝑥𝐿−𝑑𝑥𝑅
              (9) 
 
其中 dxL - dxR為左右影像在成像時橫向座標存在
的差異，稱之為視差 (Disparity)，如此一來便可
以求出目標物的深度資訊 Z。 
8 
 
 
(a)  
 
(b)  
圖 14  左右兩攝影機校正後之參數。(a) 左邊攝
影機校正後之參數；(b) 右邊攝影機校正後參
數。 
 
圖 15  經校正後各校正板相對於攝影機位置之
示意圖。 
 
 
圖 16  攝影機經立體校正後所得到的參數結
果。 
 
(三) 手指指尖偵測結果分析 
圖 17 為距離攝影機 20 cm 處之偵測結果，
可以發現由於需要由兩組攝影機作偵測，而 20
公分處的手指指尖部分所形成之視差已接近左
右影像邊緣處，因此可以了解到本系統所能偵測
到的最小距離約為 20 cm。圖 18 為距離攝影機
90 cm 處之偵測結果，研究結果顯示，若手指指
尖部份在超過 90 cm 後，其偵測結果之手勢區域
太小，將導致手指指尖偵測失敗，而無法正確取
得深度資訊。因此可以了解本系統的有效操作範
圍約介於 20 cm 至 90 cm 之間。 
 
   
(a)                 (b) 
圖 17  左右視訊影像指尖於 20 cm偵測結果。(a) 
左邊影像之指尖偵測結果；(b) 右邊影像之指尖
偵測結果 
 
   
(a)                 (b) 
圖 18  左右視訊影像指尖於 90 cm偵測結果。(a) 
左邊影像之指尖偵測結果；(b) 右邊影像之指尖
偵測結果 
 
(四) 深度資訊準確率分析 
本研究分別對 40 cm、50 cm、60 cm、70 cm
及 80 cm 處的指尖偵測結果，計算誤差率，其中
所使用的焦距，分作兩個來比較，一個是單一攝
影機經過西洋棋盤校正板標定角點後進行平面
視覺校正後之焦距，另一個則是左右兩攝影機經
過立體視覺校正後的所得到之焦距，如表一所
式。 
10 
 
Safety,” IEEE Trans. Intelligent Transportation 
Systems, vol. 8, no. 1, pp. 108-120, 2007. 
[6] A. A. Argyros and M. I. A. Lourakis, 
“Vision-Based Interpretation of Hand 
Gestures for Remote Control of a Computer 
Mouse,” Lecture Notes in Computer Science, 
vol. 3979, pp. 40-51, 2006. 
[7] C. Gratzel, T. Fong, S. Grange and C. Baur, 
“A Non-contact Mouse for Surgeon-computer 
Interaction,” Technology and Health Care, vol. 
12, no. 3, 2004. 
[8] J. W. Davis and S. Vaks, “A Perceptual User 
Interface for Recognizing Head Gesture 
Acknowledgements,” Proc. of ACM 
Perceptive User Interface Conf., pp.1-7, 2001. 
[9] G. C. De Silva, M. J. Lyons, S. Kawato and N. 
Tetsutani, “Human Factors Evaluation of a 
Vision-Based Facial Gesture Interface,” 
Computer Vision and Pattern Recognition 
Workshop, vol. 5, 2003. 
[10] M. Turk, “Perceptive Media: Machine 
Perception and Human Computer Interaction,” 
Chinese Journal of Computers, vol. 23, no. 12, 
pp. 1235-1244, 2000. 
[11] B. D. Lucas and T. Kanade, “An Iterative 
Image Registration Technique with an 
Application to Stereo Vision,” Proceedings of 
Imaging Understanding Workshop, pp. 
121-130, 1981. 
[12] R. Labayrade, D. Aubert and J.-P. Tarel, “Real 
Time Obstacle Detection in Stereovision on 
Non Flat Road Geometry Through 
"V-disparity" Representation,” IEEE Trans.on 
Intelligent Vehicle Symposium, vol. 2, pp. 
646-651, 2002. 
[13] Y. Matsumoto and A. Zelinsky, “An Algorithm 
for Real-time Stereo Vision Implementation of 
Head Pose and Gaze Direction Measurement,” 
IEEE Trans. on Automatic Face and Gesture 
Recognition, pp. 499-504, 2000.. 
[14] V. A. Oliveira and A. Conci, “Skin Detection 
using HSV color space,” Sibgrapi, no.12, 2009 
[15] H.-T. Quan, M. Meguro and M. Kaneko, 
“Skin-Color Extraction in Images with 
Complex Background and Varying 
Illumination,” Proceeding on the Sixth IEEE 
Workshop on Application of Computer Vision, 
pp. 280-285, 2002. 
[16] J. Segen, and S. Kumar, “Human-Computer 
Interaction Using Gesture Recognition and 
3D Hand Tracking,” ICIP International 
Conference on Image Processing, pp.188-192, 
vol. 3, 1998. 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：張元翔 計畫編號：99-2221-E-033-053- 
計畫名稱：運用立體視訊為基礎之手勢軌跡互動系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
