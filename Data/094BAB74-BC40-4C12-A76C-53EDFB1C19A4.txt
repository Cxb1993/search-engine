 2. Data sparseness and domain mismatch 
Regarding the issue of data sparseness, Witten-Bell smoothing 
[7] was shown to be effective in n-gram modeling [3]. Using 
this method, n-gram probability )( 1 1
−
+−
i
nii wwp  is smoothed by 
merging with (n-1)-gram )( 1 2
−
+−
i
nii wwp  through the recursive 
linear interpolation 
)()1()()( 1 2
1
1
1
1WB
−
+−
−
+−
−
+− −+= i niini niini nii wwpwwpwwp ηη .   (1) 
The factor nη−1  stands for the frequency with which we 
should use (n-1)-gram to predict the next word. Let )( 1 1⋅− +−i niwN  
denote the number of all possible words iw  adjacent to word 
sequence 1 1
−
+−
i
niw . The number of occurrence of word 
combination of 1 1
−
+−
i
niw  and iw  is denoted by )(
1
1 i
i
ni wwc
−
+− . The 
interpolation factor has a form of 
∑ − +−− +−
−
+−
+⋅
⋅=−
iw i
i
ni
i
ni
i
ni
n wwcwN
wN
)()(
)(
1 1
1
1
1
1
1η .             (2) 
It has the interpretation of giving higher weight for (n-1)-gram 
under the case that more different words iw  are connected 
after word sequence 1 1
−
+−
i
niw . When the possible words iw  
adjacent to 1 1
−
+−
i
niw  are relatively few, it is preferable to rely on 
the contribution of n-gram. We only use (n-1)-gram when there 
are no occurrences for n-gram. 
 For the problem of domain mismatch, the degraded 
performance can be compensated via tracking the newest 
domain statistics at runtime. Here, we apply the cache mixture 
n-gram modeling where the model parameters are estimated in 
online unsupervised manner [5]. The ongoing updated n-gram 
is robust to changing topics in the documents. Applying the 
cache mixture n-gram, we incrementally adapt the model 
parameters sentence by sentence. The probability of word 
sequence },,{ 1 SWWW L=  composed of S sentences is 
characterized by using sentence-level mixture model 
containing m+1 components 
∏ ∏∑∏
= =
−
+−
==
⎥⎦
⎤⎢⎣
⎡== S
s
T
i
i
niik
Gm
k
s
k
S
s
s s wwpWpWp
1 1
1
1
,
11
MX )()()( λ ,      (3) 
where skλ  is the mixture weight estimated from preceding 
sentence 1−sW  under the constraint 1=∑k skλ . And, 
)( 1 1
−
+−
i
niik wwp  denotes the k-th specific n-gram model trained 
from a different category of text documents. There are m 
mixture categories, which could be established either by 
supervised labeling or unsupervised clustering. The general n-
gram model )( 1 1
−
+−
i
niiG wwp  serves as the (m+1)-th mixture 
component, which covers non-topic content appearing in 
observed sentences. When the (s-1)-th sentence 
},,{
11
1
−=− sTs wwW L  is observed, we iteratively calculate the 
new mixture weight skλˆ  by maximizing the likelihood of 1−sW  
given the current estimate skλ , i.e. 
∑−
= − +−=
−
+−
− Σ=
1
1 1
1,,,1
1
1
1 )(
)(1ˆ s
T
i i
niij
s
jGmj
i
niik
s
k
s
s
k
wwp
wwp
T λ
λλ
L
.              (4) 
To realize this algorithm, we prepare m+1 static n-gram models 
in training phase. When test data are gradually observed, the 
mixture weights skλ  are updated and applied to determine the 
likelihood )( sWp  using the adapted language model. With the 
proper parameter skλ , we are able to bring together the cache 
mixture n-gram )(MX Wp , which continuously matches the 
newest topic knowledge in test article. 
 
 
3. Modeling long distance word associations 
Furthermore, the n-gram model is constrained by the 
insufficient modeling of associations longer than n words 
within or across sentences. Usually, the important semantic 
information is embedded in long distance words. It is crucial to 
detect long distance word associations and incorporate their 
information into language models. The trigger pair was chosen 
as the basic element for extracting information from the long 
distance document history [6][8]. In what follows, the trigger 
pair n-gram characterizing long distance language 
dependencies was described. 
 
3.1. Trigger pair n-gram models 
In natural language, if a trigger word iw  is significantly 
associated with a future word jw , the trigger pair ji ww →  is 
produced. The key issues of trigger pair n-gram aim at 
selecting and measuring trigger pairs. In trigger pair selection, 
we restrict the window size of two associated words so as to 
control the number of selected trigger pairs. Also, a simple way 
to measure the significance of the association is to measure the 
average mutual information (AMI) between words iw  and jw  
[6][8] 
)(
)(
log),(
)(
)(
log),(
)(
)(
log),(
)(
)(
log),(
j
ij
ji
j
ij
ji
j
ij
ji
j
ij
ji
wp
wwp
wwp
wp
wwp
wwp
wp
wwp
wwp
wp
wwp
wwp
++
+
,     (5) 
where ),( ji wwp  is the probability of occurring iw  but 
without jw  afterward in the window. AMI measures the 
information provided by iw  on jw . A word pair is recognized 
as a trigger pair when its AMI is high. The set of trigger pairs 
}{TR ji ww →=Ω  can be selected from training corpus. 
 Assume there is a trigger pair ji ww →  observed in 
word sequence },,,,,,{ 1 Tji wwwwW LLL= , the conditional 
probability )( ij wwp  should be considered in calculating the 
logarithmic probability using unigram models 
)(
)(
log)}()()()(log{
)}()()()(log{)(log    
1
1TR
j
ij
Tji
Tiji
wp
wwp
wpwpwpwp
wpwwpwpwpWp
+=
=
LLL
LLL
. (6) 
In (6), the second term represents the mutual information (MI) 
)()(
),(
log
)(
)(
log)(MI
ji
ji
j
ij
ji wpwp
wwp
wp
wwp
ww ==→ ,      (7) 
which reflects the degree of the preference for associations of 
iw  and jw . If the events of occurring iw  and jw  are 
independent, then 0)(MI =→ ji ww . In practice, there exist 
several trigger pairs in word sequence W . We may express the 
trigger pair based observation probability as 
  Different from data mining algorithm [1], we use the 
information-theoretic AMI for selection of association patterns 
and the mutual information for measurement of word 
associations in language modeling. The mining algorithm is 
exploited to find the association patterns ASΩ  consisted of 
different numbers of associated words },,,{
up32 aLLL L . The 
proposed association pattern n-gram is a general framework 
where the mutual information between frequent a-1 word 
sequence iaW 1−  and associated word jw  is properly merged. In 
case of 2up =a , we build the relationship between trigger 
word iw  and associated word jw . The association patterns 
become the frequent word pairs }{ ji ww →  which is also 
called the trigger pairs, i.e. 2TRAS L=Ω=Ω . Accordingly, the 
trigger pair n-gram is referred as a special realization of 
association-pattern n-gram in case of 2up =a . 
 
1 2 3 4 5 6 7
185
190
195
200
205
210
Maximal Association Step
P
er
pl
ex
ity
cache mixture n-gram
Witten-Bell smooth n-gram
association pattern n-gram
 
Figure 2: Comparison of different individual methods. 
 
120
140
160
180
200
220
240
260
Different Methods
P
er
pl
ex
ity
baseline n-gram
Witten-Bell
Witten-Bell+cache mixture
Witten-Bell+trigger pair
Witten-Bell+cache mixture+trigger pair
Witten-Bell+association pattern
Witten-Bell+cache mixture+association pattern
 
Figure 3: Comparison of different combined methods. 
 
 
4. Experiments 
4.1 Experimental setup 
To examine the performance of association pattern n-gram, we 
conduct a series of experiments and report the perplexities and 
speech recognition rates. Several databases were used. 
Dictionary was setup from the “Sinica corpus” with the size of 
five million Chinese words. We gathered 32941 frequent words 
to form the lexicon. Each word contained at most four Chinese 
characters. The articles in Sinica corpus were collected from 
different domains by the Institute of Information Science in 
Academia Sinica, Taiwan. This open source corpus was 
representative for Chinese language. We used this corpus as the 
training data to estimate n-gram models. Only bigram model 
was considered. In addition, we collected 3118 Chinese news 
documents covering eight categories: Technology, Society, 
Travel, World, Sports, Entertainment, Politics and Business. 
These classified documents were sampled from the news 
websites: CNA (http://www.cna.com.tw), ChinaTimes 
(http://news.chinatimes.com) and UDNnews 
(http://www.udnnews.com.tw), etc in Taiwan, during the 
period between April 10 and April 16 in 2001. We used Sinica 
corpus and 2234 news documents (from April 10 to 14) for 
training and the remaining 884 news documents (April 15 and 
16) for testing. Also, the experimental setup of speech 
recognition has been mentioned in [4]. Without loss of 
generality, the estimated language models were translated into 
syllable language models to perform syllable decoding of 
continuous speech. We reported the syllable recognition rates 
(%). The benchmark MAT-160 speech database was used to 
train speaker-independent HMM’s. The test set (Test500) was 
recorded via telephones and consisted of 500 sentences from 30 
outside speakers. It totally included 4754 syllables. 
 
 
4.2 Experimental results 
When the association pattern n-gram is implemented, we 
specify the maximal association step and apply the association 
pattern mining algorithm to determine all patterns covering 
different association steps. The association patterns are 
recursively extracted from single association step to maximal 
association step. In Figure 2, we compare the perplexities of 
cache mixture n-gram, Witten-Bell smooth n-gram and 
association pattern n-gram under different maximal association 
steps. Trigger pair n-gram is a special case of association 
pattern n-gram with maximal association step being two. This 
figure indicates that the association pattern n-gram is better 
than cache mixture n-gram and Witten-Bell n-gram when 
involving more than two association steps. In case of four 
association steps, the lowest perplexity 188.8 is attained. In the 
subsequent experiments, we only report the association pattern 
n-gram with maximal association step being four. Basically, 
these three methods are developed to resolve different 
problems. These methods can be combined to improve 
language modeling performance. In Figure 3, we investigate 
the perplexities of different combined approaches. Among all 
combinations, the lowest perplexity 135.8 is achieved when 
Witten-Bell smoothing, cache mixture n-gram and association 
pattern n-gram are simultaneously performed. Also, the 
proposed language models are applied for continuous Mandarin 
speech recognition. Language models are merged to speech 
recognition system in syllable level. The experiments show that 
syllable recognition rates are increased from 51.3% without 
language model to 55.8% with Witten-Bell smooth n-gram. If 
trigger pair n-gram and association pattern n-gram are fulfilled, 
syllable recognition rates are improved to 56.7% and 57%, 
respectively. These two results are comparable because the 
lengths of test utterances are not too long. 
 
 
5. Conclusion 
This project have surveyed three essential problems in 
statistical n-gram and presented the hybrid approaches to 
achieve sophisticated language modeling. The techniques of 
Witten-Bell smoothing, cache mixture n-gram and trigger pair 
n-gram were introduced to cope with the problems of data 
sparseness, domain mismatch and long distance dependency, 
respectively. To relax the constraints of trigger pair n-gram 
