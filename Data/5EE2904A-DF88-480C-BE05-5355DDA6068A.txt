behavior analysis (recognition). Recognizing human 
actions in videos is important for computer vision 
applications, such as video surveillance, video 
content analysis, and human computer interactions. 
For the first year, based on the human visual 
attention model, image segmentation using particle 
swarm optimization (PSO) and a hybrid evaluation 
function is proposed. The proposed image segmentation 
approach contains four stages, namely, color 
quantization, feature extraction, small region 
elimination, and region merging using a modified PSO 
algorithm. 
For the second year, an effective 
foreground/background segmentation approach for 
bootstrapping video sequences is proposed. First, a 
modified block representation approach is used to 
classify each block of the current video frame into 
one of the four categories, 
namely, ＇background,＇ ＇still 
object,＇ ＇illumination change,＇ and ＇moving 
object.＇ Then, a new background updating scheme is 
developed, in which the side-match measure is used to 
determine whether the background exposes. Finally, an 
improved noise removal and shadow suppression 
procedure using the edge information is used to 
enhance the final segmented foreground. 
For the third year, a human action recognition 
approach using hierarchical classification and 
spatiotemporal features is proposed. In the proposed 
approach, both spatial and temporal action features, 
including the binary motion history images (MHI), the 
form features extracted by the log-Gabor filter, and 
the motion features extracted by optical flow, are 
used to represent and recognize human actions. 
Furthermore, a hierarchical classification structure 
is employed. Based on the experimental results on 
Weizmann and KTH datasets, the average accuracy of 
the proposed approach is better than those of 
comparison approaches. 
 
英文關鍵詞： color image segmentation, visual attention model, 
 1 
行政院國家科學委員會專題研究計畫成果報告 
智慧型視覺照護與監控-子計畫二：影像/視訊分割及人類動作分析 
計畫編號：NSC 98-2221-E-194-034-MY3 
執行期限：98 年 8 月 1 日至 101 年 7 月 31 日 
主持人：柳金章   執行機構及單位名稱: 國立中正大學資訊工程學系 
計畫參與人員：蕭涵徽，林承賢，蔡憲德，林建志，趙子驊，吳季陵，劉忠亭，
游文俞，陳雅蓉，陳世彥，羅明陽，吳宜臻 
 
一、中文摘要 
 
以視覺為基礎照護及監控是一個很熱門
的研究題材，本整合型研究計畫的目標是建
立一個智慧型視覺照護與監控系統。本子計
畫的主要目標是發展智慧型視覺照護與監控
中的兩項核心技術：影像/視訊分割及人類動
作分析(辨識)。在視訊方面的人類動作分析(辨
識)是電腦視覺應用的重要系統，例如：視訊
監控、視訊內容分析、及人類與電腦的互動
行為。 
在本研究計畫第一年中，我們採用人類
視覺注意模型(human visual attention model)
為輔助，提出一種以混合評估函數 (fitness 
function)及應用粒子群最佳化(particle swarm 
optimization, PSO)的影像分割方法。結合色彩
量化、特徵擷取、小區塊去除、及以粒子群
最佳化做區域合併等四個步驟，產生符合語
意的影像分割。 
在本研究計畫第二年中，我們提出一個
有效率的前景/背景分割方法，用於初期不提
供純背景影像的視訊。首先，將每張 frame 中
的每個 block 區分為四個類別，分別是
“background” 、 “still object 、 “illumination 
change”、及“moving object”。接著，我們提出
一個新的背景更新方法，透過邊緣比對測量
(side-match measure)決定真正的背景是否已
顯露出來。最後，我們採用邊緣資訊(edge 
information)為輔助，以協助去除雜訊及陰
影，並加強前景分割的結果。 
在本研究計畫第三年中，我們提出一個
使用階層式的分類法及空間-時間特徵的人類
動作分析(辨識)的方法。在此方法中，同時使
用空間與時間上的行為特徵於描述及辨識人
類動作，其中包含二元的 motion history 
images (MHI)、利用 log-Gabor filter 取得的外
型特徵、及利用 optical flow 取得的動作特
徵。接著，使用一個階層式的分類法結構。
基於 Weizmann 和 KTH datasets 的實驗結果，
我們所提出的方法的正確率比其他比較方法
較為優異。 
 
關鍵詞：彩色影像分割、人類視覺注意模型、
粒子群最佳化、混合評估函數、前景/背景分
割、邊緣比對測量、區塊表示法、雜訊移除、
陰影抑制、人類動作辨識、階層式分類、空
間-時間特徵、光流法。 
 
Abstract 
 
Vision-based mobility-aid and surveillance 
is a popular research issue. The goal of this joint 
project is to construct a vision-based intelligent 
mobility-aid and surveillance system. The major 
goal of this sub-project is to develop two major 
technologies: image/video segmentation and 
human behavior analysis (recognition). 
Recognizing human actions in videos is 
important for computer vision applications, such 
as video surveillance, video content analysis, 
and human computer interactions. 
For the first year, based on the human 
visual attention model, image segmentation 
using particle swarm optimization (PSO) and a 
hybrid evaluation function is proposed. The 
proposed image segmentation approach contains 
four stages, namely, color quantization, feature 
extraction, small region elimination, and region 
merging using a modified PSO algorithm. 
For the second year, an effective 
foreground/background segmentation approach 
for bootstrapping video sequences is proposed. 
First, a modified block representation approach 
 3 
改進的方法，透過減少及合併這些零碎同質
性區域，得到較好的影像分割結果 [12, 17]。
另外，在 spatial segmentation 技術中，有些方
法[12, 18]擷取對於人類視覺較為強烈的 edge 
資訊，由於 edge 資訊可用來連接物体外形輪
廓(contour)，所以影像分割結果比較符合人類
視覺感受。 
Graph-based 方 法 可 視 為 hybrid 
approach，主要結合各種 features 和 spatial 
information 做進一步的 perceptual grouping 和
organization [19-20]。Perceptural grouping 處理
乃基於一些重要的準則，諸如相似性
(similarity)、近似性 (proximity)、及連續性
(continuation) [21]。當一張 image 被轉化成
graph 型式時，一張 image 會被細分成多個
elements 再藉由計算 cost function 取得最佳
值。有多位學者提出以 graph 為基底的彩色影
像分割[19-20, 22-23]，其中 Shi and Malik [20]
提出一種 normalized cut方法，由於 normalized 
cut 方法計算量較大，因此不適用於即時
(real-time)的應用。Tao et al. [4]則結合 mean 
shift 與 normalized cut 兩種技術，並減低計
算量以便成為 real-time 影像分割方法。 
Image segmentation based on evaluation 
function optimization 則藉由搜尋 search 
space，利用其 evaluation function 以衡量影像
分割結果，以找出 global optima [24-25]，通
常其 evaluation function結合使用各種 features 
或 spatial information 等相關重要資訊。在演
化式計算中較著名的基因演算法(GA)則藉自
然篩選機制 (nature selection) 及一些 GA 
operators (selection、crossover、mutation)，在
search space 中以 parallel 的方式做 seaerch。
GA 不僅可減少搜尋時間，且可避免陷入區域
最佳解(local optimal)。有多位學者[13, 24-25]
使用不同演化式演算法於 clustering algorithm
達成 image segmentation，如 clustering methods
結 合 evolutionary strategies 、 evolutionary 
programming、及 variable string-length genetic 
algorithm [13] 。 另 外 ， particle swarm 
optimization (PSO)則利用在每個世代的個體
(individual)間的合作和競爭演變以找尋最佳
解，合作及競爭演變可取代基因演算法中的
核心 GA operators [26]。在 PSO 中，每一個個
體(particle)會依據自已及鄰近個體的經驗來
調整飛行的方向。在 solution space 中，每一
個 particle 都被視為一個可能解。PSO 對於具
有 nonlinear、non-differentiable、及 multi-model
的問題具有 robust 及快速找出最佳解的能力。 
在視訊分割技術方面，許多的研究在探討
如何作視訊之前景/背景分割，其中較簡單的
做法是偵測視訊中是否有移動物體，相關的
幾類方法包含 temporal differencing [27-29]、
background subtraction [30-37]、 optic flow 
[38-40]、statistical modeling [41-45]等。 
Temporal differencing 方法是將時間維度
上相鄰的兩張影像相減，若所得的像素差值
小於設定的 threshold，判定為背景部份；反
之，則判定為移動物體(前景)。此方法的偵測
敏感性很高，對於環境的光線變化適應性良
好，但是易受背景物體干擾，例如隨風搖曳
的樹枝、噴水池、電腦螢幕的閃爍現象等。
相鄰兩張影像做相減，移動物體常常因為移
動前後的位置在時間維度上有重疊性，而發
生偵測出的移動物體形狀不完整或內部有破
碎的情形。Hu et al. [29]進一步提出使用
three-frame differencing 的方法，再利用
mixture model of Gaussians (GMM)以提高偵
測移動物體的準確度。 
Background subtraction方法常用於偵測有
無移動物體，其先利用一段時間的背景畫面
建立背景模型，再將每一張新的影像與此背
景模型相減，藉此偵測出移動物體。此方法
不會有物體內部破碎的問題，但當環境有大
幅度光線變化時，新的影像與背景模型會有
很大的差異，往往不能精確的分割出前景。
若能適時更新背景模型，則可以改善這個問
題。許多學者基於 background subtraction 發展
其他背景模型的方法，例如 Piccardi [30]整理
各種 background subtraction 技術，如 running 
Gaussian average [31-32]、 temporal median 
filter [33]、mixture of Gaussians (MoG) [41]、
kernel density estimation [37]等，並比較它們在
速度、記憶體需求、及準確性等方面的效能。 
對於靜態攝影機而言，實現 background 
subtraction 常以 pixel-based 的方式處理。Wren 
et al. [31]將畫面上每個 pixel 視為各自獨立的
個 體，使用 Gaussian probability density 
function，並參考最近一段時間的畫面，運算
出移動物體。Koller et al. [32]提出 selective 
background updating 的作法，依照正在處理的
pixel 是前景或背景，決定 binary mask 的值及
 5 
續兩張 video frames的 displacement vectors 來
偵測人類的動作 [55]。本研究計畫第二年，
即是提出的一個有效率的前景/背景切割方法
(foreground/background segmentation)，用於初
期不提供純背景影像的視訊 [56]。 
在 human motion detection 之後，將從僅含
人體 (運動物體 )的視訊資料中擷取一些
spatiotemporal features。一般來說，人體模型
可分為四類[57]：stick figure、2D contour、
volumetric model、及 hierarchical model。stick 
figure 表示法中，人體是由軀幹，頭部，四肢
所構成。Yoo et al. [58]所提出一個人體 feature
擷取方法，以二維 stick figures 建立人體模
型。Gritai 和 Shah [59]則提出另一種新的方
法，藉由收集 human body points (頭、頸、肘、
肩、手、腰、膝、足)和人體連接的關節來分
析人類行為。對於 2D contour 方法，Haritaoglu 
et al. [60]分析人體的輪廓邊界，找出人體輪廓
的端點，這些端點可做為人體的 features。Jin
和 Mokhtarian [61]提出了一種基於 statistical 
shape analysis 的多視角人體動作辨識方法，從
原始 frames 中擷取出人體的輪廓，在人體動
作辨識中這些輪廓被視為抽象的 features。針
對同時取得空間與時間上的特徵來說，Kuno 
et al. [62] 取得人類輪廓的形狀參數，而 Bird 
et al. [63] 則基於統計學的資料來捕捉人類的
高/寬比例。Gorelick et al. [64] 從視訊序列取
得 space-time shapes 來描述人類行為。Hsiao et 
al. [65] 基於 space-time shapes 來獲得 
temporal-state shape contexts 。 Shen and 
Foroosh [66] 從 point triplets 與 Junejo et al. 
[67]所提出的 hand trajectories 來取得人類行
為特徵。 
人類行為分析涉及到在視訊中辨識人類
動作和人類動作的 high-level description。現有
的人類行為分析方法大致可分為四類，即
dynamic time warping (DTW)， finite state 
machine (FSM) ， hidden Markov model 
(HMM)，及 support vector machine (SVM)。 
DTW 利用最小距離為基礎以 dynamic 
programming 找到 the best mapping。DTW 較
早應用於語音辨識，目前也有一些應用在視
訊資訊的辨識上。對視訊 matching，DTW 可
調適應其速度，並可擷取不同的 features。
Cuntoor et al. [68]分析使用不同人類的
features，並以 DTW 作人類行為辨識。Bobick
和 Wilson [69]提出了一個 state-based 方法作
人類姿勢描述和辨識。DTW 可應於視訊
matching，並利用 sequence state determination
作人類姿勢辨識。 
FSM 是一個 state-transition function，包含
有限個數的 states，可用來作參考視訊和測試
視訊 matching。FSM 可使用在許多應用領域，
例如人類行為分析和軟體工程。Bhuyan et al. 
[70]開發了一種基於 FSM 的人類姿勢描述和
辨識方法。Wilson et al. [71]提出了一個類似
於 FSM 的結構來分析人類的姿勢。 
HMM 是一種隨機狀態模型，包含許多隱
藏和可觀察參數。對於 HMM，輸出的 symbol 
sequence 是可以觀察到的，而系統的狀態是無
法直接觀察到的。HMMs 適用於許多應用，
例如語音辨識，姿態辨識，與人類行為辨識
[72-77]等。Niu 和 Abdel-Mottaleb [72]提出了
一種基於 HMM 的人類行為辨識方法。Guo
和 Miao [73]提出了一個家庭看護監控系統，
其中 HMMs 被用來建立人体運動模型，並作
人類姿勢辨識。Ahmad 及 Lee [74]使用多維度
的離散型 HMMs 用來建立模型和辨識人類行
為。Coupled hidden Markov models（CHMMs）
乃利用兩個或兩個以上的傳統 HMMs，可以
有效的來辨識複雜的行為。由於 CHMMs 可
以描述數個 HMMs 之間的關係，CHMMs 的
功能要比單一HMM來得強大。但增加HMMs
的數量，其複雜度也會相對的提高。Brand et al. 
[75]利用兩個 HMMs 代表左右手的動作，並
透過此 CHMMs 成功的辨識雙手的動作。 
SVM 是一種 supervised learning 及分類
法，可以視為一個高維度 feature space 和 input 
space 的線性函數。input space 映射到一個高
維空間以建構 hyperplane，並使不同的類別有
最大的分離度。SVM 可將分類的錯誤降到最
低，並將 geometric margin 提升到最大。SVM
已應用在許多領域，例如文字分類，臉部辨
識，和視覺圖形辨識等。Schuldt et al. [78]提
出了一個基於 local space-time video features
的方法，用來辨識複雜的人類行為，透過 local 
space-time video features 建立視訊的表示法，
並結合 SVM 作人類行為的辨識。Meng et al. 
[79]提出了一種基於 SVM 的人類行為辨識系
統，其中使用多項 features 和多類別 SVM。 
 
 
 7 






,                              ,0
,       ,/1 ,
,
mk,l
mk,lmlk
lk
Thdist
Thdistdistdist
m  (7) 
其中 lkdist , 是 kDC 與 lDC 的差異度值， mTh 與
mdist 為門檻值。 
在 local binary pattern (LBP)方面，其最主
要的目地為針對影像進行紋理分析 (texture 
analysis) [80]，主要是利用 texture unit 對整張
影像作計算，而 texture unit 則是以 3×3 為單
位，針對該 unit 內中心點的灰階值與鄰近 8
個灰階值做計算，如 Fig. 3(a)為影像中的一個
區域，Fig. 3(b)則為該區域以中心點的灰階值
為門檻值，其鄰近 8 個灰階值大於等於門檻
值則為 1，反之為 0，Fig. 3(c)為相對權重值，
Fig. 3(d)則是 Fig. 3(b)-(c)相乘後結果，最後將
Fig. 3(d)加總起來示為該 texture unit 的值，而
整張影像或區域經由 texture unit 進行計算後
的結果稱為 texture spectrum。在本計畫中，
LBP 的值將在顏色量化後的影像上計算，接
著在每塊區域都會利用 histogram 來統計各
LBP值以作為 texture feature。Fig. 4為 Fig. 2(a)
經由紋理分析後的 texture spectrum實驗結果。 
採用 color feature vector 與 local binary 
pattern 兩項 low-level featurse 是無法將一張
image 分割成具有影像語意的結果。因此我們
試圖利用以 saliency 為主的人類注意模型
(saliency-based visual attention model)所輸出
的 saliency map 作為 high-level features。在人
類注意模型(visual attention model)領域中，已
有許多學者提出不同的人類注意模型，一個
常用的人類注意模型結構[81]如 Fig. 5 所示。
一 張 影 像 的 saliency map 是 將 多 張
conspicuous maps 線性組合而成，且可用來找
到易讓人注意或顯著區域(salient region)。然
而，對於每一個 feature 都會用一組 linear 
center-surround operations 來計算，輸入影像可
以縮放至 9 種不同大小(labels 0-8)作為 image 
pyramid，接著利用 center-surround operations
來計算 fine 與 coarse scales 間的差異度。其
center 主要表示為影像灰階值在 scale 
}4,3,2{Fine ，而 surround 主要為相對灰階
值在 scale  FineCoarse ， }4,3{ 。在人
類注意模型中，saliency map 可以由計算 3 個
feature maps： color map、 intensity map、
orientation map得到。在 intensity image ( I ) 的
計算為 3/)( BGRI  ，R、G、B各為輸
入影像中的三原色，紅色(Red)、綠色(Green)
與藍色(Blue)。另外 4 個 broadly-tuned color 
channels ( R 、 G 、 B 、 Y  ) 的計算為
2/)( BGRR  ， 2/)( BRGG  ，
2/)( BRBB  ， ||2/)( GRGRY   
B2/ ，最後產生 intensity ( I ) 與 color ( R、
G、B、Y  )共五個 image pyramids。接著計
算這些 image pyramids 的 finer scale ( Fine )與
surround scale ( Coarse )間的 center-surround 
differences，其 intensity map ( IM )及 color map 
( RGM 、BYM )計算如下： 
,)()(),( CoarseIFineICoarseFineIM   (8) 
,))(')('(
))(')('(
),(
CoarseRCoarseG
FineGFineR
CoarseFineRGM

  (9) 
.))(')('(
))(')('(
),(
CoarseBCoarseY
FineYFineB
CoarseFineBYM

  (10) 
在 local orientation information 方面，則是
採用 Gabor pyramids ),( O ，其包含 9 種不
同 scales ( = 0, 1, 2, …, 8)及 4 個不同方向
(orientation) 資 訊 }135,90,45,0{  。
Orientation feature map 主要計算 center 與
surround scales 的 local orientation contrasts，
計算如下： 
.),(),(
),,(


CoarseOFineO
CoarseFineOM

 (11) 
最後將得到 42 個(6 個 intensity，12 個 color，
24 個 orientation) feature maps。 
Saliency map 主要是將數個 conspicuity 
maps 線性組合而得到，其中 conspicuity maps
又是源自於 42 張 feature maps，所以最後的
feature maps 將結合成 3 張 conspicuity maps，
分別為 intensity ( IM )，color (CM )，orientation 
(OM )，當進行線性組合計算時會先正規化(以
N 來表示)，conspicuity maps 的計算如下: 
)),,((
4
3
4
2
CoarseFineIMNIM
Fine
FineCoarseFine



  (12) 
))],,((
)),(([
4
3
4
2
CoarseFineBYMN
CoarseFineRGMNCM
Fine
FineCoarseFine

 

  (13) 
 9 
其中 ic1 及 fc1 代表 1c 的初始值與終止值， ic2 及
fc2 代表 2c 的初始值與終止值。這樣的設計可
防止 PSO 在求解初期過早收斂且在求解末期
可加速收斂。 
最後的精細影像分割(fine segmentation)
利用 modifed PSO 演算法來達成。在處理過程
中，我們將使用 high-level features：saliency 
information 來檢查區域合併的結果，主要是為
了擷取人眼注意的區域或顯著區域 (salient 
regions)。但如果顯著區域與不顯著區域合併
時 ， 則 particle 是 不 參 加 評 估 計 算
(evaluation) ， 如 此 不 僅 可 保 留 saliency 
information 且可幫助 particle 找到最佳的合併
門檻值。步驟如下： 
Step 1. 首先設定初始particles撒在 2維影
像的 search space 上。 
Step 2. 更新每一個 particle 的 velocity 與
position。 
Step 3. 計算顯著區域差異度 (saliency 
difference, SD)。若 saliency image 與合併後的
結果的區域差異度大於門檻值，則除去這次
由 PSO 所搜尋的解。顯著區域差異度定義如
下： 
 






 

P
p
ppp TDCDAD
P
SD
1 3
11
, (21) 
其中 P 為顯著區域(saleint region)的個數，
pAD 、 pCD 、 pTD 分別為第 p 個區域的正規
化區域(normalized area)、color 及 texture 在
saliency image 與相對區域的合併後的區域之
差異度。 
Step 4. 每個 particle 代入評估函數以計算
適應函數值。結合效度測量指標(Eq. (16))及顯
著區域差異度(Eq. (21))，評估函數定義如下： 
VIsSDfitness  21  , (22) 
其中 1 與 2 為權重係數。 SVI 計算如下： 
 TCS VIVIVI 
2
1
, (23) 
其中 CVI 和 TVI 則分別是 color、texture 的效度
測量值。 
Step 5. 重複 Steps (1)-(4)，直到處理程序
到達預設的演化次數。 
 
 
(B) 第二年 
本研究計畫第二年將探討使用 block 
representation approach 配 合 background 
updating scheme，並結合 noise removal and 
shadow suppression procedure 來實現用於
bootstrapping video sequences 之視訊前景/背
景分割技術。 
本計畫所提出的視訊影像分割方法包含
兩個部份，(block-wise) background modeling 
及(pixel-wise) foreground extraction，如 Fig. 7
所示。輸入的部份包含 current (gray-level) 
frame tI  和 previous (gray-level) frame 1tI ，
輸出的部分則是 modeled background frame 
tB  和 segmented foreground frame tF ， t  為
frame number (index)。 t yxI ),( 、
1
),(
t
yxI 、
t
yxB ),( 、
和 t yxF ),( 分別是
tI 、 1tI 、 tB 、和 tF 影像中的
pixels ),( yx 。Frame size 為 HW   pixels。每
張 frame 分為數個 NN  大小的 blocks。令
block index 為 ),( ji ， 1)/(,...,2,1,0  NWi ，
1)/(,...,2,1,0  NHj 。 所 以 t ji ),(b  
}1,...,2,1,0,:{ ),(  NbaI
t
bjNaiN 、 
1
),(
t
jib  
}1,...,2,1,0,:{ 1 ),( 

 NbaI
t
bjNaiN 、 和 
t
ji ),(
~
b  
}1,...,2,1,0,:{ ),(  NbaB
t
bjNaiN 分別是
tI 、
1tI 、和 tB 影像中的 blocks ),( ji 。 
Block representation approach 將 current 
frame 中每個 block 區分為四種類別，如 Fig. 8
所示，分別是“background”、“still object、
“illumination change”、及“moving object”。Fig. 
9 為 block representation approach 的流程圖，
包含 motion vector estimation 和 correlation 
coefficient computation 兩部分。首先在 the 
current frame 與 the previous frame 之間執行
motion vector estimation，其 block matching 的
方式採 sum of absolute differences (SAD)，
search range 為 2/N 。如果 motion vector 
),( vu 有最小的 SAD， ),( vumvD ，並且小於 90%
的 null-vector )0,0( 的 SAD， )0,0(mvD ，則此
block 記為“moving” block，否則為“static” 
block [47]。另一方面，在 block t ji ),(b 與 block 
1
),(
~ t
jib 之間計算 correlation coefficient ),( jiCB   
 11 
fragmented parts (noises) 和 shadows。 
為了得到更精準的 foreground frame 
tF ，我們提出一個結合 shadow suppression 
procedure approach [86] 與 edge information的
一 種 改 良 式 noise removal and shadow 
suppression procedure。令 tSFˆ 為 the current 
frame 於 HSV 色彩空間中的 saturation 
component， tEFˆ 為
tI 的 gradient image。這裡取
得 gradient image 的方法是採用 Sobel 
operator，以 tFˆ 做為 binary operation mask。因
此，segmented foreground frame tF 可以定義
為 








                                  otherwise,   ,0
),),(ˆ(
))),(ˆ(),(ˆ( if    ,1
),(
ˆ
t
E
t
E
F
t
S
t
TyxF
yxFyxF
yxF
t
S

 (31) 
其中和分別是 logical AND operator
和 logical OR operator， t
SFˆ
 是 tSFˆ 的 standard 
deviation， ET  是 threshold。根據實驗經驗，
我們將 isfT 設為 0.05， ET 設為 0.45。Fig. 14 是
一個 noise removal and shadow suppression 的
例子。 
 
(C) 第三年 
本研究計畫第三年探討使用階層式的分
類法及空間-時間特徵的人類動作分析(辨識)
的方法。在此方法中，提出的人類動作分析(辨
識)方法包含三個步驟，分別是 motion type 
analysis (運動類型分析)、feature extraction (特
徵擷取)、及 action recognition (動作辨識)，如
Fig. 15 所示。首先，根據本研究計畫第二年
所 提 出 的 前 景 / 背 景 切 割 方 法
(foreground/background segmentation)，偵測每
張 frame 裡的運動物體(即 human body、
foreground)，再由兩張連續的 video frames 得
到 human body移動的位移。接著，累加 human 
body 移動的位移，可以進一步將人類動作區
分成 low-motion 及 high-motion 兩類。同時擷
取空間與時間上的動作特徵用於描述人類動
作，其中包含二元的 motion history images 
(MHI)、利用 log-Gabor filter 取得的外型特
徵、及利用 optical flow 取得的動作特徵。最
後，使用階層式的分類結構來辨識人類動作。 
 
(1) Motion type analysis (運動類型分析) 
將 human action dataset中的 k個不同動作
分類成 q 個 low-motion actions 和 p 個
high-motion actions (p+q=k)。如 Fig. 16(a)所
示 ， 在 每 張 video frame 中 利 用
foreground/background segmentation 取 出
“binary” human body (foreground)以及剩餘的
部分(background)。令 actiontype 為動作的運動
類型，其中 },{ highlowtype 表示此動作為
low-motion action 或 high-motion action，判別
的方式如下， 


 

otherwise,    ,
  , if   ,
low
high
type
action
action
action

 (32) 
其中 lengthixxi ,...,3,2,1  |,|max 1  ，xi為
在 frame i 中 human body的中心點的 x 座標，
length 為 action video clip 的 frame 個數，為
threshold 值 ( 20 )。 
 
(2) Feature extraction (特徵擷取) 
在本研究計畫中，low-motion action 及
high-motion action 會以不同的方法擷取其
spatiotemporal features (如 Fig. 17 所示)。 
對於 high-motion action 來說 (如 Fig. 
17(a))，首先根據本研究計畫第二年所提出的
前景 /背景切割方法 (foreground/background 
segmentation)，由單一動作中連續的 video 
frames 得到“binary” human body (foreground) 
(如 Fig. 16(a))，再透過 merging 這些“binary” 
human bodies 得到一張 binary motion history 
image (MHI) (如 Fig. 16(b))。在 Fig. 16(b)中，
“white” pixels 記錄著 human body在“running”
動作中曾經移動的位置及過程。 
接著，統計各張 MHI 中“white” pixels 的
個數。以 actor j 所作的 high-motion action i 而
言，對應的 MHI中“white” pixels 的統計數值
為 ijNum  (i=1,2,…,p, j=1,2,…,n)。將同一 actor 
j 所作的 p 個 high-motion actions 當作一個
set，並算出 ,1jNum ,
2
jNum …
p
jNum 之標準差
(standard deviation) j 。因此，dataset 中有 n
個 actors，則有 n 個標準差 n ,...,, 21 。 j 也
代表 actor j 作不同的 high-motion actions 時的
速率分布情形。 
透過 jMHIT 決定這 p 個 high-motion actions
會 以 哪 種 方 式 作 feature extraction 及
recognition，如 Fig. 17(a)。 jMHIT 的計算方式如
 13 
法，dynamic clustering using PSO (DCPSO) [25]
以及 mean shift approach [7]。評估影像分割的
performance 使用 probabilistic rand index (PRI) 
[90]，將影像分割後的結果與 ground truth 
images作相似度計算，PRI值介於 0到 1之間。 
Figs. 19-23 為 Berkeley segmentation 
benchmark其中的五張 original color images以
及相對應的 ground truth images。而 Fig. 24-28
為使用 DCPSO、mean shift approach、和
proposed approach 的影像分割結果，各自的
PRI 值也列於其中。Table 1 則列出各方法對
於這 100 張測試影像的平均 PRI值。 
根據以上實驗結果顯示，DCPSO 的分割
結果通常包含較多零碎不完全的 region，雖然
mean shift approach 的分割結果比 DCPSO
好，但是仍然缺少 image semantics。而本計畫
所提出的方法，因為同時顧及 low-level 和
high-level features，所以分割的結果能保持影
像中 salient 的部份，兼具 image semantics，得
到較佳的視覺效果。在 PRI 值的表現上，本
計畫提出的方法之 PRI 值高於其他兩個比較
方法。 
 
(B) 第二年實驗結果 
本計畫所採用的測試視訊是選自於
ATON、CAVIAR、及 PETS2006 benchmark 
datasets。本報告顯示其中兩段 bootstrapping 
video sequences ， “ Highway-1” 與
“Highway-2”(皆為 15fps)的實驗結果。視訊
中每張 frame 大小為 320×240，運算的 block
大小為 16×16。本計畫實作了兩個影像分割的
比 較方法， Reddy background estimation 
(Reddy) [48] 及 self-organizing background 
subtraction (SOBS) [91]。在 Reddy方法中，只
使用 gray-level component；在 SOBS 方法中，
使用 H、S、V components。此實驗結果中，
Eq. (26)的 threshold stillT 設為 10。 
Figs. 29-30 為 “ Highway-1” 及
“Highway -2”的 frames 1、20、40、60、80、
及 100 時(a)，分別由 Reddy (b)、SOBS (c)、
及本計劃提出的方法(d)而得到的相對應的
modeled background frames 
1B 、
20B 、
40B 、
60B 、
80B 、及
100B 。 Figs. 31-33 則是以
“Highway-2”進一步分析前景分割的結果，
當 frames 為 20、40、及 80 時(a)，分別由 Reddy 
(b)、SOBS (c)、及本計劃提出的方法(d)而得
到的相對應的 segmented foreground frames 
20F 、
40F 、及
80F 。其中，標示紅色正方形
的區域為發生 ghost (false positive) objects 之
處。 
根據以上實驗結果顯示，Reddy容易因為
某些 blocks 估算錯誤，造成 error propagation，
進而影響 modeled background 及 segmented 
foreground。SOBS 在初期建構 background 期
間，會保留部分 1I 的 foreground objects，導致
tF 存有一些 ghost objects，如 Fig. 30(c)所示。
而本計畫所提出的方法，於 frame 37 之後即
得到完整的 modeled background frame，因此
也可以得到較好的、無 ghost objects 的
segmented foreground 結果。 
 
(C) 第三年實驗結果 
本研究計畫所採用的測試視訊是選自於
Weizmann dataset [64]及 KTH dataset [92]，實
驗操作上採用 leave-one-actor-out 的方式。正
確率 accuracy 的評估方法為 
,
FNTNFPTP
TNTP
accuracy


  (39) 
其中 TP 為 true positive，TN 為 true negative，
FP 為 false positive，及 FN 為 false negative。 
Weizmann dataset 由 9 位 actors 做特定的
10 個 actions，這些 actions 包含“Bend”、
“Jack”、“Jump”、“Pjump”、“Side”、“Run”、
“Walk”、“Wave1”、“Wave2”、及“Skip”。本研
究計畫提出來的方法正確率可達 98.3%。Figs. 
34-37為本研究計畫提出來的方法與其他三個
action recognition 方法[87, 93, 94]比較的結
果，以 confusion matrices 表示之。 
KTH dataset 由 25 位 actors 做特定的 6 個
actions，這些 actions 包含“Box”、“Handclap”、
“Handwave”、“Jog”、“Run”、及“Walk”。本研
究計畫提出來的方法正確率可達 97.9%。Figs. 
38-41為本研究計畫提出來的方法與其他三個
action recognition 方法[87, 95, 93]比較的結
果，以 confusion matrices 表示之。 
總結來說，如 Table 2 所示，本研究計畫
提出的human action recognition方法之平均正
確率優於其他比較的方法[67, 87, 93-99]的平
均正確率。 
 
 15 
[21] O. J. Morris, J. Lee and A. G. Constantinidies, 
“Graph theory for image analysis: an 
approach based on the shortest spanning tree,” 
in Proc. of Inst. Electr. Eng., 1986, vol. 133, 
pp. 146-152. 
[22] S. X. Yu and J. Shi, “Multiclass spectral 
clustering,” in Proc. of IEEE Int. Conf. on 
Computer Vision, 2003, pp. 313-319. 
[23] Y. Weiss, “Segmentation using eigenvectors: a 
unifying view,” in Proc. of IEEE Int. Conf. on 
Computer Vision, 1999, pp. 957-982. 
[24] X. Jin and C. H. Davis, “A genetic image 
segmentation algorithm with a fuzzy-based 
evaluation function,” in Proc. of 12th IEEE 
Int. Conf. on Fuzzy Systems, 2003, pp. 
938-943. 
[25] M. G. H. Omran, A. Salman, and A. 
Engelbrecht, “Dynamic clustering using 
particle swarm optimization with application 
in image segmentation,” Pattern Analysis and 
Applications, vol. 8, no. 4, pp. 332-344, 2005. 
[26] Y. Shi and R. Eberhart, “A modified particle 
swarm optimizer,” in Proc. of IEEE Int. Conf. 
on Evolutionary Computation, 1998, pp. 
69-73. 
[27] M. S. Lee, “Detecting people in cluttered 
indoor scenes,” in Proc. of IEEE Conf. on 
Computer Vision and Pattern Recognition, 
2000, pp. 804-809. 
[28] Q. Wu, H. Cui, X. Du, M. Wang, and T. Jin, 
“Real-time moving maritime objects 
segmentation and tracking for video 
communication,” in Proc. of Int. Conf. on 
Communication Technology, 2006, pp. 1-4. 
[29] F. Y. Hu, Y. N. Zhang, and L. Yao, “An 
effective detection algorithm for moving 
object with complex background,” in Proc. of 
Int. Conf. on Machine Learning and 
Cybernetics, 2005, pp. 5011-5015. 
[30] M. Piccardi, “Background subtraction 
techniques: a review,” in Proc. of IEEE Int. 
Conf. on Systems, Man, and Cybernetics, 
2004, pp. 3099-3104. 
[31] C. R. Wren, A. Azarbayejani, T. Darrell, and 
A. P. Pentland, “Pfinder: real-time tracking of 
the human body,” IEEE Trans. on Pattern 
Analysis and Machine Intelligence, vol. 19, 
no. 7, pp. 780-785, 1997. 
[32] D. Koller, J. Weber, T. Huang, J. Malik, G. 
Ogasawara, B. Rao, and S. Russell, “Towards 
robust automatic traffic scene analysis in 
real-time,” in Proc. of the 12th IAPR Int. Conf. 
on Pattern Recognition, 1994, pp. 126-131. 
[33] R. Cucchiara, C. Grana, M. Piccardi, and A. 
Prati, “Detecting moving objects, ghosts, and 
shadows in video streams,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence, 
vol. 25, no. 10, pp. 1337-1442, 2003. 
[34] B. Shoushtarian, and H. E. Bez, “A practical 
adaptive approach for dynamic background 
subtraction using an invariant colour model 
and object tracking,” Pattern Recognition 
Letters, vol. 26, no. 1, pp. 5-26, 2005. 
[35] S. Zhu, X. Xia, Q. Zhang, and K. Belloulata, 
“A novel spatio-temporal video object 
segmentation algorithm,” in Proc. of IEEE Int. 
Conf. on Industrial Technology, 2008, pp. 1-5. 
[36] M. Singh, A. Basu, and M. Mandal, “Human 
activity recognition based on silhouette 
directionality,” IEEE Trans. on Circuits and 
Systems for Video Technology, vol. 18, no. 9, 
pp. 1280-1292, 2008. 
[37] A. Elgammal, D. Hanvood, and L. S. Davis, 
“Nonparametric model for background 
subtraction,” in Proc. of European Conf. on 
Computer Vision, 2000, pp. 751-767. 
[38] A. Meygret and M. Thonnat, “Segmentation 
of optical flow and 3d data for the 
interpretation of mobile objects,” in Proc. of 
the Int. Conf. on Computer Vision, 1990. 
[39] H. A. Rowley and J.M. Rehg, “Analyzing 
articulated motion using 
expectation-maximization,” in Proc. of Int. 
Conf. on Pattern Recognition, 1997, pp. 
935-941. 
[40] D. Zhou and H. Zhang, “Modified GMM 
background modeling and optical flow for 
detection of moving objects,” in Proc. of 
IEEE Int. Conf. on Systems, Man and 
Cybernetics, 2005, pp. 2224-2229. 
[41] C. Stauffer and W. E. L. Grimson, “Adaptive 
background mixture models for real-time 
tracking,” in Proc. of IEEE Int. Conf. on 
Computer Vision and Pattern Recognition, 
1999, pp. 246-252. 
[42] M. S. Allili, N. Bouguila, and D. Ziou, “A 
robust video foreground segmentation by 
using generalized Gaussian mixture 
modeling,” in Proc. of the 4th Canadian Conf. 
on Computer and Robot Vision, 2007, pp. 
503-509. 
[43] J. Zhang and C. H. Chen, “Moving objects 
detection and segmentation in dynamic video 
backgrounds,” in Proc. of IEEE Int. Conf. on 
Technologies for Homeland Security, 2007, 
pp. 64-69. 
[44] Z. Liu, J. Gu, L. Shen, and Z. Zhang, 
“Efficient video object segmentation based on 
Gaussian mixture model and Markov random 
field,” in Proc. of Int. Conf. on Signal 
 17 
Intelligence, vol. 31, no. 10, pp. 1898-1905, 
2009. 
[67] I. N. Junejo, et al., “View-independent action 
recognition from temporal self-similarities,” 
IEEE Trans. on Pattern Analysis and Machine 
Intelligence, vol. 33, no. 1, pp. 172-185, 2011. 
[68] N. Cuntoor, A. Kale, and R. Chellappa, 
“Combining multiple evidences for gait 
recognition,” in Proc. of IEEE Int. Conf. on 
Acoustics, Speech, and Signal Processing, 
2003, pp. 113-116. 
[69] A. F. Bobick and A. D. Wilson, “A state-based 
approach to the representation and 
recognition of gesture,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence, 
vol. 19, pp. 1325-1337, 1997. 
[70] M. K. Bhuyan, D. Ghosh, and P. K. Bora, 
“Threshold finite state machine for vision 
based gesture recognition,” in Proc. of IEEE 
Indicon 2005 Conf., 2005, pp. 379-382. 
[71] A. D. Wilson, A. F. Bobick, and J. Cassell, 
“Temporal classification of natural gesture 
and application to video coding,” in Proc. of 
IEEE Int. Conf. on Computer Vision and 
Pattern Recognition, 1997, pp. 948-954. 
[72] F. Niu and M. Abdel-Mottaleb, “HMM-based 
segmentation and recognition of human 
activities from video sequences,” in Proc. of 
IEEE Int. Conf. on Multimedia and Expo, 
2005, pp. 804-807. 
[73] P. Guo and Z. Miao, “A home environment 
posture and behavior recognition system,” in 
Proc. of IEEE Int. Conf. on Convergence 
Information Technology, 2007, pp. 175-180. 
[74] M. Ahmad and S. W. Lee, “Human action 
recognition using multi-view image sequences 
features,” in Proc. of IEEE Int. Conf. on 
Automatic Face and Gesture Recognition, 
2006, pp. 523-528. 
[75] M. Brand, N. Oliver, and A. Pentland, 
“Coupled hidden Markov models for complex 
action recognition,” in Proc. of IEEE Int. 
Conf. on Computer Vision and Pattern 
Recognition, 1997, pp. 994-999. 
[76] C. D. Liu, Y. N. Chung, and P. C. Chung, “An 
interaction-embedded HMM framework for 
human behavior understanding: with nursing 
environments as examples,” IEEE Trans. on 
Information Technology in Biomedicine, vol. 
14, no. 5, pp. 1236-1246, 2010. 
[77] Y. C. Wu, et al., “Human action recognition 
based on layered-HMM,” in Proc. of IEEE Int. 
Conf. on Multimedia and Expo, 2008, pp. 
1453-1456. 
[78] C. Schuldt, I. Laptev, and B. Caputo, 
“Recognizing human actions: a local svm 
approach,” in Proc. of IEEE Int. Conf. on 
Pattern Recognition, 2004, pp. 32-36. 
[79] H. Meng, N. Pears, and C. Bailey, 
“Recognizing human actions based on motion 
information and svm,” in Proc. of IEEE Int. 
Conf. on Intelligent Environments, 2006, vol. 
1, pp. 239-245. 
[80] T. Ojala, M. Petrou, and D. Harwood, “A 
comparative study of texture measures with 
classification based on feature distribution,” 
Pattern Recognition, vol. 29, no. 1, pp. 51-59, 
1996. 
[81] L. Itti, C. Koch, and E. Niebur, “A model of 
saliency-based visual attention for rapid scene 
analysis,” IEEE Trans. on Pattern Analysis 
and Machine Intelligence, vol. 20, no. 11, pp. 
1254-1259, 1998. 
[82] S. C. M. Cohen and L. N. de Castro, “Data 
clustering with particle swarms,” in Proc. of 
IEEE Congress on Evolutionary Computation, 
2006, pp. 1792-1798. 
[83] D. van der Merwe and A. P. Engelbrecht, 
“Data clustering using particle swarm 
optimization,” in Proc. of IEEE Congress on 
Evolutionary Computation, 2003, pp. 
215-220. 
[84] A. Ratnaweera, S. K. Halgamuge, and H. C. 
Watson, “Self-organizing hierarchical particle 
swarm optimizer with time-varying 
acceleration coefficients,” IEEE Trans. on 
Evolutionary Computation, vol. 8, no. 3, pp. 
240-255, 2004. 
[85] T. Thaipanich, P. H. Wu, and C. C. Kuo, 
“Video error concealment with outer and 
inner boundary matching algorithms,” in Proc. 
of SPIE, 2007, pp. 1-11. 
[86] Y. P. Guan, “Spatio-temporal motion-based 
foreground segmentation and shadow 
suppression,” IET Computer Vision, vol. 4, no. 
1, pp. 50-60, 2010. 
[87] K. Schindler and L. van Gool, “Action 
Snippets: how many frames does human 
action recognition require?” in Proc. of IEEE 
Int. Conf. on Computer Vision and Pattern 
Recognition, 2008, pp. 1-8. 
[88] D. J. Field, “Relations between the statistics 
of natural images and the response properties 
of cortical cells,” Journal of The Optical 
Society of  America A, vol. 4, no. 12, pp. 
2379-2394, 1987. 
[89] D. Martin, C. Fowlkes, D. Tal, and J. Malik, 
“A database of human segmented natural 
images and its application to evaluating 
segmentation algorithms and measuring 
 19 
Color quantization
Feature 
extraction
Input color images
Final segmentation results
Small region 
elimination
Region merging using 
modified PSO
Salient 
information
Coarse 
segmentation
 
Fig. 1. The proposed color image segmentation approach. 
 
  
(a) (b) 
Fig. 2. (a) An input color image and (b) the corresponding class-map of the quantized image with 13 
distinct color vectors (classes). 
 
6 5 2
7 6 1
9 3 7
 
1 0 0
1 0
1 0 1
 
1 2 4
8 16
32 64 128
 
1 0 0
8 0
32 0 128
 
(a) (b) (c) (d) 
Fig. 3. Local binary pattern of an illustrated texture unit. (a) Nine pixel values in a 33  texture 
unit. (b) Eight neighborhood pixels are thresholded by the central pixel value. (c) The corresponding 
weighting masks. (d) The LBP value is 169. 
 
 
Fig. 4. The texture spectrum of the input color image shown in Fig. 2(a). 
 
 21 
  
Background
Still object
Moving object
Illumination change
 
(a) tI  (b) (c) 
Fig. 8. An illustrated example of block representation. 
 
Motion 
vector 
estimation
Correlation 
coefficient 
computation
Static CBB
TC 
(a) Background
(b) Still object
Moving 
(d) Moving object
(c) Illumination change
1
),(
1
),(
~
,,  t ji
tt
ji I bb
1
),( ,
tt
ji Ib
1
),(),(
~
, t ji
t
ji bb
CBB TC 
CBB TC 
CBB TC 
 
Fig. 9. The flowchart of the proposed block representation approach. 
 
    
(a) 33I  (b) 33R  (c) 32B  (d) 33B  
Fig. 10. An illustrated example of background updating for “still object” block. 
 
t
ji ),(b
1tB
1tB
1tB 1tB
      (     -     )2)( ),(t jiSM b
tI
 
1
),(
~ t
jib
1tB
1tB
1tB 1tB
      (     -     )2 )
~
( 1 ),(
t
jiSM b
1tB
 
(a) (b) 
Fig. 11. The side-match measures, )( ),(
t
jiSM b  and ),
~
( 1 ),(
t
jiSM b  of a “moving object” block in 
background updating. 
 
 
 
 
 
 23 
 
Fig. 15. The proposed human action recognition approach. 
 
 
 
    
     
(a) Binary foreground frames. 
 
(b) A binary MHI. 
Fig. 16. An illustrated MHI. 
 
 25 
   
(a) (b) (c) 
   
(d) (e) (f) 
Fig. 19. (a) The original color image; (b)-(f) the five ground truth hand segmentations [89]. 
 
   
(a) (b) (c) 
   
(d) (e) (f) 
Fig. 20. (a) The original color image; (b)-(f) the five ground truth hand segmentations [89]. 
 
   
(a) (b) (c) 
   
(d) (e) (f) 
Fig. 21. (a) The original color image; (b)-(f) the five ground truth hand segmentations [89]. 
 
 
 
 
 27 
   
   
   
(a) DCPSO  
(PRI = 0.7140) 
(b) Mean shift  
(PRI = 0.8205) 
(c) Proposed  
(PRI = 0.8829) 
Fig. 25. Image segmentation results and PRI values of Fig. 20 by DCPSO, mean shift, and the 
proposed approach. 
 
   
   
   
(a) DCPSO  
(PRI = 0.4886) 
(b) Mean shift  
(PRI = 0.3018) 
(c) Proposed  
(PRI = 0.8005) 
Fig. 26. Image segmentation results and PRI values of Fig. 21 by DCPSO, mean shift, and the 
proposed approach. 
 
   
   
   
(a) DCPSO  
(PRI = 0.5542) 
(b) Mean shift  
(PRI = 0.9229) 
(c) Proposed  
(PRI = 0.9187) 
Fig. 27. Image segmentation results and PRI values of Fig. 22 by DCPSO, mean shift, and the 
proposed approach. 
 29 
 
(a) Sequence “Highway-2” 
 
(b) Reddy 
 
(c) SOBS 
 
(d) Proposed 
Fig. 30. Some background modeling results of the sequence “Highway-2” (a) by Reddy (b), SOBS 
(c), and the proposed approach (d). 
 
    
(a) Frame 20 (b) Reddy (c) SOBS (d) Proposed 
Fig. 31. The foreground extraction results of frame 20 of the sequence “Highway” (a) by Reddy (b), 
SOBS (c), and the proposed approach (d). 
 
    
(a) Frame 40 (b) Reddy (c) SOBS (d) Proposed 
Fig. 32. The foreground extraction results of frame 40 of the sequence “Highway” (a) by Reddy (b), 
SOBS (c), and the proposed approach (d). 
 
 
 
 
 
 
 31 
 
 
Fig. 37. The confusion matrix for the proposed approach (Weizmann dataset). 
 
 
Fig. 38. The confusion matrix for the AS approach [87] (KTH dataset). 
 
 
Fig. 39. The confusion matrix for the HPM approach [95] (KTH dataset). 
 
 
 
 
 
 1 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                     101年 7 月 31 日 
報告人姓名 柳金章 
服務機構 
及職稱 
國立中正大學(資工系), 教授 
   時間 
會議 
   地點 
2012年 7月 24 ~27日 
Rome, Italy 
本會核定 
補助文號 
NSC 98-2221-E-194-034-MY3 
(國科會研究計劃核定) 
會議 
名稱 
(中文) 2012年國際訊號處理暨多媒體應用學術會議 
(英文) 2012 Int. Conf. on Signal Processing and Multimedia Applications 
(SIMAP2012) 
發表 
論文 
題目 
(中文) 一種使用空間分散模型及邊緣陰影消除之視訊前景/背景分割方法 
(英文) Video foreground/background segmentation using spatially distributed 
model and edge-based shadow cancellation 
一、參加會議經過 
2012年國際訊號處理暨多媒體應用學術會議(2012 Int. Conf. on Signal 
Processing and Multimedia Applications, SIGMAP2112)，今年在Rome, Italy 
的The Melia Rome Aurelia Antica Hotel會議中心舉行，會議時間從2012年7
月24日至7月27日共4天。SIGMAP2112是the Int. Joint Conf. on e-Business and 
Telecommunications (ICETE2012) 的一部分. ICETE2012共有從56個國家投
出的403篇論文, 其中45篇為 full papers (11%), 77篇為 short papers (19%),
及55篇 poster papers，接受率約為43.92%。本次會議4天中有54 parallel 
sessions, 並包括5 個keynote talks, 1 advanced panel, 1 best paper competition 
session等。ICETE2012包含數個 joint conferences, 包括Int. Conf. on Data 
Communication Networking, Int. Conf. on e-Business, Int. Conf. on Optical 
Communication Systems, Int. Conf. on Security and Cryptography, Int. Conf. on 
Signal Processing and Multimedia Applications, and Int. Conf. on Wireless In-
formation Networks and Systems 等, 涵蓋電子商務.多媒體. 資訊安全.通訊.
網路等數個面向及相關議題。 
本人發表1篇論文(short paper)，題目是：Video foreground/background 
segmentation using spatially distributed model and edge-based shadow cancella-
tion (一種使用空間分散模型及邊緣陰影消除之視訊前景/背景分割方法),此
附
件
三 
 3 
豐富,顯示主辦單位極為用心。  
 
二、與會心得 
本次會議: 2012年國際訊號處理暨多媒體應用學術會議 (2012 Int. Conf. 
on Signal Processing and Multimedia Applications, SIGMAP2112) 是 the Int. 
Joint Conf. on e-Business and Telecommunications (ICETE2012) 的一部分. 
雖然規模不是很大, 但由於其涵蓋電子商務.多媒體. 資訊安全.通訊. 網路
等數個面向及相關議題, 因此是一個跨領域溝通及整合的學術平臺, 包含
數個 joint conferences: Int. Conf. on Data Communication Networking, Int. 
Conf. on e-Business, Int. Conf. on Optical Communication Systems, Int. Conf. 
on Security and Cryptography, Int. Conf. on Signal Processing and Multimedia 
Applications, and Int. Conf. on Wireless Information Networks and Systems 等, 
因此在會場中可以看到並聽到來自不同領域的專家, 對一個有興趣的跨領
域相關主題進行精彩的對話和討論, 對於参與會議的人員而言, 可以從中
得到很大的收獲. 
本次會議中與會約 400 名專家.學者.研究生…等, 由於 ICETE2012(或
SIGMAP2012) 由在歐洲的組織 INSTICC 所主導, 因此參加這次會議的學
者.專家.研人員以歐洲地區為大宗. 此次參加的我國學者、專家不多.但
SIGMAP2012有 2位 program committee members, 這顯示我國在此領域的研
究水準已獲得國際的認同. 我國在此相關領域的研究人口不少,研究成果豐
碩,陣容龐大, 因此除了参與此類國際學術會議之外, 應可積極爭取主辦相
關的國際學術會議, 以擴大我國在國際相關領域的影響力及發言權.例如, 
由成大王駿發教授所組成的團隊,在經過多重競爭下, 成功的爭取到 IEEE 
ISCAS2009 的主辦權.由臺大李琳山教授所組成的團隊,在經過多重競爭下, 
成功的爭取到 IEEE ICASSP2009 的主辦權, 這兩個國際學術會議都是相
關領域最大且最重要的國際學術會議,這是對我國在這個領域研究.發展及
相關學者.專家的重大肯定. 另外, 我國相關團隊 (本人為其中成員) 已爭取
到亞洲訊號處理學會 2013 年年會 (APSIPA ASC2013) 在我國舉行, 希望
能再一次提升我國在此領域之國際地位及影響力. 
 Video Foreground/Background Segmentation using Spatially 
Distributed Model and Edge-based Shadow Cancellation 
Shian-De Tsai, Jin-Jang Leou and Han-Hui Hsiao 
Department of Computer Science and Information Engineering, National Chung Cheng University,  
621 Chiayi, Taiwan 
{tht97m,, jjleou, hhh95p}@cs.ccu.edu.tw 
Keywords: Video Foreground/Background Segmentation, Spatially Distributed Model, Edge-based Shadow 
Cancellation. 
Abstract: Video foreground/background segmentation is to extract relevant objects (the foreground) from the 
background of a video sequence, which is an important step in many computer vision applications. In this 
study, the spatially distributed model is built by a splitting process using Gaussian probability distribution 
functions in spatial and color spaces. Then, edge-based shadow cancellation is employed to obtain more 
robust segmentation results. The proposed approach can well handle illumination variations, shadow effect, 
and dynamic scenes in video sequences. Based on experimental results obtained in this study, as compared 
with two comparison approaches, the proposed approach provides the better video segmentation results. 
1 INTRODUCTION 
Video foreground/background segmentation is to 
extract relevant objects (the foreground) from the 
background of a video sequence, which is the 
important step in many computer vision applications. 
Because a video sequence may contain illumination 
variations, shadow effect, dynamic scenes, …, video 
foreground/background segmentation is a 
challenging task. 
Existing video foreground/background segmen-
tation approaches include three categories, namely, 
thresholding, background subtraction, and motion-
based. The first category of approaches is based on 
thresholding pixel differences between two related 
frames (two consecutive frames or the current frame 
and a background frame). Because segmentation 
results are sensitive to thresholding values, various 
adaptive thresholding approaches were proposed 
(Tsaig and Averbuch, 2002); (Kim and Hwang, 
2002). 
For the second category of approaches, Heikkila 
and Pietikainen (2006) proposed an efficient texture-
based method for background modeling. The local 
binary pattern (LBP) texture operator is employed, 
which has several good properties for background + 
modeling.  Zhang et al. (2008)  proposed a novel dy- 
 
This work was supported in part by National Science Council, 
Taiwan, Republic of China under Grants NSC 98-2221-E-194-
034-MY3 and NSC 99-2221-E-194-032-MY3. 
dynamic background subtraction approach based on 
the covariance matrix descriptor. The covariance 
matrix integrates the pixel-level and region-level 
features together and efficiently represents the 
correlation between features. Wang et al. (2008) 
presented three algorithms (running average, median, 
mixture of Gaussian) for modeling the background 
directly from the compressed video. Their approach 
utilizes DCT coefficients at block level to represent 
background, and adapts the background by updating 
DCT coefficients. Li et al. (2004) proposed a 
Bayesian framework that incorporates spectral, 
spatial, and temporal features to characterize the 
background appearance. A Bayes decision rule is 
derived for classification based on the statistics of 
principal features. 
For the third category of approaches, motion-
based foreground/background segmentation can be 
treated as fitting a collection of motion models to 
spatiotemporal image data. Mezaris et al. (2004) 
proposed a model-based foreground/background 
segmentation approach including three stages: initial 
segmentation of the first frame using color, motion, 
and position features, a temporal tracking algorithm, 
and a trajectory-based region merging procedure. 
Wang et al. (2005) proposed a Bayesian network to 
model interactions among the motion vector field, 
the intensity segmentation field, and the video 
segmentation field. The Markov random field is then 
89
  
Next, to detect new foreground object(s) in the 
new frame, the “unassigned” pixels in the support 
map should be detected. Here, if pixel tx
v  of a 
component satisfies 
 ,))|((log unassignCt Txp map ≤θv  (8)
where Tunassign denotes a minimum probability 
threshold, then tx
v  is determined as an “unassigned” 
pixel. 
2.2 Foreground Detection and 
Introducing Foreground Model  
Initially, all the initial model components of a frame 
are labeled as the background. A foreground 
component is detected when some region of pixels 
having a low probability under the mixture model. 
Such a region appears in the support map as a region 
having high density of “unassigned” pixels. The 
support map is divided into nonoverlapping blocks 
of size 16×16 pixels. For a block, if the number of 
“unassigned” pixels exceeds a threshold Td, it is 
detected as a “foreground” block. Initially, a single 
foreground component is built for all unassigned 
pixels in these “foreground” blocks. Then, similar to 
the background model, the splitting procedure is 
recursively applied to build the foreground model. 
After pixel assignment and foreground detection, 
the parameters of both background and foreground 
components of a frame are re-estimated. Given the 
parameters of the previous frame ,)1,( −tjθ  the new 
parameters ),( tjθ  of the current frame can be 
computed (using an adaptive learning rate) as 
 ,)-(1 )1,(),(),( −+= tjjCmapjjtj αα θθθ  (9)
where αj is a vector of learning rates updated by a 
variable factor ,wjα  which is proportional to the area 
fraction of each foreground/background component 
within a frame. 
2.3 Edge-based Shadow Cancellation 
After foreground detection, the initial foreground 
mask FMt of frame t usually contains both moving 
objects and some shadows. A shadow often appears 
in an area where the pixel (gray-level) values change 
“gradually” from the background to the shadow 
region. Here, the Canny edge detector (Canny, 1986) 
is used to generate a binary edge map CEt of frame t, 
where “1” denotes the edge and “0” denotes 
otherwise. 
Using the binary edge maps of 5 successive 
frames, the integrated Canny edge map ICEt is 
defined as 
⎪⎩
⎪⎨
⎧
===
=
=
.                                         otherwise,  edge, moving
 ,1)()()( if      edge, static
                                     ,0)( if         edge,not  
)(
5-3- xCExCExCE
xCE
xICE
ttt
t
t
vvv
v
v
 
(10)
where xv denotes a pixel. 
Based on ICEt and FMt of frame t, the moving 
Canny edge MCEt can be defined as 
  }, 
edge,Canny  moving)(|{
t
tt
FMx
xICExMCE
∈
==
v
vv
 (11)
whereas the edges of the foreground mask EFMt can 
be defined as 
  }.) ,NG( 
, ,|{
truexx
FMxFMxxEFM ttt
=′
∉′∈=
vv
vvv
 (12)
where NG( xv , x′v ) is a logic function that returns true 
when xv and x′v are 4-connected neighbors. Note that 
MCEt provides important information for seed point 
selection in region growing (as an illustrated 
example shown in Figure 2). Because some gaps 
exist in moving Canny edges MCEt, in this study, 
morphologic dilation with a 3×3 structure element is 
applied on MCEt, resulting in DMCEt.  
Because region growing is used to detect shadow 
regions, some seed points are required. The shadow 
region edges SRE t in FM t may be employed as seed 
points, which can be defined as 
}, ,-min
 ,|{
tsre
tt
MCExTxx
EFMxxSRE
∈′>′
∈=
vvv
vv
 (13)
where Tsre is a threshold confining the searching 
neighbor and ||•|| denotes the Chebyshev distance. 
Additionally, SREt also contains some sporadic 
pixels (the edges of moving objects). To remove 
sporadic pixels, the connected component algorithm 
(Haralick and Shapiro, 1992) is also used to connect 
initial seed points. Each connected region with its 
number of pixels more than a threshold Tseed is 
included in the final shadow region edges .finaltSRE
The pixels in finaltSRE serve as seed points of the 
region growing algorithm (Adam and Bischof, 1994) 
used for shadow detection, which are expanded pixel 
by pixel in FMt, resulting in the detected shadow
.shadowtFM  Note that the pixels in DMCEt should not 
be included in shadowtFM  (as an illustrated example 
shown in Figure 3). 
Based on FMt and ,shadowtFM the initial moving 
object MOt (obtained as shadowttt FMFMMO −= ) 
VideoForeground/BackgroundSegmentationusingSpatiallyDistributedModelandEdge-basedShadowCancellation
91
  
surve
Proce
Mezaris, 
Video
tempo
IEEE 
Techn
Rosin, P. 
image
Recog
Toyama, 
1999.
backg
on Co
Tsaig, Y
segme
region
System
Wang, W
backg
comp
System
Wang, Y.
based
Proce
Zhang, S
dynam
Conf.
APPE
Figure 1
segmenta
illance applica
ss., 17(7), 1168
V., Kompatsia
 object seg
ral tracking an
Trans. on C
ol., 14(6), 782
and Ioannidism
 thresholding
nition Letters, 
K., Krumm, J
 Wallflower:
round mainten
mputer Vision,
. and Ave
ntation of mov
 labeling appro
s for Video Te
., Yang, J., 
round and se
ressed video. 
s for Video Te
 et al., 2005. S
 on graphical 
ss., 14(7), 937-
. et al., 2008.
ic background
 on Pattern Rec
NDIX 
: The propose
tion approach. 
tions. IEEE 
-1177. 
ris, I., and Stri
mentation u
d trajectory-ba
ircuits and S
-795. 
, E., 2003. E
 for change 
24(14), 2345-2
., Brumitt, B
 principles 
ance. in Proc. 
 255-261. 
rbuch, A., 
ing objects in 
ach. IEEE Tra
chnol., 12(7), 5
and Gao, W
gmenting mov
IEEE Trans.
chnol., 18(5), 6
patiotemporal 
models. IEEE
947. 
 A covariance
 subtraction. in
ognition, 1-4. 
d video fore
Trans. on Im
ntzis, M. G., 2
sing Bayes-b
sed region mer
ystems for V
valuation of gl
detection. Pa
356. 
., and Meyers
and practice 
of IEEE Int. C
2002. Autom
video sequenc
ns. on Circuits
97-612. 
., 2008. Mode
ing objects 
on Circuits 
70-681. 
video segmenta
 Trans. on Im
-based method
 Proc. of IEEE
 
ground/backgro
age 
004. 
ased 
ging. 
ideo 
obal 
ttern 
, B., 
of 
onf. 
atic 
es: a 
 and 
ling 
from 
and 
tion 
age 
 for 
 Int. 
und 
Figu
EFM
F
Fig
(a) Frame 4
(c) MCE46
re 2: (a) Fram
46. 
(a) DMCE4
(c) finaSRE46
igure 3: An ill
(b) shadFM 46
ure 4: An illus
6 
 
e 46, (b) ICE
6 
l  
ustrated examp
(a) FM46 
ow  
trated example 
(b) ICE46 
(d) EFM46 
46, (c) MCE4
(b) SRE46 
(d) shadoFM 46
le of region gr
 
(c) MO46 
of shadow can
6, and (d) 
w  
owing. 
cellation. 
VideoForeground/BackgroundSegmentationusingSpatiallyDistributedModelandEdge-basedShadowCancellation
93
  
Table 2: Jaccard coefficients and total errors of 12 test sequences by SOBS, SDM, and the proposed approach (Proposed). 
Sequence Jaccard coefficient Total errors (×10
3) 
SOBS SDM Proposed SOBS SDM Proposed 
Office 0.43 0.41 0.47 13.4 15.7 13.6 
Outdoor 0.41 0.46 0.43 18.6 15.0 15.2 
Browse1 0.47 0.43 0.54 17.3 18.6 14.3 
LightSwitch 0.28 0.23 0.41 21.6 25.5 13.5 
NightCar 0.34 0.39 0.41 15.4 13.2 11.2 
IntelligenRoom 0.68 0.71 0.78 10.3 8.6 5.5 
ParkingLot 0.56 0.47 0.54 15.4 18.6 16.7 
OneLeaveShopReenter 0.32 0.41 0.48 16.8 12.3 8.5 
WavingTree1 0.23 0.52 0.68 18.3 10.4 7.3 
WavingTree2 0.23 0.54 0.62 19.7 11.9 9.8 
Raining 0.34 0.42 0.47 18.6 15.4 11.9 
Boat 0.29 0.43 0.53 15.5 14.3 11.3 
Average 0.38 0.45 0.53 16.7 14.9 11.6 
 
VideoForeground/BackgroundSegmentationusingSpatiallyDistributedModelandEdge-basedShadowCancellation
95
98年度專題研究計畫研究成果彙整表 
計畫主持人：柳金章 計畫編號：98-2221-E-194-034-MY3 
計畫名稱：智慧型視覺照護與監控--子計畫二：影像/視訊分割及人類動作分析 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
