 2 
在 本 計 畫 中 ， 我 們 將 探 討 朝 向 這 個 目 標 可 能 遭 遇 的 問 題 ， 並 嘗 試 提 出 解 決 的 方
案 。 例 如 ： 為 了 提 供 高 效 能 的 資 料 存 取 ， 檔 案 必 須 被 分 布 到 多 個 儲 存 空 間 並 且 複 製
到 多 個 伺 服 器 ， 因 此 我 們 必 須 考 慮 檔 案 分 布 、 檔 案 複 製 、 伺 服 器 安 置 和 群 體 通 訊 等
議 題 。 還 有 為 了 充 分 利 用 網 路 的 頻 寬 並 降 低 網 路 的 延 遲 ， 因 此 我 們 必 須 檔 案 多 點 傳
輸 時 的 工 作 量 分 配 問 題 。 另 外 由 於 網 格 系 統 的 分 散 性 與 異 質 性 ， 使 得 網 格 系 統 的 使
用 和 程 式 的 撰 寫 變 得 相 當 不 容 易 ， 因 此 我 們 也 將 研 發 簡 化 網 格 資 源 運 用 所 需 的 軟 體
技 術 。 我 們 計 畫 提 供 一 個 整 合 的 虛 擬 環 境 ， 讓 使 用 者 管 理 分 散 的 資 源 並 操 作 各 種 網
路 服 務 ， 同 時 促 進 網 路 使 用 者 之 間 的 合 作 。 然 後 利 用 網 路 服 務 技 術 以 簡 化 網 格 資 源
的 運 用 與 分 享 ， 再 藉 著 開 放 的 網 格 計 算 架 構 以 整 合 大 量 資 料 的 存 取 和 分 析 ， 並 且 便
利 網 格 應 用 的 開 發 。  
 
三 、  文 獻 探 討  
隨 著 儲 存 資 源 的 分 散 與 網 格 運 算 資 料 的 大 量 成 長 ， 使 用 者 對 於 網 格 資 料 管 理 系
統 的 需 求 日 益 增 加 。 近 年 來 ， 許 多 研 究 與 科 學 單 位 相 繼 投 入 網 格 資 料 管 理 與 儲 存 工
具 的 開 發 。 例 如 SDSC 的 SRB(Storage Resource Broker)[1]、 SciDAC 的 Data Grid 
Middleware[2]與 GriPhyN 的 Virtual Data System[3]等 網 格 資 料 管 理 與 儲 存 工 具 。 在
這 些 軟 體 工 具 中 ， SDSC 的 SRB 是 一 個 一 般 化 且 非 用 於 特 定 科 學 領 域 的 資 料 網 格
中 介 軟 體 ， 其 主 要 功 能 為 整 合 異 質 與 分 散 儲 存 資 源 並 提 供 虛 擬 的 存 取 介 面 。 SRB
已 被 許 多 大 型 網 格 計 畫 所 採 用 ， 因 此 在 本 計 畫 中 ， 我 們 也 採 用 SRB 做 為 我 們 管 理
網 格 資 料 與 儲 存 資 源 的 中 介 軟 體 ， 並 根 據 大 量 資 料 之 網 格 應 用 需 求 ， 擴 充 SRB 之
功 能 。  
 
四 、  研 究 方 法 與 完 成 事 項  
在 本 計 畫 中 ， 我 們 將 以 SRB 為 基 礎 來 建 置 網 格 環 境 下 的 虛 擬 儲 存 系 統 並 提 供
多 種 高 階 功 能 ， 整 體 架 構 如 圖 一 所 示 。 圖 一 伺 服 器 的 左 下 方 為 分 散 在 各 地 的 實 體
 4 
在 客 戶 端 的 下 方 為 UniGrid 資 料 管 理 函 式 庫 ， 其 主 要 的 功 能 為 與 伺 服 器 端 溝
通 ， 並 提 供 上 層 UniGrid 應 用 程 式 開 發 所 需 的 資 料 管 理 功 能 ， 目 前 我 們 提 供 了 C
語 言 版 本 與 JAVA 語 言 版 本 的 函 式 庫 。 透 過 此 函 式 庫 ， UniGrid 應 用 程 式 可 以 存 取
分 散 於 不 同 地 區 的 儲 存 資 源 並 管 理 網 格 資 料 。  
在 本 計 畫 中 ， 我 們 探 討 朝 向 這 個 目 標 時 可 能 遭 遇 的 一 些 問 題 ， 並 且 嘗 試 提 出
解 決 的 方 案 ， 實 作 於 系 統 中 。  
1. 高 效 能 資 料 傳 輸 技 術 : 日 前 Storage Resource Broker 在 傳 送 資 料 時 是 採 用 點 對
點 (圖 二 (a))多 重 資 料 串 流 (Multiple TCP Streams)的 傳 輸 模 式 ， 然 而 以 此 種 傳 輸
方 式 會 依 賴 於 單 一 發 送 端 的 頻 寬 ， 當 發 送 端 的 頻 寬 變 小 時 ， 資 料 的 傳 輸 效 能 也
會 跟 著 下 降 。 本 計 畫 將 擴 充 Storage Resource Broker 資 料 傳 輸 的 功 能 ， 當 有 多
個 相 同 資 料 分 散 在 不 同 的 機 器 上 時 ， 利 用 平 行 多 點 資 料 傳 輸 的 方 式 (圖 二 (b))以
提 高 資 料 傳 輸 服 務 的 效 能 ， 並 減 輕 資 料 伺 服 器 的 負 擔 。  
 
 
圖 二 . (a) Storage Resource Broker 所 支 援 之 資 料 傳 輸 模 式 . (b) 高 效 能 資 料 傳 輸
服 務 所 支 援 之 資 料 傳 輸 模 式 . 
 
 6 
透 過 此 一 介 面 ， 透 通 地 存 取 虛 擬 儲 存 系 統 內 的 資 料 ， 其 功 能 包 含 了 上 傳 、 下 載 、
一 般 檔 案 操 作 與 複 本 管 理 等 。  
 
 
圖 四 . 虛 擬 儲 存 系 統 使 用 者 介 面  
 
4. 資 料 網 格 建 置 與 整 合 ： 除 了 研 究 與 發 展 上 述 技 術 外 ， 我 們 還 將 建 置 整 個 Taiwan 
UniGrid 資 料 網 格 平 台 (圖 五 )， 以 整 合 各 個 大 學 與 研 究 單 位 所 分 享 的 儲 存 資 源 。
由 於 在 Taiwan UniGrid 上 有 大 量 的 儲 存 資 源 分 散 在 各 個 單 位 ， 使 用 單 一 的 資 訊
系 統 (Information Server)維 護 所 有 網 格 資 料 將 會 是 一 大 負 擔 ， 因 此 我 們 將 Taiwan 
UniGrid 上 所 有 的 儲 存 資 源 分 割 成 五 個 區 域 ： Taipei_UuiGrid, Hsinchu_UniGrid, 
Taichung_UniGrid, Tainan_UniGrid and Hualien_UniGrid。 每 個 區 域 都 有 一 個 資
訊 系 統 (即  SRB MCAT Server)負 責 維 護 該 區 域 的 資 料 ， 並 與 其 他 區 域 的 資 訊 系
統 定 期 作 資 料 同 步 ， 藉 此 網 格 使 用 者 可 以 存 取 不 同 區 域 的 儲 存 資 源 。  
 
 8 
因 此 ， 我 們 將 在 下 年 度 運 用 pee-to-peer 技 術 於 Grid storage system， 希 望 利 用
pee-to-peer 的 特 性 ， 發 展 高 效 能 與 易 於 分 享 管 理 的 Grid storage system。  
 
參 考 文 獻  
[1] Chaitanya Baru, R. Moore, A. Rajasekar and M. Wan, “ The SDSC storage resource 
broker,”  Proceedings of the 1998 conference of the Centre for Advanced Studies on 
Collaborative Research, Canada, 1998, also available at http://www.sdsc.edu/srb. 
[2] B. Allcock, A. Chervenak, I. Foster, C. Kesselman, and M. Livny, “ Data Grid tools: 
enabling science on big distributed data,”  Journal of Physics: Conference Series 16, 
2005, also available at http://www-fp.mcs.anl.gov/dsl/scidac/datagrid. 
[3] Y. Zhao, M. Wilde, I. Foster, J. Voeckler, J. Dobson, E. Glibert, T. Jordan, and E. Quigg, 
“ Virtual Data Grid Middleware Services for Data-Intensive Science,”  Concurrency and 
Computation: Practice & Experience, Vol. 18 , Issue 6, 2004, also available at 
http://vds.uchicago.edu/twiki/bin/view/VDSWeb/WebMain. 
[4] C.-M. Wang, C.-C. Hsu, P. Liu, H.-M. Chen and J.-J. Wu, “ Optimizing Server 
Placement in Hierarchical Grid Environments,”  Proceedings of the 2006 International 
Conference on Grid and Pervasive Computing, pp. 1-11, Taichung, Taiwan, May 2006. 
The proceedings also appears as Lecture Notes in Computer Science 3947, Springer, 
2006. (Note: The paper acceptance rate was 24%. Among 267 paper submissions, 
our paper was selected as one of the four best paper award winners.) 
[5] C.-M. Wang, C.-C. Hsu, H.-M. Chen and J.-J. Wu, “ Efficient Multi-Source Data 
Transfer in Data Grids,”  Proceedings of the 6th IEEE International Symposium on 
Cluster Computing and the Grid, pp. 421-424, Singapore, May 2006. (Note: The paper 
acceptance rate was 41%.) 
[6] C.-M. Wang, H.-M. Chen, C.-C. Chang and J.-J. Wu, “ A High-Performance Virtual 
Storage System for Taiwan UniGrid,”  Proceedings of the 2007 International 
Conference on Grid and Pervasive Computing, pp. 27-38, Paris, France, March 2007. 
(Note: The acceptance rate of regular papers was 26%.) 
[7] C.-M. Wang, C.-C. Hsu, P. Liu, H.-M. Chen and J.-J. Wu, “ Optimizing Server 
Placement for QoS Requirements in Hierarchical Grid Environments,”  Proceedings of 
the 2007 International Conference on Grid and Pervasive Computing, pp. 181-192, 
Paris, France, March 2007. (Note: The acceptance rate of regular papers was 26%.) 
出 席 國 際 學 術 會 議 心 得 報 告  
                                                             
計 畫 編 號  NSC95-2221-E-001-002 
計 畫 名 稱  處 理 大 量 資 料 之 網 格 應 用 的 高 效 能 協 力 架 構 之 研 究 (2/2) 
出 國 人 員 姓 名  
服 務 機 關 及 職 稱  
王 建 民  
中 央 研 究 院 資 訊 科 學 研 究 所  副 研 究 員  
會 議 時 間 地 點  民 國 96 年 5 月 2 日 至 5 月 4 日  法 國 巴 黎  
會 議 名 稱  The 2007 International Conference on Grid and Pervasive Computing 
發 表 論 文 題 目  
1. A High-Performance Virtual Storage System for Taiwan UniGrid 
2. Optimizing Server Placement for QoS Requirements in Hierarchical Grid 
Environments 
 
一 、 參 加 會 議 經 過  
 
2007 年 網 格 與 普 及 計 算 國 際 會 議 於 民 國 九 十 六 年 五 月 二 日 至 四 日 在 法 國 巴 黎 舉 行 ，
總 共 收 到 來 自 20 個 國 家 的 217 篇 論 文 投 稿 ， 經 過 嚴 格 的 論 文 審 查 程 序 後 ， 其 中 有 56 篇
被 接 受 為 完 整 論 文 ， 另 外 12 篇 為 短 篇 論 文 ， 並 且 安 排 了 四 場 專 題 演 講 。 除 此 之 外 ， 主 辦
單 位 還 安 排 一 場 Round Table On Pervasive Grids 的 討 論 會 。 估 計 大 約 有 將 近 兩 百 位 來 自
世 界 各 國 的 學 者 專 家 出 席 此 一 會 議 ， 是 網 格 與 普 及 計 算 研 究 領 域 極 為 重 要 的 國 際 學 術 會
議 。  
 
正 式 會 議 於 五 月 二 日 上 午 九 點 開 始 ， 首 先 由 加 拿 大 St. Francis Xavier University 的
Prof. Laurence T. Yang 發 表 專 題 演 講 。 隨 後 展 開 會 議 論 文 發 表 的 議 程 ， 本 次 大 會 的 議 程 被
分 為 三 組 平 行 進 行 。 五 月 三 日 也 安 排 了 一 場 專 題 演 講 ， 由 在 The European CoreGRID 
Network of Excellence 計 畫 中 擔 任 Scientific Coordinator 的 法 國 INRIA 研 究 員 Dr. Thierry 
Priol 主 講 。 五 月 四 日 則 安 排 了 兩 場 專 題 演 講 ， 演 講 者 分 別 為 日 本 University of Aizu 的 Prof. 
Minyi Guo 和 在 Grid'5000 計 畫 中 擔 任 Project co-Leader 的 法 國 INRIA 研 究 員 Dr. Franck 
Cappello。 這 些 專 題 演 講 不 但 分 享 各 國 建 置 網 格 平 台 的 經 驗 ， 還 指 點 出 未 來 網 格 與 普 及 計
算 的 研 究 方 向 ， 因 此 很 能 引 起 聽 眾 的 熱 烈 討 論 。  
 
我 們 的 兩 篇 論 文 分 別 被 安 排 在 五 月 二 日 和 三 日 的 。 同 一 時 段 各 有 其 它 兩 篇 論 文 發
表 ， 這 一 組 的 聽 眾 相 當 多 ， 發 問 和 討 論 的 過 程 不 但 十 分 踴 躍 也 很 有 深 度 ， 可 以 看 出 大 家
都 很 重 視 這 個 領 域 的 研 究 。 從 論 文 題 目 來 分 析 ， 我 們 可 以 看 出 virtual storage system, data 
placement, and resource management 仍 是 目 前 最 主 要 的 研 究 題 目 ， 而 peer-to-peer 
technology 和 Grid application 則 是 未 來 值 得 注 意 的 研 究 方 向 。  
 
 
 
A High-Performance Virtual Storage System for 
Taiwan UniGrid 
Chien-Min Wang1, Hsi-Min Chen2, Chun-Chen Hsu3, and Jan-Jan Wu1
 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science and Information Engineering, 
National Central University, Taoyuan, Taiwan 
3 Department of Computer Science and Information Engineering, 
National Taiwan University, Taipei, Taiwan 
{cmwang, seeme, tk, wuj}@iis.sinica.edu.tw 
Abstract. In Taiwan, a community of educational and research organizations 
interested in Grid computing technologies founded a Grid computing platform, 
called Taiwan UniGrid. Taiwan UniGrid consists of three primary portions: 
Computational Grid, Data Grid, and Web Portal. In this paper, we present the 
development of a virtual data storage system for Taiwan UniGrid. In addition to 
developing basic data storage functions, we identify three main requirements of 
the current development: high-performance data transfer, data sharing and 
single sing-on. For these requirements, we come up with three corresponding 
features in our data storage system: Self-Adaptation for high-performance data 
transfer, forming user groups and specifying admission control for data sharing, 
and adopting GSI authentication to enable single sing-on. Besides, we also 
develop a Java-based graphic user interface of the storage system that allows 
Grid users to manage data transparently as using local file systems.  
Keywords: Data Grid, data storage system, data transfer, web service, and 
single sign-on 
1   Introduction 
With the rapid growth of computing power and storage capacity of computers, many 
researchers and scientists have been concentrated on the development of various Grid 
systems to efficiently utilize distributed computing and storage resources in recent 
years. In Taiwan, a community of educational and research organizations interested in 
Grid computing technologies founded a Grid computing platform, called Taiwan 
UniGrid [1]. These organizations contribute their resources of computer clusters for 
sharing and collaboration. The objective of Taiwan UniGrid is to provide educational 
and research organizations with a powerful computing platform where they can study 
Grid-related issues, practice parallel programming on Grid environments and execute 
computing/data-intensive applications. 
As similar to other Grid systems, Taiwan UniGrid consists of three primary 
portions: Computational Grid, Data Grid and Web Portal. Computational Grid is 
by which Grid users only have to login once and utilize Grid resources through 
certificates, so that they have no need to keep all accounts for each machine. Besides 
these features, we also develop a Java-based graphic user interface of the storage 
system that allows Grid users to manipulate data transparently as using local file 
systems. 
The remainder of the paper is organized as follows. In Section 2, we explain the 
system framework and deployment. Section 3 presents main features, including multi-
source data transfer, data sharing, single sign-on, and the data management client. An 
operational scenario of Taiwan UniGrid is demonstrated in Section 4. Finally, we 
present some concluding remarks in the last section. 
 
 
Fig. 1. The framework of the virtual storage system for Taiwan UniGrid. 
2   System Framework and Deployment 
Figure 1 shows the framework of our virtual storage system. In the server side, the left 
bottom of the framework is a set of physical storage resources, including hard disks, 
tapes and databases, contributed by the members of Taiwan UniGrid. We adopt SRB 
as a data management middleware to integrate these scattered storage resources.  
SRB provides a list of data and storage management functions. Although SRB has 
furnished an efficient data transfer approach by using multiple TCP connections, we 
propose an alternative, called Self Adaptation, to get a higher data transfer rate in 
comparison with the original one. We will explain the detail of Self Adaptation in 
section 3. Therefore, we add the alternative (Self Adaptation Patch) into the original 
functions of SRB. A set of extended SRB APIs are built on top of SRB and the Self 
Adaptation Patch. The extended SRB APIs consist of primary APIs provided by SRB 
and the APIs for high-performance data transfer, such as MSDTReplicate() and 
MSDTCopy(). 
The right of the server side of the framework is a number of Web services used for 
data management. Web service technologies are playing an increasingly important 
role in the new generation of Grids. Such technologies encapsulate heterogeneous 
sharing policies, for instance, some resources can be shared with users registered in 
other zones, but some are utilized in private. In addition, each MCAT server 
periodically synchronizes its metadata with each other to keep the metadata 
consistency among zones. By synchronization, Grid users registered in one zone can 
access storage resources located in other zones and retrieve sharing data timely.  
The members of Taiwan UniGrid contribute their storage resources by setting up 
SRB servers. Each SRB server consists of one or more physical storage resources and 
is registered to a MCAT server. Gird users can manipulate data objects in a specified 
storage resource of a SRB server, for example uploading data objects, creating 
replicas and modifying metadata of the data objects. Then the SRB server will 
automatically ask the MCAT server, which registers the SRB server, to update the 
metadata of the operated data object and synchronize with other MCAT servers. Thus, 
a Grid user who logins to one of close SRB servers can utilize storage resources in 
any zone of Taiwan UniGrid. 
3   Main Features 
In this section, we present the main features of our system, including multi-source 
data transfer, data sharing, single sign-on, for the requirements listed in Section 1. In 
addition, we also develop a friendly graphic user interface of the virtual storage 
system that helps Grid users manage their data as using local file systems. 
 
Internet
Replica 
Source
Dist. InternetReplicaSource Dist.
Replica
Source
Replica
Source
(a) (b)
Replica 
Source
 
Fig. 3(a) The replica selection approach. (b) The multi-source data transfer approach. 
3.1   Multi-source Data Transfer 
To achieve high-performance data transfer, data replication has been a widely used 
technique that facilitates a Grid user to select a best replica site closest to the specific 
destination and transfer the selected replica to it. Instead of transferring data from the 
source site, selecting the best replica can reduce the data transfer time on the Internet. 
A number of approaches have been proposed for selecting the best replica based on 
various criteria [10, 11]. However, as shown in Figure 3(a), since such an approach 
only allows users to specify one replica for transfer in each selection, they have two 
major shortcomings: 
group workspace can assign one or more owners to manage the admission of the 
workspace. 
In general, Grid users have their own personal workspace, i.e. a user home 
directory, where they can manage their private data objects. Data objects can be files, 
directories, replicas or links. Grid users can share their private data objects with others 
via specifying access permissions on data objects. Figure 4 shows a screenshot of 
admission control for data sharing, by which Grid users can specify read or write 
permission for each data object to other users or groups. It also supports the owner 
change of a specific data object. On the other hand, Grid users can share their data by 
uploading or copying private data objects directly to the group workspaces. 
 
 
Fig. 4. A screenshot of the data management client and admission control for data 
sharing. 
3.3   Single Sign-On 
Since software components and resources within a Grid system are distributed in 
different organizations and managed independently, using one account for a Grid user 
to utilize all these software components and resources becomes a crucial issue. GSI 
(Grid Security Infrastructure) [6] is a promising solution to the issue in Grids. GSI 
uses X.509 certificates to securely authenticate users across the network. Moreover, 
SRB supports two main methods of authentication, GSI and an SRB secure password 
system known as Encrypt1. GSI in SRB makes use of the same certificates and Public 
Key Infrastructure (PKI) [14] as do Globus Toolkit [15] such as GridFTP [16].  
Since we adopt Globus Toolkit as the middleware for managing computing resources, 
in order to enable the single sign-on for utilizing Computational Grid and Data Grid, 
we choose GSI as the main user authentication mechanism in our system.   
To use Taiwan UniGrid, Grid users have to register in UniGrid Portal first. The 
users will receive certificates issued from UniGrid CA after approved by system 
administrators. Meanwhile, the users’ profiles are also registered to Computational 
Grid and Data Grid, i.e. Globus and SRB. Once users want to use Taiwan UniGrid, 
service removes the session. This prevents malicious users from using the cached 
session keys to retrieve passwords from the Account Management service. 
 
 
Fig. 6(a) The proposed authentication process enabling single sing-on for UniGrid 
Portal and the data management client. (b) The proposed authentication process 
enabling single sing-on for computing nodes. 
 
From the perspective of programs, Resource Broker delegates submitted jobs to 
computing nodes with limited proxy certificates, not full proxy certificate, for 
authentication. However, in the current implementation of SRB, the limited proxy 
certificates will be failed in accessing storage resources located in different zones. 
Only full proxy certificates are allowed to access the cross-zone resources in SRB. 
Hence, we propose an authentication process for computing nodes, as shown in 
Figure 6(b), to deal with this problem. After Resource Broker submits jobs to 
computing nodes with limited proxy certificates, the computing nodes use the limited 
proxy certificates to get full proxy certificates from the Account Management service. 
Finally, the nodes can connect to SRB servers located in different zones with full 
proxy certificates and access programs and data in the storage resources. 
Table 1.  The supported operations for data objects in the virtual storage system.  
Data object Operations 
File download, upload, delete, copy, paste, rename 
Directory create, download, upload, delete, copy, paste, rename 
Link create, download, delete, copy, paste, rename 
Replica create, delete 
3.4   The Data Management Client 
We develop two kinds of clients of the virtual storage system. One is Java-based 
standalone version not integrated with UniGrid Portal and Computational Gird. It is 
suitable for users who just want to store their data without the need of computation 
z Once the user has submitted a job, the portal asks Resource Broker to select 
computing resources based on the requirement of the submitted job. 
z Resource Broker assigns the submitted job to the selected computing nodes.  The 
selected computing nodes then retrieve programs and data from the storage system. 
z The selected computing nodes start computing.  
z Once all computing nodes finish their work, the computed results are merged and 
stored back to the storage system.  
z For reliability, the newly stored data can be replicated to other storage resources 
by the user or the AutoReplocator service.  
 
 
Fig. 8. The Taiwan UniGrid Portal. 
5   Conclusion 
In this paper, we present the development of a high-performance virtual storage 
system for Taiwan UniGrid.  We employ SRB (Storage Resource Broker) as a basis 
to implement the functions of the storage system. Besides, we identify three main 
requirements in the current implementation: high-performance data transfer, data 
sharing, and single sign-on. To meet these requirements, we propose the 
corresponding features: Self-Adaptation for high-performance data transfer, forming 
user groups and specifying admission control for data sharing, and adopting GSI 
authentication to enable single sing-on. We also develop a Java-based user interface 
of the storage system allowing Grid users to manage their data transparently without 
concerning the low-level deployment of storage resources. In the future, we will 
continue developing new features in our system to make it more useful. On the other 
hand, we will execute more data-intensive applications on our system to examine its 
reliability and scalability. 
Optimizing Server Placement for QoS
Requirements in Hierarchical Grid Environments
Chien-Min Wang1, Chun-Chen Hsu2, Pangfeng Liu2, Hsi-Min Chen3, and
Jan-Jan Wu1
1 Institute of Information Science, Academia Sinica,Taipei, Taiwan, R.O.C.
{cmwang,wuj}@iis.sinica.edu.tw
2 Department of Computer Science and Information Engineering, National Taiwan
University, Taipei, Taiwan, R.O.C.
{d95006,pangfeng}@csie.ntu.edu.tw
3 Department of Computer Science and Information Engineering, National Central
University, Taoyuan, Taiwan, R.O.C.
seeme@selab.csie.ncu.edu.tw
Abstract. This paper focuses on two problems related to QoS-aware
I/O server placement in hierarchical Grid environments. Given a hi-
erarchical network with requests from clients, the network latencies of
links, constraints on servers’ capabilities and the service quality require-
ment, the solution to the minimum server placement problem attempts
to place the minimum number of servers that meet both the constrains
on servers’ capabilities and the service quality requirement. As our model
considers both the different capabilities of servers and the network la-
tencies, it is more general than similar works in the literatures. Instead
of using a heuristic approach, we propose an optimal algorithm based
on dynamic programming to solve the problem. We also consider the
optimal service quality problem, which tries to place a given number of
servers appropriately so that the maximum expected response time is
minimized. We prove that an optimal server placement can be achieved
by combining the dynamic programming algorithm with a binary search
on the service quality requirement. The simulation results clearly show
the improvement in the number of servers and the maximum expected
response time.
1 Introduction
Grid technologies enable scientific applications to utilize a wide variety of dis-
tributed computing and data resources [1]. A Data Grid is a distributed stor-
age infrastructure that integrates distributed, independently managed data re-
sources. It addresses the problems of storage and data management, data trans-
fers and data access optimization, while maintaining high reliability and avail-
ability of the data. In recent years, a number of Data Grid projects [2, 3] have
emerged in various disciplines.
One of the research issues in Data Grid is the efficiency of data access. One
way of efficient data access is to distribute multiple copies of a file across different
Site 0
Site 5Site 4 Site 6 Site 7
Site 10 Site 11 Site 12 Site 13
Site 3
Site 2
Site 8
Site 1
Site 9
Fig. 1. The hierarchical Grid model.
path. Upon receiving the request, the I/O server sends the requested file back to
the client site if it owns a copy of the requested file. Otherwise, it forwards the
request to its parent server. This process continues up the hierarchy recursively
until a node that has the requested file is encountered or the root node is reached.
The network latency of a I/O request from a client site to a server site can be
computed as the sum of the network latencies of all intermediate links between
both sites. The root server might update the contents of a file. For each update,
corresponding update requests are sent to the other I/O servers to maintain file
consistency. Let u be the arrival rate of update requests from the root server.
For each server site j, µ′j and λ
′
j are represented as the service rate and the
arrival rate of I/O requests of server site j respectively. λ′j can be computed as:
λ′j =
∑
i∈Cj ri + u, where Cj is the set of clients served by server site j. We
assume each server in the grid system is a M/M/1 queueing system. Thus, the
expected waiting time at server j will be 1/(µ′j − λ′j) = 1/(µ′j − u −
∑
i∈Cj ri).
To simplify the notations, we will use µj = µ′j − u and λj =
∑
i∈Cj ri as the
service rate and the arrival rate of server site j throughout this paper.
µj and λj will be used to decide the expected response times of requests it
served. Suppose the I/O requests from site i are served by server j. The expected
response time of a request from site i can be defined as the sum of the network
latencies in the path and the server j’s expected waiting time, i.e., dij + 1µj−λj .
Given the service quality requirement t, a server site j must satisfy the fol-
lowing conditions: (1) the arrival rate of all requests it served is less than its
service rate, i.e., λj < µj and (2) the expected response times of all requests it
served are less than or equal to t, i.e., maxi∈Cj{dij+ 1µj−λj } ≤ t, where Cj is the
set of clients served by server site j. Let the expected response time of serverj
be the maximum expected response time of requests it served.
3 The Minimum Server Placement Problem
In this section, we formally define the minimum server placement problem and
introduce our optimal algorithm to this problem. Our first problem is to place
the minimum number of I/O servers that will satisfy capability constrains of
servers as well as the service quality requirement from clients.
Definition 1. Given the network topology, network latencies, request arrival
rates, I/O service rates and the service quality requirement, the minimum server
Lemma 4. If ω(i,m, d1, t) = 0 for some d1, then ω(i,m, d, t) = 0 for any d ≥ 0.
Based on the above lemmas, theorems for computing the minimum arrival
rate of leakage requests can be derived. We show that it can be computed in a
recursive manner.
Theorem 1. If node i is a leaf node, then ω(i,m, d, t) = λi and Ω(i,m, d, t) is
an empty set for 0 ≤ m ≤ n, d ≥ 0 and t ≥ 0.
Proof. Since a leaf node cannot be a server, all requests generated by a client
site will travel up the tree toward the leaf node’s parent. In addition, the latency
to node i must be 0. By definition, ω(i,m, d, t) = λi and Ω(i,m, d, t) is an empty
set for 0 ≤ m ≤ n, d ≥ 0 and t ≥ 0.
Theorem 2. For an intermediate node i with two child nodes, j and k, we can
derive:
λ(i,m, d, t) = min0≤r≤m{ω(j, r, d− dji, t) + ω(k,m− r, d− dki, t)}
ω(i,m, d, t) = 0 if there exists 0 ≤ d′ ≤ t such that
λ(i,m− 1, d′, t) + 1/(t− d′) ≤ µi.
ω(i,m, d, t) = λ(i,m, d, t), otherwise.
j i010ω  (j  ,p  , d−d   , t)
    (j     ,q      , d−d      , t)ω
j i2
1ij
ω    (i , m, d, t)
λ   (i  ,p  , d, t)1 2
   (i  ,p  , d, t)λ
ikij
Sitej Sitek
(i, m, d, t) = 0ω
ω(j, p, d−d   , t)
ji(j, p, d−d   , t)ω
ω(i, m, d, t) = 0
kSitejSite
Sitej Sitek
(i, m, d, t) = 0ω
j0Site
j1Site
j2Site
iSite
Site jk−1
j0Site j1Site j2Site Site jk−1
iSite
j ik−1
  (j  ,q  , d−d   , t)
  (j  ,q  , d−d   , t)
(k, m−1−p, d−d   , t)
(e)
        (i  ,p     +q      , d, t)λ
0 1
(d)
(c)
Sitei iµ
2ω
(b)
µiiSite
2ω1ω
(a)
Sitei iµ
...
...
k−1 k−1k−1
        (i  ,p     , d, t)λk−2 k−1 k−1 k−1
22ω
1 1ω
ω
Fig. 2. (a), (b) and (c) illustrate the concept of Theorem 2. (d) and (e) illustrate
the basic concept of Theorem 3.
Proof. For node i, there are two possibilities for an optimal placement of at
most m servers:
Case 1: A server is placed on node i. At most m− 1 servers can be placed on
Tj and Tk. Suppose that, in an optimal server placement, there are p servers on
Since it is an optimal server placement, all the equalities must hold. Therefore,
this theorem holds for Case 2. Since an optimal server placement must be one
of the two cases, this completes the proof of this theorem.
Theorem 3. For an intermediate node i with k child nodes j0, . . . , jk−1, the
minimum arrival rate of leakage requests that pass through node i can be com-
puted iteratively as follows:
λ0(i,m, d, t) = ω(j0,m, d− dj0i, t)
λq(i,m, d, t) = min0≤r≤m{λq−1(i, r, d, t) + ω(jq,m− r, d− djqi, t)},
1 ≤ q ≤ k − 1,
ω(i,m, d, t) = 0 if there exists 0 ≤ d′ ≤ t such that
λk−1(i,m− 1, d′, t) + 1/(t− d′) ≤ µi
ω(i,m, d, t) = λk−1(i,m, d, t), otherwise
Proof. Fig. 2(d) and 2(e) illustrate the basic concept of this theorem. To
find an optimal server placement, we can view an intermediate node with k
child nodes in Fig. 2(d) as the sub-tree in Fig. 2(e). Then, the minimum arrival
rate of leakage requests can be computed recursively along the sub-tree. As the
detailed proof of this theorem is similar to that of Theorem 3, it is omitted here.
Theorem 4. The minimum number of I/O servers that meet their constraints
can be obtained by finding the minimum m such that ω(0,m, 0, t) = 0.
Corollary 1. Let m′ be the minimum number of servers found by the dynamic
programming algorithm. m′ grows nondecreasingly when the service quality re-
quirement t decreases.
Based on Theorems 1 to 3, we can compute the minimum arrival rates of
leakage requests that start from leaf nodes and work toward the root node.
After the minimum arrival rate of leakage requests that reach the root node has
been computed, the minimum number of I/O servers that meet their constraints
can be computed according to Theorem 4. The proposed algorithm is presented
in Fig. 3.
In the first line of the algorithm, we sort all nodes according to their dis-
tances to the root node in decreasing order. This ensures that child nodes will
be computed before their parents so that Theorems 1 to 3 can be correctly ap-
plied. The execution time of this step is O(n log n). The loop in line 2 iterates
over every node in the system. Note that there are at most n values on the
maximum latency to some node i. Thus, for each leaf node, it takes O(n2) exe-
cution time in line 4. For an intermediate node that has k child nodes, it takes
O(n3) execution time in line 9, and iterates k − 1 times in line 8. This results
in O(kn3) execution time for lines 8 to 10. Lines 11 to 16 also take O(n2) ex-
ecution time. Consequently, the complexity of lines 3 to 16 is O(kn3) and the
complexity of the whole algorithm is O(n4), where n is the number of nodes in
the Grid system. The complexity can be further reduced to O(p2n2), where p is
the minimum number of servers, by computing ω(i,m, d, t) incrementally from
m = 0 to m = p.
the maximum expected response time. We can use 1/(µmax−λmin) as a proper
lower bound, where µmax is the maximum server capability of servers and λmin
is the minimum requests of clients. A upper bound can be computed by the
following steps. First, we set t to a sufficient large value and find a server place-
ment. According to Corollary 1, the number of used servers must be smaller
than or equal to m. Then we can use the maximum expected response time of
servers as a proper upper bound. Next, we can combine a binary search of the
maximum expected response time and the dynamic programming algorithm for
the minimum server placement problem to find the optimal value of the maxi-
mum expected response time. Because the lower bound and the upper bound of
the binary search are both functions of the input parameters, the algorithm is
strongly polynomial.
5 Experimental Results
In this section we conduct several experiments to evaluate the proposed algo-
rithms. Test cases are generated based on the proposed Grid model. The height
of each case is at most 8. Each node has at most 4 children. The number of
nodes in each test case is between 1250 and 1500. The request arrival rates for
the leaf nodes and the service rates for intermediate nodes are generated from a
uniform distribution. There are four testing groups. Each group has a different
range of network latencies: 0.00005∼0.00015, 0.0005∼0.0015, 0.005∼0.015, and
0.05∼0.15. We will refer them as group 1, 2, 3 and 4, respectively. There are
1000 test cases in each group. Table 1 shows the summary of these parameters.
Table 1. Parameters of experiments
Parameter Description
Height of tree ≤ 8
Number of child nodes ≤ 4
Number of nodes in each case ≈ 1300
Range of arrival rates 1∼4
Range of service rates 50∼350
Range of network latencies 0.00005∼0.00015, 0.0005∼0.0015,
0.005∼0.015 and 0.05∼0.15
First, the experiments for the minimum server placement problem are con-
ducted. We use a greedy heuristic algorithm as a performance comparison with
our dynamic programming algorithm since, to the best of our knowledge, there
are no similar studies on QoS server placement problems that both consider
the server’s capacity and the network latency. The Greedy algorithm works as
follows: in each iteration, it first selects all candidate servers that can satisfy
the service quality requirement t, i.e., the expected response time of requests it
served will less than t. Then it selects a site who has the maximum arrival rate
of I/O requests. The process is repeated until all requests are served.
As the experiments with the four testing groups show similar results , we will
present only the result with group 4. The performance metric is the difference in
We next conduct the following experiments for the optimal service quality
problem. We compare three algorithms: (1) the DP algorithm combined a binary
search as described in Section 4, (2) the Greedy algorithm combined a binary
search and (3) a waiting-time based(WTB) server placement algorithm described
in [12]. Note that there is no guarantee of performance for the Greedy algorithm
combined a binary search since the Greedy algorithm does not have the property
of Corollary 1. A binary search is only used to adjust t such that Greedy can
generate a placement with m servers. The WTB algotithm is similar to the
algorithm described in Section 4 except it only tries to minimize the maximum
waiting time of servers.
In the experiments, for each group of test cases, we use 4 different values of
server numbers m: 60, 80, 100 and 120. The performance metric is the average of
maximum expected response times of test cases. For each test case, there will be
a maximum expected response time among those m servers. We use the average
of maximum expected response times in 1000 test cases as our performance
metric. The experimental results are shown in Fig. 5. The vertical axis shows
the average expected response time, while the horizontal axis shows the number
of servers m.
In Fig. 5, it is clear that the difference in performance between DP and WTB
becomes larger as the network latency increases. When the network latency is
small with respect to the server’s waiting time, the difference of the average
expected response time is less significant. However, as the network latency in-
creases, the difference becomes larger because the expected response time is
dominated by the network latency and WTB does not take network latencies
into consideration. This result explains the advantage of DP algorithm: it takes
both the server’s waiting time and the network latency into consideration. Thus,
DP can always get the best performance no matter the expected response time
is dominated by either server’s waiting time as the result shown in Fig. 5(a) or
the network latency as the result shown in Fig. 5(d).
In Fig. 5(c) and 5(d), Greedy has a good performance when the number of
I/O servers increases and the network latency dominates the expected response
time. This is mainly due to the power of the binary search. However, as the
expected waiting time dominates the expected response time, Greedy performs
worse than WTB as shown in Fig. 5(a). Therefore, Greedy does not perform well
in all kind of situations like DP does.
6 Conclusions
In this paper, we focus on two QoS I/O server placement problems in Data
Grid environments. We consider the minimum server placement problem which
asks how to place the minimum number of servers that meet both the con-
strains on servers’ capabilities and the service quality requirement. Instead of
using a heuristic approach, we propose an optimal algorithm based on dynamic
programming as a solution to this problem.
