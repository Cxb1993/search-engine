 2
行政院國家科學委員會專題研究計畫成果報告 
基於網際網路之社群中問題搜尋之研究 
A Study on Searching Questions in Internet-based Communities 
計畫編號：NSC 95-2221-E-019-036- 
執行期限：95 年 08 月 01 日至 96 年 07 月 31 日 
主持人：林川傑 國立台灣海洋大學資訊工程學系 
計畫參與人員： 鄭順謙、劉人瑞、陳威宇、詹嘉丞、 
林哲賢、郭育旻 
 
一、中文摘要 
 
在網際網路上的社群或論壇已成為目前
許多人發問並尋求解答的重要地方。本計畫
希望能設計出一個系統，提供使用者以自然
語言發問，再去尋找討論社群中是否已有相
似問題，如此可馬上尋得別人所提供的答
案。然而發問文中常有與主題無關的詞語，
並常有一篇發問文中提出數個問題的情形，
因此在本計畫中還需研究發問文中廢話過濾
以及問題切割的策略。此外，在比對問題時，
也需引入前行問句以解決中文中常見的主題
省略問題，才能正確地找出最相似的發問，
進而提供先前已有之解答文。 
 
關鍵詞：自動問答，自然語言處理，廢話過
濾，問句分割，問句比對，網路社群。 
 
Abstract 
 
In the Internet, communities or forums 
have become important sites to ask questions 
and get answers.  This project will study how 
to design a system which allows users to ask 
questions in natural language, searches among 
questions in the communities, and replies 
similar questions along with their answering 
posts.  However, there are often phrases 
irrelevant to the main topic seen in sentences, 
not to mention several questions being 
submitted in one post.  This project also 
studies strategies to do garbage text removal 
and question segmentation, together with 
strategies to include previous questions while 
doing question comparison in order to deal 
with the famous problem in Chinese – topic 
ellipsis. 
 
Keywords: question answering, natural 
language processing, garbage text removal, 
question segmentation, question comparison, 
Internet community. 
 
二、緣由與目的 
 
在現今資訊爆炸的社會中，如何快速搜
尋並取得資訊，一向是被關注的重要研究方
向。目前網際網路上已經存在許多知名的發
問討論區，諸知 Yahoo 知識+、Usenet 網上
論壇、英文的 Google Ask 等等。這些討論區
所針對的主題各有不同，參與者形成一個個
社群，在其上發問及解答問題。若能設計出
一個系統，提供使用者以自然語言發問問
題，再去尋找討論社群中是否已有相似問
題，將可大大縮短資訊搜尋的時間。目前幾
個著名的知識網站雖提供了關鍵詞查詢的界
面，使用者再度陷入閱讀大量文件找尋資訊
的窘境。以問句為對象的語意比對技術就顯
得更形重要。而改以問句相似度方式來比
對，觀察後又發現有些技術上的問題必須先
行研究，成為本計畫要進行的方向。 
在網際網路上之發問時，發文者會以對
象是真實人類來寫作，因此文字間會有許多
諸如「大家好」、「各位大大安安」這類問
候語；「拜託各位高手幫忙」、「有沒有誰
可以告訴我」等請託語；或是「謝謝」、「感
激不盡」等感謝語的出現。在此我們將發問
文中與發問主題無關的部份都稱為「廢話」。
如果不事先判斷廢話並將之移除，反而會變
成比對時的雜訊，影響比對正確率。本計畫
首先就針對廢話過濾的可能規則進行研究。 
在觀察中我們也發現，使用者也常常在
 4
 
在這個計畫中，我們提出了一些策略來
分割問句。總共有五個步驟的策略，而其中
又以問句型態判斷為最重要且有效的策略。
以下分別描述各個策略。 
給定一篇發問文 (包括其標題部份)，而
且已經刪去廢話之後，首先就利用標點符號
或是一些斷句線索將發問文切成許多的句
子。(一) 比對這篇文章中的所有句子，將文
字完全相同的句子刪去一份；(二) 利用自動
問答系統所使用的問句類型判斷規則來判斷
各個句子，找出具有問句型態的句子 (例如
以問號結尾，或是句中出現「誰」、「怎麼
樣」這些疑問詞的句子)，僅考慮這些句子為
可能的問題候選句，其他句子均不是；(三) 比
對各問題候選句的文字，相似度高的合併為
同一個問題；(四) 若一個問題候選句是以「例
如」、「舉例來說」這類舉例型的片語開頭
的話，就將它與前一個問題合併；(五) 利用
同於第二策略所使用的問句類型判斷規則來
判斷各個問題候選詞的問題型態 (分為問人
名、地名、時間、數量、原因、方法…等等
共十一類)，將問題型態相同的問題候選句合
併。 
套用了所有策略之後，最終所得的問題
候選句集合就是最後的問句分割結果。 
 
3.4 問句比對 
 
問句比對要考慮兩個問題：兩個句子如
何計算它們的相似度，以及在問題序列中主
題省略的情形下如何計算相似度等。 
評量兩個句子相似度的方法，在過往研
究中就已多有探討，尤其是 FAQ finder 的研
究就是這個課題。我們在此選擇了 Dice 
coefficient，也就是以句子中出現的各個詞做
為比對對象，兩個句子中重疊的詞數的兩
倍，除以兩個句子個別詞數總數和，所得的
數值做為兩個句子的相似度。然而停用詞不
做比對，不在計算之列。 
要進行主題省略時的相似度比對，就必
須引入前面句子裡提到的詞句一起比對才
行。進行了幾種實驗後，發現往前追溯，並
且設定引入長度門檻值的策略最佳。當要比
對的問句長度低於引入門值時，將前一題的
問句文字也加進來比對，重覆地加入前面的
問句直到長度超過引入門檻值為止。 
 
3.5 實驗與分析 
 
廢話過濾的實驗，評估方式是以發問文
為單位，廢話完全被刪去且未誤刪本文者才
視為正確，統計正確率。 
僅僅刪去廢話關鍵詞的話，正確率
62.08%。將出現廢話關鍵詞的句子全部刪
去，正確率 24.54%。設定過濾長度門檻值
時，若所有關鍵詞類別門檻值都設為相同，
以門檻值為 3 效能最好，有 75.46%。但各類
別設定不同門檻值可達 87.73%的正確率，效
果大幅提升。 
問句切割的評估方式，則是以各發問文
所切割出來的問題數正確與否來統計正確
率。 
僅以問號  (？) 結尾者視為問題的策
略，在訓練資料中正確率 50.67%。刪去重複
句子後，正確率 59.03%。若再改以判斷是否
為問句型態句子的策略，正確率到 64.88%。
刪去相似度較過的問句，正確率可至
75.08%。合併以「舉例」類字串開頭的問句，
正確率 75.75%。最後，合併相同問句型態的
問句，正確率改進至 88.29%。在測試資料的
正確率也有 85.87%，是最好的策略。 
問句比對的評估公式我們令為 cost，為
missing 和 false alarm 的平均。Missing 意即
標準答案的問句配對中有多少比例未被系統
找出，而 false alarm 是指系統回答的問句配
對中有多少比例並非相同問題。由上可知，
cost 越低越好。 
單獨問句間的比對，以採用 Dice 
coefficient、各詞性除停用詞 (權重為 0) 外
權重相同、不考慮各詞出現頻率者效能最
佳，cost 為 0.303。而在引入前問句的各個實
驗中，以問題為引入單位的方式比以句子為
引入單位的方式好，設定引入長度門檻的方
式又比設定權重門檻的效果，而往前追溯的
引入方式比由第一問題開始往後引入的策略
又更為佳。以上這些引入資訊的策略都比未
引入資訊的策略好。 
效能最好的策略是以問題為引入單位、
往前追溯、引入長度門檻值設為 15 個詞的策
略，cost 降為 0.267。而這時，相似度在 0.3
以上的問題會被系統標為相同問題。相似度
出席國際會議心得報告 
報告人：林川傑 (國立臺灣海洋大學資訊工程學系 助理教授) 
會議名稱：第六屆 NTCIR會議 
論文名稱：Overview of the NTCIR-6 Cross-Lingual Question Answering 
(CLQA) Task 
心得： 
NTCIR是一個由日本國立情報研究所 (National Institute of Informatics) 所
舉辦的會議，內容主題為各項資訊存取技術的評比計劃  (Evaluation of 
Information Access Technologies)，包括跨語言資訊檢索技術  (Cross-Lingual 
Information Retrieval Task)、跨語言自動問答技術  (Cross-Language Question 
Answering Task)、專利檢索技術 (Patent Retrieval Task)、多平台趨勢資訊自動摘
要技術 (Multimodal Summarization of Trend Information) 以及意見資訊搜尋前期
技術 (Pilot: Opinion Task)。 
由於是跨語言技術的開發，語言種類涵蓋中文、英文、日文、韓文等，參加
隊伍更來自世界各地對亞洲語言有興趣的多個研究團隊，不乏英語系國家的研究
人員。這個會議提供了一個絕佳的溝通環境，會議的主題集中在幾個熱門或是尖
端的研究主題上，參會者的研究興趣相近，又能與來自不同國家的人相互切磋。 
東亞語言向來與拉丁語系的語言有著很大的不同，研究過程中，往往東亞語
言因其獨特的語言現象，必須比拉丁語系處理更多的問題。在拉丁語言所研究得
到的結論，無法全盤套用到亞洲語言來。因此更可突顯這個會議的重要性，也能
期待此會議對亞洲語言的自然語言處理技術帶來加速進展的可能性。 
這次本人在第六屆 NTCIR 會議所發表的論文題目是「Overview of the 
NTCIR-6 Cross-Lingual Question Answering (CLQA) Task」，主要是針對跨語言自
動問答技術評估環境建置的相關研究，包括如何蒐集中文問答集、英文問答集，
以及日文問答集，並且研究問答集建置的流程，使得三種不同語言的問答集能擁
樣同樣的問題集。也就是說，一個同樣的問題，能夠在三種不同語言的新聞語料
集中找尋到答案。問題與答案使用不同的語言，這樣就能進行跨語言自動問答的
研究。例如問句是以英文寫成，但是答案卻是在中文的新聞文章中尋得。 
在這次會議中，和來自日本、臺灣、美國、中國大陸、新加坡許多研究自動
問答技術的團隊就著這個主題做了一場討論會，並在會前會後時常有意見交流。
大家對於這項評比的建置多持正面的看法，對於以後的研究方向也給了許多的意
見。期待能再參加下一屆 NTCIR會議，能見到更新更具突破性的發展。 
2. Japanese Dataset 
 
Mainichi Newspaper Article Data 2000 – 2001  
Yomiuri Newspaper Article Data 2000 – 2001  
 
3. English Dataset 
 
Daily Yomiuri 2000-2001 
 
(Test) 
1. Chinese Dataset (traditional) 
 
CIRB020: United Daily News, Economic Daily 
News, Min Sheng Daily, United Evening News, Star 
News 1998-1999 
 
2. Japanese Dataset 
 
Mainichi Newspaper Article Data 1998 – 1999  
  
3. English Dataset 
EIRB010: Taiwan News; China Times English 
News 1998-1999 
Mainichi Daily News 1998-1999 
Korea Times 1998-1999 
Hong Kong Standard 1998-1999Corpora  
 
Scope of answers 
 
Each question has only one answer or no answer.  
Answers are restricted to Named Entities: proper 
nouns, such as the name of a person, an organization, 
various artifacts, and numerical expressions, such as 
money, size, date, etc. 
Defining NEs is a very heavy task.  So, we use 
the conventional one for Japanese.  Japanese NEs 
were clearly defined in the NE task of the IREX 
Workshop [4] 
(http://nlp.cs.nyu.edu/irex/index-e.html) 
 
The NEs defined by IREX are: 
 
- PERSON 
- LOCATION 
- ORGANIZATION 
- ARTIFACT (product name, book title, law, ...) 
- DATE 
- TIME 
- MONEY 
- PERCENT 
 
We adopt these NEs plus NUMEX for CLQA: 
 
- NUMEX 
 
We introduced NUMEX to cover various kinds of 
numerical expressions other than MONEY and 
PERCENT. 
 
 (Exception 1) 
We allow an expression of approximation to be 
included in answers, such as "about 10" and "more 
than three" following NTCIR QAC.  Basically, the 
definition of Chinese and English NEs followed the 
suite. 
 
Question construction 
 
Question construction is the most tricky and 
difficult part in carrying out an evaluation campaign 
on CLQA.  It is ideal that questions and their 
answers are all parallel between three languages, 
Chinese, English, and Japanese.  However, it is not 
easy to find news articles that report the same topics 
in the three languages.  Even if we can find such 
articles, most of the topics are big news and could be 
easily predicted by task participants. 
We improved the question creation method from 
NTCIR-5 CLQA to NTCIR-6 CLQA as follows: 
 
(A) Japanese-related CLQA 
 
(NTCIR-5 CLQA) 
- Since the Daily Yomiuri articles are English 
translations of Yomiuri Shimbun articles, we first 
manually selected corresponding articles between the 
two corpora, then created an English question by 
reading an article of the Daily Yomiuri.  A Japanese 
question of the English question was created by 
referring to the corresponding Japanese article.  
Thanks to this process, the question/answer pairs of 
JE and EJ subtasks are parallel.  
 
(NTCIR-6 CLQA) 
- A drawback of the above approach was that the 
questions were strongly affected by written styles of 
news articles.  Therefore, in NTCIR-6, we decided 
not to enjoy the parallel articles between Japanese 
and English.  First we created 300 English questions 
and answers by referring to the English corpus, 
regardless whether or not the articles are English 
translation of Japanese Newspaper.  Then, we 
searched Yomiuri Shimbun articles containing the 
same topic as the 300 English questions and created 
200 Japanese questions and answers.  Since 
Japanese test corpora is Mainichi Shimbun, we again 
search articles that can answer the Japanese questions 
in Mainichi.  The Japanese questions that found in 
Mainichi were used in the formal run.  We created 
new questions and answers by refereeing to the 
English corpus and Mainichi Shimbun.  Thanks to 
this process, Japanese questions do not resemble the 
sentences in Japanese test corpus. 
(B) Chinese-related CLQA 
 
(NTCIR-5 CLQA) 
- C-E subtask: Chinese questions of the C-E 
subtask are Chinese translations of the English 
Table 2. Number of runs submitted by participants in Japanese-related subtasks 
E-J J-J J-E 
Group 
official unofficial official unofficial official unofficial 
Forst 3 3 1 1   
HARAD   1 1   
LTI 3 2  2   
TITLF 3  3    
TTH 3 3 3 3 1 1 
Total 12 8 8 7 1 1 
 
Table 3. Number of runs submitted by participants in Chinese-related subtasks 
E-C C-C C-E E-E 
Group 
official unofficial official unofficial official unofficial official unofficial 
IASL 3 4 3 7     
ICDCU   1      
ILS 1       
LTI 3 2 3 2     
MHC06 2  2    3 1
NCUTW 1  1    
pircs 3 3 3 3    
WMMKS 1  1   1   
Total 14 9 14 12 0 1 3 1
Each question has only one answer or no answer.  
Answers are restricted to named entities: proper 
nouns, such as the name of a person, an organization, 
various artifacts, and numerical expressions, such as 
money, size, date, etc. 
 
Data specification 
 
In CLQA, the character encoding of the input was 
BIG5 for Chinese, US-ASCII for English, and 
EUC-JP for Japanese.  The input format of CLQA is 
defined as follows. 
 
[QID
 
[QID] 
[Que
ionN
[Questio
[Lang] i
[Questio
num
two 
sam
[Questio
 
Exam
  CL
 
The 
for Chin
Japanes
format. 
  
[QID],[Lang](,"[Answer]",[ArticleID],[Reserved],[Reserved])* 
 
[QID] is the same as in the question file format above.  
It must be unique in the file, and ordered 
identically within the corresponding question file.  
It is, however, allowed that some of the [QID]s 
are not listed in the file. 
[Lang] is one of JA, ZH, and EN. 
[Answer] is the answer to the question, and a 
character string. 
[ArticleID] is the identifier of the article or one of the 
articles used in the process of deriving the answer.  
 for the 
M0398, , 
年 ",  
年 ", 
he test 
 encoded 
tion for ]: "[Question]" 
is the form of 
stionSetID]-[Lang]-[QuestionNo]-[SubQuest
o]. 
nSetID] is "CLQA2". 
s one of JA, ZH, and EN. 
nNo] and [SubQuestionNo] consist of four 
eric characters starting with "S" or "T" and 
numeric characters, respectively. ("S" is for 
ple questions and "T" for test questions.) 
n] is a character string. 
ple: 
QA-EN-S0001-00: "When Queen Victoria died?" 
character encoding of the output was BIG5 
ese, US-ASCII for English and EUC-JP for 
e.  CLQA defined the following output 
The value of the <DOCNO> tag is used
identifier. 
[Reserved] is a field for the future use. 
 
(Example) 
 
CLQA-EN-S0001-00, EN, "1901",  ENY-20001101CY
CLQA-EN-S0001-00, JA, " １ ９ ０ １
JAY-20001101CYM0398, , , " 一 九 〇 一
JAY-20001101CYM0398, , 
 
Considering language scalability, t
collection, i.e., a set of gold standard files, is
in UTF-8.  The format of the test collec
CLQA is defined as follows: 
 
<?xml version="1.0" encoding="UTF-8"?> 
<QASET> 
<VERSION>[Version]</VERSION> 
 
<QA> 
<QUESTION> 
<QTYPE>[QType]</QTYPE> 
<Q LANG="[Lang]" QID="[QID]">[Question]</Q> 
Doe",  mhn_xxx_20010808_1034915, ,  
 
The first line is an answer in the target language, 
and the second line is its translation. 
 
Technique description 
 
In addition to search results, each participating 
group submitted a file with the filename 
"GRPID-TechDesc", which was a concise technique 
description for each submitted run.  As mentioned 
above, GRPID is the group ID.  In general, this file 
should contain the following information. 
 
RunID: as explained in the RunID Section. 
IndexUnit: character, bi-character, bi-word, phrase, 
etc. 
IndexTech: the techniques used to process index 
terms, 
e.g., morphology, stemming, POS, etc. 
IndexStruc: inverted file, signature file, PAT, etc. 
QueryUnit: character, word, phrase, etc. 
IRModel: vector space model, probabilistic model, 
etc. 
Ranking: ranking factor for measuring each term, e.g., 
tf, tf/idf, mutual information, word association, 
document length, etc. 
QueryExpan: techniques used to expand query or no 
query expansion 
TransTech: the translation technique used to deal 
with cross-language information retrieval, e.g., 
dictionary-based, corpus-based, MT, etc. The 
more detailed the information the better, e.g., 
select-all, select-top-N, WSD, etc. 
 
5. Evaluation method 
 
Each answer response [Answer, DOCNO] was 
judged.  There are three scores used in evaluation: 
 
Right (R): the answer is correct, and the document 
where it is from supports it. 
Unsupported (U): the answer is correct, but the 
document where it is from cannot support it as a 
correct answer.  That is, there is no sufficient 
information in the document for users to confirm by 
themselves that the answer is a correct one. 
Wrong (W): the answer is incorrect.  Note that 
even if a substring of an answer response is provided 
as a correct answer, it will not be judged as a correct 
one.  The same is true for an answer response which 
is a substring of a real answer. 
The assessment of the runs of J-E/E-J/J-J subtasks 
was conducted independent of the organizers by a 
Japanese company specializing foreign language 
communication. 
The assessment of E-C/C-C/C-E/E-E subtasks was 
conducted as follows.  Each of the [answer, docID] 
pairs proposed by the participants (in official or 
unofficial runs) was judged by three assessors.  
M rs 
s e 
f
e 
a es 
f
h 
t
e 
r ct 
a
ct 
a
6
6
-J 
s
al 
r st 
o e 
a d 
a rs 
w al ajority was taken as its score.  If three assesso
cored them differently, the organizer would do th
inal judgment. 
Evaluation results were scored by using th
ccuracy for official runs, and MRR and Top5 scor
or unofficial runs. 
 
Accuracy (Top1): is the rate of questions whic
op 1 answers are correct. 
 
MRR (Mean Reciprocal Rank): is the averag
eciprocal rank (1/n) of the highest rank n of a corre
nswer for each question. 
 
Top5: shows the rate at which at least one corre
nswer is included in the top 5 answers. 
 
. Evaluation results 
 
.1. Results of J-E/J-J/E-J subtasks 
 
Tables 4 show the evaluation results of J-E/E-J/J
ubtasks. 
In E-J subtask, 12 official runs and 8 unoffici
uns were submitted from 4 groups.  The be
fficial run was submitted by Forst group.  Th
ccuracy was 17.5% with counting only supporte
nswers.  It rises to 19.5% if unsupported answe
ere considered correct as well.  The best officiTable 4. Japanese-related CLQA accuracy 
Run ID Right 
Right + 
Unsupported 
Forst-E-J-01 0.175 0.195 
Forst-E-J-02 0.170 0.180 
Forst-E-J-03 0.170 0.180 
Forst-J-J-01 0.310 0.335 
HARAD-J-J-01 0.085 0.110 
LTI-E-J-01 0.095 0.115 
LTI-E-J-02 0.095 0.115 
LTI-E-J-03 0.070 0.115 
LTI-J-J-u-01 0.335 0.360 
LTI-J-J-u-02 0.250 0.320 
TITFL-E-J-01 0.020 0.040 
TITFL-E-J-02 0.030 0.065 
TITFL-E-J-03 0.030 0.060 
TITFL-J-J-01 0.155 0.190 
TITFL-J-J-02 0.130 0.160 
TTH-E-J-01 0.130 0.165 
TTH-E-J-02 0.120 0.155 
TTH-E-J-03 0.070 0.100 
TTH-J-J-01 0.245 0.270 
TTH-J-J-02 0.235 0.260 
TTH-J-J-03 0.270 0.295 
the translation of “失業率” Two Japanese questions 
also have slightly different expression “日本の” and 
“日本における”. Interestingly, no system could 
find a correct answer for T1123 where as official runs 
of Forst and an unofficial run of LTI could spot 
correct answer “4.1%” for T1054. This could be 
“jobless rate” was harder to translate than 
“unemployment rate”.  More interestingly, no 
system could answer these questions in the J-J 
monolingual setting.   
 
7.2. Chinese-related CLQA 
 
The ability to identify named entities is important 
in QA.  Most teams can extract popular named 
entities like person, location, and organization names.  
Teams who did not develop NE identifiers for other 
named entity types failed to answer questions of 
these types, as shown in Table 6 and Table 7.  It is 
also true for temporal and numerical expressions. 
If we only focus on questions of the types 
PERSON, ORGANIZATION, LOCATION, and 
DATE (the four largest subsets), we can see that 
modules other than NE identification also play 
important roles which make some systems more 
effective. 
For EC-subtask, most of the participants used 
available translation systems to translate English 
questions into Chinese, and then proceeded as doing 
monolingual QA.  Translation of unknown words 
especially for named entities became a crucial 
problem in CLQA. 
The effect of translation is not always negative.  
The question sets of EC and CC subtasks are made 
parallel (thus questions with QID's same in the last 
three digits are translations of each other) in order to 
compare works in MLQA and CLQA.  It is 
interesting that some questions can be answered 
correctly in CLQA but not in MLQA.  For examples, 
both LTI and PIRCS Group can correctly answer 16 
questions in such a case.  The reason is not clear.  
Maybe different translations happen to be synonyms 
or paraphrases which benefit in finding answers. 
 
8. Conclusion 
 
This paper described an overview of NTCIR-6 
CLQA.  In the Formal Run, 12 groups world-wide 
participated in CLQA and submitted 91 runs in total.  
Evaluation results showed that the performance of 
CLQA systems were heavily degraded compared to 
monolingual QA systems. 
However, CLQA is a new research area and low 
performance implies that there is a lot of room to 
improve the performance.  It is necessary to 
continue NTCIR CLQA Tasks to expand the CLQA 
test collection as a common infrastructure and as a 
test bed for researchers in Cross-Lingual QA. 
 
Acknowledgment 
 
We thank all the participants to the NTCIR-6 
CLQA task. 
Part of the research project carried out in Taiwan 
is partially supported by Excellent Search Projects of 
National Taiwan University under grant no. 
95R0062-AE00-02 and the Teaching Excellence 
Project in National Taiwan Ocean University. 
 
Bibliography 
 
[1] Bernardo Magnini, Simone Romagnoli, 
Alessandro Vallin, Jesus Herrera, Anselmo Penas, 
Victor Peinado, Felisa Verdejo, and Maarten de 
Rijke, The Multiple Language Question 
Answering Track at CLEF 2003, Working Notes 
for the CLEF 2003 Workshop, 2003. 
[2] Chuan-Jie Lin, A Study on Chinese Open-Domain 
Question Answering System, Ph.D. dissertation, 
National Taiwan University, 2004. 
[3] Yutaka Sasaki, Hsin-Hsi Chen, Kuang-hua Chen, 
Chuan-Jie Lin, Overview of the NTCIR-5 
Cross-Lingual Question Answering Task (CLQA), 
In Proc of NTCIR-5 Workshop Meeting, Tokyo, 
2005. 
[4] Satoshi Sekine and Yoshio Eriguchi, Japanese 
named entity extraction evaluation ― analysis of 
results ―,in Proc. of 18th International 
Conference on Computational Linguistics, pp. 
1106-1110, 2000. 
Table 6. Accuracy (%) by question types (CC) 
QType #Q IASL ICD 
CU 
LTI MHC NCU 
TW 
pircs WMM
KS
ARTI 7 28.57 14.29 14.29 0.00 0.00 42.86 0.00 
DATE 39 43.59 33.33 25.64 25.64 0.00 46.15 5.13 
LOC 16 87.50 50.00 31.25 18.75 25.00 50.00 37.50 
MONY 8 12.50 0.00 12.50 12.50 0.00 25.00 0.00 
NUMX 11 27.27 0.00 27.27 0.00 0.00 0.00 0.00 
ORG 16 56.25 31.25 18.75 12.50 18.75 31.25 12.50 
PRCT 4 25.00 0.00 0.00 0.00 0.00 0.00 0.00 
PRSN 47 65.96 34.04 31.91 25.53 12.77 57.45 21.28 
TIME 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
Total 150 52.00 28.67 25.33 18.67 8.67 42.00 13.33 
 
Table 7. Accuracy (%) by question types (EC) 
QType #Q IASL ILS LTI MHC NCU 
TW 
pircs WMM
KS
ARTI 7 28.57 0.00 0.00 0.00 0.00 0.00 0.00 
DATE 39 15.38 10.26 20.51 7.69 0.00 41.03 2.56 
LOC 16 37.50 6.25 18.75 0.00 0.00 43.75 12.50 
MONY 8 0.00 12.50 0.00 0.00 0.00 37.50 0.00 
NUMX 11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
ORG 16 25.00 12.50 18.75 0.00 0.00 31.25 0.00 
PRCT 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
PRSN 47 42.55 12.77 17.02 6.38 0.00 10.64 10.64 
TIME 2 0.00 0.00 0.00 0.00 0.00 100.0 0.00 
Total 150 25.33 9.33 14.67 4.00 0.00 25.33 5.33 
J-u-02 annotations -named 
entities, 
predicate-a
rgument 
file Phrase, window 
operator, 
synonym 
operator, weight 
operator 
Modeling and 
inference network 
(Indri)Block (3 
sentence chunk) 
retrieval 
entropy 
(Indri) 
expansion, 
Predicate-arg
ument 
expansion 
TITFL-
E-J-01 
character+bi-
character+tri
-character 
character 
segmentati
on 
inverted 
file 
character+bi-ch
aracter+tri-char
acter 
vector space 
model tf/idf no web translation tool 
TITFL-
E-J-02 
character+bi-
character+tri
-character 
character 
segmentati
on 
inverted 
file 
character+bi-ch
aracter+tri-char
acter 
boolean 
model+vector 
space model 
tf/idf no web translation tool 
TITFL-
E-J-03 
character+bi-
character+tri
-character 
character 
segmentati
on 
inverted 
file 
character+bi-ch
aracter+tri-char
acter 
boolean model tf/idf no web translation tool 
TITFL-
J-J-01 
character+bi-
character+tri
-character 
character 
segmentati
on 
inverted 
file 
character+bi-ch
aracter+tri-char
acter 
vector space 
model 
tf/idf no n/a 
TITFL-
J-J-02 
character+bi-
character+tri
-character 
character 
segmentati
on 
inverted 
file 
character+bi-ch
aracter+tri-char
acter 
boolean 
model+vector 
space model 
tf/idf no n/a 
TTH-E
-J-01 
English word stemming 
inverted 
file 
English word 
vector space 
model 
SMART no statistical MT based 
TTH-E
-J-02 
English word stemming 
inverted 
file 
English word 
vector space 
model 
SMART no statistical MT based 
TTH-E
-J-03 
Japanese 
word, 
bi-word, 
bi-char 
 
morphology 
inverted 
file 
Japanese word, 
bi-word, bi-char
OKAPI BM25  TF.IDF
pseudo 
relevance 
feedback 
transliteration and 
dictionary based 
TTH-J-
J-01 
Japanese 
word, bi-word 
morphology 
inverted 
file 
Japanese word
vector space 
model 
SMART  no  no 
TTH-J-
J-02 
Japanese and 
English word 
 
morphology, 
stemming 
inverted 
file 
 Japanese and 
English word 
vector space 
model SMART
English word 
translation MT based 
TTH-J-
E-01 
Japanese 
word 
morphology 
inverted 
file 
Japanese word
vector space 
model 
SMART no statistical MT based 
 
Table 9. Technique descriptions of runs in Chinese-related subtasks 
RunID Index 
Unit 
Index 
Tech 
Index 
Stru 
Query 
Unit 
IR 
Model 
Ranking Query 
Expan 
Trans 
Tech 
IASL-C
-C-01 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No N/A 
IASL-C
-C-02 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No N/A 
IASL-C
-C-03 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No N/A 
IASL-E
-C-01 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No Google Translate 
IASL-E
-C-02 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No Google Translate 
IASL-E
-C-03 
Word and 
Char 
N/A 
Inverted 
File 
Word and 
Char 
Vector space tf/idf No Google Translate 
ICDCU-
C-C-01 
word 
Stopword 
+dictionary 
Inverted 
file 
Word Probabilistic BM25 No QE 
NTCIR6
-ILS-R
un1 
word N/A N/A N/A Lucene N/A N/A AltaVista Babel Fish 
MHC word N/A N/A 
Structure
d query 
operators
Hanquery N/A N/A Dictionary, bi-words 
LTI-C-
C-01 
Word, 
annotatio
ns 
Word - 
segmented, 
annotations 
- named 
entities 
Inverted 
file 
Word Language 
Modeling and 
inference 
network 
(Indri) 
KL/cross-entropy
(Indri) + maxent 
based 
discriminative 
reranking 
N/A N/A 
