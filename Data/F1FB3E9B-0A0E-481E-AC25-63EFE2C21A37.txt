中文摘要 
 
分支預測是一個可以提升超純量處理器系統的效能的重要機制。雖然預測的精確度越
來越高，但是在高發行率與深管線超純量架構中，預測錯誤仍然會導致相當大的效能流失。
對於這個問題，雙重路徑執行技術在之前已經被提出，其重點是當遇到低可信度的條件分
叉指令時，系統硬體會同時執行兩條路徑上的指令。然而因為不同路徑的指令會共用且穿
插使用同一個暫存器更新元件，因此當有一個條件分支指令被解決且所有錯誤路徑的指令
被移除後，將會留下空的硬體位置，而且這些位置一直到確認階段前均無法被其他指令所
使用。暫存器更新元件的大小是影響效能相當重要的因素，假設可以重新使用這些被解決
或移除的硬體位置，那麼能提高多少的效能將是我們感到興趣的。因此，在有限的硬體資
源中，我們提出了一個簡易的機制去達成這個目標，並評估其效能增益。 
另外，因為資料預先擷取的時間點會影響效能的好壞，所以我們提出多路徑資料預先
擷取的機制，可以提前在遇到分支時，針對兩條路徑上所有的載入指令做資料預先擷取，
並且分析出與其他資料預先擷取機制之間效能的差異。除此之外，我們也探討了資料預先
擷取機制在多重路徑執行技術的影響性。 
 
關鍵詞：分支預測，超純量處理器，資料預先擷取。 
 
英文摘要 
 
Branch prediction is a key mechanism to boost the system performance of a superscalar processor. 
Though the prediction accuracy rate becomes higher and higher, the mispredicitons still lead to 
significant performance losses in a wide-issue deep-pipelined superscalar. To address the problem, 
the technique of multipath execution has been proposed previously, which is capable of executing 
both paths whenever a lower-confidence conditional branch is encountered.  However, because 
the instructions from different paths share a single register update unit (RUU), they are 
interleaved in the RUU. In consequence, when a conditional branch is resolved and the 
instructions on the wrong paths are squashed, all the entries in the resulting holes cannot be 
reused until they are reclaimed at the commit stage. Since the RUU size is crucial to the 
performance, it is interesting to know how much can we speedup the performance if the squashed 
RUU entries can be reused as soon as possible. We have proposed a simple mechanism with very 
limited hardware resources to achieve this goal. Finally the preliminary simulation results are 
presented.  
 Moreover, because the technique of data prefetch is capable of improving system 
performance, we have proposed a multiway data prefetch technique to prefetch data from two 
different paths whenever a conditional branch is encountered. The advantage is that data can be 
available no matter which path is the correct one. We also have studied the performance 
differences for various prefetch technologies.    
 
Keywords: Branch prediction, superscalar processor, data prefetch.
(SMT)處理器[11]中所支援的硬體內容（hardware contents）比實際上使用的引線（threads）
多時，系統會沿著那些難以預測的分支路徑做切割內容並抓取且執行。當交替的路徑大量
產生時，閒置的內容會被配置以執行新的路徑，但其必須先複製分支之前的資訊狀態。因
此 TME 提出 the Mapping Synchronization Bus 去複製目前在 thread 內容之間的規劃區域。
The selective eager execution 與 PolyPath 架構[12-14]皆使用了標籤指令及暫存器重新命名機
制以便在相同處理器管線內同時執行多路徑指令。另一方面，暫存器更新元件(Register 
Update Unit, RUU)結合 reservation station 和 reorder buffer 的功能，來自於不同路徑的指令
同時存在並相互穿插於同一個 RUU 中。動態條件執行(DCE) [15]採用一個靜態選擇機制，
其於編譯程式時間中根據目標區域性標示分支指令的猜測情形。因此，DCE 是一個靜態選
擇機制與硬體支援分支指令多重路徑執行技術。為了減少 thread 的大量產生，disjoint eager 
execution [16]從多個分支中挑選出一個可信度最低的分支，然後複製此分支路線，不再對
每個分叉指令做猜測。另外，instruction window 被分為好幾個部分，每個部分分配給一個
thread。The adaptive branch tree (ABT)不是採用靜態分支樹，相反的，當一個分支指令被解
決時，它便藉由位移的方式重新配置 ABT 分支樹[17]。對 VLIW 處理器來說，Shimajiri 和
Yoshida 則提出合併來自兩條路徑的指令到同一長指令，藉以盡可能的消除 NOP 的微指令
[18]。 
而在資料預先擷取的機制中，最簡單的作法就是同一 PC(program counter)下，根據 load 
address 的差距(stride)，判斷是否有空間區域性的現象。如果有，則去做資料預先擷取。此
種方式稱為 stride prefetch[6]。在資料預先擷取時，時間點也是一個重要影響的因素，若資
料預先擷取的時間太晚，會造成資料尚未放入快取記憶體中，而仍然會產生快取記憶體失
誤；但若資料預先擷取的時間太早，則又會有被其他資料取代的可能。除此之外，資料預
先擷取的數量也是影響效能的主要因素。資料預先擷取的機制會將資料從記憶體抓到快取
記憶體中，往往會因為快取記憶體的容量限制，造成有些資料會被取代掉，而取代掉的資
料若是有用的，則會造成額外的快取記憶體失誤，所以在做資料預先擷取時，可以利用資
料流緩衝區來存放預先擷取的資料。當發生快取記憶體失誤時，會先到這個資料流緩衝區
搜尋資料，如果找不到再去記憶體抓資料，額外多出此架構的好處是可以減少資料預先擷
取所造成快取記憶體污染的程度。然而，資料流緩衝區的放置有兩種，第一種放置位置是
L2。當 L1 快取記憶體發生失誤時會對 L2 快取記憶體以及資料流緩衝區搜尋是否有資料。
如果在 L2 快取記憶體以及資料流緩衝區都找不到資料，會先去記憶體中將資料放入 L2 快
取記憶體，此時再根據資料預先擷取的機制決定是否將的資料放入 L2 資料流緩衝區中。第
二種方式則是在 L1，其資料預先擷取的做法與第一種方式類似，處理器會直接對 L1 快取
記憶體以及資料流緩衝區去搜尋是否有資料。如果在 L1 快取記憶體以及資料流緩衝區都找
不到資料，會先去 L2 快取記憶體中將資料放入 L1 快取記憶體，此時再根據資料預先擷取
的機制決定是否要將資料放入資料流緩衝區中。 
當發生快取記憶體錯誤時，要花許多時脈(cycle)等待資料從記憶體放到快取記憶體
中，才可以讓之後的指令執行，若此時能夠猜測出此資料的值，並且用預測的值讓之後的
指令去做猜測性執行(speculative execution)，此種機制稱為 Early load retirement[19]。 
 
研究方法 
 
在原本的 RUU 設計中，藉由首和尾的指標來維持指令分配的程序順序。首指標代表第
一個還未 commit 的指令，而尾指標代表下一道指令將分配進入的第一個 RUU entry。但是，
因為 RUU 被作用為一個 ring，所以為了避免覆寫首指標所指著的 RUU entry 之內容，尾指
 圖 2. 包含 squashed 指令的 RUU 快照 
 
 圖 3. 在尾指標超越首指標之後 RUU 兩個運作之快照 
當首指標指向的指令從 RUUretired 時，相對應的 entry 則為無效。而且，首指標向前
走以找到下一個有效 entry，即是 entry 之 ordering bit 相等於 head-ordering bit。即使首指標
已超越尾指標後，程序仍繼續，如圖 4 所示。為了簡化我們的設計，當 pass pointer 不是 null，
禁止尾指標再次超越首指標。 
 
圖 4. committed 指令時 RUU 的快照 
在首指標因為指令從 RUU 被 retired 而超越 the pass pointer 之後，the pass pointer 變成
null 且 head-ordering bit 取補數從 0 到 1 如圖 5 所示。然後，首指標尋找下一個有效 entry，
其 the ordering bit 等於 the head-ordering bit，即 1。目前，在 RUU 中只有一圈，因為 the 
head-ordering bit 和 the tail-ordering bit 相等如圖 5 所示。 
 
圖 5. 首指標超越 the pass pointer 時 RUU 的快照 
每當錯誤預測發生或是一個條件分支被解決，在錯誤路徑上所有的指令將需要被
squashed。The path ids 將被使用來識別錯誤路徑上的指令。而且，the pointers 和 the ordering 
bits 也必須被回復。如果 the pass pointer 是 null，也就是只有一圈，則只有尾指標需要被回
復。否則，發生預測錯誤的條件分支指令與 the pass pointer 指向的指令的 the ordering bit 是
否相同，以決定該條件分支指令是否在第一圈。如果它們是相同的，則這分支指令是在第
表 1. 參數表 
Parameter Value 
Process core 
Window size(RUU) 256 inst. 
LSQ size 128 
Fetch Queue Size 32 inst. 
Fetch width 16 
Decode width 16 
Issue width 16 
Commit width 16 
Branch prediction 
Branch predictor 
 
 
Hybrid :  
(4K x 2-bit Bimodal, 
1K x 2-bit 2level2) 
BTB 512 entries, 4 sets 
RAS 8 entries 
 我們擴充 Hydrascalar 架構模擬工具以研究在多路徑執行中 RUU 大小的影響性。The 
baseline 的模擬處理器的規格如表一所示，此為 Hydrascalar 模擬器所提供之真實設定檔，
細節如下。The RUU, the LSQ, 和 the fetch queue 的大小分別為 256, 128, 和 32。The fetch, 
decode, issue 和 commit width 都是 16。整數元件，branch/I-shift 元件，整數乘/除法器，memory 
system load port，memory system store port，浮點數元件，浮點數乘法器，浮點數除法器/平
方根元件的總個數分別為 4, 4, 1, 2, 1, 4, 2, 1。分支預測機制是藉由 4K x 2-bit Bimodal, 1K x 
2-bit, two levels 混合而成。The branch target buffer 有 512 entry with four sets。第一層的指令
和資料 cache 為：256K, 2-way (LRU), 16B blocks, 1-cycle latency。第二層採用 unified cache，
16M, 4-way (LRU), 32B blocks, and 8-cycles latency。記憶體的 latency 為 128-cycle。信心預
測方針採用 The ones-counting [8]。在 每個 entry 是一個 n-bit
的移位暫存器。這些 bits 表示分支預 示準確的預測。如果 1
的數量不超過所設定之門檻值，則表 以如果存在有空的硬體
內容，CPU 會 forks 新的引線以同時執 ies，每個 entry 有 8 bits
且門檻值為 6。  
圖 7 顯示不同的 RUU 大小：64, 對於這六個 benchmark
而言，128 entries 的 RUU 有最好的 ctional 元件的數量範圍
只從 1 至 4，所以對於 256-entry RUU 較好的效能。我們相信
在硬體資源被提升之後，我們的方法ones-counting 方法中，表格內的
測最後 n 個分支之準確度。1 表
示信心預測器有低的信心，所
行兩條路徑。這表格有 2K entr
 128 和 256 所對應的效能增益。
效能增益。但是，因為每個 fun
的處理器我們的機制不能提供
能夠獲得更好的 speedups。 
00.5
1
1.5
2
2.5
swi
m
mg
rid app
lu gcc am
mp mc
f
IP
C
multiway prefetch p=
無限
multiway prefetch
p=2
multiway prefetch
p=4
 
圖9 在多路徑資料預先擷取中限制路徑的個數 
我們也在 hydrascalr 實驗出不同執行緒個數下效能變化，如圖 9，1T 代表 1 個執行緒、
2T 代表 2 個執行緒、4T 代表 4 個執行緒、p2 表示 1 個執行緒中加上多路徑資料預先擷取
且限制路徑的個數為 2。我們可以看出 p2 的 IPC 都稍微比 2T 高，甚至於比 4T 都還高，然
而，2T、4T 所用的資源皆比 p2 還要多。如此可以說明使用多路徑資料預先擷取時，若加
上路徑數量的限制，可以比多路徑執行的效能還要好，且付出的成本也比較低。 
我們嘗試將資料預先擷取的各種機制去結合 early load retirement 的效能改善表現，如
圖 10 所示，其中 rate = [(ER+各種機制的 IPC)/(ER 的 IPC)]/[(各種機制的 IPC)/(base 的 IPC)]。
我們看到實驗中使用 early load retirement 並沒有太多改善空間，甚至於在 mcf 的標準程式
中還會造成改善程度下降。 
 
圖9. Hydrascalar中不同執行緒的IPC 
 
圖10 early load retirement的改善程度 
 
 
[16] A.K. Uht, V. Sindagi, K. Hall, “Disjoint eager execution: an optimal form of speculative 
execution,” Proceedings of the 28th Annual International Symposium on Microarchitecture, 
pp.313 – 325, 1995.  
[17] Tien-Fu Chen, “Supporting highly speculative execution via adaptive branch trees,” Fourth 
International Symposium on High-Performance Computer Architecture, pp.185 – 194, 1998.  
[18] H. Shimajiri, T. Yoshida, “A method of speculative dual-path execution for VLIW 
processors,’ TENCON 2004, Vol. 2, pp. 195 – 198, 2004. 
[19] Nevin Kirman, Meyrem Kirman, Mainak Chaudhuri, and Jos´e F. Mart´inez, “Checkpointed 
early load retirement”, proceedings of the 11th symposium on high-performance computer 
architecture, 12-16 Feb. 2005, pages:16 - 27 
[20] 劉恩豪，超純量架構中資料預先擷取計數之影響性研究，碩士論文，國立彰化師範大學
資訊工程學系，2007。 
[21] Chao-Chin Wu, Kuan-Chou Lai, En-Hao Liu1, Jin-Yuan Chen, “The Impact of the Register 
Update Unit Size on Multipath Execution,” Proceedings of 2007 IEEE Pacific Rim 
Conference on Communications, Computers and Signal Processing, PACRIM' 07, 22-24 
August, Victoria, B.C., Canada, 2007. 
 
計畫成果自評 
 
研究成果包括了計畫書中所擬定要探討的議題：有效的使用 RUU 及結合預先資料擷取
與多重路徑執行。另外，我們也探討了最新技術 early retired load 與預先資料擷取技術之互
相間的影響性。我們將成果發表於 2007 IEEE Pacific Rim Conference on Communications, 
Computers and Signal Processing 國際研討會中，並詳述於參與計畫之研究生的畢業論文中。 
 
附錄 
 
[1] Chao-Chin Wu, Kuan-Chou Lai, En-Hao Liu1, Jin-Yuan Chen, “The Impact of the Register 
Update Unit Size on Multipath Execution,” Proceedings of 2007 IEEE Pacific Rim 
Conference on Communications, Computers and Signal Processing, PACRIM' 07, 22-24 
August, Victoria, B.C., Canada, 2007. 
 
 
 

We further illustrate the proposed mechanism by the 
following example. Assume that there are eight entries in the 
RUU and there are seven instructions from multiple paths in 
the RUU as shown in Fig. 1. Only the RUU entry pointed by 
the tail pointer has a clear va , 
it will pass beyond the head p .  
That is, the RUU is full now  
filled in. Moreover, the ord  
entries are equal to zero, indi  
instructions are in the first run
efficient way. In terms of the simulation results, our proposal 
can boost the performance up to about 5%.  
The remainder of this paper is organized as follows. 
Section 2 introduces how to reuse RUU entries in a more 
efficient way. Section 3 presents a preliminary performance 
analysis, and finally, Section 4 provides some concluding 
remarks. 
II. THE PROPOSED MECHANISM 
In the original design of the RUU, the program order of 
those allocated instructions is maintained by both the head and 
tail pointers. The head pointer points to the first uncommitted 
instruction and the tail pointer points to the first available 
RUU entry to which the next instruction will be allocated. 
However, because the RUU acts as a ring, the tail pointer 
cannot pass beyond the head pointer to prevent form 
overwriting the contents of the RUU entry pointed by the head 
pointer. To meet this constraint, if the tail pointer and the head 
pointer will point to the same entry after the next instruction is 
allocated, an RUU overflow occurs and the instructions can 
not be filled in now. Only when the head pointer steps forward, 
leaving one more free entry between it and the tail pointer, the 
instruction can be filled in. 
For multipath execution, when a conditional branch is 
resolved, the RUU entries allocated to the instructions along 
the wrong paths will be all invalidated, resulting in several 
“holes” in the RUU. The invalid RUU entries in the holes 
cannot be reused until they reach the commit stage. To 
improve the utilization efficiency of the RUU, we propose to 
allow the tail pointer to pass beyond the head pointer one time 
to have the opportunity to fill additional instructions into the 
invalid entries in the holes. 
Valid 1 1 0 0 0
Ordering 0 0 0 0 0
……      
The snapshot of the
 
 
Valid 1 1 1 1 1
Ordering 0 0 0 0 0
……      
The snapshot oFigure 1.  
ea
Head
Figure 2.  
If a conditional branch is
on the wrong path will be squ
as shown in Fig. 2. The thre
hole and they can be reused
Hence, the tail pointer can pa
that, the v of the tail-ord
pass point ints to the sam
Moreover, because the value
now, the value of the or
instructions will become one, 
the value of the ordering bits
the first run or the second run
ordering bit equal to zero are 
the ordering bit equal to one a
In our proposed mechanism, in addition to a valid bit and a 
path-id field, as required by the conventional multipath 
execution [6-8], an ordering bit is introduced to each RUU 
entry. The invalid bit indicates if the entry is squashed or not. 
The path-id specifies to which path the corresponding 
instruction belongs. The ordering bit represents the instruction 
is on the first or second run in the RUU. The instructions are 
in the first run if they are allocated before the tail pointer 
meets the head pointer. Otherwise, they are in the second run. 
Moreover, a head-ordering bit, a tail-ordering bit and a pass 
pointer are also required for maintaining the whole RUU. The 
pass pointer points to the entry that the tail pointer meets the 
head pointer. Its initial value is null. The head-ordering bit 
indicates the ordering bit of the instructions in the first run 
should be zero or one. Its initial value is zero. Whenever the 
head pointer passes beyond the pass pointer, the value of the 
head-ordering bit needs to be complemented. When the head 
pointer moves forward to find the next uncommitted 
instruction in the program order, it locates the entry whose 
valid bit set and ordering bit equal to the head-ordering bit. 
Finally, the content of the tail-ordering bit is used to specify 
that the ordering bit of the instruction to be filled in 
subsequently is on the first or second run. Its initial value is 
zero. Whenever the tail pointer meets the head pointer, the 
tail-ordering bit is complemented. When moving forward, the 
tail pointer locates the next available entry whose valid bit 
reset and ordering bit equal to the tail-ordering bit. 
Figure 3.  
Valid 1 1 1 1 0
Ordering 0 0 1 1 0
……      
The snapshot of the RU
passes beyond
H Pass Tai
When the instruction poi
from the RUU, the corres
Furthermore, the head pointer
valid entry whose ordering blid bit. If the tail moves forward
ointer and find no invalid entry
and no more instruction could be
ering bits of all the allocated
cating that all the corresponding
. 
 
Tail  
 
 
 
1 1 0 Head-ordering = 0 
0 0 0 Tail-ordering = 0 
    
1 1 0 Head-ordering = 0 
0 0 0 Tail-ordering = 0 
    
f the RUU with only one run. 
Tail 
s 
s, 
 
. 
r 
 
.  RUU with squashed instrucitons. 
 resolved, the three instruction
ashed by resetting their valid bit
e squashed RUU entries form a
 for the subsequent instructions
ss beyond the head pointer. Afte
ering bit is changed to one, the
e entry as the head pointer does
 of the tail-ordering bit is one 
dering bits of the subsequent 
as shown in Fig. 3. According to 
, we can tell an instruction is in 
. In this case, the entries with the 
in the first run and the ones with 
re in the second run.  
 
 
l ead1 1 
0 0 
  
U .with t
 the head
nted by
pondin
 moves
it equal alue 
er podH 
1 Head-ordering = 0 
0 Tail-ordering = 1 
  
wo runs after the tail-pointer 
-pointer.  
 the head pointer retires 
g entry is invalidated. 
 forward to find the next 
to the head-ordering bit. 
