 1
一、中文摘要 
所謂「以人為本」的視覺分析(Anthropocentric 
Video Analysis)目的在於探討如何截取、描述、及
組織有關個人的身份、情緒、活動或行為的狀態資
訊及狀態變遷。本計畫重點在探討「以人為本」電
腦視覺中的遠距離多目標行人追蹤及行徑歸納與異
常警示。主要目的在建構一個針對多攝影機、遠距
離、及多重目標人員長期觀察分析系統。這是一個
極具挑戰性的任務，在二十到一百公尺遠距離情況
下，監視區域大且複雜，人的形體小，雜訊相對變
大，陰影及光線的問題也就相對嚴重。要解決這類
監視問題就是使用多重辨識方式。本計畫主體架構
包含：建立背景動態模型、移動物偵測、移動物體
分類、多目標追蹤、行人辨識。我們在本計畫中提
出了很多創新的方法，包括動態背景建模、提升擷
取影像物件的完整性、運動慣性追蹤法、輪廓式行
人辨別等。同時，我們採用並改進許多前瞻先進的
方法來提昇系統的效能及改善辨識率。我們提出了
一個創新並具彈性的階層示搜尋比對演算法，把人
形影像的 Canny edge 之 distance transform 之值
依據 Fuzzy C-means 分類法建構成樹狀，加速比對
的效率，提升   SMinSSDistBigOna Ea *log 。在我
們所測試的 1000 個 frame 的 video 樣本中共計 1069
個行人物件及 568 個非行人物件，系統的正確便辨
識率分別為 89%(true positive)及 92%(true negative)，
錯誤率為 8%(false positive) 及 11%(false negative)，
符合我們所預期的結果。在監控研究及應用領域
中，很重要的是能設計一個系統可以長期自動的去
偵測有行人的區域並找出具危險性的行為，因此透
過自動與智慧型的系統進行人員追蹤與辨識，對風
險的偵測與可疑追蹤有很大的助益。研究成果可應
用到廣大的區域監測，如校園、街道、廣場、車站、
機場、軍事基地等。 
 
關鍵詞：以人為本之視覺分析、智慧型監控系統、
背景動態模型、移動物體偵測、多目標追蹤、行人
辨識。 
 
Abstract 
The Anthropocentric Video Analysis system 
integrates various automated video analysis 
methodologies aiming to extract, efficiently describe, 
and organize information regarding the state or state 
transition of individuals, interactions between 
individuals, physical characteristics of humans, and so 
forth. Such information can be utilized in a multitude of 
important applications that include, but are not limited 
to human computer interaction, ubiquitous computing, 
video characterization, classification, semantic 
annotation, intelligent video surveillance, access control, 
and other security related applications. In this project, 
we propose a system for automatically detecting, 
tracking multiple pedestrians at a distance. Pedestrian 
identification at a distance is a challenging task due to 
tiny figure in the wide area scene, noise issue, 
occlusions and shadow problems. We propose several 
robust methods that can reliably track multiple objects 
in a complex environment and distinguish whether it is 
a human. We propose many innovative schemes 
including dynamic background modeling, edge-based 
object extraction, object tracking using momentum, 
silhouette-based pedestrian identification. This work 
develops a novel and robust hierarchical search tree 
matching algorithm, in which the Distance Transform 
based pedestrian silhouette template database is 
constructed for efficient pedestrian identification. The 
proposed algorithm was implemented and its 
performance assessed. The proposed method achieved 
an accuracy of 89% true positive, 92% true negative 
and low false positive 8% rates when matching 1069 
pedestrian objects and 568 non-pedestrian objects. The 
contributions of this work are twofold. First, a novel 
pedestrian silhouette database is presented based on the 
Chamfer Distance Transform. Second, the proposed 
hierarchical search tree matching strategy utilizing 
Fuzzy C-means clustering method can be adopted for 
mapping and locating pedestrian objects with 
robustness and efficiency.  
 
Keywords ： Anthropocentric video analysis, 
intelligent surveillance, dynamic background modeling, 
motion object detection, multiple objects tracking, 
pedestrian identification. 
 
 3
用人類形狀模型去詮釋前景。這個問題在貝式平台
上被定義為模型基底的分割問題。利用Markov 
Chain Monte Carlo (MCMC) 從機率分佈中取樣來
尋找機率空間中跳躍與播散動態的最佳解。從前景
分界邊界中計算出頭部的候選資料，同時在多個人
形模型資料中去擷取人形外型的變異，例如，主要
的四肢關節。隨機的擴散可以增加局部化的正確性
與速度且導致更強力的性能。國內亦有相關研究值
得參考[18,19]。 
(二) 人形辨識 
在傳統的視訊處理工作中，通常輸入為一個物
件或物件集合，藉由分析每個物件變化時可得到外
型邊界。外型分析在物件辨識及比對中扮演重要的
角色。以形狀做為基礎的方法，主要是針對單張影
像中偵測行人。人形的模板比對最早被使用。然
而，這個方法需要將影像主要的部份分割出來，人
物自動切割本身就個很困難的問題。其他的方法是
利用分類技術偵測影像中的行人，利用影像資料庫
來訓練、學習與分類，來偵測行人[20,21]。目前人
形識別常見的方法如下： 
(1) 外型比對法 – Rohr[22]最早使用人形的模板
比對。Niyogi 與 Adelson [23]藉由匹配連續影
像得到時間與空間上的物體，且使用權重式
Euclidean 的距離來做辨識。Collins 等[24]建立
一個剪影基底的最靠近鄰居分類器(Nearest 
Neighbor Classifier) 來做辨識。其他如在
Chamfer 距離上的外型比對[25]；可能的人形
偵測[26]；以外觀及動態特徵向量矩形濾波器
[27]；相異分割和類神經網路的偵測[28]；垂
直曲線、直方圖和慣性分析[29] ；線性垂直特
徵 偵 測 、 對 稱 性 及 人 形 樣 版 的 方 式
[30,31,32] ；移轉方式[33]和立方體的 B-spline
人形背景追蹤[34]。Wang 等使用 Procrustes 外
型分析來做辨識[35]。 
(2) 人形特徵分析法 --以特徵為基礎的人形分類
偵測方式通常比直接比對來的有效率[36]，如
Haar 小波表示和 SVM 分類[37,36,38]。近來，
SVM[39, 40]提供一個物件偵測的學習機制。
行人追蹤最近被廣為討論的一種方法為中間
線 相 對 稱 候 選 者 的 Bayesian 分 類 法 ，
spatial–temporal軌道追蹤和預測型卡曼率波器
[41]。 Kale 等[42]與 Lee 等[43]使用 Hidden 
Markov Models (HMM)，對於從雜訊的剪影中
辨識出個別體。其他還有主成分分析和時間延
遲的類神經網路[44]、 plane and parallax 的分
解 [45]，小波模型表示法、機器向量分類
[38] 、區域基底特徵 [46]等。 
(3) 步伐辨識法 --人類步伐的研究最逐漸發展起
來，因它很有可能使用來作為人形識別的工
具。Cunado 等[47]對移動物體的關節擺動建立
模型與擷取步伐的蹤跡。Phillip 等[48]對步伐
辨識提供一個使用剪影相互關係的基線演算
法。其他應用週期性動作、對稱性、前景輪廓
進行分析[49]、週期性 self-similarity 分析[50]、
行人偵測的annealed粒子濾波器及可能的動作
模型 [51]。Han 與 Bhanu [52]使用步伐能量影
像。Bobick 與 Johnson [53]使用靜態特定活動
與跨步參數來實現辨識。 
(4) 運動基底法 – Bissacco 等[54]使用動態仿射
不變量進行步伐辨識，另外對於人體運動模型
的 多 種 參 數 軌 跡 學 習 動 態 系 統 [55] 。
Tanawongsuwan及Bobick [56]發展一個正規化
程序，使得步伐特徵對應到不同的速度去補償
步伐特徵因為走路的速度而產生自發性變
化。Veeraraghavan 等考慮連續外型變化[57]，
外型變化為物體提供關於它身份的線索且或
此物體自然的動作取代以 frame 為方式的外型
比較，此法為 Dynamic Time Warping (DTW)
演算法[58]的延伸，計算出外型序列之間的距
離。一個正在走路的人的序列可表示為一些外
型的序列且各個外型之間的距離可用來進行
步伐辨識。其他方法如 skeleton-based 的動態
分析、卡曼濾波器偵測[59]及空間暫存的動態
行人分析[60]。 
(三) 物件追蹤 
在移動物偵測之後，接下來要進行追蹤移動物
件，追蹤的演算法通常和移動物偵測交互執行。在
連續的frame中，使用特徵點、線或是blobs的方式，
 
圖三、第一年研究計畫架構及子系統關聯圖 
 
1、背景建構與動態更新 
    我們使用Gaussian Mixture Model(GMM)之背
景模型，同時我們提出一個簡化的法則。假定背景
區域的像素通常是相對較為穩定，短時間內變化不
很遽烈。因此基礎背景建置的方式是取輸入視訊畫
面資料一開始的連續n個幾較穩定的畫面當作背景
之初始集合，令建構出來之初始背景模型時計算每
個像素
tX 之高斯混合模型為 
1
( ) ( , , )K i i it t t t tiP X X               (1) 
其中的高斯模型可定義為： 
11 ( ) ( )
2
1
2 2
1( , , )
(2 )
T
t t t tX X
t nX e
  

    

     (2) 
利用定義中之K個高斯模型來混合出適應於目前
背景變化的機率分布函式。然而K的大小乃取決於
視訊場景變動的情形，若前景物體移動緩慢，則K
不宜太小，否則前景反而會被當成背景。然而在真
實的環境下，隨著時間或日照變化等因素，背景勢
必有動態地更新的必要。我們提出一個簡單方法，
調適性高斯混合模型更新法，其計算過程很簡單，
很容易做即時運算。 
我們設定一個影像個數參數閥值 L 做為後期
更新背景之判定比較值。在更新的過程中為避免前
景物體長期滯留而被當成背景所，所以更新的步驟
在一開始需先剔除前景影像的部分，接著再判定前
景是否處於一個穩定的狀態，是故只有穩定之像素
才有更新之必要。因此我們對於高斯混合模型之更
新提出了兩種模式的更新方式：背景統計更新模式
與以 L 為基底的更新模式。首先，在一段影像資
訊輸入的同時，背景將會被更新利用下面公式： 
 
                                        (3) 
 
 
其中 1 1 1i i iN N N   、 、 皆為下一時間點之高斯模型
之參數。然而，在經過 L 張影像個數時，此高斯
模型必定具有相當大量的觀察數據，因此，為了可
以更快速的使此背景模型更適應影像資訊，則背景
更新則變換到以下方式更新： 
 
                                        (4) 
 
 
當影像個數參數閥值L設定為較小時，表示該目前
前景像素十分穩定，當L設定較大時，表示該目前
前景像素不穩定，因此需要更長的時間進行觀察，
整個流程如圖四所示。 
 
圖四、背景動態更新流程 
2、前景物件擷取及陰影去除 
由於物體移動的因素，一般用背景相減法擷取
出來的前景物件會產生殘影，經常會呈現鋸齒狀或
零亂破碎的現象。為了彌補相減結果物體殘缺破碎
的缺點，除了將與背景相減法擷取出來的前景物件
做形態學的Closing 及Opening運算，我們需再行找
出屬於前景的邊緣，再將之整合。流程如圖五所
1 1
1
1 11
1
1
1 1 11
1
1 ( ( | ) )
1
( | ) ( )
( | )
( | ) (( )( ) )
( | )
i i i i
N N N N
i
i i iN
N N N NN i
kk
i
i i i i T iN
N N N N N N NN i
kk
p X
N
p X X
p X
p X X X
p X
   
  
  
 

 


  

  
  
      


1 1
1 1
1
1
1 1 1
1
1
1 ( ( | ) )
( | )1 ( )
( | )( )( )1 ( )
i i i i
N N N N
i
i i iN N
N N Ni
N
i i i T
i i iN N N N N
N N Ni
N
p X
L
p X X
L
p X X X
L
   
  
  

 
 


  


  
  
     
   
(a)                      (b)                   (c) 
圖六、(a)表示輸入視訊；(b)為背景相減法之結果；(c) 為我們的方法之結果。 
3、物件標示 
    影像物件之標示(labeling)在影像物件處理以
及追蹤是相當重要的一環，這個步驟如果作得好，
在下一階段進行多人分割時，誤差就會降低。不論
是用背景建構的方式或者是前後影像相減的方
式，或多或少都會參雜溢出或者遺漏的部分，是故
必造成物件在編號的過程中，會將原本是同一群體
之物件分割成編號成兩個以上的區域。本計畫利用
降階取樣(down sampling)的方式將兩個可能原本
是屬於相同一區塊之物件，重新編號為同一物件。
整個運作流程如圖七所示。 
圖七、影像物件標示流程圖 
我們依下式 (9) 將步驟二所獲得的前景物件
 yxFMaskn , 降低解析度為於原本之十六分之一，得
到  ,FMask x y↓ 。輸入  ,FMask x y↓ 再次計算connected 
component labeling的區間標號  ,Label x y↓ ，然而再
提高解析度放大回復原始尺寸  ,Label x y↑ 。然後檢
查  ,Label x yn 和  ,Label x y↑ ，相異i, j標號在所對應之
 ,Label x y↑ 中卻呈現相同之標號則，把i, j合併視為
同一物件 inVO ，i表示物件標號，n為第n個Frame。 
   
4 4
0 0
1 11  ,
4 4 4,
0
n
dx dy
if FMask x dx y dy
FMask x y
otherwrise
 
     
 
↓
 (9) 
4、物件追蹤 
    物件追蹤常用的幾種方式，第一類方法是利用
影像上的特性，如色彩紋理、區塊成分、輪廓樣板
比對等。另一種為物件的物理特性，如運動方向、
運動速度、Partical Filter等。利用色彩特性易受光
線變化所影響，物件也會有長期靜止在畫面中的可
能，而外在光源的變化可能因遮蔽前後而不同因而
誤判，然而區塊及樣板之比對較適用於特定目標，
諸如車子等區塊成分均勻之物件，或是已知且已定
義之物體，如車輛追蹤；另一種依靠運動特性之追
蹤，則較適用於運動向量較大之移動物體追蹤。在
本計畫中，我們利用相當簡潔的方式進行物件之追
蹤。我們利用Kalman濾波器來預估移動物體下一
個時間之位置。 
運用 Kalman 濾波器所預估出的中心位置，將
前一張影像所取得之物件的特徵資訊，即可對於所
預估出的物件位置內的資訊做特徵比對，若比對出
的值大於某一門檻值時，則判定為已追蹤到此一移
動物件。又因物件有可能相互遮蔽，所以失縱之視
訊物件必須額外加以處理，因為它可能只是暫時因
為被擋住而消失，經過一段時間後可能會再度出
現。系統流程如圖八所示： 
圖八、物件追蹤流程圖 
i
nVO 本身代表在第 n 個 frame 所切割出來的屬於第 i
個物件之像素集合。比對目前的視訊物件 inVO 和前
面第 k 個畫面的物件 i
kVO ，若比對成功，將目前畫
面物件 i 的編號改為與前某畫面 k 之匹配物件 j 相
同的編號，換言之，i 和 j 為同一物件，達成追蹤
的功能。若比對失敗，表示該物件在目前畫面上可
能被遮蔽或是已經移出畫面，所以將其移至遺失物
件佇列，而若對於前一影像物件與遺失影像物件皆
比對失敗，則有可能該物件是目前畫面才新加入
圖十一、追蹤演算法示意圖:(a)第n張影像、(b)第n+1張影像、(c)第n+2張影像。 
 
圖十二、遮蔽偵測示意圖:(a)第n張影像、(b)第n+1張影像發生遮蔽、(c)分析遮蔽狀況。 
 
 
圖十三、被遮蔽物件折返示意圖:(a)第n張影像、(b)第n+1 張影像圓形物件被遮蔽、(c)第n+2 影像被
遮蔽的物件有折返的現象。 
OBJn2
OBJn1
(a) (b) (c)
OBJn3 OBJn+12
OBJn+11
OBJn+13
OBJn+22
OBJn+21
OBJn+23
(d)
OBJn+32
OBJn+31
OBJn+33
OBJn4 OBJn+14 OBJn+24
OBJn+34
圖十四、多物件遮蔽時追蹤示意圖:(a)第n張影像、(b)第n+1 張影像、(c)第n+2 張影像、(d)第n+3 張
影像。
 11
2.1 人形距離轉換與樣本資料庫的建立 
人形樣本資料庫的建立，是利用本計畫第一年
所提出的影像物件擷取方式切割出移動的物件，再
用手工的方式先行區分出人形及非人形的物件，然
後將這些物件之邊緣萃取出來，接著再將所有篩選
出 來 的 人 形 物 件 依 最 小 圖 點 距 離 (distance 
transform)的計算方式做最小距離之分群。分成數
群後，取各群中最小群距之物件當該群群心樣本，
接著將群心當成節點而其它距離較遠的成員當成
該節點之後代，再反覆分群，最後可得一人形比對
之多元搜尋樹，過程如圖十五所示，詳細說明如
后。 
圖十五、人形資料庫建立流程圖 
 
圖十六、各種人形輪廓樣本 
   我們將經由人工篩選的方式從訓練影片中挑選
出數種大小的人形視訊物件(圖十六)取得物件輪
廓 S， inEVO 代表在第 n 個 Frame 編號為 i 之輪廓
座標集合， ),( yxEn 表示畫面 n之影像邊緣，  nE
代表取得邊影像之二值化圖形。圖十六代表幾個大
小不同之人形樣本。然後再對每一比資料庫中的人
形邊緣影像計算其 Distance Transform Map，其原
理是計算圖 S 之點集合中各點分別距離標準目標
輪廓圖 E 之點集合中各點的最小距離之對應值，
原 Distance Transform Map 的歐幾里得距離計算十
分耗時之運算，為求演算上之效能，我們預計改用
3-4DT[130]演算法。S 表示目標輪廓，E 為所比對
之資料庫輪廓，兩輪廓之距離則用下式(16)表示: 
   
   Syx ESE yyxxDTSyxEDist , 0000, ,
1,   (16) 
其中(x0, y0)代表輪廓 S 相對於輪廓 E 之偏移座標
(translation)，|S|表示輪廓 S 之長度。主要的物理意
義是因為輪廓 S在不同之偏移量(x0, y0)的情況下計
算取得之  00, , yxEDist SE 會計算出不同之值，不斷
變化偏移量(x0, y0)，計算何者最小，是故利用類似
Three step search(TSS)跳躍搜尋的方式，將 TSS 中
MSE(Mean Square Error)之計算用  00 ,, yxSEDistE
取代，我們將這種新的搜尋計算方法命名為
 SMinSSDistE :Minimum step search distance map。 
 
2.2 多元搜尋樹人形辨識法 
 
 
 
 
 
 
 
 
 
圖十七、人形多元搜尋樹(部分樣本) 
我們提出多元搜尋樹(MSTree)加速人形辨識
的工作。我們提出多元搜尋樹的目的是為避免訓練
資料庫過於龐大而花費過多的搜尋計算時間，當分
類後某物件的子類資料庫資料量為 n 時，Full 
search 的時間複雜度為   SMinSSDistBigOn E* ，
若 採 用 樹 狀 搜 尋 則 只 須 更 短 的 計 算 時 間
  SMinSSDistBigOna Ea *log ，a 為各節點之子節
點數量且遠小於 n。我們利用 Depth First Search 
(DFS)由上而下之方式建立搜尋樹(如圖十七)，使
用Fuzzy C-Means演算法依據各訓練人形樣本間的
 SMinSSDistE 距離進行分群，一開始第一層先將
n 筆資料分為 a(a=sqrt(n))群，取各群之群心當作節
點，再將該群其他成員當成該節點之後代
(children)，下一層的分類方式為:假設前一層的子
類各數為 m, 則分成 b(b=sqrt(m)) 群，取各群之群
五、 參考文獻 
[1] I. Haritaoglu, D. Harwood, and L. S. Davis, 
“W4:Real-time surveillance of people and 
their activities,” IEEE Trans. on PAMI, vol. 
22, pp. 809–830, Aug. 2000.  
[2] S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, 
and H. Wechsler, “Tracking groups of 
people,” Comput. Vis. Image Understanding, 
vol. 80, no. 1, pp. 42–56, 2000. 
[3] C. Stauffer and W. Grimson, “Adaptive 
background mixture models for real-time 
tracking,” in Proc. IEEE CVPR, vol. 2, pp. 
246–252, 1999. 
[4] R. T. Collins, A. J. Lipton, T. Kanade, H. 
Fujiyoshi, D. Duggins, Y. Tsin, D. Tolliver, N. 
Enomoto, O. Hasegawa, P. Burt, and L. 
Wixson, “A system for video surveillance and 
monitoring,” Tech. Rep., CMU-RI-TR-00-12, 
Carnegie Mellon Univ., Pittsburgh, PA, 2000. 
[5] A. J. Lipton, H. Fujiyoshi, and R. S. Patil, 
“Moving target classification and tracking 
from real-time video,” in Proc. IEEE 
Workshop Applications of Computer Vision, 
pp. 8–14, 1998. 
[6] D. Meyer, J. Denzler, and H. Niemann, “Model 
based extraction of articulated objects in 
image sequences for gait analysis,” in Proc. 
IEEE ICIP, pp. 78–81, 1998. 
[7]  J. Barron, D. Fleet, and S. Beauchemin, 
“Performance of optical flow techniques,” Int. 
J. Comput.Vis., vol. 12, no. 1, pp. 42–77, 
1994. 
[8] D. Meyer, J. Psl, and H. Niemann, “Gait 
classification with HMM’s for trajectories of 
body parts extracted by mixture densities,” in 
Proc. British Machine Vision Conf., pp. 
459–468, 1998. 
[9] N. Friedman and S. Russell, “Image 
segmentation in video sequences: a 
probabilistic approach,” in Proc. 13th Conf. 
Uncertainty in Artificial Intelligence, pp. 1–3, 
1997.  
[10] E. Stringa, “Morphological change detection 
algorithms for surveillance applications,” in 
Proc. British Machine Vision Conf., pp. 
402–412, 2000.  
[11] S. Haritaoglu, D. Harwood and L. S. Davis, 
“W4: Real-Time Surveillance of People and 
Their Activities,” IEEE Trans. on PAMI, vol. 
22, no. 8, pp. 809-830, 2000. 
[12] T. Zhao, R. Nevatia and F. Lv, “Segmentation 
and Tracking of Multiple Humans in Complex 
Situations, ” Proc. IEEE Conf. on CVPR, vol. 
2, pp. 194-201, Kauai, Hawaii, 2001. 
[13] N. T. Siebel and S. Maybank, “Fusion of 
Multiple Tracking Algorithm for Robust 
People Tracking,” Proc. European Conf. 
on Computer Vision; LNCS2353, pp. 373-387, 
2002. 
[14] A. M. Elgammal and L. S. Davis, “Probabilistic 
Framework for Segmenting People under 
Occlusion,” Proc. ICCV, vol. 2, pp. 145-152, 
Vancouver, Canada, 2001. 
[15] A. Mittal and L. S. Davis, “M2Tracker: A 
Multi-View Approach to Segmenting and 
Tracking People in a Cluttered Scene 
Using Region-Based Stereo,” Proc. of 
European Conf. on Computer Vision, LNCS 
2350, pp. 18-33, 2002. 
[16] H. Tao, H. S. Sawhney and R. Kumar, “A 
sampling algorithm for tracking multiple 
objects,” Proc. Workshop of Vision 
Algorithms, ICCV 99, vol.1883, pp. 53 ,2000. 
[17] M. Isard and J. MacCormick, “BraMBLe: A 
Bayesian Multiple- Blob Tracker,” Proc. 
ICCV, vol.2, pp. 34-41, Vancouver, Canada, 
2001. 
[18] 趙鴻欣, 利用透視轉換技術估計人群數目, 
 15
[38] M. Oren et al., “Pedestrian detection using 
wavelet templates,” in Proc. IEEE Computer 
Vision and Pattern Recognition Conf., 
pp.193–199, 1997. 
[39] F. Xu, X. Liu, and K. Fujimura, “Pedestrian 
Detection and Tracking With Night Vision,” 
IEEE Trans. Intelligent Transportation 
Systems., vol. 6, no. 1, pp. 63-71, March 2005 
[40] V. Vapnik, “The Nature of Statistical Learning 
Theory,” Berlin, Germany: Springer-Verlag, 
2000. 
[41] T. Darrell et al., “Integrated person tracking 
using stereo, color, and pattern detection,” in 
IEEE Conf. Computer Vision and Pattern 
Recognition, pp. 601–608, 1998. 
[42] A. Kale, A. Rajagopalan, A. Sundaresan, N. 
Cuntoor, A. Roy- Cowdhury, V. Krueger, and 
R. Chellappa, “Identification of Humans 
Using Gait,” IEEE Trans. Image Processing, 
vol. 13, pp. 1163-1173, Sept. 2004. 
[43] L. Lee, G. Dalley, and K. Tieu, “Learning 
Pedestrian Models for Silhouette Refinement,” 
Proc. Int’l Conf. Computer Vision, 2003.  
[44] U. Franke et al., “Autonomous driving goes 
downtown,” IEEE Intell. Syst., pp. 32–40, 
Nov./Dec. 1998. 
[45] H. Sawhney et al., “Independent motion 
detection in 3D scenes,” IEEE Trans. Pattern 
Anal. Machine Intell., vol. 22, pp. 1191–1199, 
Oct. 2000. 
[46] J. Foster, M. Nixon, and A. Prugel-Bennett, 
“Automatic Gait Recognition Using 
Area-Based Metrics,” Pattern 
Recognition Letters, vol. 24, pp. 2489-2497, 
2003. 
[47] D. Cunado, M. Nash, S. Nixon, and N. Carter, 
“Gait Extraction and Description by Evidence 
Gathering,” Proc. Int’l Conf. Audio and 
Video-Based Biometric Person Authentication, 
pp. 43-48, 1994. 
[48] J. Phillips, S. Sarkar, I. Robledo, P. Grother, 
and K. Bowyer, “The Gait Identification 
Challenge Problem: Data Sets and 
Baseline Algorithm,” Proc. Int’l Conf. Pattern 
Recognition, vol. 1, pp. 385-388, Aug. 2002. 
[49] I. Haritaoglu et al., “Backpack: Detection of 
people carrying objects using silhouettes,” 
presented at the Int. Conf. Computer Vision, 
Corfu, Greece, Sept. 1999. 
[50] R. Cutler and L. Davis, “Robust real-time 
periodic motion detection, analysis, and 
applications,” IEEE Trans. Pattern Anal. 
Machine Intell., vol. 22, pp. 781–797, Aug. 
2000. 
[51] Y. Song et al., “Toward detection of human 
motion,” in IEEE Conf. Computer Vision and 
Pattern Recognition, pp. 810–817, 2000. 
[52] J. Han and B. Bhanu, “Individual Recognition 
Using Gait Energy Image,” Proc. Workshop 
Multimodal User Authentication 
(MMUA 2003), pp. 181-188, Dec. 2003. 
[53] A. Bobick and A. Johnson, “Gait Recognition 
Using Static Activity-Specific Parameters,” 
Proc. Conf. Computer Vision and Pattern 
Recognition, vol. 1, pp. 423-430, Dec. 2001. 
[54] A. Bissacco, P. Saisan, and S. Soatto, “Gait 
Recognition Using Dynamic Affine 
Invariants,” Proc. Int’l Symp. Math. Theory 
of Networks and Systems, July 2004. 
[55] A. Bissacco, A. Chiuso, Y. Ma, and S. Soatto, 
“Recognition of Human Gaits,” Proc. Conf. 
Computer Vision and Pattern Recognition, vol. 
2, pp. 52-57, 2001. 
[56] R. Tanawongsuwan and A. Bobick, “Modelling 
the Effects of Walking Speed on 
Appearance-Based Gait Recognition,” 
Proc. Conf. Computer Vision and Pattern 
Recognition, vol. 2, pp. 783-790, 2004. 
 17
 可供推廣之研發成果資料表 
■ 可申請專利  ■ 可技術移轉                                      日期：98 年 9 月 28 日 
國科會補助計
畫 
計畫名稱：以人為本之視訊監控：遠距離行人追蹤與行徑分析
計畫主持人：林道通 
計畫編號：NSC96-2221-E-305-008-MY2  學門領域：資訊 
技術/創作名稱 以人為本之視訊監控：遠距離行人追蹤與行徑分析 
發明人/創作人 林道通 
所謂「以人為本」的視覺分析(Anthropocentric Video Analysis)目的
在於探討如何截取、描述、及組織有關個人的身份、情緒、活動或行為的狀
態資訊及狀態變遷。本計畫重點在探討「以人為本」電腦視覺中的遠距離多
目標行人追蹤及行徑歸納與異常警示。主要目的在建構一個針對多攝影機、
遠距離、及多重目標人員長期觀察分析系統。這是一個極具挑戰性的任務，
在二十到一百公尺遠距離情況下，監視區域大且複雜，人的形體小，雜訊相
對變大，陰影及光線的問題也就相對嚴重。要解決這類監視問題就是使用多
重辨識方式。本計畫主體架構包含：建立背景動態模型、移動物偵測、移動
物體分類、多目標追蹤、行人辨識。我們在本計畫中提出了很多創新的方法，
包括動態背景建模、提升擷取影像物件的完整性、運動慣性追蹤法、輪廓式
行人辨別等。同時，我們採用並改進許多前瞻先進的方法來提昇系統的效能
及改善辨識率。 
技術說明 
    In this project, we propose a system for automatically detecting, tracking 
multiple pedestrians at a distance. Pedestrian identification at a distance is a 
challenging task due to tiny figure in the wide area scene, noise issue, occlusions 
and shadow problems. We propose several robust methods that can reliably track 
multiple objects in a complex environment. We propose many innovative schemes 
including dynamic background modeling, object tracking using momentum. 
可利用之產業 
及 
可開發之產品 
智慧型視覺監控系統 
技術特點 
我們提出了一個創新並具彈性的階層示搜尋比對演算法，把人形影像的
Canny edge 之 distance transform 之值依據 Fuzzy C-means 分類法建構成
樹狀，加速比對的效率，提升   SMinSSDistBigOna Ea *log 。在我們所測試
的 1000 個 frame 的 video 樣本中共計 1069 個行人物件及 568 個非行人物件，
系統的正確便辨識率分別為 89%(true positive)及 92%(true negative)，錯誤率為
8%(false positive) 及 11%(false negative)，符合我們所預期的結果。 
推廣及運用的價
值 
1. 遠距離行人追蹤 
2. 前景物件擷取、陰影去除及移動殘影改善 
3. 遮蔽情況之追蹤 
4. 人形辨識 
 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
附件二 
Journal of Information Technology and Applications        
Vol. 2, No. 3, pp. 133-142, 2008        
134 
員更精確、可靠的資訊，以期能縮短報案時間與減
少社會成本。我們所提出的智慧型監控系統整合了
滯留物體偵測、人形辨識之功能。 
近十年來，國內外關於視覺監控方面的研究投
注了不少心力。國外的學者對於移動物體分割的方
法，使用影像在時間或者空間的資訊。最著名的方
法包括背景相減法[1]，形狀統計分析與追蹤[2]和
時間相差法[3]。Kim 和 Hwang [4] 透過使用一個
雙重邊地圖(double-edge map)提出了擷取視訊中物
體有效的演算(VOPs)。Ouyang、Lee 和 Du 提出了
類神經模糊(neuro-fuzzy)方法在影像中分割人物
[5]。Harwood、Haritaoglu 和 Davis [2] 提出一個
W4 系統，透過三個參數表示每一個像素的背景，
包含:最小值、最大值、和在於背景訓練時期觀察
的連續的影像之間的最大差值。針對各種不同著眼
點的追蹤系統已被許多的研究人員所發表，
Pfinder[6]早已被使用來重建一個 3D 的人類模型，
以便在一個複雜的室內環境之中追蹤人員，麻省理
工學院的監視系統使用一種調適性的混合高斯模
型來模仿背景，而此系統主要著重於分類、移動圖
案的學習和異常活動的偵測進行分類[7]。W4 系統
和它的延伸 HYDRA 系統[2]，使用輪廓分析來偵
測身體部份和追蹤多個人。Bregler 和 Malik[8]、
Roth[9]使用目前畫面中的結構和一個動態的模型
來預測此結構的下一個狀態；這些憑借影像資料所
做的預測可以是精確的。目前的濾波器使用的多重
預測是藉由在執行樣本期間透過一個以比較本身
影像資料而得到的精確動態模型來取得(可能性) 
[10][11]。 
人形辨識通常依據二個特性判定，其一，取得
影片中人物活動時外形的可能範圍，然後從此範圍
內獲取規律性之資訊，這些資訊可以指出這張影像
中人的姿態。這個方法的優點為其可以處理其色彩
分成不明顯之人形。人形偵測的問題可以被視為在
一個二維資料的特徵空間資料搜尋的問題。這個演
算法如同基因般地將搜尋的範圍編碼，然後產生許
多的衍生後代並且從中挑選出適合分數最高者
[12][13]。此演算法可以快速的收斂，然而它並無
法妥善處理多個的人形，而且吾人很難決定一個恰
當的函數。主要區塊分成法的目的在找出一個抽象
的人體模型，以便於捕捉資料集的大部份變化
量。。 
在本文中，視訊物件之抽取採用背景相減之方
式，而背景建構之方式則採用多重緩衝之方式，此
外在更新背景時，參考其畫面是否穩定，背景是否
已取得較佳的背景影像等，設定動態之閥值來判定
現階段該像素是否須被更新，其後視訊物件之取得
乃利用 Kim 和 Hwang 等改良之邊影像法[4]，用以
提升物件的輪廓取得，該部分會在第二節詳細說
明，而物體滯留之偵測則在第三節討論，第四節則
是運用資料庫搜尋等方式判定畫面中之人形數
量，之後實驗結果則呈現在第五節，最後第六節是
結論與未來發展。 
 
2. 移動物件切割 
2.1 背景建構 
所謂背景通常是指具有相對於前景較為穩定
的畫素，短時間內變化不很遽烈，且至多在一定的
小範圍內變化的像素。本文中背景建構的方式，是
基於此一假定。如圖 2.1 所示，連續四個 Frame((a)
到(d))的影像差異主要是緣起於人的移動，圖 2.1(e)
顯示(a)至(d)之間兩兩畫面差值的累計。基礎背景
建置的方式是從輸入視訊畫面資料一開始的連續
n 個 Frame 當作背景之初始集合，如 SetBackground(x, 
y)={I0(x, y), I1(x, y), … , In(x, y)} ， 其中 n 設為 8。 
   
(a)            (b)           (c) 
  
(d)             (e) 
圖 2.1 (a) Frame12；(b) Frame13；(c) Frame14； 
(d) Frame15；(e)顯示 (a)-(d)之間差值的累計 
 
令 IBackground(x,y)為建構出來之背景，並取
IBackground(x, y)等於 SetBackground(x,y)之平均值，同時計
算每個背景像素的變異數 vBackground(x, y)： 
 
( ) ( )),(, yxSetmeanyxI BackgoundBackground =         (1) 
( )
( ) ( )( )
n
yxIyxI
yx
n
i
Backgoundi
Background
∑
=
−
= 1
2,,
,ν
    (2) 
因此 IBackground(x, y)為建構出來之初始背景，
vBackground(x, y)為該背景每個點之變異數，我們也設
定變異數閥值 vThreshold(x, y)，一開始我們設定初始
值 vThreshold(x,y)=0，該數值留做後期更新背景之判
定用。 
 
2.2 前景像素擷取 
前景像素的取得，是計算輸入影像跟背景的差
值，再利用我們所提出的雙閥值之方式找出不屬於
背景之像素，為了彌補相減結果物體殘缺破碎的缺
點，我們再行找出屬於前景的邊緣，再將之整合。
整個流程如圖 2.2 所示。令目前第 n 個輸入畫面影
像 In(x, y)跟背景 IBackground(x, y)的差值為 DBn(x, y): 
 
( ) ( ) ( )yxIyxIyxDB Backgroundnn ,,, −=       (3) 
我們比較 DBn(x, y)與較小之閥值 T0，如果
DBn(x, y)比閥值 T0 大，表示可能為前景像素，令
Iforeground(x, y)代表前景像素集合，則 
( )
⎩⎨
⎧ >=
otherwrise
TyxDBifyxI
yxI nnforeground 0
, ),(
),( 0   (4) 
Journal of Information Technology and Applications        
Vol. 2, No. 3, pp. 133-142, 2008        
136 
 
圖 2.4 影像物件標號流程圖 
  
利用降階取樣(down sampling)的方式將兩個
可能原本是屬於相同一區塊之物件，重新編號為同
一物件，意即將破碎之物件補完。將 FMaskn(x, y)
利用 connected component labeling [14]取得 Frame 
n 之區間標號 TLabeln(x, y)，另一流程為降階取樣
之程序，依靠公式(10)將 FMaskn(x, y)壓縮為於本大
小之十六分之一，得到 FMaskDown(x, y)，輸入
FMaskDown(x, y) 再次計算 connected component 
labeling的一區間標號 Labeln(x, y)，並用公式(11) up 
sampling 回復原始尺寸 LabelUp(x, y): 
 
( ) ( )
⎪⎩
⎪⎨
⎧ >++×= ∑∑= =
otherwrise
dyydxxFMaskifyxFMask dx dy
n
Down
0
4
1,
44
11,
4
0
4
0
 
  (10) 
 
( ) ⎟⎟⎠
⎞⎜⎜⎝
⎛
⎥⎦
⎥⎢⎣
⎢⎥⎦
⎥⎢⎣
⎢=
4
,
4
, yxLabelyxLabel DownUp
     (11) 
之後將兩流程所計算出之 Labeln(x, y)和
LabelUp(x, y)依靠公式(12)定義各影像物件 VOni，i
表示物件標號，n 為第 n 個 Frame 
( ) ( ) ( ) ( )
( ) ( ) ⎭⎬
⎫
⎩⎨
⎧
=≠
=∪=∀=
1100
11001100
,,|
},,{},{,,,,
yxLabelyxLabelji
jyxLabeliyxLabelyxyx
VO
upup
nni
n
 
(12) 
將 物 件 標 號 以 TLabeln(x,y) 為 基 礎 將
LabelUp(x,y)為參考，若 TLabeln(x, y)中之兩相異 i, j
標號在所對應之 LabelUp(x, y)中可對應相同之標號
則 VOni= VOni∪VOnj, VOnj=ψ。 
 
2.4 背景更新 
在真實的環境下，要取得一個完全乾淨的背景
是十分困難的，系統在建置時絕少情況下是可以要
求畫面上一切物件清場的，即使能達成這樣的要
求，隨著時間的推移，日照的變化等因素，背景將
不可避免的由細微的差異逐漸累積成過大的差
值，因此，勢必背景有更新的必要。在更新的過程
中為避免靜止物體長期滯留而被當成背景所更
新，所以更新的步驟在一開始先剔除前景影像的部
分，接著再判定前景是否處於一個穩定的狀態，因
在章節的一開始就定義假設背景為相對穩定之像
素部分，是故只有穩定之像素才有更新之必要。我
們提出一個新的方法，利用變異數配合動態之閥
值，若變異數大於閥值則更新背景，且同時閥值隨
之遞增，反之則遞減。當動態閥值變大時，表示前
景十分穩定且是背景之可能提高，整個流程如圖
2.5 所示。若像素不屬於前景(FMaskn(x, y))且目前
畫面像素值穩定，亦和前兩張畫面像素平均值的差
異小於一個值(| In(x, y)-( In-1(x, y)+ In-2(x, y))/2|<5)，
當 vBackground(x, y)>vThreshold(x, y)則移除 SetBackground(x, 
y)集合中最小 t 之 It(x, y) (最早的一個畫面)，並將
新的穩定畫面 In(x, y)加入其中，且運用公式(1)、(2)
重新計算 IBackground(x, y)和 vBackground(x, y)，之後將
vThreshold(x, y)遞增(公式(13))。若否，則 vThreshold(x, y)
遞減(公式(14))。 
 
( ) ( ) ( ) 1,,, max +=< yxyxthenVyxif ThreasholdThreasholdThreashold ννν     
(13) 
( ) ( ) ( ) 1,,0, −=> yxyxthenyxif ThreasholdThreasholdThreashold ννν
     (14) 
 
圖 2.5 背景更新之流程 
 
3. 物件追蹤與滯留偵測 
3.1 物件追蹤 
我們運用物體移動之慣性特性，將前一張影像
所取得之物件加上個別物件之運動向量，即可預估
在目前影像畫面上各物件所應出現之位置，再依靠
Journal of Information Technology and Applications        
Vol. 2, No. 3, pp. 133-142, 2008        
138 
合，判定在 Frame n 之所有視訊物件是否和以紀錄
之 VO 有所交集合) 檢查是否有其它物件將其遮蔽 
 
( ) { } occludedthenVOVOiVOVOif incord φ≠∩∀∩∈ |Re
  (18) 
若無遮蔽表示該物件已移出警戒區；若有遮
蔽，則檢查遮蔽時間是否已超出遮蔽等待時間，若
超出則表示該物件已不可預期並將其至紀錄中移
除；若尚未超出，則回到遮蔽判定繼續觀察；若一
開始判定物件無遮蔽且依然在警界區中，則檢查是
否在移動中，也就是 VOPni是否趨近 0，若在移動，
則表示該物件並非滯留狀態，故將其從紀錄中移
除，反之則表示處於滯留狀態，接著檢查是否滯留
時間超出所設定之警戒範圍。是，則發出警報。表
格說明必須置於表格上方且置中。 
 
4. 人形辨識 
首先利用本文第二節所提出的影像物件擷取
方式切割出移動的物件，再用人工的方式先行區分
出人形的物件，然後將這些人形物件之邊緣萃取出
來，接著再將所有節選出來的人形物件依靠最小圖
點距離(distance transform)的計算方式做最小距離
之分群。分成數群後，取各群中最小群距之物件當
該群群心，再剔除跟群心十分近似之物件，接著將
群心當成節點而其它成員當成該節點之後代，再反
覆分群，最後得到多元搜尋樹，過程如圖 4.1 所示。 
 
圖 4.1 人形資料庫建立流程圖 
我們把經由人工篩選的方式所挑選出的物件
沿用下式(19)取得物件輪廓 S:  
 
( ) ( )( ) ( ){ }innin VOyxyxEyxEVOS ⊆=== ,,1,|, θ    (19) 
 
其中 EVOni代表在第 n 個 Frame 編號為 i 之邊
座標集合，En(x, y)表示 Frame n 之影像邊緣θ(En)
代表取得邊影像之二元圖 ，VOni 由(12)所取得然
後再對每一比資料庫中的人形邊緣影像建立
Distance Transform Map，其原理是計算圖 A 之點
集合中各點分別距離圖 B 之點集合中各點的最小
距離之對應圖，建立的方式如下式表示。 
( ) ( )yxeyxDT
SeS
,minarg, −= ∈         (20) 
上式的計算實為十分耗時之運算，為求演算上
之效能，我們改用 3-4DT[15]。S 表示目標輪廓，E
為所比對之輪廓，兩輪廓之距離則用下式表示。 
 
( ) ( )
( )∑∈ ++= Syx ESE yyxxDTSyxEDist , 0000, ,
1,
   (21) 
其中 x0, y0 代表輪廓 S 相對偏移輪廓 E 之座
標，主要的物理意義是因為輪廓 S 在不同之 x0, y0
的情況下計算取得之 EDistE,S(x0, y0)會計算出不同
之值，不斷變化 x0, y0, 計算何者最小，亦即取得( )00,],[, ,min00 yxEDist SEyx ∞−∞= ，但其計算量過高，實不適用於
即時之計算，所幸距離計算擁有越靠近區間極小值
(Local minimum)，距離相對越小的特性如圖 4.2 所
示，圖 4.2 (d)相對於(c)更靠近區間極小值，是故利
用類似 Three step search(TSS)[16]跳躍搜尋的方
式，將 TSS 中 MSE(Mean Square Error)之計算用
EDistE,S(x0, y0)取代，而 TSS 初始搜尋距離 15 則用
下式: 
 
⎟⎠
⎞⎜⎝
⎛ −+−−+−
∈∈∈∈ 10,10,10,10, 10101010
maxmax,maxmaxmax
2
1 yyyyxxxx
EyySyyExxSxx      
(22) 
取代即可，其式子中 x0, x1, y0, y1表示輪廓 S
集合或輪廓 E 集合中之元素的(x, y)座標之 x 和 y
之值，該式子之物理意義為搜尋出 S 集合之成員
點，在空間座標內分佈之最大寬和 E 集合之最大
寬度之總和值跟高度之總和值取較大者，用此值作
為最大搜尋範圍，我們將該搜尋計算結果重新命名
為 MinSSDistE(S) (Minimum step search distance)。 
     
(a)     (b)     (c)     (d)     (f) 
圖 4.2 (a) E 之輪廓；(b) S 之輪廓；(c) EDistE,S(-7, 
-12)=32.7；(d) EDistE,S(-3, -5)=18.6；(f) 輪廓 E 及
輪廓 S 差異過大時之結果 
 
然而在實驗過程中發現，若 S 遠小於 E 則同
樣可取得極小之 MinSSDistE(S)，這樣勢必不符合我
們所期待之結果，是故將 EDistE,S(x0, y0)作修改，
得下式: 
 
( ) ( )( )ESES
yxEDist
yxtEDis SESE +−−
+=′
1
,1
, 00,00,
     (23) 
Journal of Information Technology and Applications        
Vol. 2, No. 3, pp. 133-142, 2008        
140 
分影響，若否則毫無影響。 
 
 
5.2 視訊物件之切割比較 
視訊物件之切割比較，我們第二章所提出的方
法是利用背景建構的方式，分割出前景物件，並且
在此跟[16]所提之背景建構之方式作比較。比較之
方式是利用兩個相異場景之影片(例圖 5.2, 5.3)，在
室內、室外等不同畫面下將影像逐頁依照人工的方
式圈選出，人眼所認定之前景像素所在的位置(例
圖 5.2, 5.3)，將其定為 Ground truth。該測試影片及
物件 ground truth是取自 2006年中華民國影像處理
與圖形識別學會 (IPPR) 運動物體偵測技術競賽
[17]，影像內容包含一個至多個運動中的物體(人與
車輛)。我們利用下列公式(24)，評估系統視訊物件
切割和 ground truth 的重疊正確比率 R: 
 
( ) ( )
( ) ( )yxMaskyxMask
yxMaskyxMask
R
ForegroundTruthGround
ForegroundTruthGround
,,
,,
_
_
∪
∩=
  (24) 
 
其中 MaskGround_Truth(x, y)表示人工所定義之
Ground truth，MaskForeground(x, y)表示系統所截選出
之前景。若 R 值趨近於 1，表示 MaskGround_Truth(x, y)
與 MaskForeground(x, y)十分近似；反之誤差越大，R
值越趨近於 0。 
    
(Frame10)  (Frame13)   (Frame17)  (Frame21) 
    
圖 5.2 依序為測試影片一之 Frame 10、13、
17、21 及其物件切割結果 
 
    
(Frame10)  (Frame13)   (Frame17)  (Frame21) 
    
圖 5.3 依序為測試影片二之 Frame 10、13、17、
21 及其物件切割結果 
 
 分別將測試影片一、二套用本文所提之方式
和[16]所提之方式，計算出各 Frame 之 R 值(公式
(24))，將其結果分別統計於圖 5.4 及 5.5，圖中藍
線代表本文所提之方式之 R 值，紅線代表[16]所提
之方式之 R 值。在絕大部分情況中本文所提之方
式皆優於[17]所提之方式。在圖 5.4 中藍線 150 個
Frame 之平均數值 0.595603，紅線為 0.567403。而
在圖 5.5 中藍線 150 個 Frame 之平均數值
0.370630，紅線為 0.348608，約可看出在本文所提
之方式均高於[16]所提之方式約 0.025~0.03 之間，
是故可以說明本文所提之背景建構之物件切割方
式是較為優異的。主要原因是我們所提出的雙閥值
找出前景的方法比較穩定，而且為了彌補相減結果
物體殘缺破碎的缺點，我們再行找出屬於前景的邊
緣，再將之整合。除此之外，我們所提出的前景物
件編號的方式，可以將破碎之物件完整補足，利用
降階取樣的方式將兩個可能原本是屬於相同一區
塊之物件，重新編號為正確物件。 
 
Test Data 1
0
0.2
0.4
0.6
0.8
1
1.2
1 16 31 46 61 76 91 106 121 136
Frame
本文方式
比對方式
 
圖 5.4 測試影片一之分析統計結果與比較 
 
0
0.5
1
1 23 45 67 89 111 133
Frame
Test Data 2
本
比
 
圖 5.5 測試影片二之分析統計結果與比較 
 
5.3 滯留判定 
在滯留物偵測之實驗中，我們直接取常用之測
試片段”Hall Monitor”影片來做檢測，整個結果如
圖 5.6所示。在一開始 Frame 62時偵測出有物體(編
號 11)進入警戒區，之後在 Frame 71 畫面中編號 11
物體仍舊移動中並未滯留，到 Frame 127 時，另一
編號 22 之物體進入警戒區，在 Frame 141 中編號
41 之滯留物出現，從 Frame 159 到 230，系統偵測
到編號 41 之物體仍然滯留不動，其餘物件皆移動
中(編號 11 及編號 22 之物體)。直到 Frame 248 時，
編號 41 之滯留物超出我們所預設的滯留時間，系
統發出警報(以 time out 標示)，Frame 289 中，編號
41 之物體依然滯留，系統持續發出警報。對於整
段影片的滯留事件偵測率達 100%。由此可看出我
們所發展的滯留偵測系統對警戒區間之滯留物偵
測有其成效。 
Journal of Information Technology and Applications        
Vol. 2, No. 3, pp. 133-142, 2008        
142 
“Stochastic tracking of 3D human figures using 
2D image motion”, 2000, Proc. of European 
Conference on Computer Vision, pp. 5-20. 
[12] Wong, K., and Lam, K., “A reliable approach 
for human face detection using genetic 
algorithm”, 1999, Proc. of the 1999 IEEE 
International Symposium on Circuits and 
Systems, vol. 4, pp. 499-502. 
[13] Lin, C.-H. and Wu, J.-L., “Automatic facial 
feature extraction by genetic algorithms”, 1999, 
IEEE Transactions on Image Processing, vol. 8, 
no. 6, pp. 834-845. 
[14] Gonzalez, R. C. and Woods, R. E., Digital 
Image Processing, 2nd ed., 2002, Prentice Hall. 
[15] Jing, X. and Chau, L.-P., “An efficient 
three-step search algorithm for block motion 
estimation”, 2004, IEEE Transactions on 
Multimedia, vol. 6, no. 3, pp. 435 - 438 
[16] Yang, T., Pan, Q., Li, S., and Li, J., “Multiple 
layer based background maintenance in 
complex environment”, 2004, Proc. of the 
Third International Conference on Image and 
Graphics, pp. 112 – 115. 
[17] http://archer.ee.nctu.edu.tw/contest/ 
and tracking system for crowded situations [4]. Masoud and Papanikolopoulos
presents a real-time system for pedestrian tracking in sequences of grayscale
images acquired by a stationary camera for pedestrian control scheme at inter-
sections [5]. There are a number of challenging issues worth further exploration
such as camera viewpoints, distance between human and camera, non-rigid hu-
man motion, occlusion, recognition speed, etc. In this paper, we attempt to
emphasize on the real-time pedestrian identiﬁcation and present a silhouette-
based method to identify humans and this method is robust enough to detect
objects of various shapes. Inspired by [6], matching involves a hierarchical search
tree approach over a distance-transformed images database hierarchy structure.
Thus, the proposed method is able to generate an eﬃcient representation from
oﬀ-line template examples, matching proceeds on-line based on a hierarchical
database which is established in advance.
The remainder of this paper is organized as follows. Section 2 reviews the
Distance-Transform (DT) approach and presents the construction of pedestrian
silhouette database. Section 3 illustrates the object matching algorithm. Sec-
tion 4 proposes DT-based fast matching algorithm with hierarchical search tree
for pedestrian identiﬁcation. Section 5 analyzes the experimental results. Con-
clusions are ﬁnally drawn in Section 6.
2 Distance Transform and Pedestrian Silhouette
Database Construction
The proposed system initially builds a statistical background model by com-
bining the Gaussian Mixture Model [7] and the approach proposed by Kaew-
TraKulPong [8] which leads in better performance in terms of learning
speed and accuracy [9]. A pixel is considered as a background, if matched with
one of the Gaussian distributions. Otherwise, the pixel is marked as a part of a
moving object (foreground). Then, the connected component operation is per-
form. To make up the fragment eﬀect of segmentation in foreground region, we
employ the opening and closing of morphological operations. Furthermore, to
reduce the susceptibility problems arisen from shadow inﬂuence, we employ the
HSV color space to distinguish shadow pixels from the ordinary foreground ob-
jects. Afterwards, we extract the silhouettes of moving objects from the resultant
foreground objects and select the silhouettes of moving pedestrian manually to
construct a pedestrian silhouette template database. To cluster these template
Video 
Objects
Edge Extraction
Hierarchical
Database
Distance
Transform
Multiple Search
Tree Creation
Fuzzy C-Mean
Clustering
Pedestrian Objects
Selection
Fig. 1. The ﬂow chart of constructing pedestrian silhouette database
434 D.-T. Lin and L.-W. Liu
(a) (b) (c) (d) (e) (f)
Fig. 3. Examples of edge detection and distance transform. (a) and (d) are the original
object images, Fig. 3(b) and (e) show the edge images after Canny Edge Detection,
and Fig. 3(c) and (f) present the resultant Distance Transform of the corresponding
edge images.
3 Object Matching Algorithm
Let S be the source edge image, E be the target edge image. The DT image
DTS , DTE of S and E can be computed by using Equation (2). Therefore, the
chamfer distance of these two images is deﬁned as
EDistS,E(dx, dy) =
1
|S|
∑
(x,y)∈S
DTE(x + dx, y + dy), (3)
where |S| denotes the number of pixels in S and (dx, dy) denotes the shift co-
ordinate when the source edge image S map to the target edge image E by
adding the coordination shift value (dx, dy). According to the deﬁnition of DT
image, the lower distance is, the better match between S and E at location
(x + dx, y + dy) is. Thus, our goal is to ﬁnd the minimum chamfer distance by
modifying shift coordinate (dx, dy) repeatedly, (i.e., minEDistS,E(dx, dy)). How
to ﬁnd the minimum EDistS,E(dx, dy) in real-time application can be consid-
ered as the motion estimation problem. One of the simplest matching algorithm
is the full search (FS) algorithm which provides an optimal solution by searching
all candidates within the target edge image E. There are a number of matching
algorithms can reduce the time complexity of FS algorithm. We apply the three
step search (TSS) algorithm in which the matching criterion of TSS is replaced
by the minimum chamfer distance minEDistS,E(dx, dy). Fig. 5 depicts the block
diagram of template image matching algorithm.
However, the experimental result shows that when the number of edges of S
is far less than that of E, the chamfer distance EDistS,E(dx, dy) will also be
small and this observation of minimum distance could lead in a ﬂaw. Therefore
we need to modify Equation (3) to overcome this drawback as follows.
EDist′S,E(dx, dy) =
1 + EDistS,E(dx, dy)
1− (||S| − |E||)/(|S|+ |E|) . (4)
When the diﬀerence between S and E is very large, the term (||S|− |E||)/(|S|+
|E|) in Equation (4) will approximate to 1 so that the denominator of Equa-
tion (4) becomes a small number and EDist′S,E(dx, dy) will become more larger
436 D.-T. Lin and L.-W. Liu
Video Object
Current
Frame
Edge Extraction
Hierarchical
Pedestrian Database
Chamfer Match Not Pedestrian
Yes
No
Pedestrian
Feature Match
Fig. 6. The ﬂow chart of the proposed pedestrain identiﬁcation procedure
DTS . Hence, for each video object, we will traverse in the pedestrian silhouette
database and use SObjin and DTS to compare the node with the chamfer distance
between the node and the video object. A video object is considered match if
the distance measure EDist′SV O,E(dx, dy) is below a pre-deﬁned threshold θ:
EDist′S,E(dx, dy) ≤ θ. Fig. 6 shows the ﬂowchart of pedestrian identiﬁcation
algorithm.
5 Experimental Results
To evaluate the performance of the proposed pedestrian identiﬁcation system,
we ﬁrst constructed a human silhouette database using a 20 minutes video clip
containing about 18000 frames. All moving objects were extracted using the pro-
cedure illustrated in Section 2, then the human objects were selected manually
and form the pedestrian silhouette template database. We ﬁnally compiled a
database of about 20000 pedestrian silhouette, divided and normalized them to
Fig. 7. A hierarchical search tree for pedestrian identiﬁcation (partial view)
438 D.-T. Lin and L.-W. Liu
To improve the system performance, the proposed system still has some issues to
be further explored, e.g. the human shape variation and object occlusion. These
problems pose a strong challenge for fully automated pedestrian identiﬁcation
and tracking.
References
1. Wren, C.R., Azarbayejani, A., Darrell, T., Pentland, A.P.: Pﬁnder: Real-time track-
ing of the human body. IEEE Transactions on Pattern Analysis and Machine In-
telligence 19(7), 780–785 (1997)
2. Haritaoglu, I., Harwood, D., Davis, L.S., Center, I.B.M.A.R., San Jose, C.A.: W4:
real-time surveillance of people and theiractivities. IEEE Transactions on Pattern
Analysis and Machine Intelligence 22(8), 809–830 (2000)
3. Stauﬀer, C., Grimson, W.E.L.: Learning patterns of activity using real-time track-
ing. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(8), 747–
757 (2000)
4. Sidla, O., Lypetskyy, Y., Brandle, N., Seer, S.: Pedestrian detection and tracking
for counting applications in crowded situations. In: Proceedings of the IEEE In-
ternational Conference on Video and Signal Based Surveillance, pp. 70–75. IEEE
Computer Society Press, Washington (2006)
5. Masoud, O., Papanikolopoulos, N.P.: A novel method for tracking and counting
pedestrians in real-timeusing a single camera. IEEE Transactions on Vehicular
Technology 50(5), 1267–1278 (2001)
6. Gavrila, D.M., Giebel, J., Perception, M., Res, D.C., Ulm, G.: Shape-based pedes-
trian detection and tracking. In: IEEE Intelligent Vehicle Symposium, vol. 1 (2002)
7. Stauﬀer, C., Grimson, W.E.L.: Adaptive background mixture models for real-time
tracking. In: Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, vol. 2, pp. 246–252 (1999)
8. KaewTraKulPong, P., Bowden, R.: An improved adaptive background mixture
model for real-time tracking with shadow detection. In: Proc. 2nd European Work-
shop on Advanced Video Based Surveillance Systems, AVBS 2001 (2001)
9. Lin, D.-T., Lee, H.-C.: Intelligent surveillance system for halt detection and people
counting. Journal of Information Technology and Applications 2(3), 133–142 (2008)
10. Borgefors, G.: Hierarchical chamfer matching: a parametric edge matching algo-
rithm. IEEE Transactions on Pattern Analysis and Machine Intelligence 10(6),
849–865 (1988)
H.264/AVC Video Encoder Realization and
Acceleration on TI DM642 DSP
Daw-Tung Lin and Chung-Yu Yang
Department of Computer Science and Information Engineering
National Taipei University
151, University Rd., San-Shia, Taipei, 237 Taiwan
dalton@mail.ntpu.edu.tw
Abstract. This work develops and optimizes H.264/AVC video encoder
on the TM320DM642 DSP platform. In order to transplant x264 source
program onto the DSP and to accelerate the coding speed, a series of
optimization methods have been proposed in this paper, including 2-D
fast mode decision, sub-pixel optimization for motion estimation, and
weighted matrix quantization. Furthermore, based on the architectural
features of TM320DM642, various system level optimization techniques
have been utilized. This paper focuses on the reduction of algorithm
complexity. Experimental results reveal that the optimized H.264 video
encoder retains satisfactory quality with very low degradation. The im-
plemented codec can achieve the coding speed of 22.6fps and more than
40fps for VGA (640×480) and CIF (352×288) resolution, respectively.
The proposed H.264 codec can be employed in many real-time applica-
tions.
Keywords: H.264/AVC encoder, TM320DM642 DSP, mode decision,
motion estimation, quantization, optimization.
1 Introduction
H.264/AVC video coding technology provides better bit rate saving and high
ﬂexibility of use in a broad variety of domains video compression standards.
However, the computational complexity of H.264 is much higher. It is important
to optimize and realize an H.264 encoder for real time applications. Special
purpose H.264 IC and embedded system are common solutions. Besides, the
embedded software approach possesses the advantage of ﬂexibility in updating
and adding new functions. The embedded system has more potential.
In the recent years, a large amount of works have been focused on the develop-
ment and optimization of H.264 algorithms. However, rare literatures disclosed
the impact of implementing H.264 on the embedded system. We believe that
creating H.264 video encoding/decoding application with good performance will
 This work was supported in part by the National Science Council, Taiwan, R.O.C.
grants NSC96-2622-E-305-001-CC3 and Ministry of Economics grant: Construction
of Vision-Based Intelligent Environment (II).
T. Wada, F. Huang, and S. Lin (Eds.): PSIVT 2009, LNCS 5414, pp. 910–920, 2009.
c© Springer-Verlag Berlin Heidelberg 2009
912 D.-T. Lin and C.-Y. Yang
5760MIPS) and SIMD, the DM642 process can execute eight 32 bits instructions
in each clock cycle. In other words, it is able to fetch a 256 bits long instruc-
tion from program memory in each clock cycle. There are two register banks in
DM642 CPU core, four ALU units on each side, totally eight ALU units can ex-
ecute instructions simultaneously. There are 16k bytes L1 data cache, 16k bytes
L1 code cache, 256k bytes programmable L2 SRAM. There is a DMA controller
on chip for data movement. Its EMDA oﬀers 2GB per second I/O bandwidth in
64 independent channels [9].
3 The Proposed Optimization Approach
The objective of this work is to implement and optimize H.264 video encoding
system on TI DM642EVM board based on x264 open source library. The x264 is
one of the major open source encoder developed in 2004 licensed under the GPL
[10]. It performs the best in term of time complexity compared with most of the
H.264-based algorithms. The coding speed of x264 is about 40-50 times faster
than that of reference encoder JM (Joint Model) based on our simulations with
video clips Foreman, Football, Akiyo and Water fall. As the major open-source
H.264 encoder, x264 plays the role of a near-complete monopoly of H.264 encoder
in the industry and has been used by many major corporations including Google
Video, MobileASL, Speed Demos Archive, and TASvideos.
Before doing optimization, we analyze the complexity of each x264 function in-
cluding the the kernel function clock cycle, execution time and code size. Figure 1
shows the run time percentage of each function. Apparently, motion estimation
(ME) and macroblock size mode decision are the most time consuming functions.
In order to transplant x264 source program onto the DSP and to accelerate the
coding speed, a series of optimization methods have been proposed including 2-D
fast mode decision, sub-pixel optimization for motion estimation, and weighted
matrix quantization. For the other operations such as x264 clip, abs, sad, satd,
etc., we rewrote these functions with assembly code and rearrange the instruction
utilizing the features of hardware architecture.
Motion compensation, 6%
Motion Estimation, 36%
DCT/IDCT, 2%
Mode Decision, 23%
Other Operation, 15%
Entropy, 10%
Quant/Deblock/Ratecontrol,7
%
Fig. 1. Runtime percentage analysis of x264 kernel functions
914 D.-T. Lin and C.-Y. Yang
Step5: Check the remaining modes.
Step6: Stop.
3.2 Sub-pixel Motion Estimation Optimization
One of the important motion estimation optimization issues is sub-pixel motion
estimation. Sub-pixel motion estimation usually ﬁnds a better position than
full-pixel motion estimation and therefore leads to a smaller SAD and bit rate.
For sub-pixel motion estimation, half-pixel search is frequently used in H.263,
MPEG-1, MPEG-2 and MPEG-4. Quarter-pixel search is adopted in MPEG-4
and H.264/AVC to achieve more accurate motion description and higher com-
pression eﬃciency. In our system, sub-pixel ME needs sub-pixel interpolation for
the whole search area before performing diamond search. However, it costs a lot
of computation complexity. After ﬁnding the best position in full-pixel precision,
it searches only eight neighboring half-pixel positions (denoted as square block in
Fig. 3). When the motion vector moves from position ”1” to position ”3” (shown
in Fig. 3), the integer search starts from point ”1”, then the half-pixel search will
ﬁnd point ”2”. Finally point ”3” will be found by quarter-pixel search. Due to
high complexity of the sub-pixel search algorithm, we use integer-pixel search to
engage the ﬁrst sub-pixel search, then use quarter-pixel search to ﬁnish the sub-
pixel motion estimation. This search method improves the sub-pixel searching
speed avoiding complicated computation and is useful for our DSP platform.
Integer pixel search positions
Half pixel search positions
Quarter pixel search positions
Match pixel
1
2
3
Fig. 3. Sub-pixel motion estimation
3.3 Quantization and DCT Optimization
The weighting quantization method of H.264 has been introduced from high
proﬁle. Diﬀerent quantization steps will be applied for various coeﬃcients and
regions with a scalar quantizer. The quantization method of H.264 is combined
with transformation and quantization, thus the design of weighting quantization
916 D.-T. Lin and C.-Y. Yang
Eight scaling tables are deﬁned in below: including intra luma 8×8, intra
chroma U4×4, intra chroma V4×4, inter luma 4×4, inter chroma U4×4, inter
chroma V4×4, intra luma 8×8 and inter luma 8×8. The coeﬃcients of the above
mentioned equations are suggested by JVT [13].
Intra4 × 4 Luma = Intra4 × 4 ChromaU
= Intra4 × 4 ChromaV =
⎡
⎢⎢⎢⎣
0 12 19 26
12 19 26 31
19 26 31 35
26 31 35 39
⎤
⎥⎥⎥⎦
,
(3)
Inter4 × 4 Luma = Inter4 × 4 ChromaU
= Inter4 × 4 ChromaV =
⎡
⎢⎢⎢⎣
0 13 18 21
13 18 21 24
18 21 24 27
21 24 27 30
⎤
⎥⎥⎥⎦
,
(4)
Intra8 × 8 Luma =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 10 13 16 19 24 26 28
10 12 16 19 24 26 28 31
13 16 19 24 26 28 31 33
16 19 24 26 28 31 33 35
19 24 26 28 31 33 35 37
24 26 28 31 33 35 37 39
26 28 31 33 35 37 39 42
28 31 33 35 37 39 42 44
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(5)
Inter8 × 8 Luma =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 12 14 16 18 19 21 22
12 13 16 18 19 21 22 24
14 16 18 19 21 22 24 25
16 18 19 21 22 24 25 27
18 19 21 22 24 25 27 28
19 21 22 24 25 27 28 30
21 22 24 25 27 28 30 31
22 24 25 27 28 30 31 33
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(6)
Y = Cf × CTf ⊗ Ef =
⎡
⎢⎢⎢⎣
1 1 1 1
2 1 −1 −2
1 −1 −1 1
1 −2 2 −1
⎤
⎥⎥⎥⎦ ×
⎡
⎢⎢⎢⎣
1 2 1 1
1 1 −1 −2
1 −1 −1 2
1 −2 1 −1
⎤
⎥⎥⎥⎦ ⊗
⎡
⎢⎢⎢⎣
a2 ab2 a
2 ab
2
ab
2
b2
4
ab
2
b2
4
a2 ab2 a
2 ab
2
ab
2
b2
4
ab
2
b2
4
⎤
⎥⎥⎥⎦
,
(7)
918 D.-T. Lin and C.-Y. Yang
100 150 200 250 300
33
34
35
36
37
38
39
40
41
Bitrate(Kb/s)
PS
NR
(dB
)
news.cif
JM14.1
X.264
Our Encoder
150 200 250 300 350 400 450 500 550 600 650
32
33
34
35
36
37
38
39
40
Bitrate(Kb/s)
PS
NR
(dB
)
foreman.cif
JM14.1
X.264
Our Encoder
(a) News (b) Foreman
400 600 800 1000 1200 1400 1600
32
33
34
35
36
37
38
39
40
Bitrate(Kb/s)
PS
NR
(dB
)
football.cif
JM14.1
X.264
Our Encoder
(c) Football
Fig. 6. The RD-curves of three codecs tested on three video clips: (a) News, (b) Fore-
man, and (c) Football
optimized and transplanted to DM642, and achieved the coding speed of 22.6fps
for VGA (640x480) size video. The performance indices bitrate (Kbs), PSNR
(db) and FPS of each optimization approach has been measured on three bench-
mark video clips: news, football and foreman. Table 3 reveals and compares
the progress of all optimization steps, including (a) direct porting of baseline
x264 encoder, (b) quantization and DCT optimization, (c) sub-pixel motion es-
timation optimization, and (d) 2-D fast mode decision optimization. The overall
performances with DSP implementation and optimization are listed in the most
right column in Table 3.
In order to evaluate the PSNR and bitrate of the optimized encoder, we
compare our encoder with JM14.1 and x264 on Intel CPU T2250 @ 1.73GHz
platform with 1GB DDR RAM. Figures 6(a), (b) and (c) demonstrate the RD-
curves of these three codecs with diﬀerent QP values 26, 28, 30, 32 and 34 for
CIF video clips News, Foreman and Football, respectively. Although our encoder
results in less PSNR (all less than 2db), the proposed encoding speed is increased
up to 30-50 fps which is faster than the orignal X.264.
920 D.-T. Lin and C.-Y. Yang
3. Zhuo, L., Wang, Q., Feng, D.-D., Shen, L.: Optimization and implementation of
H.264 encoder on DSP platform. In: IEEE International Conference on Multimedia
and Expo., 232–235 (2007)
4. Li, Z., Xing, Q., Zhu, X.: H.264 video encoder implementation and optimization
based on DM642 DSP. In: IEEE International Conference on Networking, Sensing
and Control, ICNSC 2008, pp. 891–894 (2008)
5. Wei, Z., Cai, C.: Realization and optimization of DSP based H.264 encoder. In:
Proceedings. 2006 IEEE International Symposium on Circuits and Systems, ISCAS
2006, p. 4 (2006)
6. Wang, H.-J., Hou, Y.-Y., Li, H.: H.264/AVC video encoder algorithm optimization
based on TI TMS320DM642. In: Third International Conference on Intelligent
Information Hiding and Multimedia Signal Processing, IIHMSP 2007, vol. 1 (2007)
7. Werda, I., Chaouch, H., Samet, A., Ayed, M.A.B., Masmoudi, N., Akbal, E., Er-
gen, B., Muljadi, H., Takeda, H., Ando, K., et al.: Optimal DSP-based motion
estimation tools implementation for H.264/AVC baseline encoder. IJCSNS 7(5),
141 (2007)
8. Wiegand, T., Sullivan, G.J., Bjntegaard, G., Luthra, A.: Overview of the
H.264/AVC video coding standard. IEEE Transactions on Circuits and Systems
for Video Technology 13(7), 560–576 (2003)
9. Texas Instrument. TMS320C64x DSP Video Port/VCXO Interpolated Control
Port (2006)
10. LFree Software Foundation. GNU operating system, http://www.gnu.org/
11. Chang, C.-Y., Pan, C.-H., Chen, H.: Fast mode decision for P-frames in H. 264.
In: Picture Coding Symposium (PCS) (2004)
12. Richardson, I.E.G.: H. 264 and MPEG-4 video compression. Wiley, Chichester
(2003)
13. J.V. Team. Draft ITU-T recommendaation and ﬁnal draft international standard
of joint video speciﬁcation (March 2003)
14. Malvar, H., Hallapuro, A., Karczewicz, M., Kerofsky, L.: Low-complexity transform
and quantization in H.264/AVC. IEEE Transactions on Circuits and Systems for
Video Technology 13(7), 598–603 (2003)
15. Texas Instrument. TMS320C6000 Assembly Language Tools v6.0 Beta (2005)
16. Texas Instrument. Code Composer Studio User’s Guide (2000)
17. Texas Instrument. TMS320C6000 DSP/BIOS User’s Guide (2000)
18. Texas Instrument. TMS320C64x/C64x+ DSP CPU and instruction set reference
guide
19. Texas Instrument. Video Encoding Optimization on TMS320DM64x/C64x (2004)
20. Texas Instrument. TMS320C64x DSP Two-Level Internsl Memory Rﬀerence Guide
(2006)
