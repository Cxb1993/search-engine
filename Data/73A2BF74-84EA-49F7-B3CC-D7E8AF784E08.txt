 2
行政院國家科學委員會專題研究計畫成果報告 
研製具有通過汽車路考測驗項目技能之影像伺服自走機器人(II) 
計畫編號：NSC 95-2221-E-129 -002 
執行期限：95 年 8 月 1 日至 96 年 10 月 31 日 
主持人：簡忠漢   聖約翰科技大學電機工程系 
計畫參與人員：黃耀璋、林綉莉 
 
一、中、英文摘要 
本計畫之目標為實作一個具有通過汽車
路考測驗項目之影像伺服自走車。在本年度
的計畫中，我們針對自走車經由影像追蹤路
面標線，以進行影像導航自動駕駛之應用，
實作一套影像伺服控制系統。首先研製一套
道路標線偵測與追蹤系統，達成偵測與追蹤
路面標線參數。其次基於標線之影像量測資
訊，我們實作一套影像導航控制系統。並採
用回授線性化控制技術，設計一套影像伺服
控制法則，可控制自走車航行於路面並與標
線保持一段安全距離。最後為驗證系統效
能，我們自行設計一環場道路模型，並於其
上成功地達成自動環場航行實驗。 
 
關鍵詞：標線偵測、標線追蹤、影像導航、
影像伺服、回授線性化。 
 
Abstract 
In this project we have successfully 
implemented a visual servo system to navigate a 
nonholonomic mobile robot tracking a path 
automatically. We first propose a lane detection 
and lane tracking algorithm to track the line 
parameters of multiple lanes even subject to 
temporal disappearance. Then by utilizing the 
feedback linearization technique, we 
implemented a visual servo controller to 
successfully navigate the mobile robot following 
an elliptical path and keeping an offset distance 
parallel to the path. 
Keywords: lane detection, lane tracking, visual 
navigation, visual servo, feedback linearization. 
 
二、緣由與目的 
 
根據國際機器人協會/聯合國歐洲經濟委
員會(IFR/UNECE) 於 2004 年出版之統計資
料指出[1]：2003 年全球服務用智慧型機器人
已達 132 萬台，總價值約 37.9 億美元。預估
2004 至 2007 年服務用智慧型機器人將達
670.4 萬台，總價值約有 96.7 億美元，是一個
極俱發展潛力的市場，如圖 6 所示。因此世
界先進國家(如美國、日本、歐盟、南韓)已積
極展開許多智慧型機器人之研發計畫[2]-[5]。 
本計畫之目標即希望透過模擬實現一具
體生活化之目標，激發研究人員之興趣，以
進行智慧型機器人各項技術之實務整合研
究。研究的重點特別著重於發展機器人視
覺，主要目標是研究透過各種影像處理技
術，實現智慧型機器人之影像辨識，影像追
蹤與影像伺服控制系統，並整合此三種技
術，以達成具備通過汽車路考測驗項目之智
慧型影像伺服自走車。 
與本計畫相關類似的研究，例如 Kato 和 
Takeno 等學者提出ㄧ套驗證交通規則應用系
統(TRAS)[6]於多自走型機器人環境的基礎實
驗結果。Adorni 等人[7]探討在室內環境中多
個自走機器人自主航行之應用。透過 CCD 攝
影機擷取影像進行交通標誌辨識，並依據交
通規則協調多個自走機器人航行時可能面臨
的交通衝突情況。Nett 等學者[8]則規劃一套
可用於未來式交通管理系統的車輛協同無線
通訊合作機制。2004 年李迪章與宋開泰教授
等[9]提出一適用於家用機器人之快速影像追
蹤與停車控制器。 
在本年度的計畫中，我們針對自走車追
蹤路面標線進行影像導航之自動駕駛應用，
設計並實作其影像伺服控制系統。我們首先
研製一套道路標線偵測與追蹤系統，其採用
直線形狀建立標線之參數模型，並基於隨機
霍氏轉換(RHT)技術[10]-[12]，於經 IPM 轉換
之影像上，以最小平方差法進行直線擬合
(fitting)，以偵測路面標線直線參數。其具有
可同時追蹤多個標線直線參數之功能，且可
克服車輛航行時標線短暫消失之不理想情
 4
如文獻[10]-[12]所述 RHT 技術之首要步
驟為：隨機取 n 個像素點以估測形狀曲線參
數，如此具有降低計算複雜度以增進計算效
能之優點。故本計畫亦採用隨機取 n 個像素
點以估測影像中之可能標線。在隨機取點之
前，須先判斷道路影像邊點數是否適當；因
點數過少可能得不到足夠的道路標線資訊，
點數過多則可能是背景過於複雜，兩者皆可
能造成徒勞無功而浪費系統計算時間。 
 
演算法 1：隨機霍氏轉換偵測路面標線 
步驟一、自影像邊點資訊中隨機取 1 點，搜
集其鄰近範圍內之邊點集合。倘若隨機點
鄰近範圍邊點數過少，則重新取點。 
步驟二、以隨機點之鄰近邊點集合為資料
點，透過最小平方差法進行直線參數擬合。 
步驟三、依序將擬合的直線參數存入累計陣
列中。若新求得之參數與陣列中的參數相
近，則直接將陣列中參數與新求得參數加
權合併，並將其出現計數值增加 1。否則，
則於陣列中新增一候選直線參數。如圖 4
所示為某次直線偵測之結果，白色部分為
真實道路標線資訊，灰色虛線為擬合之直
線。其中真實道路標線皆對應 2 條以上之
擬合直線參數，因此確有合併參數之必要。 
步驟四、當累計陣列中之候選直線參數其出
現計數值超出預設門檻值時，將候選直線
與真實道路影像資訊比對並計算其相符點
數，若其點數滿足比對之門檻值，即視為
正確道路標線參數。為避免此參數於後續
重複偵測，浪費系統資源。因此須對已偵
測之正確道路標線，消去其邊點。如此對
於影像中剩餘道路邊線之隨機取點命中率
亦會提高，大大地改善計算效能。 
步驟五、是否已達偵測次數上限，若否則回
到步驟一繼續隨機取點偵測。 
 
接續前述圖 2 利用 IPM、二值化、邊緣
偵測技術取得標線影像資訊。圖 5 為以演算
法 1 進行道路標線偵測之實驗結果，其成功
地在道路邊緣影像中擬合出 3 條路面標線，
繪於圖 5(a)，並將擬合直線經透視投影轉換至
原始影像，結果以藍色直線繪於圖 5(b)。由圖
可見標線偵測擬合所得之直線與原始路面標
線影像幾乎重合。且驗證上述道路標線偵測
演算法可同時偵測比對多個路面標線參數。 
 
圖 3. 道路標線偵測流程圖 
 
 
圖 4. 某次標線偵測之結果，白色像素為真實標線邊
點，灰色虛線為擬合之直線。為避免單一標線對應多
個直線參數，須對相近直線參數進行合併。 
 
       
(a)        (b) 
圖 5. 道路標線偵測實驗。參考圖 2 所得之道路標線影
像資訊。(a)道路標線偵測並擬合於邊緣化影像之結
果。(b)偵測擬合之直線經透視投影轉換至原始影像
中，由圖可見擬合直線與車道線幾乎重合。 
 
1.3 道路標線追蹤 
在偵測到道路標線參數並將其紀錄於參
數陣列之後，本小節介紹道路標線追蹤之方
法。 
 
演算法 2：適應性多道路標線追蹤演算法 
步驟一、依據先前偵測所得道路標線參數建
立參數陣列，並為每個參數設定追蹤計數
器初值。 
步驟二、於陣列中參數對應之標線附近設定
追蹤視窗範圍，並搜集範圍內之邊點。判
斷邊點數是否足夠，如圖 6(a)所示，若上次
擬合標線(虛線)與現在真實道路標線未過
 6
φ
θ
Ld
L
 
圖 8. 自走車於 2D 平面道路上沿標線航行並保持一段
安全距離 
 
  
(a)     (b) 
  
(c)     (d) 
  
(e)     (f) 
圖 9. 自走車成功地追蹤直線標線且直行航行之實驗
片段。航行全程為 3 公尺，花費 7 秒。 
 
上述實驗經由自走車上之攝影機拍攝之
片段如圖 10 所示。在實驗初始時，刻意使自
走車航向偏向右邊(圖 10(a))，而不與道路標
線平行。隨後影像導航系統立即控制自走車
航向平行於標線，並與標線保持固定之距
離。圖中黃線為真實標線，藍線為擬合之標
線，藍線上綠色段為自走車預看點橫向與標
線之交點。自走車與標線相對偏移距離誤差
)d(dL
∗
−
記錄於圖 11(a)，航向角度誤差 )-( Φθ
則紀錄於圖 11(b)。其中為避免自走車對標線
資訊反應過度靈敏造成蛇行、顫動之不舒適
的駕駛情況，因此設定距離誤差在 ±10 
pixels(約±2 公分)內，角度誤差在±0.05 rad
內，即停止控制。由圖可知誤差在 2 秒內即
收斂於容許誤差範圍之內，足見系統之穩定
性。 
 
3.2 自走車於橢圓環場道路之航行實驗 
本小節安排自走車於我們自行設計之橢
圓道路模型上進行自動環場航行實驗，以展
示影像導航控制之自動駕駛系統效能。 
 
(a)        (b)         (c)         (d) 
 
(e)        (f)         (g)          (h) 
圖 10. 自走車直線航行實驗以影像直接量測標線之資
訊。初始航向偏向右邊標線而不與其平行。(a)~(d)為
影像導航控制系統成功地控制自走車航向平行於道路
標線，之後始終與標線保持固定之距離。 
 
 
(a)相對偏移距離誤差 )d(dL ∗− 軌跡圖 
 
(b) 航向角度誤差 )-( Φθ 軌跡圖 
圖 11. 自走車直線航行實驗追蹤誤差軌跡圖。考慮駕
駛舒適性，實驗設定距離誤差在±10 pixels (約±2 公分)
內與角度誤差在±0.05rad(約±4.5o)內即停止控制，由圖
可知誤差在 2 秒內即收斂於容許誤差範圍之內，足見
系統之穩定性。 
 
圖 12 為自走車自動駕駛航行於橢圓環場
道路之實驗片段。自走車依據影像直接量測
之標線資訊，以順時針方向，約 15 秒環繞一
周之速率航行(約 0.33 m/s)。由圖可知自走車
無論航行於直線段或曲線段道路，均能保持
沿標線方向航行，且與標線保持一段安全距
離(不壓線)。 
圖 13 為由自走車上之攝影機內拍攝之實
驗片段。由圖可知自走車會隨著道路標線彎
曲變化進行適應性追蹤，並以自走車預看點
與標線相對偏移距離為依據，伺服控制自走
車沿標線航行且與其保持安全距離。因而驗
證本文實現之自走車「標線偵測與追蹤系統」
與「影像導航伺服控制器」之正確性、即時
性及強健性。圖 14 為實驗追蹤誤差之軌跡
圖。同樣地自走車於橢圓環場航行實驗中亦
 8
有關追蹤路面標線進行影像導航控制之
成果已發表於 2006 IEEE Conference on 
Systems, Man, and Cybernetics [16] (如附件
一)。 
我們更將上述成果應用於追蹤牆壁邊
線，以實作可於室內巡邏導航之保全機器
人。並參加 96 年 3 月「家用機器人產品創作
競賽」獲得佳作，獎狀如附件二。 
五、參考文獻 
[1] 白忠哲，”智慧型服務機器人商機可期”，ITIS 產業
觀察 
[2] K. Chinzei, N. Hata, , F. Jolesz, and R. Kikinis, 
“Surgical Assist Robot for the Active Navigation in 
the Intraoperative MRI: Hardware Design Issues”, 
IEEE International Conference on Intelligent Robots 
and Systems, pp. 727-732, Oct. 2000. 
[3] K. Wada, T. Shibata, T. Saito, and K. Tanie, “Robot 
Assisted Activity for Elderly People and Nurses at a 
Day Service Center,” IEEE International Conference 
on Robotics & Automation, pp.1416–1421, May 2002. 
[4] B. Graf and A. Hans, “Robotic home assistant 
Care-O-bot II,” Second Joint Conference and the 24th 
Annual Fall Meeting of the Biomedical Engineering 
Society, pp. 2343–2344, 2002. 
[5] K. Osuka, “Rescue robot contest in 
RoboFesta-KANSAI 2001”, 26th Annual Conference 
of the IEEE Industrial Electronics Society, pp. 
132–137, Oct. 2000. 
[6] S. Kato and J. Takeno, “Fundamental studies on the 
application of traffic rules to the mobile robot world: 
proposal and feasibility study of the traffic rules 
application system (TRAS) ”, Fifth International 
Conference on Advanced Robotics, pp. 1063-1068, 
June 1991 
[7] G. Adorni, M. Mordonini, and A. Poggi, 
“Autonomous agents coordination through traffic 
signals and rules”, IEEE Conference on Intelligent 
Transportation System, pp. 290-295, Nov. 1997 
[8] E. Nett, M. Gergeleit, and M. Mock, “Mechanisms for 
a reliable cooperation of vehicles”, Sixth IEEE 
International Symposium on High Assurance Systems 
Engineering, pp. 75-81, Oct. 2001 
[9] T. Lee, C. Tsai, and K. Song, “Fast Parking Control of 
Mobile Robots: A Motion Planning Approach with 
Experimental Validation,” IEEE Trans. On Control 
Systems Technology, Vol. 12, no. 5, pp. 661-676, 
September, 2004. 
[10] L. Xu, E. Oja and P. Kultanen, “A New Curve 
Detection Method: Randomized Hough Transform 
(RHT)”, Pattern Recognition Letters, pp. 331-338, 
1990. 
[11] R. A. McLaughlin, “Randomized Hough Transform: 
Better Ellipse Detection”, IEEE TENCON-Digital 
Signal Processing Applications, pp.409-414, 1996. 
[12] A. Schuler, “The Randomized Hough Transform 
Used for Ellipse Detection”. 
[13] 吳添寶，”以形狀資訊進行目標追蹤之影像伺服
自走車”，聖約翰科技大學電機工程研究所，93 學
年碩士論文 
[14] A. Isidori, Nonlinear Control Systems, 2nd ed., 
Communications and Control Engineering Series, 
Springer, Berlin, 1989. 
[15] 簡忠漢，吳添寶，黃家賜，楊志豪，“適應性形
狀追蹤影像伺服自走車之實作”，2004 中華民國自
動控制研討會，pp. 1148-1153，大葉大學，三月，
2004 
[16] Jong-Hann Jean*, Tien-Pao Wu, and Feng-Li Lian, 
“A Visual Servo Controller for Lateral Navigation of 
Mobile Vehicles in Path Tracking Applications”, 2006 
IEEE Conference on Systems, Man, and Cybernetics, 
Taipei, Taiwan, Oct. 8-11, 2006. 
[17] Jong-Hann Jean*, Jian-Hong Lai, and Chia-Wei 
Chang, “Development of a Self-navigation Patrol 
Robot in Indoor Environments Based on Ultrasonic 
and Vision Data Fusion,” ICAR 2007, The 13th 
International Conference on Advanced Robotics, pp. 
436-441, Jeju, Korea, August 21 - 24, 2007 
 
II. FORMULATION OF THE LATERAL NAVIGATION PROBLEM 
 In this paper we consider the mobile vehicle navigating at 
low speed such that its dynamic effects can be ignored. 
Therefore, we describe the lateral navigation problem only 
using the kinematic model of the vehicle. In the following, we 
present the case of mobile vehicles of the unicycle type. It is 
easy to extend the results to those of the bicycle type. 
A. Kinematics of the Mobile Vehicle 
 Consider a wheeled mobile vehicle of the unicycle type 
carrying a fixed camera and performing the task of navigating 
along a path on a 2D plane, as depicted in Fig. 1. A coordinate 
frame {Rm} is attached to the center of mass of the vehicle, 
namely Pm, with the x-axis being along the heading direction 
of the vehicle. Let (x, y) denote the coordinate of the point, Pm, 
with respect to an inertial frame {R0} and θ be the heading 
angle of the vehicle with respect to the x-axis of {R0}. 
 We assume the path is highlighted by a lane marker, such 
as the white lane lines, the road edges, or the wall baselines, 
which can be visually detected from the camera images. As 
shown in Fig. 1, the lane marker is approximated as a line with 
an inclined angle φ  with respect to the x-axis of {R0}. 
 Assume that the vehicle moves without slipping. Then the 
kinematic equation of the vehicle can be written as follows, 
 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
ω
θ
θ
θ
sin
cos
v
v
y
x
&
&
&
, (1) 
where v and ω, are the linear velocity and angular velocity of 
the vehicle, respectively, and act as the control variables. In 
the context of lateral navigation, we only control the heading 
angle of the vehicle. Therefore, the linear velocity is a 
redundant control variable, so we can make the following 
assumption. 
Assumption 1: The vehicle always moves forward in a 
constant speed, i.e., its linear velocity v is a positive constant. 
 Consequently, only the angular velocity is treated as input 
to navigate the vehicle. Moreover, for compensating the time 
delay induced by image processing, the vehicle is actually 
navigated according to the relative position and orientation of 
the lane marker, measured through Point PL at a constant look-
ahead distance L from the vehicle. As indicated in Fig. 1, the 
relative position is measured as the look-ahead lateral offset 
Ld  and the relative orientation is then the look-ahead angle 
deviation, i.e., φθ − . 
 In case that the lane markers are curves, for example, 
around corners of road edges, we still can use the line model to 
locally approximate the lane markers to the first order. Under 
such circumstances, we measure the look-ahead lateral offset 
Ld  and the look-ahead angle deviation φθ −  with respect to 
the tangent line of the lane marker at the look-ahead distance 
L, as depicted in Fig. 2. 
φ
θ
Ld
L
 
Fig. 1 A wheeled mobile vehicle with a fixed camera navigates along a lane 
marker of line shape on a 2D plane. 
 
φ
φθ −
Ld
 
Fig. 2 Visual sensing of the relative position and orientation between the 
vehicle and the lane marker at a look-ahead distance from the vehicle. 
 
Remark: For vehicles of the bicycle type, the kinematic 
equation can be similarly described by equation (1) but with 
the angular velocity ω being replaced as 
  )tan(α
W
vω = , (2) 
where W is the distance between the front wheel and the rear 
wheel and α  is the steering angle of the front wheel. 
B. Visual Detection of the Lane Marker 
 The function of visual detection of the lane marker is 
twofold. First, it is used to determine if the vehicle is currently 
located on a safe and proper position on the path. Second, it 
provides measurements of the relative position and orientation 
between the vehicle and the lane marker, which are essential to 
navigating the vehicle along the path as mentioned. In 
addition, for realization of the lane-marker detection system, 
there are three main concerns. 
1) Real-Time Performance: The system is required to process 
an image sequence up to 30 images per second in real time. 
2) Automatic Operation: The system can automatically detect 
the lane markers and continuously track their locations. 
3) Robustness: The system needs to be robust to nonideal 
situations, e.g., subject to temporary occlusion of the lane 
markers. 
 In this paper, we adopt the lane-marker detection system 
developed in the work [17], which has the advantage to detect 
multiple lane markers in real time even subject to temporal 
occlusion. In the following we make a brief introduction on the 
implementation of the system. 
 First, due to the perspective effect, the position and 
orientation of the lane marker directly measured from the 
camera images are distorted. Consequently, the system utilizes 
the inverse perspective mapping (IPM) [19] to transform the 
24476
Equation (8) implies the error signal Ld
~  is bounded and 
convergent to zero exponentially, i.e. 
 -atLL ecd <
~  (10)  
for some positive constants cL and a. 
Define the following positive function V as  
 2~
2
1 θ=V . (11) 
If the condition (7) holds, the time derivative of V along the 
error equation (9) can be computed as 
 
at-L
at-L-atL
-at
L
L
L
e
vc
ckc
e
vc
ckce
vc
ckc-θv-c
θeckcθv-c
θdkθv
LθdV
2
1
2
2
2
1
2
22
1
2
1
2
2
1
4
)(
4
)()
2
~(
~~
~)~)~tan((
)~tan(
1
≤
+=
⋅+≤
⋅⋅−⋅−
+
=
&
 (12) 
From equation (12), we can imply the function V is bounded, 
and hence the error signal θ~  is bounded, i.e. 
 θθ c<
~  (13) 
for some positive constant θc . As a result, we have 
 
-at
L
-at
L
ecckcvVc-
θeckcθv-cV
θ21
2
2
1
2
~~
+≤
⋅+≤&  (14) 
Inequality equation (14) implies the function V together with 
the error signal θ~ converge to zero exponentially. □ 
 
Remark: The condition (7) is essential in the proof of Theorem 
1. It is obvious that condition (7) will hold if there exists a 
small positive constant δ  such that  
 δL-θdL <)
~tan( . (15) 
Above condition holds for sufficiently large look-ahead 
distance or sufficiently small angle deviation and lateral offset. 
IV. EXPERIMENTAL RESULTS 
 In this section we provide several experimental results to 
verify the performance of the visual servo controller. 
A. Mobile Robot System 
 To provide a testbed for subsequent experiments of visual 
navigation, we adopt a previously established mobile robot of 
the differential-drive type, which has two rear driving wheels 
and one front castor wheel as shown in Fig. 4.  
      
Fig. 4 A mobile robot of the differential-drive type and its hardware setup. 
 
 The mobile robot is actuated by a motion control module 
including two DC motors, batteries, and a PWM speed control 
board based on an 8051 single chip processor. The linear 
velocity v and the angular velocity ω of the mobile robot, can 
be formulated in terms of rotating rates of the two driving 
wheels, 1ψ&  and 2ψ& , as 
 
)(
)(
2
21
21
ψψω
ψψ
&&
&&
−=
+=
l
r
rv
, (16) 
where r denotes the radius of the wheels and l denotes the 
distance between the two driving wheels. 
 An NTSC CCD camera is mounted toward the heading 
direction of the mobile robot. In addition, the mobile robot is 
equipped with a single board computer including a built-in 
video capture module such that it can grabs images of 320x240 
pixels in a 30Hz frame rate and performs the path tracking task 
in real time. 
 In subsequent experiments, we implement the controller 
together with the lane detection system with Intel® open 
source computer vision (OpenCV) library [12] under 
Microsoft® Visual C++™ integrated development 
environment. 
B. Navigating along a Straight Lane 
 In this experiment, we apply the proposed visual servo 
controller to navigate the mobile robot tracking along a 
straight lane with a constant speed about 0.4m/s. To avoid 
undesirable chattering in lateral navigation, we stop using the 
visual servo controller when tracking errors fall into a 
predefined boundary layer. As shown in Fig. 5, the mobile 
robot successfully moves in coincidence with the lane 
direction and maintains a safe distance to the lane line. The 
error trajectories of the look-ahead lateral offset and the look-
ahead angle deviation are depicted in Fig. 6(a) and 6(b) 
respectively. We can see that the tracking errors of the lateral 
offset measured from the images soon converge within the 
boundary layer of ±10 pixels in about 1.5 seconds. At the same 
time, the heading angle of the mobile robot soon coincides 
with the lane direction and the tracking errors are confined 
within the boundary layer of ±0.05 radian (±3 degrees). This 
experiment validates the tracking stability of the proposed 
visual servo controller. 
44478
 Φn
n+1
θn+1 - 90o
θn - 90o
sn+1
sn
α1
α3α2
(xn+1 , yn+1)
(xn , yn)
dn(v∆t)
dn+1(0)
dn(L)
dn+1(L)
L
L
 
Fig. 9 An infinitesimal displacement of a wheeled mobile vehicle navigating 
along a lane marker of line shape on a 2D plane. 
 
 Let )( nn sd  denote the lateral offset measured at a look-
ahead distance ns viewed from the point (xn, yn) with the 
heading angle nθ . Similarly, )( 11 ++ nn sd  is the lateral offset at 
a look-ahead distance 1+ns viewed from the point (xn+1, yn+1) 
with the heading angle 1+nθ . Based on the line model of the 
lane marker, we have the following equations.  
 ))tan(()()( φ−−+= nnnnn θLsLdsd  (20) 
 ))tan(()()( 11111 φ−−+= +++++ nnnnn θLsLdsd  (21) 
 Observing the triangle with three angles denoted as 1 α , 
2 α , and 3 α , and utilizing the Sine law, we have the following 
relationship.  
 
nnnnnn θθ  ,  α   ,   αα −=−=−== +++ 111213 180 φφφφ , (22) 
 
)tan(
sin
sin
)))tan(()((                
)tan((0))(
)sin(
)(
)sin(
(0)
1
1
111
32
1
φ
φ
φφ
φ
−⋅
+−−∆+=
−⋅+=⇒
∆
=
+
+
+++
+
n
n
n
nn
nnn
nn
θL
θLtvLd
θLdLd
α
tvd
α
d
 (23) 
 Using some trigonometric identities, we can approximate the 
time rate of the lateral offset at the look-ahead distance L as 
nnnnn
nnnn
nn
ωθLωθθL
ωθLdθv
t
LdLd
)(sec))tan(tan(                   
))tan(()tan(≒)()(
2
1
1
1
φφφ
φφ
−⋅+−−⋅−
−+−⋅
∆
−
+
+
+  (24) 
 As a result, the time derivative of the look-ahead offset 
Ld  can be written as follows. 
 
)tan())tan((     
)tan()tan(
φφ
φφ
−⋅++−=
+−+−⋅=
θvωLθd
Lωθωdθvd
L
LL
&
 (25) 
  □ 
REFERENCES 
[1] S.Tsugawa, “Vision-based vehicles in Japan: the machine vision systems 
and driving control systems,” IEEE International Symposium on 
Industrial Electronics, pp. 278–285, June 1993. 
[2] Richard Bishop, “A survey of intelligent vehicle applications worldwide,” 
Proceedings of the IEEE Intelligent Vehicles Symposium 2000, pp. 25-30, 
October 3-5, 2000. 
[3] G. N. Desouza, A. C. Kak, “Vision for mobile robot navigation: a 
survey,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 
24, pp. 237-267, Feb. 2002. 
[4] K. Tomita and S. Tsugawa, “Visual navigation of a vehicle along roads: 
the algorithm and experiments,” Proceedings of Vehicle Navigation and 
Information Systems Conference, pp. 419 – 424, 1994. 
[5] S. Kato, K. Tomita, and S. Tsugawa, “Visual navigation along reference 
lines and collision avoidance for autonomous vehicles,” Proceedings of 
the 1996 IEEE Intelligent Vehicles Symposium, pp. 385-390, Sept. 1996. 
[6]  J. Kosecka, R. Blasi, C. J. Taylor, and J. Malik, “Vision-based lateral 
control of vehicles”, IEEE Conf. on Intelligent Transportation System, pp. 
900 – 905, Nov. 1997. 
[7] N. Schlegel, P. Kachroo, J. A. Ball, and J. S. Bay, ”Image processing 
based control for scaled automated vehicles,” IEEE Conference on 
Intelligent Transportation System, pp. 1022-1027, Nov. 1997. 
[8] S. Kato, K. Tomita, and S. Tsugawa, “Lane-change maneuvers for vision-
based vehicle,” IEEE Conference on Intelligent Transportation System, 
pp. 129-134, Nov. 1997. 
[9] J. Kosecka, R. Blasi, C. J. Taylor, and J. Malik, “A comparative study of 
vision-based lateral control strategies for autonomous highway driving,” 
IEEE International Conference on Robotics and Automation, pp. 1903-
1908, May 1998. 
[10] S. K. Gehrig and F. J. Stein, “A trajectory-based approach for the lateral 
control of car following systems,” IEEE International Conference on 
Systems, Man, and Cybernetics, pp. 3596-3601, Oct. 1998. 
[11] Y. Ma, J. Kosecka, and S. Sastry, “Vision guided navigation for a 
nonholonomic mobile robot,” IEEE Trans. on Robotics and Automation, 
vol. 15, no. 3, pp. 521-536, June, 1999. 
[12] J. Choi, C. Kim, J. Bae, A. Hong, M. Lee, and F. Harashima, “Vision 
based lateral control by yaw rate feedback,” IEEE Conf. on Industrial 
Electronics, pp.2135-2138, 2001. 
[13] V. Cerone, A. Chinu, and D. Regruto, “Experimental results in vision-
based lane keeping for highway vehicles,” Proceedings of the 2002 
American Control Conference, pp. 869-874, May 2002. 
[14] Y. Xu, K. Li, Maying, Gaofeng, Y. Zhao, and Yuanyi, “Human 
simulating control algorithm on vehicle lateral tracking,” Proceedings of 
American Control Conference, pp.4728-4733, July, 2004. 
[15] S. Benhimane, E. Malis, P. Rives and J. R. Azinheira, “Vision-based 
control for car platooning using homography decomposition,” IEEE 
International Conference on Robotics and Automation, pp. 2161-2166, 
April, 2005. 
[16] S. Wu, H. Chiang, J. Perng, T. Lee, and C. Chen, “The automated lane-
keeping design for an intelligent vehicle,” IEEE Intelligent Vehicles 
Symposium, pp. 508-513, June 2005. 
[17] T. Wu, “Robust Visual Servo System for Tracking an Arbitrary-Shaped 
Object by a New Active Contour Method”, Master Thesis, Dept. of 
Electrical Eng., St. John’s University, Taiwan, 2005. 
[18] L. Xu, E. Oja and P. Kultanen, “A New Curve Detection Method: 
Randomized Hough Transform (RHT)”, Pattern Recognition Letters, pp. 
331-338, 1990. 
[19] G. Y. Jiang, T. Y. Choi, S. K. Hong, J. W. Bae, and B. S. Song, “Lane 
and Obstacle Detection Based on Fast Inverse Perspective Mapping 
Algorithm”, IEEE Conf. on Systems, Man, and Cybernetics, pp. 2969-
2974, October, 2000 
[20] J. Slotine and W. Li, “Applied Nonlinear Control,” Prentice Hall, 1991. 
[21] http://sourceforge.net/projects/opencvlibrary/ 
64480
出席國際學術會議心得報告 
                                        96年 10月 6日 
報告人 
姓名 簡忠漢 服務機構 
聖約翰科技
大學電機系 職稱 副教授 
中文：2007年 SICE年會暨精密儀器、控制與資訊科技國際會議會議正式
名稱 英文：SICE Annual Conference 2007, International Conference on 
Instrumentation, Control and Information Technology 
會議時間 自 96年 9月 17日 至 96年 9月 21日 
地點（國、
州、城市） 日本高松 
報告內容應包括下列各項： 
 
一、 參加會議經過 
 
二、 與會心得 
 
三、 考察參觀活動（無是項活動者省略） 
 
四、 建議事項 
 
五、 其他 
 
 
 
  
圖 1、由我們組團之 IC11場次論文發表情形 
 
圖 2、本團成員於 SICE 2007會場香川大學合影 
 
二、與會心得 
 
在此次的參與活動中，除了能夠同時聆聽相關研究課題的論文發表之外，同時，難得有
機會與日本控制界友人，交換第一手的的研究資料，並且一起討論相關研究方向與研究心得
等活動，此項交流乃對於我國學術的成長與發展有著良性的助力。同時，就由本團隊的參與
此次活動，能夠為臺灣於國際間學術活動的提高能見度，而且與國際間重要學者的意見交流
亦會強化我國在此項研究領域的學術地位，讓國際間瞭解我國的學術研究成果與進展。 
SICE Annual Conference 2007 
Sept. 17-20, 2007, Kagawa University, Japan 
1. INTRODUCTION 
 
Incidents of home invasion, fire, or lethal gas leakage 
have been reported everyday in the newspaper and may 
happen at anytime to anyone at home. Home security 
has been the main concern of almost all persons today in 
order to protect their safety and valuable property. In 
recent years, many security robots have been developed 
[1]-[17] to assist the repetitive and hazardous tasks of a 
patrol guard for home security. 
For an intelligent patrol robot to provide a wide area 
of applications, it must be equipped with high level of 
autonomy and adaptation to the changing environment. 
Typically, the robot may need to gather environmental 
information through a variety of sensors, build up an 
understanding of its surroundings based on an internal 
model for the environment and make decisions to take a 
subsequent action. All of these must be done 
automatically by the processing units of the robot with 
little a priori knowledge about the environment. 
Besides performing the boring patrol tasks, an 
intelligent patrol robot in home environment may be 
further required to carry out some human-robot 
interactions to create a comfortable experience for 
people and provide appropriate feedback to remote 
operators. 
In this paper we mainly focus on developing three 
functionalities, namely, home security monitoring, 
network assisted interactions, and self-navigation patrol, 
for a wheeled mobile robot toward the goals of the 
afore-mentioned intelligent patrol robot system.  
The remainder of the paper is organized as follows. 
The hardware and software architecture of the mobile 
robot to provide home security function is introduced in 
Section 2. Two kinds of network assisted interactions 
are presented in Section 3. An self-navigation patrol 
system using ultrasonic and vision data fusion is 
described in Section 4. Finally, conclusions are given in 
Section 5. 
 
2. HOME SECURITY MONITORING 
 
Home security is the main goal of our patrol robot 
system. Therefore, we first integrate a variety of sensors 
on the robot, including CCD camera, ultrasonic sensors, 
temperature sensor, human sensors, and gas detectors to 
gather environmental information and detect abnormal 
events including fire alarm, intruder alert and lethal gas 
leakage. 
We adopt an X80 robot by Dr. Robot Inc. [4], to 
provide a testbed for subsequent experiments. The 
mobile robot has two rear driving wheels and one front 
castor wheel. The hardware setup and sensor allocation 
of the mobile robot is shown in Fig. 1. The sensor 
readings except the visual data are collected by a 
PMS5005 controller and are periodically accessed by a 
monitor program running on an onboard computer. The 
vision data from the CCD camera are also captured into 
the onboard computer by use of the monitor program. 
In addition, the monitor program transmits the sensor 
data to the remote control center through wireless 
network. We design a networked graphical user 
interface on the remote control center, as shown in Fig. 
2, to display readings of the sensor measurements, 
monitor the environmental images, send commands to 
the robot, and allow the remote operator to manually 
control the movement of the robot. 
When abnormal events are detected, the monitor 
program will make a loud noise and send a message to 
the control center via the Internet. 
Development of a Patrol Robot for Home Security with Network Assisted Interactions 
Chia-Wei Chang, Kuan-Ting Chen, Hsiu-Li Lin, Chih-Kai Wang, and Jong-Hann Jean 
Department of Electrical Engineering, St. John’s University, Taiwan 
(Tel : +886-2-2013131; E-mail: jhjean@mail.sju.edu.tw) 
 
Abstract:  In this paper we present the development of a patrol robot system for home security with some 
considerations on its interaction functionalities. The system integrates a variety of sensors to gather environmental 
information and detect abnormal events including fire alarm, intruder alert and lethal gas leakage. To facilitate a 
security robot to live with people in a home environment, we implement some dedicated human-robot interactions, 
including a face mask with several facial expressions and a force feedback steering wheel controller. All of the sensor 
measurements and the dedicated interactions can be remotely accessed via the Internet. We further design an indoor 
patrol algorithm that can autonomously command the mobile robot to move randomly, avoid obstacles, or steer along a 
wall baseline based on ultrasonic and vision data fusion. Finally, we apply the patrol robot system to patrol the second 
floor of EE building of our campus and provide several experimental results for validating its performance. 
 
Keywords: intelligent robots, human-robot interaction, networked systems, visual navigation. 
 
operated in self-patrol mode or remote control mode. In 
remote control mode, the remote operator manually 
navigates the robot by sending commands via wireless 
network with the aid of the monitored environmental 
images. 
In self-patrol mode, the mobile robot moves 
randomly at first. When there is an object detected in 
the front direction by the ultrasonic sensor, the robot 
will further check all sensor readings to make clear if 
the object is an obstacle or a wall. When obstacles are 
encountered, the mobile robot performs the obstacle 
avoidance algorithm developed in [18] to bypass the 
obstacle and avoid collisions. 
When a wall is detected, the mobile robot takes the 
wall-following action, because wall-following is a 
simple but robust navigation strategy and has been 
shown to be appropriate in indoor environments [19]. 
The reason why we use the wall-following method is 
that it does not require the knowledge about the layout 
or shape of the indoor environment. As a result, there is 
no need to have a CAD model of the environment in 
advance and it can be easily applied to patrol various 
complex environments with simple preparation. 
Then we apply a previously developed visual 
navigation controller [18], as will be briefly described in 
the next subsections, to steer the robot moving along a 
wall baseline based on ultrasonic and vision data fusion.  
While performing the patrol task, the mobile robot 
detects abnormal events by use of the temperature 
sensor, gas detector, and human sensors. When 
abnormal events are detected, it will send a message to 
the control center to notify the remote operator.  
 
4.2 Detection of wall baselines by ultrasonic and 
vision data fusion 
 
As indicated in Fig. 7, we can detect a wall in a 
corridor by visually detecting the wall baseline. Its 
advantage over detecting by ultrasonic sensors is that 
the wall baseline provides not only the positional 
information but also the relative orientation between the 
mobile robot and the wall, which is helpful for 
subsequent robot navigation. 
For this purpose, we apply a line detection technique 
[20] based on the RHT method to the edge image of the 
corridor. However, it seems difficult to extract wall 
baselines purely from visual information, especially in a 
cluttered corridor, because many background lines may 
be detected from the edge image, and it is not easy to 
identify which are the true wall baselines.  
Start
Remote 
Control
Self 
Patrol
Obstacle 
Detected
Random 
Walk
Obstacle 
Avoidance
Wall Baseline 
Detected
Wall 
Following
No
Yes
Yes
No
Yes
No
Sensor 
Measurements
Fire Alarm
Intruder Alert
Gas leakage
Send Message to 
Control Center
Yes
Yes
Yes
No
No
No
Send 
Message to 
Operators
 
Fig. 6 Flowchart of the self-navigation patrol algorithm 
 
   
(a)        (b)      (c) 
   
(d)         (e) 
Fig. 7 Application of the wall-baseline detection system 
to a typical image of a corridor. (a) Original image. (b) 
Edge image. (c) Filter out edges above the floor. (d) 
Lines detected by applying the RHT method. (e) Retain 
possible lines according to the range information 
obtained by ultrasonic sensors. 
As a remedy, we first use the inverse perspective 
mapping (IPM) technique [21] to filter out edges above 
the floor and then incorporate the range information 
obtained by the ultrasonic sensors to retain possible 
baseline edges. 
 
4.3 Wall-following by visual navigation 
 
In the following, we apply a previously developed 
visual servo controller [21] to achieve wall-following of 
the mobile robot. 
Consider the mobile robot navigates along a wall 
baseline in a corridor, as depicted in Fig. 8. The wall 
baseline has an inclined angle φ  with respect to the 
z-axis of {R0}. Moreover, for compensating the time 
