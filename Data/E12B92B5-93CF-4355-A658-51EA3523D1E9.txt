Abstract 
This project establishes a joint prosody model of tone, speaker and language. The 
main purposes of the joint prosodic modeling are (1) to reliably extract prosody 
information which is less sensitive to the mismatch environment, such as different 
channel and background noise and (2) to apply the extracted prosodic information to 
tone recognition, speaker verification and language recognition 
Four research topics are covered by this study include (1) Latent Prosody 
Model-assisted Robust Pitch Tracking, (2) Latent Prosody Model-based Robust 
Speaker Verification, (3) Joint Prosodic and Spectral Modeling for Robust Speaker 
Verification and (4) Latent Prosody Model-based Taiwan and Mainland Mandarin 
Accent Recognition. 
 ii
 1 
第一章 研究計畫之背景及目的 
口語對話中，人發出的語音訊號，同時傳達了語者身分（ speaker 
identification）、使用語言種類（language identification）、語音內容（speech）、語
意（meaning）與語者狀態（speaker status 與 emotion）等多項訊息，這些訊息在
溝通中非常重要。在建立 user friendly 語音人機界面（如多模式口語對話系統）
時，一個好的人機系統必須能正確偵測語音夾帶的各種訊息，同時也要能整合所
有線索互相支援，做最佳判斷與適當回應，以免發生雞同鴨講的情形。 
為了使系統能辨別使用者的語言、身份並理解其講話內容，現今各國在發展
語音辨認技術（尤其是口語對話系統，或機器人人機界面），皆包含 speech 
recognition、speaker identification、language identification、speaker status/emotion 
recognition（using prosodic cues）、prosody modeling 與 language understanding 等
領域，其中韻律處理一直是語音訊號處理的一個重要課題。 
傳統口語對話系統偵測語音內容、語者身分、語者狀態與語言等因素時，多
是分別處理，不會一起考慮，且多依賴頻譜特徵參數（如 MFCCs 等）。若有使
用韻律訊息，多侷限於輔助角色，只是將韻律訊息模型的分數與頻譜特徵參數模
型的分數做融合（fusion），而沒有考慮其交互關係。但是實際上我們必須考慮韻
律訊息中所帶有許多頻譜訊息沒有的資訊（如語者講話態度等），及其在通道雜
訊環境通常比頻譜資訊強健可靠，因此近年來有朝使用韻律訊息發展的趨勢。 
實際上語音內容、語者身分、語者狀態、語言與韻律狀態等因素為交互影響
（如要偵測語音內容，須考慮不同語者身份、語者狀態、不同語言與不同韻律模
式等因素；如要偵測語者身份，須排除不同語音內容、語者狀態、不同語言與不
同韻律模式的影響）。為了要能準確偵測各種因素，必須同時考慮所有可能因素，
建立語音、語言、語者與韻律訊息之聯合模型，或是將語音內容、韻律模式、語
言、語者身份與狀態之影響因素移除或是正規化，並據此調適整個語音辨認模型。 
因此在本計畫中，將深入研究同時考慮語音、語言、語者與韻律訊息之聯合
 3 
本計畫致力於使口語對話系統能辨別使用者的語言、身份和理解其講話內
容，預計之研究項目包含：（1）latent prosody model 與（2）applications of latent 
prosody model： 
1 Latent prosody model (LPM) 
 LPM that separately considers speaker, language, and phonetics and 
prosody status 
 J-LPM that jointly considers speaker, language, and phonetics and 
prosody status 
 Adaptation of the LPM and J-LPM 
2 Applications of latent prosody model 
2.1 LPM-assisted pitch contour extraction and tone recognition 
 LPM-assisted pitch contour extraction for Mandarin and tone 
recognition 
 LPM-assisted pitch contour extraction for Western languages 
 J-LPM-assisted pitch contour extraction and tone recognition 
2.2 LPM-assisted speaker recognition/clustering 
 LPM-assisted speaker recognition/clustering (all speakers speak the 
same language) 
 J-LPM-assisted speaker recognition/clustering (one speaker may 
speak two or more languages) 
2.3 LPM-assisted language recognition/clustering 
 LPM-assisted language recognition/clustering (one speaker speaks 
only one language) 
 J-LPM-assisted language recognition/clustering (one speaker  
may speak two or more languages) 
 
 5 
第二章  基於潛藏式韻律模型之強健性基頻求取 
2.1  緒論 
2.1.1  研究動機 
自然口語語音信號中，抽取基頻軌跡（pitch tracking）的相關技術非常多，
如傳統 RAPT（a robust algorithm for pitch tracking）【1】、YAAPT（yet another 
algorithm for pitch tracking）【2】和最近較新的瞬時頻率方法【3】等。這些方法
都有一個共通點就是都針對以音框（frame）為單位處理，因而容易發生下列幾
點問題： 
1. 基頻倍頻（pitch double）或半頻（pitch half） 
2. 受雜訊干擾 
3. 有聲或無聲的判斷 
4. 有聲音節起始位置與結束位置的判斷 
這些問題造成基頻軌跡不連續，如圖 2.1 所示，藍色點為基頻候選者（pitch 
candidate），而紅色線條是傳統 RAPT 所挑選出來的基頻軌跡，很明顯在後半面，
發生基頻倍頻錯誤，這是傳統方法在抽取基頻軌跡時，容易挑選出週期性較高的
基頻候選者，所造成的錯誤，但是這種錯誤其實可以避免。 
 
 
圖2.1：RAPT 發生倍頻的錯誤。 
 7 
聲調是中文語音特有的特性，每一個音節都具有一種聲調，所有可能的發音
裡面，總共包括五種不同聲調，一般我們分為一聲、二聲、三聲、四聲與五聲。
這裡指的聲調，是指我們發音的時候，隨者時間變化頻率會有不同高低起伏而產
生不同聲調。從基頻軌跡來觀察，我們可以發現一般單字音所發出的聲調，各自
具有其獨特的基頻軌跡分佈，基頻軌跡之標準形式如圖 2.4 所表示。但在圖 2.4
中沒有標示第五聲（一般稱為輕聲）的基頻軌跡，是因為通常第五聲的基頻軌跡
不像其它聲調具有規則性基頻軌跡，當其出現的同時，常常是一不規則軌跡，且
能量與音節長度也較其它聲調低和短。
 
 
基 頻 (H z )
時 間
第 一 聲
第 二 聲
第 三 聲 第 四 聲
 
圖2.4：中文四個聲調的基頻軌跡。 
 
從頻率分佈來看不同的聲調，可以發現一聲整體平均值較其它聲調來得高，
接下來由高到低依序為二聲、四聲、三聲，這是同一個語者（speaker）所發出頻
率的情況。一般而言，女生所發出的頻率比男生高，所以女生發出第三聲的頻率，
比男生發出第一聲的頻率還要高，故圖 2.4 只是相對性比較的結果。 
儘管我們能夠知道一般單字音當中，聲調的基頻軌跡分佈多如我們所預期的
樣子，而現實中，連續語音則會受到前後音之聲調、語音之韻律模式，甚至語意
 9 
2.1.3  章節概要 
本章總共分四個子章節，各子章節的編排與概要為：2.2 傳統基頻偵測方法，
介紹傳統 RAPT 與 YAPPT 基頻求取方法。2.3 使用 LPM 模型輔助基頻偵測，說
明韻律模型與聲調模型的訓練過程，並且如何利用模型輔助求取基頻軌跡。2.4
實驗結果與分析，分析實驗結果。2.5 結論與展望。 
2.2  傳統基頻軌跡求取方法 
求取基頻軌跡首要發展目標，是得到更多準確的基頻軌跡估測，傳統基頻軌
跡求取的演算法，大致可分成四個步驟： 
1. 前處理（pre-processing） 
2. 利用 NCCF 找出所有基頻候選者 
3. 挑選基頻候選者 
4. 動態程序（dynamic programming） 
最重要的步驟為第四步動態程序，這一步驟必須決定有聲與無聲音框，以及
挑選“正確“的基頻。 
所以我們在下列舉出目前常見的兩種基頻軌跡求取的方法，2.2.1 節介紹
RAPT 演算法，2.2.2 節介紹 YAAPT 演算法，2.2.3 節為傳統方法的實驗結果與分
析。 
 11 
幅。 
（4） 每個音框保留第三步驟所有基頻候選者的峰值。 
（5） 以動態程序對每個音框挑選有聲 NCCF 基頻候選者峰值或是無聲
的情況。 
 
表 2.1 是 RAPT 所有的符號定義。 
 
表2.1：RAPT 符號定義。 
Symbol Meaning 
Xm mth sample of the input speech signal 
Fs Sample rate of speech signal = 1/T 
Fds Reduced sample rate of speech for first-pass NCCF 
Round
（v） 
The integer that is closest to v 
N The number of samples correlated at each lag = round
（wFs） 
z
.
 
The frame step size in samples = round（tFs） 
I The analysis frame index incrementing at a rate of 1/Tz 
K The longest lag at each frame = round（Fs/F0min） 
Φi,k Normalized cross-correlation for frame I at lag k 
 
 13 
2.2.1.2  前處理 
RAPT 演算法中不需要前處理。但如果需要前處理則： 
（1） 對輸入語音信號能有好的取樣頻率（6kHz ≤ Fs ≤ 44kHz）。 
（2） 語音信號中背景雜訊如果是重要週期性組合，應試圖移除雜訊的週
期性。 
2.2.1.3  兩階段 NCCF 
NCCF 在語音處理中是用以計算週期性的函式。音框中如果具有週期性強的
地方有可能就是基頻的位置，故我們利用 NCCF 挑選出所有的基頻候選者。為
了減少計算量，我們將基頻候選者搜尋限制於一範圍內，一般用於基頻估測搜尋
的範圍是在 50≤ F0≤ 500。 
在第一階段，輸入信號為重新取樣較低的取樣頻率 Fds，定義為： 
( )( )max/ 4 0
s
ds
s
FF
round F F
=               （2.2.1） 
NCCF 計算低取樣頻率信號的所有延遲 k（Fds/F0max ≤ k≤ K）。所有的延遲如
果最大值 Φ超過（CAND_TR  X Φmax）就記錄起來。為了使原始取樣頻率會更
準確估測相關峰值區域和振幅，得到之後，會在定義為 Fds 的每個峰值三個取樣
Φ使用拋物線內插（parabolic interpolation）。假如這些峰值的數量超過（N_CANDS 
- 1），將由振幅減少的順序來排序，峰值最大的前（N_CANDS - 1）數量將會被
記錄起來。 
第二階段時，從原始的取樣語音信號計算 Φ，但是只對之前有找到延遲的鄰
近峰值再估測一次，這樣新的峰值 Φmax將被找到。如果峰值 Φ 為零，則這些延
遲不會被計算。所以峰值 Φ在超過（CAND_TR  X Φmax）時才做記錄。兩個通
過（CAND_TR  X Φmax）是在測試峰值重要的程度，因為加上雜訊可能為判斷
成真的聲音信號，而不是提供一些正規化於可能降低峰值數量的 CAND_TR。假
如超過（N_CANDS - 1）數量，他們挑選前（N_CANDS - 1）最大的峰值。 
 15 
Ii 是 音 框 i 的 基 頻候 選 者 數 量 ， 音框 i 的 基 頻 候選者 數 量 為
（1≤Ii≤N_CANDS）。然而，對於每個音框，提供了 Ii-1（有聲狀態）和一個無聲
狀態的基頻候選者。並且 Ci,j 為音框 i 中 jth 基頻候選者的峰值。Li,j為音框 i 中 jth
的取樣延遲。我們定義函式，是在當音框 i 是在有聲的狀態下： 
( ), , ,1 1 ,1i j i j i j id C L j Iβ= − − ≤ ≤        （2.2.6） 
( )min_ / / 0sLAG WT F Fβ =            （2.2.7） 
當音框 i 為一無聲的狀態下： 
( ), ,_ maxii I i jjd VO BIAS C= +               （2.2.8） 
LAG_WT 為相關最大的基頻候選者允許可以調整的程度，這相當於鼓勵選
擇較短週期的基頻候選者。（2.2.6）函式會挑選出音框為有聲狀態的 Ci,j 是近似
1.0 或者是相關值最大的基頻候選者，這時無聲狀態時 Ci,j 近似於零。VO_BIAS
是調適為有聲與無聲的判斷。 
當假設 j 和 k 在當前和前一個音框都是有聲時，音框和音框之間基頻轉移值
δ的定義為： 
( )( ){ }, , , ,_ min _ ln 2.0i j k i j j kFREQ WT DOUBLE Cδ ξ ξ= × + −     （2.2.9） 
,
, 1
1,
ln ,1 ;1i jj k i i
i k
L j I k I
L
ξ
−
−
= ≤ ≤ ≤ ≤                （2.2.10） 
DOUBL_C 是正數的常數。這些函式的轉移值是為了表示音框和音框之間適
合頻率的變化，但是隨著高八度高的跳動的值會特別大（在此的分數越小越好）。
FREQ_WT 是正數常數，是用來調整音框和音框之間基頻改變的分數。 
假設前後音框都是無聲的狀態： 
1, ,
0
i ii I I
δ
−
=                            （2.2.11） 
在音框和音框之間的轉移分數是看前一個音框到目前的音框，而前音框不需
要計算。 
 17 
和初始狀態 
0, 0 00,1 ; 2jD j I I= ≤ ≤ =         （2.2.20） 
對於每個音框的狀態儲存每個點 
, mini jq k=           （2.2.21） 
Kmin 指每個音框的候選基頻，k 是最小 Di,j，就這樣找到最佳狀態序列。每
個音框的延遲轉換成基頻的式為： 
,
,
0i j
i j
FsF
L
=         （2.2.22） 
 19 
2.2.2.2  前處理 
第一次的前處理先產生兩種版本的信號，第一個為原始信號第二為取絕對值
的信號。取絕對值的信號是因為基頻軌跡的計算從絕對值雜訊語音信號是相當類
似於無雜訊的語音。即使對於一些乾淨語音好的信號範例，基頻經由絕對值信號
處理是重要的。 
2.2.2.3  基頻候選者的估測 
兩種不同版本的信號在上一子節提到。假設語音信號中音框的基頻是不會變
的，則每個音框應該考慮語音的特性和長度。 
基本上每個音框的處理是用自相關函式（2.2.23）式計算出峰值。所以語音
信號中基頻週期強的地方存在延遲是最大振幅的相關性峰值。假如最大峰值的振
幅超過門限值（大約 0.6），則這個音框可以考慮成有聲音框。 
NCCF 定義為： 
( )
( ) ( )
0
0
N K
n
k
s n s n k
NCCF k
e e
+
=
+
=
∑
, ( )2 ,0 1n k N Kk
n k
e s n k K
= + +
=
= ≤ ≤ −∑    （2.2.23） 
 s（n）：語音信號取樣的音框 0≤k≤K-1 
基頻不管多強健性的 NCCF，仍然有可能 NCCF 最大的峰值不是“正確“的延
遲，所以最大峰值的振幅並不完全可靠的指出這個語音區域是有聲的或者是無
聲。基頻偵測基本的方法，最大峰值的振幅決定這個音框是有聲的或者是無聲
的。假如這個音框區分為有聲的，基頻週期的延遲相關則考慮最高的峰值。不幸
的，基本方法的問題結果傾向於這些峰值是錯誤的。 
我們假設這語音信號的音框是有聲的，但是基頻週期的相關延遲並沒有峰
值，所以可能是在其他不同的相關延遲才有更大的振幅。基於上面的假設，反而
只確認一個音框只有單一個最大峰值的演算法相對於每個音框搜尋多個峰值於
兩種不同信號的演算法將更準確。搜尋的規則如下： 
 
 
 21 
按照頻率來排序）。 
（5） 從第三步估測所有的有聲音框，和所有的有聲音框重新估測一次基
頻平均，和更新基頻平均值。 
（6） 從第五步再一次用 5 點中央濾波（5-point median filter）“中心平滑
化“。 
（7） 結合兩種頻譜的基頻軌跡產生新的平滑化軌跡。 
頻譜資訊的基頻軌跡是使用在從 NCCF 挑選出正確的基頻候選者和峰值。
對於有聲的音框，基頻候選者從 NCCF 測試“更接近“相關性的頻譜基頻點。 
2.2.2.5  合理且連續性的限制條件下去修正基頻候選者 
這一步是處理最後的結果，之前提到基頻候選者的矩陣，挑選出矩陣的組成
是由 NCCF 峰值和振幅、NLFER 曲線（從原始的信號）和頻譜資訊的基頻軌跡。
這些資料將使用在得到本地與轉移分數，利用動態程序挑選出最有可能的路徑。 
一開始，音框先用 NFLER 先區分有聲（區域 2，NFLER > 0.5） 和無聲（區
域 1，NFLER <= 0.5）。在無聲區域（區域 1）所有的基頻候選者設為零，無聲的
候選者設為 0.99。而有聲區域（區域 2），則有可能是有聲或者是無聲，所以在
這區域的每個音框最少有一個估測的基頻候選者，和一個無聲的候選。最後每個
候選者都有一個優勢值，這個值的範圍在 0 到 1 的範圍內，優勢值越高，就越有
可能正確的候選者。 
本地分數 
 1 .local merit= −                （2.2.25） 
merit：為優勢值。 
轉移分數 
轉移分數的算法是依據之前提的演算法，i 在這個函式為前一個音框。 
1. 前後兩個基頻候選者都是有聲的情況下： 
( ) ( ) ( ) ( ) ( )( )( )
2
2
0 0 1 0 0 1
cos 0.05*
0 _ min 0 _ min
F i F i F i F i
transition t i
F F
− − − −
∝ +   （2.2.26） 
 23 
 
3. UVE 
是參考基頻（EGG）為無聲的音框，而求取出來的基頻為有聲音框的錯誤率。 
4. VUE 
是參考基頻（EGG）為有聲的音框，而求取出來的基頻為無聲音框的錯誤率。 
5. GEC（Gross Error Count）【7】 
是比較當參考基頻（EGG）和求取出來的基頻都是有聲音框的情況下，如果
基週期差大於 1ms 的情況下，計算次數： 
( ) ( )1/ 1/ 1k k kc f f ms= − >⌢          （2.2.30） 
kf ：句子中第 k 個音框估測的基頻值 
ˆ
kf ：句子中第 k 個音框參考的基頻值 
6. FPEAV（fine pitch error-average value） 
FPEAV 是計算當 GEC 的音框是小於 1ms 時的基週期平均。 
1
1 fineN
k
kfine
c c
N
=
= ∑          （2.2.31） 
7. FPESD（fine pitch error-standard deviation） 
FPEAV 是計算當 GEC 的音框是小於 1ms 時的基週期標準差。 
22
1
1 fineN
e k
kfine
c c
N
σ
=
= −∑      （2.2.32） 
2.2.3.2  RAPT 錯誤率 
這裡我們的基頻正確答案是使用 RAPT 的方法求取 EGG 音檔基頻。測試的
部份，我們拿乾淨正常的音檔語料，結果如表 2.3 所示： 
 25 
0 50 100 150 200 250 300 350 400 450
0
50
100
150
200
250
300
08183
 
圖2.8：RAPT 發生倍頻的錯誤。 
 
在圖 2.9，在最後的 120 音框到 125 音框的所在出現了有聲音節起始位置與
結束位置的判斷錯誤。 
0 20 40 60 80 100 120 140
0
20
40
60
80
100
120
140
160
180
06935
 
圖2.9：RAPT 有聲音節起始位置與結束位置的判斷錯誤。 
 27 
2.3.1  使用 LPM 模型輔助基頻偵測 
一般來說語音實際上可以觀察到人講話的音高、聲音大小，以及講話快慢
等。音高就是所謂的基頻，聲音大小等同於能量大小，講話速度快慢則決定講一
個字的長度。 
但在中文語音中，每一個音節都具有一種聲調。在（2.3.1）式中，我們描述
整句基頻（Pi）、能量（有聲的能量 En，無聲的能量 PEn）和長度（有聲的長度
Du，無聲的長度 PDu）皆被聲調所影響。但是人類講話時都有自己的韻律，而
韻律會同時干擾基頻、能量和長度，所以我們在式子後面改寫成基頻、能量和長
度受到聲調與韻律的影響。 
( ) ( ) ( ), , , , | , , , , | , |
P
p Pi En Du PEn PDu T p Pi En Du PEn PDu P T p P T= ∑ （2.3.1） 
我們假設韻律狀態和聲調狀態是不相關，且彼此獨立。而有聲的部份主要受
到基頻軌跡、能量和長度影響；無聲的範圍，只受到能量以及長度影響，所以我
們把（2.3.1）式改寫成下式： 
( ) ( ) ( ) ( )
( ) ( )( )
,1 , , 1
1 2
| |
, , | , ,
, , , , | ,
, | ,
kNK
k k n k n
k n
p P T p P p P p P P
p Pi En Du P T voiced
p Pi En Du PEn PDu P T
p PEn PDu P unvoiced
−
= =
≈ ≈

= 

∏ ∏
 （2.3.2） 
由 2.2 節可知，傳統求取基頻軌跡方法都是以音框為單位，而我們建立的
LPM 模型則是以音節為單位。如圖 2.11 所示，一個模型可能對應到 L 長度的音
框，為了解決這個問題，我們使用音節階層（segment-level）搜尋法。 
 
 
 
 
 29 
2.3.1.1  音節階層搜尋法 
在我們實驗中，我們採用第 2.2 節 RAPT 的基頻偵測方法，並且疊加上我們
建立的 LPM 模型，最後搜尋部份則使用音節階層搜尋法。所以我們對 RAPT 的
演算法做了一些修改，主要是讓 RAPT 挑選出所有的基頻候選者，如圖 2.12 所
示。並使用 LPM 模型對每個基頻候選者做加分或扣分的動作，詳細說明如下： 
0 20 40 60 80 100 120 140
-50
0
50
100
150
200
250
300
350
cand
 
圖2.12：音框裡所有的基頻候選者 
（1） 假設目前音節為有聲的情況： 
( ) ( )( ) ( ), , , ,1 1 1 * _ ,xi j i j i j i j td C L p sco x Aα β α= − − − + ∀ ∈        （2.3.3） 
( )1 , 1 , , ,ij I i t l t t xττ τ τ ρ≤ ≤ + ≤ ≤ = − ∈                     （2.3.4） 
,
,
0i j
i j
FsF
L
=
               （2.3.5） 
( ), ,_ log 0 | pii j i j ip sco p F y = −           （2.3.6） 
α 是 LPM 的權重，
,i jC β、 和 ,i jL 如同原本 RAPT 的定義，F0 是把原本基頻
候選者的延遲值轉換成 hz，yipi 為 LPM 中第 i 的位置的基頻值，yien 為 LPM 中第
i 的位置的能量值。主要是一個音框有許多基頻候選者的值，與把所有基頻候選
 31 
1, ,
0
i ii I I
δ
−
=            （2.3.10） 
3.前個音框是從有聲轉移到無聲音框： 
( ) ( )
, , 1_ _ _ _ _ ,1ii I k i i iVTRAN C VTR S C S VTR A C rr k Iδ −= + + ≤ ≤ （2.3.11） 
4.前個音框是從無聲音轉移到有聲音框： 
( ) ( )
1, ,
_ _ _ _ _ / ,1
ii j I i i iVTRAN C VTR S C S VTR A C rr j Iδ − = + + ≤ ≤       
（2.3.12） 
音節內的所有音框如同 RAPT，做一次音框對音框的動態程序。  
{ }
1
, , 1, , ,min ,1 , 1
i
i j i j i k i j k ik I
D d D j I i tδ τ
−
−
∈
= + + ≤ ≤ + ≤ ≤     （2.3.13） 
( )_ log | , tdu sco p l x x Aτ = ∀ ∈                    （2.3.14） 
1
1
_
t
en
i
i
En mean y
l ττ = +
= ∑          （2.3.15） 
( )_ log _ |en sco p En mean x =             （2.3.16） 
{ } ( ),min * _ _xl l jpath D du sco en scoτ τ α= + +         （2.3.17） 
tA 為在時間 t 的模型。動態程序最後找到最佳的基頻軌跡和分數，而在分數
方面加上 LPM 的音節長度分數。 
最後全部動態程序處理的演算法如下,t =1,..,T： 
Initial： 0t lτ− =  
( ) ( )log( ),xi l tx path p x x AτΛ = + ∀ ∈             （2.3.18） 
Iterate： 0t lτ− >  
( ) ( ) ( ), ,min log( ( | )), ,xi l ty A i xx y path p x y x A l tττ τ ττ ρ τ∈ ∈Λ = Λ + + ∀ ∈ = −  （2.3.19） 
 
 
Track back： 
 33 
 
此處 Φ為正交向量的基底，Pi 為基頻值，當一段長度的基頻軌跡做 Legendre
多項式轉換後，我們得到四維參數。原本以音框為單位的基頻，轉變成用四維參
數描述音節基頻軌跡的形狀，如圖 2.14 所示。 
 
 
 
 
 
 
 
圖2.14：基頻軌跡轉四維參數的結構圖。 
 
2.3.2.2  LPM 模型公式 
上一子節，我們把音節的基頻軌跡，轉換成四維參數，加上原本的能量和長
度，總共變成六維參數。所以有聲的音節可以由基頻、能量和長度表示；無聲的
音節，只用能量和長度表示，可用（2.3.26）式表示： 
 
, , , 1, 2,3,4
[ , ]
V
j
U
x A En Du j
x En Du
  = =  

=
                   （2.3.26） 
Aj：音節的基頻軌跡轉換成為四維參數 
En：音節的能量 
Du：為音節的長度。 
我們假設影響有聲音節的主要因素有三種，一種為聲調，一種為韻律狀態，
一個為語者；影響無聲音節的因素有韻律狀態和語者，其影響因素具有加成性，
 35 
圖 2.15 為 LPM 模型的訓練流程，由於實驗所使用的語料庫為單一語者所錄
製，所以語者均值（speaker mean）可以經過一次計算求得，接著分別計算聲調
與韻律狀態的初始值，每次遞迴時都會再更新聲調與韻律狀態，並計算聲調與韻
律狀態的轉移機率，直到 likelihood function 收斂為止，以下將詳盡介紹訓練過
程的每一個步驟。 
 
 
圖2.15：LPM 模型訓練流程圖。 
 
 
 
 
Start 
Normalized 
Estimate Tone factor 
VQ-based prosodic 
state labeling 
Re-Estimate Tone 
factor 
Re-Estimate Prosodic 
factor 
Likelihood 
Converge? 
Estimate Prosodic 
state factor 
Re-label prosodic 
state 
Final prosodic model 
Calculate 
residueCo-variance 
Initial prosody model 
 37 
NUV ：number of  unvoiced segment. 
 
長度： 
 
 
               （2.3.31） 
 
 
 
               （2.3.32） 
 
 
 
Du ： log-duration value.  
K： number of sentence in corpus 
NtV ：number of  voiced segment. 
NtUV ：number of  unvoiced segment. 
步驟（2），解決語者因素，利用所有音節的六維數數平均，可以輕易得到
speaker mean （µp），計算式如下： 
 
（2.3.33） 
 
NkV = number of voiced syllables in an sentence 
 NkUV = number of voiced syllables in an sentence 
, ker
, ker
, ker
, ker
,
var
,
var
V V
du spea
V
du spea
UV UV
du spea
UV
du spea
Du p
voiced
Du
Du p
unvoiced
µ
µ
 −


= 
−


, ker ,
1 1
2
, ker , , ker
1 1
, ker ,
1 1
2
, ker , , ker
1 1
1
1
var ( )
1
1
var ( )
V
k
V
k
UV
k
UV
k
NK
V V
du spea k nV
k nt
NK
V V V
du spea k n du speaV
k nt
NK
UV UV
du spea k nUV
k nt
NK
UV UV UV
du spea k n du speaUV
k nt
p Du
N
Du p
N
p Du
N
Du p
N
µ
µ
µ
µ
= =
= =
= =
= =
=
= −
=
= −
∑∑
∑∑
∑∑
∑∑
,
1 1
,
1 1
1
1
V
k
UV
k
NK
V V
k nV
k nt
NK
UV UV
k nUV
k nt
p x
N
p x
N
µ
µ
= =
= =

=



=

∑∑
∑∑
 39 
為了使模型更佳準確，我們使用 ML 訓練方式。接著我們將介紹，整個訓練過程、
模型更新順序與方式。 
整個訓練過程最重要的就是模型更新順序，其更新順序依次是語者模型、聲
調模型、韻律模型，因所使用的語料庫為單一語者的語料庫，所以關於語者模型
只需要第一次的初始化後，就不需要再更新，而有聲音節部份更新順序影響最嚴
重的是聲調模型與韻律模型，所以必須特別注意這兩者更新順序不可以顛倒。 
每一次更新完聲調模型與韻律模型後，利用 ML criterion，我們可以計算出
每一次的 likelihood function L（詳細的定義在步驟（7）），重複更新並觀察 L，
當 L達到收斂之後，便可以停止更新模型的動作。 
步驟（5），得到更新後的韻律模型 PPP ，再次更新聲調模型 TPT ，其計算式
重寫如下所示： 
( ) ( )
( )
,
, ,
1 1
,
,
1 1
, 1,...,5
k
k n
k
NK
V V V
k n P k n
V k n
T k nNK
k n
k n
x PP p T T
PT T
T T
µ δ
δ
= =
= =
− − × =
= ∈
=
∑∑
∑∑
 （2.3.36） 
其中，
,k nP
PP = 在句子 k 中第 n個音節的韻律狀態模型 
步驟（6），由於我們希望我們利用聲調模型與韻律模型重建有聲音節
（
, ,k n k n
V V V
T P+ +µp PT PP ） 與 無 聲 音 節
,k n
U U
P+µp PP 能 與 觀 察 到 的 音 節
（
, ,
V U
k n k norx x ）非常相似。所以在重新標記韻律狀態時，為了使得標記正
確韻律狀態機會提升，所以我們利用兩者之間的誤差
, ,
V U
k n k nory y ，而這個
誤差可視為一個常態分佈； ( , ) ( , )V UN or N0 Rp 0 Rp ，其均值為 0，共變異
矩陣為為 V UorRp Rp ，計算式如下所示： 
 
 41 
（2.3.39） 
 
在步驟（9）的時候，我們不斷檢查 L 值與模型更新次數之間的變化關係，
並記錄下來直到收斂為止。 
2.4  實驗數據與結果分析 
2.4.1  語料庫 
本章使用的語料庫是同步錄製正常語音，及使用儀器去量測喉結的振動頻率
（EGG）語音，表 2.1.4 與表 2.5 是在錄音時的環境設定。  
 
表2.4：語料庫設定。 
麥克風 單一指向性 （uni-directional） 
錄音場所 普通房間 
錄音情境 依照所選出文稿唸出 
取樣頻率（sampling 
rate） 
16k 
發音速度 每秒約 2.5 個音節 
取樣大小 16 bits （位元） 
聲道 單聲道（mono） 
檔案格式 Wav 
 43 
0 10 20 30 40 50
-1
0
1
Tone1
0 10 20 30 40 50
-1
0
1
Tone2
0 10 20 30 40 50
-1
0
1
Tone3
0 10 20 30 40 50
-1
0
1
Tone4
0 10 20 30 40 50
-1
0
1
Tone5
 
 
Converged
Initial
從圖 2.16 我們可以推斷，重複更新模型達到約 60 次左右時，likelihood 
function 的值就不再出現大幅變化，即達到收斂，便可以停止模型的更新動作，
接著我們觀察訓練後的聲調模型與韻律模型是否如同預期一樣。在聲調狀態下，
扣掉韻律狀態的影響，圖 2.17 為初始與收斂之後的聲調狀態基頻軌跡圖，縱軸
均為頻率值（log frequency），橫軸為時間軸（frames）。圖 2.18 為聲調狀態的能
量分佈圖，縱軸為 dB 值。圖 2.19 是聲調狀態的長度分佈圖，橫軸為長度取對數
值。
 
 
 
 
 
 
 
 
 
 
圖2.17：聲調模型的基頻軌跡圖。 
 
 
 45 
0 10 20 30 40
-1
0
1
prosody1
0 10 20 30 40
-1
0
1
prosody2
0 10 20 30 40
-1
0
1
prosody3
0 10 20 30 40
-1
0
1
prosody4
0 10 20 30 40
-1
0
1
prosody5
0 10 20 30 40
-1
0
1
prosody6
0 10 20 30 40
-1
0
1
prosody7
0 10 20 30 40
-1
0
1
prosody8
 
 
Converged
Initial
在韻律狀態下，扣除掉聲調狀態的影響，圖 2.20 為初始與收斂之後的韻律
狀態形狀，縱軸為頻率值（log frequency），橫軸為時間軸（frames）。 
 
 
 
 
 
 
圖2.20：韻律模型的基頻軌跡圖。 
 
圖 2.21 為韻律狀態的能量分佈圖，縱軸為 dB 值。圖 2.22 是韻律狀態的長
度分佈圖，橫軸為長度取對數值。 
 47 
1 2 3 4 5 6 7 8
-1
0
1
2
Prosody pitch mean
1 2 3 4 5 6 7 8
-1
0
1
2
Initial Prosody pitch mean
對韻律模型而言，每個韻律狀態與狀態之間最大的差異處，是基頻平均值的
差異。因此我們統計訓練後的韻律模型，在整個語料庫的分佈狀況，與每個韻律
模型的平均基頻值分佈如圖 2.23 所示。 
 
 
 
 
 
 
 
 
圖2.23：各韻律狀態的基頻平均值分佈。 
 
 
 
 
 
 
 
 
 49 
 
2.4.2.2  使用 LPM 輔助 RAPT 求取基頻錯誤率 
我們使用 LPM 去輔助求取基頻，表 2.6 與 2.1.7 為錯誤率： 
表2.6：內部測試錯誤率。 
Error RAPT+LPM RAPT 
WGPE-I（%） 3.493 3.498 
WGPE-II（%） 1.344 1.348 
VUE（%） 1.981 1.921 
UVE（%） 3.287 3.816 
GEC（%） 2.521 2.511 
FPEAV（ms） 0.128 0.130 
FPESD（ms） 0.738 0.758 
 
表2.7：外部測試錯誤率。 
Error RAPT+ LPM RAPT 
WGPE-I（%） 3.444 3.453 
WGPE-II（%） 1.331 1.345 
VUE（%） 1.900 1.859 
UVE（%） 3.178 3.783 
GEC（%） 2.412 2.403 
FPEAV（ms） 0.124 0.122 
FPESD（ms） 0.764 0.757 
 
 
 51 
0 50 100 150 200 250 300 350
0
20
40
60
80
100
120
140
160
180
200
RAPT EGG
RAPT
RAPT + LPM
 
圖2.26：使用 LPM 模型修正 RAPT 的 Uv_v 錯誤。 
 
圖 2.17 中，在大約 140 音框左右的音節，在我們使用 LPM 模型之後，這一
個音節判斷成無聲段落，這是很嚴重的錯誤。因此我們把 LPM 模型的權重調小，
這個錯誤就可以改善回來。 
0 50 100 150 200 250 300 350 400
0
50
100
150
200
250
RAPT EGG
RAPT
RAPT + LPM
 
圖2.27：使用 LPM 模型之後發生 V_uv 錯誤。 
 53 
參考文獻 
[2.1].
 
D. Talkin, “A robust algorithm for pitch tracking（RAPT）,” in Speech coding 
and synthesis（Elsevier, ed.）,pp. 495-518,1995. 
[2.2].
 
Kavita Kasi .“Yet another algorithm for pitch tracking
（YAAPT）”,ICASSP’,2002. 
[2.3].
 
Dhany Arifianto, Takao Kobayashi.“Voiced/unvoiced determination of speech 
signal in noisy environment using harmonicity measure based on 
instantaneous frequency“,IEEE2005. 
[2.4].
 
Tseng, Chiu-yu, Pin, Shao-huang, Lee, Yeh-lin, Wang, Hsin-min and Chen, 
Yong-cheng （2005）. “Fluent speech prosody: framework and modeling,” to 
appear in Speech Communication. 
[2.5].
 
Der-Jenq Liu,Chin-Teng Lin（2001） .“Fundamental Frquency Estimation 
Based on the Joint Time-Frequency Analysis of Harmonic Spectral Structure, 
“IEEE2001. 
[2.6].
 
B. G. Secrest and G. R. Doddington, “Postprocessing techniques for voice 
pitch trackers,“in Proc. IEEE ICSASSP’82, 1982, pp. 172-175. 
[2.7].
 
L. R. Rabiner, M. J. Cheng, A. E. Rosenberg, and C. A. McGonegal, “A 
comparative performance study of several pitch detection algorithms,“IEEE 
Trans. Acoust., Speech Signal Processing, vol. ASSP-24, pp.399-417, Oct. 
1976. 
 
 
 
 55 
 
圖3.2：以權重合之方式整合系統。 
 
3.1.2  研究背景 
雜訊與通道容易導致傳統以頻譜特徵為主之語者驗證系統效能降低。而通道
特性通常與語者特性緊緊相連要將他們分離是很困難的，通常需要在測試前得到
通道特性之先驗知識(a priori knowledge)，或是使用較不受話筒干擾的語者訊
息，如利用韻律層次的語者訊息。
 
短程音高模型，是利用每一個音框的對數(log)音高及對數能量，和估計對數
音高及能量的一階微分，來建立 GMMs 模型。 
長程韻律模型則先以正規化(stylization)將音高和能量動態軌跡描述成符號
序列，在以 N-gram 或 DHMM 方式，統計其長程變化與建立模型，如圖 3.3 所示。 
 57 
如此一來，不同性質之特徵參數無論是利用長程韻律參數為輔助，將短程頻
譜參數與短程音高參數分類，或是利用短程頻譜參數與短程音高參數為輔助重新
求取韻律狀態序列，都充分利用了相互連結之關係。
 
 
 
圖3.4：基於韻律與頻譜聯合模型之強健性語者驗證。 
 
3.1.4  章節概要 
本章之子章節編排介紹如下，3.1 節為緒論，3.2 節說明本章語者確認之
基本系統及美國國家標準局(National Institute of Standards and Technology, NIST)
提供之 NIST2001 以及中文語言資訊聯盟(Chinese Corpus Consortium, CCC)提供
之語料庫，得知基本系統之效能。3.3 節為不同語者辨識系統之間分數整合。分
別利用權重合與 MIT 林肯實驗室發展之 LNKNET 工具得知一般與理想情形之系
 59 
3.2  背景知識及語者驗證基礎系統建立 
現今的語者驗證系統中，不同性質之語音特徵參數各自擁有其獨立之驗證系
統如圖 3.5 所示。這種方式雖然簡化了在求取參數與模型訓練時的複雜度，卻將
難題轉移至系統效能整合上。 
 
 
圖3.5：傳統獨立之語者驗證系統流程圖。 
 
在 3.2.1 子節將介紹本章用之基本語者驗證系統架構，包括短程頻譜參數之
語者驗證系統、短程音高參數之語者驗證系統以及長程韻律參數之語者驗證系統
其中包含著各系統參數之求法、模型訓練之方法。3.2.2 子節介紹測試正規化之
方法對我們的系統做分數上之正規化。3.2.3 子節使用 NIST2001 語料庫進行語者
 61 
 
圖3.6：頻譜參數語者驗證系統方塊圖。 
 
4.
 
語者背景模型(background speaker model):語者背景模型的目的在於語
者驗證系統中幫助辨識分數做正規化的動作，一般是將所有註冊語者訓
練語料中所求得之 MFCC 參數全部一起訓練出語者不特定模型作為
MFCC 語者背景模型。 
5.
 
語者模型訓練:將語者背景模型(Universal Background Model, UBM)利
用最大概似函數調適(Maximum A Posteriori, MAP)調適出每位語者的語
者模型，以彌補因語料不足而缺少的某些聲學特性。
  
 63 
 
圖3.8：使用最大概似函數調適的方式，將UBM模型調適出語者特定母音模型。 
 
其中對於特徵參數前處理以及語者模型之建立詳細說明如下：
 
MVA 的步驟分成三個部份，分別使用了平均消去法(Mean Subtraction, MS)、
變異數正規化法(Variance Normalization, VN)及 ARMA 濾波器，即可達到語音強
健性的效果。圖 3.9 為 MVA 流程圖。 
 
 
圖3.9：對語音特徵參數作 MVA 之流程圖。 
 
假設
,t dX 為一連串語音訊號抽取之特徵參數向量序列，其中 t 為時間，d 為
特徵參數之維度，首先經過平均消去法，處理過後的特徵參數向量序列可以(3.2.2)
及(3.2.3)式表示： 
,
1
1 T
d t d
t
X
T
µ
=
= ∑                          (3.2.2) 
'
, ,t d t d dX X µ= −                         (3.2.3) 
MS  VN ARMA 
 Filter 
Feature 
Extraction 
Robust 
Feature 
 65 
較大的高斯混合模型，其覆蓋整個語者獨立空間及包含語者聲音的廣闊聲學特性
類別，可用來表示所有語者特徵參數的分佈，在本章中我們將所有人的訓練語
料，不分男女直接收集在一起，使用向量量化與 EM 訓練出 UBM。 
最大概似函數調適方法的基本概念是想經由調適來更新背景模型的參數，以
產生出個別語者之語者模型，圖 3.10 是調適的示意圖，其中(a)是 UBM 的高斯
混合模型及調適語料；(b)是使用調適語料的統計量和 UBM 混合模型衍生出調適
後的語者高斯混合模型。
 
 
 
圖3.10：使用最大概似函數調適語者模型的示意圖。 
 
最大概似函數調適含兩個步驟，類似 EM 演算法。第一步與 EM 演算法的取
期望值步驟完全相同，主要是計算出語者訓練語料在 UBM 中對每一個混合高斯
(mixture Gaussian)的充分統計量(sufficient statistics)。第二步是調適步驟，我們使
用一個與語料相依的混合係數(data-dependent mixing coefficient)將新估算的充分
統計量與 UBM 混合參數的舊充分統計量作結合。這個混合係數取決於語者語料
對混合高斯的計數，當混合高斯有較高的計數時，最後估算的參數比較依賴新的
 67 
對於每個混合高斯和每個參數，使用一個與語料相依的適應係數 iρα ，
{ }, ,w m vρ ∈ 於上式，其定義如式(3.2.14)： 
i
i
i
n
n r
ρ
ρα = +
                        (3.2.14) 
其中 ργ 對於參數 ρ 式一個合適且固定的因數。在(3.2.8)-(3.2.10)中所描述更
新參數之式子，可以從一般 MAP 估計式子衍生出來。由於我們採用的語者分數
算方式如式(3.2.15)： 
1 1, ,
, , , ,
, ,
1 1 1,
,
1 1 1 1 1( ) log( ) ( ( ) ( ) ) ( ( ) ( ) )
2 2
w
w ww
w
N N Nj m i m T T
j m j m j m j mi i i ij m j m
i i ii mj m
w
S X o o o o
N w N N
µ µ µ µ− −
= = =
= + − − ⋅ ⋅ − − − − ⋅ ⋅ −
∑
∑ ∑ ∑ ∑ ∑
∑
       
(3.2.15) 
其中第一項會造成分數之偏移，所以在後面的實驗之中，將只對 mean 的部
份作調適。
 
3.2.1.2  短程音高參數(PITCH/ENERGY)之語者驗證系統 
本章所使用之短程音高參數語者驗證系統如圖 3.11 所示，在本節我們將整
個語者辨認基本系統，分成幾個區塊來描述，包括： 
1.
 
特徵參數萃取 (feature extraction)：音高與對數能量之求取使用
Wavesurfer/Snack 軟體，再對求取出之音高與對數能量分別求取其斜
率，以這 4 維參數作為系統參數。 
2.
 
語者驗證(speaker verification)：與頻譜參數(MFCC)語者驗證系統相同，
僅將輸入之參數由頻譜參數改為短程音高與對數能量。
 
 69 
 
圖3.12：Tokenization 架構圖。 
 
1. 我們對音高與對數能量做兩層的 stylization(格式化 )，而經過
stylization 所切割出的區段為最小的韻律單位。其中 stylization 採用
piece-wise curve fitting 的方式處理。piece-wise curve fitting 是一種對輸
入訊號找尋轉折點之方法，首先對音高執行 piece-wise curve fitting，執
行步驟如下： 
A. 將離散輸入訊號之前 3 點(視窗大小為 3)求取一段斜率，以此斜
率估算出訊號第 4 點之相對位置。 
B. 將訊號第 4 點到第 6 點也求取出其斜率，以此斜率推算出訊號第
4 點之相對位置。 
C. 計算步驟 A 與 B 所求出 2 點之距離。若距離大於門檻值(圖 2.11
中的 d)，表示訊號第 4 點即為轉折點，重複執行步驟 A，此時起
 71 
5.
 
左手邊前後兩區段間的平均能量差異(energy left jump)。 
6.
 
右手邊前後兩區段間的平均音高差異(pitch right jump)。 
7.
 
右手邊前後兩區段間的平均能量差異(energy right jump)。 
8.
 
左手邊前後兩區段間的間隔延遲(left pause duration)。 
9.
 
右手邊前後兩區段間的間隔延遲(right pause duration)。 
非母音部份包括：
 
1.
 
此區段的長度(duration)。 
此外，為了移除語句發音內容(context-information)對韻律變化的影響，必須
利用整個訓練語料所統計出來之韻律特徵參數的分佈對這些特徵參數做正
規化的動作，以移除任何非韻律特性的影響。
 
 
 
圖3.14：以某一區間為例說明 9 維韻律參數之求取。 
 73 
接著用訓練語料的韻律參數分佈對測試語料的韻律特徵參數做正規化，再利
用訓練語料所得出之韻律狀態類別對測試語料的韻律特徵參數做分類，將測試語
料的韻律特徵參數也標上相對應的狀態，即可得到測試語料之參數。
 
統計出所有訓練語料的轉移統計矩陣。即可得到訓練語料之 unigram 與
bigram 背景模型。依各別註冊語者統計之 unigram 與 bigram 之矩陣，即為語者
韻律模型。
 
3.2.2  測試正規化 
對語者辨識系統而言，每句測試語料的特徵參數 { }1,..., TX x x=   ，都可以與經
由訓練語料所訓練出來的語者高斯混合模型 sλ 、UBM 語者背景模型 bλ 計算出相
對應的 log likelihood score，以數學形式表示即： 
( ) log( ( | )) log( ( | ))s bS X P X P Xλ λ= −               (3.2.16) 
1
1
( | ) ( ( , ) )
T
T
s i s
i
P X S x d xλ λ
=
= ⋅∏                    (3.2.17) 
, 1
, ,,
,
1( , ) max{ exp( ( ) ( ) )}
2(2 )
s
s ss
s
j T
i ij ji s jj D
j
w
S x x xλ λ λλ
λ
λ µ µ
pi
−
= − − ⋅∑ ⋅ −
∑
    
(3.2.18) 
測試正規化的原理則是利用一組同儕語者模型(cohort model set)，同儕語者
模型的就是一群相似於目標語者模型(target speaker model)的模型，目的是為了估
計出相對應於每位不同的目標語者相似於目標語者的冒充語者(impostor)，測試
正規化之定義如式(3.2.19)： 
-
s I
T
I
S
S λ
µ
σ
=
                         (3.2.19) 
 
 75 
 
圖3.17：變異數正規化之影響(a)分數分佈之變化(b)DET curve 之變化。 
 
測試正規化中減去 Iµ 不僅可以將冒充語者分數的分佈之中心移至原點附
近，同時也可以拉大目標語者與冒充語者分數之分佈，而除以 Iσ 可以將冒充語
者分數的分佈之標準差限定為 1，因此提昇辨識率。 
3.2.2.2  語者驗證系統效能評估 
如圖 3.18、圖 3.19 可知語者驗證的錯誤率有兩種：一種是錯誤的拒絕(False 
Rejection, FR)，即正確語者的分數小於門檻值造成拒絕的錯誤率。另一種是錯誤
的接受(False Acceptance, FA)即仿冒語者的分數高於門檻值造成接受的錯誤率。
對語者辨識系統而言，每句測試語料的特徵參數 { }1,..., TX x x=   ，都可以與經由訓
練語料所訓練出來的 GMM 語者模型 sλ 、UBM 語者背景模型 bλ 計算出相對應的
log likelihood score，以數學形式表示即： 
( ) log( ( | )) log( ( | ))s bS X P X P Xλ λ= −               (3.2.20) 
 77 
 
圖3.19：相等錯誤率示意圖。 
 
相等錯誤率(Equal Error Rate)是一種評估語者驗證系統的方式。所謂的相等
錯誤率就是錯誤的拒絕機率與錯誤的接受機率相等的機率值。但在某些特殊的情
形中錯誤拒絕與錯誤接受的後果或重要性並不相等，舉例來說，語者驗證應用在
金融取捨的情況，為了避免冒領盜用，因此錯誤接受的機率必須減至最低。
 
偵測錯誤取捨曲線圖(Detection Error Tradeoff Curve, DET Curve)，此種評估
方式是假設目標語者和仿冒語者的對數相似度比分數為兩個不同的高斯分佈，當
改變門檻值時，DET 曲線圖可以將所有相對應的錯誤的拒絕機率與錯誤的接受
機率展現出來。
 
決策代價函數(Decision Cost Function, DCF)，數學式如下： 
| arg arg |Im Im    FR FR T et T et FA FA postor postorDCF C P P C P P= × × + × ×     (3.2.23) 
其中 FRC 和 FAC 分別代表錯誤的拒絕與錯誤的接受的代價， argT etP 和 Im postorP
分別代表目標語者與冒充語者出現的機率， | argFR T etP 和 |ImFA postorP 分別代表錯誤拒
 79 
量時，triangle filterbank 頻寬為 300-3400Hz，而 ISCSLP2006-SRE 是透過麥克風
錄音，所以在求取 filterbank 能量時，triangle filterbank 頻寬為 100-3800Hz。除
此之外也對特徵參數作 CMS/MVA 處理，進一步的消除通道不匹配的影響。 
實驗中之語者模型採用最大概似函數調適，分別先將語料庫中之註冊語者訊
練出一背景模型，此背景模型為一高斯混合模型，混合數為 1024，最大概似函
數調適時採用之尺度因子為 16，混合數保持為 1024。執行最大概似函數調適時
只對 UBM 之平均值做調適，其餘模型參數皆繼承 UBM 的參數。 
此外，NIST2001-SRE 語料庫於測試時，每句測試語料除了當目標語者測試
一次之外，亦當成不同的冒充語者測試 10 次。因此總測試數量為 22418 次(2038
次目標語者測試結果，20380 次冒充語者測試結果)，測試結果如表 3.1 所示。
ISCSLP2006-SRE 語料庫每句測試語料會從訓練語料中挑選一位做測試。總測試
數量為 11788 次(600 次目標語者測試結果，11188 次冒充語者測試結果)。 
 
表3.1：NIST2001-EVAL 語料庫，MAP-GMM 基礎實驗之相等錯誤率。 
系統種類
 
特徵參數
 EER (%) 
MAP-GMM + CMN MFCCs 11.36% 
MAP-GMM + 前處理 MFCCs 8.64% 
 
表 3.2、3.3 分別為向量量化與 EM 後之 codewords 值，表 3.4 為 NIST2001
訓練語料的轉移統計矩陣。
 
 
 81 
 
圖3.20：NIST2001-EVAL 語料庫，MAP-GMM+CMN 與 MAP-GMM+前處理之
DET Curve。 
 83 
NIST2001-SRE 系統之相等錯誤率為 28.75%，ISCSLP2006-SRE 系統之相等錯誤
率為 16.50%。 
1. 語者的音高及對數能量經由 piece-wise curve fitting 轉換成韻律特徵參
數，並且對韻律特徵參數做正規化處理，NIST2001-SRE 中對音高做
piece-wise curve fitting 時的門檻值定為 1 個半音階(semitone)，即只要估
算之音高值相差大於 1 個半音階，就判定此點為轉折點，對對數能量做
piece-wise curve fitting 時的門檻值定為 10 個 dB，即只要估算之對數能
量大小落差大於 10dB 時就判定此點為轉折點。ISCSLP2006-SRE 中為
了減少 tone 對音高做 piece-wise curve fitting 時的門檻值定為 10 個半音
階，即只要估算之 pitch 值相差大於 10 個半音階，就判定此點為轉折點，
對對數能量做 piece-wise curve fitting 時的門檻值定為 8 個 dB，即只要
估算之對數能量大小落差大於 8dB 時就判定此點為轉折點。 
2. 接下來，我們將做過正規化處理的韻律特徵參數利用向量量化分群，將
分群後所得到之 M(M=8)個 codewords 以 EM 演算法分成 8 個
codewords，則每個 codeword 可視為一特定韻律狀態，據此建立一韻律
模型。 
3. 利用建立好的韻律模型，即可以自動將輸入之韻律特徵參數標記成韻律
狀態索引序列。 
4. 將自動標記好的韻律狀態序列，統計出韻律序列組合的 unigram 與
bigram 模型，作為語者模型。 
5. 輸入測試語料得到測試語料之韻律狀態索引序列，以此序列計算出宣稱
語者之分數。 
檢查每個 codeword 質心的值，統計每個 codeword 在句子中出現的位置，與
轉移矩陣交叉驗證後，大致可將所得到的 codewords(之後將 codewords 統稱為韻
律狀態(prosodic state))分為幾類。舉例來說， 狀態 3 顯示現在區段與上一個區
 85 
表3.6：NIST 2001 語料庫之註冊語者語音資料經 EM 後之 11-state 韻律模型。 
codeword 
pitch 
slope 
energy 
slope 
length 
pitch 
left 
jump 
energy 
left 
jump 
pitch 
right 
jump 
energy 
right 
jump 
left 
pause 
duration 
right 
pause 
duration 
pause 
length 
1 -0.072 -0.114 0.049 -0.084 -0.493 0.22 0.35 -0.226 -0.286   
2 -0.012 -0.81 -0.387 -0.173 0.015 -0.251 -0.128 -0.414 -0.146   
3 -0.072 1.066 -0.274 0.247 -0.071 0.058 -0.5 2.948 -0.195   
4 0.076 -0.007 -0.497 0.012 -0.891 0.12 -0.136 -0.22 -0.204   
5 0.029 -0.439 0.112 0.026 0.003 -0.37 -0.14 -0.173 2.856   
6 0.028 -0.096 2.725 -0.009 0.111 0.075 0.596 -0.04 0.003   
7 0.008 1.053 -0.544 0.213 0.237 0.082 -0.5 0.002 -0.414   
8 0.014 -0.117 0.058 -0.036 0.534 0.019 0.246 -0.301 -0.253   
9                   6.072 
10                   28.996 
11                   13.659 
 
表3.7：NIST 2001 語料庫註冊語音資料 11 種狀態之韻律模型態轉移矩陣統計。 
State 1 2 3 4 5 6 7 8 9 10 11 
1 4722 4143 0 3378 1688 788 1010 2780 2160 23 1539 
2 1827 2929 1 1804 1151 772 3964 3795 5443 73 2768 
3 1638 1135 1 1220 598 805 838 2327 834 272 642 
4 4209 1065 0 4562 1176 718 930 830 2248 529 1052 
5 0 0 0 1 0 0 0 0 598 8880 1848 
6 739 2087 0 684 921 464 240 2054 1143 667 1289 
7 2693 3054 0 626 1045 2069 1702 8568 489 44 381 
8 1128 9017 0 989 2874 1844 2969 17430 4204 361 3971 
9 2972 719 143 2605 920 1135 5128 3515 0 0 0 
10 77 13 8807 296 418 580 105 608 0 0 0 
11 2226 365 1335 1158 703 1113 3785 2880 0 0 0 
 
 
 
 
 
 87 
 
圖3.22：NIST2001-SRE 各基礎語者驗證效能之 DET Curve。 
 
 
 
 
 
 89 
表3.11：ISCSLP2006-SRE 各基礎語者驗證技術之相等錯誤率。 
系統種類
 EER(%) 
Prosody bigram 24.70% 
Pitch/energy GMM 16.50% 
MFCC GMM 5.04% 
 
 91 
 
圖3.24：傳統獨立之語者驗證系統整合流程圖。 
在 3.3.1 節中將利用傳統權重合的方式，將第二章中所介紹三個系統之分數
做整合 3.3.2 節中將利用 MIT 林肯實驗室發展之 LNKNET 工具以其中
MLP(MULTI LAYER PERCEPTRON)之方式，將第二章中所介紹三個系統整合。
3.3.3 節中將對這兩種整合系統之方式作出簡單結論。 
3.3.1  傳統權重合 
從 3.2 節我們得到頻譜參數、短程音高參數與長程韻律參數三個語者驗證系
統之實驗結果，NIST2001-SRE 中，三個系統個別得到之相等錯誤率為 8.64%、
28.75%及 36.06%，ISCSLP2006-SRE 中，三個系統個別得到之相等錯誤率為
5.04%、16.50%及 24.70。由於頻譜參數之效能遠比其他兩系統之效能為好，以
此為基底，為了簡化討論，將頻譜參數、音高參數與頻譜參數、韻律參數兩兩相
配在以權重合的方式加以結合。
 
 
 
 93 
 
圖3.25：MFCC+pitch/energy 與 MFCC+prosody 系統之相等錯誤率曲線圖。 
 
3.3.2  MLP fusion 
同樣的利用 3.2 節中頻譜參數、短程音高參數與長程韻律參數三個語者驗證
系統之實驗結果，我們採用 LNKNET 工具加以整合，其中對 LNKNET 中的設定
為：
 
1. 以 MLP(MULTI LAYER PERCEPTRON)類神經網路之方式整合系統分
數。 
2. hidden layer 為 1， node 為 30。 
整合之結果如表 3.12，從表中我們可以看出，不同系統經由 LNKNET 整合
之系統效果。圖 3.26 為系統 DET Curve 之比較圖。 
 95 
 
圖3.26：NIST2001-SRE 基礎系統以 MLP 整合之 DET Curve。 
 
 
 
 
 
 
 
 97 
3.4  聯合模型 
語者的頻譜特徵代表的是較短程且低階層的聲學訊息，是和語者先天發音器
官生理特性相關的線索，其中梅爾頻率倒頻譜係數被廣泛使用；語者的韻律特徵
則通常為聲門資訊的特徵參數，受語者後天的社會，身份背景與個人習慣影響較
大，常被使用的包括音高，音量軌跡與停頓長短等參數。 
然而就現今的研究而言，大多數人在利用韻律訊息來強化傳統以頻譜特徵為
基礎之語者驗證系統的效能時，通常將韻律訊息與頻譜訊息視為獨立資訊，各自
建立各自的語者驗證系統，將兩套獨立系統的分數輸出，再用分數領域的整合方
式，將不同系統之效能整合。這種作法容易遇到以下問題：
 
 將參數之間的影響視為獨立無關，與事實不符。 
 利用權重合的方式必須以較有鑑別力之系統為基礎，其餘系統為輔助，
依經驗法則給予不同系統不同之權重值。
 
針對上述兩個問題，我們提出了基於韻律與頻譜聯合模型，希望能夠提升語
者驗證系統之效能。首先以 Tokenization 所產生之韻律狀態序列為基底，將頻譜
參數與音高參數依據韻律狀態分類如圖 3.27 所示，藉以產生出韻律狀態相關的
語者高斯混合模型如圖 3.28 所示。 
接著以韻律狀態中狀態與狀態間的轉移機率，描述上層語音韻律狀態的變
化，並使用與韻律狀態相關的高斯混合模型，來描述下層頻譜與音高訊息在某韻
律狀態下的統計特性，以此聯合模型(如圖 3.29)來描述韻律與頻譜訊息隨時間變
化之情形，將短程與長程參數系統整合於聯合模型內。
 
在 3.4.1 子節中將介紹我們提出的韻律狀態相關語者模型，3.4.2 子節將介紹
基於韻律與頻譜聯合模型之強健性語者驗證之架構，最後將使用 NIST2001-SRE
與 ISCSLP2006-SRE 語料庫驗證系統之效能。 
 99 
 
圖3.29：基於韻律與頻譜聯合模型之強健性語者驗證。 
 
3.4.1  韻律狀態相關之語者高斯混合模型 
在利用韻律訊息來強化傳統以頻譜特徵為基礎之語者驗證系統的效能時，通
常將韻律訊息與頻譜訊息視為獨立資訊，但是一般人在說話時，通常在句首總是
較大聲且音高較高，而在句尾總是較小聲且音高較低，說話時所處的韻律狀態，
實際上會直接影響其所發出之語音的頻譜統計特性，反之亦然。所以我們將考慮
語音的頻譜特性與所處的韻律狀態間之關聯。
 
首先對韻律狀態而言，以 3.2 節中的 Tokenization 方式(圖 3.9)，對整個語料
庫做 stylization，將所得韻律單位求取韻律參數，再以向量量化之方式，將訓練
語料中相同韻律特性的韻律單位歸類為同一狀態，以此韻律狀態對整個語料庫之
 101 
 
 
圖3.30：語音韻律之多層次結構圖。 
 
 
圖3.31：韻律片語群之音高趨勢圖。 
因此若我們可以偵測韻律狀態變化，則可以建立一 Ergodic 隱藏式馬可夫模
型，以隱藏式馬可夫模型的狀態與狀態間轉移之變化來描述語者的韻律狀態與狀
態轉移的特性，如目前是處在韻律片語的起頭、中間或尾巴，而且使用與此韻律
狀態相關的高斯混合模型，來描述在此特定韻律狀態下的頻譜統計特性。希望藉
由模型層次的直接整合，更有效同時利用頻譜與韻律訊息，強化語者驗證系統抵
抗電話話筒或通道環境不匹配的效能。
 
以圖 3.29 之架構圖為例說明之。其系統分數計算方式定義如 (3.4.1) 式： 
 103 
 
圖3.32：Viterbi 演算法搜尋最佳路徑。 
 
3.4.3  實驗結果與分析 
3.4.3.1  韻律雙聯文模型 
本章實驗條件架構如下：
 
1. 語者的音高及對數能量經由 piece-wise curve fitting 轉換成韻律特徵參
數，並且對韻律特徵參數做正規化處理，NIST2001-SRE 中對音高做
piece-wise curve fitting 時的門檻值定為 1 個半音階，即只要估算之音高
值相差大於 1 個半音階，就判定此點為轉折點，對對數能量做 piece-wise 
curve fitting 時的門檻值定為 10 個 dB，即只要估算之對數能量大小落差
大於 10dB 時就判定此點為轉折點。ISCSLP2006-SRE 中為了減少 tone
對音高做 piece-wise curve fitting 時的門檻值定為 10 個半音階，即只要
估算之音高值相差大於 10 個半音階，就判定此點為轉折點，對對數能
 105 
 
3.4.3.2  韻律相關語者高斯混合模型 
在此系統中特徵參數使用 38 維 MFCCs(12 維 MFCCs、12 維 ∆-MFCCs、12
維 ∆2-MFCCs、1 維 ∆-log energy 和 1 維 ∆2- log energy)，而實驗中之語者模型採
用最大概似函數調適，分別先將語料庫中之註冊語者訊練出一背景模型，此背景
模型為一高斯混合模型，混合數為 1024，最大概似函數調適時採用之尺度因子
為 16，混合數保持為 1024。執行最大概似函數調適時只對 UBM 之平均值做調
適，其餘模型參數皆繼承 UBM 的參數。接著利用圖 4.2 之架構，將語者模型再
使用第二次最大概似函數調適，將一個語者模型依據韻律狀態調適為 8 個有聲狀
態相關之語者模型與 3 個無聲狀態之無聲模型，而由於將 MFCCs 細分為 11 個
韻律狀態後，每個韻律狀態的訓練語料資料量也將依韻律狀態個數多寡而減少，
為了避免統計量不足而造成模型訓練時造成的偏差，將此時的最大概似函數調適
採用之尺度因子調整為 32，混合數保持為 1024，與第一次最大概似函數調適相
同的也只調適高斯混合模型之平均值部份，其餘參數皆繼承語者高斯混合模型的
參數。
 
此外根據 3.2 節中 MFCC 前處理之效能比較表(表 3.1)中可得到母音部份較
具有辨識能力，所以再將韻律相關語者高斯混合模型所區分之 11 個狀態中的 3
個無聲狀態捨去，只計算有聲狀態的分數。
 
接著將 pitch/energy-GMM 也依據所得之韻律狀態序列，以尺度因子為 32 的
方式調適。
 
 
 
 
 107 
表3.18：NIST2001-SRE 之基於韻律與頻譜聯合模型之強健性語者驗證相等錯誤率。 
系統種類
 EER(%) 
Hard decision 8.29% 
Soft decision 8.34% 
Viterbi 8.25% 
 
 
圖3.33：NIST2001-SRE 之基於韻律與頻譜聯合模型之 DET Curve。 
 
 109 
 
 
圖3.34：ISCSLP2006-SRE 之基於韻律與頻譜聯合模型之 DET Curve。 
 111 
3.5  結論與未來展望 
3.5.1  結論 
本章所提出之韻律相關語者模型，可將短程特徵參數加上長程特徵參數的意
義，對參數之完整性加以改善，使得 NIST2001-SRE 中之相等錯誤率從 8.64%下
降至 8.29%，ISCSLP2006-SRE 中之相等錯誤率從 5.04%下降至 5.0%，再經由基
於韻律與頻譜聯合模型之強健性語者驗證架構，將不同的系統於模型上整合。使
得 NIST2001-SRE 中之相等錯誤率從 8.29%下降至 8.25%，ISCSLP2006-SRE 中
之相等錯誤率從 5.0%下降至 4.94%。 
3.5.2  未來展望 
希望由本章所提出之基於韻律與頻譜聯合模型之強健性語者驗證，可以使得
語者驗證系統得到較完整之語者模型，未來可考慮更多有效條件和參數，以此提
高系統辨識能力。
 
 113 
第四章  基於潛藏式韻律模型之強健性語者驗證 
4.1  緒論 
4.1.1  研究動機與背景 
語者驗證研究將語者特性分為短程頻譜與長程韻律特性，由圖 4.1 可以得知
越下層的語者訊息受到先天發音器官的影響越大，越上層的語者訊息受到後天學
習的影響越大，且越難取得。
 
為了避免通道影響，我們可以利用長程韻律參數，幫助我們提昇語者驗證系
統的效能。但在中文的語料庫中，長程韻律參數會受到聲調和人的影響。因此我
們希望能夠利用一些方法，將韻律參數中的其他影響因素去除，這時我們在用乾
淨的長程韻律參數來輔助語者驗證系統。一般是將短程語音特徵參數與長程語音
特徵參數各自經由不同之方式求取，以不同之系統訓練模型，每個系統各自執行
驗證程序，所得到之系統效能亦不相同，最後依據經驗法則以權重合的方式，將
各個系統所得到的辨識分數加總，並期望能以截長補短的方式改善系統效能，如
圖 4.2 所示。 
 
圖4.1：不同層次之語者訊息。 
 115 
 
 
 
圖4.4：韻律片語群之音高趨勢圖。 
 
在短程音高模型中，利用每個音框的音高（pitch）及對數能量（log-energy），
和估計音高及對數能量的一階微分，來建立 Pitch-GMM【2】。 
傳統作法長程韻律模型的建構，是先以自動語音辨認器做輔助取出切割位
置，再以相對應切割位置的音高和能量軌跡求取韻律參數，並將韻律參數軌跡以
韻律狀態序列加以描述。以 N-gram 或 DHMM【2】方式來統計出長程韻律變化
及建立模型。但這種作法在具有聲調特性的語言中，例如中文，可能會抓到聲調
的變化而不是語者的韻律變化。
 
 
Prosodic phrase, PPh 
 
 
 Prosodic word, PW 
 
 Syllable 
 117 
Segment
level
Supra-segment
level 
Prosody
&Tone
StateQ
Prosody
&Tone
State1
Prosody
&Tone
State2 Prosody
&Tone
State3
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
 
圖4.6：基於 LPM 架構下之語者驗證架構圖。 
4.1.2  研究方法 
本子章節針對短程參數與長程參數狀態之資訊，並且藉由 LPM 架構下韻律
與聲調參數模型來輔助短程頻譜參數系統提高語者驗證的準確度。
 
我們想要引進韻律狀態，用圖 4.6 的架構描述整句的韻律變化，由於整句的
韻律變化較難去描述，所以我們選擇較小的單位來描述，在中文語言中我們使用
以音節為單位來進行描述。我們利用 ASR 來幫助我們切割成音節單位。在每一
個音節單位中，因為每一個音節單位的長度不一樣，所以我們利用一組 Discrete 
orthogonal polynomial 正交基底多項式來逼近音高軌跡、能量軌跡、區間長度最
後再加上前後的變化來組成長程韻律參數。
 
由於長程韻律參數會受到語者因素和聲調因素的影響，在 ISCSLP2006 這套
語料庫因為沒有聲調的資訊，所以我們利用 TRSC 這套語音資料庫來建立聲調辨
識系統的訓練部分，藉由求取長程韻律參數來辨識 ISCSLP2006 的聲調資訊，之
後我們在 LPM 架構下，我們進一步引進韻律狀態，用圖 4.6 的架構描述整句的
韻律變化。每個音節單位都有其韻律狀態來描述，在加上前後的狀態跳動的關
係。並且藉由 TRSC 訓練出來的聲調和韻律模型來輔助我們求取 ISCSLP2006 的
最佳狀態序列和語者個人的 LPM 模型，之後我們拿取狀態序列的資訊分別訓練
成 LPM-LM UBM 模型和 LPM-LM SPK 模型，並且計算語者的辨識率，最後將
 119 
4.2  背景知識及語者驗證基礎系統建立 
現今的語者驗證系統中，不同性質之語音特徵參數各自擁有其獨立之驗證系
統如圖 4.7 所示為現今較常用的幾個語者驗證參數系統。 
 
 
圖4.7：傳統獨立之語者驗證系統流程圖。 
 
在 4.2.1 節中介紹前人使用之基本語者驗證系統架構，包括短程頻譜參數之
語者驗證系統、短程音高參數之語者驗證系統以及長程韻律參數之語者驗證系統
其中包含著各系統參數之求法、模型訓練之方法。4.2.2 節將介紹語者驗證系統
效能評估之方式。2.3 節中使用 ISCSLP2006 語料庫進行語者驗證基本系統實驗，
 121 
測試語料之特徵參數向量， 1{ , , }TX x x=
 
… ，其數學式子如（4.2.1）： 
 
( )  log  ( |  ) -  log  ( |  )s bS X X P Xλ λ= Ρ            （4.2.1） 
 
 
圖4.8：頻譜參數語者驗證系統方塊圖。 
 
 
圖4.9：語音邊界處理圖。 
 123 
 
圖4.10：訓練所有語者之母音背景模型程序。 
 
 
圖4.11：使用最大概似函數調適的方式，將 UBM 模型調適出語者特定母音模
求參數 
(MFCC) 
消雜訊 
(MVA) 
訓練有聲模型 
(UBM) 
擷取Voiced 
Segment 求參數 
(PITCH ＆ ENERGY) 
求參數 
(MFCC) 
消雜訊 
(MVA) 
擷取Voiced 
Segment 求參數 
(PITCH ＆ ENERGY) 
求參數 
(MFCC) 
消雜訊 
(MVA) 
擷取Voiced 
Segment 求參數 
(PITCH ＆ ENERGY) 
語音 
語音 
語音 
 125 
2 2
,
1
1 T
d t d d
t
X
T
σ µ
=
= −∑                       （4.2.4） 
'
,''
,
t d
t d
d
X
X
σ
=                           （4.2.5） 
最後經過 ARMA 濾波器，用來緩和（smooth）特徵參數序列，以符合語音
訊號在短時間內變化較小的現象，以（4.2.6）式表示之，其中 m 為濾波器階數
（Order）： 
''' ''' '' '' ''
( ), ( 1), , ( 1), ( ),'''
, 2 1
t m d t d t d t d t m d
t d
X X X X X
X
m
− − + ++ + + + +
=
+
⋯ ⋯
         （4.2.6） 
而語音中一個很重要的特徵，也就是音高，直覺的說音高代表聲音頻率的高
低而此頻率指的是基本頻率（Fundamental Frequency），也就是母音的部份，對
於語者特性而言，聲門（glottis）開關所造成的週期性氣流，因每個人的聲門而
不同。且聲門振動週期較不受口腔變化的影響，是很重要的一項語者資訊。關於
語音中的子音部份，是讓氣流從窄縫中穿過，易造成亂流，也較易受到口腔的變
化所影響。所以我們將特徵參數裡母音部份擷取出來分析語者特性的方式，將
MFCC 中每一個碼框與其所對應到之音高存在與否做判斷，留下母音當作較具語
者鑑別力之部份。
 
語者驗證系統在實際應用情況中，不可能要求所有使用者在註冊時，錄製太
多冗長的語音，因此每個人的訓練語料可能有一些聲學特性未被涵蓋到，可能導
致在測試時造成系統效能下降，於是前人提出最大概似函數調適方法。
 
最大概似函數調適方法先利用所有人的語料訓練出涵蓋所有語者聲學特性
的 UBM 模型，在利用每個人的少量註冊語聲調適 UBM，以產生個別語者之語
者模型，使得語者模型所涵蓋的聲學特性更具完整性，這個 UBM 實際上是一個
較大的高斯混合模型，其覆蓋整個語者獨立空間及包含語者聲音的廣闊聲學特性
 127 
算的參數比較依賴舊的充分統計量。
 
  1
( )(  |   )  
( )
i i t
r i M
j j tj
w p xP i x
w p x
=
=
∑
               
（4.2.7） 
然後我們為了得到權重、平均值和變異數等參數，使用 ( | )r tP i x 和 tx 計算充分統
計量，如（4.2.8）、（4.2.9）、（4.2.10）式： 
) x| i (P  n t
T
1 t 
ri ∑
=
=                       （4.2.8） 
1
1( ) ( | )
T
i r t t
ti
E x P i x x
n
=
= ∑                    （4.2.9） 
2 2
1
1( ) ( | )
T
r t t
ti
E x P i x x
n
=
= ∑                    （4.2.10） 
這和 EM 演算法中的取期望值的步驟是完全相同的。 
最後，這些來自訓練語料的新充分統計量被用來更新原本 UBM 混合高斯 i
的充分統計量，以得到被調適的參數，如
 
（4.2.11）、（4.2.12）、（4.2.13） 式所
示：
 
/ (1 )w wi i i i iw n T wα α γ = + − 

                 
（4.2.11） 
( ) (1 )m mi i i i iE xµ α α µ= + −                   （4.2.12） 
2 2 2 2 2
ˆ( ) (1 )( )v vi i i i i i iE xσ α α σ µ µ= + − + −              （4.2.13） 
其中適應係數{ }, ,w m vi i iα α α 各自用來控制新舊權重、期望值及變異數之間的平
衡。γ 是一個尺度因子，使全部被調適的權重相加起來合為 1。 
對於每個混合高斯和每個參數，使用一個與語料相依的適應係數 iρα ，
 129 
 
圖4.14：短程音高參數語者驗證系統方塊圖。 
4.
 
語者背景模型（background speaker model）： 與頻譜參數（MFCC）語
者驗證系統相同，為了將系統之分數做正規化的動作將所有註冊語者訓
練語料中所求得之音高與對數能量參數全部一起訓練出之語者不特定
模型作為音高與對數能量語者背景模型。
 
5.
 
語者模型訓練：由於音高所包含之語者資訊較不受語音內容的影響，所
以並非採用最大概似函數調適的方式，而是採用將單獨語者之音高與對
數能量特徵參數以單獨之高斯混合模型訓練之。
 
4.2.2.3  長程韻律參數（PROSODY）Bi-gram 之語者驗證系統 
本章所使用之長程韻律參數之語者驗證系統如圖 4.15 所示，在本子節我們
將根據圖 4.15 來描述系統中各元件之建立，圖 4.15 為以 VQ 建立韻律狀態序列
架構圖。以下將對架構圖中之每一個部份做說明。
 
 
 
 131 
在有聲區間依照先前求取出 9 維的韻律特徵參數在加上前後的關係擴展為
27 維韻律特徵參數，無聲的區間依照先前求取出 5 維的韻律特徵參數在加上前
後的關係擴展為 13 維韻律參數，其中有聲區間部份包括： 
1.
 
此區段的四維音高
 
（pitch mean, pitch dim2, pitch dim3, pitch dim4）。 
2.
 
左手邊前後兩區段的四維音高差異（pitch mean left jump, pitch dim2 left 
jump, pitch dim3 left jump, pitch dim4 left jump）。 
3.
 
右手邊前後兩區段的四維音高差異（pitch mean right jump, pitch dim2 
right jump, pitch dim3 right jump, pitch dim4 right jump）。 
4.
 
此區間長度（duration）。 
5.
 
左手邊前後兩區段間的間隔延遲（left pause duration）。 
6.
 
右手邊前後兩區段間的間隔延遲（right pause duration）。 
7.
 
此區段的四維對數能量（energy mean, energy dim2, energy dim3, energy 
dim4）。 
8.
 
左手邊前後兩區段的四維對數能量差異（energy mean left jump, energy 
dim2 left jump, energy dim3 left jump, energy dim4 left jump）。 
9.
 
右手邊前後兩區段的四維對數能量差異（energy mean right jump, energy 
dim2 right jump, energy dim3 right jump, energy dim4 right jump）。 
無聲部份包括：
 
1.
 
此區段的長度（duration）。 
2.
 
此區段的四維對數能量（energy mean, energy dim2, energy dim3, energy 
dim4）。 
 133 
 
圖4.16：經過語音邊界處理後之韻律狀態標記結果。 
 
將所得到的 code-words 對註冊語者的訓練語料做標記，則每個區間都會對
應到一個 code-word 的代表值。 
接著用訓練語料的韻律參數分佈對測試語料的韻律特徵參數做正規化，再利
用訓練語料所得出之韻律狀態類別對測試語料的韻律特徵參數做分類，將測試語
料的韻律特徵參數也標上相對應的狀態，即可得到測試語料之參數。
 
統計出所有訓練語料的轉移統計矩陣。即可得到訓練語料之 uni-gram 與
bi-gram【8】背景模型。依各別註冊語者統計之 uni-gram 與 bi-gram 之矩陣，即
為語者韻律模型。
 
4.2.2.4  語者驗證系統效能評估 
相等錯誤率（Equal Error Rate）是一種評估語者驗證系統的方式。所謂的相
等錯誤率就是錯誤的拒絕機率與錯誤的接受機率相等的機率值。但在某些特殊的
情形中錯誤拒絕與錯誤接受的後果或重要性並不相等，舉例來說，語者驗證應用
在金融取捨的情況，為了避免冒領盜用，因此錯誤接受的機率必須減至最低。
 
 135 
 
4.2.3  語者驗證系統基礎實驗與討論 
4.2.3.1  實驗語料庫 
ISCSLP2006-SRE 是由中文語言資源聯盟（Chinese Corpus Consortium, CCC）所
發行的語料庫，其詳細資料如下：
 
1.
 
語者數目：800（800 位男性）。 
2.
 
訓練語料：每人約 36 秒。 
3.
 
測試語料：每人約 16 秒。 
4.
 
取樣頻率：8kHz。 
5.
 
檔案格式：WAV。 
TRSC 是由中文語言資源聯盟（Chinese Corpus Consortium, CCC）所發行的
語料庫，此資料庫是為了第三章與第四章做準備，在此先介紹。其詳細資料如下： 
1.
 
語者數目：500（250 位男性，250 位女性）。 
2.
 
訓練語料：每位語者大約唸 110 句。 
3.
 
取樣頻率：8kHz。 
4.
 
檔案格式：Miu-law。 
4.2.3.2  實驗內容與結果 
本章實驗中特徵參數使用 38 維 MFCCs（12 維 MFCCs、12 維 ∆-MFCCs、
12 維 ∆2-MFCCs、1 維 ∆-log energy 和 1 維 ∆2-log energy），ISCSLP2006-SRE 是
透過麥克風錄音，所以在求取 filterbank 能量時，triangle filterbank 頻寬為
100-3800Hz。除此之外也對特徵參數作 CMS/MVA 處理，進一步的消除通道不匹
 137 
 
 
圖4.18：ISCSLP2006語料庫，MAP-GMM+有聲區間與MAP-GMM+ASR之DET 
Curve。 
 
Pitch、energy 求取使用 Wave-surfer/Snack 軟體，針對 ISCSLP2006 語料庫
設定為 30~600Hz，在此系統中特徵參數使用音高及其微分項 ∆-pitch、對數能量
及其微分項 ∆-log-energy 這 4 維參數，語者模型使用 4、8、16 高斯混合數的
GMM，這裡的 GMM 不需經過最大概似函數調適，直接將每位語者所求得之 4
維參數訓練出語者音高與對數能量高斯混合模型和只求 ASR 求出的語音邊界的
資料來做語者音高與對數能量高斯混合模型。不過為了能將系統之分數正規化，
所以還是訓練出包含所有訓練語者音高與對數能量的音高與對數能量 UBM。 
 
 
 
 139 
 
1. 我們將做過正規化處理的韻律特徵參數利用向量量化分群，將分群後所
得到之 M（M=8）個 code-words，則每個 code-word 可視為一特定韻律
狀態，據此建立一韻律模型。 
2. 利用建立好的韻律模型，即可以自動將輸入之韻律特徵參數標記成韻律
狀態索引序列。 
3. 將自動標記好的韻律狀態序列，統計出韻律序列組合的 uni-gram 與
bi-gram 模型，作為語者模型。 
4. 輸入測試語料得到測試語料之韻律狀態索引序列，以此序列計算出宣稱
語者之分數。 
表 4.3、表 4.4 分別為 ISCSLP2006-SRE 母音區間和非母音區間訓練語料的
韻律參數經過向量量化後 code-word 值，表 4.5 為 ISCSLP2006-SRE 訓練語料的
轉移統計矩陣。檢查每個 code-word 質心的值，統計每個 code-word 在句子中出
現的位置，與轉移矩陣交叉驗證後，大致可將所得到的 code-words（之後將
code-words 統稱為韻律狀態（prosodic state））分為幾類。舉例來說， 狀態 3 顯
示現在區段與上一個區段的間隔延遲最大，歸類為語句的開始，狀態 5 顯示現在
區段與下一個區段的間隔延遲最大，歸類為語句的結束。而狀態 11 皆為長時間
的斷句，狀態 9 為短時間的斷句。接著對上面所得到的結論與狀態轉移矩陣交叉
比對可得到狀態 1 轉移至狀態 11 的個數相較其他狀態最多，此類應歸類為句尾，
狀態 11 轉移至狀態 3 的個數相較其他狀態最多，此類應歸類為句首。這些狀態
所呈現的結果足以驗證我們的所獲得的韻律特徵參數是有意義的。
 
 
 
 
 141 
Pau_L Pau_R Dur En1 En2 En3 En4 En1_L En1_R En2_L En2_R En3_L En3_R En4_L En4_R
1.0123 0.6583 -0.315 -0.1606 -0.0233 -0.0286 0.0257 -0.1947 -0.2512 0 -0.032 -0.0113 0.0262 0.0283 -
-0.9785 0.6891 0.2232 0.2969 0.1349 -0.0511 -0.0704 -0.0317 0.0755 0.0574 0.1209 0.0229 0.031 -0.0887 -
2.1067 -0.7189 -0.3356 0.6505 0.096 -0.1671 0.0787 0.3223 0.0847 0.0669 -0.0373 -0.0135 -0.0445 0.0776 0.1309
-1.1755 1.5177 0.3182 -0.0067 0.0192 -0.0393 -0.059 -0.0829 -0.199 -0.007 -0.0045 -0.0151 0.0233 -0.0728 -
1.7342 1.5505 -0.2182 0.5251 0.045 -0.1401 0.0739 0.1306 0.0587 -0.0099 0.0304 -0.0248 0.0014 0.0631 -
-1.3233 -1.4325 0.2493 -0.0186 0.0531 0.0556 -0.0761 -0.076 0.0844 0.0076 0.0146 0.0618 0.0108 -0.0736 
1.1132 -1.2836 -0.3007 0.06 -0.0017 -0.0417 0.0671 -0.0285 0.0641 -0.0152 -0.0663 -0.0204 -0.0397 0.0753 0.1334
1.1214 1.2711 -0.0807 0.006 -0.0789 -0.0557 0.0666 -0.0677 -0.1557 -0.0854 -0.0702 -0.0384 -0.0086 0.0622 
表4.4：利用 ISCSLP2006 無聲區間語料庫之註冊語者語音資料經向量量化後之
11-state 韻律模型。 
 Dur En1 En2 En3 En4 
En1 
_L 
En1 
_R 
En2 
_L 
En2 
_R 
En3 
_L 
En3 
_R 
En4 
_L 
En4 
_R 
9 2.247 0.104 -0.180 0.139 -0.054 0.821 0.7074 -0.0971 -0.0766 -0.105 -0.097 -0.0072 0.0031 
10 2.331 -0.436 -0.162 0.245 -0.070 -0.063 0.0656 -0.0004 -0.025 0.039 0.049 -0.0144 -0.0093 
11 3.282 -1.126 -0.050 0.249 -0.035 -0.843 -0.899 0.1007 0.1239 0.066 0.038 0.0267 0.0128 
 
 
 
 
 143 
 
 
圖4.20：ISCSLP2006 語料庫，VQ-LM 之 DET Curve。 
 
 
4.2.4  本子章節結論 
本 子 章 節 介 紹 本 章 使 用 之 三 種 語 者 驗 證 架 構 以 及 語 料 庫
ISCSLP2006-SRE，由實驗數據可知頻譜系統之系統效能遠比短程音高與長程韻
律系統之效能為佳。也因為如此，所以我們想要提昇長程韻律系統的效能讓他更
能夠輔助短程頻譜參數系統，所以我們希望能夠在藉由第三章的 LPM 架構來有
效的改善長程韻律系統的辨識率。
 
由表 4.1 可得知頻譜系統中之特徵參數經過前處理在加上 ASR 所求出的語
音邊界，可以有效降低附加於特徵參數上之雜訊與通道之干擾並使得參數更具有
語者說話特性，圖 4.21 為各基礎語者驗證技術之效能比較圖。 
 
 145 
4.3  LPM 架構下之語者驗證系統 
4.3.1  LPM 架構下之語者驗證 
如圖 4.6 所示，我們希望利用我們能抓取到的韻律訊息去學習語者的韻律變
化，並進一步訓練出韻律模型。由於我們想要去描述語者整句的韻律變化如圖
4.3 和圖 4.4 所示，我們要先做以下幾個步驟： 
1. 由於整句的韻律變化較難去描述，所以我們選擇較小的單位來描述，在中文
語言中我們使用以音節為單位來進行描述。如圖 4.16 所示，我們利用 ASR
來將整句切成好幾個音節單位。 
2. 在每一個音節單位中，因為每一個音節單位的長度不一樣，所以我們利用一
組 Discrete orthogonal polynomial 正交基底多項式如 （4.2.16） 式來逼近音
高軌跡、能量軌跡、區間長度最後再加上前後的變化來組成長程韻律參數
,k nX 。 
3. 但是因為在中文語料庫中長程韻律參數會受到語者、聲調、韻律影響，我們
希望建立一個 LPM 模型描述整個句子中每個音節的
,k nX 隨韻律狀態與音調
變化的關係，如（4.3.1）式所示，其中
,k nX 是我們觀察到第 k 句第 n音節的
長程韻律參數，而假設上層長程韻律參數會受到聲調因素 ( )
,k nt
PT 影響和韻
律狀態 ( )
,k npPP 因素影響。而 ,k nY 是用一個 n維高斯分佈 ( ), ;0,k nN Y R 去描述
正規化的長程韻律參數，因此我們所觀測到的韻律參數
,k nX 用（4.3.2）式來
描述。 
4. 最後每個音節都有其長程韻律參數
,k nX ，那我們要去描述整句的韻律變化如
圖 1.6 所示，所以上層狀態跳動的關係來描述下層音高軌跡、能量軌跡、區
間長度之間的關係如 （4.3.3） 式、（4.3.4） 式所示。每個音節單位都有其
 147 
Model，如此一來我們就可以作長程韻律參數語者驗證的實驗，在訓練和測試時
所使用的參數都是長程韻律參數。我們會先將 TRSC UBM 的聲調模型和 LPM 模
型訓練出來，其訓練過程在往後本章節節會介紹。
 
 
首先由於本章實驗使用的資料庫是 ISCSLP2006-SRE，在這套資料庫裡並沒
有聲調的資訊，所以我們藉由 500-people TRSC 這套有聲調的資料庫來訓練成
TRSC MLP UBM 的聲調模型，並且利用 MIT 林肯實驗室所發展出之 LNKNET
工具【9】來輔助我們辨識 ISCSLP2006 的聲調資訊，此時 ISCSLP2006 當作測試
的語料去做聲調辨識。在聲調辨識的部份會在後面章節作介紹，我們先將
500-people TRSC train set 的部份訓練成聲調模型，之後在將 ISCSLP2006-SRE 當
作測試語料利用 LNKNET 工具來幫助我們辨識 ISCSLP2006 的聲調資訊。 
有了聲調的資訊後，我們就可以利用 TRSC LPM UBM 的聲調和韻律模型來
求取 ISCSLP2006 的韻律模型。但可預期的是聲調的辨識率不是百分之百，所以
由 LNKNET MLP 所辨識出來的聲調並不是完全可以相信的，為了解決此部份的
問題，所以我們提出了兩種決策的方式：（1）硬式決策，其架構如圖 4.22 所示，
（2）軟式決策，其架構如圖 4.23 所示，在這兩種方式都是藉由 LPM 的架構下
來幫助我們求取最佳化的狀態序列，我們就拿此狀態序列訓練我們的長程韻律模
型，並且實驗長程韻律參數語者驗證系統看看是否有比第二章的長程韻律參數語
者驗證系統準確。
 
4.3.1.1  硬式決策 
 
 149 
 
4.3.1.2  軟式決策 
 
( 1)kT t +
LPM-based
Prosody state
labeler
TRSC LPM
Model
'
kφLPM-assisted
MLP tone
recognizer
MLP Tone
recognizer
kX
(0)kT
TRSC MLP 1
Tone Model
( )kT t
TRSC MLP 2
Tone Model
ISCSLP SPK
LPM-LM or LPM
kφ
 
圖4.23：軟式決策架構圖。 
 
由於從MLP Tone recognizer出來的聲調資訊並非百分之百的正確，所以我們
希望利用遞迴計算的方式來修正聲調的錯誤，當然只能做些微的修改不可能一下
子修正到百分之百的正確度，只是希望能依靠著軟式決策的方法能夠在降低聲調
辨識的錯誤率。軟式決策方法如圖4.23所示。 
 步驟（1）： kX 是我們觀測到 ISCSLP2006-SRE 的長程韻律參數，此長
程韻律參數也是先對語者影響作正規化的處理之後再求取出來的。
 
 步驟（2）：首先我們利用 TRSC MLP 1 聲調模型當作訓練模型，而
ISCSLP2006-SRE 的長程韻律參數當作測試語料進去 LNKNET MLP 作
辨識，所以我們可以得到 ISCSLP2006-SRE 的 kT 聲調資訊。 
 
 151 
4.3.2  LPM 語者模型調適 
由硬式或軟式決策解出聲調與韻律狀態後，我們就有最後的聲調和韻律狀態
資訊，並藉由這兩種資訊來幫助我們建立屬於語者個人的 LPM 模型。但因每一
語者的語料不多，因此必須要作模型內差，即先以所有語者的資料訓練出 UBM
模型，再將每個語者的 LPM 模型與 UBM 模型以 MAP 方式作內差，語者 LPM
模型調適圖如圖 4.24 所示。 
 
ISCSLP
LPM Model
MAP
內差
TRSC LPM
UBM model
Smoothed
ISCSLP SPK 
LPM model
 
圖4.24：ISCSLP LPM 語者模型調適圖。 
 
4.3.3  LPM 語者模型測試架構 
 
ISCSLP Test
Feature
ISCSLP LPM
SPK Model
TRSC LPM
UBM Model
, ,
,
1
_ log ( | , , )k
k n k n
N
k n t p
n
SPK L P X SPK SPK R
=
 
=  
  
∏
, ,
,
1
_ log ( | , , )k
k n k n
N
k n t p
n
UBM L P X UBM UBM R
=
 
=  
  
∏
Σ
-
+
_ _S P K L U B M L−
 
圖4.25：ISCSLP LPM 測試架構圖。 
 153 
（4.3.8） 
其中
,k nX 是我們觀測到第 k 個音檔第 n 個音節的韻律參數， ,k nY 則是
normalized prosody contours ，
,k nt
PT 是影響因素中的聲調（ tone），其中
{ }
,
1, 2,3, 4,5k nt ∈ ， ,k npPP 則是韻律狀態（prosodic state）的影響因素，其中
{ }
,
1, 2,3,..., 8k np p∈ = 。 
每一個音節的長程韻律參數都含有聲調與韻律因素，扣除兩個主要因素之
後，剩餘項
,k nY 可以視為成一個的高斯分佈 ,( ;0, )k nN Y R ，所以 ,k nX 也可以視為一
個高斯分佈
, ,
,
( ; , )
k n k nk n t pN X PT PP R+ ，R 為基頻軌跡的 covariance matrix，由於
我們假設長程韻律參數皆為獨立，所以此 covariance matrix 只有對角線上有值。 
在上面我們已經將長程韻律參數的聲調與韻律狀態建立出對應的模型，接下
來便是要如何初始化且接著訓練這些模型，在這一節中我們採用了 Maximum 
Likelihood （ML） criterion，利用遞迴的方式重複訓練模型，使得這些模型可
以得到最佳化。由於每個因素的更新順序是非常重要的，以本章兩個因素而言，
聲調因素為第一優先，接著為韻律狀態因素。圖 4.27 為 LPM 訓練模型的流程，
由於語者因素的影響，我們在一開始求長程韻律參數時便將每位語者做了正規化
的處理，接著分別計算 tone 與 prosodic 模型的初始值，每次遞迴時再一次更新
tone 與 prosodic 模型，並計算 prosodic state 的 transition probability，直到 likelihood 
function 收斂為止。接著我們將在下面子節一一介紹訓練的步驟。 
 
 
 
 155 
量和區間長度作正規化的處理，希望藉由此步來消除語者影響的因素。
 
    
log( / ) log
log
( )
log( ) log
log
pitch
energy
duration
pitch lin pitch mean pitch mean
pitch std
energy lin energy mean
energy std
duration duration mean
duration std
µ
µ
µ
  −   
=
  
−   
=
 
−   
=
 
              
（4.3.9） 
 步驟（2）：將經由剛剛正規化的動作，在去求取長程韻律參數。在此時
就已經先消除語者影響的因素了。
 
 步驟（3）：利用步驟（2）所求出來的韻律參數，便可以對所有區間的
韻律參數建立聲調初始模型，計算式如下：
 
    
,
, ,
1 1
,
1 1
( ) ( )
( )
k
k n t
NK
k n k n
k n
t NK
k n
K n
X t t
PT
t t
δ
δ
= =
= =
× =
=
=
∑∑
∑∑
                            
（4.3.10） 
    
其中 K ＝語料庫中所有的人數， kN ＝語料庫中某個人的句子數，
,
{1,2,3, 4,5}k nt ∈ 。 
 步驟（4）：我們有了初始化的聲調參數模型，接下來利用 VQ 分群的方
式將最後餘項
,
, k nk n tX PT− 分成 m 群，也就是 m 個韻律狀態，最後對分
完群後的所有韻律參數標記上所屬的韻律狀態
,k nP ，並且利用如下數學
式計算出
,k np
PP 。
 
 157 
    
其中
,k npPP =在第 k 句中的第 n 個音節的韻律狀態模型。 
 步驟（6）：在有了更新後的
,k nt
PT 聲調模型參數後，我們希望利用聲調
模型與韻律模型重建韻律參數（
, ,k n k nt pPT PP+ ）能與觀察到的韻律參數
（
,k nX ）非常相似，所以在重新標記韻律狀態時，為了使得標記為正確
的韻律狀態機會提升，所以我們利用兩者之間的誤差
,k nY ，而這個誤差
我們把它視為一個常態分佈 (0, )N R ，其中 mean 為 0，covariance matrix
為R ，計算式子（4.3.13）如下所示： 
    
, , , ,
, ,
1 1
( ) ( )
k
k n k n k n k n
NK
T
k n t p k n t p
k n
t
X PT PP X PT PP
R
N
= =
− − × − −
=
∑∑
      
（4.3.13） 
    
其中 tN 為語料庫中所有的音節數。 
 步驟（7）：有了上述步驟（6）的過程後，我們可以利用 Viterbi search 
algorithm 來重新標記我們的韻律狀態的狀態序列，主要目的是讓每區
間的韻律狀態能得到最佳的可能性，數學式子（4.3.14）如下： 
,
*
, , , , ,1 , , 1
1 1 2
arg max log ( | , , ) ( ) ( | )k k
k n
N NK
k n k n k n k n k k n k n
p k n n
p P X t p R P p P p p
−
= = =
     
=     
        
∏∏ ∏  
（4.3.14） 
其中
, , 1
2
( | )k
N
k n k n
n
P p p
−
=
∏ 為狀態與狀態之間的轉移機率。 
 步驟（8）：利用 Viterbi search algorithm 重新標記韻律狀態後的結果，
重新計算與更新韻律模型，更新方式與步驟（4）相同，但原本一開始
利用 VQ 所分出來的 m 個韻律狀態，經過步驟（8）之後，會將每一群
 159 
 
圖4.28：TRSC MLP Tone 模型訓練和 ISCSLP Tone recognition 圖。 
 
 
 
 
4.4  實驗數據與結果分析 
4.4.1  TRSC 聲調辨識 
對 ISCSLP2006-SRE 此語料庫作聲調辨識前，我們會先實驗 TRSC 語料庫自
己本身的聲調辨識率。我們作的 TRSC 聲調辨識實驗共有兩類：（1） 27 維韻律
參數直接作聲調辨識。（2） 27 維韻律參數扣除經由 LPM 架構求出的韻律參數
影響。 
27 維長程韻律參數聲調辨識： 
在此實驗中，我們的實驗條件：（1）MLP hidden=25 （2） MLP hidden=50 （3）
TRSC 27dim
Prosody feature
LNKNET
MLP train
TRSC MLP
Tone model
ISCSLP 27dim
Prosody feature
LNKNET MLP
Tone recognizer
ISCSLP tone
information
train
test
 161 
 
表4.8：LNKNET MLP hidden = 25 聲調辨識。 
Class 1 2 3 4 5 Total 
1 4254 716 90 787 0 5847 
2 306 5202 345 633 0 6486 
3 50 754 1850 1103 0 3757 
4 339 501 271 6477 0 7588 
5 5 19 10 34 0 68 
Total 4954 7192 2566 9034 0 23746 
 
Class Patterns #Errors %Errors 
1 5847 1593 27.24 
2 6486 1284 19.80 
3 3757 1907 50.76 
4 7588 1111 14.64 
5 68 68 100 
Overall 23746 5963 25.11 
 
 
 
 
 163 
 
 
表4.10：LNKNET MLP hidden = 75 聲調辨識。 
Class 1 2 3 4 5 Total 
1 4725 425 139 558 0 5847 
2 504 4810 737 434 1 6486 
3 91 325 2818 522 1 3757 
4 513 333 596 6146 0 7588 
5 10 16 26 16 0 68 
Total 5843 5909 4316 7676 2 23746 
 
 
 
 
 
 
 
 
 
 
Class Patterns #Errors %Errors 
1 5847 1122 19.19 
2 6486 1676 25.84 
3 3757 939 24.99 
4 7588 1442 19.00 
5 68 68 100.00 
Overall 23746 5247 22.10 
 165 
 
 
 
表4.11：LNKNET MLP hidden = 25 聲調辨識。 
 
 
 
 
 
 
 
Class 1 2 3 4 5 Total 
1 4570 554 135 588 0 5847 
2 406 4973 693 417 0 6486 
3 72 492 2646 547 0 3757 
4 533 445 598 6013 0 7588 
5 8 19 16 25 0 68 
Total 5589 6483 4088 7590 0 23746 
Class Patterns #Errors %Errors 
1 5847 1277 21.84 
2 6486 1516 23.36 
3 3757 1111 29.57 
4 7588 1576 20.77 
5 68 68 100 
Overall 23746 5548 23.36 
 167 
表4.13：LNKNET MLP hidden = 75 聲調辨識。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Class 1 2 3 4 5 Total 
1 4651 464 99 633 0 5847 
2 390 5033 588 478 0 6486 
3 62 440 2680 575 0 3757 
4 475 374 448 6292 0 7588 
5 6 19 19 24 0 68 
Total 5584 6330 3834 8002 0 23746 
Class Patterns #Errors %Errors 
1 5847 1196 20.45 
2 6486 1456 22.44 
3 3757 1077 28.67 
4 7588 1297 17.09 
5 68 68 100.00 
Overall 23746 5094 21.45 
 169 
表4.14：有聲區間 covariance matrix。 
 1 2 3 4 5 6 7 8 9 
Befor 0.669 0.169 0.040 0.014 1.00 1.097 0.315 0.329 0.072
After 0.370 0.109 0.033 0.013 0.64 0.774 0.273 0.257 0.065
 
10 11 12 13 14 15 16 17 18 19 
0.077 0.026 0.027 4.602 4.540 7.778 0.727 0.026 0.020 0.005
0.068 0.024 0.025 0.107 0.141 0.160 0.095 0.021 0.008 0.003
 
20 21 22 23 24 25 26 27 
0.1688 0.1302 0.0508 0.0478 0.0183 0.0171 0.0104 0.0103 
0.1027 0.115 0.046 0.0431 0.0162 0.0157 0.0083 0.0066 
 
表4.15：無聲區間 covariance matrix。 
 1 2 3 4 5 6 
Before 7.2585 0.293 0.0367 0.0303 0.0123 0.545 
After 0.191 0.1196 0.0335 0.0167 0.0103 0.1397 
 
7 8 9 10 11 12 13 
0.4338 0.0638 0.0606 0.0318 0.03 0.0206 0.0197 
0.1973 0.0577 0.0583 0.0311 0.0298 0.0188 0.0191 
 
 
 171 
表4.17：TRSC 無聲區間語料庫之註冊語者資料經 LPM 後之 3 狀態韻律模型。 
 Dur En1 En2 En3 En4 
En1 
_L 
En1 
_R 
En2 
_L 
En2 
_R 
En3 
_L 
En3 
_R 
En4 
_L 
En4 
_R 
9 2.371 0.305 -0.067 0.114 -0.048 1.262 0.215 -0.184 -0.022 0.002 -0.017 -0.098 -0.003 
10 2.460 0.056 -0.053 0.120 -0.049 -0.039 0.105 -0.008 -0.002 -0.005 0.014 -0.002 -0.006 
11 3.316 -0.854 0.056 0.104 0.019 -0.922 -1.013 0.080 0.100 0.053 0.012 0.045 0.052 
 
最後我們將求取出來的狀態序列分別訓練成 ISCSLP UBM bi-gram 和
ISCSLP SPK bi-gram，其中 ISCSLP SPK bi-gram 因為狀態序列個數並沒有很充
足，所以在訓練 ISCSLP SPK bi-gram 時我們利用內差的方式去調整 ISCSLP SPK 
bi-gram 的模型，其示意圖如圖 4.30 所示。表 4.18 是 ISCSLP UBM Bi-gram 的統
計量。 
 
ISCSLP SPK
Bi-gram 內差
ISCSLP UBM
Bi-gram model
ISCSLP SPK
Bi-gram model
 
圖4.30：ISCSLP SPK Bi-gram 模型建立。 
 
 
 
 173 
表4.19：LPM-LM （Hard）測試表。 
系統種類 EER（%） 
LPM-LM （Hard） 30.17% 
MFCC + 0.45* LPM-LM （Hard） 4.50% 
 
 
 
 
 
 
 
 
圖4.31：LPM-LM （Hard）和 VQ-LM EER 比較圖。 
 
 
 
 
 
 
 175 
 
圖4.33：軟式決策收斂圖。 
 
表4.20：軟式決策 likelihood 收斂表。 
次數 1 2 3 4 5 6 
Likelihoo
d 
-2128439
1 
-2127382
1 
-2127354
9 
-2127354
1 
-2127314
4 
-2127312
8 
誤差率 X 4.96e-004 1.28e-005 3.40e-007 1.86e-005 7.67e-007 
 
 
 
 177 
 
 
圖4.35：LPM-LM （Soft） 跟 MFCC 作 fusion 的比較圖。 
 
4.4.2.3  LPM 語者模型實驗 
在經由軟式決策後，我們就有較新的聲調資訊和韻律狀態資訊，雖然說這兩
種資訊都是基於利用 TRSC LPM 架構求來的，但是由表 4.19 及 4.22 來看，至少
在韻律狀態的確有改善，而在聲調辨識方面在每次更新聲調的資訊跟上一次的聲
調資訊來做比較也越來越精準，所以我們就有更新過後的韻律狀態序列和聲調資
訊。如此一來我們便可以建立每個人的 ISCSLP LPM 語者模型，而建立的流程如
圖 4.24 所示，之後我們便可以拿取 ISCSLP 測試語料進行測試，而測試的架構如
圖 4.25 及圖 4.26 所示。 
 
 
 
 179 
是 LPM-LM（Hard）跟 LPM-LM（Soft）都會比傳統的 VQ-LM 來的好，最後我
們將短程頻譜系統和短程音高與能量系統分別去和長程韻律的系統做分數上的
比較和整合可以將錯誤率降至 4.00%。 
 
表4.24：LPM 架構下各種組合之系統效能。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
系統種類 EER（%） 
（1） MAP-GMM / MVA 5.68 
（2） MAP-GMM / Voiced 5.04 
（3） MAP-GMM / ASR 4.84 
（4） Pitch-GMM 16.97 
（5） VQ-LM 38.37 
（6） LPM-LM （Hard） 30.17 
（7） LPM-LM （Soft） 29.17 
（8） LPM 29.13 
（3） + （4） 4.33 
（3） + （5） 4.67 
（3） + （6） 4.50 
（3） + （7） 4.50 
（3） + （8） 4.48 
（3） + （4） + （5） 4.33 
（3） + （4） + （6） 4.00 
（3） + （4） + （7） 4.00 
（3） + （4） + （8） 4.00 
 181 
 
參考文獻
 
[4.1].
 
Tseng, Chiu-yu, Pin, Shao-huang, Lee, Yeh-lin, Wang, Hsin-min and Chen, 
Yong-cheng （2005）. “Fluent speech prosody: framework and modeling,” to 
appear in Speech Communication. 
[4.2].
 
D. A. Reynolds et. Al., “The superSID project; exploiting highlevel 
information for high-accuracy speaker recognition, ”Proc.ICASSP’03, vol, 
IV,pp.784-787,2003. 
[4.3].
 
Chen-Yu Chiang, Xiao-Dong Wang, Yuan-Fu Liao, Yih-Ru Wang, Sin-Horng 
Chen, Keikichi Hirose, “LATENT PROSODY MODEL OF CONTINUOUS 
MANDARIN SPEECH”,  ICASSP’2007. 
[4.4].
 
The Snack Sound Toolkit, http://www.speech.kth.se/snack/  
[4.5].
 
C.P. Chen, K. Filali and J. Bilmes, “Frontend Post-Processing and Backend 
Model Enhancement on the Aurora 2.0/3.0 Databases,” Proc. ICSLP, pp. 
241-244, , 2002. 
[4.6].
 
D. Reyolds, T. Quatieri and R.Dunn, “Speaker Verification Using Adapted 
Gaus-sian Mixture Models,” Digital Signal Processing, vol. 10, pp. 19-41, 
January 2000. 
[4.7].
 
D. A. Reynolds, “Channel Robust Speaker Verification via Feature Mapping,” 
in Proc. ICASSP’03. 
[4.8].
 
D. A. Reynolds et. al., “The superSID project: exploiting high-level 
information for high-accuracy speaker recognition,” Proc. ICASSP’03, vol. IV, 
pp.784-787, 2003. 
 183 
第五章  基於潛藏性韻律模型之台灣國語與大陸
普通話腔調辨認 
5.1  緒論 
5.1.1  研究動機與背景 
語言的不同，大部分是取決於發音方法的不同，而發音的不同可經由不同單
位討論。可以以單字為單位來探討，也可以深入一點，用拼音來探討，最好的情
況就是某種發音只會出現在某種語言上，當你偵測到這種發音時，就可以明確辨
認出這種語言。當然，實際上與理想情況總是會有一段差距，因為人類發音構造
的限制，能發出的音有限，但是語言卻有這麼多種，所以發音多少會有混用的情
況，或是聽起來差不多，只有一點點微妙的不同。
 
 
 
圖5.1：不同層次之語言資訊【1】。 
 185 
在著如圖5.2的階層式韻律結構【4】，即完整的語音訊息，包含韻律片語群、韻
律片語、韻律詞與其之實際頻譜統計特性，且其表現出來的韻律軌跡變化特性，
如圖5.4的波浪圖所示(以F0為例)，而音節是我們可以實際觀測到並加以擷取的特
徵參數。  
 
 
圖5.2：語音韻律之多層次結構圖。 
 
 
圖5.3：韻律片語群之音高趨勢圖。 
 187 
 
圖5.5：使用韻律資訊來建立 LPM-LM 概念圖。 
 
Segment
level
Supra-segment
level 
Prosody
&Tone
StateQ
Prosody
&Tone
State1 Prosody
&Tone
State2 Prosody&Tone
State3
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
Prosodic
Trajectory
PDF
 
圖5.6：LPM 架構圖。 
 
5.1.2  研究方法 
本章針對短程韻律參數與長程韻律狀態之資訊，建立一隱藏式馬可夫模型，
在測試時同時使用到訓練資料的短程與長程資訊。
 
 189 
    
另外我們還使用聲調資訊來區分語言，使用的架構如圖 5.8 所示： 
 
 
圖5.8：使用聲調模型辨認語言架構圖。 
 
求取了 LPM 所需要的參數以後，就可以將這些參數拿來訓練各個聲調模
型，在測試時受測語料會同時經過兩個語言的聲調模型，最後比較那個模型的分
數比較高，就可以決定測試語料是屬於那種語言。
 
5.1.3  章節概要 
本章之子章節編排說明如下：5.1 節為緒論。5.2 節說明本章中語言辨認之基
本系統及中華民國計算語言學學會(The Association for Computational Linguistics 
and Chinese Language Processing, ACLC -LP)提供之MAT(Mandarin across Taiwan)
以及中文語言資訊聯盟(Chinese Corpus Consortium, CCC)提供之 500 people 
TRSC (Telephone Read Speech Cropus)語料庫，得知基本系統之效能。5.3 節為基
於 LPM 架構之語言辨認。5.4 節為實驗結果與分析。5.5 節為結論與未來展望。 
 191 
特徵參數是使用 30ms 為一音框及 10ms 之音框位移求得的 39 維
mel-frequency cepstral coefficiences (MFCCs)，包括 12 維 MFCCs、12 維
∆-MFCCs、12 維 ∆2-MFCCs、1 維 ∆-log energy 和 1 維 ∆2- log energy。 
2.
 
特徵參數前處理：將所求得之 39 維 mel-frequency cepstral coefficiences 
(MFCCs)的前 12 維 MFCC 經過 MV 處理，消除錄音時背景雜訊所造成
之加成性雜訊和語音經過通道所造成之旋積性雜訊，以減低雜訊與通道
對特徵參數之影響。
 
3. 音素辨認器(Phone Recognizer)： 依據音素標記，將音檔求得的參數訓
練成單音素模型，使用隱藏式馬可夫模型來表現音素模型，使用 MAT
與 500 people TRSC 資料庫來訓練音素模型 
4.
 
語言模型：
 
藉由音素辨認器，我們可以將一段句子解碼成音素序列，
多連模型可以估計在觀察音素序列的特定音素的出現機率。
 
5.
 
分類器：未知音素序列經過各個語言模型後，將會有各自的分數，經由
分類器來決定未知音素序列屬於那種語言。
  
其中對於特徵參數前處理，音素辨認器以及語言模型之建立詳細說明如下：
 
MV【6】的步驟分成三個部份，分別使用了平均消去法(Mean Subtraction, 
MS)、變異數正規化法(Variance Normalization, VN)，即可達到語音強健性的效
果。圖 5.10 為 MV 流程圖。 
 
 
圖5.10：對語音特徵參數作 MV 之流程圖。 
 193 
 
圖5.11：使用隱藏式馬可夫模型建立音素模型。 
 
語言模型是將句子藉由音素辨認器解碼為音素單位後，再去統計不同的音素
間的發生機率，(5.2.5)式就是多連語言模型的表示式。 
 
                                    (5.2.5) 
 
一句未知語音將藉由音素辨認器解碼為音素序列 1 2{ , ,..., }kA α α α= ，音素序
列 A 對應於某一語言模型的對數概似分數將會是以下列(5.2.6)式來計算。 
 
                          (5.2.6) 
 
其中 lλ 是某種語言 l的語言模型。 
在這裡我們是使用 SRLIM toolkit 來進行語言模型的建立，每個語言模型都
是三連架構，並對三連模型做了 Good-Turing discounting 與 Katz backoff 平滑化
以讓模型的機率分布不會有特別強或特別弱的部分。
 
 
1 2 3 ( 1)( | , , ,..., )k k k k k NP α α α α α− − − − −
1 2 ( 1)( | ) log ( | , ,... , )
K
l k k k k N l
k N
L A Pλ α α α α λ
− − − −
=
= ∑
 195 
 
 
我們將整個通用音素辨認器後接語言模型基本系統，分成幾個區塊來描述，
包括： 
1.
 
特徵參數萃取(feature extraction)：與平行音素辨認語言模型系統相同。 
2.
 
特徵參數前處理：同樣將將特徵參數經過 MV 處理。 
3.
 
通用音素辨認器：為了簡化系統，可以將多國的音素合併，因為人類口
腔能夠發出的聲音是有限的，所以一定會有跨語言的音素，將這些類似
的音素與其他不同的音素重新訓練成一個新的音素辨認器。
 
4.
 
語言模型：
 
與平行音素辨認語言模型系統相同。
 
5.
 
分類器：與平行音素辨認語言模型系統相同。
 
其中對於通用音素辨認器型之建立詳細說明如下：
 
通用音素辨認器是使用 MAT 與 TRSC 資料庫的語料訓練而成的，我們假設
普通話與國語的明顯差異出現在捲舌與不捲舌音，也就是台灣人唸三個翹舌音
ㄓ、ㄔ、ㄕ唸成舌齒音ㄗ、ㄘ、ㄙ。但是發音部位不像ㄗ、ㄘ、ㄙ那麼前面，約
是介於兩者之間，或是有ㄦ不捲舌念成ㄜ的情形，還有ㄥ的誤讀，所以在這幾個
音素標記使用不同的音素來表示，其他的音素則還是用與平行音素辨認器所使用
的標記一樣，於是我們得到了 63 個音素的通用音素標記，並使用這套標記來訓
練通用音素辨認器。 
5.2.1.3  位移差分化倒頻譜(Shifted Delta Cepstral)特徵高斯混合模型系統 
本章所使用之位移差分化倒頻譜特徵高斯混合模型系統如圖 5.13 所示： 
 197 
位移差分化倒頻譜參數使用了四個參數，N-d-P-k，N 是倒頻譜參數的數
目，d 是計算差量時，時間的超越與延遲，P 是中心點往前位移多少音框，k
是串聯多少個參數方塊。參數方塊是 ( ) ( ) ( )c t c t iP d c t iP d∆ = + + − + − ，最後的
參數向量就是將參數方塊串起來( ( ), ( ),..., ( ( 1) )c t c t P c t k P∆ ∆ + ∆ + − )。 
一個語言將會有一個 GMM 模型，GMM 語言模型由許多高斯密度函數組
成，每一個高斯密度函數用來表示一些語言的聲學類別，以反應出語言特性分
佈，用數學式子表達，則一個 GMM 可以是 M 個密度函數做權重合所建構出來
的。
 
 
(5.2.8) 
其中 x是一個D維度的隨機向量，密度函數 ( ), 1,...,ib x i M=
 和 , 1,...,ip i M= 為
混合權重，而每一個密度函數的形式是一個 D 維度的高斯函數。 
 
(5.2.9) 
其
中 iµ
 為平均值向量和 iΣ 為共變異數矩陣，且混合權重要滿足 1 1Mi ip=Σ = 。 
完整的高斯混合密度函數可以使用所有的密度函數的平均值向量、共變異數
矩陣和混合權重等參數描述，這些參數的集合可以由下面的符號表示，  
                    (5.2.10) 
 
每一個語言皆用其相關模型λ 表示其 GMM 模型。 
    使用 htk Hvite 指令得到比對兩個 GMM 模型，得到兩個 loglikehood 分數，
那個模型分數高就把測試語料歸類為該種語言。 
 
1
( | ) ( )
M
i i
i
p x p b xλ
=
=∑
 
-1
1/ 2/ 2
1 1(  )  exp - (  -  ) (  -   )
2( 2 ) | |
t
i i iiD
I
b x x xµ µ
pi
 
=  
 
∑
∑
    
{ }    ,   ,    ,     1, ,i i ip i Mλ µ= =∑ ⋯
 199 
5.
 
檔案格式：WAV( µ -law)。 
6.
 
Training set：男 20095 句，188 人。  女 23245 句，221 人。 
7.
 
Development set：男 3604 句，34 人。女 8990 句，86 人 
8.
 
Test set：男 1001 句，10 人。女 1041 句，10 人。 
5.2.2.2  實驗內容與結果 
本章實驗中特徵參數使用 39 維 MFCCs(13 維 MFCCs、13 維 ∆-MFCCs、13
維 ∆2-MFCCs、)，因為 MAT4500 與 TRSC 是透過電話線錄音，而 TIMIT 不是，
為了減少通道不匹配所造成的影響，所以在求取 filterbank 能量時，triangle 
filterbank 頻寬為 300-3400Hz，為了減少 TIMIT 的通道不匹配，在音檔上使用
MIRS channel 來加入與電話線類似的通道效應，除此之外也對特徵參數作 MV
處理，進一步的消除通道不匹配的影響。
 
使用 HTK【8】工具來訓練隱藏式馬可夫音素模型，MAT4500 與 TRSC 使
用了 3 個狀態，1 個高斯混合模型來建立個 50 個音素模型，TIMIT 使用了 3 個
狀態，16 個高斯混合來建立 61 個音素模型，但在音素解碼後會將類似的音素合
併，合併後音素將減少到 38 個音素。 
    
表 5.1 為各個語料庫的音素辨認相對錯誤率。 
 
表5.1：三語料庫音素辨認之相對錯誤率。 
語料庫 MAT TRSC TIMIT 
錯誤率 47.90% 40.81% 47.65% 
 
 
 201 
 
表5.3：使用 TRSC PR 與 TIMIT PR 的 PPRLM 實驗。 
高斯混合數
 1,1 1,16 32,16 
平均錯誤率
 34.38% 27.65% 31.95% 
國語錯誤率
 27.47% 25.03% 32.00% 
普通話錯誤率
 41.19% 30.23% 31.90% 
 
由實驗結果觀察得知，這樣的效果的確會比使用普通話與國語音素辨認器的
情況好，於是我便想測試如果再加上國語音素辨認器是否能讓辨認效果再好一
點，以下便是實驗結果：
 
表 5.4 使用 TRSC 音素辨認器，MAT 音素辨認器與 TIMIT 音素辨認器當前
端，音素模型為 3 state HMM。 
 
表5.4：MAT 、TRSC 、TIMIT 三種 PR 的 PPRLM 實驗。 
高斯混合數
 1,1,1 1,1,16 32,32,16 
平均錯誤率
 24.88% 25.50% 31.78% 
國語錯誤率
 31.80% 28.67% 37.58% 
普通話錯誤率
 17.90% 22.36% 26.05% 
 
 203 
表5.6：不同音素數 UPRLM 實驗。 
音素數
 63 個音素 100 個音素 
平均錯誤率
 27.50% 23.79% 
國語錯誤率
 44.94% 40.06% 
普通話錯誤率
 10.27% 7.71% 
 
由表 5.5 我們得知，使用 100 個音素的平均效果比較好，不過是建立在普通
話錯誤率非常低的情況。在辨認國語這部分仍然是效果比較差的。
  
    參考前人的論文【1】【5】後，我們使用了不同的各種參數設定來進行實驗。 
 
表5.7：SDC 參數 7317 的實驗結果。 
高斯混合數 128 256 512 1024 
平均錯誤率 40.39% 36.11% 36.06% 35.37% 
國語錯誤率 62.71% 48.18% 48.33% 49.90% 
普通話錯誤率 24.13% 24.18% 23.94% 21.10% 
 
表5.8：SDC 參數 7337 的實驗結果。 
高斯混合數 128 256 512 1024 
平均錯誤率 30.55% 29.29% 29.11% 30.37% 
國語錯誤率 39.77% 42.01% 44.44% 48.8% 
普通話錯誤率 21.43% 16.71% 13.96% 12.2% 
 205 
    
由觀察得知，高斯混合數越高效果越好，但是可能是語料庫不足的原因，高
斯混合數達到 32 時效果已經是最好了。 
5.2.3  系統整合 
利用 5.2 節平行音素辨認器後接語言模型、通用音素音素辨認器後接語言模
型與位移差分化倒頻譜特徵高斯混合模型三個語言辨認系統之實驗結果，我們採
用 LNKNET【10】工具加以整合，其中對 LNKNET 中的設定為： 
1. 以類神經網路(Multi Layer Perceptron, MLP)之方式整合系統分數。 
2. hidden layer 為 40， cycles 為 30。 
3. 使用 development set 來訓練 MLP。 
 
表5.12：系統整合錯誤率表。 
語言辨認系統 平均錯誤率 國語錯誤率 普通話錯誤率 
(1)： PPRLM 24.88% 31.80% 17.90% 
(2)： UPRLM 23.79% 40.06% 7.71% 
(3)： SDC-GMM 29.11% 44.44% 13.96% 
(4)： (1)+(2) 21.84% 22.79% 20.89% 
(5)： (1)+(3) 22.53% 29.76% 15.38% 
(6)： (1)+(2)+(3) 20.68% 27.12% 14.30% 
 
    就平均錯誤率來看，UPRLM 的效果最好，但是這是建立在普通話錯誤率很
低而把錯誤率壓到很低，PPRLM 系統的錯誤率比較接近，應該是比較可靠的，
最後將三個系統整合起來，把平均錯誤率降低到 20.68%。 
 
 
 207 
同樣的一種語言，在除去語者資訊的干擾以後，應該會有相同或類似的韻律
結構，我們想要做的，就是找出這個語言大致上的韻律結構，藉由基頻軌跡，能
量與音節長短這些參數，把語言的韻律結構量化。於是我們想建立語言的潛藏性
韻律模型 LPM(Latent Prosody Model)。 
5.3.1  潛藏性韻律模型 
5.3.1.1  公式推導 
在中文語音中，每個音節都具有一種聲調，因此在(5.3.6)式中描述了整句的
基頻(Pi)、能量(有聲能量 En，無聲能量 PEn)和長度(有聲長度 Du，無聲長度 PDu)
皆被聲調所影響，但是每個人說話都會有自己的韻律，而韻律當然也會同時干擾
基頻、能量和長度這三項，所以我們在式子後面改寫成基頻、能量和長度受到聲
調與韻律影響。 
我們使用四個 Legendre 多項式來近似基頻軌跡，如圖 5.15 所示： 
 
 
圖5.15：Legendre 多項式示意圖。 
 
以下就是 Legendre 多項式的數學表示式： 
0( ) 1
i
M
φ =
                                                       (5.3.1) 
1/ 2
1
12 1( ) [ ] [ ]
2 2
i M i
M M M
φ = −
+
                                          (5.3.2) 
 209 
最後我們把
,k nx
以以下的模型表示式來表達：
 
  
, ,
, , , ,
( | , , ) ( ; , )
k n k nk n k n k n k n t p
P x t p N x Rλ χ γ= +            (5.3.10) 
這個模型中同時包含了韻律的資訊與聲調的資訊。
 
    
有了潛藏性語言韻律模型後，我們可以對未知語料進行狀態序列解碼，並與
各語言的韻律狀態多連模型進行比對，找尋最相近的語言。
 
測試語料進行狀態序列解碼前，要先經由聲調辨認器辨認聲調，再由 LPM
進行狀態序列解碼，如圖 5.16 所示。 
 
 
圖5.16：狀態序列解碼架構圖。 
 
以下我們將說明潛藏性韻律模型的訓練與測試：
 
5.3.1.2  訓練流程 
    我們已經將基頻軌跡的聲調與韻律狀態建立出對應的模型，接下來便是要如
何初始化且接著訓練這些模型，我們採用 Maximum Likelihood (ML) criterion，利
用遞迴的方式重複訓練模型，使得這些模型可以得到最佳化。由於每個因素的更
新順序是非常重要的，以本章三個因素而言，語者因素為第一優先，接著為聲調，
其次為韻律狀態。圖 5.17 是如何建立一個 LPM 的流程圖，訓練流程的細節會在
後面章節仔細描述。 
 211 
 
5.3.1.3  初始化模型 
 步驟 1：求取資料庫的基頻軌跡，能量與音節長度。並且為了除去語者
因素，進行正規化，方法如下所述。 
(5.3.12)式為對基頻值進行正規化之數學式： 
                             
                                                                       (5.3.12)       
                                                  
 
 
 
其中 Pi： 對數基頻值。NtV： 資料庫中有聲音框數。 
(5.3.13)式為對能量值進行正規化之數學式： 
 
                                                     (5.3.13) 
 
 
 
 
 
 
 
En： 對數能量值。   
 Nt： 句子音框數。 
 NV：有聲音節數。 
 NUV ：無聲音節數。 
, ker
, ker
, ker ,
1 1
2
, ker , , ker
1 1
,
var
1
1
var ( )
V
k
V
k
pi spear
pi spea
NK
pi spea k nV
k nt
NK
pi spea k n pi speaV
k nt
Pi p
Pi voiced
p pi
N
pi p
N
µ
µ
µ
= =
= =
−
=
=
= −
∑∑
∑∑
,
,
,
1
2
, ,
1
var
1
1
var ( )
k
k
en sentence
en sentence
N
en sentence n
nt
N
en sentence n en sentence
nt
En p
En
p En
N
En p
N
µ
µ
µ
=
=
−
=
=
= −
∑
∑
( )
( )
1
1
1
,
1
,
V
UV
N
V
n
mean N
UV
n
En n voiced
N
En
En n unvoiced
N
=
=



= 



∑
∑
 213 
     k  = 語料庫所有句數 
    tN  = 語料庫中所有音節數 
 
 步驟 3：將韻律參數減去聲調影響 tP ，就是我們所要的韻律資訊，利用
VQ 分群的方法，將韻律分成 8 群，也就是 8 個狀態，再將狀態標記在
韻律參數的後面， 
                       (5.3.16) 
 
 
 
其中 {1,2,... 8}p P∈ = 。 
 
5.3.1.4  最佳化潛藏性韻律模型 
初始化後，我們得到的是粗略的潛藏性韻律模型，為了使模型更好，所以拿
新的韻律狀態模型修正聲調模型，在由新聲調模型訓練韻律狀態模型，利用 ML 
criterion，我們可以計算出每一次的 likelihood function L，藉由觀察 L是否收斂
來決定是否停止模型的更新。 
 步驟 4：得到更新後的韻律狀態模型，再次更新音調模型，如下式所示： 
                        (5.3.17) 
 
 
 
 步驟 5：我們希望我們利用聲調模型與韻律模型重建的基頻軌跡(recon 
-struct pitch contour; t tPT PP+ )能與觀察到的基頻軌跡(observer pitch 
contour; 
,k nX )相似，所以在重新標記韻律狀態的時候，為了使標記為正
,
,
1 1
1 1
( ) ( )
( )
k
k n
k
NK
k n p
k n
t NK
k n
X PP t
PT
t
δ
δ
= =
= =
− ×
=
∑∑
∑∑
,
1 1
1 1
( ) ( )
( )
k
k
NK
k n t
k n
p NK
k n
X PT p
PP
p
δ
δ
= =
= =
− ×
=
∑∑
∑∑
 215 
 步驟 9：重複步驟 5~8，直到 likehood function  L  收斂為止，以下是
likehood function L  的數學式： 
                   (5.3.20) 
 
 
 
 
 步驟 10：最後我們得到已經收斂的潛藏性韻律模型。 
5.3.2  基於潛藏性韻律模型之語言辨認 
在 5.3.1.3 節我們訓練出了潛藏性韻律模型，接下來就是測試的部分，一個
未知語料藉由維特比演算法找尋最佳路徑，得到最合適的狀態轉移序列，如下式
所示：
 
                                                          
(5.3.21) 
 
在測試的時候，
,k nt
PT 與
,k nt
PP 是合併在一起的。 
 
,
,
,
*
, , , ,
1 1
arg max{ log ( , , | , )}k
k n k nk n
NK
k n k n t k n k nPp k n
P N X PT PP Rp t p
= =
= +∑∑
, , , ,
,
, , ,
1 1
,
1 1
log[ ( | , )]
log ( | , )
log ( , , )
k
k
k n k n k n k n
NK
k n k n k n
k n
NK
k n t P t p
k n
L P X p t
P X p t
N X PT PP Rp
= =
= =
=
≈
= +
∑∑
∑∑
 217 
其中 lλ 代表某語言 l的韻律狀態多連模型。 
    找出與模型比較後分數最大的，這表示兩個情況最接近，於是我們可以把他
們視為相同的語言。 
5.3.3  藉由聲調辨認語言 
    從電視上的大陸節目我們可以觀察到，大陸人說話的抑揚頓挫比台灣人明顯
許多，所以兩種語言的聲調型狀應該是有一些差異的，於是我們便想建立一個音
調辨認器，觀察是否能區分國語與普通話。在辨識的時候，相同語言的音調辨認
分數應該會較好。 
5.3.3.1   系統架構 
    測試語料經過兩種語言的聲調模型後，將會得到一個分數，我們將比較那個
語言得到比較高的分數以決定測試語料屬於那種語言。 
5.3.3.2   訓練聲調模型 
    我們求取 27 維參數來訓練聲調模型，這 27 維參數如下： 
  dim1-4：使用四個 Legendre 多項式來表示基頻軌跡。 
  dim5-8：目前 Legendre 係數與前一音節基頻軌跡差。 
  dim9-12：目前 Legendre 係數與下一音節基頻軌跡差。 
  dim13：目前音節長度與前一音節長度差量。 
  dim14：目前音節長度與下一音節長度差量。 
  dim15：目前音節長度。 
  dim16-19：使用四個  Legendre 多項式近似能量軌跡。 
 dim20-23：目前 Legendre 係數與前一音節能量軌跡差。 
 dim24-27：目前 Legendre 係數與下一音節能量軌跡差。 
    得到 27 維參數以後，我們藉由正確的聲調標記，使用 LNKnet 訓練 5 聲聲
調模型，訓練的方法是多層感知機(Multi-Layer Perception)。 
5.3.3.3   藉由聲調模型辨認語言 
    在測試的部分，同樣的求取 27 維參數，並使用 LNKnet 進行測試，尋找最
 219 
 
5.4  實驗結果與分析 
5.4.1  潛藏性韻律模型語言辨認基本系統 
在本節我們將整個潛藏性韻律模型語言辨認基本系統，分成幾個區塊來描
述，包括： 
1.
 
基本參數求取：使用 Snack【11】 軟體求取音檔基頻值與能量大小。 
2.
 
求取音檔切割位置：使用自動聲音辨識系統(ASR)求取音檔以 syllable 為單位
的 segment 長度。 
3.
 
參數求取：使用 Legendre 多項式近似逼近基頻軌跡，並使用音檔能量與 seg 
-ment 長度來建立一 6 維參數。 
4.
 
訓練潛藏性韻律模型：利用上一章的模型建立方法，與求取出的 6 維韻律狀
態參數，訓練出國語與普通話的潛藏性韻律模型。
 
5.
 
測試語料潛藏性韻律模型比對：將測試語料輸入模型中，求取最佳解。
 
6.
 
分類器：決定測試語料屬於那種語言。
 
其中對於求取音檔切割位置，測試語料潛藏性韻律模型比對的詳細說明如
下：
     
5.4.1.1  求取音檔切割位置 
    使用 HTK 訓練 MAT 與 TRSC 資料庫的中文 411 syllable 模型，每個模型都
是 3 狀態 32 高斯混合隱藏式馬可夫模型，使用 Initial-Final 的模型來組成 syll- 
able，以下是兩個資料庫 syllable 辨識率如表 5.14 所示： 
 
 221 
 
圖5.20：使用 TRSC 資料庫訓練 LPM 之收斂曲線。 
 
從圖中可以看出使用 TRSC 資料庫訓練 LPM 大約 20 次就達到收斂了，而使
用 MAT 資料庫訓練 LPM 則大約要到 30 才會達到收斂的效果。 
    
表 5.15 與 5.16 是 MAT 與 TRSC 語料庫所訓練出來的韻律狀態，從形狀來
看是有一點點不同，但是差異並不明顯。
 
 
 223 
 
5.4.1.3  測試語料潛藏性韻律模型比對 
    
將潛藏性韻律模型展開為有聲音節 5*8 個模型，與無聲音節 3 個模型，一
未知測試語料也必需先求取 6 維參數，再經由維特比演算法求得最佳路徑，這時
我們可以得到未知測試語料的狀態轉移序列。
 
    
將測試語料與普通話三連狀態序列模型和國語三連狀態序列模型做比對，決
定本測試語料與那種語言相似。
 
5.4.1.4  實驗結果 
使用 5.3 節的 LPM 架構語言辨認器，將測試語料解碼為 11 個韻律狀態，並
與訓練語料的韻律狀態三連模型做比較，得到以下實驗結果。 
 
表5.17：使用 LPM 架構語言辨認器之語言辨認結果。 
實驗方法 (4)LPM LM 
平均錯誤率 31.34% 
國語錯誤率 33.39% 
普通話錯誤率 29.18% 
 
    所以兩種語言的韻律狀態分布與跳動的情況應該是有差別的，我們將兩種語
料合在一起建立一個 LPM，看看這兩個語言的狀態序列分布是否不同。 
 225 
表5.18：UBM 有聲韻律狀態參數平均。 
 pitchmean pitchdim2 pitchdim3 pitchdim4 energymean duration 
狀態 1 0.569241 0.125686 0.045377 0.024181 0.655245 -0.13618 
狀態 2 -0.12603 1.915334 -2.27396 0.761525 -0.29827 0.740787 
狀態 3 0.024702 0.091314 0.127304 -0.01611 -0.37256 0.03858 
狀態 4 -2.2612 -0.06551 1.010168 -0.69172 -0.80412 -0.2939 
狀態 5 0.301676 0.295171 -0.0379 -0.033 -0.9417 -2.68624 
狀態 6 -0.41761 -1.4117 -1.85891 -1.03492 -0.15362 0.90534 
狀態 7 -0.68599 2.49925 0.316904 -3.22166 -0.41784 0.445509 
狀態 8 -0.7888 -1.79217 0.172617 1.914436 -0.31127 0.796419 
 
    
狀態 1 是句子的起頭，狀態 4 是句子的結尾，狀態 3 是在句中的韻律狀態最
多的部分。
 
    
如果大陸人講話真的比較急促，那普通話無聲部分應該也會比國語要短，所
以普通話的狀態跳動容易跳入較短的無聲狀態，我們可以觀察下面的圖，看看是
否有這個現象。
 
 
 
 227 
 
在一般認知中，大陸人說話是比較急促的，與台灣的說話速度差異甚大。圖
5.18 與 5.19 放了台灣國語及大陸普通話有聲到無聲狀態跳動情形，狀態 1 至 8
為有聲，狀態 9 至 11 為無聲，我們可以看出大陸普通話有聲跳至無聲明顯跳至
狀態 10 居多，而台灣國語則跳至狀態 11 居多。表 4.6 列出了狀態 9 至 11 正規
化後的平均音節長度，由表可知狀態 10 比狀態 11 的平均音節長度要短很多，
所以可以說明大陸人說話的速度確實是比較快的。 
    
接下來就是觀察狀態序列的跳動情況是否有明顯的差異，同樣是先觀察
UBM 的 bigram 跳動情形。 
 
 
圖5.25：使用 UBM LPM 之 MAT 訓練語料狀態序列 bigram 表。 
 229 
 
 
圖5.28：TRSC LPM 國語與普通話訓練語料有聲狀態序列分布。 
 
可以看出韻律狀態分布有些微的不同。
 
 
 
圖5.29：使用國語 LPM 之 MAT 訓練語料狀態序列 bigram 表。 
 231 
 
 
圖5.32：使用普通話 LPM 之 TRSC 訓練語料狀態序列 bigram 表。 
 
同樣的兩種語料庫的狀態跳動會有微小的不同。
 
因為跳動的差異性並沒有拉得非常開，所以辨認率只能做到 30%左右的錯誤
率。
 
5.4.2  聲調模型語言辨認基本系統 
在本節我們將整個聲調模型語言辨認基本系統，分成幾個區塊來描述，包括： 
1. 基本參數求取：使用 Snack 軟體求取音檔基頻值與能量大小。 
2. 求取音檔切割位置：使用自動聲音辨識系統(ASR)求取音檔以
syllable 為單位的 segment 長度。 
3. 參數求取：求取如 3.2.2 所述之參數。 
4. 訓練聲調模型：使用 LNKnet 以類神經網路(Multi Layer Perceptron, 
MLP)之方式來建立聲調模型，使用的 hidden layer 為 100， cycles
為 30。 
 233 
表5.21：單一聲調錯誤率表。 
聲調
 
平均錯誤率
 
國語錯誤率
 
普通話錯誤率
 
一聲
 33.23% 37.15% 29.32% 
二聲
 28.16% 35.25% 21.07% 
三聲
 41.3% 48.00% 34.60% 
四聲
 24.20% 24.99% 23.42% 
輕聲
 85.41% 73.77% 97.06% 
平均錯誤率
 42.26% 43.83% 41.09% 
 
    
相同的聲調應該容易混淆，所以同聲調的錯誤率應該會比較高。
 
 
圖5.33：國語的 tone mean pattern。 
 235 
所以不管聲調辨的對不對，我們只看辨認的聲調與那一種語言比較接近，以
下是使用聲調來辨認語言的結果：
 
 
表5.22：聲調模型語言辨認系統錯誤率。 
實驗方法
 (5)聲調模型 
平均錯誤率
 30.54% 
國語錯誤率
 27.37% 
普通話錯誤率
 33.44% 
 
可以看得出來使用聲調來區分國語與普通話仍然是有效果的，但是效果與之
前的系統結果差不多。接著我們想看一看那一種聲調的鑒別力大。
 
我們將聲調辨認語言系統辨認國語及普通話的結果列於表 5.23，可以看出使
用一聲模型的時候會比較有可能把兩種語言分開，而二聲、三聲、四聲台灣國語
的錯誤率根本是不能參考的，無法讓我們拿來辨認台灣國語及大陸普通話用。 
 
 
 
 
 237 
5.4.3  系統整合 
依據圖 5.1 所示，分辨語言有頻譜的線索，有韻律的線索與音律的線索，所
以在這裡我們把所有的線索都整合起來，成為一個完整的系統，以提升辨識的效
能。我們採用 LNKNET 工具，把第二章與第三章的語言辨認系統加以整合，其
中對 LNKNET 中的設定為： 
1. 以類神經網路(Multi Layer Perceptron, MLP)之方式整合系統分數。 
2. hidden layer 為 40， cycles 為 30。 
3. 使用 development set 來訓練 MLP。 
 
表5.25：系統整合錯誤率表。 
實驗方法
 Baseline (4)LPML
M 
(5)tone Base+ (4) Base+ (5) Base+(4)
+(5) 
平均錯誤率
 20.68% 31.34% 30.54% 18.55% 20.36% 16.18% 
國語錯誤率
 27.12% 33.39% 27.37% 22.05% 24.53% 19.06% 
普通話錯誤
率
 
14.30% 29.18% 33.44% 15.03% 16.16% 13.33% 
 
我們的 Baseline 系統能夠將錯誤率下降至 20.68%，可是因為國語發音的變
化比較不固定，無法將錯誤率下降到與普通話錯誤率相近的結果，相較之下，我
們所使用的 LPM 後接模型腔調辨認系統與聲調語言辨認系統在兩種語言的錯誤
率是相差不多的，所以在系統整合時，能彌補 Baseline 系統，將兩個語言辨認的
錯誤率拉到比較接近的水平，進而降低錯誤率並在最後的系統整合
 239 
 
5.5  結論與未來展望 
5.5.1  結論 
    
藉著語言辨認的各種有用的資訊，我們讓國語與普通話辨認的效果越來越
好，我們使用 MAT 與 TRSC 語料庫進行實驗，本章實驗結果顯示傳統的音素辨
認器後接語言模型(PPRLM)、通用音素辨認器後接語言模型(UPRLM)與位移差分
化倒頻譜特徵高斯混合模型(SDC-GMM)的錯誤率介於 23.79%至 29.11%之間，將
三個系統整合，錯誤率能下降到 20.68%。如果再把 LPM 架構、tone 辨認器與前
面的系統都整合，錯誤率則可下降到 16.18%，不過這套韻律模型是建立在有聲
調的語言的的情況下，如果沒有聲調標記，做法會不太一樣。
 
5.5.2  未來展望 
希望能夠有不需要聲調標記的潛藏性韻率模型，可以在各種語言使用韻律狀
態多連模型來描述語言的特性，好用來輔助進行語言的辨認，這樣可以讓語言辨
認的系統更完整。
 
另外，訓練音素辨認器需要有大量的音素標記，這是很不方便的事，如果可
以藉由音素屬性來辨認音素，就可以節省大量的人力物力，這也會是一個有趣的
研究方向。
 
 
 241 
第六章  結論與成果發表 
6.1  結論 
計畫第一年先進行較單純之 LPM 研究，一次只考慮一個影響因素，包括獨
立的 LPM-assisted pitch contour extraction，語者與語言分群／辨認，第
二年再進行一次考慮數個因素，較複雜之 joint LPM （J-LPM)相關研究，進行
Joint-LPM-assisted 語者與語言分群／辨認，此外第二，三年逐步將一，二年
學到之 LPM-assisted 語者與語言分群／辨認經驗應用到 LPM-assisted events 
detection。 
本計畫中，除計畫中描述的作法外，我們亦以同樣的道理延計畫中描述之
作法，完成之研究工作項目包含：（1）latent prosody model，（2）applications 
of latent prosody model。  
1 Latent prosody model (LPM) 
 LPM that separately considers speaker, language, phonetics 
and prosody status 
 J-LPM that jointly considers speaker, language, phonetics and 
prosody status 
 Adaptation of the LPM and J-LPM 
2 Applications of latent prosody model 
2.1 LPM-assisted pitch contour extraction and tone recognition 
 LPM-assisted pitch contour extraction for Mandarin and 
tone recognition 
 LPM-assisted pitch contour extraction for Western 
languages 
 J-LPM-assisted pitch contour extraction and tone 
 243 
B. Conference Papers: 
1. Cheng-Chang Lee and Jing-Teng Zeng and Chi-Hui Hsu and Yuan-Fu Liao, 
"Sub-Frame Interleaving and Reference Model Weighting for Robust 
Distributed Speech Recognition", CACS’ 2006.    
2. Yuan-Fu Liao and Zhi-Rem Zeng and Zi-He Chen and Yau-Tarng Juang, 
"Exploiting Glottal and Prosodic Information for Rubust Speaker Verification", 
Speech Prosody' 2006.    
3. Jyh-Her Yang and Yuan-Fu Liao and Yih-Ru Wang and Sin-Horng Chen, "A 
New Approach of Using Temporal Information in Mandarin Speech 
Recognition", Speech Prosody' 2006.    
4. Zi-He Chen and Zhi-Ren Zeng and Yuan-Fu Liao and Yan-Tarng Juang, 
"PROBABILISTIC LATENT PROSODY ANALYSIS FOR ROBUST 
SPEAKER VERIFICATION", ICASSP' 2006.    
5. Wen-Chieh Chang and Ding-Yun Chenand Zi-He Chen and Zhi-Ren Zeng and 
Yuan-Fu Liao and Yau-Tarng Juang, "Incorporating Prosodic with Acoustic 
information for ISCSLP’2006 Speaker Recognition Evaluation- Robust 
Cross-Channel Speaker Verification", ISCSLP' 2006.    
6. Jing-Teng Zeng and Cheng-Chang Lee and Jeng-Shien Lin and Yuan-Fu Liao 
and Sen-Chia Chang, "Monte Carlo Noisy HMM Estimation and Segmental 
Differential Features on the Aurora2 Clean Training Evaluation", ISCSLP' 
2006.    
7. 張文杰  陳鼎允  陳子和  曾志仁  廖元甫  莊堯棠, "結合韻律與聲學訊
息 之 強 健 性 漢 語 語 者 驗 證 系 統 ", Chinese computational linguistics 
(ROCLING)'2006.  
8. Yuan-Fu Liao, ‘‘Sub-Frame Interleaving and Reference Model Weighting for 
Robust Distributed Speech Recognition’’, CACS Automatic Control 
Conference 2006. 
9. X. Wang, K. Hirose, J. Zhang, N. Minematsu, C. Chiang, Y. Wang, Y. Liao, 
“Tone recognition of continuous Mandarin speech based on tone nucleus 
model and neural network,”, Technical Report of IEICE, SP2006, (2006-12)  
10. Yuan-Fu Liao, ‘‘Incorporating Prosodic with Acoustic information for 
ISCSLP’ 2006 Speaker Recognition Evaluation-Robust Cross-Channel 
Speaker Verification’’, ISCSLP 2006. 
11. Yuan-Fu Liao, ‘‘Monte Carlo Noisy HMM Estimation and Segmental 
Differential Features on the Aurora2 Clean Training Evaluation’’, ISCSLP 
2006. 
 245 
 
6.3  未來展望 
人與人的口語對話帶有許多有用的資訊，如何擷取和保留我們想要的有用資
訊，需要深入研究與探討，除了目前現有文獻所發現的有用資訊，是否能再找出
新的有用資訊，或是將各個有用資訊結合，藉此增加系統辨識效能，皆是我們未
來致力研究之方向。 
 
 
Plenary分為四場，分別為 12月 17日早上 10:00~12:00由 J. Watson Research 
Center的Yuqing Gao博士和東京大學(University of Tokyo)的 Shigeki Sagayama教
授，12月 18日以及 19日早上 8:30~9:30 分別由 Google411的 Vincent Vanhoucke
和Microsoft Research Asia的 Qiang Huo由這幾位教授來為每天的會議做完美的
開端。 
Special Session分兩階段分別為 SPE1 Frontiers of HMM-based TTS，SPE2 
Computer-Assisted Language Learning。 
Oral Session 分為六個階段分別為 O1 強健性語音辨認技術(Robust Speech 
Recognition)，O2 語者與語言辨認技術(Speaker and Language Recognition)，O3 自
然語言系統(Spoken Language Systems)，O4 語音分析及語言學(Speech Analysis 
and Phonetics)，O5 語音合成技術 (Speech Synthesis)，O6 語音辨認 (Speech 
Recognition)。 
 Poster Session分為五階段，主要由展示論文海報為主，主題分別為 P1 語音
調適(Speech Applications)，P2 語音辨認(Speech Recognition)，P3 語者辨認
(Speaker Recognition)，P4 自然語言處理(Spoken Language processing)，P5 語音
處理(Speech Processing) 
 
 
圖一為主要會議廳，主持人爲演講者準備演說前的介紹 
 
 圖四為我們在演講時的報告 
 
 我們也挑了兩篇對我們的研究有相當的幫助來聽，分別是 
 
Discriminative Feedback Adaptation for GMM-UBM Speaker Verification 
Yi-Hsiang Chao, Wei-Ho Tsai, and Hsin-Min Wang 
 
摘要 
GMM-UBM 系統是目前在文本獨立的語者驗證系統中最好的方法。這個方
法的好處是目標語者模型和冒名頂替者模型有處理「未看過」聲音樣本的概念化
能力。然而，因為 GMM-UBM系統使用一個共同的反模型，及 UBM，所以對
所有的目標語者而言，在拒絕相同於目標語者聲音的冒名頂替者的聲音時，系統
能力會傾向於微弱。為了要克服這個侷限，我們提出「差別反饋適應」
（Discriminative Feedback Adaptation; DFA）架構，當我們要維持 GMM-UBM系
統的概念化能力時，能強化目標語者模型和反模型之間的鑑別性。我們不是藉由
應用傳統鑑別式訓練的技巧隨意估測，而是基於最小驗證平方誤差準則來調適
UBM 成為與目標語者相依的反模型這個方法來達到目標。以 NIST2001-SRE為
語料庫所得到的實驗結果顯示，DFA改善了傳統 GMM-UBM方法的效能。 
 
