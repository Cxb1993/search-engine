 1
行政院國家科學委員會專題研究計畫成果報告 
植基於場景分析之醫學資訊檢索與視覺化呈現 
Context-based Retrieval and Visualization of Medical Information 
計畫編號：NSC 96-2221-E-320-001-MY3 (共三年，完整報告) 
執行期限：96 年 8 月 1 日至 99 年 7 月 31 日 
主持人：劉瑞瓏  慈濟大學醫學資訊學系 
 
1. 中英文摘要 
網際網路上醫學資訊之檢索已成為醫療
專業人員與一般大眾探索與學習醫學知識的
主要途徑。為了真正提高醫學資訊之可用度
與價值，醫學資訊之自動化檢索與視覺化呈
現相當關鍵。其運作方式應是由一段使用者
提供之興趣描述片段為開始，進而探索整個
與該興趣描述片段相關之醫學資訊空間。該
興趣描述片段常是一個醫療處方、一個醫學
研究議題、一項醫學研究發現、或一段疾病
症狀的描述。此興趣描述片段又通常是以自
然語言來表達，對特定議題有明確之針對
性，且相較於一般文件而言相當簡短。據此，
本計劃研究如何在給定一個興趣描述片段
下，針對該片段進行分析，建立查詢語句、
檢索相關文件、排序相關文件、檢索相關文
件之片段，進而分析檢索出之片段並以視覺
化方式呈現。整個流程是以文件片段之字詞
場景分析為主要特色，提高文件片段之處理
與辨識之效能。本計畫第一年研發了一個可
以增進不同文件分類器效能之字詞場景辨識
技術。此場景辨識技術成功地藉由改進簡短
興趣描述之語意辨識，而提昇分類器效能。
在此基礎上，於第二年之計畫中，我們進一
步將此字詞場景及分類的觀念進行修訂，進
而完成一個可自動產生查詢語句的技術，可
在不用任何詞庫的情況下，線上快速自動產
生更佳的查詢語句，檢索出相關文件。於第
三年之計畫中，我們更進一步將此字詞場景
技術修訂應用於文件之相關度排序及片段抽
取上，並設計一個系統以分析並呈現檢索結
果。本計畫之研究成果對醫學文件檢索深具
意義，提升醫療資訊之能見度，有助於醫療
專業人員及一般民眾分享與利用醫療資訊。 
 
關鍵詞：醫學資訊檢索、字詞場景、場景辨
識、查詢語句自動產生、文件排序、資訊片
段擷取、資訊視覺化呈現 
 
Abstract 
Medical information retrieval on the 
World-Wide Web has been a main way for 
healthcare information consumers to explore 
and learn medical knowledge. To promote the 
utility of medical information, automatic 
retrieval and visualization of medical 
information are essential. They are often 
triggered by a passage of interest description 
(e.g. a prescription, research issue, research 
finding, and disease symptom) from the user. 
The passage is often short and in natural 
language form. In this project, we explore how 
and to what extent term proximity context (TPC) 
of passages may be recognized to support the 
retrieval and visualization of relevant passages. 
In the first year of the project, we developed a 
context recognition technique to improve text 
classification. Based on the context recognition 
technique and the text classification technique, 
in the second year of the project we developed 
a technique that may efficiently generate 
keyword-based queries from natural language 
passages of interest descriptions so that more 
relevant medical texts may be retrieved, 
without employing any thesauri. In the third 
year of the project, we further refined the 
context recognition technique to rank medical 
texts and extract passages from the texts. A 
prototype system was also designed to visualize 
the retrieval results. The contributions of the 
project are essential in promoting the sharing of 
healthcare information for both healthcare 
professionals and consumers.  
 
Keyword: Medical information retrieval, term 
proximity context, context recognition, query 
generation, text ranking, passage retrieval, 
 3
training documents to identify the correlation 
between each term and category. In testing, 
CTFA assesses frequencies of terms in the 
input document. Based on the assessment, the 
underlying classifier may make more proper TC 
decisions for the document. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3.1.1 Task in the training phase: 
term-category correlation identification 
CTFA employs the χ2 (chi-square) 
method to identify term-category correlation. 
For a term t and a category c, χ2(t,c) = [N × 
(A×D - B×C)2] / [(A+B) × (A+C) × (B+D) × 
(C+D)], where N is the total number of 
classifier-building documents, A is the number 
of classifier-building documents that are in c 
and contain t, B is the number of 
classifier-building documents that are not in c 
but contain t, C is the number of 
classifier-building documents that are in c but 
do not contain t, and D is the number of 
classifier-building documents that are not in c 
and do not contain t. Two types of correlation 
are identified: positively correlated and 
negatively correlated types. A term t is 
positively correlated to a category c, if A×D > 
B×C; otherwise it is negatively correlated to c. 
 
3.1.2 Task in the testing phase: term 
frequency assessment 
Based on the identified term-category 
correlation types, upon receiving a test 
document, CTFA is invoked to assess the term 
frequency (TF) of each term, which is then 
passed to the underlying classifier to make TC 
decisions. Table 1 defines the algorithm for the 
TF assessment. For a category c, CTFA returns 
a TF vector for all distinct non-stop words in 
the test document (ref. Return). 
For each category c, CTFA gives higher 
TF values to two kinds of terms in the test 
document d: (1) for positively-correlated terms, 
those that have many neighboring 
positively-correlated terms (ref. Step 4), and (2) 
for negatively-correlated terms, those that have 
many neighboring terms that are 
positively-correlated to some category other 
than c (ref. Step 5). The former helps c to 
accept d, while the latter helps c to reject d. 
Therefore, a term t would get a lower TF value 
if it just happens to appear in d with its 
neighboring terms not of the same correlation 
type. In that case, the occurrence of t should 
simply be a noise for text classification. 
More specifically, for a term t that is 
positively correlated to a category c, CTFA 
counts the number of left neighboring terms 
(ref. Step 4.1.1.1.2) and right neighboring terms 
(ref. Step 4.1.1.1.3) that are positively 
correlated to c. The higher the number is, the 
larger the TF value of t with respect to c will be, 
and if the number is 0, the TF value is 1.0 (ref. 
Step 4.1.1.4). For a term that occurs at multiple 
positions in the test document, its final TF 
value is the sum of its TF values at the 
positions (ref. Steps 4.1 and 4.1.1.4). 
On the other hand, for a term t that is 
negatively correlated to a category c, its TF 
value with respect to c is simply the maximum 
TF value among its TF values with respect to 
those categories to which t are positively 
correlated (ref. Step 5). The maximum TF 
value actually indicates the extent to which t 
suggests other categories to accept the test 
Table 1 CTFA: Context-based term 
frequency assessment 
Procedure CTFA(d), where d is a test document. 
Return: TFn×C is a matrix, where n is the number of distinct terms 
in d, C is the number of categories, and tfi,c is the assessed term 
frequency of term ti with respect to category c. 
Begin 
(1) S ← Sequence of terms in d with stop words removed; 
(2) n ← Number of distinct terms in S; 
(3) TFn×C ← 0; 
// Assess TF for each distinct positively-correlated term  
(4) For i = 1 to n, do            
(4.1) For each term ti,j that is the ith distinct term occurring 
at the jth position in S, do 
(4.1.1) For each category c, 
(4.1.1.1) If ti,j is positively correlated to c,  
(4.1.1.1.1) PositiveTypeNum ← 0; 
(4.1.1.1.2) For k = j -1 to 1, do 
(4.1.1.1.2.1) If the kth term in S is positively 
correlated to c, PositiveTypeNum ← 
PositiveTypeNum + 1; 
(4.1.1.1.2.2) Else exit for; 
(4.1.1.1.3) For k = j +1 to |S|, do 
(4.1.1.1.3.1) If the kth term in S is positively 
correlated to c, PositiveTypeNum ← 
PositiveTypeNum + 1; 
(4.1.1.1.3.2) Else exit for; 
(4.1.1.1.4) tfi,c ← tfi,c + Log2(PositiveTypeNum 
+ 2); 
// Assess TF for each distinct negatively-correlated term 
(5) For i = 1 to n, do  
(5.1) For each category c, do                
(5.1.1) If tfi,c = 0, tfi,c ← Max{tfi,c, for all categories}; 
(6) Return TFn×C; 
End. 
 
 5
both P and R are zero, F1 is zero as well. 
 
3.1.3.3 Underlying classifiers 
Each category c is associated with a 
classifier, which is based on the Rocchio 
method (RO) and the Support Vector Machine 
(SVM). Upon receiving a document d, the 
classifier makes a binary decision for d: 
accepting d or rejecting d. 
RO was commonly employed in TC, TF,  
and retrieval as well. Some studies even 
showed that its performances were more 
promising in several ways. RO constructed a 
vector for each category, and the similarity 
between a document d and a category c was 
estimated using the cosine similarity between 
the vector of d and the vector of c. More 
specifically, the vector for a category c was 
constructed by considering both relevant 
documents and non-relevant documents of c: 
η1*∑Doc∈PDoc/|P| − η2*∑Doc∈NDoc/|N|, where P 
was the set of vectors for relevant documents 
(i.e. the documents in c), while N was the set of 
vectors for non-relevant documents (i.e. the 
documents not in c). In the experiment, η1=16 
and η2=4, since previous studies showed that 
such a setting was promising. 
SVM is a popular and promising 
classification methodology. Previous studies 
often found that SVM outperforms many other 
classification methods. We employ SVMLight 
that is publicly available3 and was tested in 
many previous studies. We report results from 
two versions of SVM: (1) SVM1 that employs 
the default setting of SVMLight, and (2) SVM2 
that sets the “cost-factor” parameter to 10 (i.e. 
relative weights of errors on positive examples 
to errors on negative examples). The setting 
helps SVM2 to have the performances similar 
to the best performances reported in previous 
studies. 
Both RO and SVM required a fixed 
(predefined) feature set, which was built using 
the documents for classifier building. The 
features were selected according to their 
weights, which were estimated by the χ2 
(chi-square) weighting technique. The 
____________________________________ 
3
 
http://www.cs.cornell.edu/People/tj/svm%5Flight/old/svm_light
_v5.00.html  
technique has been shown to be more 
promising than others. As noted above, there is 
no perfect way to determine the size of the 
feature set. Setting a proper feature set size was 
often an experimental issue. Therefore, we try 5 
feature set sizes (FS) for both Reuters-21578 
and 20 Newsgroups. 
CTFA is applied to enhancing RO, 
SVM1 and SVM2, and the resulting systems 
are named RO+CTFA, SVM1+CTFA, and 
SVM2+CTFA, respectively. By comparing the 
performances of RO, SVM1, and SVM2 with 
the systems enhanced by CTFA, we may 
measure the contributions of CTFA. 
 
3.1.3.4 Main results 
Results show that CTFA may successfully 
improve RO and SVM without needing to 
make modifications to them. No expensive 
computation and huge memory are required 
either. The contribution is also published in 
[1][2]. 
For RO, on Reuters-21578, RO and 
RO+CTFA have very similar performances. 
However, CTFA helps RO to achieve better 
micro-averaged F1 under all feature set sizes, 
especially when the feature set size becomes 
larger. Moreover, on 20 Newsgroups, the 
contribution is more obvious. CTFA helps RO 
to achieve better performances in both 
micro-averaged F1 and macro-averaged F1. 
As to SVM, on 20 Newsgroups, SVM1 
(using default parameter setting of SVMLight) 
performs much worse than SVM2 (setting the 
cost-factor to 10), while after introducing 
CTFA, SVM1 may achieve similar 
performances as SVM2. Moreover, on 
Reuters-21578, CTFA greatly improves 
macro-averaged F1 of SVM1 when SVM1 
achieves its best micro-averaged F1. Similarly, 
when SVM2 achieves its best micro-averaged 
F1, CTFA successfully improves its 
macro-averaged F1 as well. 
 
3.2 Results of the 2nd year: online query 
generation 
Based on the results, in the second year of 
the project we focused on the 1st to the 3rd tasks 
of the project noted in Section 2 (i.e., 
interpreting the given passage, generating 
 7
first identifies the main topics (categories) of 
the information need by conducting 
classification for the information need. The 
classification is rough but efficient in order to 
achieve online query generation. For each 
category c, it employs locality contexts of 
words to estimate the score of c − the score 
depends on the number of consecutive words in 
the information need description that are 
positively correlated to c. The word locality 
contexts help to identify main topics more 
properly and efficiently, since semantics of a 
word often heavily depends on its neighbors. 
Based on the classification, the strings 
corresponding to the main categories (main 
topics) of the information need may be 
identified. They are simply the consecutive 
words that are positively correlated to the 
categories with the largest score. The strings 
tend to convey main topics of the information 
need, and hence may be referred as main 
strings from which query terms should be 
extracted. 
After finding the main strings, MQG 
identifies query terms by extracting from the 
main strings all single words and word pairs 
that are selected in mining, with those words 
not seen in mining considered as well. The 
single words and word pairs are then sorted in 
decreasing order of their maximum correlation 
strengths. The order reflects their priority in the 
generated query − those having stronger 
strengths get higher priority, since they tend to 
have a higher capability to indicate the 
semantic units of the strings. 
Note that, since the output query is a 
disjunction of terms, retrieval by one term may 
cover retrieval by another term, leading to 
redundant terms. For example, suppose the 
query is a sequence of terms <'a b', 'b', 'b c'>. In 
this case, MQG prefers 'a b' to 'b', and hence 'b' 
becomes redundant, since retrieval by 'b' covers 
retrieval by 'a b' (i.e., those texts containing 'b' 
include those containing both 'a' and 'b'). 
Similarly, MQG prefers 'b' to 'b c', and hence 'b 
c' becomes redundant as well. MQG removes 
redundant terms so that the query may become 
more compact and precise. 
 
3.2.3 Empirical evaluation  
MQG is implemented and empirically 
evaluated in a real-world text database of 
medical literature and information needs. The 
results show that, when compared with several 
baseline query generation techniques, MQG 
generates more effective keyword-based 
queries to retrieve relevant texts, and since the 
queries generated by MQG contain much fewer 
terms, the loading incurred to the retrieval 
systems is reduced.  
 
3.2.3.1 Experimental Data 
Experimental data is from OHSUMED, 
which is a popular database of abstracts of 
medical references. There are 348,566 
references in OHSUMED, with each reference 
indexed by MeSH (Medical Subject Headings) 
terms that indicate its main topic4. An indexing 
term corresponds to a category label. To 
conduct 4-fold cross validation, we evenly split 
the references into four parts. Experiments are 
conducted four times so that each part is used 
for testing only once (and the other three parts 
are for training). Average performance in the 
four experiments is then reported. 
OHSUMED also includes 106 
information need descriptions issued by 
healthcare professionals. Each description 
contains a description for an information 
request. OHSUMED also provides mappings 
between each information need and its 
definitively relevant references. The mappings 
serve as the basis to evaluate query generation 
techniques−when references are retrieved for an 
information need, the relevance mappings are 
consulted to determine which references are 
relevant to the information need. 
 
3.2.3.2 Evaluation criteria 
In practice, references retrieved by a query 
from a query generator are often ranked (by a 
text ranker) so that a certain number of 
references may be selected for further 
processing (e.g., clustering, relevancy 
estimation, and display). The query generator 
should thus generate proper queries to retrieve 
more relevant references that may be ranked 
high by the ranker. 
____________________________________ 
4
 http://www.nlm.nih.gov/bsd/disted/mesh/major.html. 
 9
The results show that MQG successfully 
retrieves more relevant references with much 
higher precision. More specific findings are 
summarized as follows, and the results are 
published in [3]: 
(1) All the baselines have much poorer 
precision than MQG, indicating that they may 
incur much heavier loading to subsequent text 
ranking;  
(2) MQG achieves higher recall than all the 
baselines, indicating that it successfully 
retrieves more relevant references that may be 
ranked high by both rankers;  
(3) Employing a complete dictionary leads to 
poorer recall, even when the matching is 
relaxed;  
(4) Those baselines that consider 1 grams have 
quite poor precision, incurring heavy loading 
to subsequent text ranking and hence making 
them less able to support online retrieval of 
information;  
(5) Those baselines that expand their queries 
with equivalence terms may improve their 
recall, however, they still have much poorer 
precision and recall than MQG, which does 
not require any thesauri of medical terms and 
equivalence terms at all. 
 
3.3 Results of the 3rd year: text ranking, 
passage extraction, and information 
visualization 
Based on the results in the former two 
years, in the third year of the project we 
focused on the 4th to the 6th tasks noted in 
Section 2. We employed and refined the TPC 
recognition techniques developed in the 
previous two years to develop techniques to 
improve text rankers and passage identifiers. A 
prototype system was also designed to visualize 
the results for the user to explore.  
 
3.3.1 Ranker enhancement by term 
proximity context 
We developed a technique PRE 
(Proximity-based Ranker Enhancer) to enhance 
text rankers by term proximity information. 
PRE assesses the term frequency (TF) of each 
term in the text by integrating three types of 
term proximity to measure the contextual 
completeness of query terms appearing in a 
nearby area in the text being ranked. Therefore, 
PRE may serve as a pre-processor for (or 
supplement to) those rankers that consider TF 
in ranking, without needing to change the 
algorithms and development processes of the 
rankers. Empirical evaluation shows that PRE 
significantly improves various kinds of text 
rankers, and is both better and more stable than 
several state-of-the-art techniques that 
enhanced rankers by term proximity 
information. The comprehensive applicability 
of term proximity information to ranking 
methodologies may thus be expected. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
More specifically, Table 2 presents how 
PRE assesses TF of each term by considering 
term proximity context. Given a query q and a 
text d, PRE assesses the TF of each term t in d 
by considering three types of term proximity: 
overall proximity, individual proximity, and 
collective proximity (ref. formula B2 in Table 
2), which have different degrees of restrictions 
on how terms collocate with each other. Overall 
proximity is the least restrictive. It considers 
the amount of query terms appearing in d, 
Table 2 PRE: A proximity-based 
ranker enhancer 
Procedure: ComputeRefinedTF(d,q) 
Given: A text d and a query q, with stopwords removed; 
Return: Refined term frequency RTF(t,d,q) for each term t 
in d; 
Method:  
(A) RTF(t,d,q) = TF(t,d)+TFincrement(t,d,q) 
(B) TFincrement(t,d,q) = 
(B1) 0, if t is not in q or |q|=1; 
(B2) QtermTF(d,q)IndiP(t,d,q)×CollP(t,d,q), otherwise 
(C) |q| is the number of distinct terms in q 
(D) QtermTF(d,q) =Σm∈MTF(m,d) 
(E) M is the set of distinct terms that are in both d and q 
(F) IndiP(t,d,q) =Σm∈M－
{t}SigmoidWeight(Mindist(t,m))/MaxIndiP 
(G) Mindist(x,y) = shortest distance between x and y in d 
(H) SigmoidWeight(dt) = 1/(1+e-((|q|-1)-dt)) 
(I) MaxIndiP = (|q|-1)×SigmoidWeight(1) 
(J) CollP(t,d,q) = Maxk∈K{Σm∈M－
{t}SigmoidWeight(dist(t,k,m))}/MaxCollP, where K is 
the set positions at which t appears in d 
(K) dist(t,k,m) = Distance between t (at position k) and m 
(L) MaxCollP = 
(L1) 2×[ΣdistSigmoidWeight(dist) for dist≤(|q|-1)/2], if 
(|q|-1|) is even; 
(L2) 2×[ΣdistSigmoidWeight(dist) for 
1≤dist≤(|q|-1)/2] + SigmoidWeight((|q|-1)/2+1), 
otherwise 
 11
(Proximity-based Term Occurrence Weighting) 
that assesses the occurrence weight of a term t 
at a position k by considering the term 
proximity context of t at k, based on the 
observation that semantics of a term t in a 
document d often depends on contextual 
(neighboring) terms of t in d.  
Figure 2 illustrates an overview of PTOW. 
We also empirically evaluated PTOW on 
Genomics TREC 2006. Two passage identifiers 
IBM and SiteQ were employed as the 
underlying passage identifiers to be enhanced. 
They were found to be the state-of-the-art 
passage identifiers in previous studies. Our 
results showed that, by encoding the locality 
context into the assessment of term occurrence 
weights, both IBM and SiteQ could become 
significantly more capable of identifying 
relevant passages. 
 
 
Figure 3 Visualization of the medical 
information retrieved 
 
 
3.3.3 Visualization of information 
We implemented a meta-search engine 
based on the query generation technique 
developed in the project, and designed a 
visualization interface for the users to explore 
the space of medical information retrieved.  
As shown in Figure 3, the retrieved 
evidences (medical texts) are first analyzed in 
two dimensions: time of publication (ref. 
Plot-1-1) and author of publication (ref. 
Plot-1-2) so that the reader may capture trends 
of when the evidences were published and who 
published them. When the reader clicks on a 
specific year, the system retrieves the texts in 
this year, and analyzes the citations before and 
after the year (ref. Plot-2-1). The citation 
analysis helps the reader to identify the 
most-citing and most-cited evidences, as well 
as the citation relationships between evidences. 
On the other hand, when the reader clicks on an 
author in Plot-1-2, the system retrieves all 
publications of the author so that the reader 
may capture the related expertise of the author 
more comprehensively. Finally, when the 
reader clicks on a publication in either Plot-3-2 
or Plot-2-1, the publication is shown for the 
reader to get the content of the publication. 
Therefore, the visualization technique is novel 
in relieving the reader from reading lots of texts, 
and providing an interface on which the reader 
may explore more in-depth information (time, 
author, and cross-citation) that is retrieved for a 
specific biomedical information need. 
 
4. Overall Evaluation 
In the three years of project research, we 
explored the comprehensive applicability of 
term proximity information to the retrieval, 
ranking, extraction, and visualization of 
medical information. Several techniques were 
developed and evaluated, and the results 
showed that existing text classifiers, search 
engines, text rankers, and passage identifiers 
could be enhanced by the techniques without 
calling for any revisions. 
Results in the first year of the project were 
published in [1][2]. We developed a 
context-based technique CTFA to improve 
various kinds of text classifiers. The results 
serve as the basis to recognize semantics of 
passages for medical information retrieval and 
visualization. The basic idea of CTFA is that 
existence of a term t in a document d should 
not be a good evidence to classify d into a 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 96-2221-E-320-001-MY3 
計畫名稱 植基於場景分析之醫學資訊檢索與視覺化呈現 
出國人員
姓名 
服務機關
及職稱 
劉瑞瓏 慈濟大學醫學資訊學系教授 
會議時間
地點 
Hokkaido University, Hokkaido, Japan, 21-23 October 
2009 
會議名稱 
The Fifth Asia Information Retrieval Symposium (AIRS 
2009) 
發表論文
題目 
Improving Text Rankers by Term Locality Contexts 
 
 
 
一、參加會議經過 
第五屆亞洲資訊檢索研討會 (the 5th Asia Information Retrieval 
Symposium, AIRS 2009) 於日本北海道札幌 (Sapporo) 北海道大學 
(Hokkaido University) 舉行，是一個專以資訊檢索為主題之學術研
討會。依據大會提供之資料顯示，本年度共有 82 篇投稿，共接受 
18 篇一般論文 (regular paper)及 20 篇海報論文 (poster paper)，錄取
率約為 46%。若以論文之投稿與錄取之比例來說，以中國大陸最
高，其次為日本，台灣則相對較少。論文的審查採「雙匿名」的
方式進行全文審查，且每篇論文至少由三位專家審查，相當嚴
謹，論文集並由 Springer 出版成 LNCS (Lecture Notes in Computer 
Science) 系列叢書。整體而言，AIRS 2009 是一個小而美且專注於
資訊檢索領域的研討會。 
 
在論文發表方面，本年度之論文仍多以幾個傳統方向為主
題，包括 IR theory、web search、multimedia IR、及 IR evaluation 
等。筆者所發表之論文有幸與來自美國、日本、及中國的專家學
Improving Text Rankers by Term Locality Contexts 
Rey-Long Liu and Zong-Xing Lin 
Department of Medical Informatics 
Tzu Chi University 
Hualien, Taiwan, R.O.C. 
rlliutcu@mail.tcu.edu.tw 
Abstract. When ranking texts retrieved for a query, semantics of each term t in 
the texts is a fundamental basis. The semantics often depends on locality con-
text (neighboring) terms of t in the texts. In this paper, we present a technique 
CTFA4TR that improves text rankers by encoding the term locality contexts to 
the assessment of term frequency (TF) of each term in the texts. Results of the 
TF assessment may be directly used to improve various kinds of text rankers, 
without calling for any revisions to algorithms and development processes of 
the rankers. Moreover, CTFA4TR is efficient to conduct the TF assessment on-
line, and neither training process nor training data is required. Empirical eval-
uation shows that CTFA4TR significantly improves various kinds of text rank-
ers. The contributions are of practical significance, since many text rankers 
were developed, and if they consider TF in ranking, CTFA4TR may be used to 
enhance their performance, without incurring any cost to them.  
Keywords: Text Ranking, Term Locality Context, Term Frequency Assessment 
1   Introduction 
Ranking of those texts that are retrieved by natural language queries is essential. It 
helps users to easily access those texts that are relevant to the queries, and hence sig-
nificantly reduces the gap between users and the huge information space. Main diffi-
culties of text ranking lie on identifying semantics of each term, which is often diverse 
and depends on different contexts of discussion. 
In this paper, we explore how contextual information may be encoded to improve 
existing ranking techniques so that more information relevant to natural language 
queries may be ranked higher for users to access. More specifically, we focus on con-
text-based assessment of the term frequency (TF) of each term in a document1. Since 
TF is a fundamental component of many ranking techniques, the refined TF assess-
ment may be used to improve the ranking techniques.  
Therefore, we develop a context-based TF assessment technique CTFA4TR (Con-
text-based TF Assessment for Text Ranking). To assess TF of a term t, CTFA4TR 
employs locality contexts of t, based on the observation that semantics of a term t in a 
                                                          
1
 TF of a term t in a document d is the times of occurrences of t in d. 
basic idea is that, for a term t appearing in both a query q and a text d, its semantics in 
q is more possible to be similar to its semantics in d if a higher percentage of terms in 
q appear around t (in d). In that case, TF of t in d should be amplified in order to di-
rect the ranking techniques to promoting the score of d with respect to q.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Given a document d and a query q, CTFA4TR assesses TF of each term t in d with 
respect to q. As illustrated in Fig. 2, each occurrence of t in d may have its term con-
texts, including a neighbor context and a window context. As defined in Fig. 3, for 
each occurrence of t in d, CTFA4TR computes a weighted occurrence value (Weigh-
tedOcc) based on the contexts. If a term t in document d is not in query q, its Weigh-
tedOcc is simply 1 (as done by traditional TF assessment); otherwise CTFA4TR in-
crements its WeightedOcc value by a linear combination of its neighbor context score 
and window context score (Equation B in Fig.  3). Refined TF of t is simply the sum 
of the weighted occurrence values for all occurrences of t in d (Equation A in Fig. 3). 
3.1 Neighbor Context Score 
For a query term t occurring at position k in document d, its neighbor context terms 
are those distinct query terms that consecutively appear around position k in d. For the 
example in Fig.  2, two terms (w1 and w2) serve as neighbor context terms for t at 
position t1 if they are distinct terms in query q. Similarly, at position t2, four terms 
(x1~x4) serve as neighbor context terms for t if they are distinct terms in query q. Cer-
tainly, some occurrences of t might have no neighbor terms (e.g., position t3 in Fig.  2). 
For a query term t occurring at position k in document d, CTFA4TR computes its 
neighbor context score by considering percentage of distinct query terms that serve as 
neighbor context terms (including t per se) around position k in d (ref., Equation C in 
Fig.  3). Obviously, the larger the percentage is, the larger the neighbor context score 
at position k should be. In that case, CTFA4TR increases the weighted occurrence 
value of t at position k, which in turn amplifies TF of t in order to direct various kinds 
of ranking techniques to promoting the score of d with respect to q. 
Text Ranker 
Development 
Training 
Testing 
Underlying Ranker CTFA4TR 
Text 
Ranking  
Training 
Data 
Ranked 
Texts TF in d 
User 
Query (q) 
Text (d) 
TF As-
sessment 
Fig.  1 Overview of CTFA4TR  
the window depends on the number of terms in the query−a longer query leads to a 
larger term window. 
Therefore, both neighbor context scores and window context scores are computed 
by considering percentage of distinct query terms serving as contexts. CTFA4TR 
actually conducts query-dependent TF assessment−each term in a document may get 
different TF values with respect to different queries. Moreover, the relative weight of 
the neighbor context (i.e., α in Equation B) is set to 0.4, aiming to give a little bit 
more preference to the window context, since it governs the completeness of seman-
tics of query q appearing in the term window. 
It is interesting to note that, CTFA4TR mainly considers locality of query terms in 
the document being ranked. It is thus quite efficient to conduct TF assessment online2, 
without requiring any training processes and data. Moreover, the TF assessment may 
be directly input to various kinds of ranking techniques, without requiring any revi-
sions to the algorithms and development processes of the ranking techniques. 
4   Empirical Evaluation 
CTFA4TR is applied to several ranking techniques. To measure the contributions of 
CTFA4TR, experiments are designed and summarized in Table 1.  Experimental re-
sults show that it may significantly improve the ranking techniques on ranking texts. 
4.1 Experimental Data 
Experimental data is from OHSUMED [6], which is a popular database of 348,566 
medical references. OHSUMED contains 106 queries that are descriptions of medical 
information requests. It also provides 16,140 query-reference pairs that indicate the 
relevance of a reference to a query: definitively relevant, possibly relevant, and not 
relevant. The relevance information helps to evaluate contributions of CTFA4TR. 
When ranking the references with respect to queries, we consider titles and ab-
stracts of the references (i.e., the .T and .W fields in OHSUMED) and information 
requests of the queries (i.e., the .W field in OHSUMED). A few steps are conducted to 
preprocess the data, including removing stopwords, removing non-alphanumeric cha-
racters, changing all characters into lower case, and removing 's' if it is the last charac-
ter of a term. Moreover, those references that do not have abstracts in OHSUMED are 
not employed in  training and testing if ranking is based on abstracts of the references. 
To conduct 4-fold cross validation, we evenly partition the 16,140 pairs into four 
parts. Each experiment is conducted four times so that each part is used for testing 
only once (and the other three parts are for training). Average performance in the 4-
fold experimentation is then reported. Moreover, to conduct more complete evaluation, 
                                                          
2
 Suppose query terms appear p times in document d, CTFA4TR only checks neighbor contexts 
and window contexts for the p occurrences, and hence is more efficient than those previous 
approaches (e.g., [2][11][14]) that computed the distance between each pair of occurrences. 
Table 2 Individual ranking methods 
Basic ranking methods (features) Whether TF is 
considered 
L1: ∑
∩∈
+
dqq
i
i
dqc )1),((log2
 
Yes 
L2: ∑
∩∈
+
dqq ii Cqc
C )1),((log 2
 
No 
L3: ∑
∩∈
+
dqq
i
i
qidf )1)((log2
 
No 
L4: ∑
∩∈
+
dqq
i
i
d
dqc )1),((log2
 
Yes 
L5: ∑
∩∈
+⋅
dqq
i
i
i
qidf
d
dqc )1)(),((log 2
 
Yes 
L6: ∑
∩∈
+⋅
dqq i
i
i
Cqc
C
d
dqc )1),(
),((log 2
 
Yes 
L7: log2(BM25score), where BM25score = 
)1(),(
)1(),()(
1
1
1
avgdl
d
bbkdqc
kdqcqidf
i
i
n
i
i
⋅+−⋅+
+⋅
⋅∑
=
 
where k1=2, b=0.75, and avgdl is average document length4 
Yes 
Note: qi is a term appearing in both the query and the document; c(qi,d) is the times qi appearing in document d 
(i.e. term frequency, TF); idf(qi) is the inverse document frequency of qi; |d| is the length of d (i.e., number of 
terms in d); C is the set of training documents; c(qi,C) is the times qi appearing in C; |C| is number of terms in C. 
4.3 Evaluation Criteria 
Two evaluation criteria are employed to measure the contributions of CTFA4TR: 
mean average precision (MAP) and normalized discount cumulative gain at x 
(NDCG@x) [7], which were commonly employed in previous studies in text ranking. 
MAP is defined to be 
k
jDoc
j
iP
iP
MAP
k
j ii
∑∑
==
==
1
106
1 )()(,
106
)(
 
where k is number of relevant documents for the ith query, and Doci(j) is the number of 
documents whose ranks are higher than or equal to that of the jth relevant document for 
the ith query. That is, P(i) is actually the average precision (AP) of the ith query, and 
MAP is simply the average of the AP values of the 106 queries. On the other hand, 
NDCG@x is defined to be 
                                                          
4
 When compared with BM25 defined in [12], the score computation excludes the effect of 
term frequency in each query, since all queries in OHSUMED are quite short. Moreover, it 
employs traditional IDF(ti), i.e., (1+N)/(1+ni), instead of (0.5+N-ni)/(0.5+ni), where N is total 
number of training documents and ni is number of training documents containing ti. We 
avoid the possible problem of negative logarithm values, which might occur when N-ni < ni. 
(1)  
validations are somewhat different. It is also interesting to note that, L5 performs 
better than L4, indicating that the IDF component of L5 is helpful. On the other hand, 
performance of L5 is more stable than that of L1, especially in NDCG@7~10 in which 
CTFA4TR contributes more statistically significant improvements. 
 
L1: CrossValidation1
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L1
L1+CTFA4TR
L1: CrossValidation2
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L1
L1+CTFA4TR
 
L4: CrossValidation1
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L4
L4+CTFA4TR
L4: CrossValidation2
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L4
L4+CTFA4TR
 
L5: CrossValidation1
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L5
L5+CTFA4TR
L5: CrossValidation2
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L5
L5+CTFA4TR
 
L6: CrossValidation1
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L6
L6+CTFA4TR
L6: CrossValidation2
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L6
L6+CTFA4TR
 
L7: CrossValidation1
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L7
L7+CTFA4TR
L7: CrossValidation2
0.2000
0.2500
0.3000
0.3500
0.4000
0.4500
0.5000
NDCG@1 NDCG@2 NDCG@3 NDCG@4 NDCG@5 NDCG@6 NDCG@7 NDCG@8 NDCG@9 NDCG@10 MAP
L7
L7+CTFA4TR
 
Fig. 4 Contributions to individual ranking methods 
 
L6 ranks documents by considering both normalized TF (as L4 and L5 do) and a 
component functioning like IDF (focusing on times of occurrences of terms in the 
training corpus). In this case, CTFA4TR improves L6 and provides several statistical-
ly significant improvements to L6, although the performance differences in many 
criteria are not large, which may be attributed to the fact that the component function-
Fig. 6 illustrates contributions of CTFA4TR to RankingSVM that integrates 21 
features derived from all the 7 basic ranking methods in Table 2 (recall Section 4.2). 
We aim at exploring whether CTFA4TR is still helpful when the underlying ranker 
integrates several features that do not consider TF in ranking (i.e., L2 and L3 in Table 
2). Interestingly, the results show that CTFA4TR successfully improves RankingSVM 
in all evaluation criteria under both cross validations as well, and the improvements in 
the two cross validations are somewhat different. It is also interesting to note that 
RankingSVM with 21 features achieves much better performance than all the original 
individual 5 features, however, it performs worse than RankingSVM with 15 features 
in NDCG@1~3, indicating that adding more features is not necessarily helpful. In this 
case, CTFA4TR contributes more statistically significant improvements to Ran-
kingSVM with 21 features, and makes this version of RankingSVM able to achieve 
similar performance as RankingSVM with 15 features. 
5   Conclusion 
Text ranking is essential for the access of relevant information. In this paper, we ex-
plore how various kinds of text rankers may be improved by considering term locality 
contexts in the documents being ranked. We propose a technique CTFA4TR that 
recognizes term locality contexts by considering completeness of query terms appear-
ing in both neighbor contexts and window contexts of each term in the documents 
being ranked. To improve various kinds of text rankers by the term locality contexts, 
the technique identifies term frequency (TF) as the target into which the term locality 
contexts are encoded. Given that TF is one of the most common components consi-
dered by text rankers, CTFA4TR is applicable to many text rankers. Moreover, 
CTFA4TR is efficient online without requiring any training. Empirical evaluation 
justifies the contributions of CTFA4TR, which successfully improves various kinds of 
ranking methods both individually and collectively (integrated with machine learning 
approaches) under different experimental settings. The contributions are of practical 
significance, since many text ranking techniques were developed, and if they consider 
TF in ranking, CTFA4TR may be used to enhance them without incurring any cost to 
them, since neither their algorithms nor training processes need to be changed. 
 
Acknowledgments. This research was supported by the National Science Council of 
the Republic of China under the grant NSC 96-2221-E-320-001-MY3. 
References 
1. C. Alvarez, P. Langlais, J.-Y Nie, “Word Pairs in Language Modeling for Information Re-
trieval,” in Proceedings of RIAO (Recherche d'Information Assistée par Ordina-
teur), University of Avignon (Vaucluse), France (2004), pp. 686–705 
2. Stefan Büttcher, Charles L. A. Clarke, Brad Lushman, "Term Proximity Scoring for Ad-Hoc 
Retrieval on Very Large Text Collections," in Proceedings of the 29th annual international 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
