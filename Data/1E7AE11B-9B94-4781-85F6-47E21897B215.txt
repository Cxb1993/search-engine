 1
我們的系統將克服這個難題並估算出車子
的三度空間位置。再經由座標轉換，從 WFV
攝影機座標轉換到 PTZ 攝影機的座標，此
車子在 PTZ 攝影機的三度空間位置座標將
會經由網路從 WFV 電腦傳送給 PTZ 電腦。
藉由考慮到初始估算此車子位置、此車子過
去的移動軌跡、網路傳輸速度、及 PTZ 攝
影機轉動和放大速度，我們可以預期此車子
將出現的三度空間位置，並引導 PTZ 攝影
機朝向車子預期出現的位置，並給予放大及
特寫。我們的車子偵測系統是一直在偵測車
子的，它並可以做為車子位置的微調，以便
使 PTZ 攝影機能更精確的把車子位置放在
影像正中心位置。我們這套自動監視系統將
可被應用在住宅及停車場的保全、人員門戶
進出的監視及管制(例如機場、倉庫或公司
機關的門禁管制)、軍事單位或戰場上的監
視行為。 
 
關鍵字：監視系統，車子的偵測，攝影機參
數的調整及校正，三度空間位置的估算，座
標轉換，遙控攝影機，資料傳遞經由網路，
即時系統，系統整合。 
 
Abstract 
Although surveillance cameras are 
already prevalent in airports, banks, stores, 
parking lots and streets, the criminal events 
caused from, such as, terrorists, robbers and 
stealers still keep increasing dramatically year 
over year.  No matter where we are, we 
strongly feel our fortunes and lives are 
insecure.  To protect our everyday lives from 
danger, prevent the crime from happening, 
and even find and capture the criminals before 
the crime happens or when it is happening, it 
is immediate needs to have automated 
surveillance systems in commercial, law 
enforcement and military applications. 
Mounting video cameras is cheap, but 
finding available human resources to visualize 
the output is expensive.  Almost all 
surveillance video data currently are 
investigated only after the criminal event 
happened, thus it loses its powerful advantage 
as an active and real-time medium.  Most 
vehicles of criminals at video data are blurry 
or unclear and cannot be recognized.  The 
major reason is that almost none of the 
surveillance cameras are active in order to 
automatically move and zoom toward the 
vehicles of the criminals to have a closer view.  
Although the surveillance is developing 
automated video understanding technology for 
use in future urban and battlefield surveillance 
applications, however, human visual 
monitoring is too costly, too dangerous, or 
otherwise impractical.  So novel image 
understanding technologies developed under 
the surveillance application will enable a 
single human operator to monitor activities 
over a large, complex area using a distributed 
network of video sensors.  Furthermore, most 
existing surveillance cameras are working 
individually.  It is a challenge research for us 
to use one single camera to estimate 
3-dimensional (3D) locations of the vehicle.  
In addition, different computer control 
cameras can communicate each other via 
network. 
Regarding to above existing and 
expected practical problems, we are going to 
develop a real-time surveillance system, 
which can automatically detect the 
uncooperative vehicle within a closer view at 
a distance up to 45 meters in any lighting 
environment.  This surveillance system will 
include one wide field of view (WFV) camera 
and one pan-tilt-zoom (PTZ) active camera.  
Both WFV camera and PTZ camera are 
computer controlled by two PCs, called WFV 
PC and PTZ PC, respectively.  Two PCs can 
 3
Posterior probability estimation including 
both vehicle and non-vehicle information is 
used in [12] for robust detection of vehicles of 
different sizes and poses. The joint probability 
of wavelet coefficients and their 
corresponding positions within a given region 
is estimated for likelihood evaluation. 
Unfortunately the detection process in this 
approach is time consuming. The same 
authors used a similar approach in detecting 
faces of different sizes and poses [11]. They 
used PCA with grayvalue instead of wavelet 
base and took into account a 16*16-pixel 
subregion as the position information for joint 
probability. The use of subregion achieved a 
better result than similar work considering a 
full 15*15-pixel face region for PCA process 
[10]. Researches in [8], [9], [11], [12], and [14] 
also confirm that face detection or recognition 
considering local subregions has a better result 
than that considering the full-face region. 
However the performance of the above works 
is still slow. Furthermore, the research in [8] 
does not include non-face information for its 
posterior probability estimation in order to 
reduce the false alarms. 
To overcome the weaknesses of the 
existing works, we use posterior probability 
with both vehicle and non-vehicle information 
for vehicle detection. The joint probability for 
maximum-likelihood estimation considers 
both meaningful local features and their 
corresponding positions. Moreover, in order to 
speed up the computational time in detection 
process, a vector quantization method is 
applied to classify the training images into 
several clusters. 
 
2. The Vehicle Detection System 
Our vehicle detection system consists of 
both training process and testing process, as 
shown in the workflow diagram in Figure 1. 
Currently this surveillance system is used for 
access control of school’s gateways, as shown 
in Figure 2, and thus detects the vehicles only 
from the frontal and back views, instead of 
vehicles in different poses. The size of the 
vehicle template and canonical vehicle images 
is Nr*Nc (i.e., rows*columns) pixels, where Nr 
is 56 pixels and Nc is 72 pixels. The original 
image size is 240*320 pixels. 
 
2.1. Canonical Vehicle Image Creation 
To create canonical back-viewed vehicle 
image, we manually select four corner points 
on each vehicle in the training images as 
shown in Figure 3.a.1. The initial vehicle 
template with four corner points is then 
estimated based on the average width-height 
ratio of all training vehicle images. Next each 
vehicle image is normalized and then cropped 
to fit into the vehicle template. The 
normalization process is based on the 
corresponding four corner points using affine 
transformation 
 
343334 ××× = bxA ,         (1) 
 
where A and b are the four-point matrices for 
each vehicle in the training images and the 
vehicle template, respectively; x contains 6 
parameters of affine transformation. So matrix 
x can be solved by pseudo-inverse method: 
 
3443
1
344333 )( ××
−
××× = bAAAx TT .   (2) 
 
The distribution of four corner points from all 
normalized vehicles is shown in Figure 3.b. 
The corner position of the vehicle template is 
chosen as the average position of each 
individual sub-distribution, as shown in 
 5
canonical vehicle image and is defined as the 
linear combination of the first k eigenvectors 
 
∑ == ki ii rucurcK 1 )()(),( ,      (4) 
 
where k (=59) < M (=192). Figure 4 shows 
sample kernel images representing 
back-viewed vehicles with the reconstruction 
kernel defined as 
 
∑ == ki i
i
i
i ru
FcurcK
1
)()(),( λ ,    (5) 
 
where 
)( 2ni i
iF += λ
λ  is a low-pass filter, and k = 
59 and n = 0.25. Notice that the kernel images 
inherit the left-right symmetric property from 
back-viewed vehicles. Therefore only the 
left-hand sides of vehicles are shown. The 
same reconstruction kernel images can be 
applied to frontal-viewed vehicles.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Therefore, we choose three local subregions 
containing the most significant vehicle 
features for the detection process (see Figure 
5). The reason we do not consider significant 
components around the vehicle plate area is 
because they are sensitive to diverse vehicle 
plate locations (also shown in Figure 5). 
 
2.3. Vehicle Detection Using Posterior 
Probability Function 
Our purpose is to detect both back- and 
frontal-viewed vehicles from any input image 
I by shifting a vehicle template region IT 
(Nr*Nc=56*72 pixels) pixel by pixel over the 
entire image. The vehicle is detected if found 
inside the template region. 
Posterior probability function is used in 
vehicle detection 
 
=)|( TIvehicleP  
)()|()()|(
)()|(
vehiclenonPvehiclenonIPvehiclePvehicleIP
vehiclePvehicleIP
TT
T
−−+ (6) 
 
where P(IT|vehicle) and P(IT|non-vehicle) are 
the likelihood probabilities. We assume that 
the prior probability is uniform distributed. In 
other words, P(vehicle) = P(non-vehicle) = 0.5.  
The likelihood probability is then 
 
[ ]
2/12/
,
1
,2
1
)2(
)()(
)|(
∑
−∑−−=
−
N
TCT
T
TCT
T
IIIIExp
CIP π (7) 
 
where C is either the vehicle or non-vehicle 
class; TCI ,  and ∑  are the mean vector and 
the covariance matrix of all canonical image 
vectors for class C, respectively; N is the 
number of vector dimensions. From [8], the 
Mahalanobis distance d(IT) becomes: 
 
 
Figure 4. Reconstruction kernel images for back- 
viewed vehicles. 
Figure 5. Three local feature subregions containing 
540 pixels are selected based on LFA. 
Subregion 1: 10*54 pixels 
Subregion 2: 30*18 pixels 
Subregion 3: 30*18 pixels 
1 
2 3 
1st row: back view 
2nd row: frontal view 
Plates locate diversely 
1 
2 3 
 7
processes of vehicle detection: (1) Dependent 
subregions; (2) Dependent subregions with 
position information; (3) Independent 
subregions with position information. These 
solutions not only overcome the traditional 
inefficiency problem, but also help to 
investigate different factors that affect the 
detection process. The following sections 
describe in detail the solutions applied on 
back-viewed vehicles. However keep in mind 
these solutions also apply to frontal-viewed 
vehicle detection based on all canonical 
frontal-view vehicle or non-vehicle 
subregions. 
 
2.4.1. The 1st Solution: Dependent 
Subregions 
In the training process we generate one 
eigenspace from the 1728 subregions of all 
canonical back-viewed vehicle images. Each 
canonical subregion is then projected into this 
eigenspace, which consists of 30 major 
eigenvectors, as shown in Figure 7 with k=30, 
and thus reduces its dimensions from 540 to 
30. Equation (11) becomes 
 
( ) ( )∏ ∏= ==3 1 3 1 ||i i ii CprojectionPCsubregionP (12) 
 
where projectioni represents a 30-dimensional 
weight vector for subregion i. Figure 8 
demonstrates the first 6 eigenvectors of this 
eigenspace. Subregion 1 consists of 10*54 
(row*column) pixels while subregion 2 and 3 
30*18 pixels. We can see that the first two 
eigenvectors fall more inside subregion 1; the 
last four eigenvectors more inside subregions 
2 or 3. 
In the testing process, any input vehicle 
template region (Nr*Nc pixels) IT consists of 
three subregions. Each subregion is projected 
into the eigenspace to generate a 
corresponding weight vector. Each of three 
input subregions is then compared with all the 
canonical subregions. Based on equations (12), 
(10) and (6), we can detect the existence of a 
vehicle inside this input region by the 
following criterion: 
 
))|((  thresholdTIvehiclePif ≥  
) ' (      
) (               
existtdoesnvehicleotherwise
existsvehiclethen    (13) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7. Number of eigenvalues and the 
corresponding accumulated percentage.
k=30
Eigenvalues are sorted 
in descending order. 
1st 2nd 3rd 4th 5th 6th 
Figure 8. The first six eigenvectors of the 
eigenspace generated from canonical vehicle 
subregions.   
1st 2nd 
3rd 4th 
5th 6th 
1st 
row 
2nd 
row 
1st 
eigenvector 
2nd eigenvector 
Figure 9. Weight-vector distributions corresponding 
 9
template pixel by pixel at each level. 
We apply the three different solutions 
described in previous sections to all the 
training and testing images. The experimental 
results are listed in Table 3. The 2nd solution 
achieves the best performance while the 3rd 
solution the worst. Figure 11 illustrates the 
effect of feature position information in 
vehicle detection. Figure 11.a shows the result 
of vehicle detection without considering the 
feature position information (1st solution). It 
causes false acceptances between two 
neighboring vehicles because subregion 1 
encloses the top edge line of the wall, which 
looks like the ceiling line of the vehicle. 
 
 
 
 
 
 
 
 
 
 
 
 
(training/testing) 1st S 2nd S 3rd S 2nd S+VQ
B:Detection Rate %     
F:Detection Rate %     
B:False Alarm     
F:False Alarm     
Second/Frame 4.856 1.643 2.455 0.15 
 
In Figure 11.b, the vehicle is correctly 
detected by including feature position 
information in detection process (2nd solution). 
In addition, our vehicle detection system is 
tolerant to pan and roll rotations, scaling and 
partial occlusions, as demonstrated in Figure 
12. The detailed parameters are listed in Table 
4. 
 
4. Speedup by Vector Quantization 
In order to find the maximum posterior 
probability, it is necessary to compare each 
weight vector with all the canonical 
subregions. It is very time consuming (see [3]) 
because the number of training subregions is 
huge. To speed up computation, we use vector 
quantization to classify all the training vehicle 
and non-vehicle weight vectors into clusters 
(explained later). Now the comparison occurs 
between the input weight vectors and each of 
the clustering weight-vector centers. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 B-V B-NonV F-V F-NonV
Subregion1 240 628 167 1114 
Subregion2 279 442 249 1019 
Subregion3 219 505 243 734 
 Training 
Images 
Testing 
Images 
B-Viewed 
Vehicle 
192 Images 83 Images 
F-Viewed 
Vehicle 
176 Images 86 Images 
 Pan Rotation
Roll 
Rotation 
Scaling 
(pixels) Occlusion
Tolerance -30
0~ 
+300 
-100~ 
+100 
56*72~ 
110*140 Yes 
Example Fig. 12.a Fig. 12. b,e Fig. 12.c Fig. 12.d
Table 2. Training and testing databases. B: Back, F: Frontal 
Table 4. Tolerances in vehicle detection. 
Figure 12. Tolerances of our car detection approach. 
(a) (b)
(c) (d)
(e) 
Training Data 
Testing Data 
Roll 
Tolerance 
Figure 11. (a) Vehicle detection without feature 
position information. (b) Including feature position in 
vehicle detection. 
(a) (b) 
Table 5. Clusters of vector quantization. B: Back- viewed; 
F: Frontal-viewed; V: Vehicle; NonV: Non-vehicle 
Table 3. Performance of different solutions. 
(Computer: Pentium4 2.4G Hz.  ‘B’: Back-viewed 
vehicle; ‘F’: Frontal-viewed vehicle; ‘S’: Solution.) 
 11
and Classification of Vehicles,” IEEE 
Transactions on ITS, Vol. 3, No. 1, pp. 
37-47, 2002. 
[7] D. Koller, J. W. Weber, and J. Malik, 
“Robust Multiple Vehicle Tracking with 
Occlusion Reasoning,” 
ECCV,pp.189-196, 1994. 
[8] B. Moghaddam and A. Pentland. 
“Probabilistic Visual Learning for Object 
Representation,” IEEE PAMI, 19(7), pp. 
696–710, July 1997. 
[9] P. S. Penev and J. J. Atick., “Local 
Feature Analysis: A General Statistical 
Theory for Object Representation,” 
Neural Systems 7, pp.477-500, 1996. 
[10] S. L. Phung, D. Chai and A. Bouzerdoum, 
"A Distribution-Based Face/Non-Face 
Classification Technique," The 
Australian Journal of Intelligent 
Information Processing Systems, Vol. 7, 
No. 3/4, pp. 132-138, 2001. 
[11] H. Schneiderman and T. Kanade, 
“Probabilistic Modelig of Local 
Appearance and Spatial Relationships for 
Object Recognition,” IEEE CVPR, pp. 
45-51, 1998. 
[12] H. Schneiderman and T. Kanade, “A 
Statistical Method for 3D Object 
Detection Applied to Faces and 
Vehicles,” CVPR, pp. 746-751, 2000. 
[13] C. Tzomakas and W. von Seelen, 
“Vehicle Detection in Traffic Scenes 
Using Shadows,” Tech. Rep. IRINI 
98-06, 1998. 
[14] M.H. Yang, N. Ahuja and D. Kriegman, 
“Face Recognition Using Kernel 
Eigenfaces,” Advances in Neural 
Information Processing Systems 14 
(NIPS 14), pp. 215-220, 2002. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
