3. Memory Mapping 
DRAM is usually used as main memory for 
program execution. The thermal behavior of a 
memory block in a 3D SIP is affected not 
only by the power behavior but also the heat 
dissipating ability of that block. The power 
behavior of a block is related to the 
applications run on the system while the heat 
dissipating ability is determined by the 
number of tier and the position the block 
locates. Therefore, a thermal-aware memory 
allocator should consider the following two 
points. First, allocator should consider not 
only the power behavior of a logic block but 
also the physical location during memory 
mapping, second, the changing temperature 
of a physical block during execution of 
programs. In this paper, we will propose a 
memory mapping algorithm taking into 
consideration the above-mentioned two 
points. Our technique can be classified as 
static thermal management to be applied to 
embedded software designs. 
 
關 鍵 字 (Keywords) ： SiP 、 leadframe 
routing 、 memory mapping 、 thermal 
management 
 
二、 研究計劃之背景、目的及文獻討論 
 
隨著製程的進步，晶片(Chip)間的匯流
排(Bus)所需要的速度及頻寬越來越大，而
系統的匯流排的速度及頻寬則是取決於封
裝(Package)的技術。然而封裝技術的成長
相對於晶片製程進步慢了許多，以致於兩
者之間產生了越來越大的鴻溝，傳統的封
裝技術越來越不能滿足新製程晶片對於頻
寬的需求。 
 
為了解決訊號 on-chip及 off-chip傳遞
速度愈來愈大的差異，  因此有所謂
System-on-Chip（SOC）的設計方法產生。
然而隨著系統的複雜度增加，將許多不同
製程的晶片如記憶體晶片、邏輯晶片整合
在同一晶片，所需要的成本成跳躍的增
加。因此另一方面，為了考量成本因素，
有所謂 System-in-Package 的設計方法。所
謂 System-in-Package 是指將不同製程的晶
片，各自製造。之後，再利用製程包裝的
技術將晶片堆疊。 
 
設計 System-in-Package所面臨的挑戰 
 
 由於 System-in-Package 牽涉了系統整
合、封裝技術、晶圓製程等新技術的統整，
研發上勢必面臨傳統設計所沒有的問題與
挑戰，尤其在設計軟體上，有下列的問題： 
 設計如何分割，將不同 module分放在
不同之晶片上。 
 Pad assignment，Floorplanning，Routing
以減少 leadframe之 cost。 
 晶片堆疊或者是並排的選擇及取捨 
 如何預防堆疊產生的過熱問題 
 如何建立測試的模式，使得 SiP的總體
測試成本降低 
 在傳統的Memory mapping中，主要利
用 Sequential mapping；由於使用 data
之 locality，常會導致 hot spot module，
如何利用 memory mapping，使得
performance及 hot spot 問題皆能同時
解決。 
 
System-in-Package設計流程 
 
    為了解決以上問題，我們規劃了完整
的設計流程與研究計畫，以解決上述問題。 
本計畫將探討有關 Chip Stacking，
Leadframe Routing及 Memory Mapping之
問題。 
 
在 Chip Stacking方面，過去並未有完全類
似論文探討。然而，有關於 3D floor 
planning[12] ， 3D routing[13] ， 3D via 
assignment[14]的問題皆有發表在最近會議
中。在 Leadframe 繞線方面，過去有許多
Routing的論文探討 PCB routing，及目前新
發表的有關 routing for flip-chip design[15]
新論文，然而針對 SiP leadframe繞線特性
之相關研究並沒有論文發表。在 Memory 
Mapping方面，過去僅有少數論文探討，主
要針對 memory與 logic如何整合[16][17]。 
 
 
三、 研究方法及進行步驟 
 
及熱能表現。若以傳統的方式做 sequential
的 mapping，可能會導致一些存取頻率較
高的記憶體區段，被 map到散熱效率及熱
能表現較不好的 memory module上，而導
致整個系統的溫度過高。針對照這樣的情
形，我們提出的一套完整的程式分析及記
憶體映成方法，以降低整體系統的最高溫
度。 
 
首先，在硬體部分，針對記憶體的堆
疊方式及存取特性，我們先列舉出幾組在
熱能表現上較具優勢的存取組合。在軟體
部分，我們觀察應用程式對記憶體的存取
情形，將記憶體空間依照存取頻率，切成
數個區間。最後，我們利用整數型線性規
劃(integer linear programming)的方法，去
找出硬體上最合適的存取組合，以及各記
憶體區間對 memory module的映成情形。 
 
四、 結果與討論 
 
本年度主要的研究成果，在於針對 SiP
設計中 memory mapping的部分，在同時考
慮軟體及硬體特性的情形下，提出一套完
整的設計流程，以不影響效能表現為前
提，提升系統的熱能表現，並降低系統的
最高溫度。 
 
我們的實驗結果顯示：與傳統
sequential的 mapping方式相比，在單核心
的平台環境下，我們所提出的方法，最多
可以將記憶體系統的最高溫度降低 17.1
℃。平均來說，最高溫度的降幅為 13.3℃。
同時，我們進一步將我們所提出的方法擴
展到多核心的環境。實驗顯示，在四核心
的平台環境下，我們所提出的方法可以將
最高溫度降低 9.9℃～11.6℃。 
 
我們的研究成果，已先於 2009年，在
設計、自動化暨測試全歐會議 (DATE)中發
表。在 2010年初，我們進一步的將更完整
的研究成果投稿至 ACM Transaction on 
Embedded Computing Systems (TECS)，
目前正在審查階段中。附件一及附件二，
分別為我們在 DATE 會議中發表的論文以
及投稿至 TECS 國際期刊之文稿。關於我
們所提出方法的詳細介紹，以及完整的實
驗流程，可參考附件一及附件二。 
 
 
五、 參考文獻 
 
[1] Rao R. Tummala, "Fundamentals of 
microsystems packageing," international 
edition 2001. 
[2] Joe Adam, “System-in-Package 
Roadmap,”1st workshop on 3S (SOP, SIP, 
SOC) Electronics Technologies . 
[3] W. R. Davis,J. Wilson,S. Mick,J. Xu,H. 
Hua,C. Mineo,A. M. Sule,M. Steer,and P. D. 
Franzon, "Demystifying 3D ICs: The Pros 
and Cons of Going Vertical," IEEE Design & 
Test of Computers, pp. 498-510, 2005. 
[4] Juergen Wolf, and IZMJoe Adams,"2005 
Packaging Roadmap Overview," 2005. 
[5] Bryan Black , Donald W. Nelson, Clair 
Webb, and Nick Samra, “3D Processing 
Technology and its Impact on iA32 
Microprocessors,” Proc. of ICCD’04. 
[6] R. M. Lea, I. P. Jalowiecki, D. K. 
Boughton,, J. S. Yamaguchi,A. A. Pepe,V. H. 
Ozguz, and J. C. Carson, “A 3-D Stacked 
Chip Packaging Solution for Miniaturized 
Massively Parallel Processing,”IEEE 
Transactions On Advanced Packaging , 
pp.424-432,  1999. 
[7] Rao R. Tunmala,V. Sundaram,F. Liu,G. 
White, S. Bhattacharya, R. M. Pulugurtha,M. 
Swaminathan,S. Dalmia,J. Laskar,N. M. 
Jokerst,S. Y. Chow, "High Density Packaging 
in 2010 and Beyond", International 
Symposium On Electronic Materials and 
Packaging , 2002.   
[8] V.N. Johnson, J. Jozwiak, and A. 
Moll,  “Through Wafer Interconnects on 
Active pMOS Devices,” Proceedings IEEE 
Workshop on Microelectronics and Electron 
Devices,  pp. 82-84,  2004. 
[9] "SiP (System in Package)", Toshiba 
corporation 
http://www.semicon.toshiba.co.jp/eng/ 
[10]Jordan Tsai , “System In Package 
Technology & Development,” ASE TECH 
FORUM 2003. 
[11] Jayson Hsu,"Multi-Chip Packaging for 
Lead Frame,” Ase Inc. 2004. 
[12] J. Cong and Y. Zhang, “A Thermal 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
附件一： 
(研究成果) 
於 DATE國際會議中所發表的論文 
 
 
 
 
 
 
 
 
 
TSV TSV
TSV TSV
Upper Tier
Lower Tier
Fig. 1. Bonding between TSVs and Bond Pads
for each TSV [9][10]. In addition to misalignments, TSVs
can also fail in the soldering process [11]. For example, short
circuits between two distinct TSVs or open circuits between a
TSV and its corresponding bonding pad may be formed. Other
failure mechanisms such as dislocation, process variations or
mechanical stress also decrease the fabrication yield of TSVs.
Above all, misalignment and failures on bonding are pri-
mary failure mechanisms for TSVs [11]. Both of the tech-
nologies used for alignment and bonding are very similar
to the packaging methods used in current IC industry [5].
Although the exact failure rate of TSVs is still not clear, it
is possible to use the failure rates of alignment and bonding
to perform a failure rate analysis for TSV. Considering the
TSV diameter and the size of bond pads, the failure rate of
a single TSV may ranges between 10−4 and 10−5 based on
current packaging technology. This assumption roughly meets
the yields of TSVs from the process technologies of HRI,
IMEC and IBM [12][13][14].
According to the applications and network styles, the num-
ber of TSVs in each tier can be quite different. For many-core
processors or NoC-based designs, thousands of TSVs may be
required in each tier. On the contrary, hundreds of TSVs may
be sufcient for smaller IP-based designs. In this work, we
focus on IP-based designs where TSVs are mainly used for
connections between modules on different tiers. Considering
the area of bond pads and oorplan problems, we assume that
the number of TSVs to be placed in a tier ranges from 300
to 500.
An analysis between failure rate and yield is given in
Figure 2. Assume that all dies to be stacked are known-good-
dies. Thus, only the failure rate of TSV bonding needs to be
considered. Let f stands for the failure rate of bonding one
TSV and #tier stands for the number of tiers to be stacked.
Note that the actual number of tiers that contain TSVs to be
bonded is equal to #tier - 1. For example, when #tier = 2, only
the top tier contains TSVs to be bonded. The x-axis represents
the number of TSVs to be placed in each tier (#TSV). Since a
good chip stack requires all TSVs to be successfully bonded,
the binding yield can be computed as (1−𝑓)#𝑇𝑆𝑉×(#𝑡𝑖𝑒𝑟−1).
The analysis results for 𝑓 = {0.0001, 0.00002} and #𝑡𝑖𝑒𝑟 =
{2 , 5} are shown in Figure 2. Without any redundant TSVs,
the average yield is 94.35%. And when #𝑇𝑆𝑉 = 500 and
#𝑡𝑖𝑒𝑟 = 5, the yield degrades to 81.8%. Note that dies to
be stacked are all known-good-dies. Therefore, the cost of
discarding chip stacks that are failed due to TSV bonding is
very expensive. In fact, in most failed chip stacks, only a very
80 00%
85.00%
90.00%
95.00%
100.00%
p 
St
ac
ks
 w
ith
 n
o 
Fa
ile
d 
TS
V
f = 0 0001 #tier = 2
70.00%
75.00%
.
300 350 400 450 500
%
of
 C
hi
p 
St
ac
ks
 w
ith
 n
o 
Fa
ile
d 
TS
V
#TSV
  . ,  
f=0.0001,#tier=5
f=0.00002,#tier=2
f=0.00002,#tier=5
Fig. 2. Yield Analysis
small portion of TSVs are failed. If these failed TSVs can be
recovered with circuits of reasonable cost, the yield can be
largely improved. The redundant TSV design to be proposed
in this paper provides a solution to this problem.
III. REDUNDANT TSV ARCHITECTURE
In this section, the architecture of our proposed redundant
TSV design is introduced in Section III-A. Next, a brief
introduction to the oorplan of 3D IC and its relation to our
proposed architecture are given in Section III-B.
A. Architecture Design
The proposed architecture for redundant TSV is depicted
in Figure 3. For each TSV, 2 MUXs are added to shift the
signal to neighboring TSV when one TSV is failed. To reduce
the timing effect caused by the loading capacitance of the
additional wires used for signal shifting, a pair of buffers are
added to each TSV. The TSVs are connected as a chain where
the redundant TSV is placed at the last position of the chain.
When no TSV is failed, all signals are transferred by original
TSVs as shown in Figure 4(a). When a TSV is failed, the
signal of the failed TSV needs to be shifted. This in term
causes all signals between the failed TSV and the redundant
TSV to be shifted. For example, let TSV 1 be failed. The
signal paths after shifting are shown in Figure 4(b). When a
signal is shifted, larger delay is introduced due to larger wire
length and buffers. For signals that are timing critical, this
may become a problem. We will discuss it in Section V-A.
In this architecture, only one failed TSV can be recovered in
each chain. If two or more TSVs are failed in a chain, only
one of them can be recovered. Therefore, how to determine the
number of TSVs in a chain so that an acceptable recovery rate
can be achieved is an important design issue. This issue will
be discussed in Section IV. For simplicity, the term TSV-chain
is used to refer to the structure of the proposed redundant TSV
architecture.
The MUXs in the proposed architecture are connected to an
e-fuse array which can be programmed by a scan-chain. By
default, all signals connect to MUXs are set to 0. When the
testing for TSV connectivity is done, signals are scanned in to
program the e-fuses so that each MUX receives an appropriate
control signal.
TABLE I
𝑃𝑓 𝑡𝑠𝑣=𝑛 AND 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛 WHEN 𝐹 = 0.0001
𝑁 𝑛 𝑃𝑓 𝑡𝑠𝑣=𝑛 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛
300
0 97.0444% 97.0444%
1 2.9116% 99.9560%
2 0.0435% 99.9996%
400
0 96.0788% 96.0788%
1 3.8435% 99.9223%
2 0.0767% 99.9990%
500
0 95.1227% 95.1227%
1 4.7566% 99.8793%
2 0.1187% 99.9980%
Table I shows that, when 𝑛 = 2, the values of 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛 for
𝑁 = {300, 400, 500} are all greater than 99.998%. A smaller
𝐹 will result in a larger 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛. This means, as long as
the failure rate 𝐹 is no greater than 0.0001, the probability
that three or more TSVs are failed is less than 0.002%.
Therefore, when designing TSV-chains, we can assume that
the maximum number of failed TSVs in a tier is 2. This
assumption covers 99.998% of all possible faulty free and
faulty situations.
B. Analysis on Recovery Rate
As mentioned in Section III, each TSV-chain is capable of
recovering at most one failed TSV in a TSV block. As the
number of TSVs in a TSV block increases, the probability
that all failed TSVs can be recovered decreases. To achieve
an expected recovery rate, the number of TSVs in each TSV
block must be limited. To simplify the analysis, we assume
that the number of TSVs in all TSV blocks are identical. Let
#𝐵 𝑇𝑆𝑉 stand for the number of TSVs in each TSV block
and 𝑛 stand for the number of failed TSVs. For a given value
of 𝑛, we want to analyze the relation between #𝐵 𝑇𝑆𝑉 and
recovery rate. The discussion in Section IV-A indicates that
assuming 𝑛 ≤ 2 is sufcient to covers 99.998% of all possible
faulty free and faulty situations. Therefore, we will perform
the analysis for 𝑛 = 1 and 𝑛 = 2 only.
Let 𝑁 stand for the number of TSVs in a tier. The
number of combinations of 𝑁 TSVs with 𝑛 failed TSVs can
be computed as 𝐶𝑁𝑛 . The number of combinations that all
failed TSVs can be recovered by TSV-chains is referred as
#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑡𝑖𝑜𝑛𝑠. The recovery rate discussed
in this section is dened as
#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑡𝑖𝑜𝑛𝑠
𝐶𝑁𝑛
.
When 𝑛 = 1, only one failed TSV needs to be recovered.
Since one TSV block contains one redundant TSV, regardless
of the number of TSVs in each TSV block, one failed TSV can
always be recovered. Therefore, the recovery rate for 𝑛 = 1
is 100%.
The recovery rate analysis for 𝑛 = 2 is more complicated.
Let the term #𝐵𝑙𝑜𝑐𝑘 represent the number of TSV blocks in
a tier. Under our assumptions, #𝐵𝑙𝑜𝑐𝑘 can be computed as
𝑁
#𝐵 𝑇𝑆𝑉 . To successfully recover all failed TSVs, each failed
TSVs must be located in different TSV blocks. That is, 𝑛 TSV
blocks are selected from #𝐵𝑙𝑜𝑐𝑘 TSV blocks. Each selected
TSV block contains exactly one failed TSV. The number of
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
ec
ov
er
y 
R
at
e
0.00%
10.00%
20.00%
30.00%
25 26 27 29 31 33 35 38 41 45 50 55 62 71 83 100 125 166 250
R
ec
ov
er
y 
R
at
e
Number of TSVs in a TSV Block
Fig. 6. Recovery Rate when 𝑁 = 500, 𝑛 = 2
combinations that satises this requirement can be computed
as
𝐶#𝐵𝑙𝑜𝑐𝑘𝑛 .
Inside each TSV block that contains one failed TSV, the
failed TSV can be located at #𝐵 𝑇𝑆𝑉 possible positions.
Therefore, the #𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑖𝑜𝑛𝑠 for 𝑛 = 2 can
be computed as
𝐶#𝐵𝑙𝑜𝑐𝑘2 ⋅ (#𝐵 𝑇𝑆𝑉 )2.
The relation between #𝐵 𝑇𝑆𝑉 and recovery rate for 𝑁 =
500 and 𝑛 = 2 is shown in Figure 6. For different values of
#𝐵 𝑇𝑆𝑉 that result in the same #𝐵𝑙𝑜𝑐𝑘, only the smallest
#𝐵 𝑇𝑆𝑉 is shown in the gure since the recovery rates of
them are the same.1
According to Figure 6, to achieve 90% recovery rate,
#𝐵 𝑇𝑆𝑉 needs to be no greater than 50. By limiting the
number of TSVs in each TSV block to be less than or equal
to 50, the recovery rate is greater than 90%. To achieve a
higher recovery rate, the gure shows that with 95% recovery
rate, the number of TSVs in each TSV block cannot be greater
than 25. In realistic ASIC designs, the number of TSVs in a
TSV block is usually less than 50. Therefore, in most cases,
TSV block partitioning is not required.
A further analysis is to compute the overall yield. In
Table I, when 𝑁 = 500, 𝑃𝑓 𝑡𝑠𝑣=0, 𝑃𝑓 𝑡𝑠𝑣=1, and 𝑃𝑓 𝑡𝑠𝑣=2
are 95.1227%, 4.7566%, and 0.1187%, respectively. The dis-
cussion above indicates that the recovery rate for 𝑛 = 1 is
always 100% based on our proposed architecture. Thus, let
the recovery rate for 𝑛 = 2 be set to 90%. The overall yield
can be computed as
𝑃𝑓 𝑡𝑠𝑣=0+𝑃𝑓 𝑡𝑠𝑣=1×100%+𝑃𝑓 𝑡𝑠𝑣=2×90% = 99.98613%.
This value is high enough for most applications.
V. DESIGN FLOW AND TSV-Chain DESIGN
The discussion in Section IV focuses on the recovery of
failed TSVs in terms of connectivity. Timing issues are not
concerned. As mentioned in Section III-A, when a signal is
shifted in a TSV-chain, extra delay will be incurred. For signals
that are timing critical, the delay caused by signal shifting may
1The actual computation of#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑖𝑜𝑛𝑠 is a little more
complicated since sizes of TSV blocks may differ by one due to the division
operation to compute #𝐵𝑙𝑜𝑐𝑘
Corner of a TierBoundary of a Tier
(a) Spiral-Style (b) Snake-Style (c) Hybrid
Fig. 9. Chaining Styles
probabilities to be routed through for minimum wire length.
These TSVs should be assigned as head parts of TSV-chains.
A spiral-style chaining policy is proposed for TSV-chain
design. In a TSV block, by picking a TSV in the central
position to be the starting point, spiral-style chaining results
in a routing path where all TSVs on the boundary are at one
end. The starting TSV is assigned as redundant TSV while the
other end becomes the head of a TSV-chain. An example for a
4 × 5 TSV block is shown in Figure 9(a) where TSVs in grey
are head and good for timing critical signals. In routing stage,
routers can choose to assign timing-critical signals to TSVs
that are on the boundary of a TSV block. This can reduce the
probability for a timing critical signal to be shifted.
The spiral-style chaining policy is appropriate for a TSV
block that is not on the boundary or the corner of a tier.
For a TSV block located on the boundary of a tier, most
signals assigned to that TSV block are connected from the
opposite side of the tier boundary. In this case, a snake-style
chaining policy satises the requirement. The result is shown
in Figure 9(b). For a TSV block located on the corner, most
signals assigned to that TSV block are connected from the
counter direction of the tier corner. In this case, a hybrid
chaining policy as shown in Figure 9(c) becomes the best
candidate. Based on the position of each TSV block, one of
these three chaining policies can be applied to determine the
structure of a TSV-chain.
C. Physical Design Flow Considering TSV-Chain
In current design ow for 3D ICs , 3D partitioning rst
takes place to determine which tier each design blocks to be
placed. The number of required TSVs for signals between two
consecutive tiers is determined in this stage. Next, in oorplan
stage, blocks with x area but unknown dimensions are placed
in each tier. To provide communication links between blocks
in different tiers, TSV blocks are placed. The number of sig-
nals to be assigned to each TSV block as well as the position
of each TSV block are roughly determined in this stage. Based
on the discussion in Section IV, the number of TSVs in each
TSV block should be limited. Partitioning may be required
for large TSV blocks. Based on the position of each TSV
block, the structure of each TSV-chain is determined. In place
and route stage, routers should be aware of the TSV-chain
structure in each TSV block. Based on design constraints and
requirements, router needs to decide whether to assign timing
critical signals to TSVs that are located at the head of TSV-
chains. The overall design ow for TSV-chain is shown in
Figure 10.
3D Partitioning:
TSVs required for signal on each tier is determined
Placement
3D Routing:
Assignment of signals to TSVs
Considering the structure of each TSV-chain
when perform the assignment of signals to TSVs
3D Floorplanning:
TSV Block are determined
1. Partitioning is required for large TSV blocks
2. The size of each TSV block is limited
Based on the position of each TSV block, determine 
the structure of each TSV-chain
Fig. 10. Proposed Design Flow for TSV-Chain
VI. CONCLUSION
In this paper, a new redundant TSV architecture with
reasonable cost for ASICs has been proposed. Design issues
including recovery rate and timing problem have been inves-
tigated. Required modications on the design ow has been
explained. Based on probabilistic models, the new design can
successfully recover most of the failed chips and increase the
yield of TSV bonding to 99.99%. This can effectively reduce
the cost of manufacturing 3D designs.
REFERENCES
[1] W. R. Davis, J. Wilson, S. Mick, davis demystifying 3D et al.,
“Demistifying 3D ICs: The Pros and Cons of Going Vertical,” IEEE
Design Test of Computer, vol. 22, no. 6, pp. 498-510, Nov./Dec., 2005.
[2] J. Burns, L. Mcllrath, C. Keast, et al., “Three-Dimensional Integrated
Circuit for Low Power, High-Bandwidth Systems on a Chip,” ISSCC
Dig. of Tech. Papers, pp. 268-269, Feb., 2001.
[3] S. Siesshoefer and et al., “Z-axis Interconnect Using Fine Pitch,
Nanoscale Through Silicon Vias: Process Development,” ECTC, 2004.
[4] P. Morrow, M. J. Kobrinsky, S. Ramanathan, et al., “Wafer-Level 3D
Interconnects via Cu Bonding,” Proceedings of the Advanced Metalliza-
tion Conference, pp. 125-130, 2004.
[5] Philip Garrou, Christopher Bower, and Peter Ramm, “Handbook of
3D Integration: Technology and Application of 3D Integrated Circuits
Volume 1 & 2,” poblished by WILEY-VCHVerlag GmbH& Co. KGaA,
Weinheim, 2008, ISBN: 978-3-527-32034-9.
[6] Uksong Kang, Hoe-Ju Chung, Seongmoo Heo, et al., “8Gb 3D DDR3
DRAM Using Through-Silicon-Via Technology,” ISSCC Dig. of Tech.
Papers, pp. 130-131, Feb., 2009.
[7] Igor Loi, Subhasish Mitra, Thomas H. Lee, Shinobu Fujita and Luca
Benini, “A Low-Overhead Faule Tolerance Scheme for TSV-Based 3D
Network on Chip Links,” ICCAD, 2008.
[8] Hsien-Hsin S. Lee and Krishnendu Chakrabarty, “Test Challenges for
3D Integrated Circuits,” IEEE Design & Test of Computers, vol. 26, no.
5, pp. 26-35, Sep./Oct., 2009.
[9] R. Patti, “Three-Dimensional Integrated Circuits and the Future of
System-on-Chip Designs,” Proc. of the IEEE, vol. 84, no. 6, June 2006.
[10] A. W. Topol, J. D. C. La Tulipe, L. Shi, et al., “Three Dimensional
Integrated Circuits,” IBM Journal of Research and Development, vol.
50, no. 4/5, pp. 491-506, July/Sepetember 2006.
[11] R. Patti, “Impact of Wafer-Level 3D Stacking on the Yield of ICs,”
Future Lab Intl., issue 23, July 2007.
[12] N. Miyakawa, “A 3D Prototyping Chip based on a Wafer-Level Stacking
Technology,” ASP-DAC, Jan. 2009.
[13] B. Swinnen, W. Ruythooren, et al., “3D Integration by Cu-Cu Thermo-
Compression Bonding of Extremely Thinned Bulk-Si Die Containing
10 𝜇m Pitch Through-Si Vias,” IEDM, Dec. 2006.
[14] A. W. Topol, et al., “Enabling SOI-Based Assembly Technology for
Three-Dimensional Integrated Circuits,” IEDM, Dec. 2005.
Thermal-Aware Memory Mapping in 3D Designs
ANG-CHIH HSIEH and TINGTING HWANG
Department of Computer Science, National Tsing Hua University
DRAM is usually used as main memory for program execution. The thermal behavior of a memory
block in a 3D SIP is aﬀected not only by the power behavior but also the heat dissipating ability
of that block. The power behavior of a block is related to the applications run on the system
while the heat dissipating ability is determined by the number of tier and the position the block
locates. Therefore, a thermal-aware memory allocator should consider the following two points.
First, allocator should consider not only the power behavior of a logic block but also the physical
location during memory mapping, second, the changing temperature of a physical block during
execution of programs. In this paper, we will propose a memory mapping algorithm taking into
consideration the above-mentioned two points. Our technique can be classiﬁed as static thermal
management to be applied to embedded software designs. Experiments show that, for single-core
systems, our method can reduce the temperature of memory system by 17.1∘C as compared to a
straightforward mapping in the best case, and 13.3∘C in average. For systems with 4 cores, the
temperature reductions are 9.9∘C and 11.6∘C in average when L1 cache of each core is set to 4KB
and 8KB, respectively.
Categories and Subject Descriptors: B.7.1 [Integrated Circuits]: Types and Design Styles—
VLSI (Very Large Scale Integration); B.3.1 [Memory Structures]: Semiconductor Memories;
B.8.1 [Performance and Reliability]: Reliability, Testing, and Fault-Tolerance
General Terms: Design, Reliability
Additional Key Words and Phrases: System in package (SIP), thermal management, memory
mapping
1. INTRODUCTION
System in package (SIP) provides a cost-eﬀective solution for large-scale integration
[1]. This technology has been widely used in mobile devices and embedded systems.
Current technology allows more than twenty chips to be stacked in one package [2].
With the capacity provided by SIP technology, integrating memory chips into pack-
age has become popular in recent years. Several researches on memory integration
based on SIP have been studied [3][4][5][6][7]. Though SIP technology provides
extremely high capacity for circuit integration, it suﬀers severe thermal stress be-
cause of three dimensional stacking of ICs [8]. Thermal stress will induce variation
of DRAM retention time and reliability problem [9].
Many temperature-aware researches have been conducted. They can be classi-
ﬁed into two categories, dynamic and static thermal managements. The former
techniques detect the temperature information at run-time, and stop hot units op-
erating till their temperature cools down. Examples such as voltage scaling [10],
throttling techniques [11], and non-DVS localized thermal management [12] are in
this category. Dynamic thermal management schemes can precisely monitor tem-
perature value and guarantee that the system temperature will never be higher
than a predeﬁned constraint, however, at the cost of slowdown of the processor ex-
ecution. As to static thermal management, the proﬁling data is generated ﬁrst and
then used to analyze the temperature distribution of the program. [13] proposes a
ACM Journal Name, Vol. V, No. N, Month 20YY, Pages 1–0??.
Preparing Articles for the ACM Transactions ⋅ 3
Dual Die DRAM Packages on PCB Module
DRAM Die
DRAM Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM 
Die
DRAM Dies in SIP
Fig. 1. DRAM Packages on PCB and DRAM Dies in SIP Design
have quite diﬀerent access frequencies. For example, instructions of an application
are all loaded to a consecutive memory space. But segments for instructions of
diﬀerent loops or diﬀerent functions are accessed with diﬀerent frequencies. This
situation can also be found in memory blocks for data, heap and stack. In tra-
ditional on-board DRAM chips, the mapping between these memory blocks and
physical DRAM chips can be simple since all DRAM chips are identical. However,
for SIP designs, the mapping problem becomes complicated because the behavior
of each memory block and the heat dissipating ability of each DRAM chip need to
be considered simultaneously for thermal management.
Figure 2 gives an example to present our motivation. Assume that a program is
executed with 4 stages. 4 functions named funcA(), funcB(), funcC() and funcD()
are called in each stage, as shown in Figure 2(a). When a function is called, its
corresponding memory segment is accessed. Since diﬀerent function has diﬀerent
behavior, each segment has diﬀerent access frequency. Let the access frequency
of each segment be given in Figure 2(b) where access frequency is deﬁned as the
number of accesses to a memory segment divided by the total cycle counts of that
stage. In this simpliﬁed example, we assume each memory die has only two banks.
Due to design constraints, for each memory die, only one bank can be accessed
at a time. Let a wider memory word be composed of bits from two dies. Then
2 memory dies are required to be triggered simultaneously for each access. This
means an address will map to 2 banks of 2 diﬀerent memory dies. Three mapping
policies are shown in Figure 2(c)-(e). Figure 2(c) shows a straightforward map-
ping (Mapping A) where two banks at the same relative position denoted as 𝐴, 𝐵,
𝐶, 𝐷 are accessed simultaneously. Figure 2(d) shows a mapping (Mapping B) to
avoid stacking eﬀect where banks accessed at the same tier are not in the same
vertical position and Figure 2(e) a mapping (Mapping C ) consider stacking eﬀect
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 5
32-bit Processor
Memory Controller
System Bus
DRAM 
Die 0
DRAM
Die 1
DRAM
Die 2
DRAM 
Die 3
DRAM 
Die 4
DRAM
Die 5
DRAM
Die 6
DRAM 
Die 7
DRAM 
Die 8
DRAM
Die 9
DRAM
Die 10
DRAM
Die 11
DRAM 
Die 12
DRAM
Die 13
DRAM
Die 14
DRAM 
Die 15
Group 1
Group 2
Group 3
Group 4
8-bit 8-bit 8-bit 8-bit
Fig. 3. Memory System
that mappings considering stacking eﬀect but banks located at bottom tiers (Map-
ping C ) sometimes has higher temperature than straightforward mapping. But in
both stages, the temperature is relative low because of low access frequency. The
maximum temperature occurs in Stage IV because of the highest access frequency.
Stage IV (funcD()) shows that a mapping considering stacking eﬀect (Mapping C )
and program behavior can reduce the maximum temperature by 18∘C and 12∘C as
compared to Mappings A and B respectively.
3. SYSTEM MODEL AND PROBLEM DEFINITION
In this section, we will ﬁrst give our system model. Based on the model, we will
deﬁne our problem and propose an overall design ﬂow. The data width of a modern
DRAM chip often ranges between 20-bit to 24-bit while processors have a 32-bit,
64-bit, or more data lines. Therefore, to read or write a 32-bit, 64-bit or more
bit word from memory, multiple DRAM chips need to be accessed. Figure 3 gives
an example of a system containing 32-bit processor, system bus, memory controller
and 8-bit DRAM chips. To access a 32-bit data, 4 DRAM chips need to be activated
simultaneously. Let the DRAM chips activated simultaneously form a group. Then,
in the example, DRAM Die 0 to DRAM Die 3 are in the same group. To increase
the number of words (address space) in the system, multiple groups are assembled.
In the example, there are 4 groups. Hence, the total address space is 4 times the
word capacity of one group.
In a stacked SIP system, memory dies are stacked one tier on another. In one tier,
there will be one or more dies packed. Due to intra-tier package routing constraint,
the number of dies packed in one tier is rarely greater than 4. Figure 4(a) shows
a system that has 8 tiers and 2 dies packed in one tier. Within a die, there are
multiple banks in it. The ﬂoorplan of a typical DRAM chip with 4 banks is shown in
Figure 4(b). For each memory access, Control & Pre-charge Circuits block is always
triggered. This block contains control, error correction and pre-charge circuits. The
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 7
Parameters of the memory 
system
Determination of 
Candidate Configurations
Memory reference records 
for all applications
Application Behavior 
Analysis
Segments of all 
applications
ILP Formulation for 
Segments Mapping
Mapping decisions for all 
segments
Configurations of the 
memory ststem
Candidate configurations
Fig. 6. Overall Flow
high power density due to their small area size. In general, more than 30% power
of a DRAM chip is consumed by Sense Ampliﬁer block while the area of a block is
usually less than 5% of the total area. Sense Ampliﬁer blocks are usually candidates
for hotspot. If continuous addresses in a bank are accessed, Sense Ampliﬁer blocks
stacked at the same relative position in 3D space will result in high temperature.
On the other hand, Figure 5(b) shows another access mapping where the same
dies form a group but banks in diﬀerent relative positions are selected to form a set.
In this mapping, lower temperature can be expected because the activated banks
are not in the same vertical location.
In this paper, we will study a memory mapping problem to minimize the max-
imum temperature in a stacked 3D memory system. The problem is deﬁned as
follows. Given parameters of a memory system and the proﬁling of memory refer-
ences for all application programs, the objective is to ﬁnd a memory conﬁguration
and a mapping from logical address to physical location so that the maximum
temperature is minimized.
To solve this problem, the ﬂow depicted in Figure 6 is proposed. The ﬁrst step,
Determination of Candidate Conﬁgurations is, for given parameters of a memory
system, to ﬁnd candidate memory conﬁgurations (in Section 4.1). Then, behav-
iors of applications run on the system are analyzed in the second step, Application
Behavior Analysis, where logical memory blocks that have the similar behaviors
are grouped in a segment (in Section 4.2). According to the candidate conﬁgura-
tions and segments obtained, the last step, ILP Formation for Segments Mapping,
is an ILP formulation to perform mapping so that the maximum temperature is
minimized (in Section 4.3).
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 9
Tier n
Tier n + 1
(a)
Set 0
Set 1
Set 2
Set 3 (b)
Tier n
Tier n + 1
Fig. 9. (a) Conﬁguration I ; (b) Conﬁguration II
a group. However, most of combinations of dies are not required to be considered.
Because dies in a group are accessed simultaneously, thermal behavior of a group is
determined by the die that has the worst behavior. For example, if die 7, die 6, die
5 and die 4 in Figure 4(a) are deﬁned as a group, though die 7 is on the top tier
and has the best heat dissipating ability, the actual thermal behavior of the group
is bounded by die 4. No matter how low the temperature of die 7 is, the memory
space provided by the group would not be functional if die 4 is overheated. Thus,
dies in a group should have similar environmental conditions.
Based on the discussion above, how to form a group becomes straightforward.
We should group dies on consecutive tiers into a group. In our example, because
the number of dies in a group = 4 and #𝑑𝑖𝑒 𝑜𝑛 𝑡𝑖𝑒𝑟 = 2, dies on 2 neighboring
tiers forms a group. That is, die 0, die 1, die 8, and die 9 form a group, and die 2,
die 3, die 10, and die 11 form a group,...etc.
Next, we show how to determine the banks in a set. First, banks on the same
tier have diﬀerent heat dissipating abilities when #𝑑𝑖𝑒 𝑜𝑛 𝑡𝑖𝑒𝑟 ≥ 2. For example,
suppose there are two dies on a tier as shown in Figure 8. Banks 1, 3, 4 and 6 are in
the middle area of the tier and therefore have worse thermal behavior than banks
0, 2, 5 and 7. Second, accessing banks of diﬀerent dies at the same vertical position
will result in undesirable thermal eﬀect. For example, banks 0, 8 are at the same
vertical position. If they are accessed simultaneously, heat will be generated in a
small area and cannot be dissipated in vertical directions. This situation should
be avoided. Based on the discussion above, possible sets combinations for a group
can be deﬁned through enumeration. The term conﬁguration is used to refer to a
deﬁnition of all sets in a group. We use the example in Figure 8 to explain how to
determine possible conﬁgurations where dies on two neighboring tiers form a group.
We start with deﬁning a set with best thermal behavior. As mentioned earlier,
the thermal behavior of a set is determined by the bank with the worst thermal
behavior. Therefore, to deﬁne a set with best thermal behavior, two rules should be
followed. Rule 1 is that banks in the middle area should not be grouped in the same
set and rule 2 is that banks in the same vertical position should not be grouped in
the same set. Following these two rules, Figure 9 shows two resultant conﬁgurations,
Conﬁguration I and Conﬁguration II, where banks drawn in the same patterns are
deﬁned as a set. Two conﬁgurations have their own characteristics. In Conﬁguration
I, set 0 and set 1 have good heat dissipating ability because the banks in these two
sets are all in the boundary. However, the environmental conditions of set 2 and
set 3 are worse than those of set 0 and set 1 because banks in set 2 and set 3 are
all located in the middle positions with less heat dissipating abilities. On the other
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 11
address lines connected to BA 0 and BA 1 doubles the mapping space. Table in
Figure 10(b) enumerates all mappings supported by the proposed circuit. The ﬁrst
column of the table speciﬁes whether the address lines are swapped, and the second
and third columns represent whether INV BA 0 and INV BA 1 are set to 1 or 0.
The forth column gives the address of each bank after re-mapping. Though only one
thirds of all possible mappings are supported by our proposed circuit, it is suﬃcient
to implement most of desired conﬁgurations. Figure 10(c) shows the settings for
Conﬁgurations I & II as examples.
Next, we should determine the cost of each conﬁguration under diﬀerent access
frequency to each set. In each conﬁguration and in each set, we deﬁne the relation
between temperature and access frequency by simulation. This relation can be used
to determine the cost of mapping a memory segment with given access frequency to
a set. For a set, the average power is deﬁned as follows. First, the access to memory
is divided to read access and write access. And operating power in Equation (1)
considers diﬀerent ratios of read and write access where 𝛼 represents the ratio of
read access to total access and (1− 𝛼) the ratio of write access to total access.
𝑃𝑜𝑤𝑒𝑟𝑂𝑝𝑒𝑟𝑎𝑡𝑖𝑛𝑔 = 𝑃𝑜𝑤𝑒𝑟𝑅𝑒𝑎𝑑 × 𝛼+ 𝑃𝑜𝑤𝑒𝑟𝑊𝑟𝑖𝑡𝑒 × (1 − 𝛼) (1)
Next, with diﬀerent access frequency to a 𝑠𝑒𝑡, 𝑓 , Equation (2) is deﬁned for the
average power.
𝑃𝑜𝑤𝑒𝑟𝐴𝑣𝑔 = 𝑃𝑜𝑤𝑒𝑟𝑂𝑝𝑒𝑟𝑎𝑡𝑖𝑛𝑔 × 𝑓 + 𝑃𝑜𝑤𝑒𝑟𝑆𝑡𝑎𝑛𝑑𝑏𝑦 × (1− 𝑓) (2)
Finally, the simulation of each set is done as follows. For each 𝑓 , the average power
is calculated. Then, the hardware blocks of the target set for simulation are set with
the average power while all other blocks with standby power. Next, thermal simu-
lation tool is called to obtain the steady state temperature. In this paper, HotSpot
4.0 [16] is used as our thermal simulation tool. The temperature obtained will be
used to evaluate the eﬀect of mapping a memory segment with access frequency 𝑓
to a set. Notice that this temperature computed may be underestimated since all
other surrounding blocks are assumed to be idle. That means, the interaction eﬀect
of blocks in the model is ignored. However, this underestimation is acceptable since
the temperature can still reﬂect the thermal behavior of a set under a given access
frequency. We use the term,
𝑇 (𝑗, 𝑓)
to represent the steady state temperature when set 𝑗 is accessed with frequency 𝑓 .
This term will be used to deﬁne the cost function in Section 4.3.
4.2 Application Behavior Analysis
For each program runs on the system, the memory requirement is varying over the
time. We can partition a program’s logical address space to a number of segments
each with diﬀerent access frequencies and then based on access frequency, map
each segment to diﬀerent physical locations in a 3D memory to minimize maximum
temperature.
An algorithm, Behavior Analysis Algorithm, is developed for this purpose as
shown in Figure 11. First, proﬁling of memory references for application programs
is recorded. For each cycle, whether memory is accessed and if yes, which memory
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 13
1 Algorithm : Program Behavior Analysis Algorithm()
2 Input : Memory reference record
3 Output : Memory segments
4
5 𝑐𝑜𝑢𝑛𝑡𝑒𝑟 = 0;
6 While(end of record is not reached)
7 {
8 𝑟𝑒𝑓 = ReadNextReference();
9 If(𝑟𝑒𝑓 is TRUE)
10 {
11 𝑠𝑒𝑔𝑚𝑒𝑛𝑡 = FindSegmentFor(𝑟𝑒𝑓);
12 If(𝑠𝑒𝑔𝑚𝑒𝑛𝑡 == NULL)
13 CreateNewSegmentFor(𝑟𝑒𝑓);
14 Else
15 AddInfoTo(𝑠𝑒𝑔𝑚𝑒𝑛𝑡, 𝑟𝑒𝑓);
16 }
17 𝑐𝑜𝑢𝑛𝑡𝑒𝑟++;
18 If(𝑐𝑜𝑢𝑛𝑡𝑒𝑟 == 𝑝𝑒𝑟𝑖𝑜𝑑)
19 {
20 𝑐𝑜𝑢𝑛𝑡𝑒𝑟 = 0;
21 UpdateAllSegments();
22 MergeNeighboringSegmentsWithSimilarBehavior();
23 }
24 }
25 UpdateAllSegments();
26 MergeNeighboringSegmentsWithSimilarBehavior();
Fig. 11. Algorithm for Behavior Analysis Algorithm
subject to
∑
𝑥
𝐶𝑜𝑛𝑓𝑖𝑔𝑥,𝑦 = 1, ∀ 𝑦 (4)
∑
𝑥,𝑦,𝑧
𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑖,𝑆𝑒𝑡𝑥,𝑦,𝑧 = 1, ∀ 𝑖 (5)
UNIT SET SIZE× 𝐶𝑜𝑛𝑓𝑖𝑔𝑥,𝑦 = 𝑆𝑒𝑡𝑆𝑖𝑧𝑒𝑆𝑒𝑡𝑥,𝑦,𝑧 ,
∀ 𝑥, 𝑦, 𝑧 (6)
∑
𝑖 𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑆𝑖𝑧𝑒𝑖× 𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑖,𝑆𝑒𝑡𝑥,𝑦,𝑧 ≤ 𝑆𝑒𝑡𝑆𝑖𝑧𝑒𝑆𝑒𝑡𝑥,𝑦,𝑧 ,
∀ 𝑥, 𝑦, 𝑧 (7)
The 𝐶𝑜𝑠𝑡𝑖,𝑆𝑒𝑡𝑥,𝑦,𝑧 (the detail of computing 𝐶𝑜𝑠𝑡𝑖,𝑆𝑒𝑡𝑥,𝑦,𝑧 will be explained later)
represents the temperature cost when 𝑆𝑒𝑔𝑚𝑒𝑛𝑡 𝑖 is mapped to 𝑆𝑒𝑡𝑥,𝑦,𝑧. The ob-
jective is to minimize the mapping cost. Equation (4) guarantees each group has
exactly one conﬁguration. Equation (5) is required to make sure each segment maps
to only one set. Equation (6) ensures that if 𝐶𝑜𝑛𝑓𝑖𝑔𝑥,𝑦 = 1 (conﬁguration 𝑥 is se-
lected for 𝑦 group), the size of all sets under 𝑥 conﬁguration for 𝑦 group is equal to
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 15
Table I. Parameters for Experiments
System Parameter Value
#𝑡𝑖𝑒𝑟 8
#𝑑𝑖𝑒 𝑜𝑛 𝑡𝑖𝑒𝑟 2
#𝑏𝑎𝑛𝑘 4
#𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ 𝑑𝑖𝑒 8
#𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ 𝑠𝑦𝑠𝑡𝑒𝑚 32
DRAM Chip Parameter Value
Capacity 512 Mb
Internal Clock Rate 200 MHz
Voltage 2.5 V
Max. Power 0.75 W
Standby Power 0.34 W
Total Memory Size 1 GB
L1 Cache Architecture
Single-Core
Size: 512B
Organization: Direct-Mapped
Line Size: 16-byte
(32 Sets)
Multi-Core
Size: 4KB/8KB
Organization: Direct-Mapped
Line Size: 32-byte/64-byte
(128 Sets)
Power Values (mW)
Mode Sense Ampliﬁer Cell Bank Control & Pre-charge
Standby 36.5 18.2 121.2
Active Read 186.4 94.9 211.6
Active Write 217.6 114.6 245.7
5. EXPERIMENTAL RESULTS
In this section, experimental results for diﬀerent execution conditions are presented.
The system parameters are listed in Table I where the power values are estimated
based on the 𝐼𝐷𝐷 values of a Hynix DDR400 512 Mb SDRAM chip [17]. We
assume the system supports multiprogramming with Round-Robin scheduling and
all programs run on the system are pre-loaded to memory. In Section 5.1, the
program set is composed of MediaBench [18], PowerStone [19] benchmark suites and
JM H.264/AVC CODEC [20]. The programs are duplicated to multiple instances
to simulate systems with diﬀerent memory utilization ratio. For experiments in
Section 5.2, programs from SPEC CPU2000 benchmark suite are also included.
SimpleScalar 3.0 [22] is used to generate memory reference records. lp solve 5.5
[23] is used as our ILP solver. HotSpot 4.0 [16] is used as our thermal simulation
tool. To demonstrate the eﬃciency of our method, two straightforward mappings
are tested for comparison. The ﬁrst one selects 4 DRAM dies at the same relative
positions of 4 consecutive tiers as a group and the second selects all 4 dies of 2
consecutive tiers as a group. Notice that no additional re-mapping circuits are
added in these two mappings and therefore stacking eﬀect among banks cannot
be avoided. These two mappings are referred as M 1 and M 2 while our proposed
mapping is referred as M ours in the following discussion.
In Section 5.1, a single-core system with 512B cache size is used to examine the
eﬀectiveness of our proposed memory mapping ﬂow. Next, in Section 5.2, a multi-
core system with 4KB/8KB cache size is used for experiment. For both experiments
in Section 5.1 and Section 5.2, the cache architectures are direct-mapped and the
value of 𝐿 is set to 12. The tradeoﬀ between the value of 𝐿 and the quality of
mapping result is discusses in Section 5.3.
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 17
Table II. Workload Combinations
Core 1 Core 2 Core 3 Core 4
W1 MediaBench PowerStone JM H.264 gzip
W2 MediaBench + PowerStone JM H.264 gzip gcc
W3 JM H.264 gzip gcc mcf
near saturated frequency and increasing the clock rate of processor will only lead
to limited increase in access frequency. However, for segments accessed with low
frequency, the increase in access frequency will be proportional to the increase ra-
tio of processor’s clock rate. Since maximum temperature is usually observed on
tiers with less heat dissipating ability and M ours maps segments with low access
frequency to these tiers, access frequency of these tiers is increased signiﬁcantly as
compared to other tiers. Therefore, M ours cannot provide the same temperature
reduction when clock rate of processor is increased. Still, our method reduces the
temperature by 14.5∘C and 11.9∘C as compared to M 1 and M 2 (75%, 800 MHz).
Finally, when the clock rate of processor is set to 1.2 GHz under 95% memory uti-
lization (the rightmost column), the temperature is reduced by 12.2∘C and 9.3∘C
as compared to M 1 and M 2 (95%, 800 MHz). Also, notice that in all experiments,
M 2 is consistently better than M 1, which conﬁrms our observations to form dies
in adjacent tiers in a group.
5.2 Experiments for Multi-Core System
A multi-core system is assumed in this section. The objective of experiments in this
section is to demonstrate the eﬀectiveness of our proposed method when the mem-
ory system is accessed by multiple cores running diﬀerent applications. Increasing
the number of cores leads to higher access rate to memory system. To make the
overall access rate to the memory system more reasonable, the cache memory size
(L1 Cache) of each core is increased to 4KB in the ﬁrst experiment of this section.
In this paper, the applications run on diﬀerent cores are assumed to be independent
to each other and hence data consistency is not considered. For each core, a set
of programs are assigned as its workload. To avoid the memory access behavior of
each core to be similar, 3 more programs (gcc, gzip, mcf) from SPEC CPU2000
are added to our program set. In terms of program behavior, these three programs,
as well as JM H.264/AVC CODEC, are much more complicated than other pro-
grams in our program set. Therefore, when a core is assigned with one of these
programs as its workload, it will not be assigned with any other programs. Also
note that these four complicated programs contain many consecutive memory reads
and writes. Increasing the number of cores executing these complicated programs
will largely increase the number of segments with high access frequencies. Since
SimpleScalar does not support multi-core simulation, the memory access behavior
of each core is ﬁrst recorded individually. Then, the memory access behaviors of all
cores are multiplexed to create the workload to the memory system. The system is
set to contain 4 cores, and three types of workloads, 𝑊1, 𝑊2 and 𝑊3, are used in
the experiments as listed in Table II.
The experimental results are shown in Figure 13. In these experiments, the fre-
quencies of the cores are set to 800 MHz. Similar to Section 5.1, for each workload
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 19
M_1
M_2
Fig. 15. The Highest Temperature for Diﬀerent Values of 𝐿
each core. When L1 cache size of each core is set to 8KB, the experimental results
are shown in Figure 14. Similar to Figure 13, the maximum temperature of the
system increases and the average improvements degrades as the workload combina-
tion contains more complicated programs. However, the improvement degradation
becomes more stable. The average improvements of our proposed mapping ﬂow for
W1, W2, and W3 are 13.58∘C, 11.23∘C, and 9.88∘C respectively. The experiments
indicate that the design of cache architecture is very important to 3D memory
designs since it directly aﬀects the memory access behavior.
5.3 Experiments for Segment Merging
In this section, the tradeoﬀ between the value of 𝐿 and the quality of mapping
result is discussed. The single-core system model introduced in Section 5.1 is used
in this section. The frequency of the processor and memory utilization ratio are set
to 800 MHz and 75% respectively. Diﬀerent values of 𝐿 are tested. Experiments are
performed on a Linux workstation with Intel Pentium 4 3.4 GHz CPU and 2 GB
memory. For diﬀerent values of 𝐿, the highest temperature and the computation
time are summarized in Figure 15 and Table III.
Two dash lines which represent the highest temperatures of M 1 andM 2 (straight-
forward mappings) are depicted in Figure 15 for comparison. When the values of 𝐿
are set to 4 and 6, little improvement is observed. This is because the granularity of
the merging scheme is too coarse-grained. Segments that have high access frequen-
cies are merged with segments with relatively low access frequencies. Our proposed
mapping ﬂow can not accurately map all segments that have high access frequencies
to physical locations with better heat dissipation. Therefore, the improvement is
limited. As the value of 𝐿 increases, only segments with less diﬀerence in access
frequency are allowed to be merged. Better mapping results can be obtained. When
the value of 𝐿 is set to 12, segments are merged with suﬃciently ﬁne granularity.
According to Figure 15, increasing the value of 𝐿 to be greater than 12 results in
ACM Journal Name, Vol. V, No. N, Month 20YY.
Preparing Articles for the ACM Transactions ⋅ 21
Table III. Computation Time
L #segment #var. Computation Time
4 93 2980 9 min 27 sec
6 102 3268 12 min 3 sec
8 117 3748 18 min 46 sec
10 133 4260 25 min 19 sec
12 157 5028 33 min 54 sec
14 179 5732 45 min 12 sec
16 204 6532 1 hr 3 min 7 sec
6. CONCLUSION
In this paper, we have proposed a static thermal management scheme for DRAM
dies in stacked 3D designs. Both physical and software level issues are considered
in our method. In physical level, the ﬂoorplan of DRAM die and power behavior
of bank access are analyzed to generate candidate conﬁgurations. In software level,
the memory space of the programs run on the system are partitioned to segments
based on access frequency. The conﬁguration decision and the mapping segments
to physical locations are formulated as an ILP problem. For single-core systems,
experiments show that our method can reduce temperature of memory system by
17.1∘C as compared to a straightforward mapping in the best case, and 13.3∘C
in average. For systems with 4 cores, the temperature reductions are 9.9∘C and
11.6∘C in average when L1 cache of each core is set to 4KB and 8KB, respectively.
REFERENCES
K. L. Tai, “System-In-Package (SIP): Challenges and Opportunities,” Asia and South Paciﬁc
Design Automation Conference, pp. 191-196, 2000.
Alexandru Pancescu, “Hynix Storms The NAND Industry - 24 nand memory chips only 1.4mm
thick,” SOFTPEDIA, Sep. 7, 2007.
K. L. Tai, R. C. Frye, B. J. Han, M. Y. Lau, and D. Kossives, “A chip-on-chip DSP/SRAM
multichip module,”Int’l Conf. on Multi-chip Modules, pp 466-471, 1995.
Y. L Low, R.C Frye, and K. J OConner, “Design methodology for chip-on-chip applications,”
IEEE Trans. on Components, Packaging, and Manufacturing Technology Part B, vol. 21, pp.
298-301, Aug. 1998.
M. X. Wang, K. Suzuki, W. Dai, Yee L. Low, K. J. Oconner and K. L. Tai, “Integration of
Large-Scale FPGA and DRAM in a Package Using Chip-on-Chip Technology”, Asia and South
Paciﬁc Design Automation Conference, pp. 205- 210, 2000.
Michael Wang, Katsuharu Suzuki, Wayne Dai, Atsushi Sakai, Kiwamu Watanabe, “Conﬁg-
urable Area-IO Memory for System-in-a-Package (SiP),” 27th European Solid-State Circuits
Conference, September, 2001.
Michael Wang, Katsuharu Suzuki, Wayne Dai, “Memory and Logic Integration for System-in-
a-Package,” 4th Int’l Conf. on ASIC, October, 2001.
Kiran Puttaswamy and Gabriel H. Loh, “Thermal Analysis of a 3D Die-Stacked High-
Performance Microprocessor,” ACM/IEEE Great Lakes Symposium on VLSI, pp 19-24, 2006.
Y. I. Kim, K. H. Yang, W. S. Lee, “Thermal Degradation of DRAM Retention Time: Charac-
terization and improving techniques,” Proceedings of the 42nd IEEE Int’l Reliability Physics
Symp., pp. 667-668, April 2004.
D. Brooks and M. Martonosi, “Dynamic Thermal Management for High-Performance Micropro-
cessors,” Proceedings of the Seventh International Symposium on High-Performance Computer
Architecture, February 2001.
K. Skadron, T. Abdelzaher and M. R. Stan, “Control Theoretic Techniques and Thermal-RC
Modeling for Accurate and Localized Dynamic Thermal Management,” Proceedings of the
Eighth International Symposium on High-Performance Computer Architecture, February 2002.
ACM Journal Name, Vol. V, No. N, Month 20YY.
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                日期：   年   月   日 
一、參加會議經過 
本次會議的目的為報告研究的成果。我們的論文主要是在探討 3D IC redundant TSV 設
計，以解決 TSV failure 的 recovery。除了論文報告外，亦見到許多相關領域教授，討論
甚佳。 
 
二、與會心得 
DATE 會議包含較多 system 及 software 方面的研究領域，其 submission paper 的數目
及參加人數，都大幅成長，今年更超越 DAC 及 ICCAD。參加此會議，其優點是可碰到
較多歐洲的學者，visibility 也越來越高。 
三、考察參觀活動(無是項活動者略) 
四、建議 
五、攜回資料名稱及內容 
六、其他 
 
計畫編號 NSC 98－2220－E－007－014－ 
計畫名稱 
在 System-in-Package 設計下之自動化工具研發─子計畫三：在 SiP
設計下之散熱約束晶片堆疊, Leadframe 繞線及記憶體映成之研
究(3/3) 
出國人
員姓名 黃婷婷 
服務機構
及職稱 清華大學資訊工程學系   教授 
會議時間 99 年 03 月 04 日至99 年 03 月 13 日 會議地點 德國 Dresdon 
會議名稱 (中文) (英文)2010 DATE 
發表論
文題目 
(中文) 
(英文) TSV Redundancy: Architecture and Design Issues 
TSV TSV
TSV TSV
Upper Tier
Lower Tier
Fig. 1. Bonding between TSVs and Bond Pads
for each TSV [9][10]. In addition to misalignments, TSVs
can also fail in the soldering process [11]. For example, short
circuits between two distinct TSVs or open circuits between a
TSV and its corresponding bonding pad may be formed. Other
failure mechanisms such as dislocation, process variations or
mechanical stress also decrease the fabrication yield of TSVs.
Above all, misalignment and failures on bonding are pri-
mary failure mechanisms for TSVs [11]. Both of the tech-
nologies used for alignment and bonding are very similar
to the packaging methods used in current IC industry [5].
Although the exact failure rate of TSVs is still not clear, it
is possible to use the failure rates of alignment and bonding
to perform a failure rate analysis for TSV. Considering the
TSV diameter and the size of bond pads, the failure rate of
a single TSV may ranges between 10−4 and 10−5 based on
current packaging technology. This assumption roughly meets
the yields of TSVs from the process technologies of HRI,
IMEC and IBM [12][13][14].
According to the applications and network styles, the num-
ber of TSVs in each tier can be quite different. For many-core
processors or NoC-based designs, thousands of TSVs may be
required in each tier. On the contrary, hundreds of TSVs may
be sufcient for smaller IP-based designs. In this work, we
focus on IP-based designs where TSVs are mainly used for
connections between modules on different tiers. Considering
the area of bond pads and oorplan problems, we assume that
the number of TSVs to be placed in a tier ranges from 300
to 500.
An analysis between failure rate and yield is given in
Figure 2. Assume that all dies to be stacked are known-good-
dies. Thus, only the failure rate of TSV bonding needs to be
considered. Let f stands for the failure rate of bonding one
TSV and #tier stands for the number of tiers to be stacked.
Note that the actual number of tiers that contain TSVs to be
bonded is equal to #tier - 1. For example, when #tier = 2, only
the top tier contains TSVs to be bonded. The x-axis represents
the number of TSVs to be placed in each tier (#TSV). Since a
good chip stack requires all TSVs to be successfully bonded,
the binding yield can be computed as (1−𝑓)#𝑇𝑆𝑉×(#𝑡𝑖𝑒𝑟−1).
The analysis results for 𝑓 = {0.0001, 0.00002} and #𝑡𝑖𝑒𝑟 =
{2 , 5} are shown in Figure 2. Without any redundant TSVs,
the average yield is 94.35%. And when #𝑇𝑆𝑉 = 500 and
#𝑡𝑖𝑒𝑟 = 5, the yield degrades to 81.8%. Note that dies to
be stacked are all known-good-dies. Therefore, the cost of
discarding chip stacks that are failed due to TSV bonding is
very expensive. In fact, in most failed chip stacks, only a very
80 00%
85.00%
90.00%
95.00%
100.00%
p 
St
ac
ks
 w
ith
 n
o 
Fa
ile
d 
TS
V
f = 0 0001 #tier = 2
70.00%
75.00%
.
300 350 400 450 500
%
of
 C
hi
p 
St
ac
ks
 w
ith
 n
o 
Fa
ile
d 
TS
V
#TSV
  . ,  
f=0.0001,#tier=5
f=0.00002,#tier=2
f=0.00002,#tier=5
Fig. 2. Yield Analysis
small portion of TSVs are failed. If these failed TSVs can be
recovered with circuits of reasonable cost, the yield can be
largely improved. The redundant TSV design to be proposed
in this paper provides a solution to this problem.
III. REDUNDANT TSV ARCHITECTURE
In this section, the architecture of our proposed redundant
TSV design is introduced in Section III-A. Next, a brief
introduction to the oorplan of 3D IC and its relation to our
proposed architecture are given in Section III-B.
A. Architecture Design
The proposed architecture for redundant TSV is depicted
in Figure 3. For each TSV, 2 MUXs are added to shift the
signal to neighboring TSV when one TSV is failed. To reduce
the timing effect caused by the loading capacitance of the
additional wires used for signal shifting, a pair of buffers are
added to each TSV. The TSVs are connected as a chain where
the redundant TSV is placed at the last position of the chain.
When no TSV is failed, all signals are transferred by original
TSVs as shown in Figure 4(a). When a TSV is failed, the
signal of the failed TSV needs to be shifted. This in term
causes all signals between the failed TSV and the redundant
TSV to be shifted. For example, let TSV 1 be failed. The
signal paths after shifting are shown in Figure 4(b). When a
signal is shifted, larger delay is introduced due to larger wire
length and buffers. For signals that are timing critical, this
may become a problem. We will discuss it in Section V-A.
In this architecture, only one failed TSV can be recovered in
each chain. If two or more TSVs are failed in a chain, only
one of them can be recovered. Therefore, how to determine the
number of TSVs in a chain so that an acceptable recovery rate
can be achieved is an important design issue. This issue will
be discussed in Section IV. For simplicity, the term TSV-chain
is used to refer to the structure of the proposed redundant TSV
architecture.
The MUXs in the proposed architecture are connected to an
e-fuse array which can be programmed by a scan-chain. By
default, all signals connect to MUXs are set to 0. When the
testing for TSV connectivity is done, signals are scanned in to
program the e-fuses so that each MUX receives an appropriate
control signal.
TABLE I
𝑃𝑓 𝑡𝑠𝑣=𝑛 AND 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛 WHEN 𝐹 = 0.0001
𝑁 𝑛 𝑃𝑓 𝑡𝑠𝑣=𝑛 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛
300
0 97.0444% 97.0444%
1 2.9116% 99.9560%
2 0.0435% 99.9996%
400
0 96.0788% 96.0788%
1 3.8435% 99.9223%
2 0.0767% 99.9990%
500
0 95.1227% 95.1227%
1 4.7566% 99.8793%
2 0.1187% 99.9980%
Table I shows that, when 𝑛 = 2, the values of 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛 for
𝑁 = {300, 400, 500} are all greater than 99.998%. A smaller
𝐹 will result in a larger 𝐶 𝑅𝑎𝑡𝑖𝑜𝑛. This means, as long as
the failure rate 𝐹 is no greater than 0.0001, the probability
that three or more TSVs are failed is less than 0.002%.
Therefore, when designing TSV-chains, we can assume that
the maximum number of failed TSVs in a tier is 2. This
assumption covers 99.998% of all possible faulty free and
faulty situations.
B. Analysis on Recovery Rate
As mentioned in Section III, each TSV-chain is capable of
recovering at most one failed TSV in a TSV block. As the
number of TSVs in a TSV block increases, the probability
that all failed TSVs can be recovered decreases. To achieve
an expected recovery rate, the number of TSVs in each TSV
block must be limited. To simplify the analysis, we assume
that the number of TSVs in all TSV blocks are identical. Let
#𝐵 𝑇𝑆𝑉 stand for the number of TSVs in each TSV block
and 𝑛 stand for the number of failed TSVs. For a given value
of 𝑛, we want to analyze the relation between #𝐵 𝑇𝑆𝑉 and
recovery rate. The discussion in Section IV-A indicates that
assuming 𝑛 ≤ 2 is sufcient to covers 99.998% of all possible
faulty free and faulty situations. Therefore, we will perform
the analysis for 𝑛 = 1 and 𝑛 = 2 only.
Let 𝑁 stand for the number of TSVs in a tier. The
number of combinations of 𝑁 TSVs with 𝑛 failed TSVs can
be computed as 𝐶𝑁𝑛 . The number of combinations that all
failed TSVs can be recovered by TSV-chains is referred as
#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑡𝑖𝑜𝑛𝑠. The recovery rate discussed
in this section is dened as
#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑡𝑖𝑜𝑛𝑠
𝐶𝑁𝑛
.
When 𝑛 = 1, only one failed TSV needs to be recovered.
Since one TSV block contains one redundant TSV, regardless
of the number of TSVs in each TSV block, one failed TSV can
always be recovered. Therefore, the recovery rate for 𝑛 = 1
is 100%.
The recovery rate analysis for 𝑛 = 2 is more complicated.
Let the term #𝐵𝑙𝑜𝑐𝑘 represent the number of TSV blocks in
a tier. Under our assumptions, #𝐵𝑙𝑜𝑐𝑘 can be computed as
𝑁
#𝐵 𝑇𝑆𝑉 . To successfully recover all failed TSVs, each failed
TSVs must be located in different TSV blocks. That is, 𝑛 TSV
blocks are selected from #𝐵𝑙𝑜𝑐𝑘 TSV blocks. Each selected
TSV block contains exactly one failed TSV. The number of
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
ec
ov
er
y 
R
at
e
0.00%
10.00%
20.00%
30.00%
25 26 27 29 31 33 35 38 41 45 50 55 62 71 83 100 125 166 250
R
ec
ov
er
y 
R
at
e
Number of TSVs in a TSV Block
Fig. 6. Recovery Rate when 𝑁 = 500, 𝑛 = 2
combinations that satises this requirement can be computed
as
𝐶#𝐵𝑙𝑜𝑐𝑘𝑛 .
Inside each TSV block that contains one failed TSV, the
failed TSV can be located at #𝐵 𝑇𝑆𝑉 possible positions.
Therefore, the #𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒 𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑖𝑜𝑛𝑠 for 𝑛 = 2 can
be computed as
𝐶#𝐵𝑙𝑜𝑐𝑘2 ⋅ (#𝐵 𝑇𝑆𝑉 )2.
The relation between #𝐵 𝑇𝑆𝑉 and recovery rate for 𝑁 =
500 and 𝑛 = 2 is shown in Figure 6. For different values of
#𝐵 𝑇𝑆𝑉 that result in the same #𝐵𝑙𝑜𝑐𝑘, only the smallest
#𝐵 𝑇𝑆𝑉 is shown in the gure since the recovery rates of
them are the same.1
According to Figure 6, to achieve 90% recovery rate,
#𝐵 𝑇𝑆𝑉 needs to be no greater than 50. By limiting the
number of TSVs in each TSV block to be less than or equal
to 50, the recovery rate is greater than 90%. To achieve a
higher recovery rate, the gure shows that with 95% recovery
rate, the number of TSVs in each TSV block cannot be greater
than 25. In realistic ASIC designs, the number of TSVs in a
TSV block is usually less than 50. Therefore, in most cases,
TSV block partitioning is not required.
A further analysis is to compute the overall yield. In
Table I, when 𝑁 = 500, 𝑃𝑓 𝑡𝑠𝑣=0, 𝑃𝑓 𝑡𝑠𝑣=1, and 𝑃𝑓 𝑡𝑠𝑣=2
are 95.1227%, 4.7566%, and 0.1187%, respectively. The dis-
cussion above indicates that the recovery rate for 𝑛 = 1 is
always 100% based on our proposed architecture. Thus, let
the recovery rate for 𝑛 = 2 be set to 90%. The overall yield
can be computed as
𝑃𝑓 𝑡𝑠𝑣=0+𝑃𝑓 𝑡𝑠𝑣=1×100%+𝑃𝑓 𝑡𝑠𝑣=2×90% = 99.98613%.
This value is high enough for most applications.
V. DESIGN FLOW AND TSV-Chain DESIGN
The discussion in Section IV focuses on the recovery of
failed TSVs in terms of connectivity. Timing issues are not
concerned. As mentioned in Section III-A, when a signal is
shifted in a TSV-chain, extra delay will be incurred. For signals
that are timing critical, the delay caused by signal shifting may
1The actual computation of#𝑅𝑒𝑐𝑜𝑣𝑒𝑟𝑎𝑏𝑙𝑒𝐶𝑜𝑚𝑏𝑖𝑛𝑎𝑖𝑜𝑛𝑠 is a little more
complicated since sizes of TSV blocks may differ by one due to the division
operation to compute #𝐵𝑙𝑜𝑐𝑘
Corner of a TierBoundary of a Tier
(a) Spiral-Style (b) Snake-Style (c) Hybrid
Fig. 9. Chaining Styles
probabilities to be routed through for minimum wire length.
These TSVs should be assigned as head parts of TSV-chains.
A spiral-style chaining policy is proposed for TSV-chain
design. In a TSV block, by picking a TSV in the central
position to be the starting point, spiral-style chaining results
in a routing path where all TSVs on the boundary are at one
end. The starting TSV is assigned as redundant TSV while the
other end becomes the head of a TSV-chain. An example for a
4 × 5 TSV block is shown in Figure 9(a) where TSVs in grey
are head and good for timing critical signals. In routing stage,
routers can choose to assign timing-critical signals to TSVs
that are on the boundary of a TSV block. This can reduce the
probability for a timing critical signal to be shifted.
The spiral-style chaining policy is appropriate for a TSV
block that is not on the boundary or the corner of a tier.
For a TSV block located on the boundary of a tier, most
signals assigned to that TSV block are connected from the
opposite side of the tier boundary. In this case, a snake-style
chaining policy satises the requirement. The result is shown
in Figure 9(b). For a TSV block located on the corner, most
signals assigned to that TSV block are connected from the
counter direction of the tier corner. In this case, a hybrid
chaining policy as shown in Figure 9(c) becomes the best
candidate. Based on the position of each TSV block, one of
these three chaining policies can be applied to determine the
structure of a TSV-chain.
C. Physical Design Flow Considering TSV-Chain
In current design ow for 3D ICs , 3D partitioning rst
takes place to determine which tier each design blocks to be
placed. The number of required TSVs for signals between two
consecutive tiers is determined in this stage. Next, in oorplan
stage, blocks with x area but unknown dimensions are placed
in each tier. To provide communication links between blocks
in different tiers, TSV blocks are placed. The number of sig-
nals to be assigned to each TSV block as well as the position
of each TSV block are roughly determined in this stage. Based
on the discussion in Section IV, the number of TSVs in each
TSV block should be limited. Partitioning may be required
for large TSV blocks. Based on the position of each TSV
block, the structure of each TSV-chain is determined. In place
and route stage, routers should be aware of the TSV-chain
structure in each TSV block. Based on design constraints and
requirements, router needs to decide whether to assign timing
critical signals to TSVs that are located at the head of TSV-
chains. The overall design ow for TSV-chain is shown in
Figure 10.
3D Partitioning:
TSVs required for signal on each tier is determined
Placement
3D Routing:
Assignment of signals to TSVs
Considering the structure of each TSV-chain
when perform the assignment of signals to TSVs
3D Floorplanning:
TSV Block are determined
1. Partitioning is required for large TSV blocks
2. The size of each TSV block is limited
Based on the position of each TSV block, determine 
the structure of each TSV-chain
Fig. 10. Proposed Design Flow for TSV-Chain
VI. CONCLUSION
In this paper, a new redundant TSV architecture with
reasonable cost for ASICs has been proposed. Design issues
including recovery rate and timing problem have been inves-
tigated. Required modications on the design ow has been
explained. Based on probabilistic models, the new design can
successfully recover most of the failed chips and increase the
yield of TSV bonding to 99.99%. This can effectively reduce
the cost of manufacturing 3D designs.
REFERENCES
[1] W. R. Davis, J. Wilson, S. Mick, davis demystifying 3D et al.,
“Demistifying 3D ICs: The Pros and Cons of Going Vertical,” IEEE
Design Test of Computer, vol. 22, no. 6, pp. 498-510, Nov./Dec., 2005.
[2] J. Burns, L. Mcllrath, C. Keast, et al., “Three-Dimensional Integrated
Circuit for Low Power, High-Bandwidth Systems on a Chip,” ISSCC
Dig. of Tech. Papers, pp. 268-269, Feb., 2001.
[3] S. Siesshoefer and et al., “Z-axis Interconnect Using Fine Pitch,
Nanoscale Through Silicon Vias: Process Development,” ECTC, 2004.
[4] P. Morrow, M. J. Kobrinsky, S. Ramanathan, et al., “Wafer-Level 3D
Interconnects via Cu Bonding,” Proceedings of the Advanced Metalliza-
tion Conference, pp. 125-130, 2004.
[5] Philip Garrou, Christopher Bower, and Peter Ramm, “Handbook of
3D Integration: Technology and Application of 3D Integrated Circuits
Volume 1 & 2,” poblished by WILEY-VCHVerlag GmbH& Co. KGaA,
Weinheim, 2008, ISBN: 978-3-527-32034-9.
[6] Uksong Kang, Hoe-Ju Chung, Seongmoo Heo, et al., “8Gb 3D DDR3
DRAM Using Through-Silicon-Via Technology,” ISSCC Dig. of Tech.
Papers, pp. 130-131, Feb., 2009.
[7] Igor Loi, Subhasish Mitra, Thomas H. Lee, Shinobu Fujita and Luca
Benini, “A Low-Overhead Faule Tolerance Scheme for TSV-Based 3D
Network on Chip Links,” ICCAD, 2008.
[8] Hsien-Hsin S. Lee and Krishnendu Chakrabarty, “Test Challenges for
3D Integrated Circuits,” IEEE Design & Test of Computers, vol. 26, no.
5, pp. 26-35, Sep./Oct., 2009.
[9] R. Patti, “Three-Dimensional Integrated Circuits and the Future of
System-on-Chip Designs,” Proc. of the IEEE, vol. 84, no. 6, June 2006.
[10] A. W. Topol, J. D. C. La Tulipe, L. Shi, et al., “Three Dimensional
Integrated Circuits,” IBM Journal of Research and Development, vol.
50, no. 4/5, pp. 491-506, July/Sepetember 2006.
[11] R. Patti, “Impact of Wafer-Level 3D Stacking on the Yield of ICs,”
Future Lab Intl., issue 23, July 2007.
[12] N. Miyakawa, “A 3D Prototyping Chip based on a Wafer-Level Stacking
Technology,” ASP-DAC, Jan. 2009.
[13] B. Swinnen, W. Ruythooren, et al., “3D Integration by Cu-Cu Thermo-
Compression Bonding of Extremely Thinned Bulk-Si Die Containing
10 𝜇m Pitch Through-Si Vias,” IEDM, Dec. 2006.
[14] A. W. Topol, et al., “Enabling SOI-Based Assembly Technology for
Three-Dimensional Integrated Circuits,” IEDM, Dec. 2005.
98年度專題研究計畫研究成果彙整表 
計畫主持人：黃婷婷 計畫編號：98-2220-E-007-014- 
計畫名稱：在 System-in-Package 設計下之自動化工具研發--子計畫三：在 SiP 設計下之散熱約束晶
片堆疊, Leadframe 繞線及記憶體映成之研究(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 3 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
