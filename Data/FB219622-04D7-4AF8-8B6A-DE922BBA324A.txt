 II
中文摘要 
本子計畫之主要目的在於設計並實現一基於球面超音波馬達之機械眼視覺追蹤系統。
本子計畫之主要訴求有: (一)、克服傳統視覺追蹤系統需搭配使用 Pan-tilt 機構，因體積較
大，較不適用於隱密場合之問題。(二)、球面超音波馬達所建構之伺服機構，具備運轉時安
靜且輸出響應快之特性，可達成即時準確追蹤之目的。(三)、進行多攝影機之即時視覺追蹤
研究，提昇保全監控、遠距教學、醫療照護等應用之技術層次。本子計畫分三年執行，第
一年完成之工作項目計有： 
(一)、以適應性背景消除法與無母數背景模型進行目標物偵測，並針對背景模型之更新 
與維護等相關技術進行研究與探討。 
(二)、不同光源條件對目標物偵測結果強健性之探討。 
(三)、實現多重資訊融合之相似度量測與樣板更新技術於動態視覺追蹤模式。 
(四)、將前述研究成果應用於一 Pan-tilt 視覺追蹤系統，以驗證系統效能。 
第二年完成之工作項目計有： 
(一)、對所偵測出之目標物進行分類及動作分析。 
(二)、探討不同位置預測器之移動目標物追蹤效果。 
(三)、配合其他子計畫，將一袖珍型之針孔 CCD 攝影機架設於以壓電致動之線性滑軌 
機構上，完成各項視覺追蹤實驗之整合測試。 
第三年完成之工作項目計有： 
(一)、利用改良式適應性背景相減法偵測各台攝影機影像中的移動物體，並利用多重相
似度量測法追蹤特定目標物。 
(二)、使用極線幾何限制及色彩長條圖相似度量測法來求得目標物的對應位置。 
(三)、將一袖珍型之針孔 CCD 攝影機架設於以二維壓電致動平台上，並配合其他子計
畫完成各項視覺追蹤實驗之整合測試。 
關鍵詞：移動物偵測、物件追蹤、無母數背景模型、物件分類、動作分析、改良式適應性
背景相減法、極線幾何限制、多攝影機 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 IV
目錄 
一 前言............................................................................................................................................1 
二 研究目的………........................................................................................................................3 
三 文獻回顧....................................................................................................................................3 
四 研究方法與步驟........................................................................................................................6 
子題一 改良式適應性背景相減法..........................................................................................6 
物件標示............................................................................................................................8 
重疊分類法…....................................................................................................................9 
輪廓淬取………..............................................................................................................10 
前景相似度量測…..........................................................................................................10 
靜止物體之背景更新......................................................................................................12 
子題二 極線幾何限制及色彩長條圖相似度量測法............................................................14 
五 結論與成果..............................................................................................................................17 
三攝影機系統之相似對應物區分實驗..................................................................................24 
結論..........................................................................................................................................27 
六 計畫成果自評..........................................................................................................................28 
七 這三年基於本計畫成果所發表之論文..................................................................................28 
八 參考文獻..................................................................................................................................29 
九 可供推廣之研發成果資料表..................................................................................................33 
附錄一 出席國際學術會議報告……..........................................................................................35 
 2
 
在第二年度計畫中的靜態視覺追蹤方面，本子計畫使用前景相似度與改良式適應性背
景更新法克服光源變化的問題。另外所使用之分類演算法亦能夠以自主式的方式分割類別
群聚中心與特徵向量，並有效地對移動目標物進行分類甚至動作分析。另外在動態視覺追
蹤方面，本子計畫使用Modified Filter位置預測演算，可更加準確地預測出目標物所在之位
置，並且能有效地克服視覺回授訊號延遲之問題。配合其他子計畫方面使用一由壓電致動
之線性滑軌來帶動安置在機構上的袖珍型CCD針孔攝影機(圖 2)，並完成各項視覺追蹤測
試。 
 
 
   圖 2. 壓電致動線性滑軌機構 
 
在計畫第三年度我們透過多台不同視角之攝影機，讓系統能夠即時且自動地對場景中
的多個移動物進行偵測，並追蹤特定目標物，以解決單台攝影機拍攝視野有限之問題。本
子計畫首先利用「以物件為基礎之改良式適應性背景相減法」偵測移動物體，並利用「多
重影像特徵相似度比對法」及「三步搜尋法」來對特定目標物做追蹤。而在多攝影機之物
件對應方面，本子計畫利用「極線幾何限制」及「色彩長條圖比對法」求得特定目標物在
其他攝影機中的對應位置，當使用兩台攝影機時，系統會透過「色彩長條圖相似度」及「反
射極線驗證法」求得「最佳候選對應物」；而當使用三台攝影機以上時，系統能夠透過「對
應極線之交點」求得「確切對應物位置」。接著，利用本子計畫所提出的「多攝影機溝通合
作策略」，系統能夠依照不同攝影機的拍攝狀況或物件追蹤狀況而扮演不同角色。當攝影機
能夠觀測得到目標物，並對目標物進行靜態追蹤時，該攝影機則扮演「主控型攝影機」，其
餘能夠觀測得到對應物者則扮演「協助型攝影機」，而無法觀測到對應物或無法追蹤到目標
物者則扮演「被控型攝影機」。系統透過主控型攝影機及協助型攝影機便可以求得目標物在
被控型攝影機中的對應位置，如此系統也能夠估測被遮蔽之對應物位置。而透過攝影機的
溝通合作機制，系統能夠依據目標物追蹤效果及拍攝視野，即時且自動地更換「主控型攝
影機」，利用多攝影機「交接切換」及「交錯式樣版更新」的方式，系統能夠持續地追蹤到
目標物，以解決追蹤目標物被遮蔽的問題，並提升目標物追蹤效果。另外，系統除了能夠
執行物件對應模式，也可以依照使用者需求，以多工運作的方式，讓各台攝影機執行不同
工作，因此，有些攝影機可以執行「移動物偵測」，有些攝影機則可以執行「目標物追蹤」，
而不同攝影機也能夠執行不同目標物的追蹤。實驗結果顯示利用本子計畫所提出的物件對
應法及多攝影機溝通合作策略所設計之即時視覺追蹤系統能夠在複雜的環境下正確地追蹤
特定目標物，求得對應物的位置，並解決目標物或對應物被遮蔽的問題。另外在配合其他
子計畫方面，本子計畫將一針孔CCD攝影機架設於以二維壓電致動平台上，並完成各項視
覺追蹤測試(圖 3)。 
 4
有諸多缺失，因此必須搭配其他方法才能讓對應性更加強健。 
而所謂的「特徵點比對法」則是基於幾何限制條件來得到多張影像間的對應關係，也
可視之為「以幾何條件為基礎的方法（geometry-based method）」。此類方法有些必須先完成
攝影機校正，才能求得對應關係，例如將影像上的所有特徵點背投影至三維空間座標中再
比對特徵點，因為理論上正確的影像對應點經過背投影至三維空間座標後應該是同一點
[13,14]。另外也可以使用「極線幾何」或是「平面投影限制」等幾何限制條件
[9,15,16,17,18,19,20,21,22,23,24]來完成影像點的對應。然而，本方法因為需要透過特徵點來
建立對應關係，故特徵點的選取將變得格外地重要，而有許多學者也針對特徵點的自動搜
尋與對應提出不錯的方法[25,26,27,28,29]。由以上可知，面對「多攝影機影像對應問題」之
解決辦法大多都圍繞在上述兩大類，然後再以不同的特徵或幾何限制來建立不同影像間的
對應關係。 
Qrwell 等人[11]利用色彩長條圖來比對多攝影機影像之移動物（行人）。Krumm 等人[10]
利用兩組可得到深度資訊之立體攝影機（stereo camera），每組攝影機含有三個小型攝影機
鏡頭，在一個小客廳裡對多個人進行靜態追蹤，透過立體攝影機對移動人體做定位，並以
色彩長條圖比對法來完成不同攝影機影像中之人物對應關係。Zhou 及 Aggarwal[30]融合多
種特徵，在室外場景中對多個目標物做靜態追蹤，他們結合了物體的空間位置、外型及顏
色資訊來追蹤移動目標物，透過各台攝影機中的移動物路徑並結合擴展式卡爾曼濾波器
EKF（Extended Kalman Filter）來解決物體遮蔽與耦合的問題。Mittal 等人[17,18]先將多台
攝影機校正過，並利用多台攝影機影像之區域比對法-高斯色彩模型（Gaussian color model）
來找到物體的三維座標，再利用貝式像素點分類法（Bayesian classification）及遮蔽耦合分
析（occlusion analysis）技術來分割移動物體（segmentation），並結合卡爾曼濾波器（Kalman 
Filter）來完成靜態追蹤。Chang 等人[31,32]則是以貝式網路（Bayesian network）結合極線
幾何（epipolar geometry）、地標、顏色及高度資訊來完成雙台攝影機之對應關係及靜態追
蹤。 
Chai 與 Aggrtrwal[20]將相鄰的攝影機做校正，得到相對的關係後，利用移動物的特徵
比對及極線限制（epipolar constraint）來求得對應關係。Utsumi 等人[33,14]利用移動物的中
心作為特徵點，並估測各個中心點在世界座標系（world coordinate system）的三維座標，
來建立多攝影機之間的對應關係，對多人進行靜態追蹤，並自動地選擇較佳視角，來解決
人與人之間互相遮蔽的問題。Chen 及 Wang[34]藉由同一個水平面上的物體特徵（角度或長
度）來估測出多台攝影機架設的高度與角度，再利用估測出的攝影機高度與角度，比較一
些向量在共同世界座標系上的背投影，來校正多攝影機影像之間的對應關係，其特色是在
整個校正過程中不需要透過特殊的校正板及平面轉移矩陣（homography matrix）。Black 及
Ellis[35]基於地面之平面幾何限制（ground plane homography constraint），並分析投影轉換
之誤差，求得多攝影機之間的對應關係，而透過不同方位之攝影機資訊，建立一個三維的
地理環境。他們除了利用背景相減法偵測移動物外，也利用線性卡爾曼濾波器（Linear 
Kalman Filter）對移動目標物進行靜態追蹤，解決移動物互相遮蔽、耦合與分離後身份辨識
之問題。Hu 等人[9]利用移動人物之中心垂直主軸（principal axis），基於移動物體在同平
面上之幾何限制關係來求得多目標物對應關係，並改善靜態追蹤下多目標物暫時群聚在一
起而無法分割的問題。Khan 等人[19]將多台攝影機之間的視野估測並劃分出來，使得系統
可以從各台攝影機之畫面中得知其他攝影機在本台攝影機裡的拍攝範圍，然後基於共同地
面之平面幾何限制（homography constraint），利用多人的腳底位置求得對應關係，此方式
可以估測出某攝影機畫面中的目標物是否也會出現在其他攝影機畫面中。Collins 等人[8]設
計一個自動化的視訊監控系統，利用多台已校正好的攝影機監控整個場景，並透過不同位
置與角度的攝影機畫面，對移動物進行靜態偵測及靜態追蹤，甚至透過類神經網路（neural 
network）將行人與車輛做分類，並利用線性判別分析法 LDA（Linear Discriminant Analysis）
分析出不同車輛的種類，同時也對多攝影機的應用做了詳細的介紹。 
 6
攝影機之間的合作策略與極線驗證法，使各台攝影機能在不同狀況下扮演不同角色，以溝
通合作的方式互相協助，不但可以求得目標物在各台攝影機中的確切對應位置，也可以提
升目標物的追蹤效果，同時亦能夠估測被遮蔽之對應物位置，使系統能夠持續地對特定目
標物進行偵測及追蹤。 
 
四、研究方法與步驟 
子題一:改良式適應性背景相減法 
傳統的背景相減法中，其參考背景是固定的。但在實際應用上，由於環境光源隨時都
在變化，或是原本存在於背景的物體（例如停在路旁的汽車）離開後所造成的背景模型錯
誤等，都會讓系統無法正確地偵測出變化區域及移動物資訊，因此加入適當的背景更新機
制將有助於移動物的偵測。而「適應性背景相減法」便是改善傳統的背景相減法，當背景
發生變化時適當地以漸進方式將背景模型更新，以利實際移動物的偵測。其背景模型之更
新公式如（1.1）式所示。 
 
          1
( , ) (1 ) ( , ),  ( , ) 0
( , ),                                    
t t t
t
t
B u v I u v if M u v
B
B u v otherwise
α α
+
+ − =⎧= ⎨⎩
                       （1.1） 
 
其中α 為一個介於 0 至 1 之間的適應性常數，可調整背景更新的速度，在此假定α 為 0.9。
tM 為使用連續三張影像兩兩相減之結果，也就是運動能量（motion energy），如（1.1 式所
示，以目前的影像 tI 分別與前一刻影像 1tI − 及前兩刻影像 2tI − 做比較，其目的是判斷連續畫
面中的亮度變化是否夠劇烈，假使變化夠大的話，才視該像素點擁有足夠的運動能量。 
 
1 21, ( , ) ( , )   ( , ) ( , )  ( , )
0,                                                                   
t t t t
t
if I u v I u v and I u v I u v
M u v
otherwise
ε ε− −⎧ − > − >= ⎨⎩        （1.2） 
              
而從（1.1）式及（1.2）式中可以發現，當連續影像中的亮度變化較大者（ 1tM = ），
系統將不會更新背景，而亮度變化較小者，則會以漸進的方式將目前影像更新至背景模型
中。如此一來，目前影像中的背景影像如果有些微亮度變化者，則會逐漸地被更新至背景
模型中，如此可以改善傳統背景相減法因背景光源變化而導致偵測錯誤的問題，其適應性
背景相減法之運作流程如圖 4 所示。 
雖然適應性背景相減法的偵測效果較傳統的背景相減法好，但由於此方法在判斷背景
是否要更新時是以連續三張影像之運動能量法作為依據，故當物體移動緩慢時，利用運動
能量法所偵測出的變化區域將不足以涵蓋整個目標物，導致移動目標物之部分區域因變化
量太小而被更新至背景模型中，造成錯誤的背景更新。甚至當目標物暫時停止時，透過運
動能量法也會因為偵測不出劇烈的亮度變化而使得整個目標物被更新至背景模型中。如圖
5 所示，當影像中的目標物（紅色巴士）移動緩慢時，透過連續三張影像之運動能量法所
得的 tM 會因為變化區域太少，使得大部分的目標物區域都會錯誤地被更新至背景中。 
 
 8
 
（d） 1 2t tI I ε− −− >  （e） 1t tI I ε−− >  （f） tM  
圖 5. 適應性背景相減法無法偵測移動緩慢的物體之示意圖 
 
「適應性背景相減法」雖然改善了傳統背景相減法的缺點，但對於移動緩慢或暫時停
止的目標物依然會因為背景更新錯誤而導致偵測失敗。另外，「有母數背景相減法」[50,51,52]
與「無母數背景相減法」[53,54]則是以統計的方式，觀察一個完全靜止的畫面一段時間後，
透過該靜止畫面中每一點像素之亮度變化求得一正規分佈模型，再依據此模型建立一個良
好的背景，並適時地更新背景模型。雖然其效果很好，但由於運算量過大，且相關參數及
閥值不易選取，故本子計畫採用「改良式適應性背景相減法」[55]來做移動物的偵測。 
「改良式適應性背景相減法」是一個「以物件資訊為基礎（object-based）」的背景模型
更新法，也就是紀錄並分析連續影像中所有移動物的狀況，來判斷影像中哪些像素點需要
被更新至背景模型中，如此可以有效地改善適應性背景更新法在某些情況下會有更新錯誤
的問題。改良式適應性背景相減法主要有下列四個優點： 
(1) 利用改良式適應性背景相減法中的「重疊分類法」及「前景相似度量測法」[55]，
除了可以區別目前影像中的移動目標物及真實背景外，更可以將靜止目標物區分
為靜止前景與靜止背景，其詳細內容將於後續做介紹。 
(2) 可以透過計數器來紀錄所有暫時靜止的目標物停留時間，當目標物暫時停止的時
間超過預定的閥值時，則將該靜止目標物像素資訊更新至背景模型中。 
(3) 根據物件的「前景相似度量測值」，可以決定不同目標物計數器的累加速度。例如
當物體的前景相似度較低時，代表該物體較不屬於前景，其計數器的累加速度則
比較快，故該物體將會以較短的時間更新至背景中。 
(4) 改善傳統背景相減法因環境光源變化而造成錯誤偵測之問題，同時也解決適應性
背景相減法無法處理移動緩慢或暫時停止的目標物之問題。此法不但降低了錯誤
偵測的情形，其方式較簡易，運算量較低，利於即時視覺偵測系統之實現。 
 
關於「改良式適應性背景相減法」之詳細內容將於下列數小節做介紹。 
 
物件標示 
由於改良式適應性背景相減法是以物件資訊為基礎，分析各個物件的狀況來決定該物
體為移動目標物或是靜止目標物，並透過各個物體的停留時間來決定背景的更新狀況。因
此，為了紀錄不同物體的資訊，必須先對所求得的 Blob 像素點做物件標示。而在本子計畫
中則是先使用連續標示法（sequential labeling）[56]將 Blob 影像進行物件標示，其標示流程
 10
將 Blob 區分為移動目標物或靜止目標物後，接下來要做的是將各個移動物體 Blob 的
外部輪廓淬取出來，本子計畫所使用的方式為邊界描繪法（border tracing）[57]。其概念是
由影像的左上角往右下角逐點搜尋，找到第一個物件 Blob 像素作為起始點，並以該起始點
為中心，透過搜尋方向之演算策略，搜尋並描繪整個物件 Blob 的邊界，如圖 7 所示。 
 
 
圖 7. 邊界描繪示意圖 
 
前景相似度量測 
在重疊分類法中，所有物件 Blob 已經被分類為「移動目標物」與「靜止目標物」，而
接下來便是要透過「前景相似度量測法」將「靜止目標物」區分為「靜止前景（static 
foreground）」及「靜止背景（static background）」。所謂「靜止前景」指的是移動目標物在
靜止後遮住原本該區域之背景影像所造成的靜止 Blob；而「靜止背景」則是指原本被目標
物遮住，卻因為目標物離開該區域後所顯露出來的真實背景。 
因此，前景相似度量測之目的是判斷目前影像 It中的物件 Blob 是屬於前景或是背景，
而其判別的方式是將物件 Blob 的外部輪廓與影像邊緣做比較，計算其契合度。因為當前景
物體遮住真實背景時，該區域的亮度及邊緣都會有所變化，而在經過背景相減法後，這些
變化將被偵測為 Blob，假使目前影像中被標示為 Blob 的區域為前景，則目前影像之亮度梯
度應與 Blob 的外部輪廓相近。因此，基於上述概念，在判別物件 Blob 是屬於影像中的前
景或背景時，可以透過物件 Blob 外部輪廓與影像邊緣的契合度來決定。 
在進行前景相似度量測前，必須先求得 Blob 外部輪廓及影像的邊緣，其中 Blob 的外
部輪廓是以前述的方式來淬取，而目前影像的邊緣則是透過 Sobel 運算子[4]來求得。Sobel
運算子是由兩個3 3× 的遮罩所組成，如圖 8 所示，透過圖 8（a）及圖 8（b）的遮罩可分別
求得影像在水平（horizontal）方向及垂直（vertical）方向的梯度。 
 
 12
Similarity），當前景相似度愈高，則代表所求得的 Blob 在目前影像中愈有可能是前景，反
之則愈有可能是靜止背景。其前景相似度量測法之流程如圖 9 所示，其中 It代表目前影像，
tB 代表背景影像， tb 則為經過背景相減法後所得到的 Blob 影像。 
2
2 2
I
I B
SFS
S S
= +                                                  （1.10） 
 
tI tB
tb IG BG
OEg
IS BS
 
圖 9. 前景相似度量測流程圖 
 
靜止物體之背景更新 
由前述可知，使用適應性背景相減法時，透過較大的適應性常數α來進行背景更新可減
緩背景模型的更新速度，但過慢的更新速度卻會導致背景景物移動後所造成的錯誤偵測時
間過久。因此本子計畫採用「改良式適應性背景更新法」來改善上述之問題。由於改良式
適應性背景相減法是以物件資訊為基礎來決定背景更新的機制，所有被偵測的物體都會給
予一個計數器，以紀錄物體的停留時間，當物體停留時間超過一個閥值時，物體影像會漸
漸地被更新至背景模型中。而根據偵測物體的前景相似度，系統更能夠調整不同物體的計
數器遞增值，當物體的前景相似度增加時，其對應的計數器遞增值將隨之降低，使得物體
影像更新至背景模型的所需時間增加。反過來說，當物體的前景相似度很低時，其對應的
計數器遞增值則會隨之增加，使得該物體影像很快地被更新至背景模型中，如此可有效地
改善適應性背景相減法之錯誤背景更新及錯誤偵測的狀況發生，其背景更新策略之數學表
示式如（1.11）式至（1.13）式所示。 
 
1
( , ) ,  ( , )  
0,                                       
t
t
A u v X if u v static object
A
otherwise+
+ ∈⎧= ⎨⎩                        （1.11） 
 14
子題二:極線幾何限制及色彩長條圖相似度量測法 
首先介紹極線幾何限制，以圖 11 為例，在三度空間中有一點 P ，同時被兩台不同角度
的攝影機所拍攝到， 1C、 2C 分別代表兩台攝影機之投影中心。P 分別投影至兩台攝影機 1C 、
2C 之二維影像平面，各別以 1p 、 2p 表示，而影像平面座標點 1p 、 2p 與空間座標點 P 及兩
相機中心點 1C 、 2C 是共平面的，該平面以π 表示，並稱之為「極線平面（epipolar plane）」。
由圖 11 可以知道， 1p 與 2p 之背投影（back-projection）射線會交於 P ，且這兩條射線與極
線平面π 也是共平面的。而極線平面π 與影像平面 Image 1、Image 2 相交時，會分別得到
線段 1l 及 2l ，也就是所謂的「極線（epipolar line）」。 
 
1C 2C
P
1p
2p
1e 2e
2l
π
1l
 
圖 11. 極線幾何之示意圖 
 
當 1C 與 2C 做連線時，會分別在影像平面 Image 1 及 Image 2 上交於點 1e 及 2e ，此兩點
稱之為「極點（epipole）」，而 2e 便是 1C 在 Image 2 上的投影點， 1e 則是 2C 在 Image 1 上的
投影點， 1C 與 2C 之連線則稱為「基準線（baseline）」。任何一個包含基準線的平面π 皆為一
個極線平面，如圖 12 所示，當世界座標點 P 之位置變化時（由 P 移至 'P ），它的極線平面
會以基準線做旋轉，而所有極線都會交於極點。 
 
1C 2C
2e1e1l
P
'P
1 'l
π
 
圖 12. 極線平面以基準線做旋轉之示意圖 
 
對於極線幾何有一些基本瞭解後，接下來要討論的是影像之間的對應問題，也就是當
兩台攝影機以不同角度拍攝同一場景時，如何在兩張影像中找到對應點。首先以圖 13 來做
說明，假設在三度空間中有一點 P，分別投影至攝影機 1C 、 2C 之影像平面上，L 為一向量，
其方向與攝影機 1C 之主軸（principal axis）平行，也就是與 Image 1 平面垂直。 
 
 16
 
（a）第一張影像之對應點及估測極線 （b）第二張影像之對應點及估測極線 
圖 15. 所有特徵對應點之對應極線繪製結果 
 
色彩長條圖比對法（Color Histogram Matching）是以「目標物樣版之色彩長條圖」及
「要比對的影像區域之色彩長條圖」作為判斷相似度之依據，主要是透過色彩統計的方式，
以色彩分佈來決定其相似度。由於當目標物發生轉動或形變時，其色彩分佈不會有太大的
變化，因此利用色彩長條圖比對法可適用於非剛性物體（non-rigid object）。以 RGB 色彩空
間為例，色彩長條圖則是一張區域影像中，所有點的 R、G、B 分量個別的亮度值統計，如
圖 16 所示，16（a）為一張彩色影像，16（b）為該影像之 R、G、B 分量值統計，其 R、G、
B 分量統計的水平軸為 0~255 亮度值，垂直軸則為各個亮度的數量統計值。 
 
（a）彩色影像 
R 
G 
B 
（b）RGB 色彩長條圖 
圖 16. RGB 色彩空間之色彩長條圖 
 
由於 RGB 色彩空間比較適合電腦表示用，且 RGB 色彩空間對於光源變化之敏感度較
大，故本子計畫採用 YCbCr 色彩空間來表示，其中 Y、Cb、Cr 各代表亮度分量、藍色色
度分量及紅色色度分量，其中 Y 分量比較方便人類肉眼的觀察與描述，因為它就像是一張
灰階影像，而 Cb、Cr 值嚴格來說分別是藍色色度分量及紅色色度分量與 Y 值的色差資訊，
因此 YCbCr 色彩空間是由一個亮度分量 Y 與兩個色差分量 Cb、Cr 所構成，而 Cb、Cr 影
像對人眼來說是無意義的，但卻可以拿來作為色彩範圍的訂定用，例如用來設定人體的膚
色範圍或是一些色彩閥值。其 RGB 色彩空間轉換至 YCbCr 色彩空間之數學式如（1.16）式
所示。 
 
 18
 
圖 18. 三攝影機系統之視窗畫面 
 
實驗結果如圖 19 所示，每張圖片都包含三台攝影機影像。圖 19（a）~（c）代表三台
攝影機都在執行移動物偵測，並偵測到白色移動球體，此時三台攝影機主畫面左下角之工
作模式皆顯示「Detection」。在圖 19（c）~（d）中，使用者點選第一台攝影機中的白色球
體作為追蹤目標物，此時第一台攝影機便開始執行該物體之靜態追蹤及對應，成為唯一的
「主控型攝影機」，其主畫面左下角之工作模式會顯示「Tracking + Correspondence」，攝影
機角色會顯示「Master」。同時，第二台攝影機及第三台攝影機會執行移動物偵測及目標物
對應搜尋，其畫面左下角之工作模式會顯示「Detection + Correspondence」，如果攝影機能
夠求得最佳候選對應物位置，則攝影機會扮演「協助型攝影機」，並投射對應極線至其他攝
影機中，而畫面左下角的角色也會顯示「Helper」，如圖 19（d）~（e）所示。透過各台攝
影機的對應極線交點，第二台攝影機及第三台攝影機皆找到了「確切對應物」，並將確切對
應物的框線標示為「粉紅色」。在圖 19（f）中，由於第三台攝影機的對應物被遮蔽住，故
系統無法利用第一台攝影機及第三台攝影機資訊來求得第二台攝影機中的「確切對應物」
位置，只能求得第二台攝影機中的「最佳候選對應物」，並對該物體框上藍色框線。而由於
第三台攝影機的對應物被遮蔽了，故第三台攝影機會變成「被控型攝影機」，其畫面左下角
的角色扮演會顯示「Slave」，此時系統會透過第一台主控型攝影機及第二台協助型攝影機
來估測第三台攝影機中的「被遮蔽對應物位置」，並對估測的位置標示粉紅色實心方塊
「 」。接著，在圖 19（g）中，由於第三台攝影機的對應物沒有被遮蔽了，故第三台攝影
機也會變成「協助型攝影機」，此時第二台攝影機及第三台攝影機都可以求得「確切對應物」
位置。直到圖 19（h）時，第二台攝影機的對應物也準備被遮蔽住，故在圖 19（i）~（j）
中，第二台攝影機變成了「被控型攝影機」，而第三台攝影機的「確切對應物」也變成了「最
佳候選對應物」，對應物框線也從粉紅色變成藍色。在圖 19（k）~（l）中，第一台攝影機
的目標物也因為稍微被遮蔽而導致物體變小，不利於第一台攝影機繼續做靜態追蹤，故系
統會自動改以視野較佳的第三台攝影機作為「主控型攝影機」，將原本第三台攝影機的對應
物做為追蹤目標物，而第一台攝影機則轉變成「協助型攝影機」。系統此時便改以第三台主
控型攝影機及第一台協助型攝影機來求得第二台被控型攝影機中的被遮蔽對應物位置，如
圖 19（l）~（m）所示，直到圖 19（n）時，第二台攝影機的對應物沒有被遮蔽了，系統才
得以透過三台攝影機之間的合作求得第一台攝影機及第二台攝影機的確切對應物位置。但
 20
（d）Frame 92 
（e）Frame 103 
（f）Frame 111 
（g）Frame 143 
（h）Frame 146 
 22
（n）Frame 200 
（o）Frame 209 
（p）Frame 210 
（q）Frame 211 
（r）Frame 295 
 24
（x）Frame 372 
（y）Frame 373 
（z）Frame 398 
圖 19. 三攝影機系統下目標物追蹤及對應之實驗結果 
 
由圖 19 之實驗結果可知，在使用三台攝影機時，透過本子計畫所提出的攝影機溝通合
作策略，系統除了能夠即時地對目標物進行靜態追蹤，求得確切對應物位置外，同時也可
以估測被遮蔽的對應物位置。而當追蹤目標物被遮蔽或是主控型攝影機暫時追蹤失敗時，
系統也會迅速地改以視野較佳的攝影機接替目標物追蹤的工作，使得系統能夠持續地追蹤
目標物，解決追蹤目標物因為被遮蔽而導致系統無法繼續求得對應物位置的問題。 
 
三攝影機系統之相似對應物區分實驗 
如圖 20 所示，在圖 20（a）中，三台攝影機皆偵測到一個白色移動球體。在圖 20（b）
~（c）時，使用者點選第一台攝影機的白色球體作為追蹤目標物，此時第一台攝影機便開
始進行白色球體的靜態追蹤，並在第二、三台攝影機中繪製對應極線，同時也在對應極線
上搜尋「最佳候選對應物」。在攝影機溝通合作的機制下，系統能夠透過三台攝影機資訊在
第二、三台攝影機中求得「確切對應物」位置，並標示粉紅色框線，如圖 20（c）所示。在
圖 20（d）時，第二個與目標物相同的白色球體進入了各台攝影機的畫面中，而由於第二
個白色球體沒有位於各台攝影機的對應極線上，故系統依然能夠正確地標示「確切對應物」
位置，並將第二個白色球體認定為一般的被偵測物體。在圖 20（e）時，第二台攝影機的第
 26
（d）Frame 124 
（e）Frame 126 
（f）Frame 127 
（g）Frame 129 
（h）Frame 132 
 28
六、計畫成果自評 
本計畫三年來的研究成果整理如下: 
1. 以適應性背景消除法與無母數背景模型進行移動物偵測，並針對背景模型之更新與維
護等相關技術進行研究與探討。 
2. 不同光源條件下，追蹤系統對於移動偵測結果之強健性探討。 
3. 實現多重資訊融合之相似度量測與樣板更新技術於Pan-tilt動態視覺追蹤系統。 
4. 對所偵測出之目標物進行分類及動作分析。 
5. 探討不同位置預測器之移動目標物追蹤效果。 
6. 利用改良式適應性背景相減法偵測各台攝影機影像中的移動物體，並利用多重相似度
量測法追蹤特定目標物。 
7. 使用極線幾何限制及色彩長條圖相似度量測法來求得目標物的對應位置。 
8. 建立分散式之多攝影機視覺追蹤系統。 
9. 配合其它子計畫將一針孔CCD攝影機架設於以二維壓電致動平台上，並完成視覺追蹤
實驗之整合測試。 
 
七、這三年基於本計畫成果所發表之論文： 
1. Ming-Yang Cheng, Chun-Kai Wang, and Hung-Wen Wu, "Real-time Detection of Slow 
Moving and Temporarily Stationary Objects," in Proceeding of the IEEE 2006 International 
conference on Systems, Man and Cybernetics, October 08~11, Taipei, Taiwan, 2006, pp. 
3983-3988. 
2. 賴俊良，移動目標物視覺偵測與追蹤研究，碩士論文，國立成功大學電機工程學系，
2006。 
3. Ju-Pin Su and Ming-Yang Cheng, "Development of a Visual Tracking System Capable of 
Moving Object Classification and Tracking", in Proceedings of the 2007 CACS International 
Automatic Control Conference. 
4. 蘇助彬，基於視覺之移動目標物分類與人體動作分析之研究，碩士論文，國立成功大
學電機工程學系，2007。 
5. Chun-Ming Wen and Ming-Yang Cheng, “'Positioning Accuracy Improvement of a 
Vision-based Optical Fiber Alignment Stage Powered by a Piezo-Actuator,” accepted by the 
IEEE International Workshop on Robotic and Sensors Environments. 
6. Hui-Pin Huang and Ming-Yang Cheng, “Implementation of a Real-time Visual Tracking 
System Based on Triple Cameras,” accepted by the 2008 International Automatic Control 
Conference. 
7. Hui-Pin Huang and Ming-Yang Cheng, “Implementation of a Real-time Visual Tracking 
System with Multi-cue Matching Method Based on Dual Cameras,” accepted by the 2008 
International Automatic Control Conference. 
8. 黃暉斌，基於多攝影機即時視覺追蹤系統之設計與實現，碩士論文，國立成功大學電
機工程學系，2008。 
 
 30
cameras,” in Proceedings of Third IEEE International Conference on Automatic Face and 
Gesture Recognition, pp. 498–503, 1998. 
[15] G. P. Stein, “Tracking from multiple view points: self-calibration of space and time,” in 
Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition, vol. 1, pp. 521–527, 1999. 
[16] L. Lee, R. Romano, and G. Stein, “Monitoring activities from multiple video streams: 
establishing a common coordinate frame,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 22, pp. 758–767, August 2000. 
[17] A. Mittal and L.S. Davis, “M2Tracker: A multi-view approach to segmenting and tracking 
people in a cluttered scene using region-based stereo,” in Proceedings of European 
Conference on Computer Vision, pp. 18–36, May 2002.  
[18] A. Mittal and L. Davis, “Unified multi-camera detection and tracking using 
region-matching,” in Proceedings of the IEEE Workshop on Multi-Object Tracking, 
Vancouver, BC, Canada, pp. 3–10, 2001. 
[19] S. Khan and M. Shah, “Consistent labeling of tracked objects in multiple cameras with 
overlapping fields of view,” IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 25, no. 10, pp. 1355–1360, October 2003. 
[20] Q. Chai, and J.K. Aggrtrwal, “Tracking human motion in structured environments using a 
distributed-camera system,” IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 2, no. 11, pp. 1241–1247, November 1999. 
[21] S. Calderara, R. Vezzani, A. Prati, and R. Cucchiara, “Entry edge of field of view for 
multi-camera tracking in distributed video surveillance,” in Proceedings of the IEEE 
Conference on Advanced Video and Signal Based Surveillance, pp. 93–98, 2005. 
[22] S. Calderara, R. Cucchiara, and A. Prati, “Group detection at camera handoff for 
collecting people appearance in multi-camera systems,” in Proceedings of the IEEE 
International Conference on Advanced Video and Signal-Based Surveillance, pp. 36–41, 
2006. 
[23] G. Kayumbi and A. Cavallaro, “Robust homography-based trajectory transformation for 
multi-camera scene analysis,” in Proceedings of the First ACM/IEEE International 
Conference on Distributed Smart Cameras, pp. 59–66, 2007. 
[24] S. Calderara, R. Cucchiara, and A. Prati, “Bayesian-competitive consistent labeling for 
people surveillance,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 
vol. 30, pp. 354–360, 2008. 
[25] H. P. Moravec, “Towards automatic visual obstacle avoidance,” in Proceedings of the 5th 
International Joint Conference on Artificial Intelligence, pp. 584, August 1977. 
[26] S. M. Smith and J. M. Brady, “SUSAN – A new approach to low level image processing,” 
International Journal of Computer Vision, vol. 23, pp. 45–78, 1997. 
[27] C. Harris and M. Stephens, “A combined corner and edge detector,” in Proceedings of the 
4th Alvey Vision Conference, Manchester, pp. 147–151, August 1988. 
[28] F. Mokhtarian and R. Suomela, “Robust image corner detection through curvature scale 
space,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, pp. 
 32
[44] 謝明逢，利用雙攝影機取像模組建構一大型環境監控系統，碩士論文，國立中央大
學資訊工程研究所，2005。 
[45] 張政祺，利用雙攝影機即時監視系統作人員偵測及特寫之研究，碩士論文，銘傳大
學資訊管理研究所，2003。 
[46] W. Zheng, Y. Shishikui, Y. Kanatsugu, Y. Tanaka, and I. Yuyama, “A high-precision 
camera operation parameter measurement system and its application to image motion 
inferring,” IEEE Transactions on Broadcasting, vol. 47, pp.46–55, March 2001. 
[47] R. M. Haralick, “Determining camera parameters from the perspective projection of a 
rectangle,” Pattern Recognition, vol. 22, pp. 255–230, 1989. 
[48] B. Hu, C. Brow, and A. Choi, “Acquiring an environment map through image 
mosaicking,” The University of Rochester Computer Science Department, Rochester, New 
York, November 2001. 
[49] 胡智強，全景影像之移動物偵測與追蹤系統，碩士論文，國立中央大學資訊工程研
究所，2004。 
[50] C. Stauffer and W. E. L. Grimson, “Adaptive background mixture models for real-time 
tracking,” in Proceedings of the IEEE Computer Society Conference on Computer Vision 
and Pattern Recognition, pp. 246–252, 1999. 
[51] N. Friedman and S. Russell, “Image segmentation in video sequences: A probabilistic 
approach,” in Proceedings of 13th Conference on Uncertainty in Artificial Intelligence, 
1997. 
[52] D. S. Lee, “Improved adaptive mixture learning for robust video background modeling,” 
in Proceedings of the International Association for Pattern Recognition Workshop on 
Machine Vision for Applications, pp. 443–446, 2002. 
[53] A. Elgammal, D. Harwood, and L. Davis, “Non-parametric model for background 
subtraction,” in Proceedings of the 6th European Conference on Computer Vision, pp. 
751–767, 2000. 
[54] A. Elgammal, R. Duraiswami, D. Harwood, and L. S. Davis, “Background and foreground 
modeling using nonparametric kernel density estimation for visual surveillance,” 
Proceedings of the IEEE, vol. 90, pp. 1151–1163, 2002. 
[55] 王俊凱，基於改良式適應性背景相減法與多重影像特徵比對法之多功能及時視覺追
蹤系統之設計與實現，碩士論文，國立成功大學電機工程學系，2004。 
[56] E. Davies, Machine Vision: Theory, Algorithms and Practicalities, Academic Press, 1997. 
[57] M. Sonka, V. Hlavac and R. Boyle, Image Processing, Analysis, and Machine Vision, 
Pacific Grove, 1999. 
[58] 黃暉斌，基於多攝影機即時視覺追蹤系統之設計與實現，碩士論文，國立成功大學
電機工程學系，2008。 
 
 
 
 
 
 34
推廣及運用的價值 
在安全監控方面，即時視覺偵測追蹤系統可克服以往依賴人力及錄
影，無法即時偵測不安全的資訊並於第一時間發出警告等問題。在
視訊會議與遠距教學方面，透過即時視覺追蹤技術，可使演講者之
畫面鎖定於螢幕中央。在大眾交通運輸系統方面，視覺偵測系統除
可進行交通流量監測，亦能提供駕駛人鄰近車況與道路上的動態。
1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研發成
果推廣單位（如技術移轉中心）。 
2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
3.本表若不敷使用，請自行影印使用。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
一、參加會議經過 
IEEE International Workshop on Robotic and Sensors Environments 2008 於10月17∼18 日為期兩
天，在加拿大首都渥太華市(Ottawa)的國立渥太華大學舉行(見圖一)。該會議係由IEEE(美國電子電
機工程師學會)的測試設備與量測分會(Instrumentation & measurement)等學術組織合辦，為每年一次
的學術研討會，今年是第六屆會議。 
 
 
圖一、ROSE 2008 會場 加拿大國立渥太華大學資訊工程大樓(SITE) 
 
IEEE ROSE 2008 會議共分成7 個session，約有27 多篇論文在本次大會發表，出席人員來自
美國、西班牙、法國、羅馬尼亞、加拿大、日本、中國大陸、台灣等。在本次研討會中，計畫參與
人被安排於議程第一天(十月十七日)中Robotic Manipulation session 中發表一篇論文，題目為
「Positioning Accuracy Improvement of a Vision-based Optical Fiber Alignment Stage Powered by a 
Piezo-Actuator」。本論文的重點主要是在內迴路伺服架構中使用小腦模型 (Cerebellar Model 
Articulation Controller，CMAC)作為前饋補償壓電致動器的非線性遲滯現象，並搭配PI 閉迴路控制
器來來抑制外部的干擾和穩定系統效能。典型的視覺回授控制系統具有兩種以上不同的取樣頻率，
因此衍生出控制迴路內外動態不匹配的問題。因此本計畫在在視覺迴路中使用多取樣頻率來解決視
覺感測器的取樣延遲問題。經由實驗結果顯示，本論文所提出之CMAC 前饋補償結合PI 閉迴路控
制器和多取樣頻率控制器其效果較一般視覺伺服控制迴路效果佳。在計畫參與人演講完畢後，有兩
位學者提出問題，除了詢問我們相對於其它類神經網路控制器為何選擇CMAC 外，另外他們也對
於我們所使用的視覺演算法感到興趣。針對於這些問題，計畫參與人均給予適當地答覆。 
此外並分別於會前與會後與其他國家學者討論，如來自多倫多大學的Qingkun Zhou 、渥太華
大學的Professor Emil M. Petriu 與Session 3 的Chair Dr. Hong Zhao 等電機控制學者。值得一提的
是，國外學者對於台灣的科技、教育與民主發展均有高度的肯定(圖三)。在開會期間，計畫參與人
並實際參與了幾場專題演講，如西班牙學者Dr. Sergey Y. Yurish (Technical University of Catalonia, 
Barcelona, Spain Self-Adaptive Intelligent Sensors and System)，於會議第一天之所做專題演講，題目
是『From Theory to Practical Design』。Dr. Yurish 主要是針對智慧型感測器的應用與演算法作一個
詳細的介紹。 
 
 
三、攜回資料名稱及內容:會議論文CD 一片。 
本報告附以下附件做為補充說明。附件一為IEEE International Workshop on Robotic and Sensors 
Environments (ROSE 2008) 所發表之論文，附件二為ROSE 2008 之議程。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
II. BRIEF REVIEW OF CMAC  
 
The Cerebellar Model Articulation Controller (CMAC), 
proposed by Albus in the 1970’s, is a learning structure that 
imitates the human cerebellum. The basic structure of the 
CMAC algorithm is illustrated in Fig. 2, where the input state 
vector S is quantized and divided into several discrete states, 
namely S=(s1,s2…,sN). These discrete states are mapped to 
different memory indexes. The actual output
is
y , can be 
expressed as: 
 
1
2
,1 ,2 ,i i i i i m
Nm
T
s s s s s N
w
w
y a w a a a
w
⎡ ⎤⎢ ⎥⎢ ⎥⎡ ⎤= = ⎢ ⎥⎣ ⎦ ⎢ ⎥⎢ ⎥⎣ ⎦
" #  
          ,
1
, 1,2,.....
m
i
N
s j j
j
a w i N
=
= =∑                    (1) 
 
where 
i
as is a memory index vector of the ith state, w is a 
memory weight vector, and Nm is the total physical memory 
size. 
The difference between the actual output and the desired 
output is used to modify the memory weight. In this paper, 
the memory weight updating law of the CMAC learning 
algorithm can be expressed as: 
 
, ,
1
( ) ( 1) ( 1)
ˆ( 1) [ ( 1)]
m
i i i
j j j
N
j s j s s j j
e j
w k w k w k
w k a y a w k
N
λ
=
= − + Δ −
= − + − −∑       (2)                                 
 
where k indicates the sample number, ˆ
is
y  is the target output, 
λ  is the learning rate, and eN  is the number of  memory 
locations [12]. 
 
 
Set of All
Possible
Inputs
S
Memory
Indexes
Memory
Weights
1a
2a
3a
4a
5a
6a
7a
8a
9a
10a
11a
12a
13a
Nma
#
#
#
#
Nmw
1w
3w
4w
5w
6w
7w
8w
9w
10w
11w
12w
13w
2w
Summation of 
    Selected
    Weights
Output
Learning
Algorithm
Desired
Output
Adjustment
   Weight
+
−
Σ
 
Fig. 2. Structure of CMAC [10] 
III. SUPERVISED LEARNING CONTROL STRUCTURE 
BASED ON CMAC  
 
Fig. 3 shows a block diagram of the proposed control 
structure, which consists of a CMAC-based feedforward 
compensator and a PI type feedback loop controller [14,15]. 
CMAC is used to compensate for the hysteresis effect in the 
PEA, while the PI type feedback loop controller is designed 
to stabilize the system and to eliminate disturbance. The 
proposed approach adopts the supervised learning control 
structure [16][17]. The output of CMAC is expressed as:  
                                                         
,
1
m
i i
N
Cs s j j
j
u a w
=
=∑                                                    (3) 
 
In Eq. (2), the weights wj are adjusted based on the error 
between the CMAC output uC and the total control input u, 
which  is described by: 
                       
  
( ) ( 1)
( )
( 1) i i
e
j j
e
s Cs
j
e
u
w k w k
N
u u
w k
N
λ
λ
= − +
−
= − +
                            (4) 
 
where ue is the error between the output of the CMAC output 
and the total control input. 
 
 
Cerebellar Model Articulation Controller
eu
Cu
uPIuer PI
y
PEA with 
  TableAmplifier
Learning
Algorithm
Memory
Weights
Memory
Indexes
+
−
++
+−
Strain
Gauge
mμ
 
Fig. 3. Block diagram of the proposed control structure. 
 
IV. MULTI-RATE CONTROL 
 
In the optical fiber alignment motion stage developed in 
this paper, a vision system is used to measure the alignment 
error in the image plane. However, the inherent latency 
problem [18,19] in vision sensors often leads to 
unsatisfactory system performance. In this paper, the idea of 
multi-rate sampling [20] is used to cope with the visual loop 
latency problem. Fig. 4 shows the control block diagram of 
the optical alignment motion stage developed in this paper, 
which consists of a CMAC based feedforward compensator, 
an PI type feedback controller, and a multi-rate sampler. In 
the proposed control scheme, the vision signal is updated 
every 33 ms. When waiting for next update, the displacement 
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35
40
Time (sec)
D
is
pl
ac
em
en
t (
μm
)
Command
Output
 
(a) 
0 5 10 15 20 25 30 35
-5
-4
-3
-2
-1
0
1
2
3
4
5
Time (sec)
Er
ro
r (
μm
)
 
(b) 
Fig. 8. Step-like ramp input position experiment (CMAC + PI) (a) 
displacement (b) positioning error. 
 
Table 1. Results of step-like ramp input positioning experiment. 
Performance index (μm) 
Control scheme IAE RMS AVG Max 
inverse hysteresis 
+IVSC 0.0739 0.3360 −0.0053 4.9921 
CMAC+PI 0.0683 0.1945 0.0030 3.9111 
 
C. Tracking control experiment of PEA 
 
In this experiment, the motion stage powered by a PEA 
was controlled to track a reference command. The reference 
command was a random sinusoidal input. Experiment results 
of the inverse hysteresis model combined with IVSC are 
shown in Fig. 9 and the experimental results of the proposed 
approach are shown in Fig. 10. The adverse effects of 
hysteretic nonlinearly are diminished in the proposed scheme, 
as shown in Fig. 10(c). In addition, based on the results listed 
in Table 2, a clear improvement in the tracking accuracy is 
achieved when the proposed approach is employed.  
 
0 5 10 15 20 25 30 35 40 45 50
0
5
10
15
20
25
30
35
40
Time (sec)
D
is
pl
ac
em
en
t (
μm
)
Command
Output
 
(a) 
0 5 10 15 20 25 30 35 40 45 50-3
-2
-1
0
1
2
3
Time (sec)
Er
ro
r (
μm
)
 
(b) 
0 5 10 15 20 25 30 35 400
5
10
15
20
25
30
35
40
Desired Displacement (μm)
O
ut
pu
t D
is
pl
ac
em
en
t (
μm
)
 
(c) 
Fig. 9. Random sinusoidal input tracking experiment (inverse hysteresis + 
IVSC) (a) displacement (b) tracking error (c) linearization of hysteresis 
curve.  
 
0 5 10 15 20 25 30 35 40 45 500
5
10
15
20
25
30
35
40
Time (sec)
D
is
pl
ac
em
en
t (
μm
)
Command
Output
 
(a) 
0 5 10 15 20 25 30 35 40 45 50-3
-2
-1
0
1
2
3
Time (sec)
Er
ro
r (
μm
)
 
(b) 
0 5 10 15 20 25 30 35 400
5
10
15
20
25
30
35
40
Desired Displacement (μm)
O
ut
pu
t D
is
pl
ac
em
en
t (
μm
)
 
(c) 
Fig. 10. Random sinusoidal input tracking experiment (CMAC + PI) (a) 
displacement (b) tracking error (c) linearization of hysteresis curve.  
 
[15] W. T. Miller, R. P. Hewes, F. H. Glanz, and L. G. Kraft, “Real-time 
dynamic control of an industrial manipulator using a neural-network-
based learning controller,” IEEE Trans. Robt. Autom., vol. 6, no. 1, pp. 
1–9, Feb. 1990. 
[16] S. Kumar, Neural Networks: A Classroom Approach, McGraw-Hill, 
2004. 
[17] J. M. Zurada, Introduction to Artificial Neural Systems, West 
Publishing Co.1992. 
[18] S. Hutchinson, G. D. Hager, and P. I. Corke, “A tutorial on visual servo 
control,” IEEE Trans. Robt. Autom., vol. 12, no. 5, pp. 651-670, 
Oct.1996. 
[19] P.I. Corke, and M.G. Good, “Dynamic effects in visual closed-loop 
systems,” IEEE Trans. Robt. Autom., vol. 12, no. 5, pp. 671-683, Oct. 
1996. 
[20] F. Marcassa, and R. Oboe, “Disturbance rejection in hard disk drives 
with multi-rate estimated state feedback,” Control. Eng. Pract., vol. 12, 
no. 11, pp. 1409-1421,  Nov. 2004. 
[21] H.-S. Chuang, C.-W. Lin, M.-Y. Cheng, and Y.-C. Chuang, 
“Development of a real-time vision-based optical fiber alignment 
platform,” in Proc. Int. Conf. Electrical Machines and Systems, 2006, 
20-23. 
 
University of Ottawa, CANADA 
Budapest Tech Polytechnical Institution, HUNGARY 
 11:10 - 11:30  
A Multiscale Calibration of a Photon Video Microscope for Visual Servo Control 
B. Tamadazte, S. Dembele, N. Le Fort-Piat 
University of Franche-Comt? FRANCE 
FEMTO-ST Institute, FRANCE 
 11:30 - 11:50  
Towards a Mouth Gesture Based Laparoscope Camera Command 
J. Gomez-Mendoza, F. Prieto, T. Redarce 
Universidad Nacional de Colombia, COLOMBIA 
INSA de Lyon, FRANCE 
 12:00 - 13:00 Lunch 
(provided)  
 13:00 - 14:00 Keynote 
Speaker 
Dr. Sergey Y. Yurish, Technical University of Catalonia, Barcelona, Spain 
Self-Adaptive Intelligent Sensors and Systems: From Theory to Practical Design 
 14:00 - 15:00 Session 3 Robot Control 
  Chair Voicu Groza, University of Ottawa 
 14:00 - 14:20  
Two 稺 ayer Sliding Mode Control of Pneumatic Position Synchro System with 
Feedback Linearization Based on Friction Compensation 
G. Zhao,P. Ben-Tzvi, T. Lin, A. Goldenberg 
University of Toronto, CANADA 
The George Washington University, USA 
Xi'an Jiaotong University, CHINA 
 14:20 - 14:40  
Bipedal Modeling and Decoupled Optimal Control Design of Biomechanical 
Sit-to-Stand Transfer 
A. Mughal, K. Iqbal 
University of Arkansas, USA 
 14:40 - 15:00  
Dynamic Modeling of a Rotating Beam Having a Tip Mass 
S. Bai, P. Ben-Tzvi, Q. Zhou, X. Huang 
National University of Defence Technology, CHINA 
The George Washington University, USA 
University of Toronto, CANADA 
 15:00 - 15:20 Coffee Break  
 15:20 - 16:20 Session 4 Robotic Manipulation 
  Chair Hong Zhao, University of Toronto 
 15:20 - 15:40  Estimating the 3D Orientation of a Microgripper by Processing the Focus Data 
Osaka City University, JAPAN 
 10:20 - 10:40  
Predictable Data Communication Interface for Hard Real-Time Systems 
M. Micea, G. Carstoiu, L. Ungurean, D. Chicidean, V. Cretu, V. Groza 
Politehnica University of Timisoara, ROMANIA 
University of Ottawa, CANADA 
 10:40 - 11:00 Coffee Break  
 11:00 - 12:20 Session 6 Intelligent Sensing 
  Chair Jens Kuehnle, Fraunhofer Institute of Manufacturing Engineering and Automation, 
Germany 
 11:00 - 11:20  
Intelligent Haptics Sensing and Biometric Security 
A. Kanneh, Z. Sakr 
University of Trinidad and Tobago, TRINIDAD AND TOBAGO 
 11:20 - 11:40  
Evaluation of Growing Neural Gas Networks for Selective 3D Scanning 
A.-M. Cretu, E.M. Petriu, P. Payeur 
University of Ottawa, CANADA 
 11:40 - 12:00  
Minimum Set of Feedback Sensors for High Performance Decentralized 
Cooperative Force Control of Redundant Manipulators 
D. Navarro-Alarcon, V. Parra-Vega, E. Olguin-Diaz 
Research Center for Advanced Studies, MEXICO 
 12:00 - 12:20  
Feature Selection for a Real-Time Vision-Based Inspection System 
M.M. Chetima, P. Payeur 
University of Ottawa, CANADA 
 12:30 - 13:30 Lunch 
(provided)  
 13:30 - 15:10 Session 7 Robot Vision and Imaging 
  Chair J. Gomez-Mendoza, Universidad Nacional de Colombia, COLOMBIA 
 13:30 - 13:50  
Position-Based Visual Servoing Using a Coded Structured Light Sensor 
C. Doignon, T. Heitzmann, C. Albitar, P. Graebling 
Louis Pasteur University, FRANCE 
 13:50 - 14:10  
Grasping in Depth Maps of Time-Of-Flight Cameras 
J. Kuehnle, Z. Xue 
Fraunhofer Institute of Manufacturing Engineering and Automation, GERMANY 
Research Center for Information Technology, GERMANY 
 14:10 - 14:30  An Omnidirectional Stereoscopic System for Mobile Robot Navigation 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                               97  年  10  月 27   日 
報告人姓名 
 
溫峻明 
 
就讀校院
（科系所）
                     ■博士班研究生 
國立成功大學電機系 
                     □碩士班研究生 
     時間 
會議 
     地點 
2008/10/17~2008/10/18 
 
加拿大、渥太華 
本會核定
補助文號 NSC 96-2221-E-006-038 
會議 
名稱 
 (中文) 機器人與感測器環境國際研討會 
 (英文) IEEE International Workshop on Robotic and Sensors Environments (ROSE 
2008) 
發表 
論文 
題目 
 (中文) 基於視覺回授之光纖壓電驅動平台定位精度改善 
 (英文) Positioning Accuracy Improvement of a Vision-based Optical Fiber Alignment 
Stage Powered by a Piezo-Actuator 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
二、與會心得 
    計畫參與人於大會聆聽各國學者報告時，發覺來自法國的學者之英語不甚流利，而且在口頭報
告中回答各國學者提問時夾雜使用本國的語言，使得與會者無法參與或瞭解對答內容，相當可惜。
另外於大會午餐時，計畫參與人與在多倫多大學的大陸籍學者周擎坤博士生閒聊，發現他對於台灣
蓬勃發展之電子資訊產業有相當程度的瞭解。同時他也對於台灣各大學的教育制度極有興趣，並希
望邀請成功大學相關領域的學者至多倫多大學作短期的訪問。計畫參與人對於周博士生說著幾乎沒
有口音之流利英語感到印象深刻。反觀國內各大學雖紛紛朝向加強國際化發展，然而對照目前國內
出國攻讀碩博士學位之人數大幅減少之情形，且國內大學卻又無法吸引足夠外國學生來台就讀之窘
境，計畫參與人對於我國高等教育如何在這一波全球化熱潮下走出自己的道路，感到憂心忡忡。 
 
 
圖二、多倫多大學學者Qingkun Zhou(左)與計畫參與人(右)於渥太華大學ROSE 2008會場合照 
 
三、參觀考察活動 
計畫參與人除發表論文外，還受邀參訪了渥太華大學的視覺、影像、視訊和自控系統研究實驗
室(Vision, Imaging, Video and Autonomous Systems Research Laboratory)，在參觀過程中彼此交流了
視覺伺服的相關技術與實作問題探討(見圖二)。 
 
      
圖三、參訪渥太華大學視覺、影像、視訊和自控系統研究實驗室 
ROSE 2008 – IEEE International Workshop 
on Robotic and Sensors Environments 
Ottawa – Canada, 17-18 October 2008 
Positioning Accuracy Improvement of a Vision-based Optical Fiber Alignment Stage 
Powered by a Piezo-Actuator 
 
Chun-Ming Wen and Ming-Yang Cheng 
Department of Electrical Engineering, National Cheng Kung University, 
Tainan 701, Taiwan, R.O.C. 
 
Abstract – Optical fiber communication has become mainstream in 
wired communication due to its low attenuation, low cost, and high 
bandwidth. However, optical fiber is light and thin, which makes the 
coupling procedure between fibers and optoelectronic components 
very complex and difficult. To maintain high transmission quality, it 
is imperative to develop a high-precision alignment technique for 
optical fiber fabrication. In this paper, a vision-based optical fiber 
alignment stage powered by a Piezo-Actuator (PEA) is proposed. 
The tracking performance of PEA is limited due to its inherent 
hysteretic nonlinearity and time varying parameters. In order to 
cope with this problem, a feedforward compensator based on a 
Cerebellar Model Articulation Controller (CMAC) combined with 
a PI feedback controller is developed to eliminate the effects of 
hysteresis. Multi-rate control is used to deal with the vision latency 
problem. Experimental results show that the proposed approach 
shows satisfactory performance. 
 
Keywords – Piezoelectric Actuator; Optical Fiber Alignment; 
CMAC; PI; Multi-rate Control.   
 
I. INTRODUCTION 
 
Compared to traditional copper wires, optical fiber 
cables have many superior properties; e.g., larger bandwidth, 
lower vulnerability to corrosion, lighter weight, no sensitivity 
to radio waves, more security, less signal crosstalk, and better 
transmission quality. It is not surprising that optical fiber has 
the edge over traditional copper wire in mainstream wire 
communication. However, the coupling procedure between 
optical fiber and the waveguide is very complex, so coupling 
loss due to misalignment is unavoidable. In order to maintain 
high communication quality, in general, the optical fiber 
alignment error is required to be smaller than 1.0 μm. Hence 
the development of automatic alignment technologies to 
achieve satisfactory accuracy is inevitable. 
Generally speaking, alignment techniques for optical 
fiber assembly can be divided into two categories ⎯ passive 
[1] and active [2]. In passive alignment techniques, optical 
fibers are aligned using passive alignment structures or 
patterned alignment marks [1]. The accuracy of passive 
alignment techniques greatly depends on the alignment 
device. In contrast, active alignment techniques use a motion 
stage to adjust the positions of the optical fibers to be aligned. 
In active alignment, the feedback signals for the high-
precision motion stage are obtained from power meters [2][3] 
or machine vision techniques [4][5]. A vision-based active 
alignment procedure has two stages ⎯ coarse positioning and 
fine positioning [6]. During the coarse positioning stage, the 
piezo-actuated motion stage is controlled to reduce the 
alignment error based on the vision feedback signal. During 
the fine positioning stage, the piezo-actuated motion stage is 
controlled and moved to the position that has the maximum 
output power indicated by the power meter [7,8].  
In general, the entire alignment process takes about 20 
seconds. In this paper, a vision-based optical fiber alignment 
motion stage powered by a piezo-actuator (PEA) is developed 
to speed up the coarse positioning stage of optical alignment. 
However, the tracking performance of PEA is limited due to 
its inherent hysteretic nonlinearity and time varying 
parameters [9]. Fig. 1 illustrates the nonlinear hysteretic 
effect.  It can be seen that the width of the hysteresis loop 
depends on the magnitude of the input voltage. In order to 
cope with this problem, a Cerebellar Model Articulation 
Controller (CMAC) is employed to compensate for the 
hysteresis effect in the PEA. CMAC was first proposed by 
Albus in the 1970’s [10,11]. The advantages of CMAC 
include fast learning speed, fast computation, local 
generalization, and high convergence rate. CMAC has been 
successfully applied to function approximation [12,13], and 
robot control problems [14,15]. In the proposed control 
scheme, hysteresis is mainly overcome by the CMAC-based 
feedforward compensator, while a PI type feedback controller 
is used to deal with the disturbance. In addition, the vision 
latency problem often leads to unsmooth vision command 
and performance deterioration. In order to cope with this 
difficulty, the idea of multi-rate control is used to shorten the 
sampling period of the control system. The effectiveness of 
the servo control scheme is tested under a variety of reference 
inputs. Experimental results show that the proposed approach 
has satisfactory performance.  
The rest of the paper is organized as follows. Section 2 
gives a brief review on CMAC. In Section 3, the CMAC-
based feedforward compensator is proposed. Section 4 
introduces the multi-rate control. Finally, experimental results 
and conclusions are presented in Sections 5 and 6, 
respectively. 
 
 
0 20 40 60 80 100 120 140-10
0
10
20
30
40
Input Voltage (V)
D
is
pl
ac
em
en
t (
μm
)
 
Fig. 1. Typical hysteresis effect of a PEA. 
of PEA obtained from the strain gauge is converted to the 
vision feedback signal by multiplying the reciprocal of the 
scalar image Jacobian Klens every 1 ms.  
 
eˆ
Positioning control of PEA
rX +
− −
ˆ
fXsX
Multirate 
 Sampler lens K
1
lensK
−
uT
r +
−
CMAC
PI
+
+ u Z.O.H
PEA
With
Table
y
uT
Strain
Gauge
  CCD
CamerayT
(33 )ms
  Feature
Extraction
(1 )ms
(1 )ms
 
Fig. 4. Control block diagram of the proposed approach with multi-rate 
control. 
V. EXPERIMENTAL RESULTS 
 
Two different kinds of experiments were conducted to 
verify the effectiveness of the proposed approach. Two 
control schemes were tested: the proposed approach 
(CMAC+PI) and the one (inverse hysteresis+IVSC) 
developed in [21]. Fig. 5 shows the control block diagram 
proposed in [21], which consists of an inverse hysteresis 
model, an IVSC feedback controller, and a multi-rate sampler. 
 
eˆ
Positioning control of PEA
rX +
− −
ˆ
fXsX
Multirate 
 Sampler lens K
1
lensK
−
uT
r +
−
Inverse
Model
IVSC
+
+ u Z.O.H
PEA
With
Table
y
uT
Strain
Gauge
  CCD
CamerayT
(33 )ms
  Feature
Extraction
(1 )ms
(1 )ms
 
Fig. 5. Control block diagram of the approach proposed in [21]. 
 
A. Experimental setup 
 
The PEA motion stage developed in this paper has a 
displacement range of 0~40 μm under an input voltage range 
of 0 to 150 volts. A strain gauge sensor with a 0.1 μm 
resolution was used to measure the displacement of the PEA. 
A power amplifier (Posi Con 150-3) with a gain of 30 was 
used to drive the PEA. The control input voltage, which was 
computed using a program running on a DSP (PMC32-6000), 
was sent to the actuator through a 12-bit D/A converter. The 
output displacement signal obtained from the strain gauge 
was sampled by an A/D converter. The sampling time was set 
to 1.0ms. The light source used in the vision sensor was a 
coaxial system generated by a 100W halogen type light 
source (MORITEX MHFD100LR). The camera used in this 
paper was the Sony XC-ST 50 768×494 pixel camera with a 
30Hz frame-rate, which outputs a video signal in the RS-170 
standard. The magnification factor of the telecentric lens is 10 
and the field of view is 0.64mm (Horizontal) × 0.48mm 
(Vertical). Therefore the relationship between the pixel in the 
image plane and the actual displacement is 0.64/768=0.833 
μm/pixel and 0.48/494=0.97 μm/pixel for the horizontal and 
vertical axes, respectively. The camera and the coaxial 
lighting system were mounted on top of the optical fibers, as 
shown in Fig.6. 
 
 
Fig. 6. Hardware description of the proposed vision-based optical fiber 
alignment platform.  
B. Positioning experiment of PEA 
 
The purpose of this experiment was to test the positioning 
performance of the optical fiber alignment motion stage 
developed in this paper. The reference command used in the 
experiment was a step-like ramp input. Experimental results 
of the inverse hysteresis model combined with IVSC are 
shown in Fig. 7; the experimental results of the proposed 
approach are shown in Fig 8. Experimental results of the 
integral of absolute error (IAE), root mean square of error 
(RMS), average error (AVG), and maximum error (MAX), 
are listed in Table 1. The experimental results indicate that 
the proposed approach (CMAC + PI) performs better than 
that proposed in [21]. 
 
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35
40
Time (sec)
D
is
pl
ac
em
en
t (
μm
)
Command
Output
 
(a) 
0 5 10 15 20 25 30 35-5
-4
-3
-2
-1
0
1
2
3
4
5
Time (sec)
Er
ro
r (
μm
)
 
(b) 
Fig. 7. Step-like ramp input positioning experiment (inverse hysteresis + 
IVSC)  (a) displacement (b) positioning error. 
Table 2. Results of random sinusoidal input tracking experiment. 
Performance index (μm) 
Control scheme IAE RMS AVG Max 
inverse hysteresis 
+IVSC 0.0916 0.2429 −0.0016 2.3963 
CMAC+PI 0.0616 0.0785 −0.0007 0.3452 
 
D. Optical fiber alignment experiment with multi-rate control 
 
The purpose of this experiment was to test the 
performance of the proposed approach. Note that in every 
experiment run shown in Fig. 4, Ty was set to 33ms and Tu 
was set to 1ms. In this experiment, the alignment difference 
in the image plane identified using the method developed in 
[21] was 5 pixels. Experiment results of the inverse hysteresis 
model combed with IVSC and multi-rate are shown in Fig. 11. 
The solid line in Fig. 11 represents the position of the fixed 
optical fiber in the image plane, whereas the dash line 
represents the position of the optical fiber to be aligned. From 
the results shown in Fig. 11, the fixed optical fiber is located 
at the 223rd pixel, while the initial position of the optical 
fiber to be aligned is at the 218th pixel. The total moving 
distance is 5 pixels and the total traveling time is 0.297 
seconds. The experimental results of the proposed approach 
are shown in Fig. 12. The results show that the position of the 
fixed optical fiber is at the 232nd pixel, while the initial 
position of the optical fiber to be aligned is at the 227th pixel. 
It took only 0.198 seconds to complete the displacement 
positioning experiment. Experimental results indicate that the 
proposed approach outperforms the one developed in [21]. 
 
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8218
219
220
221
222
223
224
Time (sec)
O
pt
ic
al
 F
ib
er
 P
os
iti
on
 (p
ix
el
)
 
 
Command
Output
 
Fig. 11. 5 pixels displacement position experiment (inverse hysteresis + 
IVSC). 
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8227
228
229
230
231
232
233
Time (sec)
O
pt
ic
al
 F
ib
er
 P
os
iti
on
 (p
ix
el
)
Command
Output
 
Fig. 12. 5 pixels displacement position experiment (CMAC + PI). 
VI. CONCLUSION 
 
This paper developed a vision-based automatic optical 
fiber alignment technique for the coarse positioning process. 
A CMAC based feedforward compensator combined with a 
PI type feedback loop was proposed to overcome the adverse 
effect of hysteresis in the PEA. A multi-rate control scheme 
was adopted to cope with the visual loop latency problem. 
Experimental results indicate that the proposed approach has 
satisfactory performance. 
 
ACKNOWLEDGEMENTS 
 
The authors would like to thank the National Science 
Council of the Republic of China, Taiwan, for supporting this 
research under Grant No. NSC96-2221-E-006-038. In 
particular, the authors would like to thank Dr. H.-S. Chuang 
and Mr. C.-H. Chiu for their continued assistance with this 
work. 
 
REFERENCES 
 
[1] R. Hauffe, U. Siebel, K. Petermann, R. Moosburger, J.R. Kropp, and F. 
Arndt, “Methods for passive chip coupling of integrated optical 
devices,” in Proc. Electronic Components and Technology Conf., 2001, 
pp. 238-243. 
[2] D.T. Pham and M. Castellani, “Intelligent control of fiber optic 
components assembly,” in Proc. Institution Mechanical Engineers   
Conf., 2001, vol. 215, Part B, pp. 1177-1189. 
[3] C.Y. Tseng and J.P. Wang, “Automation of multi-degree-of-freedom 
fiber-optic alignment using a modified simplex method,” Int. J. Mach. 
Tool. Manufact., vol. 45, pp.1109-1119, 2005. 
[4] M.S. Coben, M.J. DeFranza, F.J. Canora, M.F. Cina, and R.A. Rand, 
“Improvements in index alignment method for laser-fiber array 
packing,” IEEE Trans. Compon. Packag. Technol. B., vol. 17, no. 3, 
pp.402-411, 1994. 
[5] Y.T. Tseng and Y.C. Chang, “Active fiber-solder-ferrule alignment 
method for high-performance opto-electronic device packaging,” IEEE 
Trans. Compon. Packag. Technol. A., vol. 26, no.3, pp.541-547, 2003. 
[6] C.L. Chu, S.H. Lin, Z.Y. Fu, and K.K. Yen, “The development of an 
optical fiber alignment and fusion machine,” in Proc. IEEE Int. Conf. 
Mechatronics, 2005, pp.472-476.  
[7] R.M. Lewis, V. Torczon, and M.W. Trosset, “Direct search methods:  
then and now,” J. Comput. Appl. Math, vol. 124, issues 1-2, pp.191-
207, Dec. 2000. 
[8] D.T. Pham, and M. Castellani, “Evolutionary fuzzy logic system for 
intelligent fibre optic components assembly,” in Proc. Int. Mech. Eng., 
2002, vol. 216, Part C, pp. 571-581. 
[9] G. Ping and J. Jouaneh, “Tracking control of a piezoceramic actuator,” 
IEEE Trans. Control Syst. Technol., vol. 4, no.3, pp.209-216, 1996.  
[10] J. S. Albus, “A new approach to manipulator control: The cerebellar 
model articulation controller (CMAC),” Trans. ASME, J. Dyn. Syst., 
Meas. Contr., vol. 97, no.3, pp.220-227, Sept. 1975.  
[11] J. S. Albus, “Data storage in the cerebellar model articulation controller 
(CMAC),” Trans. ASME, J. Dyn. Syst., Meas. Contr., vol. 97, no.3, 
pp.228-233, Sept. 1975.   
[12] C. S. Lin and C. T. Chiang, “Learning convergence of CMAC 
technique,” IEEE Trans. Neural Networks, vol. 8, no.6, pp.1281-1292, 
1997. 
[13] C. T. Chiang and C. S. Lin, “CMAC with general basis functions,” 
IEEE Trans. Neural Network, vol. 9, no.7, pp.1199-1211, 1996.  
[14] W. T. Miller, F. H. Glanz, and L. G. Kraft, “Application of a general 
learning algorithm to the control of robotic manipulators,” Int. J. Robot. 
Res., vol. 6, no. 2, pp. 84–98, 1987. 
附件二、ROSE 2008 之議程 
Presenters should plan for a 15 minutes presentation followed by 5 minutes of discussion. The 
room will be equipped with a projector and a computer configured with usual softwares 
running on a MSWindows platform [PowerPoint and Acrobat Reader (PDF files)]. 
Advanced Program: 
 
 Friday, October 17, 2008   
 8:30 Registration  
 9:00 Welcome 
Word 
Dr. Claude D'Amours, Vice-Dean, Academic, Faculty of Engineering, University of 
Ottawa 
 9:15 - 10:35 Session 1 Collaborative Robotics 
  Chair Pierre Payeur, University of Ottawa 
 9:15 - 9:35  
Cooperation in a Swarm of Robots using RFID Landmarks 
G. Zecca, P. Couderc, M. Banatre, R. Beraldi 
INRIA, FRANCE 
Sapienza Universit?di Roma, ITALY 
 9:35 - 9:55  
Fish Shoal Inspired Movement in Robotic Collectives 
R. Cioarga, B. Panus, C. Oancea, M. Micea, V. Cretu, E.M. Petriu 
Politehnica University of Timisoara, ROMANIA 
University of Ottawa, CANADA 
 9:55 - 10:15  
Emergent Exploration and Resource Gathering in Collaborative Robotic 
Environments 
R. Cioarga, I. Nalatan, S. Tura-Bob, M. Micea, V. Cretu, M. Biriescu, V. Groza 
Politehnica University of Timisoara, ROMANIA 
University of Ottawa, CANADA 
 10:15 - 10:35  
Inter-Task Communication and Synchronization in the Hard Real-Time Compact 
Kernel HARETICK 
M. Micea, C. Certajan, V. Stangaciu, R. Ciorga, V. Cretu, E.M. Petriu 
Politehnica University of Timisoara, ROMANIA 
University of Ottawa, CANADA 
 10:35 - 10:50 Coffee Break  
 10:50 - 12:00 Session 2 Sensor Controlled Robotic Operation 
  Chair Christophe Doignon, University of Strasbourg, France 
 10:50 - 11:10  
Iterative Learning-Based Fuzzy Control System 
R.-E. Precup, S. Preitl, E. Petriu, J. Tar, J. Fodor 
Politehnica University of Timisoara, ROMANIA 
from the Images Delivered by a Videomicroscope 
G. Fortier, B. Tamadazte, S. Dembele, N. Le Fort-Piat 
FEMTO-ST Institute, FRANCE 
University of Franche-Comt? FRANCE 
 15:40 - 16:00  
Velocity Field Control for Free-Form Contour Following Tasks by a Two Link Robot
C.-Y. Chen, M.-Y. Cheng, Y.-H. Wang 
National Cheng Kung University, TAIWAN 
 16:00 - 16:20  
Positioning Accuracy Improvement of a Vision-based Optical Fiber Alignment 
Stage Powered by a Piezo-Actuator 
C.-M. Wen, M.-Y. Cheng 
National Cheng Kung University, TAIWAN 
 16:30 - 17:30 Lab Tour 
and Demos
Sensing and Modeling Research Laboratory 
School of Information Technology and Engineering 
University of Ottawa 
 19:00 - 21:30 Conference 
Dinner 
M 彋 ropolitain Restaurant 
700 Sussex Drive, Ottawa 
 
 Saturday, October 18, 2008   
 8:30 Registration  
 9:00 - 10:40 Session 5 Wireless and Distributed Sensor Networks 
  Chair Ziad Sakr, University of Trinidad and Tobago 
 9:00 - 9:20  
An Auto Load Balancing ALOHA System for Wireless Ad Hoc and Sensor 
Networks 
J. Sarker, H. Mouftah 
University of Ottawa, CANADA 
 9:20 - 9:40  
Towards a Model and Specification for Visual Programming of Massively 
Distributed Embedded Systems 
M. Wang, V. Subramanian, A. Doboli 
State University of New York at Stony Brook, USA 
 9:40 - 10:00  
Message Redundancy in Sensor Networks Implemented with Intelligent Agents 
V. Ancusa 
University Politehnica of Timisoara, ROMANIA 
 10:00 - 10:20  Use of RSSI for Motion Control of Wirelessly Networked Robot Swarm 
T. Ishimoto, S. Hara 
