2 No Proxy Buffer
Now we analyze the theoretical performance of the
VOD system mentioned above. Consider a simple case that
the system supports only one video program. Let the video
length be L, and the batch window size be B. To further
simplify our analysis, we assume L is divisible by B.
I. If the proxy has no buffer, the service model
will be:
II. The request is buffer unlimited: it joins the
nearby batch service and generates a patch
stream to retrieve the missed prefix.
III. The request is buffer limited: the proxy initiates
a dedicated stream from the content servers (or
from other proxies) for the request.
Let the arrival rate of buffer-limited requests bee, and
the arrival rate of buffer-unlimited requests be b. Let the
system execution time be T, where T is divisible by B for
simplicity. Because the server only supports one video
program, the expected bandwidth of scheduled streams is
SS = (T ÷ B) × L ÷ T,
where (T ÷ B) is the total number of batches. Therefore, the
expected bandwidth of scheduled streams will be
.S B
L
S 
Since the proxy has no buffer space (i.e., no interval
cache is available), all buffer-limited requests must be served
by dedicated streams, resulting in total Te number of
dedicated streams. So the expected bandwidth of dedicated
streams is
SD(0, 0) = Te × L ÷ T = Le.
On the other hand, buffer-unlimited requests are served by
simple batching, and some patch streams are necessary for
retrieving the missed prefix. As Fig. 1(a) illustrates, the
expected length of patches is the sum of the patch lengths
required by all requests:
2
b
0
b BTdxx
B
T B 







  
Therefore, the expected bandwidth of patch streams is
 
.
2
2
0,0
b
b
P




B
T
B
TS
From (1), the expected input bandwidth of the proxy
becomes
     
.
2
0,00,00,0
be
PDS
 

B
L
B
L
SSSS
(2)
 Using Prefix Caching Only
time
T
B
0
… …
B
(a) No prefix cache.
time
allocate K prefix cache
K
0
K
B-K
K
B-K
…
T
B B
…
(b) Using K units of prefix cache.
Fig. 1. Illustration of prefix caching.
If the proxy adopts prefix caching only, the service
model becomes:
I. The request is buffer unlimited: it joins the
nearby batch service and retrieves the missed
prefix from the prefix cache or via patching.
II. The request is buffer limited: the proxy initiates a
dedicated stream from the content servers (or
from other proxies) for the request.
Note that the prefix cache size K needs not to be larger
than the batch window size B, i.e., 0KB. This is because
each request in simple batching with batch window size B
can always find an ongoing batch stream starting not earlier
than B ago, i.e., the part of prefix cache behind B will never
be accessed.
Fig. 1 shows how a prefix cache reduces the length of
patches, i.e., only those requests issued within the last (B-K)
time interval of each batch window require patches. If the
proxy allocates K units of prefix cache (0 K B), the
expected bandwidth of patch streams SP becomes
 
 
.
2
0,
b
2
0
b
P



 

B
KB
Tdxx
B
T
KS
KB
Because no interval cache is allocated, the expected
bandwidth of dedicated streams is still SD = Le. From (1),
the expected input bandwidth of the proxy becomes
     
 
.
2
0,0,0,
b
2
e
PDS
 

B
KB
L
B
L
KSKSSKS
(3)
 Using Interval Caching Only
If the proxy adopts interval caching only, the service
model becomes:
I. The request is buffer unlimited: it joins the
nearby batch service and retrieves the missed
prefix via patching.
II. The request is buffer limited:
A. If the interval cache can aggregate the
request to the previous one, the proxy
branches a time-equivalent sub-stream to
satisfy the request.
B. Otherwise, the proxy initiates a dedicated
stream from the content servers (or from
other proxies) for the request. If the
proxy still has free buffer, it allocates an
interval cache for caching the subsequent
video frames. The interval cache will
extend its size with time if possible.
Assume the proxy allocates K units of interval cache, K
= qB + r, {0q, 0r < B}, 0KL. Note that the interval
cache size K needs not to be larger than the video length L.
This is because L units of interval cache can aggregate all
buffer-limited requests issued during the time interval L.
After the time interval L, the early requests finish their
playback and release the cache space they occupied. The
newly released cache space can be used to serve incoming
requests. Therefore, the size of interval cache for a video will
never be larger than L. This fact also implies that the
behaviors of the proxy during all periodical time intervals L
are similar. To simplify our analysis, we let the system
execution time T equal to the video length L herein.
4Assume the proxy has H units of buffer, i.e., the total
allocated buffer space can not exceed H. The prefix cache
size xi is not larger than the batch windows size Bi, and the
interval cache size yi is not larger than the video length Li.
Thus, the optimization version of our caching allocation
problem can be formally described as:
 

N
i
ii yxg
1
,:maximize (9)
  iiii
N
i
ii LyBxHyx 

0and0for,subject to
1
(10)
This typical optimization problem can be solved via a
dynamic programming algorithm similar as that in [5]. We
summarize the details of the algorithm as below:
Let G(x, y, i) represent the maximum gain of allocating
x units of prefix cache and y units of interval cache for the
first i video programs (1  i N). Then we have the
following recurrence:
      



.1if,,,1,,max
,0if0
,,
NiiyxG'iyxG
i
iyxG (11)
Where G’(x, y, i) is defined as
        iiiiCyx yxgiyyxxGiyxG iii
,1,,max,,'
,


(12)
and Ci = {(xi, yi)} be the set of possible pairs of xi and yi
that satisfy the constraint (10). Therefore, we can solve our
cache allocation problem through equations (11) and (12)
recursively. (Note that pure prefix caching and pure interval
caching are two special cases of our hybrid caching strategy
by letting x = 0 and y = 0 correspondingly.)
3.2. The proposed Proxy-Level Multicast Scheme
In this project, we propose a new multicast
infrastructure, called Buffer-Assisted On-Demand Multicast
(BAODM), for applications of proxy-level multicasting. To
deliver multicast data to receivers at their specified times, a
timing control mechanism is integrated on routing nodes
(proxies) to generate appropriate delay via sliding-interval
caching as mentioned in [6–7]. Therefore, a routing node can
branch time-variant outgoing streams from an incoming one.
By carefully scheduling the departure times of the outgoing
streams on each routing node, a BAODM tree that fulfills the
various timing requirements of multiple receivers can be
constructed. Fig. 3 shows the comparisons between two VOD
services: one uses traditional multicast, and the other uses
Buffer-Assisted On-Demand Multicast. To clarify
explanation, each client is labeled with its request time. In
Fig. 3(a), only the clients issuing at time 10:00 are served by
the multicast stream (the gray line). On the other hand, as
shown in Fig. 3(b), if the routing nodes (proxies) support the
timing control mechanism, the multicast stream can be
delivered to all receivers at their desired times, i.e., at times
10:02, 10:03, and 10:04, respectively.
(a) traditional multicast
(b) Buffer-Assisted On-Demand Multicast
Fig. 3. Comparisons of two multicast infrastructures.
In Buffer-Assisted On-Demand Multicast, a
fundamental assumption is that routing nodes (proxies) can
cache multicast packets and then forward them to different
network interfaces at specified times. Although this
cache-and-forward mechanism is still unavailable on routers
nowadays, it would be feasible in active networks [8–9], in
which the routing node can be programmed to support such
functionalities. In addition, this concept is also practical to be
implemented over peer-to-peer overlay networks [10–11] or
proxy platforms because peer nodes or proxies usually equip
some buffers. Compare to traditional multicast routing
problem, the routing of Buffer-Assisted On-Demand
Multicast is more complex because many additional
constraints must be considered. To serve a client request
efficiently, the system must carefully determine a routing
path and the corresponding buffer allocations (along the path)
for the client to maximize the throughput of the multicast
stream, i.e., reducing the total number of required multicast
streams. In this project, we formulate the BAODM routing
problem and present an optimal routing algorithm for
fully-connected overlay networks. Simulation results
demonstrate that BAODM outperforms many popular
streaming methods.
In a multicast-based streaming service, the system
performance is evaluated by the number of required multicast
streams. So in this project, we define the BAODM routing
problem as follows. Given a network topology G = (V, E)
and a source node S, let V represent the set of routing nodes
(proxies) and E represent the set of network links. All clients
are randomly associated to these routing nodes (proxies). Let
each routing node viV have a buffer of size bi (0bi), and
let each network link ej E have a bandwidth limit fj. What
is the minimum number of BAODM streams that can satisfy
all of the given requests?
Surprisingly, this problem over general graph networks
can be transformed to a longest path problem [12], which has
been proven NP-complete.
Given a graph G = (V, E), length l(e)Z+ for each edge
e E, a positive integer K, two specified vertices x, y V,
the question “is there a path in G from x to y of length K or
more?” is NP-complete.
For G = (V, E), we first remove the edges which do no
have enough available bandwidth to deliver the video stream.
The result graph G' = (V, E') is also a general graph. Let the
incoming client request be issued on y, the existent multicast
tree that delivers the desired video be GT = (VT, ET), and the
free buffer space along an edge e represent its length l(e). Let
the node x VT represent the video source for y, and K
represent the required buffer space to bridge y to x. Therefore,
the problem“if y can join the multicast stream GT?”becomes
“is there a path in G' from x to y contains K or larger buffer
space?”Since the free buffer space along the edge e is
6find a parent node that caches the required video clips as its
video source, thus reducing the server bandwidth. Since the
goal of RM is similar as the proposed BAODM, we then
compare the performances of BAODM and RM in this
section.
As the simulations in [14], we also simulate BAODM
over the IBM’s Global Network backbone, where each node
equips 375 MB (b = 50 minutes) of buffer space. To activate
the proposed routing algorithm, we construct a
fully-connected overlay network over this network topology.
The client requests are randomly generated on these routing
nodes (proxies), and the request arrival process follows a
Poisson distribution with an average inter arrival time of 1
second (λ= 1.0 request per second). We then vary the request
rate λand the buffer size b to investigate the required server
bandwidths of both methods.
0
1000
2000
3000
4000
5000
6000
7000
8000
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
request rate (request/second)
se
rv
er
ba
nd
w
id
th
(s
tre
am
)
BAODM
RM
(a) request rateλ
0
1000
2000
3000
4000
5000
6000
7000
10 20 30 40 50 60 70 80 90 100
buffer size (minute)
se
rv
er
ba
nd
w
id
th
(s
tre
am
)
BAODM
RM
(b) buffer size b
Fig. 4. Performance comparisons between RM and the
proposed BAODM.
The simulation results are depicted in Fig. 4. As Fig. 4(a)
illustrates, the server bandwidth required by BAODM
increases almost linearly as the increase of the request rateλ,
but that of RM increases sharply when λis larger than 1.0
request per second. This phenomenon implies that BAODM
is especially appropriate for heavy load VOD systems. On
the other hand, the results shown in Fig. 4(b) demonstrate
that BAODM performs well even if the given buffer space b
is tightly restricted, whilst RM is not so efficient if b is small.
That is, BAODM is also appropriate for VOD systems with
limited network buffers.
0
500
1000
1500
2000
2500
3000
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
skew factor
se
rv
er
ba
nd
w
id
th
(s
tre
am
)
BAODM
RM
Fig. 5. Impact of skew factor.
The video access probability reflects the request
behavior of users, so it would also be an important system
factor. In the next simulation, we set the request rateλas 1.0
request per second and the buffer size b as 50 minutes. Other
configurations are the same as the default settings. Then we
vary the skew factor of the Zipf’s-like distribution from 0
to 1.0 to investigate the required server bandwidths of both
methods. The simulation result is presented in Fig. 5,
illustrating that BAODM outperforms RM under all values of
.
The above simulation results demonstrate that the
proposed BAODM requires less server bandwidth than RM.
This therefore implies that BAODM utilizes network buffers
more efficiently than RM, and the buffer utilization would
dominate the performance of a VOD system with heavy load
or limited resources.
四、計劃成果自評
The contributions of this project have two. The first one is
that we propose a hybrid caching scheme to utilize the
advantages of the prefix caching and the interval caching
whilst also overcoming their drawbacks for video streaming
under heterogeneous environments. While serving
heterogeneous clients, a proxy allocates some prefix caches
for buffer-unlimited clients and some interval caches for
buffer-limited clients. In hybrid caching, the client devices
can be heterogeneous and each client can begin its playback
immediately. The second contribution of this project is that
we further proposed a proxy-level multicast scheme, named
Buffer-Assisted On-Demand Multicast, between proxies to
reduce server load and network bandwidth. We formulated
the BAODM routing problem which involves determining an
optimal routing path and the corresponding buffer allocations
for each request, and then proved that the time complexity to
solve this problem over general graph networks is
NP-complete. Besides, we proposed an optimal routing
algorithm for fully-connected overlay networks to minimize
server load. Simulation results demonstrate that BAODM
outperforms a popular streaming method: Range Multicast.
五、參考文獻
[1] A. Dan and D. Sitaram, “A Generalized Interval 
Caching Policy for Mixed Interactive and Long Video
Environments,” in Proc. IS&T SPIE Conf. Multimedia 
Computing and Networking, Jan. 1996, pp. 344–351.
[2] S. Sheu, K.A. Hua, and W. Tavanapong, “Chaining: A
Generalized Batching Technique for Video-on-Demand
Systems,” in Proc. Int. Conf. Multimedia Computing 
and Systems, Jun. 1997, pp. 110–117.
[3] E. Bommaiah, K. Guo, M. Hofmann, and S. Paul,
“Design and Implementation of a Caching System for
Streaming Media over the Internet,” in Proc. IEEE 
Real-Time Technology and Applications Symposium,
May 2000, pp. 111–121.
[4] L.-S. Juhn and L.-M. Tseng, “Fast Data Broadcasting 
and Receiving Scheme for Popular Video Service,” 
IEEE Trans. Broadcasting, vol. 44, no. 1, Mar. 1998, pp.
100–105.
[5] B. Wang, S. Sen, M. Adler, and D. Towsley, “Optimal 
Proxy Cache Allocation for Efficient Streaming Media
Distribution,” in Proc. IEEE INFOCOM’02, vol. 3, Jun.
2002, pp. 1726–1735.
[6] Dan and D. Sitaram, “Bufer Management Policy for an
On-Demand Video Server,” IBM Research Report RC 
19347.
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            95 年 8 月 24 日 
報告人姓名  黃世育 服務機構及職稱 
 
銘傳大學資訊工程系副教授 
 
     時間 
會議 
     地點 
95.8.14-95.8.16 
夏威夷  美國 
本會核定
補助文號
 
NSC 94-2213-E-130-013- 
會議 
名稱 
(中文) 網際網路與多媒體系統即應用國際會議 IMSA 2006 
(英文) Internet and Multimedia System and Applications IMSA 2006 
發表 
論文 
題目 
(中文) 以門檻值為基礎運算量分配機制演算法用於雙向運動影格內插 
(英文) Threshold-based Computation-Aware Scheme for Overlapped 
Bi-directional Motion Compensated Frame Interpolation 
報告內容應包括下列各項： 
一、參加會議經過 
IMSA2006 會議是由 IASTED 協會主辨, 今年是第十屆, 該會議每年召開一次，歷屆
會議均有眾多先進國家之專家學者與會，論文涵蓋理論與實務應用，層面廣泛而多樣。
本會議參與人士主要是美國,英國,日本人士居多, 且主要是來自技術導向的學校與機
構。會議日期為 8 月 14 日至 8 月 16 日, 其間除了論文發表之外也安排了多場的專題演
講及討論, 希望能做多元的經驗分享與交流。這次會議的主席是任職紐約大學的黃正能
教授, 他主要的研究也是在視訊壓縮與視訊通訊上。個人之論文發表安排在 8 月 16 日下
午 14:00，發表的論文為以門檻值為基礎運算量分配機制演算法用於雙向運動影格內插,
在該 Section 中第二個報告。在討論的過程內, 有學者建議可將 MPEG 中已有的 Motion 
Vector 整合在我們所提的演算法上, 這個建議相當寶貴, 規劃朝此方向繼續研發。 
 
二、與會心得 
此次會議內容, 主要是探究網際網路與多媒體系統之相關技術與應用, 此國際會議
發表之論文主要是發表實務上的應用為主, 至於在學理上的重大創新較少。參加此次研
討會在討論過程中, 除了增強了個人研究的視野及定位外, 亦獲知了相關專家的研究論
點與走勢，收獲良多, 特別是在網際網路與多媒體相關應用系統的趨勢, 個人感覺宜將
部份的研究精力置於應用系統上之研發, 可朝向產品開發努力。 
 
 
