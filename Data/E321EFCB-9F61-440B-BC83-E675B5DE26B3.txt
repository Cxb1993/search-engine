inter-case dependency and (2) sorting test cases 
based on severity and complexity. Inter-case 
dependency can be formulated as a sequential ordering 
problem (SOP), a NP-hard problem for which the 
precedence relationship exists. We apply the Ant 
Colony Optimization meta-heuristic to prioritize the 
test cases of an open-sourced software, TerpPaint 
5.0, which was developed at the University of 
Maryland. 
Results: ACO performs well for a case study problem 
taking into account of inter-case dependency, 
severity and complexity. Our approach makes sure that 
test cases follow the precedence relationship and 
test cases are prioritized based on severity and 
complexity. 
Conclusion: The paper investigates the inter-case 
dependency between test cases under the black-box 
testing environment as well as considering other 
factors severity and complexity. With the meta-
heuristic approach, our method is adaptable to number 
of test cases present in test case prioritization. 
 
英文關鍵詞： Black box testing, test case prioritization, software 
testing, inter-case dependency,Meta-heuristics, Ant 
Colony Optimization 
 
2 
行政院國家科學委員會專題研究計畫成果報告 
中文運用萬用啟發式演算法於軟體案例優先順序的排列在黑箱測試環境裡 
Prioritizing Software Test Cases based on a Meta-heuristic Framework in  
a Black Box Testing Environment 
中文摘要 
在軟體開發的流程中，軟體測試階段是非常重要卻經常被忽略的。當有限的資源和缺乏時間
及人力的情況下，軟體測試工程師必須在截止日期前完成特定的軟體計劃測試。『測試案例
優先性』是其中一種用來解決這個難題的方法，『測試案例優先性』是根據測試條件的重要
性進行排列優先順序。以現有研究文獻中，『白箱測試環境』中完成測試工作，即軟體程式
碼和內部結構已知的情況。現實環境中，測試團隊很可能在『黑箱測試環境』中執行測試，
即軟體程式碼和內部結構皆無法得知時，以使用者角色進行測試。根據多年的軟體測試實務
經驗和著重軟體測試者的觀點，我們針對測試案例提出以下幾點排列的『優先順序』：（1）
軟體測試案例嚴重性,（2）程式碼複雜度，（3）測試案例彼此之間的依賴性。根據這些的因
素我們應用了蟻群最佳化演算法來排序一個開放碼軟體測試案例—馬里蘭大學開發的 
TerpPaint 5.0。 結果呈現所提出之方法是可用於測試排序上。 
 
關鍵詞：白箱測試、黑箱測試、測試案例優先性（Test case prioritization）、軟體測試、
萬用啟發式演算法、蟻群最佳化演算法 
Abstract 
Context: Test case prioritization has been researched extensively in the past decade. Current 
researches carried out on test case prioritization are mainly concerned with independent test cases in 
the white-box testing environment. In a black-box testing environment, however, some test cases 
are inter-case dependent and must follow certain sequences of execution. For example, destructive 
test cases such as the stress testing or backup testing are usually performed after other test cases are 
done. In another scenario, a certain test case A is possibly an initialization sequence for a second 
test case B. Therefore, the precedence relationship of test cases must be taken into consideration. 
Objective: The objective of this research is to propose several "prioritizing factors" that better 
reflects real-world scenario for test case prioritization under the black-box environment: (1) 
inter-case dependency, (2) severity and (3) complexity and optimize the test case arrangement 
through the application of meta-heuristics. 
Method: The method for prioritizing the test cases can be divided into two stages: (1) solving for 
inter-case dependency and (2) sorting test cases based on severity and complexity. Inter-case 
dependency can be formulated as a sequential ordering problem (SOP), a NP-hard problem for 
which the precedence relationship exists. We apply the Ant Colony Optimization meta-heuristic to 
4 
 
一、 報告內容 
(報告內容請依國科會結案報告格式撰寫，例如：前言、研究目的、文獻探討、研究方法、結
果與討論。各節編號請自訂。) 
1. Introduction 
Software testing is one of the major activities performed in the software development cycle. 
Essentially, the software testing is the "gate keeping" stage necessary to ensure the quality of 
software has met customers' expectations. To test the software functionalities, the software testers 
would design and execute a list of test cases. A test case is a detailed step-by-step procedure which 
examines some aspects of the software, including inputs and outputs, the expected results and other 
relevant elements [1]. IEEE provides the guidelines for designing test cases in the standard IEEE 
829-1998 where the following sections should be included: test case specification identifier, test 
items, input specifications, output specifications, environmental needs, special procedural 
requirements, and inter-case dependencies [2]. A collection of test cases is collectively referred to 
as a test suite which examines all aspects (behaviors and operations) of a particular software 
program.  
Traditionally, the test suite execution planning is performed manually between the software test 
engineers and project managers. The criteria of arranging the test case include the test case 
severities and complexities for executing particular test cases. The manual approach is fine as long 
as there are less than hundreds of test cases. As the software system are becoming more and more 
complex and sophisticated, a whole lot more test cases are generated as the results. The traditional 
manual approach just would not work well.  
Test case prioritization based on certain criteria has been discussed in literatures. Rothermel, 
Untch, Chu and Harrold [3] have coined the word ― “test case prioritization problem” and given it 
the formal definition:  
Given: T, a test suite, PT, the set of permutations of T, and f, a function from PT to the real 
numbers. 
Problem: Find T PT such that ( )( )( )[ ( ) ( )]T T PT T T f T T          
In the definition, PT represents the set of all possible prioritizations (orderings) of T, and f is a 
function that, applied to any such ordering, yields an award value for that ordering. 
Several techniques have been developed by Rothermel et. al [3] to prioritize the execution of 
existing test cases by exposing faults early in the regression testing process. They have also 
developed a weighted average of the percentage of faults detected, or APFD, which corresponds to 
the function f in the definition above. Because APFD is developed with the number of faults known 
in advance, it may not be practical for the black box testing environment [4]. Instead, another metric 
based on the run time execution and test history is proposed.  
Li et al. [5] study five search algorithms for regression test case prioritization, which include 
Greedy Algorithm, Additional Greedy Algorithm, 2-Optimal Algorithm, Hill Climbing, and Genetic 
Algorithm (GAs). They conclude that the Greedy Algorithm performs much worse than other 
6 
3.2 Inter-case dependency formulated as Sequential Ordering 
 
Interdependency among test cases can be formulated as a sequential ordering problem (SOP) 
with precedence relationship. The SOP is a NP-hard problem related to the travel salesman problem. 
Escudero [10] provided the definition of sequential ordering problem (SOP) whose objective is to 
find a Hamiltonian path with minimal weights on the arcs and nodes of a directed graph without 
violating the precedence relationships among nodes.  
The mathematical model for expressing the inter-case dependency is shown below: 
 
),(    min )1(
1
1
)( 


 kn
k
k CCd   
     
subject to  if d(Ci, Cj) = -1 
       for i = (k), j = (l) 
       then l < k 
 where  
 n : number of test cases;   : test case sequence; k : location;  d : dependency;  
 Ci : test cases ( i =  1, 2, …, n); Cj : test cases ( j =  1, 2, …, n); (k) : the kth position in the 
 test case sequence; (l) : the lth position in the test case sequence; C(k) : the test case at the kth 
 position of the sequence  
If the precedence constraint exists in test cases— for example test case i cannot be executed 
ahead of test case j— then the dependency is -1 between test case i and j. If the test case i is at the 
kth position of the sequence and test case j is at the lth position of the sequence, the test case i cannot 
be executed before the test case j, then the lth position of test case j is in front of test case i (at kth 
position) or l < k. According to the precedence constraint and the inter-case dependency scores 
among the test cases, we can calculate the dependency scores for the sequence. 
2.3 Severity 
Many researches base test case severity on fault severities or number of faults. However, fault 
severities or number of fault can only be obtained in the white-box testing environment. Instead, we 
measure the test case severities based on the impact of customer’s requirements in the black-box 
environment since each test case must be mapped to requirements. Therefore, the impacts of 
requirements are closely related to the test cases; a highly important requirement may have much 
higher chance of jeopardizing the software application compared to a less important requirement. 
Combining several studies on requirements and defects reporting [11-12], we categorize 
requirements into four priority levels to be applied to functional as well as non-functional 
requirements. For each priority level, a number is assigned ranging from 1 to 4, with category 1 
being core requirements and category 4 being optional requirements. Please see Table 1 for detailed 
information. Because there may be many requirements associated with one test case (i.e., 
many-to-one), the severity score associated with each test case is proposed to be: 
8 
equation 2:  
     )()( 21 CWSWWP       ( 2 ) 
where S represents the severity of test case, C the complexity, W1 and W2 weights for severity and 
complexity, respectively. The weights are subject to the user’s perceived importance for 
adjustment. For example, if W1 is 0.7 and W2 is 0.3, it represents the test case prioritization 
emphasizes on severity over complexity. This research is impartial to either and set W1 and W2 at 
0.5. 
 
The sequential order of best set of solution is improved based on prioritization weight without 
violating the precedence constraint. 
4. Experiment Setup 
4.1  Testing environment 
1. The research is conducted under the black-box testing environment. Even though the 
researchers have access to the source code, they neither analyze nor try to understand the 
inner structure of code in order to be “non-judgmental” toward the software. Rather, the 
researchers test the system-under-test (SUT) from the perspective of a user. 
 
2. The SUT is an application called “TerpPaint 5.0” developed by TerpOffice at University of 
Maryland. As a graphical tool similar to Microsoft Paint software application, TerpPaint 5.0 is 
free and open- source written in Java. 
3. The operating system is Windows XP with JDK 1.6 for opening TerpPaint 5.0 application. 
4.2 Threats to validity 
1. Test case design is based on the perspectives of software testers and users for test case analysis 
and design. 
2. The test cases are executed under the black-box environment. Although the testers do have the 
access to the source code, testers and users do not and should not understand the internal 
structure of the source code in fear of clouding testers’ impartiality. 
3. The main objective of the test case prioritization in this research is to find the optimized 
arranged of test cases. The capability of test cases to discover potential faults is not within the 
scope of this research. 
4. In this research we assume the test cases are done sequentially. 
5. Results and Analysis 
The proposed ant colony optimization combined with MPO/AI method (ACO+MPO/AI) for 
solving test case prioritization with inter-case dependency problem for comparison with GA 
MPO/AI [15].  
The proposed ACO/AI-MPO is compared with GA/AI-MPO over the TerpPaint 5.0 application 
10 
requirements, Information and Software Technology, 39 (1998) 939-947. 
[7] IEEE, IEEE 829-1998 Standard for Software Test Documentation, in:  Test Case Specification, 
IEEE Service Center, New York, 1998. 
[8] C. Kaner, J.L. Falk, H.Q. Nguyen, Testing computer software, 2nd ed., Wiley, New York, 1999. 
[9] A.K. Onoma, W.T. Tsai, M. Poonawala, H. Suganuma, Regression testing in an industrial 
environment, Communications of the ACM, 41 (1998) 81-86. 
[10] L.F. Escudero, An inexact algorithm for the sequential ordering problem, European Journal of 
Operational Research, 37 (1988) 236-249. 
[11] Y.-S. Lee, An Approach to Generating Test Cases for Non-functional Requirements, in:  
Department of Electrical Engineering, National Chung Cheng University, Chia Yi, Taiwan, 2003. 
[12] authors, Bugs and Fixes, in, SQAtester.com, 2000. 
[13] R. Krishnamoorthi, S.A.S.A. Mary, Factor oriented requirement coverage based system test 
case prioritization of new and regression test cases, Information and Software Technology, 51 (2009) 
799. 
[14] M. Dorigo, V. Maniezzo, A. Colorni, Ant system: Optimization by a colony of cooperating 
agents, IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 26 (1996) 
29-41. 
[15] S. Chen, S.F. Smith, Commonality and genetic algorithms, in, Carnegie Mellon University, 
Pittsburgh, PA, 1996. 
三、自評 
 The research is the first of its kind to consider inter-case dependency as well as other factors 
such as severity and complexity for test case prioritization in a black-box testing environment. The 
results from the research have filled in the knowledge gaps for academia and software industry: 
1. Academia: Researches in software test case prioritization have long been focusing on the 
white-box test environment where the software internal structure and coding are known 
and tested. In reality, software test engineers rarely have access to software source codes; 
even if they do, they tend to shy away from trying to understand the source codes for fear 
that they are influenced by software developers’ thinking. Therefore, this study has 
investigated the topic of test case prioritization from different angle.   
2. Industry: Typically software testing engineers manually prioritize test cases based on their 
own experiences and intuitions. Although different software projects may demand a set of 
different criteria for arranging software test cases, the study does provide an automated 
framework for the practitioners to build on when it comes to test case prioritization. 
The concepts and sample results of the research have been published in the 2011 兩岸工業工
程與管理學術研討會 and efforts are under way to submit the completed, detailed version to 
major journal publications. 
 2
conference. 
 June 28 We attended the workshop on “Publishing in top journals”, given by Professor Jianjun Shi, a 
 professor at the Georgia Institute of Technology, USA. A prolific researcher with diverse backgrounds 
 including electrical engineering, mechanical engineering and industrial engineering, he is currently a 
 editor for IIE Transactions on Quality and Reliability. He gave the talk on his experiences for submitting 
 papers to top journals, using examples from his own researches. Later in his talk, he introduced the idea 
 of “research tree”, which helped him organized research ideas and search for new directions.  
 June 29 The second day of conference kicked off with two keynotes speeches—one given by 
 aforementioned Professor Shi and the other by Professor Richard de Neufville of MIT. For his speech, 
 Professor Shi’s topic was on “Data Fusion for In-Process Quality Improvement of complex Systems” in 
 which he discussed handling massive data for complex systems. The second speaker, Professor 
 Neufville, proposed the idea of incorporating “flexibility” into the system design to take advantages of 
 new opportunities and avoid pitfalls in the future. 
 Later that afternoon, I was responsible for co-hosting the Facility Layout and Planning session with 
 Professor Kien-Ming Ng of NUS. In total five presentations including my own were given; it was 
 interesting to note that three out of five papers were presented by researchers from Taiwan.  
 June 30 The keynote speaker at the last day of the conference was Mr. Gregory H. Watson. Mr. Watson 
 is the Senior Vice President of IIE. A distinguished speaker and writer with numerous awards from 
 academia and governmental organizations, he extolled the high standard of industrial engineering 
 education in Asia but pointed out the future challenges faced by worldwide educators and students alike, 
 especially the true identity of industrial engineering which encompasses many different disciplines. 
  Since I only needed to present one paper at the conference, for the third day of the conference  I had 
 the opportunities to listen to presentations at other sessions. I had attended the Production Planning and 
 Control session, listening to the applications of meta-heuristics to production system—a topic I am really 
 involved in and deeply interested. Additionally, I had listened to researchers presented their findings at 
 the information processing and engineering session. 
  
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/15
國科會補助計畫
計畫名稱: 運用萬用啟發式演算法於軟體案例優先順序的排列在黑箱測試環境裡
計畫主持人: 陳育欣
計畫編號: 100-2221-E-033-030- 學門領域: 生產系統規劃與管制
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
