pixels within the cells in each block are then voted 
into rectangular- and circular-type 9-bin histograms 
of oriented gradients (HOGs) in accordance with their 
gradient magnitudes and corresponding multivariate 
Gaussian-weighted windows. Finally, four cell-based 
histograms are concatenated using a tri-linear 
interpolation technique to form one 36-dimensional 
normalized HOG feature vector for each block. The 
experimental results show that the use of the 
Gaussian-weighted window approach and tri-linear 
interpolation technique in constructing the HOG 
feature vectors improves the detection performance 
from 91% to 94.5%. 
In the proposed scheme, the detection process is 
performed using a cascaded detector structure in 
which the weak classifiers and corresponding weights 
of each stage are established using the AdaBoost 
self-learning algorithm. The experimental results 
reveal that the cascaded structure not only provides 
a better detection performance than many of the 
schemes presented in the literature, but also 
achieves a significant reduction in the computational 
time required to classify each input image. 
英文關鍵詞： Pedestrian detection, histogram of orientated 
gradients, Gaussian-weighted window,  cascaded 
AdaBoost detector. 
 
 2 
Pedestrian Detection System Using Cascaded Boosting  
with Invariance of Oriented Gradients 
 
摘要 
 本研究呈現新穎的學習式 (learning-base) 行人偵測系統，本系統能夠從人群等各式各樣的背
景之中，自動偵測不同尺寸和方向的獨立個人，即使人體的部分被阻擋仍可偵測。為了使系統得以
更有效的偵測原始影像中幾何和旋轉變動的程度所造成的影響，特徵選取的過程同時使用不同大小
及各種長寬比的矩形種類和圓形種類區塊。選取的區塊會轉向他們所支配的方位，如此一來所有從
輸入影像選取出來的區塊將不會旋轉。每個區塊內的細胞單元內的像素將對矩形種類和圓形種類的
區塊的 9 個區段的有向性梯度直方圖 (HOG) 按照他們的梯度大小和相應的多元高斯加權窗口進
行投票。最後，以四個細胞單元為基礎的直方圖使用三線性插值技術進行串聯，為每個區塊形成一
個 36 維的正規化有向性梯度直方圖 (HOGs) 的特徵向量。實驗結果顯示，使用高斯加權視窗的方
法和有三線性插值技術建構有向性梯度直方圖的特徵向量，將使得檢測性能從 91％提升到 94.5％。
根據我們提出的方法，在檢測過程中使用串聯的分類器結構，並且使用 AdaBoost 自我學習演算法
建立其中每一層級的弱分類器和相應的權重。實驗結果表示，串聯結構提供的偵測效能不僅比許多
文獻中提出的方法更好，而且在縮減對輸入圖像進行分類所需的運算時間也有顯著的效果。 
 
關鍵詞: 行人偵測、方向梯度直方圖、高斯加權視窗、串接型適應性分類器 
 4 
1. Introduction 
 
Achieving the reliable automatic detection of humans is extremely challenging since the 
objects of interest have a virtually limitless range of appearances due to differences in their size, 
shape, pose, clothing and so forth. The detection problem is further exacerbated in outdoor 
environments, which are invariably characterized by cluttered backgrounds, variable lighting 
conditions, high levels of occlusion, object grouping, and so forth. However, human detection is 
a key requirement in many civil and military applications, such as surveillance systems, 
unmanned ground vehicles (UGVs), intelligent driver warning systems, assisted-driver schemes, 
and so on. Accordingly, the problem of human detection has attracted intense interest in recent 
decades, and the literature contains many detection schemes based on a variety of feature 
representations and learning methods.  
In 1999, Gavrila [6] presented a survey of recent developments in what he termed the 
“looking at people” domain, with particular emphasis on the detection of whole-body or hand 
motion rather than human face detection. In more recent studies, Han and Bhanu [9] and Pai et al. 
[19] presented human detection schemes in which a background subtraction technique was 
employed to perform an initial segmentation of the foreground moving pedestrian from the 
stationary background. Sidenbladh [29] presented a method for pedestrian detection in image 
sequences using motion information and a support vector machine (SVM) algorithm. Yang et al. 
[37] utilized an appearance model and weighted temporal texture features to detect pedestrians in 
video sequences. Wöhler [33] developed a real-time pedestrian recognition system in which a 
time-delay neural network was used to identify image sequences whose spatial-temporal 
structure characteristics were similar to those of pedestrian patterns. Kang et al. [11] applied a 
stereo-based segmentation approach and a SVM classification scheme to accomplish the 
real-time detection of pedestrians in video images. Mikolajczyk et al.[15] and Munder and 
Gavrila [18] employed a single hierarchical codebook representation and a SVM classification 
scheme based on local receptive field (LRF) features, respectively, to accomplish the detection of 
moving pedestrians in image sequences. 
Many methods for static human detection represent the object of interest into a structured 
arrangement of articulated body parts. For example, Papageorgiou and Poggio [20] presented a 
polynomial SVM method for the detection of pedestrians in which the human body was 
segmented into four components, namely the head, the legs, the left arm and the right arm, 
respectively. Felzenszwalb and Huttenlocher [5] and Ioffe and Forsyth [10] developed human 
detection schemes in which each component of the articulated body structure was detected using 
a Gaussian derivative filter with different scales and orientations. Wu and Yu [36] proposed a 
technique for detecting and tracking humans by using a two-layer statistical field model to 
characterize the prior of the pedestrian shape variations as a Boltzmann distribution, and 
 6 
Fig. 1. Workflow of the four modules within the training process. 
performance by replacing the dense HOG grid by a cascade-of-rejecter approach with HOG 
features comprising blocks of various sizes. The experimental results showed that the system 
retained a similar level of accuracy as that presented in [2], but with a computational time around 
70 times faster. 
Drawing upon the contributions of the studies presented above, the current study develops a 
pedestrian detection system consisting of a training process and a test process, as shown in Figs. 
1 and 8 (in page 15), respectively. The training process contains four modules, namely Modules 
1~3, designed to extract the feature information from the input images and to construct the 
corresponding HOG feature vectors, and Module 4, designed to construct a structured detector 
using the AdaBoost (Adaptive Boosting) self-learning algorithm [31]. The first three modules for 
the feature extraction scheme generate a large number of blocks of different shapes (i.e. 
rectangular or circular) and sizes containing feature information characterized by different 
dominant orientations. For each block, the system performs an automatic orientation rectification 
procedure in order to compensate for the effects of geometric and rotational variances among the 
input images. Note that in this regard, the current scheme differs markedly from the detection 
approaches presented by Dalal and Triggs [2] and Zhu et al. [39], respectively, in which the 
orientation information of the blocks was not considered. Significantly, the experimental results 
1. Feature extraction in gradient domain 
Positive training images Negative training images 
64 
128 
2. Block quantization and 
rotational invariance 
Ex: 
α 
Rotated 
α 
 
16 
16 
0º 
0º 
Feature Extraction Scheme: 
3. HOG feature construction: 
HOG positive or negative 
feature vector 
 
fpos={…}36 fneg={…}36 
8 8 
8 
8 
4. Learning process using cascaded AdaBoost detector 
Acceptance of 
64×128-pixel 
samples 
(positive 
images) 
Rejection of 64×128-pixel samples 
(negative images) 
64×128-pixel 
positive or 
negative 
training 
images or 
samples 
 Stage 
1 
 Stage 
2 
 Stage 
n 
Pedestrian Detection Scheme: 
 8 
cascaded AdaBoost detector module, selects a small set of distinctive and discriminative HOG 
features using an AdaBoost self-learning algorithm and constructs a cascaded detector module 
for differentiating between the human and non-human contents of an image. 
 
2.1. Feature extraction in gradient domain module 
The training images used in the current system all have a resolution of 64×128 (width × 
height) pixels. The images were taken at different times and locations in outdoor environments, 
and have significant illumination variances as a result. Therefore, this module is designed to 
extract the high-frequency gradient information of the training images for further processing 
since this information is known to be insensitive to variations in the lighting conditions [2]. The 
gradient components of all the image pixels within the positive and negative training images are 
extracted using a 1-dimensional discrete derivation mask [-1, 0, 1], as shown in Fig. 1.1. The 
central differences in the x- and y-directions at pixel location (x, y) are denoted as dx(x, y) and 
dy(x, y), respectively, and are given by 
),1(),1(),( yxIyxIyxd x  ,                          (1) 
and 
)1,()1,(),(  yxIyxIyxd y ,                          (2) 
where I(x, y) is the gray value of the pixel located at coordinates (x, y). Having computed the 
central differences of the pixel located at (x, y) in the x- and y-directions, respectively, the 
gradient magnitude m(x, y) of the pixel is computed in accordance with  
22 ),(),(),( yxdyxdyxm yx  .                         (3) 
Finally, the orientation of the gradient of the pixel at (x, y) relative to the x-axis direction, 
designated as the directional angle θ, is computed as follows: 
 ),(),(),( 1 yxdyxdtanyx xy .                        (4) 
Having processed the training image (either positive or negative), the gradient magnitude m and 
directional angle θ of each pixel in the image are passed to the second module. 
 
2.2. Block quantization and rotational invariance module 
Block quantization: In the detection scheme presented by Dalal and Triggs [2], the HOG 
features of the input images are constructed using blocks of a fixed size (16×16 pixels), where 
each block consists of a 2×2 arrangement of spatial cells (or grids), each with a size of 8×8 pixels. 
However, the use of a fixed block size inevitably limits the amount of local feature information, 
 10 
Fig. 3. Schematic illustration showing orientation rectification mechanisms in block 
quantization and rotational invariance module. 
Finally, in this module, each 64×128-pixel gradient image is scanned pixel-by-pixel from its 
top-left corner to the bottom-right corner, called scanning operation, using both the rectangular- 
and circular-type block structures, resulting in a total of 10062 blocks for each image (i.e. 5031 
rectangular-type blocks and 5031 circular-type blocks).  
 
Block rotational invariance: In determining the dominant orientation(s) of each block, this 
module adopts a similar approach to that applied by Lowe [14]. In other words, the directional 
angles of gradient components of all the pixels in the block are voted into a 36-bin orientation 
histogram in which the bins are evenly spaced over the range 0
º–360º (i.e., the interval for each 
bin is 10
º
). As shown in Fig. 3 (a), for each pixel, the weight of its contribution to the orientation 
(b) Automatic assignment of one or more dominant orientations to each block. 
Major peak  
value 
130
º
 320
º
 
80% of  
major peak 
value 
Other peak  
value 
α1 α2 
 
 
Block 
α2 
 
α1 
* 
= 
Block 
Gradient  
directional  
angles 
Gradient  
magnitude 
Gaussian- 
weighted  
window 
Weighted 
gradient 
magnitude 
36-bin orientation histogram 
(a) Voting of gradient components of the pixels in the block into a 36-bin orientation histogram. 
 12 
Fig. 5. Illustrative example that two different blocks from two different images look similar 
to each other after being rotated based on their dominant orientations to achieve the 
rotational invariance for each block.  
If only the first dominant orientation is considered, the rotated results of blocks 1 and 2 are 
still different from each other. However, if the second dominant orientation is considered as well, 
block 1 after being rotated by α1 and block 2 after being rotated by β2 look similar to each other. 
Consequently, by choosing multiple dominant orientations, the result of the rotational invariance 
for each block can be achieved much robustly. Note that in the schemes presented in [2] and [39], 
the orientation information of each block is ignored. The experiments presented later in this 
study reveal that this causes the detection performance to decrease from 94.5% to 90% at 10
-4
 
FPPW (False Positive Per Window). 
 
2.3. HOG feature construction module 
The HOG feature is a non-linear normalized gradient feature vector (or descriptor) similar 
to the Scale Invariant Feature Transform (SIFT) feature vector presented in [14]. However, in the 
SIFT approach, the aim is to extract a sparse set of scale-invariant interesting regions or blocks, 
which are rendered rotationally invariant by rectifying their dominant orientations and then 
processed individually, whereas in the HOG approach [2], the goal is to extract a dense set of 
blocks at a single scale without rectifying the dominant orientation(s) of each block. As 
described in Section 2.2, for each positive or negative 64×128-pixel training image, the block 
quantization and rotational invariance module extracts a total of 10062 blocks of different types 
and sizes, which are then rendered rotationally invariant by rotating the blocks in accordance 
with their dominant orientation(s). Having done so, the HOG feature construction module  
Rotated 
High 
similarity 
α1 
 
α2 
β1 
 
 
β2 
 
Rotated 
α1 
 
α2 
 
 
β1 
 
β2 
 
Block 1  
Block 2  
 14 
range 0
º–180º (the interval for each bin is 20º). If the directional angle θ is within the range 181 
º–360 º, we are considered as 360 º − θ. Finally, the orientation histograms of the four cells are 
concatenated to form a single 36-bin (i.e. 36-dimensional) HOG feature vector, f. Directly 
concatenating the orientation histograms of two cells induces an aliasing (i.e. suddenly changing) 
curvature problem [14]. Therefore, the HOG feature construction module applies a tri-linear 
interpolation technique [14] to smoothly distribute the weighted gradient magnitudes of the 
pixels in a block across adjacent histogram bins and neighboring cells. Having formed a 
36-dimensional HOG feature vector for each rectangular- or circular-type block, the feature 
vectors are normalized to a unit length using the L
2
-norm method presented by Lowe [14]. 
Furthermore, during the scanning operation, the weighting process can also smooth the transition 
of the orientation histograms from one block to the next and therefore minimize the risk of 
mis-registration errors [14] between the HOG feature vectors in the training image set and those 
in the test image set. Consequently, Zhu et al. [39] demonstrated that the use of variable-size 
blocks in the feature extraction process yielded a higher detection accuracy than schemes based 
upon a fixed block size such as that presented by Dalal and Triggs [2]. However, in contrast to 
the present study, the scheme presented in [39] used neither a multivariate Gaussian-weighted 
window approach nor a tri-linear interpolation technique when constructing the HOG feature 
vectors. The experimental results presented later in this study reveal that the omission of these 
two steps causes the detection performance to reduce from 94.5% to 91% at 10
-4
 FPPW.   
 
2.4. Learning process using cascaded AdaBoost detector module 
As described above, a minimum of 10062 36-dimensional normalized HOG feature vectors, 
f, are constructed from each positive or negative 64×128-pixel training image. As a result, the 
training database contains a huge number of HOG feature vectors, and therefore the learning 
process requires the use of a computationally efficient scheme to ensure that the associated time 
and cost are restricted to an acceptable level. Accordingly, in the learning process implemented 
in this study, the AdaBoost algorithm is used to select a small set of distinctive and 
discriminative HOG feature vectors, i.e. weak classifiers, which are then integrated to form a 
single strong classifier. Each weak classification having the lowest error rate at each iteration 
process is decided by evaluating the positive (or human) and negative (or non-human) images in 
the training database. 
In each iteration t of the AdaBoost algorithm, one of the normalized HOG feature vectors, 
designated as ft, is selected as a weak classifier ht. The similarity measurement function of the 
weak classifier is formulated in terms of the following normalized difference score s: 
t
t
t
f
f
x
x
fxs ),( ,                                (7) 
 16 
Fig. 7. Statistical analyses of cascaded AdaBoost detector. 
in the cascade comprises three normalized HOG feature vectors and rejects approximately 57% 
of the 64×128-pixel non-human images or samples (i.e. negative images) (see Fig. 7 (b)), whilst 
correctly detecting nearly 100% of the 64×128-pixel human images (i.e. positive images). The 
next stage comprises four normalized HOG feature vectors and rejects 79% of the non-human 
images whilst detecting almost all the human images. New stages were added incrementally until 
the false positive rate falls to almost zero, while still maintaining a high correct detection rate. 
Fig. 7 illustrates the number of weak classifiers in each stage of the cascaded AdaBoost detector 
and shows the corresponding cumulative rejection rate. 
 
3. Test Process 
 
In the test process, as shown in Fig. 8.1, each input 320×240-pixel image is iteratively 
down-sampled using a scaling factor of 8/9 from the original resolution of 320×240 pixels (level 
0) to a final resolution of 178×133 pixels (level 5). The height of the image at level 5 is slightly 
larger than the height of the detection window (64×128 pixels). During the down-sampling 
process, detection windows are generated at each level by shifting the window pixel-by-pixel 
from the top-left corner to the bottom-right corner of the image. As a result, around 70000 
windows are produced for each input image.  
Subsequently, as shown in Fig. 8.2, the stage 1 of the cascaded AdaBoost detector is applied 
to evaluate the strong classifier using equation (9) and consisting of three weak classifiers, i.e., 
three normalized HOG feature vectors (see Fig. 7 (a)), which contain parameters relating to the 
types, sizes and locations of three corresponding blocks. In addition, before inputting the block  
(a)  
Number of weak classifiers in each cascaded stage 
 
N
u
m
b
er
 o
f 
w
ea
k
 c
la
ss
if
ie
rs
 
Stage 
(b)  
Accumulated rejection rate over cascaded stages 
 
R
ej
ec
ti
o
n
 r
at
e 
Stage 
 18 
of each detection window to the detector, the feature extraction scheme is applied. First, we 
create high-frequency gradient components and then extract the local feature information based 
on corresponding type, size and location of the block. Second, we rectify the dominant 
orientation(s) of the block in order to render the block rotational invariance. Third, we utilize the 
gradient information associated with all the pixels within each rotationally-invariant block to 
construct the corresponding HOG feature vector. Accordingly, each detection window can then 
be classified into either positive window, i.e. the passing detection window, or negative window, 
i.e. the non-human or rejection window, based on the stage-1 strong classifier. Successively, the 
stage-i strong classifier is used to classify the passing detection windows from stage i-1 into 
either positive or negative windows. After being processed by all n-stage cascaded strong 
classifiers, the remaining passing detection windows are considered as the candidates of detected 
humans for the input test image. 
Redundant detection windows frequently exist in the region of the image containing a 
human object. These redundant windows may be located in the original (i.e. level 0) image or in 
any of the lower level images. In [31], it was shown that a good face detection performance 
could be obtained by combining all the overlapping candidate windows. However, imposing a 
non-overlapping constraint on the redundant detection windows is impractical for pedestrian 
detection schemes such as that considered in the present study since the objects of interest are 
commonly part of a crowd and are therefore inevitably overlapped to a greater or lesser extent in 
the 3-D image space. Accordingly, the present study resolves the overlapping window problem 
by using the non-maximum suppression method presented in [2], in which a mean-shift model 
detection procedure was used to locate a pre-defined model density (corresponding to a human in 
the current case) in the 3-D position (x, y, response) and scaling space. Therefore, the list of all 
the located models gives the final fused detection. 
 
4. Experimental Results 
 
4.1. Data collection and performance evaluation method 
Data collection: Two databases, namely the INRIA and MIT pedestrian databases, 
respectively, were downloaded from the websites referenced in [2] and [40], respectively. The 
contents of the two databases are summarized in Table 1. The INRIA pedestrian database was 
used previously in [2], [23], [27], [30], [35] and [39] and comprises a training database with 
2416 64×128-pixel human images and 1218 non-human images, and a test database consisting of 
741 images, of which contains 555 humans of various sizes and orientations in a wide variety of 
upright poses. Meanwhile, the MIT pedestrian database [40] was used previously in [4], [21], 
[22], [23], [27], [28], [34] and [35] and consists of 924 64×128-pixel human images. As shown  
 20 
Fig. 9. Comparison of detection performance obtained using four different block strategies in 
the block quantization and rotational invariance module of the feature extraction 
scheme 
Fig. 10. Comparison of detection performance obtained with and without block orientation 
rectification procedure in the block quantization and rotational invariance module. 
510 410 310 210 110
0.3 
0.2 
0.1 
0.05 
0.02 
0.01 
FPPW 
m
is
s 
ra
te
 
510 410 310 210 110
0.3 
0.2 
0.1 
0.05 
0.02 
0.01 
FPPW 
m
is
s 
ra
te
 
 22 
Fig. 12. Comparison of detection performance obtained by current system with that obtained 
by the schemes presented in [2], [23], [27], [30], [35] and [39]. 
 
Dalal and Triggs [2] showed that HOG features facilitate a better human detection 
performance than other feature extraction methods such as Harr-like wavelets [32], PCA-SIFT 
[12], or shape contexts [1]. Therefore, here we compare only recent approaches in [2], [23], [27], 
[30], [35], and [39] with our proposed system for the final experiment. The results presented in 
Fig. 12 indicate that the current system achieves a better detection performance, i.e. a lower miss 
rate, than the schemes presented in [2], [27], [30], [35] and [39] at 10
-4
 FPPW. Furthermore, 
while the current system has a poorer detection performance than the method presented in [23] at 
10
-4
 FPPW, it outperforms the method [23] at 10
-5
 FPPW. Moreover, we observe that the HOG 
features having various types, sizes, locations and orientations can achieve much higher 
detection accuracy by using the cascade AdaBoost approach. The superior performance of the 
current scheme is attributed primarily to the ability of the feature extraction process to 
distinguish between the informative feature blocks and the background region blocks. Fig. 13 
presents several typical detection results obtained using the current system. These results indicate 
that current system can detect humans with different sizes and orientations against a wide variety 
of backgrounds, including crowds, even when the individual is partially occluded. 
For the computational time evaluation based on the test data set, it was determined 
experimentally that the current system requires an average of 8.5 blocks (i.e. 8.5 normalized 
510 410
310 210 110
0.3 
0.2 
0.1 
0.05 
0.02 
0.01 
FPPW 
m
is
s 
ra
te
 
 24 
5. Conclusions 
  
This study has presented a novel learning-based pedestrian detection system consisting of a 
training process and a test process. The training process comprises four modules, namely three 
feature extraction modules and one learning module. The feature extraction process commences 
by extracting the high-frequency gradient information of each input image in order to suppress 
the effects of illumination variations and noise among the various input images. The image is 
then scanned by six blocks (three rectangular-type and three circular-type) with various sizes and 
aspect ratios, and the dominant orientation(s) of each block are identified by voting its pixels into 
a 36-bin orientation histogram. Having done so, the blocks are rendered rotationally invariant by 
performing an orientation rectification procedure in accordance with its dominant orientation(s). 
Finally, the feature information within each rectangular- or circular-type block is encoded in the 
form of a 36-dimensional HOG feature vector. In the learning module for the pedestrian 
detection process, a small set of distinctive and discriminative HOG feature vectors are 
automatically selected using the AdaBoost algorithm and are used to construct a cascaded 
detector. The detection performance of the proposed system has been evaluated by performing a 
series of experimental trials. The experimental results have shown that the use of a variable block 
size during the feature extraction process yields a significant improvement in the detection 
performance. Furthermore, it has been shown that while the use of circular-type blocks results in 
a greater detection accuracy than the use of rectangular-type blocks, the optimal detection 
performance is obtained when both the rectangular-type and the circular-type blocks are used to 
extract the features of the input image. In addition, the experimental results have shown that the 
use of the Gaussian-weighted window approach and the tri-linear interpolation technique in the 
HOG feature construction module yields a 3.5% improvement in the detection performance of 
the system (i.e. from 91.0% to 94.5%). Moreover, the cascaded AdaBoost detector structure 
enhances the efficiency of the detection process and reduces the computational time required to 
process a 320×240-pixel image to an average value of 0.55 seconds, which is around 12.3 times 
faster than that achieved by the scheme presented in [2]. Finally, when applied to the INRIA data 
set, the proposed detection system achieves a lower miss rate than the schemes presented in [2], 
[27], [30], [35], and [39] at 10
-4
 FPPW and approaches that of the method presented in [23] at 
10
-5
 FPPW.  
 
 
 
 
 26 
[14] D. Lowe, “Distinctive Image Features from Scale-Invariant Keypoints,” International 
Journal of Computer Vision, vol. 60, no. 2, pp. 91–110, 2004. 
[15] K. Mikolajczyk, B. Leibe, and B. Schiele, “Multiple Object Class Detection with a 
Generative Model,” IEEE Conference on Computer Vision and Pattern Recognition, pp. 
26–36, 2006. 
[16] K. Mikolajczyk, C. Schmid, and A. Zisserman, “Human Detection Based on a Probabilistic 
Assembly of Robust Part Detections,” European Conference on Computer Vision, pp. 69–81, 
2004. 
[17] A, Mohan, C. Papageorgiou, and T. Poggio, “Example-Based Object Detection in Images by 
Components,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 
4, pp. 349–361, 2001 
[18] S. Munder and D.M. Gavrila, “An Experimental Study on Pedestrian Classification,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 11, pp. 1863–1868, 
2006. 
[19] C.J. Pai, H.R. Tyan, Y.M. Liang, H.Y.M. Liao, and S.W. Chen, “Pedestrian Detection and 
Tracking at Crossroads,” Pattern Recognition, vol. 37, no. 5, pp. 1025–1034, 2004. 
[20] C. Papageorgiou and T. Poggio, “A Trainable System for Object Detection,” International 
Journal of Computer Vision, vol. 38, no. 1, pp. 15–33, 2000. 
[21] T.V. Phama and A.W.M. Smeuldersb, “Quadratic Boosting,” Pattern Recognition, vol. 41, 
no. 1, pp. 331–341, 2008. 
[22] R. Ronfard, C. Schmid, and B. Triggs, “Learning to Parse Pictures of People,” European 
Conference on Computer Vision, pp. 700–714, 2002. 
[23] P. Sabzmeydani and G. Mori, “Detecting Pedestrians by Learning Shapelet Features,” IEEE 
Conference on Computer Vision and Pattern Recognition, pp. 1-8, 2007. 
[24] H. Schneiderman and T. Kanade, “Object Detection Using the Statistics of Parts,” 
International Journal of Computer Vision, vol. 56, no. 3, pp. 151–177, 2004. 
[25] E. Seemann, M. Fritz, and B. Schiele, “Towards Robust Pedestrian Detection in Crowded 
Image Sequences,” IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-8, 
2007 
[26] E. Seemann, B. Leibe, and B. Schiele, “Multi-Aspect Detection of Articulated Objects,” 
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1582–1588, 2006. 
 28 
[39] Q. Zhu, S. Avidan, M.C. Yeh, and K.T. Cheng, “Fast Human Detection Using a Cascade of 
Histograms of Oriented Gradients,” IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1491–1498, 2006. 
[40] http://cbcl.mit.edu/software-datasets/PedestrianData.html 
 30 
Human Action Recognition Based on  
Graph-embedded Spatio-temporal Subspace 
 
Abstract 
Human action recognition is an important issue in the pattern recognition field, with 
applications ranging from remote surveillance to the indexing of commercial video content. 
However, human actions are characterized by non-linear dynamics and are therefore not easily 
learned and recognized. Accordingly, this study proposes a silhouette-based human action 
recognition system in which a three-step procedure is used to construct an efficient discriminant 
spatio-temporal subspace for k-NN classification purposes. In the first step, an Adaptive Locality 
Preserving Projection (ALPP) method is proposed to obtain a low-dimensional spatial subspace 
in which the linearity in the local data structure is preserved. To resolve the problem of overlaps 
in the spatial subspace resulting from the ambiguity of the human body shape among different 
action classes, temporal data are extracted using a Non-base Central-Difference Action Vector 
(NCDAV) method. Finally, the Large Margin Nearest Neighbor (LMNN) metric learning 
method is applied to construct an efficient spatio-temporal subspace for classification purposes. 
The experimental results show that the proposed system accurately recognizes a variety of 
human actions in real time and outperforms most existing methods. In addition, a robustness test 
with noisy data indicates that our system is remarkably robust toward noise in the input images. 
 
Keywords: Human action recognition, Adaptive locality preserving projection, Large margin 
nearest neighbor 
 32 
methods, such as Isometric Feature Mapping (Isomap) [29], Locally Linear Embedding (LLE) 
[30], or Laplacian Eigenmaps [31], provide the means to identify the intrinsic geometrical 
structure of a database and thus facilitate the analysis of human action motions in compact 
low-dimensional space. For example, Elgammal and Lee [32] utilized the LLE method to infer 
3D body poses from human silhouettes. Similarly, Wang [5] used a linear approximation to the 
LE method referred to as the Locality Preserving Projection (LPP) method [33] to establish 
low-dimensional feature representation for human silhouettes. LPP can extract the 
low-dimensional features of human silhouettes as a manifold by preserving both the intrinsic 
geometry and the local structure of the data via an adjacency undirected graph that incorporates 
the neighborhood information of the database [33]. However, LPP lacks clear rules for 
preserving linearity, and the rules for building the adjacency graph are not strict enough. As a 
result, the subspace obtained by LPP might be incompact; i.e., a data point in the subspace may 
be neighbored with other data points that are unrelated or similar to it.  
Several supervised manifold learning methods based on class label information have been 
proposed in recent years, including Marginal Fisher Analysis (MFA) [9], supervised-LPP [5], and 
Locality Sensitive Discriminant Analysis (LSDA) [34]. The class label information makes 
possible the discovery of the local spatial discriminant structure and therefore enables the 
separation of images with different action classes. However, in addition to spatial information 
(e.g., silhouette shape), temporal information, such as the dynamic variation of the silhouette 
shape over a sequence of video frames, is also helpful in accomplishing reliable human action 
recognition systems. Accordingly, various researchers have incorporated temporal information 
into the action recognition process. For example, Jia et al. [4] proposed a local spatio-temporal 
subspace learning method (LSTDE) in which temporal subspaces associated with the data points 
in consecutive frames were constructed in such a way as to maximize both the discriminant 
structure in accordance with the class labels and the principal angles among the temporal 
subspaces of the different classes. Meanwhile, Wang and Suter [5] modeled the temporal 
evolution of an action motion as a sequence of projection points with associated temporal orders 
and used a hidden Markov model (HMM) to capture the structural and dynamic nature of the 
corresponding motion. 
Consequently, the methods for human action recognition are mainly divided into two 
approaches: silhouette base and feature base. The first one utilizes human silhouettes as features 
which are commonly extracted by background subtraction [4-5], [25]. The second one adopts 
 34 
itself, which is to extract discriminant features for action recognition. According to the properties 
of the three approaches, the proposed system is able to not only recognize human actions in 
real-time, but also considerably tolerate noise condition. 
 
2. Learning Process in a Spatio-temporal Subspace 
 
Fig. 1 shows the overall framework of the proposed human action recognition system. As 
shown, the system comprises a learning process (see Fig. 1(a)) and a recognition process (see Fig. 
1(b)). Assume that there are M training sequences ],...,,[ 21 MXXXX   and that each sequence 
comprises in  frames. The learning process commences by extracting the human silhouettes 
from the training sequence frames using a background subtraction method. To reduce intra-class 
variations in the subject size, the silhouettes are centralized and normalized to a consistent size of 
hw  pixels. Therefore, each silhouette ix  can be represented by a D-dimensional vector 
( hwD  ), and the training set has the form 
ND
N RxxxX
 ],...,,[ 21 , where  
M
i i
nN
1
. 
 
 
 36 
 
Having constructed the training set, a three-step procedure is applied to analyze the spatial 
and temporal information of the silhouettes in the training sequences and to construct a 
spatio-temporal subspace for classification purposes. Section 2.1 describes the ALPP algorithm, 
which is used to reduce the dimensionality of the original spatial subspace while preserving the 
linearity in the local structure information. Section 2.2 describes the use of the NCDAV method 
for extracting temporal information from the training sequences and constructing the 
corresponding temporal vector. Finally, Section 2.3 describes the LMNN method, which is used 
to construct a discriminant spatio-temporal subspace for classification purposes. 
 
2.1. Adaptive Locality Preserving Projection (ALPP) for Dimensionality Reduction  
Human action sequences, as represented by a contiguous series of human silhouettes, can be 
viewed as a set of data points on nonlinear manifolds in high-dimensional space. To reduce the 
computational cost of the learning process, it is desirable to eliminate the redundant information 
within the original sequences in order to obtain a low-dimensional spatial subspace. However, in 
constructing this subspace, the local spatial structure and relations among the data points must be 
preserved. That is, the data points (images) that are close (similar) in the original 
high-dimensional space would be also close in the low-dimensional subspace. In the present 
study, this computation is achieved by using a new Adaptive Locality Preserving Projection 
(ALPP) algorithm. Importantly, ALPP retains the well-known advantages of the Locality 
Preserving Projection (LPP) method [33]. In other words, less computational complexity is 
needed than in methods such as the LE or LLE methods [30-31], which utilize a nonlinear 
spectral embedding technique. In addition, a linear transformation enables LPP to provide a low 
embedding for new data points without computing the entire matrix from scratch. Moreover, 
ALPP solves the problem of LPP, which will be discussed in the following paragraph because of 
its use of a modified graph construction process and linearity measurement. 
 38 
 
 
 
In the process of graph construction, the discriminatory power of the graph is improved by 
imposing new bi-relation connection rules between each pair of nodes, as shown in Fig. 2(b). As 
in the LPP method, ALPP also recognizes four possible relationships between each pair of nodes 
when applying the k-NN clustering method. However, in contrast to LPP, ALPP constructs an 
edge in the adjacency undirected graph only when both nodes in the pair are neighbors of one 
another. As a result, ALPP avoids the problem inherent in LPP of retaining redundant data points 
simply to satisfy the requirement for a given number of neighbors and to make the distribution of 
the same group more discriminant after dimensionality reduction (see the lower graph in Fig. 
3(b)). 
(a) 
Fig. 4. (a) A example graph constructed by ALPP. (b) The “Geodesic distance” and 
“Euclidean distance” of point A and point B. 
(b) 
A 
B 
A 
B 
Euclidean distance 
 
Geodesic distance 
 
(5-NN) (5-NN) 
Dimensionality 
Reduction 
(b) (a) 
Fig. 3. (a) The problem of LPP where an improper edge to connect the unrelated points may 
cause the congregation of two different groups and the classification error after the 
dimensionality reduction. (b) The example of ALPP which use more strict 
connection rule can avoid the mistake by keep unimportant data nearby and make 
the distribution of the same group more discriminant after the dimensionality 
reduction. 
Dimensionality 
Reduction 
The number of connected paths = 1392 
 40 
 
 
Fig. 5. An example of linearity measurement indicates that the data point C which is not the 
neighbor of A but its linearity is strong enough will add an edge between them. 
However, the data point D which is not the neighbor of A and the linearity is not 
strong enough will not add an edge between them. 
D 
lAC = 1.05 
A B 
C 
lAD = 1.3 
A B 
C 
D 
Fig. 6. The adjacency undirected graphs of the two action sequences of bend (orange edge) 
and walk (green edge): (a) Using the traditional rules. (b) Using only new bi-relation 
rules. (c) Using both new bi-relation rules and linear measurement. The red arrows 
indicate the difference between adjacency undirected graphs using traditional and 
proposed method.  
(a) 
(c) 
The number of connected paths = 848 
The number of connected paths = 716 
The number of connected paths = 1227 
(k = 5, lij = 1.3) 
(k = 5) 
(k = 5) 
(k = 5) 
(k = 5) 
(k = 5, lij = 1.3) 
The number of connected paths = 958 
The number of connected paths = 714 
(b) 
 42 
points. Combining Eq. (5) and Eq. (6), the optimization problem becomes  
1 s.t.
            minarg
aXDXa
aXLXa
TT
TT
a
.             (7) 
Then, Eq. (7) can be solved via the Lagrangian formulation as follows: 
   
0) (
           
aXDXaXLX
aXDXaaXLXa
a
aXDXaaXLXaLagrangian
TT
TTTT
TTTT









,          
(8) 
where the two matrices 
TXLX  and 
TXDX  are both symmetric and positive semi-definite. Eq. (8) 
is a generalized eigen-decomposition problem [4-5], [30-33], [35], and the transformation matrix 
dD
d RaaaA
 ],...,,[ 21  is given by the eigenvectors corresponding to the d smallest eigenvalues. Thus, 
the data in the low-dimensional subspace can be obtained as XAY
T , where 
Nd
N RyyyY
 ],...,,[ 21 . Figs. 7(a) and 7(b) show the distribution of Y in the LPP and ALPP subspace. 
As the diagrams indicate, the distribution of Y in the ALPP subspace is more compact than its distribution 
in the LPP subspace. In addition, as shown in Fig. 7, the continuity of action in the ALPP subspace is 
smoother than that in the LPP subspace (i.e., the images that are close (similar) in the original 
high-dimensional space are also close in the low-dimensional subspace). 
 44 
 
 
 
2.2. Temporal Vector Creation 
After obtaining the spatial subspace by ALPP, all of the training silhouettes are projected in 
this subspace, and thus, the computational complexity of the learning process is reduced. 
However, due to the ambiguity of the human body shape in some specific images among 
different action types (see the red round rectangles as shown in Fig. 8), an overlap occurs in the 
spatial subspace. In other words, according to the graph construction rules of ALPP, those similar 
images will be located together (see the red round rectangle as shown in Fig. 7(b)) in the ALPP 
subspace even if they belong to different action types. Accordingly, inspired from the extraction 
of feature points variation by optical flow for the facial expression recognition, a Non-base 
Central-Difference Action Vector (NCDAV) method is proposed which only considers the 
variation between consecutive data in the spatial subspace to reduce the ambiguous or corrupted 
effect, as follows:  
Fig. 9. Conceptual graph of NCDAV in the ALPP subspace. NCDAV is formed by the 
central difference which is concatenated between temporally closet data points 
and base data. 
Non-base Central-Difference Action Vector (NCDAV) 
 
(t = 2) 
 
2iy
 
1iy
 1i
y  
2iy
 
iy
 
},,,{' 2112   iiiiiiiii yyyyyyyyy  
Base data
Fig. 8. (a) Example silhouette sequences of jump and skip. (b) Example silhouette 
sequences of jack and wave2. The red round rectangles indicate the similar and 
ambiguous parts of two different actions. 
(a) (b) 
Jack 
… … 
Wave2 
… … 
… 
Jump 
… 
Skip 
… … 
… 
 46 
 
 
2.3. Spatio-temporal Subspace Creation using Large Margin Nearest Neighbor (LMNN) 
In this study, a simple and non-linear approach, namely k-nearest neighbor (k-NN) classifier, 
is applied to decide the class label of the test data in the test process. The mechanism of k-NN 
classifier is to classify the test data according to the labels of nearby neighbors. However, there 
are two issues which should be taken into consideration: First, if the extracted features for the 
training data are not discriminant enough to maximize the separability between different action 
classes, the classification results of k-NN will not be promising. Second, the distance 
measurement has effects on the determination of the nearby neighbors. As known, there are 
several distance measurements (e.g. Euclidean distance and Mahalanobis distance), which 
approach is suitable for k-NN classifier? For the past few years, many researches devoted 
themselves on this issue. Among the studies about the derivation of good distance metric, the 
Large Margin Nearest Neighbor (LMNN) method is the state-of-the-art. Also it is the first 
approach to design the distance metric based on the mechanism of k-NN classifier [22]. 
Therefore, the LMNN method, which ensures the compact grouping of data points with the same 
class while simultaneously maximizing the separation distance between data points with different 
classes is used to construct the spatio-temporal subspace required for k-NN classifier. 
The LMNN method commences by assigning to the temporal vectors ]',...,','[' 21 NyyyY   a 
corresponding class label },...,2,1{ cli  , where c is the total number of class label types. The 
distance between vectors 'iy  and 'jy  is then computed using the Mahalanobis distance metric. 
Let the transformation matrix L  with dimensions dtdt 22   be defined as follows: 
Fig. 11. Complete schematic illustration of LMNN (a) The original data distribution which 
dimension is d (b) During the LMNN process, the neighbor data with the same label 
will be pulled in, and the neighbor data with different label will be pushed out. (c) 
The result after the LMNN which dimension is d.  
(a) (b) (c) 
(c) 
 48 
 
 
3. Recognition Process Using K-NN Classification in a Spatio-temporal Subspace 
 
After the learning process, two transformation matrices are obtained that allow us to analyze 
the spatial and temporal information of new data points; the matrices are the spatial subspace 
transformation matrix A obtained from ALPP and the spatio-temporal subspace transformation 
matrix L obtained from LMNN. The recognition process commences by obtaining human 
silhouettes as input test sequences. The input sequences are transformed to the spatio-temporal 
subspace via the two transformation matrices and the NCDAV method, and they are then 
recognized using a k-NN classifier. 
As shown in Fig. 1(b), the binary silhouettes in the input test sequence are centralized and 
normalized to a consistent size of 64×48 pixels. The normalized sequence is noted 
as ],...,,[ 21
t
n
tttest xxxX  , where n is the total number of frames in the sequence. Each normalized 
frame )~1( nixti   is projected to the spatial subspace )(
testTtest XAY   by transformation 
matrix A. The spatial data are then extended to the temporal vector by the NCDAV method, as 
: bend 
: jack 
: jump 
: pjump 
: run 
: side 
: skip 
: walk 
: wave1 
: wave2 
(k = 6) 
(a) 
(b) 
Fig. 12. 3D visualization of the distribution in spatio-temporal subspace after LMNN 
method. (a) The distribution of total 10 action sequences. (b) The distribution of 
different ambiguous action sets by different angle view, such as jack and wave2, 
jump and skip. 
 50 
 
 
4.1. Database Collection 
The Weizmann database consists of 10 different actions performed by 9 different 
individuals. The 10 actions are displayed in Fig. 13(a): bending (bend), jumping jack (jack), 
jumping forward on two legs (jump), jumping in place on two legs (pjump), running (run), 
galloping sideways (side), skipping (skip), walking (walk), waving one hand (wave1), and 
waving two hands (wave2). The database contains a total of 93 sequences because some of the 
actions are performed more than once by the same individual. However, in the present 
experiments, 90 sequences were used (i.e., the repeated actions were omitted). As in the learning 
process, the silhouettes were centralized, cropped and resized to 64×48 pixels. To obtain 
unbiased estimation results, the recognition tests were performed using a nine-fold 
cross-validation technique. More specifically, in each run of a test, all of the sequences 
corresponding to a specific individual (i.e., 10 sequences with different actions) were used as test 
sequences in the recognition process, while the remaining 80 sequences in the database were 
used in the learning process. The recognition results were then averaged over nine runs, where 
each run corresponded to a different individual. 
The ORL database contains 400 images of 40 individuals. The images are gray-level 
bitmaps captured at different times and have different variations including expressions (i.e., open 
or closed eyes, smiling or non-smiling) and facial details (i.e., glasses or no glasses). Also, the 
images were taken with a tolerance for some tilting and rotation of the face up to 20 degrees, and 
resized to 64×64 pixels. The sequence of experiment is designed as same as previous studies [9], 
[34]; a random subset with p (= 4, 5, 6) images per individual was taken for training and the rest 
Fig. 13. The sample images cropped from (a) Weizmann database [24], (b) ORL database 
[36] and (c) MNIST database [37], respectively. 
bend jack jump pjump run side skip walk wave1 wave2 
(a) 
(b
) 
(c) 
 52 
 
However, when the distribution of data set is complex and the training data cannot 
represent the data distribution well, such as the case of ORL database (5-Train), ALPP appears to 
be less effective than supervised methods (LDA and LSDA), though still better than the 
unsupervised methods (LPP and PCA). In addition, when the training samples are larger, such as 
the case of ORL database (6-Train), ALPP performs better than LDA. 
 
(a) (b) 
(c) 
Fig. 14. Error rate of ALPP on Weizmann database [24], ORL database [36] and MNIST 
database [37] using different parameters: (a) reduced dimensionality (d), (b) 
linearity measurement threshold (lt) and (c) k nearest neighborhood (k). 
 54 
 
MNIST and Weizmann databases. 
However, the optimal value of k for ORL database (5-Train) is 3 and the difference of the 
error rate by using k = 3 and k = 5 is 2.5%. It’s because the training number of each class in ORL 
database is smaller than others. Hence, smaller k can help ALPP preserve local structure of the 
data distribution when the training number of each class is small. 
 
4.3. Performance Comparison of Different Temporal Vector Types 
To investigate the contribution of the spatio-temporal information toward the performance 
of the proposed human action recognition system, a series of experiments was performed in 
which the recognition accuracy of the proposed ALPP+NCDAV+LMNN scheme was compared 
with that of ALPP+LMNN schemes in which NCDAV was replaced by four forms of temporal 
vectors: Action Trajectory Vector (ATV), Adjacent-Difference Action Vector (ADAV), 
Central-Difference Action Vector (CDAV), and Non-base Adjacent-Difference Action Vector 
(NADAV). ATV is formed by incorporating the feature vectors in the spatial subspace of those 
data that are temporally close to the base data as additional information. ADAV calculated the 
difference between two adjacent data; this type of difference is called an adjacent difference. 
Then, ADAV is formed by incorporating the adjacent difference as additional information. 
CDAV is formed by adding the difference between the base data and those data that are close to 
it as additional information. Finally, NADAV also uses the adjacent difference of the data that are 
close to the base data as additional information, but it eliminates the base data. The various 
spatio-temporal vectors are illustrated schematically in Fig. 15. 
Fig. 16. Comparison bar chart based on various combinations of methods. 
 56 
 
 
4.4 Performance Evaluation under Noise-corrupted Silhouettes 
To test the robustness of the proposed ALPP+NCDAV+LMNN framework, an additional 
series of experiments was performed in which salt and pepper noise with a density of between 
0.1 ~ 0.35 was added to the input silhouette images. Note that the noise density V indicates the 
number of corrupted pixels as a fraction of the total number of pixels in the image. Fig. 19 
illustrates the effects of the various density values V on one frame within the run silhouette 
sequence in the Weizmann database. For each action in the Weizmann database, the uncorrupted 
silhouettes were used as training data. Then, the noise-corrupted silhouettes were classified 
individually using the ALPP+NCDAV+LMNN and ALPP+CDAV+LMNN methods, respectively. 
The classification results obtained using the two frameworks are summarized in Table 2. As 
shown, the ALPP+CDAV+LMNN method is more sensitive to noise than the 
ALPP+NCDAV+LMNN method. This result is to be expected because the corrupted base data is 
retained within the temporal vector constructed using CDAV but excluded by NCDAV. For both 
frameworks, the recognition performance deteriorates as the noise density increases. However, 
the proposed ALPP+NCDAV+LMNN framework achieves a consistently high recognition 
performance (i.e., > 92.2%) provided that the noise density does not exceed 0.2. 
 
Fig. 18. Recognition performance of our approach measured using confusion matrices: (a) 
Using ALPP+CDAV+LMNN. (b) Using ALPP+NCDAV+LMNN. Vertical rows 
show ground truth, and horizontal columns indicate recognition results. 
(a) (b) 
 58 
 
Table 4 
Recognition accuracy of some other feature-based approaches on Weizmann database. 
Noted, the extracted features and the recognition strategy are organized for each approach, 
and all of these approaches use evaluation method of leaving one out cross validation. 
Methods Features Recognition approach 
Recognition 
accuracy (%) 
Our approach  Spatial-temporal 
vectors  
LMNN+KNN 98.9% 
Bregonzio et al. 
[16] 
Clouds of interest 
points 
Nearest neighbor classifier 
and support vector 
machine 
96.6%  
Chaudhry et al. [6] Histogram of oriented 
optical flow features 
Non-linear dynamical 
systems 
94.4% 
Lee et al. [21] Histogram-based interest 
points 
Bhattacharyya coefficient 
measurement 
84.4%  
 
Filipovych et al. [42] 2D and 3D Interest 
sub-regions 
Pose models and motion 
dynamics model 
88.9% 
Ali et al. [43] Trajectories of body joints Phase space embedding 92.6% 
Neibles et al. [12] Spatial-temporal interest 
points 
Hierarchical model 72.8% 
 
approach. Specifically, LSTDE uses LSDA to generate a spatial subspace and then applies a canonical 
correlation technique to mesh the temporal information with the spatial information. However, the 
LSTDE method achieves lower recognition accuracy (90.9%) than the ALPP+NCDAV+LMNN method 
proposed in this study. Furthermore, the ALPP+NCDAV+LMNN method allows the human action 
recognition process to be accomplished in real time. For example, given a value of t = 2 in the NCDAV 
algorithm, each frame in the input sequence can be classified within 33 ms. The recognition accuracy 
achieved using the method proposed by Wu et al. [40] is identical to that achieved using the proposed 
method. However, the computational cost of the method in [40] is significantly higher than that of the 
ALPP+NCDAV+LMNN method. 
Table 4 compares the recognition accuracy of the proposed method with that of several existing 
methods in which certain local features of the Weizmann database (e.g., optical flow features [6] or sparse 
spatio-temporal interest points [16]) are taken as the input to the recognition process rather than the entire 
silhouette image. The results show that the ALPP+NCDAV+LMNN method can provide the promising 
results.  
 
5. Conclusions 
This paper has proposed a silhouette-based human action recognition system in which a discriminant 
spatio-temporal subspace is constructed in a learning process, and unknown human actions are then 
 60 
References 
[1] R. Xiao, W. Li, Y. Tian, and X. Tang, Joint Boosting Feature Selection for Robust Face 
Recognition, IEEE Conference on Computer Vision and Pattern Recognition, pp. 1415-1422, 
2006. 
[2] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry and Yi Ma, Robust Face Recognition via 
Sparse Representation, IEEE Transactions on Pattern Analysis and Machine Intelligence 31 
(2) (2009) 210-227. 
[3] H. Zhou, P. Miller and J. Zhang, Age Classification using Radon Transform and Entropy 
Based Scaling SVM, Proceeding of British Machine Vision Conference, pp. 28.1-28.12, 
2011. 
[4] L.K. Jia and D.Y. Yeung, Human Action Recognition Using Local Spatio-Temporal 
Discriminant Embedding, IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 1-8, 2008. 
[5] L. Wang and D. Suter, Visual Learning and Recognition of Sequential Data Manifolds with 
Applications to Human Movement Analysis, Computer Vision and Image Understanding 
110 (2) (2008) 152-172. 
[6] R. Chaudhry, Avinash Ravichandran, G. Hager, and R. Vidal, Histograms of Oriented 
Optical Flow and Binet-Cauchy Kernels on Nonlinear Dynamical Systems for the 
Recognition of Human Actions, IEEE Conference on Computer Vision and Pattern 
Recognition, pp. 1932-1939, 2009. 
[7] A. Efros, A. Berg, G. Mori, and J. Malik, Recognizing Action at A Distance, IEEE 
International Conference on Computer Vision, vol. 2, pp. 726-733, 2003. 
[8] C. Schuldt, I. Laptev, and B. Caputo, Recognizing Human Actions: A Local SVM Approach, 
IEEE International Conference on Pattern Recognition, vol. 3, pp. 32-36, 2004. 
[9] S. Yan, D. Xu, B. Zhang, and H.J. Zhang, Graph Embedding and Extensions: A General 
Framework for Dimensionality Reduction, IEEE Transactions on Pattern Analysis and 
Machine Intelligence 29 (1) (2007) 40-51. 
[10] A. Bissacco, A. Chiuso, Y. Ma, and S. Soatto, Recognition of Human Gaits, IEEE 
Conference on Computer Vision and Pattern Recognition, vol. 2, pp. 52-57, 2001. 
[11] C. Bregler, Learning and Recognizing Human Dynamics in Video Sequences, IEEE 
Conference on Computer Vision and Pattern Recognition, pp. 568-574, 1997. 
 62 
[25] R. Poppe and M. Poel, Discriminative Human Action Recognition using Pairwise CSP 
Classifiers, IEEE International Conference on Automatic Face and Gesture Recognition, pp. 
1-6, 2008. 
[26] L. Wang and D. Suter, Recognizing Human Activities from Silhouettes: Motion Subspace 
and Factorial Discriminative Graphical Model, IEEE International Conference on Pattern 
Recognition, pp. 1-8, 2007. 
[27] L. Wang and D. Suter, Learning and Matching of Dynamic Shape Manifolds for Human 
Action Recognition, IEEE Transactions on Image Processing 16 (6) (2007) 1646-1661. 
[28] D. Weinland and Edmond Boyer, Action Recognition using Exemplar-based Embedding, 
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-7, 2008. 
[29] J.B. Tenenbaum, V.D. Silva, and J.C. Langford, A Global Geometric Framework for 
Nonlinear Dimensionality Reduction, Science 290 (5500) (2000) 2319-2323. 
[30] S. Roweis and L. Saul, Nonlinear Dimensionality Reduction by Locally Linear Embedding, 
Science 290 (5500) (2000) 2322-2326. 
[31] M. Belkin and P. Niyogi, Laplacian Eigenmaps and Spectral Techniques for Embedding and 
Clustering, Advances in Neural Information Processing Systems 14, pp.585-591, 2002. 
[32] A. Elgammal and C.S. Lee, Inferring 3D Body Pose from Silhouettes using Activity 
Manifold Learning, IEEE Conference on Computer Vision and Pattern Recognition vol. 2, 
pp. 681-688, 2004. 
[33] X. He and P. Niyogi, Locality Preserving Projections, Advances in Neural Information 
Processing Systems 16, pp. 152-160, 2003. 
[34] D. Cai, X. He, K. Zhou, J. Han, and H. Bao, Locality Sensitive Discriminant Analysis, 
International Joint Conferences on Artificail Intelligence, pp. 708-713, 2007. 
[35] R. Wang and X. Chen, Manifold Discriminant Analysis, IEEE Conference on Computer 
Vision and Pattern Recognition, pp. 429-436, 2009. 
[36] http://www.cam-orl.co.uk/facedatabase.html. 
[37] http://yann.lecun.com/exdb/mnist/index.html. 
[38] M. Turk and A. Pentland, Face Recognition using Eigenfaces, IEEE International 
Conference on Pattern Recognition, pp. 586-591, 1991. 
[39] P. N. Belhumeur, J. P. Hepanha, and D. J. Kriegman, Eigenfaces vs. Fisherfaces: 
Recognition using Class Specific Linear Projection. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 19(7) (1997) 711–720. 
 64 
Frontal Facial Expression Synthesis 
 
摘要 
 當輸入一段從正面沒表情到側面有表情的影像時，本篇研究透過將影像分為三區塊並
個別使用漸進式投影模型，來分離固定式頭部轉動與非固定式臉部表情且移除掉固定式頭
部轉動。在分離固定式頭部轉動與非固定式臉部表情之後，本篇研究必須將三區塊的結果
影像合併。本篇研究考慮三區塊中間有兩個重疊區塊，對於每個重疊區塊而言，本篇研究
一開始透過內插法給每個畫素初始值並藉由在個別子區塊中找到較佳的畫素值來更新每個
畫素，如此本篇研究就能克服將三區塊合併時的邊界問題。本篇研究發現會有未知區塊問
題出現在分離固定式頭部轉動與非固定式臉部表情的結果影像，因為原本側面影像就不包
含該人臉區塊資訊，所以本篇研究需藉由局部線性回歸方法合成出虛擬的表情影像。最後，
本篇研究透過漸進式投影模型將局部線性回歸結果轉置到固定式頭部轉動與非固定式臉部
表情結果，並藉由此轉置結果，本篇研究可以將分離固定式頭部轉動與非固定式臉部表情
結果的未知區塊取代掉並將取代的區塊與分離的結果區塊合併，重建出最後沒有未知區塊
問題的影像結果。 
 66 
1. Introduction 
 
Computer vision researchers have developed various techniques on automatic facial 
expression recognition. Some exiting systems [7], [12], [13], [15], [19] apply the facial images 
without taking rigid and non-rigid motion separation process, and the tolerance of rotation angles 
are not mentioned. Work in [2] uses Adaboost for feature selection, and then classifying the 
selected outputs by a support vector machine (SVM). The study in [20] integrated the dynamic 
Bayesian networks (DBNs) and the facial action units for modeling the dynamic and stochastic 
behaviors of expressions ±30
o
 out-of-plane rotations. In their tracking technique, active IR 
illumination is used to provide reliable visual information under variable lightings and head 
motions. The work in [1] used SVM to classify the facial motions for recognizing six basic 
emotions associated with unique expressions, and the limitations of head rotation angles are ±30
o
 
in pan rotation, ±20
o
 in tilt rotation and ±10
o
 in roll rotation. The work in [9] also used SVM to 
classify five facial motions with one deformable model and stereo system. A limitation of above 
studies is unable to separate rigid and non-rigid motions that makes the expression recognition 
fail to achieve accurate result in real-world image sequences. 
The affine model is used in [11] to solve the separation problem, but it can’t work for large 
out-of-plane rotation due to the perspective effects caused by depth variations. The facial 
expression scheme in [4] applied the 8-parameters perspective projection model and affine model 
with a curvature function to parameterize large head rotations and non-rigid facial motions, 
respectively. Facial expression recognition was achieved by assigning different threshold values 
to different motion parameters of the models. However, this approach will reduce the recognition 
sensitivity and accuracy while there is slightly different between facial expressions. The work in 
[5] estimates the camera parameters and reconstructs the corresponding 3D facial geometry of 
the subject, and the 3D pose is recovered by the Markov Chain Monte Carlo method and each 
image is warped to frontal-view canonical face image. 
Some of the studies above [1], [7], [9], [13], [19] recognized the limited six basic 
expressions, like mouth opening and closing which occurred relatively infrequently in daily life. 
However, the facial expressions often occurred with changes in many features. Consequently, 
other studies [2], [5], [12], [15], [20] used the Facial Action Coding System (FACS) [8] in which 
the defined action units (AUs) represent the smallest visibly distrainable muscle action. 
 
 
 
 
 
 68 
virtual frontal-view facial expression in Section 3. (3) Our system (Fig. 1.c) recovers the missing 
region by reconstructing expression image in frontal view in Section 4. The missing regions in 
the rigid and non-rigid motion results could be replaced by the virtual frontal facial expression. 
On the basis of this method, we could generate the frontal facial expression result without the 
missing region, even the head rotation angle is more than ±30
o
 in pan rotation. 
 
2. Incremental Perspective Motion Model for Rigid and Non-Rigid Motion Separation 
 
We developed a system to separate non-rigid facial expression from large rigid head motion 
over an image sequence based on the incremental perspective motion model. Since the 
parameters of this motion model are not only able to represent the global rigid head motion but 
also localize the non-rigid facial expression motion, thus this motion model overcomes the 
limitations of existing methods, the affine model and the 8-parameter perspective projection 
model, in large head rotation angles. 
 
2.1. Incremental Perspective Motion Model 
Since during the facial expression, large percentage of facial area experience global motion 
while the local facial features, i.e., eyebrows, eyes, nose and mouth, are experiencing non-rigid 
motion obviously. Therefore, the incremental perspective model presented in [10], [14] is used to 
estimate one global perspective transformation for registering two images by assuming that the 
interesting region is planar. 
To register two corresponding facial images, I0(x) and I1(x
’
), with different views, the 
warping image is computed as x
’~= M．x, where M is parametric motion model. The 
transformation matrix M is iteratively updated by using M←(I+d)M, where d represents the 
deformation (i.e. incremental motion). The minimizing squared error metric is formulated as: 
 
 2
2
011
2
01
)()(
~
)(
~
)()(
~
)(













i
i
T
i
T
i
i
i
i
ii
i
ii
edJgxId
d
x
xIxI
xIxIdE
 
(1
) 
where ei= I1 
~(xi)-I0(xi) is the intensity (grayvalue) error, gTi =▽ I1 
~(xi) is the image gradient of 
I1 
~ at xi, and Ji=Jd(xi) is the Jacobian of the resampled point coordinate x"i with respect to 
d=-(A
T
A)
-1
A
T
b. The equation is rewritten as:  






j
j
Pi
iij
j
jj
Pi
T
iij
j
T
ijj
gebwithbJb
ggAwithJAJA
      
         
 
(
2) 
The computation of this patch-based algorithm only needs to evaluate Jj and accumulate A  
 70 
overlap region to modify this algorithm in [17], [18], every time, after dealing with this mask 
window, shifting the width of mask until crossing the width of overlap region. By this method, 
our system calculates the adapting region size hj in two constructed overlap regions of I 
~e
f  
through the sum of the gray value in mask window of each constructed overlaps, the equation is 
presented as:  



jk
k
gray
jk
k
grayj I-I
H
 hze region siadapting 21
2
 :  
 
(3) 
where I k1gray and I k2gray are constructed overlap of two sub-regions in gray value and H is the 
height of overlap region. After deciding the adapting region size in the mask window, our system 
synthesizes the initially overlap texture by interpolation method, and updating the texture pixels 
by error equation that is presented as:  
 
i
iiiii ))(pN - (p)N  )pG(p) - G(( }) E(p,{p
22
 
(4) 
where the index i means the all pixels of input overlap regions, p presents the output pixel, pi 
presents the corresponding input pixels, G(p), G(pi) are the gray value of p and pi, and Ni(p), 
Ni(pi) are the neighborhoods of p and pi in the same source region. The error function is 
calculated as a weighted sum of the absolute value with {G(p), Ni(p)} and {G(pi), Ni(pi)}, and 
weight values would be decide by the distance between the point p in adapting region to the 
border of adapting region, in other words, the weight values is the ramp weighting. To minimize 
the error function, our system fixes the p and selects the best {pi} that each individual error term 
||G(p) –G(pi)||
2
 + ||Ni(p) - Ni(pi)||
2
 would be minimized, then our system updates the p pixel as the 
weighted sum of {pi}, finally, the edge problem of combining sub-regions could improve 
obviously. 
 72 
3.1. Local Linear Regression Model 
Our research modifies the local linear regression (LLR) [6] method to the relation between 
the neutral face image in frontal view and the expression face image in frontal view. Our system 
tries to synthesize the virtual expression image of one given neutral face image based on training 
data by a regression method, it is defined as: Let {X
n
, X
e
} be the training set, which is the 
corresponding images with the neutral face, n, and some relatively expression face, e, where X
n
 
is composed of (x n1 , x
 n
2 ,…, x
 n
N ) defines that neutral training data composed N images, and 
X
e
=(x e1 , x
 e
2 ,…, x
 e
N ) is the corresponding neutral face image in X
n
, the x ni  and x
 e
i  would 
be the same person with neutral face and expression face. Our system considers the linear 
assumption; the neutral faces warping to expression faces in training data could be assumption 
as: 
ne X A X   (5) 
where A is the uniform affine function. By pseudo inverse, the uniform affine function could be 
computed as: 
        'n-n'nnne XXX  X, where X XA 1   (6) 
the (X
n
)
+
 is pseudo inverse term of X
n
. 
Because the uniform affine A is computed through the neutral facial images and the expression 
facial images in the training set, when input one neutral facial image in frontal view I nf , the 
corresponding virtual expression image I ~ef ' in frontal-view could be computed by the same 
uniform affine transformation A:  
  nfnenfef IX X I A I
'

~
 (7) 
because the uniform affine A could not present the variant of local facial motion, our research 
rewrite the above equation as:  
  nfneef IX α α  X I
'


  where,
~
 (8) 
I ~ef' could be synthesized by product of the expression facial images in training data and the α 
as the weighting value. Based on this algorithm, the virtual expression image could be 
synthesized by two steps: the first step is computing the synthesis weighting value between the 
input neutral image and neutral face images in training data; the second step is the virtual image 
could be synthesized through the expression face images in training data. 
Under the linear assumption, the structure of the face is non-planar; it could reduce the virtual 
result based on the global linear regression, so we consider the face image is composed of many 
local planar patches. Based on this ideal, the study in [6] proposes the local linear regression 
(LLR) method, which linear regression is based on the regular patches, and each patch could be 
generated by linear regression method. Under this concept, our system synthesizes the virtual 
expression image in frontal view based on the local linear regression. 
 
 74 
weighting values αi for i-th patch in In by:  
  (i,n)(i,n)i IX  α    (9) 
where X
(i,n)
= X1
(i,n)…XN
(i,n)
 is the i-th patches in neutral facial images from training data. The 
second step, the virtual expression facial patch could be computed by:  
i(i,e)(i,e)
f α X I 
~
 (10) 
where X
(i,e)
= X1
(i,e)…XN
(i,e) 
is the i-th patch form the expression facial images in the training data. 
Our purpose is to synthesize the virtual frontal-view expression image in patch based, we modify 
the study in [6], and the weighting values αi of the corresponding images in frontal view are the 
expression facial images and the neutral facial images. But neither of the expression structures 
are the same, if our system computes the weighting values α1 directly; the virtual expression 
result should be terrible, because the expression structures are all different, under this situation, 
the corresponding expression patch shouldn’t be correct. As shown in Fig. 4, to overcome this 
problem, our system brings up an ideal that modifying each expression structure to make them 
no difference. Under this ideal, our training data would have N neutral images, one target neutral 
image and target expression image with the same person before training process. In training 
process as the Fig. 4, our system has two steps, the first step is calculating the every weighting 
value between the target neutral image and each neutral image in training data in patch based, the 
second step is basing on patch method to synthesize the corresponding expression images of each 
neutral image excluding the target neutral image in training data. 
 
4. Missing Region Recovery for Frontal Expression Synthesis 
 
Before solving the missing problem of the rigid head motion and non-rigid facial 
expression separation result, our system has already detected the missing region during the 
side-view expression image I eN  to reconstruct frontal-view expression image I 
~e
f  in the 
sequence warping. When the image I eN  is warping to frontal view image I
 n
1  by sequential 
incremental transformations M, there would be some missing region appearing at the border of 
I ~ef, the missing region is due to the  I eN  , which didn’t contain some information, so when 
warping I ~ef  to I n1  will also miss these information, when missing region appearing, our 
system could detect the missing region, and save the missing region size at each warping time. 
After confirming the missing region through the side-view image I eN  to reconstructing 
frontal-view image I ~ef , our system tries to solve the missing problem of reconstructing 
frontal-view image I ~ef. As shown in Fig. 1.c, I 
~e
f" is warping result that our system warps I 
~e
f ' 
to I ~ef based on the perspective motion model, our system  
 76 
5. Experimental Results 
 
Table 1:  
Testing database. The image size and the average facial size are 640×320-pixel resolution and 
220×250 pixels, respectively. 
Subjects 
Upper/ Lower 
facial expressions 
Pan 
50 
AU4, AU1+2 /  
AU12, AU15+17 
0
o
~+35
o
 and  
0
o
~-35
o
 
Subjects 
Upper/ Lower 
facial expressions 
Pan 
 
As shown in Table 1, the testing database contains 50 Asian subjects (35 males and 15 
females) that all the subjects did not wear eyeglasses. Each subject performs two upper facial 
expressions (i.e. AU4 and AU1+2) and two lower facial expressions (i.e. AU12 and AU15+17) 
with pan rotation. 
Fig. 5 compares the separation (warping) results of the affine model, 8-parameter model, 
global incremental perspective motion model and our local incremental perspective motion 
model in the same criterion. The subtraction images with frame 1 represent the differences 
between the warping result of the current frame and the image at the frame 1. Because the affine 
model doesn’t contain the factors of the depth variations, the separation results cannot take care 
of the image sequence with out-of-plane motion, as shown in Fig. 5.b. The parameter of 
8-parameter motion model is susceptible to local minimum in estimation process, so the warping 
results are distorted, as shown in Fig. 5.c. The global incremental perspective motion model is 
estimated based on the entire facial image with large local motions, i.e. facial expressions, may 
affect the global transformation estimation, as shown in Fig. 5.d. The nose region in the global 
incremental motion model’s result is not registered as well as local incremental perspective 
motion model. Our local incremental perspective motion model results, as shown in Fig. 5.e, are 
better than the other three models since one sub-region wouldn’t affect the transformation 
estimations of the others. Fig. 6 demonstrates some testing results with four FACE AU facial 
expressions. The frontal-view image means the last image of the sequence that the subject 
demonstrated expression without head rotation. The side-view image means the last image of the 
sequence demonstrated expression with head rotation; the separation result is the separation of 
side-view image and the red mask in dotted line is the missing region. 
 78 
four final system result sets, and each set has the incremental perspective result, local linear 
regression result and the final expression result in frontal view without the missing. 
 
6. Conclusions 
 
The feature of our incremental perspective transformation estimates the motions between 
sub-regions with multi-resolution approach. The incremental motion model overcomes the 
limitations of the affine and the 8-parameter perspective projection models in large head rotation 
angles. By taking account of sub-regions, the influences of depth variation between facial 
features are reduced and furthermore the transformation estimations of each sub-region are not 
affected by other sub-regions. Then, the used multi-resolution approach prevents the parameter 
estimation get trapped into local minimum. To combine the reconstructed sub-regions, our 
system applies the adaptive region that is computed with the contrast of corresponding regions. 
After rigid head motion and non-rigid facial expression separation, the results would have 
missing problem. By modeling the relation between the neutral facial images in training data and 
the given neutral facial image with local linear regression model, our system could synthesize the 
virtual expression image from the facial images in training data. The local linear regression result 
is warped to the incremental perspective result by incremental perspective transformation. The 
corresponding region in the incremental perspective result is replaced to produce the virtual 
frontal view expression without missing. 
 80 
Facial Sketches Using Direct Combined Model,” IEEE Trans. on SMC, Part B. (submitted 
for publication in 2009) 
[17] L.Y. Wei, “Texture Synthesis by Fixed Neighborhood Searching,” PhD thesis, Stanford 
University, 2002. 
[18] L.Y. Wei, “Texture Synthesis from Multiple Sources,” ACM SIGGRAPH, pp. 1-1, 2003. 
[19]  Y. Yacoob, and L.S. Davis, “Recognizing Human Facial Expressions from Long Image 
Sequence Using Optical Flow,” IEEE Trans. on PAMI, Vol. 18, No. 6, pp. 636-642, 1996. 
[20] Y. Zhang, and Q. Ji, “Active and Dynamic Information Fusion for Facial Expression 
Understanding from Image Sequences,” IEEE Trans. on PAMI, Vol. 27, No. 5, pp. 699-714, 
2005. 
  
I2
3 1 3~1
1-NN,M  
3~1
1-NN,
 IN  
100年度專題研究計畫研究成果彙整表 
計畫主持人：連震杰 計畫編號：100-2628-E-006-013- 
計畫名稱：機器人與人之互動: 人體和人臉偵測, 行為辨識與表情合成及辨識(2/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 6 6 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 3 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
相關論文成果已有 3篇發表於 SCI&EI，其中兩篇發表於國際知名期刊 - Pattern 
Recognition。 
另外發表相關研討會論文，包含 6篇發表於國內研討會 CVGIP，1 篇發表在國際研討會 
PSIVT，2 篇發表在國際研討會 ACCV。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
一、已發表重要期刊論文與重要國際會議論文： 
1. C.C. Wang and J.J. Lien,＇Pedestrian Detection System Using Cascaded Boosting 
with Invariance of Oriented Gradients,＇International Journal of Pattern 
Recognition and Artificial Intelligence, Vol. 23, No. 4, pp. 801-823, 2009. (SCI 
& EI) Impact Factor = 0.66, Ranking = 80.8% (76/94) 
2. C.C. Wang and J.J. Lien, ＇AdaBoost Learning for Human Detection Based on 
Histograms of Oriented Gradients,＇ ACCV, Japan, pp. 885-895, November 2007. 
(Oral) 被推薦競爭最佳論文獎，推薦率約 1% (8 篇/640 篇). 
3. C.C. Tseng, J.C. Chen, C.H. Fang, and J.J. Lien,＇Human Action Recognition Based 
on Graph-embedded Spatio-temporal Subspace,＇Pattern Recognition, Vol. 45, No. 
10, pp. 3611-3624, 2012. (SCI & EI) Impact Factor = 2.682, Ranking =7.29% (18/247)
4. C.H. Fang, J.C. Chen, C.C. Tseng, and J.J. Lien, ＇Human Action Recognition 
Using Spatio-Temporal Classification,＇ ACCV, Xi An, China, pp. 98-109, September 
2009. 
5. T.H. Wang and J.J. Lien, ＇Facial Expression Recognition System Based on Rigid 
