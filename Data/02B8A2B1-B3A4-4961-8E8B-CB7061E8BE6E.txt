 2
行政院國家科學委員會專題研究計畫成果報告 
研製具有探索未知室內環境功能之影像導航自走車 
計畫編號：NSC 97-2221-E129-005-MY2 
執行期限：97 年 8 月 1 日至 99 年 7 月 31 日 
主持人：簡忠漢   聖約翰科技大學電機工程系 
計畫參與人員：楊才緯、林芷瑋、張書銘、李啟銘、洪嘉駿、林正偉 
 
 
一、中、英文摘要 
 
本計畫為研究發展輪型智慧型機器人
(或稱自走車)自主探索室內環境之適應能
力，主要著重於「基於超音波與影像感測
器融合之未知環境地圖建置」。為達到此功
能，我們自行改裝一自走車(P3-DX)，利用
其既有之控制系統、超音波感測器與慣性
導航系統，並自行架設一 CCD 攝影機與實
作各感測器之軟硬體介面。首先，在單獨
基於超音波感測部分，本計畫提出利用多
超音波感測融合與機率格點地圖技術以建
置環境地圖，以改善單顆超音波感測器因
反射所造成之誤判。其次，藉由影像感測
之輔助，本計畫提出「改良式牆壁邊線偵
測系統」，以取得影像牆壁邊緣資訊，加速
建立格狀環境地圖之時間。為驗證上述技
術之效能，我們於本校電資大樓三樓局部
環境進行自走車自主建置環境地圖之實驗，
成功地驗證前述兩類環境建置與探索技
術。 
 
關鍵詞：影像伺服、影像追蹤、標線辨識、
地圖建置 
 
Abstract 
In this project we present the 
development of an exploration system for a 
mobile robot in an unknown indoor 
environment, mainly focusing on integration 
of multiple ultrasonic sensors and an image 
sensor to autonomously build the 
environment map. For this purpose, we 
reconstruct the mobile robot, P3-DX, by 
utilizing its ultrasonic sensors and inertial 
sensors and adding an additional CCD 
camera system. First, by fusion of multiple 
ultrasonic sensors, we propose an exploration 
strategy based on the probabilistic grid map 
method to solve the sensing defects of a 
single ultrasonic sensor and improve the 
quality of map-building. Then, we develop 
another exploration strategy by additionally 
incorporating the image sensor to detect the 
wall baselines and accelerate the speed of 
map building. Finally, we apply the proposed 
indoor exploration system to locally patrol 
the third floor of EE building of our campus 
and validate its performance. 
Keywords: wall-baseline detection, sensor 
fusion, visual navigation, probabilistic grid 
map 
 
二、前言 
 
根據國際機器人協會/聯合國歐洲經濟
委員會(IFR/UNECE) 於 2008 年 10 月出版
之統計資料指出[1]：2007 年家務用機器人
(如鋤草機、吸塵器等)已達 340 萬台，休閒
娛樂用機器人 200 萬台。預估 2008 至 2011
年家務用機器人將達 460 萬台，休閒娛樂
用機器人 735 萬台，總和將超過一千二百
多萬台，未來四年間其總價值合計約有 57
億美元，如圖 1、圖 2 所示。 
而我國機器人產業目前正處於萌芽階
段如圖 3，行政院 2005 年產業科技策略會
議(SRB 會議)已將發展「智慧型機器人產
業」列為我國重要科技產業政策之ㄧ。第
一階段目標預估在2008年產值將達到新台
幣300億元，發展之產品領域以導覽服務、
休閒娛樂、家庭服務、生產製造為重點。 
智慧型機器人必須能透過各種感官來
認知環境與現狀，且做為其正確的決策與
行為之依據，進而達成感知、規劃、反應
之行為模式。本計畫之題目為「研製具有
探索未知室內環境功能之影像導航自走
 4
一些主動式感測器，尤其是雷射測距儀
(Laser range Finder)以取得環境地標特徵
(Landmark Feature)位置，再結合 Kalman 
Filter 等估測演算法做為資料連結 (Data 
Association)的依據達成定位。基於此估測
器之理論架構，Gamini 等學者[18]提出一
套解決 SLAM 問題之演算法，並證明其估
測之地圖可逐漸收斂到真確之環境地圖。
黃漢邦教授則提出 RA-SLAM 技術[19]，結
合了絕對與相對之地圖資訊，來得到更精
確的機器人定位與地圖。 
然而雷射測距儀此類感測器由於價格
昂貴，且對於單一地標而言，無法提供足
夠資訊以解決重複定位之問題。因此本計
畫將結合多種價格低廉之感測器，例如超
音波感測器、慣性導航系統、CCD/CMOS
攝影機等。其中特別著重於影像之感測，
因其可經由設計各種不同的影像處理演算
法，充分利用一個影像感測器之量測資訊，
過濾出多種有用的資訊。並可依據萃取後
之視覺資訊以規劃機器人之策略並導航其
移動。 
 
四、研究方法 
 
本計畫採用之自走機器人系統架構如
圖 4，並已完成一套室內牆壁邊線偵測與追
蹤系統，結合超音波與影像感測資訊以自
動辨識並追蹤牆壁邊線位置。據以提供室
內機器人與牆壁之相對位置關係，提供導
航控制與重建環境之重要資訊。並完成結
合超音波、里程計與影像感測資訊融合以
牆壁邊線重建 2D 室內環境技術。自走機器
人系統與各模組研究之方法分述如後。 
1. 輪式自走機器人系統 
在本計畫中，我們已完成組裝一套影
像追蹤伺服自走車系統，如圖 5 所示。其
中自走車使用 Pioneer 3-DX 為主架構，其
配有 8 方位之超音波感測器。於自走車上
我們自行搭載一套筆記型電腦與一組 PT
攝影機做為核心影像處理之用。筆記型電
腦根據超音波感測與影像處理數據，自動
辨識並追蹤牆壁邊線位置，並重建 2D 室內
環境資訊。未來將進一步自主決定自走車
的行進方向與速度，透過並列埠下達控制
命令控制自走車。 
 
圖 4 影像追蹤伺服自走車架構圖 
 
圖 5 影像追蹤伺服自走車實體圖 
本計畫使用之自走車移動控制方式，
主要是使用差動運動控制 (Differential 
Drive Kinematics)系統[47]，它是由兩個驅
動輪建置於一共同軸上，每個車輪皆可獨
立驅動前進或後退。雖然每個車輪都可以
自由控制前進後退，但因兩輪在共軸上，
因此在自走車旋轉的時候皆依照一瞬間旋
轉中心 ICC(Instaneous Center of Curvature)
如圖 6 所示，其中ሺ ௟ܸ, ௥ܸሻ分別為兩輪速度，
l 為兩輪間距離，R 為自走車旋轉半徑(ICC
到兩輪軸中心點之距離)，߱為旋轉速度，
其關係式如(2)、(3)與(4)。 
 
 
圖 6 以差動系統為基礎之自走車之物理運動模型示意圖 
 6
由於我們的目的是要建置未知室內環
境地圖，因此加入地圖參數 m (圖 9)。其地
圖相關機率函數如式(9)。 
                  pሺ݉௜|ݔଵ:௧, ݑଵ:௧, ݖଵ:௧ሻ              ሺ9ሻ 
但地圖建置與自走車控制無直接相關，因
此式(9)中之控制相關參數可以忽略，改寫
為式(10)，關係貝式網路則如圖 10。 
                    pሺ݉௜|ݔଵ:௧, ݖଵ:௧ሻ                   ሺ10ሻ 
 
圖 9 利用圖 8 之 DBN 加入地圖參數 m 
 
 
圖 10 利用已知資訊建立環境地圖之貝氏網路模型 
2. 以超音波感測器為基礎之室內環境探索
技術 
本章節中，我們將會介紹如何利用超
音波感測器來達到自走車探索室內環境之
功能。由於超音波與慣性導航系統感測器
在自走車探索環境時，扮演重要之角色，
故我們設計其相對應之實驗，來驗證感測
器之效能。在探索環境方面，我們利用建
立格點地圖(Grid Map)來判斷周遭環境情
況，由於超音波感測器在做環境掃描資訊
是透過波形反射，所以在離障礙物角度過
大或是遇到牆角或狹小走道類之室內環境
時，常常會產生感測器之誤差，於是最後
將介紹利用多超音波之結合來改善其誤差
的產生。 
自走車之未知室內環境地圖建置設計 
地圖建置(Map Building)採取目前廣泛
使 用 的 佔 據 格 狀 演 算 法 (Algorithm 
Occupancy Grid Map)。格點(Grid)的粗細，
用以改變地圖表現的精緻程度，格點越細，
地圖越精緻，相對於計算量也會越大，可
在實際應用上面做取捨。每一個格點會給
予一個機率值 GP(Grid Probability，GP)，
值介於 0~1 之間，尚未探索之區域預設為
0.5；若數值越接近 1，表示該格點可能是
障礙物的機率越高；反之為 0，代表是空曠
的空間。如果格點地圖 m 是由 n 個格點所
組成，則地圖中第 i 點之機率值在經過時間
t 之累積，可透過方程式(10)描述。 
利用障礙物ሺm୧ ൌ 1ሻ與空曠ሺm୧ ൌ 0ሻ
區域之比值帶入式(10)，之後利用貝氏定理
(Bayes’ Theorem)展開如方程式(11) 
 
pሺ݉௜|ݖଵ:௧, ݔଵ:௧ሻ ൌ 1 െ ቀ1 ൅
୮ሺ௠೔|௭೟షభ,௫೟షభሻ
ଵି୮ሺ௠೔|௭೟షభ,௫೟షభሻ
כ
ଵି୮ሺ௠೔ሻ
୮ሺ௠೔ሻ
כ ୮
ሺ௠೔|௭೟,௫೟ሻ
ଵି୮ሺ௠೔|௭೟,௫೟ሻ
ቁ
ିଵ
              (11) 
 
由於直接透過貝氏定理來計算m୧之計
算時間複雜度(big-O)非常大，所以我們透
過 二 元 貝 氏 濾 波 器 演 算 法 (Algorithm 
Binary Bayesian Filter)換算為對數比(Log 
Odds)[48]如式(12)以簡化計算複雜度。 
 
݈௧,௜ ൌ
log ௣
ሺ௠೔|௭భ:೟షభ,௫భ:೟షభሻ
ଵି௣ሺ௠೔|௭భ:೟షభ,௫భ:೟షభሻ
൅ log ௣
ሺ௠೔|௭೟,௫೟ሻ
ଵି௣ሺ௠೔|௭೟,௫೟ሻ
െ
log ௣
ሺ௠೔ሻ
ଵି௣ሺ௠೔ሻ
                        (12) 
 
定 義 ݈௧ିଵ,௜ ൌ log ୮
ሺ௠೔|௭భ:೟షభ,௫భ:೟షభሻ
ଵି୮ሺ௠೔|௭భ:೟షభ,௫భ:೟షభሻ
、
反向量測模型ሺ݉௜, ݔ௧, ݖ௧ሻ ൌ log ୮
ሺ௠೔|௭೟,௫೟ሻ
ଵି୮ሺ௠೔|௭೟,௫೟ሻ
與݈଴ ൌ log ௣
ሺ௠೔ሻ
ଵି௣ሺ௠೔ሻ
，則݈௧,௜可改寫為式(13) 
 
݈௧,௜ ൌ ݈௧ିଵ,௜ ൅ 反向量測模型ሺ݉௜, ݔ௧, ݖ௧ሻ െ ݈଴           
(13) 
 8
周遭環境資訊。 
步驟四、 利用超音波偵測結果，取得該方
位之障礙物位置，利用 12 之演算法繪
製出該障礙物之機率分布(圖 17)。 
步驟五、 取 得 自 走 車 所 在 位 置 資 訊
x୲ ൌ ൫x, y,θ൯
T利用式(14)自走車之移
動 與 旋 轉 參 數 將 區 域 格 點 地 圖
m୧ ൌ ൫x୫౟, y୫౟൯累加至全域格點地圖
l୲,୧。 
 
݈௧,௜ ൌ ݈௧ିଵ,௜ ൅ ൤
cos ߠ െ sin ߠ
sin ߠ cosθ ൨ ൤
ݔ௠೔ െ ݔ௟
ݔ௠೔ െ ݕ௟
൨ െ ݈଴   (14) 
                                                                   
步驟六、 透過方程式(15)將概似格點地圖
(Likelihood Grid Map)轉為機率格點地
圖(Probability Grid Map)，使地圖格點
值位於 0~1 之間表示該格點為障礙物
(值為 1)與空曠地區(值為 0)之機率
值。 
 
    pሺ݉௜|ݖଵ:௧, ݔଵ:௧ሻ ൌ 1 െ
ଵ
ଵାୣ୶୮൛௟೟,೔ൟ
 (15) 
 
步驟七、 判斷是否行動結束、如果不是則
回到步驟二。 
 
讀入超音波值(D0~D7)
Yes
繪製區域
感測器地圖
Yes
No
No
轉換為機率格點地圖
並顯示全域地圖
Probability 
grid map
Likelihood 
grid map
根據權重值計算區域地圖之
障礙物(0~1)與空曠(-1~0)分佈
是否移動
超出範圍?
將區域地圖值累加至
相對於全域地圖範圍
車子位移?
(Displacement)
累計區域數值
平移與旋轉區域
開始
 
圖 15 超音波結合影像邊緣偵測之未知環境地圖建立流程 
 
圖 16 自走車區域格點地圖 
-
++ + + + + +
- - - - -
- - - - -
- - - - -
- - - - -
- - -
- - -
- - -
- - -
- - -
- - -
-
-
-
 
圖 17 利用超音波距離之格點加分與減分分佈 
 
圖 18 自走車移動建置地圖示意圖 
最後，我們利用本校電資大樓 3F 之部
分區域(圖 19)做感測器建立未知環境地圖
之環境，並透過不同路徑來觀察地圖建置
之狀態，主要分為四種情況。 
y 第一種：路徑(A) → (B) → (C) → 
(D)，主要觀察在寬廣走道自走車經
過 1 次之地圖建置。(圖 19(a)) 
y 第二種：路徑(D) → (E)，觀察自走
車重覆相同走道之情況。(圖 19(b)) 
y 第三種：路徑(E) → (F) → (G)，自
走車進入狹小巷弄之地圖建置。(圖
(c)) 
y 第四種：路徑(G) → (H)，兩個主道
交錯。(圖 19(d)) 
由地圖建置結果可發現，在(圖 20(c))
中發現仍有許多地方產生誤判之情況，下
一小節將介紹其改良方式。 
 10
讀入超音波感測器數值
(D0~D7)
超音波數值
是否超過閥值?
Yes
No
判斷周遭環境(寬闊或是狹窄地區)
並設定超音波閥值
利用相鄰超音波值
繪製減分區域
繪製加分區域與
減分區域
判斷超音波
是否符合期望值?
Yes
No
利用相鄰超音波值之80%
繪製減分區域
判斷相鄰超音波
是否符合期望值?
Yes
No
利用相鄰與側邊超音波值
選擇各超音波之期望值
(用以判定二次反射數據)
結束
圖 22 利用多超音波感測器結合之格點地圖加減分測略流程圖 
 
 
(a) 牆壁反射(相同反射點) 
 
(b) 牆角反射(相異反射點) 
圖 23 超音波感測器遇到障礙物產生二次反射之情況 
 
超出閥值或
不符期望
 
(a)相鄰超音波符合期望 (b) 相鄰超音波不符合期望 
圖 24 利用相鄰超音波感測器距離來判斷地圖繪製種類 
(a)寬廣走道 (b)重複寬廣走道 
(c)狹小巷道 (d)主道相接 
圖 25 利用多超音波感測器結合之未知環境地圖建置 
 
單超音波與多超音波結合用於未知室內環
境地圖建置比較 
最後我們將單一超音波與多超音波感
測器結合建置地圖做一比較(表 1)。透過不
同區域的比較結果，我們能夠判定多超音
波結合會比單超音波感測器能夠建置較為
完整之室內環境地圖。 
 
表 1 超音波建置地圖比較表 
 單超音波 多超音波結合 
寬
廣
道
路
行
走
一
次
  
比
較
在寬廣的走道行進時，由於多超音波結合比單超音波感
測器能取得更多的資訊量，所以在中間走道減分狀況也
會比較明顯。 
寬
廣
道
路
行
走
二
次
  
比
較 此階段，多超音波結合能比單超音波累計更多環境資訊。
 演算法
步驟一
在資
影像
步驟二
(圖
轉換
之工
算影
像之
顯邊
步驟三
(PM
之位
所示
中介
步驟四
交集
步驟五
隨
(起
斜率
步驟六
回到
 
 
(a)環
(c) 影像邊
訊濾除 
(e)影像遮罩
交集 
圖
3：改良式
、 利用超音
訊，之後
(圖 29 (a
、 首先將彩
29(b))，並
成邊緣影
作。我們
像處理技
環境雜訊
緣影像(圖
、 利用超音
)運算，求
置，並建
，其中詳
紹。 
、 將邊緣二
運算(圖
、 將其步驟
機霍氏轉換
點與終點影
，其偵測
、 是否已達
步驟二。
境原始影像 
緣化、二值化與
域二值化結果
 29 改良式牆壁
牆壁邊線偵
波感測器
便開始擷
))序列。 
色影像轉
利用 Sob
像，以進
並採用形
術，來濾
部份，以
29 (c))。
波所得資
出超音波
立一影像遮
細演算法
值化影像
29(e))。 
四所得之
法偵測出
像座標)，
結果如圖
偵測次數
 
(b)
雜 (d)建立
做 (f) 偵測
參數 
邊線偵測系統
測演算法
取得自走車
取一連續彩
成灰階影
el 邊緣偵測
行影像二值
態學之侵蝕
除掉二值化
得到環境之
 
訊做透視
投影在影像
罩如圖 2
在與下一小
與影像遮罩
邊緣影像
其直線參
並建立其直
29(f)所示
上限，若否
 環境灰階影像
之超音波影像遮
出之正確牆壁邊
實驗過程圖 
12
 
所
色
像
法
化
運
影
明
法
上
9(d)
節
做
，以
數
線
。 
則
 
罩 
線
經過
遮罩
牆壁
會有
細的
驟如
圖
演算
壁邊
步驟
步驟
步驟
利用
遮罩
最容
與走
之後將介
透視法 PM
。主要之
兩種，由
不同的特
介紹，主
演算法 4
讀入
前方障礙物演
建
30 利用超音波感
法 4：利用
緣偵測遮
一、 讀 取
(D0~D7)
二、 判斷
若無障礙
之空間，
邊緣位置
則執行步
三、 判斷
屬於前方
測器建立
就利用左
物遮罩。
 
超音波感
 
在自走車
易遇到的
道牆壁形
紹利用超音
，來建立影
牆壁邊緣可
於牆壁在不
性，所以後
要程式流程
： 
Yes
有障礙物?
超音波感測器距
(D0~D7)
算法
前方有障礙物
Yes
側邊
立超音波之影像
開始
結束
測之資訊建立影
超音波感測
罩演算法 
超 音 波
。 
自走車周圍
物則表示自
無法利用既
，結束演算
驟三。 
牆壁位於自
障礙物則利
前方障礙物
右兩邊感測
 
測器建立左
探索室內環
場景就是走
成水平關係
波感測器
像牆壁邊
分為前方
同方位在
面小節會
如圖 30，
No
離值
?
障礙物演算法
No
遮罩
像牆壁邊線偵
資訊建立
感 測 器 之
是否有障
走車位於
有資訊判
法；若以
走車之位
用前方超
影像遮罩
器建立側
右兩邊牆
境地圖的
道，由於
，主要感
的資訊
緣偵測
與側邊
影像上
再做詳
執行步
 
測之遮罩 
影像牆
數 值
礙物，
一空曠
斷牆壁
障礙物
置，若
音波感
，否則
邊障礙
壁邊線
過程中，
自走車
測距離
 14
利用超音波感測器建立前方牆壁邊線遮罩 
與側邊偵測牆壁邊緣不同，前方之障
礙物可利用透視法(PM)可以將前方(D1~D6)
超音波感測器投影至影像平面上如圖34所
示(詳細數值見表 2)，其影像點可以將影像
分為七個區域，因超音波偵測角度不同，
所以我們可以依不同區域選擇不同的遮罩
規則與邊線斜率過濾方式，但由於感測器
特性，相鄰之感測器較不容易同時偵測到
障礙物，所以無法直接利用各感測器透視
法結果圍成區域來做影像遮罩。所以需要
比較各相鄰感測器之數值，並擴大影像遮
罩之範圍，程式步驟如演算法 6。 
 
 
圖 34 影像遮罩區域與障礙物偵測範圍示意圖 
表 2 影像遮罩區域與障礙物偵測範圍 
 
 
演算法 6：利用超音波感測資訊(D1~D6)建
立前方影像牆壁邊緣偵測遮罩演算法 
步驟一、 利用超音波感測器取得前方
(D1~D6)兩邊之距離。 
步驟二、 建 立 一 超 音 波 暫 存 區 矩 陣
(D1’~D6’)，用以暫存判斷兩顆相鄰超
音波數值大小數值。 
步驟三、 分別判斷兩兩相鄰之超音波感測
器之數值，取其較小之值填入暫存器
內，其流程如圖 35 判斷相鄰超音波之
數值所示，分別從 D1 到 D6 依序判
斷。 
步驟四、 將暫存器之超音波暫存值透過透
視(PM)法投影於影像平面上，分別為
ሺα
ଵ
,β
ଵ
ሻ~ ሺα
଺
,β
଺
ሻ。 
步驟五、 ሺ320,β
଴
ሻ  →  ሺ0,β
଴
ሻ  → 
ሺ320,β
଴
ሻ  →  ሺα
ଵ
,β
ଵ
ሻ  →  …  → 
ሺα
଺
,β
଺
ሻ → ሺ320,β
଺
ሻ →ሺ320,β
଴
ሻ
之區域，如圖 36。 
步驟六、 由於超音波之最大偵測距離為
5000mm，所以對於離自走車較遠之未
知區域將不予以偵測，圖 37 所示。 
步驟七、 回到演算法 3 以建立超音波之影
像遮罩。 
 
 
圖 35 判斷相鄰超音波之數值 
 
(a) 示意圖 
 
(b) 影像遮罩 
圖 36 將超音波暫存區(D1’~D6’)透視(PM)投影於影像平面 
 利用影像
(Grid M
透過
之牆壁
(IPM)將
平面上
平面之位
位置，為
aα、aβ
圖 39，可
其中 α 為
 
ቂ
ݔ
ݕቃ ൌ ୲ୟ୬
 
圖 39
 
(a)
圖 40 利用
礙物與空曠
1: 反
2: 設
3: ׎
4: if
5: if
6: if
7: e
+ + + +
- - -
- -
-
牆壁邊緣
ap) 
超音波感
邊緣偵測之
影像牆壁
，公式如式
置，ሺα, β
ሺα଴,β଴ሻ影
為攝影機校
求出影像
偵測障礙
ିு
൫௔ഁሺఉିఉబሻ൯
ቈ
si
co
利用影像邊緣偵
近距離障礙物 
影像邊緣來偵測
區域之加(黑)減
向量測模型
定ݔ௜, ݕ௜為
影像偵測之
點分別為(
ൌ atan2ሺݕ
 ሺ୷ି୷భሻ
ሺ௫ି௫భሻ
൐
׎ ൐ atan2
or ׎ ൏
x୧ሻ then  r
 ቚሺ୷ି୷
ሺ௫ି௫
then  return
 ሺ୷ି୷భሻ
ሺ௫ି௫భሻ
൏
ሺ
ሺ
݈௙௥௘௘ 
nd if 
- -
++ + + + + + +
-- - - - - - -
-- - - - - - -
-- - - - - - - -
-- - - - - - - -
-- - - - - - - -
- - - - - - - -
- - - - - - -
- - - - - -
- - - - - -
- - - - -
- - - -
- - -
- -
建立區域
測器資訊
結果，可
邊緣線段
(16)，其中
ሻ為影像直
像上無窮
正參數，
牆壁邊緣偵
物之厚度
n൫ܽఈሺߙ െ ߙ଴ሻ
s൫ܽఈሺߙ െ ߙ଴
測之未知室內環
 
 
(b)遠
不同距離之障礙
(白)分分佈。 
(݉௜, ݔ௧, ݖ
地圖݉௜之
牆壁邊線
(x1,y1),(x2,y
௜ െ ݕ, ݔ௜ െ
ሺ୷మି୷భሻ
ሺ௫మି௫భሻ
൅ α
ሺyଵ െ y୧, xଵ
atan2ሺyଶ െ
eturn ݈଴ 
భሻ
భሻ
െ
ሺ୷మି୷భሻ
ሺ௫మି௫భሻ
 ݈௢௖௖ 
୷మି୷భሻ
௫మି௫భሻ
  the
格點地圖
建利影像遮
利用反透視
轉換至自走
(x, y)為實
線偵測之端
遠處之位置
透過演算法
測地圖模
。 
൯
ሻ൯
቉ 
境地圖演算法
 
距離障礙物
物所產生之疑
௧) 
中心點,
左右端
2)) 
ݔሻ െ ߠ 
/2  or 
െ x୧ሻ  
y୧, xଶ െ
 ቚ ൑ α/2  
n  return 
16
罩
法
車
體
點
，
如
型，
(16) 
 
 
似障
利用
機率
測器
多為
但因
器人
量的
Feat
計算
研究
為基
器人
這兩
估算
立環
 
音波
地圖
方之
感測
環境
內環
 
 
 
五、
 
Acti
超音波感
格點地圖
一般室內
量測及影
使用雷射
雷射測距
相關研究
部分則較
ure Transfo
複雜，要
係以超音
礎，藉由
的控制命
種感測器
出機器人
境地圖。
 
表 6 超音波感
如圖 41 所
主要偵測
，而影像
資訊；而
器能夠取
資訊量。
境地圖建
(a)超音波
圖 41 利用超
結果與討
本 節 中
veMedia 公
測器與影像
(Probabilit
探索所採取
像量測，而
測距儀(Las
儀的成本較
而言是一大
多使用 SI
rm)演算法
達到即時處
波感測器(
感測器所得
令，並結合
之優點(如表
當時所在環
 
測器與影像感
示，可以明
範圍為自走
部分則則注
在資料取得
得比超音波
利用超音波
置實驗結果
 
 
 
 
 
音波與影像感測
論 
， 我 們 利
司的 P3-D
資訊結合
y Grid Ma
的量測方
感測器量
er range F
為昂貴，
負擔；而
FT(Scale I
，但 SIFT
理較為困
Sonar)的量
到的測量
影像資訊
6)予以結
境情況資
測器之優缺點比
顯得比較
車鄰近之
重於自走
範圍而言
感測器來
與影像結
見下節。
(b)影像牆壁邊
器建立地圖之比
用 自 行 改
X 自走車
以建立
p) 
式為感
測部分
inder)，
對於機
影像測
nvariant 
演算法
難。本
測方法
值與機
，利用
合，以
訊，建
較 
出來超
障礙物
車正前
，影像
的多的
合之室
 
線 
較 
裝 之
，以驗證
 
 18
  
 
 
(a) (b) 
  
 
 
(c) (d) 
  
 
 
(e) (f) 
 
 
 
 
 
 
 
(g) (h) 
  
(i) (j) 
  
 
 
 
(k) (l) 
  
 
 
 
(m) (n) 
  
 
 
 
(o) (p) 
  
 
 
 
(q) (r) 
  
 
 
 
(s) (t) 
 
 
 
 
 
(u) 
圖 43 測邊牆壁邊線偵測實驗影片片段 
 
 
在透過前方超音波建立影像牆壁邊線
偵測實驗(圖 42 圖)中主要可分為三個區塊
(圖 42(a))，各區塊分別為影像邊緣二值化
(左上)、裡用超音波建立之影像遮罩(左下)
以及實際擷取影像與牆壁邊線偵測結果
(右)。在實際影像之牆壁邊線偵測結果所繪
出之線段，可透過不同顏色線段來判斷是
透過哪一個區域參數來做牆壁邊線之判斷。
而透過建立側邊影像牆壁邊線遮罩之實驗
(圖 43)中則是分為四個區塊(圖 43(a))，分
別為影像邊緣二值化(左上)、裡用超音波建
立之影像遮罩(左下)、將影像邊緣二值化與
影像遮罩做交集(右下)以及實際擷取影像
與牆壁邊線偵測結果(右上)。 
 
自走車探索未知環境並建立環境地圖實驗 
本節中，我們以本校電資大樓三樓環
境實際驗證上述結合超音波與影像感測資
訊融合以牆壁邊線重建 2D 室內環境技術。
並成功地重建三樓局部環境之室內地圖。
其移動路線如圖 44 所示。 
 
 
 
 
 20
訊，自動產生導航策略成功探索環境。 
為驗證上述技術之效能，我們並於本
校電資大樓三樓部分環境進行自走車自主
性探索環境之實驗。而經由實驗結果驗證
本計畫所研製的兩類環境探索技術，皆能
有效達成室內探索與巡邏功能。預期用於
智慧型機器人於室內探索環境之應用，例
如保全巡邏、導覽服務等具有實質之貢
獻。 
 
七、參考文獻 
[1]  中國科學院計算機網路信息中心 
[2]  World Robotics 2008, 
http://www.worldrobotics.org/index.php 
[3]  日本愛知世博會資訊網, 
http://www.expo2005.or.jp/tcn/index.html 
[4]  行政院, http://www.ey.gov.tw/ 
[5]  機器人世界情報網, 
http://www.robotworld.org.tw/ 
[6]  G. N. Desouza and A.C. Kak,s “Vision for mobile 
robot navigation: a survey,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 
24, pp.237-267,Issue, 2, Feb. 2002. 
[7]  S. Atiya and G.D. Hager, “Real-Time 
Vision-Based Robot Localization,” IEEE 
Transactions on Robotics and Automation, vol. 9, 
pp. 785-800, Dec. 1993. 
[8]  A. Kosaka and A.C. Kak, “Fast Vision-Guided 
Mobile Robot Navigation Using Model-Based 
Reasoning and Prediction of Uncertainties,” 
Computer Vision, Graphics, and Image 
Processing—Image Understanding, vol. 56, no. 3, 
pp. 271-329, 1992. 
[9]  M. Meng and A.C. Kak, “NEURO-NAV: A Neural 
Network Based Architecture for Vision-Guided 
Mobile Robot Navigation Using Non-Metrical 
Models of the Environment,” IEEE Int'l Conf. on 
Robotics and Automation, vol. 2, pp. 750-757, 
1993. 
[10]  M.R. Kabuka and A.E. Arenas, “Position 
Verification of a Mobile Robot Using Standard 
Pattern,” IEEE Journal of Robotics and 
Automation, vol. 3, no. 6, pp. 505-516, Dec. 1987. 
[11]  Ching-Chih Tsai, Ssu-Min Hu, Hsu-Chih Huang, 
and Shih-Min Hsieh, “Fuzzy Hybrid Navigation 
of an Active Mobile Robotic Assistant,” 
Proceedings of 2007 CACS International 
Automatic Control Conference, National Chung 
Hsing University, Taichung, Taiwan, Nov. 9-11, 
2007 
[12]  H.P. Moravec and A. Elfes, “High Resolution 
Maps from Wide Angle Sonar,” Proc. IEEE Int’l 
Conf. Robotics and Automation, pp. 116-121, 
1985. 
[13]  H. Choset, I. Konukseven, and A. Rizzi, “Sensor 
Based Planning: A Control Law for Generating the 
Generalized Voronoi Graph,” Proc. Eighth IEEE 
Int’l Conf. Advanced Robotics, pp. 333-8, 1997. 
[14]  E. Chown, S. Kaplan, and D. Kortenkamp, 
“Prototypes, Location, and Associative Networks 
(PLAN): Towards a Unified Theory of Cognitive 
Mapping,” Cognitive Science, vol. 19, no. 1, pp. 
1-51, Jan. 1995. 
[15]  S. Thrun, “Learning Metric-Topological Maps 
for Indoor Mobile Robot Navigation,” Artificial 
Intelligence, vol. 99, no. 1, pp. 21-71, 
[16]  N. Ayache and O. D. Faugeras, “Maintaining 
Representations of the Environment of a Mobile 
Robot,” IEEE Transactions on Robotics and 
Automation, vol. 5, no. 6, pp. 804-819, 1989. Feb. 
1998. 
[17]  Michael Csorba , “Simultaneous Localisation 
and Map Building”, PhD Thesis, the Department 
of Engineering Science, University of Oxford. 
[18]  M. W. M. Gamini Dissanayake, Paul Newman, 
Steven Clark, Hugh F. Durrant-Whyte, Member, 
and M. Csorba, “A Solution to the Simultaneous 
Localization and Map Building (SLAM) Problem,” 
IEEE Transactions on Robotics and Automation, 
vol. 17, no. 3, June 2001 
[19]  S.Y.Chung, H.P.Huang” Relative-Absolute Map 
Filter for Simultaneous Localization and 
Mapping,” 2006 IEEE/RSJ Intl. Conf. on 
Intelligent Robots and Systems, 7-12, Oct. 2006 
[20]  S. Se, D. Lowe, and J. Little, “Mobile robot 
localization and mapping with uncertainty using 
scale-invariant visual landmarks,” The 
International Journal of Robotics Research, vol. 
21, no. 8, pp. 735–758, Aug. 2002. 
[21]  Stephen Se, David G. Lowe, and James J. Little, 
“Vision-Based Global Localization and Mapping 
for Mobile Robots,” IEEE Transactions on 
Robotics, VOL. 21, NO. 3, June 2005 
[22]  W. Y. Jeong and K. M. Lee, “CV-SLAM A new 
Ceiling Vision-based SLAM techinique.” 
[23]  M. B. Holer, M. M. Trivedi, and S. B. Marapane, 
“Mobile robot navigation by wall following using 
a rotating ultrasonic scanner,” Electrical and 
Computer Engineering Department, University of 
California at San Diego, 1996. 
[24]  賴建宏, ”研製具有探索室內環境功能之影像
導航自走車”, 私立聖約翰科技大學電機工程研
究所碩士論文, 2006 年 7 月. 
[25]  L. Xu, E. Oja and P. Kultanen, “A New Curve 
Detection Method: Randomized Hough Transform 
(RHT),” Pattern Recognition Letters, pp. 331-338, 
1990. 
[26]  G. Y. Jiang, T. Y. Choi, S. K. Hong, J. W. Bae, 
and B. S. Song, “Lane and Obstacle Detection 
Bbased on Fast Inverse Perspective Mapping 
Algorithm,” 2000 IEEE International Conference 
on Systems, Man, and Cybernetics, pp. 2969-2974, 
October, 2000 
[27]  A. Anwander, B. Neyran, and A. Baskurt, 
“Multiscale Colour Gradient for Image 
Segmentation,” Knowledge-Based Intelligent 
Engineering Systems and Allied Technologies, 
2000. Proceedings. Fourth International 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           99 年 7 月 22 日 
報告人姓名 簡忠漢 服務機構及職稱 聖約翰科技大學電機系副教授 
時間 
會議 
地點 
自 99 年 7 月 13 日 
至 99 年 7 月 16 日 
日本大阪 
本會核定
補助文號
 
會議 
名稱 
 (中文) 2010 第八屆 IEEE 工業資訊學國際會議 
 (英文) 2010 8th IEEE International Conference on Industrial Informatics(2010 
INDIN) 
發表 
論文 
題目 
 (英文) Information Rate Control for Characterizing Moving Objects in 
Networked Visual Applications 
報告內容應包括下列各項： 
一、參加會議經過 
 
 
二、與會心得 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
四、建議 
 
 
五、攜回資料名稱及內容 
 
 
六、其他 
 
 
 
 
表 1、TT9-4 議程表 
TT9-4 Industrial Informatics Applications  
Co-Chairs: Kazuhisa Seta, Takaaki Yamada  
Lecture room 2 (7F),     15:30-16:45 15th July 
Session No. Paper Title Country Author 
TT9-4-1 Modeling CAN Network Using PRISM Taiwan
Cheng-Min Lin Chen-Wei Yang, 
Hui-Kang Teng, Ming-Cheng 
Chung, Kuo-Chen Lang, Heng-Fa 
Teng 
TT9-4-2 
Information Rate Control for 
Characterizing Moving Objects in 
Networked Visual Applications 
Taiwan Feng-Li Lian, Yi-Chun Lin, 
Jong-Hann Jean 
 
 
除此之外，我參加了幾個不同主題之常態性論文發表。例如 A Router for Improved Fault 
Isolation, Scalability and Diagnosis in CAN; Maglev Platform Networked Control: A Profibus DP 
Application; Integration of WirelessHART Networks in Distributed Control Systems using 
PROFINET IO; Evaluating the impact of communication latency on applications running over 
on-chip multiprocessing platforms: a layered approach; Design and Evaluation of A Fieldbus 
Porotocol Over IPv6; Design and Evaluation of IPv4/IPv6 Translator for IP Based Industrial 
Network Protocol; Model-based Decentralised Allocation of Product Flow Paths, Adaptive 
real-time video transmission over DDS 等。 
 
   
圖 1、由我們組團之 TT9-4 場次論文發表情形 
  
圖 2、大會註冊區與專題演講會場 
 
 
Information Rate Control for Characterizing Moving 
Objects in Networked Visual Applications 
 
Feng-Li Lian(a), Yi-Chun Lin(a), and Jong-Hann Jean(b) 
(a) Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan,  
(b) Department of Electrical Engineering, St. John’s University, Taipei, Taiwan 
E-mail: fengli@ntu.edu.tw 
 
 
Abstract- This paper analyzes the information rate control for 
characterizing moving objects in networked visual applications 
where limited computational and communication resources are 
used in the characterization task. Several key processing units in 
networked visual applications are presented. The goal is to 
utilize suitable image processing techniques for identifying the 
key parameters of the important objects in the video scene and 
transmit the best information in terms of both temporal and 
spatial domains to proper users. In the end, all the 
interconnected users can properly share their information under 
limited computational power and communication bandwidth. 
Particularly, three scenarios of information rate control for 
characterizing moving objects in visual applications are 
discussed. 
I. INTRODUCTION 
The paper deals with different aspects of information rate 
control for characterizing moving objects in networked visual 
applications where limited computational and communication 
resources are used in the characterization task. Information 
rate control is one of the most important and fundamental 
technologies to develop a multiple networked visual system 
such as outdoor monitoring systems for classifying or 
identifying activities from multiple videos, target detection 
and tracking in security and traffic monitoring systems, 
distributed mobile robots patrolling in security area, 
videophone and video conferencing. 
Generally speaking, the information rate can be classified 
into two types: namely temporal rate and spatial rate. The 
temporal rate is typically denoted as the acquisition or 
sampling rate of the visual sensor, e.g., camera, and the 
spatial rate can be denoted as the compression ratio of each 
visual frame for data storage or transmission. 
From the temporal point of view, the information of visual 
data, e.g., video streams, can be characterized by the 
acquisition frequency of a camera. The higher the frequency, 
the more information can be obtained by the camera. 
However, if the objects captured by the camera are moving 
slowly or the motion of the camera is slow, the information 
difference between two consecutive frames is small and 
higher acquisition rates do not generate much more useful 
information, but consume too much computational power and 
utilize large storage size and communication bandwidth. 
On the other hand, from the spatial point of view, the 
information of visual data can be characterized by the size of 
the video frame. The bigger the frame, the more information 
can be obtained. Similarly, if some objects captured by the 
camera are not important, it is unnecessary to use large image 
size to represent (or code) these objects for storage and/or 
network transmission. Hence, the compression ratio of an 
image can be adaptively adjusted based on the importance of 
each individual object captured by the camera. That is, if an 
image contains multiple objects or areas of different 
importance, different compression ratios can be applied into 
each object or area. Therefore, the image size or the contained 
information of an image can be represented by a just-in-size 
code. 
The whole procedure of the temporal-spatial 
characterization on visual data can be summarized as follows: 
1. Capture image from multiple cameras; 
2. Identify useful information from the image frames; 
3. Transmit processed video to others through Internet; 
4. Fulfill the request of different commanding users; 
5. Re-adjust the capturing rate and frame size of the camera 
system. 
Hence, under limited computational power and 
communication bandwidth, only proper information should 
be processed at local sites as well as at remote sites and, when 
required, transmitted through communication network. 
In order to fully characterize the information quality and 
quantity, related methodologies in image acquisition, image 
processing, image display or animation, camera control, 
direction control, image compression, communication rate 
control should be carefully studied and implemented. Key 
techniques for realizing these methodologies include image 
acquisition rate control, moving object detection, background 
subtraction, temporal sampling, spatial sampling, camera 
control, communication rate control.  
The ultimate goal of  implementing these techniques are to 
dynamically, adaptively adjust the detection block, 
acquisition rate, compression ratio, and/or computational rate. 
The adjustment of the detection block can be based on the 
size of moving object. Also, the adjustment of the acquisition 
rate can be based on the dynamical behavior (such as moving 
speed/direction) of moving objects and/or the importance of 
one image frame, that is, performing temporal sampling 
among a sequence of consecutive video frames. Similarly, the 
adjustment of the compression ratio on different range of one 
image frame could be based on the importance of moving 
objects, background objects, etc., that is, performing spatial 
sampling, and that of the computational effort on displaying 
transformation and employed split-and-merge contour models 
to detect and track of moving objects from a moving camera 
image sequence [8]. Arnell and Petersson segmented moving 
objects from images taken with a moving camera based on a 
special representation of optical flow, on which u-disparity is 
applied [9]. Berrabah et al. used GMM approach to model the 
background and combined the background with the motion 
cue in MAP-MRF framework [10]. Yuan et al. utilized the 
consistent with the multi-view geometric constraints result 
from camera motion to segment the video frames into a static 
background and a number of motion regions [11].   
Instead of knowing the pan/tilt angles from active camera 
to perform background compensation, the method proposed 
in [12: Murray & Basu 1994] is used to detect block motion 
estimation between two consecutive image frames to identify 
camera motion. The result helps find the moving objects and 
reduces the motion points of static background due to camera 
motion. 
III. RATE CONTROL FOR VISUAL RECONSTRUCTION 
By selecting an appropriate image acquisition rate, the 
computational load of the 3D positioning as well as the 
memory cost can be reduced. The designed methods are 
based on the states of moving objects by analyzing the 
sequence of images obtained from the cameras. That is, the 
control method for adjusting image acquisition rate is based 
on the image information which is the velocity of object 
centroid in the image domain calculated from the captured 
images. In this case, all cameras must adjust to a same image 
acquisition rate synchronously. Therefore, the problem of no 
captured images in pairs can be solved. If captured images are 
in pairs, the object positions in the physical domain can be 
estimated by the step of 3-D positioning. 
In order to achieve the goal of adjusting all cameras to a 
same image acquisition rate, the comparison of predicted 
image acquisition rate from each camera must be done. By 
the result of the comparison, a specific image acquisition rate 
for all cameras can be decided. Each camera predicts an 
image acquisition rate for next image. This control algorithm 
compares the result of the predicted image acquisition rate for 
each camera. The specific image acquisition rate is chosen as 
the highest image acquisition rate predicted from each 
camera. At last, all cameras adjust to this specific image 
acquisition rate synchronously. Therefore, the captured 
images from the all cameras can be guaranteed in pairs. 
A camera-ball system with two cameras and three balls are 
proposed to test the scenario. The 3D model of these balls is 
reconstructed by using the images taken by the camera. 
According to the difference of the ball velocity, the 
corresponding acquisition rate of camera is used. A general 
case for determining image acquisition rate by ball velocity in 
the physical domain can be shown in Fig. 1. Namely, when 
the velocity of ball is quicker, the sample rate is set faster. At 
the same time, the quality of information is maintained. 
 
…………………………………
Ball Velocity
(Slowest ) (Fastest)
Rate 2 Rate 3 Rate N
V1 V2 V3 VN-1 VN
……
V0
Rate 1
Fig. 1. A general case for determining image acquisition rate by 
object velocity in the physical domain 
Three kinds of sampling rate: 1 time/sec, 2 times/sec, and 4 
times/sec are used. If the ball velocity is less than 32 mm/sec, 
within 32 mm/sec and 48 mm/sec, and bigger than 48  
mm/sec, then the sampling rate uses 1 time/sec, 2 times/sec, 
and 4 times/sec, respectively. The values of 32 mm/sec and 
48 mm/sec are obtained by performing 20 different 
experiments and averaging these velocity data. Fig. 2(a) 
shows the original placement of the three balls. Fig. 2(b) 
shows the 3-D positioning result in the virtual environment. 
Table I shows the comparison of the accuracy of ball position 
at different acquisition type. 
 
  
-10
0
10
20
0
20
40
60
80
-15
-10
-5
0
5
10
15
X-axis Z-axis
3D Positioning Result
camera
camera
Y-
a
xi
s
(a)                                    (b) 
Fig. 2. An example for the 3-D positioning results of the three 
balls. (a) A snapshot of the original placement; (b) The 3D 
positioning result in the virtual environment. 
TABLE I 
COMPARISON OF THE ACCURACY OF BALL’S POSITION OF DIFFERENT 
ACQUISITION TYPES 
Acquisition 
type Rate 
Accuracy of 
ball’s position 
Dynamic 
V<32: 1 times/sec 
32 ≤ V ≤ 48: 2 
times/sec 
V>48: 4 times/sec 
71.1% 
Fixed 1 times/sec 67.7% 
V: Ball’s velocity (mm/sec) 
IV. OBJECT LOCATION PREDICTION 
Many computer and machine vision applications employ 
foreground detection methods. The purpose is to segment an 
image sequence into interesting foreground objects and 
background areas. If the direction of object motion is 
predicted accurately, then the finding range in the foreground 
detection method can be narrow. Although the motion is 
helpful to detect foreground objects, it requires a heavy 
computational load when detecting all the motions of an 
image. In previous applications, some fast search algorithms 
are proposed to reduce the computational load of motion 
estimation. However, not all the motions are important in an 
image. Only the motion related to the foreground objects is 
critical a foreground detection system. The background 
motion is not necessary for detecting foreground. Therefore, 
the motion estimation process has no need to be applied in the 
background area.  
the result of moving object detection, the temporal sampling 
chooses the frames in F frames based on the importance of 
each frame. When /F N  is determined, R is also 
determined. Then, wavelet coding algorithm is used to 
compress the chosen frame by R times. 
The second one is that N and R are decided in each frame. 
That is, R and /F N  are assumed and adjusted in each 
frame based on the result of the moving object detection of 
past frames and the number of chosen frame in this second. 
Table IV compares the two approaches in terms of delay and 
quality aspects. 
TABLE IV 
THE COMPARISON BETWEEN TWO PRINCIPLES 
Approach Decision Interval 
Delay of each 
received frame 
Miss of 
important 
information 
1 Each second ( )p tF T T× +  Low 
2 Each frame p tT T+  High 
A. Temporal Sampling 
It is necessary to reduce data sampling when task meets the 
limitations of bandwidth on data transmission. A direct 
solution to this problem is to reduce the amount of image 
captured on the time axis. To select the clearest one in a 
sequence of images, the fuzzy images are removed based on 
edges count. That is, the clearest image of the most edges 
count is selected. However, during the processing of image 
reduction, to preserve the data integrity, an algorithm is 
proposed. For example, the clearest image of every three 
images could be selected. Based on the currently available 
bandwidth, the strategy on the number selected can be tuned. 
Fig. 4 shows the case in selecting the one of the clearest one 
of every 3 images. 
Edges Count 1754 1805 1793 1767 1774 1917
Frame No.
1 2 3 4 5 6
I(t-1) I(t)
Fig. 4.  Temporal Sampling in the case of selecting the one of the clearest 
one every 3 images.  
B. Set ROI 
Since the camera is mounted on one mobile robot, the 
camera is moving. In order to obtain the important 
information needed from an image, the background 
information should be eliminated. The algorithm for 
background compensation is then explored. The method 
includes Varying Edge Detection, Detecting Block Motion 
Estimation, and Moving Edge Detection. These techniques 
are discussed in the following. Fig. 5 shows an example on 
evaluating different motion vectors. In this example, five 
searching blocks are used to decide the number of the pixel 
length to shift the image. Use the decision algorithm to decide 
the most suitable shift vector and eliminate the vector with 
large difference compared with RMS. 
Fig. 5.  Detail evaluation and example of selecting better shift vector in 
Detecting Block Motion.  
C. Spatial Sampling 
In the part of spatial sampling, the method by the level of 
detail is presented. After setting ROI, the foreground 
information is obtained. Furthermore, the transmission data in 
spatial can be reduced. Around the edge of region of interest, 
the level of detail is applied. As shown in Fig. 6, different 
levels of detail are set according to the importance of 
information. That is, right on the edge, the lowest level 
sampling is set and the highest level sampling is applied on 
the region far from the edge. In this case of Level 0,  none of 
them are sampled. In the case of Level 1, every 4 pixels are 
used to sample that region. The black dots represent 
information used. In Level 1, only 25-point data are 
transmitted instead of sending 81-point data. And in the case 
of Level 2, every 16 points are sampled and this result in 
sending only 9-point data after sampling. 
Set Levels
Region of Interest
1 0 0 2
1 0 1 2
0 0 1 2
1 1 2 2
Level = 1
Level = 0
Level = 2
Fig. 6.  Level of Detail. Right side three images is the sampling level. 
Downer left is the detail of setting levels around edges of ROI.  
D. Experimental Test 
An experiment is performed as follows: one moving 
camera turns right as recorded as Frame 0 to Frame 37, and 
then turns left as recorded as Frame 38 to Frame 78. From 
Frame19 to Frame 43, there are no moving objects. 
Temporal sampling is used in the first step of data 
reduction. The amount of contour in a sequence of images is 
recorded and the clearest one of every 3 images is decided. In 
order to quantify the motion difference with and without 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
籌備 SICE2010 國際會議，協辦 SICE WEEK 救援機器人展示， 
入圍 99年智慧型機器人產品創意競賽 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
