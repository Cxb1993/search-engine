摘要 
 
 
 
動態姿態辨識是先進人機互動介面的核心技術。在這個計畫，動態姿態
時間對位與辨識是分開處理的。我們建立一個相關濾波器來快速偵測目
標物。並且度量目標物體狀態與每一個可能狀態的期望值。這個期望值
是建構在使用粒子過濾器方法所估計的事後機率。由於粒子過濾器可以
處理非線性與非高斯追蹤問題，我們的方法可以應付較複雜的目標物運
動。在與動態姿態模型對準後，我們抽取具鑑別力的狀態來進行辨識。
經過實際影像測試，初步的實驗結果顯視我們所提出的方法具有高度可
行性。 
 
 
 
 
 
 
 
關鍵字: 基於核函數之可變形形狀模型; 粒子過濾器; 動態時間校正; 
動態姿態辨識 
In this work, we integrate three algorithms, namely, the kernel-based deformable shape
model [1], the dynamic time warping algorithm [2–6], and nonlinear/non-Gaussian Bayesian
tracking [7–11] together to recognize dynamic gesture patterns. Due to the high dimension-
ality of dynamic human gestures, Lichtenauer et al. [12] suggest that alignment of dynamic
gesture patterns and classification should be separated Here, a framework based on that of
Lichtenauer et al. [12] is used to recognize dynamic human gestures. The main advantages
of the proposed approach are as follows.
• A deformable shape model is adopted to detect the target body part. This step can
be carried out efficiently because of making use of the fast Fourier transform.
• The particle filter is used to estimate the posterior probability density function of
the position of the target body part. Since the particle filter has the capability of the
non-Gaussion and non-linear Bayesian tracking, the proposed approach is capable of
handling a more complicated motion of the target body part.
• Due to making use of the Lichtenauer approach, the alignment and recognition is
separated here, and the classification can be more effective.
In the following sections, Section 2 reviews the approach to vision-based analysis of
human activities. Section 3 presents the proposed approach. Section 4 shows the experi-
mental results. The concluding remarks are drawn in the last section.
2 Related Researches
Several comprehensive survey papers [13–22] have shown that the systems of vision-based
analysis of human activities can be categorized from various perspectives. A four-phase
framework for a taxonomy of such systems is adopted by Moeslund and Granum [14, 15],
and Wang et al. [22]. This framework describes that the vision-based system for analyzing
human activities generally has the following four phases.
• The initialization/preprocessing phase configures the system for the current scene
and a proper model for the target body part.
• The tracking phase tracks and segments the target body part.
• The pose estimation phase estimates the pose parameters.
• The recognition phase interprets the human activity.
Mitra et al. [13] highlighted some techniques useful for dynamic gesture recognition such
as the hidden Markov model and the particle filter. Aggarwal and Cai [20] reviewed
the techniques for target human body modeling, tracking, and recognition. Gravrila [18]
categorized such systems into the 3D approach, and the 2D approach with or without
2
algorithm. Hence, the system to recognize dynamic human gestures should have a
global detector for the target body part as well as the tracking algorithm.
• Recognition. How to classify the captured motion into one of the predefined actions
is the main concern of the recognition stage. The state vectors of the target body part
corresponding to the captured motion may be the feature of the captured motion.
Besides, trajectory recognition is often based on the state vectors directly obtained by
the tracking process [25,26]. The approach to model-parameter estimation has three
approaches, namely, model-free, indirect-model-use, and direct-model-use [14,15].
– The model-free category:
∗ The bottom-up approach firstly detects the individual body parts and then
assembles together nearby body parts consistent with the constraint of a
human body to best match the observation [24].
∗ The example-based approach estimates the pose parameters by comparing
the observed image with a database of learned exemplars with known pose
parameters [27].
∗ The learning-based approach uses non-linear regression functions to map
the observed target human body to the pose parameters. Agarwal and
Triggs [28] used the non-negative matrix factorization [29, 30] to derive a
local part-based representation of the human upper body, and applies non-
linear regression to map the local part-based representation to the 3D pose
of the upper body. Their approach can recognize the pose of the upper
body in cluttered environments.
– The indirect-model-use category:
∗ A priori parametric model is used as a reference to guide the interpretation
of observed human body parts [31].
– The direct-model-use category:
The approach of the direct-model-use category estimates the model parameters
of a parametric model.
∗ If the target body part is rigid, classical techniques such as the 2D-to-2D,
2D-to-3D, and 3D-to-3D approaches [32] may be applied.
∗ For estimating the model parameters of an articulated or elastic object,
related researches can be found in [33].
∗ The preceding two approaches both need feature correspondences. However,
establishing feature correspondences is not a trivial problem in practice. In
order not to rely on feature correspondences, the top-down approach, which
is an analysis-by-synthesis approach, is an alternative [34]. The main dif-
ficulty of the top-down approach is high computation cost for exploring
4
system, the dynamic gesture recognition system should be based on a simple individual
model and capable of recognizing dynamic gestures in cluttered scenes.
3 The proposed approach
The proposed system has two phases, namely, the training phase and the detection phase,
which are shown in Figures 1 and 2. The input of the training phase is a set of training
dynamic gesture patterns of M classes. The target body part in each frame of the training
dynamic gesture pattern is cropped and aligned together manually, and represented by the
extended shape vector [1]. The training phase makes use of the kernel PCA to retain the
main feature of the training target body part, and adopt the one-class SVM to produce
a correlation filter for detecting the target body part. Meanwhile, the k-means algorithm
is also employed for vector quantization, and s cluster centers are thus produced as the
representative target body part. A correlation filter is also generated for each cluster center.
Each training target body part is represented by an s-vector of the ratio of the fitness of
the target body part to the s representative target body parts. Thus, the training dynamic
gesture pattern is transferred to a sequence of s-vectors, each of which can be regarded
as a state. Finally, the discriminatory state for each class of dynamic gesture patterns is
determined for classification.
The detection phase makes use of the correlation filter for detecting the position of the
target body part in each input frame. The position of the target body part across the
input frames is tracked by the particle filter. The purpose of the particle filter here is to
estimate the posterior probability density function of the position of the target body part.
Each frame is represented by an s-vector of the ratios of the expectations of the fitness of
a frame to the s representative target body parts. Then, the sequence of input frames are
aligned with each model of dynamic gesture patterns by the dynamic time-warping (DTW)
algorithm. At last, the discriminatory state is extracted from the aligned sequence, and
the similarity of the input frames to a model dynamic gesture pattern is determined by
the discriminatory state of the model and the matching counterpart of the input frames.
In the following, the kernel deformable shape model, the nonlinear/non-Gaussian Bayesian
tracking, and the Lichtenauer approach, which are the cornerstones of the proposed ap-
proach, are reviewed and some necessary modifications or configurations of these algorithms
are also presented.
3.1 The kernel-based deformable shape model
The kernel-based deformable shape model [1] is a prototype-based deformable model. By
using the kernel method [46–48], this model incorporates the information about edge ori-
entations, which is valuable for shape detection [49], into the shape representation. This
scheme leads to a more robust and accurate deformable model than that only utilizing
6
the information about the edge position. In fact, this model results in a set of correlation
filters, and thus the deformable shape detection can be carried out efficiently and globally
based on the cross-correlation theorem.
Let xi, i = 1, 2, ..., n, be the edge points of a shape S, and Φ(x) is a nonlinear function
mapping x to a vector in a kernel feature space. Define the shape vector, sˆ, of Shape S as:
sˆ =
n∑
i=1
Φ(xi), (1)
which is a sum of the mapped vectors of the edge points in the kernel feature space. Let sˆ
′
denote the shape vector of the shape S
′
which consists of n
′
edge points. The dot product
of sˆ and sˆ
′
may be computed by
〈
sˆ, sˆ
′
〉
=
〈
n∑
i=1
Φ(xi),
n
′∑
j=1
Φ(x
′
j)
〉
=
n∑
i=1
n
′∑
j=1
〈
Φ(xi),Φ(x
′
j)
〉
=
n∑
i=1
n
′∑
j=1
k(xi,x
′
j), (2)
where the function k is the Gaussian kernel function k(x,y) = g(y1 − x1, y2 − x2) =
exp(−γ((y1 − x1)2 + (y2 − x2)2)) is chosen where x = [x1 x2]T and y = [y1 y2]T .
This shape representation has at least two advantages. First, it is capable of modeling
a shape consisting of closed or opened boundaries. Second, this shape representation is
invariant to the order of the edge points of the shape. Furthermore, the shape vector can
be extended to exploit edge orientations for shape detection as follows.
Denote by ρi and θi, i = 1, 2, ..., n, the edge magnitudes and edge orientations of the
edge points, xi, i = 1, 2, ..., n, of Shape S. The information about edge orientations and
magnitudes can be incorporated into the shape representation in the following manner.
Let sˆp, p = 0, ...,P − 1, denote the P vectors for Shape S, where sˆp is defined as a weighted
sum of the mapped vectors of the edge points with edge orientations at most ±180◦P away
from 180
◦
P × p:
sˆp =
n∑
i=1
ϕ(ρi)ψ
p(θi)Φ(xi), (3)
where ϕ(ρ) is defined as
ϕ(ρ) =

0 if ρ < α1
ρ−α1
α2−α1 if α1 ≤ ρ < α2
1 if ρ ≥ α2
with α1 and α2 a pre-specified thresholds and ψ
p(θ), p = 0, 1, ...,P − 1, are defined as
ψp(θ) =

|mod(θ,180)− 180pP |
180/P if |mod(θ, 180)− 180pP | < 180P
|mod(θ,180)−180|
180/P if p = 0 and |mod(θ, 180)− 180| < 180P
0 elsewhere.
8
expected fitness measure between yt and the target body part at state s by
Q(yt|s) = Ext|y1:T [h(yt|s,xt)] =
∫
xt
h(yt|s,xt)p(xt|y1:T )dxt.
3.2.1 The particle filtering
The particle filter uses the principle of importance sampling to accurately approximate a
distribution with a finite number of samples. The particles are drawn from a distribution
pi, which is the so-called importance function, and have assigned weights. The sequential
important sampling (SIS) method approximates the posterior filtered density p(xt|y1:t) by
p(xt|y1:t) ≈
N∑
i=1
w
(i)
t δ(xt − x(i)t ), (5)
where the weight update rule is
w
(i)
t ∝ w(i)t−1
p(yt|x(i)t )p(x(i)t |x(i)t−1)
pi(x
(i)
t |x(i)t−1,yt)
,
and w(i) is normalized such that
∑N
i=1w
(i) = 1, with pi(x
(i)
t |x(i)t−1,yt) the importance func-
tion. It can be shown that as N →∞, the approximation (5) approaches the true posterior
density [7]. In addition, the important function results in the minimum variance of the
weights has been shown to be p(xt|x(i)t−1,yt), and thus the weight update rule is
w
(i)
t ∝ w(i)t−1p(yt|x(i)t−1)
= w
(i)
t−1
∫
p(yt|x′t)p(x′t|x(i)t−1)dx′t.
Furthermore, it is common to choose the importance function as pi(xt|x(i)t−1,yt) = p(xt|x(i)t−1),
and then the weight update rule becomes
w
(i)
t ∝ w(i)t−1p(yt|x(i)t ).
Here, the state transition probability p(xt+1|xt) may be defined as:
p(xt+1|xt) = 1
σ
√
2pi
exp(−‖xt+1 − xt‖
2
2
2σ2
)
based the locality constraint, and the conditional probability p(yt|xt) may be defined in
terms of the magnitude of the correlation map generated by the kernel-based deformable
shape model as follows:
p(yt|xt) = exp(λCt(xt))∑
x exp(λCt(x))
,
where Ct denotes the correlation map at time t.
10
where
Q˜(yt|s = i) = Q(yt|s = i)p(s = i)∑c
j=1Q(yt|s = j)p(s = j)
.
Thus, we have f1, . . . , fT for an input sequence of T frames. In the training phase, we also
establish such a representation for each ofM model dynamic gestures asm1;i, . . . ,mTi;i, i =
1, . . . ,M . The proposed approach uses the DTW algorithm to match the input frames
against every dynamic gesture model, and the input frames are identified as the model
with the highest matching score. To align f1, . . . , fT against model i, the DTW formula is
defined as follows.
W (f1, . . . , fT ;m1;i, . . . ,mTi;i) = ‖mTi;i − fT‖2 +min
W (f1, . . . , fT−1;m1;i, . . . ,mTi;i);W (f1, . . . , fT−1;m1;i, . . . ,mTi−1;i).
The local constraint on the mapping path is defined in such a way that every key frame
should have at leat one matching input frame, and every input frame should have one
and only one matching key frame. After aligning against model i, the best mapping
path is then traced out and the input sequence having aligned with model i, denoted by
f¯j;i, j = 1, . . . , Ti, can be obtained by
f¯j;i = the average of fk such that fk and mj;i are matching.
3.4 The Lichtenauer approach
Development of the proposed system is also based on the approach of Lichtenauer et al. [12],
which suggests that time warping and classification should be separated for recognizing
dynamic human gestures. Figure 3 shows a system diagram of the Lichtenauer approach.
Their approach obtains M aligned gesture patterns by aligning the captured dynamic
gesture pattern against M models by time warping, where M is the number of gesture
classes. Since the M aligned gesture patterns may have different dimensions, they used
quadratic classifiers to classify the captured motion into the class with the maximum
likelihood.
In this work, we have found that the matching score of the DTW algorithm may not be
suitable for classification because dynamic gesture patterns of different classes may share
many common states. Here, we focus on the discriminatory state which is common to the
dynamic gesture pattern of a class, but rare to the gesture pattern of the other classes.
Specifically, in the training phase, the discriminability of a state o with respect to class i
is defined as follows:
# of occurrences of state o in the training dynamic patterns of class i
# of occurrences of state o in all training dynamic gesture patterns
.
The similarity of the input frames to a model gesture pattern is calculated by the cosine
between the s-vectors corresponding to the discriminatory state of the model and that
12
Figure 4: The first model of dynamic hand gestures, where the numbers shown in every
frame are the frame number and the state number, and the frame with a red frame rectangle
is the discriminatory state.
14
Figure 6: The third of dynamic hand gestures.
Figure 7: A test sequence.
16
Figure 10: The estimated posteriors with the particles for the test sequence, where the
numbers shown in every frame are the frame number and the state mapping to model 3.
the alignment and recognition for recognizing dynamic gesture pattern has been shown
promising. In the future, a larger scale of experiments should be conducted. Meanwhile,
automatically gesture spotting is also worth of investigation.
18
[13] S. Mitra and T. Acharya, “Gesture recognition: A survey,” IEEE Trans. on Systems,
Man, and Cybernetics-Part C: Applications and Reviews, vol. 37, no. 3, pp. 311–324,
2007.
[14] T. Moeslund and E. Granum, “A survey of computer vision-based human motion
capture,” Computer Vision and Image Understanding, vol. 81, pp. 231–268, 2001.
[15] T. B. Moeslund, A. Hilton, and V. Kruger, “A survey of advances in vision-based
human motion capture and analysis,” Computer Vision and Image Understanding,
vol. 104, pp. 90–126, 2006.
[16] R. Poppe, “Vision-based human motion analysis: An overview,” Computer Vision
and Image Understanding, vol. 108, pp. 4–18, 2007.
[17] A. Erol, G. Bebis, M. Nicolescu, R. D. Boyle, and X. Twombly, “Vision-based hand
pose estimation: A review,” Computer Vision and Image Understanding, vol. 108,
pp. 52–73, 2007.
[18] D. M. Gavrila, “The visual analysis of human movement: A survey,” Computer Vision
and Image Understanding, vol. 73, pp. 82–89, Jan. 1999.
[19] A. Jaimes and N. Sebe, “Multimodal human-computer interaction: A survey,” Com-
puter Vision and Image Understanding, vol. 108, pp. 116–134, 2007.
[20] J. K. Aggarwal and Q. Cai, “Human motion analysis: A review,” Computer Vision
and Image Understanding, vol. 73, no. 3, pp. 428–440, 1999.
[21] V. I. Pavlovic, R. Sharma, and T. S. Huang, “Visual interpretation of hand gestures
for human-computer interation: a review,” IEEE Trans. on Patt. Anal. and Machine
Intell., vol. 19, no. 7, pp. 677–695, 1997.
[22] L. Wang, W. Hu, and T. Tan, “Recent developments in human motion analysis,”
Pattern Recognition, vol. 36, pp. 585–601, 2003.
[23] M. Isard and A. Blake, “CONDENSATION-conditional density propagation for visual
tracking,” International Journal of Computer Vision, pp. 5–28, 1998.
[24] D. Ramanan, D. A. Forsyth, and A. Zisserman, “Tracking people by learning their
appearance,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 29,
no. 1, pp. 65–81, 2007.
[25] M. K. Bhuyan, P. K. Bora, and D. Ghosh, “Trajectory guided recognition of hand ges-
tures having only global motion,” International Journal of Computer Science, vol. 3,
pp. 222–233, 2008.
20
[38] P. Dreuw, T. Deselaers, D. Rybach, D. Keysers, and H. Ney, “Tracking using dynamic
programming for appearance-based sign language recognition,” in Proceedings of the
7th international Conference on Automatic Face and Gesture Recognition (FGR’06),
pp. 293–298, 2006.
[39] D. Kim, J. Song, and D. Kim, “Simultaneous gesture segmentation and recogni-
tion based on forward spotting accumulative HMMs,” Pattern Recognition, vol. 40,
pp. 3012–3026, 2007.
[40] M. S. Yang and N. Ahuja, “Recognizing hand gesture using motion trajectories,” in
Proceeding of IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 466–
472, 1999.
[41] C. Kwok, D. Fox, and M. Meila, “Real-time particle filters,” Proceedings of the IEEE,
vol. 92, no. 3, pp. 469–484, 2004.
[42] M. J. Black and D. D. Jepson, “A probabilistic framework for matching temporal tra-
jectories: Condensation-based recognition of gestures and expressions,” in Proceedings
of 5th Eurpean Conference on Computer Vision, vol. 1, pp. 909–924, 1998.
[43] M. Isard and A. Blake, “A mixed-state CONDENSATION tracker with automatic
model-switching,” in Proceedings of 6th International Conference on Computer Vision,
(Mumbai, India), pp. 107–112, 1998.
[44] M. Yeasin and S. Chaudhuri, “Visual understanding of dynamic hand gestures,” Pat-
tern Recognition, vol. 33, pp. 1805–1817, 2000.
[45] J. Alon, V. Athitsos, Q. Yuan, and S. Sclaroff, “Simultaneous localization and recog-
nition of dynamic hand gestures,” in Proceedings of the IEEE Workshop on Motion
and Video Computing (WACV/MOTION’05), 2005.
[46] O. Barzilay and V. L. Brailovsky, “On domain knowledge and feature selection using
a support vector machine,” Pattern Recognition Letters, vol. 20, pp. 475–484, 1999.
[47] V. L. Brailovsky, O. Barzilay, and R. Shahave, “On global, local, mixed and neigh-
borhood kernels for support vector machines,” Pattern Recognition Letters, vol. 20,
pp. 1183–1190, 1999.
[48] B. Scho¨lkopf and A. J. Smola, Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. Cambridge, Massachusetts: The MIT
Press, 2002.
[49] D. H. Ballard, “Generalizing the Hough transform to detect arbitrary shapes,” Pattern
Recognition, vol. 13, no. 2, pp. 111–122, 1981.
22
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
