1 
???????????????????? 
???????????????????????-- 
??????????????????? 
Rapid Job Definition and Autonomous Execution Systems for 
Next Generation Service Robots – Project 5: Localization,  
Navigation, and Dispatch of a Service Robot 
? ? ? ??NSC 99-2221-E-011-100 
? ? ? ??99? 08? 01?? 100? 07? 31? 
? ? ?????   ????????????? 
?????????   ????????????? 
??????????  ?????????????,  
???  ????????????? 
 
?????? 
??????????????????
????????????????????
???????????????????
????????????????????
???????????????????
????????????????????
????????????????????
????????????????????
????????????????????
??????????????????
????????????????????
????????????????????
??????????????????? 
?????????????????
????????????????????
???????????????????
????????????????RGV??
?????????AGV?????????
????????????????????
???????????????????
????????????????????
????????????????????
???????????????????? 
?????????????????
????????????????????
????????????????????
?? CCD???????????????
????????????????????
??????CCD??????? CCD??
????????????????????
???????????????????
????????????????????
????????????multi-layer per-
ception, MLP????? CCD???????
???????????????????
??????????????? 320 × 240 
????????????????????
?????????????? 1000 mm?
3 
????????????????????
??????????? 
?????????????? 
?????????????????
????????????????????
???????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
???????????? 
3.1??????? 
 ??????????????????
????????????????????
????????????????????
??????????? Nuttall[12]????
????????????????????
?????????????????(a)?
????(b)??????????? 
 
    (a) 
 
    (b) 
???(a)Nuttall????[12]?(b)Nuttall??
????? 
 ?????????????????
????????????????????
? N5? N9???????????????
????????????????????
????????????????????
???????????????????
?? 
 
???????????? 
item specification
width of south pointing chariot 120 mm 
diameter of wheel 60 mm 
 
????????? 
number of gear 
(Ni) 
module pitch di-
ameter  
number of 
teeth 
i = 1?2?3?7?
8?9 
0.8 16 20 
i = 4?6 1 20 20 
i = 5 1 40 40 
 
 
??????????? 
  
 ??????????????????
??? 0.25 ??????? AB ?????
???Robot-Electronics?????????
? CMPS09 ??????????????
???????? 
3.2????????? 
 ??????????????????
????????????????????
????????????????????
???????????????? 
5 
 
 
(a)             (b) 
?? (a) AutoCAD??????(b)???
?? 
 ???????????? 1.3????
???????????????????
???????????????????? 
3.3.2 ?????????? 
 ?????????????????
???????????????????
????????????????????
????????????????????
????????????????????
????????????????????
?????????????? 1 m????
???????????????????
????????? 4 m?????????
????????????????????
?????? 
   
(a)             (b) 
???(a)???????(b)???????
??? 
   
(a)               (b) 
???(a)???????(b)?????? 
 ?????????????????
????????????????????
????????????????????
????????????????????
????? 
 
????????????(??????) 
 
????????????(??????) 
 
 ??????????????????
????????????????????
????????????????????
?????? 6?????????????
??????????? 
?????????????????
????????????????????
??????? 
 
????????(????) 
7 
 
(a) 
 
(b) 
????(a)Z???????(????)?(b)Z
???????(????) 
 
 ??????????????????
????????????????????
????????????????????
???(b)???????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????? Z ?????
??????????????????
????????????????????
????????????????????
????????????????????
???? 1???????????????
????? 1??????????????
?? 0??? 1?????????????
????????????????????
????????????????????
????????????????????
???????? 
 
????????????????????
??? 
????????????? 
4.1???? 
  ?????????????????
????????????????????
?? 30 FPS???? 320 ? 240?????
??? 80 mm??????????????
???phase space motion capture??????
P*???????????????????
????????????????????
1??????? P*?????? 1000??
???????????? 
??????
??CCD???
?????
??CCD???
?????
MLP
?????
????
????
???
????Z?????
 
??????????????? 
 
9 
 ??????? P*?????????
? *OP
uuuuv
???????????? x, y, z??
??????????? 
from motion capture
from stereo 
visual system
O (x, y, z)
P1 (x, y, z)
P2 (x, y, z)
P* (x, y, z) * 1000
(unit mm)
Camera 1 (px1, py1)
Camera 2 (px2, py2)
(unit pixel)
the coordinate 
of visual system
the scalar project of 
OP* onto X, Y and Z
* 1000
px1
py1
px2
py2
ANN
BPN
X’
Y’
Z’
??????????O, P1, 
P2?P*???????
??P*??????
????
??P*??????
????
?????????
???????? ??????
 
?????????? 
 ???? 1000?????? 600???
???????????400???????
??? 
4.3??????? 
???????????MATLAB?
Neural Network Toolbox?????????
???????????? nntool?????
????????????????????
????????????????????
????????????????????
???????????????????
????????????????????
????????????????????
?????????????????? 
 
???????????? nntool?????
? 
??????????????????
????????????????????
????????????????????
??????????????????
????????????????????
????????????????????
???????????MSE??????
????????????????????
???????? TRAIN????????
?? TRAIN??????????????
????????? 
????
????
????
???
????
???????????? 
??????????? 
??????????????????
?????????????? TRAINGDA
? TRAINRP?????????????
????????????????????
????? TRAINGD??????????
TRAINGDA???????????????
TRAINGDM?????????? ????
?????? TRAINCGF?Fletcher-Reeves
???????TRAINCGP?Polak-Ribiére ?
??????TRAINCGB?Powell-Beale ??
?????TRAINSCG?Scaled???????
?????? TRAINBFG? BFGS qua-
si-Newton ? ? ? ? ? TRAINCLM
?Levenberg-Marquardt????? 
4.4?????????????? 
????????? 200 mm??? 200 
11 
 
????????????? 
??????????????????
????????????????????
????????????????????
????? 
 
????????????????????
?? 
 
???????????????????? 
 
?????????????????? 
  
???? 
??????????????????
???????????????????
????????????????????
????????????????????
????????????????????
????????????????????
????????????????????
???????????????????
????????????????????
???????????? 4 m??????
????????? 3??????????
?????? 70 mm???????????
?????????????? 590 mm??
???????????????????
?? 
??????????????????
???????????????????
????????????????????
???????????????????
????????????????????
????????????????????
???????????????????
?????????????? 
?????????????????
?????????????? 200 mm??
? 200 mm?1000 mm?????????
????????????????????
  
  
Abstract—The objective of this research was twofold. First 
was to develop an orientation sensor to help the service robot 
platform detect the change of its orientation. An innovative idea 
of using the old south-pointing chariot on the platform was 
realized. This sensor based on the south-pointing chariot is cost 
effective, and free from the electromagnetic interference. The 
experimental results showed that the sensor can be employed to 
detect the change of the orientation of the mobile platform. The 
information provided by the south-pointing chariot can be used 
to control the platform to move along a straight line or in any 
desired pattern. 
The second objective was to implement a stereoscopic vision 
based positioning system. The vision positioning system is 
composed of two web cameras. These two cameras are not 
necessary to be placed in parallel. Their orientations can be 
arranged according to practical considerations. In order to deal 
with the unknown orientation relationships of the two cameras, a 
supervised neural network model was constructed to identify the 
spatial coordinates relative to the camera system. The camera 
resolution is 320 × 240 pixels with 60 frames per second. With 
this resolution, the positioning error of a spatial marker is within 
25 mm in a 200 mm measurement range and 80 mm in a 1000 
mm measurement range. The vision positioning system can guide 
the robot to approach the target from a long distance. The closer 
the robot moves to the target, the less error the vision system will 
have. So eventually the robot with the vision positioning system 
can navigate to the target accurately. 
I. INTRODUCTION 
OUTH-POINTING chariot is a differential wheel drive 
whose shafts connect to differential gear sets. By properly 
designing the diameter of the two wheels, the distance 
between them, and the gear ratios among the gear sets, the 
direction of an arrow or any similar direction indicator 
connected to one of the gears remains fixed all the time as long 
as there is no slippage between the wheels and the ground. 
South-pointing chariot has a history of more than one 
thousand years. According to the record, it has been used in 
                                                          
Manuscript received September 13, 2011. This work was supported in 
part by the National Science Council under Grant NSC 99-2221-E-011-100.  
W. C. Lee is with the Department of Mechanical Engineering, National 
Taiwan University of Science and Technology, Taipei 10607, Taiwan, ROC 
(phone: +886-2-2737-6478; fax: +886-2-2737-6460; e-mail: wclee@ 
mail.ntust.edu.tw).  
H. C. Chou, is with the Department of Electrical Engineering, National 
Taiwan University of Science and Technology, Taipei, 10607, Taiwan, ROC  
(e-mail: M9807306@mail.ntust.edu.tw). 
C. H. Kuo, is with the Department of Electrical Engineering, National 
Taiwan University of Science and Technology, Taipei, 10607, Taiwan, ROC  
(e-mail: chkuo@mail.ntust.edu.tw). 
C. W. Cai is with the Department of Mechanical Engineering, National 
Taiwan University of Science and Technology, Taipei 10607, Taiwan, ROC 
(e-mail: M9803118@ mail.ntust.edu.tw). 
battles for direction-guiding long before the compass was 
invented. Since the appearance of the compasses, 
south-pointing chariots have been employed mostly for 
education and recreation purposes. For example, Hsieh et al. 
[1] developed eight different south-pointing chariots by using 
planetary gears. Yan et al. [2] also analyzed the mechanisms 
of various south-pointing chariots by using topology.  
In the development of indoor mobile robot platform, most 
people use electronic compasses or gyroscopes as the 
embedded orientation sensors. However, electronic 
compasses are easily disturbed by the ferromagnetic materials 
such as iron and steel, which are commonly used in the 
household environment. Gyroscopes generate drift errors and 
accumulated errors. To resolve these issues, we came up with 
the idea by taking advantages of the south-pointing chariot 
and combining it with an optical encoder to build an 
embedded orientation sensor. To explain it in more detail, if 
the south-pointing chariot moves forward, the angular change 
of the direction indicator relative to the south-point chariot is 
zero. If the south-pointing chariot turns left by x degrees, then 
the direction indicator will rotate –x degrees relative to the 
south-pointing chariot. So when a person standing on the 
ground, he will see the direction indicator remain fixed.  We 
simply replace the direction indicator on a south-point chariot 
by an optical encoder. The optical encoder will record the 
angular change of the direction indicator relative to the 
south-pointing chariot. When it records 0 degrees, we know 
that the chariot is moving forward. When it records y degrees, 
we know the chariot is rotating –y degrees. Then we can apply 
this special south-pointing chariot to the robot platform of the 
differential drive as an orientation sensor for localization. 
Some people may argue that by installing two optical 
encoders in the two wheels of the differential drive can 
achieve the same purpose. However, our idea to use the 
south-pointing chariot requires only one optical encoder and 
several plastic gears and metal shafts, which has the cost 
advantage compared to the use of two optical encoders. 
Therefore, it is worthwhile to perform research on this topic, 
which is one of the two topics that we will be studying in this 
research. Another topic that we would like to investigate is the 
visual positioning system. By combining the orientation 
indicator and the visual position system gives an indoor robot 
better capability to move to the target without the help of any 
foreign beacons or infrared emitters. 
Visual positioning system [3] – [5] are new trends for 
developing positioning and localization systems. The 
positioning system can help autonomous robots to localize the 
Localization and Stereoscopic Vision Positioning Systems for a 
Service Robot Platform 
Wei-chen Lee, Hung-Chyun Chou, Chung-Hsien Kuo, and Cong-Wei Cai  
S 
  
The Features of Left-Hand -
Side (LHS) Web Camera
The Features of Right-Hand-
Side-Web (RHS) Camera
Neural Network 
Model of Stereo 
Vision System
Depth of Each 
Feature?Z?
Training Patterns from
Motion Capture 
Systems
 
Fig. 4.  Neural network based stereoscopic vision system architecture. 
 
P*
P1
P2
OStereoscopic 
Vision Module
Motion Capture 
System
 
Fig. 5.  Experimental setups for training stages. 
 
The coordinate system of the stereoscopic vision module is 
defined as shown in (1). The scalar projection of *
uuuuv
OP  onto 
the coordinate system of the stereoscopic vision module is 
indicated in (2). In this manner, the spatial coordinates (x, y, z) 
represent the position of point P* in the coordinate system of 
the stereoscopic vision module. It is noted that the global 
coordinate system of the motion capture system [10] is not 
directly used in the neural network model. 
     1 2 1 2
1 2 1 2
, ,
×= = = ×
uuuv uuuuv uuuv uuuuvv v v
uuuv uuuuv uuuv uuuuvOP OP OP OPX Y Z
OP OP OP OP
 (1) 
      * * *, , .= ⋅ = ⋅ = ⋅uuuuv uuuuv uuuuvv v vx OP X y OP Y z OP Z  (2) 
The backpropagation neural network (BPN) is used to 
establish the nonlinear parametric relationships between the 
inputs (cameras’ coordinates) and outputs (spatial coordinates 
relative to the coordinate system of the stereoscopic vision 
module). The BPN is a supervised learning algorithm. It is 
popularly used in various neural network applications. In this 
paper, the BPN is formed as different layers, including input 
layer, hidden layers and output layer. A typical BPN structure 
is shown in Fig. 6. 
h
Inputs First layer Second layer N-th layer Outputs
Input layer Hidden layer Output layer
 
Fig. 6.  Architecture of a typical BPN structure. 
 
The input of the neural network is the pixels of the target P* 
in both cameras’ coordinate systems and the output is the 
scalar projection of *
uuuuv
OP  in the vision coordinate system, 
which can be represented by  x, y and z. The experimental flow 
chart is shown in Fig. 7.  
From motion capture
From stereoscopic 
vision module
O (x, y, z)
P1 (x, y, z)
P2 (x, y, z)
P* (x, y, z) * 1000
(unit mm)
Camera 1 (px1, py1)
Camera 2 (px2, py2)
(unit pixel)
Relative coordinate 
of vision system
the scalar project of 
OP* onto X, Y and Z
* 1000
px1
py1
px2
py2
BPN
X’
Y’
Z’
 
Fig. 7.  Experimental flow chart. 
IV. EXPERIMENTAL RESULTS AND DISCUSSION 
A. Orientation Sensor 
For testing the performance of the orientation sensor we 
developed in this research, we built a robot platform and 
installed the south-pointing chariot underneath the platform 
as shown in Fig. 8. Two toy servo motors were used to drive 
the two wheels of the differential drive. The wheels of the 
south-pointing chariot and the wheels of the differential 
touch the ground simultaneously. 
 
Fig. 8.  Robot platform with the orientation sensor. 
  
network architecture is shown in Fig. 13. As a consequence, 
the regressions of training, validation and testing of the BPN 
model are shown in Figs. 14 – 15. The BPN was also validated 
in terms of the testing data. Fig. 16 shows the test result of the 
mean squared error (MSE) of the near distance data. With the 
320 × 240 pixels resolution, the positioning error of a spatial 
marker is within 25 mm in a 200 mm measurement range.  
 
Fig. 12.  Robot platform with two web cameras. 
 
TABLE I 
PROPERTIES OF DIFFERENT LAYERS 
Layer Number of Neurons Transfer Function 
1 5 TANSIG 
2 5 PURELIN 
3 3 PURELIN 
 
 
Fig. 13.  BPN model for near distances. 
 
 
Fig. 14.  Mean squared error of training, validation and testing.  
With the far distance set, the neural network model uses the 
feed-forward backpropagation. We still used the same training 
function and the adaption learning function. The number of 
layers is also 3, and they are described as shown in TABLE II. 
The neural network architecture is the same as Fig. 13. As a 
consequence, the regressions of training, validation and 
testing of the BPN model are shown in Figs. 17 – 18. The BPN 
was also validated in terms of the testing data. Fig. 19 shows 
the test result of the MSE of the far distance data. With the 
same pixels resolution, the positioning error of a spatial 
marker is within 80 mm in a 1000 mm measurement range.  
 
 
Fig. 15.  Regressions of training, validation and testing.  
 
 
Fig. 16.  Neural network model testing result. 
 
 
TABLE II 
PROPERTIES OF DIFFERENT LAYERS 
Layer Number of Neurons Transfer Function 
1 10 TANSIG 
2 10 PURELIN 
3 5 PURELIN 
 1
????????????????????????? 
                                    ???99? 10? 20? 
???????? 
  ???? 2010 IEEE International Conference on Systems, Man, and Cybernetics(??
?? SMC2010)??????????????????????(IEEE System, Man, 
& Cybernetics Society)???? Bogaziçi University??????????????
????????????????????????????????????
????????????????????????????????????
????????????????????????????????????
????????????????????????????????????
?????????????? 
???? NSC 99 - 2221 - E - 011 - 100 - 
???? ??????????????????????????????
????????????? 
????
?? ??? 
????
??? ????????????? 
???? 
 99? 10? 10?
? 
 99? 10? 13? 
???? ??????? 
???? 
(??)2010 IEEE??????????????? 
(??)2010 IEEE International Conference on Systems, Man, and 
Cybernetics (SMC2010) 
????
?? 
(??)???????? 
(??) A Novel Design of a Prosthetic Hand 
 3
???????????????????????????? 
4. ???????????????????????????????????
???????????????????? 
5. ???????????????????????????????????
?????????????????? 
 
???????? 
  ??????????????????????????????????
?????????????????????????? - ?????????
?????????????????????????????????????
?????温??? 10-15?????????????????????????
????????????????????????????????????
????????????????????????????????????
?????????????????????????????? 
  ??????????????????????????????????
????????????????????????????????????
?????????? 1500????????????????????????
????????????????????????????????????
????????????????????????????????????
???????????????????????????????? 
SMC2010 notification.txt
寄件者: SMC2010 [smc2010_0@easychair.org]
寄件日期: 2010年5月26日星期三 上午 11:40
收件者: wclee@mail.ntust.edu.tw
主旨: SMC2010 notification
Dear Authors,
It is our pleasure to inform you that the paper referenced below, for w ich you
are 
listed as the corresponding and/or sole author, has been accepted for 
presentation at 
the 2010 IEEE Conference on Systems, Man and Cybernetics (SMC2010) to be held in
Istanbul, Turkey during October 10-13, 2010. It will also be published in the 
SMC2010 
conference CDproceedings and will subsequently appear on the IEEE Xplore. Please
review carefully this message as well the attachment (if any) sinc  it contains 
the 
important and relevant information regarding your paper and the conf rence.
The conference technical program consists, as announced, of regular- e sion 
paper (RS) 
papers and special-session (SS) papers in both lecture and poster s s io s 
following the 
tradition of SMC conferences. However, both RS and SS papers have b en subject
to the 
same reviewing procedure by several independent. Please observe eviewers' 
comments and the Editorial Board’s summary for your manuscript also comprising 
the 
PC/TC recommendation as well as reviewers’ comments and critics. The conference
policy requires that at least one author of a paper pay a full "member or 
non-member" 
registration fee before the final paper can be uploaded.
Acceptance of your paper is made with the understanding that at least on author
will 
attend the conference to present the paper. "No shows" cause severe disruption 
to 
session schedules hence are not acceptable.
Please follow the news on the website of the SMC2010 Conference, where the 
Technical 
Program will also be announced. The advanced program announcement is subject to 
change and hence the date and time of your presentation may also change.
Congratulations on your success to have a paper accepted for the prominent SMC 
conferences among about 900 submitted manuscripts.
                                                      SMC2010 Program
Committee
第 1 頁
                         
A Novel Design of a Prosthetic Hand 
 
Wei-chen Lee, Chih-Wei Wu 
Department of Mechanical Engineering 
National Taiwan University of Science and Technology 
Taipei, Taiwan, Republic of China 
wclee@mail.ntust.edu.tw 
 
 
Abstract—An innovative prosthetic hand prototype is 
presented in this paper. To increase the grasping stability, an 
underactuated finger is embedded in a traditional gripper. There 
are a total of three motors employed in the hand system: the 
gripper, the embedded finger, and the wrist. The preliminary 
performance shows that this prosthetic hand is capable of 
performing various daily jobs such as turning a key, pinching a 
card, grasping tubes of various sizes, etc. The maximum grasping 
force for the gripper is about 2 kg and that for the embedded 
finger is about 0.3 kg.  The closing time for both the gripper and 
the embedded finger is about 0.2 sec, which is relative fast among 
similar prosthetic hands. The performance of this innovative 
prosthetic hand prototype shows the idea of the embedded finger 
is promising and further improvements will continue. 
Keywords—prosthetic hand, gripper, embedded finger 
I.  INTRODUCTION 
With the advent of advanced personal safety equipment 
used in the war, the fatality rate reduces but the number of 
amputees increases in recent wars in Iraq and Afghanistan[1].  
Various diseases and accidents also result in many people 
losing their hands. For people without hands, the inconvenience 
in their daily life is obvious; to let them have a life of quality, 
prosthetic hands are indispensable.  
One of the most popular prosthetic hands is Touch Bionics’ 
iLimb[2], which was one of the TIME’s best inventions of 
2008. This hand uses five actuators to control its five fingers, 
and the control signals make use of users’ EMG signals. The 
grasping force of this hand is about 4.5 kg. Its underactuated 
finger-type design can have better stability of grasping round 
tubes or irregular-shaped objects. iLimb can also pinch a credit 
card as well as grasp a mug, so the dexterity of iLimb is 
obvious. Another similar prosthetic hand is DARPA’s 
Revolutionizing Prosthesis 2009 (RP 2009)[3]. From the 
limited literature open to public, it can be observed that the 
mechanical design looks similar to iLimb. Both of these hands 
use underactuated finger design. However, the control signals 
of RP 2009 use nerves, requiring an invasive operation to 
reroute the nerves. Noise interference may be reduced, and 
more complicated motion may be achieved.  
For the commercial prosthetic hand, Otto Bock has both the 
gripper-type and finger-type prosthetic hands.  These hands 
have one or two degrees of freedom, and can be easily learned 
to do uncomplicated jobs. They are also controlled by EMG 
signals. The speed for the hand to close can be as high as 300 
mm/sec and the maximum grasping force is about 10 kg.  
There are many other prosthetic hands that are developed 
by various research institutes. Iowa Hand[4] used springs to be 
its fingers. Cables are used to buckle the springs to control their 
motions. The structure is very simple, but the grasping force 
cannot be too large because it is related to the rigidity of the 
springs and the springs will not buckle if the rigidity is too 
large. Zhao et al.[5] developed a five-finger prosthetic hand by 
using three DC motors only. Each of the thumb and index 
finger are controlled by a motor, respectively, and the rest of 
the fingers are controlled by the third motor. The control 
signals are also from amputees’ EMG signals. The mechanism 
of the hand is complicated and the pinch force is only 0.5 kg. 
In addition to the traditional prosthetic hand actuated by 
motors, Kargov et al.[6] applied a small hydraulic pump to a 
prosthetic hand. The total weight of this hand is 353 g, and the 
maximum grasp force is 11 kg, which is quite large due to the 
hydraulic force. The time for the fingers to move from full 
open state to full close state is less than 1 sec. From its 
specifications, the performance of this hand is quite impressive. 
However, no experimental data was reported at that time. 
Based on the previous discussion and our other survey, Fig. 
1 shows the ways to actuate the fingers, which include DC 
motors with cables (or with cables and torsion springs), DC 
motors with belts, DC motors with linkages, and DC motors 
with spur gears. Other actuations include hydraulic motors and 
shape memory alloy. From a practical point of view, the use of 
DC motors with cables, belts and linkages are most popular. 
Usually, people design underactuated fingers by using DC 
motors with cables or belts and design the gripper by using DC 
motors with linkages. The underactuated finger can adjust 
according to the shape of the object to be grasped. However, 
due to the limitation of the rigidity of the underactuated finger, 
it usually cannot provide large grasping force. The traditional 
gripper usually has a large grasping force but lacks flexibility 
to deform which result in less grasping stability. Therefore, the 
objective of this research was to redesign a prosthetic hand so 
that it can have both the benefits of the underactuated finger 
and the traditional gripper. In the following sections, the 
mechanical design, the control scheme and the preliminary 
evaluation results of the prototype of this innovative prosthetic 
hand will be presented. 
This work was supported in part by the National Science Council, 
Republic of China, under Grant NSC98-2221-E-011-117.  
,(((
1821
  
 
 
                           (a)                                                             (b) 
Figure 5.  (a) top view of the gripper; (b) the corresponding mechanism of the 
gripper. 
Summing up the square of Eqs. (1) and (2) gives 
 
1 4 2 4 3cos sin 0,k k kθ θ+ + =
 
(3) 
where 
 
1 2 12 ( cos cos ),k c a dθ θ= − +
 
(4) 
 
2 2 12 ( sin sin ),k c a dθ θ= − +
 
(5) 
 
2 2 2 2
3 2 1 2 1( cos cos ) ( sin sin ) .k a d a d c bθ θ θ θ= + + + + −
 
(6) 
By using the tangent half angle formula and Eq. (3), we can 
obtain the following equation: 
 
( ) 2 4 43 1 2 1 3tan 2 tan 0.2 2k k k k k
θ θ⎛ ⎞ ⎛ ⎞
− + + + =⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠  (7) 
Thus we can solve Eq. (7) to obtain the relationship 
between the output angle θ4 and the input angle θ2 as Eq. (8). 
When the gripper is in its fully open position and fully closed 
position, the input angles are 140° and 183°, respectively. By 
using Eq. (8), we can plot the output angles when the input 
angle varies from 140° to183° as shown in Fig. 6, and find out 
that the output angles corresponding to the fully open position 
and fully closed position are 26° and 1°, respectively. The input 
angle is the angle of rotation of the longer finger of the grip and 
the output angle is the angle of rotation of the shorter finger. It 
turns out that during the whole movement of the gripper, the 
longer finger rotates 43° in total and the shorter finger rotates 
25° in total, which mimics human’s hand motion if we form a 
gripper by using index finger as the longer finger of the gripper 
and thumb as the shorter finger of the gripper. 
 
( ) ( )( )
( )
2
2 2 3 1 1 31
4
3 1
2 2 4
2 tan
2
k k k k k k
k k
θ −
⎛ ⎞
− − − − +⎜ ⎟
= ⎜ ⎟
−⎝ ⎠  
(8) 
0
5
10
15
20
25
30
140 145 150 155 160 165 170 175 180 185
input angle (degree)
ou
tp
ut
 a
ng
le
 (d
eg
re
e)
 
Figure 6.  The relationship between the input angle and the output angle of 
the gripper. 
 
IV. ACTUATOR 
The gripper, the embedded finger, and the wrist use one 
motor, respectively. The motors that are employed for the 
gripper and the wrist are ROBOTIS’ Dynamixel RX-64 DC 
servo motors, and the one employed for the embedded finger is 
Dynamixel RX-28 DC servo motors. There are several reasons 
for us to use ROBOTIS’ Dynamixel DC motors. First, the 
controller, the driver and the encoder are integrated into the 
motors to make it compact and easy to use. Second, it can 
provide large torque. For example, RX-28 and RX-64 can 
provide 28 kg-cm and 64 kg-cm maximum holding torque 
respectively. Their weight is another consideration.  RX-28 
weights only 72 g and RX-64 weights only 116 g. Here we did 
not use very expensive mini or micro motors simply because 
the emphasis of this paper is to present the idea of the 
embedded finger rather than build a prosthetic hand that are 
ready to be used on amputees. 
 
V. CONTROL SCHEME 
The control scheme of the prosthetic hand was designed in 
the way as shown in Fig. 7. Since the motor controller is built 
in each motor, we can simplify the control scheme by using a 
DSP board to interpret the EMG signals and directly send the 
commands to the motors to move the mechanism to the 
desired position. The DSP board is still under construction, so 
we directly controlled the motors during the performance tests. 
Pressure
Sensors on 
Gripper
Wrist
Driver
Wrist 
Motor
Wrist
Encoder
DSP Board
EMG control
Gripper
Driver
Gripper 
Motor 
Gripper 
Encoder
A/D
Finger
Driver
Finger 
Motor
Finger
Encoder
Wrist
Controller
Finger
Controller
Gripper
Controller
 
Figure 7.  The control scheme of the prosthetic hand 
1823
國科會補助計畫衍生研發成果推廣資料表
日期:2011/09/23
國科會補助計畫
計畫名稱: 子計畫五：服務型機器人之定位導航與派遣
計畫主持人: 李維楨
計畫編號: 99-2221-E-011-100- 學門領域: 自動化系統整合技術 
研發成果名稱
(中文) 應用差速齒輪組搭配編碼器感測移動平台旋轉角度
(英文) An orientation sensor based on the differential gear sets and an optical encoder
成果歸屬機構
國立臺灣科技大學 發明人
(創作人)
李維楨,林其禹,蔡淙偉
技術說明
(中文) 此方位感測器模組是應用差速齒輪組(類似使用在指南車上的齒輪組)傳動的原理
搭配一個編碼器製成，可裝載在平台之下做為驅動輪或從動輪，藉由左右兩輪所
輸入的角速度，經由傳動機構控制中間輸出桿件的旋轉角度，再由編碼器讀取輸
出桿件的旋轉角度。本專利以Nuttall型的指南車做為製作參考(但可使用任何的
指南車機構)，此指南車使用九顆傘齒輪做為角速度的傳動，其中六顆齒輪組成
兩組差速器。正中間的傘齒輪與編碼器經由連軸器連接。藉此由輪子轉動的角速
度，經由齒輪傳遞到正中間的傘齒輪，再經由連軸器傳動給編碼器得到指南車旋
轉的角度。 
(英文) The orientation sensor is based on the differential gear sets, similar to the ones used in 
traditional south-pointing chariot, and an optical encoder. The sensor can be installed 
under a robot platform as driving wheels or driven wheels. The optical encoder can read 
the angular difference between the left and the right wheels of the differential drive so as 
to know the orientation of the robot platform. 
產業別 電機及電子機械器材業；研究發展服務業
技術/產品應用範圍
清潔機器人, 自走式機器人平台 
技術移轉可行性及
預期效益
此專利實用性及技轉可行性高, 全球每年清潔機器人的市場約有數十億新台幣, 如蒙採
用其預期效益應很顯著.
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
