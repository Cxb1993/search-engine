\rightarrow 0$, $\sum_t \mu(t) = \infty$ and $\sum_t 
\mu(t)^2 < \infty$. Moreover, ${\mathbf w}(t)$ 
converges with probability one to the location where 
$\nabla_{\mathbf w} V({\mathbf w}) = {\mathbf 0}$. 
英文關鍵詞： Fault Tolearnt Learning, Noise, Convergence, 
Objective Functions 
 
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 2
MLPs with sigmoid output node. Three new analytical results
will be elucidated in this paper.
1) We show that if the step size fulfils certain mild
conditions, the convergence of the algorithm based on
combining weight noise injection (either multiplicative
or additive) with weight decay during training a MLP
with a linear output node2 is with probability one and
the weight vector converges to a local minimum of the
corresponding objective function.
2) The objective functions for the algorithms based on
combining weight noise injection (either multiplicative
or additive) with weight decay during training a MLP
with sigmoid output node is derived.
3) We show that if the step size fulfils certain mild
conditions, the convergence of the algorithm based on
combining weight noise injection (either multiplicative
or additive) with weight decay during training MLPs
with sigmoid output node is with probability one and the
weight vector generated by this algorithm converges to a
local minimum of the corresponding objective function.
C. Organization of the paper
The rest of the paper will present the main convergence
theorems and the corresponding proofs for these weight noise
injection-based algorithms for MLPs. In the next section, the
models of the MLP with linear and sigmoid output node will
be introduced. Their corresponding weight decay training algo-
rithms will be described. Then in Section III, the algorithms
MWN-WD and AWN-WD for MLP with linear or sigmoid
output node will be delineated. Their corresponding objective
functions are derived. In Section 4, the boundedness condition
for 퐸[∥w(푡)∥22] in MWN-WD algorithm and the boundedness
condition for 퐸[∥w(푡)∥22] in AWN-WD algorithm will be
proved, where w(푡) is the weight vector generated by the
algorithms. Based on the results obtained in Section IV,
the existence of lim푡→∞ ∥w(푡)∥22 is proved in Section V.
Moreover, we show in the same section that if 휇(푡) → 0,∑
푡 휇(푡) =∞ and
∑
푡 휇(푡)
2 <∞, lim푡→∞∇w푉 (w(푡)) = 0,
where 푉 (w(푡)) is the objective function. To illustrate the
convergence behaviors of the algorithms, we present in Sec-
tion VI a few simulations which are conducted based on a
nonlinear regression problem and the XOR problem. Finally,
the conclusion is presented in the last section.
II. BACKGROUND
We assume that the training data set 풟 = {(x푘, 푦푘)}푁푘=1 is
generated by an unknown system, where x푘 ∈ 푅푛 is the input
vector of the 푘푡ℎ sample data and 푦푘 ∈ 푅 is the corresponding
output.
A. MLP with linear output node
If this unknown system is approximated by a MLP with
푛 input nodes, 푚 hidden nodes, and one linear output node,
defined as follows3 :
푓(x푘,d,A, c) = d
푇 z(x푘,A, c), (1)
2One should be noted that the back-propagation algorithms for training
MLP with linear output node and with sigmoid output node are defined in
two different settings. The former one is based on the gradient descent of the
mean square error. While the latter is defined as the gradient descent of the
cross entropy error.
3For a MLP with output bias, the steps of analysis are essentially the same.
where A = [a1, ⋅ ⋅ ⋅ ,a푚] ∈ 푅푛×푚 is the input-to-hidden
weight matrix, a푖 ∈ 푅푛 is the input weight vector associated
with the 푖푡ℎ hidden node, c = (푐1, ⋅ ⋅ ⋅ , 푐푚)푇 ∈ 푅푚 is the
input-to-hidden bias vector, d ∈ 푅푚 is the hidden-to-output
weight vector, and z = (푧1, ⋅ ⋅ ⋅ , 푧푚)푇 ∈ 푅푚 is the output
vector of the hidden layer. Moreover, z is a vector function of
the input x푘, the weight matrix A and the bias vector c. The
푖푡ℎ element of z is defined as
푧푖(x푘,a푖, 푐푖) =
1
1 + exp(−(a푇푖 x푘 + 푐푖))
(2)
for 푖 = 1, 2, ⋅ ⋅ ⋅ ,푚.
For the sake of presentation, we let w푖 ∈ 푅(푛+2) be the
parametric vector associated to the 푖푡ℎ hidden node, i.e.
w푖 = (푑푖,a
푇
푖 , 푐푖)
푇 , (3)
and w ∈ 푅푚(푛+2) be a parametric vector augmenting all the
parametric vectors, i.e. w = [w푇1 ,w
푇
2 , ⋅ ⋅ ⋅ ,w푇푚]푇 . The output
is denoted as 푓(x푘,w). Throughout the paper, we call w1,
w2, ⋅ ⋅ ⋅ ,w푚 and w the weight vectors.
Next, we let g(x푘,w) be ∇w푓(x푘,w) and g푖(x푘,w푖) be
∇w푖푓(x푘,w). We get that
g(x푘,w) = (g1(x푘,w1)
푇 , ⋅ ⋅ ⋅ ,g푚(x푘,w푚)푇 )푇 ,
in which
g푖(x푘,w푖) =
[
푧푖(x푘,a푖, 푐푖)
푑푖푧푖(x푘,a푖, 푐푖)(1− 푧푖(x푘,a푖, 푐푖))x푘
푑푖푧푖(x푘,a푖, 푐푖)(1− 푧푖(x푘,a푖, 푐푖))
]
.
(4)
Furthermore, we let ∇wg(x,w) be ∇∇w푓(x,w) and
∇w푖g푖(x,w) be ∇∇w푖푓(x,w) for all 푖 = 1, ⋅ ⋅ ⋅ ,푚, it can
be showed that
∇wg(x,w) =
⎡⎢⎣ ∇w1g1(x,w1) ⋅ ⋅ ⋅ 0(푛+2)×(푛+2)... . . . ...
0(푛+2)×(푛+2) ⋅ ⋅ ⋅ ∇w푚g푚(x,w푚)
⎤⎥⎦ .
(5)
Applying online weight decay training, a sample is ran-
domly drawn from the dataset 풟 at each update step. We
denote the sample being selected at the 푡푡ℎ step as {x푡, 푦푡}.
Once the input x푡 has been fed into the MLP, the output is
calculated by (1) and (2) :
푓(x푡,w(푡)) = d(푡)
푇 z(푡) (6)
z(푡) = z(x푡,A(푡), c(푡)). (7)
By replacing w푖 and x푘 in (4) by w푖(푡) and x푡 respectively,
we have
g푖(x푡,w푖(푡)) =
[
푧푖(푡)
푑푖(푡)푧푖(푡)(1− 푧푖(푡))x푡
푑푖(푡)푧푖(푡)(1− 푧푖(푡))
]
. (8)
Here, we denote 푧푖(x푡,a푖(푡), 푐푖(푡)) by 푧푖(푡) owing to save
space.
The update equations for the weight vectors w푖 (for 푖 =
1, 2, ⋅ ⋅ ⋅ ,푚) can thus be written as follows :
w푖(푡+ 1) = w푖(푡) + 휇(푡) {(푦푡 − 푓(x푡,w(푡)))g푖(x푡,w푖(푡))
−훼w푖(푡)} , (9)
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 4
Taking derivative of (23) with respect to 푤휋푖 and then multiply
by (−1/2), we can get that
−1
2
∂
∂푤휋푖
ℰ푀 (x푡, 푦푡,w)
= (푦푡 − 푓(x푡,w)) ∂푓
∂푤휋푖
− 푆푏
푀푤∑
휋푗=1
∂푓
∂푤휋푗
∂2푓
∂푤휋푖∂푤휋푗
푤2휋푗
−푆푏
(
∂푓
∂푤휋푖
)2
푤휋푖 + 푆푏(푦푡 − 푓(x푡,w))
∂2푓
∂푤2휋푖
푤휋푖
+
푆푏
2
(푦푡 − 푓(x푡,w))
푀푤∑
휋푗=1
∂3푓
∂푤휋푖∂푤
2
휋푗
푤2휋푗
−푆푏
2
∂푓
∂푤휋푖
푀푤∑
휋푗=1
∂2푓
∂푤2휋푗
푤2휋푗 . (24)
On the other hand, we can take the expectation of (푦푡 −
푓(x푡, w˜))푔휋푖(x푡, w˜) over the random vector b and get that
퐸[(푦푡 − 푓(x푡, w˜))푔휋푖(x푡, w˜)∣w]
= (푦푡 − 푓(x푡,w)) ∂푓
∂푤휋푖
− 푆푏
푀푤∑
휋푗=1
∂푓
∂푤휋푗
∂2푓
∂푤휋푗∂푤휋푖
푤2휋푗
+
푆푏
2
(푦푡 − 푓(x푡,w))
푀푤∑
휋푗=1
∂3푓
∂푤2휋푗∂푤휋푖
푤2휋푗
−푆푏
2
∂푓
∂푤휋푖
푀푤∑
휋푗=1
∂2푓
∂푤2휋푗
푤2휋푗 . (25)
By the property that 푓(x푡,w) is differentiable to infinite order,
∂2푓
∂푤휋푖∂푤휋푗
=
∂2푓
∂푤휋푗∂푤휋푖
, (26)
∂3푓
∂푤휋푖∂푤
2
휋푗
=
∂3푓
∂푤2휋푗∂푤휋푖
. (27)
Thus, by comparing (24) and (25),
퐸[(푦푡 − 푓(x푡, w˜))푔휋푖(x푡, w˜)∣w]
= −1
2
∂
∂푤휋푖
ℰ푀 (x푡, 푦푡,w) + 푆푏
(
∂푓
∂푤휋푖
)2
푤휋푖
−푆푏(푦푡 − 푓(x푡,w)) ∂
2푓
∂푤2휋푖
푤휋푖
= −1
2
∂
∂푤휋푖
ℰ푀 (x푡, 푦푡,w) + 푆푏푤휋푖
2
∂2
∂푤2휋푖
(푦푡 − 푓(x푡,w))2.
(28)
In vector form,
퐸[(푦푡 − 푓(x푡, w˜))g푖(x푡, w˜)∣w]
= −1
2
∇w푖ℰ푀 (x푡, 푦푡,w)
+
푆푏
2
w푖 ⊗ diag{∇∇w푖(푦푡 − 푓(x푡,w))2}. (29)
By (16) and (29),
퐸[w푖(푡+ 1)∣w(푡)] = w푖(푡)− 휇(푡)∇w푖푉⊗(w(푡)), (30)
for all 푖 = 1, ⋅ ⋅ ⋅ ,푚, where5
푉⊗(w) =
1
2
{
1
푁
푁∑
푘=1
ℰ푀 (x푘, 푦푘,w) + 훼∥w∥22
− 푆푏
푁
푁∑
푘=1
∫
u(x푘,w)dw
}
. (31)
In (31),
u(x푘,w) = w ⊗ diag{∇∇w(푦푘 − 푓(x푘,w))2}. (32)
The last term in (31) is a line integral. It is clear that 푉⊗(w)
is differentiable up to infinite order.
While we have this new objective function, it is important
to check if the discussion in [21] about the weight magnitude
still holds true (see Section III.C in [21]). Let
ℒ(w) = 1
푁
푁∑
푘=1
ℰ푀 (x푘, 푦푘,w) + 훼∥w∥22, (33)
wˆ∗ and w∗ the local minima of ℒ(wˆ∗) and 푉⊗(w∗) respec-
tively. That is to say, ∇wℒ(wˆ∗) = 0 and ∇w푉⊗(w∗) = 0.
Considering the situation that 푆푏 is small, we can assume that
wˆ∗ and w∗ are close to each other. Therefore, we can get that
∇wℒ(wˆ∗) ≈ ∇wℒ(w∗) +∇∇wℒ(w∗)(wˆ∗ −w∗) (34)
By (31) and (33), we get that
∇wℒ(w∗) ≈ 2∇w푉⊗(w∗) + 푆푏
푁
푁∑
푘=1
u(x푘,w
∗). (35)
As ∇wℒ(wˆ∗) = 0 and ∇w푉⊗(w∗) = 0, we thus get that
푆푏
푁
푁∑
푘=1
u(x푘,w
∗) +∇∇wℒ(w∗)(wˆ∗ −w∗) ≈ 0.
In other word,
∇∇wℒ(w∗)wˆ∗ ≈ {∇∇wℒ(w∗)− 푆푏H(w∗)}w∗, (36)
where
H(w∗) = diag
{
∇∇w 1
푁
푁∑
푘=1
(푦푘 − 푓(x푘,w∗))2
}
. (37)
By rewriting (36), we can get that
wˆ∗ ≈ {퐼푀푤×푀푤 − 푆푏∇∇wℒ(w∗)−1H(w∗)}w∗. (38)
For 푆푏 is small, we can assume that ∇∇wℒ(w∗) and
∇∇w 1푁
∑푁
푘=1(푦푘 − 푓(x푘,w∗))2 are positive definite. The
diagonal elements of ∇∇w 1푁
∑푁
푘=1(푦푘− 푓(x푘,w∗))2 are all
positive. Hence by (37) and (38), ∥wˆ∗∥2 < ∥w∗∥2. Therefore,
the effect of the integral term in 푉⊗(푤) is to enlarge the
magnitude of the weight vector.
The discussion in [21] still holds true for the case that the
approximations of 푓(x푡, w˜) and g푖(x푡, w˜푖) are expanded up
to second order term.
5In this paper, we denote the objective functions for multiplicative weight
noise-based algorithms as 푉⊗(w) and 푉¯⊗(w). For additive weight noise-
based algorithms, we denote their objective functions as 푉⊕(w) and 푉¯⊕(w).
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 6
푔휋푗 (x푡, w˜(푡)) up to second order, we get that
퐸b[(푦푡 − 휙(x푡, w˜(푡)))푔휋푗 (x푡, w˜(푡))∣w(푡)]
= (푦푡 − 휙) ∂푓
∂푤휋푗
− 1
2
휙(1− 휙) ∂푓
∂푤휋푗
푀푤∑
휋푖=1
∂2푓
∂푤2휋푖
퐸[Δ푤2휋푖 ]
+
1
2
(푦푡 − 휙)
푀푤∑
휋푖=1
∂3푓
∂푤휋푗∂푤
2
휋푖
퐸[Δ푤2휋푖 ]
−1
2
휙(1− 휙)(1− 2휙) ∂푓
∂푤휋푗
푀푤∑
휋푖=1
(
∂푓
∂푤휋푖
)2
퐸[Δ푤2휋푖 ]
−휙(1− 휙)
푀푤∑
휋푖=1
∂푓
∂푤휋푗
∂2푓
∂푤휋푗∂푤휋푖
퐸[Δ푤2휋푖 ]. (51)
Comparing (50) and (51), we can get that
퐸b[(푦푡 − 휙(x푡, w˜(푡)))푔휋푗 (x푡, w˜(푡))∣w(푡)]
= − ∂
∂푤휋푗
ℰ퐶(x푡, 푦푡,w(푡))
−1
2
(푦푡 − 휙) ∂
2푓
∂푤2휋푗
∂
∂푤휋푗
퐸[Δ푤2휋푗 ]
+
1
2
휙(1− 휙)
(
∂푓
∂푤휋푗
)2
∂
∂푤휋푗
퐸[Δ푤2휋푗 ]. (52)
With (52), we are now able to get the mean update equations
and thus the corresponding objective functions for MWN-WD
and AWN-WD which are applied to train a MLP with sigmoid
output node.
Recall that 퐸[Δ푤휋푖Δ푤휋푗 ∣w] = 0 if 휋푖 ∕= 휋푗 . For multi-
plicative weight noise,
퐸[Δ푤휋푖Δ푤휋푗 ∣w] =
{
푆푏푤
2
휋푖 if 휋푖 = 휋푗 ,
0 if 휋푖 ∕= 휋푗 . (53)
For additive weight noise,
퐸[Δ푤휋푖Δ푤휋푗 ∣w] =
{
푆푏 if 휋푖 = 휋푗 ,
0 if 휋푖 ∕= 휋푗 . (54)
Similar to the case of linear output node, we denote
ℰ퐶푀 (x푡, 푦푡,w) as the expected cross entropy error if the noise
is multiplicative and ℰ퐶퐴(x푡, 푦푡,w) as the expected cross
entropy error if the noise is additive.
1) MWN-WD Algorithm: By (52) and (53), the mean update
equation for multiplicative noise is given by
퐸[푤휋푗 (푡+ 1)∣w(푡)]− 푤휋푗 (푡)
= −휇(푡)
{
∂
∂푤휋푗
1
푁
푁∑
푘=1
ℰ퐶푀 (x푘, 푦푘,w) + 훼푤휋푗 (푡)
+
푆푏
푁
푁∑
푘=1
(푦푘 − 휙(x푘,w(푡))) ∂
2푓
∂푤2휋푗
푤휋푗 (푡)
− 푆푏
푁
푁∑
푘=1
휙′(x푘,w(푡))
(
∂푓
∂푤휋푗
)2
푤휋푗 (푡)
}
, (55)
where 휙′(x푘,w(푡)) = 휙(x푘,w(푡))(1− 휙(x푘,w(푡))). As
∂2
∂푤2휋푗
풞(x푘, 푦푘,w(푡)) = −(푦푘 − 휙(x푘,w(푡))) ∂
2푓
∂푤2휋푗
푤휋푗 (푡)
+휙′(x푘,w(푡))
(
∂푓
∂푤휋푗
)2
푤휋푗 (푡),
퐸[푤휋푗 (푡+ 1)∣w(푡)]− 푤휋푗 (푡)
= −휇(푡)
{
∂
∂푤휋푗
1
푁
푁∑
푘=1
ℰ퐶푀 (x푘, 푦푘,w(푡)) + 훼푤휋푗 (푡)
− 푆푏푤휋푗 (푡)
∂2
∂푤2휋푗
1
푁
푁∑
푘=1
풞(x푘, 푦푘,w(푡))
}
. (56)
Hence, the objective function is given by
푉¯⊗(w) =
1
푁
푁∑
푘=1
ℰ퐶푀 (x푘, 푦푘,w) + 훼
2
∥w∥22
−푆푏
푁
∫
u1(x푘,w)dw, (57)
where
u1(x푘,w) = w ⊗ diag{∇∇w풞(x푘, 푦푘,w)}. (58)
It is clear that 푉¯⊗(w) is differentiable up to infinite order.
2) AWN-WD Algorithm: By (52) and (54), the mean update
equation for additive noise is given by
퐸[푤휋푗 (푡+ 1)∣w(푡)]− 푤휋푗 (푡)
= −휇(푡)
{
∂
∂푤휋푗
1
푁
푁∑
푘=1
ℰ퐶퐴(x푘, 푦푘,w) + 훼푤휋푗 (푡)
}
.(59)
Hence, the objective function is given by
푉¯⊕(w) =
1
푁
푁∑
푘=1
ℰ퐶퐴(x푘, 푦푘,w) + 훼
2
∥w∥22. (60)
Note that 푉¯⊕(w) is differentiable up to infinite order.
IV. BOUNDEDNESS OF 퐸[∥w(푡)∥22]
Before proceed to the convergence analysis, we need to
show that 퐸[∥w(푡)∥22] < ∞ for all 푡. This boundedness
condition is the key to prove that w(푡) converges to a local
minimum of the corresponding objective function. Both the
cases that the MLP with linear and sigmoid output node will
be analyzed in this section. We accomplish the proof in the
following steps.
S1: We show that 퐸[∥d(푡)∥22] and 퐸[∥d(푡)∥42] are bounded.
S2: We use the result in S1 to show that 퐸[∥a푖(푡)∥22] for all
푖 = 1, ⋅ ⋅ ⋅ ,푚 and 퐸[∥c(푡)∥22] are bounded.
S3: By the results in S1 and S2, we conclude that 퐸[∥w(푡)∥22]
is bounded and imply that 퐸[∥w˜(푡)∥22] is bounded.
Here, we assume that 휇(푡) → 0,∀푡 ≥ 0. This condition
is sufficient for boundedness analysis but not for convergence
analysis.
Here and after, we let bd(푡) be the random vector associated
with the output vector d. That is,
bd(푡) = (푏11(푡), 푏21(푡), ⋅ ⋅ ⋅ , 푏푚1(푡))푇 , (61)
where 푏푖1(푡) is the first element in b푖(푡). Besides, we use the
notation 퐸d[⋅∣w(푡)] denoting the conditional expectation that
is taken over the random vector bd(푡) only.
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 8
Γ1, Γ2, Γ3 and Γ4 in (79). Finally, we get by expanding 푒(푡)
in (79) that
퐸d[∥d(푡+ 1)∥42∣w(푡)]
= (1− 휇(푡)훼)4∥d(푡)∥42
+4휇(푡)(1− 휇(푡)훼)3∥d(푡)∥22푦푡z˜푇 (푡)d(푡)
−4휇(푡)(1− 휇(푡)훼)3∥d(푡)∥22(z˜푇 (푡)d(푡))2. (80)
By the fact that 2푦푡z˜푇 (푡)d(푡) ≤ 푦2푡 + (z˜푇 (푡)d(푡))2 and∥d(푡)∥22(z˜푇 (푡)d(푡))2 ≥ 0, we get that7
퐸d[∥d(푡+ 1)∥42∣w(푡)] ≤ (1− 휇(푡)훼)∥d(푡)∥42
+2휇(푡)휅2∥d(푡)∥22, (81)
where 휅2 = max ∣푦푡∣2. Thus, we get that
퐸[∥d(푡+ 1)∥42] ≤ (1− 휇(푡)훼)퐸[∥d(푡)∥42]
+2휇(푡)휅2퐸[∥d(푡)∥22]. (82)
By (74), the second term in the RHS of (82) is bounded by
2휇(푡)휅2휅
2
1/훼
2. Therefore, we can prove by induction that
퐸[∥d(푡)∥42] ≤
2휅2휅
2
1
훼3
(83)
for all 푡 ≥ 0. Then, the proof is completed. Q.E.D.
One should note that the boundedness of 퐸[∥d(푡)∥22]
and 퐸[∥d(푡)∥42] imply the boundedness of 퐸[푑휋1(푡)푑휋2(푡)],
퐸[푑휋1(푡)푑휋2(푡)푑휋3(푡)] and 퐸[푑휋1(푡)푑휋2(푡)푑휋3(푡)푑휋4(푡)].
By (1), (16) and (17), the update of a푖(푡) can be expressed
as follows : [
a푖(푡+ 1)
푐푖(푡+ 1)
]
= (1− 휇(푡)훼)
[
a푖(푡)
푐푖(푡)
]
+휇(푡)푣푖(푡)푑˜푖(푡)
(
푦푡 − z˜푇 (푡)d˜(푡)
)[
x푡
1
]
, (84)
where 푣˜푖(푡) = 푧˜푖(푡)(1 − 푧˜푖(푡)). Note from (21) and (61) that
푑˜푖(푡) = 푑푖(푡) + 푏푖1(푡)푑푖(푡) and
푑˜푖(푡)d˜(푡) = (푑푖(푡) + 푏푖1(푡)푑푖(푡)) (bd(푡)⊗ d(푡)) . (85)
Lemma 2: For the algorithm based on (16) and (21)
with 훼 > 0, if 0 < 휇(푡)훼 < 1, with probability one
퐸[∥(a푖(푡), 푐푖(푡))∥22] <∞ for all 푡 and 푖 = 1, 2 ⋅ ⋅ ⋅ ,푚.
Proof: Let 푎푖푗(푡) be the 푗푡ℎ element in a푖(푡).
푎푖푗(푡+1) = (1− 휇(푡)훼)푎푖푗(푡) + 휇(푡)푣˜푖(푡)푥푡푗 푒˜(푡)푑˜푖(푡). (86)
Squaring both sides and then taking expectation over bd(푡),
we get that
퐸d[푎푖푗(푡+ 1)
2∣w(푡)]
= (1− 휇(푡)훼)2푎푖푗(푡)2
+2휇(푡)(1− 휇(푡)훼)푎푖푗(푡)푥푡푗퐸d[푣˜푖(푡)푒˜(푡)푑˜푖(푡)∣w(푡)]
+휇(푡)2푥2푡푗퐸d
[(
푣˜푖(푡)푒˜(푡)푑˜푖(푡)
)2
∣w(푡)
]
. (87)
7Note that (1− 휇(푡)훼)4 < (1− 휇(푡)훼).
By the fact that 푣˜푖(푡) < 1/4, we can get that
퐸d[푎푖푗(푡+ 1)
2∣w(푡)]
≤ (1− 휇(푡)훼)2푎푖푗(푡)2
+2휇(푡)(1− 휇(푡)훼)휅3∣푎푖푗(푡)∣퐸d[∣푒˜(푡)푑˜푖(푡)∣∣w(푡)]
+휇(푡)2휅23퐸d
[(
푒˜(푡)푑˜푖(푡)
)2
∣w(푡)
]
, (88)
where 휅3 = max{∣푥푡푗 ∣∣푡 ≥ 0, 푗 = 1, ⋅ ⋅ ⋅ , 푛}. Now, we let
휆1(d(푡)) = 퐸d[∣푒˜(푡)푑˜푖(푡)∣∣w(푡)].
휆2(d(푡)) = 퐸d
[(
푒˜(푡)푑˜푖(푡)
)2
∣w(푡)
]
.
Clearly, 휆1(d(푡)) is a function of 푑1(푡), ⋅ ⋅ ⋅ , 푑푚(푡) up to
second order and 휆2(d(푡)) is a function of 푑1(푡), ⋅ ⋅ ⋅ , 푑푚(푡)
up to forth order. Therefore, we can take expectation of (88)
for b(푡) and get that
퐸[푎푖푗(푡+ 1)
2∣w(푡)]
≤ (1− 휇(푡)훼)2푎푖푗(푡)2
+2휇(푡)(1− 휇(푡)훼)휅3∣푎푖푗(푡)∣휆1(d(푡))
+휇(푡)2휅23휆2(d(푡)). (89)
Then by taking expectation of (89) for all b(휏) for 0 ≤ 휏 ≤
푡, we get that
퐸[푎푖푗(푡+ 1)
2]
≤ (1− 휇(푡)훼)2퐸[푎푖푗(푡)2]
+2휇(푡)(1− 휇(푡)훼)휅3퐸[∣푎푖푗(푡)∣휆1(d(푡))]
+휇(푡)2휅23퐸[휆2(d(푡))]. (90)
By Cauchy-Schwarz Inequality [36],
퐸[∣푎푖푗(푡)∣휆1(d(푡))] ≤
(
퐸[푎푖푗(푡)
2]퐸[휆21(d(푡))]
)1/2
. (91)
As 휆21(d(푡)) and 휆2(d(푡)) are functions of 푑1(푡), ⋅ ⋅ ⋅ , 푑푚(푡)
up to forth order, 퐸[휆21(d(푡))] and 퐸[휆2(d(푡))] must be
bounded for all 푡 ≥ 0. We let 휅4 be the upper bound of these
two values. Thus, taking square root on both sides of (90), we
get that√
퐸[푎푖푗(푡+ 1)2] ≤ (1− 휇(푡)훼)
√
퐸[푎푖푗(푡)2] + 휇(푡)휅3
√
휅4. (92)
Clearly, we can prove by induction that
퐸[푎푖푗(푡)
2] ≤ 휅
2
3휅4
훼2
. (93)
Following the same steps as for 푎푖푗(푡) and replacing 휅3 by
one, we can show that
퐸[푐푖(푡)
2] ≤ 휅4
훼2
. (94)
So, we can conclude that 퐸[∥(a푖(푡), 푐푖(푡))∥22] < ∞ for all
푡 and 푖 = 1, 2 ⋅ ⋅ ⋅ ,푚. The proof is completed. Q.E.D.
As a direct implication from Lemma 1 and Lemma 2, we
state without proof the following theorem for the weight vector
w(푡).
Theorem 1 (MWN-WD for MLP with linear output):
For the algorithm based on (16) and (21) with 훼 > 0, if
0 < 휇(푡)훼 < 1, with probability one 퐸[∥w(푡)∥22] <∞ for all
푡 ≥ 0.
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 10
where 휉푇 = exp
{
−훼∑푇휏2=1 휇(휏2)}. For 휇(푡)→ 0,
푇∑
휏1=1
휇(휏1) exp
{
훼
휏1∑
휏2=1
휇(휏2)
}
=
∫ 푇¯
0
exp (훼푥) 푑푥, (106)
where 푇¯ =
∑푇
휏=1 휇(휏). By (105) and (106), we can get that
푇∑
휏1=1
휇(휏1)
푇∏
휏2=휏1+1
(1− 휇(휏2)훼) ≤ 훼−1
{
1− exp(−훼푇¯ ))}
and
lim
푇→∞
푇∑
휏1=푡+1
휇(휏1)
푇∏
휏2=휏1+1
(1− 휇(휏2)훼) ≤ 훼−1.
Hence, 퐸[훽(푡)] ≤ 퐸[훽(0)] ≤ 휅1훼 . By Martingale Convergence
Theorem (p.109 in [43]), lim푡→∞ 훽(푡) and lim푡→∞ ∥d(푡)∥2
both exist with probability one.
As lim푡→∞ ∥d(푡)∥2 exists, the limit of the second term in
(84) must be bounded. That is to say, there exists a bounded
region Ω and 푡∗ such that w(푡) ∈ Ω for all 푡 ≥ 푡∗.
We can define 훽′(푡) in the same way as 훽(푡). For 푡 ≥ 푡∗,
훽′(푡) =
∥∥∥∥[ a푖(푡)푐푖(푡)
]∥∥∥∥
2
∞∏
휏=푡+1
(1− 휇(휏)훼)
+휅10
∞∑
휏1=푡+1
휇(휏1)
∞∏
휏2=휏1+1
(1− 휇(휏2)훼),(107)
where 휅10 is the bound for∥∥∥∥푣푖(푡)푑˜푖(푡)(푦푡 − z˜푇 (푡)d˜(푡))[ x푡1
]∥∥∥∥
2
.
Following the same steps as for ∥d(푡)∥2, we can then show
that with probability one lim푡→∞
∥∥∥∥[ a푖(푡)푐푖(푡)
]∥∥∥∥
2
exists for all
푖 = 1, ⋅ ⋅ ⋅ ,푚.
In conclusion, lim푡→∞ ∥w(푡)∥2 exists with probability one.
The proof is completed. Q.E.D.
Now, we can state in the following theorem the convergence
of algorithm based on (16) and (21).
Theorem 5 (MWN-WD for MLP with linear output):
For the algorithm based on (16) and (21) with 훼 > 0, if
0 < 휇(푡)훼 < 1,
∑
푡 휇(푡) = ∞ and
∑
푡 휇(푡)
2 < ∞, with
probability one lim푡→∞∇w푉⊗(w(푡)) = 0, where 푉⊗(w(푡))
is given by (31).
Proof: By Lemma 3, there exists a bounded region Ω and
푡∗ such that w(푡) ∈ Ω for all 푡 ≥ 푡∗. It further implies that
푉⊗(w(푡)) is bounded. The eigenvalues of the Hessian matrix
∇∇w푉⊗(w(푡)) must be finite.
Now, we can expand 푉⊗(w(푡 + 1)) around w(푡) and get
that
푉⊗(w(푡+ 1)) = 푉⊗(w(푡)) +∇w푉⊗(w(푡))훿w(푡)
+
1
2
훿w(푡)푇∇∇w푉⊗(w(푡))훿w(푡),
(108)
where 훿w(푡) = w(푡+ 1)−w(푡). For 푡 ≥ 푡∗, we can let 휅11
be the maximum eigenvalue of ∇∇w푉⊗(w(푡)) and then get
that
푉⊗(w(푡+ 1)) ≤ 푉⊗(w(푡)) +∇w푉⊗(w(푡))훿w(푡)
+
휅11
2
훿w(푡)푇 훿w(푡). (109)
퐸[푉⊗(w(푡+ 1))∣w(푡)] ≤ 푉⊗(w(푡))− 휇(푡) ∥∇w푉⊗(w(푡))∥22
+
휅11휇(푡)
2
2
퐸[∥h(푡)∥22], (110)
where h(푡) = (h1(푡)푇 , ⋅ ⋅ ⋅ ,h푚(푡)푇 )푇 and
h푖(푡) = (푦푡 − 푓(x푡, w˜(푡)))g푖(x푡, w˜푖(푡))− 훼w푖(푡).
The boundedness of 퐸[∥h(푡)∥22] can then be proved by
Lemma 1 and Theorem 1. We let this bound be 휅12. As a
result, we can get that
lim
푡→∞퐸[푉⊗(w(푡))∣w(푡
∗)]
≤ 푉⊗(w(푡∗))−
∑
푡≥푡∗
휇(푡)퐸[∥∇w푉⊗(w(푡))∥22 ∣w(푡∗)]
+
휅11휅12
2
∑
푡≥푡∗
휇(푡)2. (111)
As
∑
푡≥푡∗ 휇(푡)
2, lim푡→∞퐸[푉⊗(w(푡))∣w(푡∗)] and 푉⊗(w(푡∗))
are all finite,∑
푡≥푡∗
휇(푡)퐸
[
∥∇w푉⊗(w(푡))∥22 ∣w(푡∗)
]
<∞.
By the condition that
∑
푡≥푡∗ 휇(푡) = ∞, we can prove by
contradiction that
lim
푡→∞퐸
[
∥∇w푉⊗(w(푡))∥22 ∣w(푡∗)
]
= 0.
In other words,
lim
푡→∞∇w푉⊗(w(푡)) = 0.
The proof is completed. Q.E.D.
As the steps of proof for other cases are almost the same,
we state without proof the following theorems.
Theorem 6 (MWN-WD for MLP with sigmoid output):
For the algorithm based on (47) and (21) with 훼 > 0, if
0 < 휇(푡)훼 < 1,
∑
푡 휇(푡) = ∞ and
∑
푡 휇(푡)
2 < ∞, with
probability one lim푡→∞∇w푉¯⊗(w(푡)) = 0, where 푉¯⊗(w(푡))
is given by (57).
Theorem 7 (AWN-WD for MLP with linear output): For
the algorithm based on (16) and (39) with 훼 > 0, if
0 < 휇(푡)훼 < 1,
∑
푡 휇(푡) = ∞ and
∑
푡 휇(푡)
2 < ∞, with
probability one lim푡→∞∇w푉¯⊕(w(푡)) = 0, where 푉⊕(w(푡))
is given by (46).
Theorem 8 (AWN-WD for MLP with sigmoid output):
For the algorithm based on (47) and (39) with 훼 > 0, if
0 < 휇(푡)훼 < 1,
∑
푡 휇(푡) = ∞ and
∑
푡 휇(푡)
2 < ∞, with
probability one lim푡→∞∇w푉¯⊕(w(푡)) = 0, where 푉¯⊕(w(푡))
is given by (60).
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. XX, NO. XX, 201X 12
CE Input Weights
(a) 0 20 40 60 80 1000.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0 20 40 60 80 100
0
10
20
30
40
50
60
70
(b) 0 20 40 60 80 1000.1
0.15
0.2
0.25
0 20 40 60 80 100
1
2
3
4
5
6
7
8
9
Fig. 3. Change of the MSE and weights against time for the XOR problem
with multiplicative weight noise injection. (a) (훼, 푆푏) is (0, 10−3) and (b)
(훼, 푆푏) is (10−4, 10−3). Note that the data is captured in every 1000 steps.
CE Input Weights
0 20 40 60 80 100
0
0.05
0.1
0.15
0.2
0.25
0 20 40 60 80 100
−30
−20
−10
0
10
20
30
40
50
0 20 40 60 80 100
0
0.05
0.1
0.15
0.2
0.25
0 20 40 60 80 100
−5
−4
−3
−2
−1
0
1
2
3
4
5
Fig. 4. Change of the MSE and weights against time for the XOR problem
with additive weight noise injection. (a) (훼, 푆푏) is (0, 10−3) and (b) (훼, 푆푏)
is (10−4, 10−3). Note that the data is captured in every 1000 steps.
not guarantee the convergence of weights, especially when
훼 = 0. Previous studies, such as in [1] and [22], about the
convergence behaviors of these weight noise injection-based
algorithms are not completed.
VII. CONCLUSION
In this paper, we follow our previous work in [21] to present
the convergence analyses on two on-line algorithms which
combine the idea of weight noise injection and weight decay.
For each of the algorithms, we show that with probability
one the weight vector converge to a local minimum of the
corresponding objective function. Table VII summarizes the
key results presented in this paper.
While most of the previous works are analyzing the effect
of weight noise on the performance of a neural network [1],
[2], [6], [23], [32], [33], our works presented in this paper and
other recent papers [20], [21] give the first attempt to study the
effect of weight noise on the learning algorithms. In recent
years, there is an increasing interest in the study of the effect
of noise in human brains and behaviors [12], [17], [18], [27],
[31], [35]. Extend our work to study the effect of noise (either
TABLE I
SUMMARY OF THE KEY RESULTS.
O.N. Noise 퐸[∥w∥22] O.F. Local Min.
LN MWN Theorem 1 푉⊗(w) (*) Theorem 5
SN MWN Theorem 2 푉⊕(w)(*) Theorem 6
LN AWN Theorem 3 푉¯⊗(w) (*) Theorem 7
SN AWN Theorem 4 푉¯⊕(w)(*) Theorem 8
O.F.: Objective function, O.N.: Output Node, LN: Linear node,
SN: Sigmoid node,MWN: Multiplicative weight noise,
AWN: Additive weight noise. (*) See (31), (57), (46), (60)
brain noise or mental noise) on lifespan learning should be a
valuable future direction.
Acknowledgement
The authors would like to express their gratitude to the
reviewers who gave valuable comments on the earlier version
of the paper. Their comments are crucial and indispensable.
The research work reported in this paper is supported in part
by Taiwan National Science Council (NSC) Research Grants
97-2221-E-005-050, 98-2221-E-005-048 and 99-2221-E-005-
090.
REFERENCES
[1] An G. The effects of adding noise during backpropagation training on a
generalization performance, Neural Computation, Vol.8, 643-674, 1996.
[2] Basalyga G. and E. Salinas, When response variability increases neural
network robustness to synaptic noise, Neural Computation, Vol.18,
1349-1379, 2006.
[3] Bernier J.L. et al, Obtaining fault tolerance multilayer perceptrons using
an explicit regularization, Neural Processing Letters, Vol.12, 107-113,
2000.
[4] Bernier J.L. et al, A quantitative study of fault tolerance, noise immunity
and generalization ability of MLPs, Neural Computation, Vol.12, 2941-
2964, 2000.
[5] Bernier J.L. et al, Improving the tolerance of multilayer perceptrons by
minimizing the statistical sensitivity to weight deviations, Neurocomput-
ing, Vol.31, 87-103, 2000.
[6] Bernier J.L. et al, Assessing the noise immunity and generalization of
radial basis function networks, Neural Processing Letter, Vol.18(1), 35-
48, 2003.
[7] Bishop C.M., Training with noise is equivalent to Tikhonov regulariza-
tion, Neural Computation, Vol.7, 108-116, 1995.
[8] Bottou L., Stochastic gradient learning in neural networks, NEURO
NIMES’91, 687-706, 1991.
[9] Bottou L., On-line learning and stochastic approximations, in On-line
Learning in Neural Networks, David Saad (Ed), pp. 9-42, Cambridge
University Press, 1999.
[10] Z. Brezezniak and T. Zastawniak, Basic Stochastic Processes, Springer-
Verlag Berlin Heidelberg New York, 1998.
[11] Chen B., Y. Zhu, and J. Hu, Mean-Square Convergence Analysis of
ADALINE Training With Minimum Error Entropy Criterion, IEEE
Transactions on Neural Networks, in press.
[12] Chen B.S., C.W. Li, On the Noise-Enhancing Ability of Stochastic
Hodgkin-Huxley Neuron Systems, Neural Computation, Vol.22, 1737-
1763, 2010.
[13] Chen S., Local regularization assisted orthogonal least squares regres-
sion, Neurocomputing, pp. 559–585, 2006.
[14] J. L. Doob, Stochastic processes, John Wiley and Sons, New York, 1953.
[15] Edwards P.J. and A.F. Murray, Can deterministic penalty terms model
the effects of synaptic weight noise on network fault-tolerance? Inter-
national Journal of Neural Systems, 6(4):401-16, 1995.
[16] Edwards P.J. and A.F. Murray, Fault tolerant via weight noise in analog
VLSI implementations of MLP’s – A case study with EPSILON, IEEE
Transactions on Circuits and Systems II: Analog and Digital Signal
Processing, Vol.45, No.9, p.1255-1262, Sep 1998.
[17] Faisal, A. Aldo, Luc P.J. Selen, Daniel M. Wolpert, Noise in nervous
system, Nature Reviews: Neuroscience, Vol.9, 292-303, April 2008.
[18] Flehmig H.C., M. Steinborn, R. Langner, K. Westhoff, Neuroticism and
the mental noise hypothesis: Relationships to lapses of attention and
slips of action in everyday life, Psychology Science, Vol.49(4), 343-360,
2007.
[19] Ho K., C.S. Leung, and J. Sum, On weight-noise-injection training,
M.Koeppen, N.Kasabov and G.Coghill (Eds.), Advances in Neuro-
Information Processing, Springer LNCS 5507, p.919-926, 2009.
Report on the Research Collaboration with Prof. Chi-Sing Leung in the 
Department of Electronic Engineering, City University of Hong Kong, on March, 
2011. 
 
Prof. Chi-sing Leung is one of my close research collaborators for more than 15 years. 
Since I have been working in the National Chung Hsing University, we start to have 
regular research meeting (ARM), once a year. I fly to Hong Kong and meet him in his 
department, normally for one week long. During the visit, we will have intensive 
meeting on our on-going research works and identifying new research topics. 
 
Works Have Been Done in the City University of Hong Kong 
1. Working with Prof. Leung on a paper entitled Convergence Analysis of a Noise 
Injection Training Algorithm for MLP. This paper is planned to submit to IEEE 
Transactions on Neural Networks. 
2. Discuss with Prof. Leung and his students on Rank Learning, to see if any 
extended work can be done along. 
3. Meeting(s) with your fellow students sharing with them my research experiences 
and the research problems that I am thinking of. 
 
Apart from discussing on the above issues, we have also discussed the extended 
works that can be done along the following published papers 
 
1. John Sum, Kevin Ho, SNIWD: Simultaneous weight noise injection with weight 
decay for MLP training, C.S. Leung and M. Lee and J.H. Chan (Eds), Neural 
Information Processing, Springer LNCS No.5863, p.494-501, 2009. 
Spring-Verlag Berlin Heidelberg. 
2. John Sum, Yen-Lun Liang, Chi-sing Leung, Kevin Ho, Empirical studies on 
weight noise injection based online learning algorithms, Proc. TAAI 2010 
(Domestic Track).  
3. Kevin Ho, Chi-sing Leung, John Sum, Siu-chung Lau, Convergence Analysis of 
Multiplicative Weight Noise Injection During Training, Proc. TAAI 2010 
(International Track). (Merit Paper Award - International Track)  
4. Kevin Ho, Chi-sing Leung, John Sum, Objective functions of the online weight 
noise injection training algorithms for MLP, IEEE Transactions on Neural 
Networks, Vol.22(2), 317-323, Feb 2011. (SCI) 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：沈培輝 計畫編號：99-2221-E-005-090- 
計畫名稱：利用 Weight Decay 結合隨機 fault 或加注雜訊的在線容錯訓練之特性研究(2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 1 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 1 100% 
人次 
 
