  2 
中文摘要 
人口的老化為近年來世界普遍的現象之一，並且獨居老人比例逐年上升，照護人力成本也跟著上
升。為了節省醫療資源費用，並且提升醫生診斷效率，本研究提出一套不必限定行走方向的步態分析
監測與評估系統，改善過去方法較不便之處，可運用於家中、醫院、診所等場所，讓使用者了解自我
身體健康的狀況。本研究使用多方向的走路影像，分析人類走路的姿勢，使用步伐寬度變化、頸幹線
傾斜角度的變化、行走角度等特徵來評估走路的姿勢。首先透過 Otsu 的方法取得二值化影像所需的門
檻值，再利用二值化影像和二值化背景影像相減後所得到的結果中取出人體輪廓影像進行特徵擷取與
分析。利用來自不同行走角度的步伐寬度週期變化，評估走路姿勢是否正常，利用頸幹線傾斜角度的
變化，評估身體上半身姿勢是否異常。本研究成功的擷取多種角度的步伐特徵輪廓影像，並將拍攝角
度互相垂直的兩台攝影機所得的特徵值加以整合處理，再與行走方向垂直於攝影機所得到的理想狀態
步伐特徵做比較。實驗結果顯示，相關係數可高達 86%。證明以兩台視線互相垂直之攝影機進行步態
分析，可解決行走方向上的限制，很方便地提供長期的走路姿勢紀錄。 
關鍵字：輪廓影像、走路姿勢分析，雙攝影機、視線正交 
Abstract 
Population aging is one of the common phenomenal in the world. The ratio of elderly living alone 
increases year by year and the cost of human care follows accordingly as well. To reduce the cost of medical 
resource and improve the doctor’s diagnosis efficiency, this study proposes a human gait monitoring and 
evaluation system with no restriction on walking direction. This system improves the inconvenience situation 
in previous methods and can be applied at home, hospitals, clinics and other places for the self-evaluation of a 
user’s health condition. This study uses the images obtained from multi-viewing angles and performs human 
gait analysis through walking features of gait width variation, neck line inclination variation and walking 
direction angle. First, we obtain the threshold needed for image binarization by using the Otsu’s method. Then 
we subtract a binary image from the binary background image to get human silhouette image for feature 
extraction and analysis. We evaluate the gait normality by using the periodicity of gait width variation from 
various walking directions, and evaluate the normality of upper body by the tilt angle variation of neck line. 
This study successfully extracts gait feature silhouette images of various walking directions and integrates the 
features obtained from two orthogorally arranged cameras. The integrated feature is then compared with the 
ideal gait feature obtained from the camera whose viewing angle is vertical to walking direction, resulting in a 
high correlation coefficient of 86%. This proves that a vision-based gait analyzer based on two orthogonally 
arranged cameras can remove the walking direction restriction and provide a long-term gait record 
conveniently. 
 
Keywords: Silhouette Image, Gait Analysis, Two Cameras, Orthogonal Line of Sight 
 
  4 
訓練和人體運動研究等方面。Kong 和 Tomizuka 認為 GCF (Ground Contact Forces)於偵測人類走路方向
上提供許多必要的資訊[16]，利用裝有 GCF 感測器的鞋子(每雙鞋子於鞋墊背面裝上四個感測器)，得
到足底壓力和行走方向等資訊，並且應用於動力輔助裝置上提供病人使用，以及幫助醫生診斷走路方
面的疾病。Zhang 等人提到由於人口的老化，導致老年人的健康問題越來越被重視，而老年人跌倒導
致死亡的比率更是高達 75%，而 50%-70%的跌倒是發生在走路的時候[17]。因此該篇文章找出一般步
伐下年紀與走路穩定度的關係，利用 DTW (Dynamic Time Warping)方法去分析。該文中的作法是在受
測者身上貼上 Marker，分析走路時身上 Maker 位置的變化以及速度與加速度的改變，並且統計出年齡
與走路速度和加速度之間的關係。 
多攝影機大多應用在監視系統、人臉辨識系統和影像追蹤系統等等。Shakhnarovich 等人指出人類
追蹤與辨識系統應該整合多視角影像，並且應有良好的辨識率，即使目標物距離攝影機有一段距離。
他們個別使用一台攝影機捕捉正面臉部影像，和側面走路姿勢影像，這兩台攝影機位置可因目標物在
監視區域內移動，而同時改變位置[18]。Black 和 Ellis 提到使用多攝影機進行走路追蹤，和其他大部分
方法最大的差異在於多攝影機能解決影像遮蔽重疊的問題[19]。 
相較於佩帶輔助儀器進行訊號擷取與分析，單純使用電腦視覺進行步伐分析更加方便，因此 Lin
使用以視覺為基礎的方法進行步伐分析[20]，包括使用側面步伐影像進行走路步伐的分析。Goffredo 等
人也是使用視覺的方法進行步伐辨識[21]，但是使用正面步伐影像進行分析辨識。本研究提出一套以在
攝影方向上正交的兩台攝影機進行走路姿勢分析系統，能夠同時改善方向限制和影像遮蔽等問題，增
加系統使用的方便性與實用性。 
四、 研究方法 
4-1 前景物偵測與計算行走角度的系統架構與流程 
本系統架設兩台視線上互相垂直的 PTZ 攝影機拍攝走路行為，如圖 1 所示，其中的直線表示攝影
機的視線方向，Camera A 和 Camera B 到視線交點中心不必是等距的。本研究針對下面狀況進行走路方
向判斷與姿勢分析：(1) 第一種為與攝影機垂直的理想走路方向，如圖 2 中虛線所示的行走方向；(2) 第
二種為任意角度的直線走路方向，如圖 3 所示，其中 Camera C 架設於使其視線與行走方向垂直且通過
圖 1 所示由 Camera A 和 Camera B 的視線交點中心。為了之後解說的方便，我們將行走角度 定義為
Camera C 之影像平面與 Camera A 之影像平面的夾角，由 Camera C 的影像平面為起點，到 Camera A
影像平面順時針方向為正且取銳角。圖 4 顯示前景物偵測及行走角度計算的流程圖，其中前景物偵測
採用的是較常見的標準方法，故在精簡報告的精簡原則下將其省略，因而以下只針對行走角度計算的
部份加以詳述。 
 
  6 
 
圖 3 行走方向之角度定義示意圖。 
 
           圖 4 前景物偵測及行走角度計算的流程圖。 
4-2 攝影機成像原理 
三度空間中的物體透過攝影機的成像畫面，會因為攝影機擺設的位置或者攝影機本身構成元件的
不同，而有不同的成像。而物體在三度空間中的座標與在攝影機影像平面上的座標，存在著對應的關
係，此關係由攝影機的成像幾何模型所決定。 
以下說明將真實世界座標系統(world coordinate system)中的ㄧ個三維座標點，投影到一個二維影像
平面的透視投影轉換。如圖 5 所示，其中 f 是攝影機的焦距(focal length)。假設 P 為三度空間中的ㄧ個
點，其座標為(X, Y, Z)，經過透視投影法投影到影像平面上的 p 點，其座標為(-u, -v)。利用相似三角形
的幾何關係，可以到下面的式子：  
影像輸入 
背景建立 
前景物擷取 
連通元件標記 
形態學填補 
計算行走角度 
雜訊去除 
走路特徵分析 

camera A BC  
影像平面 
Camera A 
Camera B 
Camera A 
影像平面 
Camera C 
影像平面 
Camera C 

行走方向 
  8 
根據針孔投射原理[14]可得到： 
           
1HhHh es                         (5) 
其中 hs和 he 為影像平面上輪廓影像的高度。在此式中，由於終點的位置較起點靠近攝影機，所以 he > 
hs。參考圖 7，由 SEM 可知： 
                  
FMFS
HH


 1tan                 (6) 
將(3)、(4)和(5)式的結果代入(6)式即可換算出行走角度： 
                








ebse
es
xhxh
hhf )(
a r c t a n                       (7) 
 
圖 6 計算行走方向示意圖。 
O s(xs, 0) 
e(xe, 0) 
he 
M 
g 
E 
F 
E’ 
WD 
影像平面 
透鏡中心 S 
M’ 
       
 
 
f 
H 
H1 
+ 
+ 
- - 
 
X 
x 
Y 
 
y 
Z 
實際影像 
hs 
  10 
輪廓影像高度變化的誤差。最後將所有得到的角度再加以平均[如(12)式]，所得到的結果即為行走角度。 
                  


n
i
in
nn 1
21
1
). . .(
1
                         (12) 
 
4-4 兩台攝影機資訊整合 
 同一時間兩台視線上垂直的攝影機所取得的輪廓影像資訊不同，當一台攝影機因行走角度的關係
會產生步伐輪廓影像較大的遮蔽時，另一台攝影機的遮蔽則會較小，甚致無遮蔽，此處的遮蔽是指身
體輪廓影像因視角不同被自己身體擋住視線的結果，而不是外物造成的遮蔽。如何將由兩台攝影機所
得部分遮蔽的結果加以整合，以得到接近無遮蔽攝影機所得的結果是一個有趣的探討課題。在此藉由
圖 8 解釋兩台互相垂直的攝影機所得的資訊應如何整合。 
       
圖 8 兩台互相垂直的攝影機整合示意圖。 
    假設一行走者從點 S 走到點 E。定義 CC' 為在行走方向與 Camera C 視線方向垂直的影像平面上所
移動的距離，表為 dC； AA' 為呈現在 Camera A 的影像平面上所移動的距離，表為 dA； BB' 為呈現在
Camera B 的影像平面上中所移動的距離，表為 dB；fA、fB和 fC分別為 Camera A、B 和 C 的焦距，此實
驗中使用相同攝影機，因此焦距相同，在方程式中均以 f 表示。起點 S 與 Camera A、Camera B 和 Camrea 
C 之透鏡中心的距離分別為 HA、HB和 HC，虛線為物體在現實環境中移動的距離以 D 表示， 為行走
Camera C 
影像平面 
Camera A 
影像平面 
C’ 
EB 
EA 
dC 
Camera B 
影像平面 
B S 
E 
C 
A 
fC 
fA 
fB 
A’ 
B’ 
dA 
dB 
  12 
與攝影機的距離拉進而升高的波形，很難判斷走路姿勢是否正常；加以正規化後，雖會顯示出有規律
的波形，但仍然無法和水平方向重心變化波形圖做比較。圖 9(a)為攝影機擺設位置說明圖，圖 9(b)和(c)
為所擷取行走方向原始圖；圖 10(a)和(b)分別為圖 9(b)和(c)所偵測出未經過正規化之前的重心變化波型
圖；圖 10(c)和(d)則為正規化之後的結果；因此利用兩台方向上垂直的攝影機所得到的資訊，加以整合
後，呈現能夠與水平方向的重心變化特徵波形做比較的結果。圖 11(a)為兩台攝影機所得到不同角度重
心變化特徵利用(19)式整合的結果，最後和垂直於攝影機方向行走所得到的結果做比較，如圖 11(b)所
示。這兩個方向的走路重心變化實驗結果看來，在正規化之前不是很明顯，受到波形當中雜訊的影響，
完全看不出有什麼變化的規則，在正規化之後，走路週期中重心上下起伒的波形完全被突顯出來，雖
然如此卻還是受到角度影響，不容易判斷出走路姿勢是否正常；經過兩台攝影機所得到的結果合成之
後，就能明顯的看出重心變化的規律，並且也與垂直於攝影機所得到的結果相近。 
 
                       (a) 
 
                             (b)                                 (c) 
圖 9 行走方向示意圖。(a)攝影機架設說明圖；(b) A 攝影機所擷取畫面；(c) B 攝影機所擷取畫面。 
行走方向 行走方向 
Camera A 
Camera B 
行走方向 
  14 
 
 
圖 11 重心變化波型圖。(a)整合 Camera A 和 Camera B 兩方向重心變化的結果圖；(b)水平方向行走重心
變化波形圖。 
 步伐寬時重心會較步伐窄時低，且距離攝影機近時重心會較離攝影機遠時高，這些都是影響重心
變化主要的因素，因此重心變化整合後的結果仍然沒有步伐寬度整合後的結果理想。 
5-2 步伐寬度變化特徵 
因為行走方向並非與攝影機方向垂直，使得當距離攝影機較近的腳往前跨出時，遮蔽了另外一隻
腳，而造成左右腳步伐寬度並非相等，圖 12 分別說明了未產生遮被與產生遮蔽之間的差異；圖 12 中(a)
與(b)為同ㄧ時間裡的同ㄧ個動作，經不同方向攝影機所得到不同的輪廓影像，由此可得當圖 12(a)中
Camera A 取得被遮蔽時的腳部資訊時，Camera B 在同ㄧ個時間可取得完整的腳部資訊，因此本實驗利
用兩台攝影機個別所得到的資訊，回復成行走方向與攝影機垂直時所得的資訊，來解決行走方向上的
限制。 
擷取步伐寬度特徵時攝影機架設如圖 9(a)，圖 13(a)和(b)分別為由圖 9(b)和(c)兩個方向偵測步伐寬
度變化所得到的結果圖，由圖中雖可看出規律性，但是無法用來判斷走路姿勢，因此將這兩個圖利用(19)
式整合成圖 14(a)，最後與圖 14(b)水平方向攝影機所得到的結果圖做比較。步伐寬度特徵較重心變化與
頸幹線傾角特徵明顯，擷取特徵時較不會受到影像處理誤差影響，因此整合後的結果也比另外兩項特
徵理想。 
 
(b) 
(a) 
  16 
 
圖 14 步伐寬度特徵圖。(a)整合 Camera A 和 Camera B 兩方向步伐寬度特徵所得結果圖；(b)水平方向行
走步伐寬度特徵圖。 
5-3 與行走方向垂直於攝影機方向比較的結果 
最後使用行走方向與攝影機垂直之理想狀況下的結果與本研究提出的方法做比較，統計兩者在一
個完整步伐週期中步伐寬度變化測詴結果的相關係數。假設有兩組樣本 ),...,,( 21 nxxx 及 ),...,,( 21 nyyy ，
其樣本平均數分別為 x 和 y ，樣本標準差分別為 xS 與 yS ，且兩組樣本之樣本共變異數(Covariance)為
xyS ，以上變數分別定義為 
     


n
i
ix
n
x
1
1
                                     (20) 



n
i
iy
n
y
1
1
                                     (21) 





n
i
ix xx
n
S
1
2)(
1
1
                               (22) 





n
i
iy yy
n
S
1
2)(
1
1
                              (23) 





n
i
iixy yyxx
n
S
1
))((
1
1
                            (24) 
則相關系數 r 定義為 
(a) 
(b) 
  18 
表 1 一位受測人員完整步伐週期寬度的統計結果 
Method 
Event 
Ideal situation (Pixels) 
Proposed method 
(using 30m )(pixels) 
Foot strike 29 22 
Opposite foot-off 22 19 
Reversal of fore-aft shear 24 20 
Opposite foot strike 31 23 
Foot-off 20 17 
Foot clearance 8 14 
Tibia vertical 19 20 
Second foot strike 28 23 
 
表 2 六位受測人員各完整步伐週期寬度資料的平均值與標準差 
Method 
Event 
Ideal situation (Pixels) 
Proposed method 
(using 30m )(pixels) 
Foot strike 30.74 3.27 22.45 4.79 
Opposite foot-off 22.52 2.42 18.96 2.91 
Reversal of fore-aft shear 24.73 3.50 20.47 3.58 
Opposite foot strike 31.66 2.13 23.82 3.47 
Foot-off 20.41 2.04 18.06 2.69 
Foot clearance 8.23 0.72 13.58 1.86 
Tibia vertical 18.94 2.18 19.34 3.94 
Second foot strike 29.17 2.96 22.94 4.39 
 
表 3 六位受測人員共 40 筆資料與行走方向垂直於攝影機方向的相關係數統計 
 Correlation Coefficient 
方向 1 0.78 
方向 2 0.86 
方向 3 0.81 
 
六、 結論 
本系統分成兩大區塊，分別為前端的前景物輪廓影像擷取與後端的走路姿勢步態分析這兩部分。
本研究使用輪廓影像進行走路姿勢分析，所以必頇先做到有效且快速的影像前處理，以確保後端步態
姿勢分析時不會因為影像雜訊、輪廓影像擷取不完整等問題，而影響了最後的結果。對於前景物擷取，
我們是用 Otsu 所提供的門檻值估計法對影像進行二值化，因為 Otsu 所提出方法能夠依照不同影像自動
計算出符合各自的門檻值，因此能夠有效率且準確的得到我們所需的影像；再透過一些濾波處理減低
  20 
[18] G. Shakhnarovich, L. Lee and T. Darrell, “Integrated face and gait recognition from multiple views,” in 
Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition, vol. 1, pp. 439-446, 2001. 
[19] J. Black and T. Ellis, “Multi camera image tracking,” Performance Evaluation of Tracking and 
Surveillance, vol. 24, pp. 1256-1267, Nov. 2006. 
[20] K. W. Lin, Vision-based gait analysis, Master Thesis, Department of Electrical Engineering, National 
Cheng Kung University, Taiwan, 2005. 
[21] M. Goffredo, J. N. Carter and M. S. Nixon, “Front-view gait recognition,” in Proc. IEEE Int. Conf. on 
Biometrics: Theory, Applications and Systems, pp. 1-6, Oct. 2008. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期：100 年 09 月 26 日 
                                 
一、 參加會議經過 
此次參加的研討會的名稱為「The 2nd International Conference on Multimedia Technology – 
ICMT2011」，時間是 2011 年 7 月 26 日到 29 日，會議地點在中國大陸的杭州。此會議為 IEEE 贊助
的會議，會議論文集也由 IEEE 所出版，且所有接受的文章都會登錄在可被方便檢索的 IEEE 相關資
料庫 IEEE Explore 中。 
三天的會議當中包括了七個很不錯的 Keynote speech，聽完後感覺很有收穫，其中有不少對我
而言新的課題、新的概念和技術。我對七個講題中的五個特別感興趣，這五個演講課題分別是： 
1. Computer Vision in Virtual Reality 
2. 3D Visualization and Its Applications 
3. Advances on Topological Registration Approaches with Applications 
4. Navigating Large Image Databases 
5. Facial shape, Texture and Reflectance from a Single View 
 除了 Keynote speech 以外，就是一般論文的報告。在本次研討會中，本人的實驗室發表了一
篇近年由國科會計畫贊助的研究成果，論文題目為「A Gait Analysis System Using Two Cameras 
With Orthogonal View」。為了增加與參加者的互動性，我們選擇了以 Poster 的方式發表，也收到
了預期的效果。除了參加 Poster Section 與許多學者直接互動以外，也選擇性到幾個 Oral Sections
去學習，和學者專家交換意見與心得。當中包括許多大陸的學者專家和外籍人士，還包括幾位從
台灣過來的教授與研究生。 
計畫編號 NSC 99-2221-E-033-050 
計畫名稱 以熱感影像進行步態與睡眠品質分析 
出國人員
姓名 
繆紹綱 
服務機構
及職稱 
中原大學 教授 
會議時間 
100 年 7 月 26 日
至 100 年 7 月 28
日 
會議地點 
中國-杭州 
會議名稱 
(中文) 第二屆多媒體技術國際會議 
(英文) The 2nd International Multimedia Technology Conference 
發表論文
題目 
(中文) A Gait Analysis System Using Two Cameras With Orthogonal Views 
(英文) 使用兩台方向上垂直的攝影機進行步態分析的系統 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
日期：100 年 09 月 26 日 
                                 
一、參加會議經過 
本次參加之研討會的名稱為 The 2nd International Conference on Multimedia Technology，會議時
間為 2011 年 7 月 26 日到 28 日，會議地點是在中國大陸杭州市中的百瑞國際大酒店。這次會議在一
開始時，網站上的相關訊息很不足，只有告知會議日期，以及會後有一日遊的行程設計。由於我們
是在第一次徵稿時，就已經投稿並被接受了，因此為了避免旅遊旺季機票不好訂且不想浪費時間在
轉機上，所以依照網站所提供的會議時間及早訂了長榮直達杭州的班機。只是整個會議流程到會議
前一週才公告，所以在看到相關訊息後，已難以去變更航班與行程做更好的安排，於是就依照原訂
計畫在 7 月 25 日晚間抵達杭州。 
三天的會議中，第一天是報到與註冊時間，因此在早上報到完成後，我和同行的老師和同學們
就自行安排活動，包括在會議地點附近參觀；第二天的活動中，幾乎一整天都是主題演講，因此我
們就在會場聽了一天的演講，到了晚上，才開始 Oral Sections。這個研討會分了許多的小房間，都
有各自的領域在做 Oral 的報告，但短短兩個小時在我投稿的這個領域 Multimedia Technology & 
Information Science 就有 38 篇論文。而在一開始大會也沒有告知是使用 Poster 還是 Oral，因此我們
在事前準備的形式是 Poster，而當下在每個 Conference room 裡面也沒有按照表訂的順序去報告，而
是每個人當下去決定自己要報告的形式。最後一天只有早上有安排 Poster 以及 Oral 的行程，而我的
Poster 就在最後一天早上去張貼進行發表，時間很長。而原本的會後一日遊提早改到最後一天的下
午，成為半日遊，因此整個行程都被縮短了，而在半日遊結束後這次的會議也就結束了。 
 
計畫編號 NSC 99-2221-E-033-050 
計畫名稱 以熱感影像進行步態與睡眠品質分析 
出國人員
姓名 
洪嘉澤 
服務機構
及職稱 
中原大學 研究生 
會議時間 
100 年 7 月 26 日
至 100 年 7 月 28
日 
會議地點 
中國-杭州 
會議名稱 
(中文) 第二屆多媒體技術國際會議 
(英文) The 2nd International Multimedia Technology Conference 
發表論文
題目 
(中文) A Gait Analysis System Using Two Cameras With Orthogonal Views 
(英文) 使用兩台方向上垂直的攝影機進行步態分析的系統 
 附錄  所發表的論文 
(已收錄於 IEEE Explore 資料庫) 
 
Li, Y. R., Miaou, S. G., Hung, C. K., and Sese, J. T., "A Gait Analysis System Using Two Cameras 
with Orthogonal Views," IEEE Int. Conf. on Multimedia Technology, July 26-28, pp. 2841-2844, 
Hangzhou, China, 2011. (EI) (NSC99-2221-E-033-050) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
whereas 1H is the vertical distance from the lens center to 
the end point E . Point 'M and Point M are the intersections 
indicated in Figure 2. Points s and e are the projected points 
of S and E , respectively, onto the image plane, and their 
image plane coordinates are ( sx , 0) and ( ex , 0), respectively. 
Referring to Figure 2, we can obtain the following 
equations according to the similarity property between 
SOF and sOg , and that between 'M OF and eOg : 
'
e s
x xf
H FSFM

                    (1) 
Similarly for 'EOE and eOg , we have 
1
e
Hf
x FM
                      (2) 
The principles of pinhole projection [11] give us 
1s e
h H h H                     (3) 
where sh and eh are the highs of silhouette on image plane at 
the starting point and the end point, respectively. Here, the 
end position is closer to the camera than the starting point. 
So
s eh h . Referring to SEM in Figure 2, we can see that 
1
tan
H H
FS FM




                   (4) 
Combining the results from Eq. (1) to (4) gives the WDA: 
( )
arctan
s e
e s b e
f h h
h x h x




 
 
 
                 (5) 
 
Figure 2 A schematic diagram for WDA calculation. 
B. Actual WDA Calculation 
In Eq. (5), f is available from camera specifications, 
while the other four variables ( sh , eh , sx and ex ) need to be 
estimated. The estimation process for each variable is given 
from Eq. (6) to Eq. (9), respectively. 
( 2 1)
2 ( 1) 1
1
( ) [ ], 1, 2, ...,
i a
s
r i a
h i h r i n
a

  
             (6) 
2
( 2 1) 1
1
( ) [ ], 1, 2, ...,
ia
e
r i a
h i h r i n
a   
             (7) 
( 2 1)
2 ( 1) 1
1
( ) [ ], 1, 2, ...,
i a
s
r i a
x i x r i n
a

  
             (8) 
2
( 2 1) 1
1
( ) [ ], 1, 2, ...,
ia
e
r i a
x i x r i n
a   
             (9) 
where i  denotes the i th  image frame. The idea is to 
find the average height (Eq. (6)) and average moving 
distance (Eq. (8)) of silhouette images taken in consecutive 
frames, where [ ]h r  in Eq. (6) is the height of silhouette 
image taken at the r th  image frame, and [ ]x r  in Eq. (8) 
is the head center taken at the r th  image frame. Then we 
calculate the average height (Eq. (7)) and average moving 
distance (Eq. (9)) of the next a frames. In these four 
equations, the value of n is related to image capturing time, 
i.e., longer time corresponds to larger value of n . 
In Eqs, (8) and (9), a head center is served as the 
reference point because it is more stable than other choices. 
While walking, hand and foot movements cause too much 
swing; hand swing creates significant interference on the 
calculation of center of gravity (COG) of silhouette image, 
resulting in poor pace estimation; the variation of shoulder 
point affected by walking directions is quite obvious. If we 
had only used the COG taken from the first image to find 
the starting position and the COG taken from the last one to 
find the end position (i.e., using COG as the reference point 
and no average calculation from a images), the calculation 
error would have been significantly greater than that 
obtained by the current approach due to large variation in 
distance between the test subject and the camera. 
We record the head center and silhouette high from the 
first image, take the average for every five images in our 
experiment (i.e., 5a  ), and calculate the WDA based on 
Eqs. (5) - (9). The averaging operation can reduce the 
calculation error in silhouette height caused by body 
fluctuation when walking. Finally, we take the average of 
all angles obtained and produce the final angle: 
1 2
1
1 1
( ... )
n
n i
in n
    

                 (10) 
III. Integrating Information Taken From Two Cameras 
The silhouette image taken by two orthogonally 
arranged cameras at the same time is usually different. For 
example, when a larger occlusion of silhouette image 
occurs in the image plane of one camera, the occlusion 
appears in another camera may be smaller or even none. 
The occlusion here means the blocking of camera view by 
some body parts of a test subject (not by any foreign 
objects). It is interesting to discuss how to integrate the 
information in two partially occluded images taken by two 
physical cameras and generate more useful information as 
if it was obtained from a less occluded situation taken by an 
imaginary camera. Figure 3 illustrates such an idea for two 
cameras having perpendicular views. 
 
Figure 3. A schematic diagram showing how to integrate the information 
obtained from two cameras having perpendicular views. 
Suppose the test subject walks from Point S to Point E . 
This movement sweeps across a distance 'Cd C C , 
'Ad A A , and 'Bd B B  in the image planes of Camera C, 
Camera A, and Camera B, respectively. The focal length is 
expressed as Af , Bf  and Cf  for Camera A, Camera B and 
Camera C, respectively. For our experiment we use the 
same type of camera whose focal length will be denoted 
as f . The distance between the starting point S and the 
center of the lens is denoted by AH , BH , and CH with respect 
to Camera A, Camera B and Camera C. The dashed line 
represents the actual moving distance D of the test subject 
in real world. Let   be the WDA, 
then BE SE   , sinB AE E SE D   , and cosAEE D  . The 
similarity property of triangle gives us 
views are orthogonal; Camera C is installed such that its 
camera view is always perpendicular to the walking path. 
To facilitate the discussion, the method using both Camera 
A and Camera B with the information combination 
technique is called the proposed method; the method based 
on the use of Camera C is called the ideal case. In the 
experiment, six subjects are invited and 40 pieces of pace 
length data are collected for each subject. The pace length 
is a complete gait cycle containing 8 standard events.  
 
(a) (b) 
Figure 8 Foot occlusion. (a) snapshot from Camera A showing occlusion 
effect; (b) snapshot from Camera B showing no occlusion. 
 
Figure 9 Pace variation. (a) Result after applying the integration approach; 
(b) Result obtained from Camera C. 
A statistics for Path 2 is shown in Table I. The standard 
deviation of the proposed method is slightly higher than 
that of the ideal case. Many factors can contribute to this 
difference, such as the camera setup (two cameras do not 
actually have orthogonal views, the cameras are not in the 
same height or tilt angles, etc). Table II shows the 
correlation between the ideal case and the proposed method. 
An average correlation of 0.82 reflects high correlation in 
the results obtained by using the ideal case and the 
proposed method. This indirectly demonstrates the 
effectiveness of the proposed method. The best 
performance is in the case of Path 2, where it equally 
divides the angle formed by two cameras (45 degrees each). 
This is expected because a better complementary silhouette 
images can be captured in this case. 
 
Figure 10 The experiment setup with three different walking paths. 
V. Conclusions 
The experimental results show the feasibility that the 
restriction of walking direction can be reduced or even 
removed with the proposed approach and this is not 
possible when only one camera is used. Removing the 
restriction in walking direction is important especially 
when the system is deployed in a casual environment (say 
at home) for a long term gait analysis. The proposed 
approach is not perfect as its accuracy can be affected by 
many factors, including the capturing of incomplete 
silhouette image, occlusion by body parts, etc. More work 
is needed to improve the proposed system. 
TABLE I.  COMPARISON RESULT OF PATH 2 FOR THE 8 EVENTS IN A 
COMPLETE GAIT CYCLE 
 
TABLE II.  THE CORRELATION COEFFICIENT STATISTICS FOR THE 
IDEAL CASE AND THE PROPOSED METHOD  
 
VI. References 
[1] J. K. Aggarwal and N. Nandhakumar,“On the computation of motion 
of sequences of images-a review,” Proc. IEEE, vol. 76, no. 8, pp. 
917-934, 1988. 
[2] Q. Cai, A. Mitiche, and J. K. Aggarwal,“Tracking human motion in 
an indoor environment,”in Proc. IEEE Int. Conf. on Image 
Processing, vol. 1, pp. 215-218, Oct. 1995. 
[3] Q. Cai and J. K. Aggarwal,“Tracking human motion using multiple 
cameras,”in Proc. IEEE Int. Conf. on Pattern Recognition, pp. 68-72, 
Aug. 1996. 
[4] X. Li, S. J. Maybank, S. Yan, D. Tao and D. Xu, “Gait components 
and their application to gender recognition,” IEEE Trans. on Systems, 
Man, and Cybernetics, Part C: Applications and Reviews, vol. 38, no. 
2, pp. 145-155, 2008. 
[5] P. J. Phillips, S. Sarkar, I. Robledo, P. Grother, and K. W. Bowyer, 
“The gait identification challenge problem: Data sets and baseline 
algorithm,” in Proc. IEEE Int. Conf. on Pattern Recognition, vol. 1, 
pp. 385-388, Aug. 2002. 
[6] A. Sunderesan, A. K. Roy Chowdhury, and R. Chellappa, “A hidden 
markov model based framework for recognition of humans from gait 
sequences,” in Proc. IEEE Int. Conf. on Image Processing, Vol. 3, pp. 
II-93-6, Sept. 2003. 
[7] Y. Liu, R. Collins, and Y. Tsin, “Gait sequence analysis using frieze 
patterns,” in Proc. European Conf. on Computer Vision, pp. 657-671, 
May 2002. 
[8] H. Su and F. G. Huang, “Human gait recognition based on motion 
analysis,” in Proc. IEEE Int. Conf. on Machine Learning and 
Cybernetics, vol. 7, pp. 4464-4468. Aug. 2005. 
[9] J. Han and B. Bhanu, “Individual recognition using gait energy 
image,” IEEE Trans. on Pattern Analysis and Machine Intelligence, 
vol. 28, pp. 316-322, Feb. 2006. 
[10] B. Zhang, K. Yan, S. Jiang and D. Wei, “Walking stability analysis 
by age based on dynamic time warping,” in Proc. IEEE Int. Conf. on 
Computer and Information Technology, pp. 544-548, Jul. 2008. 
[11] G. Shakhnarovich, L. Lee and T. Darrell, “Integrated face and gait 
recognition from multiple views,” in Proc. IEEE Int. Conf. on 
Computer Vision and Pattern Recognition, vol. 1, pp. 439-446, 2001. 
[12] J. Black and T. Ellis, “Multi camera image tracking,” Performance 
Evaluation of Tracking and Surveillance, vol. 24, pp. 1256-1267, 
Nov. 2006. 
[13] K. W. Lin, Vision-based gait analysis, Master Thesis, Department of 
Electrical Engineering, National Cheng Kung University, Taiwan, 
2005. 
[14] M. Goffredo, J. N. Carter and M. S. Nixon, “Front-view gait 
recognition,” in Proc. IEEE Int. Conf. on Biometrics: Theory, 
Applications and Systems, pp. 1-6, Oct. 2008. 
(b) 
(a) 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：繆紹綱 計畫編號：99-2221-E-033-050- 
計畫名稱：以熱感影像進行步態與睡眠品質分析 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
