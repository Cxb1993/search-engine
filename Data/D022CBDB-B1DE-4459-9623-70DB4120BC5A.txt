 1
 
一、前言與研究目的 
 
Software testing and debugging are 
essential operations in the development of 
reliable software systems. During the testing 
and debugging phase of a system, some records 
about the execution of the underlying software 
may be collected, e.g., resource consumption, 
raw data about faults and failures, and the time 
required to fix bugs [1]. Scientific analysis of 
those records could provide insight into the 
status and characteristics of the software system. 
Such records constitute a valuable source of 
information that can help project managers 
guide the execution of software projects. For 
example, the information enables managers to 
assess the progress of the testing phase, 
determine whether the allocated testing 
resources are sufficient, analyze the failure 
process, and decide the optimal time to stop 
testing and release the system. Among the 
collected data, the records of software failures 
are particularly important for ensuring the 
quality of the underlying software systems. 
The software reliability growth model 
(SRGM), a type of parametric technique, is one 
of the most effective approaches for predicting 
the software failure process [2, 3, 4, 5]. To date, 
more than 100 models have been proposed [6]. 
However, Tausworthe and Lyu [2] observed 
that the underlying assumptions of most 
SRGMs may oversimplify the failure process. 
Some non-parametric techniques have also been 
proposed for predicting software failure 
processes, e.g., the artificial neural network 
(ANN), genetic algorithm (GA), and support 
vector machine (SVM) [7, 8]. ANN, one of the 
most effective non-parametric techniques, is a 
learning-based framework that can approximate 
any non-linear continuous function based on the 
given data patterns. Although the successful 
application of ANN in reliability modeling has 
been demonstrated by numerous research results 
[9, 10, 11], most ANN training algorithms 
encounter the over-fitting problem. In other 
words, the fitting bias of the training set is very 
small for known data, but it becomes 
unexpectedly significant when new data is 
applied to the network [11]. How to determine 
the appropriate number of neurons is also a 
common problem in ANN-based frameworks 
[12]. 
Besides the above parametric and non-
parametric methods, a novel rate-based 
simulation approach has been proposed to 
simulate software failure processes [2, 13]. 
Tausworthe and Lyu [2, 14] suggested that the 
approach can relax certain impractical 
assumptions that are common in model-based 
approaches. In recent years, the approach has 
been used to analyze several stochastic failure 
processes under various conditions [2, 14, 15, 
16, 17]. However, most simulation approaches 
assume that each time a failure occurs, the 
corresponding fault will be removed with 
absolute certainty [2, 15]. This assumption 
simplifies the simulation process significantly. 
In fact, debugging, which may be complicated 
in a large-scale software system, can be divided 
into three steps: fault detection, fault isolation 
(identifying the root cause of the problem), and 
fault correction [18]. Because fault correction 
inevitably includes modification of the original 
source code or environment, it is unreasonable 
to assume that no new faults will be created. 
This phenomenon is known as imperfect 
debugging [17, 19, 20, 21, 22].  
If we ignore the effects of imperfect 
debugging when analyzing failure processes, the 
number of faults and the staffing needs for 
software debugging may be underestimated. 
Gokhale et al. [16] considered the possibility of 
imperfect debugging in their simulation 
approach, but they focused on some specific 
pending faults and ignored the testing process, 
debugging process, and staffing needs. By 
contrast, in this research, we incorporate the 
concept of imperfect debugging into the 
simulation-based approach. Through the 
proposed framework, we also quantify the 
effects of imperfect debugging on software 
debugging operations. 
The remainder of this report is organized 
as follows. Section 2 reviews some existing 
approaches that predict software failure 
processes. In Section 3, we discuss the effects of 
imperfect debugging on debugging operations. 
We also introduce some simulation procedures 
to analyze the fault detection and correction 
processes under an imperfect debugging 
environment. Two case studies based on the 
proposed approach are presented in Section 4. 
Section 5 contains some concluding remarks. 
 
二、文獻探討 
 
2.1 Parametric Approach: Non-homogeneous 
Poisson Process Model 
 
The Non-homogeneous Poisson Process 
(NHPP) SRGM is one of the most popular 
parametric approaches for predicting failure 
processes. In the literatures, many NHPP 
 3
development, such as document construction, 
document inspection, code integration, code 
inspection, fault identification, and fault repair. 
Building on the approach in [2], Gokhale and 
Lyu [15] developed a simulation technique to 
model structure-based software reliability, and 
extended the simulation to reliability analysis on 
the application level. They also considered 
explicit repairs in the simulation approach, i.e. 
the removal of detected faults, which takes non-
negligible time. Moreover, Hu et al. [33] and 
Gokhale et al. [16] considered the phenomenon 
of fault introduction when simulating the fault 
repair operation. Recently, Lin and Huang [13] 
simulated the fault detection process based on 
pure birth NHCTMC. They also simulated the 
life cycle of a detected fault by exploiting the 
concept of the G/G/m queueing model (i.e. the 
arrival rate and the service rate of the model are 
generally distributed, and the number of service 
channels is finite); and they used simulation-
based approaches to analyze the staffing needs 
of software debugging teams. 
 
三、研究方法及步驟 
 
In this section, we first review the 
fundamentals of the queuing system. Then, to 
model software debugging operations, we 
analyze the fault detection and fault correction 
processes in a perfect debugging environment 
by constructing a simulation procedure called 
PROCEDURE_PERFECT_DEBUGGING, 
which is based on the queueing system. Finally, 
we investigate the effects of imperfect 
debugging on a simulation, and propose a 
generalized procedure called 
PROCEDURE_IMPERFECT_DEBUGGING.  
 
3.1 Fundamentals of the Queueing System 
 
A queueing system can be defined as 
customers arriving for service, waiting for 
service if it is not immediate, and leaving the 
system after being served [34, 35, 36]. Below 
we summarize some common terms [34, 36]: 
 Arrival rate: the average rate of customers 
joining the queueing system. The time-
independent arrival rate is defined as 
stationary. Conversely, if the rate changes 
with time, it is defined as non-stationary. 
 Service rate: the number of customers 
served in a unit. Similar to the arrival rate , 
the service rate can be stationary or non-
stationary; 
 Number of service channels: the number of 
service stations available to serve customers 
at the same time; 
 System capacity: the limitation on the 
amount of waiting space. For a queue with 
finite waiting space, no new customers are 
allowed to join the queue until space 
becomes available; 
 Queue discipline: the rule used to select 
customers for service when the queue is not 
empty. The rule can be FIFO, LIFO, random, 
or a priority scheme. 
 Number of Queues: the number of queues in 
a system. If all the services of a 
multichannel system share the same queue, 
we call it a single-queue multichannel 
queuing system. Otherwise, there is a 
specific queue for each service. 
 
Based on the above terms, a queueing 
system is usually abbreviated as A/B/X/Y/Z. A 
indicates the interarrival-time distribution 
whereas B represents the service pattern as 
described by the distribution for service time. 
For both A and B, let M, D, Ek, and G denote 
exponential distributions, deterministic 
distributions, Erlang type k distributions, and 
general distributions respectively. X means the 
number of parallel service channels. Y is usually 
omitted if there are no restrictions on the length 
of the queue, and Z can be omitted when the 
discipline is FIFO [34]. 
 
3.2 Simulation Procedure under the 
Assumption of Perfect Debugging 
 
Fig. 2 illustrates the software testing and 
debugging operations in a perfect debugging 
environment. As shown in the figure, the life 
cycle of a detected fault consists of being 
assigned to a debugging team, waiting to be 
fixed if no debugger is available, and leaving 
the debugging team after being fixed. If the 
faults detected by testers, the faults fixed by 
debuggers, and the staffing level of the 
debugging team (i.e. the number of debuggers) 
are viewed as the arrival, the departure, and the 
number of service channels in a queueing 
system respectively, it is appropriate to 
represent a debugging system as a single-queue 
multichannel queuing system [34], as shown in 
Fig. 3. We design a simulation procedure based 
on the queueing model in the figure. We make 
the following assumptions [2, 13, 15, 17]: 
(1) The software system is subject to failures 
caused by the remaining faults, and all 
faults are independent and equally 
detectable. 
 5
.)| (Prob. dt
e
dtetTdttTt
s
s
t
t
sssss  

 

   (5) 
The pseudo code of CORRECTION( ) is 
shown in Fig. 6. Similar to the function 
DETECTION( ), CORRECTION( ) is implemented 
to determine whether each fault fcorrecting (i.e. the 
fault being repaired by a debugger) can be 
removed in this run. Building on Equation (5), 
the success of fault removal depends on the 
comparison between a random number x and the 
value of service_rate×dt, where service_rate is 
the mean service rate of each debugger. If 
CORRECTION( ) returns true, the fault will be 
moved from Set R to Set F (i.e. Lines 17-18 in 
Fig. 4). Note that because the assumption of 
instantaneous repair is impractical, it is assumed 
that fault removal is explicit in the procedure. In 
other words, the function CORRECTION( ) is only 
invoked for a fault that was detected prior to the 
current run. Consequently, although faults in 
both C and R are currently being repaired by 
debuggers, CORRECTION( ) only checks the 
faults in Set R. If the removal is successful (i.e. 
CORRECTION( ) returns true), the fault fcorrecting 
will be removed from R to F. At the end of this 
step, we move all faults in Set C to Set R to 
ensure that when CORRECTION( ) is invoked in 
the next run, it will deal with the faults detected 
in the current run (i.e. Lines 19-20 in Fig. 4).  
Note that the above steps (i.e. Allocation, 
Detection, and Correction) are executed 
repeatedly until the end of the simulation.  
 
3.3. Simulation Procedure under the 
Assumption of Imperfect Debugging 
 
3.3.1 The Influence of Imperfect Debugging 
 
PROCEDURE_PERFECT_DEBUGGING 
is based on the assumption of perfect debugging, 
i.e. the assumption that fault removal does not 
affect fault detection. In reality, imperfect 
debugging probably has a considerable effect on 
software development; therefore, it is worth 
investigating. Fig. 7 shows the software testing 
and debugging operations associated with 
imperfect debugging. As shown in the figure, 
the faults created while fixing existing bugs 
may cause the system to fail later, and may then 
be observed by the testers. A considerable 
amount of research has been dedicated to the 
problem of imperfect debugging [37, 38, 39, 40, 
41]. Tokuno and Yamada [37, 38] posited that 
the faults that cause software failures can be 
divided into two types: 1) faults that exist 
latently exist in the software system before 
testing, and 2) faults that are introduced due to 
imperfect debugging or some other reason. In 
this research, we call them the inherent fault 
and the introduced fault, respectively. Ohba and 
Chou [20] investigated a large-scale system 
software development project, and reported that 
about 14 percent of the faults detected and 
removed during the observation period were 
introduced. Thus, they integrated a parameter, 
called the fault introduction rate, into the 
conventional SRGM to parameterize the degree 
of imperfect debugging. Goel and Okumoto also 
found that an imperfect debugging model can 
perform well on a land-based radar system [42].  
Pham et al. [21, 43] developed a series of 
imperfect debugging models to address specific 
problems, such as multiple failures [21] and 
changes in the fault introduction rate [43]. 
Building on the unified theory, Huang and Lin 
[5] generalized NHPP-based SRGMs with 
multiple change-points in an imperfect 
debugging environment. In addition to the 
SRGM framework, Gokhale et al. [44] 
considered the possibility of imperfect 
debugging in the NHCTMC-based simulation 
approach. They used the simulation approach to 
examine various fault removal policies, and 
derived a cost model to determine the optimal 
time to release a tested software product [16]. 
However, they only focused on some specific 
pending faults, and they ignored the influence of 
staffing levels on the debugging process. 
In addition to reliability prediction, a 
number of works on imperfect debugging 
consider the estimation of software 
development costs. Xie and Yang [45] posited 
that imperfect debugging may affect the 
determination of the optimal software release 
time or operational budget. Therefore, they 
extended a widely used cost model to address 
the case of imperfect debugging. More recently, 
Hu et al. [33] incorporated the issue of 
imperfect correction into a simulation-based 
framework, and revised the traditional cost 
model accordingly. The effects of imperfect 
debugging have been investigated extensively in 
the field of software reliability. However, in our 
survey of the literature, we found that most 
related works do not consider its influence on 
the debugging resources, such as the need to 
adjust the staffing levels when debugging 
systems. We address this problem in Section 
3.3.2, by enhancing PROCEDURE_PERFECT_ 
DEBUGGING. 
 
3.3.2 Incorporating the Phenomenon of 
Imperfect Debugging into Simulation Approach 
 
 7
example, milestones are often used to monitor 
the progress of a project. At each milestone, the 
project manager can exercise the simulation 
procedure to evaluate the possible fault 
detection and correction processes under 
different staffing levels. If the results indicate 
that an unacceptable number of detected faults 
cannot be fixed by the current staff, the manager 
can compare the simulation results with those 
derived for other staffing levels, and select the 
most suitable level. The staffing level of the 
debugging team can then be adjusted 
accordingly. 
 
四、實驗結果 
 
4.1 Description of Numerical Examples 
 
4.1.1. The Selected Data Sets 
 
In this section, we utilize the proposed 
simulation procedures to determine the 
appropriate staffing level for a debugging 
system when the degree of imperfect debugging 
changes. We apply the procedures on two data 
sets: a failure dataset (DS-1) and a failure 
dataset (DS-2), both of which were collected in 
interval-domain format. DS-1 was collected 
from a medium-sized software project [25]. A 
total of 144 faults were detected in a 17-week 
period, and 143 out of the 144 detected faults 
were repaired. DS-2 was collected from system 
T1, a project of the Rome Laboratory (formerly 
known as the Rome Air Development Center 
[3]. The software system, which contains 
21,700 delivered object instructions, was 
developed for real-time command and control 
purposes. During a 21-week period, 136 faults 
were detected and repaired. Tables 1 and 2 
show the failure data for DS-1 and DS-2 
respectively. 
 
 
4.1.2. Measures Used to Evaluate the 
Effectiveness of Debugging Systems 
 
To evaluate the effectiveness of each 
staffing level of a debugging team, we utilize 
three types of measures: throughput, time 
consumption, and personnel utilization.  
Throughput-related measures:  
 Number of Inherent Faults: the 
cumulative number of inherent faults 
detected in a specific period; 
 Number of Introduced Faults: the 
cumulative number of introduced faults 
detected in a specific period; 
 Number of Repaired Faults: the 
cumulative number of faults (both 
inherent and introduced faults) repaired 
in a specific period; 
 Number of Remaining Faults: the 
cumulative number faults (both 
inherent and introduced faults) detected 
but not repaired in a specific period. 
Time-related measures: 
 Waiting Time: the average time that a 
fault remains in the pending queue; 
 Response Time: the average time that a 
fault spends in the queueing system, 
comprised of the time spent in the 
pending queue and the time taken by 
the debugger to fix the fault. 
A comparison of these two measures is 
shown in Fig. 12. 
Utilization-related measure: 
 Personnel (i.e. debugger) utilization: 
the percentage of time that all allocated 
personnel spend on fault correction. 
 
4.2 Case study 1: DS-1  
 
In the first case study, we demonstrate the 
usage of 
PROCEDURE_PERFECT_DEBUGGING and 
PROCEDURE_IMPERFECT_DEBUGGING on 
dataset DS-1. Given different degrees of 
imperfect debugging, we analyze the 
appropriate staffing levels for a debugging team 
in terms of throughput, time consumption, and 
personnel utilization. We let  represent the 
degree of imperfect debugging (i.e. 
introduction_rate in 
PROCEDURE_IMPERFECT_DEBUGGING) 
and consider four cases: Case 1:  =0, Case 2:  
=0.1, Case 3:  =0.2, and Case 4:  =0.3. In 
addition, we assume all the faults in DS-1 are 
inherent, and simulate the fault detection and 
correction processes in Cases 1-4. The value of 
failure_rate is modeled by the rate function of 
the Goel-Okumoto model [23]; that is, the fault 
detection data in Table 1 are used to 
parameterize the Goel-Okumoto model via LSE. 
All the statistics for the rate function as 
predictions for one week ahead [2, 13]. That is, 
all the failure data prior to a specific time point 
are used to forecast the number of failures in the 
next time unit. Unlike failure_rate, service_rate 
is associated with each debugger, not the whole 
debugging team. We cannot estimate the value 
of service_rate via the cumulative number of 
removed faults in Table 1. Since the information 
related to the observation of introduced faults is 
not provided, we assume that service_rate = 
 9
personnel utilization increases with the growth 
of . Nevertheless, when the debuggers cannot 
handle the load, all debuggers are kept busy 
most of the time. Consequently, even though the 
higher introduction rate may lead to more 
introduced faults, personnel utilization may not 
increase significantly. If we let 90% be the 
acceptable degree of debugger utilization, the 
staffing levels that satisfy this requirement are 6 
debuggers for Cases 1-2, 7 debuggers for Case 3, 
and 8 debuggers for Case 4. 
To sum up, it appears that, for the 
debugging teams in Cases 1-4, the most cost-
effective staffing levels are 6, 6, 7 and 8 
personnel respectively. In other words, 
compared with the case of perfect debugging, 
the staffing needs may increase by 0% for  
=0.1, 16.67% for  =0.2, and 33.33% for  =0.3. 
 
4.3 Case study 2: DS-2  
 
In this case study, we utilize the data set 
DS-2. Similar to the first case study, we discuss 
the staffing needs for debugging systems in four 
cases, i.e. Case 1:  =0, Case 2:  =0.1, Case 3: 
 =0.2, and Case 4:  =0.3. We started the 
simulation with stop_time = 21, dt = 0.001, 
service_rate = 4, and exposing_rate = 4. The 
fault detection was simulated by using the rate 
function of the Inflected S-shaped model [23]; 
and the statistics were obtained by repeating the 
simulation 5,000 times and taking the average. 
Again, the staffing needs are discussed from the 
standpoints of throughput, time metrics, and 
personnel utilization. 
 
Throughput 
Table 6 shows the throughput-related 
measures for the debugging teams with various 
staffing levels. It is clear that, except for the 
number of inherent faults, other throughput-
related measures increase with the degree of 
imperfect debugging. Besides, the number of 
introduced faults and repaired faults both 
increase as the staff level increases, but the 
number of remaining faults is negatively 
correlated with the staffing level. Note that, in 
Case 4, increasing the staff level beyond 12 
debuggers will not improve the performance in 
Case 4. This may mean that a staff of 12 
debuggers is definitely sufficient if does not 
exceed 0.3. Fig. 16 shows the increase in 
detected faults and corrected faults versus time. 
Similar to Case study 1, the plots only include 
the cases of 1, 3, 5, 7, 9, and 11 debuggers. 
From Fig. 16, the number of repaired faults 
associated with more debuggers grows more 
rapidly in all cases. Considering the fault 
detection process, the gap between the growth 
curves widens as the degree of imperfect 
debugging increases. 
Fig. 17 traces the number of open-
remaining faults for Cases 1-4 when the staffing 
level ranges between 7 and 12. In the perfect 
debugging environment, we found that only 2 
detected faults were not removed by the end of 
21 weeks if 8 or more debuggers were allocated. 
Moreover, when the degree of imperfect 
debugging increases from  =0.1 to  =0.3, the 
staffing levels required to minimize the number 
of remaining faults are 9, 11, and 12, 
respectively. In terms of throughput, 8, 9, 11, 
and 12 debuggers are a suitable staffing levels 
for Cases 1, 2, 3, and 4 respectively. Fig. 18 
illustrates the effect of changes in  and the 
staffing level on the number of open-remaining 
faults. We observe that the number of remaining 
faults is positively correlated to , but 
negatively correlated to the staffing level. 
 
Time consumption 
Table 7 shows the variations in time-
related measures for different staffing levels in 
Cases 1-4. We observe that the higher the value 
of  , the longer will be the waiting time and 
response time. This is because a more 
significant imperfect debugging environment 
may increase the number of introduced faults 
and add to the the debugging team’s workload. 
In contrast, the debugging team with a higher 
staffing level may be able to handle more 
detected faults at the same time, so the time-
related measures decrease accordingly. Both 
time-related measures decrease significantly 
with the increase in staffing levels when the 
debugging teams in Cases 1, 2, 3, and 4 contain 
at most 6, 6, 7, and 8 personnel respectively. 
However, the improvement will be relatively 
minor if more personnel are assigned to fault 
correction tasks.  
 
Personnel utilization 
Table 8 shows the personnel utilization of 
each debugging team in Cases 1-4. We observe 
that, personnel utilization decreased 
monotonically with the growth of the debugging 
team in all situations. Besides, the utilization 
rate increases with the growth of . Note that, in 
most cases, the utilization is less than 90%. If 
good throughput-related and time-related 
measures are required, the corresponding 
debugger utilization may be unacceptable. 
Because most cases of debugger utilization in 
Table 8 are not desirable, a high requirement for 
debugger utilization may be meaningless. If we 
let 50% be the requirement for Cases 1, 2, 3, 
 11
degree of imperfect debugging 
between two projects). Note that 
we do not include the solution 
regarding Step 2 and Step 3 in this 
research.  
 
五、結果與討論 
 
In software development, some 
characteristics of a software project, such as the 
experience of the debuggers and the complexity 
of the software system, may impact the 
possibility of imperfect debugging. In order to 
cost-beneficially reduce the open-remaining 
faults, the project manager may need to adjust 
the staffing level of the debugging team based 
on the degree of imperfect debugging. Thus, in 
this research, we incorporate the single-queue 
multichannel queuing model with feedback into 
the rate-based simulation approach. The 
proposed procedures may approximate reality 
much more closely when modeling the testing 
and debugging activities. The application of the 
proposed approach was demonstrated via two 
case studies. In those studies, we considered the 
throughput, time consumption, and personnel 
utilization of debugging systems under different 
degrees of imperfect debugging when the 
number of available debuggers changes. From 
the analytical results, the project manager may 
gain insight into the staffing need adjustment 
according to the changes in the degree of 
imperfect debugging. Currently, we are 
investigating more measures that can 
objectively reflect the influence of imperfect 
debugging. The measures may help project 
mangers determine the staffing needs more 
accurately. We are also investigating a 
systematic approach to quantify the variations in 
imperfect debugging, and expect to present the 
research results in the near future. 
 
六、計畫相關論文發表 
 
Some papers related to this research have 
been accepted and published. They includes a  
journal paper and four conference papers: 
[J1]Chu-Ti Lin, “Analyzing the effect of 
imperfect debugging on software fault 
detection and correction processes via a 
simulation framework,” Mathematical and 
Computer Modelling, Vol. 54, No. 11-12, 
pp. 3046-3064, December 2011. (SCI) 
[C1]Chu-Ti Lin, Cheng-Ding Chen, Pei-Shan 
Wang, and Kai-Wei Tang, “Predicting the 
Number of Software Faults When Test 
Phase Transitions Occur in Software 
Development,” Proceedings of the 2011 
International Conference on Business And 
Information, July 2011. 
[C2]Chu-Ti Lin, Kai-Wei Tang, Jun-Ru Chang, 
and Chin-Yu Huang, “An Investigation into 
Whether the NHPP Framework Is Suitable 
For Software Reliability Prediction and 
Estimation,” Proceedings of the 2010 IEEE 
International Conference on Industrial 
Engineering and Engineering Management, 
pp. 626-630, December 2010. (EI) 
[C3]Kai-Wei Tang and Chu-Ti Lin, “測試套
件成本精簡框架 ,” Proceedings of 2011 
Joint Conference on Taiwan Software 
Engineering (TCSE) and Object-Oriented 
Technology and Applications, pp. 425-430, 
Taipei, Taiwan, July 2011. (Best Paper 
Award) 
[C4]Pei-Shan Wang and Chu-Ti Lin, “適用於
大型 multi-state weighted k-out-of-n 系統
之可靠度運算” Proceedings of the First 
National Conference on Web Intelligence 
and Applications, pp. 564-569, Kaohsiung, 
Taiwan, April 2011. 
 
Besides, the complete research result has 
been submitted to the premier journal, IEEE 
Transactions on Computers. 
 
七、參考文獻 
 
[1] M. Pezzè and M. Young, Software Testing 
and Analysis: Process, Principles, and 
Techniques, John Wiley & Sons, 2008. 
[2] M.R. Lyu, Handbook of Software 
Reliability Engineering, McGraw Hill, 
1996. 
[3] J.D. Musa, A. Iannino, and K. Okumoto, 
Software Reliability, Measurement, 
Prediction and Application, McGraw Hill, 
1987. 
[4] T. Dohi, S. Osaki, and K.S. Trivedi, An 
infinite server queueing approach for 
describing software reliability growth: 
unified modeling and estimation 
framework, in: Proceedings of the 11th 
Asia-Pacific Software Engineering 
Conference, pp. 110-119, Busan, Korea, 
December 2004. 
[5] C.Y. Huang and C.T. Lin, Reliability 
Prediction and Assessment of Fielded 
 13
[29] Q.P. Hu, M. Xie, and S.H. Ng, Early 
software reliability prediction with ANN 
models, in: Proceedings of the 12th IEEE 
International Symposium on Pacific Rim 
Dependable Computing, pp. 210-220, 
Riverside, USA, December 2006. 
[30] Q.P. Hu, M. Xie, S.H. Ng, and G. Levitin, 
Robust recurrent neural network modeling 
for software fault detection and correction 
prediction, Reliability Engineering and 
System Safety 92 (3) (2007) 332-340. 
[31] S.L. Ho, M. Xie, and T.N. Goh, A study of 
the connectionist models for software 
reliability prediction, Computer and 
Mathematics with Applications 46 (7) 
(2003) 1037-1045. 
[32] J. Zheng, Predicting software reliability 
with neural network ensembles, Expert 
Systems with Applications 36 (2) (2009) 
2116-2122. 
[33] Q.P. Hu, R. Peng, S.H. Ng, and H.Q. Wang, 
Simulation-based FDP & FCP analysis 
with queueing models, in: Proceedings of 
the 2008 IEEE International Conference 
on Industrial Engineering and Engineering 
Management, pp.1577-1581, Singapore, 
December 2008. 
[34] D. Gross and C. Harris, Fundamentals of 
Queueing Theory, third ed., John Wiley & 
Sons, 1998. 
[35] G. Antoniol, A. Cimitile, G.A. Di Lucca, 
and M. Di Penta, Assessing staffing needs 
for a software maintenance project through 
queuing simulation, IEEE Trans. on 
Software Engineering 30 (1) (2004) 43-58. 
[36] K.S. Trivedi, Probability and Statistics 
with Reliability, Queueing, and Computer 
Science Application, second ed., John 
Wiley and Sons, 2002. 
[37] S. Yamada, Software reliability growth 
models incorporating imperfect debugging 
with introduced faults, Electronics and 
Communications in Japan- Part III: 
Fundamental Electronic Science 81 (4) 
(1998) 33-41. 
[38] K. Tokuno and S. Yamada, An imperfect 
debugging model with two types of hazard 
rates for software reliability measurement 
and assessment, Mathematical and 
Computer Modelling 31 (2000) 343-352. 
[39] K. Tokuno and S. Yamada, Markovian 
software reliability measurement with a 
geometrically decreasing perfect 
debugging rate, Mathematical and 
Computer Modelling 38 (2003) 1443-1451. 
[40] V. Sridharan and P.R. Jayashree, Transient 
solutions of a software model with 
imperfect debugging and generation of 
errors by two servers, International Journal 
of Computer Mathematics 66 (1998) 217-
225. 
[41] L. Tian and A. Noore, Modeling 
distributed software defect removal 
effectiveness in the presence of code churn, 
Mathematical and Computer Modelling 41 
(2005) 379-389. 
[42] A.L. Goel and K. Okumoto, An analysis of 
recurrent software errors in a real-yime 
control system, in: Proceedings of the 
1978 Annual Conference, pp. 496-501, 
Washington, D.C., December 1978. 
[43] H. Pham and X. Zhang, An NHPP 
software reliability model and its 
comparison, International Journal of 
Reliability, Quality and Safety Engineering 
4 (3) (1997) 269-282. 
[44] S. Gokhale, M.R. Lyu, and K.S. Trivedi, 
Analysis of software fault removal policies 
using a non-homogeneous continuous time 
Markov chain, Software Quality Journal 
12 (2004) 211-230. 
[45] M. Xie and B. Yang, A study of the effect 
of imperfect debugging on software 
development cost, IEEE Trans. on 
Software Engineering 29 (5) (2003) 471-
473. 
[46] I. Sommerville, Software Engineering, 
Addison-Wesley, ninth ed., 2010. 
 
八、圖表 
 
 
Fig. 1. State transitions of a Pure Birth Non-homogeneous Continuous Time Markov Chain (Pure 
Birth NHCTMC) 
 15
 
 
 
 
 
CORRECTION (service_rate, dt) 
01: x ← a random number between 0 and 1 
02: if x   service_rate*dt 
03:  then return true 
04: else return false.  
Fig. 6. Pseudo code of Function CORRECTION( ) 
 
 
 
 
Enqueue: takes place when testers report newly detected faults
Dequeue: takes place when a fault is assigned to an unoccupied debugger 
 
Fig. 7. Software testing and debugging operations associated with imperfect debugging 
 
 
 
 
 
Fig. 8. A single-queue multichannel queueing system with feedback 
 
 
 
 
 17
 
Fault detection     Fault correction 
(a) Case 1: perfect debugging 
 
Fault detection     Fault correction 
(b) Case 2: introduction rate=0.1 
 
Fault detection     Fault correction 
(c) Case 3: introduction rate=0.2 
 
Fault detection     Fault correction 
(d) Case 4: introduction rate=0.3 
 
Fig. 13. The number of detected faults (both inherent and introduced faults) and the number of corrected 
faults versus time consumption (DS-1) 
 19
 
Fault detection     Fault correction 
(a) Case 1: perfect debugging 
 
Fault detection     Fault correction 
(b) Case 2: introduction rate=0.1 
 
Fault detection     Fault correction 
(c) Case 3: introduction rate=0.2 
 
Fault detection     Fault correction 
(d) Case 4: introduction rate=0.3 
 
Fig. 16. The number of detected faults (both inherent and introduced faults) and the number of corrected 
faults versus time consumption (DS-2) 
 21
 
 
 
Table 1. The first data set (DS-1) 
Weeks CDF1 CRF2 Weeks CDF1 CRF2 Weeks CDF1 CRF2 
1 
2 
3 
4 
5 
6 
12 
23 
43 
64 
84 
97 
3 
3 
12 
32 
53 
78 
7 
8 
9 
10 
11 
12 
109 
111 
112 
114 
116 
123 
89 
98 
107 
109 
113 
120 
13 
14 
15 
16 
17 
126 
128 
132 
141 
144 
125 
127 
127 
135 
143 
1 CDF denotes the cumulative number of detected faults. 
2 CRF denotes the cumulative number of removed faults. 
 
 
 
Table 2. The second data set (DS-2) 
Weeks CDF1 CRF2 Weeks CDF1 CRF2 Weeks CDF1 CRF2 
1 
2 
3 
4 
5 
6 
7 
2 
2 
2 
3 
4 
6 
7 
1 
2 
2 
3 
4 
4 
5 
8 
9 
10 
11 
12 
13 
14 
16 
29 
31 
42 
44 
55 
69 
7 
13 
17 
18 
32 
37 
56 
15 
16 
17 
18 
19 
20 
21 
87 
99 
111 
126 
132 
135 
136 
75 
85 
97 
117 
129 
131 
136 
1 CDF denotes the cumulative number of detected faults. 
2 CRF denotes the cumulative number of removed faults. 
 
 
 
Table 3. Comparison of the Throughput of Debugging Teams under Different Degrees of Imperfect 
Debugging (DS-1) 
Throughput of Debugging Team
Number of debuggersValues of  Types of Faults 1 2 3 4 5 6 7 8 9 10 11 12  
Inherent 144 144 144 144 144 144 144 144 144 144 144 144 144
Introduced 0 0 0 0 0 0 0 0 0 0 0 0 0
Repaired  25 50 77 107 127 144 144 144 144 144 144 144 144
Case 1:  
=0 
Remaining  119 94 67 37 17 0 0 0 0 0 0 0 0
Inherent 144 144 144 144 144 144 144 144 144 144 144 144 144
Introduced 1 3 5 7 9 10 10 10 10 10 10 10 10
Repaired 25 50 77 107 127 152 152 152 152 152 152 152 152
Case 2: 
=0.1 
Remaining  120 97 72 44 26 2 2 2 2 2 2 2 2
Inherent 144 144 144 144 144 144 144 144 144 144 144 144 144
Introduced 1 6 11 16 18 22 26 26 26 26 26 26 26
Repaired 25 50 77 107 127 154 167 167 167 167 167 167 167
Case 3: 
=0.2 
Remaining  120 100 78 53 35 12 3 3 3 3 3 3 3
Inherent 144 144 144 144 144 144 144 144 144 144 144 144 144
Introduced 6 14 20 27 31 35 43 45 45 45 45 45 45
Repaired  25 50 77 107 127 154 182 188 188 188 188 188 188
Case 4: 
=0.3 
Remaining  125 108 87 64 48 25 5 1 1 1 1 1 1
Note: The values in this table are evaluated based on the simulation by the end of 17 weeks. 
 
 
 23
 
Table 7. The Time-related Measures for the Debugging Teams under Different Degrees of Imperfect 
Debugging (DS-2) 
Time Metrics of Debugging Teams1 
Number of debuggers Values of  Types of Time 12 22 3 4 5 6 7 8 9 10 11 12 
Waiting — — 7.44 4.02 2.15 1.03 0.37 0.10 0.03 0.01 0.01 0.002 0.00Case 1: =0 Response — — 8.11 4.69 2.81 1.69 1.03 0.77 0.70 0.68 0.67 0.67 0.66
Waiting — — 8.92 5.10 2.94 1.58 0.73 0.23 0.08 0.03 0.015 0.008 0.00Case 2: =0.1 Response — — 9.59 5.77 3.61 2.26 1.40 0.90 0.75 0.70 0.69 0.68 0.67
Waiting — — 11.69 7.02 4.27 2.59 1.41 0.65 0.22 0.06 0.023 0.009 0.00Case 3: =0.2 Response — — 12.36 7.69 4.94 3.26 2.08 1.32 0.89 0.73 0.69 0.68 0.67
Waiting — — 16.12 10.31 6.86 4.62 3.14 2.05 1.23 0.61 0.28 0.10 0.00Case 4: =0.3 Response — — 16.78 10.97 7.52 5.29 3.81 2.71 1.89 1.27 0.94 0.77 0.66
1 The values in this table are all evaluated based on the simulation by the end of 21 weeks. 
2 Because most of the detected faults are not handled by the debuggers under these two staffing levels, the 
statistics are not meaningful. 
 
 
Table 8. Personnel utilization (DS-2) 
Utilization (%) 
Staffing level 
Values of  1 2 3 4 5 6 7 8 9 10 11 12 
Case 1: =0 84.67 77.26 73.75 71.15 68.61 66.15 62.75 55.18 49.05 44.14 40.13 36.79 —
Case 2: =0.1 89.29 80.60 76.48 73.29 70.61 68.41 66.14 62.34 55.55 49.97 45.45 41.66 —
Case 3: =0.2 89.29 80.60 76.48 73.92 72.06 70.08 68.50 66.65 63.68 58.00 52.73 48.34 —
Case 4: =0.3 90.52 82.83 77.40 74.56 72.62 71.02 69.30 67.89 66.66 65.44 62.60 57.64 —
Note: The values in this table are evaluated based on the simulation by the end of 21 weeks. 
 
二、與會心得 
BAI2011與會人士包含全球各地之商業與資訊領域的學者專家，大家齊聚一堂共同研討。除學術
與經驗之交流外，亦可藉此機會結識國際友人，增進彼此視野及後續之交流，實屬難得的經驗。 
 
三、考察參觀活動(無是項活動者略) 
無 
 
四、建議 
International Conference on Business and Information (BAI)歷年會議的論文發表內容包含商業與資
訊領域的理論、應用，及實務議題，是一個可以廣泛汲取商業與資訊領域相關新知，並增進國際交
流，拓展視野之會議，值得鼓勵國內學者積極參與。 
 
五、攜回資料名稱及內容 
BAI 2011國際會議提供一個由CD-ROM存放的會議論文集，內容收入此次會議所有發表的regular 
papers以及poster papers。其中，本人發表的論文收錄在此會議論文集中的Volumn-Technology and 
Innovation Management。 
 
六、其他 
 
以下為本人在會場拍攝的照片。 
 
 1
PREDICTING THE NUMBER OF SOFTWARE FAULTS WHEN 
TEST PHASE TRANSITIONS OCCUR IN SOFTWARE 
DEVELOPMENT 
 
Chu-Ti Lin, Cheng-Ding Chen, Pei-Shan Wang, and Kai-Wei Tang 
Department of Computer Science and Information Engineering, 
National Chiayi University, 
300, Syuefu Rd., Chiayi City 60004, Taiwan ROC 
{chutilin, s0990394, s0990401, s0980399}@mail.ncyu.edu.tw 
 
 
ABSTRACT 
 It is common to evaluate the quality of a software product by analyzing its 
reliability. In general, software reliability will increase with the going of testing. 
However, due to the budget limitation and time constraint, it is impractical to undergo 
a very long period of software testing. Thus, project managers should strike a balance 
between the cost of testing and the penalty of remaining faults. Software reliability 
growth models (SRGMs) can be used to predict the growth on the number of software 
faults, and further guide project managers to determine a cost-effective time to stop 
testing and to release the software product. So far, few existing SRGMs take into 
account the influence of test phase transition. In fact, test phase transition may 
significantly affect the growth of fault detections. Therefore, in this paper, we will 
propose a software reliability modelling framework. The SRGMs obtained from the 
proposed modelling framework will be able to address the influence of test phase 
transition. Finally, a failure data set collected from a real software project will be used 
to validate the models’ performance. 
 
Keyword: Non-homogeneous Poisson Process (NHPP), Optimal Software Release 
Time, Software Reliability, Software Reliability Growth Model (SRGM), Software 
Testing, Test Phase Transition, Testing-effort Consumption 
 
 
1. INTRODUCTION 
Because the applications of software have become diverse and important in recent 
decades, the complexity of software products has also increased rapidly. Thus, how to 
guarantee the quality of a large scale software product is a key challenge facing 
software development technique. Software reliability is one of most important 
measures of software quality. It can be defined as the probability of failure-free 
software operation for a specified period of time in a specified environment (Pham, 
2000; Xie, 1991). In software development, there exist many activities which are 
dedicated to reliability improvement of software products. Software testing is a 
critical and common activity to ensure the quality of software system. With the going 
of testing and debugging, fewer and fewer software bugs remain in the products, but 
more and more testing resources are spent as well. Delivering a product with many 
detects may lead to customers’ dissatisfaction and may further damage the reputation 
of a software company. However, due to the budget limitation and time constraint, the 
testing effort consumptions in a project are finite. It is very important to determine a 
cost-effective time moment, called optimal software release time, to stop software 
testing and deliver the product to market (Lin, & Huang, 2008; Pham, 2000; Xie, 
 3
SRGMs (Lyu, 1996; Musa et al., 1987; Pham, 2000; Xie, 1991;). It was modeled 
based on the following assumptions: 
(1.1) The software fault removal process follows the NHPP. 
(1.2) The failure occurrences of software system are caused by the remaining faults 
in the system at random times. 
(1.3) Each time a failure occurs, the fault associated with it will be removed 
immediately, and no new faults will introduced when fixing the system. 
(1.4) In the software system, all faults are mutually independent from the standpoint 
of failure detection. 
(1.5) The mean number of faults detected in the time interval (t, t+t] is 
proportional to the mean number of remaining faults in the system, and the 
proportionality is a constant. 
Based on the above assumptions, we obtain 
)],([)( tmar
dt
tdm   ,10,0  ra                (2) 
where )(tm means the expected number of cumulative detected faults in time period (0, 
t], a is the expected number of initial faults in the software system, and r is the fault 
detection rate. 
Solving Equation (2) under the boundary condition 0)0( m , the mean value function 
of the Goel-Okumoto model is given by  rteatm  1)( , a>0, 0<r<1.                (3) 
It forms an exponential growth curve, so that the failure intensity function, dm(t)/dt, 
decrease monotonically with the increase of t. 
Building upon the framework of NHPP, some other representative SRGMs were 
proposed according to some modified assumptions. Some other well-known SRGMs 
are shown as follows (Lyu, 1996; Musa et al., 1987; Pham, 2000; Xie, 1991): 
1) Yamada Delayed S-Shaped Model: This model was proposed by Yamada et 
al. (Lyu, 1996; Xie, 1991), and looks at the fault isolation process. The mean 
value function is shown as  rtertatm  )1(1)( , a>0, r>0.              (4) 
The software fault detection process modeled by such an S-shaped curve can be 
considered a learning process caused by the improvement in the testers' skills.  
2) Inflected S-Shaped Model: This model was proposed by Ohba (Lyu, 1996; 
Xie, 1991). Its main concept is that the observed software reliability growth 
becomes S-shaped if faults in a program are mutually dependent. The mean value 
function is given by 
rt
rt
ec
eatm 



1
1)( , a>0, r>0, c>0.              (5) 
3) Generalized Goel NHPP Model: This model is a variation of the GO Model 
and can also be called the generalized Goel-Okumoto Model (Lyu, 1996; Xie, 
1991). The mean value function is given as  crteatm  1)( , a>0, r>0, c>1.              (6) 
It can be used to model the phenomenon of the failure rate initially increasing and 
then later decreasing during the testing process. 
 
 
 5
and the Weibull function (Yamada et al., 1986). If we let N represents the total 
amount of expected testing effort expenditures, represents the scale parameter, 
and  represents the shape parameter, the three Weibull-type functions are 
formulated as follows. 
 Exponential function  
 teNtW  1)( .                    (8) 
 Rayleigh function:   2211)( teNtW   .                   (9) 
 Weibull function:    teNtW  1)( .                  (10) 
2) Review of Logisitc testing-effort function: Parr (Parr, 1980) ever proposed 
the Logisitc curve to model the resource consumption over the software 
development life cycle. Thus, Huang and Kuo (Huang & Kuo, 2002) applied the 
Logistic function to represent the testing-effort consumption curve, and showed 
that it can work better than the Weibull-type curves in a real data set. The Logistic 
function is given by 
teA
NtW  1)( ,                   (11) 
where A is a constant parameter. 
 
2.3 Incorporating testing-effort functions into software reliability modelling 
Because testing effort functions can reflect the testing resource consumption well, 
they have ever been incorporated into software reliability modeling to enhance the 
prediction capability. The SRGMs incorporated with testing-effort function are 
usually modeled based on the following assumption (Huang & Kuo, 2002; Lin & 
Huang, 2008; Yamada et al., 1986; Yamada et al., 1993): 
(2.1) The software fault removal process follows the NHPP. 
(2.2) The failure occurrences of software system are caused by the remaining faults 
in the system at random times.  
(2.3) Each time a failure occurs, the fault associated with it will be removed 
immediately, and no new faults will introduced when fixing the system. 
(2.4) The consumption of testing-effort is described by a testing-effort function, 
such as one of the Weibull-type functions and or Logistic function. 
(2.5) The mean number of faults detected in the time interval (t, t+t] by the current 
testing-effort consumptions is proportional to the mean number of remaining 
faults in the system. Besides, the proportionality is constant over time. 
Based on Assumption (2.5), we obtain 
)],([
)(
1)( tmar
twdt
tdm   ,10,0  ra            (12) 
where a is the expected number of initial faults in the software system, r is the fault 
detection rate per unit of testing effort, and w(t) is a testing effort function to model 
the instantaneous testing effort expenditures at time t. 
Solving Equation (12) under the boundary condition m(0)=0,  we can obtain  ))0()((1)( WtWreatm  ,                   (13) 
 7
(3.3) Each time a failure occurs, the fault associated with it will be removed 
immediately, and no new faults will introduced when fixing the system. 
(3.4) The consumption of testing-effort is described by a testing-effort function. 
(3.5) The mean number of faults detected in the time interval (t, t+t] by the current 
testing-effort consumptions is proportional to the mean number of remaining 
faults in the system. 
(3.6) The proportionality in Assumption (3.5) depends on the ease of test in the 
current test phase. 
Based on the above assumptions, the software reliability modelling associated with 
test phase transitions can be described as follows. 
1) Case of no transition: if no test phase transition occurs, we can obtain the 
differential equation which is similar to Equation (12), and the reliability model 
is accordingly obtained by  ))0()((0 1)( WtWreatm  .                  (14) 
2) Case of one transition only: in the case that one test phase transition 
happens during the software development, the ease of test may be different 
between the two test phases. Thus, we let t1 be the time moment when the test 
phase transits, and replace the fault detection rate r in Equation (12) by the 
following step function 




,,
,,
)(
1101
10
ttcrr
ttr
tr                    (15) 
where r0 is fault detection rate in the first test phase, and c1 is a parameter to 
reflect the change in the ease of test during the transition. As a result, Equation 
(12) is replaced by 
)],([)(
)(
1)(
1
1 tmatr
twdt
tdm   ,0a .          (16) 
Solving Equations (15) and (16) under the boundary condition m(0)=0, we have  
  


 

.,1
,,1
)(
1
))0()(())()((
1
))0()((
1 1011
0
ttea
ttea
tm WtWrtWtWr
WtWr
       (17) 
In fact, if more than one test phase transition occurs in software development, the 
fault detection rate function the mean value function can also be obtained through 
similar steps. 
 
 
4. NUMERICAL ANALYSIS 
 
4.1 Descriptions of Numerical Analyses 
In this section, the usage of the proposed framework will be illustrated through the 
data set discussed in Section 2.2. The models selected for performance comparisons 
are shown in TABLE 1. Among the six models selected for comparisons, Models 1-3 
are modelled according to the proposed framework. Considering the testing-effort 
functions reviewed in Section 2, only Equation (7) (i.e. the identity function), 
Equation (8) (i.e. the exponential function), and Equation (10) (i.e. the Weibulll 
function) are selected to describe the testing-effort consumption because they can fit 
the data patterns of testing-effort consumption well. We incorporate Equations (7), (8), 
and (10) into Equation (17), and obtain Models 1-3. The other models, i.e. Models 4-6, 
are some traditional models reviewed in Section 2.1. 
 9
patterns (i.e. the failure data during 1-35 weeks), and validate the model’s prediction 
capability based on the last 11 failure data patterns (i.e. the failure data during 36-46 
weeks). TABLE 3 shows the parameter estimations and the MSE values for the fitting 
part and the predicted part, respectively, while FIGURE 4(a)-4(f) demonstrate all 
models’ mean value functions with the actual failure data. Compared to other models, 
Models 1-3 not only give the better fitting ability again in the fitting part, but also 
provide the good MSE values for the predicted part. Additionally, FIGURE 4(a)-4(c) 
also indicate that the trend of fault detections predicted by Models 1-3 are very close 
to the actual data. Especially for Model 3, the prediction almost hit all actual data in 
the last seven weeks. Nevertheless, Models 4-5 overestimate the growth of fault 
detections in the last seven weeks, and Model 6 demonstrates a significant 
underestimation. On a whole, it is true that the proposed models demonstrate excellent 
capability of both data fitting and prediction in this case study.  
 
 
TABLE 2 
PARAMETER ESTIMATIONS AND FITTING CAPABILITY 
COMPARISONS 
Estimated Parameters Model 
N b m a r c1 
MSE 
Model 1 — — — 288.27 0.035925 0.469626 98.61 
Model 2 1643.01 0.057524 — 430.81 0.000347 0.220720 59.78 
Model 3 1652.95 0.058583 0.98937 425.66 0.000352 0.219805 60.37 
Model 4 — — — 423.72 0.023173 — 147.88 
Model 5 — — — 599.10 0.021142 0.887623 142.84 
Model 6 — — — 280.32 0.097213 — 345.30 
Note: the above estimations and criteria evaluation are obtained based on the 
failure data ranging from the 1st week to the 46th week. 
 
 
TABLE 3 
PARAMETER ESTIMATIONS AND PREDICTION CAPABILITY 
COMPARISONS 
Estimated Parameters1 Model N b m a r c1 
MSE2 
(fitting) 
MSE3 
(predicted)
Model 1 — — — 274.08 0.03865 0.444290 123.25 65.49
Model 2 1583.25 0.061525 — 689.98 0.000202 0.247786 71.62 38.66
Model 3 1510.43 0.056011 1.0687 562.71 0.000253 0.215426 70.99 13.31
Model 4 — — — 491.73 0.019055 — 171.85 131.27
Model 5 — — — 555.51 0.020423 0.929267 161.79 99.88
Model 6 — — — 245.94 0.114793 — 400.80 724.12
Note: 1 the above estimations are obtained based on the failure data ranging from the 
1st week to the 35th week 
2 this value means the sum of all deviations between the fitting curve and the 
actual data ranging from the 1st week to the 35th week 
3 this value means the sum of all deviations between the predicted curve and 
the actual data ranging from the 36th week to the 46th week 
 
 11
 
 
 
 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 1 
․․․․ Actual data
 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 2 
․․․․ Actual data
 
(a) Model 1       (b) Model 2 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 3 
․․․․ Actual data
 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 4 
․․․․ Actual data
 
(c) Model 3       (d) Model 4 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 5 
․․․․ Actual data
 
 
10 20 30 40
50
100
150
200
250
Num. of de te cte d fa ults  
Time  units  (Days )
|- - - - - Module  Te s t - - - - | - - - - - Combina tion Tes t - - - -|
Fitting pa rt P re dicte d pa rt 
──── Model 6 
․․․․ Actual data
 
(e) Model 5       (f) Model 6 
FIGURE 4.  ACTUAL FAILURE DATA AND MEAN VALUE FUNCTIONS OF ALL 
SELECTED MODELS (FITTING FROM THE 1ST WEEK TO THE 35TH WEEK).  
 
 13
Lin, C. T. & Huang, C. Y. 2008. Enhancing and measuring the predictive capabilities 
of testing-effort dependent software reliability models. Journal of Systems and 
Software, 81(6): 1025-1038. 
Lyu, M. R. & Nikora, A. 1992. Applying software reliability models more effectively. 
IEEE Software, 9: 43-52. 
Lyu, M. R. 1996. Handbook of software reliability engineering. McGraw Hill. 
Malaiya, Y. K., Mayrhauser, A. von, & Srimani, P. K. 1993. An examination of fault 
exposure ratio. IEEE Trans. on Software Engineering, 19(11): 1087-1094. 
Musa, J. D., Iannino, A., & Okumoto, K. 1987. Software reliability, measurement, 
prediction and application, McGraw Hill. 
Okamura, H., Furumura, H., & Dohi, T. 2006. On the effect of fault removal in 
software testing-Bayesian reliability estimation approach. Paper presented at 
the 17th International Symposium on Software Reliability Engineering, 
Raleigh, North Carolina. 
Parr, F. N. 1980. An alternative to the Rayleigh curve for software development effort. 
IEEE Trans. Software Engineering, 6(3): 291-296. 
Pfleeger, S. L. & Atlee, J. M. 2006. Software engineering: theory and practice (3rd 
ed.). Pearson Practice Hall.  
Pham, H. 2000. Software reliability. Springer-Verlag. 
Putnam, L. 1978. A general empirical solution to the macro software sizing and 
estimating problem. IEEE Tran. Software Engineering, 4 (4): 345-361. 
Shepperd, M. & Schofield, C. 1997. Estimating software project effort using 
analogies. IEEE Trans. Software Engineering, 23(1): 736-743. 
Sommerville, I. 2007. Software engineering (8th ed.). Pearson Education. 
Srinivasan, K. & Fisher, D. 1995. Machine learning approaches to estimating software 
development effort. IEEE Trans. Software Engineering, 21(2): 126-136. 
Tohma, Y., Jacoby, R., Murata, Y., & Yamamoto, M. 1989. Hyper-geometric 
distribution model to estimate the number of residual software faults. Paper 
presented at the 13th Annual International Computer Software and 
Applications Conference, Orlando. 
Tohma, Y., Tokunaga, K., Nagase, S., & Murata, Y. 1989. Structural approach to the 
estimation of the number of residual software faults based on the 
hyper-geometric distribution. IEEE Trans. Software Engineering, 15(3): 
345-355. 
Xie, M. 1991. Software reliability modeling. World Scientific Publishing Company. 
Yamada, S., Hishitani, J., & Osaki, S. 1993. Software reliability growth with a 
weibull test-effort: a model & application. IEEE Trans. on Reliability, 42(1): 
100-106. 
Yamada. S., Ohtera, H., & Narihisa, H. 1986. Software reliability growth models with 
test-effort. IEEE Trans. on Reliability, 35(1): 19-23. 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：林楚迪 計畫編號：98-2218-E-415-002-MY2 
計畫名稱：運用模擬方法評估與管理軟體測試與除錯 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
榮獲 2011 Joint
Conference on 
Taiwan Software 
Engineering and 
Object-Oriented 
Technology and 
Applications 最
佳論文獎 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
國外 論文著作 期刊論文 1 2 100% 篇 
部分成果已經發
表 於
Mathematical and 
Computer 
Modelling，該期
刊刊登許多軟體
可靠度工程領域
重 要 學 者 的 著
作，是一個受到高
度重視的學術刊
物。此外，本研究
的完整成果亦已
投稿至電腦工程
領域的頂尖期刊
IEEE 
Transactions on 
Computes。 
