 2
化技術。其中一種是使用經驗法則 (heuristic)，在可行解範圍逐步尋求最佳化。此
類演算法包括基因演算法 (Genetic algorithm) [1, 2]、模擬退火演算法 (Simulated 
annealing) [3, 4]、螞蟻族群演算法 (Ant colony algorithm) [5]、粒子群最佳化 
(Particle Swarm Optimization) [6, 7]… 等等。這類方法藉由模擬自然界的運作來達
到最佳化目的。這類方法不再受限於目標函數的數學特性，可以應用於非線性、不
可微分、或是不連續函數。無法用數學函數描述的問題，都可以設計模型，根據模
擬得到的回饋進行最佳化演算。只要兩組解的優勝劣敗能夠被某種方式比較，甚至
連不存在目標函數的問題也能適用，例如：個人化之樂音片段產生 [8]。此類演算
法的可行性與實用性非常高，具有一定的求解能力，在有限時間內通常可以獲得在
品質方面可被接受解，因此漸漸地被廣泛應用於現實世界問題。 
四、 研究方法 
1. 變數型態之研究與分析 
以目前現有的許多最佳化問題而論，我們依據常見的參數型態給予分
類並討論分析。舉以一個小偷的背包問題為例子，分別對三種型態問題作
一情境模擬。此問題設定是，小偷的背包有固定的重量限制，而現在有金、
銀、銅三種不同材質的製品，其重量跟單價都不一樣，小偷該如何選擇才
能在條件限制下獲得最大利益。 
 布林值 (Boolean values) 
布林變數常見的被使用在決策性變數上，已經確知有數個選項，
每個選項可以用單一布林變數來表示選取或不選取。布林變數的問題
通常也就是一般的排列組合問題。當小偷問題中的三種製品都只有一
個時，即可用三個布林變數分別表示要帶走或不帶走情況，此即為典
型的布林參數問題。 
 整數 (Integers) 
整數是處理離散資料的型態，一些對應到實體個數的參數問題常
常就必須用整數來表示。若小偷的背包問題中，三種製品分別都有一
個以上之數量，則可以用三個整數參數來記錄，構成整數參數最佳化
的問題。 
 實數 (Real numbers) 
現實世界的工程問題大多是運作在實數域上，因此實數參數也就
是最常被使用的型態，通常我們可以用實數向量來表示一組問題解。
因為實數的連續性，除了在特定的問題類型之下 (例如：線性規劃問
題或是可以實數近似之最佳化問題)，實數最佳解的搜尋經常比布林
與整數型態的解還來得困難許多。假想在小偷的背包問題中，如果小
偷有工具可以對三種製品做切割動作，那此問題就必須使用實數參數
來表示帶走某種製品的數量，此問題則轉為實數最佳化問題。 
 4
表格 2: 邊際乘積機率模型之整數範例 
目前族群 樣式 次數 
3472 
1624 
0314 
6715 
4360 
7164 
0*0* 
0*1* 
0*2* 
... 
... 
7*6* 
7*7* 
0 
1 
0 
. 
. 
1 
0
而整數延伸式精簡基因演算法修改邊際乘積機率模型統計對象
為整數參數，對於同樣一組群組 s = [1, 3, 4]，若整數範圍 d = u-l，
則總共要對 d|s| 種不同樣式作出現次數統計，如表格 2所示。除了修
改邊際乘積機率模型以符合整數特性之外，模型複雜度估計的運算公
式也必須加以修改。整數延伸式精簡基因演算法將二進位布林參數樣
式的兩種情形擴展到整數範圍的 d 種情形，因此 Model Complexity
公式修正為公式 (1)。而 Compressed Population Complexity和其他部
分的機制都和原延伸式精簡基因演算法相同。 



m
i
sidN
1
2log  Complexity Model
     
(1) 
2.2 實數延伸式精簡基因演算法 
在討論實數延伸式精簡基因演算法之前，必須先介紹本實驗團隊
過去所成功發展的連續值域適應性之離散化演算法「隨選分割」 
(Split-on-demand, SoD)。此演算法將一連續值域分割成數個區間，使
得每個區間內的搜尋個體數目小於 N*λ，其中N為族群大小、λ為分
割比率，可以用來平衡全域搜尋 (Global search) 跟區域搜尋 (Local 
search) 的強度與比重，也就是試圖在探索  (Exploration) 和利用 
(Exploitation) 間找到適當的平衡。圖表 1 顯示一組隨選分割的範
例。經過隨選分割處理，可以技巧性地將實數離散化為整數。 
 
2 3 1
-100 
 
圖表 1: 隨選分割範例 
延伸式精簡基因演算法原本是設計處理二進位資料的方法，為了
能夠處理實數參數，我們將隨選分割機制整合在整數延伸式精簡基因
演算法的流程中。因此實數延伸式精簡基因演算法架構中的個體分別
有實數向量和整數向量兩種基因態，在目標函數的評估運算和存活個
 6
 參與人員獲得以下之訓練： 
 培養研究生分工合作之能力； 
 訓練參與人員研究、統合與論文寫作能力； 
 統整研究成果並發表學術論文； 
 學習實作系統之實務經驗； 
 強化參與人員之資料分析、演化計算、機械學習、數值分析與最佳化技術
等相關技能。 
 設計最佳化演算架構: 提出可適用於含有各種不同型態決策變數之問題的新
型最佳化技術，以因應真實世界狀況中高度複雜之工程問題與困難。 
 實作最佳化計算架構: 將所提出之技術，實作為獨立的最佳化工具與服務，以
供本計畫之相關人員，甚至是其他研究領域之人員分享與使用。已完成之原始
程式碼，可由此網址下載： 
http://nclab.tw/SM/2010/01/ 
 撰寫報告並投稿論文。基於國科會之補助，本實驗室發表了以下的相關論文： 
 期刊論文： 
 Chuang, C.-Y., & Chen, Y.-p. (2010). Sensibility of linkage information 
and effectiveness of estimated distributions. Evolutionary Computation, 
18(4). doi: 10.1162/EVCO_a_00010. (SCI). 
 Chen, Y.-p., & Jiang, P. (2010). Analysis on the facet of particle 
interaction in particle swarm optimization. Theoretical Computer Science, 
411(21), 2101–2115. doi: 10.1016/j.tcs.2010.03.003. (SCI, EI). 
 會議論文： 
 Huang Y.-w. & Chen, Y.-p. (2010). Detecting General Problem 
Structures with Inductive Linkage Identification. In Proceedings of the 
2010 Conference on Technologies and Applications of Artificial 
Intelligence (TAAI 2010). (Accepted). 
 Lin J.-H. & Chen, Y.-p. (2010). XCS with Bit Masks. In Proceedings of 
the 2010 Conference on Technologies and Applications of Artificial 
Intelligence (TAAI 2010). (Accepted). 
 Chen, Y.-p. (2010). Estimation of distribution algorithms: Basic ideas 
and future directions. In Proceedings of World Automation Congress 
2010 (WAC 2010) (pp. IFMIP–152). (Invited). 
 Chen, C.-M., Chen, Y.-p., Shen, T.-C., & Zao, J. (2010). On the 
optimization of degree distributions in LT codes with covariance matrix 
adaptation evolution strategy. In Proceedings of 2010 IEEE Congress on 
Evolutionary Computation (CEC 2010) (pp. 3531–3538). doi: 
10.1109/CEC.2010.5586202. (EI). 
 8
附錄 
期刊論文： 
1. Chuang, C.-Y., & Chen, Y.-p. (2010). Sensibility of linkage information and 
effectiveness of estimated distributions. Evolutionary Computation, 18(4). doi: 
10.1162/EVCO_a_00010. (SCI). 
2. Chen, Y.-p., & Jiang, P. (2010). Analysis on the facet of particle interaction in 
particle swarm optimization. Theoretical Computer Science, 411(21), 2101–2115. 
doi: 10.1016/j.tcs.2010.03.003. (SCI, EI). 
會議論文： 
3. Huang Y.-w. & Chen, Y.-p. (2010). Detecting General Problem Structures with 
Inductive Linkage Identification. In Proceedings of the 2010 Conference on 
Technologies and Applications of Artificial Intelligence (TAAI 2010). (Accepted). 
4. Lin J.-H. & Chen, Y.-p. (2010). XCS with Bit Masks. In Proceedings of the 2010 
Conference on Technologies and Applications of Artificial Intelligence (TAAI 2010). 
(Accepted). 
5. Chen, Y.-p. (2010). Estimation of distribution algorithms: Basic ideas and future 
directions. In Proceedings of World Automation Congress 2010 (WAC 2010) (pp. 
IFMIP–152). (Invited). 
6. Chen, C.-M., Chen, Y.-p., Shen, T.-C., & Zao, J. (2010). On the optimization of 
degree distributions in LT codes with covariance matrix adaptation evolution 
strategy. In Proceedings of 2010 IEEE Congress on Evolutionary Computation 
(CEC 2010) (pp. 3531–3538). doi: 10.1109/CEC.2010.5586202. (EI). 
7. Chen, C.-M., Chen, Y.-p., Shen, T.-C., & Zao, J. (2010). Optimizing degree 
distributions in LT codes by using the multiobjective evolutionary algorithm based 
on decomposition. In Proceedings of 2010 IEEE Congress on Evolutionary 
Computation (CEC 2010) (pp. 3635–3642). doi: 10.1109/CEC.2010.5586340. (EI). 
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
entire structure of the problem at a given time because the set of selected solutions on
which the probabilistic model is built contains insufficient information regarding some
parts of the problem and renders EDAs incapable of processing these parts accurately.
This paper starts by observing the evolutionary process of an EDA when dealing
with an exponentially scaled problem, and recognizing that the population on which
the probabilistic model is built does not necessarily contain sufficient information for all
problem structures to be detected completely and accurately. Based on this observation,
this study proposes a general concept that estimated probabilistic models should be
inspected to reveal the effective search directions, and we provide a practical approach
that utilizes a reserved set of solutions to examine the built model for the fragments
that may be inconsistent with the actual problem structure. Furthermore, the proposed
approach is implemented on the extended compact genetic algorithm (ECGA; Harik,
1999) and experimented on several sets of additively separable problems with different
scaling difficulties (Goldberg, 2002) to demonstrate the applicability.
The following section briefly reviews the research topics concerning this study. Sec-
tion 3 then demonstrates the interaction between the scaling difficulty and probabilistic
model building performed by EDAs.More specifically, wewill investigate how the scal-
ing difficulty shadows the ability of EDAs to recognize problem structures and causes
inaccurate processing on the part of some solutions. Accordingly, a general approach
will be proposed in Section 4 to resolve this issue and enforce accurate processing dur-
ing the optimization process. In Section 5, an implementation of the proposed approach
on the extended compact genetic algorithm will be detailed. Section 6 presents the
empirical results, followed by discussion and analysis in Section 7. Finally, Section 8
concludes the paper.
2 Background
Genetic algorithms (GAs; Holland, 1992; Goldberg, 1989) are search techniques loosely
based on the paradigm of natural evolution, in which species of creatures tend to adapt
to their living environments through mutation and inheritance of useful traits. Ge-
netic algorithms mimic this mechanism by introducing artificial selections and genetic
operators to discover and recombine partial solutions. By properly growing and mix-
ing promising partial solutions, which are often referred to as building blocks (BBs;
Goldberg, 2002), GAs are capable of efficiently solving a host of problems. The ability
to implicitly process a large number of partial solutions has been recognized as an im-
portant source of the computational power of GAs. According to the Schema theorem
(Holland, 1992), short, low-order, and highly fit subsolutions increase their share in the
final combined solution. Further, as stated in the building block hypothesis (Goldberg,
1989), GAs implicitly decompose a problem into subproblems by processing building
blocks. This decompositional bias is a good strategy for tackling many real-world prob-
lems, because real-world problems can oftentimes be reliably solved by combining the
pieces of promising solutions in the form of problem decomposition.
However, proper growth and mixing of building blocks are not always achieved.
GAs in the simplest form employ fixed representations and problem-independent re-
combination operators, which often breaks promising partial solutions while perform-
ing crossovers. This can cause crucial building blocks to vanish, thus leading to a
convergence to local optima. In order to overcome this building block disruption prob-
lem, various techniques have been proposed. In this study, we focus on one line of effort
often called the estimation of distribution algorithm (EDA;Mu¨hlenbein and Paaß, 1996;
2 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
on the convergence behavior of exponentially scaled problems (Thierens et al., 1998),
an extension of that model to building blocks more than one variable long (Lobo et al.,
2000), and a convergence model of linkage learning genetic algorithms (LLGAs; Harik,
1997) on problems with different scaling setups (Chen and Goldberg, 2005).
Although the aforementioned scaling difficulty exists in a number of problems
and degrades the performance of many evolutionary algorithms (EAs), there are scant
investigations concerning the behavior of EDAs in the presence of scaling difficulties.
Therefore, this study attempts to explore how the scaling difficulty affects EDAs, and
proposes a practical countermeasure to assist EDAs on problemswith different scalings.
Specifically, we propose the notion that the estimated probabilistic models should be
examined to enforce accurate processing of building blocks and prevent random drift
from taking place. In the remainder of this paper, our approach will be demonstrated
and evaluated on the test problems constructed by concatenating several trap functions.
A k-bit trap function is a function of unitation2 which can be expressed as
ftrapk (s1s2 · · · sk) =
{
k, if u = k
k − 1 − u, otherwise ,
whereu is the number of ones in the binary string s1s2 · · · sk . The trap functionswereused
pervasively in the studies concerning EDAs and other evolutionary algorithms because
they provide well-defined structures among variables, and the ability to recognize
intervariable relationships is essential to solve the problems consisting of traps (Deb
and Goldberg, 1993, 1994).
3 Linkage Sensibility
The ability of EDAs to handle the building block disruption problem comes primarily
from the explicit modeling of selected promising solutions using probabilistic models.
The model construction algorithms, though they differ in their representative power,
capture the likely structures of good solutions by processing the population-wise statis-
tics collected from the selected solutions. By reasoning the dependencies among differ-
ent parts of the problem and the possible formations of good solutions, reliable mixing
and growing of building blocks can be achieved. As noted by Harik (1999), learning
a good probability distribution is equivalent to learning linkage, where linkage refers
to the dependencies among variables. Bosman and Thierens (1999) further recognized
that in order to achieve reliable optimization, linkage information should be utilized
in a way such that each corresponding building block can be identified and used as a
whole.
Inmost studies on EDAs, it is presumed that EDAs can detect linkage and recognize
building blocks according to the information contained in the set of selected solutions.
However, in this study, we argue that in some situations, accurate and complete linkage
information cannot be acquired by distribution estimation because the selected set of
solutions on which the model is built contains insufficient information on the lower
fitness parts of the problem. For example, consider a 16-bit maximization problem
2A function in which the function value depends only on the number of ones in the binary input
string.
4 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
that the disruption problem still exists in the insensible portion of the problem because
that part of the problem cannot be modeled properly. Although the above example is
an extreme case of scaling, in that each subproblem is exponentially scaled, in real-
world problems, it is often the case that the constitutive subproblems are weighted
significantly differently, which implies that the linkage might be only partially sensible.
In addition to the building block disruption problem, the random drift of the less
salient parts of the problemmentioned in Section 2 further worsens the situation. These
situations and issues are usually handled by increasing population size when EDAs are
adopted. However, we may gain a newway to deal with these situations if it is possible
to distinguish a sensible linkage from an insensible linkage.
4 Effective Distributions
The idea of sensible linkage can be closely mapped into another notion called effective
distributions. By effective distributions, we mean that by sampling these distributions,
the solution quality can be reliably advanced.Hence, the crucial criteria for effective dis-
tributions are the consistency with building blocks and the provision of good directions
for further search. If it is possible to extract effective marginal distributions from the
built probabilistic model, we can perform partial sampling using only these marginal
distributions, and leave the remainingparts of the solutions unchanged. Thus, the diver-
sity is maintained and we are free from the building block disruption and random drift
problems. For instance, returning to the earlier 16-bit optimization problem, if it is pos-
sible to identify those partial models that are built on the sensible linkage like [s1 s2 s3 s4]
in the first generation and [s5 s6 s7 s8] in the second generation, we can sample only the
correspondingmarginal distributionswhich are, in this case, effective. That is, in the first
generation, for each solution string, we resample only s1s2s3s4 according to themarginal
distribution and keep s5s6 · · · s16 unchanged. In the second generation,we resample only
s1 to s8 according to themarginal distributions and keep s9s10 · · · s16 with the same values
(note that s1s2s3s4 are converged). In this way, we do not have to resort to increasing the
population size todealwith theproblems causedby thedisparate buildingblock scaling.
The above thoughts leave us one complication: the identification of effective distri-
butions. However, the direct identification of effective distributions may be a difficult
if not impossible task. It may be wise to adopt a complementary approach—to iden-
tify those marginal distributions that are not likely to be effective. If there is a way to
identify the ineffective distributions, we can bypass them and use only the rest of the
probabilistic model, and thus approximate the result of knowing effective distributions.
Our idea is that we can split the entire population into two subpopulations, use only
one of the subpopulations for building the probabilistic model, and utilize the other
subpopulation to collect some statistics for possible indications of ineffectiveness of cer-
tain marginal distributions in the probabilistic model built on the first subpopulation.
That is, with some appropriate heuristics or criteria, we can prune the likely ineffective
portions of the model.
In the next section, our implementation in ECGA of the proposed concept will
be detailed. More specifically, a judging criterion will be proposed to detect the likely
ineffective marginal distributions of a given marginal product model.
5 ECGA with Model Pruning
This section starts with a brief review of the (ECGA; Harik, 1999). Based on the idea
of detecting the inconsistency of statistics gathered from two subpopulations of the
6 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
subsets each of size ki , i = 1 . . . m, such that  =
∑m
i=1 ki . Then the marginal distribution
corresponding to the ith variable subset requires 2ki − 1 frequency counts to be com-
pletely specified. Taking into account that each frequency count is of length log2(n + 1)
bits, where n is the population size, the model complexity, Cm, can be defined as
Cm = log2(n + 1)
m∑
i=1
(
2ki − 1) .
The compressed population complexity, Cp, quantifies the suitability of the model
in terms of the number of bits required to store the entire selected population (the
set of promising solutions picked by the selection operator) under an ideal compres-
sion scheme. The compression scheme is based on the partition of the variables. Each
subset of the variables specifies an independent “compression block” on which the
corresponding partial solutions are optimally compressed. Theoretically, the optimal
compression method encodes a message of probability pi using − log2 pi bits. Thus,
taking into account all possible messages, the expected length of a compressed mes-
sage is
∑
i −pi log2 pi bits, which is optimal. In information theory (Cover and Thomas,
1991), the quantity − log2 pi is called the information of that message and
∑
i −pi log2 pi
is called the entropy of the corresponding distribution. Based on information theory, the
compressed population complexity, Cp, can be derived as
Cp = n
m∑
i=1
2ki∑
j=1
−pij log2 pij ,
where pij is the frequency of the j th possible partial solution to the ith variable subset
observed in the selected population.
Note that in the calculation ofCp, it is assumed that the j th possible partial solution
to the ith variable subset is encoded using − log2 pij bits. This assumption is funda-
mental to our technique of identifying the likely ineffective marginal distributions.
More precisely, the information of the partial solutions, − log2 pij , is a good indicator of
inconsistency of statistics gathered from two separate subpopulations.
5.2 Model Pruning
Our technique of identifying the possibly ineffective fragments of a marginal product
model is based on the notion that ECGA uses compression performance to quantify the
suitability of aprobabilisticmodel for a given set of solutions. Thedegree of compression
is a quite representative metric to the fitness of modeling, because all good compression
methods are based on capturing and utilizing the relationships among data (Gru¨nwald,
2007). Thus, if the compression scheme of the MPM built on one set of solutions is
incapable of compressing another set of solutions produced under the same condition,5
then we can speculate that some of the constitutive marginal models observed in the
first set of solutions are likely inconsistent with the distribution of the corresponding
partial solutions observed in the second set of solutions. Such inconsistency can be seen
5For example, if all individuals are produced by sampling the same probabilistic model and selected
using the same selection technique under the same pressure.
8 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
Algorithm 1 ECGA with Model Pruning
Initialize a population P with n solutions of length .
while the stopping criteria are not met do
Evaluate the solutions in P .
Divide P into two subpopulations S and T at random.
S ′ ← apply t-wise tournament selection on S.
T ′ ← apply t-wise tournament selection on T .
M ← build the MPM on S ′ with greedy search.
M ′ ← prune M based on the inconsistency with T ′.
for each remaining marginal distribution D in M ′ do
for each solution s = s1s2 · · · s in P do
Change the values in s partially by sampling D.
end for
end for
end while
T ′ to locate the possible drift portions of the solutions and identify the likely ineffective
parts within the whole model. By removing these likely ineffective parts, we can forge
a partial but more effective model.
An issue in practice concerning the calculation of the inequality is that sometimes
one or more possible partial solutions are absent in the set of selected solutions, leaving
− log2 pij undefined because pij = 0. In the present work, we handle this practical
problem by assigning a very small value, smaller than 1/n, to the pij ’s that are zero and
normalizing them such that pij ’s sum to 1 (i.e.,
∑
j pij = 1).
5.3 Integration
In this section, the optimization process incorporating ECGA and the proposed tech-
nique is described. This combination helps ECGA to achieve better performance when
a disparate scale exists among different parts of the problem.
The procedure is presented in Algorithm 1. This process starts with initializing a
population of solutions. After initialization, the solutions are evaluated, and then the
entire population is randomly split into two subpopulations. Selection operations are
performed on the two subpopulations separately with the same operator and selec-
tion pressure. Model building is performed on one of the subpopulations. The other
subpopulation is used to prune the built model using the technique described pre-
viously. Finally, all solutions in the population are altered by sampling the remaining
marginal distributions,which are considered effective, in the prunedmodel. These steps
are repeated until the stopping criteria are satisfied.
A prominent difference between the above process and the regular EDAs is that
the sampling might not include all variables. As introduced in Section 4, the existing
solutions are altered by sampling only the marginal distributions surviving the model
pruning process. Thus, a solution string might not be entirely modified in an iteration.
This technique hence avoids random drift and inaccurate processing of low-fitness
building blocks by postponing the processing until sufficient linkage information is
available. Similar to the concept proposed by Bosman and Thierens (1999) that link-
age information estimated from the selected solutions has to be utilized to recognize
10 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
Table 4: Marginal product models before and after pruning when solving a 16-bit
problem of the overloaded scaling with the proposed approach.
Generation Marginal product model (before and after pruning)
1 Before [s1 s2 s3 s4] [s5 s6 s7 s8] [s9 s16] [s10 s14 s15] [s11 s13] [s12]
After [s1 s2 s3 s4] [s5 s6 s7 s8]
2 Before [s1 s2 s3 s4] [s5 s6 s7 s8] [s9 s13 s14] [s10 s12] [s11 s15] [s16]
After [s1 s2 s3 s4] [s5 s6 s7 s8]
3 Before [s1] [s2] [s3] [s4] [s5] [s6] [s7] [s8] [s9 s10 s11 s12] [s13 s14 s15 s16]
After [s1] [s2] [s3] [s4] [s5] [s6] [s7] [s8] [s9 s10 s11 s12] [s13 s14 s15 s16]
4 Before [s1] [s2] [s3] [s4] [s5] [s6] [s7] [s8] [s9 s10 s11 s12] [s13 s14 s15 s16]
After [s1] [s2] [s3] [s4] [s5] [s6] [s7] [s8] [s9 s10 s11 s12] [s13 s14 s15 s16]
bound the scaling performance of an algorithm at two extremes, the power law cases
enable us to see the behavior in between. Based on the different scalings, three sets of
test functions are constructed using ftrapk as the elemental function:
Exponential:
m−1∑
i=0
(k + 1)iftrapk (sk×i+1sk×i+2 · · · sk×i+k)
Power law:
m−1∑
i=0
(i + 1)3ftrapk (sk×i+1sk×i+2 · · · sk×i+k)
Uniform:
m−1∑
i=0
ftrapk (sk×i+1sk×i+2 · · · sk×i+k)
By adopting different scaling setups, we can compare the original ECGA with our
approach under different degrees of linkage sensibilities. By varying k and m, we can
observe the behavior of the proposed method with respect to different problem and
subproblem sizes in a controlled manner. Furthermore, various selection pressures are
also taken into consideration t make a more thorough observation.
The purpose of the following experiments is to understand the impact of the pro-
posed method on the computational resource (population size and function evaluations)
required to solve a problem. Thus, we do not use solution quality as a measure of
comparison but treat it as a minimum requirement. More precisely, we use a bisection
method (Sastry, 2001) to bound the minimum population size capable of achieving
reliable convergence to the optimum. Of course, solution quality can be an important
indicator for evaluating a newly invented approach. However, the primary goal of this
study is to design a more economic approach for solving problems, and the experiments
are designed to evaluate the ability of the proposed approach in this aspect.
6.1 Effect of Selection Pressure
This section describes the experiments designed for observing the effect of selection
pressure on both the original ECGA and the ECGA combined with the proposed ap-
proach. The purpose of these experiments is twofold.
• First, we want to determine the range of selection pressure with which the pro-
posed approach works as we designed. Appropriate selection pressure is quite
12 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
8 12 16 20 24
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
5500
Tournament Sizes
P
op
ul
at
io
n 
S
iz
es
ECGA, = 40
ECGA, = 80
ECGA+MP, = 40
ECGA+MP, = 80
(a) Population Sizes
8 12 16 20 24
0
1
2
3
4
5
6
7
8
9
10
11
12
x 104
Tournament Sizes
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA, = 40
ECGA, = 80
ECGA+MP, = 40
ECGA+MP, = 80
(b) Function Evaluations
Figure 2: Empirical results of theproposedmethodandoriginal ECGAon40- and80-bit
(k = 4,m = 10 and 20) power law scaled problems. Five tournament sizes ranging from
8 to 24 were used to observe the behavior of the algorithms under different selection
pressures.
8 12 16 20 24
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
5500
Tournament Sizes
P
op
ul
at
io
n 
S
iz
es
ECGA, = 40
ECGA, = 80
ECGA+MP, = 40
ECGA+MP, = 80
(a) Population Sizes
8 12 16 20 24
0
1
2
3
4
5
6
7
8
9
10
11
12
x 104
Tournament Sizes
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA, = 40
ECGA, = 80
ECGA+MP, = 40
ECGA+MP, = 80
(b) Function Evaluations
Figure 3: Empirical results of theproposedmethodandoriginal ECGAon40- and80-bit
(k = 4, m = 10 and 20) uniformly scaled problems. Five tournament sizes ranging from
8 to 24 were used to observe the behavior of the algorithms under different selection
pressures.
are listed in Table 5. This demonstrates that adopting a lower selection pressure does
not yield better performance for ECGA or for our approach.
The results of these experiments give some insights into the pruning mechanism. It
can be observed that the appropriateness of a particular selection pressure is related to
the linkage sensibility of the problem at hand. This property could cause inconvenience
in choosing selection pressure for the algorithm because when dealing with black box
optimization, we usually do not have any information about the problem at hand.
Fortunately, Figures 1(b), 2(b), and3(b) also suggest that under tournament sizes ranging
14 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
40 44 48 52 56 60 64 68 72 76 80
0
500
1000
1500
2000
2500
3000
Problem Sizes (Bits)
P
op
ul
at
io
n 
S
iz
es
ECGA, t = 12
ECGA, t = 16
ECGA+MP, t = 12
ECGA+MP, t = 16
(a) Population Sizes
40 44 48 52 56 60 64 68 72 76 80
1
2
3
4
5
6
x 104
Problem Sizes (Bits)
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA, t = 12
ECGA, t = 16
ECGA+MP, t = 12
ECGA+MP, t = 16
(b) Function Evaluations
Figure 4: Empirical results of the proposed method compared to the original ECGA on
exponential scaled problems with tournament sizes t = 12 and t = 16. Problem sizes
ranging from 40 to 80 bits (k = 4, m = 10 . . . 20) were used to observe the performance
of the algorithms.
population size is determined by a bisection method such that on average,m − 1 build-
ing blocks converge to the correct values in 50 runs.
6.2.2 Results and Observations
The empirical results for exponentially scaled problems are shown in Figure 4. The
minimum population sizes required by the proposed method are much smaller than
the sizes needed by the original ECGA, and grow at a relatively slow rate. The same sit-
uation is also observed in the function evaluations for which our approach performed
remarkably well. This improvement can be explained by the previous discussion on
random drift and linkage sensibility presented in earlier sections. If simultaneous de-
tection and processing of all building blocks cannot be achieved, additional costs have
to be paid for the inaccurate processing and random drift of subsolutions. By adopting
the pruningmechanism,we can save these costs by detecting possibly ineffective partial
models and postponing the changes on them until accurate processing can be made.
Figure 5 shows the results for power law scaled problems. The results of the mini-
mum population sizes are similar to those obtained in the previous set of experiments.
The proposed method still uses fewer function evaluations, but the differences are re-
duced. This is because the linkage sensibility of the power law scaled problems is less
limited compared to that of the exponential scaled problems.
The empirical results for uniformly scaled problems are presented in Figure 6. As
expected, the proposedmethod requires larger population sizes thanwhichwas needed
by the original ECGA. Due to the fact that for uniformly scaled problems, the model
building process can correctly identify all building blocks, the verification on the built
model may just be useless and wasteful. The results also suggest that the function
evaluations used by the proposed method are about twice as the number of what was
needed by the original ECGA.
In order to support the significance of the observations, we have also performed
Welch’s t-test on the results. For each problem size, a t-test of the null hypothesis that the
16 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
Table 6: Welch’s t-test on empirical results presented in Figures 4, 5, and 6. The null
hypothesis is that the number of function evaluations spent by ECGA and the number
of function evaluations spent by ECGA-MP with equal means against the alternative
that the means are not equal. The first three rows indicate whether the null hypothesis
is rejected, the p-value, and the t-statistics from the tests, respectively. The last row lists
whether the number of average function evaluations needed by ECGA-MP is smaller
(<) or larger (>) than the number needed by the original ECGA.
Problem size 40 48 56 64 72 80
(a) Exponential scaled cases with tournament size 12
Reject null True True True True True True
p-value 4.409 × 10−48 2.137 × 10−60 2.031 × 10−69 1.102 × 10−56 2.053 × 10−70 5.563 × 10−63
t-statistics 41.9194 64.0567 82.6409 66.3387 97.2660 82.4819
Comparison < < < < < <
(b) Exponential scaled cases with tournament size 16
Reject null True True True True True True
p-value 1.458 × 10−48 6.409 × 10−53 2.149 × 10−54 1.847 × 10−58 4.341 × 10−60 8.518 × 10−58
t-statistics 40.7834 60.9651 71.7845 81.9204 89.1136 87.8952
Comparison < < < < < <
(c) Power law scaled cases with tournament size 12
Reject null True True True True True True
p-value 1.094 × 10−25 1.208 × 10−33 5.515 × 10−48 4.294 × 10−61 6.05 × 10−53 1.608 × 10−72
t-statistics 14.5298 18.4542 29.4004 48.5933 44.0576 63.2243
Comparison < < < < < <
(d) Power law scaled cases with tournament size 16
Reject null True True True True True True
p-value 1.582 × 10−22 6.032 × 10−50 1.047 × 10−47 1.717 × 10−59 3.383 × 10−56 7.91 × 10−69
t-statistics 12.7581 30.2641 28.5023 37.5145 38.8386 49.2693
Comparison < < < < < <
(e) Uniformly scaled cases with tournament size 12
Reject null True True True True True True
p-value 3.356 × 10−35 1.23 × 10−39 4.264 × 10−33 3.399 × 10−33 4.006 × 10−45 4.903 × 10−53
t-statistic –25.4683 –26.7928 –23.7365 –22.9905 –29.0505 –33.4524
Comparison > > > > > >
(f) Uniformly scaled cases with tournament size 16
Reject null True True True True True True
p-value 1.126 × 10−34 1.776 × 10−33 8.802 × 10−40 6.649 × 10−32 3.684 × 10−38 4.062 × 10−44
t-statistic –25.7066 –25.9356 –31.5460 –25.0263 –28.6037 –34.0405
Comparison > > > > > >
to observe the behavior of the proposed approach when the size of the constitutive sub-
problem changes (i.e., varying k while fixing m). As in the previous set of experiments,
the original ECGA will also be tested for comparison.
6.3.1 Experimental Settings
In contrast to the previous set of experiments, we use trap functions of different sizes to
form our test problems.While the size of constituting subproblem varies, the number of
the subproblems remains fixed. The problem instances are constructed by concatenating
18 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
3 4 5
0
500
1000
1500
2000
2500
3000
3500
Trap Sizes (Bits)
Po
pu
la
tio
n 
Si
ze
s
ECGA
ECGA+MP
(a) Population Sizes
3 4 5
0
1
2
3
4
5
x 104
Trap Sizes (Bits)
Fu
nc
tio
n 
E
va
lu
at
io
ns
ECGA
ECGA+MP
(b) Function Evaluations
Figure 8: Empirical results of the proposedmethod compared to the original ECGA for
power law scaled problems composed of subproblems of sizes 3, 4, and 5 (k = 3, 4, and
5). In this experiment, tournament size t = 16was used and the number of subfunctions
forming the test problems was fixed at 10 (i.e., m = 10).
3 4 5
0
500
1000
1500
2000
2500
3000
3500
Trap Sizes (Bits)
Po
pu
la
tio
n 
Si
ze
s
ECGA
ECGA+MP
(a) Population Sizes
3 4 5
0
1
2
3
4
5
x 104
Trap Sizes (Bits)
Fu
nc
tio
n 
E
va
lu
at
io
ns
ECGA
ECGA+MP
(b) Function Evaluations
Figure 9: Empirical results of the proposedmethod compared to the original ECGA for
uniformly scaled problems composed of subproblems of sizes 3, 4, and 5 (k = 3, 4, and
5). In this experiment, tournament size t = 16was used and the number of subfunctions
forming the test problems was fixed at 10 (i.e., m = 10).
adopted. It presents the experimental results to illustrate the behavior under different
scalings. The purpose for performing these experiments is twofold:
• First, we would like to observe how the splitting ratio is related to the scaling or
linkage sensibility of a problem.
• Second, we wish to empirically study the change in performance obtained from
decreasing or increasing the proportion of population for checking the model.
It is important in practice to spend function evaluations wisely. Since using too large a
proportion of the population for pruning may result in a waste of resources, it should
20 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
250
500
750
1000
1250
1500
1750
2000
2250
Population Ratio (|T|/|S+T|)
P
op
ul
at
io
n 
S
iz
es
ECGA+MP, t = 12
ECGA+MP, t = 16
(a) Population Sizes
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0.5
1
1.5
2
2.5
3
3.5
4
x 104
Population Ratio (|T|/|S+T|)
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA+MP, t = 12
ECGA+MP, t = 16
(b) Function Evaluations
Figure 11: Empirical results of the proposed method for a 60-bit power law scaled
problem with different splitting ratios between the two subpopulations. The splitting
ratio (|T |/|S + T |) ranging from 0.0 (ECGAwithout pruning) to 0.8 was used to observe
the change in performance of the proposed approach.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
750
1000
1250
1500
1750
2000
2250
2500
2750
3000
3250
Population Ratio (|T|/|S+T|)
P
op
ul
at
io
n 
S
iz
es
ECGA+MP, t = 12
ECGA+MP, t = 16
(a) Population Sizes
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
1
2
3
4
5
6
7
8
9
10
x 104
Population Ratio (|T|/|S+T|)
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA+MP, t = 12
ECGA+MP, t = 16
(b) Function Evaluations
Figure 12: Empirical results of the proposed method for a 60-bit uniformly scaled
problem with different splitting ratios between the two subpopulations. The splitting
ratio (|T |/|S + T |) ranging from 0.0 (ECGAwithout pruning) to 0.8 was used to observe
the change in performance of the proposed approach.
Figure 12 shows the results foruniformly scaledproblems.As expected, Figures 12(a)
and 12(b) both share a common pattern in which the population size and the number of
function evaluations increase with the splitting ratio. This is because in the uniformly
scaled case, the linkage is always completely sensible, and there is no need to verify or
prune the built probabilistic model.
These experimental results demonstrate that under different scaling setups, the
behavior of theproposedapproach corresponding to the splitting ratio varies differently.
The empirical results suggest that if the given problem is evidentlywith distinguishable
prominence among the constituting subproblems, usinghigher splitting ratioswill yield
22 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0
1000
2000
3000
4000
Population Ratio (|T|/|S+T|)
P
op
ul
at
io
n 
S
iz
es
ECGA+MP, k = 3
ECGA+MP, k = 4
ECGA+MP, k = 5
(a) Population Sizes
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0
1
2
3
4
5
6
7
8
9
x 104
Population Ratio (|T|/|S+T|)
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA+MP, k = 3
ECGA+MP, k = 4
ECGA+MP, k = 5
(b) Function Evaluations
Figure 13: Empirical results of the proposed method using different splitting ratios
(|T |/|S + T |) for exponential scaled problems composed of subproblems of sizes 3, 4, or
5 (k = 3, 4, or 5). In this experiment, tournament size t = 16 was used and the number
of subfunctions forming the test problems was fixed at 10 (i.e., m = 10)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0
1000
2000
3000
4000
Population Ratio (|T|/|S+T|)
P
op
ul
at
io
n 
S
iz
es
ECGA+MP, k = 3
ECGA+MP, k = 4
ECGA+MP, k = 5
(a) Population Sizes
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
0
1
2
3
4
5
6
7
8
9
x 104
Population Ratio (|T|/|S+T|)
F
un
ct
io
n 
E
va
lu
at
io
ns
ECGA+MP, k = 3
ECGA+MP, k = 4
ECGA+MP, k = 5
(b) Function Evaluations
Figure 14: Empirical results of the proposed method using different splitting ratios
(|T |/|S + T |) for power law scaled problems composed of subproblems of sizes 3, 4, or
5 (k = 3, 4, or 5). In this experiment, tournament size t = 16 was used and the number
of subfunctions forming the test problems was fixed at 10 (i.e., m = 10)
model. Furthermore, similar to what we have observed in the experiments described
in Section 6.3, the degree of improvement over the original ECGA (splitting ratio = 0.0)
increases with the size of the constitutive subproblem.
7 Discussion
We utilized the existence of disparate scales in problems to create a controlled experi-
mental environment in order to study the situation in which complete, accurate linkage
information may or may not be available for the estimation of distribution algorithms.
24 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
Table 7: Empirical results of the original ECGA using tournament size 12. Experiments
were conducted on 40-bit and 80-bit uniformly scaled problems formed by concatenat-
ing 4-bit trap functions. The symbols , n, g, and fev denote problem size, population
size, generation, and function evaluations, respectively.
 n g SD g fev SD fev
ECGA 40 646 8.36 0.92 5,400.56 594.65
80 2042 10.72 1.01 21,890.24 2,064.38
Table 8: Empirical results of the proposed approach using a tournament size of 12.
Experiments were conducted on 40-bit and 80-bit uniformly scaled problems formed
by concatenating 4-bit trap functions. The symbols , 2n, g, and fev denote problem size,
twice of the population size required by the original ECGA, generation, and function
evaluations, respectively.
 2n g SD g fev SD fev
ECGA+MP 40 1,292 9.24 0.89 11,938.08 1,154.42
80 4,084 10.58 0.70 43,208.72 2,868.90
listed in Table 8. It can be observed that the function evaluations spent by the proposed
approach for 40-bit and 80-bit problems are about twice the amount of the original
ECGA needed in each case.
Although the inference together with the empirical validation can serve as an
intuitive explanation, it cannot fully explain the results presented in Section 6.2. As il-
lustrated in Figure 6(a), theminimumpopulation sizes needed by the proposedmethod
is not exactly twice that required by the original ECGA. In fact, the numbers are much
lower than twicewhat is needed by the original ECGA.On the other hand, our approach
uses more generations compared to the original ECGA because the subpopulation for
model building was not sufficiently large for all problem structures to be detected
properly in the beginning of the process. In this situation, the processing was slowed
down because the pruning mechanism removed certain parts of the model exhibiting
statistical inconsistencies. As a consequence, the originally expected simultaneous pro-
cessing of building blocks was not fully achieved and delay of convergence occurred.
Nevertheless, spending more generations seems to yield an equivalent use of function
evaluations as the hypothetical case described above. We think that the pruning mech-
anism introduces an additional interaction between population size and generations.
Further empirical or theoretical studies are needed to investigate such an interaction.
7.2 A Deeper Look at the Pruning Criterion
This section provides a more detailed elaboration on the adequacy of the proposed
pruning metric. To start this discussion, let
λi =
2ki∑
j=1
qij (− log2 pij )
which is the quantity to be examined by the pruning criterion (i.e., whether λi ≥ ki).
Based on λi , we can reformulate the issue of adequacy more concisely as “is it possible
26 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
where Xi , i = 1 . . . , are variables, H (Xi |i) is the conditional entropy of Xi given its
parenti in the network, and n is the population size. The conditional entropyH (Xi |i)
is given by
H (Xi |i) = −
∑
xi ,πi
p(xi, πi) log2 p(xi |πi),
where p(xi, πi) is the probability of instances with Xi = xi , i = πi , and p(xi |πi) is the
conditional probability of instances with Xi = xi given that i = πi .
The term
∑
i=1 n × H (Xi |i) provides the same functionality as the compressed
population complexity (Cp) in ECGA because H (Xi |i) denotes the average number of
bits required to store a value ofXi with compression given the information ofi . Thus,
we can check whether or not variableXi should be pruned away by using the following
inequality
−
∑
xi ,πi
q(xi, πi) log2 p(xi |πi) > 1,
where q(xi, πi) is the frequency ofXi = xi , andi = πi is observed in the set of solutions
selected from the reserved subpopulation. Using the idea described in Section 5.2, if this
inequality holds, Xi should be removed because it encodes a one-bit partial solution to
a bit string with an expected length of more than one bit.
However, despite the similarities in ideas, some technical complications remain
to be overcome before we can finish the design of a pruning mechanism for network-
based probabilisticmodels. For instance,what if a variablewhichwe intend to prune is a
parent node of some other variables? In summary, pruning network-based probabilistic
models is potentially feasible, but requires further investigation.
8 Summary and Conclusions
This paper reviewed previous studies on EDAs and scaling difficulties. It then illus-
trated how the scaling difficulty shadows the EDA ability in recognizing building
blocks. Following that, a notion called linkage sensibility was introduced to describe the
observation, and we used the term sensible linkage to refer to the problem structures
that can be extracted by inspecting only the set of selected solutions. Based on this
concept, we briefly defined the effectiveness of distributions estimated by probabilistic
model building and proposed a general approach to achieve more effective modeling.
Finally, an implementation of the proposed approach on ECGA was introduced and
experiments were done using several test functions of different scaling difficulties. In
this section, we briefly summarize the major results derived from this work and outline
the possible future extensions of this research.
8.1 Contributions
In this work, we have shown that the underlying facilities for EDAs to solve problems
efficiently and reliably do not work as expected when the problem at hand is composed
of subparts of unequal fitness contributions. More specifically, under this situation, the
model built from the selected solutions cannot fully reflect the true problem structures.
Although there are previous studies and discussion on the parameter selection (Pelikan
et al., 2002; Lima and Lobo, 2004; Pelikan and Lin, 2004; Yu et al., 2007), selection
28 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
algorithm from constructing models that truly reflect the problem structure may be an
inherent property of the underlying problem (e.g., different scales among constitutive
subproblems). Thus, we think that adapting pruning mechanisms will provide a more
appropriate circumstance for the model-based enhancement techniques to work.
8.2 Future Work
In this paper, we demonstrated a pruning mechanism design and its integration into
ECGA. It may also serve as a basis for developing other techniques for more efficient
and robust optimization. Some possible extensions of this work are outlined as follows.
First of all, the immediate direction is to design pruning mechanisms for other
EDAs. As illustrated in Section 7.3, we can extend the pruning metric described in
this paper to handle network-based models with a Bayesian information criterion.
However, a pruningmechanism for network-basedmodels requires more than that. We
also need to consider the possible disruption of variable dependencies after pruning a
particular variable. The simplest solution is to consider only those variables that are not
depended upon by other variables as possible candidates for pruning. However, the
validity of such an approach requires further investigation. Amore promising yet more
sophisticated approach is to first identify the tightly related components (e.g. cliques
or strongly connected subgraphs) in the model, and then process each component as a
unit which is similar to how we process the marginal product models in this work.
Another direction for future research is to assist efficiency enhancement techniques
that use the information contained in the built model. As described previously in
Section 8.1, some model-based efficiency enhancement techniques for EDAs crucially
rely on the structural accuracy of the probabilistic models. However, most of those
studies implicitly assume the information contained in the given population is suffi-
cient for learning accurate model structures. As demonstrated in the previous sections
by nonuniformly scaled problems, this assumption does not always hold. From this
perspective, incorporating pruning mechanisms to preprocess the built model for these
enhancement techniques is a promisingdirection for designingmore robust approaches.
From an abstract point of view, this work also demonstrates an instance of a new
class of techniques operating on built models to control, adapt, or regulate the opti-
mization process. Another example based on this viewpoint is the termination criterion
proposed by Ocenasek (2006) which uses an entropy-based measurement to evaluate
the built model for detecting an appropriate stopping point. According to the informa-
tion collected in the model, we can gain better control over the process compared to
the conventional evolutionary algorithms. Such an idea may be carried over to other
designs of EDAs so that more robust and efficient optimization can be realized.
Acknowledgments
The work was supported in part by the National Science Council of Taiwan under
Grant NSC-98-2221-E-009-072. The authors are grateful to theNational Center for High-
performance Computing for computer time and facilities.
References
Baluja, S. (1994). Population-based incremental learning: Amethod for integrating genetic search
based function optimization and competitive learning. Tech. Rep. CMU-CS-94-163, Carnegie
Mellon University, Pittsburgh, Pennsylvania.
30 Evolutionary Computation Volume xx, Number x
P1: QPU
Evolutionary Computation EVCO/EVCO_a_00010-Chen July 19, 2010 15:58
U
nc
or
re
ct
ed
Pr
oo
f
C.-Y. Chuang and Y.-p. Chen
Hauschild, M. W., Pelikan, M., Sastry, K., and Goldberg, D. E. (2008). Using previous models to
bias structural learning in the hierarchical BOA. In Proceedings of ACM SIGEVO Genetic and
Evolutionary Computation Conference (GECCO-2008), pp. 415–422.
Holland, J. H. (1992). Adaptation in natural and artificial systems. Cambridge, MA: MIT Press.
Larran˜aga, P., and Lozano, J. A. (2001). Estimation of distribution algorithms: A new tool for evolution-
ary computation, Vol. 2 of Genetic Algorithms and Evolutionary Computation. Dordrecht, The
Netherlands: Kluwer Academic Publishers.
Lima, C. F., andLobo, F. G. (2004). Parameter-less optimizationwith the extended compact genetic
algorithm and iterated local search. In Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO-2004), pp. 1328–1339.
Lima, C. F., Lobo, F. G., and Pelikan, M. (2008). From mating pool distributions to model overfit-
ting. InProceedings of ACM SIGEVO Genetic and Evolutionary Computation Conference (GECCO-
2008), pp. 431–438.
Lima, C. F., Pelikan,M., Goldberg,D. E., Lobo, F. G., Sastry, K., andHauschild,M. (2007). Influence
of selection and replacement strategies on linkage learning in BOA. In Proceedings of 2007
IEEE Congress on Evolutionary Computation (CEC 2007), pp. 1083–1090.
Lima,C. F., Pelikan,M., Sastry,K., Butz,M.V.,Goldberg,D. E., andLobo, F.G. (2006). Substructural
neighborhoods for local search in the Bayesian optimization algorithm. In Proceedings of
the 9th International Conference on Parallel Problem Solving from Nature (PPSN IX), pp. 232–
241.
Lima, C. F., Sastry, K., Goldberg, D. E., and Lobo, F. G. (2005). Combining competent crossover
and mutation operators: A probabilistic model building approach. In Proceedings of ACM
SIGEVO Genetic and Evolutionary Computation Conference (GECCO-2005), pp. 735–742.
Lobo, F. G., Goldberg, D. E., and Pelikan, M. (2000). Time complexity of genetic algorithms
on exponentially scaled problems. In D. Whitley, D. Goldberg, E. Cantu´-Paz, L. Spector,
I. Parmee, and H.-G. Beyer (Eds.), Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO-2000), pp. 151–158.
Mitchell, T. M. (1997). Machine learning. New York: McGraw-Hill Higher Education.
Mu¨hlenbein, H., and Ho¨ns, R. (2005). The estimation of distributions and the minimum relative
entropy principle. Evolutionary Computation, 13(1):1–27.
Mu¨hlenbein, H., andMahnig, T. (1999). FDA: A scalable evolutionary algorithm for the optimiza-
tion of additively decomposed functions. Evolutionary Computation, 7(4):353–376.
Mu¨hlenbein, H., and Paaß, G. (1996). From recombination of genes to the estimation of distribu-
tions. I. Binary parameters. In Proceedings of the 4th International Conference on Parallel Problem
Solving from Nature (PPSN IV), pp. 178–187.
Ocenasek, J. (2006). Entropy-based convergence measurement in discrete estimation of distribu-
tion algorithms. In J. A. Lozano, P. Larran˜aga, I. Inza, and E. Bengoetxea (Eds.), Towards a new
evolutionary computation. Advances in estimation of distribution algorithms, Vol. 192 of Studies in
Fuzziness and Soft Computing (pp. 39–50). Berlin: Springer.
Pelikan,M.,Goldberg,D. E., andCantu´-Paz, E. (1999). BOA:TheBayesianoptimization algorithm.
In W. Banzhaf, J. Daida, A. E. Eiben, M. H. Garzon, V. Honavar, M. Jakiela, and R. E. Smith
(Eds.), Proceedings of the Genetic and Evolutionary Computation Conference GECCO-99, pp. 525–
532.
Pelikan, M., Goldberg, D. E., and Lobo, F. G. (2002). A survey of optimization by building and
using probabilistic models. Computational Optimization and Applications, 21(1):5–20.
32 Evolutionary Computation Volume xx, Number x
This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
http://www.elsevier.com/copyright
Author's personal copy
2102 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
interaction and derive the expected progress rate of the swarm on the sphere function. Next, we will look into the variance
of the particle positions and show that the swarm will converge under certain condition in Section 4. Finally, Section 5
summarizes and concludes this paper.
2. PSO and particle interaction
In this section, we will firstly describe the standard PSO algorithm and then discuss the operations of PSO step by step,
followed by the proposal of our statistical interpretation.
2.1. The standard PSO algorithm
First of all, for easily making an abstraction of PSO based on statistics and probabilistic distributions, we restate the
standard PSO system as the following algorithm:
Algorithm 1 (Standard PSO).
procedure Standard PSO(Objective function F : Rn → R)
Initialize a swarm ofm particles
while the stopping criterion is not satisfied do
Evaluate each particle
for particle i, i = 1, 2, . . . ,m do F Update the best positions
if F (Xi) < F (Pbi) then
Pbi ← Xi
if F (Pbi) < F (Nb) then
Nb← Pbi
end if
end if
end for
for particle i, i = 1, 2, . . . ,m do F Generate the next generation
Vi(t+ 1)← wVi(t)+ Cp ⊗ (Pbi − Xi)+ Cn ⊗ (Nb− Xi)
Xi(t+ 1)← Xi(t)+ Vi(t+ 1)
end for
end while
end procedure
Throughout this paper, boldface is used to distinguish vectors from scalars, and ‖·‖ denotes the L2 norm of a vector. The
notation ⊗ indicates component-by-component multiplication. According to Algorithm 1, we can see that a standard PSO
system comprises the following two main operations regarding the information sharing and utilizing:
(1) Updating attractors: Update the personal best position, Pbi, found by each particle, and the neighborhood best position,
Nb, found by any member within the neighborhood. Since Pbi and Nb exert gravity on other particles, they are referred
to as attractors in this study.
(2) Updating particles: Update the velocities at time t by using a linear combination of the inertia, Vi(t), and the gravitation
from the cognitive part, Pbi, and the social part,Nb, respectively.w is the weight for the inertia and is usually a constant.
Cp and Cn are random vectors with each component sampled from uniform distributions U(0, cp) and U(0, cn) with
cp > 0 and cn > 0 as acceleration coefficients. The position is then assigned according to the current position with
application of the updated velocity.
As we can observe, the inherent characteristics of PSO – the interactions among particles – are implemented with the
shared knowledge on the best position found by neighbors. When a particle within the neighborhood locates a position of
an objective value which is better than F (Nb), the other particles will make corresponding adjustments and tend to go
toward that position. Therefore, the neighborhood attractor can be viewed as a channel through which each particle can
emulate the others, and the update of the neighborhood attractor can be considered as a signal urging the swarm to adjust
their movements in order to respond to the new discovery in the search space.
2.2. A macroscopic view of PSO
In spite of its importance, the effect of particle interaction in PSO is hardly investigated in the literature. Although there
are a number of remarkable theoretical studies that bring insights into the properties and behavior of PSO conducted in
the past, most of those studies are based on the assumption that the attractor is fixed, e.g., the trajectory analysis [4,5]
mentioned in Section 1. Such a setting seems an inevitable path to simplify the PSO system to the extent that rigorous
analysis can be done because the highly decentralized property of a particle swarm leads the system away from a unified
depiction of the entire swarm. Each particle keeps its own position andmemory, in the form of the inertia and the cognitive
part, Pbi. In addition to the personal experience, the swarm also shares collective knowledge, Nb, and any slight change in
Author's personal copy
2104 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
Table 1
Average p-values of normality tests.
Swarm size Normality tests
Shapiro–Wilk [17] Anderson–Darling [18] D’Agostino–Pearson [19]
10 0.3879 0.3621 0.3985
20 0.3257 0.2842 0.3393
30 0.2903 0.2518 0.2876
Algorithm 2 (Statistical interpretation of PSO).
procedure PSO(Objective function F : Rn → R)
Initialize θ
while the stopping criterion is not satisfied do
for i = 1, 2, . . . ,m do
Pi ∼ θ
end for
P∗ = argmin{F (P1),F (P2), . . . ,F (Pm)}
for i = 1, 2, . . . ,m do
P′i ← Pi + Ci ⊗ (P∗ − Pi)
end for
µt+1 ← (
∑m
i=1 P
′
i)/m
σ 2t+1 ← MLE(P′1, P′2, . . . , P′m)
θ ← θ(µt+1, σ 2t+1)
t ← t + 1
end while
end procedure
In order to validate the utilization of normal distributions for describing swarms, we conducted three well-known
normality tests: the Shapiro–Wilk test [17], the Anderson–Darling test [18], and the D’Agostino–Pearson test [19] on the
social-only PSO on the sphere function. Table 1 displays the test results, which were obtained for 100 independent runs and
10 iterations in each run. The weight for the inertia is 0.73 and the acceleration coefficient is 1.49. Since all p-values of the
three normality tests significantly surpass the conventional significance level 0.05, none of these tests are able to reject the
null hypothesis. As a result, in this study, adopting the normal distribution as the description of swarms is an acceptable
assumption.
In summary, the macrostate model transforms the detailed configuration of PSO into a corresponding stochastic
representation embodied by normal distributions. As a consequence, the update of particles is simplified as themodification
of the parameters of normal distributions. In each iteration, Algorithm 2 generates a swarm of particles by means of
sampling from the current distribution, and thereafter, the distribution is updated according to particle interaction. In
others words, a state of Algorithm 2 is a distribution, and the sampled swarm serves as a medium for state transition.
In this manner, the analysis of the behavior of the entire swarm is thus reduced to the analysis of parameterized
distributions. The inclusion of particle interaction into analysis supplies numerous facets of PSO typically absent in related
theoretical studies, e.g., the progress rate and the influence of objective functions, because the restriction of fixed attractors
makes objective functions irrelevant. Since the No-Free-Lunch theorem states that all optimization algorithms perform
identically on average [20], the effectiveness of PSO can hardly be theoretically identified unless the scope of functions is
specified.
In the remainder of this paper, Algorithm 2 will be the study subject and be formally investigated on the sphere
function,which is commonly adopted in the theoretical analysis of evolutionary algorithms (e.g., [21]) and can be formulated
as
F (x) =
n∑
i=1
x2i ,
where x = (x1, x2, . . . , xn) ∈ Rn.
3. Progress rate analysis
The major benefit to develop and adopt the abstraction based on probabilistic distributions of PSO is that the
mathematical model can be analyzed without the assumption of fixed attractors, because particles are in essence random
vectors in the search space and consequently their behavior can be described and predicted in a statistical sense. In this
section, we will demonstrate how the statistical interpretation of PSO proposed in the present work facilitates the analysis
of inter-particle effects and how these effects are accounted for the progress rate of a swarm. We will begin with the n-ball
hitting probability.
Author's personal copy
2106 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
As a result, we can get
HB(k, θ(r, σ 2)) = Prob
{−k− r ≤ Z1 ≤ k− r, 0 ≤ ‖Z′‖2 ≤ k2 − (r + Z1)2}
=
∫ k−r
x=−k−r
∫ k2−(x+r)2
y=0
p(Z1, x)p(W , y) dy dx
=
∫ k−r
x=−k−r
p(Z1, x)
∫ k2−(x+r)2
y=0
1
σ 2
(
y
σ 2
) n′
2 −1
exp
(
−y
2σ 2
)
2
n′
2 Γ
(
n′
2
) dy dx
(let u := y/σ 2) =
∫ k−r
x=−k−r
p(Z1, x)
∫ k2−(x+r)2
σ2
u=0
u
n′
2 −1 exp
(−u
2
)
2
n′
2 Γ
(
n′
2
) du dx
=
∫ k−r
x=−k−r
p(Z1, x)P
(
n′
2
,
k2 − (x+ r)2
2σ 2
)
dx ,
where P (·) is the regularized Gamma function.
Remark 2. If an asymptotic approximation is desired for the n-ball hitting probability, HB(k, θ(r, σ 2)), we can utilize the
normal approximation to the regularized Gamma function [23, chapter 7] as
P
(
n′
2
,
k2 − (x+ r)2
2σ 2
)
≈ Φ
(
1√
2n′
[
k2 − (x+ r)2
σ 2
− n′
])
.
For the asymptotic approximation, when n is sufficiently large, the term (1/
√
2n′)[k2 − (x + r)2]/σ 2 vanishes. Thanks to
the continuity ofΦ(·), we can obtain
P
(
n′
2
,
k2 − (x+ r)2
2σ 2
)
≈ Φ
(
−
√
n′
2
)
.
Hence,
HB(k, θ(r, σ 2)) ≈ Φ
(
−
√
n′
2
)∫ k−r
x=−k−r
p(Z1, x) dx
= Φ
(
−
√
n′
2
)[
Φ
(
k− r
σ
)
− Φ
(−k− r
σ
)]
= Φ
(
−
√
n′
2
)[
Φ
(
r + k
σ
)
− Φ
(
r − k
σ
)]
.
In addition to the asymptotic properties ofHB(k, θ(r, σ 2)), it would be helpful to derive a lower bound forHB(k, θ(r, σ 2))
to facilitate our analysis in the present work.
Lemma 3 (Lower Bound for HB(k, θ(r, σ 2))).
HB(k, θ(r, σ 2)) ≥
[
Φ
(
r + k√n
σ
)
− Φ
(
r − k√n
σ
)][
1− 2Φ
( −k√
nσ
)]n−1
.
Proof. Let Y := c(t) + Z, where c(t) = (r, 0, 0, . . . , 0), and Z = (Z1, Z2, . . . , Zn). LetD := [−k/√n, k/√n]n ⊆ Rn. For all
x ∈ D , because ‖x‖ ≤ √n‖x‖∞ ≤ √n(k/√n) = k, we can know that x ∈ Bk(o). Hence,D ⊆ Bk(o), and
Prob {Y ∈ Bk(o)} ≥ Prob {Y ∈ D} = Prob
{
− k√
n
− r ≤ Z1 ≤ k√n − r
} n∏
i=2
Prob
{
− k√
n
≤ Zi ≤ k√n
}
=
[
Φ
( k√
n − r
σ
)
− Φ
(− k√n − r
σ
)][
Φ
( k√
n
σ
)
− Φ
(− k√n
σ
)]n−1
=
[
Φ
(
r + k√n
σ
)
− Φ
(
r − k√n
σ
)][
1− 2Φ
( −k√
nσ
)]n−1
. 
Author's personal copy
2108 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
The expected particle norm describes how close on average a swarm is to the global optimum, i.e., the origin, of the
sphere function. In order to capture the characteristic of the essential mechanism of PSO – particle interaction – we also
need to investigate the attractor. As stated in the previous section, the attractor is the best observed value, i.e., in our case,
the particle with theminimum objective value within the neighborhood in the current swarm. Under the adopted statistical
interpretation of PSO, the expectedminimumobjective value of a swarmbecomes traceable through order statistics, because
particles are viewed as random vectors over Rn.
Let P(i,m) denote the ith order statistic of ‖P1‖, ‖P2‖, . . ., ‖Pm‖, e.g., P(1,m) = min{‖P1‖, ‖P2‖, . . ., ‖Pm‖}. Denoting the
event ‖Pi‖ = x as {‖Pi‖ = x}, the density of P(1,m) at a non-negative real number x can be given as
Prob
{
P(1,m) = x
} = Prob{ m⋃
i=1
[
{‖Pi‖ = x}
⋂( ⋂
j∈{1,2,...,m}\{i}
{‖Pj‖ > x})]}
=
∫ k−r
x=−k−r
(
m
1
)
HS(x, θ(r, σ 2))
[
1− HB(x, θ(r, σ 2))
]m
dx .
Denoting E
[
P(1,m)
]
as P(1,m), a naive upper bound for P(1,m) is derived in the following lemma.
Lemma 6. P(1,m) ≤ P
Proof. The general upper bound for the expected ith order statistic states
P(i,m) ≤ P + (Var [‖c(t)+ Z‖]) 12
√
i− 1
m− i+ 1 .
As a result,
P(1,m) ≤ P + (Var [‖c(t)+ Z‖]) 12
√
1− 1
m− 1+ 1 = P. 
Lemma 6 causes no surprise. The expected minimum particle norm is obviously less than or equal to the expected norm.
However, inspired by Lemma 6, we can seek another upper bound for P(1,m) by definition.
Lemma 7 (Upper Bound for P(1,m)). (1)
P(1,m) =
∫ ∞
x=0
[
1− HB(x, θ(r, σ 2))
]m
dx,
and (2)
P(1,m) ≤
(
lim
h→∞
[h− ψ(h)]
)m
2
.
Proof. (1) For any random variable X , E [|X |]r = r ∫∞0 t r−1Prob {|X | > t} dt with r > 0 [24]. Since P(1,m) is a non-negative
random variable, by letting r = 1 we have
P(1,m) =
∫ ∞
x=0
Prob
{
P(1,m) > x
}
dx
=
∫ ∞
x=0
Prob
{
m⋂
i=1
{‖Pi‖ > x}
}
dx
=
∫ ∞
x=0
[
1− HB(x, θ(r, σ 2))
]m
dx.
(2) Based on the result of (1), we obtain
P(1,m) =
∫ ∞
x=0
[
1− HB(x, θ(r, σ 2))
]m
dx ≤
∫ ∞
x=0
[
1− ψ ′(x)]m dx.
By resorting to Hölder’s inequality, we can movem outside of the integration to obtain a more comprehensible bound as∫ ∞
x=0
[
1− ψ ′(x)]m dx ≤ (∫ ∞
x=0
[
1− ψ ′(x)]2 dx)m2
≤
(∫ ∞
x=0
[
1− ψ ′(x)] dx)m2
=
(
lim
h→∞
[h− ψ(h)]
)m
2
.
The last equation follows from [h− ψ(h)]|h=0 = 0. 
Author's personal copy
2110 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
3.3. Lower and upper bounds for the expected progress rate
After the work was done in the previous sections, the progress rate of the social-only model PSO can now be formally
investigated under the proposed statistical interpretation. The term ‘‘progress rate’’ was introduced by Rechenberg in
1973 [26]. As the name suggests, progress rate should be a quantity indicating how a particle swarm progresses, and hence
in the present work, it is defined as the difference of the norms of the two distribution centers in successive time steps,
because the distance to the optimum is the L2 norm for the sphere function. Given the current center of distribution c(t) =
(r, 0, 0, . . . , 0) and a random vector Z = (Z1, Z2, . . . , Zn) with Z1, Z2, . . . , Zn ∼ N(0, σ 2), the m particles P1, P2, . . . , Pm
are sampled as c(t) + Z. Let P(i,m) denote the ith order statistic of ‖P1‖, ‖P2‖, . . ., ‖Pm‖. Let P∗ := argmin{F (P1),F (P2),
. . . ,F (Pm)}. By definition, ‖P∗‖ = P(1,m). According to the update rules described in Section 2.2, the updated position P′i
is computed as P′i = Pi + Ci ⊗ (P∗ − Pi), where each coordinate of Ci is distributed according to U(0, c) with c being the
coefficient representing the compound effect of both the inertia weight and the acceleration coefficient of the social part.
For simplicity, we still call c the acceleration coefficient in this paper because the inertia weight is usually constant. The
center of distribution in the next step c(t+ 1) is the mean of P′1, P′2, . . . , P′m, i.e., c(t+ 1) = (
∑m
i=1 P
′
i)/m.
Definition 9. Given c(t) = (r, 0, 0, . . . , 0), the progress rate∆t := ‖c(t)‖ − ‖c(t+ 1)‖ = r − ‖c(t+ 1)‖.
The following theorem shows that, when c ≤ 1/2, the expected norm of the center of distribution in the next time step
is bounded from above by a linear combination of the expected particle norm P and the expected minimum of the particle
norm P(1,m).
Lemma 10. Suppose C = (C1, C2, . . . , Cn) is a random vector of Rn with i.i.d. components and X is a random vector of Rn. If C
and X are independent, then E [‖C⊗ X‖] ≤ √µ′2 E [‖X‖], where µ′2 is the second moment of Ci.
Proof. For any fixed vector x = (x1, x2, . . . , xn) ∈ Rn,
E [‖C⊗ x‖] = E
[√∑
i=1:n
C2i x
2
i
]
≤
√√√√E[∑
i=1:n
C2i x
2
i
]
=
√∑
i=1:n
E
[
C2i
]
x2i
=
√
µ′2 ‖x‖ .
Since C and X are independent, by the law of total expectation conditional on X, this lemma is proved. 
Theorem 11 (Upper Bound for the Expected Norm of the Next Center). (1) E [‖c(t+ 1)‖] ≤ E [|1− C |]] P + E [|C |] P(1,m); and
(2) If c ≤ 1/2, E [‖c(t+ 1)‖] ≤ (1− c)P + cP(1,m); otherwise, E [‖c(t+ 1)‖] ≤ [(2c2 − 2c + 1)/2c]P + cP(1,m).
Proof. This result is derived from the triangle inequality for L2-norm and the previous lemma:
E [‖c(t+ 1)‖] = E

∥∥∥∥∥∥∥∥∥
m∑
i=1
[
Pi + Ci ⊗
(
P∗ − Pi
)]
m
∥∥∥∥∥∥∥∥∥

=
(
1
m
)
E
[∥∥∥∥∥ m∑
i=1
(1− Ci)⊗ Pi +mCi ⊗ P∗
∥∥∥∥∥
]
≤
(
1
m
)( m∑
i=1
E [‖(1− Ci)⊗ Pi‖]+mE
[‖Ci ⊗ P∗‖])
≤ (c2/3− c + 1)1/2 P + (c2/3)1/2 P(1,m). 
Corollary 12 (Lower Bound for the Progress Rate). E [∆t ] ≥ r −
(
c2/3− c + 1)1/2 P − (c2/3)1/2 P(1,m).
After the lower bound for E [∆t ] is established in Corollary 12, the next theorem sets a lower bound for E [‖c(t+ 1)‖]. An
upper bound for E [∆t ] will be accordingly obtained as a corollary.
Author's personal copy
2112 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
4. Convergence analysis
As stated in Section 2.2, the transition from the current time step to the next time step consists of updating positions
of particles, calculating the distribution center by means of the updated positions, and using the maximum likelihood
estimation to calculate the distribution variance. The issues related to the centers of distributions have been addressed
in Section 3. Thus, the part of variance is considered in this section. While the center of a distribution can be viewed as the
indication of the average quality of the swarm at a specific time step, the variance is a direct measurement of convergence,
because from the viewpoint of statistical interpretation, a swarm converges as the variance of the distribution reduces to
zero. The word ‘‘converge’’ is not a unified term in the research domain of PSO [27, p. 132]. It has been used to describe
the behavior of a swarm approaching the local optimum in some papers, while it simply indicates the phenomenon that
a swarm of particles crowds into a specific point, sometimes called the equilibrium, not necessarily the local optimum, in
the search space in other papers. Here in the present work, we adopt the latter definition. We concentrate on the condition
underwhich a swarm of particlesmay go into a stable state.Wewill demonstrate that if certain condition of the relationship
between the swarm size and the acceleration coefficient is satisfied, a swarm in the social-only model does converge under
the mechanism of particle interaction.
Given m observed vectors y1, y2, . . . , ym that stand for the updated positions and the distribution center is denoted
as c(t+ 1) = y := (Σmi=1yi)/m. Let Y1, Y2, . . . , Ym be random vectors sampled from θ(‖y‖, σ 2t+1). These vectors are
n-dimensional random vectors centered at y, and the coordinate on each dimension is a random variable sampled from
N(0, σ 2t+1), where σ
2
t+1 is the variance that we wish to estimate. In order to estimate the variance, the likelihood function of
σ 2t+1, L(σ
2
t+1), can be defined as the joint probability:
L(σ 2t+1) :=
m∏
i=1
(
1√
2piσt+1
)n
exp
(
−d (yi, y)2
2σ 2t+1
)
=
(
1√
2piσt+1
)mn
exp

−
m∑
i=1
d (yi, y)2
2σ 2t+1

= Kσ−mnt+1 exp
(
−R
2σ 2t+1
)
,
where
K :=
(
1√
2pi
)mn
, R :=
m∑
i=1
d(yi, y)2 .
In order to get the σ 2t+1 that maximizes L(σ
2
t+1), we differentiate L(σ
2
t+1)with respect to σ
2
t+1:
L′(σ 2t+1) = −
mn
2
K · σ−mn−2t+1 · exp
(
−R
2σ 2t+1
)
+ R
2
K · σ−mn−4t+1 · exp
(
−R
2σ 2t+1
)
.
L′(σ 2t+1) = 0 implies σ 2t+1 = R/(mn), and it is routine to check the maximality. Since both m and n are fixed, the only
quantity needs to be examined is R, the sum of square of the distance between each updated particle and the center.
Given c(t) = (r, 0, 0, . . . , 0) and Z = (Z1, Z2, . . . , Zn) with Z1, Z2, . . . , Zn ∼ N(0, σ 2t ), the m particles P1, P2, . . . , Pm
are sampled from c(t) + Z, and the updated position is calculated as Pi + Ci ⊗ (P∗ − Pi), where P∗ is the attractor. Since
c(t+ 1) =∑mi=1 [Pi + Ci ⊗ (P∗ − Pi)] /m, R, as a random variable, can be defined by P1, P2, . . . , Pm and P∗:
R =
m∑
i=1
∥∥∥∥∥∥∥∥∥∥
Pi + Ci ⊗ (P∗ − Pi)−
m∑
j=1
(
Pj + Ci ⊗ (P∗ − Pj)
)
m
∥∥∥∥∥∥∥∥∥∥
2
.
Denoting Pi’s and P∗’s kth coordinate as Pik and P∗k , respectively, the expectation of R, E [R], can be derived in the following
lemma:
Lemma 15. Given the swarm size, m, and the variance of distribution at time t, σ 2t = σ 2,
E
[
σ 2t+1
] ≤ (m− 1)σ 2
12m
{(
5+
√
3(m− 1)
n
)
c2 − 6c + 12
}
.
Author's personal copy
2114 Y.-p. Chen, P. Jiang / Theoretical Computer Science 411 (2010) 2101–2115
Proof. Suppose for contradiction that there exists some  > 0 and δ > 0 such that, for allN0 ∈ N, there exists anN(N0) > N0
with Prob
{
σ 2N(N0) ≥ 
}
≥ δ. However, since Prob
{
σ 2N(N0) ≥ 
}
≥ δ implies E
[
σ 2N(N0)
]
≥ δ, for all N0 ∈ N, there exists an
N(N0) > N0 such that E
[
σ 2N(N0)
]
≥ δ, limt→∞
{
E
[
σ 2t
]} = 0 is contradicted. 
Theorem 16 and Corollary 17 indicate that as long as the specified condition is satisfied, a swarm will converge in
probability. However, it must be noted that the acceleration coefficient, c , used in this study is the coefficient for the
compound effect of both the inertia weight and the common acceleration coefficient for the neighborhood or global best
position as described in Section 3.3. Therefore, further investigations are needed to gain understandings on the compound
effect and clarify the relationship of these parameters such that the derived results in the present work can be applied in
practice.
5. Summary and conclusions
In this study, we made the first attempt to analyze the behavior of particle swarm optimization on the facet of particle
interaction. We firstly proposed a statistical interpretation of particle swarm optimization and modeled the essential PSO
mechanisms with the operations on probabilistic distributions. In order to investigate the PSO behavior based on particle
interaction, we focused on the social-only model of PSO, in which the personal experience of particles is ignored. From the
viewpoint of macrostates, we obtained the lower and upper bounds of the expected progress rate for a swarm on the sphere
function. By examining in detail the variance of the particle distribution, we further showed that under certain condition, a
swarm will converge in probability due to the mechanism of particle interaction, i.e., exchanging and sharing information,
which is commonly believed to be an essential mechanism of PSO but seldom theoretically analyzed in the literature.
With regard to the practical implications of this study, we demonstrated that the optimization process of PSO can be
interpreted as the interplay between the attractor and the overall swarm, as shown in Theorem 11 that the expected norm
of the next center is upper-bounded by a linear combination of P and P(1,m) as well as that the acceleration coefficient is the
weight balancing the effects of these two quantities. The major resistance in the optimization process of PSO on the sphere
function is the number of dimensions, as it can be observed in Corollary 14 that the progress rate deteriorates drasticallywith
respect to the number of dimensions. On the other hand, the swarm size is the primary factor counteracting the increasing
dimensions, for the exploratory capability of the swarm is augmented in accordance with the number of particles. It is
noteworthy that in a variety of theoretical studies on PSO, the effect of the objective function has been rarely taken into
consideration due to the assumption of fixed attractors. By means of characterizing a swarm as a unity, the analysis of the
influence of the objective function becomes possible.
With this study,wepropose an alternativeway to analyze particle swarmoptimization from the viewpoint ofmacrostates
instead of tracing the trajectory of each particle. The immediate follow-upwork of this study includes the clarification of the
compound effect of the inertia weight and the neighborhood acceleration coefficient for carrying over the theoretical results
to practice and for suggesting applicable parameter settings. Moreover, tighter bounds may be derived to more accurately
describe the behavior of PSO, and a complete PSOmodel may be considered instead of the social-only model adopted in the
present work. Finally, in the long run, a unified behavioral model of PSO might be established by integrating the theoretical
results from the two ends – macrostates and microstates – such that better, more robust optimization frameworks can be
accordingly designed and developed.
Acknowledgements
The work was supported in part by the National Science Council of Taiwan under Grant NSC 98-2221-E-009-072. The
authors are grateful to the National Center for High-performance Computing for computer time and facilities.
References
[1] J. Kennedy, R. Eberhart, Particle swarmoptimization, in: Proceedings of 1995 IEEE International Conference onNeural Networks, 1995, pp. 1942–1948.
[2] Y. Shi, R.C. Eberhart, Empirical study of particle swarm optimization, in: Proceedings of 1999 IEEE Congress on Evolutionary Computation, CEC 99,
1999, pp. 1945–1950.
[3] J. Kennedy, The behavior of particles, in: Proceedings of the 7th International Conference on Evolutionary Programming, 1998, pp. 581–589.
[4] E. Ozcan, C.K. Mohan, Analysis of a simple particle swarm optimization system, Intelligent Engineering Systems Through Artificial Neural Networks 8
(1998) 253–258.
[5] E. Ozcan, C. K. Mohan, Particle swarm optimization: surfing the waves, in: Proceedings of 1999 IEEE Congress on Evolutionary Computation, CEC 99,
1999, pp. 1939–1944.
[6] M. Clerc, J. Kennedy, The particle swarm-explosion, stability, and convergence in amultidimensional complex space, IEEE Transactions on Evolutionary
Computation 6 (1) (2002) 58–73.
[7] Y. Shi, R.C. Eberhart, Parameter selection in particle swarm optimization, in: Proceedings of the 7th International Conference on Evolutionary
Programming, 1998, pp. 591–600.
[8] F. van den Bergh, An analysis of particle swarm optimizers, Ph.D. Thesis, University of Pretoria, 2002.
[9] K. Yasuda, A. Ide, N. Iwasaki, Adaptive particle swarm optimization, in: Proceedings of 1999 IEEE International Conference on Systems, Man and
Cybernetics, 2003, pp. 1554–1559.
[10] Y.-L. Zheng, L.-H. Ma, L.-Y. Zhang, J.-X. Qian, On the convergence analysis and parameter selection in particle swarm optimization, in: Proceedings of
the Second International Conference on Machine Learning and Cybernetics, 2003, pp. 1802–1807.
Detecting General Problem Structures with Inductive Linkage
Identification
Yuan-wei Huang and Ying-ping Chen
Abstract— Genetic algorithms and the descendant methods
have been deemed robust, effective, and practical for the past
decades. In order to enhance the features and capabilities of
genetic algorithms, tremendous effort has been invested within
the research community. One of the major development trends
to improve genetic algorithms is trying to extract and exploit
the relationship among decision variables, such as estimation
of distribution algorithms and perturbation-based methods. In
this study, we make an attempt to enable a perturbation-based
method, inductive linkage identification (ILI), to detect general
problem structures, in which one decision variable can link
to an arbitrary number of other variables. Experiments on
circular problem structures composed of order-4 and order-5 trap
functions are conducted. The results indicate that the proposed
technique requires a population size growing logarithmically with
the problem size as the original ILI does on non-overlapping
building blocks as well as that the population requirement is
insensitive to the problem structure consisting of similar sub-
structures as long as the overall problem size is identical.
I. INTRODUCTION
As practical optimization frameworks, genetic algorithms
(GAs) have shown properties of flexibility, robustness, and
easy-of-use since they were proposed [1], [2]. These methods
usually get good performance when the adopted genetic oper-
ators are aware of the relationship among decision variables.
Crossover operators in early genetic algorithms are likely to
break promising solutions of sub-problems, which are referred
to as build blocks (BBs) [3]. As a consequence, the overall
performance is greatly reduced, or the problem cannot be
solved [4]. In order to alleviate this issue, in recent studies,
crossover operators or equivalent mechanisms that maintain
the structure and diversity of building blocks have been
proposed, developed, and examined. These techniques sig-
nificantly increase the performance of genetic algorithms. To
provide the capability of appropriately and effectively handling
sub-solutions/building blocks, two key mechanisms, building-
block identification and building-block exchange, have to be
utilized and integrated. In this study, we focus on the mech-
anism of building-block identification, generalize the concept
regarding the detection of building blocks, and propose the
use of inductive linkage identification [5] to detect general
problem structures.
Most of building-block/linkage identifying methods pro-
posed and utilized in previous studies can be broadly classified
into the following three categories [6]:
1) Estimation of distribution algorithms;
Yuan-wei Huang and Ying-ping Chen are with the Department of Computer
Science, National Chiao Tung University, 1001 Ta Hsueh Road, Hsinchu,
TAIWAN (email: {ywhuang, ypchen}@nclab.tw).
2) Linkage learning techniques;
3) Perturbation-based methods.
In the first category, estimation of distribution algorithms
construct probabilistic models from the selected individuals of
the population and describe the relationship among decision
variables in a statistical way [7]. Early studies assume no
interaction among variables, such as the population-based
incremental learning [8] and the compact genetic algorithm
[9]. Subsequent researchers use conditional probabilities to
capture pairwise and/or multi-variate interactions, e.g., the
mutual information maximizing input clustering [10], Baluja’s
dependency tree approach [11], the bivariate marginal distribu-
tion algorithm [12], the factorized distribution algorithm [13],
and the Bayesian optimization algorithm [14]. Methods in
this category are usually quite efficient from the traditional
viewpoint of computational cost in evolutionary computation
because they do not need additional fitness evaluations. Never-
theless, less salient building blocks, which contribute little to
the total fitness, are less statistically significant and therefore
might be ignored and undetected [15].
For the methods of the second category, building-block
identification is oftentimes viewed as the (gene/variable) order-
ing problem. By rearranging variables during the evolutionary
process, interdependent variables are put closer according to
the adopted coding scheme such that these variables are less
likely to be split apart by subsequent operations. In these
studies, the messy genetic algorithm [4] and its more efficient
descent, the fast messy genetic algorithm [16], exploit building
blocks to identify linkages. Since the rearranging mechanism
often acts too slow to cooperate with the selection operator,
such a condition usually leads to premature convergence. The
linkage learning genetic algorithm [17] performs two-point
crossover on a specifically designed circular chromosome
representation such that tight linkages among related variables
can be formed on the chromosome and preserved during the
evolutionary process.
Methods in the last category analyze the fitness differ-
ence caused by perturbing variables to identify linkages. For
example, the gene expression messy genetic algorithm [18]
incorporates a special genotype for pairwise relations and a
function involving perturbation to find linkage sets. Linkage
identification by nonlinear check [6] uses the linear summation
of different and non-overlapping building blocks to detect
linkages. Borrowing the idea from estimation of distribution
algorithms, the dependency detection for distribution derived
from the fitness difference [15] clusters variables according to
the fitness difference values caused by perturbation. Because a
0 k−1 k
0
k−1
k
Unitation
Fu
nc
tio
n 
V
al
ue
 
 
trapk
Fig. 1
A k-TRAP FUNCTION.
identification procedure. Then, we describe the modified ver-
sion of ILI for detecting general problem structures. A simple
example is also given for illustration.
A. ID3 for Recognizing Linkage
The ID3 decision tree construction algorithm is a supervised
categorization method working on discrete data sets, in which
the datum entries consist of several decision variables and
each decision variable is limited to certain predefined values.
ID3 aims to build a decision tree according to entropy and
information gain. By eliminating the most useless variable, the
training data set can be split into two subsets. One contains the
datum entries using that variable, and the other does the rest.
Then, the described procedure is applied to these two subsets
recursively.
For the perturbation procedure, the fitness difference values,
denoted as df , are obtained by subtracting the fitness value af-
ter perturbation from the original fitness value. This operation
implicitly isolates the affected portions of the whole problem
structure and reveals them as fitness difference values. The
k-trap function [21], [22] is employed in this study as an
illustrative example as well as the elementary sub-problem for
composing larger problem instances:
trapk(s1s2s3 . . . sk) =
{
k, if u = k;
k − u− 1, otherwise. ,
where u is the number of 1’s in the solution string. Figure 1
shows the characteristic of a k-trap function.
With k-trap functions as elementary sub-problems, more
complicated problem instances can be created following the
ADF model. For example, an 8-bit function composed of a
3-trap and a 5-trap function can be defined as
f(s1s2s3 . . . s8) = trap3(s1s2s3) + trap5(s4s5s6s7s8) .
s1s2s3 . . . s8 f df
111 01001 5 3
111 10100 5 3
111 01111 3 3
000 11111 7 1
000 00100 5 1
000 00001 5 1
000 10110 3 1
000 11100 3 1
001 01000 4 1
001 00011 3 1
001 00011 3 1
001 10100 3 1
010 01000 4 1
010 00100 4 1
010 01100 3 1
010 10100 3 1
010 00111 2 1
010 11011 1 1
100 00000 5 -1
100 00100 4 -1
110 11111 5 -1
110 01101 1 -1
110 01111 0 -1
110 11011 0 -1
101 10000 3 -1
101 01101 1 -1
101 11110 0 -1
011 00001 3 -3
011 00110 2 -3
011 01111 0 -3
TABLE I
RESULTS OBTAINED BY PERTURBING VARIABLE s1 .
By conducting perturbation on a binary variable s1, the
fitness difference df is obtained as
df = f(s1s2s3 . . . s8)− f(s1s2s3 . . . s8)
= (trap3(s1s2s3) + trap5(s4s5s6s7s8))
− (trap3(s1s2s3) + trap5(s4s5s6s7s8))
= trap3(s1s2s3)− trap3(s1s2s3) .
(1)
Equation (1) gives a mathematical explanation that df is
only affected by the perturbed variable s1 and those variables
belonging to the same sub-problem as s1. Table I is the
example of Equation (1) and shows that permutations of s1,
s2, and s3 yield the identical df value.
ILI considers the distinct df values as the classification
categories and each variable as the decision variable for the
ID3 algorithm. By performing ID3 on the perturbed variable
as the tree root, a decision tree is accordingly constructed.
The internal nodes on the decision tree are then collected
as a linkage set Vi. Figure 2 shows the built decision tree
corresponding to Table I, and the internal nodes s1, s2, and
s3 forms a linkage set.
B. Original ILI
The original ILI [5] can handle only those problem struc-
tures composed of non-overlapping sub-problems. After per-
turbing a variable and constructing a decision tree as shown in
Figure 2, a linkage set is identified, and the used variables are
removed from the variable set. The procedure of perturbation
Algorithm 1 Modified ILI for general problem structures.
1: procedure ILI(f , ℓ, n)
2: Initialize a population P with n strings of length ℓ
3: Evaluate the fitness of strings in P using f
4: V ← Shuffle(1, 2, 3, . . . , ℓ)
5: Mℓ×ℓ ← 0ℓ×ℓ
6: for each v in V do
7: for each si = si1si2si3 . . . sil in P do
8: Perturb siv
9: df i ← calculate the fitness difference
10: end for
11: Build an ID3 tree using (P, df) with v as root
12: for each internal node vj in the tree do
13: mv,j ← 1
14: mj,v ← 1
15: end for
16: end for
17: Return the structure matrix M
18: end procedure
the perturbation and ID3 tree construction are performed on
variable s1, the resultant linkage set is {s1, s2, s3, s4} and the
rest elements are {s5, s6} where the relations between {s3, s4}
and {s5, s6} are lost. One of the proposed modifications is to
perturb and perform ID3 on each variable si without removing
any variable such that all variables can be examined repeatedly
by ID3.
Another modification is to make ILI not directly return
linkage sets corresponding to sub-problems, which are also
referred to as building blocks. As aforementioned, the concept
of building blocks is not very clear when sub-problems are
overlapping. In order to determine the overall problem struc-
ture, a ℓ-by-ℓ matrix Mℓ×ℓ is employed, where ℓ is the number
of variables. The element mi,j = 1 if there is a connection
between variables si and sj ; otherwise, mi,j = 0. In this
study, we make linkages undirected. After si is perturbed and
a linkage set containing sj is constructed, not only mi,j but
mj,i are also marked.
Algorithm 1 shows the modified inductive linkage identifi-
cation procedure. For further illustration, the modified ILI is
demonstrated by an example composed of two 4-trap functions
with two shared variables defined as
f(s1s2s3s4s5s6) = trap4(s1s2s3s4) + trap4(s3s4s5s6) (2)
and shown in Figure 4. Initially, the structure matrix M6×6
is a zero-matrix indicating that there is no known interaction
among any variables as showed in Figure 4(a). After initializa-
tion, ILI begins to perturb variables in a randomly determined
order: s1, s3, s2, s5, s4, and s6. By perturbing and performing
ID3 on variable s1, a linkage set {s1, s2, s3, s4} is recognized
and indicates that s1 interacts with s2, s3, and s4. Figure 4(b)
shows the detected partial structure. Notice that although s2,
s3, and s4 belong to the same sub-problem as defined in
Equation (2), there is no interaction among them detected at
the current iteration. Next, when s3 is perturbed, ID3 identifies
(a) Initial state. (b) After perturbing s1.
(c) After perturbing s3. (d) After perturbing s2.
(e) After perturbing s5. (f) Final state.
Fig. 4
PROBLEM STRUCTURES DETECTED DURING THE ILI PROCESS. DASHED
AND SOLID LINES REPRESENT KNOWN AND NEWLY DISCOVERED
INTERACTIONS RESPECTIVELY.
that s3 interacts with all other five variables since it belongs to
both sub-problems as shown in Figure 4(c). The procedure is
repeated on s2, s5, s4, and s6 sequentially. After all variables
are proceeded, the final structure is constructed as shown in
Figure 4(f).
Because there is no controlling parameter for the problem
order/complexity, the obtained linkage information of the
overall problem structure is unconstrained by any assumptions
on the complexity of sub-problems. The only key factor in
this condition regarding the correctness is whether or not the
employed population is large enough for ILI to avoid getting
confused by the fitness difference noise. Our preliminary
experiments involving different problem structures have shown
that the proposed ILI modification is able to construct correct
problem structures as long as sufficiently large populations are
utilized. For the purpose of gaining more understanding of the
population size requirement by the modified ILI, in the next
section, we design and conduct more experiments to observe
the scalability and flexibility of the proposed modification.
IV. EXPERIMENTS AND RESULTS
Experiments and results on circular structures are examined
in this section. Circular structures hold certain good properties
for experimental control. The number of linkages increases
linearly with the number of sub-problems and so does the
number of nodes. These easily controlled properties enable us
to concentrate on the population requirement.
0 10 20 30 40 50
400
500
600
700
800
900
1000
1100
1200
sub−problems: n
po
pu
la
tio
n 
siz
e
 
 
C4
n
y = 209.85 × ln(x) + 265.89
(a) trap4 functions as sub-problems.
0 10 20 30 40 50
2000
2500
3000
3500
4000
4500
5000
5500
sub−problems: n
po
pu
la
tio
n 
siz
e
 
 
C5
n
y = 1106.3 × ln(x) + 1013.7
(b) trap5 functions as sub-problems.
Fig. 6
THE POPULATION REQUIREMENT FOR THE MODIFIED ILI TO CORRECTLY
DETECT CIRCULAR PROBLEM STRUCTURES COMPOSED OF trap4 AND
trap5 FUNCTIONS.
correctly identify the isolated as well as the interdependent
parts of a large problem structure without additional cost.
V. SUMMARY AND CONCLUSIONS
In this paper, we extended the inductive linkage identi-
fication to detect general problem structures composed of
overlapping sub-problems and conducted experiments by using
circular overlapping structures for gaining more insights and
understandings. According to the experimental observations,
the proposed technique was found able to correctly detect
circular problem structures and require a population size
growing logarithmically with the problem size. The population
requirement was observed insensitive to the problem structure
consisting of similar sub-structures for the identical overall
problem size.
One of the major differences between ILI and most of the
other existing linkage learning methods is the absence of algo-
(a) C42
n
(b) C43
n
Fig. 7
CIRCULAR PROBLEM STRUCTURES COMPOSED OF SEPARATE
SUB-STRUCTURES, WHERE n = 5.
0 10 20 30 40 50
400
500
600
700
800
900
1000
1100
1200
sub−problems: n
po
pu
la
tio
n 
siz
e
 
 
C4
n
C42
n
C43
n
y = 209.85 × ln(x) + 265.89
Fig. 8
CIRCULAR PROBLEM STRUCTURES COMPOSED OF ONE, TWO, AND THREE
SUB-STRUCTURES WITH trap4 AS THE ELEMENTARY SUB-PROBLEMS.
rithmic parameters for the complexity of sub-problems. The
proposed modification of ILI keeps this feature unchanged.
Since ILI performs the task of linkage identification without
assumptions on the problem structure, such as the chosen
probabilistic model or the maximum degree of interactions, the
relationship among variables should be extracted as authentic
as possible.
Since the modified ILI is capable of detecting general
problem structures, it may be applied in two ways. Firstly,
by serving as a preprocessing step of genetic algorithms, the
proposed techniques describes the variable dependencies with
a graph such that delicately-designed genetic operators or
processing mechanisms can utilize the linkage information to
preserve the building blocks. Secondly, the proposed technique
can be used as a tool to inspect and extract the relationship
among decision variables for understanding the inner structure
of the problem at hand in order to assist any further applicable
operations.
XCS with Bit Masks
Jia-Huei Lin and Ying-ping Chen
Abstract—In this paper, a modified XCS is proposed to reduce
the numbers of learned rules. XCS is a type of learning classifier
systems and has been proven able to find accurate, maximal gen-
eralizations. However, XCS usually produces too many rules such
that the readability of the classification model is greatly reduced.
As a result, XCS users may not be able to obtain the desired
knowledge or useful information from the learned rule set. In
our attempt to handle this problem, a new mechanism, called bit
masks, is devised in order to reduce the number of classification
rules and therefore to improve the readability of the generated
model. A series of n-bit multiplexer experiments, including 6-bit,
11-bit, and 20-bit multiplexers, to examine the performance of
the proposed framework. For the problem composed of integer-
typed variables, two synthetic oblique datasets, Random-Data2
and Random-Data9, are adopted to compare the performance
of XCS and that of the proposed method. According to the
experimental results, XCS with bit masks can perform similarly
as XCS on n-bit multiplexers and generates significantly fewer
rules on integer-typed problems.
I. INTRODUCTION
Learning classifier systems (LCS) [1] are machine learning
systems designed to combine reinforcement learning, evolu-
tionary computation, and other heuristics to produce efficient
adaptive systems. These rule-based machine learning algo-
rithms originated and have evolved in the cradle of evolution-
ary computation and artificial intelligence. There have been
a number of studies on the architecture and performance of
LCS. In recent years, a simplified version of LCS, XCS [2], [3]
has become one of the most important XCS variations since
XCS was shown to be able to solve real-world classification
problems with high accuracy. XCS is designed to evolve a
representation of the best solution as well as to evolve a
complete and accurate payoff map of all possible solutions
for all possible problem instances. That is, XCS evolves rules
that improve the ability to obtain the environmental reward
and mine the environment for prediction patterns, which are
expressed in the form of classifiers. The repeatedly refined
prediction patterns allow the XCS system to make better
decisions for consecutive actions.
However, some shortcomings still exist in XCS. For real-
world applications, frequent pattern mining [4] often incurs
numerous frequent item sets and rules, which much decrease
the effectiveness of data mining since users have to go through
a large number of mined rules in order to find useful ones. The
great number of classification rules lowers the readability of
the classification model in real-world applications. Since the
XCS also produces numerous rules, in this study, XCS with bit
masks is proposed to handle such a problem. The developed
Jia-Huei Lin and Ying-ping Chen are with the Department of Computer
Science, National Chiao Tung University, 1001 Ta Hsueh Road, Hsinchu,
TAIWAN (email: {jhlin, ypchen}@nclab.tw).
mechanism of bit masks is used to detect stable building
blocks in classifiers and to prevent crossover and/or mutation
operators from unnecessarily altering them. Consequently, the
resultant classification model needs fewer rules than that
evolved by the original XCS to achieve the same level of
accuracy. A series of n-bit multiplexer experiments, including
6-bit, 11-bit, and 20-bit multiplexers, are exploited to examine
the performance of the proposed method. For the integer-typed
problems, two synthetic oblique datasets [5], Random-Data2
and Random-Data9, are used to compare the performance of
XCS and that of the proposed method. According to the exper-
imental results, XCS with bit masks can perform similarly as
XCS on n-bit multiplexers and generates significantly fewer
rules on integer-typed problems.
For the remainder of this paper, section II briefly reviews
XCS. Section III introduces the representation and the algorith-
mic structure of XCS with bit masks. Section IV describes the
experiments and provides the experimental results, followed by
section V which concludes this paper.
II. A BRIEF REVIEW OF XCS
In this paper, we firstly describe the framework of XCS,
followed by an introduction of XCSI, which is an adaptation
of XCS for integer-typed problems. Finally, we discuss the
related work of this research.
A. XCS
XCS, introduced by Wilson in 1995 [2], is an important
branch of LCS [1]. XCS has become known as one of the most
reliable learning classier systems for handling data mining
and machine learning problems. In this section, we give
an overview of the key components of XCS, including the
representation, the performance component, the reinforcement
component, the discovery component, the macroclassifiers,
and the covering and subsumption deletion.
1) Representation: XCS evolves a set of condition-action
rules which are called the population of classifiers. The
condition-action rules is the representation of the knowledge
gained from the environment. Each classifier consists of five
main components and several additional estimates.
• Condition: The condition part C checks if the classifier
matches the environment event.
• Action: The action part A specifies the decided action
when the condition matches the environment event.
• Payoff prediction: The payoff prediction p estimates the
average payoff after executing the action in response to
the environment event.
• Prediction error: The prediction error e estimates the
average error of the payoff prediction.
the generalization capability of XCS. There are two forms
of subsumption, GA-subsumption and Action-subsumption. In
GA-subsumption, when new classifiers are generated, they are
examined to see whether their conditions are subsumed by
their parent classifiers or not. If the parent classifiers are more
general than the new classifiers, the new classifiers are sub-
sumed by the parents. The new classifiers will not be added to
[P ], but the numerosity of the parent classifiers is incremented.
Otherwise, the system puts the new classifiers into [P ]. Action-
subsumption is different from GA subsumption. Each action
set is searched for the most general classifier R. All other
classifiers in the set are compared to R to see whether R
subsumes them. The subsumed classifiers are deleted from [P ].
7) Flow of XCS: Firstly, XCS initializes the rule set with
zero reward randomly. There are four steps for the rule
evaluation cycle. The steps are
1) The state of the environment is detected by detectors.
2) The system examines the condition part of each rule to
determine the match set.
3) The match set will be grouped into different sets based
on their own actions, and the prediction payoff for each
action is calculated to determinate the chosen action.
4) Effectors implement the action in the environment, get
the reward, and distribute it to the rules in the action set.
After a specified period of time, GA is executed to generate
new rules and delete unfit rules in the rule discovery cycle.
Wilson [3] indicated that they can find the classification rules
with high accuracy with this framework.
B. XCSI
Since many problems involve integer attributes, a variation
of XCS, called XCSI [5], for integer-typed problems is pro-
posed. The modification in XCSI includes the presentation,
mutation, covering, and subsumption. In XCSI, the presenta-
tion of the classifier condition part is changed from a string
of {0, 1,#} to a concatenation of the interval predicates,
inti = (li, ui), where li and ui are integers and denote the
lower bound and the upper bound. A classifier matches an
event x with attributes xi if and only if ∀xi li ≤ xi ≤ ui.
For the mutation operator in XCSI, Wilson indicated that the
best method to mutate an allele is adding a value ±rand(m0),
where m0 is a fixed integer, rand picks an integer randomly
from (0,m0], and the sign is selected equiprobably. The cover-
ing operator occurs if there is no classifier matches x. In XCSI,
the new condition has components {l0, u0, . . . , ln, un}, where
each li = xi− randi(r0) and each ui = xi+ randi(r0). r0 is
also a fixed integer and randi picks an integer randomly from
[0, r0]. An interval predicate i subsumes another predicate j
if li ≤ lj and ui ≥ lj . The subsumption of a classifier defined
if every interval predicate in the first classifier’s condition
subsumes the predicate in the second classifier’s condition.
III. XCS WITH BIT MASKS
As aforementioned, XCS is a promising methodology be-
cause of its versatility and capability. However, XCS is known
to generate too many rules, which lower the readability of the
resultant classification model. That is, the XCS user may be
unable to get the needed knowledge or useful information out
of the generated model.
XCS with bit masks is proposed in this study to handle such
a problem. The proposed mechanism is used to detect stable
building blocks in classifiers and to prevent crossover and
mutation operators from unnecessarily altering these building
blocks. Consequently, the resultant classification model needs
fewer rules than that evolved by the original XCS to achieve
the same level of accuracy.
In this section, we will firstly introduce the concept and
mechanism of bit masks into XCS. Then, we discuss how bit
masks are implemented in the XCS framework, followed by
describing how XCS with bit masks is applied in different
environments.
A. Representation
In order to introduce bit masks to XCS classifiers, the repre-
sentation of XCS rules is modified to make capable of finding
a set of stable building blocks composed of the attributes that
should not be altered. For this purpose, a parameter, bit masks
(BM), is added into the classifier representation as
< Classifier >::= < Condition >:< Action >:< BM >:
< Payoff prediction >:< Payoff error >:
< Fitness >
BM indicates the condition attributes that should not be altered
in mutation and/or crossover operators. Rules with BM will
be stabler than the standard XCS rules, and fewer classifiers
will be created when the mutation and crossover operation is
triggered. For example, if the rules of the condition and the
action are set as Table I, attributes B and D are determined
as stable building blocks in BM. Different from the standard
XCS, when the mutation and crossover operation occur, the
condition attributes in BM will not be altered to avoid gener-
ating redundant rules.
TABLE I
EXAMPLE OF A BIT MASK DATA SET (BM = {B, D}).
A B C D E Class
Event 1 0 1 0 1 2
Rule1 1 0 1 0 1 2
Rule2 # 0 1 0 1 2
Rule3 1 0 # 0 1 2
Rule4 1 0 1 0 # 2
The purpose of BM is to prevent unnecessary alteration.
The rules generated by mutation and crossover operations in
the standard XCS may not match the original event and some
redundant rules might be created. Through the mechanism
of bit masks, rules with BM can prevent stable building
blocks from being altered. The collection of rules will strongly
support the input event and may cover more subset of cases.
IV. EXPERIMENTAL RESULTS
We employ XCS and XCS with bit masks to handle three
series of experiments and compare their performance, system
errors, and population sizes. Performance refers to the fraction
of the last 50 exploit trials that were correct. System error
refers to the average of the absolute difference between the
system prediction for the chosen action and the actual external
payoff, divided by the total payoff range, which is 1000 in this
study, over the last 50 exploit trials. Population size refers
to the number of macroclassifiers. We use the XCS system
implementation publicly available on the Internet [10]. The
XCS system is modified to integrate with the BM mechanism.
Each experiment is conducted for 200 independent runs, and
the averaged statistics are reported.
A. Experimental Datasets
1) Boolean Multiplexers: Firstly, we use XCS and XCS
with bit masks to tackle Boolean multiplexers of three different
sizes: 6 bits, 11 bits, and 20 bits. Boolean multiplexers are
defined for binary strings of length ℓ = k + 2k. The function
value is determined by treating the first k bits as an address
that indexes into the remaining 2k bits, and the value of the
indexed bit, either 0 or 1, is the function value.
2) Integer Test Functions: Secondly, we use XCS and XCS
with bit masks to deal with integer datasets. The integer
datasets are synthetic oblique data sets [5]. The first dataset,
Random-Data2 is constructed by random vectors (x1, x2),
with each xi a random integer from [1, 10]. The outcome
o(x1, x2) for each vector is defined as
o(x1, x2) =
{
1 x1 + x2 ≥ 11
0 Otherwise . (1)
An instance of Random-Data2 is composed by a vector and
its outcome. The second integer dataset, Random-Data9, is
constructed similar to Random-Data2 as follows. Random-
Data9 has 9 dimensions, and the expression determining the
outcome is defined as
o(~x) =
{
1
∑
xi ≥ 50
0 Otherwise . (2)
3) Wisconsin Breast Cancer (WBC): Finally, we use XCS
and XCS with bit masks to handle a real-world problem, which
is the Wisconsin Breast Cancer (WBC) database, donated to
the UCI repository [11] by Prof. Olvi Mangasarian. WBC
contains 699 instances collected over time by Dr. William H.
Wolberg. Each instance in WBC has 9 attributes which are
Clump Thickness, Uniformity of Cell Size, Uniformity of Cell
Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare
Nuclei, Bland Chromatin, Normal Nucleoli, and Mitoses. Each
attribute has a value between 1 and 10 inclusive. Data rows
look like
1000025, 5, 1, 1, 1, 2, 1, 3, 1, 1, 2
1017122, 8, 10, 10, 8, 7, 10, 9, 7, 1, 4
1016277, 6, 8, 8, 1, 3, 4, 3, 7, 1, 2
The first number is a label, the following 9 numbers are the
attributes, and the last number is the class level, where 2 stands
for Benign and 4 for Malignant.
TABLE II
EXPERIMENTAL PARAMETERS FOR BOOLEAN MULTIPLEXERS
N α β γ θga ε χ µ P#
6-bit 400 0.1 0.2 0.95 25 10 0.8 0.04 0.5
11-bit 800 0.1 0.2 0.95 25 10 0.8 0.04 0.5
20-bit 1600 0.1 0.2 0.95 25 10 0.8 0.04 0.5
B. Results for Boolean Multiplexers
Boolean multiplexers of three different sizes, 6-bits, 11-bits,
and 20-bits, are experimented on in this series of experiments,
and the parameters are listed in Table II.
1) 6-bit Multiplexer: Figure 5(a) shows the experimental
results for the 6-bit Boolean multiplexer. As we can observe,
XCS gets approximately 100% performance in around 8000
exploit trails, and XCS with bit masks also gets approximately
100% performance in around 8000 exploit trails. For the
system error, XCS gets approximately 0.5% system error in
around 8000 exploit trails, and XCS with bit masks gets
approximately 0.3% system error in around 8000 exploit trails.
Finally, XCS evolves a population with 29.47 classifiers on
average, and XCS with bit masks evolves a population with
25.01 classifiers on average.
We can find that XCS and XCS with bit masks can achieve
similar performance and system error rate the number of
exploit trails is appropriate. Hence, that XCS and XCS with
bit masks have the same speed of convergence is shown, and
the effect of integrating bit masks into XCS appears. XCS
with bit masks can save on average 15.13% of the population
size for the 6-bit multiplexer over 200 runs.
2) 11-bit Multiplexer: Figure 5(b) shows the results for the
11-bit Boolean multiplexer. From the figure, the performance
of XCS reaches approximately 99% in around 9000 exploit
trails, and the performance of XCS with bit masks also reaches
approximately 99% in around 90000 exploit trails. The system
error of XCS gets approximately 1% in around 13000 exploit
trails, and the system error of XCS with bit masks gets
approximately 0.8% in around 13000 exploit trails. For the
population size, XCS creates 81.51 classifiers on average, and
XCS with bit masks creates 74.85 classifiers on average.
From the experimental results, we can know that the out-
come for the 11-bit Boolean multiplexer is similar to that for
the 6-bit one. In this experiment, XCS with bit masks saves
on average 8.17% of the population size over 200 runs.
3) 20-bit Multiplexer: Figure 5(c) demonstrates the exper-
imental results for the 20-bit Boolean multiplexer. From the
result, XCS gets approximately 99% performance in around
39000 exploit trails, and XCS with bit masks also gets approx-
imately 99% performance in around 39000 exploit trails. XCS
gets approximately 1% system error in around 60500 exploit
trails, and XCS with bit masks also gets approximately 1%
system error in around 60500 exploit trails. For the population
size, XCS evolves a population with 261.52 classifiers on
average, and XCS with bit masks evolves a population with
247.67 classifiers on average. XCS with bit masks saves on
average 5.30% of the population size.
0 50 100 150 200 250 300 350 400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Explore problems
 
 
XCS Bit Mask Performance
XCS Performance
XCS Bit Mask Population Size
XCS Population Size
XCS Bit Mask Error Rate
XCS Error Rate
(a) 2 dimensions. Population sizes are divided by 150.
50 100 150 200 250 300 350 400 450 500
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Explore problems
 
 
XCS Bit Mask Performance
XCS Performance
XCS Bit Mask Population Size
XCS Population Size
XCS Bit Mask Error Rate
XCS Error Rate
(b) 9 dimensions. Population sizes are divided by 800.
Fig. 6. Results for integer test functions averaged over 200 runs. Explore
problems are in exploit trails divided by 100.
β = 0.2,γ = 0.95, θga = 25, ε0 = 10, χ = 0.8, µ = 0.04,
and P# = 0.5.
Based on the experimental results for the WBC database,
we can find that XCS and XCS with bit masks obtain similar
performance and system error rate, while for the population
size, the effect of adopting bit masks becomes remarkably
significant that XCS with bit masks saves on average 65.13%
of the population size over 200 runs.
In order to further justify the results on the WBC database,
a tenfold cross-validation test is conducted on the WBC
database. Table IV shows the results. We can observe that
on average, XCS gets an accuracy of 93.20%, and XCS
with bit masks gets 92.50%. The cross-validation reveals
that compared to XCS, XCS with bit masks trades in an
insignificant amount of performance for a significant save on
the population size.
0 50 100 150 200 250 300
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Explore problems
 
 
XCS Bit Mask Performance
XCS Performance
XCS Bit Mask Population Size
XCS Population Size
XCS Bit Mask Error Rate
XCS Error Rate
Fig. 7. Experimental results for the WBC database averaged over 200 runs.
Population sizes are divided by 350. Explore problems are in exploit trails
divided by 100.
TABLE IV
RESULTS OF A TENFOLD CROSS-VALIDATION ON THE WBC DATABASE.
XCS XCS with bit masks
#1 0.94 0.90
#2 0.94 0.91
#3 0.97 0.94
#4 0.91 0.93
#5 0.90 0.96
#6 0.96 0.91
#7 0.96 0.90
#8 0.90 0.91
#9 0.93 0.96
#10 0.91 0.93
Avg. 0.9320 0.9250
E. Discussion
Based on the experiment results presented in the previous
sections, we can find two interesting points. The bit mask
mechanism can help XCS to save the population size more in
integer domains than it can in Boolean domains. As for the
6-bit, 11-bit, and 20-bit Boolean multiplexers, XCS with bit
masks only saves less than 20% of the population size, while
in integer domains, it saves more than 30%, or even more than
60% for the WBC database, of the population size.
Thus, we can know that the difference between Boolean
attributes and integer attributes is quite significant in classi-
fication problems. When XCS is applied to handle Boolean
multiplexers, the representation of rules is {0, 1,#}, and it is
relatively easy to for the classification system to gain knowl-
edge from the environment by matching events and modifying
rules. However, in integer domains, the rule representation is
inti = (li, ui), where li and ui are integers, denoting the
lower bound and the upper bound. A rule matches an event
x if and only if li ≤ xi ≤ ui for all attribute xi. It is
easy to see that flexibility embedded in the representation not
only help to handle the high cardinality of integers but also
World Automation Congress © 2010 TSI Press. 
 
ESTIMATION OF DISTRIBUTION ALGORITHMS: BASIC IDEAS AND 
FUTURE DIRECTIONS 
 
YING-PING CHEN 
Department of Computer Science 
National Chiao Tung University 
HsinChu City 300, Taiwan 
ypchen@nclab.tw 
 
 
ABSTRACT— 
Estimation of distribution algorithms (EDAs) are a class of evolutionary algorithms which can be 
regarded as abstraction of genetic algorithms (GAs) because in the design of EDAs, the population, 
one of the GA distinctive features, is replaced by probabilistic models/distributions. Building and 
sampling from the models substitute for the common genetic operators, such as crossover and 
mutation. Due to their excellent optimization performance, EDAs have been intensively studied and 
extensively applied in recent years. In order to interest more people to join the research of EDAs, 
this paper plays as an entry level introduction to EDAs. It starts with introducing the origination and 
basic ideas of EDAs, followed by presenting the current EDA frameworks, which are broadly 
applied in many scientific and engineering disciplines. Finally, this paper also describes some 
ongoing topics and potential directions in the hope that readers may get further insights into EDAs. 
 
Key Words: Estimation of distribution algorithm, probabilistic model building genetic algorithm, 
global optimization, evolutionary algorithm, evolutionary computation, computational intelligence. 
1. INTRODUCTION 
Genetic algorithms (GAs) were proposed by Holland [1] with the inspiration of Darwinian view on the 
evolutionary mechanisms in nature. They were initially designed for generating classifiers in learning 
classifier systems as well as handling combinatorial optimization problems. Brought to the attention of 
many researchers by Goldberg’s book [2], genetic algorithms have been widely and successfully applied to 
solving all kinds of search and optimization problems existing in numerous disciplines for the past decades. 
The proposal of genetic algorithms is remarkably intriguing because it strongly connects several seemingly 
not-so-related fields, such as biology, mathematical programming (optimization), artificial intelligence, etc., 
places itself in a unique position among these fields to stir innovations, and makes a major contribution to 
the creation of evolutionary computation. Similar to the progress of most scientific and engineering 
development, soon after its birth, the GA taskforce splits and focuses on topics of different origins and 
requirements. Some researchers explore potential applications of GAs, while others try to improve GA 
performance by incorporating natural, biological mechanisms or by advancing algorithmic designs with 
mathematical techniques. Among these attempts to devise better genetic algorithms or, more broadly, 
evolutionary algorithms, is the development of estimation of distribution algorithms. 
By focusing on the performance and discarding the biological plausibility, estimation of distribution 
algorithms (EDAs) successfully achieve the design goal and can be viewed as abstraction of GAs because 
in EDAs, the population, one of the GA distinctive features, is replaced by some mathematical construction, 
and genetic operators are correspondingly changed to work with the adopted mathematical construction. 
According to the traditional GA performance indicator, function evaluations vs. solution quality, EDAs 
outperform GAs in most cases because the design of EDAs makes the search explicitly centralized by 
processing global statistics. Even if the significant computational cost of the mathematical construction is 
taken into consideration, the performance of EDAs is still usually superior to that of GAs. Thanks to their 
desirable features and properties, EDAs have been studied, improved, and broadly utilized for more than 
fifteen years. Given their importance in evolutionary computation and usefulness in application domains, 
an entry level introduction to EDAs is needed for those interested in getting familiar with and utilizing 
EDAs in a short time. Consequently, this paper is written to fulfill such a purpose. In particular, basic ideas, 
existing frameworks, and potential research directions of EDAs are briefly described. Note that this paper is 
not intended to be a complete survey or to provide details of EDAs. Interested readers may refer to [3, 4]. 
Estimation of Distribution Algorithms: Basic Ideas And Future Directions 
probabilistic models are the chosen mathematical construction to “describe” populations. Since the process 
to find a probabilistic model for a given population is to estimate the probabilistic distributions on decision 
variables, such algorithms are called estimation of distribution algorithms, or sometimes, probabilistic 
model building genetic algorithms (PMBGAs). After gathering and mining the information existing in the 
form of individuals, the offspring individuals are then created by sampling the built probabilistic model to 
implement the process of information exploitation. Figure 1(b) shows an EDA in its simplest form. We can 
see between Figure 1(a) and Figure 1(b) that the key differences between GAs and EDAs are using 
probabilistic distributions to model populations and replacing genetic operators with the functionally 
equivalent mechanisms—probabilistic model building and sampling. 
2.3 Estimation of Distribution Algorithms 
There have been numerous variants of estimation of distribution algorithms proposed in the literature. 
Some of them adopt probabilistic models of different types or complexities, while others employ different 
techniques to build model. All these studies and developments on estimation of distribution algorithms 
started after the proposal of population-based incremental learning (PBIL) by Baluja [4] in 1994, while the 
name of “estimation of distribution algorithms” was firstly proposed by H. Mühlenbein and G. Paaß [5] in 
1996. PBIL uses a probability vector to replace the population. Slightly different from most existing EDAs 
in which the probabilistic model is built from scratch at every generation as shown in Figure 1(b), PBIL 
retains some memory or experience of which the weight can be adjusted by the user. If the weight is set to 
zero, PBIL becomes a commonly structured EDA which is exactly the univariate marginal distribution 
algorithm (UMDA) proposed by Mühlenbein in [6] 1997. Early studies on EDAs began with simple 
probabilistic models of which the decision variables of optimization problems were assumed independent 
of each other. More and more complicated probabilistic models were used in the follow-up work along this 
line, which will be discussed in the following section. 
3. EXISTING EDA FRAMEWORKS 
This section will introduce some popular EDA frameworks proposed in the literature and widely used 
in both research and practice. Since probabilistic models are the key component in EDAs, we will introduce 
the EDAs employing simple models first and then those adopting complex models. Although the EDAs that 
adopt complex models usually provide excellent performance, one must keep in mind that complex models 
themselves may induce spurious variable relationships. If such spurious relationships become an obstacle 
which prevents the EDA from solving problems, EDAs with simpler models, i.e., more suitable for the 
problem structure, should be used to obtain better performance. Because the frameworks described in this 
section will be only a fraction of all existing EDAs, interested readers should consult other materials [7, 8]. 
3.1 All Variables Are Considered Independent 
The simplest, reasonable probabilistic model to work with EDAs is assuming that no interaction exists 
between variables. EDAs employing such a model estimate the probabilistic distributions of values in 
different ways, including PBIL [4], UMDA [6], and the compact genetic algorithm (cGA) [9]. These EDAs 
work very well on problems composed of building blocks of order one and may encounter difficulties when 
facing problems consisting of longer, misleading building blocks. 
3.2 Interactions between Two Variables Are Considered 
In order to take into account the interactions between variables, probabilistic models considering 
pairwise interactions are intuitive choices. The mutual information maximization for input clustering 
(MIMIC) [10] algorithm assumes that the pairs of interacting variables are chained by their relationships, 
while the combining optimizers with mutual information trees (COMIT) [11] algorithm models the all the 
pairwise relationships with a dependency tree. The bivariate marginal distribution algorithm (BMDA) [12] 
further considers that all the pairwise relationships can be modeled with several independent dependency 
trees, i.e., a forest. 
3.3 Interactions among More Than Two Variables Are Considered 
Finally, the probabilistic models considering multivariate dependencies are adopted in EDAs. As a rule 
of thumb, EDAs with more general, complicated probabilistic models are able to handle more difficult 
5. CONCLUSIONS 
In this paper, estimation of distribution algorithms (EDAs) as a popular class of evolutionary 
algorithms have been reviewed. EDAs can be regarded as abstraction of genetic algorithms (GAs) because 
in EDAs, the population, one of the GA distinctive features, is replaced by probabilistic models, and the 
common genetic operators, e.g., crossover, mutation, etc., are replaced by building and sampling from the 
adopted probabilistic model. By pursuing optimization performance instead of insisting on biological 
plausibility, EDAs successfully accomplish their design goal and become more and more popular in recent 
years. This paper was written with the intention to provide an entry level introduction to EDAs for 
researchers and practitioners who are in need and interested in knowing and using EDAs in a short time. 
Basic ideas, existing frameworks, and potential research directions of EDAs were briefly described in the 
hope that more and more taskforces will join the research as well as applications of EDAs. 
ACKNOWLEDGMENTS 
The work was supported in part by the National Science Council of Taiwan under Grant NSC-98-
2221-E-009-072. The author is grateful to the National Center for High-performance Computing for 
computer time and facilities. 
REFERENCES 
1. J. H. Holland, Adaptation in natural and artificial systems, University of Michigan Press, 1975. 
2. D. E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Addison-
Wesley Publishing Co., 1989. 
3. D. E. Goldberg, K. Deb, and J. H. Clark, “Genetic algorithms, noise, and the sizing of populations,” 
Complex Systems, vol. 6, no. 4, 1992, pp. 333-362. 
4. S. Baluja, Population-based incremental learning: A method for integrating genetic search based 
function optimization and competitive learning, Tech. Rep. No. CMU-CS-94-163, Carnegie Mellon 
University, Pittsburgh, PA, 1994. 
5. H. Mühlenbein and G. Paaß, “From recombination of genes to the estimation of distributions I. 
Binary parameters,” Proceedings of the Fourth Parallel Problem Solving from Nature (PPSN IV), 
1996, pp. 178-187. 
6. H. Mühlenbein, “The Equation for Response to Selection and Its Use for Prediction,” Evolutionary 
Computation, vol. 5, no. 3, 1997, pp. 303-346. 
7. M. Pelikan, D. E. Goldberg, and F. G. Lobo, “A Survey of Optimization by Building and Using 
Probabilistic Models,” Computational Optimization and Applications, vol. 21, 2002, pp. 5-20. 
8. P. Larrañaga and J. A. Lozano, eds., Estimation of Distribution Algorithms: A New Tool for 
Evolutionary Computation, Kluwer Academic Publishers, 2002. 
9. G. R. Harik, F. G. Lobo, and D. E. Goldberg, “The compact genetic algorithm,” Proceedings of the 
International Conference on Evolutionary Computation (ICEC'98), 1998, pp. 523-528. 
10. J. S. De Bonet, C. L. Isbell, and P. Viola, “MIMIC: Finding optima by estimating probability 
densities,” Advances in Neural Information Processing Systems, vol. 9, 1997, pp. 424. 
11. S. Baluja and S. Davies, “Using optimal dependency-trees for combinatorial optimization: Learning 
the structure of the search space,” Proceedings of the 14the International Conference on Machine 
Learning, 1997, pp. 30-38. 
12. M. Pelikan and H. Mühlenbein, “The bivariate marginal distribution algorithm,” Advances in Soft 
Computing—Engineering Design and Manufacturing, 1999, pp. 521-535. 
13. M. Pelikan, D. E. Goldberg, and E. Cantú-Paz, “BOA: The Bayesian optimization algorithm,” 
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-99), 1999, pp. 
525-532. 
14. R. Etxeberria and P. Larrañaga, “Global optimization with Bayesian networks,” Proceedings of the 
Second Symposium on Artificial Intelligence (CIMAF-99), 1999, pp. 332-339. 
15. D. J. Coffin and R. E. Smith, “The Limitations of Distribution Sampling for Linkage Learning,” 
Proceedings of 2007 IEEE Congress on Evolutionary Computation (CEC 2007), 2007, pp. 364-369. 
16. D. J. Coffin and R. E. Smith, “Why Is Parity Hard for Estimation of Distribution Algorithms,” 
Proceedings of ACM SIGEVO Genetic and Evolutionary Computation Conference 2007 (GECCO-
2007), 2007, pp. 624. 
On the Optimization of Degree Distributions in LT Code with
Covariance Matrix Adaptation Evolution Strategy
Chih-Ming Chen, Student Member, IEEE, Ying-ping Chen, Member, IEEE,
Tzu-Ching Shen, and John K. Zao, Senior Member, IEEE
Abstract— Luby Transform code (LT code) has been a popular
and practical technique in the field of channel coding since its
proposal. One of the key components of LT code is a degree
distribution which is used to determine the relationship between
source data and codewords. Luby in his proposal suggested two
general methods to construct feasible degree distributions. Such
general designs work appropriately in typical situations but not
optimally in most cases. To explore the full potential of LT code,
in this work, we make the first attempt to introduce evolutionary
algorithms to optimize the degree distribution in LT code. Degree
distributions are encoded as real-valued vectors and evaluated by
numerical simulation of LT code. For applications of different
natures, two objectives are implemented to search good degree
distributions with different decoding behavior. Compared with
the original design, the experimental results are quite promising
and demonstrate that the degree distribution can be customized
for different purposes. In addition to manually adjusting the
degree distribution as the common practice, the work presented
in this paper provides an efficient alternative approach to use
and adapt LT code for both practitioners and researchers.
I. INTRODUCTION
Digital fountain code [1] is a popular class of erasure
code in the field of communication. The concept of fountain
code was first introduced by Byers et al. [2] in 1998. Firstly,
source data are divided into several pieces with an identical
length. The length of each piece can be any bits or even
several bytes. Sender generates encoding packets, or called
encoding symbols when the packet length is one bit, by
some particular encoding operation. The encoding and sending
procedure may repeat independently and unlimitedly. Infinite
encoding packets are sent out continuously like a fountain,
which is an important property of fountain code called rateless.
If a receiver is interested in receiving the data, it can receive
the packet flow at any time and collect the packets in any
combination. Once sufficient packets, of which the amount
is usually slightly more than that of the source data, are
obtained, the source data can be fully recovered. During the
process, no further communication is required between sender
and receiver. Encoding information can be embedded in each
packet. As a result, digital fountain code is especially useful
in broadcast or other situations in which back channels are un-
available. Moreover, because source data can be reconstructed
no matter which packets are received, fountain code is also
considered reliable to handle the problem of packet loss.
Chih-Ming Chen, Ying-ping Chen, Tzu-Ching Shen, and John K. Zao
are with the Department of Computer Science, National Chiao Tung Uni-
versity, 1001 Ta Hsueh Road, Hsinchu, TAIWAN (email: ccming@nclab.tw,
ypchen@nclab.tw, Stecko.cs97g@nctu.edu.tw, jkzao@cs.nctu.edu.tw).
Luby Transform code (LT code) [3] proposed by Luby
in 2002 is the first practical framework of fountain code.
A novel coding mechanism based on a specifically designed
degree distribution is proposed in the introduction of LT code.
The performance of LT code totally depends on the adopted
degree distribution. In his proposal, Luby deigned general
methods to construct an appropriate degree distribution to
be used in LT code, and the degree distribution was named
soliton distribution. Via theoretical analysis, the feasibility of
soliton distribution was proven in the literature [4]. Recently,
researchers started to optimize the degree distribution in order
to improve the performance of LT code [5], [6], but the ob-
tained improvement is quite limited. In these studies, only the
parameters of soliton distribution were tuned and considered
as decision variables, while in the present work, we directly
consider the degree distribution itself as our decision variables.
Based on LT code, an improved framework call Raptor
codes [7], [8] was proposed by Shokrollahi. Shokrollahi in-
tegrated LT code with a pre-coding layer. Compared with
pure LT code, the design of Raptor codes requires a degree
distribution, called weakened LT, with some very different
behavior and properties. Several instances were given in [9]
for certain particular sizes of source symbols, but there are no
existing guidelines regarding how to construct suitable degree
distributions for other sizes. In this regard, we demonstrate
the use of optimization techniques proposed in evolutionary
computation for generating degree distributions of different,
desired properties.
In this paper, according to our limited knowledge, we
make the first attempt to utilize evolutionary computation
techniques to optimize the degree distribution for LT code and
demonstrate the feasibility of customizing degree distributions
for different purposes. Particularly, we adopt the covariance
matrix adaptation evolution strategy (CMA-ES) [10] to di-
rectly optimize degree distributions for two goals: reducing
the overhead and lowering the failure rate. The experimental
results are remarkably promising and show that significantly
reduced overheads and lower failure rates can be achieved for
LT code with the obtained degree distribution for a wide range
of source symbol sizes.
The remainder of this paper is organized as follows. Sec-
tion II describes the detailed operations of LT code, including
the coding process and soliton distribution proposed by Luby.
Section III introduces the evolutionary algorithm used in this
paper. Experiments and results are given in section IV. Finally,
section V concludes this paper.
WCCI 2010 IEEE World Congress on Computational Intelligence 
July, 18-23, 2010 - CCIB, Barcelona, Spain CEC IEEE
978-1-4244-8126-2/10/$26.00 c©2010 IEEE 3531
However, ideal soliton distribution works poorly in practice.
Belief propagation may be suspended by a small variance
of the stochastic encoding/decoding situation in which no
ripple exists, because the expected ripple size is only one at
any moment. According to the theory of random walk, the
probability with which a random walk of length k deviates
from its mean by more than ln(k/δ)
√
k is at most δ. It is a
baseline of ripple sizes which must be maintained to complete
the decoding process. Hence, in the same paper by Luby, a
modified version called robust soliton distribution, µ(d), was
also proposed.
Robust soliton distribution µ(d):
R = c · ln(k/δ)
√
k
τ(d) =
 R/ik for d = 1, . . . , k/R− 1R ln(R/δ)/k for d = k/R0 for d = k/R+ 1, . . . , k . (2)
β =
k∑
d=1
(ρ(d) + τ(d))
µ(d) =
ρ(d) + τ(d)
β
for d = 1, . . . , k (3)
c and δ are two parameters for tuning robust soliton distribu-
tion. c controls the mean of the degree distribution. Smaller
values of c increase the probability of low degrees and larger
ones decrease it. δ estimates that there are ln(k/δ)
√
k expected
ripple size as described. Fig. 1(b) is an example of robust
soliton distribution with c = 0.1 and δ = 0.1. Robust soliton
distribution can ensure that only K = k + O(ln2(k/δ)√k)
encoding symbols are required to recover the source data with
a successful probability at least 1-δ.
Robust soliton distribution is not only viable but also
practical. The analysis of robust soliton distribution based on
probability and statistics is sound if k is infinite. However,
in practice, source data cannot be divided into infinite pieces,
and as a consequence, the behavior of LT code will not exactly
match the mathematical analysis, especially when k is small.
Furthermore, robust soliton distribution is a general purpose
design. It provides a convenient way to construct a distribution
works well but not optimally. In this work, we try to customize
the degree distribution by using optimization tools proposed
in the field of evolutionary computation.
III. OPTIMIZATION METHOD
Evolution strategies (ES) are a major branch of evolutionary
computation and have been developed since early 1960s. The
key idea of ES is to evolve strategic parameters as well as deci-
sion variables. ES is well-known to be quite capable of dealing
with continuous optimization problems. One of the simplest
ES is (1+1)-ES where only one child is produced by Gaussian
mutation to compete with its parent in each generation, and
the other is (1, 1)-ES which is equivalent to random walk.
Current general versions of ES are denoted as (µ+, λ)-ES.
The covariance matrix adaptation evolution strategy (CMA-
ES) [10] was firstly introduced by Hansen in 1996 and is one
of the most popular real-parameter optimization methods in
evolutionary computation. There are some variants of CMA-
ES proposed in the literature [12], [13], [14]. The search ability
of CMA-ES has been theoretically analyzed and empirically
verified on certain classic optimization problems, such as Ack-
ley’s function, Griewank’s function, and Rastrigin’s function.
In CMA-ES, only a few algorithmic parameters need to be
decided because CMA-ES inherits the mechanism to adapt
strategic parameters during the evolutionary process. In this
work, CMA-ES is utilized to optimize the degree distribution
in LT framework for a wide range of k, the size of source
symbols. In the remainder of this section, the way to adopt
CMA-ES to handle the optimization of degree distributions
are presented in detail.
A. Decision Variables
The first step to use an evolutionary algorithm is to encode
the decision variables of the optimization problem. It is
not difficult in this study because a degree distribution can
directly form a real-number vector. In the evaluation phase,
a real-number vector of arbitrary values can be interpreted
as a probability distribution, i.e., a degree distribution, with
normalization. Such an operation does not change the fea-
sibility, although the problem complexity may be slightly
increased. The definition of degree distributions tells us that
d ≤ k. For a specific source symbol size k, obviously the
problem dimensions is at most k. However, according to the
LT encoding/decoding operations, we usually do not need a
non-zero probability on every single degree. Observing the
soliton distributions and considering the belief propagation
algorithm, there is no necessary degree except 1, which ensures
the start of belief propagation. As a result, we optimize a
selected subset of degrees in the present work. We choose
some degrees called tags to form the vector v(i) of decision
variables according to the Fibonacci numbers smaller than half
of k. A degree distribution used in this paper hence can be
represented as the following formula.
Optimized degree distribution ω(d):
ω(d) =
{
v(i) d = the i-th Fibonacci number, d < k/2
0 otherwise .
(4)
B. Objectives
We try to use two indicators to evaluate degree distributions
for LT code in this paper. The first one is the efficiency of
the LT code with the optimized degree distribution which has
been discussed in section II-B. ε denotes the expected rate of
overhead to transmit data. For example, ε = 1.2 means that in
addition to the size of source data, 20% extra data are needed
to recover the complete source data. This objective is to obtain
some degree distribution for a specific k with the smallest
ε. LT code is rateless, and the coding process depends on
randomness and probability. Source data recovered by a fixed
amount of encoding symbols cannot be guaranteed. Therefore,
3533
1 2 3 5 8 13 21 34
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.2 1.4 1.6 1.8 2
0
0.2
0.4
0.6
0.8
1
   AVG : 1.2438
Overhead
Su
cc
es
sf
ul
 ra
te
(a) k = 100
1 2 3 5 8 13 21 34 55 89 144
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4 1.5 1.6
0
0.2
0.4
0.6
0.8
1
   AVG : 1.1465
Overhead
Su
cc
es
sf
ul
 ra
te
(b) k = 400
1 2 3 5 8 13 21 34 55 89 144 233
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4
0
0.2
0.4
0.6
0.8
1
   AVG : 1.126
Overhead
Su
cc
es
sf
ul
 ra
te
(c) k = 700
1 2 3 5 8 13 21 34 55 89 144 233 377
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4
0
0.2
0.4
0.6
0.8
1
   AVG : 1.1083
Overhead
Su
cc
es
sf
ul
 ra
te
(d) k = 1000
Fig. 4
LEFT FIGURES SHOW THE OPTIMIZED DEGREE DISTRIBUTIONS. ONLY TAGS ARE PRESENTED. RIGHT FIGURES ARE THE HISTOGRAM AND
ACCUMULATED CURVE OF SUCCESSFUL RATE IN 1000 INDEPENDENT SIMULATION RUNS
3535
0 100 200 300 400 500 600 700 800 900 1000
10−2
10−1
100
Function Evaluations
Fa
il 
ra
te
 
 
k = 100
k = 400
k = 700
k = 1000
Fig. 6
EVOLUTIONARY PROCESS DURING THE OPTIMIZATION OF FAILURE RATE
TABLE II
THE BEST INDIVIDUALS FOR THE OPTIMIZATION OF FAILURE RATE
Degree k=100 k = 400 k = 400 k = 1000
1 0.083997 0.102892 0.116854 0.115278
2 0.573671 0.383164 0.29678 0.333564
3 0.161178 0.237312 0.31115 0.241065
5 0.08038 0.186475 0.171342 0.184027
8 0.096245 0.030706 0.033393 0.046818
13 0.001267 0.039075 0.025977 0.022223
21 0.002963 0.015193 0.023452 0.022914
34 0.000299 0.000167 0.016096 0.020526
55 0 0.001276 0.002602 0.00643
89 0 0.000303 0.000268 0.004594
144 0 0.003436 0.002072 0.001422
233 0 0 0.000015 0.000883
377 0 0 0 0.000257
The value becomes smaller when k increases, and that
is why the trend of Fig. 3 shows a declination. The val-
ues of overhead are reduced at least 10% for all k’s with
the optimized degree distributions. Some distributions of the
best individuals are given in Table I. Fig. 4 illustrates each
distribution and shows the histogram of successful rate in
1000 simulation runs on the right side. Compared with similar
simulation results of robust soliton distribution in Fig. 5, the
improvement is quite significant.
B. Failure rate
Unlike the original LT code, we are concerned with how
many source symbols can be recovered in the second set
of experiments. The objective value is the average number
of source symbols that cannot be recovered with a constant
overhead ε. Optimization results are shown in Fig. 6. More
function evaluations are needed to search for good degree
distributions. The failure rate of the final results are less than
10−1 for all k’s when ε = 1.1. In other words, more than
90 percent of source symbols can be recovered if extra 10
percent of encoding symbols are collected. Table II gives
1 2 3 5 8 13 21 34
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Degree
Pr
ob
ab
ili
ty
1 1.05 1.1 1.15 1.2
10−2
10−1
100
Overhead
Fa
il 
ra
te
 o
f s
ym
bo
ls
 
 
Uniform
Robust Soliton
Optimized
(a) k = 100
1 2 3 5 8 13 21 34 55 89 144
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.05 1.1 1.15 1.2
10−2
10−1
100
Overhead
Fa
il 
ra
te
 o
f s
ym
bo
ls
 
 
Uniform
Robust Soliton
Optimized
(b) k = 400
1 2 3 5 8 13 21 34 55 89 144233
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Degree
Pr
ob
ab
ili
ty
1 1.05 1.1 1.15 1.2
10−3
10−2
10−1
100
Overhead
Fa
il 
ra
te
 o
f s
ym
bo
ls
 
 
Uniform
Robust Soliton
Optimized
(c) k = 700
1 2 3 5 8 13 21 34 55 89 144233377
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Degree
Pr
ob
ab
ili
ty
1 1.05 1.1 1.15 1.2
10−3
10−2
10−1
100
Overhead
Fa
il 
ra
te
 o
f s
ym
bo
ls
 
 
Uniform
Robust Soliton
Optimized
(d) k = 1000
Fig. 7
THE FIGURE SHOWS THE SIGNIFICANT DIFFERENCE OF FAILURE RATE
AFTER OPTIMIZATION. SIMILAR TO THAT IN FIG. 4, ONLY TAGS ARE
SHOWN IN THE FIGURES
the best probability distributions found in the evolutionary
process for k = 100, k = 400, k = 700, and k = 1000.
The simulation results of a constant overhead are presented
in Fig. 7. The red line denotes the behavior of uniform
distribution, which is the initial value of optimization. Most
of the source symbols remain covered except for those of
which the degree is one, i.e., with probability 1/k. The same
situation happens to robust soliton distributions because the
3537
Optimizing Degree Distributions in LT Codes by Using The
Multiobjective Evolutionary Algorithm Based on Decomposition
Chih-Ming Chen, Student Member, IEEE, Ying-ping Chen, Member, IEEE,
Tzu-Ching Shen, and John K. Zao, Senior Member, IEEE
Abstract— Luby Transform code (LT code) is the first practical
digital fountain code and has been widely used as basic compo-
nents in many communication applications. The coding behavior
of LT code is mainly decided by a probability distribution of
codeword degrees. In order to customize a degree distribution
for different purposes, multi-objective evolutionary algorithm is
introduced to optimize degree distributions in this paper. Two
critical performance indicators of LT code are considered in our
experiments. Some applications hope to minimize the overhead of
extra packets and some require to limit the computational cost of
the coding system. To handle this problem, MOEA/D is applied
to optimize two objectives simultaneously. We expect to obtain
the Pareto front (PF) formed by partial optimal solutions and
provide those available degree distributions to different LT code
applications. Not only promising results are represented in this
paper but also the behavior of LT code is thoroughly explored by
optimizing the degree distribution according to multi-objectives.
I. INTRODUCTION
Digital fountain code [1] is a popular class of erasure code
in the field of communication. The concept of fountain code
was introduced by Byers et al. [2] in 1998. Firstly, source data
are divided into several pieces with an identical length. The
length of each piece can be any number of bits or even several
bytes. Sender generates encoding packets, or called encoding
symbols, when the packet length is one bit, by certain encoding
operation. The encoding procedure may repeat independently
and indefinitely so infinite encoding packets are sent out
continuously like a fountain, which is an important property
of fountain code called rateless. If a receiver is interested in
receiving the data, it can receive the packet flow at any time
and collect the packets in any combination. Once sufficient
packets, of which the amount is usually slightly more than
that of the source data, are obtained, the source data can be
fully recovered. During the process, no further communication
is required between sender and receiver. Encoding information
can be embedded in each packet. As a result, digital fountain
code is especially useful in broadcast or other situations
in which back channels are unavailable. Moreover, because
source data can be reconstructed no matter which packets are
received, fountain code is also considered reliable to handle
the problem of packet loss.
Luby Transform code (LT code) [3] proposed by Luby
in 2002 is the first practical framework and implementation
of fountain code. A novel coding mechanism based on a
Chih-Ming Chen, Ying-ping Chen, Tzu-Ching Shen, and John K. Zao
are with the Department of Computer Science, National Chiao Tung Uni-
versity, 1001 Ta Hsueh Road, Hsinchu, TAIWAN (email: ccming@nclab.tw,
ypchen@nclab.tw, Stecko.cs97g@nctu.edu.tw, jkzao@cs.nctu.edu.tw).
specifically designed degree distribution is proposed in the
introduction of LT code. The performance of LT code totally
depends on the adopted degree distribution. In his proposal,
Luby designed general methods to construct appropriate de-
gree distributions to co-operate with LT code, and the degree
distributions were named soliton distribution. Via theoretical
analysis, the feasibility of soliton distribution was proven [4].
Recently, researchers started to optimize the degree distribu-
tion in order to improve the performance of LT code [5], [6],
but the obtained improvement is marginal and quite limited.
In these studies, only the parameters of soliton distribution
were tuned and considered as decision variables, while in our
present work, we directly consider the degree distribution itself
as our decision variables.
In the design of LT code, redundant data and encoding
computation are used to trade for the ability of forward error
correction. For most applications, while the error correction
ability is maintained, both costs are required to be as lower as
possible, and apparently there is a trade-off among these fac-
tors. Furthermore, applications of different types and purposes
have different requirements of each kind of cost. Some LT
code applications which transmit data through an expensive
communication channel have to reduce the data overhead.
Other applications with a huge package size expect fewer
executions of the encoding operator. In order to simultaneously
satisfy these applications, multi-objectives are considered for
optimizing the LT code degree distribution in the present work.
The most important motivation of this study is to fully explore
the LT coding behavior with arbitrary degree distributions
and to empirically provide a proof of concept that multiple
requirements on LT code can be satisfied via optimizing degree
distributions with existing optimization techniques.
The remainder of this paper is organized as follows. Sec-
tion II describes the detailed operations of LT code, including
the coding process and soliton distribution. Section III intro-
duces the background of multi-objective problems and the
evolutionary algorithm used in this paper. Experiments and
results are given in sections IV and V. Finally, section VI
concludes this paper.
II. LT CODE
Luby introduced a practical fountain code framework and
gave the details of coding operation in 2002 [3]. Similar to
other fountain codes, source symbols are uniformly randomly
chosen to be encoded into codewords (encoding symbols). The
encoding operation is achieved by a simple boolean operator,
WCCI 2010 IEEE World Congress on Computational Intelligence 
July, 18-23, 2010 - CCIB, Barcelona, Spain CEC IEEE
978-1-4244-8126-2/10/$26.00 c©2010 IEEE 3635
Ideal soliton distribution ρ(d):
ρ(d) =
{ 1
k for d = 1
1
d(d−1) for d = 2, 3, . . . , k
. (1)
Ideal soliton distribution guarantees that all the release prob-
abilities are identical to 1/k at each subsequent step. Hence,
there is one expected ripple generated at each processing step
when the encoding symbol size is k. After k processing step,
the source data can be ideally recovered. Fig. 1(a) shows an
example of Ideal soliton distribution for k = 30.
However, ideal soliton distribution works poorly in practice.
Belief propagation may be suspended by a small variance
of the stochastic encoding/decoding situation in which no
ripple exists, because the expected ripple size is only one at
any moment. According to the theory of random walk, the
probability with which a random walk of length k deviates
from its mean by more than ln(k/δ)
√
k is at most δ. It is a
baseline of the ripple queue size which must be maintained
to complete a decoding process. Hence, in the same paper
by Luby, a modified version called robust soliton distribution,
µ(d), was also proposed.
Robust soliton distribution µ(d):
R = c · ln(k/δ)
√
k
τ(d) =
 R/ik for d = 1, . . . , k/R− 1R ln(R/δ)/k for d = k/R0 for d = k/R+ 1, . . . , k . (2)
β =
k∑
d=1
(ρ(d) + τ(d))
µ(d) =
ρ(d) + τ(d)
β
for d = 1, . . . , k (3)
c and δ are two parameters for tuning robust soliton distribu-
tion. c controls the mean of the degree distribution. Smaller
values of c increase the probability of low degrees, and larger
ones decrease it. δ estimates that there are ln(k/δ)
√
k expected
ripples as described. Fig. 1(b) is an example of robust soliton
distribution with c = 0.1 and δ = 0.1. Robust soliton
distribution can ensure that only K = k + O(ln2(k/δ)√k)
encoding symbols are required to recover the source data with
a successful probability at least 1-δ.
Robust soliton distribution is not only viable but also
practical. The analysis of robust soliton distribution based on
probability and statistics is sound if k is infinite. However,
in practice, source data cannot be divided into infinite pieces,
and as a consequence, the behavior of LT code will not exactly
match the mathematical analysis, especially when k is small.
Furthermore, robust soliton distribution is a general purpose
design. It provides a convenient way to construct a distribution
works well but not optimally. In this work, we try to customize
the degree distribution by using multi-objective optimization
tools proposed in the field of evolutionary computation to
simultaneously satisfy multiple performance requirements.
III. MULTI-OBJECTIVE PROBLEMS
Multi-objective optimization problems (MOPs) are very
important in real-world applications. There are two or more
objectives to be considered simultaneously, and these ob-
jectives usually conflict with each other. The most intuitive
approach to deal with MOPs is to transform them into single
objective problems (SOPs) by using weights on the objectives
and creating a weighted sum. The approach makes the problem
solvable by available tools based on mathematics or heuristics
for SOPs. However, such weights oftentimes cannot be pre-
determined, especially when the domain knowledge of the
problem is unavailable. Furthermore, the best solution to the
transformed single-objective problem is merely one solution
on the Pareto front (PF) of the MOP. Hence, better opti-
mization frameworks must be developed to fulfill the need
of handling MOPs.
Due to the limitation of traditional mathematical methods
for MOPs, more and more researchers try to solve MOPs
in a direct way and to approximate the Pareto front as
complete as possible. Their goal is to provide a set of solutions
which are partially optimal. Many advanced multi-objective
algorithms have been proposed in the literature. Some of them
try to approximate the PF by using mathematical models,
and others are developed based on evolutionary algorithms.
A hybrid framework makes use of decomposition methods in
mathematics and the optimization paradigm in evolutionary
computation was proposed and called multiobjective evolu-
tionary algorithm based on decomposition (MOEA/D) [8].
MOEA/D was proposed and shown to perform well on MOPs
with complicated Pareto set shapes [9].
In this paper, we propose the use of MOEA/D to opti-
mize the multiple objectives of LT code. Degree distribu-
tions significantly better than robust soliton distribution are
expected. Moreover, exploring a complete Pareto front can
help researchers to analyze the trade-off between overhead
and operational cost of LT code. In the following section, we
will give the formal description of MOPs and the MOEA/D
framework, respectively.
A. Formal description of MOPs
In real-world applications, many problems are actually
multi-objective optimization problems, and single-objective
problems are special cases. A multi-objective problem can be
formally stated as:
minimize F (x) = (f1(x), . . . , fm(x))
subject to
{
x ∈ Ω
C(x) = (c1(x), . . . , ct(x)) ≥ 0
, (4)
where Ω is called the decision space or variable space, and
Rm is the objective space. C(x) represents the problem con-
straints and defines the feasible regions in the decision space
according to problem properties [10]. F : Ω→ Rm consists of
m objective functions. If Ω is a closed and connected region
in Rn and all the objective functions are continuous, we call
the problem a continuous MOP.
3637
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5
0
50
100
150
200
250
300
350
Overhead
O
pe
ra
tio
na
l C
os
t
 
 
Gen. 10
Gen. 50
Gen. 100
Gen. 150
Fig. 2
EVOLUTIONARY PROCESS DURING THE OPTIMIZATION FOR k = 100
1 2 3 4 5 6 7
0
200
400
600
800
1000
1200
Overhead
O
pe
ra
tio
na
l C
os
t
 
 
Gen. 10
Gen. 50
Gen. 100
Gen. 150
Fig. 3
EVOLUTIONARY PROCESS DURING THE OPTIMIZATION OF k = 300
our simulation of LT code, encoding symbols are generated
until source data are fully recovered. The average required
codewords are calculated as the fitness. The other objective is
the computational cost of the encoding and decoding process.
Such an objective value can be estimated with the mean degree
of degree distributions. If Md denotes the mean value of
a degree distribution, the number of how many times XOR
is executed can denote as (Md − 1) ∗ ε. There is a trade-
off between ε and Md because when Md is greater, fewer
encoding symbols may be required, and therefore, ε is less.
On the other hand, Md is the operational cost, which is the
average number of XOR operations that have to be executed.
V. EXPERIMENTAL RESULTS
In most multi-objective problems, there is usually a trade-
off between objectives. For an n-objective problem, a solution
can be represented as a point in the n-dimensional space.
All points which denote the non-dominated solutions form
a partial optimal set called the Pareto front. The mission of
multi-objective algorithms is to approximate the Pareto front
1 2 3 4 5 6 7
0
500
1000
1500
2000
Overhead
O
pe
ra
tio
na
l C
os
t
 
 
Gen. 10
Gen. 50
Gen. 100
Gen. 150
Fig. 4
EVOLUTIONARY PROCESS DURING THE OPTIMIZATION OF k = 500
as complete as possible. In other words, solutions should well
spread to provide sufficient choices to decision markers. In
our experiments, overhead and operational cost of LT code
are both minimized together and the minimal value of each
objective is expected. Clearly, a degree distribution with the
minimal operational cost has only non-zero probability on
degree one because in such a case, no encoding operation is
needed. The case is the pure transmission without any channel
coding, and it is a special case in the LT code framework.
As for the other objective, overhead has a lower bound at
ratio 1. Each encoding symbol can generate a new ripple to
recover a source symbol ideally such that at least k encoding
symbols are required to reconstruct the original data. Different
from the operational cost, such a degree distribution is not
yet discovered and even its existence is not proved. Fig. 2
shows the optimization process and the final result. After 150
generations, a significant PF is represented by fifty individual
points. The solution with the minimal operational cost in
expectation has been found, but the best overhead is 1.2068.
Several individuals are listed in Table II, where the best value
of overhead and operational cost are presented in columns 2
and 3, respectively. Columns 4 and 5 give the average overhead
and execution counts of XOR in the numerical simulation.
Figs. 3 and 4 display similar results as that shown in Fig. 2
for k = 300 and k = 500. Fig. 7 presents the distribution and
simulation results for each individual listed in Table II.
To our limited knowledge, there is no guideline to design a
robust soliton distribution for some particular coding behav-
iors. In order to fairly compare our optimized results with that
of robust soliton distribution, MOEA/D is also applied to op-
timize the parameters of robust soliton distribution, which are
c and δ. The PF of the optimized robust soliton distributions
is presented in Fig. 5. In the dimension of operational cost,
the optimized robust soliton distributions deliver very similar
results because robust soliton distributions can also become
the degree distribution with only non-zero probability on
degree one if some appropriate parameters are given. However,
3639
1 2 3 4 5 6 7 8 9 10 11
0
0.2
0.4
0.6
0.8
1
Degree
Pr
ob
ab
ili
ty
2 4 6 8 10 12
0
0.2
0.4
0.6
0.8
1
   AVG : 5.2179
Overhead
Su
cc
es
sf
ul
 ra
te
(a) Individual 1, c = 9.72 and δ = 0.00107
1 2 3 4 5 6 7 8 9 10 11
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Degree
Pr
ob
ab
ili
ty
2 4 6 8 10
0
0.2
0.4
0.6
0.8
1
   AVG : 4.0975
Overhead
Su
cc
es
sf
ul
 ra
te
(b) Individual 25, c = 1.634 and δ = 0.185
1 2 3 4 5 6 7 8 9 10 11 12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Degree
Pr
ob
ab
ili
ty
1 2 3 4 5 6
0
0.2
0.4
0.6
0.8
1
   AVG : 2.5998
Overhead
Su
cc
es
sf
ul
 ra
te
(c) Individual 35, c = 2.146 and δ = 0.978
1 2 3 4 5 6 7 8 9 10 11 12 13
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Degree
Pr
ob
ab
ili
ty
1 1.5 2 2.5 3 3.5 4
0
0.2
0.4
0.6
0.8
1
   AVG : 1.9328
Overhead
Su
cc
es
sf
ul
 ra
te
(d) Individual 45, c = 0.96 and δ = 0.601
0 10 20 30 40 50
0
0.1
0.2
0.3
0.4
0.5
Degree
Pr
ob
ab
ili
ty
1 1.5 2 2.5
0
0.2
0.4
0.6
0.8
1
   AVG : 1.3174
Overhead
Su
cc
es
sf
ul
 ra
te
(e) Individual 50, c = 0.0521 and δ = 0.931
Fig. 6
SIMULATION RESULTS OF OPTIMIZED ROBUST SOLITON DISTRIBUTIONS
3641
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期： 99 年 8 月 17 日 
一、參加會議經過 
今年 (2010) 的 IEEE 演化計算學術會議 (IEEE Congress on Evolutionary 
Computation) 於 7 月 18 日至 7 月 23 日在西班牙巴塞隆那 (Barcelona, Spain) 
的 Centre de Convencions Internacional de Barcelona 會議中心舉行。 
整個會議的議程包括多場精彩的 Plenary Talks、十數場 Tutorials、Workshops、
數項演化計算方法成果競賽、以及論文之口頭報告和海報展。其內容之豐富、主
題之精彩，實為演化計算界最高水準之國際學術會議，並足顯演化計算領域蓬勃
發展，受到各界之重視。 
 
 
計畫編號 NSC 98 － 2221 － E － 009 － 072 － 
計畫名稱 研究與發展專為無線網路系統客製化之最佳化演算架構 
出國人員
姓名 陳穎平 
服務機構
及職稱 國立交通大學資訊工程系副教授 
會議時間 99 年 7 月 18 日至 99 年 7 月 23 日 會議地點 Barcelona, Spain 
會議名稱 
(中文) 2010 IEEE 演化計算學術會議 
(英文) 2010 IEEE Congress on Evolutionary Computation 
發表論文
題目 
(中文) 研究以 CMA-ES 進行 LT Codes 所採用之度分佈進行最佳化 
(英文) On the Optimization of Degree Distributions in LT Codes with 
Covariance Matrix Adaptation Evolution Strategy 
附件四 
 3
出席補助制度，方能迅速提升台灣學界在全世界的能見度、從而全面性地增進台
灣的研究水準與學術國力。 
 
五、攜回資料名稱及內容 
攜回資料有研討會論文集之光碟一片。 
 
六、其他 
無。 
 
II. LT CODE
Luby introduced a new fountain code framework and gave
the detail of coding operation in 2002 [3]. Similar to other
fountain codes, source symbols are randomly chosen to be
encoded into codewords (encoding symbols). The encoding
operation is achieved by a simple boolean operator, XOR.
The relation between source data and encoding symbols can
be modeled as a sparse bipartite graph. A critical change
in LT code is to decide the degree of each vertex in the
bipartite graph with a probability distribution. The connectivity
can be recorded as a encoding matrix and each column
represents an encoding symbol. Originally, k source symbols
can be fully decoding by Gaussian elimination if there exist k
linearly independent columns. However, Gaussian elimination
is prohibitively expensive for its computational complexity of
O(k3). Therefore, the belief propagation (BP) algorithm [11]
is introduced to replace the expensive Gaussian elimination in
the LT decoding phase. Overhead of coding is used to trade
computing time because belief propagation is more efficient
but more encoding symbols are needed for successful decod-
ing. Moreover, the performance of LT code is very sensitive to
the degree distribution. A good degree distribution is necessary
to co-operate with belief propagation. Luby suggested soliton
distributions for LT framework in his proposal of LT code.
According to the mathematical verification, the properties of
soliton distribution have been confirmed. In this section, details
of coding operations and soliton distributions are described.
A. Encoding and decoding
Given the source data, we suppose that the source data
can be cut into k source symbols with the same length of
ℓ bits. Before every codeword is generated, a degree d is
chosen at random according to the adopted degree distribution
ρ(d), where 1 ≤ d ≤ k and ∑kd=1 ρ(d) = 1. The degree d
decides the how many distinct source symbols will be chosen
to compose an encoding symbol. d source symbols, called
neighbors, are chosen uniformly randomly and accumulated
by XOR. In the design of LT code, random numbers play
an essential role during the encoding process. The approach
employed by LT code for a sender to inform receivers of all
encoding information is achieved by synchronizing a random
number generator with a specified random number seed.
At the receiver side, when K encoding symbols were arrived
which is usually slightly larger than k, belief propagation is
used to reconstruct the source data step by step. All encoding
symbols are initially covered in the beginning. For the first
step, all encoding symbols with only one neighbor can be
directly released to recover their unique neighbor. When a
source symbol has been recovered but not processed, it is
called a ripple and will be stored in a queue. At each
subsequent step, ripples are popped as a processing target one
by one. A ripple is removed from all encoding symbols which
have it as neighbor. If an encoding symbols has only one
remaining neighbor after the removing, the releasing action
repeats and may produce new ripples to maintain a stable
size of the queue. Maintaining the size of the ripple queue is
0 5 10 15 20 25 30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degree
D
is
tri
bu
tio
n
(a) Ideal soliton distribution
0 5 10 15 20 25 30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degree
D
is
tri
bu
tio
n
(b) Robust soliton distribution
Fig. 1
EXAMPLE OF SOLITON DISTRIBUTIONS (K = 30)
important because the decoding process fails when the ripple
queue is empty and some source symbols remain uncovered.
In other words, more encoding symbols are required in the
decoding process. Ideally, the process succeeds if all source
symbols are recovered at the end of the decoding process.
B. Soliton distribution
The behavior of LT code is completely determined by the
degree distribution, ρ(d), and the number of encoding symbols
received, K, by receiver. The overhead ε = K/k denotes the
performance of LT code, and ε depends on a given degree
distribution. Based on his theoretical analysis, Luby proposed
the ideal soliton distribution of which the overhead is 1, the
best performance, in the ideal case.
Ideal soliton distribution ρ(d):
ρ(d) =
{ 1
k for d = 1
1
d(d−1) for d = 2, 3, . . . , k
. (1)
Ideal soliton distribution guarantees that all the release prob-
abilities are identical to 1/k at each subsequent step. Hence,
there is exactly one expected ripple generated at each pro-
cessing step when the encoding symbol size is k. After k
processing step, the source data can be ideally recovered.
Fig. 1(a) shows an example of ideal soliton distribution for
k = 30.
3532
0 20 40 60 80 100 120 140 160 180 200
1
1.5
2
2.5
Function Evaluations
O
ve
rh
ea
d
 
 
k = 100
k = 400
k = 700
k = 1000
Fig. 2
EVOLUTIONARY PROCESS DURING THE OPTIMIZATION OF OVERHEAD
TABLE I
THE BEST INDIVIDUALS FOR THE OPTIMIZATION OF OVERHEAD
Degree k=100 k = 400 k = 400 k = 1000
1 0.091397 0.116375 0.16058 0.129707
2 0.310884 0.255701 0.148543 0.266133
3 0.367223 0.34174 0.412275 0.321489
5 0.042648 0.112072 0.119163 0.077045
8 0.053247 0.071726 0.052843 0.124503
13 0.048949 0.028076 0.024701 0.000258
21 0.011876 0.013169 0.035112 0.019594
34 0.073776 0.030397 0.017738 0.033607
55 0 0.000264 0.002094 0.01543
89 0 0.01109 0.009837 0.00095
144 0 0.01939 0.002946 0.000143
233 0 0 0.014167 0.00075
377 0 0 0 0.010391
in order to evaluate ε, we provide infinite encoding symbols,
in the form of a stream of encoding symbols, to simulate
the decoding process until all source data are recovered. The
average of required encoding symbols per simulation is the
fitness value of degree distributions.
The second indicator is the amount of source symbols
that cannot be recovered when a constant ratio of encoding
symbols are received. In raptor codes, Low-density-parity-
check (LDPC) [15] is introduced as a second layer pre-coding
into LT code. LDPC is a kind of forward error correction
codes. More information on LDPC can be found in [16], [17].
LDPC can fix errors of data without extra information as long
as the error rate is lower than certain restriction. In such a
condition, the mission of LT code is no longer to achieve full
decoding. Instead, most of source symbols can be recovered
with a small overhead is sufficient. For this purpose, we try to
minimize the number of un-recovered source symbols given a
constant overhead ε.
100 200 300 400 500 600 700 800 900 1000
1.05
1.1
1.15
1.2
1.25
1.3
1.35
1.4
1.45
1.5
1.55
Source symbols size k
O
ve
rh
ea
d
 
 
Robust Soliton Distribution
Optimization Result
Fig. 3
AVERAGE PERFORMANCE INDICATORS ARE COMPARED BETWEEN ROBUST
SOLITON DISTRIBUTION AND OPTIMIZED DEGREE DISTRIBUTIONS FOR
DIFFERENT NUMBERS OF SOURCE SYMBOLS (k)
IV. EXPERIMENTS AND RESULTS
Two series of experiments are implemented for the two
different objectives as described in the previous section. In
each experiment, tags are determined by Fibonacci numbers
and the specified source symbols size k. Tags are encoded
as an individual, v(i), and represent that only these degrees
have non-zero probabilities. Initial values of tags are set
as 1/|v| uniformly, and then CMA-ES is applied without
any customization or modification. After a new individual is
created, it is normalized to be a valid probability distribution
and evaluated for the fitness value by simulating the LT coding
process. One hundred independent runs of simulation are
conducted for each function evaluation. In the first series of
experiments, we minimize the expected number of encoding
symbols for full decoding. In the second, the average number
of source symbols that cannot be recovered for a constant
ε = 1.1 is considered. We call the second indicator as failure
rate. The default parameter settings given in the source code
of CMA-ES are adopted in this study except for λ = 10.
A. Overhead
In these experiments, we minimize the overhead ε for differ-
ent k sizes, and the results are shown in Table I and Figs. 2–
5. Fig. 2 presents the improvement during the evolutionary
process. Individuals are initially uniform distributions. It is
expected that overheads are quite high in the beginning and the
curves descend quickly after around 100 function evaluations.
Finally, the fitness almost converges after 200 function evalua-
tions. Fig. 3 shows the comparison of ε between robust soliton
distribution and the optimized distributions. The expected
overhead of robust soliton distribution is given as
k +O(log2(k/δ)√k)
k
= 1 +O
(
log2(k/δ)√
k
)
.
3534
0 5 10 15 20
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.2 1.4 1.6 1.8 2
0
0.2
0.4
0.6
0.8
1
   AVG : 1.3987
Overhead
Su
cc
es
sf
ul
 ra
te
(a) k = 100
0 5 10 15 20 25 30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4 1.5 1.6
0
0.2
0.4
0.6
0.8
1
   AVG : 1.2805
Overhead
Su
cc
es
sf
ul
 ra
te
(b) k = 400
0 5 10 15 20 25 30 35
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4
0
0.2
0.4
0.6
0.8
1
   AVG : 1.2377
Overhead
Su
cc
es
sf
ul
 ra
te
(c) k = 700
0 10 20 30 40
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Degree
Pr
ob
ab
ili
ty
1 1.1 1.2 1.3 1.4
0
0.2
0.4
0.6
0.8
1
   AVG : 1.2188
Overhead
Su
cc
es
sf
ul
 ra
te
(d) k = 1000
Fig. 5
FOR THE COMPARISON WITH SAME k’S, ROBUST SOLITON DISTRIBUTIONS AND THE CORRESPONDING PERFORMANCE INDICATORS ARE SHOWN SIMILAR
TO THAT IN FIG. 4. NOTE THAT ONLY PARTS OF ROBUST SOLITON DISTRIBUTIONS ARE PLOTTED FOR CLARITY
3536
amount of extra encoding symbols is not sufficient to complete
the BP decoding process. The behavior of LT process with
the optimized degree distributions is totally different and fully
satisfies the requirement of weakened LT.
V. CONCLUSIONS
In this work, the first attempt to algorithmically optimize
the degree distribution adopted in LT code was proposed.
Evolutionary computation techniques were introduced to ac-
complish the optimization task. Different from the previous
studies reported in the literature, each probability of degrees
were directly encoded as an individual to optimize. Promising
experimental results were obtained in both sets of experiments:
One was to minimize the overhead, and the other was to
reduce the decoding failure rate. Our experiments showed that
CMA-ES was indeed capable of finding good degree distribu-
tions for different purposes without any guideline or human
intervention. Compared with robust soliton distribution, the
optimized overhead was decreased as least 10% for every k in
the experiments. The results of failure rate minimization were
also remarkably promising and able to support applications of
different types and requirements.
This study creates a new research topic in which the design
of degree distributions in LT code can now be algorithmic
and no longer has to be manually tuning parameters of robust
soliton distribution. We have empirically proved that directly
manipulating the probability value for each degree is viable
and worth pursuing. Given a specific k and some expected
overhead, a degree distribution can be customized with exist-
ing optimization techniques. In addition, we will extend the
experiments to larger k for more kinds of potential applications
in the near future. The results empirically obtained by using
evolutionary algorithms will be theoretically analyzed, and
general guidelines, like robust soliton distribution, that are
able to be customized for different goals and requirements
for designing degree distributions are expected.
ACKNOWLEDGMENTS
The authors would like to thank Martin Hornansky for
fruitful discussion and conducting certain related numerical
experiments. The work was supported in part by the National
Science Council of Taiwan under Grant NSC-98-2221-E-009-
072. The authors are grateful to the National Center for High-
performance Computing for computer time and facilities.
REFERENCES
[1] D. J. C. MacKay, “Fountain codes,” in The IEE Seminar on Sparse-
Graph Codes, 2004, pp. 1–8.
[2] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege, “A digital
fountain approach to reliable distribution of bulk data,” in Proceedings
of the ACM SIGCOMM ’98 Conference on Applications, Technologies,
Architectures, and Protocols for Computer Communication, 1998, pp.
56–67.
[3] M. Luby, “LT codes,” in Proceedings of the 43rd Symposium on
Foundations of Computer Science. IEEE Computer Society, 2002, p.
271.
[4] R. Karp, M. Luby, and A. Shokrollahi, “Finite length analysis of
LT codes,” in Proceedings of the IEEE International Symposium on
Information Theory 2004 (ISIT 2004), 2004, p. 39.
[5] E. A. Bodine and M. K. Cheng, “Characterization of luby transform
codes with small message size for low-latency decoding,” in IEEE
International Conference on Communications (ICC ’08), 2008, pp.
1195–1199.
[6] E. Hyytia, T. Tirronen, and J. Virtamo, “Optimal degree distribution for
LT codes with small message length,” in Proceedings of the 26th IEEE
International Conference on Computer Communications (INFOCOM
2007), 2007, pp. 2576–2580.
[7] O. Etesami, M. Molkaraie, and A. Shokrollahi, “Raptor codes on sym-
metric channels,” in Proceedings of the IEEE International Symposium
on Information Theory 2004 (ISIT 2004), 2004, p. 38.
[8] O. Etesami and A. Shokrollahi, “Raptor codes on binary memoryless
symmetric channels,” IEEE Transactions on Information Theory, vol. 52,
no. 5, pp. 2033–2051, 2006.
[9] A. Shokrollahi, “Raptor codes,” IEEE Transactions on Information
Theory, vol. 52, no. 6, pp. 2551–2567, 2006.
[10] N. Hansen and A. Ostermeier, “Adapting arbitrary normal mutation
distributions in evolution strategies: the covariance matrix adaptation,”
in Proceedings of IEEE International Conference on Evolutionary
Computation, 1996, pp. 312–317.
[11] J. Pearl, “Reverend bayes on inference engines: A distributed hierarchi-
cal approach,” in Proceedings of the American Association of Artificial
Intelligence National Conference on AI, 1982, pp. 133–136.
[12] A. Auger and N. Hansen, “Performance evaluation of an advanced
local search evolutionary algorithm,” in Proceedings of the 2005 IEEE
Congress on Evolutionary Computation (CEC 2005), 2005, pp. 1777–
1784.
[13] ——, “A restart cma evolution strategy with increasing population size,”
in Proceedings of the 2005 IEEE Congress on Evolutionary Computation
(CEC 2005), 2005, pp. 1769–1776.
[14] R. Ros and N. Hansen, “A simple modification in CMA-ES achieving
linear time and space complexity,” in Proceedings of the Tenth Interna-
tional Conference on Parallel Problem Solving from Nature (PPSN X),
2008, pp. 296–305.
[15] R. Gallager, “Low-density parity-check codes,” IRE Transactions on
Information Theory, vol. 8, no. 1, pp. 21–28, 1962.
[16] D. Changyan, D. Proietti, I. E. Telatar, T. J. Richardson, and R. L.
Urbanke, “Finite-length analysis of low-density parity-check codes on
the binary erasure channel,” IEEE Transactions on Information Theory,
vol. 48, no. 6, pp. 1570–1579, 2002.
[17] E. Paolini and M. Chiani, “Improved low-density parity-check codes for
burst erasure channels,” in IEEE International Conference on Commu-
nications (ICC ’06), vol. 3, 2006, pp. 1183–1188.
3538
0 5 10 15 20 25 30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degree
D
is
tri
bu
tio
n
(a) Ideal soliton distribution
0 5 10 15 20 25 30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Degree
D
is
tri
bu
tio
n
(b) Robust soliton distribution
Fig. 1
EXAMPLE OF SOLITON DISTRIBUTIONS (k = 30)
XOR. The relation between source data and encoding symbols
can be modeled as a sparse bipartite graph. A key design
of LT code is to decide the degree of each vertex in the
bipartite graph with a probability distribution. The connectivity
can be recorded as an encoding matrix and each column
represents an encoding symbol. Originally, k source symbols
can be fully decoded by Gaussian elimination if there exist k
linearly independent columns. However, Gaussian elimination
is prohibitively expensive for its computational complexity of
O(k3). Therefore, the belief propagation (BP) algorithm [7]
is introduced to replace the expensive Gaussian elimination in
the LT decoding phase. Overhead of coding is used to trade
computing time because belief propagation is more efficient
but more encoding symbols are needed for successful decod-
ing. Moreover, the performance of LT code is very sensitive to
the degree distribution. A good degree distribution is necessary
to co-operate with belief propagation. Luby suggested soliton
distributions for LT framework in his proposal of LT code.
According to the mathematical verification, the properties of
soliton distribution have been confirmed. In this section, details
of coding operations and soliton distributions are described.
A. Encoding and decoding
Given the source data, we suppose that the source data
can be cut into k source symbols with the same length of
ℓ bits. Before every codeword is generated, a degree d is
chosen at random according to the adopted degree distribution
ρ(d), where 1 ≤ d ≤ k and ∑kd=1 ρ(d) = 1. The degree d
decides the how many distinct source symbols will be chosen
to compose an encoding symbol. d source symbols, called
neighbors, are chosen uniformly randomly and accumulated
by XOR. In the design of LT code, random numbers play
an essential role during the encoding process. The approach
employed by LT code for a sender to inform receivers of all
encoding information is achieved by synchronizing a random
number generator with a specified random number seed.
At the receiver side, when K encoding symbols were
arrived, where K is usually slightly larger than k, belief
propagation is used to reconstruct the source data step by step.
All encoding symbols are initially covered in the beginning.
For the first step, all encoding symbols with only one neighbor
can be directly released to recover their unique neighbor. When
a source symbol has been recovered but not processed, it
is called a ripple and will be stored in a queue. At each
subsequent step, ripples are popped as a processing target one
by one. A ripple is removed from all encoding symbols which
have it as neighbor. If an encoding symbols has only one
remaining neighbor after the removing, the releasing action
repeats and may produce new ripples to maintain a stable
size of the queue. Maintaining the size of the ripple queue is
important because the decoding process fails when the ripple
queue is empty and some source symbols remain covered.
In other words, more encoding symbols are required in the
decoding process. Ideally, the process succeeds if all source
symbols are recovered at the end of the decoding process.
Both encoding and decoding, as the LT coding operations,
are achieved by XOR. As a result, the computational complex-
ity of LT code can be measured by how many times of XOR is
executed. XOR operator is applied to build the connectivity in
the conceptualized bipartite graph and to eliminate a ripple
from the neighbors of codewords. It is evident that d − 1
XOR operators are necessary to generated a codeword with
degree d or recover an encoding symbol. In the encoding
phase, all encoding symbols are generated independently, and
the computational complexity to produce codewords solely
depends on the mean degree of the adopted degree distribution.
In other words, the cost of each encoding symbol is decided
by the mean of degree distributions. Hence, in practice, the
mean degree is an important LT performance indicator since
it represents the operational cost.
B. Soliton distribution
The behavior of LT code is completely determined by the
degree distribution, ρ(d), and the number of encoding symbols
received, K, by receiver. The overhead ε = K/k denotes the
performance of LT code, and ε depends on a given degree
distribution. Based on his theoretical analysis, Luby proposed
the ideal soliton distribution of which the overhead is 1, the
best performance, in the ideal case.
3636
In order to consider the trade-off between objectives, the
concept of domination between solutions is defined. Let u =
(u1, . . . , um), v = (v1, . . . , vm) ∈ Rm be two vectors. u is
said to dominate v if ui ≤ vi for all i = 1, . . . ,m, and u 6= v.
A point x∗ ∈ Ω is Pareto optimal if there is no x ∈ Ω such
that F (x) dominates F (x∗). The set of all the Pareto optimal
points is called Pareto set (PS) and the set of all the objective
vectors corresponding to the PS is called Pareto front (PF),
where PF = {F (x) ∈ Rm|x ∈ PS} [11].
Instead of searching for a single or just a few (Pareto)
optimal solutions as in solving single-objective problems, the
goal of handling multi-objective problems is to find the Pareto
front as well as the Pareto set of the problem. Given the limited
computational resource, including time and storage, how to
provide good solutions in terms of both quality and spread is
the key and challenging task for multi-objective optimization.
B. MOEA based on decomposition
One of the key ideas of MOEA/D is the use of a de-
composition method to transform a MOP into a number
of single-objective optimization problems. MOEA/D attempts
to optimize these single-objective problems collectively and
simultaneously instead of trying to directly approximate the
Pareto front as many other evolutionary algorithms do because
each optimal solution to these SOPs is a Pareto optimal
solution to the given MOP. The collection of these optimal
solutions is an approximation of the Pareto front. Weighted
sum, Tchebycheff approach, boundary intersection, and other
decomposition approaches can serve this purpose. In the
present work, the Tchebycheff approach [11] is adopted. A
single-objective optimization problem obtained by decompos-
ing the given MOP can be represented as
minimize g(x|λ, z∗) = max1≤i≤m{λi|fi(x)− z∗i |}
subject to x ∈ Ω (5)
where λ = (λ1, . . . , λm) is a vector of weights, i.e., λi ≥ 0
for all i = 1, . . . ,m and
∑m
i=1 λi = 1. z
∗ = (z∗1 , . . . , z
∗
m)
is the reference point, i.e., z∗i = min{fi(x)|x ∈ Ω} for each
i = 1, . . . ,m.
Let λ1, . . . , λN be a set of N weight vectors. If we use
a large N and select the weight vectors properly, all the
optimal solutions of the SOPs transformed from decompo-
sition will well approximate the Pareto front. Moreover, we
can define a neighborhood relationship for each SOP by
computing Euclidean distances between weight vectors. SOPs
which are considered neighbors are assumed to have similar
fitness landscapes and their optimal solutions should be close
in the decision space. MOEA/D exploits the information
sharing among SOPs which are neighbors to accomplish the
optimization task effectively and efficiently. The specification
of MOEA/D is stated as follows:
• Inputs:
– decision variables.
– objective functions.
– N : the number of subproblems.
– T : the number of neighbors for each subproblem.
TABLE I
PARAMETER SETTINGS OF MOEA/D
Parameter Value
N 50
T 10
Crossover rate 1
Mutation rate 1/m
Max Gen. 150
– stopping criteria.
• Outputs:
– Approximation to the PS: x1, . . . , xN .
– Approximation to the PF: F (x1), . . . , F (xN ).
IV. EXPERIMENTS
The experiment implementation is described in this section.
MOEA/D is a well-developed tool and has the characteristic
of black-box optimization like other evolutionary algorithms.
As described in section III-B, only input and output should be
handled properly. Section IV-A shows how to encode a degree
distribution into decision variables, and the objective functions
are given in section IV-B. Table I lists the other algorithmic
parameter settings of MOEA/D.
A. Decision variables
The first step to use an evolutionary algorithm is to encode
the decision variables of the optimization problem. It is not
difficult in this study because a degree distribution can directly
form a real-valued vector. In the evaluation phase, a real-
valued vector of arbitrary values can be interpreted as a
probability distribution, i.e., a degree distribution, with nor-
malization. Such an operation does not change the feasibility,
although the problem complexity may be slightly increased.
The definition of degree distributions tells us that d ≤ k.
For a specific source symbol size k, obviously the problem
dimensions is at most k. However, according to the LT
encoding/decoding operations, we usually do not need a non-
zero probability on every single degree. Observing the soliton
distribution and considering the belief propagation algorithm,
there is no necessary degree except 1, which ensures the start
of belief propagation. As a result, we optimize a selected
subset of degrees in the present work. We choose some par-
ticular degrees, {1,2,3,4,5,7,9,13,17,23} to form the decision
variables according to the experience. Different subsets of
degrees may change the numerical results of experiments
results, but the soundness of this paper will be not be affected.
B. Objectives
In this paper, degree distributions are optimized for two
different objectives. The first indicator to evaluate efficiency
of LT code is overhead ε. The redundancy is traded for the
benefit of fountain code and those extra encoding symbols
increase the cost when they are transmitted to the receiver.
In most application, overhead is required to be as low as
possible because the transmission is usually expensive. In
3638
TABLE II
OPTIMIZED ARBITRARY DEGREE DISTRIBUTIONS
Individual Best Overhead Best Cost AVG. Overhead XOR
1 4.8442 0.00042 5.1958 0.038
25 2.5608 1.29873 2.6655 407.026
35 2.0294 1.85193 2.1485 558.667
45 1.4564 2.5135 1.57211 603.742
50 1.2068 2.93541 1.2718 843.669
TABLE III
OPTIMIZED ROBUST SOLITON DISTRIBUTIONS
Individual Best Overhead Best Cost AVG. Overhead XOR
1 4.8080 0.00552 5.1323 0.377
25 3.1244 1.74314 4.1662 125.455
35 2.0708 2.76115 2.6217 324.580
45 1.5194 3.41297 1.9278 471.753
50 1.2530 6.71008 1.3097 1141.46
1 1.5 2 2.5 3 3.5 4 4.5 5
0
100
200
300
400
500
600
700
Overhead
O
pe
ra
tio
na
l C
os
t
 
 
Robust Soliton
Optimized
Fig. 5
COMPARISON BETWEEN THE OPTIMIZED ARBITRARY DEGREE
DISTRIBUTION AND ROBUST SOLITON DISTRIBUTION
there are significant differences along the other axis. The
performance is quite limited, and such a situation is caused
by the fixed formula of robust soliton distribution. The figure
demonstrates numerous better degree distributions that are
very different from robust soliton distribution. These degree
distributions can be discovered by optimization algorithms
proposed in the realm of evolutionary computation.
VI. CONCLUSIONS
This paper proposed the use of multi-objective evolutionary
algorithms to optimize the degree distribution in LT code.
Overhead and operational cost were considered as two objec-
tives and optimized simultaneously by using MOEA/D. The
experimental results were promising and indicated that the
Pareto front was well described. These results might also help
researchers to better understand the behavior of LT code. For
applications of different types and natures, LT code will be
more efficient if choosing a specifically appropriate degree
distribution is possible. Not only more choices of degree dis-
tributions are available, but also much better performance than
that delivered by robust soliton distribution can be achieved,
because most robust soliton distributions are dominated by the
solutions discovered with MOEA/D in the experiments.
An alternation solution which designs degree distribution
better than robust soliton is given in the work. While LT
code is employed in real-world apparitions, the degree dis-
tribution can be customized to satisfy different requirements
by using evolutionary algorithms. Fitter degree distributions
will enhance the performance of those applications. Moreover,
better understandings of the behavior of LT code will help the
improvement of LT code. The final results show that some
better distributions are beyond the model of robust soliton
distribution. The theoretical analysis will also be applied to
them just like the development of soliton distributions in our
future work. An advanced model in which the performance is
close to that of the Pareto front is in expectation.
ACKNOWLEDGMENTS
The authors would like to thank Martin Hornansky for
fruitful discussion and conducting certain related numerical
experiments. The work was supported in part by the National
Science Council of Taiwan under Grant NSC-98-2221-E-009-
072. The authors are grateful to the National Center for High-
performance Computing for computer time and facilities.
REFERENCES
[1] D. J. C. MacKay, “Fountain codes,” in The IEE Seminar on Sparse-
Graph Codes, 2004, pp. 1–8.
[2] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege, “A digital
fountain approach to reliable distribution of bulk data,” in Proceedings
of the ACM SIGCOMM ’98 Conference on Applications, Technologies,
Architectures, and Protocols for Computer Communication, 1998, pp.
56–67.
[3] M. Luby, “LT codes,” in Proceedings of the 43rd Symposium on
Foundations of Computer Science, 2002, p. 271.
[4] R. Karp, M. Luby, and A. Shokrollahi, “Finite length analysis of
LT codes,” in Proceedings of the IEEE International Symposium on
Information Theory 2004 (ISIT 2004), 2004, p. 39.
[5] E. A. Bodine and M. K. Cheng, “Characterization of luby transform
codes with small message size for low-latency decoding,” in IEEE
International Conference on Communications (ICC ’08), 2008, pp.
1195–1199.
[6] E. Hyytia, T. Tirronen, and J. Virtamo, “Optimal degree distribution for
LT codes with small message length,” in Proceedings of the 26th IEEE
International Conference on Computer Communications (INFOCOM
2007), 2007, pp. 2576–2580.
[7] J. Pearl, “Reverend bayes on inference engines: A distributed hierarchi-
cal approach,” in Proceedings of the American Association of Artificial
Intelligence National Conference on AI, 1982, pp. 133–136.
[8] Q. Zhang and H. Li, “MOEA/D: A multiobjective evolutionary algorithm
based on decomposition,” IEEE Transactions on Evolutionary Compu-
tation, vol. 11, no. 6, pp. 712–731, 2007.
[9] H. Li and Q. Zhang, “Multiobjective optimization problems with com-
plicated pareto set, MOEA/D and NSGA-II,” IEEE Transactions on
Evolutionary Computation, vol. 13, no. 2, pp. 284–302, 2009.
[10] K. Deb, A. Pratap, and T. Meyarivan, “Constrained test problems
for multi-objective evolutionary optimization,” in First International
Conference on Evolutionary Multi-Criterion Optimization. Springer
Verlag, 2001, pp. 284–298.
[11] K. Miettinen, Nonlinear Multiobjective Optimization. Kluwer Aca-
demic, 1999.
3640
1 2 3 4 5 7 9 13 17 23
0
0.2
0.4
0.6
0.8
1
Degree
Pr
ob
ab
ili
ty
2 4 6 8 10 12
0
0.2
0.4
0.6
0.8
1
   AVG : 5.2266
Overhead
Su
cc
es
sf
ul
 ra
te
(a) Individual 1
1 2 3 4 5 7 9 13 17 23
0
0.2
0.4
0.6
0.8
1
Degree
Pr
ob
ab
ili
ty
1 2 3 4 5 6
0
0.2
0.4
0.6
0.8
1
   AVG : 2.6646
Overhead
Su
cc
es
sf
ul
 ra
te
(b) Individual 25
1 2 3 4 5 7 9 13 17 23
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Degree
Pr
ob
ab
ili
ty
1 2 3 4 5
0
0.2
0.4
0.6
0.8
1
   AVG : 2.1394
Overhead
Su
cc
es
sf
ul
 ra
te
(c) Individual 35
1 2 3 4 5 7 9 13 17 23
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Degree
Pr
ob
ab
ili
ty
1 1.5 2 2.5 3
0
0.2
0.4
0.6
0.8
1
   AVG : 1.5682
Overhead
Su
cc
es
sf
ul
 ra
te
(d) Individual 45
1 2 3 4 5 7 9 13 17 23
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Degree
Pr
ob
ab
ili
ty
1 1.2 1.4 1.6 1.8 2
0
0.2
0.4
0.6
0.8
1
   AVG : 1.2771
Overhead
Su
cc
es
sf
ul
 ra
te
(e) Individual 50
Fig. 7
SIMULATION RESULTS OF OPTIMIZED ARBITRARY DEGREE DISTRIBUTIONS
3642
98年度專題研究計畫研究成果彙整表 
計畫主持人：陳穎平 計畫編號：98-2221-E-009-072- 
計畫名稱：研究與發展專為無線網路系統客製化之最佳化演算架構 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
