 II
16-21, 2006. (NSC 93-2213-E-006-130) 
4. Yen-Ping Chen and Jeen-Shing Wang, "Identification of chaotic systems using a 
self-constructing recurrent neural network," Proc. of 2005 IEEE International Conference 
on Systems, Man & Cybernetics, The Big Island, Hawaii, pp. 2150-2155, Oct. 10-12, 
2005. (NSC 93-2213-E-006-130) 
5. Yen-Ping Chen and Jeen-Shing Wang, "An automated Hammerstein recurrent neural 
network for dynamic applications," Proc. of IASTED International Conference on 
Computational Intelligence, Calgary, Canada, July 4-6, 2005. (NSC 93-2213-E-006-130) 
6. Jen-Chieh Chiang and Jeen-Shing Wang, “A validity-guided support vector clustering 
algorithm for identification of optimal cluster configuration ,” Proc. of 2004 IEEE 
International Conference on Systems, Man & Cybernetics, Hague, Netherlands, pp. 
3613-3618, October 10-13, 2004. (NSC 93-2213-E-006-130) 
7. Jiun-Kai Wang and Jeen-Shing Wang, “Efficient function approximation using an online 
self-regulating clustering algorithm,” Proc. of 2004 IEEE International Conference on 
Systems, Man & Cybernetics, Hague, Netherlands, pp. 5935-5940, October 10-13, 2004. 
(NSC 93-2213-E-006-130) 
 
關鍵字：非線性系統、系統鑑別、控制、網路拓撲、類神經模糊系統，遞迴類神經網路、
最小模型鑑別、維度估測、參數最佳化 
 
 IV
2. Jeen-Shing Wang and Yen-Ping Chen, "A Hammerstein neuro-fuzzy network with an 
online hybrid construction algorithm for dynamic applications," Proc. of 2006 IEEE 
World Congress on Computational Intelligence, Vancouver, Canada, pp. 9946-9953, July 
16-21, 2006. (NSC 93-2213-E-006-130) 
3. Yen-Ping Chen and Jeen-Shing Wang, "A novel nonlinear dynamical systems control 
using linear controllers with nonlinearity eliminators," Proc. of 2006 IEEE World 
Congress on Computational Intelligence, Vancouver, Canada, pp. 10737-10744, July 
16-21, 2006. (NSC 93-2213-E-006-130) 
4. Yen-Ping Chen and Jeen-Shing Wang, "Identification of chaotic systems using a 
self-constructing recurrent neural network," Proc. of 2005 IEEE International Conference 
on Systems, Man & Cybernetics, The Big Island, Hawaii, pp. 2150-2155, Oct. 10-12, 
2005. (NSC 93-2213-E-006-130) 
5. Yen-Ping Chen and Jeen-Shing Wang, "An automated Hammerstein recurrent neural 
network for dynamic applications," Proc. of IASTED International Conference on 
Computational Intelligence, Calgary, Canada, July 4-6, 2005. (NSC 93-2213-E-006-130) 
6. Jen-Chieh Chiang and Jeen-Shing Wang, “A validity-guided support vector clustering 
algorithm for identification of optimal cluster configuration ,” Proc. of 2004 IEEE 
International Conference on Systems, Man & Cybernetics, Hague, Netherlands, pp. 
3613-3618, October 10-13, 2004. (NSC 93-2213-E-006-130) 
7. Jiun-Kai Wang and Jeen-Shing Wang, “Efficient function approximation using an online 
self-regulating clustering algorithm,” Proc. of 2004 IEEE International Conference on 
Systems, Man & Cybernetics, Hague, Netherlands, pp. 5935-5940, October 10-13, 2004. 
(NSC 93-2213-E-006-130) 
 
Keywords: Nonlinear systems, system identification, control, network topology, neuro-fuzzy 
systems, recurrent neural network, minimal model determination, order estimation, parameter 
optimization 
 1
I. INTRODUCTION 
Successful system identification usually requires taking into consideration three major 
procedures: 1) model structure selection, 2) model order estimation, and 3) model 
parameterization [24]. Model structure selection plays a decisive role in successful system 
identification because a good model structure offers the flexibility and capability of describing 
different input-output maps with satisfactory accuracy. Among the diverse model structures, 
recurrent neural networks (RNNs) have been recognized as an effective model for nonlinear 
system identification due to their flexibility of incorporating dynamic elements in describing 
complex dynamical behavior and their capability of adjusting parameters to adapt to a changing 
environment. However, due to their flexibility, two major drawbacks in using RNNs have been 
pointed out: lack of a generic architecture and difficulty in training weights [15]. According to 
Tsoi and Back [38], [39], there are altogether 54 possible combinations of RNN architectures 
which can be derived from a three-layered neural network based on the authors' definition of 
network components. These combinations can be classified into two major categories with 
respect to the dynamic connections: internal dynamics and external dynamics. The proper 
architecture to use is, thus, problem-dependent. Moreover, because of the fact that a weight 
adjustment affects the states all the time during the course of network state evolution, obtaining 
the error gradient information in training RNNs becomes a rather complicated procedure. To 
maintain the stability during/after the weight training is still an active research topic [3], [34], 
[4]. 
Enjoying a judicious integration of the merits from neural networks and fuzzy systems, 
neuro-fuzzy systems (NFS) realize the elements and functions of a traditional fuzzy logic system 
(FLS) by a multi-layered connectionist structure. This well-structured connectionist model can 
explicitly translate the system behavior into a set of human-comprehensible IF-THEN rules; 
however, its performance in dynamical applications becomes unsatisfactory due to the lack of 
“dynamics” in the network architecture. In the past decade, a growing research interest has been 
directed to incorporating dynamic elements in the diverse feedback connections for FLS/NFS so 
as to handle temporal relations [14], [18], [47], [25], [26]. To name a few, Gorrini and Bersini 
[14] decomposed a fuzzy rule into two parts, the external rule and the internal rule, both of which 
form a hierarchical relation. The same method was adopted by [18], [22] and [23] for their 
proposed five-layered recurrent NFS architecture, called RSONFIN. Theocharis and 
Vachtsevanost [37] introduced recurrence to the prototype of FLS by recursively feeding a 
particular number of tapped delayed outputs back to the input of FLS. Similarly, Zhang and 
Morris [47] proposed a recurrent neuro-fuzzy whose output is fed back to the network input 
through one or more time delays. The resulting architecture for such output feedback recurrence, 
however, is actually a static representation, and the total number of inputs is increased by the 
number of delayed output feedbacks, which may eventually encounter a serious problem of 
modeling—the curse of dimensionality. Other researchers embedded temporal relations within 
feedback connections in a different layer of their proposed networks. For example, Lee and Teng 
[25] developed a four-layered recurrent fuzzy neural system by embedding the feedback 
connections in the input membership layer, while Lin and Wai [26] proposed the feedback 
 3
representation. The proposed network structure consists of a static nonlinear part cascaded by a 
linear dynamic part. Our network structure is similar to a Hammerstein model, a block-oriented 
system composed of zero memory nonlinear elements followed by a linear dynamic system [24]. 
Recent studies [7], [10] have shown the excellent capability of the Hammerstein model to 
represent complex dynamic systems whose dynamics is emulated by its linear dynamic system 
while the complexity of nonlinearity is captured by the static nonlinear elements. This fact 
ensures the capability of the proposed structure in complex dynamic system identification. In 
addition, the novelty of the proposed network is that the structure itself can be mapped into a 
state-space equation. The advantages of this feature include: 1) the characteristics of the trained 
FARNN can be analyzed by its associated state-space equation using the well-developed theory 
of linear systems; and 2) the overall network size can be determined if the system order of the 
unknown system is obtained.  
To obtain an optimal network structure and an effective controller, we have developed a 
fully automated construction algorithm that performs system order determination, parameter 
initialization and optimization, as well as the design of the controller in a fully autonomous 
manner. The order determination scheme is based on the Lipschitz criterion, first proposed by He 
and Asada [17], to identify the best-case system order from input-output measurements. With the 
identified system order, we can construct the proposed network. Atiya and Ji [1] have shown that 
parameter initialization plays a decisive role in the generalization performance as well as 
learning convergence of networks. We proposed a hybrid parameter initialization scheme to 
initialize the nonlinear parameters of the static nonlinear part and the linear parameters of the 
linear dynamic part separately. The hybrid initialization scheme comprises an optimal parameter 
initialization method modified from [46] for the nonlinear parameters and the prediction error 
method (PEM) [24] for the linear parameters. Finally, a recursive recurrent learning algorithm 
derived from the concept of the ordered derivatives [42] with a momentum term was developed 
to online optimize the overall performance and to accelerate the learning convergence. Based on 
the identified FARNN which models the dynamics of the unknown system, we first generated a 
nonlinearity eliminator (NLE) to cancel the nonlinear effects caused by the static nonlinear 
model. This results in an unknown system with the trained NLE that behaves like the dynamical 
linear model of the FARNN. With the linear model, we can design a linear controller by using the 
well-developed linear control theory effortlessly.  
Computer simulations on benchmark examples of noise cancellation, unknown nonlinear 
dynamic system identification and control have successfully validated the effectiveness of the 
proposed recurrent systems, consisting of the state-space NFS and the fully automated RNN, and 
the corresponding construction algorithms. 
The rest of this project reporter is organized as follows. In Section II, we shall introduce the 
proposed architectures of the recurrent neuro-fuzzy system and the recurrent neural network with 
their primitive ideas. The online minimal realization learning algorithm and the fully automated 
construction algorithm for establishing parsimonious networks are presented in Section III. In 
Section IV computer simulations on examples of nonlinear dynamic applications have conducted 
to validate the effectiveness of the proposed recurrent architectures. Conclusions are summarized 
in Section V.  
 5
networks which incorporate dynamic elements in such a way that each state corresponds to a 
neuron that linearly combines its own output history and the outputs from other neurons. Such a 
deployment of dynamic elements enables the proposed structure to be mapped into a state-space 
equation from its internal structure. The neurons develop internal dynamics by their 
self-feedback and weighted connections with the feedback connections from other neurons. That 
is, the dynamic linear layer integrates the current input information from the nonlinear part and 
the state history (stored in the memory device) of the neurons in the dynamic layer to infer the 
current states x(k) of the network. Finally, the outputs of the network are obtained by the linear 
combination of the states and their associated parameters. 
The difference between these two models is their nonlinear static parts. In the state-space 
recurrent NFS structures, a simple fuzzy membership function network is selected as the 
antecedent part of a conventional (static) fuzzy system. The membership function is defined as: 
 
1
( ) ( )j
i
n
j iMi
p uμ==∏u ,   (1) 
where u = [u1, u2,…, un]T is the input vector andμM ij (ui) is the Gaussian membership function 
defined asμ
M i
j (ui) = exp{−((ui − mij ) /σ ij )2}, where mijandσ ij are the centers and widths of the 
Gaussian functions, respectively. Then, we chose the consequent constituent as a singleton value 
to describe the control/decision action of a dynamic system. For a multi-input-multi-output 
(MIMO) dynamic system, the jth fuzzy dynamic rule at time k of the NFS can be expressed as:  
 1 1
1 1
Rule :  ( ) is  and  and ( ) is ,
 ( ) is ( ) and  and ( ) is ( ),
j j
n n
j j
m m
j u k M u k M
y k f k y k f k
IF
THEN
"
"      (2) 
where ( ) ( ),j jl l jf k c x k= ui(i = 1, 2,…, n) and yl(l = 1, 2,…, m) are the input and output variables, 
respectively, and Mi
j  are the input fuzzy term sets. Thecl
j x j (k)is singleton constituents. From 
(2), the state variables are invisible in the antecedent parts. They are used to reflect the current 
input information combined with the past information in the consequent parts. The whole 
inference process agrees with the state-space concept of linear systems. The state-space 
equations of the recurrent NFS can be written as  
 
( 1) ( ) ( ( ))
     ( ) ( ),  
k k k
k k
+ = +
=
x Ax BP u
y Cx
    (3) 
where x = [x1, x2,…, xn]T is the state vector, J is the total number of state variables and is equal to 
the number of rules, and P = [p1, p2,…, pn]T is the firing strengths with respect to the input values. 
A and B J J×∈\ , and C m J×∈\ . The elements of matrix A stand for the degrees of 
inter-correlation among rules. B is a diagonal matrix with diagonal elements [b1, b2,…, bn] 
representing the weights of the firing strengths. The elements of matrix C are the output 
singleton values. We have demonstrated that both linguistic rules and state-space equations can 
be directly extracted from the proposed network topology effortlessly. The state-space equations 
obtained from the proposed recurrent NFS can be utilized for further analysis using the 
well-developed theory and concepts of linear systems.  
For the FARNN structure, the nonlinear static part is implemented by one simple 
 7
we are able to construct an initial network based on the number of the state variables (or the 
system order). Parameter initialization methods can be performed to initialize the parameters of 
the network during or after the order determination process. After the structure learning phase, 
we continuously fine-tune the parameters of the network to closely capture the dynamic behavior 
of the unknown system via recursive learning algorithms. Here, we summarize the two 
algorithms into three parts, consisting of order estimation, parameter initialization and 
optimization.  
A. System Order Determination Methods 
Groundwork for good system identification is the determination of appropriate system 
orders [17]. The authors in [27] have empirically validated the effect of model orders on the 
performance of NARX networks. They showed that good identification of memory orders can 
improve not only the learning accuracy of NARX networks but also their generalization 
capability in identification. Their results show that the overall system performance degrades 
when the number of identified orders is fewer than the true order of the system. Contrarily, the 
identified system with redundant orders increases the complexity of network computation but not 
system performance.  
Over the last decade, plenty of the dynamic system identification literature has been 
concentrated on the investigation of parameter estimation based on a general assumption: system 
orders are known a priori [8], [29]. Although many excellent results have been reported, the 
parameter estimation turns into a trial-and-error process in finding suitable system orders when 
system orders are unknown. Since the system orders are almost invariably unknown, including 
the bounds of the orders, the determination of the orders should be made before solving the 
parameter estimation problem. To acquire the information of system orders, only a few 
researchers have devoted their effort to the investigation of determining system orders using 
input-output measurements [2], [17], [33]. He and Asada [17] first proposed an order 
determination method for identifying the orders of a nonlinear input-output model based on the 
continuity property of nonlinear functions. In general, the order of input-output models can be 
cast as the memory of the system or the region of the past states that affects the evolution of 
system dynamics. Since the proposed recurrent network can be interpreted into a state-space 
representation, the structure of the network is established once the order is determined. Hence, 
we adopted the idea in [17] for the order determination method that utilizes the Lipschitz 
quotient for the order determination. This approach was originally applicable only to 
single-input-single-output (SISO) systems. We further extended applicability to estimate the 
order of multi-input-single-output (MISO) systems. The extension of the algorithm to a 
multi-input-multi-output (MIMO) system is straightforward since the MIMO system can always 
be decomposed into a set of MISO subsystems. 
A general model representation of SISO nonlinear dynamic systems can be expressed as 
 1( ) ( ( 1), ( 1), ( 2), ( 2),..., ( ), ( )) ( , , ),oy k f y k u k y k u k y k J u k r f χ χ= − − − − − − = …  (7) 
where y(k) and u(k) are the output and input of the system, respectively, χ(k) = [χi], i = 1, …, o, 
are the potential input variables, J is the order of the output/system, r is the order of the input, 
and o is the total number of input variables. f(⋅) is a nonlinear function that is assumed to be 
continuous and smooth. Without a priori information of the system orders, He and Asada [17] 
 9
Substep 2: Set the order index oi = oi +1 and obtain h, the sum of the vector o. h 
represents the amount of input variables in (11); 
      Substep 3: Set the input variable hχ be the oi-th lag of iψ ; 
      Substep 4: Determine the Lipschitz quotient ( )hgmq using (9); 
Substep 5: If q(n) satisfies the following stop criterion  
                ( 1) ( ) ( )max(1, ) ,h h hgm gm gmq q q ε+ − <  
when h > l+1 and where ε  > 0 is a pre-specified threshold, then set si = 1 and 
oi = oi−1. This means that the order of iψ is found. In our empirical experience,ε  
= 0.1 is suitable for most cases; 
Substep 6: Set i = i + 1. If i is larger than l+1, then set i = 1; 
Repeat until si = 1 for all i = 1, …, l+1. 
Step 4: The system order is equal to o1, i.e. J in (4) equals to o1, and the order of each input is 
equal to oi, i = 2, …, l+1, i.e. p in (4) equals to the sum of oi. 
From our experience, the Lipschitz quotient is effective in identifying the orders of nonlinear 
systems only when a reasonable N is selected. If N is too small, the Lipschitz quotients will fail 
to provide the correct information for the order identification. Contrarily, the computational load 
becomes excessive when N gets larger. For instance, in order to obtain the index value for a 
specific number of input variables, say h, there is a total of N(N-1)/2 Lipschitz quotient 
computations needed to perform. To exempt the selection of N from a trial-and-error process and 
reduce the computational cost, we propose an online order determination method that initially 
monitors the smallest possible system order and dynamically increases the number of candidate 
orders through the online order determination process. Compared with the approaches in 
previous works in [17], [43], our approach possesses three novel features: 1) Our approach is 
capable of simultaneously computing the Lipschitz quotients for different system orders in an 
online fashion; 2) our algorithm is equipped with an order-growing mechanism that increases the 
number of monitored orders dynamically; and 3) the decision about the system order is 
automatically made without any user manipulations. These three features greatly relieve the 
computational load since the computation of the Lipschitz quotients is performed only on the 
monitored orders and is distributed over the time interval from the beginning to when the final 
decision of the system order is made. Moreover, due to the internal dynamics/recurrence of our 
proposed recurrent network, using only the information about current inputs is enough to identify 
the order of an unknown system. This further alleviates the computational burden of the 
Lipschitz quotients.  
To explain the idea of our approach, we consider the following MISO nonlinear model 
representation with only current inputs: 
 1( ) ( ( 1), ( 1), , ( 1), ( 2), , ( )),ny k f y k u k u k y k y k J= − − − − −… …   (12) 
where n is the number of nonlinear system inputs and J is the system order that we want to 
identify. The extension to MIMO systems is straightforward. One can decompose a MIMO 
system into a set of MISO subsystems, and the maximum estimated order among these 
subsystems can be adopted as the order of the entire MIMO system. Now, we present our online 
 11
Substep 6: [Order growing mechanisms.] If none of the changes satisfies the above 
criterion, namely, the curve of the index values does not reach the specified 
saturation region, the number of monitored orders increases by setting nm = nm 
+ 1.  
Step 4: [Final order decision.] The final order decision is made when the following stop criterion 
holds.  
 
n n
n
( ( ) ( ))
0.8,
( )
wo k wo k
wo k
α β
α
− >  (15) 
wherem ( )wo kα andm ( )wo kβ are the largest and second largest accumulated weighting 
values and α is selected as the final system order. Otherwise, goto Step 3.  
The order determination algorithm monitors several candidate orders simultaneously during 
the online process, only one of which will be selected as the final system order. 
B. Parameter Initialization Methods 
1) Initialization Method of State-Space Recurrent Neuro-Fuzzy System 
Due to the simplicity of the k-means algorithm, we modified the k-means and introduced 
the concept of the mapping consistence proposed in our previous work [44] to extract the 
mapping relations of the input-output measurements during the order determination process. That 
is, the same input-output data are used for the monitored cluster configurations whose number of 
clusters equals the number of orders. The final selected cluster configuration, described by 
Gaussian distributions with centers and variances of the clusters, can be directly used to 
construct our recurrent network by using the input and output cluster information as the 
antecedents and consequents of the fuzzy rules, respectively. 
For each cluster, say cluster α, mIα (σIα) and mOα denote the centroids (variances) in the 
input space (I) and output space (O), respectively, and let cα be the counter representing the 
number of patterns in cluster α. Note that we successively estimate the centroids as well as 
variances during the clustering process so that the monitored cluster configuration can be 
obtained simultaneously. In addition, the following k-means algorithm is similar to the order 
determination algorithm that monitors several candidate orders/cluster configurations. That is, 
the number of monitored cluster configurations increases as the number of monitored system 
orders. We now summarize the proposed online k-means in the following steps: 
Step 1: [System initialization.] Obtain the system order, nm, from the order determination 
algorithm and initialize a cluster configuration with nm clusters.  
Step 2: Substep 1: [Data query.] Feed the kth input-output measurement, {[u(k), y(k) ], where u ∈ 
\n and y ∈ \}; 
Substep 2: [Initialization of seed clusters.] Initialize new clusters with the centroids and 
variances for both the input and output data spaces. If the current number of 
seed clusters, i, is less than nm, then set ci = 1, assign mIi = u(k) and σIi =0 for 
the input space, assign mOi = y(k) for the output space, and goto Substep 5. 
Otherwise, goto Substep 3. 
 13
 
minimize 
subject to ,=
z
Dz b
  (21) 
where D∈\q×(l×l), b∈\q, q ≤ (l×l), rank D = q, and q is the number of time steps. The objective of 
this problem is thus to find a solution z* such that Dz* = b and ||z*|| ≤ ||z|| for any z that satisfies 
Dz = b. We next present Kaczmarz’s algorithm for solving the above optimization problem. The 
algorithm is executed right after the completion of the order determination algorithm. 
Step 1: [Initialization.] Set k = k +1 and an initial condition for z(k). 
Step2: [Update rule.] Update the solution points by 
 ( )( 1) ( ) ( ( ) ( ) ( )) ,
( ) ( )
T
T
kk k b k k k
k k
dz z d z
d d
μ+ = + −   (22) 
where μ is the step size of the update rule and can be chosen from 0 < μ < 2. A guideline 
to select μ can be found in [9], [40]. Here we set μ = 0.1.  
Step 3: [Stop criterion.] If ||z(k+1)−z(k)|| ≤ 0.01, then the algorithm stops. Otherwise, set k = k +1 
and go to Step 2.    
Tanabe has proved that the sequence {z(k)} always converges. Here “always” means that rank (D) 
can be smaller than J×J, that q can be larger or smaller than J×J, and Dz = b can be inconsistent 
[40]. The parameter initialization is completed after this algorithm. 
2) Initialization Method of Fully Automated Recurrent Neural Network 
A hybrid initialization method consists of two initialization methods to initialize the 
parameters of the static nonlinear model and the dynamic linear model. The static nonlinear 
model of the FARNN is a single-layer neural network. Yam and Chow [46] proposed a 
multidimensional geometrical approach to accelerate the training speed of a multilayer neural 
network and to prevent the network from getting stuck in the beginning of the training phase. 
They initialize the parameters of the network so that all activation functions, such as log sigmoid 
functions, can be operated in active regions. Since the derivative of activation functions within 
the active region has a more significant change than the one in the saturation region, it is highly 
possible to rapidly reach a local minimum for parameter optimization algorithms. Here, we 
extend their result for hyperbolic tangent sigmoid functions, which can provide a dual polarity 
signal to the output of the network, to initialize the parameters of the static nonlinear model of 
the FARNN. 
Our objective is to initialize the parameters wji and dj in (5) so that the outputs of the 
neurons are guaranteed to operate in the active region. The weights wji are assumed to be 
independent and identically distributed (i.i.d.) uniform random variables within the range [−wmax, 
wmax]. The maximum value of the weights between the input and hidden layers and the value of 
the bias can be derived into the following equations: 
 
4.356 3 ,max maxw D p
=  (23) 
 
1
,
P
bound
j i ji
i
d c w
=
= −∑  (24) 
where Dmax  represents the maximum possible Euclidean distance among input data points, p is 
 15
calculating the activities of all nodes on each layer, and the corresponding functions are 
summarized as 
 
1
( ) ( ) ( 1) ( ).
J
T
j j
j
y k k c k x kc x
=
= = −∑  (28) 
 
1 1
( ) ( ( 1)) ( 1).
J J
ji i jj j
j i
k a x k b p k
= =
= − + −∑ ∑x   (29) 
 ( ) 2
1
( ( )) exp[ ( ) ].
j
i i
j
i
n u k m
j i
p k σ
−
== −∏u  (30) 
According to the error function (25) and the functions in (28)-(30), we can derive the recursive 
update rules for the parameters as follows. The update rule of cj, the output link weights, is 
 
( )( ) ( 1) ( ),  j j abc
j
E kc k c k
c
ξ
+∂= − + − ∂  (31) 
where ξabc is the learning rate for adjusting aji, bjj and cj, according to (28), ∂+E/∂cj = ∂E/∂cj = 
−e(k)xj(k) is an ordinary derivative because there is no indirect effect on changing cj.  
To update the rest of the parameters (aji, bjj, jim and jiσ ), we have to propagate the current 
error signal not only to the current state but also to the previous states. According to (29), the 
update rule of aji (the elements of the state matrix) is 
 ( )( ) ( 1) ( ),ji ji abc
ji
E ka k a k
a
ξ
+∂= − + − ∂  (32) 
and  
 
( )( ) ( ) ,
( )
j
ji j ji
x kE k E k
a x k a
++ ∂∂ ∂=∂ ∂ ∂   (33) 
where ( ) ( 1) ( )j jE k x c k e k∂ ∂ = − − and 
 
( ) ( ) ( ) ( 1)
,
( 1)
j j j j
ji ji j ji
x k x k x k x k
a a x k a
+ +∂ ∂ ∂ ∂ −= +∂ ∂ ∂ − ∂  (34) 
where ( ) ( 1),j ji ix k a x k∂ ∂ = − ( ) ( 1) ( 1),j j jjx k x k a k∂ ∂ − = − and (1) (0)j ji ix a x∂ ∂ = when k = 1. 
Similarly, the update rule of bjj (the elements of the input diagonal matrix) is 
 ( )( ) ( 1) ( ),jj jj abc
jj
E kb k b k
b
ξ
+∂= − + − ∂  (35) 
where bji = 0 for all i ≠ j and  
 
( )( ) ( ) ,
( )
j
jj j jj
x kE k E k
b x k b
++ ∂∂ ∂=∂ ∂ ∂   (36) 
where 
 17
 
( ) ( ) ( 1)
,
( 1)
j j j
j j
i j i
x k x k p k
p kσ σ
∂ ∂ ∂ −=∂ ∂ − ∂  (46) 
where 
 
2
3
( ( 1) )
( )
( 1)
2( ) ( 1).
j
i i
j
i
u k mj
jj
i
p k
p kσσ
− −∂ − = −∂  (47) 
Using (46) and (47), we can obtain ( ) ( )j jj i j ix k x kσ σ+∂ ∂ = ∂ ∂ when k = 1. 
The values ,j jix a
+∂ ∂ ,j jjx b+∂ ∂ ,jj ix m+∂ ∂ and jj ix σ+∂ ∂ are set to zero initially. They are 
recursively accumulated as the error signal generated by each training pattern and are reset to 
zero after a period of learning to avoid a big accumulated error 
2) Learning Algorithm of Fully Automated Recurrent Neural Network 
To accelerate the convergence of the learning algorithm, the correction +w applied to w is 
defined by the ordered derivative with a momentum term. 
 ( ) ( )+ ( 1),Ew k w k
w
ξ α
+∂Δ = − Δ −∂  (48) 
where ξ is a learning rate and ∂+E/∂w is the ordered derivative that considers the direct (current 
state) and indirect (previous states) effects of changing a structure parameter. The term (α+w) is 
the momentum where α∈[0, 1] is a learning rate. The adjustable parameters w of the proposed 
network consist of the output link weights (C∈\m×J), the elements of the state matrix (A∈\J×J), 
the elements of the input diagonal matrix (B∈\J×J), and the link weights (wji) and bias (dj) of the 
hidden layer. The following equations summarize the parameter update rules. 
 ( ) ( 1) ( ( ) ( ) ( 1));j j ac j jc k c k e k x k c kη α= − + + Δ −  (49) 
 
( ) ( 1) ( ( ) ( 1)[ ( 1)
( 1)
( 1) ] ( 1));
ji ji ac j i
j
jj ji
ji
a k a k e k c k x k
x k
a k a k
a
η
α
+
= − + − − +
∂ −− + Δ −∂
 (50) 
 
( ) ( 1) ( ( ) ( 1)[ ( 1)
( 1)
( 1) ] ( 1));
jj jj j j
j
jj jj
jj
b k b k e k c k n k
x k
a k b k
b
η
α
+
= − + − − +
∂ −− + Δ −∂
 (51) 
 
2
( ) ( 1) ( ( ) ( 1) [ ( ) ( 1)
( 1)4 ( 1) ] ( 1));
( )j j
ji ji j i jj
j
jj jip p
ji
w k w k e k c k u k b k
x k
a k w k
we e
η
α
+
−
= − + − × − ×
∂ −+ − + Δ −∂+
 (52) 
 
 
 19
summarized as follows: 
 (2) (2) (2)
1
,
p
s sr r s
r
f w v d
=
= +∑  (56) 
 
(2) (2)
(2)
(2) (2)
exp( ) exp( )
exp( ) exp( )
s s
s
s s
f fo
f f
− −= + − ,  (57) 
 (3) (3) (2) (3) (3)
1
,
J
q qs s q q
s
f w o d o
=
= + =∑  (58) 
where (2)srw  is the weight between the rth input neuron and the sth hidden neuron and 
(2)
sd  is 
the bias of the sth hidden neuron; (3)qsw  is the weight between the sth hidden neuron and the qth 
output neuron and (3)qd  is the bias of the qth output neuron. Likewise, the initial values of the 
weights and biases can be determined by the initialization method of the proposed FACA.  
Based on the error between the linear reference model and the unknown plant with the NLE, 
we can define the error function with respect to the adjustable parameters (w) as: 
 2 21 1ˆ( , ) ( ( ) ( )) ( ),2 2l p lE k y k y k e k= − =w  (59) 
where ˆ( ) ( ) ( )l pe k y k y k= −  and yp(k) and ˆ( )y k  are the actual output of the unknown plant and 
the desired linear reference output, respectively. The error signal in (59) is not directly applicable 
for tuning the NLE parameters. It needs to be propagated from the system output through the 
FARNN to the corresponding parameters of the NLE. We now derive recursive update rules for 
the parameters of the NLE as follows. The update rule of (3)qsw  is: 
 (3) (3) (3)
( )( ) ( 1) ( ),lqs qs l
qs
E kw k w k
w
ζ
+∂= − + − ∂  (60) 
where lζ  is the learning rate for adjusting all parameters of the NLE, and  
 
(3)
(3) (3) (3)
( )( ) ( ) ,
( )
ql l
qs q qs
o kE k E k
w o k w
+ + ∂∂ ∂=∂ ∂ ∂  (61) 
where (3)q io u= in (5), i = q, and 
 
(3) (3) 3
(2)
(3) 3 (3)
( ) ( ) ( )
.
( )
q q q
s
qs q qs
o k o k f k
o
w f k w
∂ ∂ ∂= =∂ ∂ ∂  (62) 
According to (4)-(6), we derive (3)( ) ( )l qE k o k
+∂ ∂  as 
 (3) (3) (3) (3)
1 1
( ) ( ) ( ) ( 1)( ) ( ) ( ) ,
( ) ( ) ( ) ( ) ( ) ( 1) ( )
J J
j j j jl l l
j jq j q j q j q
x k x k x k x kE k E k E k
o k x k o k x k o k x k o k
+ ++
= =
⎛ ⎞∂ ∂ ∂ ∂ −∂ ∂ ∂= = +⎜ ⎟⎜ ⎟∂ ∂ ∂ ∂ ∂ ∂ − ∂⎝ ⎠∑ ∑  (63) 
 
 21
100 200 300 400
1
2
3
Time steps
M
on
ito
re
d 
or
de
rs
Submodel I
100 200 300 400
1
2
3
4
Time steps
M
on
ito
re
d 
or
de
rs
Submodel II
 
Fig. 4. The results of the online order estimation process for Example 1 and the symbols ‘*’ 
indicate when the number of monitored order increases. 
 
 
(3) (2)
(2) (3) (2) (2)
1
( )( ) ( )( ) ,
( )
p
ql l s
qs q s s
o kE k E k o
d o k o d
+ +
=
∂∂ ∂ ∂=∂ ∂ ∂ ∂∑  (71) 
and  
 
(2) (2) (2)
(2) (2) (2) (2) (2) 2
( ) ( ) 4 .
( ) (exp( ) exp( ))
s s s
s s s s s
o o k f k
d f k d f f
∂ ∂ ∂= =∂ ∂ ∂ + −  (72) 
If the NLE learning is perfect, the unknown system cascaded with the NLE will behave 
exactly like the linear reference model. Consequently, a conventional linear feedback controller 
can be designed effortlessly to control the unknown system, shown in Fig. 3.  
 
IV. SIMULATION RESULTS 
To validate the effectiveness of our approach, extensive computer simulations have been 
conducted for diverse dynamic applications. Due to the limited space, we only provide three 
benchmark nonlinear dynamic applications, including an example of 
multiple-input-multiple-output (MIMO) dynamic plant identification, an example of 
single-input-single-output (SISO) dynamic plant control, and an example of noise canceling to 
test the capability and effectiveness of our recurrent architectures with the corresponding 
construction algorithms to handle temporal relationships.  
 
Example 1: MIMO Dynamic Plant Identification 
The MIMO plant with two inputs and two outputs is the same as that used in [29], [18], [36]. 
The plant is specified by the following equation: 
 
1
1 12
2
1 2
2 22
2
( )
( 1) 0.5[ ( )],
(1 ( ))
( ) ( )
( 1) 0.5[ ( )].
(1 ( ))
p
p
p
p p
p
p
y k
y k u k
y k
y k y k
y k u k
y k
+ = ++
+ = ++
 (73) 
There are two inputs, u1(k) and u2(k), and two outputs, yp1(k) and yp2(k) in our recurrent 
neuro-fuzzy network. We used a similar procedure in [18] to train our state-space recurrent NFS.  
 23
TABLE II 
PERFORMANCE COMPARISONS OF OUR NETWORK WITH EXISTING RECURRENT NETWORKS FOR 
EXAMPLE 1 
Network 
structure No. of rules 
Training time 
steps MSE 
No. of 
parameters 
Our NFS 2 11,000 yp1=3.84×10
-3 
yp2=5.46×10-3 
18 
RSONFIS 
[18] 7 11,000 
yp1=1.24×10-2 
yp2=1.97×10-2 
77 
MNN [36] — 77,000 yp1=1.86×10
-2 
yp2=3.27×10-2 
131 
 
The state-space equations are written as 
 
0.15952 0.18809 2.3736 0
( 1) ( ) ( ( ));
0.044518 0.18833 0 2.5537
0.40925 0.40891
( ) ( ),
0.193 0.28213p
k k k
k k
⎡ ⎤ ⎡ ⎤+ = +⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
−⎡ ⎤= ⎢ ⎥−⎣ ⎦
x x P u
y x
 (74) 
where x(k) = [x1(k), x2(k)]T are the state variables. The eigenvalues of the matrix A are 0.0813 
and 0.2666, which implies the trained network is stable. Eq. (75) adopted from [18] is used as the 
testing input signal to verify the identification performance of our proposed network. 
 
sin( / 25),                                0 250
1.0,                                            250 500
( ) 1.0,                                          500 750   
0.3sin( / 25) 0.1sin( / 32)
0
i
k k
k
u k k
k k
π
π π
≤ <
≤ <
= − ≤ <
+
+ .6sin( /10),                        750 1000k kπ
⎧⎪⎪⎪⎨⎪⎪ ≤ <⎪⎩
 (75) 
Fig. 5 shows the simulation results. The solid curves represent the desired outputs of the MIMO 
plant and the dotted curves the outputs of our network. Table II shows the performance 
comparisons of our network with two existing recurrent networks for this MIMO identification 
task. From Table II, our network outperforms those two recurrent networks in identification 
accuracy as well as in network compactness. 
 
Example 2: SISO Dynamic Plant Control 
The controlled plant is the same as in [18], [29], [36] and is given by 
 2 2
( ) ( 1)( ( ) 2.5)
( 1) 0.35 ( ) .
1 ( ) ( 1)
p p p
p
p p
y k y k y k
y k u k
y k y k
⎡ ⎤− ++ = +⎢ ⎥+ + −⎢ ⎥⎣ ⎦
 (76) 
The tracking model is a second-order linear system given by 
 ( 1) 0.6 ( ) 0.2 ( 1) 0.1 ( ).m m my k y k y k r k+ = + − +   (77) 
where r(k) is a bounded reference input. This objective of the control problem is to determine a 
bounded control force u(k) such that limk→∞ ec(k) = ym(k) – yp(k) = 0. Note that in our simulations, 
the plant model was only used for the generation of input-output data. None of the plant 
information is used in the modeling or control procedures. 
We first perform the system modeling of the above dynamic plant using the FARNN with  
 25
200 400 600 800 1000 1200 1400 1600 1800 2000
6
4
2
0
0.2
0.4
0.6
Time Steps
(a)  
Ou
tp
ut
 
Fig. 8. The hybrid control with a PID controller (dotted curve) and the desired trajectory (solid 
curve). 
 
With the obtained system order, the FARNN is constructed and its parameters are 
subsequently initialized and optimized using the initialization technique and the online recursive 
recurrent learning algorithm, receptively. The total number of parameters of the FARNN is 12, 
and the state-space equations extracted from the trained FARNN are written as  
 
[ ]
0.2786 0.1255 0.6610 0
( 1) ( ) ( ( )),
0.0401 0.0894 0 0.9691
( ) 0.7058 0.2575 ( ),
k k u k
y k k
− −⎡ ⎤ ⎡ ⎤+ = +⎢ ⎥ ⎢ ⎥−⎣ ⎦ ⎣ ⎦
= −
x x N
x
 (79) 
where x(k) = [x1(k), x2(k)] are the state variables. Fig. 7 illustrates the outputs of the unknown 
plant and the trained FARNN for the test input signal in (75). The FARNN can closely emulate 
the given plant and achieve the accuracy of a mean squared error (MSE) = 6.2565×10-4. 
In the NLE constructing phase, we first build a reference linear system with the matrices A 
and C directly copied from the above trained FARNN. The least-squares method is applied to 
determine the matrix B′ to solve (55) with 100 time steps using a random input signal uniformly 
distributed in the interval [-2, 2]. The linear reference system is obtained and expressed as 
 
[ ]
0.2786 0.1255 0.2371
ˆ ˆ( 1) ( ) ( ),
0.0401 0.0894 0.6104
ˆ ˆ( ) 0.7058 0.2575 ( ).
k k u k
y k k
− − −⎡ ⎤ ⎡ ⎤+ = +⎢ ⎥ ⎢ ⎥−⎣ ⎦ ⎣ ⎦
= −
x x
x
 (80) 
The above linear system is controllable since its controllability matrix has a full rank. A total of 
900 time steps, including 400 time steps of an i.i.d. uniform sequence within the limits [-2, 2] 
and a sinusoid signal given by sin(πk/45) for the remaining time, are generated to train the NLE 
with the learning rate equal to 0.1. The training structure is shown in Fig. 2.  The NLE contains 
two hidden neurons and one output neuron and the total number of parameters is 7. After training 
the NLE, a linear controller based on the linear reference system is designed to control the 
unknown system. In this example, three linear controllers, including PID [12], pole placement 
[13] and optimal control [28], are designed to verify the flexibility and effectiveness of our 
hybrid control scheme. For the PID controller, according to the linear reference model, the gain 
was set to Ku = 1.6052 and the oscillation period was set to Pu = 3.1416. We hence obtained the 
parameters of the PID, Kp = 0.9631, TI = 1.5708 and TD = 0.3927. For the pole placement 
controller, the state feedback gain K and the state command matrix N  were [0.5651 0.116] and 
2.4605, respectively, since the locations of the system roots were set to [0.15 –0.25]. The optimal 
tracker parameters were obtained by P = R = 1 and Q = 500. To test these hybrid controllers, we 
used the following reference input [36]. 
 27
100 200 300 400 500 600 700
1
2
3
4
Time steps
M
on
ito
re
d 
or
de
rs
 
Fig. 10. The results of the online order estimation process for Example 3 and the symbols ‘*’ 
indicate when the number of monitored order increases. 
 
0 2000 4000 6000 8000 10000 12000 14000 16000
0
2x 10
4
Time steps
A
m
pl
itu
de
0 2000 4000 6000 8000 10000 12000 14000 160002
0
2x 10
4
Time steps
A
m
pl
itu
de
0 2000 4000 6000 8000 10000 12000 14000 160002
0
2x 10
4
Time steps
A
m
pl
itu
de
 
Fig. 11. (a) The original speech signal s(k). (b) The corrupted signal x(k). (c) The recovered 
signal e(k) by our filter. 
 
TABLE III 
PERCENTAGE OF EACH MONITORED ORDER FOR EXAMPLE2 
Order 1 2 3 4 
Percentage 0.0028% 16.6438% 83.3533% - 
 
can be used as the error signal for tuning the parameters of the filter. 
In this example, the original speech is a sequence of Mandarin digits obtained from [19] and 
the noise signal is adopted from the NOISEX-92 database [41]. The relation between the 
reference noise r(k) and the corrupting noise n(k) is expressed by the following dynamic 
nonlinear equation [19]. 
 
 29
0 1 2 3 4 5
0
5
10
SN
R
m
(a)
Digital "1"
0 1 2 3 4 5
0
5
10
SN
R
m
(b)
Digital "2"
0 1 2 3 4 5
0
5
10
SN
R
m
(c)
Digital "3"
0 1 2 3 4 5
0
5
10
SN
R
m
(d)
Digital "4"
Our filter
RAFF
Elman
Original
 
Fig. 13. The original SNR values (“*”) and the enhanced SNR values by our filter (“◦”), 
RAFF (“+”), and Elman filter (“x”) for different values of m and different Mandarin digits. (a) 
“1”. (b) “2”. (c) “3”. (d) “4”. 
 
TABLE V 
THE COMPARISONS BETWEEN THE PROPOSED MODEL AND ELMAN MODEL IN THE LOGN-DELAY 
ENVIRONMENT 
 Delay Samples 
Model 0 1 2 3 4 5 
No. of 
parameters
Original -1 dB -1 dB -1 dB -1 dB -1dB -1 dB - 
Our filter 9.56 dB 8.73 dB 7.42 dB 6.90 dB 5.86 dB 4.57 dB 27 
Elman 5.16 dB 5.15 dB 4.42 dB 3.15 dB 1.71 dB 0.58 dB 28 
 
1 2
1 2
1 2
1
1 2
Rule 1:   ( ) is (0.5976,  0.5998) and ( ) is (0.9702,  0.3351)
             ( 1) is 0.5830 ( ).
Rule 2 :   ( ) is (0.1685,  0.4921) and ( ) is (0.4733,  0.5658)
             
u u
u u
u k u u k u
y k x k
u k u u k u
+
IF
THEN
IF
THEN
1 2
2
1 2
3
( 1) is 0.1476 ( ).
Rule 3 :   ( ) is (0.0281,  0.5878) and ( ) is (0.0215,  0.4674 ) 
             ( 1) is 0.4345 ( ).
u u
y k x k
u k u u k u
y k x k
+
+
IF
THEN
 
Fig. 12(a) and (b) show the membership functions of the learned filter for the two inputs, r(k) 
and r(k-1), respectively. The state-space equations extracted from the learned filter are written as  
 31
proposed FARNN is an ad hoc network that describes an unknown dynamic system as a 
state-space representation and employs the proposed fully automated construction performance. 
Although the proposed FARNN has a restricted model representation, without a priori 
information of the system, it is difficult, if not impossible, to conclude what kind of model 
representation will fit the system best. Hence, we aimed at the development of a systematic 
framework that can work robustly, reliably, and automatically. The both proposed architectures 
are appropriate structures that ease the design of effective structure and parameter initialization 
and learning algorithms. We believe that developing effective initialization and learning 
algorithms for a selected model representation would guarantee the modeling result to a certain 
degree of satisfaction. In addition, the issues of identification and control are usually treated as 
two separate tasks in most controller design problems. Here, we regard these two tasks as 
integral and take full advantage of the procedures conducted for system modeling to design an 
effective linear controller. Based on this identified FARNN, the design of the NLE and the linear 
feedback controllers was performed systematically and effortlessly. Unlike a traditional 
linearization method that linearizes unknown systems with respect to operating points and 
applies the well-developed linear control theory to design controllers for an unknown system, our 
linear reference model is obtained by removing the global nonlinear behavior of the unknown 
system using the NLE. If the system modeling and the NLE are perfect, the compound model, 
the unknown system cascaded with the inverse model, will behave like a pure linear model. 
Hence, the feedback linear controller designed based on the linear model can perform well in a 
wide operating range. Several remarkable results on benchmark examples of nonlinear dynamic 
applications have confirmed the effectiveness of the proposed recurrent structures in constructing 
parsimonious networks and controlling unknown systems with superior performance in an 
automatic manner. 
 
VI. REFERENCES 
[1] A. Atiya and C. Ji, “How initial conditions affect generalization performance in large 
networks,” IEEE Trans. Neural Networks, vol. 8, no. 2, pp. 448-451, 1997. 
[2] H. Akaike, “A new look at the statistical model identification,” IEEE Trans. Automat. 
Contr., vol. AC-19, pp. 716-723, Dec. 1974. 
[3] N. E. Barabanov and D.V. Prokhorov, “Stability analysis of discrete-time recurrent neural 
networks,” IEEE Trans. Neural Networks, vol. 13, no. 2, pp. 292-303, 2002. 
[4] P. Campolucci, A.Uncini, F. Piazza and B. D. Rao, “Online learning algorithms for 
locally recurrent neural networks,” IEEE Trans. Neural Networks, vol. 10, no. 2, pp. 
253-271, 1999. 
[5] S. Chen, F. N. Cowan and P. M. Grant, “Orthogonal least squares learning algorithm for 
radial basis function networks,” IEEE Trans. Neural Networks, vol. 2, no. 2. pp. 302-309, 
1991. 
[6] S. L. Chiu, “Fuzzy model identification based on cluster estimation,” J. Intell. Fuzzy 
Syst., vol. 2, pp. 267-278, 1994. 
[7] G. Chen, Y. Chen, and H. Ogmen, “Identifying chaotic systems via a Wiener-type 
 33
2001. 
[27] T. Lin, B. G. Horne, C. L. Giles, and S. Y. Kung, “What to remember: how memory order 
affects the performance of NARX neural networks,” IEEE World Conf. on 
Computational Intelligence, pp. 1051-1056, 1998. 
[28] F. L. Lewis and V.L. Syrmos, Optimal Control, John Wiley, 2nd, Edition, New York, 
1995. 
[29] K. S. Narendra and K. Parthasarathy, “Identification and control of dynamical systems 
using neural networks,” IEEE Trans. Neural Networks, vol. 1, no. 1, pp. 4-27, 1990. 
[30] S. Mitra and Y. Hayashi, “Neuro-fuzzy rule generation: Survey in soft computing 
framework,” IEEE Trans. Neural Networks, vol. 11, no. 3. pp. 748-768, 2000. 
[31] P. A. Mastorocostas and J. B. Theocharis, “An orthogonal least-squares method for 
recurrent fuzzy-neural modeling,” Fuzzy Sets & Systems, vol. 140, pp. 285-300, 2003. 
[32] P. A. Mastorocostas and J. B. Theocharis, “A recurrent fuzzy-neural model for dynamic 
system identification,” IEEE Trans. Syst. Man & Cyber.-Part B, vol. 32, no. 2, pp. 
176-190, 2002. 
[33] S. L. Marple, Jr., Digital Spectral Analysis with Applications, Prentice Hall, 1987. 
[34] S. C. Sivakumar, W. Robertson and W. J. Phillips, “On-line stabilization of 
block-diagonal recurrent neural networks,” IEEE Trans. Neural Networks, vol. 13, no. 2, 
pp. 292-303, 2002. 
[35] B. De Schutter, “Minimal state-space realization in linear system theory: An overview,” 
Journal of Computational & Applied Math., vol. 121, no. 1-2, p.331-354, 2000.   
[36] P. S. Sastry, G. Santharam and K. P. Unnikrishnan, “Memory neuron networks for 
identification and control of dynamical systems,” IEEE Trans. Neural Networks, vol. 5, 
no. 2, pp. 306 -319, 1994. 
[37] J. Theocharis and G. Vachtsevanost, “Recursive learning algorithms for training fuzzy 
recurrent models,” Int'l Journal of Intel. Syst., vol. 11, pp. 1059-1098, 1996. 
[38] A. C. Tsoi and A. D. Back, “Locally recurrent globally feedforward Networks: A critical 
review of architectures,” IEEE Trans. Neural Networks, vol. 5, no. 2, pp. 229-239, 1994. 
[39] A. C. Tsoi and A. D. Back, “Discrete time recurrent neural network architectures: A 
unifying review,” Neurocomputing, vol. 15, pp. 183-223, 1997. 
[40] K. Tanabe, “Projection method for solving a singular system of linear equations and its 
applications,” Numer. Math., vol. 17, pp. 203–214, 1971. 
[41] A. Varga, H. J. M. Stecncken, M. Tomlinson, and D. Joncs, “The NOISEX-92 study on 
the effect of additive noise automatic speech recognition,” in Description of RSG. 10 and 
Esprit SAM Experiment and Database, Malvern, U. K.: DRA Speech Res., 1992. 
[42] P. Werbos, Beyond Regression: New Tools for Prediction and Analysis in the Behavior 
Sciences, Ph.D. dissertation, Harvard Univ., Cambridge, MA, 1974. 
[43] J. S. Wang and Y. P. Chen, "A fully automated recurrent neural network for unknown 
dynamic system identification and control," IEEE Trans. Circuits and Systems-I, vol. 56, 
no. 6, pp. 1363-1372, 2006. 
[44] J. S. Wang and C. S. G. Lee, “Self-adaptive neuro-fuzzy inference systems for 
classification applications,” IEEE Trans. Fuzzy Systems, vol. 10, no. 6, pp. 790-802, 
2002. 
 35
International Conference Papers: 
1. Jeen-Shing Wang and Jen-Chieh Chiang, "Support vector clustering with a novel cluster 
validity method," Proc. of 2006 IEEE International Conference on Systems, Man & 
Cybernetics, Taipei, Taiwan, pp. 3715-3720, October, 2006. (NSC 93-2213-E-006-130) 
2. Jeen-Shing Wang and Yen-Ping Chen, "A Hammerstein neuro-fuzzy network with an 
online hybrid construction algorithm for dynamic applications," Proc. of 2006 IEEE 
World Congress on Computational Intelligence, Vancouver, Canada, pp. 9946-9953, July 
16-21, 2006. (NSC 93-2213-E-006-130) 
3. Yen-Ping Chen and Jeen-Shing Wang, "A novel nonlinear dynamical systems control 
using linear controllers with nonlinearity eliminators," Proc. of 2006 IEEE World 
Congress on Computational Intelligence, Vancouver, Canada, pp. 10737-10744, July 
16-21, 2006. (NSC 93-2213-E-006-130) 
4. Yen-Ping Chen and Jeen-Shing Wang, "Identification of chaotic systems using a 
self-constructing recurrent neural network," Proc. of 2005 IEEE International Conference 
on Systems, Man & Cybernetics, The Big Island, Hawaii, pp. 2150-2155, Oct. 10-12, 
2005. (NSC 93-2213-E-006-130) 
5. Yen-Ping Chen and Jeen-Shing Wang, "An automated Hammerstein recurrent neural 
network for dynamic applications," Proc. of IASTED International Conference on 
Computational Intelligence, Calgary, Canada, July 4-6, 2005. (NSC 93-2213-E-006-130) 
6. Jen-Chieh Chiang and Jeen-Shing Wang, “A validity-guided support vector clustering 
algorithm for identification of optimal cluster configuration ,” Proc. of 2004 IEEE 
International Conference on Systems, Man & Cybernetics, Hague, Netherlands, pp. 
3613-3618, October 10-13, 2004. (NSC 93-2213-E-006-130) 
7. Jiun-Kai Wang and Jeen-Shing Wang, “Efficient function approximation using an online 
self-regulating clustering algorithm,” Proc. of 2004 IEEE International Conference on 
Systems, Man & Cybernetics, Hague, Netherlands, pp. 5935-5940, October 10-13, 2004. 
(NSC 93-2213-E-006-130) 
 
 
1364 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 53, NO. 6, JUNE 2006
Fig. 1. (a) Topology of the proposed novel RNN. (b) Block diagram of the
proposed network.
effectiveness of the FARNN in constructing a parsimonious
network with satisfactory performance.
The rest of this paper is organized as follows. In Section II, we
shall introduce the proposed RNN architecture with its primitive
ideas. A fully automated construction algorithm for establishing
a parsimonious network is presented in Section III. Section IV
provides the computer simulations of nonlinear dynamic appli-
cations to validate the effectiveness of the proposed FARNN.
Finally, conclusions are summarized in the last section.
II. STRUCTURE OF RNNs
For system identification problems, a good model selection
can guarantee satisfactions on: 1) the flexibility of describing
different systems, 2) the algorithm complexity of constructing
quality models, and 3) the conciseness of model parameters.
Here, we integrated a neural framework with the concept of
states in linear systems to develop a novel RNN structure. The
structure incorporates dynamic elements in such a way that each
state corresponds to a neuron that linearly combines its own
output history and the outputs from other neurons. Such a de-
ployment of dynamic elements enables the proposed structure to
be mapped into a state-space equation from its internal structure.
Fig. 1(a) shows the proposed recurrent structure. The structure
consists of four layers, which can be expressed by the block di-
agram shown in Fig. 1(b). The input layer is only responsible
for the transmission of input values to the network. The neurons
in the hidden layer integrate all input variables and perform a
nonlinear transformation via nonlinear activation functions. In
the dynamic layer, the neurons develop internal dynamics by
their self-feedback and weighted connections with the feedback
connections from other neurons. That is, the dynamic layer in-
tegrates the current input information from the second layer and
the state history (stored in the memory device) of the neurons in
the dynamic layer to infer the current states of the network.
Finally, the outputs of the network are obtained by the linear
combination of the states and their associated parameters.
The whole network can be classified into two major compo-
nents: a static nonlinear part and a dynamic linear part. The static
nonlinear part maps the input space into a state space via a non-
linear transformation, and then the state space is transformed
into the output space through a linear dynamic mapping. The
state space equations of the proposed network can be written as
(1)
where
is the input vector, is the
output vector, and and are the dimensions of the input and
output layers, respectively. The elements of matrix stand for
the degree of inter-correlation among the states. is a diagonal
matrix with diagonal elements , representing the
weights of the inputs of the dynamic layer. The elements of ma-
trix are the weights of the states. is the
nonlinear function vector. is the state vector
where is the total number of state variables and is equal to the
number of neurons in the hidden layer and the dynamic layer.
The current th output and the state variables are ob-
tained by calculating the activities of all nodes on each layer and
the corresponding functions are summarized as
(2)
(3)
(4)
(5)
where is the weight between the th input neuron and the
th hidden neuron; is the bias of the th hidden neuron. The
hyperbolic tangent sigmoid function in (4) is selected as the non-
linear activation function of the hidden layer because it can pro-
vide a dual polarity signal to the output.
So far, we have shown that state space equations can be ex-
tracted from the proposed network topology directly. Note that
the system order is defined as the number of the state variables,
denoted as . Since the number of nodes in the hidden layer
as well as in the dynamic layer is the same as the number of
state variables, the overall network size is determined once the
system order is obtained. The total number of parameters equals
to . The state-space equations obtained from
the proposed recurrent network can be utilized for further the-
oretical analysis using the well-developed theory of linear sys-
tems. For example, one can use simple algebraic conditions to
monitor the matrix of the FARNN for a stability examina-
tion. That is, the overall system is stable if all eigenvalues lie to
the inside of the unit circle. A fully automated construction al-
gorithm will be introduced in the next section to determine the
minimal order of the network, to initialize the parameters with
1366 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 53, NO. 6, JUNE 2006
The following index first proposed in [11] is used to estimate
an appropriate order.
(9)
where is the th largest Lipschitz quotient among all
with the input variables and is a positive number (usu-
ally selected to be – ). The index is equal
to the geometric mean of the sequence ,
for avoiding the effect of measurement noise. The following
condition is utilized to select an appropriate order :
(10)
Based on the above discussion, we extend the aforemen-
tioned approach to develop our order determination algorithm
for MISO systems. Given input–output training patterns,
a nonlinear MISO dynamic system can be represented by the
following difference equation:
(11)
where and are the output and inputs of the system,
respectively, and is the order of the output. ,
is the order of each input. Now, we summarize our order deter-
mination algorithm for MISO nonlinear systems as follows.
Step 1) Let and
be denoted as the index vector
of the corresponding orders. Set and
initially. The objective of the
order determination is to find the appropriate order
of each for the input–output
model in (11). Next, we utilize a stop index vector
with zeros initially for determining the
completion of order determination of , if the
value is turned into one.
Step 2) Set ;
Step 3) Repeat
Substep 1: If the stop index , then goto
Substep 2; otherwise, goto Substep 6;
Substep 2: Set the order index and
obtain , the sum of the vector . represents
the amount of input variables in (11);
Substep 3: Set the input variable be the th
lag of ;
Substep 4: Determine the Lipschitz quotient
using (9);
Substep 5: If satisfies the following stop
criterion:
when and where is a pre-
specified threshold, then set and
. This means that the order of is found.
In our empirical experience, is suitable
for most cases;
Substep 6: Set . If is larger than
, then set ; Repeat until for
all .
Step 4) The system order is equal to , i.e., in (1) equals
to , and the order of each input is equal to
, i.e., in (1) equals to the sum of .
We will establish the structure of the proposed network by
utilizing the above order determination algorithm to obtain the
order of the system. Once the network is established, we then
assign the network parameters by the following hybrid param-
eter initialization method.
B. Hybrid Parameter Initialization Method
A plenty amount of research has shown that the selection of
initial parameters plays a critical role in the result of learning
convergence and the system performance. The parameter initial-
ization guides learning algorithms to search particular regions
of the parameter space. Here, we have developed a hybrid ini-
tialization method, consisting of two initialization techniques to
initialize the parameters of the static nonlinear part and the dy-
namic linear part of the proposed recurrent network.
1) Parameter Initialization of Static Nonlinear Subsystem:
The static nonlinear system of the proposed network is equiva-
lent to a single-layer neural network. Therefore, we can cast the
parameter initialization as a single-layer neural network initial-
ization problem. Yam and Chow [22] proposed a multidimen-
sional geometrical approach to accelerate the training speed of
feedforward networks. The underlying idea is to initialize the
parameters of the network so that all activation functions can
always be operated in an active region. Since the derivative of
activation functions within the active region has a more signif-
icant change than the one in the saturation region, it is highly
possible to rapidly reach a local minimum for parameter opti-
mization algorithms. In addition, the initialized parameters can
prevent the network from getting stuck in the beginning of the
training phase. Their study have validated that the number of
training iterations for achieving the pre-specified error criterion
was greatly reduced. Because hyperbolic tangent sigmoid func-
tions can provide a dual polarity signal to the output of the net-
work, we modified their approach for such functions to initialize
the parameters of the proposed static nonlinear system.
Our objective is to initialize the parameters and in (5)
so that the outputs of the neurons are guaranteed to be oper-
ated in the active region. The weights are assumed to be
independent and identically distributed (i.i.d.) uniform random
variables within the range . For each neuron, if
the value of in (5) falls beyond the active region, the neuron
enters the saturation region. Here, we define the active region
as the region where the derivative of the activation function is
greater than one-twentieth of the maximum derivative. Hence,
for hyperbolic tangent sigmoid functions, if we set the partial
derivative of (4) with respect to equal to one-twentieth of the
maximum derivative, the region that is considered
as the active region. Next, we shall introduce our parameter ini-
tialization method.
1368 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 53, NO. 6, JUNE 2006
where is the Euclidian -norm. The estimate is defined
by the minimization of (22)
(23)
The more detailed information for the PEM can be found in
[15], [18]. In the proposed structure, the matrix in (1) is set
as an identity matrix initially while the matrices and are
obtained by the PEM. In our investigation, the matrix plays
a critical role for the system stability. We usually select the el-
ements of the matrix with small values to avoid instability
during the parameter learning. After the determination of the
structure with initialized parameters, we further optimize the
overall network performance by tuning all the parameters with
a recursive recurrent learning algorithm.
C. Recursive Recurrent Learning Algorithm
In this learning phase, the parameters of the whole network
are recursively adjusted to memorize the desired dynamic
trajectories of unknown systems. Here, an online recursive
learning algorithm based on the ordered derivative [21] com-
bined with momentum terms is developed for the parameter
learning. The inclusion of momentum terms with a suitable
learning rate usually yields the convergence to a local min-
imum with a less number of learning epochs [10]. To ease our
discussion, the optimization target is characterized to minimize
the following error function with respect to the adjustable
parameters of the network for the case of a MISO system
(24)
where and are the desired
output and the current output, respectively. Let be the ad-
justable parameter of a node and the correction applied to
is defined by the ordered derivative with a momentum term
(25)
where is a learning rate and is the ordered deriva-
tive that considers the direct (current state) and indirect (pre-
vious states) effects of changing a structure parameter. The term
is the momentum where is a learning rate. The
general update rule is expressed as
(26)
The adjustable parameters of the proposed network consist of
the output link weights , the elements of the state
matrix , the elements of the input diagonal matrix
, and the link weights and bias of the
hidden layer.
According to the error function defined in (24) and the func-
tions in (2)–(5), we can derive recursive update rules for the pa-
rameters of the proposed network as follows. The update rule of
(the output link weights) is
(27)
where, according to (2),
is an ordinary derivative because there is no indirect effect on
changing . To update the rest of parameters ( and
), we have to propagate the current error signal to not only the
current state but also to the previous states. Thus, according to
(3), the update rule of (the elements of the state matrix) is
(28)
where is the learning rate for adjusting and , and
(29)
where and
(30)
where
, and when . Similarly,
the update rules for the rest adjustable parameters can be
derived based on the same procedure of the above derivations.
For the detailed derivations, please refer to [5].
D. Steps of Fully Automated Construction Algorithm (FACA)
We now summarize the proposed FACA algorithm as the fol-
lowing steps.
1) Identify the structure order based on the Lipschitz quotient
with the input–output training data.
2) Construct an initial network according to the identified
system order.
3) Initialize the parameters of the nonlinear static subsystem
with the maximum magnitude according to (17).
4) Initialize the parameters of the linear dynamic subsystem
using the prediction error method (PEM).
5) Update the parameters of the network via the online recur-
sive recurrent learning rules until a pre-specified stop cri-
terion is reached.
Note that, in Step 4, before initializing of the parameters
for the linear dynamic subsystem, we need to obtain the input
training data for the PEM. These input data are the outputs of
the nonlinear static subsystem (4) evaluated by the same input
training data used in Step 1.
IV. SIMULATION RESULTS
Extensive computer simulations have been conducted to val-
idate the performance of the proposed FARNN on various dy-
namic examples. Two benchmark examples—dynamic system
1370 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 53, NO. 6, JUNE 2006
TABLE I
COMPARISON BETWEEN RANDOM AND PROPOSED INITIALIZATION METHODS
initialization method, we set the matrix of (1) as the uni-
form distributed random variables within and the re-
maining parameters within . For the proposed initializa-
tion method, the nonlinear parameters are initialized within the
range obtained by (17) and the linear parameters by (23). We
conducted the same experiment for 30 times on each initializa-
tion method to test the robustness of learning convergence as
well as the identification performance by the aforementioned
training procedure with the learning rates and
. Seven out of 30 times were diverged during the param-
eter training process, i.e., the learning became unstable, when
the random initialization method was applied. The comparison
results, including the mean, standard deviation (std.) and min-
imum MSE, are shown in Table I. These results demonstrate that
the proposed initialization method is more robust in learning
convergence and superior in identification performance.
(34)
Part (b): MIMO dynamic plant identification. The MIMO
plant shown in (34) with two inputs and two outputs is the same
as in [19] and is used to demonstrate the capability of the pro-
posed algorithm in identifying a MIMO plant. Indeed, the order
determination technique and the recursive learning algorithm
perform well in dealing with MIMO systems if we decompose a
MIMO system into a collection of MISO subsystems. For iden-
tifying a MIMO system, the order determination process is ap-
plied to estimate the system order of each MISO subsystem. The
maximum estimated order among these subsystems is adopted
as the order of the entire MIMO system, and this order is used in
the FARNN construction. The stop criterion of the determina-
tion algorithm is satisfied at and for the two sub-
models, and thus the order of the system as well as each input
order is selected as 2.
Since the FARNN is a recurrent network with internal re-
currence/dynamics, using only the information of current in-
puts is enough to ensure the FARNN’s good performance in
modeling unknown systems. Hence, the so-called curse of di-
mensionality problem does not exist in using the proposed ap-
proach even for high-dimensional systems. To validate this, we
tested the FARNN with only current input variables (denoted as
FARNN I) for this example. That is, we only used the identi-
fied system order to construct the FARNN without considering
the input number obtained by the algorithm. Table II shows the
comparison results. Surprisingly, the result of the FARNN I is as
good as the FARNN with the input orders identified by the deter-
mination algorithm (denoted as FARNN II). The total number
TABLE II
PERFORMANCE COMPARISONS OF FARNN WITH SOME RECURRENT NETWORKS
Fig. 4. Outputs of the MIMO plant (solid curve) and FARNN II (dotted curve)
for Example 1.
of parameters for the FARNN II is 20, and its state-space equa-
tion extracted from the trained network is written as
(35)
where are the state variables. The
eigenvalues of the matrix are (0.1939, 0.4620) that indicate
the learning is converged. Fig. 4 depicts the outputs of the
MIMO plant and the trained FARNN II for the test input signal
in (32). The FARNN II can closely emulate the given plant.
Example 2: Dynamic plant control. This example is utilized
to demonstrate the capability of the FARNN in controlling a
1372 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 53, NO. 6, JUNE 2006
[4] S. Chen, S. A. Billings, and P. M. Grant, “Nonlinear system iden-
tification using neural networks,” Int. J. Contr., vol. 51, no. 6, pp.
1191–1214, 1990.
[5] Y. P. Chen and J. S. Wang, “A novel recurrent neural network with min-
imal representation for dynamic system identification,” in Proc. IEEE
Int. Joint Conf. Neural Netw., Jul. 2004, pp. 849–854.
[6] C. T. Chou and J. M. Maciejowski, “System identification using bal-
anced parameterizations,” IEEE Trans. Autom. Contr., vol. 42, no. 7,
pp. 956–974, Jul. 1997.
[7] E. J. Dempsey and D. T. Westwick, “Identification of Hammerstein
models with cubic spline nonlinearities,” IEEE Trans. Biomed. Eng.,
vol. 51, no. 2, pp. 237–245, Feb. 2004.
[8] W. Favoreel, B. De Moor, and P. Van Overschee, “Subspace state space
system identification for industrial processes,” J. Process Control, vol.
10, pp. 149–155, 2000.
[9] G. B. Giannakis and E. Serpedin, “A bibliography on nonlinear system
identification,” Signal Process., vol. 83, no. 3, pp. 533–580, 2001.
[10] S. Haykin, Neural Networks: A Comprehensive Foundation. Upper
Saddle River, NJ: Prentice-Hall, 1999.
[11] X. He and H. Asada, “A new method for identifying orders of
input–output models for nonlinear dynamic systems,” in Proc. Autom.
Contr. Conf., 1993, pp. 2520–2523.
[12] C. F. Juang and C. T. Lin, “A recurrent self-organizing neural fuzzy
inference network,” IEEE Trans. Neural Netw., vol. 10, no. 4, pp.
828–845, Aug. 1999.
[13] J.-N. Juang, Applied System Identification. Englewood Cliffs, NJ:
Prentice-Hall, 1994.
[14] T. Lin, B. G. Horne, C. L. Giles, and S. Y. Kung, “What to re-
member: How memory order affects the performance of NARX neural
networks,” in Proc. IEEE World Congr. Comput. Intell., 1998, pp.
1051–1056.
[15] L. Ljung, System Identification: Theory for the User. Upper Saddle
River, NJ: Prentice-Hall, 1999.
[16] L. Ljung, “Prediction error estimation methods,” Circuits Syst. Signal
Process., vol. 21, no. 1, pp. 11–21, 2002.
[17] S. L. Marple Jr., Digital Spectral Analysis with Applications. Engle-
wood Cliffs, NJ: Prentice-Hall, 1987.
[18] T. McKelvey, “Identification of State-Space Models from Time and
Frequency Data,” Ph.D. thesis, Department of Electrical Engineering,
Linkoping University, Linkoping, Sweden, 1995.
[19] K. S. Narendra and K. Parthasarathy, “Identification and control of dy-
namical systems using neural networks,” IEEE Trans. Neural Netw.,
vol. 1, no. 1, pp. 4–27, Feb. 1990.
[20] P. S. Sastry, G. Santharam, and K. P. Unnikrishnan, “Memory neuron
networks for identification and control of dynamical systems,” IEEE
Trans. Neural Netw., vol. 5, no. 2, pp. 306–319, Apr. 1994.
[21] P. Werbos, “Beyond Regression: New Tools for Prediction and Anal-
ysis in the Behavior Sciences,” Ph.D. dissertation, Harvard Univ., Cam-
bridge, MA, 1974.
[22] Y. F. Yam and T. W. S. Chow, “Feedforward networks training speed
enhancement by optimal initialization of the synaptic coefficients,”
IEEE Trans. Neural Netw., vol. 2, no. 2, pp. 430–434, Apr. 2001.
Jeen-Shing Wang (S’94–M’02) received the B.S.
and M.S. degrees from the University of Missouri,
Columbia, MO, in 1996 and 1997, respectively,
and the Ph.D. degree in electrical engineering from
Purdue University, West Lafayette, IN, in 2001.
Currently he is an Assistant Professor in the
Department of Electrical Engineering, National
Cheng Kung University, Tainan, Taiwan, R.O.C. His
research interests include soft computing, intelligent
control, and optimization.
Yen-Ping Chen (S’04) received the B.S. degree in
engineering science and the M.S. degree in electrical
engineering from National Cheng Kung University,
Tainan, Taiwan, R.O.C., in 2002 and 2004, respec-
tively. He is currently working toward the Ph.D. de-
gree in electrical engineering at the same university.
His research interests include computational intel-
ligence, intelligent control, and systems biology.
The organization of this paper is as follows. In Section II, 
we briefly introduce the SVC algorithm. Section III presents a 
novel cluster validity measure for the SVC algorithm. 
Computer simulations on the identification of optimal cluster 
configurations for artificial and benchmark examples are 
illustrated in Section IV. Finally, the conclusions are given in 
Section V. 
II. SVC ALGORITHM  
The mathematical formulation of the SVC algorithm is 
shown as follows. Assume given a data set {xi} ⊆ χ of N points, 
with χ ⊆ ℜd, the input data space. A nonlinear mapping 
function Φ is used to map χ into a high-dimensional feature 
space. The objective is to search for the smallest enclosing 
sphere of radius R expressed in the following optimization 
problem: 
2
2 2
min  
subject to ( ) -       
j
j
j j
R C
R j
ξ
ξ
+
Φ ≤ + ∀
∑
x a
         (1) 
where ║·║is the Euclidean norm, a is the center of the sphere, 
and ξj are slack variables allowing for soft boundaries. That is, 
these variables allow some data points lying out of the sphere. 
The above problem is usually solved by the Lagrangian 
function:  
22 2( , , , , ) - ( - ( ) - ) - ,j j j j j j j j j
j
L R R R Cξ β μ ξ β ξ μ ξ= + Φ +∑ ∑ ∑a x a  
(2) 
where βj ≥ 0 and µj ≥ 0 are the Lagrange multipliers, C is a 
constant, and C∑ξj is a penalty term. The function L is to be 
minimized with respect to R, a, and ξj, and to be maximized 
with respect to βj and µj. The Karush-Kuhn-Tucker (KKT) 
conditions allow the problem to be rewritten as 
2
,
max  ( ) - ( ) ( )
subject to  0  and 1, ,
j j i j i j
j i j
j j
j
W
C j
β β β
β β
= Φ Φ ⋅Φ
≤ ≤ = ∀
∑ ∑
∑
x x x
       (3) 
where the dot product of (Φ(xi)⋅Φ(xj)) represents the Mercer 
kernel K(xi, xj) by  
( ,  ) ( ) ( )i j i jK ≡ Φ ⋅Φx x x x .                               (4) 
In this study, we use Gaussian kernels K(xi, xj) = exp(-q║xi - 
xj║2). The Lagrangian W now can be written as 
2
,
max  ( ) - ( , )
subject to  0  and 1, .
j j i j i j
j i j
j j
j
W K K
C j
β β β
β β
=
≤ ≤ = ∀
∑ ∑
∑
x x x
         (5) 
We can optimize Eq. (5) to obtain the Lagrangian 
multipliers βj. There are data points, denoted as support 
vectors (SVs), lying on the surface of the sphere in the feature 
space when 0 < βj < C. When βj = C, the data points located 
outside the feature space are defined as bounded support 
vectors (BSVs). The SVs can be used to describe the 
hypersphere in the feature space. For each point x, the distance 
of its image in the feature space from the center of the sphere is 
given by: 
22 ( ) ( ) - .R = Φx x a                        (6) 
Eq. (6) can be rewritten as: 
2
,
( ) ( , ) - 2 ( , ) ( , ).j j i j i j
j i j
R K K Kβ β β= +∑ ∑x x x x x x x    (7) 
The radius R of the sphere can be obtained by: 
R = { R(xi) | xi is a support vector}.                      (8) 
In practice, the average of the above set is used as the radius 
R. The SVs, BSVs, and the other points are located on the 
cluster boundaries, the outside of the boundaries, and the 
inside of the boundaries, respectively. From the above 
discussion, one can find that the number of clusters and their 
shapes of boundaries are governed by two parameters, q and 
C. 
The contour description does not show which data points 
belong to which clusters. If there are two data points xi and xj 
that belong to the same cluster in the input space, one can 
check if the line segment between them is inside the high 
dimensional sphere in the feature space. Checking the line 
segment can be done by sampling a number of points on the 
segment (usually 10-20 points). The two data points xi and xj 
satisfying this condition are defined as connected components. 
An adjacency matrix A is defined to identify the connected 
components of a cluster. The components of A, aij, between 
pairs of points xi and xj are defined as: 
if for all  on the line segment 
1,
connecting  and ,  ( ) . =
0, otherwise.
i jij R Ra
⎧⎪ ≤⎨⎪⎩
y
x x y         (9) 
The values of y can be obtained by sampling a number of 
points from the line segment connecting xi and xj.  From the 
matrix A, if aij = 1, that means xi and xj belong to the same 
cluster; otherwise, they are in different clusters. In general, the 
labeling step that checks the connectivity for each pair of 
samples is more time-consuming than the SVC training step. 
The time complexity of this procedure is O(mN2), where m is 
the number of samples on the line segment. For efficient 
cluster labeling methods, Lee and Lee [13] have proposed an 
approach based on a topological property of a trained kernel 
radius function. 
III. A NOVEL CLUSTER VALIDITY METHOD FOR SVC 
Clustering is an unsupervised process since there are no 
predefined classes and no examples that would indicate the 
grouping properties of the data set [9]. With different settings, 
the same clustering algorithm can produce totally different 
results. Thus, in the SVC, one crucial problem that deserves 
further investigation is how to determine the parameters so 
that the optimal number of clusters can be identified. 
Generally speaking, if cluster analysis is to make an important 
contribution to real applications, much more attention must be 
paid to the issue of cluster validity, which is concerned with 
checking the quality of clustering results and determining the 
optimal number of clusters. Various methods have been 
proposed for cluster validity from a review of the literature 
3698716
Perform SVC algorithm. 
Set C = 1 and increase the 
value of q.
YES
Is the validity measure
minimal? 
Obtain the clustering 
results
NO
Input the data 
set
{ } di χ⊆ ⊆x
Final optimal 
clustering 
results
Does the result contain 
singleton clusters?
Decrease the values of C
and reset the value of q.
NO
YES
Fig. 1. The flowchart of the SVC with a novel cluster validity method. 
IV. SIMULATION RESULTS 
The effectiveness of the proposed cluster validity measure 
for the SVC algorithm has been validated through extensive 
computer simulations for different examples. We provide four 
examples that contain three benchmark data sets without noise 
and one artificial data set with noise. The three benchmark 
data sets include the crescent data set [1], Bensaid data set [17], 
and five-cluster data set [19]. As we mentioned before, these 
three data sets possess their own characteristics and are good 
representatives to use in evaluating the robustness of 
clustering algorithms. 
Example 1: Crescent data set 
This data set consists of 70 data points (i.e., N = 70) in a 
two-dimensional space, with 35 points in the upper cluster and 
35 points in the lower one. Obviously, the clusters in this data 
set are not linearly separable. This is suitable to demonstrate 
the capability of the proposed approach in handling clusters 
with arbitrary shapes. Our proposed cluster validity measure 
found that the optimal cluster number is 2, the value of q is 4, 
and there is no outlier; thus, C is set as 1. Figure 2 shows the 
results of the final cluster validity measure for the crescent 
data set. The values of the cluster validity measures and their 
corresponding number of clusters with different q are shown 
in Table 1.  
Example 2: Bensaid data set 
This data set contains 49 data points with different-sized 
clusters. There are two small clusters with sparse data points 
and a large cluster. From our proposed cluster validity 
measures, the optimal cluster number is 3, the value of q is 0.2, 
and the value of C is 1. Figure 3 shows the results of the final 
cluster validity measure for the Bensaid data set. The values of 
the cluster validity measures and their corresponding number 
of clusters of different q are shown in Table 2. We compare 
our validity measure with four popular validity measures: 
Bezdek’s partition coefficient (PC) [6], Bezdek’s partition 
entropy (PE) [20], Xie-Beni’s separation measure (XB) [7], 
and Fukuyama-Sugeno’s validity measure (FS) [18] and show 
the results in Table 3. From Table 3, only the PC, XB, and our 
validity measures are able to identify the correct cluster 
number. 
0 0.5 1 1.5 2 2.5 3 3.5 40
0.5
1
1.5
2
2.5
 x1
x 2
 
Fig 2. The final result of the cluster validity measure for the crescent data set. 
The SVs are marked with diamonds. 
 
TABLE 1 
CLUSTER VALIDITY MEASURE OF THE CRESCENT DATA SET 
Values of q No. of Clusters (m) V(m) 
4 2 3.1456 
8 2 4.3646 
12 2 4.3622 
16 2 4.3948 
20 2 4.3884 
24 2 4.4067 
28 3 5.6441 
32 6 3.9873 
36 7 3.6498 
 
0 2 4 6 8 10 12
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
x1
x 2
 
Fig. 3. The final result of the cluster validity measure for the Bensaid data set. 
The SVs are marked with diamonds. 
370018
TABLE 6 
CLUSTER VALIDITY MEASURE OF THE FIVE-CLUSTER DATA SET WITH NOISE 
Values of q Values of C No. of Clusters (m) V(m) 
1.4 0.02 5 0.4743 
1.6 0.02 5 0.5069 
1.8 0.02 5 0.5749 
2.0 0.02 5 0.6224 
2.2 0.02 5 0.7646 
2.4 0.02 5 0.6416 
2.6 0.02 5 0.6295 
2.8 0.02 5 0.6180 
 
V. CONCLUSION 
A novel cluster validity method has been proposed for the 
SVC algorithm. Without a priori knowledge of the data sets, 
our validity measure is able to automatically determine 
suitable ranges of the kernel parameter and the soft margin 
constant as well. Using these parameters, the SVC algorithm is 
capable of identifying the optimal cluster number in an 
effective way.  Finally, simulation results, including three 
benchmark data sets and one artificial data set with noise, have 
successfully validated the effectiveness of the proposed cluster 
validity method for the SVC. 
 
REFERENCES 
[1] A. Ben-Hur, D. Horn, H. T. Siegelmann, and V. Vapnik, “A support 
vector clustering method,” Proceedings of International Conference 
on Pattern Recognition, vol. 2, pp. 724-727, 2000. 
[2] A. Ben-Hur, D. Horn, H. T. Siegelmann, and V. Vapnik, “Support 
vector clustering,” Journal of Machine Learning Research, vol. 2, pp. 
125-137, 2001. 
[3] B. Boser, I. Guyon, and V. Vapnik, “A training algorithm for optimal 
margin classifiers,” Proceedings of the Fifth Annual Workshop on 
Computational Learning Theory, vol. 5, pp. 144-152, 1992. 
[4] C. Cortes and V. Vapnik, “Support-vector network,” Machine 
Learning, vol. 20, pp. 273-297, 1995. 
[5] A. K. Jain and R. C. Dubes, Algorithms for Clustering Data, Prentice 
Hall, Englewood Cliffs, New Jersey, 1988. 
[6] J. C. Bezdek, “Numerical taxonomy with fuzzy sets,” Journal of Math. 
Biology, vol. 1, pp. 57-71, 1974. 
[7] X. L. Xie and G. Beni, “A validity measure for fuzzy clustering,” IEEE 
Trans. on Pattern Analysis and Machine Intelligence, vol. 13, no. 8, pp. 
841-847, 1991. 
[8] H. S. Rhee and K. W. Oh, “A validity measure for fuzzy clustering and 
its use in selecting optimal number of clusters,” Proceedings of IEEE 
International Conference on Fuzzy Systems, vol. 2, pp. 
1020-1025, 1996. 
[9] J. C. Dunn, “A fuzzy relative of the ISODATA process and its use in 
detecting compact well-separation clusters,” Journal of Cybernetics, 
vol. 3, pp. 32-57, 1974. 
[10] D. L. Davies and D. W. Bouldin, “A cluster separation measure,” IEEE 
Trans. on Pattern Analysis and Machine Intelligence, vol. 1, no. 2, pp. 
224-227, 1979. 
[11] J. H. Yang and I. Lee, “Cluster validity through graph-based boundary 
analysis,” The 2004 International Conference on Information and 
Knowledge Engineering, pp. 204-210, 2004. 
[12] T. Ban and S. Abe, “Spatially chunking support vector cluster 
algorithm,” Proceedings of the 2004 International Joint Conference on 
Neural Networks, vol. 1, pp. 413-418, 2004. 
[13] J. W. Lee and D. W. Lee, “An improved cluster labeling method for 
support vector clustering,” IEEE Trans. on Pattern Analysis and 
Machine Intelligence, vol. 27, no. 3, pp. 461-464, 2005. 
[14] M. J. A. Berry and G. Linoff, Data Mining Techniques for Marketing, 
Sales and Customer Support, John Wiley & Sons, Inc., USA, 1996. 
[15] C. H. Chou, M. C. Su, and E. Lai, “A new cluster validity measure for 
clusters with different densities,” IASTED International Conference 
Intelligent Systems and Control, pp. 276-281, 2003. 
[16] J. C. Bezdek and N. R. Pal, “Some new indexes of cluster validity,” 
IEEE Trans. on Systems, Man, and Cybernetics—Part B: Cybernetics, 
vol. 28, no. 3, pp. 301-315, 1998. 
[17] A. M. Bensaid, L. O. Hall, J. C. Bezdek, L. P. Clarke, M. L. Silbiger, J. 
A. Arrington, and R. F. Murtagh, “Validity-guided (re)clustering with 
applications to image segmentation,” IEEE Trans. on Fuzzy Systems, 
vol. 4, no. 2, pp. 112-123, 1996. 
[18] Y. Fukuyama and M. Sugeno, “A new method of choosing the number 
of clusters for the fuzzy c-means method,” Proceedings of Fifth Fuzzy 
Systems Symposium, pp. 247-250, 1989. 
[19] M. C. Su, A New Index of Cluster Validity, 
http://www.cs.missouri.edu/~skubic/8820/ClusterValid.pdf. 2004. 
[20] J.C. Bezdek, “Cluster Validity with Fuzzy Sets,” J.  Cybernetics, vol. 3, 
pp. 58-72, 1974. 
370220
 2
states of a dynamic system. In addition, the states are usually 
inter-correlated in a certain degree. We shall use weights in the 
form of feedback connections to represent the 
inter-correlation. 
To implement the above ideas, we select a 
fuzzy-basis-function (FBF) network [11] as the antecedent 
part of a conventional (static) fuzzy system. The FBF network 
has been proved to be a universal approximator for any 
nonlinear function on a compact set. The function of a FBF (pj) 
is defined as: 
 
1
11
( )
( )
( )
j
i
j
i
n
iMi
j J n
iMij
u
p
u
u
μ
μ
=
==
=
∏
∑ ∏ , (1) 
 
where u = [u1, u2,…, un]T is the input vector and ( )j
i
iM
uμ  is the 
Gaussian membership function defined as ( )j
i
iM
uμ  
2exp{ (( ) / ) }j ji i iu m σ= − − , where jim  and jiσ  are the centers 
and widths of the Gaussian functions, respectively. Each 
output of the FBF network represents a normalized firing 
strength to reflect the input information. A dynamic layer 
consisting of dynamic elements and feedback connections is 
directly connected to the output of the FBF network to form a 
recurrent FBF network. Fig. 1(a) shows the topology of the 
proposed recurrent FBF network and Fig. 1(b) shows the 
corresponding block diagram. The dynamic layer integrates 
the current input information and the firing history of the rules 
stored in the memory device to infer the current states x(k) of 
the system. 
For a multi-input-multi-output (MIMO) dynamic system, 
the jth fuzzy dynamic rule at time k can be expressed as:  
 
1 1
1 1
Rule :  ( ) is  and  and ( ) is ,
( ) is ( ) and  and ( ) is ( ),
j j
n n
j j
m m
j u k M u k M
y k f k y k f k
IF
THEN
"
"               (2) 
where ( ) ( ),j jl l jf k c x k=  ui(i = 1, 2,…, n) and yl(l = 1, 2,…, m) 
are the input and output variables, respectively, and jiM  are 
the input fuzzy term sets and ( )jl jc x k are singleton 
constituents. From (2), the state variables are invisible in the 
antecedent parts. They are used to reflect the current input 
information combined with the past information in the 
consequent parts. The whole inference process agrees with the 
state-space concept of linear systems. 
The state-space equation extracted from the recurrent NFS 
can be written as  
 
( 1) ( ) ( ( ))
     ( ) ( ),
k k k
k k
x Ax BP u
y Cx
+ = +
=
 (3) 
 
where x and P∈\J; x = [x1, x2,…, xJ]T is the state vector, J is 
the total number of state variables and is equal to the number 
of rules, and P = [p1, p2,…, pJ]T is the firing strengths with 
respect to the input values. A and B∈\J×J, and C∈\m×J. The 
elements of matrix A stand for the degrees of inter-correlation 
among rules. B is a diagonal matrix with diagonal elements [b1, 
b2,…, bJ] representing the weights of the firing strengths. The 
elements of matrix C are the output singleton values. We have 
demonstrated that both linguistic rules and state-space 
equations can be directly extracted from the proposed network 
topology effortlessly. To effectively construct the internal 
network structures of the proposed network, an online hybrid 
construction algorithm is introduced to perform the structure 
learning and the parameter learning in the next section. 
 
III. ONLINE HYBRID CONSTRUCTION ALGORITHM 
The online hybrid construction algorithm consists of two 
learning phases—the structure learning and the parameter 
learning. In the structure learning phase, we cast the rule 
formation as a combination of an order selection and a 
clustering problem. First, we utilize an order determination 
technique to identify a minimal set of state variables (rules). 
The term “minimum set” means that the set is the same as or 
close to the true one of the given system. During the order 
determination process, a k-mean clustering algorithm is 
applied to identify the configuration of the clusters (rules). 
Upon the completion of the order determination and clustering 
process, we are able to construct an initial network based on 
the number of the state variables (or the system order) and the 
result of clustering configuration. After the structure learning 
phase, we continuously fine-tune the parameters of the 
network to closely capture the dynamic behavior of the 
unknown system via a recursive learning algorithm. We now 
introduce the proposed learning algorithm by the following 
three parts: A) a system order determination method, B) an 
online k-mean method, and C) a recursive learning algorithm. 
u1(k) R1R1
RJ RJ
R2 R2
p1(u(k))a11
a12
a1J
aJ2aJJ
aJ1
a2J
a22a21
x1(k)
x2(k)
xJ(k)
b1
b2
bJ
pJ(u(k))
(a)
(b)
Dynamic
   Layer
P(u(k)) +
A
u(k) x(k)Z-1
Z-1Σ
Z-1Σ
Z-1Σ
FBF
Network
    State
Variables
un(k)
B
 
 
Fig. 1.  (a) Recurrent FBF topology. (b) Block diagram of recurrent FBF. 
 
2105
 4
Step 2: [Process begins] Set k = 0 and initialize a temporary 
matrix M to store the first g largest Lipschitz 
quotients, where g is equal to the nearest integers of 
k/100. 
Step 3: Repeat 
Substep 1: Feed kth input-output data pair and store it in the 
memory. 
Substep 2: If k > 0 then goto Substep 3; otherwise goto Substep 
7. 
Substep 3: [Determination of Lipschitz quotients] Determine 
the Lipschitz quotients ( )hijq , h = 1,…, l + nm, by 
using (6), where i and j represent a new data match 
indexes. This new match index matrix is [1, …, k−1; 
k, …, k].  
  Substep 4: [Determination of geometric mean] Obtain the first 
g largest Lipschitz quotients among the new ( )hijq  
from Substep 3 and the temporary matrix M to 
determine the geometric mean of Lipschitz 
quotients ( )hgmq using (7). Update the temporary 
matrix M by storing the current g largest Lipschitz 
quotients. 
Substep 5: [Determination of order candidates] Determine the 
variation rate of Lipschitz quotients by the 
following criterion:
 
( 1) ( )
( )
variation( ) , 1, , 1
max(1, )
h h
gm gm
mh
gm
q q
h h n l
q
+ −= = + −… . 
The decision of the order candidate is based on 
finding the first index hv that makes the criterion 
variation(hv) < ε, where ε > 0 is a pre-specified 
threshold, then set the order candidate in the time 
step k be hv–l. If no one satisfies the above criterion, 
the number of the monitored order will be extended 
by setting nm = nm + 1. In our empirical experience, 
ε = 0.3 is suitable for most cases. The value of ε is 
larger than that in the offline method, since we 
restrict the number of delayed inputs. 
Substep 6: [Weighting procedure] The decision of order 
selection is easy to make with more input-output 
data.  We determine weighted percentages of the 
monitored orders by the weighted function 
1/(1+exp(−0.05×(k-150))).  
Substep 7: [Increment of time steps] Set k = k + 1; 
Repeat until the largest weighted percentage, 
denoted as wp1, satisfies the following stop 
criterion: 
( 1 2) 0.8
1
wp wp
wp
− > , (8) 
where wp2 is the second largest weighted 
percentage. 
Step 4: The system order is equal to the monitored order, 
which has the largest weighted percentage. 
The above algorithm can online estimate system orders 
and performs well even in the MIMO case if we decompose a 
MIMO system into a collection of MISO susbsystems. For 
identifying a MIMO system, the order determination process 
is applied to estimate the system order of each MISO 
subsystem, and it will persist until each sub estimation process 
satisfies the stop criterion (8) in Step 3. The maximum 
estimated order among these subsystems is selected as the 
order of the entire MIMO system. 
 
B. Online K-Mean Algorithm 
Clustering analysis has been widely applied to the fuzzy 
rule-base construction by extracting the representative 
knowledge from numerical data [9]. The basic idea is to treat 
each pair of the generated input and output clusters as a fuzzy 
rule. Thus, “clusters” and “rules” are usually interchangeable 
in these methods. One substantial problem in performing a 
cluster analysis is deciding on the number of clusters in the 
data set. The proposed order determination algorithm is 
developed to solve this problem. The resulted cluster 
configuration obtained by the k-mean algorithm can be 
directly applied to construct an initial network structure by 
treating each cluster as a fuzzy rule, i.e. a system state. For 
each cluster α, let mIα (σIα) and mOα (σOα) be the locations 
(variances) in the input space (I) and output space (O), 
respectively, and cα be the counter representing the number of 
patterns of this cluster. The online k-mean clustering algorithm 
can be summarized in the following steps: 
Step 1: [System initialization] Give a number of clusters: J. 
Step 2: Repeat: 
Substep 1: Feed kth input-output data, {[uk, yk ], where u∈\n 
and y∈\m}; 
Substep 2: [Initialization of seed clusters] Perform the 
initialization of new clusters for both the input and 
output spaces. If k <= J, then set ck = 1, assign mIk = 
uk and σIk =0 for input space, allocate mOk = yk and 
σOk = 0 for output space, and goto Substep 5; else 
goto Substep 3; 
Substep 3: [Winner cluster determination] The seed cluster 
closest to the current data point is defined as the 
winner cluster. For only input seed clusters, 
perform: winnerI = γ such that∥mIγ – u∥is 
minimal, and assign winnerO = γ since we assume 
that the input-output mapping relationship is 
consistent. 
Substep 4: [Parameter update] Update the parameters of the 
winner cluster; that is, the centers and variances for 
both input and output spaces and the counter. For 
the input space, the expected centers and variances 
of the winner cluster are successively estimated 
with the incoming data and will be used to 
determine the initialization of the structure. 
First, set cwinner = cwinner + 1; 
2 2 2( 1)( )2 2
_ ;
_ ;
_ .
Iwinner
winner
winner Iwinner Iwinner
winner
Iwinner Iwinner c
c
Iwinner Iwinnerc
Iwinner Iwinner
temp
temp
temp
u m
σ m u
m m
σ m
m m
−
− + +
= +
= −
=
 
Similarly, for the output space, set 
2107
 6
 
START
Set
No. of monitored order = nm
Stop_order = 0
Ini_cluster = 0
k=0
Stop_order = 1?
Feed kth input-
output data
k>0?
Determine Lipschitz 
quotients
qij(h), h=1, ,nm+l
i=1, ,k-1
j=k, ,k
Determine 
geometric mean
qgm(h), h=1, ,nm+l
variation(h)<0.3 
Increase no. of 
monitored order
nm=nm+1
Set k=k+1
Order candidate = hv-l
such that 
variation(hv)<0.3 and 
hv is minimal.
Determine weighted 
percentage.
1 2 0.8?
1
wp wp
wp
Stop_order = 1
Construct initial 
recurrent NFS with 
the corresponding 
result of k-mean.
k nm?
Initialization of seed 
clusters: ck(i)=1
mIk(i) = uk and Ik(i) =0
mOk(i) = yk and Ok(i) = 0
for i = k, ,nm
Ini_cluster(k)=1
k 1?
Winner clusters 
determination for 
ith monitored order,
i belongs to the set 
such that 
Ini_cluster(i) = 1
Update the 
parameters of the 
winner clusters
Feed kth input-
output data
Determine output 
error e(k)
Update all parameters 
of the recurrent NFS 
using the recursive 
recurrent learning 
algorithms
Satisfy
Performance 
requirements?
END
mn
YesNo
Yes
Yes
YesYes
Yes
Yes
No
oNoNoN
No
No
 
 
Fig. 2. The flowchart of the online hybrid construction algorithm. 
2109
 8
network [8], our approach outperforms these two recurrent 
networks in both accuracy and compactness of the network 
structure. 
 
V. CONCLUSION 
This paper presents a recurrent neuro-fuzzy network with 
online hybrid construction algorithm. The deployment of 
dynamic elements is the core issue of developing a recurrent 
network since it directly affects not only the articulate 
translation of linguistic rules but the further development of 
construction algorithms. The advantages of the proposed 
approach include: 1) a systematic and effective construction 
algorithm for constructing a parsimonious structure, and 2) the 
knowledge representation of the system that can be expressed 
in both a compact set of dynamic fuzzy rules and state-space 
equations. Computer simulations have validated the better 
performance of the proposed approach in higher accuracy, 
faster learning convergence and more compact structure than 
those of some existing recurrent networks. 
REFERENCES 
[1] V. Gorrini and H. Bersini, “Recurrent fuzzy systems,” Proc. of IEEE 
Int'l Conf. on Fuzzy Systems, 1994, vol. 1, pp. 193-198. 
[2] J. Theocharis and G. Vachtsevanost, “Recursive learning algorithms 
for training fuzzy recurrent models,” Int'l Journal of Intel. Syst., vol. 11, 
pp. 1059-1098, 1996. 
[3] Q. Gan and C. J. Harris, “Linearization and state estimation of 
unknown discrete-time nonlinear dynamic systems using recurrent 
neurofuzzy networks,” IEEE Trans. Syst. Man Cybern., vol. 29, no. 6, 
pp. 802-817, 1999. 
[4] C. F. Juang and C. T. Lin, “A recurrent self-organizing neural fuzzy 
inference Network,” IEEE Trans. on Neural Networks, vol. 10, no. 4, 
pp. 828-845, 1999. 
[5] J. Zhang and A. J. Morris, “Recurrent neuro-fuzzy networks for 
nonlinear process modeling,” IEEE Trans. on Neural Networks, vol. 
10, no. 2, pp. 313-326, March 1999. 
[6] C.-H. Lee and C.-C. Teng, “Identification and control of dynamic 
systems using rcurrent fzzy nural ntworks,” IEEE Trans. on Fuzzy 
Systems, vol. 8, no. 4, pp. 349-366, 2000. 
[7] F-J Lin and R-J Wai, “Hybrid control using recurrent fuzzy neural 
network for linear induction motor servo drive,” IEEE Trans. on Fuzzy 
Systems, vol. 9, no. 1, pp. 102 -115, 2001. 
[8] P. S. Sastry, G. Santharam and K. P. Unnikrishnan, “Memory neuron 
networks for identification and control of dynamical systems,” IEEE 
Trans. on Neural Networks, vol. 5, no. 2, pp. 306 -319, 1994. 
[9] J.-S. Wang and C. S. G. Lee, "Self-Adaptive Neuro-Fuzzy Inference 
Systems for Classification Applications," IEEE Transactions on Fuzzy 
Systems, vol. 10, no. 6, pp. 790-802, 2002. 
[10] K. S. Narendra and K. Parthasarathy, “Identification and control of 
dynamical systems using neural networks,” IEEE Trans. on Neural 
Networks, vol. 1, no. 1, pp. 4-27, 1990. 
[11] L. X. Wang and J. M. Mendel, “Fuzzy basis functions, universal 
approximation, and orthogonal least-squares learning,” IEEE Trans. 
on Neural Networks, vol. 3, no. 5, pp. 807-814, 1992. 
[12] P. Werbos, Beyond Regression: New Tools for Prediction and Analysis 
in the Behavior Sciences, Ph.D. dissertation, Harvard Univ., 
Cambridge, MA, Aug. 1974. 
[13] X. He and H. Asada, “A new method for identifying orders of 
input-output models for nonlinear dynamic systems,” Proc. of 
Automatic Control Conference, pp. 2520-2523, 1993. 
 
2111
controller design problems. Here, we regard these two tasks as 
an integral and take full advantage of the procedures 
conducted for system modeling to design an effective linear 
controller. 
The rest of this paper is organized as follows. In Section II, 
we shall introduce the proposed recurrent neural network 
architecture with its primitive ideas. A learning algorithm for 
establishing an efficient nonlinearity eliminator is presented in 
Section III. Section IV provides the computer simulations of 
nonlinear dynamic applications to validate the effectiveness of 
the proposed FARNN. Finally, conclusions are summarized in 
the last section. 
II. A NOVEL HYBRID CONTROL SCHEME USING A FULLY 
AUTOMATED RECURRENT NEURAL NETWORK  
Here, we propose a novel recurrent-network based control 
scheme that unifies a state-space based recurrent network with 
a fully automatic construction algorithm. Our approach 
systematizes the designs of all necessary control components 
without a trial-and-error approach or user manipulations. We 
named our proposed network with the learning algorithm as a 
fully automated recurrent neural network (FARNN). To 
explain our philosophy in developing such a control scheme, 
we first introduce the structure of our state-space based 
recurrent neural network and then present the overall control 
scheme and the design of its required components.         
A. Structure of Fully Automated Recurrent Neural Network 
(FARNN) 
The model identification of unknown systems (plants) is an 
important and integral part of control design methodology. 
This is because from the model we can gain a physical 
understanding of the system and develop a simulation from 
which a control law is designed [9]. To obtain good model 
identification for the system requires the following 
considerations: 1) the selection of models for describing the 
system, 2) the construction of quality models to best fit the 
system, and 3) the accuracy of models in representing the 
system. These issues should be integral parts for the 
development of effective system identification tools. With the 
considerations of the above issues, our objective is to devise a 
fully automated recurrent neural network (FARNN) that is 
capable of precisely capturing the dynamics of the true system 
with a transparent and concise network representation. 
The novelty of this network is to incorporate dynamic 
elements in such a way that each state corresponds to a neuron 
that linearly combines its own output history and the outputs 
from other neurons. Such a deployment of dynamic elements 
enables the proposed structure to be mapped into a state-space 
equation from its internal structure. Fig. 1 (a) shows the 
structure of the proposed recurrent network. The structure 
consists of four layers, which can be expressed by the block 
diagram shown in Fig. 1 (b). The input layer is only 
responsible for the transmission of input values to the network. 
The neurons in the hidden layer will integrate all input 
variables and perform a nonlinear transformation via nonlinear 
activation functions. In the dynamic layer, the neurons 
develop internal dynamics by integrating their self-feedback 
and weighted connections with the feedback connections from 
other neurons. That is, the dynamic layer integrates the current 
input information from the second layer with the state history 
(stored in the memory device) of the neurons in the dynamic 
layer to infer the current states x(k) of the network. Finally, the 
outputs of the network are obtained by the linear combination 
of the states and their associated parameters. 
The whole network can be classified into two major 
components: a static nonlinear model and a dynamic linear 
model. The static nonlinear model maps the input space into a 
state space via a nonlinear transformation, and then the state 
space is transformed into the output space through a linear 
dynamic mapping. The state space equations of the proposed 
network is expresses as  
 
( 1) ( ) ( ( )),
     ( ) ( ),
k k k
k k
+ = +
=
x Ax BN u
y Cx
 (1) 
 
where A∈\J×J, B∈\J×J, C∈\m×J, N∈\J, u = [u1 ,…, up]T is the 
input vector, y = [y1 ,…, ym]T is the output vector, and p and m 
are the dimensions of the input and output layers, respectively. 
The elements of matrix A stand for the degree of 
inter-correlation among the states. B is a diagonal matrix with 
diagonal elements [b11, …, bJJ]T, representing the weights of 
the inputs of the dynamic layer. The elements of matrix C are 
the weights of the states. N = [n1 ,…, nJ]T is the nonlinear 
function vector. x = [x1 ,…, xJ]T is the state vector where J is 
the total number of state variables and is equal to the number 
of neurons in the hidden layer and the dynamic layer. The 
current ith output yi(k) and the state variables x(k) are obtained 
by calculating the activities of all nodes on each layer and the 
corresponding functions are summarized as follows:  
 
b2
w1p
w21 a21
bJ
a11
wJ1
a12
b1
wJp
w2p
w11
cm2
c12
cm1
cmJ
c1J
c11
a2J
aJ1
aJJ
a1J
a22
dJ
f(Æ)
aJ2
up(k)
f(Æ)
f(Æ)
d2
d1
A
xJ(k)
ym(k)
x2(k)
x1(k)
y1(k)u1(k)
N(u(k)) y(k)u(k) x(k) CB
Static nonlinear model Dynamic linear model
 
 
Fig. 1.  (a) The topology of the proposed recurrent neural network. (b) The 
network block diagram. 
 
5282
phase, the NLE is continuously adapted to minimize the 
difference between the actual output of unknown system and 
the desired trajectory. That is, the robustness of the control 
scheme can be improved via the online parameter tuning of the 
NLE.   
III. SYSTEM MODELING AND CONTROLLER DESIGN 
The proposed hybrid control scheme requires three design 
procedures: 1) the model identification of the unknown system 
using the FARNN, 2) the construction of a nonlinearity 
eliminator (NLE) for removing the unknown system 
nonlinearity, and 3) the design of linear controller based on the 
approximated dynamic linear model of the FARNN. We first 
introduce the proposed fully automated construction algorithm 
that enables the construction of the FARNN in a fully 
automatic manner for effectively modeling the dynamic 
behaviors of the unknown system. With the identified FARNN, 
we demonstrate the detailed derivation of the learning 
algorithm for the nonlinearity eliminator to remove the 
inherent nonlinearity of the unknown system. Finally, based 
on the linear model reference, we provide several approaches 
for conventional feedback linear controller design to control 
the compound system, the NLE cascaded with the unknown 
system, to follow desired input signals.  
A. Fully Automated Construction Algorithm 
To perform system identification robustly, reliably, and 
automatically, without human’s manipulation if possible, we 
have developed a fully automated construction algorithm 
(FACA) to systematically construct a compact but efficient 
network. The proposed FACA consists of two learning 
phases—a structure learning and a parameter learning. The 
structure learning phase is operated offline, and the parameter 
learning phase online.  
The objective of the structure learning phase is to identify a 
minimal network size with an optimal parameter initialization. 
First, we utilize an order determination technique based on 
Lipschitz quotients to identify a minimal set of state variables. 
By the minimal set, we mean that the set is the same as or close 
to the true one of the unknown dynamic system. Upon the 
completion of the order determination, we are able to construct 
an initial network based on the number of the state variables 
(or the system order). A hybrid parameter initialization is 
subsequently performed to initialize the parameters of a 
nonlinear static model and a linear dynamic model. After the 
structure learning phase, we continuously fine-tune the 
parameters of the network to closely capture the dynamic 
behavior of the unknown system via a recursive learning 
algorithm. The detailed information and derivation for the 
proposed FACA can be found in our previous work [14]. 
B. Learning Algorithm of Nonlinearity Eliminator  
In our hybrid control scheme, the NLE plays a role to 
remove the nonlinear behavior of the unknown system so that 
the compound model, the system with the trained NLE, 
behaves like the dynamic linear model approximated from the 
FARNN. Based on the dynamic linear model, the design task 
of a feedback controller for the compound model becomes 
simple since the well-developed linear feedback control theory 
can be directly applied for such a task. From Fig. 1 (b), the 
FARNN is a nonlinear dynamic model consisting of a static 
nonlinear model cascaded with a dynamic linear model. In 
establishing a dynamic linear model as a reference model, we 
can utilize the information of the dynamic linear model of the 
FARNN. If we compare (1) with the following general linear 
state-space model, 
 
ˆ ˆ( 1) ( ) ( ),
ˆ ˆ     ( ) ( ),
k k k
k k
x Ax B u
y Cx
′+ = +
=   (6) 
 
the difference between them is the input transformation. 
Obviously, the transformation in (1) is nonlinear while in (6) 
linear. Hence, to best approximate the FARNN as in (1) with 
(6), we can solve the following least-squares problem:  
 
arg min ( ) .′ ′= −B BN u B u  (7) 
 
With the above linear reference model, we now want to 
construct the NLE that removing the nonlinear behavior of the 
unknown system using the MRAC scheme as shown in Fig. 4. 
The NLE is a three-layered neural network whose numbers of 
the input and output neurons are the same as that of the inputs 
of the unknown system and the dimension of the hidden layer 
is the same as the estimated system order. The activation 
functions are selected as the hyperbolic tangent sigmoid 
transfer functions. The functions of the NLE are summarized 
in the followings: 
 
(2) (2) (2)
1
,
p
s sr r s
r
f w v d
=
= +∑  (8) 
os
(2) = exp( fs
(2) ) − exp(− fs(2) )
exp( fs
(2) ) + exp(− fs(2) )
,  (9) 
(3) (3) (2) (3) (3)
1
,
J
q qs s q q
s
f w o d o
=
= + =∑  (10) 
 
where (2)srw  is the weight between the rth input neuron and the 
sth hidden neuron and (2)sd  is the bias of the sth hidden neuron; 
(3)
qsw  is the weight between the sth hidden neuron and the qth 
output neuron and (3)qd  is the bias of the qth output neuron. 
Likewise, the initial values of the weights and biases can be 
determined by the initialization method of the proposed 
FACA.  
Based on the error between the linear reference model and 
the unknown plant with the NLE, we can define the error 
function with respected to the adjustable parameters (w) as: 
 
2 21 1ˆ( , ) ( ( ) ( )) ( ),2 2l p lE k y k y k e k= − =w  (11) 
5284
u x= +N N KN  can be determined by the following equation: 
 
,J J x J m
m p u m m
× ×
× ×
′−⎡ ⎤ ⎡ ⎤ ⎡ ⎤=⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦⎣ ⎦
A I B N 0
C 0 N I
 (25) 
 
where Nx∈\J×m, and Nu∈\p×m. The more detailed information 
can be found in [3]. 
2) Optimal control: The optimal control problem can be 
stated as the need to determine the control u(k) which 
minimize the cost function 
 
1
1 ( ( ) ( )) ( ( ) ( ))
2
1   [( ( ) ( )) ( ( ) ( )) ( ) ( )]
2
i
N
k i
J N N N N
N N N N k k
−
=
= − −
+ − − +∑
T
T T
y r P y r
y r Q y r u Ru
(26) 
 
where P ≥ 0, Q ≥ 0, R > 0 , y(N) is the desired output signal, 
r(N) is the reference signal, and N is the control time. The 
following equations summarize the optimal tracking control 
algorithm (please refer to [8] for the detail.): 
 
( )N = TS C PC , ( ) ( )N N= Tv C Pr  (27) 
1( 1) ( ( 1) ) ( 1)x k k k
−′ ′ ′+ = + + +T TK B S B R B S A  (28) 
( ) ( 1)( ( ))xk k k′= + +T TS A S A - B K C QC  (29) 
( ) ( ( )) ( 1) ( )xk k k k′= + +T Tv A - B K v C Qr  (30) 
1( ) ( ( 1) )v k k
−′ ′ ′= + +T TK B S B R B  (31) 
ˆ( ) ( ) ( ) ( ) ( 1),x vk k k k k= − + +u K x K v  (32) 
 
where S ∈ \J×J, v ∈ \J, Kx ∈ \p×J, Kv ∈ \p×J, ˆ ( ) .Jkx ∈\  S(N) 
and v(N) are the boundary conditions, Kx(k) is a feedback gain, 
and Kv(k) is a feedforward gain. 
During the control stage (as shown in Fig. 5), the pole 
placement and optimal control methods require the actual 
state-variables feedback. In fact, the state-variables of 
unknown systems are unavailable. Here, we use the 
state-variables of the FARNN, which has been trained to 
approximate the unknown system dynamics, as the actual 
state-variables and feed them to the controllers. 
 
IV. SIMULATION RESULTS 
Extensive computer simulations have been performed to 
validate the performance of the proposed hybrid control 
scheme for the applications of unknown system control. Due 
to the limited space, we only present an example of a 
multiple-input-multiple-output (MIMO) system control. 
Example: MIMO dynamic plant control. The MIMO plant 
with two inputs and two outputs is the same as that used in [11]. 
The plant is specified by  
 
1
1 12
2
1 2
2 22
2
( )
( 1) 0.5 ( ) ,
1 ( )
( ) ( )
( 1) 0.5 ( ) .
1 ( )
p
p
p
p p
p
p
y k
y k u k
y k
y k y k
y k u k
y k
⎡ ⎤+ = +⎢ ⎥+⎢ ⎥⎣ ⎦
⎡ ⎤+ = +⎢ ⎥+⎢ ⎥⎣ ⎦
 (33) 
 
The tracking model is a second-order linear system given by 
 
( 1) 0.6 ( ) 0.2 ( 1) 0.1 ( ),m m mk k k k+ = + − +y y y r   (34) 
 
where r(k) is a bounded reference input vector. This objective 
of the control problem is to determine bounded control force 
u(k) such that limk→∞ ec(k) = ym(k) – yp(k) = 0. Note that in our 
simulations, the plant model was only used for the generation 
of input-output data. None of the plant information is used in 
the modeling or control procedures.  
We first perform the system modeling of the above 
dynamical plant using the FARNN with the input-output 
measurements. The modeling procedure consists of two 
phases: the structure learning phase and the parameter learning 
phase, as shown in Fig. 6. The FARNN training procedure is 
TABLE I 
THE IDENTIFICATION PERFORMANCE COMPARISONS OF THE FARNN WITH TWO 
EXISTING RECURRENT NETWORKS 
Network Type No. of Parameters
Training Time 
(time steps) MSE 
FARNN 16 11,000 yp1=9.5891×10
-4
yp2= 4.7872×10-3
RSONFIN [5] 77 11,000 yp1=1.24×10
-2 
yp2=1.97×10-2 
MNN [13] 131 77,000 yp1=1.86×10
-2 
yp2=3.27×10-2 
0 100 200 300 400 500 600 700 800 900 1000
5
0
0.5
1
Time Steps
0 100 200 300 400 500 600 700 800 900 1000
0.5
0
0.5
1
Time Steps
Ou
tp
ut
 1
Ou
tp
ut
 2
 
Fig. 7. The actual outputs of the MIMO plant (solid curves) and the FARNN 
(dotted curves). 
FARNN
u(k)
(a) (b)
e(k)
-
+
yp(k) Unknown 
Plant
u(k) yp(k)
FARNN
Unknown 
Plant
 
Fig. 6. System modeling of unknown plants using the FARNN. (a) The structure 
learning phase. (b) The parameter learning phase. 
  
5286
the procedures conducted for system modeling to design an 
effective linear controller. The design of the proposed hybrid 
control scheme is based on the structure of the FARNN that 
models an unknown system with a static nonlinear model and 
dynamic linear model in a state-space representation.  A fully 
automated construction algorithm has been devised to 
construct a parsimonious FARNN with accurate identification 
capability for unknown system modeling. Based on this 
identified FARNN, the design of the NLE and the linear 
feedback controllers was performed systematically and 
effortlessly. Unlike a traditional linearization method that 
linearizes unknown systems with respect to operating points 
and applies the well-developed linear control theory to design 
controllers for unknown system, our linear reference model is 
obtained by removing the global nonlinear behavior of the 
unknown system using the NLE. If the system modeling and 
the NLE are perfect, the compound model, the unknown 
system cascaded with the inverse model, will behave like a 
pure linear model. Hence, the feedback linear controller 
designed based on the liner model can perform well in a wide 
operating range. The remarkable results on unknown system 
control problems have confirmed the effectiveness of the 
proposed hybrid control scheme with superior performance.  
REFERENCES 
[1] S. P. Bhattacharyya, H. Chapellat, and L. H. Keel, Robust Control, the 
Parametric Approach, Upper Saddle River, New Jersey: Prentice-Hall, 
1995. 
[2] T. W. S. Chow and Y. Fang, “A recurrent neural-network-based 
real-time learning control strategy applying to nonlinear systems with 
unknown dynamics,” IEEE Trans. Industrial Electronics, vol. 45, no. 1, 
pp. 151-161, 1998. 
[3] G. F. Franklin, J. D. Powell and M. L. Workman, Digital Control of 
Dynamic Systems, 3rd ed .   Reading, MA: Addison-Wesley, 1997.  
[4] J. Q. Huang and F. L. Lewis, “Neural-network predictive control for 
nonlinear dynamic systems with time-delay,” IEEE Trans. Neural 
Networks, vol. 14, no. 2, pp. 377-388, 2003. 
[5] C. F. Juang and C. T. Lin, “A recurrent self-organizing neural fuzzy 
inference network,” IEEE Trans. Neural Networks, vol. 10, no. 4, pp. 
828-845, 1999.  
[6] C. C. Ku and K. L. Lee, “Diagonal recurrent neural networks for 
dynamic systems control,” IEEE Trans. Neural Networks, vol. 6, no. 1, 
pp. 144-156, 1995.  
[7] H. K. Khalil, Nonlinear Systems, Upper Saddle River, New Jersey: 
Prentice-Hall, 1996. 
[8] F. L. Lewis and V.L. Syrmos, Optimal Control, John Wiley, 2nd, 
Edition, New York, 1995.  
[9] G. Lightbody and G. W. Irwin, “Nonlinear control structures based on 
embedded neural system models,” IEEE Trans. Neural Networks, vol. 
8, no. 3, pp. 553-567, 1997.  
[10] Kuljaca, N. Swarny, F. L. Lewis, and C. M. Kwan, “Design and 
implementation of industrial neural network controller using 
backstepping,” IEEE Trans. Industrial Electronics, vol. 50, no. 1, pp. 
193-201, 2003. 
[11] K. S. Narendra and K. Parthasarathy, “Identification and contrl of 
dynamical systems using neural network,” IEEE Trans. Neural 
Networks, vol. 1, no. 1, pp. 4-27, 1990.  
[12] K. S. Narendra and A. M. Annaswamy, Stable Adaptive Systems, 
Englewood Cliffs, New Jersey: Prentice-Hall, 1989. 
[13] P. S. Sastry, G. Santharam and K. P. Unnikrishnan, “Memory neuron 
networks for identification and control of dynamical systems,” IEEE 
Trans. Neural Networks, vol. 5, no. 2, pp. 306-319, 1994.  
[14] J. S. Wang and Y. P. Chen, “A Fully Automated Recurrent Neural 
Network for Unknown Dynamic System Identification and Control,” 
accepted by IEEE Trans. Circuits and System-I, November, 2005.  
[15] R. J. Williams and D. Zipser, “A learning algorithm for continually 
running fully recurrent neural networks,” Neural Computation, vol. 1, 
pp. 270-280, 1989. 
[16] Q. Zho and L. Guo, “Stable adaptive neurocontrol for nonlinear 
discrete-time systems,” IEEE Trans. Neural Networks, vol. 15, no. 3, 
pp. 653-622, 2004. 
 
5288
 an outstanding method or algorithm for modeling a RNN. 
One of the problems is the lack of a generic architecture. 
According to Tsoi et al. [11], there are altogether 54 
possible combinations of RNN architectures, not counting 
dynamic feedforward connections (a finite impulse 
response (FIR) filter, an infinite impulse response (IIR) or 
a lattice filter), can be derived from a three-layered neural 
network based on their definitions of network components. 
The proper architecture to use is problem dependent. In 
general, one needs to experiment on a number of 
architectures, before “homing” in a suitable one. 
 The Williams-Zipser recurrent architecture [12], 
allowing all nodes in the networks to be fully connected to 
all other nodes, typically suffers from a stability problem 
and slow convergence. The Elman recurrent network [5] 
feeds back the output signals from each hidden node to all 
hidden nodes, and treats the feedback connections as unity 
weights to exempt from the stability problem for training 
feedback weights. Other recurrent architectures, such as the 
Frasconi-Gori-Soda locally recurrent networks [6] and the 
Back-Tsoi recurrent network with FIR and IIR synapses 
[11], have been proposed to incorporate recurrence via 
feedback or feedforward connections. 
u1(k)
a11
a12a1J
aJ2 aJJ
aJ1
a2J
a22a21
x1(k)
x2(k)
xJ(k)
b1
b2
bJ
nJ(u(k))
(a)
(b) N(u(k)) +
A
u(k) x(k)Z-1
Z-1
Z-1
Z-1up(k)
B
y1(k)
ym(k)
C y(k)
u2(k)
w11
w12 w1p
w21
wj1
w22
wj2
w2p
wjp
f( ).
f( ).
f( ).
c11
c12
c1j
cm1
cm2
cmj
Input Layer Hidden Layer Dynamic Layer Output Layer   State
Variables
d2
d1
dj
 
Fig. 1. (a) The proposed state-space-based recurrent neural 
network topology. (b) The block diagram of the SCRNN. 
 To provide a solution for the aforementioned 
problems, we have focused on the development of a 
systematic framework to simultaneously solve the problem 
of lack of a generic structure as well as the problem of the 
parsimonious structure. We first integrated a neural 
framework and the concept of states in linear systems to 
develop a novel recurrent neural network architecture that 
incorporates dynamic elements in such a way that each 
state corresponds to a neuron that linearly combines its 
own output history and those from other neurons. A self-
constructing algorithm has been developed to identify the 
minimal realization of the proposed self-constructing 
recurrent neural network (SCRNN) and will be discussed 
in Section III. 
 Figure 1(a) shows the proposed SCRNN structure. 
The structure consists of four layers which can be 
expressed by the block diagram shown in Fig. 1 (b). The 
input layer is only responsible for the transmission of input 
values to the network. The neurons in the hidden layer will 
integrate all input variables and perform a nonlinear 
transformation via sigmoid functions or other nonlinear 
functions. The input layer and hidden layer containing the 
complexity of nonlinearity represent the static nonlinear 
block of a Hammerstein system. In the dynamic layer, the 
neurons develop internal dynamics by their self-feedback 
and weighted connections with other feedback connections 
from other neurons. That is, the dynamic layer integrates 
the current input information from the hidden layer and the 
output history (stored in the memory device) of the neurons 
in the dynamic layer to infer the current states x(k) of the 
network. The outputs of the network are obtained by the 
linear combination of the states and their associated 
parameters. The dynamic layer and output layer involving 
the feature of dynamics essentially constitute the linear 
dynamic part of a Hammerstein system. 
 The state space equations of the proposed SCRNN can 
be written as  
 
 x(k+1) = A x(k) + BN(u(k)), 
 y(k) = Cx(k). (2) 
 
where u = [u1 ,…, up ]T , u is the input vector, and y = [y1 
,…, ym ]T , y is the output vector, p and m are the 
dimensions of the input and output layer, respectively; x 
and N J∈R , x = [x1 ,…, xj ]T is the state vector, J is the total 
number of state variables and is equal to the number of 
neurons in the hidden layer and the dynamic layer. A, 
B J J×∈R and C m J×∈R . The elements of matrix A stand for 
the degree of inter-correlation among states. B is a diagonal 
matrix with diagonal elements [b11, …, bjj]T representing 
the weights of the inputs of the dynamic, and the element of 
matrix C are the weights of the states. N = [n1 ,…, nj]T is 
the nonlinear function vector. The input variable and 
resultant value of each function is obtained by Eq. (3) and 
Eq. (4), respectively.  
 
 
1
( ) + 
p
j ji i j
i
p w u d
=
= ∑ ; (3) 
 ( ) ( )
j j
j j
p p
j j p p
e en u f p
e e
−
−
−= = + , (4) 
 
where wji is the weight between the ith input neuron and 
the jth hidden neuron; dj is the jth bias value in its hidden 
neuron. Here, a hyperbolic tangent sigmoid transform 
function is employed as the activitation function of neurons 
  
where the denominator can be interpreted as the length of 
the j-th neuron’s weight vector. Since the weights wji are 
i.i.d. uniform random variables within [−wmax, wmax], the 
length can be expressed as  
 
 
2
2 2
1
= ( ) ,
3
p
max
ji
i
pw
w p E
=
× =∑ w   (7) 
 
where E(w2) is the variance of the weights that equals to 
2 / 3maxw  as the weights is assumed to be uniformly 
distributed with the mean equal to zero. Now, in order for 
the neurons to operate within the boundaries for all input 
data, the maximum possible Euclidean distance between 
input data points, denoted as Dmax, must be within the 
boundaries. Namely, the distance Din should be less than or 
equal to the distance of the boundaries, i.e. hbound. First, we 
define two input vectors umax and umin as the upper and 
lower bounds of the input data.  
 
 
1 11 1
1 11 1
, , max( ), , max( ) ;
, , min ( ), , min ( ) ,
TTmax max max
p i ipi N i N
TTmin min min
p i ipi N i N
u u u u
u u u u
≤ ≤ ≤ ≤
≤ ≤ ≤ ≤
⎡ ⎤⎡ ⎤= =⎣ ⎦ ⎢ ⎥⎣ ⎦
⎡ ⎤⎡ ⎤= =⎣ ⎦ ⎢ ⎥⎣ ⎦
u
u
" "
" "
 (8) 
 
where N is the total number of the input data and p 
represents the number of input variables. Then Dmax can be 
expressed as  
 
 ( )2
1
.
p
max max min
j j
j
D u u
=
= −∑   (9) 
 
From Eqs. (6), (7) and (9), we conclude that the maximum 
value of the weights between the input and hidden layers is: 
 
 4.356 3 .max maxw pD
=   (10) 
 
By initializing the bias dj, we want to align the 
hyperplane H(0), the center of the upper and lower 
boundaries, with the center of Dmax. The reason for doing 
this is that we can fully utilize the dynamic range of the 
active region. The center of Dmax in the hyperplane, denoted 
as Cbound, is given by  
 
 ( ) 11 , , ,2 Tbound max min bound boundpC c c⎡ ⎤= − = ⎣ ⎦u u …   (11) 
 
and the bias dj is given by  
 
 
1
.
p
bound
j i ji
i
d c w
=
= −∑  (12) 
 
The parameters of the output matrix C are again 
assumed to be i.i.d. uniform random variables within [−cmax, 
cmax], and the value of cmax is evaluated in a similar fashion.  
 
 3.772 ,maxc J
=   (13) 
 
where J is the total number of the hidden nodes. 
 Based on the above derivation, the link weights of the 
hidden layer, the biases of the neurons, and the parameters 
of C can be initialized by Eq. (10), Eq. (12), and Eq. (13) 
respectively. After the initialization of the nonlinear static 
part is completed, we use this part to generate the input 
training data for the linear dynamic part. 
3.1.2 Weight initialization of linear dynamic 
sybsystem 
 After the initial parameters of the static nonlinear part 
have been assigned, each piece of the input data is used to 
evaluate the nonlinear function vector via Eq. (4). This 
vector is then used as the input variables for the linear 
system part of the proposed network, so that the linear 
system part can be interpreted as a state space equation 
with multi-variables. We can use the identification 
technique for multivariable system to deal with the 
initialization problem of the linear system part. The 
multivariable state-space model can be identified directly 
from input-output data by black-box model methods such 
as the prediction error method (PEM) [9]. The PEM is a 
powerful tool to provide accurate parameter estimate. 
Hence, we adopted the PEM for identifying the remained 
non-initialized matrix A. In this study, the initial matrix A 
is set as a diagonal matrix, and the elements of A are then 
estimated by the PEM from the System Identification 
Toolbox of MATLAB®. 
 After the assignment of the initial parameters via the 
hybrid initialization method, we can further fine-tune these 
parameters to closely capture the system dynamics. Next, a 
recursive recurrent learning algorithm is introduced in the 
following. 
3.2 Recursive recurrent learning algorithm 
   The parameter learning problem is to adjust the 
feedforward and feedback weights of the SCRNN so that 
the desired dynamic trajectories can be “caught” or 
“memorized” by the internal weights and recurrence. 
Because of the fact that a weight adjustment affects the 
states at all times during the course of network state 
evolution, obtaining the error gradient information is a 
rather complicated procedure. In the parameter learning 
phase, an online recursive learning algorithm based on the 
ordered derivative associated with momentum terms has 
been derived for this supervised learning. The associated 
 -0.32893 -0.038374 0.02713
( 1) -0.028909 -0.1998 0.052587 ( )            
0.010544 -0.024554 -0.10463
k k
⎡ ⎤⎢ ⎥+ = ⎢ ⎥⎢ ⎥⎣ ⎦
x x
  
[ ]
1.0191 0 0
               + 0 1.1719 0 ( ( ));
0 0 1.1847
( )  -1.2785 -0.93796 0.85516 ( ),
k
y k k
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
=
N u
x
 (23) 
 
where x(k) = [x1(k) , x2(k), x3(k)]T are the state variables of 
the SCRNN. The eigenvalues of matrix A are (−0.3379, 
−0.1746, −0.1208). The result is shown in Fig. 4, where the 
MSE is 4.093×10-5. 
Example 2: The Henon mapping system [8] is give as  
 
 x(k + 1) = −αx(k) + βx(k − 1) + 1, (24) 
 
where α = 1.4 and β = 0.3. The discrete-time Hennon 
system is the typical case of chaotic systems and is not 
simple since it is a second-order dynamic system with one 
delay and two sensitive parameters [1]. To train the SISO 
SCRNN, the initial point [0.1, 0.6]T is used to generate the 
1000 time steps as the training data. We set the system 
order of the SCRNN be 5 and set the learning rates ξac = 
0.0006 and ξ = 0.006. The number of the learning epochs is 
100. The testing data with 1000 time steps is generated by 
the initial point [0.4, 0.4]T to validate the trained network. 
The result is shown in Fig. 5, where the MSE is 2.245×10-5. 
-2 -1 0 1 2
-1.5
-1
-0.5
0
0.5
1
1.5
x(k)
(a)
x(
k-
1)
-2 -1 0 1 2
-1.5
-1
-0.5
0
0.5
1
1.5
x(k)
(b)
x(
k-
1)
 
Fig. 5. (a) Testing data of this chaotic system. (b) 
Identification result using the SCRNN for this chaotic 
system. 
5 Conclusions 
 A self-constructing recurrent neural network for 
identifying chaotic systems has been proposed in this paper. 
We have fused the concept of states in the theory of linear 
systems to develop a novel recurrent neural network. The 
proposed self-constructing algorithm consists of two 
mechanisms, a hybrid weight initialization method and a 
parameter learning method to exempt trial and error from 
structure initialization and parameterization as well. The 
results of computer simulations have validated the 
effectiveness of the proposed SCRNN in terms of accuracy 
and compact structures. 
References 
[1] G. Chen, Y. Chen and H. Ogmen, “Identifying 
chaotic systems via a wiener-type cascade model,” 
IEEE Trans. Control Systems, pp. 29-36, October, 
1997. 
[2] C. L. P. Chen and J. Z. Wan, “A rapid learning and 
dynamic stepwise updating algorithm for flat neural 
networks and the application to time-series 
prediction,” IEEE Trans. Systems, Man, and 
Cybernetics—Part B: Cybernetics, vol. 29, no. 1, 
February, 1999. 
[3] Y. P. Chen and J. S. Wang, “A novel recurrent neural 
network with minimal representation for dynamic 
system identification,” Proc. of 2004 IEEE 
International Joint Conference on Neural Networks, 
Budapest, Hungary, pp. 849-854, July 25-29, 2004. 
[4] Y. P. Chen and J. S. Wang, “An automated 
Hammerstein recurrent neural network for dynamic 
applications,” Proc. of IASTED International 
Conference on Computational Intelligence, Calgary, 
Canada, pp. 193-198, July 4-6, 2005. 
[5] J. L. Elman, “Finding structure in Time,” Cognitive 
Science, vol. 14, pp. 179-211, 1990. 
[6] P. Frasconi and M. Gori, and G. Soda, “Local 
feedback multilayerednetworks,” Neural Comput., 
vol. 4, no. 1, pp. 120-130, 1992. 
[7] Min Han, J. Xi, S. Xu and F. L. Yin, “Prediction of 
chaotic time series based on the recurrent predictor 
neural network,” IEEE Trans. Signal Processing, vol. 
52, no. 12, pp. 3409-3416, December, 2004. 
[8] C.-H. Lee and C.-C. Teng, “Identification and control 
of dynamic systems using recurrent fuzzy neural 
networks," IEEE Trans. Fuzzy Systems, vol. 8, no. 4, 
pp. 349-366, 2000. 
[9] L. Ljung, System Identification: Theory for the User, 
Prentice-Hall, January, 1999. 
[10] K. S. Narendra and P. G. Gallman, “An Iterative 
method for the identification of nonlinear system 
using a Hammerstein model,” IEEE Trans. Automatic 
Control, vol. 11, pp. 546-550, 1966. 
[11] A. C. Tsoi and A. D. Back, “Locally recurrent 
globally feedforward networks: A critical review of 
architectures,” IEEE Trans. Neural Networks, vol. 5, 
no. 2, pp. 229-239, 1994. 
[12] R. J. Williams and D. Zipser, “A learning algorithm 
for continually running fully recurrent neural 
networks," Neural Comput., vol. 1, no. 2, pp. 270-
280, 1989. 
[13] Y. F. Yam and T. W. S. Chow, “Feedforward 
networks training speed enhancement by optimal 
initialization of the synaptic coefficients,” IEEE 
Trans. Neural Networks, vol.2, no. 2, pp. 430-434, 
2001. 
better representing sharp corners and piecewise linear 
shapes. Recently, neural-network approaches have been 
proposed for the identification of Hammerstein systems [4]. 
Janczak [5] and Al-Duwaish [6] et al. proposed their 
neural-network-based Hammerstein models for dynamic 
system identification. A common feature among these 
approaches is the lack of automated modeling procedure 
that is capable of identifying minimal representations with 
satisfactory performance.  
To provide a solution for the above problem, we first 
integrated a neural framework and the concept of states in 
linear systems to develop a novel recurrent neural network  
u1(k)
a11
a12a1J
aJ2 aJJ
aJ1
a2J
a22a21
x1(k)
x2(k)
xJ(k)
b1
b2
bJ
nJ(u(k))
(a)
(b) N(u(k)) +
A
u(k) x(k)Z-1
Z-1
Z-1
Z-1up(k)
B
y1(k)
ym(k)
C y(k)
u2(k)
w11
w12 w1p
w21
wj1
w22
wj2
w2p
wjp
f( ).
f( ).
f( ).
c11
c12
c1j
cm1
cm2
cmj
Input Layer Hidden Layer Dynamic Layer Output Layer   State
Variables
d2
d1
dj
 
Fig. 2: (a) The proposed Hammerstein recurrent neural network (HRNN) 
topology. (b) The block diagram of the HRNN. 
 
that incorporates dynamic elements in such a way that each 
state corresponds to a neuron that linearly combines its own 
output history and those from other neurons. Figure 2(a) 
shows the proposed HRNN structure. The structure consists 
of four layers which can be expressed by the block diagram 
shown in Fig. 2 (b). The input layer is only responsible for 
the transmission of input values to the network. The 
neurons in the hidden layer integrate all input variables and 
perform a nonlinear transformation via sigmoid functions or 
other nonlinear functions. The input layer and hidden layer 
containing the complexity of nonlinearity represent the 
static nonlinear block of a Hammerstein system. In the 
dynamic layer, the neurons develop internal dynamics by 
their self-feedback and weighted connections with other 
feedback connections from other neurons. That is, the 
dynamic layer integrates the current input information from 
the hidden layer and the output history (stored in the 
memory device) of the neurons in the dynamic layer to infer 
the current states x(k) of the network. The outputs of the 
network are obtained by the linear combination of the states 
and their associated parameters. The dynamic layer and 
output layer involving the feature of dynamics essentially 
constitute the linear dynamic part of a Hammerstein model. 
The state space equations of the proposed HRNN can be 
written as  
x(k+1) = A x(k) + BN(u(k)), 
y(k) = Cx(k).                (1) 
where u = [u1 ,…, up ]T , u is the input vector, and y = [y1 
,…, ym ]T, y is the output vector, p and m are the dimension 
of the input and output layer, respectively; x and N J∈R , x 
= [x1 ,…, xj ]T is the state vector, J is the total number of 
state variables and is equal to the number of neurons in the 
hidden layer and the dynamic layer. A and B J J×∈R and 
C m J×∈R . The elements of matrix A stand for the degree of 
inter-correlation among states. B is a diagonal matrix with 
diagonal elements [b11, …, bjj]T representing the weights of 
the inputs of the dynamic, and the element of matrix C are 
the weights of the states. N = [n1 ,…, nj]T is the nonlinear 
function vector. The input variable and resultant value of 
each function is obtained by Eq. (2) and Eq. (3), 
respectively.  
                 
1
( ) + 
p
j ji i j
i
p w u d
=
= ∑ ,           (2) 
             ( ) ( )
j j
j j
p p
j j p p
e en u f p
e e
−
−
−= = +           (3) 
where wji is the weight between the ith input neuron and the 
jth hidden neuron; dj is the jth bias value in its hidden 
neuron. Here, a hyperbolic tangent sigmoid transform 
function is employed as the activitation function of neurons 
because of its capability of providing a dual polarity signal 
to the output. We have shown that state space equations can 
be directly extracted from the proposed network topology 
easily. The state-space equations obtained from the 
proposed recurrent neural networks can be utilized to check 
the learning convergence by the eigenvalues of the matrix A. 
Moreover, it enables further theoretical analysis using the 
well-developed theory of linear systems. 
To effectively identify a compact representation for the 
HRNN, a systematic self-construction learning algorithm is 
introduced to perform the structure identification and the 
parameter learning in the following section. 
 
3. Self-Construction Algorithm 
 
A principal requirement in many dynamic applications 
is that the algorithms for system identification should work 
robustly, reliably, and automatically, without human’s 
manipulation. This paper proposed a systematic 
self-construction algorithm consisting of two learning 
phases—a structure learning and a parameter learning. In 
the structure learning phase, we apply an order 
determination technique to identify the compact set of state 
variables. Once the state variables are determined, an initial 
HRNN structure will be established. To accelerate the 
convergence of parameter learning phase, we adopted a 
multidimensional geometrical approach for parameter 
initialization [9]. The parameter learning phase is proceeded 
subsequently to fine-tune the parameters for capturing the 
dynamic behavior of the unknown system by a recursive 
learning algorithm. 
 
 
the method effectively reduces the number of training 
iterations to achieve the error criterion. We modified their 
approach for hyperbolic tangent functions used in the static 
nonlinear system of the proposed network. 
Our objective is to use the multidimensional geometrical 
approach for the initialization of wji and dj in Eq. (2) such 
that the outputs of the subsystem are guaranteed in the 
active region of the activation functions. That is, the 
initialized parameters can prevent the network from getting 
stuck in the beginning of the training phase. The weights wji 
are supposed to be independent and identically distributed 
(i.i.d.) uniform random variables within the range [−wmax, 
wmax]. The active region means that where the derivative of 
the sigmoid function has a large value fall in. In general, if 
|pj| in Eq. (2) falls beyond the active region, the neuron 
enters the saturation region. In the saturation region, the 
change of weight is tiny due to the small derivative of the 
sigmoid function. In the tangent sigmoid case, the region 
where the derivative of the activation function is greater 
than one-twentieth of the maximum derivative is assumed 
to be the active region, i.e. the region that |pj| ≤ 2.178. 
Consider a hyperplane H(pj) in the input space  
   
1
( ) ( ) + ,
p
j ji i j j
i
H p w u d p
=
= −∑           (10) 
and the maximum possible distance Din between two points 
of the input space is represented as  
        [ ]2in
1
max( ) min( ) ,
p
i i
i
D
=
= −∑ u u            (11) 
where ui is the ith input set. The hyperplanes H(-2.178) and 
H(2.178) are the boundaries between the active and 
saturation regions. The distance between the boundaries 
should be greater than or equal to the distance Din, so that 
the outputs of the hidden neurons are within the active 
region. The distance din between the two hyperplanes 
H(-2.178) and H(2.178) is given by  
               in 2
1
4.356 .  
p
ji
i
d w
=
= ∑           (12)  
Eq. (13) is used to evaluate the length of the weight vector 
so that the distance Din and din are equal to ensure the 
outputs of the hidden neurons are within the active region. 
             2 2in
1
4.356= ( ),
p
ji
i
w p E
D=
= ×∑ w       (13) 
where E(w2) is the variance of the weights and the variance 
is equal to 2max / 3w  as the weights is assumed to be an 
uniform distribution. The maximum magnitude of the 
weights between the input and hidden layers is evaluated by 
the following equation: 
                 max in
4.356 3 .w
pD
=             (14) 
The derivative of the sigmoid function has the maximum 
value at the hyperplane H(0). H(0) should cross the center 
of the input space for entirely employing the dynamic range 
of the activation function. Hence, the center of the input 
space is given by  
in 1 1
in in
1
max( ) min( )max( ) min( )( , , )
2 2
     ( , , ) .
p p T
T
p
u uu uC
c c
++=
=
…
…
 (15) 
In order to move the hyperplane H(0) to cross Cin, the bias 
dj is given by  
                 in
1
.
P
j i ji
i
d c w
=
= −∑               (16) 
Based on the above derivation, the magnitude region of 
the weights of the proposed network between the input 
layer and the hidden layer can be evaluated by Eq. (14), and 
the bias of the hidden nodes can be evaluated by Eq. (16). 
  
3.2.2 Weight Initialization of Linear Dynamic Subsystem 
    
After the initial parameters of the static nonlinear part 
have been assigned, the input data are used to evaluate the 
nonlinear function vector via Eq. (3). The vector is turned 
into the input variable for the linear system part of the 
proposed network, so that the linear system part can be 
interpreted as a state space equation with multi-variables. 
We can use the identification technique for multivariable 
system to deal with the initialization problem of the linear 
system part. The identification of multivariable systems is 
important in practice. The multivariable state-space model 
can be identified directly from input-output data by 
black-box model methods such as the prediction error 
method (PEM) [10]. The PEM is a powerful tool to provide 
accurate parameter estimate. We adopted the PEM for 
identifying the initial linear dynamic model. A full 
parameterization of the sate-space model is used by the 
PEM [11]. It means that all elements in the state-space 
model are tunable parameters. In the PEM identification, a 
number of real-valued parameters are collected in a 
parameter vector θ. The search for the estimated model then 
becomes the problem of estimating or determining θ. 
Assume the input-output data is given by 
           { ( ), ( ) | 1,..., }.iN iZ y k k k N= =N        (17) 
The prediction error is the vector 
               ˆ( , ) ( ) ( , )k y k y kε θ θ= −           (18) 
with the predicted output ˆ( , )y k θ . The objective is to find 
the value θ which minimizes the criterion ( , )i
i
N
NV Zθ  
defined as  
2
1
1( , ) ( , ) ,
i
i
i
N
N
N
ki
V Z k
N
θ ε θ
=
= ∑        (19) 
where | ⋅ | is the Euclidian l2-norm. The estimate ˆ iNθ  is 
defined by minimization of Eq. (19): 
            ˆ ( ) argmin ( , ),i i
i i
N N
N NZ V Zθθ θ=        (20) 
where argmin means “the minimizing argument of the 
function.” The matrices A, B and C in Eq. (1) can be 
constructed from the parameter vector θ. The detail process 
of the PEM can be found in [10, 11]. One drawback of the 
PEM is that it needs a priori information on the model 
structure, i.e. the system order. However, since the 
information of the system order is obtained via the above 
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Orders
M
S
E
NARX
Elman
HRNN
 
Fig. 3. The performance comparison for networks with different system 
orders described in TABLE I. 
 
TABLE I 
 The numbers of parameters of the networks with different system orders 
No. of Parameters Order Elman NARX HRNN 
1 6 11 6 
2 13 19 14 
3 22 29 24 
4 33 41 36 
5 46 55 50 
6 61 71 66 
 
0 100 200 300 400 500 600 700 800 900 1000
-2
-1
0
1
2
Time Steps
O
ut
pu
t
 
Fig. 4. The outputs of the MISO plant (solid curve) and HRNN (dotted 
curve) for this example. 
 
The identification result of this example is as follows. 
The total number of HRNN parameters is 18, and the 
state-space equation extracted from the trained network are 
written as 
[ ]
0.4066 -0.0058 0.1895 0
( 1) ( ) ( ( ))       
-0.6711 0.2216 0 0.7287
( ) -4.2074 1.3621 ( ),                                            (25)
k k k
y k k
⎡ ⎤ ⎡ ⎤+ = +⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
=
x x N u
x
where x(k) = [x1(k), x2(k)]T are the state variables of the 
HRNN. The eigenvalues of matrix A are (0.4256, 0.2025) 
that indicate the learning is converged. Figure 4 depicts the 
outputs of the MISO plant and the trained HRNN for the 
test input signal. The HRNN can closely emulate the given 
plant.  
 
5. Conclusions 
 
This paper presents a automated Hammerstein recurrent 
neural network for dynamic applications. We have fused the 
concept of states in the theory of linear systems to develop a 
novel recurrent neural network so that the structure can be 
mapped into a state-space representation. The proposed 
self-construction algorithm consists of three mechanisms, 
an order determination scheme, a hybrid weight 
initialization method, and a parameter learning method for 
the minimal order identification and parameter optimization, 
respectively. The results of computer simulations have 
validated the satisfactory performance of the proposed 
HRNN in terms of accuracy and compact structures. 
 
6. Acknowledgements 
 
This work was supported by the National Science Council, 
Taiwan, R.O.C., under Grant NSC 94-2213-E-006-036.  
 
References 
 
[1] L. Ho and R. E. Kalman, “Effective construction of linear 
state-variable models from input-output functions,” Proc. 3rd Ann. 
Allerton Conf. Circuit and System Theory, pp. 449-459, 1965. 
[2] K. S. Narendra and P. G. Gallman, “An iterative method for the 
identification of nonlinear systems using a Hammerstein model,” 
IEEE Trans. on Automatic Control, vol. 11, pp. 546-550, 1966. 
[3] E. J. Dempsey, and D. T. Westwick, “Identification of Hammerstein 
models with cubic spline nonlinearities,” IEEE Trans. Biomed. 
Eng., vol. 51, no. 2, pp. 237-245, 2004. 
[4] A. Janczak, “Neural network approach for identification of 
Hammerstein systems,” International Journal of Control, vol. 76, 
no. 17, pp. 1749-1766, 2003. 
[5] A. Janczak, “Identification of a class of nonlinear systems using 
neural networks,” Proc. of the International Conference on MMAR, 
pp. 697-702, 1995. 
[6] H. Al-Duwaish, M. Nazuml Karim, and V. Chandrasekar, 
“Hammerstein model identification by multiplayer feedforward 
neural networks,” International Journal of Systems Science, pp. 
49-54, 1997. 
[7] T. Lin, B. G. Horne, C. L. Giles, and S. Y. Kung, “What to 
remember: How memory order affects the performance of NARX 
neural networks,” IEEE World Conference on Computational 
Intelligence, pp. 1051-1056, 1998. 
[8] X. He and H. Asada, “A new method for identifying orders of 
input-output models for nonlinear dynamic systems,” Proc. of the 
ACC, pp. 2520-2523, 1993. 
[9] Y. F. Yam and T. W. S. Chow, “Feedforward networks training 
speed enhancement by optimal initialization of the synaptic 
coefficients,” IEEE Trans. on Neural Networks, vol.2, no. 2, pp. 
430-434, 2001. 
[10] L. Ljung, System Identification: Theory for the User, Prentice-Hall, 
Jan. 1999. 
[11] T. McKelvey, Identification of State-Space Models from Time and. 
Frequency Data, Ph.D. thesis, Department of Electrical 
Engineering, Linkoping University, Linkoping, Sweden, 1995. 
[12] Y.-P. Chen and J.-S. Wang, “A novel recurrent neural network with 
minimal representation for dynamic system identification,” Proc. of 
2004 IEEE International Joint Conference on Neural Networks, 
Budapest, Hungary, pp. 849-854, July 25-29, 2004. 
[13] C. F. Juang and C. T. Lin, “A recurrent self-organizing neural 
fuzzy inference network,” IEEE Trans. on Neural Networks, vol. 
10, no. 4, pp. 828-845, 1999. 
 
SVC algorithm to identify an optimal cluster number with 
compact and smooth arbitrary-shaped cluster boundaries. 
The organization of this paper is as follows. In 
Section 2, we briefly introduce the support vector 
clustering algorithm. Section 3 presents a validity-guided 
support clustering algorithm. Computer simulations on the 
identification of optimal cluster configurations for 
different examples are illustrated in Section 4. Finally, 
conclusions are summarized in Section 5. 
2 SVC Algorithm 
The mathematical fonnnlation of SVC algorithm is 
as follows. Assume given a data set {x i }  c x of N points, 
with x Rd, the input data space. A nonlinear mapping 
function @ is used to map x into a high-dimensional 
feature space. The objective is to search for the smallest 
enclosing sphere of radius R expressed in the following 
optimization problem: 
min R’ + 
subjectto I l @ ( x j ) - a l 1 2 <  R 2 + 5 ,  V j ,  ( I )  
where a is the center and 4 are slack variables allowing 
for soft boundaries. That is, they allow some data points 
lying out of the sphere. The above problem is usually 
solved by the Lagrangian function using its dual problem: 
L = R ’ - x ~ ( R * +  4- II @(xj) - 0  I1 2 ) ~ j - x & ~ j +  a&, (2)  
where aj t 0 and pj 2 0 are the Lagrangian multipliers, C 
is a constant, and E& is a penalty term. The Kamsh- 
Kuhn-Tucker (KKT) conditions allow the problem to he 
rewritten as 
max w = X j @ ( x j ) ’ a j -  & a j a j a ( x i ) .  @(xj) 
subject to 0 < uj< C, Eaj= l , j  = I ,..., N, (3) 
where the dot product Q(xi) . mixj) represents the 
Gaussian kemel transformation by K ( x * , x ~ )  = ;++f . 
The Lagrangian Wnow can be written as 
max w= XjK(Xj,  x,) aj ~ c, ai UjK(Xi, x,) 
subject toO<uj<C,Xaj=l , j=I  ,..., N .  (4) 
We can optimize Eq. (4) to obtain the Lagrangian 
multipliers aj. There exist data points lying on the surface 
of the sphere in the feature space, and these data points 
are called support vectors (SVs) when 0 < aj < C. The 
data points that lie outside the feature space with uj = C 
are called bounded support vectors (SSVs). SVs can he 
used to describe the hypersphere in the feature space. For 
each point x, the distance of its image in the feature space 
from the center of the sphere is given by 
R 2 ( ~ ) = K ( x , ~ ) - 2 E j K ( ~ j , ~ ) ~ j - & a i U j K ( X i , X j ) .  (5) 
The radius R of the sphere can be obtained by 
Ri= { R(xJ I xi is a support vector}. (6)  
In practice, the average over all support vectors is 
used as the radius R. The cluster boundaries are then 
constructed by the data points that satisfy R(x) = R .  The 
SVs, BSVs, and all the other points are located on the 
cluster boundaries, the outside of the boundaries, and the 
inside of the boundaries, respectively. The number of 
clusters and their shapes of boundaries are govemed by 
two parameters, q and C. The example shown in Fig. 1 
demonstrates how the value of q affects the cluster 
boundary as well as the numbers of clusters. Table 1 
shows the values of q and its corresponding number of 
clusters and SVs. 
J . . . ~ , ’  ; , , . !4 1 .  ~ ~; . . . * . ! 
(d) (e) 
Fig. 1. Different values of q result in different cluster 
configuration (a) Original data distribution containing 116 
points. (b) q = 0.06 (c) q = 0.3 (d) q = 0.9 (e) q = 1.5. 
3614 
2) Perform the SVC algorithm to obtain the number of 
clusters. 
3) If the number of cluster < 2, increase the value of q, 
and go to Step 2. 
4) Compute the validity measure (ratio G). 
5) If the number of cluster # N-I, increase the value of q,  
and go to Step 2. 
Else, stop the SVC algorithm. From Eq. (12), the 
optimal number of clusters can be found and the 
range of q as well. Go to Step 6 and Step 7. 
6) Identify compact and smooth cluster configuration 
from the range of q. 
7) Identify the optimal value of q. 
The above strategy incorporates the SVC algorithm 
with the proposed validity measure to search the optimum 
cluster configuration. Please note that in Step 2, the SVC 
algorithm is performed to get the Lagrangian multipliers 
and SVs, and then the calculation of the radius R is made. 
Finally, the cluster number, nc, can be found. In step 3, if 
nc < 2, we increase the values of q until nc ? 2. 
Io Fig. 2, the algorithm evaluates the clustering 
performance by the proposed validity measure for 
different q values in a certain range. That is, for each q 
value, the SVC forms a number of clusters. The validity 
ratio is then calculated with respect to these clusters until 
the minimal G value is identified. Consequently, with a 
fixed number of clusters, the value of q is slowly 
increased until the clusters cover their corresponding data 
with compact and smooth boundaries. 
4 Simulation Results 
The effectiveness of the proposed validity-guided 
SVC algorithm has been validated through extensive 
computer simulations for different applications. Due to 
the limited space, two examples, an artificial data set and 
a benchmark data adopted from [IO] are demonstrated in 
this paper. 
Example I: Artificial data 
The artificial data contain four clusters. First, 
according to the validity measure, the identified optimal 
number of clusters equal to four is found from the range 
of q (0.04 < q < 0.3). Through the second phase of the 
proposed algorithm, the values of the validity measures 
and their corresponding number of clusters for different 
ranges of q are shown in Table 2. Fig. 3 shows the 
clustering results with different q values. Form the above 
range of q, the final optimal value of q is 0.35. 
Example 2: Benchmark data 
The second data set contains artificially generated 
synthetic data. A random number generator produced 55 
data points. We compared three fuzzy cluster validity 
indexes, VK [a], V, [3], VK [9], using the fuzzy c- 
partition obtained from the FCM. First, according to the 
validity measure, the identified optimal number of clusters 
equal to four is found from the range of q (0.15 < q < 
0.17). Through the second phase of the proposed 
algorithm, the values of the validity measures and their 
corresponding number of clusters for different ranges of q 
are shown in Table 3. Figure 4 illustrates the cluster 
configurations with different values of q. The comparison 
results are listed on Table 4. Form Table 3, the optimal 
number of cluster is 4 with q = 0.16. However, the 
optimal number of cluster is 3 for the validity measures by 
V, and V,. Vpc determined its optimal number of cluster 
is 2. Since the shapes of cluster boundaries generated by 
the SVC algorithm are arbitrary, our validity measure 
provides a guideline for determining a suitable 
configuration. 
Unlabeled Data Input 
Perform SVC algorithm 
using different q values 
-'the validity 
4 YES 
STOP 
Fig. 2. The flowchart of validity-guided SVC algorithm. 
Table 2. Validity measures for example 1 
4 Cluster number G 
0.04 2 0.1944 
0.06 4 0.1565* 
0.08 4 0.1565' 
0.1 4 0.1565* 
0.2 4 0.1565* 
0.3 5 1.7844 
0.5 5 1.7844 
0.8 6 1.9488 
1 6 1.9488 
3616 
References 
[ l ]  A. Ben-Hur, D. Hon, H. T. Siegelmann, and V. N. 
Vapnik, “A support vector clustering method,” Proc. of 
Int’l Con$ on Panem Recog., vol. 2, pp. 728-732,2000. 
[2] A. Ben-Hur, D. Hon, H. T. Siegelmann, and V. N. 
Vapnik, “Support vector clustering”. Joumal of Machine 
Leaming Research, vol. 2, pp. 125-137,2001, 
[3] X. L. Xie and G. Beni, “A validity measure for fuzzy 
clustering,” IEEE Trans. on Pattem Analysis and 
Machine Intelligence, vol. 13, no. 8, pp. 841-847, 1991. 
[4] H. S .  Rhee, K. W. Oh, “A validity measure for fuzzy 
clustering and its use in selecting optimal number of 
clusters,” Proc. of IEEE Intemational Conference on 
F u q  Systems, vol. 2, pp. 1020-1025, 1996. 
[5] J. H. Chiang and P. Y. Hao, “A new kemel-based 
fuzzy clustering approach Support vector clustering with 
cell growing,” IEEE Trans. on Furry Sysiems, vol. 11, no. 
4, pp. 518-527,2003, 
[6] 
sets,” Joumal ofMath. Biologv, vol. 1, pp. 57-71, 1974. 
[7] J. C. Dunn, “A fuzzy relative of the ISODATA 
process and its use in detecting compact well-separation 
clusters,’’ Journal of Cybemet., vol. 3, pp. 32-57, 1974. 
[SI D. L. Davies and D. W. Bouldin, “A cluster 
separation measure,” IEEE Truns. on Puttem Anaijsis 
and Machine Intelligence, vol. 1, no. 2, pp. 224-227,1979. 
[9] S. H. Kwon, “Cluster validity index for fuzzy 
clustering,” Electron. Letters, vol. 34, no. 22, pp, 2176- 
2177,1998. 
[IO] D.-W. Kim, K. H. Lee, and Doheon Lee, “Fuzzy 
cluster validation index based on inter-cluster proximity,” 
Paitem Recogniiion Letfers, vol. 24, no. 15, pp. 2561- 
2574,2003. 
J. C. Bezdek, ‘‘Numerical taxonomy with fuzzy 
i 
1 ...+. .::<:::::* . . . . . . . . . . . . . . . . . I:::::::: .x::::: . . . . . . . . .,.... . . .~.  , 
(4 
Fig. 4. Different values of q result in different cluster 
configuration (a) Original data distribution containing 55 
points. (b) q =  0.15 (c) q = 0.16 (d) q = 0.17. 
361 8 
regulating vector R, defined by a vertex of the rectangle, is 
initially assigned with a random vector that will gradually 
wnsmlr Plot 
/ 0.sr 
. ,, . ."\ . . . . . . J+-.-> 
. .  . .  
".gl 
0 . 2 -  ; 
. .  . 0.I . . ... 
. .  .  
0.26 
0.2 . .  :  . .  
. . . . . . . . . ..... 
0.6 0.7 0.11 
Dlmensbn t 
Fig. 1, The dynamic trajectory of the regulating vector R. 
approach to the virtual cluster spread y during the 
clustering process. Once the average distance between R 
and y is below a pre-specified value, the cluster boundary 
estimation is converged and the spread of the estimated 
cluster is equal to the distance between the regulating 
vector and the mean of the cluster. 
Figure 1 shows an example of the cluster boundary 
estimation. The regulating vector is randomly assigned and 
it approaches a vertex of the rectangle formed by the 
radiuses of the estimated cluster. Note that both y and R 
are dynamically changed with respect to the incoming data. 
The moving trajectory of the regulating vector illustrated 
in Fig 1 shows the convergence of the estimation of R and 
y. This estimation is the so-called cluster boundary 
estimation. 
The successive estimations for the mean (M = [ M I ,  
M2, ..., Mi]') and the standard deviation (0 = [UI,  U>. ..., 
Q]') are expressed by Eq. (1) and (2), respectively. 
where I.= [xl, x2, .._, xk]' is the n-th data grouped to the 
same cluster. We next introduce the temporary regulating 
vector R,,' which is obtained by 
(3) 
To update the regulating vector %, there are two cases to 
consider. One is R,,' located in the inside of the virtual 
spread and the other case is the outside. For the first case, 
the regulating vector should be pushed outward to the 
vertex of the rectangle formed by the virtual spread in the 
opposite direction of the vector R,,'. Contrarily, the 
regulating vector should be equal to R.' The following 
criterion has been utilized to the update of the regulating 
vector. 
Criterion: 
Compute the distance d between the updated boundary 
vector and the mean vector in each dimension.; 
I fa l ld ,<(ro,) , i=l ,  ... k,thenRn=2%.,-R;, 
Else & = R,,', 
where d = [dl, d2,. . ., dk 1,  a = [oI, 02,. . ., 4. 
So far, we did not consider the case where the 
incoming data is located in the overlapping areas of 
clusters. To update such data, we have adopted the idea of 
fuzzy c-mean clustering algorithm. We used membership 
functions to assist the aforementioned update procedures. 
The membership function we choose is 
where U, is the membership grade of the i-th cluster ,c is 
the number of clusters, and c' is the number of overlapping 
clusters. Considering the membership grade, we modified 
Eqs. ( I ) ,  (2) and (3) as 
where Mi,. is the center of the ith cluster, uLn is the 
membership grade of the i-th cluster. 
In [ I O ] ,  Kaymak and Setnes proposed the 
hypersphere-shaped prototype to group original data and 
used a merging mechanism to improve the compactness of 
5936 
2.51 
4 
Fig. 3. Cluster split-up: one cluster splits into two clusters. 
If all value of A!, i=l, ..., c, is less than a pre-specified 
threshold, the existing cluster configuration is converged 
and the merging or splitting mechanism proceeds then. 
Otherwise, the SRCA will bypass the two mechanisms 
because it needs more data to construct the cluster 
configuration. 
To merge clusters, we adopted the idea of the 
mapping-constrained agglomerative (MCA) clustering 
algorithm from our previous work [3]. The MCA merges 
clusters when the mapping relation between the input and 
the output space is consistent. The Euclidean distance of 
two clusters shown in Eq. (11) is used as a criterion of 
consistency. If the values 4. of two clusters in the input 
space and the output space are smallest, these two clusters 
are merged. Otherwise, the current cluster configuration 
remains the same. 
3.3 Splitting Mechanism 
In [4], the average variances of the input clusters are 
used to determine the splitting criterion of clusters. Here 
we use the average variances of the output clusters as the 
splitting criterion because the output clusters directly affect 
the outcome of the cluster configuration. The average 
variances for the output clusters are obtained by the 
following equation. 
where 7. represents the average variance, U: is the 
variance of the i-th cluster, and Ni is the counter of the ith 
cluster. If the average value increases over a pre-specified 
threshold (q), the SRCA will split the cluster whose 
variance is higher than the average variance. Such a cluster 
will split into two small clusters. The centers of these two 
clusters are placed on the diagonal line of the rectangle 
that corresponds to the regulating vector of the cluster that 
will be split. The diagonal line is divided into three equal 
intervals with four points. The inner two points are used as 
the new cluster centers and the outer two points will be 
treated as the corresponding regulating vectors. Figure 3 
illustrates the idea of cluster split-up. AAer the split-up, the 
SRCA will reset the counter of the new clusters. 
Finally, the stopping criterion of the SRCA is that all 
clusters must be convergent, that is, no merging or 
splitting mechanism is active. Next section, we use the 
cluster configuration constructed by the SRCA to initialize 
the RBFs of REiFNN for function approximation. 
4 Simulations 
Example I: The first benchmark example is adopted 
from [SI and the function to be approximated is as follows. 
1; ( x )  = 0.2 + 0.8(x+ 0.7sin(2nx)), x E [O,I], (13) 
Our objective is to approximate the function by a 
RBFN constructed by the SRCA. The obtained cluster 
configuration is mapped into a set of radial basis functions 
expressed by the following equation: 
lil 
where wj is the associated weight and & is the radial basis 
function. The RBF we use here is the Gaussian function. 
The training data are 21 equidistant input-output 
training points from the interval of [0, I]. The training 
samples are indicated by circles and the results by triangles 
as shown in Fig. 4(a). The Levenberg-Marquardt 
optimization algorithm was used to fine tune the 
parameters. The normalized root mean squared error 
(NRMSE) [SI was calculated to evaluate the performance. 
100 equidistant points sampled from [0, 11 are used as 
testing data. The approximation for a RBFN with four 
radial basis functions (four clusters) is shown Fig.4 e). 
Table I illustrates the comparisons with other approaches. 
5938 
