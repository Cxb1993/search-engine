 II
Abstract 
This sub-project is to accomplish the analysis, processing and interpretation techniques of 3D 
audio in which recording, acquisition, compression and demonstration are realized. Particularly, the 
adequate audio field is generated according to a user’s listening angle. First, multiple microphones 
are adopted to record audio and speech signals. Then, the recorded signals are analyzed and 
classified. Second, the blind source separation scheme based on orthogonalization is employed to 
raise separation performance. Notably, a special pre-processing technique is developed to 
compensate the non-linearity of the room acoustic and recording system. Such an approach can 
effectively improve quality of separated sounds. Third, the separated sounds under multiple 
channels are further perceived to understand their positions based to the time delay of arrival. 
Fourth, to effectively minimize a channel bandwidth, the MPEG surround coding method is used to 
encode 17 audio channels in which one channel indicates a low-frequency signal. Finally, the audio 
field is built according to a user’s listening angel where the Head-Related Transfer Function (HRTF) 
is utilized based on the locations of sounds. Particularly, as well as two speakers, we implement the 
reconstructed audio field that can also allow a 5.1 channel playing system to demonstrate a 
transparent and real sound effect. The results of this sub-project integrated with those of the other 
sub-projects focused on 3D video can successfully provide a 3D audio-visual demonstration from 
any view angle. These core technologies developed from this sub-project can be beneficial to 3D 
TV and raise the competence of our information technology industry in the world.  
 
Keywords: 3D Audio, MPEG Surround, Head-Related-Transfer-Function, Blind Source Separation, 
Gram-Schmidt Orthogonalization, Adaptive Whitening 
 
 
 
 
 
 
 
 IV
圖目錄 
圖 1.1  3D AV 音訊處理、儲存與音場重建流程圖 ..................................................................................................... 2 
圖 2.1  3D 音訊錄製實驗室 ............................................................................................................................................ 4 
圖 2.2  錄音設備連接示意圖 .......................................................................................................................................... 4 
圖 2.3  3D 音訊擷取設備 ................................................................................................................................................ 5 
圖 2.4  影音同步錄製之攝影棚環境.............................................................................................................................. 6 
圖 2.5  麥克風位置擺設示意圖...................................................................................................................................... 7 
圖 3.1  實驗中所使用之量測輸入波形 )(te ................................................................................................................. 10 
圖 3.2  WAVEFORMS OF )(thrdn  AND )(' th rdn ..................................................................................................... 11 
圖 3.3  2.3 SIGNALS OF )(terec  AND )(thoverall ....................................................................................................... 11 
圖 3.4  ph 與 h
~訊號的頻譜及時域波形 ....................................................................................................................... 13 
圖 3.5  錄音環境之三維空間示意圖............................................................................................................................ 14 
圖 3.6  錄得混合訊號及經過前處理的混合訊號與模擬合成的混合訊號頻譜 ....................................................... 15 
圖 3.7  多輸入多輸出盲蔽訊號分離的架構圖............................................................................................................ 16 
圖 3.8  二維調適性正交化盲蔽訊號分離系統架構圖................................................................................................ 19 
圖 3.9 GSO 正交集概念 ................................................................................................................................................. 23 
圖 3.10  語音(上)與音樂(下)的原始訊號時域波形 .................................................................................................... 24 
圖 3.11  混合訊號 1X (上)與 2X (下)的訊號時域波形 ............................................................................................... 24 
圖 3.12  經過 GSO 且單位化之 1U (上)與 2U (下)的訊號時域波形.......................................................................... 24 
圖 3.13  二支麥克風擺放的示意圖.............................................................................................................................. 26 
圖 3.14  四支麥克風擺放的示意圖.............................................................................................................................. 27 
圖 4.1  以麥克風陣列進行音源定位的組態示意圖.................................................................................................... 31 
圖 5.1  MPEG SURROUND 架構圖 ........................................................................................................................... 35 
圖 5.2  16+1 音軌應用於 MPEG SURROUND 架構示意圖 ..................................................................................... 36 
圖 6.1  3 維空間音源插補示意圖................................................................................................................................. 37 
圖 6.2  實際錄音資訊與插補資訊之間的比較............................................................................................................ 38 
圖 7.1  不同視角的音場對應圖.................................................................................................................................... 39 
 VI
表目錄 
表 3.1  COMPARISON OF THE SEPARATED RESULT MEASURED BY SIR ................................................ 15 
表 3.2  訊號源、混合訊號以及正交混合訊號之相關係數比較 ............................................................................... 24 
表 3.4  不同改善方案與它種分離演算法分離效能 MSE 值比較 ............................................................................. 28 
表 3.5  不同改善方案與 FAST ICA 的分離效能 SIR 值比較................................................................................... 28 
表 3.6  不同改善方案與它種分離演算法效能 MSE 值比較 ..................................................................................... 30 
表 4.1  模擬四個麥克風對四個音源的多音源定位結果............................................................................................ 32 
表 4.2  模擬在空間中僅有兩個音源的狀況下進行四麥克風對四音源分離並定位的結果 ................................... 33 
表 4.3  實際空間中僅有兩個音源的狀況下進行四麥克風對四音源分離並定位的結果 ....................................... 33 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 2
 
圖 1.1  3D AV 音訊處理、儲存與音場重建流程圖 
 
 
 
 
 
 4
圖 2.1  3D 音訊錄製實驗室 
 
 
圖 2.2  錄音設備連接示意圖 
 
 
 
 
 
 
 
 
 
(a) 
 
(b)  
 
(c)  
 
(d)  
 
(e)  
 
(f)  
 6
 
圖 2.4  影音同步錄製之攝影棚環境 
 
由於本子計畫的最終目的是要建構一套自由聆聽角度之音場呈現與互動結合的介面，
因此我們首先必須知道三維平面上任一位置的音源訊號。在此本子計畫主要是依據同等收
音範圍的原則來決定麥克風陣列的擺設位置，首先我們在直徑 300 公分的水平圓面上，以
每 45 度間隔放置距離地面 90 公分的麥克風。接著將水平面軸心轉移 22.5 度，再以每 45 度
間隔放置距離地面 190 公分的麥克風。而在錄製音訊時需要特別注意三點，第一點是為了
希望可以錄製到真實環境中的音訊，我們會針對這些麥克風本身的雜訊以及其頻譜做一些
補償，也就是對於音訊之錄製的前處理；第二點是每一個麥克風陣列必須可以同步錄音，
因為這樣才可以在同一時間錄製到各個角度的音訊。另外第三點所需注意的是音訊與視訊
的錄製也必須同步，才能達到影音同步。圖 2.5 為我們擺設的麥克風陣列架設與位置擺放示
意圖，其中 M1, M3, M5, …, M15 為高腳架，M2, M4, M6, …, M16 為低腳架。 
 8
2.3 低音之錄製 
關於低音錄製的部分，由於其與一般的音源比較不同，主要是取其 20Hz~200Hz 頻段的
音源做為低頻成份，也是將來呈現 5.1 或 7.1 聲道的 LFE(Low Frequency Enhancement)聲道
輸出部份。所以本子計畫在進行 3D 音訊錄製的實驗時，另外使用了在低頻時具有較良好頻
譜響應的麥克風放置於其中一處位置；其低頻時具有較良好頻譜響應的麥克風除了與其他
15 支麥克風進行同步擷取全頻域音訊以外，我們將利用人耳聽覺對於低頻方向性較為不敏
銳，且人類聽覺模型僅能接收能量較高的低頻的特點，在後處理時將其上述低頻時具有較
良好頻譜響應麥克風之音軌的低頻成分取出並加以放大，成為含有 16 軌全頻域、以及一軌
重低音輸出，共計 17 軌的 3D 音訊源擷取環境。 
 
2.4 後方音場之錄製 
通常在聆聽環繞音效時，除了前面的聲道(Left、Right、Center)能將音源及定位資訊傳
達給使用者，然而主要提供進一步的聆聽效果則是依靠前一段所述的重低音聲道以及後方
聲場(包含 Left Surround、Right Surround)。而後方音場的來源有不同的產生方式，有依靠後
製作或是人為合成而產生者，也有在進行錄音時即擺設專為接收後方音場之反射、繞射或
後方音源成分的麥克風[1]。本子計畫採用後製作的方式，由於表演者位於 16 支麥克風環場
架設之中心點，所以我們將後方音場經前場錄製到的音訊做合理的計算與推估，再合成為
後方音場，希冀藉此一方式收錄 3D 音訊中音場的其它成分，必能增加後方音場的臨場感。
而後音場之製作於錄音時亦使用有限數量麥克風接收音訊，對於自由視角(free-view point)
所需但麥克風錄音角度沒接收到的方向，我們將對音源利用合成與插補的方式產生。 
 
 
 
 
 
 
 
 
 10
3.2.1 伸展派波用於系統響應量測之驗證 
在實際的錄音環境中，我們所求得的系統響應其參數的正確性與播放聲源的揚聲器
(loud speaker)、紀錄的儀器之頻譜特性有極大關聯。整體的系統響應成分至少包含了揚聲
器、空間、麥克風，以及數位/類比轉換器(AD/DA convertor)內部的響應等。有此可知具有
平坦頻譜響應之儀器將可增進所量得之數據的精確性。欲求得錄音現場的整體系統響應，
直觀而言首先想到使用單位脈衝，因為其頻率響應平坦，並且可將麥克風收錄的波形直接
作為系統響應的參數，然而，我們在實驗中卻發現單位脈衝的能量太低，造成量得的參數
訊雜比(SNR)不高，再加上現有之揚聲器由於反應不夠快，量測的參數帶有揚聲器本身所產
生而非空間所故有之殘響，降低系統響應的準確性。因此我們以弦波為基礎，使用伸展脈
波解決準確性與訊雜比的問題。本計畫所使用的伸展脈波[28][30]之時域波形請見圖 3.1，其
頻率隨時間升高，振幅依 Hamming window 變化，以利揚聲器播放時能由緩和至迅速地震
動，而能量由小至大再由大至小，務必使揚聲器振膜的反應能跟上伸展脈波。其方程式如
式(3.1)。 
 
=)(te Hamming ))(sin( 2ta ×           (3.1) 
 
 
圖 3.1  實驗中所使用之量測輸入波形 )(te  
 
由於離散取樣的表示法，導致圖 3.1 中伸展脈波在某些頻段的交接處會產生失真，然而
透過接下來的驗證，仍然證明此一輸入訊號用以量測系統響應的可行性。先定義 )( tTe − 為 )(te
之反時域波形，圖 3.2(a)為我們任意產生的函式 )(thrdn ，我們將 )(te 與 )(thrdn 進行摺積得到 )(' te ，
再將 )(' te 與 )( tTe − 再度摺積而得到 )(thrdn 的還原結果 )(' th rdn ，如圖 3.2(b)。有鑒於對 )(thrdn 的還
原效果良好，因此我們就以 )(te 式(3.1)作為量測錄音環境系統響應的測試輸入訊號。在接下
來的實驗中，我們將無從得知各訊號源相對於某一麥克風的響應 H，只能由上述方式推算
之。 
 12
 
3.2.3 應用於多個信號源 
BSS 所要處理的主題是多個訊號源之摺積性混合 (convolutive mixture)的解混合
(demix)。為了簡化多音源與多麥克風之間的情形，考慮式(3.1)，我們可導出典型的兩麥克
風接收兩音訊訊號源的矩陣表示式： 
 
⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡==⎥⎦
⎤⎢⎣
⎡
2
1
2212
2111
2
1
s
s
hh
hh
SH
x
x
p
                             (3.4) 
 
ppp hshsx 2211 +=  2,1=p                               (3.5) 
 
px 代表第p個麥克風接收的麥克風訊號， qph  (q=1…2)代表的是第q個音源與第p個麥克風之
間的響應。根據此系統，我們利用3.2.2節所述的方法驗證不同方位的音源相對於某一個麥
克風的系統響應： 
 
)()()(
)(*))]()((*)([
)(*)](*)()(*)([
21
21
21
nhnhnh
Tnenhnhne
Tnenhnenhne
ppp
pp
pp
=+=
−+=
−+
                     (3.6) 
 
為了簡化 ph 的推導，假設 ph1 與 ph2 具有相似的系統響應，差別為相位及振幅，因兩個喇叭發
出相同訊號在不同位置，所以可將式(3.6)更進一步的簡化成： 
 
)(*~)(*~     
)(~)(~     
)()(
2211
2211
21
δδδδ
δδ
−+−=
−+−=
+=
nahnah
nhanha
nhnhh ppp
                        (3.7) 
 
h~為不同的喇叭及麥克風因位置及傳遞所造成延遲的系統響應， 1a 及 2a 個別為 ph1 與 ph2 的振
幅，因此式(3.5)可再進一步推導成： 
 
)]()([*~
)](*~[*)()](*~[*)(
)(*)()(*)(
222111
222111
2211
δδ
δδδδ
−+−=
−+−=
+=
nsansah
nahnsnahns
nhnsnhnsx ppp
                  (3.8) 
 14
3.2.4 音源分離前處理結果 
此實驗使用4個麥克風補捉2個音源，麥克風陣列(microphone array)與音源之空間關係
如圖3.5；兩個音源的類型分別是S1音樂(music)和S2語音(speech)，取樣頻率(sampling 
frequency)為44.1kHz，長度6秒，錄音的環境為一個6×3×3(單位為m)的視聽室。 
 
M1  (0,-60,0)
M4  (-18,-60,70) M3  (-18,60,70)
M2  (0,60,0)
S1  (80,-60,0) S2  (80,60,0)
x
y
z
 
圖 3.5  錄音環境之三維空間示意圖 
 
接下來使用了數種知名的 ICA 演算法做為參考工具，以驗證經過前處理之混合訊號作
為演算法的輸入的確能提升分離效能，這些演算法包含了許多類型的獨立成份判斷準則，
像是最大概似法則(maximum likelihood, ML)、高階統計量(high order statistics)以及負消息量
(negentropy)等等，而我們所使用的分別是 Fast ICA[5]、JADE[6]、NP ICA[7]以及 Radical 
ICA[8]。在本段中，所有量測的結果都是以 SIR(signal to interference ratio)為基準，SIR 的定
義如下： 
 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
−
=
∑
∑
=
=
N
n
mm
N
n
m
SS
S
dBSIR
1
22
1
2
)ˆ(
10log10)(
                            (3.9) 
 
其中 mSˆ 表示重建後的訊號， mS 表示訊號源，SIR 值越高代表分離效能越好。原始未經處理
與經過前處理的分離結果見表 3.1。 
 
 
 
 
 
 
 16
3.3 音源的分離 
音訊處理在今天已經是多媒體領域的一門重要學問，應用在音訊上的盲蔽訊號分離演
算法亦佔有一席之地。傳統上，獨立成份分析與調適性盲蔽訊號分離已經廣為使用，特別
是應用在特徵值擷取、資料分群(Data clustering)、語者辨識、影像分析等眾多領域。本計畫
主要是先介紹與獨立成份分析有關的數學模型以及相關假設，再來利用調適性正交化盲蔽
訊號分離的原理[31][32]，討論空間中由 2 個麥克風接收 2 個音源而來的混合訊號，分析當
中不同的獨立成份並將其分離，最後再延伸至多維混合訊號的情形。在此使用 m 階間隔熵
值估測演算法(m-spacing Entropy Estimation)來做為分離的演算法，並利用混合訊號的自相關
函數進行不同的調適性白化前處理搭配，並以旋轉矩陣為更新學習機制，使預測的模型更
加接近訊號源。此外，在進行白化程序前利用 GSO 正交化提早將獨立分量初步抽出，使後
續的白化程序和分離演算法更有效率。 
 
盲蔽訊號分離演算法可以被視為多輸入多輸出的系統 (Multi Input Multi Output, 
MIMO)[5]。圖 3.7 是多輸入多輸出之盲蔽訊號由混合至分離的系統圖。 
 
...
...
...
Σ
Σ
Σ
Σ
...
...
...
 
圖 3.7  多輸入多輸出盲蔽訊號分離的架構圖 
 
首先假設音源訊號的數目與麥克風的數目為相等。那麼音源訊號 qS ， Qq ,......1= 跟麥克風
訊號 pX ， Pp ,......1= 的關係式如式(3.10)所示： 
 18
3.3.1 時域上以熵值估測為基礎的調適性正交化盲蔽訊號分離演算法 
由圖 3.7 我們可以知道原始訊號 S、混合訊號 X、以及分離出的信號 Y 彼此在盲蔽訊號
分離系統中的關係，假設訊號源 S 之中的成份為統計性獨立且混合訊號 X 已經過前置白化
處理，則盲蔽訊號分離的問題可以簡化為只需要透過尋找出一個旋轉矩陣 (Rotation 
Matrix)，就可以還原出訊號源 S 以及混合矩陣 A [15][16]；因此，接下來的討論都將以 X 經
過白化處理為前提。 
 
3.3.1.1 目標函數的推導 
盲蔽訊號分離演算法通常都由一個目標函數(Objective Function)，亦稱價值函數，以及
分離演算法所構成。我們首先將由目標函數開始討論[5]，目標函數列式如下： 
 
              
∑
∫
=
=
−=
Π=
=
D
i
Di
i
D
iD
D
D
D
YYYHYH
ypyyypKL
d
ypypyp
yyyp
yyypYJ
1
21
121
21
,21
21
),...,,()(        
})(||),...,,({        
)(),...,(),(
)...,,(
log),...,,()( μ
               (3.12) 
 
其中 Dyydyd ...21=μ ， )(YH 稱為多維連續隨機變數 Y 的差動熵值(Differential Entropy)[17]，
KL(.)是 Kullback-Leibler divergence[18]，式(3.12)的用途及相關文獻已詳細記載於[5][19]。若
隨機變數 1Y ， 2Y ，…， DY 互為獨立，則式(3.12)將變為： 
 
         
0        
1log),...,,(        
)(),...,(),(
)...,,(
log),...,,()(
21
21
,21
21
=
=
=
∫
∫
μ
μ
dyyyp
d
ypypyp
yyyp
yyypYJ
D
D
D
D
                (3.13) 
 
由於式(3.13)須符合隨機變數全部互為獨立的條件，因此可用於測量獨立性。若 )(YJ 為 X 和
W 的函式[5][20]，則目標函數 )(YJ 可寫為： 
 
                 )(...)()(minarg 21 DW YHYHYHW +++=                        (3.14) 
 20
 
                  
⎥⎦
⎤⎢⎣
⎡=
⎥⎦
⎤⎢⎣
⎡=⎥⎦
⎤⎢⎣
⎡=
N
N
N
N
XXX
XXX
XXX
XXX
X
X
X
22221
11211
2
2
2
1
2
1
2
1
1
1
2
1
    L
L
L
L
                         (3.15) 
 
其中 ijX 代表第 i 維的第 j 個成分，訊號長度為 N，每列代表一軌麥克風信號。而調適性正交
化盲蔽訊號分離演算法的目的是一次處理兩維，因此決定獨立成份的準則便由式(3.14)改寫
為以下： 
 
)()(minarg *2
*
1
* YHYHW
W
+=                          (3.16) 
 
其中 *W 是一個旋轉矩陣，又由於演算法一次處理兩維的訊號，因此 *W 表示如下： 
 
                         ⎥⎦
⎤⎢⎣
⎡ −= θθ
θθ
cossin
sincos*W                              (3.17) 
 
很明顯的， *W 是一個θ 的函數，因此我們可以對θ做全域搜尋找出一個旋轉矩陣以滿足式
(3.16)的獨立成份準則判斷式。值得注意的是，搜尋的範圍其實只要介於 ]
2
,0[ π 即可，因為
任何 90°的旋轉都會使得成份彼此之間不相關。全域搜尋的參數為 K，K 的值取決於在 ]
2
,0[ π
所希望的解析度；舉例來說，我們若希望演算法每 5°搜尋一次，則 K=18；每 1°搜尋一次，
則 K=90；每 0.5°搜尋一次，則 K=180… 依此類推。當我們在 1X ， 2X 之間找到符合式(3.16)
的準則條件下的θ值之 *θ 時，則將 *θ 代回式(3.17)，即可求得互為獨立、近似於信號源 S 的
Y，其架構圖如圖 3.8 所示。 
 
                         
WXY
W
=
⎥⎦
⎤⎢⎣
⎡ −=
**
**
cossin
sincos
θθ
θθ
                          (3.18) 
 22
         
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
−
=
1000
0)cos()sin(0
0)sin()cos(0
0001
),,(
LLL
MOMOMOM
LLL
MOMOMOM
LLL
MOMOMOM
LLL
θθ
θθ
θjiJocobi                   (3.21) 
 
其中正弦和餘弦函數出現在矩陣的第 i 個及第 j 個列與行。由式(3.20)，演算法具有更新學
習機制，W 與 Y 的更新學習機制為： 
 
                          *),,( WjiJacobiWnew θ=                             (3.22) 
 
                              *YWY newnew =                                 (3.23) 
 
其中 *W 的初始值給定為一個對稱矩陣(Identity Matrix)， *Y 的初始值則是經過混合信號 X 進
行內插可加性高斯、順序統計量排序與 m 階間隔熵值估測之後由 XWY ** = 求得。 
 
3.3.2 改善分離效能之方案 
由 3.3.1 節所述的演算法推導與相關條件的角度而言，混合信號的前處理佔了極為重要
的角色，根據先前分離演算法的實驗證明，使用不同的前處理方法可以使分離效能得到不
同的結果。為了提升盲蔽訊號分離結果的可辨識度，本計畫除了 3.2 節伸展脈波的前處理，
再提出了數種前處理的方案，希望能對分離效能更有所改善。 
 
3.3.2.1 Gram-Schmidt 正交化 
Gram-Schmidt 正交化(Gram-Schmidt Orthogonalization)亦稱 GSO，其目標為將多維的隨
機向量透過逐步削減彼此投影分量的方式，使得隨機向量間彼此互為正交成為正交集；經
GSO 處理後的多維隨機向量再經單位化(Normalization)成為正交集，且經白化處理之後，就
能夠提高各訊號彼此之間統計上的獨立性。GSO 單位正交的概念為圖 3.9，數學模型見式
(3.24)： 
 24
圖 3.10  語音(上)與音樂(下)的原始訊號時域波形 
 
 
圖 3.11  混合訊號 1X (上)與 2X (下)的訊號時域波形 
 
 
圖 3.12  經過 GSO 且單位化之 1U (上)與 2U (下)的訊號時域波形 
 
為進一步驗證本計畫提出的方法的確有效，我們比較了訊號源與混合訊號之間的相關
係數，以及訊號源與經 GSO 正交化後的混合信號間的相關係數如下表所列。 
 
表 3.2  訊號源、混合訊號以及正交混合訊號之相關係數比較 
 1X  1U  2X  2U  
1S  0.57538 0.57538 0.42889 0.16379 
2S  0.69491 0.69491 0.80336 0.88419 
 
由於 1X = 1U ，故訊號源 1S 與 1X 及 1U 之間的相關係數會相同，另外 2S 與 1X 及 1U 之間也
是如此。但其實重點在於經過 GSO 之後， 1S 與 2X 的相關係數會較 1S 與 2U 為高，這是因為
2U 已經從 1X 之中去除了 1S 的成分，也因為如此， 2S 與 2X 的相關係數會較 2S 與 2U 為低，
使的 2U 更接近 2S 的成分。 
 
3.3.2.2 調適性白化機制 
白化處理最後會產生一個白化轉換矩陣 V，而根據式(3.9)，任何矩陣乘以 V 都會產生另
 26
                         
SS
T
TTT
TT
T
XX
T
YY
R
I
IWW
WVXXVEW
WXXWE
WWR
YYER
=
=
=
=
=
=
=
      
      
      
][      
][      
      
][
**
**                             (3.30) 
 
實際上，白化矩陣很可能沒有絕對的好壞之分，不同白化矩陣對於不同的混合信號實際上
會產生各異的去相關效果，因此，本計畫將上述之白化方式全部參與在演算法中，目前使
用上的判定標準為：若某一白化後的 V 其彼此間的相關係數最小，則取用之。 
 
3.3.3 實驗結果 
在陳述完了調適性正交化盲蔽訊號分離的核心演算法以及改善方案之後，接下來這一
小節主要是結合上述一切，驗證分離效果。在實驗中測試了我們兩種不同組合的改善方案，
分別是 Proposed Method (A) (ICA+GSO+最佳白化) 與 Proposed Method (B) (ICA+最佳白
化)，並與 Fast ICA[5]以及 NP ICA[26]進行比較，比較的基準同 3.2.4 節的輸出信號的訊號
干擾比(Signal to Interference Ratio, SIR)。 
 
我們將分別對於 2 個語音訊號、2 個音樂訊號以及 1 個語音訊號與 1 個音樂訊號等不同
的情形進行模擬錄音實驗，並比較輸出訊號與訊號源之間的 SIR 值及對應的 MSE 值。圖 3.13
為兩支麥克風擺放的示意圖，麥克風在三維空間中的座標分別為 X1(0,-160,0)與
X2(0,160,0)。四支麥克風擺放的示意圖，四支麥克風在三維空間中的座標分別為
X1(0,-160,0)、X2(0,160,0)、X3(0,0,-120)、X4(0,0,120)，如圖 3.14 所示。 
 
圖 3.13  二支麥克風擺放的示意圖 
 28
(violin & piano) S2(230,-25,15) 0.46181 0.42915 0.61242 0.38688 
S1(300,-90,60) 4.0211 4.018 1.9397 4.0308 2 music 
(saxophone & 
piano) 
S2(180,60,40) 2.8281 2.7934 3.4749 2.8611 
S1(300,-90,60) 4.6722 4.46837 4.4289 4.6684 1 speech + music 
(female & piano) S2(180,60,40) 5.4529 5.5814 5.4173 5.3419 
 
 
表 3.4  不同改善方案與它種分離演算法分離效能 MSE 值比較 
音源類型 音源位置 
Propose 
Method (A) 
(dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) 
NP ICA 
(dB) 
S1(150,0,-85) 0.0051603 0.005165 0.0099406 0.005165 2 speech 
(male & female) S2(-140,90,40) 0.0078189 0.0078227 0.0089639 0.0078227 
S1(140,-80,40) 0.019343 0.019363 0.026927 0.019363 2 music 
(violin & piano) S2(230,-25,15) 0.020068 0.020097 0.027352 0.020097 
S1(300,-90,60) 0.0096029 0.0096095 0.023374 0.0096095 2 music 
(saxophone & 
piano) 
S2(180,60,40) 0.011654 0.01166 0.01415 0.01166 
S1(300,-90,60) 0.0046864 0.0046739 0.0049562 0.0046739 1 speech + music 
(female & piano) S2(180,60,40) 0.0063224 0.006138 0.0090493 0.006138 
 
 
 
表 3.5  不同改善方案與 Fast ICA 的分離效能 SIR 值比較 
音源類型 音源位置 
Propose 
Method (A) 
(dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) 
NP ICA 
(dB) 
2.7888 2.8659 3.0058 2.6891 
S1(150,0,-85) 
0.68389 -0.050336 0.76756 -1.637 
2 speech 
(male & female) 
S2(-140,90,40) 3.2957 3.1431 -0.00519 2.3184 
 30
表 3.6  不同改善方案與它種分離演算法效能 MSE 值比較 
音源類型 音源位置 
Propose 
Method (A) 
 (dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) 
NP ICA (dB) 
0.0041216 0.004049 0.0039207 0.0042173 
S1(150,0,-85) 
0.0066914 0.0079239 0.0065626 0.011416 
0.0064339 0.0040552 0.012188 0.0080576 
2 speech 
(male & female) 
S2(-140,90,40) 
0.011426 0.0067408 0.021137 0.01079 
0.013879 0.014106 0.012539 0.01994 
S1(140,-80,40) 
0.021003 0.021331 0.028224 0.021827 
0.01689 0.017102 0.20142 0.016456 
2 music 
(violin & piano) 
S2(230,-25,15) 
0.023334 0.023694 0.031854 0.023883 
0.010654 0.010685 0.0080715 0.010995 
S1(300,-90,60) 
0.014967 0.014969 0.01117 0.016187 
0.11061 0.011118 0.012642 0.01127 
2 music 
(saxophone & 
piano) S2(180,60,40) 
0.018831 0.019071 0.016375 0.018562 
0.004639 0.0041032 0.0038522 0.005823 
S1(300,-90,60) 
0.0068311 0.006063 0.0047167 0.0042884 
0.0063687 0.0063904 0.0070443 0.0064864 
1 speech + music 
(female & piano) 
S2(180,60,40) 
0.010728 0.010508 0.010511 0.010641 
 
 
 
 
 
 
 
 
 
 
 
 
 32
0, 50)；表 4.1 代表經過四個麥克風對四個音源進行盲蔽訊號分離演算法後，原始音源位置
與實際定位音源位置的比較，數據在三維座標軸中以 cm 為單位；理論值代表模擬音源在三
維座標軸之中的位置，估測值則是對模擬音源定位之後估算出的值。其中，音源的配置分
別如以下所示。 
 
(1) Allocation 1： 
s1(47.16, 0.99, 0.99)，s2(47.16, 49.01, 0.99)，s3(47.16, 49.01, 49.01)，s4(47.16, 0.99, 49.01) 
(2) Allocation 2： 
s1(47.16, 0.99, 0.99)，s2(47.16, 0.99, 0.99)，s3(47.16, 0.99, 0.99)，s4(47.16, 0.99, 0.99) 
(3) Allocation 3： 
s1(47.16, 0.99, 0.99)，s2(47.16, 0.99, 0.99)，s3(47.16, 49.01, 49.01)，s4(47.16, 0.99, 49.01) 
(4) Allocation 4： 
s1(17.34, 1.09, 1.09)，s2(29.67, 50.33, -0.33)，s3(37.80, 49.45, 49.45)，s4(47.16, 0.99, 49.01) 
(5) Allocation 5： 
s1(18.05, -8.95, -8.94)，s2(31.97, 56.03, -16.10)，s3(50.62, 48.43, 48.43)，s4(45.34, -12.07, 39.13) 
 
表 4.1  模擬四個麥克風對四個音源的多音源定位結果 
 
 
由表 4.1 可知，除了 Allocation 3 之中 Source 2 的估測結果為不存在的虛數以外，絕大
 34
 
 
根據實驗結果，Source 3 以及 Source 4 仍然並不存在，如同先前的模擬，而 Source 1
及 Source 2 則可估測出大略的方位。在實驗中，我們所使用的麥克風以及音源(揚聲器)相對
於實驗環境的建置來說是體積稍大的，且由於本實驗所使用的音源定位演算法在取樣速率
越高時，對於樣本數延遲就越敏感，以 48kHz 的取樣速率來說，僅 3~5 個以內的樣本數延
遲就會對實驗的定位結果產生與實際情況極大的差異，因此針對音訊有作 normalization 和
up-sampling 來增加麥克風所錄製聲源之 correlation 計算的正確性，所以在方向性上仍有參
考價值。 
 
 
 
 
 
 
 
 
 
 
 
 
 36
如圖 5.1 所示，我們將所需傳輸之音軌做為 MPEG-D 的輸入，其音軌數量為 N(N<=17,
包含 16 軌全音域及 1 軌重低音域)，進入空間音訊編碼器(SAC Encoder)做有效的降混
(Downmix)及空間參數(Spatial Parameter)估測與編碼，而音訊壓縮部份依使用者需求即可用
MP3 或 AAC 等多種雙(單)聲道壓縮技術傳輸至接收端，空間參數亦隨著音訊壓縮技術中的
位元串(Bitstream)傳輸，接收端再經由空間音訊解碼器(SAC Decoder)做多通道音軌之重建。 
 
MPEG-D 因應使用者需求可提供多種音訊播放之環境，倘若使用者只有傳統的雙聲道
播放系統，其音訊經壓縮技術解碼後，不必經過 MPEG-D 空間音訊解碼器，即可聆聽優美
旋律；若使用者具有家庭劇院(Home Theater)，經過 MPEG-D 空間音訊解碼器亦可解出相對
的 5.1 聲道或 7.1 聲道；MPEG-D 可有效的降低傳輸位元率(Bit rate)，所以適用於可攜式設
備上之耳機收聽，並提供雙聲道音訊及具有 3D 效果的聆聽環境(Binaural Rendering)。 
 
    本子計劃應用於 MPEG-D 架構示意圖如圖 5.2 所示，其 M1, M2, …, M16 為 16 軌全音
域之音訊，而 LFE 重低音軌，合計 17 軌經空間音訊編碼後，選用雙聲道進入音訊編解碼器，
於接收端經過空間音訊的解碼器 MPEG-D 空間音訊解碼後，產生 R1, R2, …, R16 重建之 16
軌全音域之音訊及 REL 重建之重低音軌，之後再透過自由視角音場重建依使用者選用單聲
道、立體聲道及 5.1 聲道等不同播放系統聆聽其相對音場之音訊。 
 
… … …
…
 
 
圖 5.2  16+1 音軌應用於 MPEG SURROUND 架構示意圖 
 
 
 
 
 38
為了驗證插補出的麥克風資訊的正確性，本子計劃在實驗中參照圖 6.1 所述之麥克風陣
列的配置下預先擺放了 M4，並且與 M1、M2、M3 等麥克風一同進行同步錄音，其後依據
音源與麥克風陣列間的距離關係，由上述之推導以 M1 為參考資訊求出 M4 的預測信號，其
插補結果與實際信號時域波形間的比較如圖 6.2 所示，兩信號樣本間的均方誤差為 0.012。
至於兩信號振幅的差異，則是因為麥克風與音源間隨著距離增加而接收能力呈指數衰減的
緣故；由於 M1 距離音源較遠，其接收之衰減較 M4 為大，因此 M1 的信號在延遲後即使依
據相對於 M4 與音源間的關係放大，但所插補出的資訊其振幅仍較真正的 M4 小。不過兩訊
號以人耳實際收聽時的判定而言音量並不會相差太多。 
 
 
0 2 4 6 8 10 12 14 16 18
x 10
4
-0.5
0
0.5
0 2 4 6 8 10 12 14 16 18
x 10
4
-1
-0.5
0
0.5
1
 
圖 6.2  實際錄音資訊與插補資訊之間的比較 
 
實際上，由於 3D 音訊錄製環境的因素，僅使用一個參考訊號對任意位置進行音訊的內
插有時只適用於理想狀態下，比如說點狀音源、或是無方向性且均勻擴散的音源。除了麥
克風間的時間差與能量差以外，未來我們也必須考慮到聲音的繞射、反射以及麥克風接收
的衰減等因素，進而研究出一個適合用於任意聆聽角度的音訊合成方式。 
 
 
 
 
 
 
 
 
 
 
 
 40
7.1 自由聆聽角立體聲音場重建之研究 
在自由視角立體聲音場重建方面，本子計畫之做法是考慮到利用使用者對互動式介面
所下達之視角資訊(如方向角 θ、高度角 ψ等)來進行立體聲之左聲道與右聲道的音場重建，
而一般我們使用立體聲的播放系統時，會把一對立體聲揚聲器放在人的前面；而當揚聲器
朝向人面，並與鼻子中心點形成一個正三角形時為最佳的聆聽位置。因此在重建音場時我
們主要根據聆聽角的位置來重建其立體聲的音場，以下我們將以圖 7.2 來開始說明如何重建
空間上任一點的音場訊號。 
 
 
圖 7.2  自由視角選擇音場示意圖 
 
如圖 7.2 所示，基於表演者位於 16 支麥克風環場架設之中心點的前提下，我們利用 16
支麥克風完成環場的音訊錄製，每支間隔 22.5 度，其 A、B 二點為其中兩支麥克風，若使
用者的視角選擇於 22.5*N(N=0,1,2,…15)度上，我們即以錄製到的音訊做有效的處理實現相
對應的立體聲音場；若使用者選擇到 F 點，則必須利用相鄰 A、B 點等所錄製到的音訊做
插補，再合成出相應的立體聲音場，圖 7.3 即解釋如何重建立體聲音場。 
 
 42
重建，本子計劃已完成與影像部份同步之音場，並有環場每 90 度間隔之四種視角音場示範
影片如圖 7.6 所示，詳情請參照網址http://www.dsp.ee.ccu.edu.tw/ochen/demo.rar。 
 
 
 
 (a) (b)  (c) 
圖 7.4  重建立體聲音場之三種實施例 
(a)左：B，右：D 
(b)左：B + C*0.5，右：D + C*0.5 
(c)左：A + B，右：D + E 
 
 
 
圖 7.5  聲波同相重疊示意圖 
 44
7.2 自由聆聽角 5.1 聲道音場重建之研究 
欲重建 5.1 聲道的音場，首先我們必須先知道根據 ITU(International Telecommunication 
Union)所制定之標準 5.1 聲道於建立音場時其喇叭所放置的位置，如圖 7.8 所示。 
 
 
圖 7.8  5.1 聲道音場示意圖 
 
    由圖 7.8 可以知道 5.1 聲道當初在建立音場時其左聲道(L)、右聲道(R)、中置聲道(C)、
左環繞聲道(Ls)以及右環繞聲道(Rs)的放置點，而低音喇叭(LFE)則是因為人類的聽覺系統對
於低頻訊號之方位感知較不靈敏，所以不用考慮其擺放位置，只要放置點接近音場環境即
可，一般來說皆放置於環繞音場的前方。而在知道當初標準的 5.1 聲道其音場是如何建立以
後，接著我們便可以使用所錄製到的多通道麥克風音源訊號來重建 5.1 聲道的音場，在此本
計畫所採用的方式與在重建立體聲音場的技巧類似。首先同樣地我們必須先知道使用者選
擇自由視角的方向角以及高度角，接著以該視角為中心點便可以重建出各聲道之音源訊
號，圖 7.9 為針對某一視角所重建出的 5.1 聲道音場。 
 46
 
圖 7.10  自由視角 7.1 聲道音場重建示意圖(前置環場增強) 
 
 
圖 7.11  自由視角 7.1 聲道音場重建示意圖(後置環場增強) 
 
 
 
 
 
 
 
 
 48
參考文獻 
[1] B. S. Olswang and Z. Cvetkovic, “Separation of audio signals into direct and diffuse 
soundfields for surround sound,” IEEE ICASSP, vol. 5, pp. 14-19, May. 2006. 
[2] A. Hyvärinen, J. Karhunen and E. Oja, Independent Component Analysis, New York: Wiley 
& Sons, Inc., 2001. 
[3] P. Comon, “Independent component analysis, a new concept,” Signal Processing, vol. 36, no. 
3, pp. 287-314, Apr. 1994. 
[4] C. -T. Chan and O. T.-C. Chen, “A 3D sound using the adaptive head model and measured 
pinna data,” Proc. of IEEE International Conference on Multimedia and Expo, New York, 
USA, vol. 2, pp. 807-810, July 2000 
[5] A. Hyvärinen  “Fast and robust fixed-point algorithms for independent component 
analysis.” IEEE Transactions on Neural Networks, vol. 10, no.3, pp. 626-634, 1999. 
[6] J. F. Cardoso and A. Souloumiac, “Blind beamforming for non-Gaussian signals,” Proc.of 
IEE.Radar, Signal Processing, no. 160, vol.6, pp. 362–370, Dec. 1993. 
[7] R. Boscolo, H. Pan and V. P. Roychowdhury, “Independent component analysis based on 
non-parametric density estimation,” IEEE Trans. on Neural Networks, vol. 15, no. 1, pp. 
55-65, 2004. 
[8] E. G. L. –Miller and J. W. Fisher III, “ICA Using Spacings Estimates of Entropy,” Journal 
of Machine Learning Research, vol. 4, pp. 1271-1295, 2003. 
[9] Hyvärinen, J. Karhunen and E. Oja, Independent Component Analysis, New York: Wiley & 
Sons, Inc., 2001. 
[10] T. M. Cover and J. A. Thomas, Elements of Information Theory, New York: Wiley, 1991. 
[11] L. Molgedey and H.G. Schuster, “Separation of a mixture of independent signals using time 
delayed correlations,” Physical Review Letters, vol. 72, pp. 3634-3636, 1994. 
[12] L. Tong, R.-W. Liu, V. C. Soon and Y.-F. Huang, “Indeterminacy and identifiability of blind 
identification,” IEEE Trans. on Circuits and Systems, vol. 38, pp. 499-509, 1991. 
[13] E. Weinstein, M. Feder and A. Oppenheim, “Multi-channel signal separation by 
decorrelation,” IEEE Trans. on Speech and Audio Processing, vol 1, no. 4, pp. 405-413, Oct. 
1993. 
[14] S. Van Gerven and D. Van Compernolle, “Signal separation by symmetric adaptive 
decorrelation: stability, convergence, and uniqueness,” IEEE Trans. on Signal Processing, 
vol. 43, no. 7, pp. 1602-1612, 1995. 
[15] Hyvärinen, “Blind source separation by nonstationarity of variance: A cumulant-based 
 50
Signal and Image Processing, pp. 236-240, Aug. 2009. 
[31] Meng-Lin Hsia, Chih-Geng Tseng, Meng-Hsuan Chan and O. T.-C. Chen, “Low-complexity 
frame-size down-scaling integrated with IDCT,” Proc. of IEEE Workshop on Signal 
Processing System, pp. 046-050, Oct. 2009. 
[32] O. T.-C. Chen and Meng-Lin Hsia, 低計算複雜度之資料反轉換與次取樣的方法, 
“Method of data inverse transform and downscaling having low computational 
complexity,” 中正大學審查通過,目前正申請美國與臺灣發明專利. 
[33] Wen-Chih Wu and O. T.-C. Chen, “Multiple-sound-source location scheme based on 
feedback-architecture source separation,” Proc. of IEEE 52nd Midwest Symposium on Circuit 
and Systems, pp. 669-672, Aug. 2009.  
[34] Yi-Hsiang Cheng, Chun-Hung Chiu, Wen-Chih Wu, Oscal T.-C. Chen, “Pre-processing 
Scheme to effectively compensate environment and equipment factors for sound source 
separation,” submitted to IEEE International Conference on Multimedia & Expo, Singapore, 
July 19-23, 2010. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
計劃成果自評 
本子計畫中，對於音源分離的前處理部份提出了以外加刺激訊號“伸展脈波”，經由麥克
 52
可供推廣之研發成果資料表 (Ⅰ) 
□ 可申請專利  ■ 可技術移轉                       日期： 98  年 12  月 7  日 
國科會補助計畫 
計畫名稱：3D 音視訊的內容製作、處理與互動式呈現 
-子計畫一: 3D 音訊的擷取、處理與互動播放 
計畫主持人：陳自強教授 
計畫編號：NSC96-2628-E-194-004-MY2 學門領域：訊號處理 
技術/創作名稱 
應用於盲蔽訊號分離之實際環境混合訊號源前處理方案 
(Pre-processing Scheme to effectively compensate environment and 
equipment factors for sound source separation) 
發明人/創作人 陳自強、鄭亦翔、邱俊宏 
中文：本技術是關於一種在實際環境錄音時，針對所錄得的訊號進
行的前處理。在我們所提出的方法中，將產生一種適用於揚聲器的
測試訊號並由數個麥克風接收，藉以求出每個音源相對於每個麥克
風之間的整體系統響應；在實際錄音時每個麥克風將收錄到由許多
音訊訊號源所組成的混合訊號，我們將在不影響音源與麥克風之空
間相對位置之前提下去除此一響應。此一前處理方法的目的主要有
三項：首先，濾除在錄音環境內不必要的雜訊，提升音質，其次，
經前處理的混合訊號可改善盲蔽訊號源分離 (Blind Source 
Separation, BSS)演算法的分離結果，最後，經分離演算法分離出來
之訊號仍可還原錄音環境現場之空間資訊，有助於 3D 音訊製作。
技術說明 
英文：The proposal technique discloses a preprocessing scheme to
effectively enhance the performance of sound source separation. In the 
beginning, time stretched pulses are employed to stimulate an 
environment with loudspeakers and microphones. The signals recorded 
from microphones are used to determine physical parameters, 
including environment and equipment factors, of the system response 
associated with a real-world environment. According to the determined 
parameters, the mixture signals are preprocessed by de-convolution to 
decrease the influence of non-linearity and noise accompanied with 
such a recording process. The experimental results reveal that our
preprocessing scheme can apparently improve the performance at the 
popularly-used blind source separation algorithms. Therefore, the 
scheme proposed herein can be beneficial to various multimedia 
applications demanding sound source separation.  
可利用之產業 
及 
可開發之產品 
可利用之產業類別有：行動多媒體、數位內容、資訊服務等等。
 
可開發之產品有：電腦/通信/視聽電子產品等等。  
技術特點 
經記錄刺激訊號”伸展脈波”，取得麥克風的系統響應，此系統響應
包含非線性及雜訊系統響應，針對不同的麥克風所錄得的音訊檔濾
除不必要的雜訊、非線性響應，進而提升音質及錄音的品質。 
 54
可供推廣之研發成果資料表 (Ⅱ) 
■可申請專利  □可技術移轉                       日期： 98  年 12  月 7  日 
國科會補助計畫 
計畫名稱：3D 音視訊的內容製作、處理與互動式呈現 
-子計畫一: 3D 音訊的擷取、處理與互動播放 
計畫主持人：陳自強教授 
計畫編號：NSC96-2628-E-194-004-MY2 學門領域：訊號處理 
技術/創作名稱 
低計算複雜度之資料反轉換與次取樣的方法 
(Method of data inverse transform and downscaling having low
computational complexity) 
發明人/創作人 陳自強、夏夢麟 
中文：本發明提出的低計算複雜度之資料反轉換與次取樣的方法，
係將繁複的資料反轉換過程簡化，並可同時對資料作次取樣的動
作，特別是簡化的流程具有一個係數補償的方法，解決因簡化流程
可能造成畫面失真的問題。此方法可應用於視訊或靜態影像解碼器
中的頻率域與空間域間係數反轉換解碼與畫面尺寸縮小轉換，降低
畫面尺寸縮小轉換的計算複雜度，以增加解碼速率。除了可應用在
畫面長度與寬度等比例縮小，也可應用在畫面長度與寬度非等比例
縮小。 
技術說明 
英文：The present invention relates to an inverse transform and 
sub-sampling method, that is capable of simplifying a complicated 
inverse transform process, while performing sub-sampling 
simultaneously; and in particular to a low-complexity method applied 
to video decompression and frame-size reduction, that is used to 
simplify a process of frequency-domain coefficients inversely 
transformed to time-domain data and downsizing these 
inversely-transformed data by using a coefficient compensation 
technique, so as to overcome the distortion problem as caused by the 
simplified process. Particularly, it can be utilized in a video or image 
codec for inverse transform and downscaling from a frequency domain 
to a spatial domain, hereby reducing the computational complexity and 
increasing decoding speed. The method of the present invention can be 
applied to a frame length and width with equal and unequal reduction 
ratios. 
可利用之產業 
及 
可開發之產品 
可利用之產業類別有：行動通訊、半導體、數位內容、資訊科技、
數位電視、電腦/通信/視聽電子產品等等。 
 
可開發之產品有：數位 IC、電腦/通信/視聽電子產品等等。 
 56
可供推廣之研發成果資料表 (Ⅲ) 
□ 可申請專利  ■ 可技術移轉                       日期： 98  年 12  月 7  日 
國科會補助計畫 
計畫名稱：3D 音視訊的內容製作、處理與互動式呈現 
-子計畫一: 3D 音訊的擷取、處理與互動播放 
計畫主持人：陳自強教授 
計畫編號：NSC96-2628-E-194-004-MY2 學門領域：訊號處理 
技術/創作名稱 藉由調適性訊號正交化以完成盲蔽訊號源分離 (Blind Source Separation by Adaptive Signal Orthogonalization) 
發明人/創作人 陳自強、鄭亦翔 
中文：音訊處理在今天已經是多媒體領域的一門重要學問，應用在
音訊上的盲蔽訊號分離演算法亦佔有一席之地。傳統上，獨立成份
分析與調適性盲蔽訊號分離已經廣為使用，特別是應用在特徵值擷
取、資料分群(Data clustering)、語者辨識、影像分析等眾多領
域。本技術主要是利用獨立成份分析來建構數學模型以及相關假
設，再來利用調適性正交化盲蔽訊號分離的原理，討論空間中由 2
個麥克風接收 2個音源而來的混合訊號，分析當中不同的獨立成份
並將其分離，最後再延伸至多維混合訊號的情形。在此使用 m階間
隔熵值估測演算法(m-spacing Entropy Estimation)來做為分離的
演算法，並利用混合訊號的自相關函數進行不同的調適性白化前處
理搭配，並以旋轉矩陣為更新學習機制，使預測的模型更加接近訊
號源。此外，在進行白化程序前利用 GSO 正交化提早將獨立分量初
步抽出，使後續的白化程序和分離演算法更有效率。 
技術說明 
英文：Nowadays, audio signal processing has become a critical 
technique in multimedia applications. Particularly, blind source 
separation plays an essential role in audio processing where 
independent component analysis and adaptive blind source separation 
are widely adopted in feature extraction, data clustering, speaker 
recognition and content analysis. This technique is to establish the 
theoretical foundation with respect to independent component analysis. 
Second, the principle of adaptive signal orthogonalization is employed 
to explore 2 sound sources separated from the recorded signals. 
Additionally, the recorded signals from 2 and 4 mixtures of 
microphones are analyzed by using blind source separation. Finally, 
the proposed method is extensively investigated at the situation of 
multiple-dimension mixtures. Here, the m-spacing entropy estimator is 
utilized to determine independent components, and the auto-correlation 
characteristics of the mixtures are applied to conduct pre-whitening. 
Moreover, rotation matrices act as an adaptive learning mechanism to 
allow that our prediction model is closer to original sources. In order to 
efficiently computing independent component extraction, we applied 
Gram-Schmidt Orthogonalization (GSO) prior to pre-whitening to 
make the pr-processing and separation effectively.  
參加 IEEE ISCAS 2009 與 IASTED 
SIP 2009 會議報告 
 
中正大學電機所 
陳自強 教授 
E-mail: oscal@ee.ccu.edu.tw
 
(1) IEEE ISCAS 2009: 
 
摘要 
2009 國際電路與系統研討會(2009 IEEE International Symposium on Circuits 
and Systems (ISCAS 2009) ) 於 2009 年 5 月 24 日至 27 日在台灣台北召開，該會
議由IEEE Circuits and Systems Society舉辦，此會議共受理了 1631 篇論文投稿，
接受了 734 篇論文，接受比率為 45%，其中大致包括了類比與數位電路設計、生
醫電路與系統、blind訊號處理、VLSI系統與應用、數位訊號處理、圖型理論與
計算、奈米科技、生命科學與應用、感測系統、電力系統與電力電子、多媒體系
統與應用、視覺訊號處理與通訊等相關領域。此會議聚集了全世界與電路及系統
相關的研究人員，所以藉由此會議所發表之技術論文，我們將可探索與明瞭各領
域之研究方向與未來的發展趨勢。特別是IEEE Circuits and Systems Society之學
會官員將在此會議中另有學會推廣與技術規畫會議，因此這次聚會可為電路與系
統上之重要會議。在此次會議行程中，第一天為tutorial，而第二天至第四天為各
領域之論文發表。而筆者此次參加會議，有參與Multimedia Systems and 
Applications Technical Committee、主持一場lecture session、發表一篇論文以及聆
聽keynote speech和多個 poster 與oral sessions。 
 
研究環境，希望大家有機會可以參訪Univ. of Wisconsin Madison。 
  
 
二、5 月 26 日 
當天第一場是由日本東京大學Prof. Ken Sakamura發表keynote演說，其重點
是在人性為中心的設計概念，並以TRON房子為例，說明各種不同網路的整合與
計算應用。接下來，筆者參與H.264/AVC軟體與硬體架構設計session，這些論文
主要探討低複雜與高輸出率的CAVLC解碼器、硬體支援之軟體解碼模型、利用
multi-rate MAC來執行video stream解碼、intra prediction利用transform-domain部份
預測方法來完成等。中午時，筆者參與Multimedia Systems and Applications技術
委員會議，在此會議中我們討論多媒體技術領域的過去、現在與未來的相關活
動，特別是選出新任的secretary，這次有兩位傑出學者角逐，最後由工作於美國
Intel公司Dr. Chen當選。下午筆者參與低功率電路session，其中有提出低功率電
路與系統，包含操作於sub-threshold region之低功率Flip-Flop實現、低功率之UHF 
passive RFID硬體設計、利用power gating技巧來達到電路低功率之目的、透過
on-chip process、電壓和溫度之間的互補技巧來設計出低電壓之數位電路等。透
過這些流程介紹以及相對應之硬體實現架構之說明，將對筆者在低功率研究上有
所幫助。晚上大會在圓山飯店頂樓會議廳舉辦晚宴，這次晚宴特別請到優人神鼓
作表演，將台灣的傳統藝術做綜合的呈現。在此晚宴中大會和學會有做些說明和
公告，特別是介紹明年會議舉辦地點在法國巴黎。在此餐會中筆者與眾多學者互
動，交換研究心得並受益良多，然而美中不足的是打鼓表演似乎不適合晚餐時舉
行。 
 
三、5 月 27 日 
早上 keynote 演講是由工研院李鍾熙院長提出金融風暴後技術創新的看
法，特別是對國內、外科技的發展與願景提出報告，並且針對工研院開發的成果
(2) IASTED SIP 2009: 
 
摘要 
2009 國際科學與技術發展聯合研討會(2009 IEEE International Association 
of Science and Technology for Development – Signal and Image Processing (IASTED 
SIP 2009) ) 於 2009 年 8 月 17 日至 19 日在美國夏威夷檀香山市舉行，該會議的 
sponsors 為 Technical Committee on Image Processing and Signal Processing。會議
涵蓋領域相當廣泛，凡關於  audio and speech processing, image and video 
processing, signal processing 與相關應用都包含在內。此會議聚集了訊號處理相
關的研究人員，所以藉由此會議所發表之技術論文，我們將可一窺各領域之研究
方向與未來的發展趨勢。在為期三天之會議行程中，筆者參與了多個 sessions，
並聆聽invited、plenary、keynote speech 以及發表一篇論文。 
 
會議議程 
此次會議主要安排的議程，包含三天行程的設計，第一天有 invited 與
plenary 演講，第二天與第三天有 keynote 演講，第三天的最後一場演講為
tutorial，其餘時間皆為緊湊的論文發表 sessions。其中論文報告的 session 可依其
性質分為 8 個主要領域，這些領域之報告均為 oral sessions。以下將依第一天以
後大會所安排之會議順序，逐一的介紹並且探討筆者所參與之演講與 sessions。 
 
 
一、8 月 17 日 
早上的 invited 演講是由美國 University of California – Berkeley 教授 Lotfi 
A. Zadeh 發表演說，其重點是在 soft computing 與 computational intelligence  
的設計概念和應用。接下來，筆者參與 Session 1 – Computer Vision, Coding & 
Decoding，主要探討利用 active contour models (curvature-dependent magnitude 
發表  “Sensing Human Interaction” 演說，其重點在於介紹目前應用於 
video-based 和  sensor-based 人類手勢與動作辨識之最新技術與相關探討。    
然後，筆者參與了 Session 6 – Image and Signal Processing Applications 與 
Session 7 – Signal Processing and Applications，包含 texture-based 影像分析之方
法研究、如何減少 OFDM-based Power Line 通訊系統中的 impulsive 雜訊、動
態雷射 Musical Instrument Digital Interface 控制器之設計 等等。之後筆者於
Session 8 – Video Processing and Coding 發表論文，此 session 有五篇論文報告，
重點分別為 Modified Wavelet Difference Reduction (MWDR) 應用於 intra-frame 
之理論實現、利用  texture synthesis model 之影像壓縮技術來減少  texture 
flattening、應用於 Scalable Video Coding 中 enhancement layer 之快速模式演算
法以及有效建立影像中的 table-of-contents 與重要資訊。筆者所研究之範圍在於
設計低複雜度之 2-D Inverse Discrete Cosine Transform，報告過程中有其他學者提
問其他應用之可行性，如 H.264/AVC。會後筆者與眾多學者互動，交換研究心得
並受益良多，而報告的學者從理論到模擬實驗都有不錯的成果。 
 
 
結論 
    IASTED SIP 2009 是一個在訊號與音視訊處理上的會議，會中除了安排常態
性的論文討論，並且特別邀請多位美國學術地位崇高之教育者提出精彩的演說，
而筆者亦有一篇論文的發表。筆者參與此次會議，攜回一片論文光碟以及IASTED 
call-for-papers資訊。藉由這些資料的蒐集與會議的參與，希冀能更了解訊號與音
視訊處理研究領域的趨勢。 
 
determination, radix-4 Booth decoding and left-to-right 
summation. The DRD unit can increase the chance of the 
partial products in high precision being zero. The left-to-right 
summation structure in the PPS unit can lower switching 
activities of partial products in high precision being added, 
and does not increase switching activities of partial-product 
summation in low precision. Accordingly, the PPS unit based 
on the left-to-right structure is preferred in our low-power 
multiplier. As compared to the conventional left-to-right 
multipliers, the proposed multiplier employs the modified 
upper/lower left-to-right structure in which two types of full 
adders are adopted to decrease power consumption and 
critical delay of the PPS unit. The detailed description 
associated with functional blocks of the proposed low-power 
multiplier is presented as follows. 
 
  
(a)                                                          (b) 
Fig. 1.  Architectures of conventional and proposed multipliers. (a) 
Conventional multiplier. (b) Proposed multiplier. 
 
 
Fig. 2.  Example of a multiplication using DRD, radix-4 Booth decoding and 
left-to-right summation. 
 
Fig. 3.  Dynamic-range determination unit [1]. 
 
Fig. 4.  Partial-product generation unit [3]. 
A. DRD and PPG Unit 
The block diagram of the DRD unit is depicted in Fig. 3 
[1]. In this figure, three bits per group are detected owing to 
radix-4 Booth decoding. The effective dynamic ranges of two 
input data are compared to see which one is smaller, and to 
select the input datum with a smaller effective dynamic range 
as a multiplier for Booth decoding. Figure 4 shows the PPG 
unit which can reduce glitches and power consumption of the 
PPS unit [3]. 
B. Modified PPS Unit 
In order to decrease power consumption and critical delay, 
the PPS unit using the modified ULLR structure, as shown in 
Fig. 5, is adopted in the proposed 16×16-bit multiplier where 
the partial products, PP0~PP7, are partitioned into two parts: 
upper and lower partial products. Such partition can allow 
parallel summations of these two parts to reduce the delay of 
the PPS unit with an upper/lower structure. In the 
conventional left-to-right summation, the summation in the 
lower part of the PPS unit usually consumes more power than 
the summation in the upper part of the PPS unit because of 
the glitch effect accumulated from the upper part. To 
minimize the glitch effect, when the upper and lower parts of 
the PPS unit are individually added in parallel, the power 
consumed from transient states can be reduced. Figure 5(b) 
and (c) depicts the proposed summations of the upper and 
lower structures, respectively, which are implemented by 
many Full Adders (FA) and Half Adders (HA). 
Figure 6 displays the difference between the conventional 
and proposed upper/lower left-to-right PPS. The correction 
3043
Load (fF)
0 50 100 150 200
P
ow
er
 (u
W
)
0
2
4
6
8
10
12
14
16
18
20
13A
9B
SERF 
13B
9A
18A
 
(a)  
Load (fF)
0 50 100 150 200
P
ow
er
 (u
W
)
0
1
2
3
4
5
6
7
8
13A
9B
SERF
13B
9A
18A
 
(b) 
Load (fF)
0 50 100 150 200
P
ow
er
 (u
W
)
0
2
4
6
8
10
12
14
16
18
20
13A
9B 
SERF 
13B
9A
18A 
 
(c) 
Fig. 7.  Power consumption under different loading conditions. (a) 1-bit 
switching activities. (b) 2-bit switching activities. (c) 3-bit switching 
activities. 
  
(a)                                                          (b) 
Fig. 8.  10-transistor full adders. (a) 13A full adder. (b) 9B full adder. 
III. ANALYSES AND COMPARISONS 
Table I lists areas, critical delays and power consumption 
of the proposed and conventional 16×16-bit multipliers. To 
do fair comparison, all multipliers are operated at a frequency 
of 100MHz and a supply voltage of 1.8V. Particularly, all of 
the proposed and conventional multipliers adopt the DRD 
units to switch input data. Hence, the major difference 
between the proposed and conventional multipliers occurs at 
the PPS unit. Here, the 13A and 9B full adders are 
implemented by the standard cell-based design flow to 
become in-house cells. These in-house cells with the standard 
cells of the TSMC 1P6M 0.18-μm CMOS technology are 
employed to realize the proposed and conventional 
multipliers. Power-mill and Time-mill tools are used to 
estimate their power consumption and critical delays, 
respectively. From Table I, the proposed multiplier consumes 
the least power than the conventional ones to realize the 
ADPCM audio coder, G.723.1 speech coder and the wavelet 
transform. In addition, the product of area, delay and power 
consumption in the proposed low-power multiplier is the least 
Therefore, the proposed multiplier with the modified ULLR 
structure and adequate adder cells can effectively reduce 
power dissipation in various multimedia computing. 
TABLE I.  COMPARISON BETWEEN THE PROPOSED AND CONVENTIONAL 
MULTIPLIERS. 
 
IV. CONCLUSION 
In this work, we developed a multiplier using the DRD 
unit, the modified ULLR structure in the PPS unit, and the 
13A and 9B adder cells to realize low power dissipation. By 
using the TSMC 1P6M 0.18-μm CMOS technology, the 
proposed multiplier is implemented and demonstrated to have 
the least power consumption and the least product of area, 
delay and power consumption than the conventional 
multipliers. Therefore, the proposed multiplier can be a good 
candidate to achieve low-power computations for various 
multimedia applications. 
REFERENCES 
[1] O. T.-C. Chen, S. Wang and Y. W. Wu, “Minimization of switching activities 
of partial products for designing low-power multipliers,” IEEE Trans. on 
VLSI Systems, vol. 11, no. 3, pp. 418–433, June 2003. 
[2] S. K. Hsu, S. K. Mathew, M. A. Anders, B. R. Zeydel, V. G. Oklobdzija, R. 
K. Krishnamurthy and S. Y. Borkar, “A 110 GOPS/W 16-bit multiplier and 
reconfigurable PLA loop in 90-nm CMOS,” IEEE Journal of Solid-State 
Circuits, vol. 41, no. 1, pp. 256–264, Jan. 2006. 
[3] Z. Huang and M. D. Ercegovac, “High-performance low-power left-to-right 
array multiplier design,” IEEE Transactions on Computers, vol. 54, no. 3, pp. 
272–283, Mar. 2005. 
[4] J.-Y. Kang and J.-L. Gaudiot, “A simple high-speed multiplier design,” IEEE 
Transactions on Computers, vol. 55, no. 10, pp. 1253–1258, Oct. 2006. 
[5] O. T.-C. Chen, R. R.-B. Sheen and S. Wang, “A low-power adder operating 
on effective dynamic data ranges,” IEEE Transactions on VLSI Systems, vol. 
10, no. 4, pp. 435–453, Aug. 2002. 
[6] H. T. Bui, Y. Wang and Y. Jiang, “Design and analysis of low-power 10-
transistor full adders using XOR-XNOR gates,” IEEE Trans. on Circuits and 
Systems II: Analog and Digital Signal Processing, vol. 49, no. 1, pp. 25–30, 
Jan. 2002. 
[7] S. Goel, A. Kumar and M. A. Bayoumi, “Design of robust, energy-efficient 
full adders for deep-submicrometer design using hybrid-CMOS logic style,” 
IEEE Transactions on VLSI Systems, vol. 14, no. 12, pp. 1309–1321, Dec. 
2006. 
[8] J.-F. Lin, Y.-T. Hwang, M.-H. Sheu and C.-C. Ho, “A novel high-speed and 
energy efficient 10-transistor full adder design,” IEEE Transactions on 
Circuits and Systems I: Regular Papers, vol. 54, no. 5, pp. 1050–1059, May 
2007. 
3045
determines which components of Y8×8 are needed for 
computation. Based on the zig-zag scan pattern, positions 
of all points before the EOB point are understood to 
determine the transform dimensions on the row and 
column. When EOB=14, only coefficients located at gray 
areas in Fig. 2 are executed. Restated,  and DN×N are 
minimized to 8 5  and D4×8, respectively. Hence, the 
proposed method can effectively reduce computational 
complexity of IDCT in which 1-D and 2-D inverse 
transforms are illustrated as follows. 
T
N ND ×
TD ×
 
 
(a) 
 
(b) 
Figure 1. Conventional 8×8-point IDCT under EOB=14. 
(a) Column computation. (b) Row computation 
 
 
(a) 
×
D4×8 ×
D4×8
×
D4×8
+
+
=X8×8
1
2
3
4
5
6
8
7
1
8
Y8×4
 
(b) 
Figure 2. Proposed 8×8-point IDCT under EOB=14. (a) 
Column computation. (b) Row computation 
2.1 Low-Complexity 1-D IDCT 
 
Generally, an N-point 1-D IDCT can be formulated as [1] 
 
1
0
(2 1)( ) cos
2
N
n k
k
k nX C n Y
N
π−
=
+= ⋅∑  (2) 
 
where C(n)= 1/ N   if n=0 and C(n)= 2 / N  otherwise. 
The fast 1-D IDCT, as shown in Fig. 3, developed by 
Plonka et al. is adopted in our proposed method where 
2 cos( /16)iC iπ= , 2 sin( /16)iS iπ=  and 2α = [3]. The 
reason to use this 1-D structure is that the larger one of M 
and A, and M+A are smaller than the corresponding ones 
of the other conventional methods. Here, M and A denote 
the numbers of multiplication and addition operations, 
respectively. Such a 1-D IDCT can be fulfilled by a 
digital signal processor to reduce the execution time and 
increase the throughput rate. 
 In order to lower computational complexity, the 8-
point IDCT with 8 inputs and outputs, as shown in Fig. 3, 
is further reduced to become the IDCT with i inputs and 8 
outputs where i ranges from 1 to 8. Figure 4 display a 
simplified IDCT with 5 inputs and 8 outputs.  Apparently, 
it reveals that the 5-input/8-output IDCT only requires 11 
multiplications and 20 additions/subtractions that are 
lower than those of the 8-input/8-output IDCT. Table I 
lists the numbers of multiplications and 
additions/subtractions required by the IDCT with i inputs 
and 8 outputs for 1 8i≤ ≤  where each output coefficient 
need be normalized by a scaling factor 1/ 8 . 
 
 
Figure 3.  8-point 1-D IDCT proposed by Plonka et al 
 
 
Figure 4.  Simplified 1-D IDCT with 5 inputs and 8 
outputs 
237
3. Computer Simulations 
 
In the computer simulations, the proposed method is 
compared to the conventional methods [4] and [5] in 
terms of computational complexity. The floating-point 2-
D IDCT from MPEG-2 is adopted. The MPEG-2 videos 
with 352×288 (CIF), 720×480 (SDTV) and 1920×1080 
(HDTV) pixels are conducted. Additionally, bit rates of 
the CIF, SDTV and HDTV video sequences are 2Mbps, 
5Mbps and 45Mbps, respectively. Tables III, IV and V 
list the average computational complexity per block of the 
proposed and conventional methods at CIF, SDTV and 
HDTV video sequences, respectively. From these 
simulation results, the proposed method requires the least 
computational complexity. As compared to the 
conventional normal 8×8-point IDCT, the numbers of 
multiplications and additions/subtractions demanded by 
the proposed method can be reduced by 25.4% - 62.9% 
and 26.9% - 64.4%, respectively. The overhead of the 
proposed method is to select one of 15 operation modes 
based on an EOB point. 
 The conventional method proposed by Murata et al. 
can be treated as a special case of the proposed method 
[4]. Winger’s method includes an improved technique to 
reduce the execution time [5]. From simulation results, 
the computational complexity of Winger’s method is 
lower and higher than that of Murata’s method and the 
proposed method, respectively. This is because Winger 
utilized the EOB point to select one of five operation 
modes that are more efficient than four operation modes 
developed by Murata et al. Particularly, the proposed 
method explores all probable operation modes according 
to EOB points to reach the least computational 
complexity. In the proposed 2-D IDCT method, the 
smaller is the EOB point, the lower is the computational 
complexity. Therefore, the proposed method can exhibit 
apparently superior reduction on the computational 
complexity of 2-D IDCTs. 
 
 
4. Conclusion 
 
This work proposes a low-complexity method to 
accomplish a 2-D IDCT based on an EOB point. Due to 
reduction of the transform matrixes, the simplified 1-D 
IDCT is adopted to lower computational complexity. 
According to the EOB point, one of 15 operation modes is 
determined. Additionally, the computation order of row-
after-column or column-after-row is adequately chosen 
for each block to minimize the matrix computations. As 
compared to the conventional methods, the proposed 
method demonstrates the least computational complexity 
under the same picture quality. Therefore, the method 
proposed herein can be widely applied to various 
applications demanding 2-D IDCTs to reach low-
complexity computation. 
 
 
 
Acknowledgement 
 
This work was partially supported by National Science 
Council, Taiwan under the contract numbers of NSC 96-
2221-E-194-040-MY2 and NSC 96-2628-E-194-004-
MY2. 
 
 
References 
 
[1] V. Britanak, P. C. Yip and K. R. Rao, Discrete 
Cosine and Sine Transforms. New York: Academic Press, 
2007. 
[2] C. Loeffler, A. Ligtenberg and G. S. Moshytz, 
“Practical fast 1-D DCT algorithms with 11 
multiplications,” Proc. of IEEE International Conference 
on Acoustics, Speech and Signal Processing, vol. 2, pp. 
988–991, May 1989. 
[3] G. Plonka and M. Tasche, “Fast and numerically 
stable algorithms for discrete cosine transforms,” Linear 
Algebra and its Applications, vol. 394, no. 1, pp. 309–345, 
Jan. 2005. 
[4] E. Murata, M. Ikekawa and I. Kuroda, “Fast 2D 
IDCT implementation with multimedia instructions for a 
software MPEG2 decoder,” Proc. of IEEE International 
Conference on Acoustics, Speech and Signal Processing, 
vol. 5, pp. 3105–3108, May 1998. 
[5] Lowell Winger, “Source adaptive system and 
method for 2D DCT,” U.S. Patent Number 7,366,236, 
Apr. 29, 2008. 
[6] C.-C. Chen and O. T.-C. Chen, “A low-complexity 
computation scheme of discrete cosine transform and 
quantization for video compression,” Proc. of IEEE 
International Conference on Multimedia and Expo, pp. 
241–244, Aug. 2001. 
[7] V. Kober, “Fast algorithms for the computation of 
sliding discrete sinusoidal transforms,” IEEE 
Transactions on Signal Processing, vol. 52, no. 6, pp. 
1704–1710, June 2004. 
[8] J. A. Nikara, J. H. Takala and J. T. Astola, 
“Discrete cosine and sine transforms – regular algorithms 
and pipeline architectures,” Signal Processing, vol. 86, no. 
2, pp. 230–249, Feb. 2006. 
[9] Qionghai Dai, Xinjian Chen and Chuang Lin, “Fast 
algorithms for multidimensional DCT-to-DCT 
computation between a block and its associated 
subblocks,” IEEE Transactions on Signal Processing, vol. 
53, no. 8, pp. 3219–3225, Aug. 2005. 
[10] Danian Gong, Yun He and Zhigang Cao, “New 
cost-effective VLSI implementation of a 2-D discrete 
cosine transform and its inverse,” IEEE Transactions on 
Circuits and Systems for Video Technology, vol. 14, no. 
4, , pp. 405–415, Apr. 2004. 
[11] J. Takala, D.Akopian, J. Astola and J. Sarinen, 
“Constant geometry algorithm for cosine transform,” 
IEEE Transactions on Signal Processing, vol. 48, no. 6, 
pp. 1840–1843, June 2000. 
 
239
