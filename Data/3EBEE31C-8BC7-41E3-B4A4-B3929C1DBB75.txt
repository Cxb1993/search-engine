model, did not yield satisfactory results under such a design experiment. Therefore, this study aims to 
find a robust nonparametric methodology that can correctly classify objects into appropriate classes. 
The basic idea for subsequent data analysis is mainly derived from the work of Herman [7,8], with the 
additional adoption of nonparametric discrimination. In this study, two goals are set to achieve. The 
first is to recognize vehicle type, and the second is to recognize the lane in which vehicles is traveling. 
The remainder of this paper is organized as follows: Section 2 contains the nonparametric 
discrimination of functional data using three types of proximity, namely PCA-, PLS- and 
Derivatives-type semi-metrics. The PLS regression method is also mentioned simultaneously. Section 3 
presents an empirical example of vehicle recognition using microwave radar detector. Finally, 
conclusions and discussions are presented in the Section 4. 
2. Model specification and methodology 
Functional data sets appear in numerous scientific fields. Although each data point may be treated as 
a large finite-dimensional vector it is preferable to think of them as functional data. In this section, 
some notations of functional data and a brief introduction of nonparametric discrimination using three 
types of proximity are introduced. 
2.1. Preliminaries 
We first introduce some common notations and the terminology that generally used in mathematics. 
Definition 1. A random variable X is called functional random variable (f.r.v.) if it takes values in an 
infinite dimentional space. An observation x of X is called a functional data. The X denotes a random 
curve, such that . { ( ); T}X X t t= ∈
Definition 2. || . || is a semi-norm on some space F as long as: 
1.  F,  || || 0x x∀ ∈ ≥
2.  ( , ) F,  || || | ||| ||a x ax a x∀ ∈ℜ× =
3. ( , ) F F,  || + || || || || ||x y x y x∀ ∈ × ≤ + y  
Definiton 3. d is a semi-metric on some space F as long as: 
1. . 0),( ,F =∈∀ xxdx
2. . 0),( ,FF),( ≥×∈∀ yxdyx
3. ),(),(),( ,FFF),,( yzdzxdyxdzyx +≤××∈∀ . 
Notice that a semi-norm || . || does not have the property that the fact || x || = 0 implies x = 0. Similarly, 
a semi-metric does not have the property that the fact d(x, y) = 0 implies x = y. 
2.2. Three types of semi-metrics 
2.2.1. Semi-metrics based on functional PCA 
Functional Principal Components Analysis (FPCA) is a tool for computing proximities between 
curves in a reduced dimensional space. As long as 2[ ( ) ]E X t dt∫  is finite, the FPCA of the f.r.v. X has 
the following expansion [2]: 
2 
 
first presented as an algorithm analogous to the power method and was subsequently suitably 
interpreted in a statistical framework [4,6,9,12]. 
Let X be the zero-mean (n x N) matrix and Y be the zero-mean (n x M) matrix, where n denotes the 
number of data sample. PLS decomposes X and Y into the form 
      , (8) 
X = TP' + E
Y = UQ' + F
where the T, U are (n x p) matrix of the p extracted components, the (N x p) matrix P and the (M x p) 
matrix Q represent loading matrices and the (n x N) matrix E and the (n x M) matrix F are the residual 
matrices. PLS is based on the NonLinear Iterative Partial Least Squares (NIPALS) algorithm and finds 
weight vectors w, c such that 
4 
 
| | | | 1ov(t, u)] [cov( w, c)] max [cov( r, s)]r s= == =X Y X Y     [c , (9) 2 2 2
where the term cov(t, u) = t’u / n denotes the sample covariance between the components t and u. The 
NIPALS algorithm starts with the random initial value of the component u and repeats sequentially till 
the convergence is achieved. 
step 1. w = X’u / (u’u) (estimate X weights) 
step 2. ||w|| → 1 
step 3. t = Xw (estimate X component) 
step 4. c = Y’t / (t’t) (estimate Y weights) 
step 5. ||c|| → 1 
step 6. u = Yc (estimate Y component) 
Notice that if M = 1 we have the fact that u = y, and Y, denoted by y, is a one-dimensional vector. In 
this case the NIPALS procedure converges in a single iteration. Furthermore, it can be shown that the 
weight vector w also corresponds to the first eigenvector of the following series of equations: 
      ∝ ∝ ∝ ∝w X'u X'Yc X'YY't X'YY'Xw
This shows that the weight vector w is the right singular vector of the matrix X’Y. Similarly, the weight 
vectors c is the left singular vector of X’Y. The eigenvectors t and u are given as t = Xw and u = Yc, 
where the weight vector c is define in step 4 and 5 of NIPALS. 
2.2.2.1. Forms of PLS 
PLS is an iterative process. After the extraction of eigenvectors t and u, the matrices X and Y are 
derived from subtracting their rank-one approximations based on t and u. Different extractions yield 
several variants of PLS’s. By equation (1) the eigenvectors p and q are computed as coefficients of 
regressing X on t and Y on u, respectively. Then, the eigenvectors can be solved by p = X’t / (t’t) and 
q = Y’u / (u’u). 
2.2.3. Semi-Metrics Based on Derivatives 
The semi-metric based on derivatives of two observed curves xi and xi’ can be defined as: 
     ( ) ( ) 2'( , ') ( ( ) ( ))
deriv q q
q i id xi xi x t x t d= −∫ t , (11) 
where x(q) denotes the q-th derivative of x. Note that 0 ( ,0)
derivd x  is the classical L2-norm of x. The 
computation of successive derivatives is very sensitive numerically. In order to overcome the numerical 
stability problem, we can use a B-spline [1] approximation for the curves. Once we have obtained an 
analytical B-spline expansion for each curve, the successive derivatives are directly computed by 
differentiating several times their analytic form. Let {BB1,…, BBB} be a B-spline basis, then the discrete 
approximate form of the curve xi = (xi(t1),…, xi(tj)) is as follows: 
     B
1 B
J B 2
1 B b b( ,..., ) 1 b 1
ˆ ˆ ˆ( ,..., ) arg inf ( ( ) B ( ))i i i i j jj x t tα αβ β β α∈ = == = −∑ ∑ℜ . (12) 
This produces a good approximation of the solution of the minimization problem 
     B
1 B
B 2
b b( ,..., ) b 1
arg inf ( ( ) B ( ))i jx tα α α∈ =−∑∫ℜ
b 1
t dt . (13) 
Therefore, the approximate form of the curve xi = (xi(t1),…, xi(tJ))’ is B b bˆˆ (.) B (.)i ix β==∑ . Because the 
analytic expression of the Bb’s is well-known, the successive derivatives can be exactly computed and 
we can differentiate easily the approximated curves: 
     . (14) 
B( ) ( )
b bb 1
ˆˆ (.) B (.)q qi ix β==∑
Then semi-metric based on derivatives of two observed curves xi and xi’, can be computed by 
     ( ) ( ) 2' 'ˆ ˆ( , ) ( ( ) ( ))
deriv q q
q i i i id x x x t x t d= −∫ t . (15) 
2.3. Nonparametric classification of functional data 
Classification or discrimination of functional data is employed when we observe a f.r.v.  and a 
categorical response Y which gives the group of each functional observation. The main purpose is to 
classify new observations into appropriate groups. In this study, we hope to find a robust method for 
assigning each functional observation to some homogeneous group. We first review the nonparametric 
discrimination method [3]. 
2.3.1. A brief review 
Let (Xi, Yi) i = 1,…,n be a sample of n independent pairs, identically distributed as (X, Y) and valued in 
6 
 
which ensure that the estimated probabilities are forming a discrete distribution. 
3. Choosing the bandwidth. According to the shape of the kernel estimator, we have to choose the 
smoothing parameter h. As usual, h is constructed from minimizing a loss function Loss as 
 where the function Loss can be built from  and y)(minarg hLossh hLoss = sxp ihg )'(ˆ , i’s. The 
misclassification rate is a nature choice among different types of Loss functions. Therefore, the 
functional classification can be performed as follows: 
. Ⅰ Training step 
  for  H∈h
   for i =1, 2,…, n 
    for g = 1, 2,…, G 
                      ∑
∑
=
−
=
−
← n
i ii
gyi ii
ihg
xxdhk
xxdhk
xp i
1' '
1
}:'{ '
1
,
)),((
)),((
)(ˆ '  
endfor 
endfor 
endfor 
     . )(minarg hLossh hLoss H∈=
 . Predicting stepⅡ  
     Let x be a new functional observation and  be its estimated group: )(ˆ xy
           )}(ˆ{minarg)(ˆ , xpgxy Losshg←
where  is a set of suitable values for h and k is a known kernel. ℜ⊂ H
2.3.2. k-Nearest Neighbors (kNN) estimator 
The choices of the bandwidth h and the semi-metric d have great influence on the behavior of the 
kernel estimator. It is inefficient to choose bandwidth h among the positive real number subset from a 
computational perspective. So, let us consider a general way which is the kNN version of kernel 
estimator. We can thus replace a choice of real parameter among an infinite number of values with an 
integer parameter k (among a finite subset). The main idea of the kNN estimator is to replace the 
parameter h with hk which is the bandwidth allowing us to take into account k terms in the weighted 
average. The pg at x is estimated by 
     ∑
∑
=
−
=
−
= n
i ik
n
gyi ik
kg
xxdhk
xxdhk
xp i
1
1
}:{
1
,
)),((
)),((
)(ˆ , (21) 
where hk is the bandwidth such that the number of {i: d(x, xi) < hk} is k. The minimization problem on h 
over a subset of ℜ  is replaced with a minimization on k over a finite subset {1, 2,…, K}: 
     
'
   and)(minarg }K,...,2,1{ LosskLosskLoss hhkLossk ←← ∈     
where the loss function Loss is built based on  and ysxp ihg )'(ˆ , i’s. 
8 
 
 Fig 1. Illustration of installment of Radar detector and the range of Radar detection. 
Due to the symmetry of the radar signal, travelling whether from the transmitter to the target or from 
the target to the receiver, the experiment only requires half of the 512 points within the radar detection 
range. Subsequent analysis thus focuses on the data set for this group of 256 points. Furthermore, in 
accordance with the radar equation, the intensity of the radar signal is in inverse square proportion to 
the distance between the transmitter and the target or the distance from the target to the receiver 
resulting in the fact that the intensity of the received signal is in inverse proportion to the distance with 
the power of 4. The weakest signals thus are those received from the fourth lane, while the strongest are 
those on the first lane. Combining this information on the distance and the size of a specific car 
contained in the received signal, this study aims at classifying vehicles into groups defined by the size 
and the lane on which vehicles are traveling using the measure of proximity described in the previous 
section. 
3.1. Data description and data analysis 
Vehicle data received via the microwave radar detector is presented in the form of intensities in the 
unit of voltage, and is considered functional data in this study. Notably, all the data are negative 
numbers. In this study, the absolute values of the data were used. Vehicles were divided into small and 
large vehicles, and a total 162 vehicles were sampled. Trucks and trailers are considered large vehicles 
while sedans are considered small vehicles. Due to the geographical condition of the place where the 
experiment was performed, large vehicles traveled only in the second and the third lanes while small 
vehicles traveled in all four lanes. Therefore, the data collected from Lanes 2 and 3 will be used for the 
recognition of vehicle types. Lane 2 contained 28 and 7 small and large vehicle, respectively. 
Meanwhile, Lane 3 contained 35 and 10 small and large vehicles, respectively. To achieve the second 
goal of this study, the data of small vehicles recorded in all lanes were used for the analysis. Lanes 1 to 
4 were observed to contain 34, 28, 35, and 26 vehicles, respectively. All these data excluded equivocal 
data resulting from technical problems. 
The number of files of each vehicle depends on the time spent within the radar detection range, 
which results from various factors including vehicle body length, speed and some technical problems 
10 
 
1.85
1.9
1.95
2
2.05
2.1
2.15
2.2
2.25
2.3
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Time
V
ol
ta
ge
 
(c) 
2
2.05
2.1
2.15
2.2
2.25
2.3
2.35
2.4
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Time
V
ol
ta
ge
 
(d) 
Fig 2. Illustration of the adjusted data. Small vehicles in Lanes 2 and 3 are shown in (a) and (b), 
respectively. Large vehicles in Lanes 2 and 3 are shown in (c) and (d), respectively 
Based on the idea proposed by Herman [7,8], as mentioned in the previous section, this study 
directly analyze the adjusted data. The nonparametric discrimination using three types of proximity is 
used for the analysis. The forms of the three proximity measures are as follows: 
1. PLS-type semi-metrics with 2, 3, 4, 5, 6, 7, 8 and 9 factors. 
2. PCA-type semi-metrics with 2, 3, 4, 5, 6, 7 and 8 factors. 
3. Derivative-type semi-metrics zero derivatives (classical L2-norm). 
Furthermore, to assess the capability of the nonparametric approach, two samples, one for training and 
the other for authenticating, were therefore randomly assigned, and their misclassification rate was 
calculated. Notice that approximately two third of the whole data were assigned as training data, and 
the others were for authentication. The analysis procedure was repeated 5000 times yielding 5000 
misclassification rates, and the average misclassification rate was considered the criterion for the 
assessment. 
12 
 
