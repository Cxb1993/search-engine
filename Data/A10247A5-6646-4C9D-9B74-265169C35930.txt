adjustment is in a negative proportion the gradient of 
the error cost function. However, it is well known that 
the slow convergent speed and easy plunging into 
local minimum are two main problems existing in the 
steepest gradient decent search method. In many real 
control and signal processing applications, slow 
convergent speed means the neural model has a slow 
reaction capability. In addition, neural network will 
has an incomplete or ill learning if it plunged into a 
local minimum on its training process. Both of two 
phenomena will cause a serious problem to neural 
network while it is used in a real-line application. 
Many research articles have proposed how to 
accelerate the learning speed of neural model [10-23], 
but rare researches focused on how to increase the 
probability of neural network’s training that could 
escape from local minimum and reach the globe 
minimum. 
 
In this research, an efficient neural network 
learning algorithm is studied and developed. Such a 
new learning algorithm is expected to make neural 
model have faster and better training. In our initial 
study, the general linear least squares method, 
including multi-regression method [24], recursive 
least squares method and so on, are used to find the 
optimal weights of nonlinear neural structure. Based 
on the new learning algorithm we developed, the 
neural network not only has a fast learning capability, 
but also has an optimal training.  
 
2. Neural Network and Its Learning 
Algorithms 
 
In current neural network models, the multi-
layered feed-forward network with supervised 
learning is a most popular neural type used in real 
applications. As mentioned above, BP learning 
algorithm is a training way widely adopted by neural 
model. Therefore, our initial study will focus on the 
multi-layered feed-forward network and BP training 
algorithm.  
 
2.1 Sigma-Pi Polynomial Network 
 
In our initial work, the sigma-pi polynomial 
network was studied. A neural network structure 
known as Sigma-Pi network is reported in [25]. Sigma 
units and Pi unit are used in hidden layer and output 
layer, respectively. The law of weights adjustment 
based on the iterative gradient BP procedure is 
described as follows [25].  
 
Suppose there are h units in the hidden layer. Let 
jkz be the output unit j on the kth pattern and ijω  be a 
weight association with the connection from the ith 
unit to the jth unit. If j is an input unit, then 
jkjk xz = . If j is an output unit, then jkjk yz = . 
For hidden units, the output can be expressed as  
∑=
i
xz ijikjk ω  ,   (2-1) 
where, i is over all the inputs having connections to 
unit j.  
 
For output of the network, the signal is given by 
∏
=
= h
l
lkzy
1
k .    (2-2) 
The objective of the network is to map given input 
patterns Xk=[x0k, x2k, …,xnk] onto the desired output 
patterns sk. Let yk be the actual output of the network 
on the presentation pattern k. We usually set a cost 
function as 
2
kk2
1 )(E ∑ −= ys .   (2-3) 
To achieve the desired mapping, we need to 
minimize the cumulative sum-square error E. In fact, 
this can be achieved by adjusting the weights; 
ij
ijk
E
ωω ∂
∂∝－△ .   (2.4) 
Using the chain rule 
ij
jk
jkij
EE
ωω ∂
∂
∂
∂=∂
∂ z
z
.   (2-5) 
Let 
jk
k
E
z∂
∂−=δ .   (2-6) 
From equations (2-2) and (2-3) 
∏
≠=
−=∂
∂ h
jll
lkzysz )(,1
kk
jk
)-(2E ,  (2-7) 
jk
ij
jk x
z =∂
∂
ω .    (2-8) 
By the results of equations (2-7), (2-8), the weights 
adjustments for k-th pattern can be rewritten as 
ikkijkijk xnn ηδωω +=+ )()1( ,  (2-9) 
where η is a learning rate. 
 
2.1.1 Modified Learning Algorithm (BP+MR) 
 
In the learning algorithm we developed, the 
method proposed by article [26] is referred. Both of 
linear optimal solution method and iterative BP 
method are combined and used in the training process 
of the network. In our studies, MR method is taken. 
However, any other linear optimal solution methods, 
including least mean-squared (LMS), recursive least-
squared (RLS) and so on, could also be used. For 
finding the proper values of weights, the network is 
firstly trained by using BP learning rule with a fixed 
iteration number. Then, MR method is adopted in 
finding the better weights of the network.  
 
The overall learning process is summarized as 
follows. 
 
All the training input-output pair must be 
presented cyclically until all the weights of network 
are convergent. 
 
2.2.1 Modified Learning Algorithm (BP+MR) 
 
In our initial studies, the articles [15-17, 22] were 
studied and analyzed. According to the structure of 
sigmoid neural network, the relationship between 
input and output of each neuron k can be expressed as 
follows. 
( ) 1 (1 exp( ))k k ky f net net= = + −  (2-20) 
ln( (1 ))k k knet y y= −                                 (2-21) (2-20) 
If the desired response ks  is available for the neuron 
k in output layer, then the best value of ky  should be 
equal to ks  under the best training. In other words, 
the best internal response knet  should be equal to, 
))1/(ln( kkk ssnet −=  (2-21) 
where, 10 << ks .  
As we know, the value of knet  can be expressed as, 
hhkkkk xxxnet ωωω +++= ......2211   (2-22) 
Where, hkω  is the weight connection between neuron 
h  in the hidden layer and neuron k in the output layer, 
and hx  is the output value of neuron h  in the hidden 
layer. Therefore, for the whole training data set with p 
patterns, the internal value of output layer’s neuron 
can be written as a  linear system which is described 
as, netX =ω . Where, net  is a 1p×  vector, ω  is 
a 1h×  vector, and X  is a p h×  matrix with rank 
h .  
 
Generally p  value is great than h  value. 
Similarly, for finding the optimal weights between 
hidden layer and output layer, MR method is used in 
our study.  
 
3. Simulations 
 
(a)  Experiment 1 : System modeling by Sigma-Pi    
neural model 
 
In this experiment, a nonlinear system modeling 
by Sigma-Pi neural network is firstly experimented. 
The system needs to be approximated is expressed as 
follows. 
 
)1()(2.0                
)(1.0)1(6.0)(3.0)1(
2
3
−−+
+−+=+
kuku
kukykyky
 (3-1)  
)50/2sin(5.0)20/2sin(5.0)( kkku ππ +=     (3-2) 
In this system, 1200 pairs of data set are 
generated. In which, 1000 pairs of data set are used as 
training and the rest of 200 pairs of data set are used 
as testing. 
A size of 4-4-1 neural model is used in this study. 
The related parameters of neural model are listed as 
follows. 
Desired output: y(k+1) 
Inputs: 1, y(k-1), y(k), u(k) 
 
The modified learning algorithm of section 2.2.1 
is used as training method of neural network. The 
MR method was taken one time at each 50-epoch of 
BP learning. Figure 1(a) to Figure 1(i) show the 
training error curves performed by BP and modified 
(BP+MR) learning algorithms with various learning 
rates (lr). Also, Figure 2(a) to Figure 2(i) show the 
test error curves performed by BP and modified 
(BP+MR) learning algorithms, respectively. 
Obviously, in both of training and testing cases, the 
performances by modified learning algorithm are 
much faster and better than traditional BP learning 
method.  
 
(b) Experiment 2 : System modeling by Sigmoid  
neural model 
In this experiment, the nonlinear system 
modeling by Sigmoid neural network is experimented. 
The system needs to be identified is expressed as 
follows. 
 
1)-0.4u(k1)-(k0.2u-                  
)k)0.3sin(3u(-  ))(sin(6.0                  
)1(6.0)(3.0)1(
3 +
+
−+=+
ππku
kykyky
( 3-3) 
)cos()1.0sin()( 21 πδπδ +=ku                    (3-4) 
1δ  and 2δ , both are uniformly distributed random 
numbers within (0, 1). 
In this system, 700 pairs of data set are generated. In 
which, 400 pairs are used as training and the rest of 
300 pairs of data set are used as testing. 
 
A size of 5-6-1 neural model is used in this study. 
The related parameters of neural model are listed as 
follows. 
Desired output: y(k+1) 
Inputs: 1, y(k-1), y(k), u(k-1), u(k) 
Learning rate η  : 0.7 
 
In article [22], the output data was compressed 
within [0.1, 0.9]. However, there is no detailed 
description to explain the reason why for this step. To 
find the reason, we compressed the output data within 
three different ranges, including [0,1], [0.05, 0.95] 
and [0.1, 0.9]. In our studies, MR method is taken 
once whenever the neural network has 1000 training 
epochs by using BP learning algorithm. Then, the 
weights solved will be used to evaluate the modeling 
result. Table 1 and Table 2 show the error statistics of 
Energy Systems, Vol. 24, Issue 3, pp. 245-250, 
March 2002. 
[10] R. A. Jacobs, “Increased Rates of Convergence 
Through Learning Rate Adaptation”, Neural 
Networks, Vol. 1, pp. 295-307, 1988. 
[11] S. A. Solla, E. Levin and M. Fieisher, 
“Accelerated Learning in Layered Neural 
Networks”, Complex Systems, Vol. 2, pp. 625-
640, 1988.  
[12] Silva and Almedia, “Speeding-up 
Backpropagation”, Advanced Neural Computers, 
North-Holland, Amsterdam, pp. 151-156, 1990. 
[13] Y. Liping and Y. Wanzhen, “Backpropagation 
with Homotopy,” Neural Computation, Vol. 5, 
No. 3, pp. 363-366, 1993. 
[14] Y. Yam and T. Chow, “Extended 
Backpropagation Algorithm,” Electronic Letter, 
Vol. 29, No. 19, pp. 1701-1702, 1993. 
[15] M. R. Azmi and R. Liou, “Fast Learning 
Process of MLP NN’s Using Recursive Least 
Squares Methods,” IEEE Trans. Signal 
Processing, Vol. 4, pp. 446-450, 1993. 
[16] F. Biegler and F. Barmann, “A Learning 
Algorithm for Multilayer Neural Networks 
Based on Linear Least Squares Problems,’ 
Neural Networks, Vol. 6, pp. 127-131, 1993. 
[17] B. K. Verma and J. J. Mulawka, “Training of 
the Multilayer Perceptron Using Direct Solution 
Methods, Proceedings of 12th IASTED Int. Conf. 
Appl. Inform., pp. 18-24, Annecy, France, May 
17-20, 1994. 
[18] J. J. Mulawka and B. K. Verma, “Improving the 
Training Time of the Backpropagation 
Algorithm,” Int. Journal of Microcomputer 
Application, Canada, Vol. 13, No. 2, pp. 85-89, 
1994 
[19] B. K. Verma and J. J. Mulawka, “A Modified 
Backpropagation Algorithm,” Proceedinds of 
World Congr. Computa. Intell., Orlando, FL, 
Vol. 2, pp. 840-846, 1994. 
[20] R. Salomon and J. L. Van Hemmen, 
“Accelerating Backpropagation Through 
Dynamic Self-Adaptation”, Neural Networks, 
Vol. 9, pp. 589-601, 1996. 
[21] Y. Q. Chen, T. Yin, H. A. Babri, “A Stochastic 
Backpropagation Algorithm for Training Neural 
Networks”, Proceedings of International 
Conference on Information, Communication 
and Signal Processing (ICICS’97), pp. 703-707, 
1997. 
[22] B. Verma, “Fast Training of Multilayer 
Perceptrons,” IEEE Transactions on Neural 
Networks, Vol. 8, No. 6, pp. 1314-1320, 1997. 
[23] A. F. Atiya and A. G. Parlos, “New Results on 
Recurrent Network Training: Unifying the 
Algorithms and Accelerating Convergence,” 
IEEE Transactions on Neural Networks, Vol.11, 
No. 3, pp. 697-709, 2000.  
[24] R. J. Schilling, S. L. Harris, Applied Numerical 
Methods for Engineers Using MATLAB and C, 
Brooks/Cole, Thomson Learning, 2000. 
[25] A. J. Patrikar, Neural Network Paradigms for 
Adaptive Signal Processing and Control, Ph.D. 
Thesis, Southern Method University, Texas, 
U.S.A, 1992.  
[26] Yu-Ju Chen, Chih-Sheng Lee, Huang-Chu 
Huang, Rey-Chue Hwang, “Sigma-Pi Network 
with a Modified Learning Algorithm”, 
Proceedings of 2003 International Conference 
on Informatics. Cybernetics and Systems, pp. 
1694-1701, Dec. 14-16, Kaohsiung, Taiwan, 
2003. 
 
 
Table 1: Error statistics of training data for experiment 2.  
(Compression range: 0~1, learning rate: 0.7) 
 
BP+M.R. Conventional BP BP+M.R. Conventional BP Training 
epochs Absolute error Absolute error Square error Square error 
1000 5.7042990E-02 1.6885996E-02 5.1749777E-03 7.1020203E-04 
2000 6.9116555E-02 1.6432179E-02 6.9030370E-03 6.3441525E-04 
3000 5.4848954E-02 1.7714811E-02 4.6152393E-03 6.9002772E-04 
4000 5.7367448E-02 1.8110948E-02 4.9849250E-03 7.1121909E-04 
5000 6.0978606E-02 1.8949017E-02 5.6922953E-03 7.5321482E-04 
6000 4.6959534E-02 2.0428944E-02 3.9226804E-03 8.2163326E-04 
7000 5.6183495E-02 1.5823519E-02 4.5954622E-03 4.9431255E-04 
8000 6.7909501E-02 1.5458780E-02 5.9157079E-03 4.5448475E-04 
9000 6.2231377E-02 1.5307976E-02 5.2627553E-03 4.4297351E-04 
10000 5.7896730E-02 1.5077843E-02 5.1658265E-03 4.2918124E-04 
 
 
 
 
Table 5: Error statistics of training data for experiment 2. 
         (Compression range: 0.05~0.95, learning rate: 0.7) 
 
BP with M.R. Conventional BP BP with M.R. Conventional BP Training 
epochs Absolute error Absolute error Square error Square error 
1000 1.2944694E-02 1.1777928E-02 2.6612257E-04 2.7118472E-04 
2000 1.1154603E-02 1.3059549E-02 2.1180791E-04 3.1000871E-04 
3000 1.0655378E-02 1.2224791E-02 2.0037001E-04 2.6972042E-04 
4000 1.0497970E-02 1.1821474E-02 1.9743013E-04 2.5268376E-04 
5000 1.0449817E-02 1.1580326E-02 1.9670578E-04 2.4340182E-04 
6000 1.0438324E-02 1.1287280E-02 1.9656029E-04 2.3743024E-04 
7000 1.0430429E-02 1.1185860E-02 1.9662264E-04 2.3328193E-04 
8000 1.0438190E-02 1.1104691E-02 1.9692787E-04 2.3015242E-04 
9000 1.0449822E-02 1.1039887E-02 1.9737313E-04 2.2771610E-04 
10000 1.0460597E-02 1.0986490E-02 1.9779196E-04 2.2584919E-04 
 
 
Table 6: Error statistics of test data for experiment 2. 
(Compression range: 0.05~0.95, learning rate: 0.7) 
BP with M.R. Conventional BP BP with M.R. Conventional BP Training 
epochs Absolute error Absolute error Square error Square error 
1000 1.1494880E-02 9.3021691E-03 2.2134850E-04 1.6875341E-04 
2000 9.9549117E-03 1.0917590E-02 1.7167212E-04 2.2207359E-04 
3000 9.5805414E-03 1.0374608E-02 1.5887071E-04 1.9861263E-04 
4000 9.5248828E-03 1.0124844E-02 1.5608915E-04 1.8955638E-04 
5000 9.5433500E-03 1.0000094E-02 1.5632129E-04 1.8502442E-04 
6000 9.5674610E-03 9.9259289E-03 1.5728911E-04 1.8222661E-04 
7000 9.5887668E-03 9.8742228E-03 1.5825078E-04 1.8041866E-04 
8000 9.6040871E-03 9.7913854E-03 1.5911250E-04 1.7894141E-04 
9000 9.6187619E-03 9.7624352E-03 1.5983380E-04 1.7778455E-04 
10000 9.6267732E-03 9.7395070E-03 1.6030199E-04 1.7683566E-04 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
lr = 0.01
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
                   
lr = 0.02
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
 
(a)                                                                                                  (b) 
 
lr = 0.03
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
                   
lr = 0.04
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
 
(c)                                                                                                  (d) 
 
lr = 0.05
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
                   
lr = 0.06
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
 
 (e)                                                                                                  (f) 
 
lr = 0.07
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
                   
lr = 0.08
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
 
(g)                                                                                                 (h) 
 
lr = 0.09
0
0.005
0.01
0.015
0.02
0.025
0.03
1 3 5 7 9 11 13 15 17 19
BP+MR
BP
 
                                                                        (i) 
 
Figure 2: The test error curves of experiment 1 by BP and modified (BP+MR) learning algorithms. 
 
