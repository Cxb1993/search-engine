page and its related pages. 
The proposed method is composed of six 
parts/concepts: 
1. schemas as definitions of data items within web 
pages 
2. annotations to associate tags within web pages 
with schemas 
3. schema registries for storing schemas 
4. schema verifiers for verifying data items against 
schemas 
5. information extractors for extracting information 
from web pages based on schemas 
6. javascript functions providing site-level 
relationship information among web pages 
With our approach, the web will be transformed into a 
large database with a tremendous amount of 
information. 
 
英文關鍵詞： Digital Content, Information Extraction, Web Database
 
2 
 
a single web page and its related pages. 
The proposed method is composed of six parts/concepts: 
1. schemas as definitions of data items within web pages 
2. annotations to associate tags within web pages with schemas 
3. schema registries for storing schemas 
4. schema verifiers for verifying data items against schemas 
5. information extractors for extracting information from web pages 
based on schemas 
6. javascript functions providing site-level relationship information 
among web pages 
With our approach, the web will be transformed into a large database with 
a tremendous amount of information. 
 
Keywords: Digital Content, Information Extraction, Web Database
4 
 
For example, it is not unreasonable that two different bookstore web sites both 
define book information as book::=(bookName, author+, description?, price, 
comment*). 
3. The developers of a web site should understand the schemas used in their web 
site. Also, they have clear concept about where to adopt which schema. 
Based on the above assumptions, a structural annotation method is proposed in 
this research. Web site developers either create their own schemas or select suitable 
ones from publicly accessible registries and annotate data items within their web 
pages with the selected schemas. When performing information extraction, extractors 
verify the extracting target against the specified schema information, construct a parse 
tree based on the schema information if verified, and then extract data according to 
the parse tree. Web sites can be modified arbitrarily provided that schemas are not 
violated. Hence, the extraction process can become much more stable. 
In addition, although most of existing information extraction technologies does 
not perform site-level extraction, we plan to implement the feature in our system. In 
our currently ongoing research, a virtual web browsing environment is under 
development. With the virtual browsing environment, it is possible to invoke a 
javascript function on a web page from another web page and even from an external 
application. By utilizing the approach, we can simply define some javascript functions 
that will return site-level information. For example, it is reasonable to define a 
javascript function “class_of( )” for the “class_of” relationship between this and other 
web pages and to define “next_page( )” and “prev_page( )” functions to represent the 
navigation order among several web pages. By invoking these javascript functions 
during the information extraction process, relationships among web pages can be 
discovered. As a result, site-level information extraction can be achieved. 
Methodology 
Conceptually, a database can be defined with the entity-relationship model (the ER 
model). Two main components of the ER model are entities and relationships. In 
modern relational databases, the definition of an entity is expressed using schemas. A 
schema contains field names, data types, and other related information. On the other 
hand, relationships describe the links between entities. Although this research does 
not aim at studying relational databases, to achieve the goal, web of data, the concept 
of ER model is still applicable. 
Until now, data items on most web pages are not governed by schema. The 
advantage of adopting a schema is quite obvious. If the data items on a web page 
conform to a commonly accepted schema, then they can be parsed and understood by 
6 
 
technology is adopted. H2X is aimed at providing a transformation framework for 
generating XML documents from annotation-augmented HTML documents. To 
address the latter issue, the author’s previous work, the VWBE (Virtual Web 
Browsing Environment) technology [Tseng 2011 b], with some additional 
configurations is adopted. VWBE simulates browsers’ behavior and runs at the 
server-side. With VWBE, scripting functions defined in Web pages become invokable 
Web services. The following sub sections explain the proposed approach for 
implementing Web of data as well as how the above technologies are used for 
implementing Web of data. 
The Data View 
As discussed previously, the data-driven usage pattern focuses more on structured 
data, Web data associated with grammars. The most widely adopted HTML does not 
allow explicitly associated data grammar and existing approaches for inferring 
grammars implicitly are not stable enough. A possible solution is to re-write Web 
resources with structural languages such as XML. However, the efforts needed make 
the solution impractical. Another possible solution is to utilize transformation 
technologies such as XSLT. A problem of such technologies is the requirement of 
programming skills. On the other hand, adopting H2X requires only augmenting the 
original HTML documents with some simple attributes. The simplicity makes 
constructing data view for existing Web resources with H2X appears more feasible. 
The Data View 
A cover page not only shows the link structure of a group of Web pages but also 
plays the role as a configuration file. The construction of cover pages can be divided 
into two steps. The first step is to complete a configuration file, which is actually an 
XML file. The figure below shows the XML schema used: 
As shown below, each link is represented using the “from” and “to” attributes, 
which contain the id of pages. The “from” part is optional and a dummy root node is 
used instead if not specified. Each page can be injected with a set of JavaScript files. 
Besides, a default set of JavaScript functions used for navigation will always be 
injected. Note that it is allowed to define cover page-level scripts. These scripts 
provide cover page-level services for software agents (there is no default cover 
page-level scripts). 
The next step is the rendering of cover pages. Here, a simple XSLT-based 
approach is adopted. A simple XSL file will be used as the default style sheet that 
8 
 
Whenever a user requests for a Web page, Group Resolver is responsible for resolving 
the given Web page URL to its containing groups. A Web page may be referenced in 
more than one group, so a group list will be returned for the user to select an 
appropriate group. Furthermore, since the functionalities provided by ordinary Web 
browsers can by no means support all above navigation operations, an additional 
Viewer component is needed. The Viewer component is activated after the desired 
group is chosen by a user. 
References 
Chang, C.H. and Girgis, M. 2006. A survey of web information extraction systems. 
IEEE Transactions on Knowledge and Data Engineering 18, 1411-1428. 
Tseng, C.H.: Developer-Friendly Annotation-Based HTML-to-XML Transformation 
Technology. In Proceedings of the 2011 ACM Symposium on Document 
Engineering, 2011, page 73-76. 
Tseng, C.H.: Virtual Browsing Environment for Mashups. In Proceedings of the 2011 
International Conference on Advanced Information Technologies, 2011, page 
154. 
 2
時間在路程上，沒有趕上第一天的 keynote speech。幸好我擔任議程主持人及發表
的場次是在下午，沒有誤了時間。7/19、7/20 由於路線已經比較熟悉，所以在路程
上就沒有問題。 
 
二、與會心得 
ICIEA 其實是比較偏向 EE 領域的研討會，不過其會議主題包含 artificial 
intelligence 和 data mining，所以和本人的研究還是有符合之處。比較可惜的是，
議程安排實在太密集了，同一時間往往有 4～5個 session 安排在一起，不得不挑著
參與。我始終覺得不同領域的知識有時會觸發意外的靈感，因此選了一些跟我的研
究主題並不相符的 session，我參與的包含 Adaptive and Intelligent Control、
Artificial Neural Network、Intelligent Systems、Machine Learning 等等主題。
其實最驚訝的是獲邀擔任議程主持人，因為我本身並沒有主持國際會議的經驗，且
會議的主題大部分與我的研究領域是不同的。在出發前我已向大會要求該 session
所有發表人的論文先行了解，並且準備提問，所幸該 session 提問還算踴躍，總算
沒有出現紕漏。 
這次我本身發表的主題是 Implementing Web of Data with VWBE & H2X，是嘗試利
用本人之前完成的 VWBE 和 H2X 技術來將網路上既有的資料（網頁）以資料庫的形式
呈現出來。該研究目前仍在雛形階段，因此這次發表的是一個初步的結果。我認為
網際網路上的資料有兩個問題：其一，非結構化，沒有文法資訊，導致資料難以應
用。其二，資料本身互相連結的特性沒有被考慮進去，雖然超連結可以解決部分的
問題，但許多沒有被建立超連結的資料之間可能仍然有關聯，因此現有的網路技術
 4
能再多考慮一下 session 的分配與研究主題的契合度，並且將議程分散到四天，應
該會比較適當。 
 
五、攜回資料名稱及內容 
本次研討會帶回兩份資料： 
1. 紙本論文集 
2. 光碟電子版論文集 
P0430.pdf. Please strictly adhere to the format given in the template for ICIEA 2012 while
preparing your final paper.
In addition to excellent technical sessions, we will also have stimulating and enriching keynote
speeches by renowned researchers. For  the  most  updated information  on  the  conference,
please check the conference website at http://www.ieeeiciea.org/. The Preliminary Program will
be available at the website in early May 2012.
I would like to take this opportunity to thank you for choosing ICIEA 2012 to present your
research results.
I look forward to seeing you in Singapore!
Yours sincerely,
Xing ZHU
Technical Program Chair
for ICIEA 2012
Comments from Reviewer(s):
__________ Informa
on from ESET NOD32 An
virus, version of virus signature database 6863
(20120206) __________
The message was checked by ESET NOD32 An
virus.
http://www.eset.com
Paper P0430 has been Accepted for ICIEA 2012  
2 of 2 2012/7/31 上午 10:31
                                                                                        
 
searching keywords to decide the sequence of showing pages 
of searching results. 
To discover the relationships among web pages more 
accurately, semantic web technologies are sometimes adopted. 
[17] used latent semantic indexing method to assist finding 
common descriptive and meaningful topic phrases for the final 
document clusters and stated that Using semantic information 
for clustering Web snippets is able to make search engine 
results easy to browse and help users quickly find Web 
information interested. [11] presented an automatic approach 
based on semantic clustering to enhance the navigation 
structure of Web sites. However, according to the survey of 
[18], without labeling process, existing web clustering 
mechanisms can only achieve 58.83% accuracy. Besides, in 
[13], the authors proposed a user-centric approach that utilized 
user feedbacks for grouping related Web pages. The proposed 
method relied on a specifically designed scoring system to 
measure the correctness and usefulness of a Web page cluster. 
B. Data Web 
As mentioned in [12], the Web as a global information 
space is developing from a Web of documents to a Web of data. 
This development opens new ways for addressing complex 
information needs. The research also put emphasis on schema-
level mappings for integrating heterogeneous Web information 
sources. In [2], approaches for tackling the challenges of 
utilizing the Web as a platform for data was proposed. The 
research focused on the Linked Data paradigm and recognized 
some benefits of the use of paradigm: uniformity, de-
referencability, coherence, integrability, and timeliness. The 
paper also highlighted the importance of large-scale RDF data 
management for establishing Web of data. In [3], the concept 
of Web join for combining hyperlinked Web data was proposed. 
The syntax, semantics, and algorithm of Web join operator 
were discussed in the paper. 
The survey made in this research shows that the importance 
of the concept of Web of data has already been recognized by 
existing researches. Past works highly focused on the 
infrastructure basis for building a data Web; however, few 
works concern the usability part. On the other hand, this paper 
aims to propose a mechanism to build a data Web system that 
is user-friendly to ordinary Web users. 
C. Web Information Extraction 
According to Arocena and Mendelzon [1], the purpose of 
web information extraction systems is to transform web pages 
into program-friendly structures. Unlike information retrieval 
(IR), which focuses on how to identify relevant documents, the 
purpose of information extraction is to produce structured data 
for post-processing. Web information extraction can be divided 
into the following steps: acquiring of input, analyzing of input, 
performing of extraction, and generating of output [7]. Among 
them, the most important step is the analyzing of input since 
this step covers several critical tasks including recognizing 
regions and discovering regularities. However, this step can be 
very tedious and difficult due to the fact that HTML is actually 
a language for the visual representation purpose only and the 
HTML source of a web page may be changed frequently. 
Moreover, even if two web pages contain similar contents with 
similar semantics, the representation may be very different [10]. 
Roughly speaking, web information extraction systems can 
be divided into four different types: manually constructed 
systems, supervised systems, semi-supervised systems, and 
unsupervised systems [4]. Manually constructed systems such 
as WebOQL [1] require users to use provided programming or 
query languages for extraction and thus become extremely 
expensive. Supervised systems such as DEByE [8] require 
users to provide a set of pre-labeled web pages for training and 
then will automatically infer rules for extracting data from 
similar web pages. Semi-supervised systems such as OLERA 
[5] require no pre-labeled training pages, but post-efforts from 
the user is required to choose the target pattern and indicate the 
data to be extracted. Unsupervised systems, on the other hand, 
require no user interactions. However, they may only be 
applicable for data-rich regions, for example, the table part, of 
a web page. The system developed by Wang and Lochovsky 
[16] is an unsupervised Web information extraction system. In 
[14], the author proposed H2X, which is an annotation-based 
Web information extraction mechanism. 
 
III. WEB OF DATA 
According to a survey made by WorldWideWebSize2, there 
were already more than 12.4 billion Web pages on 2011/11/02. 
That is a huge amount of information. The traditional 
definition of the World Wide Web is “a system of 
interlinked hypertext documents accessed via the 
Internet”3. However, Web users today are not satisfied 
with simple Web surfing anymore, and as a result, in this 
paper, the author would like to take a different point of view 
about the current Web.  
In addition to the traditional definition of the Web, inter-
linked hypertext documents, we propose to put more emphasis 
on the concept of "Web of Data", which refers to a data-driven 
usage pattern of the Web. A significant difference between 
these two points of view is that by “Web of Data”, new 
technologies to publish, access, and integrate data on the Web 
are emphasized more. Of course, the data-driven usage pattern 
is different from other Web usage behaviors such as social 
communication or electric commerce. The followings are the 
characteristics the author believes a data-driven Web should 
have: 
1. Web resources must adhere to some grammars, either 
implicitly or explicitly, to become usable data. Such 
type resource is referred to as structured data. 
2. Sets of data can be inter-related and users of Web of 
data can benefit from such relatedness. 
3. Due to the relatedness of data, some navigation 
facilities can be needed. 
In the above list, the differences between explicitly 
structured and implicitly structured data deserves more 
                                                           
2
 http://www.worldwidewebsize.com/ 
3
 http://en.wikipedia.org/wiki/World_Wide_Web 
                                                                                        
 
</tbody> 
</ul>  
</body> 
 
These additional attributes associate the HTML snippet 
with XML nodes defined in an XML schema. H2X then 
transforms the original HTML snippet into the following XML 
snippet: 
 
<books xmlns="test1"> 
 <book>  
<title>Book1</title>   
<author>Author1</author>   
<price>100</price> 
 </book> 
</books> 
 
Adopting H2X not only enables the switch between 
presentation view and data view of Web pages but also makes 
it easier for software agents to process Web resources. 
B. The Cover Page 
A cover page not only shows the link structure of a group 
of Web pages but also plays the role as a configuration file. 
The construction of cover pages can be divided into two steps. 
The first step is to complete a configuration file, which is 
actually an XML file. The figure below shows the XML 
schema used: 
 
 
Figure 1. The XML schema of the configuration file. 
 
As shown above, each link is represented using the “from” 
and “to” attributes, which contain the id of pages. The “from” 
part is optional and a dummy root node is used instead if not 
specified. Each page can be injected with a set of JavaScript 
files. Besides, a default set of JavaScript functions used for 
navigation will always be injected. Note that it is allowed to 
define cover page-level scripts. These scripts provide cover 
page-level services for software agents (there is no default 
cover page-level scripts). 
The next step is the rendering of cover pages. Here, a 
simple XSLT-based approach is adopted. A simple XSL file 
will be used as the default style sheet that generates tree-like 
user interfaces. Of course, customized style sheets are allowed. 
The following figure shows the source XML file and the 
generated HTML page of an example cover page: 
 
 
Figure 2. An example cover page. 
 
Note that the default style sheet allows only tree link 
structure. 
C. Navigation 
It is without doubt that well-designed navigation 
facilities/operations can result in good usability. In this 
implementation, 4 types of navigation functionalities are 
defined: linking to the cover page, switching between 
presentation view/data view, quick navigation, and personal 
navigation. To realize these functionalities, 4 main 
components are needed: 
1. Group Resolver: accepts a URL and returns all Web 
page groups containing this URL 
2. Session Manager: after the target group being 
selected, a user session will be generated; a Context 
object will be created for keeping session-wide 
resources and a session id will be returned; Session 
Manager is also responsible for maintaining saved 
user sessions 
3. Context: a Context object stores session-wide 
resources including the cover page instance, the 
navigation state, and a VWBE instance, etc. 
4. Viewer: the major GUI for interacting with users; the 
Viewer component is actually a Web page  
Whenever a user requests for a Web page, Group Resolver 
is responsible for resolving the given Web page URL to its 
containing groups. A Web page may be referenced in more 
than one group, so a group list will be returned for the user to 
                                                                                        
 
Web page classification/clustering algorithms into the 
next version of the implementation 
Acknowledgments 
This paper was supported by NSC project NSC100-2221-E-
262-030 and was prepared in collaboration with my lab 
members in Department of Computer Information and Network 
Engineering, Lunghwa University of Science and Technology, 
Taoyuan, Taiwan. I would like to acknowledge them for the 
completion of this research. 
REFERENCES 
 
[1] Arocena, G.O. and Mendelzon, A.O.: WebOQL: Restructuring 
documents, databases, and webs. In Proceedings of the 14th IEEE 
international conference on Data engineering, 1998, page 24-33. 
[2] Auer, S. Creating knowledge out of interlinked data: making the web a 
data washing machine. In Proceedings of the International Conference 
on Web Intelligence, Mining and Semantics, 2011, page 4:1-4:8. 
[3] Bhowmick, S.S., Ng, W.K., Madria, S.K., and Mohania, M.K. 
Constraint-Free Join Processing on Hyperlinked Web Data. In 
Proceedings of the 4th International Conference on Data Warehousing 
and Knowledge Discovery, 2002, page 255-264. 
[4] Chang, C.H. and Girgis, M.:  A survey of web information extraction 
systems. IEEE Transactions on Knowledge and Data Engineering 18, 
2006, page 1411-1428. 
[5] Chang, C.H. and Kuo, S.C.: OLERA: A semisupervised approach for 
web data extraction with visual support. IEEE Intelligent Systems 19, 
2004, page 56-64. 
[6] Crabtree, D., Andreae, P., and Gao X. Query directed web page 
clustering, In Proceedings of Web Intelligence, 2006, page 202-210. 
[7] Jirkovský, V. and Jel'ınek, I.: Proposing of modular system for web 
information extraction. In Proceedings of the 2009 international 
conference on Computer systems and technologies, 2009, page 1-4. 
[8] Laender, A.H.F., Ribeiro, B., and Silva, A.S.: DEByE---Data extraction 
by example. Data and Knowledge  40, 2002, page 121-154. 
[9] Li, T. and Chen, Y. Web Page Clustering Based on Searching Keywords. 
In Proceedings of International Conference on Intelligent Computation 
Technology and Automation, 2010, page 1163-1166. 
[10] Meng, X., Hu, D., and Li, C.: Schema-guided wrapper maintenance for 
web-data extraction. In Proceedings of the 5th ACM international 
workshop on Web information and data management, 2003, page 1-8. 
[11] Scanniello, G., Distante, D., and Risi, M. Using Semantic clustering to 
enhance the navigation structure of Web sites,  In Proceedings of the 
Web Site Evolution, 2008, page 55-64. 
[12] Tran, T., Wang, H., and Haase, P. Hermes: Data Web search on a pay-
as-you-go integration infrastructure. Web Semant. 7, 2009, page 189-
203. 
[13] Tseng, C.H., Huang, C.H., Liao, S.H., and Wang, W.W. Web of Web: A 
user-centric technology for relation-oriented Web querying. In 
Proceedings of Fourth International Conference on Ubi-Media 
Computing, 2011, page 236-240. 
[14] Tseng, C.H.: Developer-Friendly Annotation-Based HTML-to-XML 
Transformation Technology. In Proceedings of the 2011 ACM 
Symposium on Document Engineering, 2011, page 73-76. 
[15] Tseng, C.H.: Virtual Browsing Environment for Mashups. In 
Proceedings of the 2011 International Conference on Advanced 
Information Technologies, 2011, page 154. 
[16] Wang, J. and Lochovsky, F.H.: Data extraction and label assignment for 
web databases. In Proceedings of the 12th international workshop on 
World wide web, 2003, page 187-196. 
[17] Wen, H., Huang, G, and Li, Z. Clustering web search results using 
semantic information, Machine Learning and Cybernetics 3, page 1504-
1509. 
[18] Yu, J. and Ye, N. Automatic web query classification using large 
unlabeled web pages. In Proceedings of Web-Age Information 
Management, 2008, page 211-215. 
 
100年度專題研究計畫研究成果彙整表 
計畫主持人：曾俊雄 計畫編號：100-2221-E-262-030- 
計畫名稱：以結構化標註與全網站資訊擷取技術實現資料網 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
