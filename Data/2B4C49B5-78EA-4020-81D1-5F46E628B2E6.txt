Abstract
The demand of new wireless communication systems with much higher data rates
that allow, e.g., mobile wireless broadband Internet connections inspires a quick
advance in wireless transmission technology. So far most systems rely on an approach
where the channel state is measured with the help of regularly transmitted training
sequences. The detection of the transmitted data is then done under the assumption
of perfect knowledge of the channel state. This approach will not be sufficient
anymore for very high data rate systems since the loss of bandwidth due to the
training sequences is too large. Therefore, the research interest on joint estimation
and detection schemes has been increased considerably.
Apart from potentially higher data rates a further advantage of such a system is
that it allows for a fair analysis of the theoretical upper limit, the so-called channel
capacity. “Fair” is used here in the sense that the capacity analysis does not ignore
the estimation part of the system, i.e., it takes into account the need of the receiver
to gain some knowledge about the channel state without restricting it to assume
some particular form (particularly, this approach does also include the approach
with training sequences!). The capacity of such a joint estimation and detection
scheme is often also known as non-coherent capacity.
Recent studies investigating the non-coherent capacity of fading channels have
shown very unexpected results. In stark contrast to the capacity with perfect channel
knowledge at the receiver, it has been shown that non-coherent fading channels
become very power-inefficient at high signal-to-noise ratios (SNR) in the sense that
increasing the transmission rate by an additional bit requires squaring the necessary
SNR. Since transmission in such a regime will be highly inefficient, it is crucial to
better understand this behavior and to be able to give an estimation as to where
the inefficient regime starts. One parameter that provides a good approximation to
such a border between the power-efficient low-SNR and the power-inefficient high-
SNR regime is the so-called fading number which is defined as the second term in
the high-SNR asymptotic expansion of channel capacity. The results of this report
concern this fading number.
New detection schemes, however, will not be sufficient for future mobile trans-
mission systems in order to achieve the aimed high data rates. Another promising
development step is to incorporate multiple antennas, both at transmitter and re-
ceiver. It therefore becomes important to evaluate the channel capacity of such a
multiple-input multiple-output (MIMO) system.
In this report, both innovations—joint estimation and detection and multiple
antennas—are studied together. The main result is an exact expression of the fad-
ing number of a MIMO fading channel without temporal memory. While we restrict
ourselves to fading processes that are temporally independent and identically dis-
tributed (IID), we do allow dependencies between the fading of the different paths
corresponding to the different antennas (spatial memory). Furthermore, the fading
law is not restricted to be Gaussian, but is assumed to be a general regular law with
spatial (but without temporal) memory. This result can be seen as a further step
towards the final goal of the fading number of general MIMO fading channels with
memory.
Contents
Acknowledgments 1
1 Introduction 2
1.1 General Background . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 The Fading Number . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Definitions and Notation 8
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2 The Channel Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 Main Results 12
3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.1.1 Escaping to Infinity . . . . . . . . . . . . . . . . . . . . . . . 12
3.1.2 An Upper Bound on Channel Capacity . . . . . . . . . . . . 13
3.1.3 Capacity-Achieving Input Distributions and Circular Symmetry 13
3.2 Fading Number of General Memoryless MIMO Fading Channels . . . 14
4 Special Cases 16
4.1 Some Known Special Cases . . . . . . . . . . . . . . . . . . . . . . . 16
4.2 Gaussian Fading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.2.1 Scalar Line-of-Sight Matrix . . . . . . . . . . . . . . . . . . . 18
4.2.2 General Line-of-Sight Matrix . . . . . . . . . . . . . . . . . . 20
5 Proof of the Main Result 22
5.1 Derivation of an Upper Bound . . . . . . . . . . . . . . . . . . . . . 23
5.2 Derivation of a Lower Bound . . . . . . . . . . . . . . . . . . . . . . 29
6 Discussion & Conclusion 34
A Some Derivations 36
A.1 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
A.2 Derivation of Bounds (4.25) . . . . . . . . . . . . . . . . . . . . . . . 36
A.3 Proof of Corollary 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.4 Proof of Proposition 14 . . . . . . . . . . . . . . . . . . . . . . . . . 38
A.5 Additional Derivation for the Proof of the Lower Bound . . . . . . . 39
Bibliography 41
Chapter 1
Introduction
1.1 General Background
The importance of mobile communication systems nowadays needs not to be em-
phasized. Worldwide millions of people rely daily on their mobile phone. While for
the user a mobile phone looks very similar to a old-fashioned wired telephone, the
engineering technique behind it is very much different. The reason for this is that
in a wireless communication system several physical effects occur that change the
behavior of the channel completely compared with wired communication:
• The signal may find many different paths from the sender to the receiver via
various different reflections (buildings, trees, etc.). Therefore the receiver re-
ceives multiple copies of the same signal, however, since each path has different
length and different attenuation, the various copies of the signal will arrive at
different times and with different strength.
• Since the transmitter and/or the receiver might be in motion while transmit-
ting, a physical phenomenon called Doppler effect occurs: the frequency of
the transmitted signal is shifted depending on the relative movement between
receiver and transmitter.
• Since receiver and transmitter are moving and because the environment is
permanently changing (e.g., movements by wind, passing cars, people, etc.),
the different signal paths are constantly changing.
The first two effects lead to a channel that not only adds noise to the transmitted
signal (as this is the case for the traditional wired communication channel), but also
changes the amplitude of the signal (so called fading) and in extreme cases intro-
duces inter-symbol interference. Both effects can be combatted using appropriate
transmissions schemes and coding.
The fact of the time variant nature of the channel is more difficult to deal with.
Nowadays, usually a wireless communication system uses training sequences that are
regularly transmitted between real data in order to measure the channel state, and
then this knowledge is used to detect the data. This approach has the advantage
that the system design can be split into two parts: one part dealing with estimating
the channel and one part doing the detection under the assumption that the channel
state is perfectly known.
The big disadvantage of the separate estimation and detection is that it is rather
inefficient because bandwidth is lost for the transmission of the training sequences.
2
In [2] and extracts thereof published separately [3], [4], [5], [6], [7], [8], [9], [10],
[11], [12], [13], [14] large progress has been made in tackling this problem: a tech-
nique has been proposed for the derivation of upper bounds on channel capacity.2
It is based on a dual expression for channel capacity where the maximization (of
mutual information) over distributions on the channel input alphabet is replaced
with a minimization (of average relative entropy) over distributions on the channel
output alphabet. Every choice of an output distribution leads to an upper bound on
mutual information. The chosen output distribution need not correspond to some
distribution on the channel input. With a judicious choice of output distributions
one can often derive tight upper bounds on channel capacity.
Furthermore, in [2] a technique has been proposed for the analysis of the asymp-
totic capacity of general cost-constrained channels, i.e., the channel capacity in the
limited of unrestricted cost.3 The technique is based on the observation that—under
fairly mild conditions on the channel—every input distribution that achieves a mu-
tual information with the same growth-rate in the cost constraint as the channel
capacity must escape to infinity ; i.e., under such a distribution for some finite cost,
the probability of the set of input symbols of lesser cost tends to zero as the cost
constraint tends to infinity. For more details about this concept see Section 3.1.1.
Both techniques have proven very effective: they have been successfully applied
to various channel models:
• the free-space optical intensity channel [2], [6], [8];
• an optical intensity channel with input-dependent noise [2];
• the Poisson channel [2], [6], [8], [9];
• multiple-antenna flat fading channels with memory where the fading process is
assumed to be regular (i.e., of finite entropy rate4) and where the realization
of the fading process is unknown at the transmitter and unknown (or only
partially known) at the receiver [2], [4], [7];
• multiple-antenna flat fading channels with memory where the fading process
may be irregular (i.e., of possibly infinite entropy rate) and where the realiza-
tion of the fading process is unknown (or only partially known) at the receiver
[15], [16], [17], [18], [19];
• fading channels with feedback [20], [2], [5];
• non-coherent fading networks [21], [22];
• a phase noise channel [23], [24].
The bounds that have been derived in these contributions are often very tight. For
various cases the asymptotic capacity in the limit when the available power (signal-
to-noise ratio SNR) tends to infinity has been derived precisely. This is for example
the case for the regular single-input multiple-output (SIMO) fading channel with
memory and for the regular memoryless multiple-input single-output (MISO) fading
channel. In other cases the capacity pre-log (i.e., the ratio of channel capacity to
2The technique works for general channels, not fading channels only.
3In the situation of fading channels the cost is the available power.
4I.e., a process is called regular when the actual fading realization cannot be predicted even if
the infinite past of the process is known.
4
1.2 The Fading Number
In an attempt to quantify this threshold more precisely, the fading number has been
introduced [7], [2]. The fading number is defined as the second term in the high-SNR
capacity, i.e., at high SNR the channel capacity can be expressed as
C(snr) = log log snr+ χ+ o(1), (1.1)
where o(1) denotes terms that tend to zero as the SNR tends to infinity and where
χ is the fading number. For a mathematically more precise definition we refer to
Section 2.2.
We now would like to motivate our claim that the fading number is related to
the threshold between the efficient regime where capacity grows like log snr and the
inefficient regime where capacity only grows like log log snr. To that goal we need to
specify how to define this threshold. A very natural definition is as follows: we say
that wireless communication system operates in the inefficient high-SNR regime, if
its capacity can be well approximated by
C(snr) ≈ log log snr+ χ, (1.2)
i.e., the o(1)-terms in (1.1) are small. Note that in the low- to medium-SNR regime
these terms are dominating over the log log snr-term.
Now consider the following situation: assume for the moment that the threshold
snr0 lies somewhere between 30 and 80 dB (it can be shown that this is a reasonable
assumption for many channels that are encountered in practice). In this case, the
threshold capacity C0 = C(snr0) must be somewhere in the following interval:
log log(30 dB) + χ ≤ C0 ≤ log log(80 dB) + χ, (1.3)
=⇒ χ+ 2.1 nats ≤ C0 ≤ χ+ 3 nats. (1.4)
From this immediately follows the following rule of thumb:
Conjecture 1. A system that operates at rates appreciably above χ + 2 nats is in
the high-SNR regime and therefore extremely power-inefficient.
Hence the fading number can be regarded as quality attribute of the channel: the
larger the fading number is, the higher is the maximum rate at which the channel
can be used without being extremely power-inefficient.
Moreover, it follows from this observation that a system needs to be designed
such as to have a large fading number. However, in order to understand how the
fading number is influenced by the various design parameters like the number of
antennas, feedback, etc., we need to know more about the exact value of χ. So far
explicit expressions of the fading number were given for a number of fading models.
For channels with memory, the fading number of single-input single-output (SISO)
fading channels is derived in [7], [2] and the single-input multiple-output (SIMO)
case is derived in [3], [2].
For memoryless fading channels, the fading number is known in the situation of
only one antenna at transmitter and receiver (SISO)
χ(H) = log π + E
[
log |H|2
]
− h(H); (1.5)
in the situation of a SIMO fading channel5
χ(H) = hλ
(
HˆeiΘ
)
+ nRE
[
log ‖H‖2
]
− log 2− h(H) (1.6)
5For a precise definition of the notation used in this paper we refer to Section 2.1.
6
Chapter 2
Definitions and Notation
2.1 Notation
We try to use upper-case letters for random quantities and lower-case letters for
their realizations. This rule, however, is broken when dealing with matrices and
some constants. To better differentiate between scalars, vectors, and matrices we
have resorted to using different fonts for the different quantities. Upper-case letters
such as X are used to denote scalar random variables taking value in the reals R or
in the complex plane C. Their realizations are typically written in lower-case, e.g.,
x. For random vectors we use bold face capitals, e.g., X and bold lower-case for
their realizations, e.g., x. Deterministic matrices are denoted by upper-case letters
but of a special font, e.g., H; and random matrices are denoted using another special
upper-case font, e.g., H. The capacity is denoted by C, the energy per symbol by
E , and the signal-to-noise ratio SNR is denoted by snr.
We use the shorthand Hba for (Ha, Ha+1, . . . , Hb). For more complicated expres-
sions, such as (HTaxˆa,H
T
a+1xˆa+1, . . . ,H
T
b xˆb), we use the dummy variable ℓ to clarify
notation: {HTℓ xˆℓ}
b
ℓ=a.
Hermitian conjugation is denoted by (·)†, and (·)T stands for the transpose (with-
out conjugation) of a matrix or vector. The trace of a matrix is denoted by tr (·).
We use ‖ · ‖ to denote the Euclidean norm of vectors or the Euclidean operator
norm of matrices. That is,
‖x‖ ,
√√√√ m∑
t=1
|x(t)|2, x ∈ Cm (2.1)
‖A‖ , max
‖wˆ‖=1
‖Awˆ‖. (2.2)
Thus, ‖A‖ is the maximal singular value of the matrix A.
The Frobenius norm of matrices is denoted by ‖ · ‖F and is given by the square
root of the sum of the squared magnitudes of the elements of the matrix, i.e.,
‖A‖F ,
√
tr (A†A). (2.3)
Note that for every matrix A
‖A‖ ≤ ‖A‖F (2.4)
as can be verified by upper-bounding the squared magnitude of each of the compo-
nents of Awˆ using the Cauchy-Schwarz inequality.
8
Proof. Omitted.
We shall write X ∼ NC(µ,K) if X − µ is a circularly symmetric, zero-mean,
Gaussian random vector of covariance matrix E
[
(X− µ)(X− µ)†
]
= K. By X ∼
U ([a, b]) we denote a random variable that is uniformly distributed on the interval
[a, b]. The probability distribution of a random variable X or random vector X is
denoted by QX or QX, respectively.
Throughout the paper eiΘ denotes a complex random variable that is uniformly
distributed over the unit circle
eiΘ ∼ Uniform on {z ∈ C : |z| = 1}. (2.14)
When it appears in formulas with other random variables, eiΘ is always assumed to
be independent of these other variables.
All rates specified in this paper are in nats per channel use, i.e., log(·) denotes
the natural logarithmic function.
2.2 The Channel Model
We consider a channel with nT transmit antennas and nR receive antennas whose
time-k output Yk ∈ C
nR is given by
Yk = Hkxk + Zk. (2.15)
Here xk ∈ C
nT denotes the time-k input vector; the random matrix Hk ∈ C
nR×nT
denotes the time-k fading matrix; and the random vector Zk ∈ C
nR denotes the
time-k additive noise vector.
We assume that the random vectors {Zk} are spatially and temporally white,
zero-mean, circularly symmetric, complex Gaussian random vectors, i.e., {Zk} ∼
IID NC
(
0, σ2InR
)
for some σ2 > 0. Here InR denotes the nR × nR identity matrix.
As for the matrix-valued fading process {Hk} we will not specify a particular
distribution, but shall only assume that it is stationary, ergodic, of a finite-energy
fading gain, i.e.,
E
[
‖Hk‖
2
F
]
<∞ (2.16)
and regular, i.e., its differential entropy rate is finite
h({Hk}) , lim
n↑∞
1
n
h(H1, . . . ,Hn) > −∞. (2.17)
Furthermore, we will restrict ourselves to the memoryless case, i.e., we assume that
{Hk} is IID with respect to time k. Since there is no memory in the channel, an IID
input process {Xk} will be sufficient to achieve capacity and we will therefore drop
the time index k hereafter, i.e., (2.15) simplifies to
Y = Hx+ Z. (2.18)
Note that while we assume that there is no temporal memory in the channel,
we do not restrict the spatial memory, i.e., the different fading components H(i,j) of
the fading matrix H may be dependent.
We assume that the fading H and the additive noise Z are independent and of a
joint law that does not depend on the channel input x.
10
Chapter 3
Main Results
3.1 Preliminaries
Before we can state our main result we need to introduce three concepts: The first
concerns probability distributions that escape to infinity, the second a technique of
upper-bounding mutual information, and the third concept concerns circular sym-
metry.
3.1.1 Escaping to Infinity
We start with a discussion about the concept of capacity-achieving input distribu-
tions that escape to infinity.
A sequence of input distributions parametrized by the allowed cost (in our case
of fading channels the cost is the available power or SNR) is said to escape to infinity
if it assigns to every fixed compact set a probability that tends to zero as the allowed
cost tends to infinity. In other words this means that in the limit—when the allowed
cost tends to infinity—such a distribution does not use finite-cost symbols.
This notion is important because the asymptotic capacity of many channels of
interest can only be achieved by input distributions that escape to infinity. As
a matter of fact one can show that every input distribution that only achieves a
mutual information of identical asymptotic growth rate as the capacity must escape
to infinity. Loosely speaking, for many channels it is not favorable to use finite-cost
input symbols whenever the cost constraint is loosened completely.
In the following we will only state this result specialized to the situation at hand.
For a more general description and for all proofs we refer to [3, Sec. VII.C.3], [2,
Sec. 2.6].
Definition 3. Let {QX,E}E≥0 be a family of input distributions for the memoryless
fading channel (2.18), where this family is parametrized by the available average
power E such that
EQX,E
[
‖X‖2
]
≤ E , E ≥ 0. (3.1)
We say that the input distributions {QX,E}E≥0 escape to infinity if for every E0 > 0
lim
E↑∞
QX,E
(
‖X‖2 ≤ E0
)
= 0. (3.2)
We now have the following:
Lemma 4. Let the memoryless MIMO fading channel be given as in (2.18) and let
{QX,E}E≥0 be a family of distributions on the channel input that satisfy the power
12
Proof. A proof is given in Appendix A.1.
Remark 7. Note that the proof of Lemma 6 relies only on the fact that the additive
noise is assumed to be circularly symmetric.
3.2 Fading Number of General Memoryless MIMO Fad-
ing Channels
We are now ready for the main result, i.e., the fading number of a memoryless
MIMO fading channel:
Theorem 8. Consider the memoryless MIMO fading channel (2.18) where
the random fading matrix H takes value in CnR×nT and satisfies
h(H) > −∞ (3.6)
and
E
[
‖H‖2F
]
<∞. (3.7)
Then, irrespective of whether a peak-power constraint (2.21) or an average-
power constraint (2.22) is imposed on the input, the fading number χ(H) is
given by
χ(H) = sup
Q
Xˆ
{
hλ
(
HXˆ
‖HXˆ‖
)
+ nRE
[
log ‖HXˆ‖2
]
− log 2− h
(
HXˆ
∣∣ Xˆ)
}
. (3.8)
Here Xˆ denotes a random vector of unit length and Q
Xˆ
denotes its probability
law, i.e., the supremum is taken over all distributions of the random unit
vector Xˆ. Note that the expectation in the second term is understood jointly
over H and Xˆ.
Moreover, this fading number is achievable by a random vector X = Xˆ · R
where Xˆ is distributed according to the distribution that achieves the fading
number in (3.8) and where R is a non-negative random variable independent
of Xˆ such that
logR2 ∼ U ([log log E , log E ]) . (3.9)
Proof. A proof is given in Chapter 5.
Note that—even if it might not be obvious at first sight—it is not hard to show
that the distributionQ
Xˆ
that achieves the supremum in (3.8) is circularly symmetric.
This is in agreement with Lemma 6.
The evaluation of (3.8) can be pretty awkward mainly due to the first term, i.e.,
the differential entropy with respect to the surface area measure λ. We therefore
will next derive an upper bound to the fading number that is easier to evaluate.
14
Chapter 4
Special Cases
4.1 Some Known Special Cases
In this section we will briefly show how some already known results of various fading
numbers can be derived as special cases from this new more general result.
We start with the situation of a fading matrix that is rotation-commutative in the
generalized sense, i.e., the fading matrix H is such that for every constant unitary
nT × nT matrix Vt there exists an nR × nR constant unitary matrix Vr such that
VrH
L
= HVt (4.1)
where
L
= stands for “has the same law”; and for every constant unitary nR × nR
matrix Vr there exists a constant unitary nT × nT matrix Vt such that (4.1) holds
[7, Def. 4.37], [2, Def. 6.37].
The property of rotation-commutativity for random matrices is a generalization
of the isotropic distribution of random vectors, i.e., we have the following:
Lemma 10. Let H be rotation-commutative in the generalized sense. Then the
following two statements hold:
• If Xˆ ∈ CnT is an isotropically distributed random vector that is independent
of H, then HXˆ ∈ CnR is isotropically distributed.
• If eˆ, eˆ′ ∈ CnT are two constant unit vectors, then
‖Heˆ‖
L
= ‖Heˆ′‖, ‖eˆ‖ = ‖eˆ′‖ = 1 (4.2)
h
(
Heˆ
)
= h
(
Heˆ′
)
, ‖eˆ‖ = ‖eˆ′‖ = 1. (4.3)
Proof. For a proof see, e.g., [7, Lem. 4.38], [2, Lem. 6.38].
From Lemma 10 it immediately follows that in the situation of rotation-com-
mutative fading the only term in the expression of the fading number (3.8) that
depends on Q
Xˆ
is
hλ
(
HXˆ
‖HXˆ‖
)
.
This entropy is maximized if HXˆ
‖HXˆ‖
is uniformly distributed on the surface of the
nR-dimensional complex unit sphere, which can be achieved according to Lemma 10
16
Finally, the SISO case is a combination of the arguments of the SIMO and MISO
case, i.e., using
hλ
(
eiΘ
)
= log 2π (4.12)
we get
χ(H) = log 2π + E
[
log |H|2
]
− log 2− h(H) (4.13)
= log π + E
[
log |H|2
]
− h(H). (4.14)
4.2 Gaussian Fading
The evaluation of the fading number is rather difficult even for the usually simpler
situation of Gaussian fading processes. However, we are able to give the exact value
for some important special cases, and we will give bounds on some others.
Throughout this section we assume that the fading matrix H can be written as
H = D + H˜ (4.15)
where all components of H˜ are independent of each other and zero-mean, unit-
variance Gaussian distributed, and where D denotes a constant line-of-sight matrix.
Note that for some constant unitary nR×nR matrix U and some constant unitary
nT × nT matrix V the law of UH˜V is identical to the law of H˜. Therefore, without
loss of generality, we may restrict ourselves to matrices D that are “diagonal”, i.e.,
for nR ≤ nT,
D =
(
D˜ 0nR×(nT−nR)
)
(4.16)
or, for nR > nT,
D =
(
D˜
0(nR−nT)×nT
)
(4.17)
where D˜ is a min{nR, nT} ×min{nR, nT} diagonal matrix with the singular values
of D on the diagonal.
4.2.1 Scalar Line-of-Sight Matrix
We start with a scalar line-of-sight matrix, i.e., we assume D˜ = dI where I denotes
the identity matrix.
Under these assumptions the fading number has been known already for nR =
nT = m, in which case the fading matrix H is rotation commutative [7], [2]:
χ(H) = mgm
(
|d|2
)
−m− log Γ(m). (4.18)
Here gm(·) is a continuous, monotonically increasing, concave function defined as
gm(ξ) ,


log(ξ)− Ei(−ξ) +
m−1∑
j=1
(−1)j
[
e−ξ(j − 1)!
−
(m− 1)!
j(m− 1− j)!
](
1
ξ
)j
, ξ > 0
ψ(m), ξ = 0
(4.19)
for m ∈ N, where Ei(·) denotes the exponential integral function defined as
Ei(−x) , −
∫ ∞
x
e−t
t
dt, x > 0 (4.20)
18
where H˜ ∼ NC(0, InR). Hence,
h
(
HXˆ
∣∣ Xˆ) = h(H˜) = nR log πe; (4.30)
nRE
[
log ‖HXˆ‖2
]
= nRgnR
(
|d|2‖Ξ‖2
)
≤ nRgnR
(
|d|2
)
; (4.31)
hλ
(
HXˆ
‖HXˆ‖
)
≤ log
2πnR
Γ(nR)
. (4.32)
Here, the equality in (4.31) follows from the fact that ‖dΞ + H˜‖2 is non-central
chi-square distributed and from (4.22); the inequality in (4.31) follows from the
monotonicity of gm(·) and is tight if ‖Ξ‖ = 1, i.e., Ξ′ = 0; and the inequality in
(4.32) follows from (2.9) and (2.10) and is tight if Ξ is uniformly distributed on the
unit sphere in CnR so that HXˆ is isotropically distributed. The result now follows
from Theorem 8.
The case nR > nT is more difficult since then (4.32) is in general not tight. We
will only state an upper bound:
Proposition 12. Assume nR > nT and a Gaussian fading matrix as given in (4.15).
Let the line-of-sight matrix D be given as
D = d
(
InT
0(nR−nT)×nT
)
. (4.33)
Then
χ(H) ≤ nT log
(
1 +
|d|2
nT
)
+ nR log nR − nR − log Γ(nR). (4.34)
Proof. This result is a special case of Proposition 14 below (see also [7, (128)], [2,
(6.224)]).
4.2.2 General Line-of-Sight Matrix
Next we assume Gaussian fading as defined in (4.15) with a general line-of-sight
matrix D having singular values d1, . . . , dmin{nR,nT}. Hence, D˜, defined in (4.16) and
(4.17), is given as
D˜ = diag
(
d1, . . . , dmin{nR,nT}
)
(4.35)
where |d1| ≥ |d2| ≥ . . . ≥ |dmin{nR,nT}| > 0.
We again start with the case nR ≤ nT.
Corollary 13. Assume nR ≤ nT and a Gaussian fading matrix as given in (4.15).
Let the line-of-sight matrix D have singular values d1, . . . , dnR, where |d1| ≥ |d2| ≥
. . . ≥ |dnR | > 0. Then
χ(H) ≤ nRgnR
(
‖D‖2
)
− nR − log Γ(nR) (4.36)
where gm(·) is given in (4.19) and where ‖D‖
2 = |d1|
2.
Proof. A proof is given in Appendix A.3.
The situation nR > nT is again more complicated. We include this case in a new
upper bound based on (3.12) which holds independently of the particular relation
between nR and nT:
20
Chapter 5
Proof of the Main Result
The proof of Theorem 8 consists of two parts: firstly we derive an upper bound to
the fading number assuming an average-power constraint (2.22) on the input. The
key ingredients here are the preliminary results from Section 3.1.
In a second part we then show that this upper bound can actually be achieved
by an input that satisfies the peak-power constraint (2.21). Since a peak-power
constraint is more restrictive than the corresponding average-power constraint, the
theorem follows.
Because the proof is rather technical, we will give a short overview to clarify the
main ideas.
The upper bound relies strongly on Lemma 4 which says that the input can
be assumed to take on large values only, i.e., at high SNR the additive noise will
become negligible so that we can bound
I(X;Y) / I(X;HX). (5.1)
This term is then split into a term that only considers the magnitude of HX and a
term that takes into account the direction:
I(X;HX) = I(X; ‖HX‖) + I
(
X;
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖
)
. (5.2)
For the first term—which is related to MISO fading—we then use the bounding
technique of Lemma 5.
Because Lemma 4 only holds in the limit when E tends to infinity, we introduce
an event ‖X‖2 > E0 for some fixed E0 ≥ 0 and condition everything on this event.
To derive a lower bound on capacity we choose a specific input distribution of
the form
X = R · Xˆ (5.3)
where the distribution of R is such that it achieves the fading number of a SIMO
fading channel and where the distribution of Xˆ is independent of R and will be only
specified at the very end of the derivation (it will be chosen to maximize the fading
number). We then split the mutual information into two terms:
I(X;Y) = I
(
R;Y
∣∣ Xˆ)+ I(Xˆ;Y). (5.4)
The first term (almost) corresponds to a SIMO fading channel with side-information
for which the fading number is known. The second term is treated separately.
22
= I
(
X;HX
∣∣E = 1) (5.20)
= I
(
X; ‖HX‖,
HX
‖HX‖
∣∣∣∣ E = 1
)
(5.21)
= I
(
X; ‖HX‖,
HXˆ
‖HXˆ‖
∣∣∣∣∣ E = 1
)
(5.22)
= I
(
X; ‖HX‖
∣∣E = 1)
+ I
(
X;
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
(5.23)
≤ I
(
X; ‖HX‖, eiΘ
∣∣E = 1)
+ I
(
X;
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
(5.24)
= I
(
X; ‖HX‖eiΘ
∣∣E = 1)
+ I
(
X;
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
. (5.25)
Here, (5.17) follows from adding an additional random vector Z to the argument
of the mutual information; the subsequent equality from subtracting the known
vector Z from Y; the subsequent two equalities follow from the chain rule and the
independence between the noise and all other random quantities; then we split HX
into magnitude and direction vector and use the chain rule again; (5.24) follows
from adding a random variable to mutual information: we introduce eiΘ that is
independent of all the other random quantities and that is uniformly distributed on
the complex unit circle; and the last equality holds because from ‖HX‖eiΘ we can
easily get back ‖HX‖ and eiΘ .
We next apply Lemma 5 to the first term in (5.25), i.e., we choose S = X and
T = ‖HX‖eiΘ . Note that we need to condition everything on the event E = 1:
I
(
X; ‖HX‖eiΘ
∣∣E = 1)
≤ −h
(
‖HX‖eiΘ
∣∣X, E = 1)+ log π + α log β
+ log Γ
(
α,
ν
β
)
+ (1− α)E
[
log
(
‖HX‖2 + ν
) ∣∣ E = 1]
+
1
β
E
[
‖HX‖2
∣∣ E = 1]+ ν
β
(5.26)
where α, β > 0, and ν ≥ 0 can be chosen freely, but must not depend on X.
Notice that from a conditional version of Lemma 2 with m = 1 follows that
h
(
‖HX‖eiΘ
∣∣X = x, E = 1)
= hλ
(
eiΘ
∣∣X = x, E = 1)+ h(‖HX‖ ∣∣ eiΘ ,X = x, E = 1)
+ E[ log ‖HX‖ | X = x, E = 1] (5.27)
= log 2π + h
(
‖HX‖
∣∣X = x, E = 1)
+ E[ log ‖HX‖ | X = x, E = 1] (5.28)
where we have used that eiΘ is independent of all other random quantities and
uniformly distributed on the unit circle. Taking the expectation over X conditional
24
Plugging (5.32), (5.37), and (5.41) into (5.26) we yield
I
(
X; ‖HX‖eiΘ
∣∣E = 1)
≤ − log 2− h
(
‖HXˆ‖
∣∣ Xˆ, E = 1)− 2E[ logR | E = 1]
− E
[
log ‖HXˆ‖
∣∣∣ E = 1]+ α log β + log Γ(α, ν
β
)
+ (1− α)E
[
log ‖HX‖2
∣∣ E = 1]+ ǫν
+
1
β
sup
xˆ
E
[
‖Hxˆ‖2
] E
p
+
ν
β
. (5.45)
Next we continue with the second term in (5.25):
I
(
X;
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
= hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HXˆ‖ ·R, Xˆ, R,E = 1
)
(5.46)
= hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HX‖, E = 1
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HXˆ‖, Xˆ, R,E = 1
)
(5.47)
≤ hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ E = 1
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HXˆ‖, Xˆ, E = 1
)
. (5.48)
Here, the last inequality follows because conditioning cannot increase entropy and
because given Xˆ and ‖HXˆ‖, the term HXˆ/‖HXˆ‖ does not depend on R.
Hence, using (5.48), (5.45), and (5.25) in (5.13) we get
I(X;Y)
≤ Hb(p) + (1− p)C(E0)− log 2− h
(
‖HXˆ‖
∣∣ Xˆ, E = 1)
− 2E[ logR | E = 1]− E
[
log ‖HXˆ‖
∣∣∣ E = 1]+ α log β
+ log Γ
(
α,
ν
β
)
+ (1− α)E
[
log ‖HX‖2
∣∣ E = 1]
+ ǫν +
1
β
sup
xˆ
E
[
‖Hxˆ‖2
] E
p
+
ν
β
+ hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ E = 1
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ ‖HXˆ‖, Xˆ, E = 1
)
(5.49)
= Hb(p) + (1− p)C(E0)− log 2− h
(
HXˆ
∣∣ Xˆ, E = 1)
+ (2nR − 1)E
[
log ‖HXˆ‖
∣∣∣ E = 1]− 2E[ logR | E = 1]
− E
[
log ‖HXˆ‖
∣∣∣ E = 1]+ α log β + log Γ(α, ν
β
)
26
+ nRE
[
log ‖HXˆ‖2
]
+ log Γ
(
α,
ν
β
)
+
1
β
sup
xˆ
E
[
‖Hxˆ‖2
] E
p
+
ν
β
+ ǫν +Hb(p)
+ (1− p)C(E0) + α
(
log β − log E0 − ξ
)}
− log
(
1 + log
(
1 +
E
σ2
))}
(5.59)
= lim
E↑∞
{
sup
Q
Xˆ
{
hλ
(
HXˆ
‖HXˆ‖
)
− h
(
HXˆ
∣∣ Xˆ)− log 2
+ nRE
[
log ‖HXˆ‖2
]}
+ log Γ
(
α,
ν
β
)
+
1
β
sup
xˆ
E
[
‖Hxˆ‖2
] E
p
+
ν
β
+ ǫν +Hb(p)
+ (1− p)C(E0) + α
(
log β − log E0 − ξ
)
− log
(
1 + log
(
1 +
E
σ2
))}
(5.60)
= sup
Q
Xˆ
{
hλ
(
HXˆ
‖HXˆ‖
)
− h
(
HXˆ
∣∣ Xˆ)− log 2
+ nRE
[
log ‖HXˆ‖2
]}
+ lim
E↑∞
{
log Γ
(
α,
ν
β
)
− log
1
α
+
1
β
sup
xˆ
E
[
‖Hxˆ‖2
] E
p
+
ν
β
+ ǫν +Hb(p)
+ (1− p)C(E0) + α
(
log β − log E0 − ξ
)
+ log
1
α
− log
(
1 + log
(
1 +
E
σ2
))}
(5.61)
= sup
Q
Xˆ
{
hλ
(
HXˆ
‖HXˆ‖
)
− h
(
HXˆ
∣∣ Xˆ)+ nRE[log ‖HXˆ‖2]
− log 2
}
+ log
(
1− e−ν
)
+ ν + ǫν − log ν. (5.62)
Here the first two equalities follows from the definition of the fading number (2.24);
the subsequent inequality from (5.52); (5.60) follows because the parameters α, β,
and ν must not depend on the input distribution Q
Xˆ
(however, note that we are
allowed to let them depend on E); the subsequent equality follows since the first four
terms do not depend on E ; and in the last equality we have used (5.15) and made
the following choices on the free parameters α and β:
α , α(E) =
ν
log E + log supxˆ E[‖Hxˆ‖
2]
(5.63)
28
Here we have introduced a new random variable Θ ∼ U ([0, 2π]) which is assumed
to be independent of every other random quantity.
The last two terms can be rearranged as follows:
−I
(
R;YeiΘ
∣∣ Xˆ)+ I(R;Y ∣∣ Xˆ)
= −h
(
YeiΘ
∣∣ Xˆ)+ h(YeiΘ ∣∣ Xˆ, R)+ h(Y ∣∣ Xˆ)
− h
(
Y
∣∣ Xˆ, R) (5.79)
= −h
(
YeiΘ
∣∣ Xˆ)+ h(YeiΘ ∣∣ Xˆ, R)+ h(YeiΘ ∣∣ Xˆ, eiΘ)
− h
(
YeiΘ
∣∣ Xˆ, R, eiΘ) (5.80)
= −I
(
eiΘ ;YeiΘ
∣∣ Xˆ)+ I(eiΘ ;YeiΘ ∣∣ Xˆ, R). (5.81)
Here the second equality follows because eiΘ is independent of everything else so that
we can add it to the conditioning part of the entropy without changing its values,
and because differential entropy remains unchanged if its argument is multiplied by
a constant complex number of magnitude 1.
Combining this with (5.78) we yield
C(E) ≥ I
(
Xˆ;Y
)
+ I
(
R, eiΘ ;YeiΘ
∣∣ Xˆ)− I(eiΘ ;YeiΘ ∣∣ Xˆ) (5.82)
= I
(
Xˆ;Y
)
+ I
(
ReiΘ ;YeiΘ
∣∣ Xˆ)− I(eiΘ ;YeiΘ ∣∣ Xˆ) (5.83)
where the last equality follows because from ReiΘ the random variables R and eiΘ
can be gained back.
We continue with bounding the first term in (5.83):
I
(
Xˆ;Y
)
= I
(
Xˆ;Y,Z
)
− I
(
Xˆ;Z
∣∣Y)︸ ︷︷ ︸
≤ǫ(xmin)
(5.84)
≥ I
(
Xˆ;Y,Z
)
− ǫ(xmin) (5.85)
= I
(
Xˆ;HXˆR
)
− ǫ(xmin) (5.86)
= I
(
Xˆ;
HXˆ
‖HXˆ‖
, ‖HXˆ‖ ·R
)
− ǫ(xmin) (5.87)
= I
(
Xˆ;
HXˆ
‖HXˆ‖
)
+ I
(
Xˆ; ‖HXˆ‖ ·R
∣∣∣∣∣ HXˆ‖HXˆ‖
)
− ǫ(xmin). (5.88)
Here the first equality follows from the chain rule; in the subsequent inequality we
lower-bound the second term by −ǫ(xmin) which is defined in Appendix A.5 and is
shown there to be independent of the input distribution QX and to tend to zero as
xmin ↑ ∞; in the subsequent equality we use Z in order to extract HXˆR from Y and
then drop (Y,Z) since given HXˆR it is independent of the other random variables;
and the last equality follows again from the chain rule.
Similarly, we bound the third term in (5.83):
I
(
eiΘ ;YeiΘ
∣∣ Xˆ)
≤ I
(
eiΘ ;YeiΘ ,ZeiΘ
∣∣ Xˆ) (5.89)
= I
(
eiΘ ;HXeiΘ ,ZeiΘ
∣∣ Xˆ) (5.90)
= I
(
eiΘ ;HXeiΘ
∣∣ Xˆ)+ I(eiΘ ;ZeiΘ ∣∣HXeiΘ , Xˆ) (5.91)
= I
(
eiΘ ;HXeiΘ
∣∣ Xˆ) (5.92)
30
Here, the inequality follows from conditioning that reduces entropy; and the second
last equality holds because we have assumed Xˆ to be circularly symmetric, i.e., Xˆ
“destroys” the random phase shift of eiΘ .
Therefore, we are left with the following bound:
C(E) ≥ I
(
ReiΘ ;YeiΘ
∣∣ Xˆ)+ I
(
Xˆ;
HXˆ
‖HXˆ‖
)
− I
(
eiΘ ;
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
− ǫ(xmin). (5.102)
Now, we rewrite the second and third term as follows:
I
(
Xˆ;
HXˆ
‖HXˆ‖
)
− I
(
eiΘ ;
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
= hλ
(
HXˆ
‖HXˆ‖
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ Xˆ
)
− hλ
(
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
+ hλ
(
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ, eiΘ
)
(5.103)
= hλ
(
HXˆ
‖HXˆ‖
)
− hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ Xˆ
)
− hλ
(
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
+ hλ
(
HXˆ
‖HXˆ‖
∣∣∣∣∣ Xˆ
)
(5.104)
= hλ
(
HXˆ
‖HXˆ‖
)
− hλ
(
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
(5.105)
where the second equality follows from (2.8) with a choice U = e−iθInR and from the
fact that eiΘ is independent of all other random quantities.
This leaves us with
C(E) ≥ I
(
ReiΘ ;YeiΘ
∣∣ Xˆ)+ hλ
(
HXˆ
‖HXˆ‖
)
− hλ
(
HXˆ
‖HXˆ‖
eiΘ
∣∣∣∣∣ Xˆ
)
− ǫ(xmin). (5.106)
We next let the power grow to infinity E → ∞ and use the definition of the fading
number (2.24). Since ReiΘ is circularly symmetric with a magnitude distributed
according to (5.72), we know from [7, (108) and Th. 4.8], [2, (6.194) and Th. 6.15],
that ReiΘ achieves the fading number of a memoryless SIMO fading channel with
partial side-information. In our situation we have
I
(
ReiΘ ;YeiΘ
∣∣ Xˆ) = I(ReiΘ ;HXˆReiΘ + Z ∣∣ Xˆ) (5.107)
= I
(
ReiΘ ;HXˆReiΘ + Z, Xˆ
)
(5.108)
where Xˆ serves as partial receiver side-information (that is independent of the SIMO
input ReiΘ). Note that a random vector A is said to contain only partial side-
information about B if h(B|A) > −∞, i.e., in our case we need
h
(
HXˆ
∣∣Xˆ) > −∞ (5.109)
32
Chapter 6
Discussion & Conclusion
We have derived the fading number of a MIMO fading channel of general fading
law including spatial, but without temporal memory. Since the fading number is
the second term after the double-logarithmic term of the high-SNR expansion of
channel capacity, this means that we have precisely specified the behavior of the
channel capacity asymptotically when the power grows to infinity. The result shows
that the asymptotic capacity can be achieved by an input that consists of the product
of two independent random quantities: a circularly symmetric random unit vector
(the direction) and a non-negative (i.e., real) random variable (the magnitude). The
distribution of the random direction is chosen such as to maximize the fading number
and therefore depends on the particular law of the fading process. The non-negative
random variable is such that (3.9) is satisfied. This is the well-known choice that
also achieves the fading number in the SISO and SIMO case and is also used in the
MISO case where it is multiplied by a constant beam-direction xˆ. All these special
cases follow nicely from this new result.
We have then derived some new results for the important special situation of
Gaussian fading. For the case of a scalar line-of-sight matrix (4.26) assuming at
least as many transmit as receive antennas nR ≤ nT we have been able to state the
fading number precisely:
χ = nRgnR
(
|d|2
)
− nR − log Γ(nR) (6.1)
where gm(·) denotes the expected value of a non-central chi-square random variable
(see (4.19)). We see that the asymptotic capacity only depends on the number of
receive antennas and is growing proportionally to nR log |d|
2.
For a general line-of-sight matrix we have shown an upper bound that grows like
min{nR, nT} log δ
2 where δ2 is a certain kind of average of all singular values of the
line-of-sight matrix (see (4.37) and (4.38)).
Once more we would like to emphasize that even though all results on the fading
number are asymptotic results for the theoretical situation of infinite power, they
are still of relevance for finite SNR values: it has been shown that the approximation
C(snr) ≈ log(1 + log(1 + snr)) + χ (6.2)
holds already for moderate values of the SNR. Moreover, since usually the double-
logarithmic term is so slowly growing, the fading number χ will be dominating
over log(1 + log(1 + snr)) for these moderate SNR values. Hence, the threshold
C0 , C(snr0) is closely related to the fading number. We have seen in Conjecture 1
that once the capacity is appreciably above χ + 2 nats, the approximation (6.2) is
likely to be valid and the transmission system in the highly inefficient log log-regime.
34
Appendix A
Some Derivations
A.1 Proof of Lemma 6
Assume that Θ ∼ U ([0, 2π]), independent of every other random quantity. Then
I(X;Y) = I
(
X;Y
∣∣ eiΘ) (A.1)
= I
(
XeiΘ ;YeiΘ
∣∣ eiΘ) (A.2)
= I
(
XeiΘ ;HXeiΘ + Z
∣∣ eiΘ) (A.3)
= I
(
X˜;HX˜+ Z
∣∣ eiΘ) (A.4)
= h
(
HX˜+ Z
∣∣ eiΘ)− h(HX˜+ Z ∣∣ X˜, eiΘ) (A.5)
= h
(
HX˜+ Z
∣∣ eiΘ)− h(HX˜+ Z ∣∣ X˜) (A.6)
≤ h
(
HX˜+ Z
)
− h
(
HX˜+ Z
∣∣ X˜) (A.7)
= I
(
X˜;HX˜+ Z
)
. (A.8)
Here the first equality follows because Θ is independent of every other random quan-
tity; the third equality follows because Z is circularly symmetric; in the subsequent
equality we substitute X˜ = XeiΘ ; and the inequality follows since conditioning
reduces entropy.
Hence, a circularly symmetric input achieves a mutual information that is at
least as big as the original mutual information.
A.2 Derivation of Bounds (4.25)
In this appendix we will derive the bounds (4.25) on gm(·). We start with the upper
bound which follows directly from (4.22) and (4.23) and from Jensen’s inequality:
gm(s
2) = E

log

 m∑
j=1
∣∣Uj + µj∣∣2



 (A.9)
≤ log

 m∑
j=1
E
[∣∣Uj + µj∣∣2]

 (A.10)
= log

 m∑
j=1
(
1 + |µj |
2
) (A.11)
= log(m+ s2). (A.12)
36
and that
ζ2(Xˆ) =
|Xˆ(1)|2
|d1|2
+ · · ·+
|Xˆ(nR)|2
|dnR |
2
+
|Xˆ(nR+1)|2
|d1|2
+ · · ·+
|Xˆ(nT)|2
|d1|2
(A.24)
≥
|Xˆ(1)|2
|d1|2
+ · · ·+
|Xˆ(nT)|2
|d1|2
(A.25)
=
1
|d1|2
(
|Xˆ(1)|2 + · · ·+ |Xˆ(nT)|2
)
(A.26)
=
1
|d1|2
=
1
‖D‖2
(A.27)
where the inequality follows since |d1| ≥ |d2| ≥ . . . ≥ |dnR |.
A.4 Proof of Proposition 14
This upper bound is based on the upper bound given in Corollary 9 for a choice of
B = InT . If nR > nT we choose for A
A , diag
(
a
d1
, . . . ,
a
dnT
, b, . . . , b
)
(A.28)
with
b ,
(
δ2
nT
) nT
2nR
(A.29)
for δ as given in (4.38), and with a such that detA = 1, i.e.,
a ,
(
d1 · · · · · dnT
) 1
nT · b
nT−nR
nT . (A.30)
For such a choice we note that
AHxˆ = a
(
xˆ
0
)
+
(
NC
(
0,
|a|2
|d1|2
)
, . . . ,NC
(
0,
|a|2
|dnT |
2
)
,
NC
(
0, b2
)
, . . . ,NC
(
0, b2
))T
(A.31)
so that
E
[
‖AHxˆ‖2
]
= δ2
(
b2
)nT−nR
nT + (nR − nT)b
2 (A.32)
= nR
(
δ2
nT
)nT
nR
. (A.33)
Hence, using Jensen’s inequality and the fact that detA = 1 we get
nRE
[
log ‖AHxˆ‖2
]
− h
(
AHxˆ
)
≤ nR log E
[
‖AHxˆ‖2
]
− log detA− h
(
Hxˆ
)
(A.34)
= nR log
(
nR
(
δ2
nT
)nT/nR)
− nR log πe. (A.35)
38
and to show that ǫ(xmin) does not depend on the input distribution QX and tends
to zero as xmin tends to infinity.
Such a bound can be found as follows:
I
(
Xˆ;Z
∣∣Y) = h(Z|Y)− h(Z ∣∣Y, Xˆ) (A.48)
≤ h(Z)− h
(
Z
∣∣Y, Xˆ, R) (A.49)
= h(Z)− h
(
Z
∣∣HXˆR+ Z, Xˆ, R) (A.50)
≤ h(Z)− inf
xˆ
inf
r≥xmin
h
(
Z
∣∣Hxˆr + Z) (A.51)
= h(Z)− inf
xˆ
h
(
Z
∣∣Hxˆxmin + Z) (A.52)
= sup
xˆ
I
(
Z;Hxˆxmin + Z
)
(A.53)
= sup
xˆ
I
(
Z
xmin
;Hxˆ+
Z
xmin
)
(A.54)
= sup
xˆ
{
h
(
Hxˆ+
Z
xmin
)
− h
(
Hxˆ
)}
(A.55)
, ǫ(xmin) (A.56)
where we have used the fact that we have chosen R such that R ≥ xmin. Note that
(A.55) does not depend on the input X anymore. The convergence
lim
xmin↑∞
ǫ(xmin) = 0 (A.57)
follows from [7, Lem. 6.11], [2, Lem. A.19].
40
[12] ——, “On the fading number of multi-antenna systems,” in Proceedings IEEE
Information Theory Workshop (ITW), Cairns, Australia, September 2–7, 2001,
pp. 110–111.
[13] ——, “Convex-programming bounds on the capacity of flat-fading channels,”
in Proceedings IEEE International Symposium on Information Theory (ISIT),
Washington DC, USA, June 24–29, 2001, p. 52.
[14] ——, “Limits on reliable communication over flat-fading channels,” in Proceed-
ings Winter School on Coding and Information Theory, Schloss Reisensburg,
Gu¨nzburg, University of Ulm, Germany, December 17–20, 2000.
[15] A. Lapidoth, “On the asymptotic capacity of stationary Gaussian fading chan-
nels,” IEEE Transactions on Information Theory, vol. 51, no. 2, pp. 437–446,
February 2005.
[16] T. Koch and A. Lapidoth, “Degrees of freedom in non-coherent stationary
MIMO fading channels,” in Proceedings Winter School on Coding and Infor-
mation Theory, Bratislava, Slovakia, February 20–25, 2005, pp. 91–97.
[17] ——, “The fading number and degrees of freedom in non-coherent MIMO fading
channels: a peace pipe,” in Proceedings IEEE International Symposium on
Information Theory (ISIT), Adelaide, Australia, September 4–9, 2005, pp. 661–
665.
[18] A. Lapidoth, “On the high SNR capacity of stationary Gaussian fading chan-
nels,” in Proceedings Forty-First Allerton Conference on Communication, Con-
trol and Computing, Allerton House, Monticello, IL, USA, October 1–3, 2003,
pp. 410–419.
[19] T. Koch, “On the asymptotic capacity of multiple-input single-output fading
channels with memory,” Master’s thesis, Signal and Information Processing
Laboratory, ETH Zurich, Switzerland, April 2004, supervised by Prof. Dr. Amos
Lapidoth.
[20] A. Lapidoth and S. M. Moser, “On non-coherent fading channels with feed-
back,” in Proceedings Winter School on Coding and Information Theory,
Bratislava, Slovakia, February 20–25, 2005, pp. 113–118.
[21] A. Lapidoth, “On the high-SNR capacity of noncoherent networks,” IEEE
Transactions on Information Theory, vol. 51, no. 9, pp. 3025–3036, Septem-
ber 2005.
[22] ——, “On the capacity of non-coherent fading networks,” in Proceedings Third
Joint Workshop on Communications and Coding (JWCC), Donnini-Firenze,
Italy, October 14–17, 2004.
[23] ——, “Capacity bounds via duality: a phase noise example,” in Proceedings
Second Asian-European Workshop on Information Theory, Breisach, Germany,
June 26–29, 2002.
[24] ——, “On phase noise channels at high SNR,” in Proceedings IEEE Information
Theory Workshop (ITW), Bangalore, India, October 20–25, 2002, pp. 1–4.
42
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報
告 
                                                          96 年 07 月  02 日 
報告人姓名  
莫詩台方 
 
服務機構 
及職稱 
 
交通大學電信工程學系 
 
     時間 
會議 
     地點 
2007-6/24-29; 法國尼斯 本會核定 
補助文號 
NSC 95-2221-E-009-046 
會議 
名稱 
 (中文)2007國際電機電子協會 資訊技術國際研討會 
(英文) 2007 IEEE International Symposium on Information Theory(ISIT2007) 
發表 
論文 
題目 
 (中文) 
(英文) The Fading Number of Multiple-Input Multiple-Output Fading Channels with Memory 
報告內容應包括下列各項： 
一、參加會議經過 process 
 
The International Symposium on Information Theory (ISIT) is the world most famous 
and best yearly conference in the field of information theory. The ISIT was organized 
in nine parallel sessions during four full and one half day. Each session was centered 
on one of the various hot topics in information theory. In addition there was a plenary 
speech every morning given by some of the world’s best researcher in the field: 
Michelle Effros (Caltech), Shlomo Shamai (Technion), H. Vincent Poor (Princeton), 
and Emery Brown (MIT/Harvard). Moreover, the famous Shannon Lecture was hold 
by the recipient of the Shannon Award 2006, Sergio Verdú. The Shannon award 
represents the highest honor any researcher can achieve in the field of information 
theory. Additional events included the Awards Luncheon, and the banquet. 
 
 
二、與會心得 review/gain 
 
There are two main purposes when attending an international high-quality 
conference: firstly, one is given the great opportunity to present the newest own 
results and discuss them with other researchers from around the world. This is a very 
fruitful process leading to new ideas and solutions to problems that one could not 
solve beforehand. Secondly, one attends many talks of outstanding researchers and 
附
件
三 
The Fading Number of Multiple-Input
Multiple-Output Fading Channels with Memory
Stefan M. Moser
Department of Communication Engineering
National Chiao Tung University (NCTU)
Hsinchu, Taiwan
Email: stefan.moser@ieee.org
Abstract—The fading number of a general (not necessarily
Gaussian) regular multiple-input multiple-output (MIMO) fading
channel with arbitrary temporal and spatial memory is derived.
The channel is assumed to be non-coherent, i.e., neither receiver
nor transmitter have knowledge about the channel state, but
they only know the probability law of the fading process. The
fading number is the second term in the asymptotic expansion
of channel capacity when the signal-to-noise ratio (SNR) tends
to infinity.
It is shown that the fading number can be achieved by an input
that is the product of two independent processes: a stationary
and circularly symmetric direction- (or unit-) vector process
whose distribution needs to be chosen such that it maximizes
the fading number, and a non-negative magnitude process that
is independent and identically distributed (IID) and that escapes
to infinity.
Additionally, in the more general context of an arbitrary
stationary channel model satisfying some weak conditions on the
channel law, it is shown that the optimal input distribution is
stationary apart from some edge effects.
I. INTRODUCTION
In recent years there has been an ever increasing interest in
the fundamental theoretical understanding of wireless mobile
communication systems, and in particular in the channel
capacity which gives an ultimate limit on the information rate
that can be transmitted reliably over these channels if we do
not constrain delay and computing complexity.
Unfortunately, it turns out that the capacity, especially in the
high signal-to-noise ratio (SNR) regime, is highly sensitive
to some of the basic assumptions made in the modeling of
the channel. For example, there is a tremendous difference in
the high-SNR capacity depending on the assumptions made
about the channel state information that is directly or indirectly
available to the receiver. If the channel state is perfectly
known to the receiver (coherent detection), the capacity grows
logarithmically in the SNR similar to the situation without
fading [1]. If the channel state is not available directly, but
needs to be estimated by the receiver based on the received
sequence of channel output symbols (non-coherent detection),
the capacity depends highly on the assumptions made about
the fading process: for regular fading1 the capacity grows
only double-logarithmically in the SNR [2], [3], i.e., at high
SNR these channels become extremely power-inefficient in the
1For a mathematical definition of regularity see Section II.
sense that for every additional bit capacity the SNR needs to
be squared or, respectively, on a dB-scale the SNR needs to
be doubled! For non-regular Gaussian fading the high-SNR
behavior of capacity depends on the specific power spectral
density and can be anything between the logarithmic and the
double-logarithmic growth [4].
In an attempt to specify the threshold between the efficient
low- to medium-SNR regime and the highly inefficient high-
SNR regime of regular fading channels, [3], [5] define the
fading number χ as the second term in the high-SNR asymp-
totic expansion of capacity, i.e., the capacity at high SNR can
be written as
C(SNR) = log log SNR + χ+ o(1) (1)
where o(1) denotes some terms that tend to zero as SNR →∞.
We define high-SNR to be the region where the o(1)-terms in
(1) are negligible. Note that due to the extremely slow growth
of log log SNR, the fading number is usually the dominant
term in the lower range of the high-SNR regime. Hence, it is
of great practical interest to have a system with large fading
number.
So far, the fading number has been successfully derived in
some special cases only: the case of single-input multiple-
output (SIMO) fading channels with memory has been solved
in [6], [5], the fading number of memoryless multiple-input
single-output (MISO) fading channels has been derived in [3],
[5], and very recently the memoryless multiple-input multiple-
output (MIMO) case was solved in [7].
In this paper we present the fading number for the remaining
cases of MISO and MIMO fading channels with memory.
The rest of this paper is structured as follows: after some
remarks about notation we will define the channel model in the
following section. In Section III we give some auxiliary results
that are interesting also in a more general context. Section IV
contains the main result and an outline of the proof. Before
we conclude in Section VI we specialize the main result to
some interesting cases in Section V.
We will often split a complex vector v ∈ Cm up into its
magnitude ‖v‖ and its direction vˆ , v‖v‖ where we reserve
this notation exclusively for unit vectors, i.e., throughout
the paper every vector carrying a hat, vˆ or Vˆ, denotes a
(deterministic or random, respectively) vector of unit length
‖vˆ‖ = ‖Vˆ‖ = 1. To be able to work with such direction
5) The first η − 1 vectors and the last 2(η − 1) vectors
satisfy the power constraint possibly strictly:
E
[
‖Xℓ‖
2
]
≤ E , ℓ ∈ {1, . . . , η−1}∪{n−2η+3, . . . , n}.
(12)
Proof: The proof is based on a shift-and-mix argument
similar to a proof given in [6] using the fact that a deterministic
zero at the input yields zero information.
Remark 2: Neglecting the edge-effects for the moment,
Theorem 1 basically says that, for every µ ≤ κ, every block
of µ + 1 adjacent vectors has the same distribution indepen-
dent of the time shift. From this immediately follows that
the distribution of every subset of (not necessarily adjacent)
vectors of a µ+1 block does not change when the vectors are
shifted in time (simply marginalize those vectors out that are
not member of the subset). Hence, Theorem 1 almost proves
that the capacity-achieving input distribution is stationary: the
only problems are the edge effects and the fixed (but freely
selectable) value of κ.2
B. Capacity-Achieving Input Distributions and Circular Sym-
metry
The second preliminary remark concerns circular symmetry.
We say that a a vector random process {Wk} is circularly
symmetric if
{Wk}
L
= {Wke
iΘk}, (13)
where L= stands for “equal in law” and where the process
{Θk} is IID ∼ U ([0, 2π]) and independent of {Wk}. Note
that this is not to be confused with isotropically distributed,
which means that a vector has equal probability to point in
every direction.
Remark 3: Note an important subtlety of this definition:
being circularly symmetric does not only imply that for every
time k the corresponding random vector Wk is circularly
symmetric, but also that from past vectors Wk−1−∞ one cannot
gain any knowledge about the present phase, i.e., the phase is
IID.
Lemma 4: Assume a channel as given in (3). Then the
capacity-achieving input process can be assumed to be cir-
cularly symmetric, i.e., the input {Xk} can be replaced by
{Xke
iΘk}, where {Θk} is IID ∼ U ([0, 2π]) and independent
of every other random quantity.
Remark 5: The proof of Lemma 4 relies only on the fact
that the additive noise is assumed to be circularly symmetric.
Hence, for the lemma to hold the noise need not be Gaussian
distributed and may even have memory as long as it is
circularly symmetric.
IV. THE FADING NUMBER OF MIMO FADING CHANNELS
WITH MEMORY
Theorem 6: Consider a MIMO fading channel with mem-
ory (3) where the stationary and ergodic fading process {Hk}
takes value in CnR×nT and satisfies h({Hk}) > −∞ and
2As a matter of fact one can choose κ arbitrarily large, however, note that
the size of the edges where the lemma does not hold depends on κ!
E
[
‖Hk‖2F
]
< ∞. Then, irrespective of whether a peak-power
constraint (5) or an average-power constraint (6) is imposed
on the input, the fading number χ
(
{Hk}
)
is given by
χ
(
{Hk}
)
= sup
Q{Xˆk}
stationary
circ. sym.

hλ

 H0Xˆ0
‖H0Xˆ0‖
∣∣∣∣∣∣
{
HℓXˆℓ
‖HℓXˆℓ‖
}−1
ℓ=−∞


+ nRE
[
log ‖H0Xˆ0‖
2
]
− log 2
− h
(
H0Xˆ0
∣∣ {HℓXˆℓ}−1ℓ=−∞, Xˆ0−∞)

 (14)
where the maximization is over all stochastic unit-vector
processes {Xˆk} that are stationary and circularly symmetric.
Moreover, the fading number is achievable by a stationary
input that can be expressed as a product of two independent
processes:
Xk = Rk · Xˆk, (15)
where {Xˆk} ∈ CnT is a stationary and circularly symmetric
unit-vector process with the probability distribution that max-
imizes (14), and where {Rk} ∈ R+0 is a scalar non-negative
IID random process such that
logR2k ∼ U ([log log E , log E ]) . (16)
Note that this input satisfies the peak-power constraint (5) (and
therefore also the average-power constraint (6)).
Proof: The proof is rather long and technical. We will
give here only an outline. The proof consists of two parts: in
a first part we derive an upper bound on the fading number
assuming an average-power constraint (6) on the channel input.
In a second part we then derive a lower bound on the fading
number by assuming one particular input distribution that
satisfies the peak-power constraint (5). We then show that
both bounds coincide. Since a peak-power constraint is more
restrictive than the corresponding average-power constraint the
theorem follows.
a) Outline of Upper Bound: Similarly to the proof of the
SIMO fading number [6], [5] we use the chain rule to write
1
n
I
(
X
n
1 ;Y
n
1
)
=
1
n
n∑
k=1
I
(
X
n
1 ;Yk
∣∣Yk−11 ) (17)
and then split each term on the RHS of the above into terms
that are memoryless and terms that take care of the memory:
I
(
X
n
1 ;Yk
∣∣Yk−11 )
≤ I
(
Xk;Yk
)
− I

 HkXˆk
‖HkXˆk‖
;
{
HℓXˆℓ
‖HℓXˆℓ‖
}k−1
ℓ=1


+ I
(
HkXˆk; {HℓXˆℓ}
k−1
ℓ=1
∣∣ Xˆn1 ). (18)
Note that in the situation of multiple-antennas at both trans-
mitter and receiver it is not possible to gain full knowledge
about all fading coefficients even if both X and Y are known!
