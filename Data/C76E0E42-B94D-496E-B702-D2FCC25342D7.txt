結合統計與結構之積木式機器翻譯系統與評估之研究 (3/3) 
計劃編號：95-2221-E-007-182-MY3 
執行期限：2008.08.01 至 2009.07.31 
主持人：張俊盛 清華大學資訊工程系 
 
中文摘要  
 
在三年計畫的最後一年，我們發展出一套統計式機
器翻譯系統。此系統透過自動機制，學習基於 
bracket transduction grammar 的文字順序重排
的模型。不同於先前的研究，這個模型強調利用翻
譯中的雙語資料的語言學屬性，包括字塊的字數、
文字、詞性、語意詞群編號等。我們利用這些屬性
來訓練最佳熵值文字重排模型。我們利用這個模
型，結合到片語式、結構化的機器翻譯解碼器，時
做了一套中文到英文機器翻譯系統。實驗結果顯式
這個利用雙語語言學屬性的模型，效能優於最新的
結構化片語式翻譯系統。以  BLEU 評分而言，可
以達到  4.9 – 11.4%。 
 
關鍵詞：統計式機器翻譯、文字重排模型 
 
 
 
Abstract 
In this paper, we propose a method for learning 
reordering model for BTG-based statistical 
machine translation (SMT). The model focuses 
on linguistic features from bilingual phrases. 
Our method involves extracting reordering 
examples as well as features such as 
part-of-speech and word class from aligned 
parallel sentences. The features are classified 
with special considerations of phrase lengths. 
We then use these features to train the maximum 
entropy (ME) reordering model. With the model, 
we performed Chinese-to-English translation 
tasks. Experimental results show that our 
bilingual linguistic model outperforms the 
state-of-the-art phrase-based and BTG-based 
SMT systems by relatively improvements of 
11.4% and 4.9% in BLEU score. 
1 Introduction 
Bracketing Transduction Grammar (BTG) is a 
special case of Synchronous Context Free 
Grammar (SCFG), with binary branching rules 
that are either straight or inverted. BTG is 
widely adopted in SMT systems, because of its 
good trade-off between efficiency and 
expressiveness (Wu, 1996). In BTG, the ratio of 
legal alignments and all possible alignment in a 
translation pair drops drastically especially for 
long sentences, yet it still covers most of the 
syntactic diversities between two languages. 
It is common to utilize phrase translation in 
BTG systems. For example in (Xiong et al., 
2006), source sentences are segmented into 
phrases. Each sequences of consecutive phrases, 
mapping to cells in a CKY matrix, are then 
translated through a bilingual phrase table and 
scored as implemented in (Koehn et al., 2005; 
Chiang, 2005). In other words, their system 
shares the same phrase table with standard 
phrase-based SMT systems.  
 
 3年 前   3年 後  
three   after  !A   
years 
!A  
  three   
ago  2A   years 
!A  
  
(a) (b) 
 
Figure 1: Two reordering examples: (a) straight 
rule applied (b) inverted rule applied 
 
On the other hand, there are various 
proposed BTG reordering models to predict 
correct orientations between neighboring blocks 
(bilingual phrases). In Figure 1, for example, the 
role of reordering model is to predict correct 
orientations of neighboring blocks A1 and A2. In 
flat model (Wu, 1996; Zens et al., 2004; Kumar 
and Byrne, 2005), reordering probabilities are 
assigned uniformly during decoding, and can be 
tuned depending on different language pairs. It 
is clear, however, that this kind of model would 
suffer when the dominant rule is wrongly 
applied. 
Predicting orientations in BTG depending on 
context information can be achieved with 
lexical features. For example, Xiong et al. 
(2006) proposed MEBTG, based on maximum 
entropy (ME) classification with words as 
features. In MEBTG, first words of blocks are 
considered as the features, which are then used 
to train a ME model for predicting orientations 
of neighboring blocks. Xiong et al. (2008b) 
proposed a linguistically annotated BTG 
(LABTG), in which linguistic features such as 
 3 
only when source phrases are not syntactic 
phrases. 
In contrast to the previous works, we present 
a reordering model for BTG that uses bilingual 
information including class-level features of 
POS and word classes. Moreover, our model is 
dedicated to boundary features and considers 
different combinations of phrase lengths, rather 
than only first/tail words. In addition, current 
state-of-the-art Chinese parsers, including the 
one used in LABTG (Xiong et al., 2005), lag 
beyond in inaccuracy, compared with English 
parsers (Klein and Manning, 2003; Petrov and 
Klein 2007). In our work, we only use more 
reliable information such as Chinese word 
segmentation and POS tagging (Ma and Chen, 
2003). 
3 The Model 
Following Wu (1996) and Xiong et al. (2006), 
we implement BTG-based SMT as our system, 
in which three rules are applied during decoding: 
 
[ ]!" AAA!    (1) 
!" AAA!        (2) 
yxA /!        (3) 
 
where A1 and A2 are blocks in source order. 
Straight rule (1) and inverted rule (2) are 
reordering rules. They are applied for predicting 
target-side order when combining two blocks, 
and form the reordering model with the 
distributions 
 
reo
orderAA
!)(P ,,reo !"  
 
where order !{straight, inverted}. 
In MEBTG, a ME reordering model is trained 
using features extracted from reordering 
examples of aligned parallel corpus. First words 
on neighboring blocks are used as features. In 
reordering example (a), for example, the feature 
set is 
 
{"S1L=three", "S2L=ago", "T1L=3", "T2L=前"} 
 
where "S1" and "T1" denote source and target 
phrases from the block A1. 
Rule (3) is lexical translation rule, which 
translates source phrase x into target phrase y. 
We use the same feature functions as typical 
phrase-based SMT systems (Koehn et al., 2005): 
 
654
321
ee)|(
)|()|()|()|(Ptrans
!!!
!!!
y
lw
lw
xyp
yxpxypyxpyx
"""
""=
 
where 43 )|()|( !! xypyxp lwlw " , 
5
e
! and 6e !y  
are lexical translation probabilities in both 
directions, phrase penalty and word penalty.  
During decoding, the blocks are produced by 
applying either one of two reordering rules on 
two smaller blocks, or applying lexical rule (3) 
on some source phrase. Therefore, the score of a 
block A is defined as 
 
reolm
orderAAAA
AAA
!! ),,(P),(P
)P()P()P(
reo21lm
21
!""#"
"=
 
or 
)|(P)(P)P( translm yxAA
lm
!=
"  
 
where lmA !)(Plm  and 
lm
AA
!),(P 21lm"  are 
respectively the usual and incremental score of 
language model. 
Let B be the set of all blocks with source side 
sentence C. Then the best translation of C is the 
target side of the block A , where 
 
)P(argmaxA A
BA!
=  
4 Bilingual Linguistic Model 
In this section, we formally describe the problem 
we want to address and the proposed method. 
Problem Statement 
We focus on extracting features representative 
of the two neighboring blocks being considered 
for reordering by the decoder, as described in 
Section 3. We define S(A) and T(A) as the 
information on source and target side of a block 
A. For two neighboring blocks A1 and A2, the set 
of features extracted from information of them is 
denoted as feature set function F(S(A1), S(A2), 
T(A1), S(A2)). In Figure 1 (b), for example, S(A1) 
and T(A1) are simply the both sides sentences "3 
年" and "three years", and F(S(A1), S(A2), T(A1), 
 5 
{"S1L=P","S2R=Na"}! F(S(A1),S(A2),T(A1), S(A2)) 
 
Therefore, without distinguishing the special 
case, the features would represent quite different 
cases with the same feature, possibly leading to 
failure to predict orientations of two blocks.  
We propose a method to alleviate the problem 
of features with considerations of lengths of two 
adjacent phrases by classifying both the both 
source and target phrase pairs into one of four 
classes: M, L, R and B, corresponding to 
different combinations of phrase lengths. 
Suppose we are given two neighboring blocks 
A1 and A2, with target phrases P1 and P2 
respectively. Then the feature set from target 
side is classified into one of the classes as 
follows. We give examples of feature set for 
each class according to Figure 4. 
 
1. M class. The lengths of P1 and P2 are both 1. 
In Figure 4 (a), for example, the feature set 
is 
{"M1=Nh", "M2=VE"} 
 
2. L class. The length of P1 is 1, and the length 
of P2 is greater than 1. In Figure 4 (b), for 
example, the feature set is 
 
{"L1=P", "L2=Neqa", "L3=Na"} 
 
 基於 P 
這些 原因 
Neqa  Na   
在 約旦 
P    Nc 
舉行 會議 
VC    
Na 
 hold meeting  !A  for 
!A   
 
these 
reasons  2A   
in 
jordan !A   
(a) (b) 
 
Figure 3: Two reordering examples with 
ambiguous features on source side. 
 
3. R class. The length of P1 is greater than 1, 
and the length of P2 is 1. In Figure 4 (c), for 
example, the feature set is 
 
{"R1=Na", "R2=Caa", "R3=Na"} 
 
4. B class. The lengths of P1 and P2 are both 
greater than 1. In Figure 4 (d), for example, 
the feature set is 
 
{"B1=P", "B2=Nc", "B3=VC", "B4=Na"} 
 
 
A1  A2  A1  A2 
我 
Nh 
 認為 
VE 
 基於 
P 
 這些  原因 
    Neqa    
Na 
I  think  for  these  reasons 
(a) M class            (b)  L class 
A1  A2  A1  A2 
技術  和 
Na  Caa 
 設備 
Na 
 在  約旦 
P     
Nc 
 舉行  會議 
      VC      
Na 
technology and  equipment in  Jordan  hold  meeting 
(c) Rclass                (d) B class 
      
Figure 4: Examples of different length 
combinations, mapping to four classes. 
 
We use the same scheme to classify the two 
target phrases. Since both source and target 
words are classified as described in Section 4.2, 
the feature sets are more representative and tend 
to lead to consistent prediction of orientation. 
Additionally, the length-based features are easy 
to fit into memory, in contrast to lexical features 
in MEBTG. 
To summarize, we extract features based on 
word lengths, target-language word classes, and 
fine-grained, semantic oriented parts of speech. 
To illustrate, we use the neighboring blocks 
from Figure 2 to show an example of complete 
bilingual linguistic feature set: 
 
{"S.B1=Nes", "S.B2=Nv", "S.B3=DE", 
"S.B4=Na", "T.B1=14", "T.B2=18", "T.B3=14", 
"T.B4=50"} 
 
where "S." and "T." denote source and target 
sides. 
 
We name the new reordering model trained from 
these new linguistic feature sets MLRB. In the 
next section, we describe the process of 
preparing the feature data and training an ME 
model. In Section 7, we perform evaluations of 
this ME-based reordering model against 
standard phrase-based SMT and previous work 
based on ME and BTG. 
 
 
 7 
 
 
System BLEU-4 
MEBTG 22.78 
MLRB-WC 23.12 
MLRB 23.70 
 
Table 3: Comparisons of three systems. 
(MLRB-WC is the MLRB model without word 
classification) 
 
To test the effectiveness of word classification, 
Table 3 compares MLRB model with 
MLRB-WC model which uses only lexical 
features without classifying words, but considers 
length combinations described in Section 4. The 
results show that we are able to ameliorate the 
sparseness problem by classifying words, and 
produce more representative feature sets by 
considering phrase length. Moreover, these two 
improvements are scalable when performed 
together. 
8 Conclusion ands Future Works 
We have proposed a bilingual linguistic 
reordering model to improve current BTG-based 
SMT systems, based on two drawbacks of 
previously proposed reordering model, which 
are sparseness and representative problem. 
First, to solve the sparseness problem in 
previously proposed lexicalized model, we 
perform word classification on both sides. POS 
tagging are performed on Chinese and a word 
classification toolkit based on 
maximum-likelihood principle is used to classify 
English. 
Secondly, we present a more representative 
feature extraction method. This involves 
considering length combinations of adjacent 
phrases, which are then classified based on 
whether the phrase length is greater than 1. 
The experimental results of 
Chinese-to-English task show that our proposed 
MLRB model outperforms baseline 
phrase-based and BTG systems. The MLRB 
model has two-fold effect on the translation: 
reducing the model size and producing the better 
translations. 
We will investigate more linguistic ways to 
classify words in future work, especially on 
target language. For example, using word 
hierarchical structures in WordNet (Fellbaum, 
1998) system provides more linguistic and 
semantic information than statistically-motivated 
classification tools. 
References   
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In 
Proceedings of ACL 2005, pp. 263-270. 
Christiane Fellbaum, editor. 1998. WordNet: An 
Electronic Lexical Database. MIT Press, 
Cambridge, Massachusetts. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of HLT/NAACL 2003. 
Philipp Koehn. 2004. Pharaoh: a Beam Search  
Decoder for Phrased-Based Statistical Machine 
Translation Models. In Proceedings of AMTA 
2004. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgh System 
Description for the 2005 IWSLT Speech 
Translation Evaluation. In International Workshop 
on Spoken Language Translation. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola 
Bertoldi, Brooke Cowan,Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constrantin, and Evan Herbst. 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proceedings of ACL 2007, 
Demonstration Session. 
Dan Klein and Christopher D. Manning. 2003. 
Accurate Unlexicalized Parsing. In Proceedings of 
ACL 2003. 
Shankar Kumar and William Byrne. 2005. Local 
phrase reordering models for statistical machine 
translation. In Proceedings of HLT-EMNLP 2005. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. 
Introduction to CKIP Chinese Word Segmentation 
System for the First International Chinese Word 
Segmentation Bakeoff. In Proceedings of ACL, 
Second SIGHAN Workshop on Chinese Language 
Processing, pp168-171.  
Franz Josef Och. 1999. An efficient method for 
determining bilingual word classes. In EACL ’99: 
Ninth Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
71–76, Bergen, Norway, June. 
Franz Josef Och and Hermann Ney. 2003. A 
Systematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics, 
29:19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
ACL, pages 311–318. 
