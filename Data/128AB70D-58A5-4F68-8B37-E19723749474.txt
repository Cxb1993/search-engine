This joined project, entitled ’Criti-core: 
Reliability-Central SoC systems Beyond Multicore’ 
will address the following research goals: 
I. Beyond multicore：develop next-generation many-
core architecture, attack those challenges of 
manycore complexity, and consider reliability-centric 
manycore, including scalable interconnection, 
reliable memory hierarchy, fully-debugging support. 
We will also develop a multicore-based SOC ESL 
platform. 
II. Operating Systems platform for multicore: The 
main tasks include development of a power/thermal-
aware multithreading OS and also reentrant kernel, in 
which allows multiple threads to enter kernel at the 
same time. 
III. Develop the system software optimization tools: 
provide primitive speculative multithreading library, 
develop adaptive, reliable, and scalable software-
level power and thermal optimization techniques. 
IV. Develop advanced thermal sensor and power 
management methodology: Power management circuits, 
Thermal sensing and feedback circuits. 
V. Develop advanced application software for 
manycore: Real-time 3-D video player for mobile 
devices, Real-time panoramic video for video 
surveillance, Real-time 3-D panoramic video player 
for advanced video applications. 
英文關鍵詞： Manycore system、highly reliable SoC systems、Power 
management 
 
2 
 
 
 
一、 中、英文摘要 .................................................................................................................................. 3 
 中文摘要 .......................................................................................................................................... 3 
 英文摘要 .......................................................................................................................................... 3 
二、 研究計畫之背景、目的及重要性 .................................................................................................. 5 
三、 相關文獻 .......................................................................................................................................... 9 
A. 超越多核心架構 .............................................................................................................................. 9 
B. 多核心系統軟體平台 .................................................................................................................... 14 
C. 前瞻多核心系統應用軟體 ............................................................................................................ 23 
四、 研究方法 ........................................................................................................................................ 26 
 A novel non-uniform cache architecture based on Single-Cycle Ring ........................................... 26 
 Non-Uniform Debugging Architecture ........................................................................................... 30 
 Criticore Operating System ............................................................................................................ 42 
 A fast and accurate power consumption estimation ....................................................................... 43 
 Thermal Sensor and Power Management Circuit Design ............................................................... 51 
 Design and Implementation of Panoramic Stereo Video Playing System ...................................... 56 
五、 結果與討論 .................................................................................................................................... 80 
 Non-uniform cache architecture ..................................................................................................... 80 
 NUDA Debugging .......................................................................................................................... 84 
 Criticore Operating System ............................................................................................................ 87 
 Power estimation & SpMT ............................................................................................................. 88 
 All-digital Thermal Sensor ............................................................................................................. 92 
 Parallelization of H.264 Video Decoder & 3-D Depth Map Generation ........................................ 97 
 
4 
 
 
develop a multicore-based SOC ESL platform. 
II. Operating Systems platform for multicore: The main tasks include development of a 
power/thermal-aware multithreading OS and also reentrant kernel, in which allows multiple 
threads to enter kernel at the same time. 
III. Develop the system software optimization tools: provide primitive speculative 
multithreading library, develop adaptive, reliable, and scalable software-level power and 
thermal optimization techniques. 
IV. Develop advanced thermal sensor and power management methodology: Power 
management circuits, Thermal sensing and feedback circuits. 
V. Develop advanced application software for manycore: Real-time 3-D video player for 
mobile devices, Real-time panoramic video for video surveillance, Real-time 3-D panoramic 
video player for advanced video applications。 
6 
 
 
設計相呼應之作業系統與系統工具。 
 Multi-core 發展的技術走向與方法分析 
追求高效能一直是處理器發展的目標之一，這類型的設計主要都圍繞在 3 個 P，
他們分別是 Instruction-Level Parallelism (ILP)、Thread-Level Parallelism (TLP) 和
Data-Level Parallelism(DLP)。在 ILP 方面有 superscalar 和 VLIW 這兩大類的設計能夠
有效的提高平行度。 TLP 抑或稱為 Task-Level parallelism 則在單核心時代為處理器提
供了有效的平行模式（ Paradigm），但是，此一模式所伴隨的大量本文切換
（context-switch）與自旋指令（swap or test-and-set）等待卻造成效能與能源上的損失。
有時，錯誤的利用 TLP 甚至會讓平行後的效能低於原本序列（Sequential）執行的程
式。但是在 multi-core 環境上，不同的本文可以依附在不同的核心之上，雖然仍需要
本文切換來達到分時的效果，但是已較單一核心處理器的切換頻率來的低。早期的 DLP 
有 SIMD (Single Instruction Multiple Data)技術來支援。此一技術是利用數個微運算單元
同時對單一筆資料的不同區段（向量）進行運算，Berkeley Univ.的 VIRAM、 Intel 的
MMX 與 AMD 的 3D Now！技術皆屬此列。近年來，OpenMP 與 GPGPU 的出現，為
DLP 提供了另一種良好的高階程式模型去拆解程式中具備高度平行的資料，讓使用者
可以更方便的撰寫 DLP 的程式，亦為一大進步。這樣的概念大約起源於 1990 年代，
此後百家爭鳴，發展出許多不同的 multi-core 架構。 
 Multi-core 架構研究走向 
Many-core 架構一般認為有下列幾項重要因素需要克服，這也造成多核心世代的
許多研究改進的機會。 
 Architecture 
 Cache/memory hierarchy 
 Interconnection 
 Programming model 
 Debugging and development environment 
 OS-Issue & task scheduling 
 Power/thermal management 
分析學術界近年來相關重要會議與對於多核心平台方面的論文研究。我們可以發現
multi-core 技術發展主要的走向就硬體架構上來說，核心數量上除了逐年提昇外，所專注的
重點也從以往改進超純量管線以提昇 ILP 的平行度，推進到以 TLP、DLP 為主的粗略式平
行（Coarse-grand parallels）。一個頗為經典的譬喻：『我們無法利用十個孕婦在一個月內
產下一個嬰兒』，揚棄以高額的硬體成本提高 ILP 以企圖提昇低度平行程式的平行度，而
將注意力轉往本身已具備高度平行但是尚未被平行化的程式上。在核心的 interconnection 
方面，傳統 shared bus 的連線方式會由於核心個數的增加，效能大幅度的下降，如何有效
8 
 
 
上，常會因熱點(hot spots)的效應而顯得特別突出，從而使得電路產生暫時性、甚至永久性
的錯誤，高熱更是長久以來系統穩定度的殺手。因此，溫度的變異是另一個設計必須面對
的困難。而利用軟體如作業系統、函式庫、編譯器來做耗電與散熱的最佳化，由於軟體具
有便於修改與富有彈性等特色，應用在多核心處理器上節能。利用有效的設計作業系統核
心排程器，讓工作之間可以很順暢的執行，而不是互相的等待甚至造成死結。 
總結來說，多核心環境與多處理器架構看似相同，但是在  architecture、  cores 
interconnection 、 cache/memory hierarchy 、 programming model 、 debugging 、
power/thermalmanagement、operation system design 上多有許多的差異，這也造成多核心世
代的許多研究改進的機會。本計畫期望 Criti-core 能夠充分整合上述軟硬體並提出有效且實
用的成果。 
 many-core 的發展與為何要超越 multi-core 
Multi-core processor 在這幾年提供應用程式一個提高效能的好機會，由於製程的進
步，使的單一處理器可容納的核心個數仍持續性的增加，目前摩爾定律已有將電晶體數量
改為核心數的態勢，換句話說，單一商用桌上型電腦處理器上的核心數目將會在 1.5 年內
增加一倍。根據此一增加速率，我們將會在不到十年內遇見超過一百個核心的處理器。Intel 
最近展示一顆實驗室發展中的 80 核心處理器，與 AMD 所發表的 crossfire mode 技術，其
號稱”TeraFlop in a box”將 CPU 和 GPU 結合的策略，已經宣告 many-core 時代的到來。 
在應用方面，除了科學運算具有大量的資料平行度之外，生活上的應用也有許多具有
資料平行度的特性，像是影音壓縮、立體影像計算、物體追蹤、人臉辨識、語音辨識、搜
尋引擎、翻譯服務、手寫辨識等應用。這類型的程式具有 fine-grained parallelism 的特性，
大量平行的資料存在這些應用中，但是每筆資料的運算量卻又不大。由別於過去科學運算，
fine-grained parallelism 中的程式之間會有大量溝通，這些溝通的成本在過去限制了
fine-grained program 的發展。但是 many-core 架構大幅縮小核心之間溝通的成本，這給了
上述應用一個良好的機會。 
在行動通訊設備發展方面，近幾年來的發展除了基本的通訊功能之外，結合高畫素相
機、高階多媒體功能(影片、電視及大容量 MP3 音樂播放)、無線上網和商務需求的高階通
訊設備為我國帶來巨大的產值，台灣的行動電話在 2007 年整體產值約為 2,333 億元，總
產量 1.08 億支，在高階市場方面，單宏達電的 Touch 智慧型手機單一機種在 2007 年就有
100 萬支的銷售量。多媒體行動裝置採用雙核心的架構已經行之有年，像是著名國外大廠
Qualcomm、TI、Freescale 和 Infineon 的架構大都採用 ARM core 和自行開發的 DSP 提供
複雜的多媒體運算。最新發表的產品更是朝向多核心的設計，Qualcomm MSM76XX 系列
除了原本的 ARM9 和 DSP 之外，還外加一個 ARM11，TI OMAP3 更是直接採用 ARM 公
司所推出 Multi-Core 版本的 Cortex-A8 處理器，最高可達到 4 core。在加上一些特殊化圖
形、影像的加速器，整合於 SoC 單一晶片提供高效能、低成本的運算能力。 
10 
 
 
 
 
Figure 1 (a) shows the 3-ported circuit-switched router, which is configured by the BTPC 
routing-path arbiter. Each router supports a set of read/write ports for the processor core or IP 
(namely the tile) conection. In Figure 1(b)(c), the request/status signals from tiles are provided for 
arbitration, and the BTPC arbiter delivers the routing configuration and grant signals after the 
computation. Figure 1 (d)(e) presents a single-cycle (SC) transaction on the clockwise ring, and 
another SC transaction on the counterclockwise ring. In the best case, it allows N SC transactions 
to take place concurrently on the proposed ring interconnection. In addition, this work seeks to 
optimize the interconnection of 4~16 routers in terms of the logic delay (MUX and arbitration) 
and the limited layers in the 3D chip. 
 NUCA (Non-uniform cache architecture) 
Table 2: The comparison of NUCA design in CMP. 
Paper 
Data placement 
policy 
Data movement 
policy 
Data Replication 
policy 
CMP-NUCA [A-20] Addr’s sub-bits 
to the bankset 
Gradual 
migration policy 
- 
CMP-NuRapid [A-2] Core’s own data 
array 
Gradual 
migration policy 
Tag copy → 
Data copy 
Cooperative Caching 
[A-21] 
Core’s own data 
array 
Send victim data 
to other bank 
Sharing of clean 
data 
Victim Replication 
[A-22] 
Core’s own data 
array 
- 
Replicate all 
primary cache 
victims 
ASR [A-23] Core’s own data 
array 
- 
Replicate on 
workload 
demand 
This work 
MMU table 
mapping 
Migrate to dest. 
directly 
Share with 
page’s owner 
BTPC routing-path arbiter (max. N SC_transactions)
Routing
, grant
0
N-3N-1
2
d
1R1W for core 
or IP (Tile)
1
N-2 N/2
e
a
b
Single-cycle (SC) transaction (clockwise)
Req, status
c
Single-cycle (SC) transaction (counterclockwise)
Circuit-
switched
router
 
Figure 1: Conceptual view of the Single-Cycle Ring (SC_Ring). 
12 
 
 
treated as a set-associative, spreading across multiple banks. Beckmann et al. [A-20] proposed the 
behavior of block migration in CMPs using a variant of D-NUCA with Transmission Line Cache 
(TLC) [A-27] adopts transmission line technology to fetch remote blocks. The benefits are 
limited by the tendency for shared data to migrate to the center of the chip. Another design 
separates tag banks from data bank. The private tag bank design reduces the latencies of cache 
access with incremental search policy as shown in Figure 3. Chishti et al. [A-2] introduced a 
CMP-NuRAPID architecture and Chang et al. [A-28] proposed Cooperative Caching to improve 
the drawbacks of the pure shared cache which does not allow replication of read-only sharing 
data in Beckmann et al. [A-20] through private L2 caches and limited replication under certain 
criteria, whereas Victim Replication [A-22] has a shared L2 cache and allows replication. ASR 
[A-23] dynamically monitors workload behavior to control replication. 
 
 
The issue of non-uniform caches recently attracts many attentions in the literature. Basically, 
the approaches for the non-uniform caches can be characterized by the following design aspects: 
 Data Placement Policy: 
Following the traditional cache, the associativity of a set in non-uniform caches determines 
the number of blocks mapped to the same set. However, a number of blocks with various access 
times will be equally allocated in a set to have a fair mapping for all sets. Once a new data block 
is fetched from memory on a miss, it may occupy either the slot in a set which is the closest to the 
 
Figure 3: Architecture of CMP-NuRAPID 
14 
 
 
[A-14] S. Cho, L. Jin, and K. Lee, "Achieving Predictable Performance with On-Chip Shared L2 Caches for 
Manycore-Based Real-Time Systems." pp. 3-11. 
[A-15] M. Kandemir, F. Li, M. Irwin et al., "A novel migration-based NUCA design for chip multiprocessors." 
[A-16] L. Jin, H. Lee, and S. Cho, "A flexible data to L2 cache mapping approach for future multicore processors." 
pp. 92-101. 
[A-17] F. Li, C. Nicopoulos, T. Richardson et al., “Design and management of 3D chip multiprocessors using 
network-in-memory,” ACM SIGARCH Computer Architecture News, vol. 34, no. 2, pp. 130-141, 2006. 
[A-18] G. Loh, Y. Xie, and B. Black, “Processor design in 3D die-stacking technologies,” IEEE Micro, vol. 27, no. 3, 
pp. 31-48, 2007. 
[A-19] V. Pavlidis, and E. Friedman, “3-D topologies for networks-on-chip,” IEEE Transactions on Very Large Scale 
Integration (VLSI) Systems, vol. 15, no. 10, pp. 1081-1090, 2007. 
[A-20] B. Beckmann, and D. Wood, "Managing wire delay in large chip-multiprocessor caches." pp. 319-330. 
[A-21] J. Chang, and G. Sohi, “Cooperative cache partitioning for chip multiprocessors.” 
[A-22] M. Zhang, and K. Asanovic, "Victim replication: Maximizing capacity while hiding wire delay in tiled chip 
multiprocessors." pp. 336-345. 
[A-23] B. Beckmann, M. Marty, and D. Wood, "ASR: Adaptive selective replication for CMP caches." pp. 443-454. 
[A-24] K. Changkyu, B. Doug, and W. K. Stephen, "An Adaptive Cache Structure for Future High-Performance 
Systems," 2007. 
[A-25] Z. Chishti, M. Powell, and T. Vijaykumar, "Distance associativity for high-performance energy-efficient 
non-uniform cache architectures." pp. 55-66. 
[A-26] C. Kim, D. Burger, and S. Keckler, "An adaptive, non-uniform cache structure for wire-delay dominated 
on-chip caches." pp. 211-222. 
[A-27] B. Beckmann, and D. Wood, "TLC: Transmission line caches." 
[A-28] J. Chang, and G. Sohi, “Cooperative caching for chip multiprocessors,” ACM SIGARCH Computer 
Architecture News, vol. 34, no. 2, pp. 264-276, 2006. 
[A-29] H. Song, D. Zhihui, D. Bader et al., "A Prediction Based CMP Cache Migration Policy." pp. 374-381. 
[A-30] S. Wilton, and N. Jouppi, “CACTI: An enhanced cache access and cycle time model,” IEEE Journal of 
Solid-State Circuits, vol. 31, no. 5, pp. 677-688, 1996. 
[A-31] A. Nguyen, M. Michael, A. Sharma et al., “The augmint multiprocessor simulation toolkit for intel x86 
architectures,” Urbana, vol. 51, pp. 61081. 
[A-32] S. Woo, M. Ohara, E. Torrie et al., "The SPLASH-2 programs: Characterization and methodological 
considerations." pp. 24-36. 
[A-33] R. Bagrodia, R. Meyer, M. Takai et al., “Parsec: A parallel simulation environment for complex systems,” 
1998. 
 
B. 多核心系統軟體平台 
 Dynamic voltage and frequency scaling 
這一類的研究主要的目的是以軟體的方式動態調整電壓以及時間計數頻率來達到
執行程式時降低耗電的的要求並且符合程式執行的限制。舉例來說，現代的CPU如 Intel 
的 StrongARM  都有省電模式（power saving modes）的設計，不同的省電模式所需要
的電壓以及時間計數頻率並不相同。而利用軟體主要的技術是利用不同的程式片段具
16 
 
 
Design and Implementation , June 2003. 
[B-10] F. Xie and M. Martonosi and S. Malik. Compile time dynamic voltage scaling settings: Opportunities 
and limits. In Proceedings of the ACM SIGPLAN Conference on Programming Languages Design and 
Implementation , June 2003. 
[B-11] Amitabh Menon, S.K.Nandy, and Mahesh Mehendale. Multivoltage Scheduling with Voltage 
Partitioned Variable Storage. In Proceedings of the International Symposium on Low-Power 
Electronics and Design , August 2003. 
[B-12] A. Azevedo, I. Issenin, R. Cornea, R. Gupta, N. Dutt, A. Veidenbaum, and A. Nicolau. Pro.le-based  
dynamic voltage scheduling using program checkpoints in the COPPER framework. In Proceedings of 
Design,Automation and Test in Europe Conference, March 2002. 
[B-13] L. Chandrasena and M. Liebelt. A rate selection algorithm for quantized undithered dynamic supply 
voltage scaling. In Proceedings of the International Symposium on Low-Power Electronics and 
Design , August 2000. 
[B-14] Grunwald, P. Levis, K. Farkas, C. Morrey III, and M. Neufeld. Policies for dynamic clock scheduling. 
In Proceedings of the 4th Symposium on Operating System Design and Implementation , October 
2000. 
[B-15] T. Ishihara and H. Yasuura. Voltage scheduling problem for dynamically variable voltage processors. In 
International Symposium on Low Power Electronics and Design , pages 197?02, August 1998. 
[B-16] K. Flautner, S. Reinhardt, and T. Mudge. Automatic performance-setting for dynamic voltage scaling. 
In Proceedings of the 7th Annual International Conference on Mobile Computing and Networking, 
July 2001. 
[B-17] J. Lorch and A. Smith. Improving dynamic voltage algorithms with PACE. In Proceedings of the 
International Conference on Measurement and Modeling of Computer Systems , June 2001. 
[B-18] Z. Lu, J. Hein, M. Humphrey, M. Stan, J. Lach, and K. Skadron. Control-theoretic dynamic frequency 
and voltage scaling for multimedia workloads. In Proceedings of the 2002 International Conference on 
Compilers, Architectures, and Synthesis for Embedded Systems, October 2002. 
[B-19] D. Marculescu. On the use of microarchitecture-driven dynamic voltage scaling. In Workshop on 
Complexity-E.ective Design, June 2000. 
[B-20] F. Xie and M. Martonosi and S. Malik. Compile time dynamic voltage scaling settings: Opportunities 
and limits. In Proceedings of the ACM SIGPLAN Conference on Programming Languages Design and 
Implementation , June 2003. 
[B-21] A. Miyoshi, C. Lefurgy, E. Hensbergen, and R. Rajkumar. Critical power slope: Understanding the 
runtime effects of frequency scaling. In Proceedings of the 16th Annual ACM International Conference 
on Supercomputing, June 2002. 
[B-22] D. Moss, H. Aydin, B. Childers, and R. Melhem. Compiler-assisted dynamic power-aware scheduling 
for real-time applications. In Workshop on Compiler and Operating Systems for Low Power , October 
2000. 
[B-23] T. Pering, T. Burd, and R. Brodersen. The simulation and evaluation of dynamic voltage scaling 
algorithms. In Proceedings of 1998 International Symposium on Low Power Electronics and Design , 
pages 76-81,August 1998. 
[B-24] P. Pillai and K. Shin. Real-time dynamic voltage scaling for low-power embedded operating systems. 
18 
 
 
[B-31] J. Hom, and  U. kremer. Energy Management of Virtual Memory on Diskless Devices, Workshop on 
Compilers and Operating Systems for Low Power, Oct. 2000. 
[B-32] T. Heath, E. Pinherio, J. Hom, U. Kremer, and R. Bianchini. Application Transformations for Energy 
and Performance-Aware Device Management, International Conference on Parallel Architectures and 
Compilation Techniques, Sep. 2002. 
[B-33] K. Flautner and T. Mudge. Vertigo: Automatic performance-setting for linux. In Proceedings of the 5th 
Symposium on Operating Systems Design and Implementation, December 2002. 
[B-34] S. Kim and A. Somani. Characterization of an extended multimedia benchmark on a general purpose 
microprocessor architecture. Technical Report DCNL-CA-2000-002, Electrical and Computer 
Engineering Department, Iowa State University, 2000. 
[B-35] R. Rajamony and R. Bianchini. Energy management for server clusters. In Tutorial, 16th Annual ACM 
International Conference on Supercomputing, June 2002. 
 Multi-Threading APIs and Languages 
這類的研究主要是在提供程式設計師程式介面或是平行化的的程式語言，讓程式
設計師以更簡單的方式來寫出多執行緒的程式。在過去寫多執行緒程式需要使用者去
控制每一個 thread 的運作方式，這樣的方式有點麻煩，在程式的編寫上，也會複雜不
少。透過使用這類的 API，程式設計師不太需要像過去那樣需要去寫出那樣複雜的程
式碼，只需要簡單的加入一些程式碼，甚至只需要加入一些 hint，就可以產生出多執
行緒的程式了。 
像是 OpenMP 已經被許多的編譯器所支援，在 C、C++、Fortran 的編譯器上， 
像是 GNU、IBM、Sun Microsystems、Intel、Portland Group Compilers and Tools、Absoft 
Pro FortranMP、Lahey/Fujitsu Fortran 95、PathScale、HP、Microsoft 等。 
這樣降低了平行運算(parallel computing)的難度和複雜度，這樣程式設計師可以把
更多的精力投入到程式目的本身，而非其具體實現細節，將多執行緒的複雜度轉移到
API，甚至編譯器身上，這樣可以。使用這類的 API 可以提供更好的靈活性，甚至使
程式很容易就可以移到其他平台上。 
 
Reference 
 
[B-36] MPIF. MPI: A message passing interface standard. In International Journal of Supercomputer 
Applications, pages 165–416, 1994.  
[B-37] MPIF. MPI-2: Extensions to the Message-Passing Interface. Technical Report, University of Tennessee, 
Knoxville, 1996. 
[B-38] L. Kal´e and S. Krishnan. CHARM++: A portable concurrent object oriented system based on C++. In 
A. Paepcke, editor, Proceedings of OOPSLA’93, pages 91–108. ACM Press, September 1993. 
[B-39] F. Labonte, P. Mattson, I. Buck, C. Kozyrakis, and M. Horowitz. The stream virtual machine. In 
Proceedings of the 2004 International Conference on Parallel Architectures and Compilation 
Techniques, Antibes Juan-les-pins, France, September 2004. 
[B-40] D. Bonachea. Gasnet Specification, V1.1. Technical Report. UMI Order Number: CSD-02-1207., 
University of California at Berkeley. 
[B-41] B. B. Fraguela, J. Guo, G. Bikshandi, M. J. Garzarán, G. Almási, J. Moreira, and D. Padua. The 
20 
 
 
的方式，如 ALU 的計算、register filer 的動作、memory access 的狀況、I/O 的使用等等，
藉由紀錄這些資訊，來計算出電量的使用情形，甚至可以算出每個元件溫度的狀態，
可以提供做為改良的依據，用以修改設計，來得到更好的結果。編譯器專家常以步驟
進行此項最佳化： 
(1) 進行程式分析 
(2) 將應用程式分成區塊 
(3) 給予每一區塊一個電量值 
(4) 以機率的方式推算總共耗電的情形 
 
用軟體的方式來做這方面的評估，主要的優點如下 
 比較簡單 
 成本較低 
 設備需求較少 
 比較有彈性 
 
主要的缺點在於： 
 估量的結果比較不準確 
 並未考慮與硬體同步、溝通的問題 
 
Reference 
 
[B-58] K. M. Buyuksahin, P. Patra, F. N. Najm. ESTIMA: An Architectural-Level Power Estimator for 
Multi-Ported Pipelined Register Files. International symposium on low power electronics and design,      
August 2003. 
[B-59] P. H. Chou, C. Park, J. Park, K. Pham, J. Liu. B#: a Battery Emulator and Power Profiling Instrument. 
International symposium on low power electronics and design, August 2003. 
[B-60] K. Natarajan, H. Hanson, S. W. Keckler, C. R. Moore, D. Burger. Microprocessor Pipeline Energy 
Analysis. International symposium on low power electronics and design, August 2003. 
[B-61] D. Brooks, V. Tiwari, and M. Martonosi. Wattch: A framework for architectural-level power analysis 
and  optimizations. In Proc. Intl Symposium on Computer Architecture, Vancouver, BC, June 2000. 
[B-62] P. Shivakumar and N. P. Jouppi. CACTI 3.0: An integrated cache timing, power and area model. 
Technical report, Compaq Western Research Laboratory, 250 University Avenue Palo Alto, California 
94301 USA, Aug. 2001. 
[B-63] L. Benini, G. Castelli, A. Marcii, E. Macii, M. Poncino, and R. Scarsi. A discrete-time battery model 
for high-level power estimation. In Proceedings of DATE, pages 35–39, 2000. 
[B-64] A. Buyuktosunoglu, S. Schuster, D. Brooks, P. Bose, P. Cook, and D. Albonesi. An adaptive issue 
queue for reduced power at high performance. In Workshop on Power-Aware Computers Systems, held 
in conjunction with ASPLOS, Nov 2000. 
[B-65] R. Desikan, D. Burger, and S. W. Keckler. Measuring experimental error in microprocessor simulation. 
In Proceedings of the 28th Annual Symposium on Computer Architecture, pages 266-277,2001. 
[B-66] M. R. Stan, K. Skadron, M. Barcella, W. Huang, K. Sankaranarayanan, and S. Velusamy. Hotspot: a 
22 
 
 
[B-76] T. AlEnawy and H. Aydin. Energy-aware task allocation for Rate Monotonic scheduling. In 
Proceedings of the 11th IEEE Real Time on Embedded Technology and Applications Symposium, 
pages 213–223, 2005. 
[B-77] S. Park, W. Jiang, Y. Zhou, and S. Adve. Managing energy-performance tradeoﬀs for multithreaded 
applications on multiprocessor architectures. SIGMETRICS Perform. Eval. Rev., 35(1):169–180,2007. 
[B-78] F. Cazorla, P. Knijnenburg, R. Sakellariou, E. Fern´andez, A. Ramirez, and M. Valero. Predictable 
performance in SMT processors: Synergy between the OS and SMTs. IEEE Transactions on 
Computers, 55(7):785–799, 2006. 
[B-79] A. El-Haj-Mahmoud, A.AL-Zawawi, A. Anantaraman,and E. Rotenberg. Virtual multiprocessor: 
ananalyzable, high-performance architecture for real-time computing. In Proceedings of the 2005 
International Conference on Compilers, Architectures and Synthesis for Embedded Systems, pages 
213–224,2005. 
[B-80] A. El-Haj-Mahmoud and E. Rotenberg. Safely exploiting multithreaded processors to tolerate memory 
latency in real-time systems. In Proceedings of the 2004 International Conference on Compilers, 
Architecture, and Synthesis for Embedded Systems, pages 2–13, 2004. 
[B-81] H. Aydin and Q. Yang. Energy-aware partitioning for multiprocessor real-time systems. In Proceedings 
of the 17th International Parallel and Distributed Processing Symposium, Workshop on Parallel and 
Distributed Real-Time Systems, 2003. 
[B-82] C. Isci et al., Run-time power monitoring in high-end processors: Methodology and empirical data," in 
Proc. Int. Symp. Microarchitecture, Dec. 2003. 
[B-83] K.-J. Lee et al., Using performance counters for runtime temperature sensing in high-performance 
processors," in Proc. Wkshp. High-Performance Power-Aware Computing, Apr. 2005. 
[B-84] L. Shang, L.-S. Peh, A. Kumar and N. K. Jha, Thermal modeling, characterization and management of 
on-chip networks," in Proc. Int. Symp. Microarchitecture, Dec. 2004. 
[B-85] K. Skadron et al., \Temperature-aware microarchitecture," in Proc. Int. Symp. Computer Architecture, 
June 2003, pp. 1-12. 
[B-86] A. Weissel et al., \Dynamic thermal management for distributed systems," in Proc. Wkshp. 
Temperature-Aware Computer Systems, June 2004. 
 
 Other optimizations 
其它的的研究較為分散，這些研究議題包含如何減少記憶體、快取記憶體的存取
以減少電力的損耗、快取記憶體的大小對省電的影響、利用 just in time instruction 
delivery 的技術達到省電的目的、利用 buffering 的技術處理程式中 loop 結構以達到省
電的目的、減少 pipelining hazards 以達到省電的目的等，不易歸類。以下僅列相關文
獻做為參考 
 
Reference 
 
[B-87] N. AbouGhazaleh, D. Moss, B. Childers, and R. Melhem. Toward the placement of power management 
points in real time applications. In Proceedings of the Workshop on Compilers and Operating Systems 
for Low Power, September 2001. 
24 
 
 
Thus, it becomes a research issue to generate depth map from a single 2-D image[2-6]. There 
have been some methods proposed to generate the depth map based on a single 2-D image, 
including image classification00, vanishing point detection00, mean shift segmentation0, and 
post-processing method0 using Joint Bilateral Filter (JBF). However, all these methods suffer 
from high computational complexity, which makes them hard to be realized in real-time 
applications. This phenomenon motivates the proposed low complexity depth map generation 
algorithm. In the proposed algorithm, we carefully examine the characteristics of the 2-D images 
and estimate the feasible depth map by the proposed low complexity techniques that are 
optimized with good performance as compared to the methods in the literature [2-6]. With good 
quality in the generated depth map, we achieve about 90% in complexity reduction as compared 
to that using the original techniques. 
With the coming of the age of high definition video, video compression technologies in new 
generation such as VC-100, H.2640, and AVS0, have been developed to replace the traditional 
video compression technologies like MPEG-10, MPEG-20, and MPEG-40. VC-1 is an open 
standard that is standardized by SMPTE (Society of Motion Picture and Television Engineers). Its 
primary technology comes from the WMV-9 (Windows Media Video 9) developed by Microsoft. 
H.264 is a standard standardized by JVT (Joint Video Team) that is composed of ITU-T VCEG 
and ISO/IEC MPEG. The coding efficiency and video quality of VC-1 and H264 are better than 
those of the traditional video standards. AVS is a standard developed by the Audio Video Coding 
Working Group of China. In AVS, AVS-P2 defines the basic video compression for common use. 
For developing mobile applications, AVS has an additional version named AVS-M0. AVS-M is 
also called AVS-P7 since it is fully defined in the part 7 of AVS. In general, the decoding flow 
and main components in AVS are almost the same as those in H.264. However, the complexity of 
AVS is less than that of H.264 but the quality of AVS is close to that of H.26400. Now that the 
traditional video compression technologies are not sufficient to satisfy the request for high 
definition video, video compression technologies in new generation will introduce some features 
that can enhance the quality of compressed video. Firstly, they do the transform in integer mode 
so that the precision problem can be easily handled. Secondly, they employ the in-loop filter to 
eliminate the block effect in order to make the video more elegant. Thirdly, they use more 
complicated interpolation to achieve better quality for the sake of minimizing the error caused by 
the motion prediction. However, it encounters more complicated computation when pursuing 
better video quality. For this reason, we propose a series of software optimization schemes from 
the aspects of both algorithm-level and code-level so as to achieve real-time decoding of the new 
generation videos. In average, we reduce about 80% ~ 90% of complexity after optimization as 
compared to the original reference codes. The proposed low complexity new generation video 
decoders can achieve about CIF@12fps ~ 14fps and QCIF@47 ~ 50fps when running on ARM9 
processor at 200 MHz. 
In the past thirty years, the performance of the processor has increased at a rapid speed by the 
Moore’s Law. However, successive increment of the clock rate causes some problems about 
power dissipation. As a result, the evolution of the processor starts to change and developing 
multi-core processors becomes the new trend. With the advancement in VLSI technology, we can 
26 
 
 
R. O. Duda and P. E. Hart, “Use of the Hough Transformation to Detect Lines and Curves in Pictures”, 
Communications of the ACM, vol. 15, no. 1, pp. 11-15, Jan. 1972. 
Philips 42-inch 3D Display 42-3D6W02. 
http://www.business-sites.philips.com/3dsolutions/home/ 
 
四、 研究方法 
 A novel non-uniform cache architecture based on Single-Cycle Ring 
 System Architecture 
The embedded multicore SoC is critical in memory architecture design for both performance 
and low power advantages. Figure 4 represents the proposed level-1 shared cache architecture. 
Different from coherence-based mechanism, the snoop2 level-1 non-uniform cache architecture 
(Snoop2-NUCA) with single data copy is adopted as a unified L1 shared cache for embedded 
multicore SoC. It not only eliminates snoopy coherence drawbacks but also improves the hit-rate 
of cache by better utilization. 
Figure 4 (a) demonstrates the data access in local cache slice. In snoop protocol, 
cache-to-cache transfer (1-hop) provides fast data communication. However, the main limitation 
of snoop protocol is unnecessary broadcast, this means snoop protocol can increase average 
request latency and lead to large amounts of interconnect traffic in network on chip. Vast 
literature has been investigated on the problem. 
To utilize on-chip network bandwidth, we use another two kinds of non-uniform shared 
cache access without broadcasting shown in Figure 4 (b and c). First, the core can access a shared 
cache slice of its neighbor by hitting the replicated tag (shadow-tag) which is discussed in our 
shadow-tag mechanism for efficient location search as the best case, and another one is to access 
remote shared cache slice with longer search latency and transfer time. As shown in Figure 4 (d), 
external memory access is caused by cache miss occurring in total cache slices of ring. Based on, 
we adopt circuit-switched ring interconnections for benefiting ultra-low latency and low power 
under an acceptable frequency. 
 
28 
 
 
 
Figure 5: Using shadow-tag to find block location. 
 
On-chip interconnection bandwidth can be saved by hiding neighbor cache slices queries in 
local cache slice. Consequently, on-chip bandwidth would not be hurt by the alternative local 
multicast, which does not resemble previous multicast search mechanism. The example in Figure 
5 illustrates that the local cache slices can decide that the data is mapped to the local cache slice, 
the two nearest cache slices, or none of them. When a data is requested by a processor (Core 1), it 
first checks three tag arrays reside in local slice (slice 1).  
If the tag of requested address is hit in the local L1 data cache, data responses directly to the 
querying core shown in Figure 5 (a). If a request misses in the cache slice of core 1 by comparing 
the level-1 tag and shadow-tag arrays, the block is guaranteed not to be in core 1 and neighbor 
cores, that is, core 0 and core 2. As well as, the request would be transferred to other remainder 
slices as shown in Figure 5 (b). To utilize on-chip network bandwidth and avoid unnecessary 
search overhead, neighbor cache slices would be checked by snoop2 shadow-tag arrays hiding in 
the nearest cache slice without broadcasting shown in Figure 5 (c) (d). 
 Parallel Search Policy on Snoop2 
30 
 
 
the request would be checked on shadow-tag arrays again with one cycle latency. Besides the 
examples given above, Figure 5- 4 (c) illustrates that if the requested data is not located in local 
cache slice and the miss is detected early enough through Bloom Filter. 
 
 
Figure 7: Using bloom filter for energy saving. 
 
 Non-Uniform Debugging Architecture 
Most of today’s many-core debugging is trace-based and relies on the off-line verification of 
history or execution sequencing. The drawback of this methodology is an unacceptably large 
storage requirement and slow operating cycles. In order to support versatile real-time debugging 
requirements, the Non-Uniform Debugging Architecture (NUDA) is a technique that has been 
proposed for debugging sequentially consistent parallel programs on many-core systems. It has a 
flexible and configurable infrastructure to handle growing numbers of cores in the future. 
However, debugging events are usually triggered by core/memory/device components, and too 
many unnecessary events will cause a traffic jam of the NUDA’s interconnections and degrade 
the system performance. Moreover, unnecessary events may also intrude in the histogram-table in 
the NUDA node and lower the debugging capacity. Therefore, an efficient and hardware-feasible 
event-filter is urgently required. Race detection, nearly unlimited break/watchpoints and 
cross-event trigger are critical for multi-core/multithreading real-time debugging, and are all 
relative to memory access to a certain address. 
Figure 8 shows the NUDA system architecture, with a 64-core system as a fundamental 
platform. Without loss the granularity, the fundamental platform is unrestricted in its memory 
architecture, interconnection between cores, I/O facilities, etc. In fact, the NUDA is a scalable 
non-intrusive subsidiary architecture that does not share any resources with the fundamental 
platform. In brief, the NUDA framework contains three major parts: the NUDA architecture itself, 
32 
 
 
those of temporary/special locality and we can handle them immediately; in terms of the concept 
of time-to-space, the inter-cluster debugging events happen infrequently. Finally, from the 
perspective of the trend of parallel software structure, let the related tasks aggregate into a 
clusters provide better performance and programmability. 
A NUDA node is capable of gathering and organizing related information as monitored 
records. It also provides a programmable way for the user to reconfigure it, such as user-defined 
assertions. Moreover, a NUDA node handles the search/migrate/update to the non-uniform 
memory. From the programmer's viewpoint, a NUDA node is like a simple sub-system that 
contains a NUDA node controller with a separated local memory and a global memory 
(non-uniform memory) system. As shown in Figure 9, the debugging co-processor (DCP) plays 
the role of the interface between the fundamental platform and the NUDA. Massive histogram 
information is gathered by the DCP on each core and then sent to the NUDA node. Hence, the 
DCP should be lightweight and filterable. In contrast to the NUDA node, the DCP receives the 
NUDA node’s command packet, decodes it, and then invokes the indicated debugging operations, 
such as stop/step/continue executing a core.  
A NUDA also contains flexible interconnections that can be extended as the system grows. 
In this work, we use a ring as the interconnection structure. The reason for using the ring 
interconnections is not only to reduce costs but also to take advantage of the ring broadcast 
mechanism for the synchronization of core debugging events.  
In addition, most of the essential debugging functions are based on histogram data 
comparison. A larger storage space is acquired as the system grows. As shown in Figure 8, the 
NUDA is based on distributed cluster architecture; this work uses a local storage in each NUDA 
node and then unifies those local storages into non-uniform memory architecture (NUMA), 
which is totally isolated from the memory system of the fundamental platform. First, the local 
storage is well-prepared by a content-addressable memory (CAM) as a map for fast intra-cluster 
event checking. Second, each NUDA node’s local storage is a page-based non-uniform memory, 
and each page in this non-uniform memory is mapped into a global dictionary in the many-core 
ICE in case of an intra-cluster check failure. Once the dictionary indicates where the page is, the 
inter-cluster event checking will be invoked automatically. 
 The Structure of The NUDA Node 
As shown in Figure 10, the NUDA node structure consists of three major parts: computing, 
memory, and communication. The specific router handles a NUDA node’s communication with 
its neighboring NUDA nodes, local debugging bus (Dbus), and the internal computing part. The 
specific router uses a packet-switched design for node-to-node communication, but a 
circuit-switched design to connect Dbus and the computing part directly with intra-cluster core 
control signals. 
The key component in the computing part is a pro-grammable nano-processor (nP) that is 
triggered by user-defined debugging events and activates the corresponding debugging processes. 
The nP replaces dedicated hardware functions and also handles events by predefined software 
routines. The particular routines are stored in a small instruction memory within each node. The 
event handler is composed of an interface and a queue to receive events and stimulate the 
34 
 
 
performance. The embedding of a nano-processor in the NUDA node is demonstrated to provide 
flexibility and reusability at an affordable cost. 
 
 
Figure 11: Functionalities of a NUDA node. 
 
Furthermore, non-uniform debugging memory is or-ganized with non-uniform access for 
greater scalability, and it creates larger monitor capacity for runtime debugging on many-core 
systems. The distributed event synchronization mechanism by "Sync-Tokens" helps with global 
synchronization in debugging. Overall, each NUDA node includes a programmable design 
implementation by software to satisfy various requirements. 
 Non-Uniform Debugging Memory Design Space 
The architecture-dependent construction for debugging purposes may suffer from mutual 
influences be-tween normal and debugging operations. For example, a debugging storage aligned 
with cache hierarchies must inevitably be polluted by useless debugging events that are used for 
core execution, and the useful debugging histogram can be lost or swapped out during a cache 
miss. On the contrary, an independent debugging memory can be well utilized for runtime 
monitoring without intruding unnecessary events. Therefore, our design strategy use the 
independent memory for runtime monitoring, but it is distributed, rather than centralized, as a 
non-uniform cache architecture (NUCA), as shown in Figure 12(b). 
The conventional NUCA design features larger shared memory for the benefit of multi-core 
communication, especially in some multithreading programs. It usually has a lower miss-rate and 
sometimes a lower memory usage. However, it sometimes has the drawbacks of long access 
latency due to the access distance and ineffective data migrations. In this work, the NUDA 
contributes a page-based non-uniform memory design for runtime de-bugging. It gains the 
benefits of NUCA, of having a larger monitor capacity shared by multiple NUDA nodes without 
36 
 
 
In general programs, access to shared variables is governed by data locality. For example, 
two threads may access the same shared variables, but at different time slices, by the protected 
locks, which means that a quantity of debugging events are generated from a thread at a given 
time slice. If the thread and the monitor page are in the same NUDA cluster, the debugging event 
triggers an intra-cluster histogram access. On the other hand, three steps are required for an 
inter-cluster histogram access, with longer processing latency. In order to eliminate the global 
stall caused by too much inter-cluster histogram access, a good page migration mechanism is 
necessary, and the information of lock/unlock is a possible reference for prediction. 
Figure 12(1-3) describes the inter-cluster debugging histogram access flow. At first, the 
debugging event accesses the local NUDA node to search for the matching monitor region. If the 
search fails, the debugging event goes to many-core ICE for a central location directory checking. 
Then, the many-core ICE passes the event to the remote NUDA node for the inter-cluster 
debugging histogram access. Monitor page migration is necessary when different threads are 
accessing the same shared variables at different time slices. In our design strategy, a page queue 
in the many-core ICE dynamically records inter-cluster access counts of current frequent-used 
pages and makes a decision for migration if the count is over a defined ratio. If a monitor page 
migration is triggered in a NUDA node with no available storage, it selects a monitor page to 
replace, and the replaced page is swapped to neighbor NUDA nodes or exchanged with the 
migration page. 
The difference is that with fixed granularity which depends on the cache size. The monitor 
granularity of NUDA is flexible and user declarable; high granularity G represents a small 
peephole and precisely monitors the target memory, low granularity represents coarse monitoring, 
larger monitoring capacity, and fast examination. The relationship between monitor capacity and 
memory usage is shown in Figure 13. Essentially, there are two major storage spaces in each 
NUDA node, the CAM memory and the SRAM memory. The SRAM memory can be configured 
whatever the user demands. However, the entry numbers and granularity should follow the 
mapping constraint to prevent the memory from going out of bounds. 
 
Figure 13: Monitor granularity, capacity, and memory usage. 
 
The proposed non-uniform memory design is flexible to satisfy the requirements of versatile 
runtime debug-ging facilities, especially in address-based monitor and its histogram management. 
For example, it is able to implement runtime race detection and trace for order recording. In 
38 
 
 
resent at NUDA node N7. When all the tokens across the NUDA nodes count down to zero, all 
the NUDA nodes can be synchronized for non-intrusive control. 
Non-intrusive debugging control is one of the main debugging targets of the NUDA system 
for greater scala-bility in future many-core systems. The proposed "Sync-Token" mechanism 
ensures "synchronously stopping and starting all cores" to tolerate the latency caused overload 
debugging processes without disturbing the original execution ordering. Indeed, it causes some 
system performance degradation, but very little, because overload conditions rarely happen. 
 
 
Figure 14: Example of "Sync-Token" for many-core non-intrusive control. 
 Event Filter and Memory Size Reduction 
Debugging events are usually triggered by core/memory/ device components, and too many 
unnecessary events can cause a traffic jam on the NUDA’s communication network and degrade 
the system performance (all cores are stalled when the event queue is full). Moreover, 
unnecessary events may also intrude in the monitor histogram in the NUDA nodes and lead to 
low debugging capacity. Consequently, it is crucial to embed an efficient and hardware-feasible 
event-filter in the debugging co-processor (DCP). 
Figure 15 shows the block diagram of the DCP. Instead of a full in-circuit emulator (ICE) in 
a single core, the DCP supports the minimal control (control the core and communicate with 
NUDA) for feasible hardware cost on many-core debugging. The key component, the 
configurable filter (CF) unit, is used to providing the above mentioned filter functions to improve 
NUDA architecture efficiency. Our target is to filter out only the necessary debugging events in 
DCP and send them to the NUDA node. However, precise filtering is unfeasible because it 
requires many registers and comparators for each distributed debugging events (e.g., the X86 
processor only supports four hardware breakpoints). Currently, verify multiple events by a 
signature as testing is popular, because the information can be compressed into a fixed length 
signature without loss to much accuracy. Therefore, using signature for filtering is a good design 
style for runtime debugging. It can perform at a high filter-rate and reduce the workload on the 
interconnections. This work provides a configurable filter (CF) unit with two filtering 
mechanisms for efficiently filtering out most unnecessary events with low cost overhead. 
Figure 15 also depicts the composition of the CF unit, which mainly includes a set of 
registers and selectable data-paths. The set of registers can be assumed to be the temporal storage 
of some candidates that are compressed for filtering, and those registers are totally reusable in the 
proposed multiple functions. Data paths are built out of multiple selectable paths with several 
basic operations, including "AND", "OR", "XOR", "Shifter", "Compare", "One-Hot encoding", 
and so on, by selecting a certain data-path for the desired operation and combing registers for 
reading and updating. We plan to configure two filtering mechanisms ("Vector-filter" and 
40 
 
 
take out a certain range of the monitor-event’s address (e.g., chunk range 64 KB takes the 
adr[15:10]), and translate it as one-hot code. The one-hot intersects with VF-reg by selectable 
data-paths in the CF unit. If the result is non-empty, this means that the event is likely necessary 
for debugging. The "Vector filter" logic is quite simple and low-complexity and can easily be 
embedded in each core. The starting address and range of the chunk region can be adaptively 
defined by users due to different fragmental monitor spaces. However, if the fragmental monitor 
spaces are too sparse, the filter rate will be degraded due to the coarser granularity of 
"Vector-filter". 
 
Figure 17: "Bloom-filter" for sparse fragmental monitor spaces. 
 
In order to address the problem of the low filter-rate caused by sparse fragmental monitor 
spaces, another alternative is to use a "Bloom-filter". As shown in Figure 17, the basic idea is to 
compress monitor events into a Bloom filter vector (BFVector), which is used to filter out the 
desired monitor events in runtime. The compressed BFVector is stored in the set of registers in 
the CF unit at a static time. Therefore, in runtime debugging, a monitor event just intersects with 
the BFVector register to decide whether or not it is a candidate. The method shows a 
hardware-feasible solution for sparse fragmental monitor spaces, and it has very low miss 
judgments to allow unnecessary events to pass though. In our experimental results, either of the 
two filtering mechanisms can perform well, depending on the specific debugging requirements. 
 Software Assistance: RunAssert 
The NUDA framework includes the software solution for a better experience with parallel 
program debugging. We used directive #pragma in C language as a programming model, which is 
composed of directives, macro functions and assertion expressions in a library. Because it has the 
characteristics of runtime assertion, we call this tool RunAssert. Users can specify the assertion 
type, range, scope and target of directives and then define particular rules by macro functions. 
Finally, assertion expression is an option used to indicate the particular conditions of the 
debugging rules. The concept of a non-intrusive programming model for parallel program 
debugging is to decouple the debugging and guarding operations from the ordinary program 
execution.  
There are two categories of RunAssert directives. The first type is used for parallel program 
42 
 
 
ESR(void (*fp)(void*)) assists users to indicate the excep-tion service routine that is executed as 
the rules are established. Moreover, the macro LOCK assists users to identify a guarding lock, 
and the following macro MONITOR assists users to list the monitoring shared objects. In order to 
express the lock hierarchy in the source code, we use the horizontal expression to identify nested 
locks. In Figure 18, the shared variable X is under the nested locks S1 and S2, so the RunAssert 
representative is LOCK(t1.S1 && t4.S2). The main benefit of using the horizontal expression is 
that users can narrow the monitor scope in the inner locks. For instance, we only need to use 
LOCK(t1.S2) to describe the process of monitoring S2. 
 
 Criticore Operating System 
計畫的一開始，我們便採用以Micro C/OS-II為我們的的OS藍本，進行設計能在 criticore
環境上運行的 OS。Micro C/OS-II 是一具有 portable, Romable, Scalable, preemptive, real-time, 
multitasking 特性的 OS kernel。他是由 ANSI C 寫成，並包含了一部分的組合語言，使其能
適應於不同架構的處理器核心。至今已移植於超過 40 種，範圍從 8 位元到 64 位元的處理
器架構中。其使用組合語言寫的部分已減至最少，在移植與修改的難度相對於其他的一些
作業系統較低。 
 而我們試著配合 Micro C/OS-II 的 scalable 和 real-time 的特性，取用 ASMP 的概念來進
行開發設計，所謂 ASMP (asymmetric multiprocessing)是一種由異質多核心系統發展出來的
平行運算策略。Asymmetric 指的是對待每個 CPU 的方式，如果系統平等的對待每個 CPU
的話我們稱這個系統為 SMP(symmetric multiprocessing)，如果系統分配給每個 CPU 不同的
資源和運作模式的話則稱作 ASMP。相對於 SMP(symmetric multiprocessing)平等的對待每
個 CPU，ASMP 則是把特定的工作交給特定的核心處理。一個系統中可能會出現由某一個
CPU 負責全部的中斷處理，另一個 CPU 處理全部的 I/O 這種依照運算單元的特性分工的情
況，也因為能把 IO 工作集中到特定核心去處理的關係所以也能令 I/O 系統的設計獲得簡
化。ASMP 架構下的程式如果沒有 host processor 和其他 core 之間的互動機制支援的話，其
應用程式並不容易撰寫。 
 實作上，我們使用 Linux 與 CCOS(以 Micro C/OS-II 為底的 OS)，Linux 負責當作特定
的核心處理，控管所有的工作，而 CCOS 接收從 Linux 分配來的工作進行處理。CCOS 與
Linux 之間的互動是透過 IPI(inter processor interrupt)和預先設定好，位於 CCOS 記憶體區段
的共用記憶體來達成。採用 CCOS 區段的記憶體當作共用記憶體的除了能提高可移植性
外，而且也可以避免更動 Linux 的記憶體管理機制。當系統中發生事件，如 Linux 需要指
派工作到 CCOS，或是 CCOS 的工作完成時，可以透過 IPI 來通知對象核心有事件發生。而
實質的溝通則是透過 share memory 交換訊息達成。 
 在設計介面上，我們為了讓一般多核心開發者可以快速的上手我們的系統，因此藉由
仿 pthread library 的介面，讓使用者要進行平行化處理時，只需依照 pthread 的使用方式來
進行程式撰寫。在實作上，我們設計一套 library 來包裝 Micro C/OS-II 的工作處理函式，透
過這套 library，使用者可使用 thread 函式來控制 micro C/OS-II 的 task，包涵 task create、task 
exit、lock 等。在與子計畫 3b 整合中，3b 所建立的 SpMT library 可使用我們這套 library 來
進行他們的工作，且移植上是容易進行的。 
 然而 Linux 與 CCOS 之間的設計機制相當複雜，在開發時間上需求一定的時間來完成
與整合。因此，為了能與其他子計畫開始進行整合工作，將近行至一半的架構修改為 Micro 
44 
 
 
 
(B) 功率估算之模擬器內部流程 
Figure 19: 流程 
 
我們以這些資訊為基礎，搭配一些模擬器可以提供的資訊去推算出可能的功率消耗方
式，整個流程如 Figure 19 所示，圖中的 Power Information 代表子計畫四提供的精準結果，
當模擬器執行一個程式時，我們會去統計指行指令的類型，而執行的指令必須再做進一步
的轉換才能和子計畫四提供的功率資訊做結合，因為子計畫四的結果是以功能單元為基
本，而這些存取都是可以由指令統計作轉換來取得，所以將統計的指令結果，轉換成對功
能單元的存取，然後再計算出相對應的功率結果。而整體而言，除了功率結果外，我們還
可以得到指令的統計，以及功能單元存取的次數。如果在對這方面做延伸，去計算單位時
間的指令密度，配合處理器的平面圖(floor plan)，可以在求得溫度的結果。 
 
Table 3: Criticore(單一核心)平均功率消耗結果 
Instance Leakage Power(nW) Dynamic 
Power(nW)  Total Power(nW)  
MULTIPLY 1181884.74 85677.52 1267562.27 
SHIFTER 55271.98 24479.45 79751.43 
DECODER 27692.41 19021.72 46714.13 
REG_CONTROLSIGNAL 23729.91 48931.25 72661.16 
ALU 19575.41 32710.43 52285.85 
ENCODER 11402.42 1100.56 12502.98 
BRANCH 10531.07 6961.06 17492.13 
DataToRF 10540.46 17695.70 28236.16 
mrf_top 147002.11 182759.88 329761.98 
mfe_top 60813.55 190672.66 251486.21 
 
Table 4 是指令對功能單元的轉換表，表中的數字那類指令一次取功能單元的次數，有
出現 N 則代表，次數可能會跟指令的內容有關係。 
Table 4: 令對功能單元次數轉換 
 
REG ALU MULT 
DECODE
R 
ENCODE
R 
BRANCH 
… 
DP1 2 1 0 1 0 0 … 
46 
 
 
功率/能量資訊來求得更好的結果。這部分在第二年的溫度管理上也可以使用大部分的機制
與方法。 
實作的方式可能會透過幾種方式： 
a. 夾帶在程式的開頭，讓 Loader 在載入程式的資料時，順便把這些資訊一起讀
取進去。不過使用這種方面比較適合附上整隻程式的行為及趨勢，把這部分資訊當作是全
域資訊來處理(global information)。 
b. 使用指令來傳遞資訊，這種方式可以反應更為及時的資料，因為指令的方式，
可以反映程式目前執行的資料路徑，換言之，就是可以反映最真實程式的行為，但是這個
缺點就是需要增加那些指令以及執行那些指令對程式本身帶來的影響。而這部分反映的資
訊可以當作是區域資訊(local information)。 
 
3. Power management 
電源管理則是一些可以控制的機制，像是動態頻率調整技術(DFS)、clock gating 等方
式，這些方式可以供處理器核心的省電方式，我們這邊也可以配合之前的分析去求出使用
哪些管理的方式是最合適的。 
 
 
Figure 20: 電源管理之流程 
 
在來是主要目的是要提供多執行緒函式庫，在加上具可靠性動態頻率調整技術(DFS)，
利用功率評估模型將程式分析且平行化處理之後，產生多執行緒之程式，這個程式在多核
心處理器平台上運作時，OS 會針對程式之中的 power hint，調整多核心處理器的速度，讓
運算量較低之處理器以較低的頻率運行，這樣在幾乎不影響整體效能的情況下，完成所指
派的部分，並達到省電之效果。以下將詳細描述研究方法的每一步驟： 
 
48 
 
 
2. Hint 
這部分是跟如何傳遞資訊給所需的部分，像是傳遞一些跟溫度處理有關係的資訊給作業
系統，讓作業系統可以利用這些資訊，去做出更好的選擇，像是排程器可以透過得到的功
率/能量資訊來求得更好的結果。這部分在第二年的溫度管理上也可以使用大部分的機制與
方法。 
 
實作的方式可能會透過幾種方式： 
a. 夾帶在程式的開頭，讓 Loader 在載入程式的資料時，順便把這些資訊一起讀
取進去。不過使用這種方面比較適合附上整隻程式的行為及趨勢，把這部分資訊當作是全
域資訊來處理(global information)。 
b. 使用指令來傳遞資訊，這種方式可以反應更為及時的資料，因為指令的方式，
可以反映程式目前執行的資料路徑，換言之，就是可以反映最真實程式的行為，但是這個
缺點就是需要增加那些指令以及執行那些指令對程式本身帶來的影響。而這部分反映的資
訊可以當作是區域資訊(local information)。 
 
3. Thermal management 
這邊主要是對於溫度的管理。主要是一些可以控制的機制，像是動態頻率調整技術
(DFS)、clock gating 以及 core migration 等方式，這些方式可以供處理器核心的降溫方式，
我們這邊也可以配合之前的分析去求出使用哪些管理的方式是最合適的。 
在來是主要目的是要提供多執行緒函式庫，在加上具可靠性動態頻率調整技術(DFS)，
利用功率評估模型將程式分析且平行化處理之後，產生多執行緒之程式，這個程式在多核
心處理器平台上運作時，OS 會針對程式之中的 power hint，調整多核心處理器的速度，讓
運算量較低之處理器以較低的頻率運行，這樣在幾乎不影響整體效能的情況下，完成所指
派的部分，並達到省電之效果。以下將詳細描述研究方法的每一步驟： 
 
a. 首先先做 code analysis，利用平行化分析之結果，將分出來的每個執行緒視為一個
單位，建立出有向圖形（directed graph），其中的節點（node）為一執行緒，節點之間根據
control flow 產生連結（edge）。 
 
b. 利用前一步驟有向圖形，針對每個節點做 latency 及 deadline requirement 等分析，
計算出每個節點所需要的時間以及必須何時完成，依照這些資訊，決定各個節點的
執行頻率。 
 
c. 依據每個 multiple thread 的情形，指派執行的處理器核心，若是並加入 thread 間的
頻率不同，必須加入 overhead，將指派的結果，作溫度以及程式執行時間的頻估。 
 
d. 調整使用的核心總數，重新指派工作，反覆的執行，求出各種情況下的耗電量，及
執行時間。 
 
e. 將上面產生出來的情報，加入到程式當中，並配合編譯器及底層硬體的回饋機制
（thermal hint）做為動態排程的參考基礎，提供給作業系統使用，以便可以判斷是
50 
 
 
類方式： 
 
Table 5 
Relationship 
Data type 
Scalar variable Array 
Order-independence Type 1 Type 1 
Flow dependence Type 2 Type 3 
Output dependence Type 4 Type 4 
 
2 針對 TLS 平行化技術之自動化轉換技術的研發 
首先，我們描述所設計的 compiler directives 與支援的 clauses，使用方式如下所示： 
 
 
設計 pragma 的方式有很多種，不過我們學習的對象是 OpenMP 的格式，主要的原因是
希望之後有機會能與 OpenMP 做整合。 
 
再來看到 Clause 的部份，可以使用 var_s、var_raw 兩種。Var_s 是針對 Type 1 所設計
的，參數的部份則是放入 Type 1 的變數名稱。Var_raw 則是對 Type 2, Type 3 所設計，參數
的部份則是放入有的變數名稱。下面是一個例子： 
 
 
我們撰寫 translator 所採用的是 Nanos group 所研發提供的 Mecurium Compiler。
Mercumium Compiler 是一個 source-to-source 的 compiler，目前可以支援 C/C++、Fortran 三
種語言。它可以針對輸入的程式碼做重寫、插入、刪除、轉換等工作，因此他是一個相當
適合拿來測試即將被實做的新語法。Mercurium Compiler 本身也支援 OpenMP  pragma 的
轉換，而關於這部份的轉換可以使用 Mercurium Compiler 已經寫好的，也可以自行重新設
計一個新的轉換方式。 
和一般的 source-to-source compiler 做轉換不同的地方是，它使用的是「所寫即所見」
的轉換方式，而傳統的則是層層包覆起來，Figure 22 則是一個傳統的表示轉換的表示方式： 
#pragma SpMT parallel [clause[[,] clause] …] new-line 
#pragma SpMT parallel var_raw(sum) 
for (j = 0; j < 10000; j++)  
  if (sum == 100)  
    sum = j * 12; 
52 
 
 
入的穩定參考時脈 (Reference Clock)，Reference pulse generator 可以在 PVT Variations 之
下，產生固定波寬度的 Pulse，經由後端的 Auto-Calibration Circuit，便可以在已知晶片平
衡溫度下(例如室溫 25°C)，校正出溫度曲線的斜率與截距，因此便可完成整個全數位溫度
感測器的溫度歸零校正。接著溫度感測器回復到正常工作模式，由 PTAT Pulse Generator 所
產生之與絕對溫度正比的 Pulse，經由時間對數位轉換器(Time-to-digital Converter, TDC)量
化後，產生數位資訊(TDC_code)，再由 Temperature Calculator 經由已知的溫度曲線的斜率
與截距，計算目前的區域溫度數值。 
因為本計畫所提出之校正方式是適用於包含多顆溫度感測器的系統，因此晶片平衡溫度
需要由一顆經由傳統兩點校正的溫度感測器提供。其餘溫度感測器則是在系統重置後，自
動經由本計畫所提出的 Auto-Calibration Circuit，校正出溫度曲線的斜率與截距，完成整個
全數位溫度感測器的溫度歸零校正。因為只需要對一個溫度感測器進行兩點校正，因此本
計畫所提出之可自我校正溫度感測器技術，可以大幅降低對多顆溫度感測器的溫度零點校
正的測試成本。 
 
Figure 24：The proposed main sensor circuit. 
Sensor circuit 的細部架構如 Figure 24 所示，主要可以分為 Delay pulse generator 以及 
Time-to-digital converter (TDC) 兩個部份，由於 Delay line 中的 Delay Cell 其 propagation 
delay 會隨著溫度上升而增加，所以 Delay pulse 資訊經過後部 TDC 的量化，我們便可以得
到一個 Proportional to Absolute Temperature (PTAT) 的 Digital code。 
在產生 PTAT 的 Digital Code 之後，其 Calibration Circuit 可以利用一開始校正的結
果，計算出當下獲得的 Digital Code 所對應的溫度為何並輸出溫度資訊，有關 Calibration 
Circuit 詳細架構如 Figure 24 所示。圖四中的電路，會每次累積一個 Slope 的值或是減少
一個 Slope 的值，利用線性關係，找出目前的溫度讀值。會採用此架構的原因，是因為需
要節省溫度感測器的面積成本，因此使用多個參考時脈週期來計算目前的溫度讀值。 
 
54 
 
 
Multiplier 與 Divider 之類的較複雜運算電路。藉由複製此 Critical Path 於 Delay Monitor 
電路，便可以在實際晶片運作的時候，透過 FF1/FF2 的相位比較電路，判斷出目前在 SoC 
晶片中，該區域的 PVT Variations 和 OCV 的綜合影響，會不會導致 Criti-Core CPU 的最
長延遲路徑 Delay 超過目前設定的時脈週期。如果會的話，就代表系統需要調降時脈速
度，才能確保程式執行的正確性。 
 
Figure 28：The proposed low cost error detection circuit (fine-tuning). 
然而 Figure 27 中的 delay monitor sensor circuit 只能當作對系統時脈調整的粗調電路使
用，這是因為 Critical Path Replica Circuit 的延遲時間，多少都會與真正的 Critical Path 電
路時間有些差異。因此需要一個更精準的偵測電路，微調時脈週期速度，不然單獨使用圖
六的電路可能會導致對系統可工作時脈速度太過悲觀的估測。因此本計畫提出以 
Razor-based 的 Delay Monitor 電路，如圖七所示。 
 
Figure 29：The DLX CPU with the Delay Monitor. 
Figure 28 為本計畫所提出之 low-cost error detection circuit。本計畫所提出之 error 
detection 電路，會偵測在 CK 訊號正源後一段時間內(Detection Window)，是否 D0 – D7 的
資料訊號有改變。因為正常資料變化的時間應該是在時脈正源前就應該要提早穩定，才能
讓下一級 pipeline register 能夠正常取值。因此如果在正源後的 Detection Window 內，資料
有變化的話，就代表目前 Criti-Core CPU 的最長延遲路徑 Delay 超過目前設定的時脈週
期，因此系統需要調降時脈速度，才能確保程式執行的正確性。因為此電路是嵌入於 
56 
 
 
 Design and Implementation of Panoramic Stereo Video Playing System 
 Software Optimization Methodology of New Generation Video Decoders 
In order to achieve real-time decoding of new generation videos such as VC-1, H.264, and AVS, 
we propose some optimization techniques that can be roughly divided into two major categories, 
i.e. algorithm-level and code-level optimization. In the algorithm-level optimization, we propose 
various techniques like fast interpolation scheme, zero-skipping technique for texture decoding, 
fast boundary strength decision for in-loop filter, and so on. In the code-level optimization, we 
propose several coding guidelines to minimize the computation complexity. By means of 
software optimization, we can efficiently reduce the complexity of new generation video 
decoders and increases their real-time processing performance. 
 Algorithm-Level Optimization 
Algorithmic Optimization on Inverse Transform 
In new generation video decoders, the inverse transform is simplified to operate in integer mode. 
In other words, computation precision will no longer be a confusing problem in the inverse 
transform. However, it still needs a fast algorithm to lower down its computation complexity. 
Both VC-1 and AVS adopt block type of 8×8 to process image blocks in the inverse transform. 
Therefore, the kernel operation is the 1-D 8-point inverse transform since a 2-D 8×8 inverse 
transform can be realized by 16 1-D 8-point inverse transforms according to the row/column 
decomposition scheme. In the 1-D 8-point inverse transform, we can express its main matrix 
multiplications as 
( ) 7,,1,0  ,,7
0
K== ∑
=
nnkAxy
k
kn  (1) 
where  


























−−−−
−−−−
−−−−
−−−−
−−−−
−−−−
−−−−
=
75311357
62266226
51733715
44444444
37155173
26622662
13577531
00000000
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
A
 
(2) 
On the other hand, we find a relationship shown in (3) existing in matrix A. 
( ) ( ) ( )jiAjiA i ,17, −=−  (3) 
 
From this relationship, we can organize output data to form four pairs (y0, y7), (y1, y6), (y2, y5), 
and (y3, y4). We can calculate them as follows. 
 
58 
 
 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]
[ ] [ ] [ ] [ ]
[ ] [ ] [ ] [ ]7331571562264400
7331571562264400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
3
0
12
3
0
2
7
0
2
2,72,32,52,1
2,62,22,42,0
2,342,142,242,4
2,122,22,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
k
k
−−++−−−=
+−++++−+−=
++++
+++=
++++++=
++==
∑∑∑∑
∑∑∑
=
+
=
+
=
+
=
=
+
==
 (8) 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]{ }
[ ] [ ] [ ] [ ]{ }
[ ] [ ] [ ] [ ]{ }7331571562264400
7331571562264400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
12
3
0
12
2
3
0
2
7
0
7
0
5
2,72,32,52,1
2,62,22,42,0
2,342,142,242,4
2,1212,21
2,15,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAx
kAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
r
r
k
k
k
k
k
−−+−−−−=
+−++−+−+−=
+++−
+++=






+++−++=
+−+−=
−==
∑∑∑∑
∑∑
∑∑
=
+
=
+
=
+
=
+
=
+
=
==
 
(9) 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]
[ ] [ ] [ ] [ ]
[ ] [ ] [ ] [ ]7135531766224400
7135531766224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
3
0
12
3
0
2
7
0
3
3,73,33,53,1
3,63,23,43,0
3,343,143,243,4
3,123,23,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
k
k
+−+++−+=
−−+++−−++=
++++
+++=
++++++=
++==
∑∑∑∑
∑∑∑
=
+
=
+
=
+
=
=
+
==
 (10) 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]{ }
[ ] [ ] [ ] [ ]{ }
[ ] [ ] [ ] [ ]{ }7135531766224400
7135531766224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
12
3
0
12
2
3
0
2
7
0
7
0
4
3,73,33,53,1
3,63,23,43,0
3,343,143,243,4
3,1213,21
3,14,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAx
kAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
r
r
k
k
k
k
k
+−+−+−+=
−−++−−−++=
+++−
+++=






+++−++=
+−+−=
−==
∑∑∑∑
∑∑
∑∑
=
+
=
+
=
+
=
+
=
+
=
==
 
(11) 
 
Then we can obtain the data flow charts as shown in Figure 31, Figure 32, Figure 33, and Figure 
34 for computing the 1-D 8-point inverse transform. 
60 
 
 
-
-
-
x[0]
x[4]
x[2]
x[6]
x[1]
x[5]
x[3]
x[7]
a0
a4
a2
a6
a7
a3
a5
a1
y[4]
y[3]
 
Figure 34. Data flow for fast 1-D 8-point inverse transform (Part IV for y[3] and y[4]) 
 
According to the proposed algorithm, we share lots of partial computation results so that we can 
use the minimum operations to calculate 1-D inverse transform. The proposed fast 1-D 8-point 
inverse transform only takes 22 multiplications and 32 additions while the original one has to 
take 64 multiplications and 56 additions. It is seen that we save 65.6% multiplications and 42.9% 
additions for 1-D 8-point inverse transform. In consequence, a lot of redundant multiplications 
and additions in the inverse transform of VC-1 and AVS can be saved through the proposed fast 
algorithm. 
Zero Skipping for Inverse Transform 
In the video decoding process, the occurrence of zero values means computation can be reduced 
or simplified. As shown in Table 6, we have performed some probability analysis for zero 
occurrence in the inverse transform. According to the analysis, all the cases listed in Table 6 
cover about 90% of occurring samples in VC-1 and 93% of occurring samples in AVS. So we can 
detect these cases before doing the inverse transform to avoid the unnecessary computation. 
 
Table 6. Analysis for zero occurrence in 1-D 8-point inverse transform 
Cases (where xi is input data) 
Occurrence Probability 
(in VC-1) 
Occurrence Probability 
(in AVS) 
x0=x1=x2=x3=x4=x5=x6=x7=0 33.2 % 35.3 % 
x1=x2=x3=x4=x5=x6=x7=0 20.3 % 11.6 % 
x2=x3=x4=x5=x6=x7=0 9.5 % 12.3 % 
x4=x5=x6=x7=0 3.9 % 9.5 % 
x1=x3=x5=x7=0 1.5 % 6.6 % 
x0=x2=x4=x6=0 11.7 % 9.5 % 
x6=x7=0 10.4 % 8.5 % 
62 
 
 
Current 8×8 
block is all-zero?
Current 4×4 
block is all-zero?
Inverse Transform
VLD & Inverse 
Quantization
Reconstruct 
block4×4
Copy block8×8
All 4×4 blocks 
are processed?
All 8×8 blocks 
are processed?
Copy block4×4
End
No
Yes
Yes
No
Yes
No
Yes
No
 
Figure 35. Skipping of all-zero residual blocks 
Early Termination for Deblocking 
In AVS-M, deblocking will be applied to all the boundaries among the blocks in order to reduce 
the block effect since AVS-M also divides each picture into a lot of small blocks for coding. 
Different from H.264, AVS-M doesn’t check the boundary strength. The way of deblocking is 
mainly based on the type of the macroblock. When the current macroblock is intra coded, the 
intra mode deblocking will be performed. On the other hand, the inter mode deblocking will be 
performed when the current marcoblock is inter coded and not skipped or the QP of the current 
marcoblock is greater than a threshold. Essentially, the deblocking is to analyze and fine-tune the 
pixels located on both sides of the block boundary. As shown in Figure 36, the key factor in 
fine-tuning is the variable “delta”. However, the variable “delta” often becomes zero resulting 
from the parameter “CI”. In this situation, all the analysis before fine-tuning is meaningless 
because the pixels will not be changed after fine-tuning. Therefore, we add a detection of the 
parameter “CI” in order to make an early termination for deblocking. Figure 36 shows the 
proposed processing flow of intra mode deblocking. Similarly, the proposed early termination can 
also be easily applied to the inter mode deblocking because the processing flows for these two 
modes are almost the same. By means of the proposed early terminated deblocking, we can skip 
unnecessary pixel analysis so that considerable execution cycles will be reduced. 
64 
 
 
d
bC ED
K ML
A
F
S
N
B
G IH
O
T
QP
J
R
n
a c
h j
t
aa
hh
e f
i
p
g
k
q r
m
 
Figure 37. Luma interpolation of the AVS-M video decoder 
 Code-Level Optimization 
Sometimes, programmers may write the program in a straightforward way because they just want 
to realize a specific function without considering much on its performance when executed on an 
embedded processor. Nevertheless, there may be some poorly written codes that will decrease the 
performance. In order to enhance the execution performance of new generation video decoders, 
we improve them in code-level according to the following coding guidelines. 
 Moving the loop-independent branches out of the loop 
In general, branches are the operations we don’t want to see at all. However it is impossible 
because branches are necessary for carrying out any decisions in the program. Now that we 
cannot forsake branches, we have to try our best to minimize them. This guideline means that we 
have to minimize the number of branches in any loop since loops will largely increase the amount 
of branches. As shown in Figure 38, when a branch with the condition C2 is included in a loop, 
we should move this branch out of the loop. In other words, we should check the condition C2 
first and use independent loops to deal with the work S1 and S2, respectively. However, there is an 
important constraint that we have to pay attention to. The constraint is that the condition of the 
branch must be independent of the loop. If the condition comes from any calculation in the loop, 
such rewrites may cause some errors in the correctness of the execution results. 
while ( C1 )
{
    if ( C2 )
       S1
    else
       S2
}
if ( C2 )
{
    while ( C1 )
    {
        S1
    }
}
else
{
    while ( C1 )
    {
        S2
    }
}
 
Figure 38. Illustration for Guideline 1 
 Replacing predictable calculation with look-up table 
This guideline tells us that we should pre-calculate some operations that are predictable and save 
the results of them in a look-up table so as to skip the original operations at run time. For example, 
the clipping operations are frequently used in video processing. Nevertheless, they have to check 
66 
 
 
ones and call one of them based on what the case is. 
Func( ... )
{
    if ( ... ) // case A
       S1
    else if ( ... ) // case B
       S2
}
Func_A( ... ) // for case A
{ 
       S1
 }
Func_B( ... ) // for case B
{
       S2
}
 
Figure 42. Illustration for Guideline 5 
 Making the repeating number of loop be a constant value 
In software optimization, loop unrolling is an approach in common use. Such an approach can 
avoid the branches brought by loops and improve the execution performance. But the compiler 
won’t unroll the loop without knowing the repeating number of the loop. This guideline is 
established to let compiler know the repeating number of a loop at compile time so as to unroll 
the loop automatically. So we make the repeating number of the loop be a constant value if 
possible. 
 Moving the cases with higher probability forward 
In video processing, there exist many modes in order to deal with all kinds of situations. However, 
a lot of branches will appear if we just arrange them arbitrarily. In fact, not all of the cases will be 
hit averagely. According to our experience, it has more than 50% possibility to hit someone of all 
cases. For example, there are four kinds transform in VC-1, such as 4×4, 4×8, 8×4, and 8×8 but 
the case of 8×8 transform is the most frequently used one. This guideline means that we have to 
sort the code segment with many cases according to the probability of each case as shown in 
Figure 43 so that the hit rate of the code segment will be increased. 
if ( C1 )
    S1
else if ( C2 )
    S2
else if ( C3 )
    S3
else if ( C4 )
    S4
Higher probability
Lower probability
 
Figure 43. Illustration for Guideline 7 
 Replacing “MOD” operation with “AND” operation 
Generally speaking, MOD operations are always implemented by division operations. However, 
embedded processors like ARM processors don’t have division instructions to handle division 
operations. For this reason, they use a subroutine that can take advantage of some essential 
arithmetic operations like additions and shifts to carry out the division operations. So the 
processors will waste a lot of cycles on executing any MOD operation. This guideline is based on 
a simple concept that doing “A MOD B” operation can be replaced by doing “A AND (B-1)” 
when B is power of two. For example, “A MOD 256” is equal to “A AND 0xFF”. With such an 
ingenious transformation, we can improve the execution of the MOD operation under the 
68 
 
 
as MPEG-2/4 at the same bit-rate. However, H.264 is much more complicated than previous 
standards. H.264 defines thee profiles with different tools such as Intra/Inter prediction, CAVLC, 
CABAC, FMO, MBAFF and so on. Figure 45 shows the H.264 decoding flow. However, H.264 
decoding needs powerful processors to achieve real-time processing since it has very high 
computational complexity. Multi-core processor is one of the best choices because it is embedded 
several processing elements so that it can provide very powerful computing performance. This 
work is to use multi-core processor to parallelize the H.264 video decoder. 
 
Figure 45. Block diagram of the decoding process for H.264 decoder 
 Parallelization Challenge 
In general, it is hard to parallelize H.264 decoder because of the data dependency. H.264 video 
decodes at sequential MB order. Figure 46 shows the possible data dependency for a macroblock 
(MB). Intra prediction dependency exists when the current MB requires data from the neighbor 
MBs as shown in Figure 46(A). Motion vector (MV) prediction dependency exists when the 
current MB requires data from the MB located on its reference frame as shown in Figure 46(B). 
Deblocking filter exists when the current MB requires data from the neighbor MBs as shown in 
Figure 46(C). We could not parallelize H.264 decoder without solving data dependency problem. 
In addition, another terrible data dependency comes from entropy decoding since we don’t know 
how many bits each MB will use. For such a reason, traditional H.264 decoder has to decode the 
MBs one by one. Therefore we will propose a brand-new method to conquer this constraint in the 
following sections. 
 Parallelization of H.264 Video Decoder 
Figure 47 is the proposed system architecture. As shown in the figure, we add the pre-parsing 
stage in the flow for the sake of parallelizing H.264 decoding. Here we allocate a local buffer for 
sharing data among the cores in the processor and reducing the memory bandwidth. However, the 
original reference software (JM) cannot be executed based on this architecture. For this reason, 
we have to adjust the data structure and change the decoding flow. 
 
70 
 
 
Analysis of Local Buffer 
Figure 49 shows the partial processing schedule at designate time T when decoding QCIF video 
with four cores. From this figure, we can see the processing order of each MB is decided by the 
data dependency among the cores. Therefore, we can get the overall processing schedule as 
shown in Figure 50. Here we suppose each MB has the same decoding time. 
 
Figure 49. Partial processing schedule at designate time T when decoding QCIF video with four 
cores 
In order to reduce the memory bandwidth and access time, we use the local buffer to store the 
reference data for sharing data among the cores. For supporting full HD video, we need ((N-1)×
3+120+N) storage units. N is the number of cores. For example, if N=4, core1, core2, core3 will 
store 3 decoded MB data to local buffer and core4 will store all of the decoded MB data to local 
buffer (3×3+120). In addition, we also need to store the reference data from the left MB since the 
current MB needs it when decoding. 
 
 
Figure 50. Overall processing schedule when decoding QCIF video with four cores 
Rearrangement of Data Structure 
We cannot directly execute the reference software (JM) as a parallel program because its data 
structure does not support. Figure 51 shows the data structure defined in JM. As shown in Figure 
51, the structure ”img” includes the sub-structure “currentSlice” and the sub-structure 
72 
 
 
 
Figure 53. Proposed algorithm of depth map generation 
Figure 53 shows the proposed algorithm of depth map generation based on a single 2-D image. 
Figure 54 shows three categories of input images that the proposed algorithm can deal with. They 
are normal images (with vanishing point (VP)), scenery images (without VP), and close-up 
images. In the proposed algorithm, we first classify the input images into the above-mentioned 
three categories, and process them for generating the depth map according to different flows with 
the processing steps, including edge detection using Sobel filter, line detection using 5×5 Hough 
Transform, Vanishing Region Detection (VRD), segmentation, depth map merging, depth map 
post-processing by JBF, and 8×8 contrast filter to identify the foreground objects in the close-up 
images. The input images are classified into close-up images and the rest according to the 
characteristics of having enough vanishing points or not. For the rest images, they can be 
classified into normal images and scenery images dependent on if there is an intersection point 
among the vanishing lines. Scenery images will have almost no intersection point. Of course, a 
suitable threshold value is needed to differentiate the categories of normal images and scenery 
images. 
To make the proposed algorithm suitable for real-time applications, we have optimized the 
above-mentioned processing steps to reduce the computing complexity while preserving good 
visual quality. In the following, we will illustrate each processing step in more details.  
 
       
(a) Normal image (with VP)   (b) Scenery image (without VP)      (c) Close-up image 
Figure 54. Three image categories that the proposed algorithm can deal with, where VP denotes 
74 
 
 
In order to reduce the complexity in Hough transform, we adopt the Hough transform on 5×5 
blocks and only detect the pixel positions having non-zero values in the edge map. For the 5×5 
Hough transform example shown in Figure 56, the black pixel point S is the current processing 
pixel which has a non-zero value. We first detect the white zone to search if the pixel position has 
value 1. If position A in Figure 56 has value 1, then we will search the positions X and Y in the 
gray zone. If either X or Y has value 1, we perform Hough transform to record (ρ, θ) for 147~168 
degrees. By using the proposed 5×5 Hough transform, we can reduce about 56% of complexity 
compared to that of searching all pixel positions from 0~180 degrees. After processing all pixels 
in the edge map, we sort these (ρ, θ) data to find out the line having the most pixels as the 
vanishing lines. In the proposed algorithm, we record about 24 lines as vanishing lines in average, 
which is enough to detect the vanishing point.  
 
Figure 56. Example of 5×5 Hough transform in the proposed algorithm 
Vanishing Region Detection with Classification 
In order to reduce the complexity of detecting the vanish point, we propose the concept of 
Vanishing Region Detection (VRD) to detect the vanishing region in the image according to the 
vanishing line information obtained from Hough transform. The reason to detect the vanishing 
region instead of vanishing point is that vanishing region would be close to the associated 
vanishing point so that we can locate the vanishing region with fewer vanishing lines. According 
to the vanishing line information, we also classify which type of input images is under 
processing. 
The proposed image classification is shown in Figure 57. In first step of VRD, we analyze the 
number of points and lines from Sobel and Hough transform. If the number is smaller than a 
pre-defined threshold, we classify it as a “close-up” type of images. In the second step, we start 
calculating the intersection point of vanishing lines. If it there is no intersection point, we classify 
it as a “scenery” type of images. After calculating all intersection points of vanishing lines, we 
use an 8×8 region to group the nearest points in the image which is also called the vanishing 
region. According to the position of vanishing regions, there are two cases. One is that the 
vanishing region is inside the image. The other one is that it is outside the image. If the vanishing 
region is outside the image, we adopt the image boundary of the two vanishing lines as the new 
vanishing region. Then, we generate the Gradient Depth Map (GDM) according to the distance 
between every pixel and the vanishing region for the “normal” type of images. The example of 
the GDM for the image in Figure 54(a) is shown in Figure 58(a). 
For the “scenery” type of images, we only use a simple method to generate the GDM. According 
to the features of scenery image, there is usually sky or mountain on the top of the image. So we 
generate a GDM that defines the vanishing region is on the top of image, as shown in Figure 
76 
 
 
result in the proposed algorithm for the input image shown in Figure 54(b). 
Merging Depth Map 
Merging depth map is used to modify the GDM according to both the segmentation result and the 
edge map. We modify the depth value in the GDM if it belongs to the same object in the 
segmentation and in the same side of edge map. After merging depth map, we modify the depth 
map of the same object as the same in the proposed algorithm, as the examples shown in Figure 
60(a) and Figure 60(b). 
     
(a) Normal image (with VP)     (b) Scenery image (without VP) 
Figure 60. Merging depth map for the images in Figure 54 (a) and Figure 54 (b) 
Joint Bilateral Filter 
Joint Bilateral Filter (JBF) is used to post-process the merged depth map by strengthening the 
edge information of the objects related to the original image in the proposed algorithm. In the 
original JBF algorithm, it is used to modify the target image according to a reference image. In 
the proposed algorithm, we use the input image as the reference image in JBF to modify the 
target depth map from the merged depth map. Eq. (16) shows the original JBF formula, where the 
Iq is the target image pixel data, Ip is the reference pixel data, and Ip’ is the final output pixel data. 
There is a weighted function in the JBF, including a spatial distance function (g) for calculating 
the distance of p and q, and an intensity range function (r) for calculating the difference of pixel 
data. 
)()(  where
/
,
,,
qpqp
sq
qp
sq
qqpp
IIrqpgW
WIWI
−−=
=′ ∑∑
∈∈
 
(16) 
In the original spatial distance function, it uses a function, called edge stop function, to detect the 
edges. If there is no edge, the weighted function will be used in JBF. In the proposed algorithm, 
there is no need to detect the edge again. Therefore, we simplify the JBF by removing the edge 
stop function, which reduces about 26% of computational complexity. The JBF results for the 
images in Figure 54(a) and Figure 54 (b) are shown in Figure 61(a) and Figure 61(b), 
respectively.  
     
(a) Normal image (with VP)     (b) Scenery image (without VP) 
78 
 
 
 
Figure 63. Profiling of computational complexity in 3-D depth map generator 
  
(a) (b) 
  
(c) (d) 
Figure 64. Data partition for parallelization of 3-D depth map generator 
Method 1 
In this method, we partition the image data along the vertical direction according to the number 
of the threads we use. For example, we will partition the image data into four regions as shown in 
Figure 54(a) if we use four threads. Each of these four threads is going to process the dispatched 
region. 
Method 2 
Similar to method 1, method 2 will also partition the image data according to the number of the 
threads we use. However, the partition is made along the horizontal direction as shown in Figure 
54(b). It is because we want to perform an analysis for the influence of the partition in different 
direction on the execution performance. 
Method 3 
In this method, we partition the image data along the vertical direction according to the double 
number of the threads as shown in Figure 54(c). Each thread gets one partitioned region first and 
it will get another one when it finishes processing the previous one. Each of the threads will 
continue the steps mentioned above until all the partitioned regions are processed. 
Method 4 
80 
 
 
 
五、 結果與討論 
 Non-uniform cache architecture 
 Simulation Parameter 
Table 7: Target System Simulation Parameters. 
Configuration Parameters 
Snoop2 Ideal PL1 L1&L2 
#cores/ ISA/ Freq. 8/  Alpha/ 1GHz 
Cache Line Size 64B 
L1I$ Size/ 
banks/Assoc. 
32 KB each bank/ 
8 banks/4-ways 
8 KB each bank/ 
8 banks/4-ways 
L1D$ Size/Assoc. 32 KB each bank/ 8 banks/4-ways 
8 KB each bank/ 
8 banks/4-ways 
L1 Load-to-Use Latency 1 cycle/ 
slice 1 cycle 
L2 Cache Size/Assoc. 
L2 Avg. Latency N/A 
128KB/ 
32-way 
20 cycles 
Network Configuration SC_Ring Bus 
Snoop 
Bus 
Coherence Type (MOESI) N/A Snoop 
Worst Case L1 Hit Latency 
(Contention-free) 11 cycles N/A 
External Memory Latency 100 ns 
 
For Snoop2-NUCA experiment, we simulate an Alpha ISA-based 8-core chip-multiprocessor 
which is implemented by a full-system execution-driven simulator M5. We also use Ruby 
simulator to model cache coherence state, and combine it with M5. Table 7 shows the target 
system parameters. We use Ideal configuration as our Snoop2 architecture’s perfect status. This 
work evaluated the SPLASH-2 and PARSEC benchmarks with inputs which are shown in Table 8. 
This experiment constructs snoop2-NUCA, uniform L1 cache architecture, (i.e., ideal in this 
experiment), private L1 with coherency, DCC architecture and tradition L1 and L2 cache 
architecture. There are some latency, bandwidth, and power results for comparison. 
 Benchmark Workloads 
The study focuses on parallel workloads, we use shared memory programs from the 
PARSEC and SPLASH-2 benchmark suits. All of the PARSEC benchmarks use the medium 
dataset. Table 8 also shows the detail of the input data of SPLASH-2 workload used in our 
experiments. The analysis is carried out to study the region of interest (ROI) of SPLASH-2 and 
PARSEC, respectively. Following the approach supported by M5 simulator with creating a 
checkpoint file to forward fast to the beginning of ROI. Results are collected over the end of ROI. 
 
Table 8: Benchmark Workloads and Inputs. 
Shared Memory Applications 
SPLASH-2 PARSEC 2.1 
Program Input Program 
barnes 65,536 particles blackscholes fluidanimate 
cholesky tk29.O bodytrack freqmine 
fft 4,194,304 data points canneal streamcluster 
fmm 65,536 particles dedup swaptions 
lucontig 1024×1024 matrix, 64×64 blocks facesim vips 
lunoncontig 1024×1024 matrix, 64×64 blocks ferret x264 
82 
 
 
 
0.7% 3.7% 12% 1.8% 1.8% 2% 0.4% 3.6% 4.6% 1.3% 1.8% 0.4% 0.1% 2.6% 
0.0
1.0
2.0
3.0
4.0
5.0
6.0
N
o
rm
a
liz
ed
 
to
 
PL
1
L1 Miss Rate
PL1
L1L2
Ideal_L1NUCA
 
Figure 66: Comparisons of L1 miss ratio in PARSEC 2.1. 
 
 Off-Chip Memory Bandwidth 
The Figure 67 and Figure 68 show the results of last level cache (LLC) bandwidth reduction 
in SPLASH2 and PARSEC 2.1. 
Because there is only one cache level hierarchy, we estimate L1 as last level cache for PL1 
and ideal L1-NUCA architectures. The LLC bandwidth stands for requesting data from off-chip 
memory. We expect there is lower bandwidth in cache architecture, because of bandwidth wall 
issue. As shown in Figure 8- 3 and Figure 8- 4, ideal L1-NUCA has smallest off-chip bandwidth 
than other cache architecture. Average bandwidth reduction is 25% in SPLASH2 and 19% in 
PARSEC 2.1 in ideal L1-NUCA compared with private L1 cache. 
 
123 57 70 6 76 1389 44 1045 854 190 294 159 359 
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
N
o
rm
a
liz
e
d 
to
 
PL
1 
(M
B)
Total Last Level Cache Bandwidth
PL1
L1L2
Ideal_L1NUCA
 
Figure 67: Comparisons of LLC bandwidth reduction (MB) in SPLASH2. 
84 
 
 
 
588 1301 2845 998 1332 639 353 2043 1795 994 943 107 148 1083 
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
N
o
rm
a
liz
e
d 
to
 
PL
1 
M
B
/s
Average Last Level Cache Bandwidth
PL1
L1L2
Ideal_L1NUCA
 
Figure 70: Comparisons of LLC bandwidth reduction (MB/s) in PARSEC 2.1. 
 NUDA Debugging 
In this work, we contribute two different simulation environments for the NUDA evaluation. 
Table 1 shows a parallel simulator mcore in the context of the SPLASH2 benchmarks. In order to 
support more benchmarks, we also use Intel PIN for this purpose. PIN is a dynamic 
instrumentation tool that allows users to contribute their PIN tools for different purposes. We 
used seven race-free SPLASH2 benchmarks to evaluate the NUDA. Those benchmarks work 
with a lock-based multithreading programming model. 
 
Table 9: Target System Parameters. 
 
86 
 
 
positives. The system offers negligible slowdown (0.51~3.06%↓), and supports user defined 
assertions. Table 10-1 also shows the related hardware cost estimates for several popular 
many-core processors, interconnection units, and memory, by CACTI 5.3. We assume that the 
many-core environment is composed of 64 Intel Atom processors, with 16KB I/D cache for each 
core, and 16MB L2-NUCA for sharing. The total estimated area is 564 mm2 under 65nm 
technology. Comparing HARD and NUDA, the main factors in our estimates were memory-usage 
and interconnections. Other elements (logic, FSM, etc.) do not impact the chip area. In HARD, 
the BFvector (2B/per line) in the L1 cache is estimated from SRAM usage, while the BFvector 
(2B/per line) in the L2 cache is estimated from DRAM usage. There are 8 NUDA nodes in our 
proposed system. In addition, the width of the ring interconnection that links all 9 nodes (include 
the many-core ICE) is 1B, and we also include the ring routers. As shown in Table III, the HARD 
has 0.98% (5.53/564) of the area cost compared with the proposed many-core system, and this 
work has 0.37%. 
 
Table 11: Comparisons of Race Detection Methods (64-Core) 
 
 
Table 12 compares the race detection results. We modified the SPLASH2 macro to create 
delta locks, which involve assigning different locks every time a lock is acquired. The software, 
produced too many false positives and the execution speed is slower. This work does not compare 
with HARD because HARD cannot be modeled precisely in a 64-core system. However, 
according to the paper of HARD, it reports false positive and false negatives on account of the 
Bloom filter and L1 cache misses. In addition, we emphasize that the NUDA is not only a race 
detector but is also a debugging platform. 
 
88 
 
 
 
Figure 73：單核心的 Linux，CCOS 與 pthread 之間的關係。CCOS 可以執行於 core2~core3，而 Linux 執行於
core1，CCOS 提供 pthread interface。 
 
 Power estimation & SpMT 
本研究計畫的目的在於提升 low power compilation techniques及 thermal management 至
世界水準，設定研究技術指標作為研究追求的目標，並打算將研究成果投稿至 low 
power、temperature 或者 compiler 相關一流的國際會議以及期刊，具體的敘述如下表
所列。 
 
研究題目 擬達成之指標 擬發表之研討會 
擬發表之
期刊 
Reliable, adaptive variable DFSs Power saving up to 20% - 30% in total 
ACM 
ISLPED’09,    
ACM PLDI’09, 
IEEE ICPP’09   
or others 
ACM TECS,  
ACM TPLS, 
or others 
 
實作項目 
 
 Power estimation 
 Reliable, adaptive variable DFSs 
 Multithreading library 
 Application program interface 
 Runtime library 
 Power optimization tool 
 Code analysis 
 Evaluating power model 
 Multiple & variable DFSs 
 
Power estimation 目前結果 
 
利用之前所提到的方法，求出的結果如表三所示，其中列出了每個功能單元的功率結果，
與實際結果之誤差，表中 pattern-1、pattern-3、pattern-4、pattern-5 是使用來逼近之程式，
90 
 
 
 
Figure 74 所示，平均的動態功率所佔之比率，由於 mrf_top 這個功能單元所占之比例
高達 45%，而我們評估的結果，在這個功能單元上誤差率都小於 2%之內，由於這個原
因，使得我們的結果表現非常的準確，不過可以評估的非常準確是因為 mrf_top 這個
功能單元本身是暫存器，所以被存取的模式非常好去追蹤出來，所以導致結果非常之
準確。在 Figure 75 中也可以看到 mrf_top 這個功能單元之結果，是處於一個誤差非常
小的狀態。 
 
 
Figure 74. 每個功能單元之動態功率所占之比例 
 
 
Figure 75. 正規劃之平均動態攻略消耗 
 
而在於整體而言，那些誤差較大的功能單元是因為有些因素尚未考慮進去，這部分將
會去參考子計畫一實際硬體設計的資料，從這部分抓到更準確的行為，進而估算出更
準確的值。 
92 
 
 
 All-digital Thermal Sensor 
 全數位自動校正智慧型溫度偵測晶片 
 
Figure 76：The microphoto of the proposed all-digital thermal sensor. 
本計畫所提出之可自我校正全數位溫度感測器，經由聯電 65nm Standard Performance 
CMOS process 下線測試，晶片的顯微鏡圖如 Figure 76 所示。我們將三顆 Sensor 從溫度 0°C 
~ 60°C，以每隔 5°C 的間距來做量測，其量測結果如 Figure 77 所示，其中 predict_temperature 
為理想的溫度曲線 (slope = 1)。由於我們只能針對 Fast, Typical 和 Slow 三種 process corner
來做模擬，算出各個 corner 的曲線 slope，所以除非晶片在製程上剛好落在此三種 corner
上，不然就會產生額外的誤差；從 Figure 77 中我們可以看出，量測曲線的 slope 都比理想
溫度曲線的 slope 來的小，也就是說我們的 chip 有可能是落在 Fast case 與 Typical case 之間
並且 sensor 選擇了 Fast case 的 slope；或者是落在 Typical case 與 Slow case 之間，並選擇
了 Typical case 的 slope。Table 15 為此三顆 Chip 的量測數據分析與統計。 
Table 15：The accuracy of the three chips measurement results. 
 Max. Min. Avg. Unit 
Errors 3.6 - 5.8 ± 2.39 °C 
 
在總計畫 Criti-Core 架構設計與系統環境建置方面的配合上，我們也嘗試在 FPGA 板上
實現 all-digital thermal sensor，並配合子計畫一，將 Thermal Sensor 與子計畫一的 Criti-Core 
CPU 來整合並實現多核心 CPU 架構的溫度監控，如 Figure 78 所示。因為在 FPGA 板上
所實現的溫度感測器準度較差，因此本項測試只是為了配合子計畫一與子計畫三與總計
畫，呈現跨子計畫間的整合，因此可以呈現出目前的系統溫度。透過使用吹風機直吹 FPGA 
94 
 
 
本計畫所提出之 Delay Monitor，經由聯電 65nm Standard Performance CMOS process 
下線測試，晶片的顯微鏡圖如 Figure 79 所示。Figure 80 為測試晶片的量測結果，可以看出
在不同的電壓變化下，本計畫所提出之 Delay Monitor 都可以自動調整工作頻率，確保系
統工作的正確性。 
 
Figure 80：The optimal frequency of the test chip with voltage variations. 
 功率消耗模型 
本計畫中建立子計畫一所提供的 Criti-Core CPU 的功率消耗模型(Power Model)，並且提
供給子計畫三，來做為編譯器在編譯時，可以達到降低功率消耗的排程與動態功率最佳化
使用。Figure 81 顯示為子計畫一 Criti-Core CPU 的架構圖。 
 
Figure 81：子計畫一的 Criti-Core CPU 架構圖. 
首先以電路的觀點來建立其 Power Model，並且使用 Cadence 的 RTL Compiler 來輔助。
我們使用聯電 65nm 製程，在工作電壓為 1.0V 下，使用 RTL Compiler 合成 Criti-Core 
CPU。 Criti-Core CPU 可分類出幾項較大的 Functional Unit，有 Multiply, Shifter, Decoder, 
Reg_ControlSignal, ALU, Encoder, Branch, Data_to_RegFile, Mrf_top and Mre_top 共 10 種。此
外使用 8 種 Test Patterns 來模擬電路，Table 16 為這些 Test Pattern 的說明與 instruction count 
資訊。其中 pattern_8 與 pattern_9 為 MPEG-4 Encoder 程式的例子，使用此大型程式來
96 
 
 
因此藉由此 Power Model，與這些 Functional Unit 的執行次數，就能估算出 Criti-Core 
CPU 的功率消耗，此外我們也提供個別 Test Pattern，利用 RTL-Compiler 所模擬出來的功率
消耗，如 Table 18 所示。由表四中可以觀察到，因為 Criti-Core CPU 當初設計時，並沒有
將未使用到 Functional Unit 如 Multiply 做輸入的阻隔，因此造成各式 pattern 跑出來的平
均功率差異度並不是很大。CPU 工作消耗的功率主要還是由工作頻率所決定。 
 
Figure 82：Instruction-type Power Model by PowerMixerIP. 
接著我們使用 PowerMixerIP，將 Power Model 從電路層級，變為指令層級，歸納整理
成幾種 Instruction Type，有 MEM_LOAD, MEM_STORE, BRANCH, BOOLEAN_OP, MOVE, 
ARITHMETIC_ADD_BUS, ARITHMETIC_MUL, ARITHMETIC_DIV and OTHERS，其中
OTHERS 我們是將其餘使用頻率較少的指令歸納成一種。當執 PowerMixerIP 模擬之後就會
顯示分類過後 Instruction Type 的 Power Model，如 Figure 82 所示。我們一樣是使用先前的
8 個 Test Patterns 來做測試。 
Table 19：Comparison between PowerMixer and PowerMixerIP 
Pattern 
Name 
PowerMixer 
Power(mW) 
*PowerMixerIP 
Power(mW) Error(%) 
Pattern_1 8.04 8.81 9.57 
Pattern_3 7.56 8.37 10.7 
Pattern_4 7.94 8.68 9.31 
Pattern_5 7.7 8.78 14 
Pattern_6 8.56 8.64 0.93 
Pattern_7 8.27 8.76 5.93 
Pattern_9 8.98 8.8 2 
PowerMixer 為 Gate-level 層級的 Power 分析軟體，PowerMixerIP 則是將其跑出的數
據，建立成 IP 層級的 Power Model，為了檢測 Figure 82 由 PowerMixerIP 所建立的 Power 
Model 的準確性，我們將其結果與 PowerMixer 比較，並顯示於表五。由表五中可以看出，
98 
 
 
in-loop filter, and X86 PC with 8 cores. As shown in the table, we have sped up the performance 
of H.264 decoding about 1.15 to 1.76 times. 
Table 21. Performance improvement of the proposed parallelized H.264 decoder 
Sequence Resolution 1 Thread (sec) 10 Threads (sec) Improvement 
Foreman QCIF 1.65 1.43 1.15 
Foreman CIF 3.33 2.83 1.18 
Harbour D1 12.73 9.61 1.32 
CoralReefAdventure HD720 22.44 15.62 1.44 
CoralReefAdventure HD1080 60.30 34.17 1.76 
 
 3-D Depth Map Generation 
Proposed 3-D Depth Map Generator 
The experimental platform is a PC with Intel Core 2 Duo 2.13GHz, 2GB RAM, Windows 
XP, and Visual Studio 2008. The adopted image resolutions are 1920×1088 (HD1080), 1024×768 
(HD720), and 320×240 (QVGA). Table 22 shows the average execution time of the test images of 
different resolutions. It also includes the execution time of using original methods. By using the 
proposed algorithm in QVGA test images, we can achieve about 90% of complexity reduction in 
terms of execution time. 
According to the proposed depth map, we output the depth map and the original image to the 
3-D monitor developed by Philips to verify the quality of the generated depth map visually. 
Referring to the literature, we use a similar subjective quality evaluation approach performed by 
30 persons. All the input images are given a score that is decided by all the 30 persons to decide if 
the input image owns senses of stereo and reality. Using the proposed algorithm, most of the 
input images obtain high scores, which mean a good quality of the proposed depth map. 
Table 22. Average execution time of the proposed algorithm (Unit: second) 
 
Original Proposed algorithm 
Resolution 320×240 (QVGA) 
320×240 
(QVGA) 
1024×768 
(HD720) 
1920×1088 
(HD1080) 
Sobel filter 0.047 0.016 0.266 0.688 
Hough Transform 0.219 0.047 0.234 0.453 
VRD 0.016 0.016 0.016 0.016 
Segmentation 0.940 0.001 0.016 0.062 
Merging depth 0.015 0.015 0.047 0.172 
JBF 0.043 0.032 0.296 0.828 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/12/12
國科會補助計畫
計畫名稱: 總計畫(2/2)
計畫主持人: 陳添福
計畫編號: 99-2220-E-009-070- 學門領域: 晶片科技計畫--整合型學術研究
計畫
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 產出晶片 50 顆。 
2. 1 場國內研討會及 1場國際研討會 
3. FPGA 雛形系統展示 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
