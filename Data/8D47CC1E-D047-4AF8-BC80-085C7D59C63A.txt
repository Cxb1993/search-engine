 2 
illumination conditions. Based on the extracted surface 
characteristics, a generative model of the object [14], 
which approximates the object’s appearance under a 
restricted range of illumination conditions, was 
determined. 
Since multiple images were used by Weiss and 
Yuille et al., the applicability of their techniques is 
somewhat limited. Tappen et al. [29] proposed a 
method for recovering intrinsic components from a 
single image. A set of derivative filters are first applied 
to the input image giving rise to a set of derivative 
images. The pixels of the derivative images are 
classified as being reflectance-related or 
illumination-related based on their color and intensity. 
However, unsatisfactory results were observed. Tappen 
introduced a process, called the generalized belief 
propagation process, to improve the results. Thereupon, 
a de-convolution process was applied to the classified 
derivative images to obtain the intrinsic images of the 
input image. The Tappen method took about six 
minutes to categorize pixels and another six minutes to 
perform the generalized belief propagation process. To 
use with real-time applications, the time complexity of 
the Tappen method should be reduced. 
Recently, Matsushita et al. [21] introduced an 
illumination eigenspace into Tappen’s computational 
framework. The eigenspace, which is built in advance, 
provides information for categorizing the pixels of 
derivative images. Since no information is computed 
during pixel classification, the Matsushita approach can 
operate in real time. However, over a period of 120 
days Matsushita collected a set of 2048 images from a 
scene for generating its illumination eigenspace. 
Apparently, Matsushita’s method can not be applied to 
time-varying scenes (i.e., dynamic scenes). To be 
applicable to dynamic scenes, the information for 
classifying pixels of derivative images must be 
computed directly from the input image, and for 
real-time applications, the computation should be 
efficient.  
Both information selection and classification 
strategy play critical roles in determining the goodness 
of classification result, which in turn determines the 
robustness of the proposed intrinsic image 
decomposition method. In this study, three measures 
(chromatic, intensity contrast and edge sharpness) and a 
hierarchical classification strategy are proposed for 
classifying the pixels of derivative images. The details 
of the overall process of intrinsic image decomposition 
are given in Section 2. Section 3 is devoted to a 
discussion of invariant chromatic characteristics that 
are used to define the chromatic measure. Experimental 
results are presented in Section 4 and concluding 
remarks and future work in Section 5.  
2. Intrinsic Image Decomposition 
Figure 1 shows a flowchart for the proposed 
approach to extracting intrinsic images from a single 
color image. The approach consists of four major stages: 
logarithmic edge generation, characteristic measure 
calculation, edge classification, and intrinsic image 
formation. Let ( , , )r g bI I I I=  denote the input color 
image, where ,r gI I  and bI  are the red, green and 
blue components of the input image. Each color 
component iI  ( , ,i r g b= ) is modeled as 
i i iI R L= × , where iR  (reflectance) and iL  
(illumination) are the intrinsic images of iI . The process 
of Figure 1 is applied to each of the color components. 
The intrinsic images, R and L, of the input image are 
finally formed from iR  and iL by ( , , )r g bR R R R=  
and ( , , )r g bL L L L= . 
2.1. Logarithmic edge generation 
Let iI  be any color component image. In the 
logarithmic edge generation stage, iI  is first 
transformed into the logarithmic domain, 
              
log log( ) log logi i i i i i i iI I R L R L R L′ ′ ′= = × = + = +
,           (1) 
resulting in an additive composition of reflectance 
and illumination. This also reduces the dynamic range of 
iI  so as to increase its intensity contrast. The 
transformed image iI ′  is next convolved with a 
horizontal derivative filter hf  and a vertical derivative 
filter vf , resulting in two derivative component images 
i
hI ′  and ivI ′ . In this study, the Prewitt derivative filters 
are utilized. From the derivative component images, we 
generate an edge map iEM  of size r c× , which is the 
same as the image size. Each element e of map iEM  
contains a derivative magnitude ( )im e  and a derivative 
orientation ( )io e , where 
2 2 1/ 2( ) ( ( ) ( ))i i ih vm e I e I e′ ′= +  and 
1( ) tan ( ( ) / ( ))i i ih vo e I e I e− ′ ′= .     (2) 
 
Fig. 1. Proposed flowchart for extracting intrinsic images 
from a single image. 
 
2.2. Characteristic measure calculation 
In the second stage of characteristic measure 
calculation, three measures, chromatic, intensity contrast 
and edge sharpness, are calculated for each image pixel. 
The calculated measures will be used in the next stage for 
classifying the pixels of derivative component images into 
illumination-related or reflectance-related edge pixels. In 
the following, we will temporarily treat pixels as edge 
pixels when calculating their characteristic measures. 
A. Chromatic measure 
Consider a pixel p and let pw  be a neighborhood 
window centered at p. We look for a straight line l passing 
through p within pw . The line is formed from the pixels 
whose sum of derivative magnitudes is maximal. This line 
 4 
successfully interpret the edge types of all the line 
segments in Figure 3. However, in reality various 
complicated situations may occur, such as specularities, 
inter-reflections, and scenes that are too bright or too 
dark. These can result in indeterminate or numerically 
unreliable values for characteristic measures. Therefore, 
in this study we restrict ourselves to outdoor scenes 
during daytime and scenes without very high or very 
low brightness. Note that outdoor scenes provide ample 
fields of view for cameras so that the portion of 
specularities is very small relative to the entire scene.  
During the classification process, some pixels 
have not been classified. For these pixels, we introduce 
an evidence following process which incrementally 
determines their edge types through progressive 
propagations of local evidence. Let iGM  be the array, 
called the classification map, which contains the 
classification results of all pixels. Let e be some 
element of the map whose edge type has not yet been 
determined. Examining edge map iEM , let ( )im e  
and ( )io e  be the derivative magnitude and derivative 
orientation of e. We check its two adjacent neighbors, 
say 
1n  and 2n , in the direction ( )
io e  and select the 
one with the larger derivative magnitude. If the edge 
type of the selected neighbor has been decided, we set 
( )iGM e  to the edge type of that neighbor,  
1 2
( )
( ) arg (max{ ( ), ( )})
i
iG
i i i
G
M n
M e m n m n= .  
However, if the neighbor has not yet been 
classified, we continue to the next undetermined 
element of iGM . The above process is repeated until 
all the elements of iGM  are classified. This completes 
the evidence following process. After this, every 
element of iGM  contains a symbol, either Rs  or Is , 
which denote reflectance- and illumination-related edge 
types, respectively.  
The classification maps, iGM  (i = r, g, b), 
obtained from different color components may not be 
mutually consistent. We say a pixel is ambiguous if it 
has different edge types in the three classification maps. 
For such a pixel, we use a vote to decide a unique edge 
type for the pixel. Let p be such an ambiguous pixel. 
We add up its derivative magnitudes to obtain sums 
( )RS p  and ( )IS p  according to the edge types of 
the pixel determined in the classification maps, i.e., 
, , ( )
( ) ( )
i
RG
i
R
i r g b M p S
S p m p
= =
= ∑ ∑  and 
, , ( )
( ) ( )
i
IG
i
I
i r g b M p S
S p m p
= =
= ∑ ∑ . We then decide the 
edge type of p in map 
GM  by 
,
( ) arg (max{ ( ), ( )})
R I
G R I
S S
M p S p S p= . For 
non-ambiguous pixels, we simply copy their edge types 
to map 
GM . 
Based on the resultant classification map GM , we 
generate reflectance-related ihR , ivR  and 
illumination-related ihL , ivL  derivative component 
images from derivative component images ihI ′  and ivI ′ . 
For a pixel p, 
if ( )G RM p s= , then 
' '( ) ( ), ( ) ( ) i i i ih h v vR p I p R p I p= =  and 
( ) 0, ( ) 0 i ih vL p L p= = ;  (4) 
if ( )G IM p s= , then ( ) 0, ( ) 0 i ih vR p R p= =  and 
' '( ) ( ), ( ) ( ) i i i ih h v vL p I p L p I p= = . 
This step separates derivative component images as 
reflectance-related or illumination-related. We call 
i
hR , ivR , ihL , and ivL  the intrinsic derivative component 
images. 
2.4. Intrinsic image formation 
    At the final stage of intrinsic image formation, 
de-convolution [31] is applied to the intrinsic derivative 
component images, from which the logarithmic 
reflectance iR ′ and illumination iL′  component images 
are computed, 
,
*( * )i r ij j
j h v
R g f R
=
′ = ∑ ,  
,
*( * )i r ij j
j h v
L g f L
=
′ = ∑ ,           (5) 
where rjf  is the reversed function of jf  defined 
as ( ) ( )rj jf p f p= − . The symbol * denotes discrete 
convolution, and g is a normalization term satisfying 
,
( )rj j
j h v
g f f δ
=
∗ ∗ =∑ , 
where δ is the Kronecker delta function.  
To compute iR ′ , we convolve the both sides of 
Equation (5) with 
,
r
j j
j h v
f f
=
∗∑ , 
, , ,
, ,
, ,
( ) ( ) [ *( * )]
                          [( ) ]*( * )
                           ( * ) *
r i r r i
j j j j j j
j h v j h v j h v
r r i
j j j j
j h v j h v
r i r i
j j j j
j h v j h v
f f R f f g f R
f f g f R
f R f Rδ
= = =
= =
= =
′∗ ∗ = ∗ ∗
= ∗ ∗
= ∗ =
∑ ∑ ∑
∑ ∑
∑ ∑
. 
Applying Fourier transform ℑ  to the above equation, 
, ,
(( ) ) ( * )r i r ij j j j
j h v j h v
f f R f R
= =
′ℑ ∗ ∗ = ℑ∑ ∑ . 
By the convolution theorem, 
 6 
2( , ) ( , )[(1 ( )) ( , ) ( )]p p p p pE i Rλ λ ρ λ ρ= − + ,    
(8) 
where ( , )pi λ  is the illumination spectral energy, 
( )pρ  is the Fresnel surface reflectance, and 
( , )pR λ  is the material reflectivity, which depends on 
the surface geometry and the viewing and incidence 
angles of light. The Geusebroek reflectance model is 
still idealized. Several assumptions have been 
incorporated in the Geusebroek model, including thick 
materials, planar surface patches, and uniform colored 
patches. For outdoor scenes, these assumptions seem to 
be reasobable. In addition to the above assumptions, 
different imaging conditions were imposed when 
Geusebroek et al. derived invariant chromatic 
characteristics. The imaging conditions imposed 
include equal energy spectrum, matte, dull surfaces, 
uniform illumination, colored illumination, and 
uniformly colored surfaces. Accordingly, five groups of 
invariant chromatic characteristics, denoted by H, C, W, 
N, and U, are derived. Each group consists of a 
fundamental invariant chromatic characteristic and a 
hierarchy of spectral and spatial derivatives of the 
fundamental characteristic. 
3.3. Chromatic invariants 
The fundamental invariant chromatic 
characteristics of the five groups are 
EH
E
λ
λλ
= , EC
E
λ= , pEW
E
= , 
2
p pE E E EN U
E
λ λ−= = .        (9) 
They are invariant under the five different imaging 
conditions mentioned above. For example, assuming an 
equal energy spectrum, i.e., ( , ) ( )p pi iλ = , λ∀ , Eq. 
(8) becomes 
 
2( , ) ( )[(1 ( )) ( , ) ( )]p p p p pE i Rλ ρ λ ρ= − + .    (10) 
Differentiating the above equation with respect to λ  
twice, we obtain 
2( , ) ( )(1 ( )) ( , )E i Rλ λλ ρ λ= −p p p p  and 
2( , ) ( )(1 ( )) ( , )E i Rλλ λλλ ρ λ= −p p p p . 
Substituting these equations into /H E Eλ λλ= , 
we obtain ( , ) / ( , )H R Rλ λλλ λ= p p . It is clear that 
H depends only on the material reflectivity ( , )pR λ . 
If we repeatedly differentiate H with respect to λ  and 
p, a hierarchy pm nHλ  of spectral and spatial 
derivatives of H can be obtained. The hierarchy 
pm nHλ  also depends on ( , )pR λ  only.  
Let us further assume that the scene consists of 
only matte, dull surfaces. Since the Fresnel reflectance 
coefficients of matte, dull surfaces are close to zero, i.e., 
( ) 0ρ ≈p , Eq. (10) reduces to 
( , ) ( ) ( , )E i Rλ λ=p p p . Differentiating with respect to 
λ  gives ( , ) ( ) ( , )E i Rλ λλ λ=p p p . Substituting this into 
/C E Eλ= , we get ( , ) / ( , )C R Rλ λ λ= p p  and see 
that C and its hierarchy of derivatives depend only on the 
material reflectivity ( , )pR λ  too. The same discussion 
can be applied to the other groups of chromatic 
characteristics under different imaging conditions.  
Rather than using all the invariant chromatic 
characteristics suggested by Geusebroek, eleven 
chromatic characteristics, H, pH , C, Cλ , pC , pCλ , 
W, Wλ , Wλλ , N, and Nλ , are adopted for this study. 
They are given below and in Eq. (9),  
2 2
p v hH H H= + , where 2 2i ii
E E E EH
E E
λλ λ λ λλ
λ λλ
−= + , 
,i v h= , 
EC
E
λλ
λ = , 2 2p v hC C C= + , where 
2
i i
i
E E E EC
E
λ λ−= , 
2 2
p v hC C Cλ λ λ= + , where 2i ii
E E E EC
E
λλ λλ
λ
−= ,               
(11) 
2 2
v hW W Wλ λ λ= + , where ii EW E
λ
λ = , 
2 2
v hW W Wλλ λλ λλ= + , where ii EW E
λλ
λλ = ,  
2 2
v hN N Nλ = + , where 
2 2
3
2 2i i i i
i
E E E E E E E E E EN
E
λλ λλ λ λ λ− − += . 
Note that the hue component of a color is defined as 
1
maxtan λ−  where max /E Eλ λλλ = − . The characteristic 
H ( /E Eλ λλ= ) is related to the hue and the dominant 
color of the material. The characteristic H p , which is the 
spatial derivative of H, is associated with the hue gradient 
and can be used to detect color edges. The characteristic C 
is the normalized color; its spectral derivative Cλ , 
spatial derivative pC , and spatio-spectral derivative 
pCλ  can be use to detect spectral/spatial transitions in 
object reflectance. The chromatic characteristic W 
indicates normalized edge magnitude. The associated 
Wλ  and Wλλ  represent the spectral slope and curvature 
of normalized edge magnitude, respectively. Finally, the 
characteristics N and Nλ  indicate material transitions by 
detecting changes in object reflectance. 
3.4. Measurements 
One of the major reasons we chose the above 
chromatic characteristics is because they can be 
 8 
database to study the properties of chromatic 
characteristics. 
Let 1I , 2I  and 3I  be the images of the same 
scene S taken under diffuse, ambient and direct lighting 
conditions, respectively. Let C specify any chromatic 
characteristic and 1( )C p , 2 ( )C p  and 3( )C p  
represent the values of the chromatic characteristic at 
pixel p for the three images, respectively. Ideally, 
1 2 3( ) ( ) ( )C C C= =p p p , indicating that the 
chromatic characteristic of the same material are 
invariant under different lighting conditions. Let 
( )Cσ p  denote the standard deviation of 1( )C p , 
2 ( )C p  and 3( )C p , i.e.,  
 
3
2 1/ 2
1
1( ) [ ( ( ) ( )) ]
2
p p pC i
i
C Cσ
=
= −∑ , where 
3
1
1( ) ( )
3 ii
C C
=
= ∑p p . 
We define the degree of invariance of chromatic 
characteristic C for scene S as 
( )
( )
p
p
pC C
n
S ε σ= +∑A ,  
where np  is the number of image pixels and ε  is a 
small positive number to prevent the denominator from 
being zero.  
Figure 5 shows the calculated degrees of 
invariance of distinct chromatic characteristics. In this 
figure, the horizontal and the vertical axes represent 
“scene object” and “degree of invariance”, respectively. 
There are eleven curves in the figure, each 
corresponding to a particular chromatic characteristic. 
The curves can be roughly divided in terms of variation 
of degrees of invariance into two groups, one associated 
with the characteristics Cp, Cλp, W, Wλ, Wλλ, N, Nλ. and 
the other associated with H, Hp, C, Cλ. The first group 
has a much larger variation than the second. A large 
variation indicates a low degree of invariance with 
respect to different scene objects. Accordingly, we 
choose the group H, Hp, C, Cλ for further experiments. 
 
  Fig. 5. Degrees of invariance of chromatic 
characteristics of the same objects under different 
lighting conditions. 
 
In the next experiment, we examine the invariance 
properties of chromatic characteristics with different 
objects under the same lighting conditions. Let 1I , 2I  
and 3I  be the images of three different objects under 
the same lighting condition L. Similar to the previous 
experiment, we compute the standard deviation 
( )Cσ p  for the three images and then the degree of 
invariance, ( )C LA , of chromatic characteristic C. 
Figures 6 shows the calculated ( )C LA  values of 
chromatic characteristics H, Hp, C, Cλ under diffuse, 
ambient and direct lighting conditions. In this figure, the 
vertical axes represent “degree of invariance” and the 
horizontal axes represent “set of three distinct objects”. 
There are four curves corresponding to the four chromatic 
characteristic H, Hp, C, Cλ, in each of the four plots in the 
figure. In all the plots, the curve associated with the 
characteristic Hp has less invariance than that associated 
with H, C, Cλ. A similar situation is also observed for the 
case of distinct scene objects under different lighting 
conditions (see Figure 7). As a result of these observations, 
we select chromatic characteristics H, C, Cλ for the 
experiments of intrinsic image extraction. 
 
Fig. 6. Degrees of invariance of chromatic 
characteristics of distinct objects under (a) diffuse lighting, 
(b) ambient lighting, and (c) direct lighting. 
Fig. 7. Degrees of invariance of chromatic 
characteristics of distinct objects under different lighting 
conditions. 
 
4.2 Intrinsic Image Extraction 
Both synthetic and real images were used in the 
experiments of intrinsic image extraction. Each input 
image is an RGB color image with a size of 320 by 240 
pixels. The output results include a reflectance image and 
an illumination image extracted from the input image. The 
source code was written in C++ run on a 2.4 GHz 
Pentium based PC. The program took about 3 to 4 
seconds to decompose an input image into its reflectance 
and illumination images. 
A. Synthetic Images 
Figure 8(a) shows a synthetic image used as the 
input image. There are three objects of different colors 
(green, red and blue) present in the image. The green 
object has two thin line segments marked on it. Another 
line segment is drawn across the boundary between the 
blue and the red objects. The red object also contains a 
small ink stain. In addition, a bright highlight appears 
near the bottom of the blue object and a shadow is cast 
across the three objects. By applying the proposed image 
decomposition technique to the input image, the 
decomposed reflectance and illumination images are 
depicted in Figures 8(b) and 8(c), respectively. Note that 
we have reversed the pixels’ color values of the 
illumination image in order to bring out the detected 
highlight and shadow.  
 
Fig. 8. Example using a synthetic image: (a) input 
image, (b) reflectance image, (c) illumination image, and 
(d) illumination image by I / R. 
 
In order to examine the computational error, we 
calculate the intensity mean square error (MSE) between 
the estimated reflectance image and the ground truth. The 
resultant MSE is 0.000127, which indicates the accuracy 
of our computational model. Recall that we modeled an 
 10 
noise removal: (a) input image, (b) edge classification 
map, (c) reflectance image,(d) illumination image, (e) 
edge classification map after noise removal, (f) 
resulting reflectance image, and (g) illumination image. 
 
5. Concluding Remarks 
In this paper, we presented an approach to 
extracting intrinsic images (including reflectance and 
illumination images) from one single image. Three 
measures: chromatic, intensity contrast and edge 
sharpness, which are efficiently calculated from the 
input image by providing the (R,G, B) values of each 
pixel, were employed to classify the pixels of derivative 
images into reflectance-related or illumination-related 
edge pixels. The performance of the proposed approach 
depends on the goodness of the classification result. In 
order to improve the classification result, we used a 
noise removal method and an edge following technique 
to link broken edges. However, these methods are 
post-processing. We prefer pre-processing methods, 
such as increasing the robustness and reliability of the 
current measures and investigating into new ones. We 
leave this work in the future study. 
The proposed approach extracts intrinsic images 
from a single image so that the approach can be applied 
to images of dynamic scenes. Our method takes about 3 
to 4 seconds to complete the intrinsic image 
decomposition of an image. Compared with the 
previous methods [29], [31], [32], the time complex of 
the proposed approach has been greatly improved. 
However, for real-time applications the processing time 
would be further reduced. Referring to Figure 1, there 
are four major steps involved in the intrinsic image 
decomposition process. Of these four steps, the edge 
classification and the intrinsic image formation steps 
together took about 70% of the entire processing time. 
Note that there is an evidence following process that is 
iterative in nature in the edge classification step and 
that the deconvolution process in the intrinsic image 
formation step performs FFT 7 times each taking about 
0.1 to 0.15 seconds. We will concentrate on reducing 
the processing times of the aforementioned 
time-consuming processes in our future work. 
6. References 
[1] E. Angelopoulou, S. W. Lee, and R. Bajcsy, “Spectral 
Gradient: A Material Descriptor Invariant to Geometry 
and Incident Illumination,” The 7th IEEE Int’l Conf. on 
Computer Vision, pp. 861-867, 1999.   
[2] H. G. Barrow, and J. M. Tenenbaum, “Recovering Intrinsic 
Scene Characteristics from Images”, Computer Vision 
Systems, A. R. Hanson and E. M. Riseman (eds.), 
Academic Press, pp. 3–26, 1978. 
[3] M. Bell and W. T. Freeman, “Learning Local Evidence for 
Shading and Reflection,” Int’l Conf. on Computer Vision, 
pp. 670-677, 2001. 
[4] S. L. Chang, L. S. Chen, Y. C. Chung, and S. W. Chen, 
“Automatic license plate recognition,” IEEE Trans. on 
Intelligent Transportation Systems, vol. 5, no. 1, pp.42-54, 
2004.  
[5] M. S. Drew, G. D. Finlayson, and S. D. Hordley, 
“Recovery of Chromaticity Image Free from Shadows via 
Illumination Invariance,” IEEE Workshop on Color and 
Photometric Methods in Computer Vision, pp. 32-39, 2003. 
[6] C. Y. Fang, S. W. Chen, and C. S. Fuh, “Automatic Change 
Detection of Driving Environments in a Vision-Based Driver 
Assistance System,” IEEE Trans. on Neural Networks, vol. 
14, no. 3, pp. 646-657, 2003. 
[7] H. farid and E. H. Adelson, “Separating Reflections from 
Images by Use of Independent Component Analysis,” 
Journal of the Optical Society of America, vol. 16, no. 9, pp. 
2136-2145, 1999. 
[8] G.. D. Finlayson and S. D. Hordley, “Color Constancy at a 
Pixel,” Journal of the Optical Society of America, 18(2), pp. 
253-264, 2001. 
[9] G. D. Finlayson, M. S. Drew, and C. Lu, “Intrinsic Images 
by Entropy Minimization,” European Conference on 
Computer Vision, pp. 582-595, Prague, 2004. 
[10] G. D. Finlayson, S. D. Hordley, L. Cheng and M. S. Drew, 
“On the Removal of Shadows from Images,” IEEE Trans. on 
Pattern Analysis and Machine Intelligence, vol. 28, no. 1, pp. 
59 – 68, 2006. 
[11] D. A. Forsyth and J. Ponce, Computer Vision, A Modern 
Approach, Chapter 1, Prentice Hall, New Jersey, pp. 17-18, 
2003. 
[12] B. V. Funt, M. S. Drew, and M, Brockington, “Recovering 
Shading from Color Images,” The 2nd European Conf. on 
Computer Vision, pp. 124-132, 1992. 
[13] A. S. Georghiades, A.W. M. Smeulders, and R. van den 
Boomgaard, “Measurement of Color Invariants,” The IEEE 
Conf. on Computer Vision and Pattern Recognition, vol. 1, 
pp. 50-57, 2000. 
[14] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman, 
“From Few to Mant: Generative Models for Recognition 
under Variable Pose and Illumination,” IEEE Int’l Conf. on 
Automatic Face and Gesture Recognition, pp. 277-284, 
2000. 
[15] J. M. Geusebroek, R. van den Boomgaard, A.W. M. 
Smeulders, and H. Geerts, “Color invariance,” IEEE Trans. 
on Pattern Analysis and Machine Intelligence, vol. 23, no. 
12, pp.1338-1350, 2001.  
[16] J. M. Geusebroek, T. Gevers, and A. W. M. Smeulders, 
“The Kubelka-Munk Theory for Color Image Invariant 
Properties,” The 1st Conf. on Color in Graphics, Imaging, 
and Vision, pp. 463-467, 2002. 
[17] G. Healey and A. Jain, “Retrieving Multispectral Satellite 
Images Using Physics-Based Invariant Representations,” 
IEEE Trans. on Pattern Analysis and Machine Intelligence, 
vol. 18, pp.842-848, 1996. 
[18] S. Kamijo, Y. Matsushita, K. Ikeuchi, and M. Sakauchi, 
“Traffic Monitoring and Accident Detection at 
Intersections,” IEEE Trans. on Intelligent Transportation 
Systems, vol.1, no. 2, pp.108–118, 2000.  
[19] T. Leung and J. Malik, “Recognizing Surfaces Using 
Three-Dimensional Texons,” The 7th IEEE Int’l Conf. on 
Computer Vision, pp. 1010-1017, 1999. 
[20] R. F. K. Martin, O. Masoud, and N. Papanikolopoulos, 
“Using Intrinsic Images for Shadow Handling,” The IEEE 
5th Int’l Conf. on Intelligent Transportation Systems, pp. 
1526-155, 2002. 
[21] Y. Matsushita, K. Nishino, K. Ikeuchi, and M. Sakauchi, 
“Illumination Normalization with Time-Dependent Intrinsic 
Images for Video Surveillance”, IEEE Conf. on Computer 
Vision and Pattern Recognition, vol. 1, pp. 3-10, 2003. 
[22] S. G. Narasimhan and S. K. Nayar, “Chromatic Framework 
for Vision in Bad Weather,” The IEEE Conf. on Computer 
Vision and Pattern Recognition, vol. 1, pp. 598-605, 2000. 
 12 
 
 Fig. 2. Hierarchical edge classification process. 
Self shadow
Cast shadow
Light source
B
A
C
E
DI
J G
F
H
 
Fig. 3. Example for illustrating edge classification. 
         
(a)                   (b)                   (c) 
Fig. 4. A doll under (a) diffuse, (b) ambient, and (c) direct lighting conditions. 
 14 
 
      (c) 
Fig. 6. Degrees of invariance of chromatic characteristics of distinct objects under (a) diffuse lighting, (b) 
ambient lighting, and (c) direct lighting.    
 
Fig. 7. Degrees of invariance of chromatic characteristics of distinct objects under different lighting conditions. 
  
(a)                 (b) 
  
                    (c)                 (d) 
Fig. 8. Example using a synthetic image: (a) input image, (b) reflectance image, (c) illumination image, and (d) 
illumination image by I / R. 
 
 16 
   
   
    
        (a)                (b)                    (c) 
Fig. 11. Examples using real scenes: (a) input images, (b) reflectance images, and (c) illumination images. 
   
(a)               (b)            (c) 
  
              (d)            (e) 
Fig. 12. The importance of edge classification: (a) input image, (b) edge classification map, (c) illumination 
image, (d) improved edge classification map, and (e) improved illumination image. 
1 
出席國際會議報告書 
 
報告人姓名: 陳世旺 
服務機構及職稱: 國立台灣師範大學資訊工程學系  教授 
會議時間: 95 年 9 月 17 日至 20 日  
會議地點: 加拿大，多倫多 
會議名稱(中文): IEEE 2006 年智慧型運輸系統國際研討會 
會議名稱(英文): IEEE 2006 International Conference on Intelligent Transportation System 
發表論文題目(中文): 以模糊推理派翠西網路為基礎的危險駕駛事件分析系統 
發表論文題目(英文): Dangerous Driving Event Analysis System by a Cascaded Fuzzy Reasoning  
                  Petri Net 
 
 
一、參加會議經過 
近年來由於各類交通工具在速度上的改良有顯著的進步，人們對於交通傳輸
效率的要求越來越高；而生活水準的普遍提高，幾乎每個家庭都擁有自己的車輛，
使得世界各地的交通工具數量暴增許多，進而導致交通安全問題日益嚴重。另一方
面，由於電腦在軟硬體方面皆進步神速，再加上網路迅速普及，使得發展智慧型運
輸系統（Intelligent Transportation Systems, ITS）來改善並解決各類交通問題成為一
個熱門的研究課題。智慧型運輸系統係藉由先進之電腦、資訊、電子、通訊與感測
等設備的連結與技術的應用，傳遞即時資訊，改善人、車、路等運輸次系統間的互
動關係，進而增進運輸系統之安全、效率與舒適，同時減少交通對環境衝擊之整合
型運輸系統。 
IEEE 2006 年智慧型運輸系統國際研討會於二ＯＯ六年九月十七日至二十日
在加拿大多倫多舉行。會議參與者來自世界各地，包括加拿大、美國、德國、英國、
法國、日本、韓國、台灣以及中國大陸等學者專家，共同探討各國智慧型運輸系統
目前之現況、面臨之困難、解決之方案、與未來之發展等議題，除了學術上理論的
探討外，更重實際應用的可能性。因此，參與的專家學者中，不乏各國交通運輸相
關部門的決策與執行人士。 
3 
除了它們的偵測結果可能互相衝突與矛盾外，子系統個別發出的頻繁的警告訊息對
駕駛者更是容易造成困擾，延遲行車決策時間。若能將偵測子系統偵測到的結果加
以分析，輸出經過整合後的結論供駕駛者參考，對駕駛者的行車安全會更有幫助。
因此，本論文希望能建立危險行車事件分析子系統，目的在分析偵測子系統的偵測
結果，並建立完善的整合機制，輸出較適當與穩定的警告訊息供駕駛者參考。 
本論文在開發危險行車事件分析子系統之前，先建立了高速公路行車事件模擬
子系統，主要是因為要完整的蒐集高速公路上的行車事件資料並不容易，而且在高
速公路上模擬真實的危險行車事件對實驗者的生命安全也會造成莫大的威脅，所以
我們開發了高速公路行車事件模擬子系統，並將模擬結果做為危險行車事件分析子
系統的資料輸入。 
危險行車事件分析子系統是採用 cascaded fuzzy reasoning Petri net(CFRPN)模組
來進行事件的分析、推理，首先將模擬子系統的模擬結果當作資料輸入，再經由
CFRPN 的多階段推導，整合出行車狀況的結論。若系統認為目前車輛處在危險的行
車狀態，則輸出警告訊息提醒駕駛者注意。最後，我們以實驗來驗證危險行車事件
分析子系統的適用性，並希望未來能將此系統嵌入駕駛安全輔助系統中，不僅幫助
駕駛者更輕鬆地開車，同時也提升駕駛的安全性。 
二、與會心得 
本次會議中來自世界各地的專家學者到會議中發表精彩的演講與研究結果，
使我們得以藉此了解世界各國對智慧型運輸系統的看法、實施方式、以及未來的發
展趨勢。另外，利用這個機會也可以與各國的專家學者做意見交流，特別是交通、
運輸、土木等其他非影像處理與電腦視覺領域的與會專家學者，他們提供了許多不
同領域的觀點與見解，非常有助於我們未來研究時所可能面臨之問題的解決。 
2007 年的第十屆 ITSC 研討會將在美國華盛頓州的西雅圖舉行，希望明年仍
5 
Dangerous Driving Event Analysis System  
by a Cascaded Fuzzy Reasoning Petri Net 
*C. Y. Fang, *H. L. Hsueh, and **S. W. Chen 
*Department of Information and Computer Education 
**Department of Computer Science and Information Engineering 
National Taiwan Normal University 
Taipei, Taiwan, Republic of China 
 
Abstract 
This study applies a modified cascaded fuzzy reasoning 
Petri net (CFRPN) model to analyze dangerous driving 
events on a freeway. The dangerous driving events can be 
divided into two groups: (1) the interaction between a 
driver’s vehicle and the road environment, and (2) the 
interaction between a driver’s vehicle and nearby vehicles. 
These two classes of driving events may occur 
simultaneously and lead to certain serious traffic situations. 
The proposed system analyzes these two kinds of events 
determines dangerous situations from data collected by 
various sensors. Since collecting real driving event data on 
freeway is dangerous and time consuming, a data generation 
system is developed to generate the experimental data. Such 
data can help evaluate the performance of the proposed 
analysis system. Finally, experimental results show that the 
proposed system is accurate and robust. 
Keywords：Active driver assistance system, driving event 
analysis, fuzzy reasoning Petri net 
1. Introduction 
Diverse methods for enhancing driving safety have 
been proposed. Such methods can be roughly classified as 
passive or active. Passive methods (e.g., seat-belts, airbags, 
and anti-lock braking systems), which have significantly 
reduced traffic fatalities, were originally introduced to 
diminish the degree of injury from an accident. By contrast, 
active methods are designed to prevent accidents from 
occurring. Driver assistance systems (DAS) [Sch00] are 
designed to bring to alert the driver as quickly as possible to 
a potentially dangerous situation.  
Figure 1 illustrates a block diagram of a DAS, which 
can be split into two major components, for detection and 
analysis. The systems included in the detection component 
are used to collect the relevant data. These data can be 
obtained by various equipments, such as visual sensors, 
infrared ray sensors, laser sensors, ultrasonic wave sensors, 
ladar sensors and global positioning systems (GPS). Other 
sensors are adopted to detect the driver’s stress and 
drowsiness. Many researchers have developed detection 
systems to collect data accurately and quickly from these 
sensors. However, these detection systems generally work 
independently of each other. Few researchers [Mit03] are 
interested in integrating the high-level, abstract information 
from the collected data, for example to determine whether 
dangerous driving events occur. Furthermore, since every 
sensor has its limitations, we believe that integrating the 
detection results from different systems can improve the 
accuracy and efficiency of understanding driving situations. 
This study proposes a dangerous driving event analysis 
system to integrate the outputs from detection systems, and 
thus obtain some reasonable conclusions about the degree of 
danger in the driving environment. 
2. Dangerous Driving Event Analysis System 
Figure 2 shows the flowchart of the dangerous driving 
event analysis system. In this system, the data of driving 
environment acquired by various sensors can be divided into 
two classes, static and dynamic, which are obtained from 
different detection systems. The static data are those 
acquired from the road environment, such as the type or 
content of road signs, the number of lanes and the position 
of tunnel entrances or exits. The dynamic data are those that 
describe the motions of the vehicles, including the driver’s 
vehicle and those nearby.  
The dangerous driving events handled by the proposed 
system are divided into two classes that correspond to the 
two classes of data. The first class contains events relating 
to the behavior of the vehicle in the road environment (i.e. 
static data). The second class is events relating to the 
behaviors of our vehicle on nearby vehicles. Since driving 
events may occur simultaneously and result in certain 
serious traffic accidents, an integration stage is required to 
detect and measure such complicated driving events. The 
measured degree indicates whether the driver is in danger. 
In practice, the real data of the dangerous driving events 
are very difficult to collect because of the danger and time 
taken. Therefore, this study presents a data generation 
system to generate static and dynamic data to measure the 
performance of a dangerous driving event analysis system. 
However, the data generation system can only create driving 
environment data for a freeway. 
3. Cascaded Fuzzy Reasoning Petri Net 
This study develops a modified cascaded fuzzy 
reasoning Petri net (CFRPN) to analyze driving events. The 
CFRPN [Dua01] is designed for syllogistic fuzzy reasoning, 
and comprises several fuzzy reasoning Petri nets (FRPNs) 
that connect with others one by one as a 1D structure. This 
study extends the structure of CFRPN from 1D to 2D, and 
modifies the fuzzy reasoning mechanism in the FRPN to fit 
our applications. The modified CFRPN is also divided into 
several stages, but each stage contains several FRPNs, as 
7 
In the second stage, FRPNs are utilized to reason more 
complex driving events. FRPNs can evaluate the degree of 
danger between a single nearby vehicle and ours. For 
example, “Will any dangerous driving event occur between 
the right vehicle and our vehicle.” In the third stage, FRPNs 
integrate the outputs from the FRPNs in the second stage to 
make a final conclusion on whether a dangerous driving 
event occurs in the entire driving environment. 
The proposed dangerous driving event analysis system 
only deals with motion of vehicles close to the driver’s 
vehicle. Therefore, eight neighboring vehicles positions are 
defined as forward, backward, left, right, left forward, left 
backward, right forward and right backward. The 
relationship between each neighborhood vehicle and our 
vehicle is reasoned by two FRPNs, one for relative lane 
change and the other for relative speed. Therefore, the first 
stage of CFRPN requires 16 FRPNs. 
First, the relative lane change relationship is determined 
by one of the eight FRPNs. The terms )( 11 θp , )( 22 θp , 
)( 33 θp , )( 44 θp , )( 55 θp , )( 66 θp , )( 77 θp , and )( 88 θp  
are defined as follows: 
(1) )( 11 θp : The driver’s vehicle moves without lane change. 
(2) )( 22 θp : The driver’s vehicle changes to the left lane. 
(3) )( 33 θp : The driver’s vehicle changes to the right lane. 
(4) )( 44 θp : The left-forward vehicle moves without 
changing lane. 
(5) )( 55 θp : The left-forward vehicle changes to the left 
lane. 
(6) )( 66 θp : The left-forward vehicle changes to the right 
lane. 
(7) )( 77 θp : The left-forward vehicle and ours are moving in 
the same lane. 
(8) )( 88 θp : The left-forward vehicle and ours are moving in 
different lanes. 
The previous six places, 621 ,...,, ppp , express the 
premises, and the following two ones, 7p  and 8p , 
express the consequences of rules. The FRPN in Fig. 6 can 
be adopted to reason the following two production rules 
comprising these premises and consequences.  
(1) )())()(())()(( 7744226611 θθθθθ ppppp →∧∨∧  
(2) 
)())()((
))()(())()(())()((
))()(())()(())()((
886633
553344336622
552255114411
θθθ
θθθθθθ
θθθθθθ
ppp
pppppp
pppppp
→∧∨
∧∨∧∨∧∨
∧∨∧∨∧
 
The first production rule derives the possibility that the 
left-forward vehicle and the driver’s vehicle are moving on 
the same lane, while the second rule derives the possibility 
that the left-forward vehicle and the driver’s vehicle are 
moving on different lanes. A result of 87 θθ >  reveals the 
first event has a larger probability of occurring than the 
second event. Furthermore, the confidence values of the 
transitions in the FRPN may be associated with the 
significance of the corresponding terms in the production 
rules.  
Second, one of the eight FRPNs is depicted to derive 
the relative vertical distance relationship. Let Vo denote our 
vehicle, and Vlf  be our left-forward vehicle. The terms 
)'(' 11 θp , )'(' 22 θp , )'(' 33 θp , )'(' 44 θp , )'(' 55 θp , 
)'(' 66 θp , and )'(' 77 θp  can then be defined as follows: 
(1) )'(' 11 θp : The vertical distance between Vo and Vlf is 
close. 
(2) )'(' 22 θp : The speed of Vo is slower than that of Vlf. 
(3) )'(' 33 θp : The speeds of Vo and Vlf are equal. 
(4) )'(' 44 θp : The speed of Vo is faster than that of Vlf. 
(5) )'(' 55 θp : The vertical distance between Vo and Vlf  
increases. 
(6) )'(' 66 θp : The vertical distance between Vo and Vlf 
remains the same. 
(7) )'(' 77 θp : The vertical distance between Vo and Vlf  
decreases. 
The previous four places, 421 ',...,',' ppp , represent the 
premises, and the following three places, 65  , pp  and 7'p , 
represent the consequences of the rules. The FRPN shown 
in Fig. 7 can lead to the following production rules.  
(1) )'(')'(')'(' 552211 θθθ ppp →∧  
(2) )'(')'(')'(' 663311 θθθ ppp →∧  
(3) )'(')'(')'(' 774411 θθθ ppp →∧  
These production rules determine the possibility that 
the vertical distance between the left forward vehicle 
increases, decreases, and remains the same, respectively. 
The finding ( 65 '' θθ >  and 75 '' θθ > ) indicates a large 
possibility that the vertical distance between the left forward 
vehicle and our vehicle is currently rising. 
The FRPNs in the second stage are used to analyze 
complex driving events. Let )"(" θp  denote “the degree of 
danger of motions between the left forward vehicle and 
ours.” Figure 8 shows the FRPN that reasons the following 
production rule of the complex driving event )"(" θp . 
)"("))'(')(())'(')((
))'(')(())'(')((
))'(')(())'(')((
77886688
55887777
66775577
θθθθθ
θθθθ
θθθθ
ppppp
pppp
pppp
→∧∨∧
∨∧∨∧
∨∧∨∧
 
Similarly, the confidence values of the transitions in the 
FRPN are associated with the importance of the 
corresponding terms in the production rule. For instance, the 
term )'(')( 5588 θθ pp ∧  signifies that events “the left 
forward vehicle and ours are moving on the different lane” 
and “their relative distance increases” both occur. This case 
is safe for the driver. Thus a larger output value of this term 
implies a lower degree of danger. Hence, the confidence 
degree 4"c  of the corresponding transition should be set 
close to zero. 
The second stage of the CFRPN comprises a total of 
eight FRPNs, denoting the dangerous driving events 
between each nearby vehicle and ours, respectively. 
Therefore, eight degrees of danger can be obtained from 
these eight FRPNs at this stage,. 
In the third stage of CFRPN, only one FRPN is utilized 
9 
events is dangerous and time-consuming, a data generation 
system is also presented to generate the experimental data. 
These data are useful for measuring the performance of the 
dangerous driving event analysis system. However, the data 
generation system only generates partial driving events on 
freeway driving environment. This data generation system 
will be further improved continuously. 
We believe that if the system can refer not only to the 
spatial information but also to the temporal information in 
the data sequences, then it could predict dangerous driving 
events and alarm the driver as early as possible in the future. 
Acknowledgment 
The authors would like to thank the National Science 
Council of the Republic of China, Taiwan for financially 
supporting this research under Contract No. NSC 
94-2213-E-003-008. 
 
References 
[Dua01] I. C. Duan and F. L. Chung, “Cascaded Fuzzy 
Neural Network Model Based on Syllogistic Fuzzy 
Reasoning,” IEEE Transactions on Fuzzy Systems, Vol. 9, 
pp. 293-1306, 2003.  
[Gao03] M. Gao, M. Zhou, X. Huang, and X. Wu, “Fuzzy 
Reasoning Petri Nets,” IEEE Transactions on Systems, 
Man and Cybernetics—Part A: Systems and Humans, Vol. 
33, pp. 314-324, 2003. 
[Gao04] M. Gao, M. Zhou, and Y. Tang, “Intelligent 
Decision Making in Disassembly Process Based on Fuzzy 
Reasoning Petri Nets,” IEEE Transactions on Systems, 
Man and Cybernetics—Part B: Cybernetics, Vol. 34, pp. 
2029-2034, 2004. 
[Mit03] D. Mitrović, “Reliable Method for Driving Events 
Recognition,” IEEE Transactions on Intelligent 
Transportation Systems, Vol. 6, pp. 198-205, 2005. 
[Sch00] M. Schraut, K. Naab, and T. Bachmann, “BMW's 
Driver Assistance Concept for Integrated Longitudinal 
Support,” The 7th Intelligent Transport Systems World 
Congress, Paper No. 2121, Turin, Italy, 2000. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Structure of a cascaded fuzzy reasoning Petri net 
(CFRPN). 
 
•
•
•
•
•
•
•
)( 11 θP
)( 22 θP
)( mmP θ
)( 11 ++ mmP θ
Figure 4: An FRPN presents the production rule in
Eq. (1). 
c
•
•
•
•
•
•
•
• 
• 
• 
)( 11 θP
)( 22 θP
)( mmP θ
)( 11 ++ mmP θ
2c
mc
Figure 5: An FRPN presents the production rule in 
Eq. (2). 
1c
Detection 
Component 
Sensor 1 Sensor 2 Sensor n
Detection 
system 1
Detection 
system 2 
Detection 
system n
Analysis 
Component Dangerous driving event analysis system
Warning output 
Figure 1: Block diagram of the detection and analysis 
components of a DAS. 
… 
… 
System start 
Data acquisition 
Driving event integration 
Degree of danger computation 
Degree of danger output 
Figure 2: Flowchart of the dangerous driving event analysis 
system. 
Analysis the interaction 
between a driver’s vehicle 
and the road environment 
Analysis the interaction 
between a driver’s vehicle 
and nearby vehicles 
1 
出席國際會議報告書 
 
報告人姓名: 陳世旺 
服務機構及職稱: 國立台灣師範大學資訊工程學系  教授 
會議時間: 95 年 9 月 17 日至 20 日  
會議地點: 加拿大，多倫多 
會議名稱(中文): IEEE 2006 年智慧型運輸系統國際研討會 
會議名稱(英文): IEEE 2006 International Conference on Intelligent Transportation System 
發表論文題目(中文): 以模糊推理派翠西網路為基礎的危險駕駛事件分析系統 
發表論文題目(英文): Dangerous Driving Event Analysis System by a Cascaded Fuzzy Reasoning  
                  Petri Net 
 
 
一、參加會議經過 
近年來由於各類交通工具在速度上的改良有顯著的進步，人們對於交通傳輸
效率的要求越來越高；而生活水準的普遍提高，幾乎每個家庭都擁有自己的車輛，
使得世界各地的交通工具數量暴增許多，進而導致交通安全問題日益嚴重。另一方
面，由於電腦在軟硬體方面皆進步神速，再加上網路迅速普及，使得發展智慧型運
輸系統（Intelligent Transportation Systems, ITS）來改善並解決各類交通問題成為一
個熱門的研究課題。智慧型運輸系統係藉由先進之電腦、資訊、電子、通訊與感測
等設備的連結與技術的應用，傳遞即時資訊，改善人、車、路等運輸次系統間的互
動關係，進而增進運輸系統之安全、效率與舒適，同時減少交通對環境衝擊之整合
型運輸系統。 
IEEE 2006 年智慧型運輸系統國際研討會於二ＯＯ六年九月十七日至二十日
在加拿大多倫多舉行。會議參與者來自世界各地，包括加拿大、美國、德國、英國、
法國、日本、韓國、台灣以及中國大陸等學者專家，共同探討各國智慧型運輸系統
目前之現況、面臨之困難、解決之方案、與未來之發展等議題，除了學術上理論的
探討外，更重實際應用的可能性。因此，參與的專家學者中，不乏各國交通運輸相
關部門的決策與執行人士。 
3 
除了它們的偵測結果可能互相衝突與矛盾外，子系統個別發出的頻繁的警告訊息對
駕駛者更是容易造成困擾，延遲行車決策時間。若能將偵測子系統偵測到的結果加
以分析，輸出經過整合後的結論供駕駛者參考，對駕駛者的行車安全會更有幫助。
因此，本論文希望能建立危險行車事件分析子系統，目的在分析偵測子系統的偵測
結果，並建立完善的整合機制，輸出較適當與穩定的警告訊息供駕駛者參考。 
本論文在開發危險行車事件分析子系統之前，先建立了高速公路行車事件模擬
子系統，主要是因為要完整的蒐集高速公路上的行車事件資料並不容易，而且在高
速公路上模擬真實的危險行車事件對實驗者的生命安全也會造成莫大的威脅，所以
我們開發了高速公路行車事件模擬子系統，並將模擬結果做為危險行車事件分析子
系統的資料輸入。 
危險行車事件分析子系統是採用 cascaded fuzzy reasoning Petri net(CFRPN)模組
來進行事件的分析、推理，首先將模擬子系統的模擬結果當作資料輸入，再經由
CFRPN 的多階段推導，整合出行車狀況的結論。若系統認為目前車輛處在危險的行
車狀態，則輸出警告訊息提醒駕駛者注意。最後，我們以實驗來驗證危險行車事件
分析子系統的適用性，並希望未來能將此系統嵌入駕駛安全輔助系統中，不僅幫助
駕駛者更輕鬆地開車，同時也提升駕駛的安全性。 
二、與會心得 
本次會議中來自世界各地的專家學者到會議中發表精彩的演講與研究結果，
使我們得以藉此了解世界各國對智慧型運輸系統的看法、實施方式、以及未來的發
展趨勢。另外，利用這個機會也可以與各國的專家學者做意見交流，特別是交通、
運輸、土木等其他非影像處理與電腦視覺領域的與會專家學者，他們提供了許多不
同領域的觀點與見解，非常有助於我們未來研究時所可能面臨之問題的解決。 
2007 年的第十屆 ITSC 研討會將在美國華盛頓州的西雅圖舉行，希望明年仍
5 
Dangerous Driving Event Analysis System  
by a Cascaded Fuzzy Reasoning Petri Net 
*C. Y. Fang, *H. L. Hsueh, and **S. W. Chen 
*Department of Information and Computer Education 
**Department of Computer Science and Information Engineering 
National Taiwan Normal University 
Taipei, Taiwan, Republic of China 
 
Abstract 
This study applies a modified cascaded fuzzy reasoning 
Petri net (CFRPN) model to analyze dangerous driving 
events on a freeway. The dangerous driving events can be 
divided into two groups: (1) the interaction between a 
driver’s vehicle and the road environment, and (2) the 
interaction between a driver’s vehicle and nearby vehicles. 
These two classes of driving events may occur 
simultaneously and lead to certain serious traffic situations. 
The proposed system analyzes these two kinds of events 
determines dangerous situations from data collected by 
various sensors. Since collecting real driving event data on 
freeway is dangerous and time consuming, a data generation 
system is developed to generate the experimental data. Such 
data can help evaluate the performance of the proposed 
analysis system. Finally, experimental results show that the 
proposed system is accurate and robust. 
Keywords：Active driver assistance system, driving event 
analysis, fuzzy reasoning Petri net 
1. Introduction 
Diverse methods for enhancing driving safety have 
been proposed. Such methods can be roughly classified as 
passive or active. Passive methods (e.g., seat-belts, airbags, 
and anti-lock braking systems), which have significantly 
reduced traffic fatalities, were originally introduced to 
diminish the degree of injury from an accident. By contrast, 
active methods are designed to prevent accidents from 
occurring. Driver assistance systems (DAS) [Sch00] are 
designed to bring to alert the driver as quickly as possible to 
a potentially dangerous situation.  
Figure 1 illustrates a block diagram of a DAS, which 
can be split into two major components, for detection and 
analysis. The systems included in the detection component 
are used to collect the relevant data. These data can be 
obtained by various equipments, such as visual sensors, 
infrared ray sensors, laser sensors, ultrasonic wave sensors, 
ladar sensors and global positioning systems (GPS). Other 
sensors are adopted to detect the driver’s stress and 
drowsiness. Many researchers have developed detection 
systems to collect data accurately and quickly from these 
sensors. However, these detection systems generally work 
independently of each other. Few researchers [Mit03] are 
interested in integrating the high-level, abstract information 
from the collected data, for example to determine whether 
dangerous driving events occur. Furthermore, since every 
sensor has its limitations, we believe that integrating the 
detection results from different systems can improve the 
accuracy and efficiency of understanding driving situations. 
This study proposes a dangerous driving event analysis 
system to integrate the outputs from detection systems, and 
thus obtain some reasonable conclusions about the degree of 
danger in the driving environment. 
2. Dangerous Driving Event Analysis System 
Figure 2 shows the flowchart of the dangerous driving 
event analysis system. In this system, the data of driving 
environment acquired by various sensors can be divided into 
two classes, static and dynamic, which are obtained from 
different detection systems. The static data are those 
acquired from the road environment, such as the type or 
content of road signs, the number of lanes and the position 
of tunnel entrances or exits. The dynamic data are those that 
describe the motions of the vehicles, including the driver’s 
vehicle and those nearby.  
The dangerous driving events handled by the proposed 
system are divided into two classes that correspond to the 
two classes of data. The first class contains events relating 
to the behavior of the vehicle in the road environment (i.e. 
static data). The second class is events relating to the 
behaviors of our vehicle on nearby vehicles. Since driving 
events may occur simultaneously and result in certain 
serious traffic accidents, an integration stage is required to 
detect and measure such complicated driving events. The 
measured degree indicates whether the driver is in danger. 
In practice, the real data of the dangerous driving events 
are very difficult to collect because of the danger and time 
taken. Therefore, this study presents a data generation 
system to generate static and dynamic data to measure the 
performance of a dangerous driving event analysis system. 
However, the data generation system can only create driving 
environment data for a freeway. 
3. Cascaded Fuzzy Reasoning Petri Net 
This study develops a modified cascaded fuzzy 
reasoning Petri net (CFRPN) to analyze driving events. The 
CFRPN [Dua01] is designed for syllogistic fuzzy reasoning, 
and comprises several fuzzy reasoning Petri nets (FRPNs) 
that connect with others one by one as a 1D structure. This 
study extends the structure of CFRPN from 1D to 2D, and 
modifies the fuzzy reasoning mechanism in the FRPN to fit 
our applications. The modified CFRPN is also divided into 
several stages, but each stage contains several FRPNs, as 
7 
In the second stage, FRPNs are utilized to reason more 
complex driving events. FRPNs can evaluate the degree of 
danger between a single nearby vehicle and ours. For 
example, “Will any dangerous driving event occur between 
the right vehicle and our vehicle.” In the third stage, FRPNs 
integrate the outputs from the FRPNs in the second stage to 
make a final conclusion on whether a dangerous driving 
event occurs in the entire driving environment. 
The proposed dangerous driving event analysis system 
only deals with motion of vehicles close to the driver’s 
vehicle. Therefore, eight neighboring vehicles positions are 
defined as forward, backward, left, right, left forward, left 
backward, right forward and right backward. The 
relationship between each neighborhood vehicle and our 
vehicle is reasoned by two FRPNs, one for relative lane 
change and the other for relative speed. Therefore, the first 
stage of CFRPN requires 16 FRPNs. 
First, the relative lane change relationship is determined 
by one of the eight FRPNs. The terms )( 11 θp , )( 22 θp , 
)( 33 θp , )( 44 θp , )( 55 θp , )( 66 θp , )( 77 θp , and )( 88 θp  
are defined as follows: 
(1) )( 11 θp : The driver’s vehicle moves without lane change. 
(2) )( 22 θp : The driver’s vehicle changes to the left lane. 
(3) )( 33 θp : The driver’s vehicle changes to the right lane. 
(4) )( 44 θp : The left-forward vehicle moves without 
changing lane. 
(5) )( 55 θp : The left-forward vehicle changes to the left 
lane. 
(6) )( 66 θp : The left-forward vehicle changes to the right 
lane. 
(7) )( 77 θp : The left-forward vehicle and ours are moving in 
the same lane. 
(8) )( 88 θp : The left-forward vehicle and ours are moving in 
different lanes. 
The previous six places, 621 ,...,, ppp , express the 
premises, and the following two ones, 7p  and 8p , 
express the consequences of rules. The FRPN in Fig. 6 can 
be adopted to reason the following two production rules 
comprising these premises and consequences.  
(1) )())()(())()(( 7744226611 θθθθθ ppppp →∧∨∧  
(2) 
)())()((
))()(())()(())()((
))()(())()(())()((
886633
553344336622
552255114411
θθθ
θθθθθθ
θθθθθθ
ppp
pppppp
pppppp
→∧∨
∧∨∧∨∧∨
∧∨∧∨∧
 
The first production rule derives the possibility that the 
left-forward vehicle and the driver’s vehicle are moving on 
the same lane, while the second rule derives the possibility 
that the left-forward vehicle and the driver’s vehicle are 
moving on different lanes. A result of 87 θθ >  reveals the 
first event has a larger probability of occurring than the 
second event. Furthermore, the confidence values of the 
transitions in the FRPN may be associated with the 
significance of the corresponding terms in the production 
rules.  
Second, one of the eight FRPNs is depicted to derive 
the relative vertical distance relationship. Let Vo denote our 
vehicle, and Vlf  be our left-forward vehicle. The terms 
)'(' 11 θp , )'(' 22 θp , )'(' 33 θp , )'(' 44 θp , )'(' 55 θp , 
)'(' 66 θp , and )'(' 77 θp  can then be defined as follows: 
(1) )'(' 11 θp : The vertical distance between Vo and Vlf is 
close. 
(2) )'(' 22 θp : The speed of Vo is slower than that of Vlf. 
(3) )'(' 33 θp : The speeds of Vo and Vlf are equal. 
(4) )'(' 44 θp : The speed of Vo is faster than that of Vlf. 
(5) )'(' 55 θp : The vertical distance between Vo and Vlf  
increases. 
(6) )'(' 66 θp : The vertical distance between Vo and Vlf 
remains the same. 
(7) )'(' 77 θp : The vertical distance between Vo and Vlf  
decreases. 
The previous four places, 421 ',...,',' ppp , represent the 
premises, and the following three places, 65  , pp  and 7'p , 
represent the consequences of the rules. The FRPN shown 
in Fig. 7 can lead to the following production rules.  
(1) )'(')'(')'(' 552211 θθθ ppp →∧  
(2) )'(')'(')'(' 663311 θθθ ppp →∧  
(3) )'(')'(')'(' 774411 θθθ ppp →∧  
These production rules determine the possibility that 
the vertical distance between the left forward vehicle 
increases, decreases, and remains the same, respectively. 
The finding ( 65 '' θθ >  and 75 '' θθ > ) indicates a large 
possibility that the vertical distance between the left forward 
vehicle and our vehicle is currently rising. 
The FRPNs in the second stage are used to analyze 
complex driving events. Let )"(" θp  denote “the degree of 
danger of motions between the left forward vehicle and 
ours.” Figure 8 shows the FRPN that reasons the following 
production rule of the complex driving event )"(" θp . 
)"("))'(')(())'(')((
))'(')(())'(')((
))'(')(())'(')((
77886688
55887777
66775577
θθθθθ
θθθθ
θθθθ
ppppp
pppp
pppp
→∧∨∧
∨∧∨∧
∨∧∨∧
 
Similarly, the confidence values of the transitions in the 
FRPN are associated with the importance of the 
corresponding terms in the production rule. For instance, the 
term )'(')( 5588 θθ pp ∧  signifies that events “the left 
forward vehicle and ours are moving on the different lane” 
and “their relative distance increases” both occur. This case 
is safe for the driver. Thus a larger output value of this term 
implies a lower degree of danger. Hence, the confidence 
degree 4"c  of the corresponding transition should be set 
close to zero. 
The second stage of the CFRPN comprises a total of 
eight FRPNs, denoting the dangerous driving events 
between each nearby vehicle and ours, respectively. 
Therefore, eight degrees of danger can be obtained from 
these eight FRPNs at this stage,. 
In the third stage of CFRPN, only one FRPN is utilized 
9 
events is dangerous and time-consuming, a data generation 
system is also presented to generate the experimental data. 
These data are useful for measuring the performance of the 
dangerous driving event analysis system. However, the data 
generation system only generates partial driving events on 
freeway driving environment. This data generation system 
will be further improved continuously. 
We believe that if the system can refer not only to the 
spatial information but also to the temporal information in 
the data sequences, then it could predict dangerous driving 
events and alarm the driver as early as possible in the future. 
Acknowledgment 
The authors would like to thank the National Science 
Council of the Republic of China, Taiwan for financially 
supporting this research under Contract No. NSC 
94-2213-E-003-008. 
 
References 
[Dua01] I. C. Duan and F. L. Chung, “Cascaded Fuzzy 
Neural Network Model Based on Syllogistic Fuzzy 
Reasoning,” IEEE Transactions on Fuzzy Systems, Vol. 9, 
pp. 293-1306, 2003.  
[Gao03] M. Gao, M. Zhou, X. Huang, and X. Wu, “Fuzzy 
Reasoning Petri Nets,” IEEE Transactions on Systems, 
Man and Cybernetics—Part A: Systems and Humans, Vol. 
33, pp. 314-324, 2003. 
[Gao04] M. Gao, M. Zhou, and Y. Tang, “Intelligent 
Decision Making in Disassembly Process Based on Fuzzy 
Reasoning Petri Nets,” IEEE Transactions on Systems, 
Man and Cybernetics—Part B: Cybernetics, Vol. 34, pp. 
2029-2034, 2004. 
[Mit03] D. Mitrović, “Reliable Method for Driving Events 
Recognition,” IEEE Transactions on Intelligent 
Transportation Systems, Vol. 6, pp. 198-205, 2005. 
[Sch00] M. Schraut, K. Naab, and T. Bachmann, “BMW's 
Driver Assistance Concept for Integrated Longitudinal 
Support,” The 7th Intelligent Transport Systems World 
Congress, Paper No. 2121, Turin, Italy, 2000. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Structure of a cascaded fuzzy reasoning Petri net 
(CFRPN). 
 
•
•
•
•
•
•
•
)( 11 θP
)( 22 θP
)( mmP θ
)( 11 ++ mmP θ
Figure 4: An FRPN presents the production rule in
Eq. (1). 
c
•
•
•
•
•
•
•
• 
• 
• 
)( 11 θP
)( 22 θP
)( mmP θ
)( 11 ++ mmP θ
2c
mc
Figure 5: An FRPN presents the production rule in 
Eq. (2). 
1c
Detection 
Component 
Sensor 1 Sensor 2 Sensor n
Detection 
system 1
Detection 
system 2 
Detection 
system n
Analysis 
Component Dangerous driving event analysis system
Warning output 
Figure 1: Block diagram of the detection and analysis 
components of a DAS. 
… 
… 
System start 
Data acquisition 
Driving event integration 
Degree of danger computation 
Degree of danger output 
Figure 2: Flowchart of the dangerous driving event analysis 
system. 
Analysis the interaction 
between a driver’s vehicle 
and the road environment 
Analysis the interaction 
between a driver’s vehicle 
and nearby vehicles 
