 2
image storage and computer-intensive analysis of 
large blocks of data. These requirements make 
problematical long-term study of the behavior of 
laboratory animal populations. Some researchers 
have tried to minimize these problems. For example, 
one study attempted a system based on infrared 
beams [12], but the result involved a serious loss of 
resolution regarding the animal’s position. As an 
additional issue, existing literature and commercial 
systems tend to be designed for a particular 
experimental purpose, making it difficult to reuse 
these expensive systems for new animal experiments. 
Thus the traditional systems lack flexibility and 
customization abilities, limitations which constitute 
a waste of resources and time.  
In response to these issues, this study develops a 
novel economical system for flexible long-term 
video-based stand-alone measurement of laboratory 
animals. It will be seen that computation speed is 
very fast, data storage requirements are small and 
hardware requirements are minimal. The presented 
system includes a novel algorithm which processes 
the video data, performs tracking, calculates a 
motion plot for the animal. The focus of the 
designed system is laboratory rats as observed from 
overhead by a conventional low-cost CCD 
(charge-coupled device) camera. The presented 
hardware is built around an FPGA (field 
programmable gate array) and a MCU 
(microcontroller). The FPGA processes the color 
video signal, detects the target’s position and can be 
reconfigured by ISP (in-system programming) so 
system firmware can be upgraded without 
disassembling the device to physically replace 
memory. The MCU can reconfigure the FPGA by 
serial synchronous MCU interface control, with the 
configuration file stored on a SD (secure digital) 
card.  
Further, this paper presents a novel algorithm 
which performs multi-object video 
detection/tracking. After FPGA processing of the 
color video data, the developed algorithm allows 
very fast detection of several objects from the 
complex image, after which the data is reduced to a 
set of coordinates representing the momentary 
position of the target, then the MCU saves the data 
in the SD card.  
 
2. Methodology 
2.1. Hardware 
 
The system block diagram is shown in Fig. 1. 
The input source of the video signal is a commercial 
CCD camera. The CVBS (composite video 
broadcast signal) analog signal from the camera is 
connected to a video decoder implemented here by a 
Philips SAA7113H whose output supports the 
ITU-656 format.  
A key feature of the presented system is that it 
uses a reprogrammable SRAM-based FPGA circuit 
implemented by an ACEX 1K100 (Altera, USA) to 
process the ITU-656 video data and implement the 
tracking algorithm. In our prototype setup, the 
FPGA is configured to collect the video signal and 
transfer it to SDRAM. The FPGA includes OSD (on 
screen display) functions so that the other useful 
information can be shown on the LCM at the same 
time as the current video frame. The video decoder 
in this system is controlled by a MCU through an 
I2C bus. A Philips LPC2132 MCU is adopted here to 
control the video decoder. The LPC2132 is based on 
a 32-bit ARM7TDMI-S CPU but combines the 
MCU with 64kB of embedded high speed flash 
memory. In addition to controlling the video decoder 
and performing data routing such as directing results 
to flash memory card or to PC via RS232. In the 
prototype discussed, all tracking information data is 
stored to SD card.  
A photo of our lab-built prototype board is 
shown in Fig. 2. The entire structure, even when 
lab-implemented using discreet components, is 
compact and convenient. The system is fully capable 
of recording animal locomotion as a stand-alone 
device, without PC or network support.  
 
 
Fig. 1: System block diagram 
 
 4
are sent to the FPGA’s OSD routine for display on 
the LCM. The OSD produces a composite of the 
video camera image and on-screen data. An example 
can be seen in Fig. 5, showing the original video 
image, the most recently computed position of one 
rat, the cross-hair cursor, the boundaries of the 
tracking arena, etc. The FPGA communicates with 
the MCU through the memory expansion interface.  
 
Fig. 4: FPGA function block diagram 
 
Fig. 5: Photo of LCD display during operation; large 
crosshairs are cursor; numbers in upper left shows 
momentary parameter values; centered large 
rectangle shows tracking arena; inner divisions of 
large rectangle are space parameters for behavioral 
analysis.  
 
2.3. Program structure 
2.3.1. Calibration 
 
It is necessary to convert the raw CCD 
pixel-to-pixel distances into actual laboratory animal 
confinement space centimeters before starting an 
experiment. This is done during an initial calibration 
which consists of four steps. The first step defines 
the upper left corner of the experimental tracking 
arena, which is assumed to equal or be smaller than 
the video data from the camera. To do so, the cross 
cursor on the LCM (Fig. 5) is adjusted to coincide 
with the upper left corner of the desired visual arena 
and the OK button (Fig. 2) is pressed. This process 
is repeated for the lower right corner of the visual 
arena. This establishes an OSD rectangle on the 
LCM which defines the experimental tracking arena. 
The size of the physical experimental arena is 
determined by physical measurement, and these 
dimensions are input to system memory via the key 
pad. The behavior analysis program than can 
compute the cm/pixel relation of the arena and, 
therefore, the actual distance moved by an animal. 
 
2.3.2. Tracking algorithm 
 
Object detection relative to a background is a 
crucial part of any tracking algorithm. Many 
different methods have been presented for 
differentiating between an object and its background, 
with advantages and disadvantages to each method. 
Our proposed method was specifically conceived for 
fast low-cost stand-alone application. So we will 
consider the two video tracking methods used in 
animal tracking systems for laboratory animals [9], 
with specific reference to the commercial 
EthoVision animal tracking systems. 
The first is grey scaling; the second is 
grey-scaling subtraction. Grey-scaling avoids 
problems of color by using monochrome imaging. 
Further, grey scale methods are always faster than 
comparable color methods. Simple grey scaling is 
based on the range values between black and white. 
If one of the grey values can be assumed to be a 
characteristic of the target, and if all of the other 
grey values in an image can be assumed to be of a 
different grey value, then the detection of the 
specified grey value in an image can be assumed to 
indicate detection of the target. 
Both of our color methods are similar to grey 
scaling, as discussed below. For reference, simple 
grey scaling is fast but must assign a single 
grey-scale value per target. No other pixel in the 
image is allowed the same grey-scale value. Clearly, 
this is a restrictive limitation, since the food, feces 
and the floor itself in a cage may have the same grey 
scale value as the target.  
In consequence, various more complicated 
methods have been added to supplement the basic 
grey scale method. Subtraction methodology is a 
good example. It involves subtracting a reference 
image from the most recent video image, where the 
reference image is a video frame of the confinement 
arena and all the objects in it, but without the rat. 
Subtraction then produces a complete silhouette of 
 6
⎪⎩
⎪⎨
⎧
+=
=
+=
128)-(CB*1.59616)-(Y*1.164B 
128)-(CB*0.392-128)-(CR*0.813-16)-(Y*1.164G 
128)-(CR*1.59616)-(Y*1.164R 
      (1) 
The relative behavior of the YCbCr and RGB 
color spaces under conditions of varying luminance 
will now be demonstrated by use of actual 
experiment. Our system’s CCD camera is directed at 
a color sample at a given illumination value. The 
pink-ish color being viewed by the camera can be 
seen in the small square inset in the upper left corner 
of the LabVIEW (National Instruments, USA.) 
software display in Fig. 7. Various plots can be seen 
in this figure, where the x-axis of each plot shows 
the same concurrent time scale and the y-axis of 
each plot shows a color intensity value. The single 
plot inset in the upper right of the image (labeled 
“G-SET”) indicates the reference color used to track 
a mouse as expressed in Cb and Cr values. The three 
plots in the first row (G_Y, G_CR and G_CB) show 
the camera data as expressed in YCbCr color space, 
while the 3 plots in second row (G_R, G_G and G_B) 
show the same camera data as expressed in RGB 
color space. Twice during the time period of the 
experiment, the illumination of the image was 
changed, the first moderately about ¼ of the way 
through the experiment, and again more strongly 
about ½ of the way through the experiment.  
The relative immunity of the two color spaces to 
brief luminance changes can be seen in the two plots 
in the bottom row, the left being for the RGB color 
space, the right being for the YCbCr color space. In 
each of these two plots there are three horizontal 
tracks, one for each of the constituents in the color 
spaces. Thus, in the first plot (labeled RGB_diff), 
there are three tracks, one for red, one for green, one 
for blue. These tracks do not directly represent the 
color value itself, but rather the difference between 
momentary value of that track and the same track’s 
value a short time earlier (time value seen in small 
box inset in upper center of figure and labeled 
“milliseconds to wait”). Since the tracking color did 
not change during the test, the color values of a 
perfectly objective camera system should not change, 
so the difference between any two points in any 
track should be zero. Real systems of course have 
normal error, so the RGB_diff values can be 
expected to be in the range of zero. However, the 
illumination value of the system was changed by 
moving a white florescent lamp closer to the target, 
then moving it further away, then moving it even 
closer than before, then away again to the starting 
position. The RGB_diff plot shows major changes in 
the values of all three tracks when the illumination 
changes. However, the YCbCr_diff plot shows only 
a major change in the track which represents the Y 
(illumination) value, while the Cb and Cr values 
show only small change when the illumination value 
changes. This is clear experimental confirmation of 
the relative immunity of YCbCr color space to 
illumination changes. Such immunity is of potential 
use in animal tracking systems and is why it is 
employed in our prototype, and is why we 
recommend its consideration here by future 
designers of tracking systems. 
 
 
Fig. 7: Experimental comparison of illumination 
change for same color as viewed by CCD camera 
under RGB and YCbCr color spaces.  
 
2.3.3. Behavior analysis 
 
Traditional evaluation of such plots typically 
uses a human to look at the plot and arrive at simple 
evaluations such as “excited” or “tranquil,” i.e. the 
analytic results are subjectively biased and vary 
from observer to observer. In consequence there has 
been significant push for many years to develop 
objective animal behavior analysis systems. Recent 
years have seen a variety of automatic analysis 
systems implemented with the tracking system set 
up as a peripheral to a PC [9][13][14].  
The behavioral analysis system implemented in 
our prototype is fast and efficient. Nevertheless, it is 
not intended for specific study of an animal 
population and so incorporates only a few basic 
analytic parameters commonly found in other 
studies of the behavior of small laboratory animals 
[15][16]. These are intended to demonstrate some of 
the capacity of the system, but it should be 
remembered that virtually any contemporary 
behavioral analysis can be performed by the 
presented hardware. After the animal location is 
computed by the FPGA stage, the location data is 
input to the MCU for processing by the behavioral 
analysis algorithm. The results of the analysis are 
 8
(A)
 
(B)  
Fig. 9: Data displayed on the two LCDs of Fig. 8. (A) 
Input LCD: one frame of simulated arena containing 
3 mice, each with different color; this image is 
viewed by CCD camera (as in Fig. 8) and input to 
the tracking system; (B) Output LCD: same frame as 
Fig.9(A) after processing by tracking system with 
tracking system set to detect the color of the central 
mouse of Fig 9(A).  
 
Essential tracking accuracy can be observed in 
Fig. 10(A), where the blue trace is the mouse track 
simulated on the LCD and the pink trace is the 
CCD data as determined by our tracking system. 
Clearly, the two tracks are virtually identical.  
In a second test, the tracking problem is made 
more complicated. This time three simulated mouse 
traces with three separate colors are run 
concurrently. The task is to extract the trace of one 
simulated mouse from the three simultaneous traces. 
Fig. 10(B) shows these three LCD traces and the 
single extracted trace. Clearly the extracted (pink) 
trace matches one and only one of the three initial 
traces, indicating successful isolation of a single 
trace from the complex multi-trace. 
Finally, the tracing results as computed by our 
tracking system are compared, frame by frame, by 
simple linear correlation coefficient(r) according to 
the formula 2: 
( )( )
( ) ( )
n
Yi
Y
n
Xi
X
YYiXXi
YYiXXi
r
n
i
n
i
n
i
∑∑
∑∑
∑
==
−−
−−
=
==
=
,
1
2
1
2
1                    (2) 
The computed correlation coefficient r for 
single-object tracking for the above tests is 
0.996674, while r for multi-object tracking is 
0.996112. This strong correlation confirms that our 
tracking system provides reliable measurement of 
both single-object and multi-object small-animal 
locomotion. 
(A)
Single object
0
50
100
150
200
250
300
350
400
450
0 100 200 300 400 500 600
PC
SYSTEM
 
(B)
Multi-object
0
100
200
300
400
0 100 200 300 400 500 600
DATA-2
DATA-3
DATA-1
SYSTEM
 
Fig. 10: Tracking data example. (A) Single-object 
tracking; (B) Multi-object tracking. 
 
4. Discussion 
 
Many medical and behavioral applications 
require the ability to monitor and quantify the 
behavior of small animals. In general these animals 
are confined in small cages. Often these situations 
involve very large numbers of cages. Minimizing the 
cost of the monitoring systems is therefore an 
important factor. This applies to both hardware costs 
and the cost of human support. The system presented 
 10
force-plate actometer for quantitating rodent 
behaviors: illustrative data on locomotion, 
rotation, spatial patterning, stereotypies, and 
tremor. Journal of Neuroscience Methods, 
2001; 107: 107-24. 
[5] Ye SP, Bell WJ. A Simple Video 
Position-Digitizer for Studying Animal 
Movement Patterns. Journal of Neuroscience 
Methods, 1991; 37: 215-25. 
[6] Jana Chra´skova´, Yulij Kaminsky, Ivan 
Krekule. An automatic 3D tracking system 
with a PC and a single TV camera. Journal of 
Neuroscience Methods, 1999; 88: 195-200. 
[7] Wu BM, Chan FHY, Lam FK, Poon PWF, 
Poon AMS. A novel system for simultaneous 
monitoring of locomotor and sound activities 
in animals. Journal of Neuroscience Methods, 
2000; 101: 69-73. 
[8] Martin PD, Nishijo H, Ono T. A combined 
electrophysiological and video data acquisition 
system using a single computer. Journal of 
Neuroscience Methods, 1999; 92: 169-77. 
[9] Spink AJ, Tegelenbosch RAJ, Buma MOS, 
Noldus L. The EthoVision video tracking 
system-A tool for behavioral phenotyping of 
transgenic mice. Physiology and Behavior, 
2001; 73: 731-44. 
[10] Grossmann M, Skinner MH. A simple 
computer based system to analyze Morris 
water maze trials on-line. Journal of 
Neuroscience Methods, 1996; 70: 171-5. 
[11] Wolfer DP, Madani R, Valenti P, Lipp HP. 
Extended analysis of path data from mutant 
mice using the public domain software 
Wintrack. Physiology & Behavior, 2001; 73: 
745-53. 
[12] Koob AO, Cirillo J, Babbs CF. A novel open 
field activity detector to determine spatial and 
temporal movement of laboratory animals after 
injury and disease. Journal of Neuroscience 
Methods, 2006; 157: 330-6. 
[13] Wolfer DP, Madani R, Valenti P, Lipp HP. 
Extended analysis of path data from mutant 
mice using the public domain software 
Wintrack. Physiology & Behavior, 2001; 73: 
745-53. 
[14] Pan WHT, Lee CR, Lim LH. A new video path 
analyzer to monitor travel distance, rearing, 
and stereotypic movement of rats. Journal of 
Neuroscience Methods, 1996; 70: 39-43. 
[15] Kathyrne M, Eileen MH, David RC. Another 
look at amphetamine-induced stereotyped 
locomotor activity in rats using a new statistic 
to measure locomotor stereotypy. 
Psychopharmacology, 1989; V97: 74-9. 
[16] Kathyrne M, Paul MK, David W, Cynthia H. 
Time course of amphetamine-induced 
locomotor stereotypy in an open field. 
Psychopharmacology, 1989; V99: 501-7. 
[17] YCbCr to RGB Considerations, Application 
Note AN9717, InterSil Corp. 
Track 13 Particle Therpy Physics and Systems 
Track 14 Image Processing, Analysis, and Visualization 
Track 15 Bioelectricity and Biomagnetism 
Track 16 Biomechanics and Rehabilitation Engineering 
Track 17 Medical Robotics and Computer Assisted Surgery 
Track 18 Artificial Organs 
Track 19 Drug Delivery and Therapeutic Systems 
Track 20 Biomaterial and Tissue Engineering 
Track 21 Cardiovascular and Pulmonary Systems 
Track 22 Neural Systems and Engineering  
Track 23 Physics and Engineering in Oriental Medicine 
Track 24 Clinical Engineering 
Track 25 Education, Training and Professional Development 
 
 
