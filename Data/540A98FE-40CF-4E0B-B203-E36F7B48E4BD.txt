I 
行政院科學委員會補助專題研究計畫 ■ 成 果 報 告   
□期末進度報告 
 
小型人形機器人之設計、研製及應用－子計畫四：視覺系統與智慧型控
制之研發及其於人形機器人足球賽之應用(3/3) 
 
 
計畫類別：□ 個別型計畫  ■ 整合型計畫 
計畫編號：NSC 95-2221-E-006 -382 -MY3 
執行期間：自民國95年08月01日至民國98年07月31日 
 
計畫主持人：李祖聖 特聘教授 國立成功大學電機工程學系 
計畫參與人員：蘇育德、胡振嘉、黃秋杰、呂明峰、許嘉玲、胡俊陽、
劉紹先、陳文建、張峻銘、薛伯彥、賴劭韋、張家揚、
王德凱、洪慈欣 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
5出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，;二年後可公開查詢 
          
執行單位：國立成功大學電機工程學系 
III 
目錄 
 
一 前言.......................................................................................................................... 4 
二 計畫緣由及目的...................................................................................................... 4 
三 研究方向與進行步驟.............................................................................................. 5 
子題一 Hardware of the Humanoid Robot ......................................................................... 5 
1 Design of Mechanism .............................................................................................. 5 
2 The System Structure of the Robot .......................................................................... 7 
子題二 Dynamic System Modeling and ZMP Trajectory Planning ................................. 19 
1 The Overview of System Structure........................................................................ 19 
2 The ZMP Trajectory Planning................................................................................ 28 
3 Fuzzy Logic Controller .......................................................................................... 35 
4 Genetic Algorithms................................................................................................ 44 
子題三 Vision system....................................................................................................... 52 
1 Object Recognition for Tracking ........................................................................... 52 
2 Vocabularies and Numbers Recognition with Speech ........................................... 63 
子題四 Control Strategy System ...................................................................................... 69 
1 Introduction............................................................................................................ 69 
2 The Control Strategy System for PK Event in FIRA............................................. 70 
3 The Control Strategy System for Bowling............................................................. 83 
4 The Control Strategy System for Recognition....................................................... 88 
四 Experiment Results and Conclusions..................................................................... 94 
1 Experiment Results ................................................................................................ 94 
2 Conclusions.......................................................................................................... 108 
3 Future Works........................................................................................................ 109 
五 近期發表之論文...................................................................................................110 
六 參考文獻...............................................................................................................111 
七 計畫成果自評.......................................................................................................114 
八 可供推廣之研發成果資料表...............................................................................115 
 
 
  5 
三 研究方向與進行步驟 
Task 1  Hardware of the Humanoid Robot 
1.1 Design of Mechanism 
In order to make the motion analysis of the robot more accurate in the future, the 
framework lengths of the robot are the important reference resources. Therefore, we 
design all the mechanism structure by ourselves. The robust of the mechanism affects 
whether the motions move smoothly or not. 
The A5052 aluminum-magnesium alloy with 1mm thickness is chosen for the 
materials of the whole body. The A5052 is light and easy to be manufactured, so the 
weight of the robot will be reduced, and the load of motors will be eased.  
A flat Al-Mg alloy may be deformed easily as it is exerted by an external force. 
According to the mechanics of material, any component will deform by applying 
excessive stresses. The common two conditions of the deformed Al-Mg alloy as 
shown in Fig. 1.1(a). In order to avoid this fault, both the sides of the component are 
bent in the shape I for an appropriate length as shown in the Fig. 1.1(b). Besides, the 
weight of aiRobot-2 is considered. To avoid that aiRobot-2 is too heavy to move, we 
dig small holes in the middle of the component without influencing the structure 
robustness of the component. All the components of aiRobot-2 are designed via the 
3D mechanism design software, SolidWorks. The complete structure of our humanoid 
robot is shown in Fig. 1.2(a) and the disposition of motors is shown in Fig. 1.2(b). 
 
 
  7 
1.2 The System Structure of the Robot 
 The hardware of the small-sized humanoid robot includes: twenty nine 
servomotors, a CMOS sensor, eight pressure sensors, an accelerometer, four Li-Po 
batteries, Nios II evaluation board, and an integrated circuit board. The photos of 
aiRobot-2 are shown in Fig. 1.3. Table 1.1 shows the basic specification of the robot. 
All the information of the sensors is delivered to the Nios II FPGA board. After 
calculating by the CPU, the actuators act according to the result information 
computed by the FPGA board. We shall introduce the hardware and software briefly 
via five sections including actuators, center process unit, the power system, the vision 
system and the speech system. 
 
(a) The front view (b) The rear view 
Fig. 1.3 The photos of aiRobot-2 
 
  9 
Table 1.2 The specification of the motors 
 KONDO KRS-2350HV GWS NARO 
Dimension 203841 ×× mm 35.2124.1122 ×× mm 
Weight 62g 8.8g 
Torque 29.5kg-cm 0.8kg-cm 
Speed 0.13sec/60° 0.11sec/60° 
Voltage 9~12V 5V 
 
 
  
(a) KRS-2350HV (b) GWS NARO 
Fig. 1.4 The figures of the actuators 
 
 
 
  11 
 
Table 1.3 The specification of Nios II evaluation board 
LEs 18752 
M4K RAM Blocks 52 
Embedded Multipliers 26 
Total RAM bits 239,616
PLLs 4 
Maximum user I/O pins/Available 315/128
Internal 1.2V 
Voltage 
I/O 3.3V 
 
 
 
Fig. 1.6 The structure of the overall system 
 
 
 
  13 
1.2.3 Power System 
 The power source of our robot is used the Li-Po battery and the specification is 
shown as Table 1.4. There are four Li-Po batteries supply all the devices. Two of them 
are equipped on the hip and the others are placed on the feet. The batteries on the hip 
provide the power for the motors of the upper body, the FPGA board and the sensors. 
The motors of both legs are fed by the other two batteries on the feet. However, we 
need regulator ICs to adjust the voltage because of the different working voltages for 
different devices. The IC LM2576 is used to convert the voltage from 12 volts to 5 
volts. The disposition of the power system is shown in Fig. 1.9. 
 
Table 1.4 The specification of Li-Po batteries 
Dimension 143560 ×× mm 
Weight 74g 
Voltage 12V 
 
Fig. 1.9 The disposition of the power system 
 
 
  15 
program and debug by combining the Nios II IDE with the BCB interface. Moreover, 
the communication shown in Fig. 1.11 is delivered among the Nios II, PC and the 
CMOS-EYE. We not only deliver the image data from the CMOS-EYE to Nios II 
through parallel bus, but communicate between Nios II and PC with RS232. 
 
Table 1.5 The specification of the CMOS-EYE vision sensor 
Model CMOS-EYE 
Dimension 39×46×3 mm 
Weight 10 g 
Resolution 160×120 
Voltage 4.8-15 V 
Transmission Type RS232，Parallel Bus 
 
Fig. 1.11 The communication among the Nios II, PC, and CMOS-EYE 
  17 
driver on the Emic TTS is only 300mW, we design an audio amplifier circuit board 
with LM386N-3 IC and connect an 8Ω 500mW speaker to amplify the audio volume 
from the Emic TTS shown in Fig. 1.15. The advantages of this module are small and 
simple to use. Table 1.7 shows the specification of the LM386N-3 IC. LM386N-3 is a 
low voltage audio power amplifier. The default gain of the IC is 20dB. The gain 
which can be maximum 200dB can be adjusted by the capacity between the pin1 and 
pin8. The PCB layout by the Protel 99 SE is shown in Fig. 1.16. 
Table 1.6 The specification of the TTS module 
Model Emic TTS #30006
Dimension 51×35×3 mm 
Weight 10 g 
Speaker Driver 8Ω  300mW 
Voltage 4.5-5.5 V 
Transmission Type RS232 
    
Fig. 1.13 Emic Text-To-Speech module  Fig. 1.14 Communication with the host 
device 
  19 
Task 2 Dynamic System Modeling and ZMP Trajectory Planning 
2.1 The Overview of System Structure 
The system structure includes four modules, which are sensor modules, dynamic 
model, behavior decision center, and intelligent controller. We introduced the 
hardware and sensors of aiRobot-2 in Chapter 2, and will explain the next two 
modules in this chapter. In the system of aiRobot-2, it has many undefined parameters 
which can involve the stability, such as the length of links and the height of body. The 
humanoid robot can be seen as a nonlinear plant of controlled system. It needs to 
derive dynamic equations for controlling purpose. It is relative to the position of COM 
and the angle of motors. After establishing dynamic equations, we should design our 
objectives for walking, that is the desired ZMP trajectory. aiRobot-2 is stable means 
the actual ZMP trajectory always locates on stable regions. The accelerometer and 
force sensors are adopted to help us to compute the actual ZMP. The designed 
motions stored in aiRobot-2 regard as original abilities like human. These motions can 
be trained and modified to make progress by the fuzzy logic controller and genetic 
algorithms. The decision of FLC depends on the error and the error derivative with 
regard to the inclination angle and the actual ZMP. The genetic algorithm is through 
the natural selection to search the best resolution of system. The FLC and GA are 
combined into an intelligent controller which will be introduced in next chapter. 
 
 
 
 
  21 
length of the sole, and pleft_x and pleft_y are the ZMP of left sole for single supporting 
phase. The ZMP of right sole is measured in the same way. 
 
 
Fig.2.1 (a) Simplified dynamic model for biped robot walked in the frontal plane. 
       (b) Simplified dynamic model for biped robot walked in the sagittal plane. 
  23 
 
Fig. 2.3 The distance of two soles 
Therefore, we just control ten motors (five joints per leg) to satisfy these 
equations from (2-1) to (2-8) while walking, and the robot will work correctly. It can 
greatly decrease the quantity of data processed by NIOS CPU and the time of 
response. Because the more constraints we define, the more difficulties it results on 
implementations. The simplified dynamic model is built up with inverse kinematics, 
and we use two types of sensor to control our robot, the accelerometer and force 
sensors. Its control method will be introduced in next chapter. 
2.1.2 The Motion Pattern 
After organizing the dynamic model of aiRobot-2, we should set up some 
human-like motions on it. For example, walking and climbing stairs. How to generate 
these actions is an important thing to humanoid robot. The KONDO motor which we 
install in our robot only has no feedback signal and always works on position control 
mode namely angle control mode. Hence, we use the accelerometer and force sensors 
  25 
 
Fig. 2.5 (a) Motion Shifting Transform for itself (b) MST for different motions 
We have produced the basic one step walking motion, as shown in Fig. 2.6. We 
assume that it has six stable states during a cycle, as shown in Fig. 2.7. It can be found 
out easily that the second and third states are similar to the fifth and sixth states. For 
this reason, we record the data of corresponding motors in these states and project 
these data into other states with MST. After setting up the data of all motors in every 
state, we have to connect the six parts of this motion. Therefore, we use the method of 
interpolation in these states to accomplish this motion, as shown in Fig. 2.8. The more 
numbers of the interpolation, the slower the robot acts because every command keeps 
motion for equal time, 0.1 second, as shown in Fig. 2.9. Because these parts are stable, 
we expect this walking motion combined all states and insertions are steady and 
smooth. 
It is easy to create other motions such as climbing base on the completed walking 
motion. We use MST to transfer the forth state to the similar state in climbing, as 
shown in Fig. 2.5 (b). The climbing motion will generate automatically by itself, as 
  27 
 
Fig. 2.8 The interpolation interface 
 
Fig. 2.9 The matrix table after interpolations 
 
Fig.2.10 The one step climbing motion in sagittal plane 
  29 
to suit different conditions while walking.  
 
Fig. 2.11 The ZMP diagram of the two steps walking motion 
  31 
 _
( / )( 2 ), for 2 2
( 2 )( ) sin( ) ,      for   2 3
2
for 3 3
( / )(3 ),
x d
d
d x x x d d
d
d
x d
K t t T T t T t
t Tp t S A K T t t T t
T t
T t t T
K t T t
π
⎧ − − ≤ ≤ +⎪ −⎪− = − − + ≤ ≤ −⎨ −⎪ − ≤ ≤⎪ − −⎩
 (2-10c) 
 _
4 ( / )( 2 ), for 2 2
( ) 5 ,      for   2 3
5 ( / )(( (3 )), for 3 3
y y d d
d y y d d
y y d d d
K K t t T T t T t
p t K T t t T t
K K t t T t T t t T
⎧ + − ≤ ≤ +⎪= + ≤ ≤ −⎨⎪ + − − − ≤ ≤⎩
 (2-10d) 
An important thing that the origin of the single supporting phase and the origin 
of the double supporting phase are not match in our definition will occur while 
walking. For this situation, we introduce a transfer vector to shift the robot coordinate 
when the supporting phase is changed, as shown in Fig. 2.11. If we do not use it, the 
ZMP will show its original data of the robot coordinate from the force sensor, as 
shown dotted lines in Fig. 2.12. How to apply the transfer vector is relative to the 
posture of the robot at that time. The transfer vector (tx, ty) is computed by (2-7), (2-8) 
and the parameter of sole as follows, 
 x x widtht d l= +  (2-11) 
 y yt d=  (2-12) 
where dx, dy and lwidth are obtained in Fig. 2.12. Note that the distance of two soles is 
an approximate with respect to the robot coordinate because it is calculated by the 
actual data of motors at that time. We apply the transfer vector two times in a cycle of 
the two steps walking motion to make the origin of the robot coordinate fixed in the 
original point of the world coordinate. Hence, we can design a normal and simple 
ideal ZMP trajectory to layout the robot motion with this transferring coordinate 
approach. We expect the robot can walk smoothly as human as possible, and it 
represents our actual ZMP to match this ideal trajectory as close as possible. 
 
  33 
The actual ZMP do not match completely the ideal ZMP trajectory that is why 
we should modify the actual ZMP trajectory. We control the motor of two legs to 
adjust the posture of aiRobot-2 and actual ZMP will go along with it. The dynamic 
model can be seen as an inverted pendulum, and its COM position is represented as 
follows in Fig. 2.14, 
 _ _sin sinx link hip x link ankle x xc l l Cθ θ= × + × +  (2-13) 
 _ _sin siny link hip y link ankle y yc l l Cθ θ= × + × +  (2-14) 
 _ _cos cosz link hip x link ankle x zc l l Cθ θ= × + × +  (2-15a) 
 or   _ _cos cosz link hip y link ankle y zc l l Cθ θ= × + × +  (2-15b) 
where cx, cy and cz are the position of COM with respect to robot coordinate, θhip_x and 
θhip_y are the angle of hip joints, θankle_x and θankle_y are the angle of ankle joints, llink is 
the length of a thigh or a shank, Cx, Cy and Cz are the internal parameter of original 
posture. We assume these C are constant values to represent the COM in robot system 
correctly. In other words, the constant is not equal with regard to different robots. 
Besides, we suppose that the length of thigh and shank are the same. 
  35 
2.3 Fuzzy Logic Controller 
 
Fig.2.16 The structure of FLC 
2.3.1 The Concept of a Fuzzy Logic Controller (FLC) 
A basic fuzzy logic controller is shown in Fig. 2.16, which consists of four 
principal components: (1) fuzzification interface (FI), which transfers the range of the 
continuous input signal into linguistic fuzzy variables; (2) decision-making logic 
(DML), which infers fuzzy control employing fuzzy implication and the rule 
inference which make by human experience; (3) knowledge base (KB), which is used 
to define linguistic control rules; (4) defuzzification interface (DFI), which converts 
the inferred fuzzy control action back to a crisp continuous signal. The design of the 
four principal components of the fuzzy logic controller is detail described as follows: 
I. Fuzzification Interface (FI) 
The fuzzification plays an important role in dealing with the uncertain 
information, which might be objective or subjective in nature. The fuzzification 
interface involves the following functions: (1) measuring the values of input variables; 
(2) performs a scale mapping that transfers the range of values of input variables into 
  37 
The method yields 
 
( )
( )
1
1
n
c j j
j
crisp n
c j
j
c c
C
c
μ
μ
=
=
×
=
∑
∑
 (2-16) 
where n  is ¨the number of fuzzy sets of the control input, μc(cj) is the membership 
function value of the control input, and cj is the support of each fuzzy set j. 
2.3.2 The Application of FLC 
In Subchapter 2, we have introduced the generation of motion patterns, which 
can be seen as the control input of the robot system. According to the fuzzy control 
theory in Section 2.3.2, we design a fuzzy logic controller with an accelerometer and 
force sensors, as shown in Fig.2.17. The input of FLC includes two types of data, 
which are the ZMP computed by force sensors and the inclination of body received by 
the accelerometer. The both data have two dimensions, x and y directions. The ZMP is 
calculated by (2-5) and (2-6) in Section 2.2.1, and the inclination of the robot is 
transformed as follows, 
 _ _ _ _( ) /(( ) / 90),      ,i current i vertical i max i min i i x yΦ = Φ −Φ Φ −Φ =  (2-17) 
where Φcurrent_i are the current angle value transformed from accelerometer 
measurement in x and y directions, Φvertical_i are the angle value measured in x and y 
directions when the robot stands upright, Φmax_i and Φmin_i are the maximum and 
minimum angle values in x and y directions when the robot lies down on the ground, 
90 means the degree from vertical to horizontal, and Φi are the actual inclination 
values of the body in x and y directions. The e and e‧ of ZMP and the inclination are 
written as follows, 
  39 
changes. For example, the ZMP error in y direction of the robot coordinate can be 
normalized by a half of the sole length so that it is impossible to exceed 1 for this case. 
Besides, since the robot need swing and extend its body during walking, we let the 
number of fuzzy sets with respect to the inclination less than the number of fuzzy sets 
with respect to the actual ZMP, as shown in Fig. 2.20. 
 
Fig.2.18 The diagrams of Φy and the fuzzy sets to the inclination angle 
  41 
because the membership function of the singleton output can be tuned for different 
case by GA. In Fig. 4.10, the criterion of control input as follows, 
 
_
[ ]
[ ] [ ] [ 1]motion table
u k
k k k
θ
θ θ θ θ
Δ =
= + − + Δ  (2-19) 
where θ[k] is the input of our control system relative to (2-13), (2-14) and (2-15) in 
Section 2.3.2, θmotion_table is the original data of motion table, and u[k] is the singleton 
output from FLC. These θs are commands of motor introduced in Section 2.2.2. 
Therefore, aiRobot-2 will walk with the original motion table, and the angle of hip and 
ankles will be modified at any moment by FLC when the actual ZMP and the 
inclination are not match the desired values. For example, if the output in x direction 
is PB and its angle change about 14 degrees (PWM transform is 100), we will modify 
the data of original motion pattern base and send the latest command of position 
control to motors, as shown in Fig. 4.10. Note that the PWM value of motion table 
described in Section 2.3.1 is from 700 to 2300, the interval is 0.1 second between our 
commands of motion table, and the sample time of sensors is 0.3 second. It means the 
output of FLC will be result within 0.3 second to change the next command of motors 
and every motor regardless of its position command needing modifying will hold its 
position for 0.1 second. 
 
  43 
Table 2.1 The four inputs fuzzy rule table 
 
 
 
Fig. 2.23 The membership function of u 
 
  45 
approach is called binary genetic algorithm (BGA). For instance, a point (1, 3, 7) in a 
three-dimensional parameter space can be represented as a concatenated binary string: 
NNN
71 3
001011111 
In which each coordinate value is encoded as a gene composed of four binary bits 
using binary coding. Encoding schemes provide a way of translating problem-specific 
knowledge directly into the GA framework, and thus play a key role in determining 
GA’s performance. 
II. Initial population 
 It usually uses random number to fill with the bit string of initial population. The 
number of individuals in initial population depends on the complexity of problem. 
Every individual represents a solution of the problem, hence the more complexity the 
plant has the more parameters the GA need.  
III. Fitness evaluation 
 The first step after creating a generation is to calculate the fitness value of each 
member in the population. The fitness is the performance of GA to decide the 
goodness of individuals. The performance of fitness will move up after evolutions. 
IV. Reproduction 
 Reproduction operator is the process where members of the population 
reproduced according to the relative fitness of the individuals, where the 
chromosomes with higher fitness have higher probabilities of having more copies in 
the coming generation. There are a number of selection schemes available for 
  47 
Hence, we use GA to modify the membership function of fuzzy sets designed in 
Section 2.3.2, and expect this intelligent controller combined FLC and GA can find 
the most adapted walking motion from original motion base. First of all, we should 
define the meaning of the individual in GA. We use binary strings to represent our 
individuals. The transform is written as follows, 
 2 ( ) /(2 1)Lr B D UB LB LB= × − − +  (2-20) 
where r is the real number of gene in decimal system, B2D is the decimal number 
transferred from binary, UB is the maximum value of gene in decimal system, LB is 
the minimum value of gene in decimal system, and L is the bit length in an individual. 
In our approach, we assume that the range of PS is 0.01~0.5, the range of PB is 
0.51~1 and L is 5. Hence the value of UB is 0.5, and the value of LB is 0.01 in the 
tuning PS case; the value of UB is 1, and the value of LB is 0.51 in the tuning PB case. 
Besides, we suppose the membership function of PB and NB are the same but reverse 
sign, similarly as PS and NS. Next, we divide one individual into six chromosomes to 
provide enough binary bits for six fuzzy sets. The binary string includes two fuzzy 
sets of the error, two fuzzy sets of the error derivative and two fuzzy sets of the output. 
The total number of genes in an individual is 30 because each string of a chromosome 
has five bits, as shown in Fig. 2.25. It shows that we encode the membership function 
of fuzzy sets in Fig.2.20 into the binary string. The first and the second chromosomes 
represent the error, the third and the forth chromosomes represent the error derivative 
and the others represent the output. Consequently, we produce an initial population in 
randomize. It is a normal approach to build up each bit in the binary string. We fill out 
the total genes in an individual, and redo the random method seven times to produce 
other individuals, and let them become our first population. It means that there are 
eight individuals in every population. After having the first population, we start to 
  49 
 
Fig. 2.25 The diagram of chromosome in an individual 
 
 
Fig. 2.26 The error of the two ZMP trajectories 
2.4.3 The Realization of an Intelligent Controller 
 In order to balance the robot itself on a seesaw, it needs to use a FLC to adjust its 
body momentarily. How to climb stairs up and down is a complex equation for a 
humanoid robot. Hence, we produce a basic climbing motion and employ GA to 
  51 
(8) Finish the whole system when the robot has went down from the seesaw. 
 
Fig. 2.28 The flow chart of the control strategy with genetic algorithms for stairs 
climbing 
The complete self learning control strategy of stairs climbing is designed as 
follows and shown in Fig. 2.28: 
(1) Use MST to design a basic motion pattern for climbing stairs. 
(2) Define the desired ZMP trajectory and the desired inclination for this climbing 
motion. 
(3) Create the first population of GA with random sequences 
(4) The robot climbs stairs with a cycle, and the fuzzy controller runs during climbing. 
(5) Record the total actual ZMP data to calculate the fitness. 
(6) Run GA and generate the second population. 
(7) Back to step (4), until the number of generation reach our defined threshold. 
(8) Finish. 
 
 
 
 
  53 
about 0.5 second is increased a lot than before. 
According to the timing chart in the data sheet of the CMOS-EYE module shown 
in the Fig.3.2, we write the Verilog HDL to control the signal sequences of memory 
addresses and the data bus on the module [8]. The timing between the data transmitted 
is very important. If we send the signal at the wrong timing, the data may not receive 
correctly. The data format which stored in the SRAM is the Bayer RGB and the 
address for 160 120×  color image for each 19200 bytes are the red, green, and blue 
pixels, respectively illustrated as Fig. 3.3. 
 
Fig. 3.2 Timing chart of the SRAM control 
 
Fig. 3.3 The data format in the SRAM 
  55 
 
Fig. 3.4 3 3× structuring element 
The noises decrease obviously after the image processing with this algorithm which as 
shown in Fig. 3.5. 
3.1.3 Color Detection 
As regards identifying the object, we need to know the features of the object. The 
color of the object is one of the features [11]. When the image is sent to the BCB 
interface, we utilize the mouse to click the object. We save the RGB color ranges of 
the clicked pixel which can be more accurately adjusted by the interface and the 
interface shows the area in the RGB color ranges with light green. 
3.1.4 Edge Detection 
The color is only one of the features of the object. In order to achieve more 
accurate, we have to perform the edge detection before identification. Many methods 
  
(a) The original image (b) The processed image 
Fig. 3.5 The results of the MLM algorithm 
  57 
 
 13 3 7 9 11 15 17 19 23 8 12 14 1816* ( ) 2*( )G Z Z Z Z Z Z Z Z Z Z Z Z Z= − + + + + + + + − + + +  (3.10) 
 
 
 
 
 
 
 
Fig. 3.6 The result of gray level Fig. 3.7 LoG three dimensions curve 
  
  
  
Fig 3.8 LoG operator mask Fig 3.9 LoG structuring element 
  
  
  59 
figured out by ALA for tracking the ball [19]. In general, the Hough transform method 
[20-21] is used to detect the shape of circle in an image. Nevertheless, the time 
complexity of the Hough transform method takes too much, so we choose another 
detective method. The method we choose is the RCD (Randomized Circle Detection) 
[22-23]. This method can take better accuracy and detection speed into account than 
previous. With the result of the LoG, we can obtain the edge set {( , )}V x y= of the 
image and then pick four edge points out randomly in the set V . In the geometry, any 
four points can form four circles as shown in Fig. 3.12. If the four points which pick 
out come from the same circle, the circle can be taken as a candidate circle. The RCD 
method is on basis of the above basic theory. 
We usually represent the standard equation of the circle as 
 2 2 2( ) ( )x a y b r− + − =  (3.11) 
And then we can rewrite (3.11) as 
 
2 22 2xa yb d x y+ + = +  (3.12) 
where 2 2 2d r a b= − − . 
Assuming there are three edge points 1 2 3, ,V V V  which are picked out randomly 
and non-collinear, they can form a circle called 123C  and then we can figure out the 
center of the circle 123 123( , )a b  and the radius 123r . Substitute the coordinates of the 
 
Fig. 3.12 Four circles formed by any four points 
  61 
pixel process with (3.17); otherwise, we don’t think that pixel is an edge point on the 
circle. The procedure can reduce the unnecessary computation. Finally, if the value of  
Fig. 3.13 The adjustable searching range used in RCD 
the counter nC  is larger than gT  that is the threshold of the supporting circle, the 
circle 123C  is identified a real circle. We test this method with images in some 
conditions. The result of this algorithm is shown in Fig. 3.14. The circle found by the 
RCD method is drawn in the gray color. 
  63 
3.2 Vocabularies and Numbers Recognition with Speech 
If the humanoid robot can only execute tasks unilaterally and can not interact 
with human, it loses the charms to human. Take account of the market to the teaching 
robot. If the children can be educated by the humanoid robot, they may be interested 
in the humanoid robot. Furthermore, we can achieve the objective that aiRobot-2 
interacts with human. On the basis of above reasons, we design two events including 
the English vocabularies recognition and numbers comparison to show the teaching 
functions with aiRobot-2. 
The steps of recognition can be divided into the vision system and the control 
strategy system as shown in Fig. 3.15. We will discuss each step in detail individually 
thereunder. 
Fig. 3.15 The flowchart of the recognition 
3.2.1 Image Capture and Pre-process 
 The image is also captured by the CMOS-EYE and delivered to Nios II as Fig. 
3.16. After we got the image, we need to pre-process the image. The pre-process 
which is very important to the accuracy for recognition is similar to the Section 3.2. 
Firstly, we have to filter out the noises, and convert the color image to the gray level 
image. Finally, we convert the gray level image to the binary image as Fig. 3.17. In 
order to speed up the processing, the background in our experiments is white. Thus, 
  65 
coordinate of the pixel in the structuring element, and ( , )i j  is the coordinate of the 
pixel in the original image. 
On the other hand, the erosion operation is to find the minimum value in the 
structuring element. The symbol Θ  in (3.19) represents the erosion operator. In the 
programming, we use the logic operators “|” and “&” to substitute the maximum and 
minimum calculation to reduce the computation, respectively. Fig. 3.18 shows the 
result of the dilation operation. We can see that isolated dots around the word are also 
be dilated simultaneously. However, the isolated dots disappeared with the erosion 
operation in Fig. 3.19 and the characters are also rough. To compensate the image and 
  
(a) The dilation image for ”LION” (b) The dilation image for “CAT” 
Fig. 3.18 The dilation images 
  
(a) The erosion image for ”LION” (b) The erosion image for “CAT” 
Fig. 3.19 The erosion images 
  
(a) The combination image for ”LION” (b) The combination image for “CAT” 
Fig. 3.20 The combination images 
  67 
existed in two characters. The explanation of the method is shown in Fig. 3.21. We 
scan the binary image column by column from left to right. Once we detect the 
character pixel, we record the previous column of the detected column called the 
character start point. Continue scanning until there is no character pixel on the whole 
column. We record the next column of the detected column called the character finish 
point. So far, one possible character region searching is accomplished. Then, we keep 
on scanning till last column. Similarly, we can locate the up and lower bound for each 
character by using this method. The results are shown in Fig. 3.22. 
3.2.4 Valid Region Judgment 
 After each possible character region found in the image, we are going to judge 
which is the proper character region. We set a threshold according to the length and 
the width in our experiments. Adjudge the boundary of each possible region that 
satisfies the threshold or not. If the possible region satisfies the threshold, we judge 
the one is the proper region. Others which don’t satisfy the threshold are not be 
considered to reduce the computation and increase the accuracy of the recognition.  
 
 
  
(a) LION (b) CAT 
Fig. 3.22 The results of the region segmentation 
  69 
Task 4  Control Strategy System 
4.1 Introduction 
In this chapter, we focus on the control strategy system of aiRobot-2, which is built on 
the Altera Nios II FPGA board. In order to let the target locate in the center of the 
image plane, we regulate the rotation angles of the motors according to the fuzzy 
logic controller (FLC) with the error between the position of the target and the central 
coordinate of the image, and the difference of the error as the inputs [29]. We also 
figure out the distance between the target and aiRobot-2 without the stereo vision. The 
visual servoing system is implemented by the CMOS-EYE and the twenty nine RC 
servo motors on the humanoid robot. Fig. 4.1 shows the framework of the vision 
control system. We design four strategies to demonstrate the multi-function of 
aiRobot-2. The FIRA PK event and the bowling event track to achieve the goal by the 
FLC. The numbers comparison and the vocabulary recognition demonstrate the 
interaction of aiRobot-2 with motions and speech. This strategies work based on the 
trained motions. 
  71 
see the ball in his sight. However, sometimes the ball is placed out of the robot’s sight, 
and aiRobot-2 needs to search the ball. The two RC servo motors on aiRobot-2’s head 
can rotate left, right, upward and downward. We utilize the two motors for searching 
and tracking the ball. In order to decrease the searching time, we design nine fixed 
directions for the searching. For the horizontal plane, we design three directions and 
also three directions for the vertical plane as shown in Fig. 4.2. The nine directions 
can involve the field of view in a half circle with the radius 40 cm. If the ball can not 
be found in the nine directions, aiRobot-2 walks forward an appropriate distance and 
the head returns to the last place to search again. 
 
(b) The horizontal plane 
 
 
(a) The vertical plane (c) The field of view 
Fig. 4.2 The searching directions 
  73 
Fig. 4.4 The common configuration of the FLC close-loop system 
 After getting the target position ( , )x y , we have to transform this coordinate into 
the normalized position ( , )n nx y which is in the range of [-1 1]. This procedure is 
called the normalization for the FLC input. We calculate with (4.1) and (4.2) to 
normalize the difference between the target and the center of the image plane for the 
FLC input. Note that the coordinate of the most left-top and the most right-bottom 
points are (0,0) and (159,119), respectively as shown in Fig. 4.3. 
 
80    ,   80
79
80   ,   80
80
n
x x
x
x x
−⎧ ≥⎪⎪= ⎨ −⎪− <⎪⎩
 (4.1) 
 
60    ,   60
59
60   ,   60
60
n
y y
y
y y
−⎧ ≥⎪⎪= ⎨ −⎪− <⎪⎩
 (4.2) 
The FLC that combines multi-valued logic, probability theory, and knowledge 
base is the digital control methodology. Based on the expert knowledge, the FLC can 
convert linguistic control strategy into automatic control. A common configuration of 
the FLC system is shown in Fig. 4.4 which includes four mainly structures：(1) a 
fuzzification interface (FI) (2) a knowledge base (KB) (3) a decision-making logic 
(DML) (4) a defuzzfication interface (DFI). Each structure is discussed below. 
(1) Fuzzification Interface (FI) 
  75 
 
(b) The input ( , )n nx y   
 
(c) The output ( , )x yθ θ  
Fig. 4.5 The membership functions of the input and output 
 
(2) Knowledge Base (KB) 
The KB of the FLC is composed of two components which are a data base and a 
control rule base. The data base defines the fuzzy linguistic rules and the fuzzy data 
manipulation. The control rule base which is inferred by experience and engineering 
judgment uses the linguistic control conditions to define the control strategies. The 
construction of the KB in FLC is application dependent. Upon the statement above, 
the data base comprises the membership of the FI and DFI and the control rule base is 
the rule table in the DML. 
  77 
Table 4.1 The Fuzzy rule table 
 
 
(4) Defuzzification Interface(DFI) 
The DFI is the mapping that from a space of fuzzy control actions defined over an 
output universe of discourse into a space of crisp control actions. The membership 
function of the output is shown in Fig. 4.5 (c). The defuzzification strategy we use in 
our FLC is the combination of COA (Center of Area) and MOM (Mean of Maximum) 
methods shown as 
 
 
( )
( )
1
1
x i ii
i i
n
x x
i
x n
x x
i
Wθ μ μ
θ
μ μ
=
=
× ×
=
×
∑
∑


 (4.5) 
 
 
 
( )
( )
1
1
y i ii
i i
n
y y
i
y n
y y
i
Wθ μ μ
θ
μ μ
=
=
× ×
=
×
∑
∑


 (4.6) 
where n  is the number of fuzzy sets for the control input. 
  79 
 
Fig. 4.6 The configuration of motors mounted on the head 
4.2.3 The Strategy for Offense 
 When the ball is tracked to the middle of the image, we are going to figure out 
the distance and the orientation between the ball and aiRobot-2. Because there is only 
one eye on aiRobot-2, the depth calculation of the stereo vision can not be utilized 
here. Thus, we combine the two motors on the head and the one eye to figure out by 
geometric mathematics. We can neglect the distance between the motor and the image 
sensor because the image sensor is flat enough that we can consider the motor and the 
sensor are in the same plane. The analysis for the tilt motor is illustrated in Fig. 4.7. 
The distance between the ball and aiRobot-2 can be calculated by the trigonometric 
function. Performing some simple mathematical calculations, the distance between 
aiRobot-2 and the surface of the target denoted s  may be estimated as 
 cos ( sin ) tany y ys L L Hθ θ θ= + +  (4.11) 
  81 
as in the case 1, aiRobot-2 turns right an appropriate angle for shooting. Similarly 
does the case 3. The case 2 is a special case that the corner is absent to aiRobot-2. 
However, if Ld  is smaller than Rd , aiRobot-2 turns right a small angle, otherwise, 
aiRobot-2 turns left a small angle. 
 
   
(a) Case 1 (b) Case 2 (c) Case 3 
Fig. 4.9 The analysis for the goal and the goalkeeper 
 
After aiRobot-2 fixes the orientation, the ball may not close the right foot which 
is for shooting. Therefore, aiRobot-2 tracks the ball again. By shifting left or shifting 
right and walking forward, the ball is located at the right foot. Finally, the ball is near 
the right tiptoe and is kicked by the right foot of aiRobot-2. The flowchart of the 
offense control strategy of the FIRA PK event is illustrated in Fig. 4.10. 
 In ideal circumstances, aiRobot-2 has a strong probability to accomplish the 
offense mode successfully in the PK event through above steps. However, in the 
actual environment for most competitions, the light and the color of objects are not 
uniform perfectly, and the shadows of many spectators and miscellaneous objects in 
the field of play may influence the results from image analysis. Besides, an unstable 
motion of the robot may also result in unexpected errors and failure. 
 
  83 
4.3 The Control Strategy System for Bowling 
 For more entertainments with the humanoid robot, aiRobot-2 can not only kick 
the ball, but throw the ball. We design another event that aiRobot-2 can play the 
bowling. The environment of the bowling event is composed of ten bowling pins, a 
bowling alley ,a Foul line, two gutters and a bowl as shown in Fig. 4.11. The bowling 
pins, the bowling alley and the bowl are made of the drink bottles of Yakult, the 
cardboard and the golf ball, respectively. The two sides of the bowling alley which 
mark black are the gutters. The length and the width of the bowling alley are 155cm 
and 55cm, respectively. Although the proportional is not similar to the real 
environment, the length is still larger than the width. The flowchart of the bowling 
event and the detailed strategies are also discussed below. 
 
Fig. 4.11 The environment of the bowling event 
4.3.1 The Targets for Tracking 
The image processing of the bowling event is similar to the PK event. The 
  85 
 
 
  
(a) The tilt motor (b) The pan motor 
Fig. 4.13 The analysis of the bowling 
 
Still we can firstly figure out the distance S  from Fig. 4.13(a)  
 cos ( sin ) tany y yS L L Hθ θ θ= + +  (4.11) 
as described in Section 4.2.2. In order to figure the rotation angle 'ρ , we have to 
define some variables in Fig. 4.13(b) as 
 W  is the distance from the motor of the head to the right hand. 
 cW  is the horizontal distance from the target and the motor of the head. 
 D  is the distance parameter of the alley. 
 φ  is the initial modified angel of the right hand motor. 
 ρ  is the rotation angel of the head panned motor. 
 'ρ  is the rotation angel of the right hand motor. 
  87 
Fig. 4.14 The flowchart of the bowling event 
  89 
result. The shortest distance means the least error. If there are rest proper character 
regions, keep on the matching until last one. 
4.4.2 Vocabulary Recognition 
Fig. 4.15 shows the result of the recognition displayed on the Nios II IDE. The 
first line is the number of the segmented region. The second line is the recognition 
result. The third line is the result of the valid region judgment. The result information 
of the recognition and instructions are sent to the TTS by Nios II for speaking. The 
TTS communicates with the Nios II by UART and the data are ASCII code. So we 
convert the recognition result to the ASCII data for transmitting to the TTS. We can 
send the corresponding command in ASCII code as in Table 4.2 to control the state of 
the TTS. Note that the TTS processes when it receives the termination character “;” 
(semicolon). The flowchart of the vocabulary recognition is described in Fig. 4.16. 
 
 
 
  
(a) ”LION” (b) ”CAT” 
Fig. 4.15 The results of the recognition 
  91 
Fig. 4.16 The flowchart of the vocabulary recognition 
4.4.3 Numbers Comparison 
This strategy is a calculation for aiRobot-2. aiRobot-2 recognizes the two 
  93 
 
Fig. 4.18 The flowchart of the numbers comparison 
  95 
  
(a) The original image (b) The result 
Fig. 4.1 The results of the RCD method for image with lines and circles 
 
  
(a) The original image (b) The result 
Fig. 4.2 The results of the RCD method for image with a triangle and circles 
  
(a) The original image (b) The result 
Fig. 4.3 The results of the RCD method for image with some geometries 
(a) (b)
 
(c) (d)
 
  97 
4.1.2 Experimental Results of Strategy for PK Event 
 The environment of the PK event is based on the PK rule of FIRA. We placed a 
goalkeeper to be the defender. The ball is the orange plastic ball. The color of the door 
is light blue. We have tested this PK strategy in the competition of 2008 IEEE Hands 
on which is an annual international student experimental project competition via the 
internet and were awarded to the 2nd price. The following images which are shown in 
Fig. 4.6 are captured from the video of Hands on competition. aiRobot-2 firstly scans 
the environment for searching the orange ball from left to right in Figs. 4.6 (a)-(b). 
Then, aiRobot-2 has tracked and found the position of the ball and rotates left in Fig. 
4.6 (c). Fig. 4.6(d) shows that aiRobot-2 starts to walk straightly to approach the ball. 
aiRobot-2 will stop walking in front of the ball and check if the ball is near the tiptoe 
as shown in Fig. 4.6 (e). Next, aiRobot-2 is going to analyze the goal situation by 
raising his head in Fig. 4.6 (f) because the ball is near the tiptoe. After that, aiRobot-2 
rotates right to face the best shooting orientation. Unfortunately, the ball is moved by 
aiRobot-2 unexpectedly when aiRobot-2 is rotating in Fig. 4.6 (g). Therefore, 
aiRobot-2 tries to track the ball again and analyze the goal again in Figs. 4.6 (h)-(j). 
The ball now is near the tiptoe and in the shooting area by checking with the vision 
system in Fig. 4.6 (k). Finally, aiRobot-2 kicks the ball and the ball is shot into the 
goal as shown in Figs. 4.6 (l)-(n). 
 
 
 
 
 
 
  99 
 
(k) (l)
 
(m) (n)
 
Fig. 4.6 The results of the strategy of the PK 
 
 
 
 
 
 
 
  101 
 
(a) 
 
(b)
 
(c) 
 
(d)
 
(e) 
 
(f)
 
(g) 
 
(h)
 
 
 
 
  103 
(a) 
 
(b)
 
(c) 
 
(d)
 
(e) 
 
(f)
 
(g) 
 
(h)
 
 
 
 
  105 
4.1.4 Experimental Results of Strategy for Numbers Comparison 
This strategy shows the intelligence and the robust recognizing ability of 
aiRobot-2. To ensure that the numbers are not designated before, we choose the 
numbers by throwing the dice. Firstly, the dice is thrown twice to choose the numbers 
for comparison as in Figs. 4.9(a)-(b). The results of the dice show five and three. Next, 
we put the number cards on the board as in Figs. 4.9(c)-(d). Then, Fig. 4.9(e) shows 
aiRobot-2 starts to recognize since he found two numbers. After finishing recognizing 
and calculating, Figs. 4.9(f)-(g) shows aiRobot-2 raises his right hand because the 
number of the right side is bigger. aiRobot-2 also speaks out the sentence which is 
“three is smaller than five” to explain the relation of the two numbers. Finally, 
aiRobot-2t puts down his hand and returns to the initial state as in Fig. 4.9(h). Fig. 
4.10 shows another result of the experiment. aiRobot-2 raises both hands when the 
two numbers are the same. 
(a) 
 
(b)
 
(c) 
 
(d)
 
    
  107 
4.1.5 Experimental Results of Strategy for Vocabulary Recognition 
The results of the vocabulary recognition are shown in Fig. 4.11. The vocabulary 
“LION” is that aiRobot-2 needs to recognize. Firstly, we put the vocabulary card on 
the board as in Fig. 4.11(a). aiRobot-2 then is going to recognize the seeing word. 
After finishing identifying the vocabulary, aiRobot-2 speaks out the word “LION” 
which are shown in Figs. 4.11(b)-(c). Figs. 4.11(d)-(g) are the original image captured 
from the CMOS-EYE and the results of the recognition algorithm which display on 
the lower and upper picture of the BCB interface, respectively. Finally, the 
recognition result is shown on the Nios II IDE as in Fig. 4.11(h). The first line is the 
region segmentation block. The second line is the result of the recognition. The last 
line is the valid segmentation block. 
 
(a) 
 
(b)
 
(c) 
 
(d)
 
    
  109 
before the vocabulary recognition. We have presented a region segmentation method 
for locating the characters in the vocabulary recognition and the performance is good 
for the experiment. Then, the pixel ratio method is used to encode for matching. 
Finally, the improved template matching method via Euclidean distance has presented 
on the recognition and the results have been spoken by the TTS module.  
Four control strategies including the PK event, the bowling event, the vocabulary 
recognition and the numbers comparison have been presented in this thesis. aiRobot-2 
can successfully track specific objects via the CMOS sensor controlled by two small 
servomotors. The motions of aiRobot-2 are decided by analyzing the condition of the 
environment from the feedback image. Finally, the experiment results show the 
capability, the efficiency and validity of the vision system in the 2007 IEEE 
International Student Experimental Hands-on Project Competition. 
4.3 Future Works 
 There are three important issues are recommended for future works. The first is 
that the humanoid robot should have more interactions with human. For example, the 
humanoid robot and the human can talk to each other, even play games each other. 
The second is the development of the wireless communication system. Humanoid 
robots should communicate with each other through Zigbee or Bluetooth wireless 
tran-receivers. Last, the noise of the image depends on the length of the transmissive 
bus is the drawback of the vision system. We have to choose a better image sensor for 
the vision system in the future. 
 
 
 
  111 
六 References 
[1] I. W. Park, J. Y. Kim, S. W. Park and J. H. Oh, “Development of humanoid 
robot platform KHR-2 (KAIST Humanoid Robot-2),” in Proc. IEEE/RAS Int. 
Conf. on Humanoid Robots, vol. 1, pp. 292-310, Nov. 2004. 
[2] K. Kaneko, F. Kanehiro, S. Kajita, H. Hirukawa, T. Kawasaki, M. Hirata, K. 
Akachi and T. Isozumi, “Humanoid robot HRP-2,” in Proc. IEEE Int. Conf. on 
Robotics and Automation, vol. 2, pp. 1083-1090, Apr. 2004. 
[3] H. H. Lund, “Modern artificial intelligence for human-robot interaction,” in 
Proc. IEEE, vol. 92, Issue 11, pp. 1821-1838, Nov. 2004. 
[4] J. A. Adams and H. K. Keskinpala, “Analysis of perceived workload when 
using a PDA for mobile robot teleoperation,” in Proc. IEEE Int. Conf. on 
Robotics and Automation, vol. 4, pp. 4128-4133, Apr. 2004. 
[5] J. Ido, Y. Matsumoto, T. Ogasawara and R. Nisimura, “Humanoid with 
interaction ability using vision and speech information,” in Proc. IEEE/RSJ Int. 
Conf. on Intelligent Robots and Systems, pp. 1316-1321, Oct. 2006. 
[6] FIRA, http://www.fira.net 
[7] J. Y. Kim, I. W. Park, J. Lee and J. H. Oh, “Experiments of vision guided 
walking of humanoid robot, KHR-2,” in Proc. IEEE-RAS Int. Conf. on 
Humanoid Robots, pp. 135-140, Dec. 2005. 
[8] S. Palnitkar, Verilog HDL A Guide to Digital Design and Synthesis, 
PRENTICE-HALL, Inc. 1996. 
[9] R. Wichman and Y. Neuvo, “Multilevel median filters for image processing,” in 
Proc. IEEE Int. Symposium Circuits and Systems, vol. 1, pp. 412-415, Jun. 
1991. 
[10] A. Taguchi and Y. Murata, “Improving the performance of multilevel median 
filters for image processing,” in Proc. 35th Midwest Symposium Circuits and 
Systems, vol. 1, pp. 369-372, Aug. 1992. 
[11] J. Luo and D. Crandall, “Color object detection using spatial-color joint 
probability functions,” IEEE Transactions on Image Processing, vol. 15, Issue 6, 
pp. 1443-1453, Jun. 2006. 
[12] S. Suthaharan, “Image and edge detail detection algorithm for object-based 
coding,” Pattern Recognition Letters, vol. 21, Issues 6-7, pp. 549-557, Jun. 
2000. 
[13] S. T. Acton and D. P. Mukherjee “Area operators for edge detection,” Pattern 
Recognition Letters, vol. 21, Issue 8, pp. 771-777, Jul. 2000. 
[14] Y. H. Yua and C. C. Chang, “A new edge detection approach based on image 
context analysis,” Image and Vision Computing, vol. 24, Issue 10, pp. 
  113 
[27] T. Gevers, “Robust Segmentation and Tracking of Colored Objects in Video,” 
IEEE Transactions on Circuits and Systems for Video Technology, vol. 14, Issue 
6, pp. 776-781, Jun. 2004. 
[28] R. G. Casey and E. Lecolinet, “A survey of methods and strategies in character 
segmentation,” IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 18, Issue 7, pp. 690-706, Jul. 1996. 
[29] K. Hirota and Y. Kyo, “A target tracking robot based on fuzzy control,” in Proc. 
IEEE International Workshop on Robot and Human Communication, pp. 
335-340, Sep. 1992. 
[30] A. Jain, “Object tracking using fuzzy logic for Khepera-II robot, “ Illinois 
Journal of Undergraduate Research at the University of Illinois at 
Urbana-Champaign, vol. 1, 2006. 
[31] I. Harmati and K. Skrzypczyk, “Robot team coordination for target tracking 
using fuzzy logic controller in game theoretic framework,” Robotics and 
Autonomous Systems, in Press, Corrected Proof, Feb. 2008. 
[32] X. Fan, N. Zhang and S. Teng, “Trajectory planning and tracking of ball and 
plate system using hierarchical fuzzy control scheme,” Fuzzy Sets and Systems, 
vol. 144, Issue 2, pp. 297-312, Jun. 2004. 
[33] S. H. Hahn, J. H. Lee and J. H. Kim, “A study on utilizing OCR technology in 
building text database,” in Proc. Int. Workshop on Database and Expert Systems 
Applications, pp. 582-586, Sep. 1999. 
[34] A. Busch, W. W. Boles and S. Sridharan, “Texture for script identification,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, Issue 
11, pp. 1720-1732, Nov. 2005. 
[35] LM386N-3 IC, http://cache.national.com/ds/LM/LM386.pdf  
[36] C. R. David and C. Jordi, “Camera-based digit recognition system,” in Proc. 
IEEE Int. Conf. Electronics, Circuits and Systems, pp. 756-759, Dec. 2006. 
[37]  Y. Choi, D. Kim, Y. Oh and B. J. You, “Posture/walking control for humanoid 
robot based on kinematic resolution of CoM Jacobian with embedded motion,” 
IEEE Transactions on Robotics, vol. 23, no. 6, pp. 1285-1293, Dec. 2007. 
 
 
 
  115 
八 可供推廣之研發成果資料表 
可供推廣之研發成果資料表 
■ 可申請專利  ■ 可技術移轉                          日期：98 年 10 月 29 日 
國科會補助計畫 
計畫名稱： 
小型人形機器人之設計、研製及應用－子計畫四：視覺系統與智慧
型控制之研發及其於人形機器人足球賽之應用(3/3) 
計畫主持人：李祖聖  教授 
計畫編號：NSC 95-2221-E-006 -382 -MY3 學門領域：控制學門
技術/創作名稱 小型人形機器人 
發明人/創作人  
本計畫主要分成兩部份探討。第一部份著重於步伐的設計，利用模
糊理論結合基因演算法，我們可以設計出一智慧型步伐產生器，讓
小型人形機器人可以適應不同的環境並行走。第二部份則在於實現
視覺處理系統，利用隨機測圓演算法可以找出所要的目標物，並讓
小人型機器人進行目標追蹤。此外，結合本計畫所開發之技術，我
們可以實現四種控制策略：PK 事件、打保齡球、字元辨識及數字
比較。 
技術說明 This sub-project emphasizes on two parts. The first part is to design 
an intelligent gait generator with the FLC and GA theory. Utilizing the 
intelligent gait generator, the robot can adjust different environment 
and walking. 
The second part is to develop an image processing system which can 
recognize the object and track the target fore the small-sized humanoid 
robot. The object recognition identifies the target via the RCD method. 
Combing the techniques proposed in this sub-project, we can design
four control strategies: the PK event, the bowling event, the vocabulary 
recognition and the numbers comparison.  
可利用之產業 
及 
可開發之產品 
本計畫之可利用產業包含自主式機器人設計與開發，並可延伸至娛
樂、教育機器人之領域。 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 95-2221-E-006-382-MY3 
計畫名稱 小型人形機器人之設計、研製及應用--子計畫四：視覺系統與智慧型控制之研發及其於人形機器人足球賽之應用 
出國人員姓名 
服務機關及職稱 
國立成功大學電機系 李祖聖 特聘教授 
會議時間地點 June 14-17, Ft. Mason's Festival Pavilion, San Francisco, USA 
會議名稱 2007 FIRA RoboWorld Cup 
發表論文題目 Design and Implementation of Fuzzy Auto-Balance Control for Humanoid Robot 
 
一、參加會議經過 
2007 FIRA RoboWorld Cup 在美國舊金山的 Ft. Mason's Festival Pavilion 舉辦。今年
的賽事與RoboGames 2007同時舉行，共有 28 國, 62 種競賽, 249 隊伍, 865個機器人, 以
及 846 人與會。 
計畫主持人發表論文題目是 Design and Implementation of Fuzzy Auto-Balance Control 
for Humanoid Robot，本論文重點是運用加速度計與壓力感測器，為人形機器人設計自我
平衡的模糊控制器。除了論文發表外，這也是計畫主持人第二次帶隊參加 FIRA 
RoboWorld Cup。 
 
二、與會心得 
計畫主持人實驗室隊伍，獲得人形機器人組(HuroCup League)之 PK (Penalty Kick)賽
第三名及總成績第四名。計畫主持人也已應邀擔任 FIRA(Federation of International 
Robosoccer Association )之理監事(BOG)。 
 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 95-2221-E-006-382-MY3 
計畫名稱 小型人形機器人之設計、研製及應用--子計畫四：視覺系統與智慧型控制之研發及其於人形機器人足球賽之應用 
出國人員姓名 
服務機關及職稱 
國立成功大學電機系 李祖聖 特聘教授 
會議時間地點 Oct. 27-30, 2007, Shanghai, China 
會議名稱 The 2nd International Symposium on Nonlinear Dynamics (2007 ISND) 
發表論文題目 
1. Interval Type 2 Fuzzy Sliding-Mode Control of a Unified Chaotic System 
2. Robust H∞  Fuzzy Control of a Class of Fuzzy Bilinear Systems with 
Time-Delay 
 
五、參加會議經過 
The 2nd International Symposium on Nonlinear Dynamics (2007 ISND)係 International 
Journal of Nonlinear Dynamics and Nonlinear Simulations 舉辦之國際會議。計畫主持人應
邀組織一 Mini-Symposium，其主題為 Fuzzy modeling, estimation, and control of nonlinear 
dynamic systems，本 Mini-Symposium 共 10 篇論文。 
計畫主持人共發表兩篇論文，第一篇是“Interval Type 2 Fuzzy Sliding-Mode Control of 
a Unified Chaotic System”，本論文重點是設計 Type 2 模糊控制器以穩定聯合是混沌系
統。另一篇論文是“Robust H∞  Fuzzy Control of a Class of Fuzzy Bilinear Systems with 
Time-Delay”，本論文重點是提出穩定模糊雙線性時延系統之強韌性模糊控制步驟與方
法。 
 
 
六、與會心得 
由於參加 2007 ISND，計畫主持人已受邀擔任 International Journal of Nonlinear 
Dynamics and Nonlinear Simulations 之 Editor。 
 
