                                           2
行政院國家科學委員會專題研究計畫報告 
 
總計畫: 結合生物反饋之新世代腦機介面及其在移動載具
控制之應用(II) 
 
子計畫四:以機器視覺為基礎之人意向移動載具獲知與追蹤(II) 
 
Subproject IV: A Machine Vision-Based Human-Intending Moving Vehicle 
Capture and Tracking (II) 
 
計畫編號 : NSC–97–2221–E–009–140 
 
執行期限：97/08/01–98/07/31 
 
主持人：張志永    交通大學電機與控制工程學系教授 
 
 
一、 中文摘要 
 
本計畫以機器視覺為基礎之人意向移動載
具獲知與追蹤課題。腦波生理訊號，通常受試驗
者的視覺、聽覺、感受等影響，本子計畫的目的，
在於利用視覺狀態回饋給使用者，探討其對的人
腦波訊號之影響。利用本計畫結果，子計畫三可
據此鑑別岀重要視覺腦波生理訊號的位置，即頻
道，及訊號強弱變化，一起與子計畫五方向感腦
波訊號、子計畫二注意力腦波訊號，整體訓練學
習後，能提昇在移動載具的控制效率。本子計畫
執行所需之移動載具，擬以電動代步車為實驗平
台。第一年將探索在室內環境情況下，人坐在移
動載具車之視訊獲知及其方位獲知方法。透過人
體輪廓樣板比對及前景人物所朝的方向，我們提
出的方法對於人體方位偵測的準確度相當高，角
度誤差低於 4°。本 (第二) 年將探索在室內環境
情況下，視訊影像之人體區域分離及其深度距離
估測方法。 
在本文中，我們結合了影像中特徵、輪
廓和空間分佈等資訊來判別不同的分割區
域，將代表著同一個物體區域分割，再由人
臉偵測技術，希望能在場景中將人體抽取出
來。首先，我們利用了膚色資訊與橢圓樣板
比對找出人臉在影像中的位置，接著我們提
出了一套改良式自動種子區域成長演算法來
分割影像，並根據人體的形態完成前景人物
偵測，最後再依據人物在影像中的垂直 y 座
標值，對其做深度估測，達成重建二維平面
影像的三維景深資訊。 
 
關鍵詞：皮膚偵測、區域生長、人體偵測、人深
度距離估測 
 
英文摘要 
 
This project is concerned with machine 
vision-based moving vehicle capture and tracking. 
It is known that brain signals are dependent on the 
excitations of vision, audio, and feeling of a tester. 
The purpose of this project is to provide  various 
feedback to a user and then investigate its effect on 
brain signaling. With an excitation, subproject III 
will identify the responsive brain channel and its 
                                                                                      4 
 
                                        
second component is human recognition. The 
interested human region is extracted by using some 
semantic rules and merging the adjacent regions 
with high similarity. The third component is depth 
estimation. The humans’ y-coordinates in the image 
are first detected and the depths are estimated based 
on the depth look-up table of the camera. 
 
II. Human Face Detection 
 
In this section, we introduce the face extraction, 
which is helpful to locate the subject  of interests. 
Firstly, we utilize skin-color extraction method to 
detect skin  in the  color space. Secondly, we 
utilize an elliptical model to match human face. 
Consequently, we can combine skin-color 
extraction and ellipse fitting to better estimate the 
location and size of a human face.  
 
A. Skin-Color Extraction 
The first stage in the face detection algorithm 
makes use of skin-color extraction to distinguish the 
face-region from non-face-region. Several color 
spaces suitable for segmenting the skin-color in an 
image have been proposed. Choosing the 
representative and discriminative color space for 
the skin-color modeling becomes very important. In 
[4], rbCYC  and HSV color spaces for skin-color 
segmentation have been investigated. It was 
concluded that the skin color distribution in rbCYC  
color space is more centralized than HSV color 
space. The color space of ,CYC rb  which revises 
the color space of YUV, can divide luminance 
component ),Y(  and two chromatic blueness 
component ( bC ), redness component ( rC ).  
It is known from [5] that the suitable ranges for 
skin-color regions are ]127 ,77[=
bC
R  and 
].173 ,133[=
rC
R  Garcia and Tziritas [4] constructed a 
more complex skin color decision boundary up of 
eight planes in the rbCYC  space. 
Chi et al. [6] presented a robust skin-color 
extraction technique and resulted in less erroneous 
pixels. In Fig. 1, for each rC  in the skin-color 
region, the maximum and minimum bC  are used to 
estimate the upper and lower quadratic functions. A 
pixel is labeled as skin candidate if it falls within 
the locus. Skin pixels of the proposed approach are 
determined by 
⎩⎨
⎧ Γ>⋅Γ<=
otherwise
))(())(( 
  
,0 
1, 
),(  rbottombrupbrb
CCCC
CCSkin      (1) 
where
01
2
2)( ααα ++=Γ xxxup  and 
01
2
2)( βββ ++=Γ xxxbottom . For the upper bound, the 
quadratic coefficients found are 0225.02 −=α , 
1251.61 =α , and 2900 −=α  while the lower bound 
coefficients are 0284.02 =β , 1477.91 −=β , and 
8360 =β . 
 
 
Fig. 1.  Samples distribution of skin-color pixels in 
rbCC  space [6]. 
 
B. Elliptical Face Matching 
An ellipse can be described by the following 
equation: 
 
1
)()(
2
2
0
2
2
0 =−+−
yx S
yy
S
xx                 (2) 
where Tyx ) ,( 00  is the center of the ellipse, yS  and 
xS  are the major and minor radiuses satisfied by 
                                                                                      6 
 
                                        
candidate or not. A pixel could be considered as an 
initial seed if ( ) .3 , ≤yxR  Some adjacent  regions 
having  similar intensity but are different parts in 
the real world may be detected no edge and cause 
discontiguous edges. The restriction described 
above can solve such problem and avoid regions 
over-integrated.  
 
  
 
(a) 
 
(b) 
 
Fig. 2. (a) A general 55×  mask located at ( ), , yx  
(b) a predefined mask coefficients 
( ) 2. , ,2  2, , ,2  , , …… −=−= vuvuw  
 
B. Seed Labeling 
The initial seeds with 8-connectivity are grouped 
as one set and assigned the same label. We utilize 
the connectivity concept to find the connected seed 
regions.    Connectivity between pixels is a 
fundamental concept that simplifies the definition 
of numerous digital image concepts, such as regions 
and boundaries. To establish if two pixels are 
connected, it must be determined if they are 
neighbors and if their gray levels satisfy a 
specified criterion of similarity. 
  
C. Seeded Region Growing (SRG) 
After the initial seed regions are generated and 
labeled, the regions start growing from the 
unclassified pixel located along region boundary 
with the minimum color distance to one of its 
neighbors. This procedure is repeated until all 
pixels are classified and labeled. The seeded region 
growing algorithm is described as follows: 
● Step 1: Assign a label to each seed region after 
the initial seeds are selected automatically. Let 
nSSS  , , , 21 …  be the initial seeds which have been 
grouped into n  sets and nAAA  , , , 21 …  be their 
corresponding regions. 
● Step 2: T denotes the set of all unclassified 
pixels which are neighbors of at least one of the 
labeled region [8].  
 ( ) ( ) ⎭⎬
⎫
⎩⎨
⎧ ≠∩∉=
==
∩∪ n
i
i
n
i
i AyxNAyxT
11
 ,   , φ           (4) 
where ( )yxN  ,  is the set of 4-neighbors, 
( ) ( ) ( )  , ,1 ,1 , , ,1 yxyxyx +−−  ( ),1 , +yx  of the pixel 
( ). , yx          For ( ) , , Tyx ∈  we have that 
( )yxN  ,  meets just one of the iA  and define 
( ) { }  , 2, ,1  , nyx …∈γ  to be that index such that 
( ) ( ) . ,  , φγ ≠∩ yxAyxN  
● Step 3: The relative Euclidean distance 
( )iAyxd  , ,  between the pixel ( )yx  ,  and its adjacent 
labeled region is calculate as [10] 
     
( ) ( ) ( ) ( )( ) ( ) ( )yxCyxCyxY
CyxCCyxCYyxY
Ayxd
rb
irribbi
i
 , , , 
) ,() ,() ,( 
 , ,
222
222
++
−+−+−=              
(5)  
where ( ), , yxi γ∈  and )C , ,( r iibi CY  are the mean 
values of iY , ibC , and irC  components in that 
region .iA  Then a sorted list T  records neighbors 
of all regions which satisfy Eq. (4) in a decreasing 
order of distances obtained from Eq. (5). 
● Step 4: While T  is not empty, remove the 
first point ( )11  , yx  with the minimum distance value 
and check its 4-neighbors. If all labeled neighbors 
of ( )11  , yx  have the same label, set ( )11  , yx  to this 
label. If the labeled neighbors of ( )11  , yx  have two 
or more labels, calculate the distances between 
( )11  , yx  and all labeled neighbors and classify 
( )11  , yx  to the nearest region.  
                                                                                      8 
 
                                        
 
(a) (b) 
 
(c) (d) 
 
Fig. 4.  (a) Original color image, (b) the result by 
JSEG algorithm [11], (c) the result by Shih [10], 
and (d) the result by our algorithm. 
 
IV. Experimental Results 
  
In our experiment, we test our system on images 
with some static subjects ahead the camera. Human 
with different depths are extracted firstly by 
combining the face detection method and semantic 
human body model, then the depths are estimated. 
 
A. Human Extraction 
The human body is determined by analyzing 
semantic human bodies. We utilize a skin-color 
map and the elliptical shape to detect the human 
face and to locate the human position. Then the 
human body is extracted. In Fig 5(a), the white line 
represents the human face location estimated, the 
green rectangle confines the body ranges to be 
searched, and the purple rectangle confines the 
ranges of body region candidates.  
A simplified graph of human body ratio is shown 
in Fig 5(b), and the rough body ranges are confined 
as below. The ellipse’s minor radius xS  of detected 
face and the width of human body are defined to be 
of ratio 1:9, experimentally. The ellipse’s major 
radius yS  of detected face and the height of human 
body are defined to be of ratio 1:18.  
Firstly, our proposed seed region growing 
method is utilized to segment the image. To avoid 
the over-segmented problem, we merge the regions 
with high  similarity  of  hue and  intensity 
values.    It is to be noted that two  similar  
regions are  merged only when they are both 
inside, or both outside the defined body ranges. 
When the merge procedure is completed, all pixels 
in the same segmented region are labeled with the 
same color. 
 
 
(a) (b) 
 
Fig. 5  (a) Human face, body ranges, and the 
ranges of body region candidates are represented as 
a blue ellipse, a green and purple rectangle, and (b) 
the human body ratio. 
 
Not all regions within the defined body ranges 
are the body regions. If the x-coordinate value of a 
pixel is within the range [ ]xx SxSx ⋅+⋅− 2 ,2 00  and its 
y-coordinate value is within the range 
]19 ,[ 00 yy SySy ⋅++ , where ( )00  , yx  is the face 
ellipse’s center, the pixel’s  label is marked as the 
body region candidate. For all body region 
                                                                                      10 
 
                                        
experimental results to show the effectiveness of 
our proposed algorithms on detecting the human 
objects and estimating the depths. Using Panasonic 
TZ-2, we will illustrate the performance of all the 
techniques presented above. Two scene images are 
analyzed below. 
Fig. 6 shows an example of human extraction 
and depth estimation. Fig. 6(a) is a color image. Fig. 
6(b) shows the extracted edges by performing 
Canny edge detector. Figs. 6(c) and 6(d) show the 
skin-color extraction and ellipse fitting, and human 
location is determined. Figs. 6(e)–(g) are the seeded 
region growing result. The extracted human is 
shown in Fig. 6(h). Fig. 6(i) show the ground-truth 
depth map. The extracted human depth map are 
very similar to  Fig. 6(i), as shown in Fig. 6(j). On 
the other hand, the human depths can be estimated 
using the look-up table according to the vertical 
y-coordinate as shown in Table III, and the 
accuracy, which is very good, of the depth 
estimated is shown in Table IV. On testing other 
images, similar good  results are obtained [17]. 
 
 
Table III. The Depth Estimation Based on the 
Look-Up Table 
 
Detected vertical 
y-coordinate value of 
human position in the 
image 
The 
estimated 
depths 
The acual 
depths in 
the world 
964 (person 1) 2.20M 2M 
824 (person 2) 2.98M 3M 
728 (person 3) 4.13M 4.5M 
 
 
 
 
Table IV. The Accuracy of the Depth Estimation 
 
 
Person 
1 
Person 
2 
Person 
3 
Average
Depth 
estimation 
accuracy (%) 
90.0% 99.3% 91.8% 93.7% 
 
  
 
(a) (b) 
(c) (d) 
(e) (f) 
                                                                                      12 
 
                                        
subject extraction. In our approach, the initial 
seeds are chosen automatically if a pixel is 
inside the edge region. Then the initial seed 
regions are generated and labeled, and the 
regions start growing from the unclassified 
pixel located along region boundary with the 
minimum color distance to one of its neighbors. 
After all pixels are classified and labeled, the 
region merging procedure is performed if any 
two neighboring regions have high similarity. 
Application of our proposed image 
segmentation algorithm to human extraction 
and depth estimation is discussed. Firstly, the 
skin-color detection and elliptical template 
matching are utilized to extract the human face. 
The human body is determined by analyzing 
semantic human body rules. Then the human is 
extracted by combining the detected human 
face and human body. At last, the relative depth 
is described as the depth gradient map and the 
absolute depth can be estimated based on either 
the look-up table of a camera. Experiment 
results have shown that our approach can 
simplify the automatic seed generation 
procedure, reduce the computation burden, and 
obtain good results on human extraction and 
depth estimation. 
 
Acknowledgments 
This research was supported in part by the 
National Science Council under grants NSC 
97-2221-E-009 -140, Taiwan, R.O.C. 
 
References 
[1] E. Hjelmas and B. K. Low, “Face detection: A 
survey,” Computer Vision and  Image 
Understanding, vol. 83, no. 3, pp. 236–274, 2001. 
[2] C. Y. Tang, Z. Chen, and Y. P. Hung, “Automatic 
detection and tracking of Human Heads using an 
active stereo vision system,” International Journal 
of Pattern Recognition and Artificial Intelligence, 
vol. 14, no.2, pp. 137–166, 2000.  
[3] Y. Dai and Y. Nakano, “Face-texture model based 
on SGLD and its application in face detection in a 
color scene,” Pattern Recognition, vol. 29, no. 6, pp. 
1007–1017, 1996. 
[4] C. Garcia and G. Tziritas, “Face detection using 
quantized skin color regions  merging and wavelet 
packet analysis,” IEEE Trans. Multimedia, vol. 1, no. 
3, pp. 264–277, September 1999. 
[5] D. Chai and K. N. Ngan, “Face segmentation using 
skin-color map in videophone applications,” IEEE 
Trans. Circuits and Systems for Video Technology, 
vol. 9, no. 4, pp. 551–564, June 1999. 
[6] M. C. Chi, J. A. Jhu, and M. J. Chen, “H.263+ 
region-of-interest video coding with efficient 
skin-color extraction,”  in Proc. IEEE Conference 
on Consumer Electronics, pp. 381－382, 2006. 
[7] H. D. Cheng, X. H. Jiang, Y. Sun, and J. L. Wang, 
“Color image segmentation: Advances and 
prospects,” Pattern Recognition, vol. 34, no. 12, pp. 
2259–2281, December 2001. 
[8] R. Adams and L. Bischof, “Seeded region growing,” 
IEEE Trans. Pattern Anal. Machine Intell., vol. 16, 
no. 6, pp. 641–647, June 1994. 
[9] J. Fan, D. K. Y. Yau, A. K. Elmagarmid, and W. G. 
Aref, “Automatic image segmentation by integrating 
color-edge extraction and seeded region growing,” 
IEEE Trans. Image Processing, vol.10, no.10, 
pp.1454–1466, October 2001. 
[10] F. Y. Shih and S. Cheng, “Automatic seeded region 
growing for color image segmentation,” Image and 
Vision Computing, vol. 23, pp. 877–886, 2005.  
[11] Y. Deng and B. S. Manjunath, “Unsupervised 
segmentation of color-texture regions in images and 
                                                                                            1 
 
國立交通大學補助教師 
 
出席國際學術會議報告 
 
 
 
 
研究計畫所屬類別：自然■工程生物人文科教 
 
報告人服務機構單位： 國立交通大學電機工程學系 
 
報告人姓名： 張志永教授 
 
會議期間：  2009 年 7 月 8 日至 7 月 10 日 
 
會議地點： 俄羅斯 聖彼得堡市 
 
會議名稱： 2009 年 IEEE 系統與控制聯合會議 (2009 IEEE 
Multi-conference on Systems and Control) 
 
發表論文：“模糊法則推論之人類動作辨識方法＂張志永、徐家杰合
作 
 
“Fuzzy Rule Inference Based Human Activity Recognition” 
by J.Y. Chang and Jia-Jie Shyu  
 
 
電話：（公）(03) 5712121 (ext) 54336 
（宅）(03) 5722472 
 
 
報告日期： 中華民國九十八年七月三十一日 
                                                                                            3 
(5) Prof. Eyad H. Abed (University of Maryland, College Park, USA), 
講題：Monitoring the Vulnerability of Complex Uncertain Systems)  
(6) Prof. Lino Guzzella (ETH Zurich, Switzerland), 
講題：The Automobile of the Future–Options for Efficient Individual Mobility； 
Workshop 討論會演講於 7 月 7 日舉行，包括：  
(1) Organizer: Alexey Ushakov, GM Chief Scientist Russia & Eastern Europe; 
Panelist: Hans-Georg Frischkorn, Man-Feng Chang, Jeffrey Glover, Alexey 
Ushakov,  
講題：Automotive Industry and Control: What is After Crisis? 
(2)  Dr. Farshad Khorrami, United States 
講題： Autonomous Unmanned Vehicles: Collaborative Planning, Obstacle 
Avoidance, and Control； 
(3) Organizer: Alexey S. Matveev (Russia),  
Panelist: Prof. B. R. Andrievsky, N. E. Barabanov, 等人 
講題：Frequency and matrix inequalities in nonlinear control system design: 
      theory and applications. (A tribute to Vladimir A. Yakubovich.) 
等共 3 場次，每場次三小時，相當廣泛豐富。 
與會期間，本人除參與研究領域與我相近論文發表場次外，每天均參加大會
演講，以借鏡大師風範，吸收研發最新成果與方向。我的論文，“模糊法則推論
之人類動作辨識方法＂我及學生徐家合作 (“Fuzzy Rule Inference Based Human 
Activity Recognition” by J.Y. Chang and Jia-Jie Shy) 發表場次，於 7 月 8 日下午智
慧控制場次張貼發表，論文重點如下：首先，每一張影像的前景人物利用一個基
於前後影像比值而建立之統計背景模型抽取出來，並將抽取出來影像轉換成二值
化的影像格式；此方法可以減少照明對前景人物抽取的影響。為達到較精準與可
分別度，二值化影像經由特徵空間及標準空間轉換，投影至標準空間。最後人類
動作的識別在標準空間中完成。經由樣板比對的方法可將三張影像序列，此影像
