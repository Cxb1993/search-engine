行政院國家科學委員會專題研究計畫報告 
導覽系統之點對點多媒體串流設計 
計劃編號: NSC 96-2221-E-036-037 
執行期限：96 年 08 月 01 日 至 97 年 07 月 31 日 
主持人：蔡佳勝 助理教授 大同大學資訊工程系(所) 
計劃參與人員：林永誠、黃怡蓉、陳郁婷 
            大同大學資訊工程系(所) 
 
 
摘要 
以往到博物館或是展覽館參訪的時候，解說人員都會發一些有關展覽品的紙
面資訊，帶回家後那些資訊通常都是丟進垃圾桶，等到需要時就再也找不到它，
再者展覽館通常都備有PDA提供參觀者租借，或是參觀者自行攜帶PDA，租借的PDA
折舊率高；參觀者自行攜帶PDA也很不方便，所以我們打算轉換成另一種模式，
在每個展覽品底下放置一台PDA，PDA中的資訊就代表著此展覽品資訊，參觀者只
需攜帶平時常用的悠遊卡(RFID卡)，就可以暢遊展覽館，手中也無需額外的負擔
(拿展覽品資訊或PDA)，只要刷一下卡片，相關資訊就會寄到參觀者信箱中，等
到需要使用時，就可以列印出參考。優點：(1).使用者只需要具備RFIF卡片(悠
遊卡)，透過我們所準備的PDA上面之RFID Reader，就可以開啟介面使用。 
(2).記錄每個使用者專屬的 RFIF 卡片(悠遊卡)之編號，並重新配置依個新 ID，
使管理者可用新的使用者 ID 做管理及分類。(3).除了寄發展覽品相關消息給使
用者以外，也提供使用者直接在 PDA 上瀏覽展覽品規格等資訊。此外，我們的方
法是根據 Peer 上傳的速度來決定選擇跟它下載哪個片斷，而 Peer 速度是指先
前此 Peer 對自己的平均速度，會想使用先前的平均速度來決定有幾點理由：
(1)Peer 設定的上傳速度不一定是真正最後的傳輸速度、(2)假設 Peer 上傳速
度不會變化太大、(3)根據自己測的速度是最可信的、(4)任何 P2P 系統都不需
要修改任何協定就可以知道的資訊。VBR 的影片適合於事先將影片依時間切割成
數個檔案，而 CBR 事先切不切割都可以，事先切割的話，只要知道切割後每個
檔案的播放時間與檔案大小，就可以使用我們的片斷選擇法來選擇要下載的檔
案，因為對於速度慢的 Peer，我們會去計算適合下載的片斷，這時需要這些資
訊，沒有事先切割的影音檔案，則必須另外想辦法知道整個影音檔案的每個時間
點的 Bit Rate，另外預估目前已下載的部分可以連續播放多長時間與還需要多
久再開始播放可以完整連續的播放，這也都是必要的資訊。 
 
一、系統架構： 
  
 
選擇工具列 
 
建立屬性 
三、GUI介面： 
 
瀏覽 
 
 
使用者介面 
 
z PeerCast 
PeerCast[25]是由 Hrishikesh Deshpande 和 Mayank Bawa 兩人所實做出來的 P2P
影音視訊串流系統，原始程式可由網路下載。PeerCast 使用 RTSP/RTP/RTCP 等
開放標準（open standard）網路協定（network protocol），同時 client 端的影音
播放器（media player）及 server 端的串流伺服器（streaming server）也分別使用
QTP（Quick-Time Player）及 Darwin Server 等常用的多媒體串流工具軟體。
PeerCast 本身有支援基本『Peer Join』及『Peer Leave』功能。對於一個新的 peer 
節點加入時，PeerCast 以 RANDOM 的方式去幫助此新的 peer 選擇一個 server 
peer。 
 
z Narada 
Narada[26]是一個 P2P 的系統，可以自我組織，也可以自我蒐集資料選擇對自己
最適當的 peer 來加入，建立一個 spanning trees 的 mesh，能夠應付現今網路環
境的動態性、不預期性和差異性。在 Narada 中，當有一個 peer P 要加入（join）
Narada 時，Narada 假設 P 可以得到目前在 Narada 內的某些團體成員（group 
members）資訊。（P 可以不必完整的得到目前在 Narada 的所有團體成員資訊，
但至少要知道一個還存活在 Narada 內團體成員的資訊。）P 得到這些資訊後，
P 會從其已知的團體成員中 random 選擇一些成員，對這些成員依序發出要成為
他們鄰居的訊息，直到這些成員中的某一個成員（假設其為 M1）回傳加入許可
的訊息。這樣 P 就會成為 M1 的鄰居而且順利加入這個團體。P 加入完成後會
與 M1 開始交換『更新訊息』，『更新訊息』是用來讓新加入的成員和這團體
中其他的成員能夠快速認識彼此。 
 
z Nice 
Nice[27]同於 Narada 而是使用階層式架構來管理他的 peer。整個架構類似一個
樹狀，階層的最上方就是 root，由 root 供檔給下一層的 peers，由下一層的 peers 
在供檔給更下一層的 peers，以此類推。 
 
z GnuStream 
GnuStream[28]是以 Gnutella[29]為基礎所實作出 P2P Media Streaming 系統。
GnuStream 的架構分為三層，分別為 Network Abstract Layer、Streaming Control 
Layer 及 Media Player layer。Network Abstract Layer 使用 Gnutella 架構，Streaming 
Control Layer 用來與 Network Abstract Layer 和 Media Player Player 做聯繫的管
道，Media Player Player 用來接收視訊和播放視訊之用。在 GnuStream 中，如果
peer P 要收看視訊，首先，他得連線到 Gnutella network 中。由於 Gnutella 是 P2P
系統，沒有 server 的，所以 P 要先跟他已知道的一些 peer 節點（Gnutella peers）
做連線。 
 
在此上所討論的五個系統中（PeerCast, Narada, Nice, ZigZag, and GnuStream），
上不考慮去更改這部分，因為標準 BT 的 Tracker 給詢問者隨機的 Peer 清單，
已經擁有負載平衡的功能，不會使得所有的 Peer 都只去跟某幾個 Peer 請求下
載資料。 
利用 BT 技術建構隨選視訊傳輸系統裡，影片發佈者將要發佈的檔案做成 
torrent 檔前，會考慮到是否事先按照時間將檔案切成數個小檔案，這樣下載時
可以從前面的檔案抓起，主要的考量是目前的播放器幾乎都是要檔案完整抓完才
可以開始播，而且支援檔案未下載完整即可播放的軟體所支援的影音格式不多，
所以假設將影片事先切割成數個小檔案，這樣就可以將已完成的檔案用支援該影
片格式的播放器軟體來播放。事先切割的缺點則是，因為要使得切割出來的檔案
都能單獨播放，所以每個切割出來的檔案都會依照格式在檔頭存有影片的資訊，
所以切出來的檔案總大小會比原來的檔案大小來的大，而且這種額外付出 
(overhead)，如果依照越短的檔案間隔去切，overhead 就會越大。 
 
片斷選擇法 
我們在此針對 P2P 環境提出一個動態的選擇片斷方法，因此不只能用在 BT 
上，還可以使用在大多數 P2P 系統上，由於此方法是動態的選擇，所以 Peer 忽
然離開或加入新的 Peer 都不需要什麼特殊處理。以下是我們的動態片斷選擇法
的分析與說明。 
 
基本原理是根據 Peer 上傳的速度來決定跟它下載哪個片斷，而 Peer 速度是指
先前此 Peer 的平均傳輸速度，會用先前的平均速度來決定有幾點理由：(1)Peer 
設定的上傳速度不一定是真正最後的傳輸速度；(2)Peer 上傳速度是會改變的；
(3)根據自己測的速度是最可信的；(4)任何 P2P 系統都不需要修改任何協定。所
以接下來問題就可以分成兩類，第一個是向某個 Peer 發出第一次下載請求時，
要如何選擇片斷？第二個是向已知先前速度的 Peer 請求時，如何選擇片斷？ 
 
假設已知速度情況下，最簡單的方法都是照順序要，即一開始就同時跟 Peer 清
單中的所有 Peer 要片斷，跟第一個 Peer 要第一個片斷，跟剩下的 Peer 照片斷
順序一直要下去，若是有任何 Peer 傳完前次分配的片斷或者有新的 Peer，我們
就照剩下片斷的順序繼續要下去，直到傳完所有 Peer 為止，但是由於每個來源 
Peer 的上傳速度的差異，所以會造成片斷下載完成的順序不是照順序的，假如
順序較前面的片斷剛好是跟一個比較慢的 Peer 要求下載的話，就可能會影響到
從頭連續播放的片斷即時到達率，即會發生跳格的情形。雖然有一種方法避免這
種情形，就是設定限定期限，假設限定期限前，原本分配出去的某個片斷無法下
載完成，就丟棄這個未完成的片斷，然後重新跟別的 Peer 要求，但是如果常常
遇到這種情況，會造成浪費的下載動作。 
而我們的方法是，當向某 Peer 發出第一次下載請求時，若是沒有辦法得知
此 Peer 的上傳速度，就要先經過實測速度，所以我們採取先分配最後面的片斷
給它，不管速度快慢影響都比較小，當抓完一個子片斷，就有速度的數據來當做
 myEvalvid_Sink：myEvalvid_Sink 負責的工作是接收由 my_UDP 所傳送出
的封包，並且紀錄接收的時間、封包的識別和封包負載大小，紀錄在 TCL 
script 所設定的檔案中。 
 
 
 
由上面兩個圖分析出來的結果可以得到下列的統計數據：在此次的網路模擬傳輸
中，總共傳送了 163682 個封包，其中包含了 28770 個 I-frame 封包、45339 個封
包及89573個B-frame封包，而遺失的封包總共1595個，其中包含了281個 I-frame
的封包、433 個 P-frame 的封包及 881 個 B-frame 個封包。此外也可以獲得以下
的相關 frame 的統計數據：總共傳輸了 89998 個畫面，其中包含了 7500 個
I-frame、22500 個 P-frame 及 59998 個 B-frame，而遺失的畫面總共 1578 個，其
中包含了 275 個 I-frame、424 個 P-frame 及 879 個 B-frame，因此可以得到可解
畫面比例為 0.913543 的大小。並且我們也可以經由以上的結果，得到其他的相
關資訊，如 total directly decodable frame 這個值，它代表的意義為這個畫面所分
割出來的封包全被接收端所接收到的畫面數，也就是說，當一個 frame 被切割成
三個封包時，在這三個封包都被正確接收到的時候，我們稱此 frame 為 directly 
decodable frame，此值所代表的就是所有到達這個狀態的 frame 數。有了上述的
資訊，我們就可以計算在一個 GOP 中，每個畫面可以解的機率，因此計算出來
的整部電影可被解出來的畫面數與我們所模擬的結果會呈現非常的接近。另外，
除了獲得可畫面比例之外，我們也可以經繪圖(Gnuplot)的方式得知各項的圖表數
據。 
 
五、結果與討論 
本計劃以手持裝置為平台的導覽系統，就必須在無線的環境下，以往支援無
線的導覽系統幾乎在室內，然而，假使在戶外的話，無線所必須涵蓋的範圍必須
非常大，所支援的頻寬也是有限的。所以如果能利用 P2P 的技術，就能夠增加傳
[8] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan, “Chord: A 
scalable peer-to-peer lookup service for internet applications,” in Proceedings of 
ACM SIGCOMM, San Diego, pp. 160–177, August 2001. 
[9] F. Dabek, E. Brunskill, M. Frans Kaashoek, D. Karger, R. Morris, I. Stoica, and H. 
Balakrishnan., “Building Peer-to-Peer Systems With Chord, a Distributed Lookup 
Service,” 8th Workshop on Hot Topics in Operating Systems, Germany, May 
2001. 
[10] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker, “A scalable 
content-addressable network,” in Proceedings of ACM SIGCOMM, San Diego,  
pp. 149–160, August 2001. 
[11] B. Y. Zhao, J. Kubiatowicz and A. Joseph, “Tapestry: An Infrastructure for 
Fault-tolerant Wide-area Location and Routing,” UCB Tech. Report 
UCB/CSD-01-1141. 
[12] B. Y. Zhao, Y. Duan, L. Huang, A. D. Joseph and J. D. Kubiatowicz, “Brocade: 
landmark routing on overlay networks,” First International Workshop on 
Peer-to-Peer Systems (IPTPS) Cambridge, MA, March 2002. 
[13] B. Y. Zhao, A. D. Joseph, and J. D. Kubiatowicz, “Locality-aware Mechanisms 
for Large-scale Networks, ” Workshop on Future Directions in Distributed 
Computing Bertinoro, Italy, June 2002. 
[14] P. Ganesan, Q. Sun, and H. Garcia-Molina, “YAPPERS: A Peer-to-Peer Lookup 
Service Over Arbitrary Topology,” Infocom, 2003. 
[15] B. Yang and H. Garcia-Molina, “Improving Search in Peer-to-Peer Systems,” 
ICDCS, 2002. 
[16] P. Druschel and A. Rowstron, "PAST: A large-scale, persistent peer-to-peer 
storage utility," HotOS VIII, Schoss Elmau, Germany, May 2001. 
[17] A. Rowstron and P. Druschel, "Pastry: Scalable, distributed object location and 
routing for large-scale peer-to-peer systems, " IFIP/ACM International Conference 
on Distributed Systems Platforms , Heidelberg, Germany, pages 329-350, 
November 2001. 
[18] D. Karger, E. Lehman, F. Leighton, M. Levine, D. Lewin, and R. panigrahy, 
“Consistent hashing and random trees: Distributed caching protocols for relieving 
hot spots on the World Wide Web,” in the proceedings of ACM symposium on 
Theory of Computing, 1997. 
[19] D. lewin, “Consistent hashing and random trees: Algorithms for caching in 
distributed networks,” Master’s thesis, Department of EECS, MIT, 1998. 
Available at the MIT Library, http://thesis,mit.edu/ 
[20] A. Rowstron, P. Druschel, “Pastry: Scalable, distributed object location and 
routing for large-scale peer-to-peer systems,” in IFIP/ACM International 
Conference on Distributed System Platform, 2001. 
