 
文獻探討 
 
Aerial images and ground-level images are two major types of image sources used by many existing 
image-based urban 3D modeling approaches. City models can be constructed solely from aerial images 
if the heights of the buildings can be obtained from airborne laser scanners or calculated from stereo 
image views [4][5][9][13]. However, the resultant city model usually lacks a realistic impression at 
ground level since the aerial image can only provide very limited texture information for buildings’ side 
views. Hence, there have been a number of approaches to automated texture mapping of 3D models 
using the available ground-level images [3][6][8][12]. The automated pose recovery algorithm for 
multiview images was considered time-consuming and the textures generated from different views 
usually causes a visible seam due to lighting condition and image resolution variations. Wang et al. 
[14][15] proposed to use cylindrical panoramic images for texture mapping propose. In their method, a 
rough location of each panoramic image was assumed provided and the registration between the 
panoramic image and aerial image was done through a voting process. Since only sparse panoramic 
images were used, some building might be lack of texture or there exist some visible seams while 
textures overlapped. 
 
    從 2001 年起，C. Fruh 和 A. Zakhor 便陸續發表城市模型建模系列的文章 [1][2][3]。他們
的方法採用多種儀器設備並融合多種資訊，其 inputs 包括 aerial images、aerial range data、
ground-leveled images 以及 ground-leveled range data，拍攝地面影像的車子上裝有 two laser 
scanners 和 one digital camera。Camera pose estimation was done by matching successive horizontal 
scans with each other in order to determine an estimate of the vehicle’s motion. The edge map has been 
generated from aerial images or DSM data. In order to estimate camera’s initial location, it is to assume 
that features such as buildings are visible from both ground-based and airborne view. Global correction 
of camera’s location was performed by applying Monte-Carlo localization technique. First, point cloud 
was produced from range data, then, they were triangulated into meshes. Finally, their program 
calculated texture for each mesh from the available ground-leveled images and performed texture 
mapping to create the 3D model of the city. 此方法的結果呈現於 Fig. 1。 
 
 
Figure 1：C. Fruh 和 A. Zakhor 方法之建模結果。 
 
    L. Wang、S. You 和 U. Neumann 所發表城市模型建模的文章[14][15]中所提到的方法方法融
合空拍與地面環場影像與本計畫之目標相似，模型建構的流程中有多處需要人工輔助的部分，其
地面環場影像是由 16 張平面影像組合而成，the relative rotation of those 16 planar images are 
known，但並非真正接合成 360 度環場影像。在此方法中 buildings’ outlines in the aerial image are 
specified by users. Moreover, a possible camera location has been also defined by user. All the straight 
points (outliers) were removed by performing RANSAC algorithm based on the epipolar geometry 
constraint. Camera poses represented by the rotation matrix and the translation vector were recovered 
from the essential matrix by assuming the forward motion. The scale of the translation was estimated by 
a linear closed form 1-point algorithm on corresponding 3D points by triangulation. The point cloud of 
matches between two adjacent panoramic images was transformed into depth map by multi-view 
superpixel stereo method. Finally, the depth maps were fused by an algorithm, which combines 
advantages of volumetric and viewpoint based fusion methods. The texture of each mesh was obtained 
from the panoramic images. 此方法的結果呈現於 Fig. 4。 
  
Figure 4：B. Micusik 和 J. Kosecka 方法之建模結果。 
 
 
研究方法 
 
Reconstruction Framework 
     
The framework takes two types of image sources as input. One is a dense set of ground-leveled 
panoramic images and the other is a nearly-orthogonal aerial image of that area. The 360x360-degree 
panoramic images were captured by Point Grey Ladybug3 mounted on the top of a car, which captured 
five “regular” planner images looking horizontally outwards and one looking upward, each with frame 
rate of 15 images per second. Six images are then stitched together using the multi-perspective plane 
sweep approach of [7]. This allows producing a set of spherical panoramic images of resolution 
2048x1024. The camera and an example of the captured panoramic image are shown in Fig. 5. The 
aerial image of the desired area can be obtained from Google Map by stitching together different image 
portions, each shows the largest zoom in view, of that area. 
 
Figure 5: Left: Point Grey Ladybug3 panoramic camera. Right: an example of the captured spherical 
panoramic image. 
 
    Some reasonable assumptions about the input data of our approach is described as follows: The 
Camera Trajectory Recovery 
 
The first half of the framework deals with the camera trajectory recovery task, which can be considered 
as a preparation step so that the registration of two types of input images can take place. Currently, 
registering the recovered camera trajectory to the aerial image is done manually by specifying two end 
point locations of the path on the map. Consequently, an image’s coordinates representing a position on 
the aerial image associated with each source panoramic image could be derived.  
 
    In order to recover the relative image capturing positions and orientations of the source panoramas, 
we first estimated the spatial relationship between each adjacent pair of panoramic images, and then 
integrated those pairwise results. The spatial relationship in terms of a rotation matrix and a translation 
vector, referred to as the external camera parameters, can be derived from the essential matrix describing 
the epipolar constraint between the image correspondences in two panoramas. 
 
    The image point correspondences can be established by Scale-invariant Feature Transform (SIFT) 
detection plus SIFT-based matching. A single threshold DSIFT was used to determine if a match was 
acceptable in the SIFT-based matching algorithm. The smaller the value, the more image 
correspondences were identified, and the higher possibility that the result would include false matches. 
The eight-point algorithm was employed to estimate the essential matrix. A two-pass approach was 
proposed to obtain the final essential matrix. First, an initial essential matrix was derived according to a 
smaller set of image corresponding points, which was the matching result associated to a relatively large 
threshold value DSIFT. Those sparse corresponding points were believed to be more accurate but less 
descriptive. Next, a smaller threshold value was assigned to obtain a larger set of point matches. The 
initial essential matrix was then used to serve as a constraint to filter out the incorrect matches. In other 
words, the matching outliers were filtered by epipolar constraint. The remaining point matches were 
then used to compute the final essential matrix. 
 
    The derived essential matrix was used to solve for the external camera parameters R and T, which 
stand for the rotation matrix and the translation vector, respectively. Pairwise external camera 
parameters were first determined and then integrated one by one to obtain the global camera motion and 
thus the camera’s moving trajectory. During the integration process, the scene points based on the 
already processed panoramic image were reconstructed with respect to the 3D world coordinate system, 
which are used as the references to estimate the next camera location. An example illustrating an 
intermediate result of the reconstructed scene point cloud is given in Fig. 7. The major drawback of such 
method is that the camera parameter estimation error would propagate through the integration process. 
One way to correct such drift is by a path closing strategy, which however does not always work well. 
Moreover, identifying two or more panoramic images captured at the same street intersection but at 
different locations and times is a very difficult problem. In order to deliver an efficient and relatively 
more accurate solution to this problem, we have chosen to incorporate GPS information. Since the 
accuracy of GPS system varies from 1 to 5 meters, it is sensible to correct the trajectory drift every 50 
meters based on the GPS reading. The final camera’s moving trajectory was determined by a series of 
bundle adjustments on the recovered camera locations. 
further reduce the searching space by defining a vertical range [vt, vb], where vt < vb and vt, vb are in {1, 
2, …, H}. These two boundary values can be obtained by the elevation of the camera location, denoted 
as h, the shortest distance between the camera and the building, denoted as d, and the maximum possible 
height of the building, denoted as b. We have 
 
The image region bounded by top-left corner (ul, vt) and bottom-right corner (ur, vb) is denoted by IR. We 
applied Canny edge detection to region IR and back-project the resultant binary image to a planar surface, 
denoted as IP. Then, Hough transform was employed to detect straight lines. We have posed constraints 
that the length of the line must be greater than half of the width of region IR and the angle between the 
straight line and a horizontal axis should be less than 45. The set of detected straight lines potentially 
contains the desired building rooftop edge. Let S denote the number of resultant straight lines. 
 
    The building footprint boundary on the aerial image, the one facing the camera, was projected to 
the panoramic image by various b values within reasonable building height range B, and as well as at the 
same time back-projected onto IP. A similarity test was then performed to calculate the number of 
overlapped pixels between each of the back-projected building footprint boundaries and the set of 
detected straight lines. The desired building rooftop edge Im, can be estimated by the followings: 
 
where In indicates the detected straight line indexed as n, fb indicates the back-projected building 
footprint boundary with height value equals to b, and function Row returns the average image row of fb 
in IP. The obtained value of r indicates the height of the building. The building elevation view image 
was generated by first back-project the color panoramic image region IR to a planar surface, denoted as 
IQ. The resolution of color image IQ is identical to the resolution of binary image IP. In general, the 
resultant texture image IQ is not in rectangular shape. Rectangular image textures of building elevation 
views are preferred for texture mapping task. Therefore, image IQ needed to be rectified by the 
perspective transformation provided in OpenCV before it was used for texture mapping. 
 
Figure 8: The aerial view of the interested building to be reconstructed. The red line segment indicates 
the building footprint boundary and the red dot on the street indicates the camera location where Fig. 
5(right) was captured. 
been performed to obtain a rectangular building texture illustrated on the right of Fig. 10. Another 
building texture example is shown in Fig. 11 and the corresponding result is shown in Fig. 9(left). 
 
Conclusion 
 
A street 3D modeling framework was proposed, which takes two types of images as input sources, 
namely a set of spherical panoramic images captured along a continuous path and an orthogonal aerial 
image of that area. The relative orientations and positions of all the panoramic images can be recovered 
through a fully automatic process with the help of sparse GPS data. The footprints of major buildings to 
be reconstructed in the aerial image are assumed given. The developed program is able to automatically 
estimate the height information and generate a rectangular front view texture image of each building for 
large-scaled 3D city modeling. 
 
Figure 10: Building texture generation example. (Top) the captured panoramic image. The red rectangle 
encloses the region that contains the desired building texture. (Left) the Canny edge detection result of 
the building region after performing the back-projected. The image has been faded to emphasize the 
detected straight lines. (Middle) the generated building texture, and (right) the result after performing the 
perspective transformation. 
 
 
Figure 11: Another texture rectification example. (a) is the generated building texture, and (b) is the 
result after performing the perspective transformation. 
 
計畫成果自評 
 
本研究計畫相關 近年發表或指導之論文 
 
《Journal》    F. Huang, K.-K. Fehrs, G. Hartmann, and R. Klette. "Wide-angle Vision for Road Views", to 
appear in Opto-Electronics Review, vol. 21, no. 1 (2012).  [SCI] 
《Journal》   F. Huang, A. Torii, and R. Klette. "Geometries of Panoramic Images and 3D Vision", 
Machine GRAPHICS & VISION, Vol. 19, No. 4 (2010), pp. 463~477.  [EI] 
《Journal》   K. Scheibe, F. Huang and R. Klette. "Pose Estimation of Rotating Sensors in the Context of 
Accurate 3D Scene Modeling", Journal of Universal Computer Science, Vol. 16, No. 10 
(2010), pp. 1269~1290.  [SCI] 
《Conference》F. Huang, Y.-J. Wu, J.-S. Hsu, and A. Tsai. "3D Modeling of Street Buildings from 
Panoramic Video Sequences and Google Map Image", in Proc. VISGRAPP’12, Rome, Italy, 
February, 2012, Springer, pp. 109~114.  [EI] 
《Conference》F. Huang, Y.-J. Wu, A. Torii, A. Tsai, and J.-Y. Tsai. "Towards 3D City Modeling through 
Combining Ground Level Panoramic and Orthogonal Aerial Imagery", in Proc. 
DMDCM’11, Hangzhou, China, May, 2011, pp. 66~71.  EI] 
《Conference》F. Huang, R. Klette, J.-C. Tien, and Y.-W. Chang. "Rotating Sensor-Matrix Camera 
Calibration", in Proc. ICIP’10, Hong Kong, September, 2010, pp. 4245~4248. EI] 
《Thesis》    吳依茹. "以地面環場影像和空照影像建構三維城市模型", CSIE, NIU, 2011. 
《Thesis》    許敬祥. "以環場影像為基礎之三維街景建模方法", CSIE, NIU, 2012. 
 
 
 
本研究計畫之貢獻  
 
提出一個方法能夠結合環場影像與空照影像並自動化產生三維城市幾何模組，所開發之核心演算
法在學術上，無論是在電腦視覺或電腦圖學領域都有顯著的貢獻，包括： 
。 Panoramic image-based algorithm for accurate camera poses and trajectory estimation; 
。 Automatic building textures extraction algorithm from panoramic images. 
本研究計畫最大的優點是自動化處理所有需要靠專業判斷的建模程序，只要將空照圖與建築物邊
緣以及有順序的環場影像檔配合部分GPS資訊輸入所開發之程式，任何會使用電腦的人員均可完成
真實且大範圍的三維城市街景建構工作。 
 
    本計畫之相關技術研究有助於地理資訊應用與系統之發展，以更有效率更高品質的方式建置
3D城市資訊，亦可以3D的方式呈現或儲存建築物資訊，除了單純地理資訊儲存，3D城市模型可應
用於各種動畫製作、擴增實境系統以及行車導航系統等，期待對提升台灣科技的競爭力有所貢獻。 
PSIVT 2011 研討會議程表 
 
 
會議主持議程表 
Oral Session III -  Computer Vision Applications 
Nov. 21 (Mon.) 16:40-18:00 
Chair: Fay Huang, National Ilan University, Taiwan 
SLAM and Navigation in Indoor Environments Shang-Yen Lin; Yung-Chang Chen 
Recovering depth map from video with moving objects Hsiao-Wei Chen; Shang-Hong Lai 
Pedestrian Image Segmentation via Shape-Prior 
Constrained Random Walks 
Ke-Chun Li; Hong-Ren Su; Shang-Hong Lai 
Real-time Image Mosaicing using Non-rigid 
Registration 
Rafael Henrique Castanheira de Souza;  
Masatoshi Okutomi; Akihiko Torii 
 
 
   
 2
四、攜回資料名稱及內容 
 
Conference proceedings and program guide are attached in the following pages.  
 
 
 
 
 4



國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/05
國科會補助計畫
計畫名稱: 以環場影像與空照圖為基礎之3D城市建模方法
計畫主持人: 黃于飛
計畫編號: 100-2221-E-197-028- 學門領域: 計算機圖學
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
