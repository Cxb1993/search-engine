as data structure to maintain the signal correlation 
while describing the problem into a SAT instance. 
However, with ever increasing size of circuits, ATPG 
is getting more difficult and thus makes the 
verification and test of next-generation ICs a new 
challenge. 
Multi-core processor and distributed system is a 
pervasive computing resource nowadays. In order to 
fully utilize these resources, a new software 
developing paradigm – parallel and distributed 
computing – rapidly emerges. Considering the high 
complexity of the ATPG problem and IC design scale, 
developing a parallel-and-distributed ATPG algorithm 
is becoming a interesting and practical application. 
Our proposal is to explore the parallelism of the 
state-of-the-art implication-graph ATPG algorithm and 
to develop the program for diverse parallel 
architectures (including multi-core SOC and 
distributed systems). 
The first year of this project is to study the 
parallelism of implication-graph ATPG and implement a 
multi-threaded program onto a multi-core system； 
during the second year, we will modify and port our 
program to massively many-core systems, such as 
CUDA； in the third year, a hybrid distributed system 
is going to be developed to bridge the power of many-
core systems over the network. Last but not least, 
partial results of this proposal will be published as 
a Multiple-Core course module sponsored by Ministry 
of Education and Intel MTL jointly. 
英文關鍵詞： Automatic Test Pattern Generation (ATPG), parallel 
and distributed computing, implication graph,  
massively many-core 
 
 I 
摘 要 
 
自動測試向量產生(Automatic Test Pattern Generation, ATPG)器一直是
積體電路驗證與測試的核心問題，而其本身的問題複雜度本身就是
NP-complete。晚近隨著 ATPG演算法的發展從結構式的路徑搜尋方法到以布林可
滿足性(Satisfiability, SAT)引擎為基礎解決方法，成功地提供了在實務上的
諸多應用。最後，一個結合 SAT解決效能與電路結構資訊的新的 ATPG法被證明
成功且有效率地應用在大型電路設計上：它利用推理圖(implication-graph)的
資料結構保持了電路裡訊號的結構關係，而又把問題描述成 SAT的邏輯關係方便
推導。而隨著電路設計規模快速上升導致 ATPG的問題愈趨困難，下一代晶片驗
證與測試的向量產生方法也面臨新的挑戰。 
多核心處理器與分散式系統平台已是今日非常普及的計算資源，而為了能充
分的利用這些計算資源，新的軟體開發風格—平行與分散式計算—快速的興起。
為了克服 ATPG問題的高複雜度與電路設計規模快速上升，以平行與分散式計算
為基礎的 ATPG演算法開發成為一個有趣又實際的應用。本計劃的重點即在以最
先進的推導圖(implication-graph) ATPG 為基礎，探討其本身演算法上可平行
度的開發與在不同的平行架構(包含多核心單晶片系統，分散式系統等)上的演算
法設計。 
這個計畫的第一年先要探索決定性 implication-graph ATPG的可平行化空
間，針對多核心單晶片系統進行多引線(multi-threaded)程式開發。其後，在第
二年的目標會針對大量多核心(massively many-core)平台如 CUDA 進行程式設
計。第三年的發展目標是希冀結合多單晶片系統而開發分散式計算方法。而本計
劃之部分研究成果也將出版在教育部/Intel MTL 共同開發的多核心技術課程模
組中。 
 
 
關鍵字：自動測試向量產生 ; 平行與分散式計算; 推導圖 ; 大量多核心平台; 
 
 
 
 
 
 
 III 
Contents 
 
List of Figures                 IV 
List of Tables                 IV 
 
Chapter 1 Introduction………….………………………………………………….….1 
 
Chapter 2 Literature Review…………………………………………………………..2 
 2.1 Introduction to ATPG………………………………………………..……….2 
2.2 Previous review of parallel ATPG study…………………………...……3 
 2.3 Objective and importance of the proposal……………………………………4 
  
Chapter 3 ATPG with Parallel Programming Techniques……………………….…….5 
 3.1 Problem Formulation and Expected Difficulties..……………………………5 
 3.2 Implication-graph algorithm for APTG………………………………………6 
  3.2.1 Implementation Implication-graph ATPG………………………..…...7 
   3.2.1.1 Transfer CUT to IG……………………………………………8 
   3.2.1.2 Break fault location…………………………………………..11 
   3.2.1.3 Implication operation……………………...…………………13 
   3.2.1.4 Justification operation…………………………………….….13 
3.2.1.5 Propagation operation………………………………………..15 
 3.3 Test Inflation Aware Parallel ATPG via Fault Ordering……………....…….17 
  3.3.1 Thread………………………………………………………….……18 
   3.3.1.2 Pthreads………………………………………..……………..19 
3.3.2 Flow of Test Inflation Aware Parallel ATPG………………………...21 
 
Chapter 4 Experimental results……………………………………………...……….23 
 
Chapter 5Conclusion………………………………………………………………....25 
 
References……………………………………………………………………………26 
 
 
 
 
 
 
 1 
Chapter 1  
Introduction 
For a long time, the processor design and manufacturing companies have allied 
together to boost processors’ performance by increasing clock frequencies. However, 
along with the technology nodes advances into the nanometer era and the number of 
transistors exceeds the million/billion scales, traditional computer architectures reach 
the bottleneck on frequency and more worse the power consumption grows 
proportionally as the frequency increases. Under such scenario, the leading IC design 
companies start to adopt the parallel strategies of multi-threading and multi-core 
architectures. 
The development of multi-core processors starts by IBM in 2001, and they 
announced the first double-core 64-bit RISC Power4 processor. In 2005, IBM, 
SONY and Toshiba jointly unveiled Cell, a heterogeneous multi-core microprocessor, 
and applied it to PS3 and XBox360 game machines. A Cell processor mainly consists 
of a Power Processor Element (PPE), eight Synergistic Processing Elements (SPEs) 
and a coherent Element Interconnect Bus (EIB). Moreover, the CUDA (Compute 
Unified Device Architecture) [NVI 2007] proposed by Nvidia Corp. 2007 opens the 
personal supercomputing era with the advances of the GPU technology. In 2010, 
their next-generation CUDA, Fermi, features up to 512 CUDA cores and 3.0 billion 
transistors and delivers over a Teraflop per GPU. 
 
 
Figure 1.1: Layers of Parallelisms 
 3 
even some of ATPGs such as LASAR are proposed to excite critical path regardless 
of any fault model. Among fault-dependent algorithms, ATPG for stuck-at faults is 
the most fundamental and have been investigated extensively such as D-algorithm 
and PODEM. However, an implication-graph based approach proposed by 
Tafertshofer in [Taf 2000] combines the power of circuit structure and SAT solving 
and becomes the state-of-the-art. Therefore, in this work, implication-graph ATPG 
will serve as a baseline onto which various parallelization techniques will be 
deployed. 
 
2.2 Previous review of Parallel ATPG Study 
 
 
 (a) shared memory (b) message-passing 
Figure 2.2.1: Communication protocols 
 
Various parallel ATPG architectures and task partitioning techniques were 
proposed on the basis of two different communication protocols: (a) shared-memory 
[Kri 1997, Cai 2010] and (b) message-passing [Agu 1993, But 2000] as shown in 
Figure 2.2.1. A shared-memory system typically executes all related processes (or 
threads) on one machine whereas a message-passing system distributes processes (or 
threads) onto different machines. Many other parallelization techniques including 
search space partitioning, fault partitioning and circuit partitioning are also proposed 
in [Rao 1993, Wol 1996, But 2000]. 
An ATPG algorithm involves three major steps, e.g. test generation, fault 
simulation and test compression. Most of previous works can only parallelize one of 
these three steps. Furthermore, the baseline algorithms to be parallelized are typically 
D-algorithm or PODEM so that the performance improvement due to parallelization 
 5 
environment and develop parallel programs using MPI in the second year. CUDA is 
the lowest-level programming language compared with other pervious parallel 
programming languages, but it provides the most computing power. Therefore, we 
will port the parallelized ATPG to GUGPU in the third year.  
From a different perspective, we’d like to propose/develop the parallel ATPG 
to improve efficiency on multi-core and many-core computing systems, which can 
bring together the great computation power in the design process. Numerical 
simulation of scientific/engineering problems often utilizes parallel computing to 
realize repetitive calculations on large amounts of data to give valid results. Since 
ATPG happens to require a huge amount of computations, parallel computing also 
works for this application.  
 
Chapter 3  
ATPG with Parallel Programming 
Techniques 
3.1  Problem Formulation and Expected Difficulties 
ATPG (automatic test pattern generation) is one of Electronic design 
automation (EDA)  tools which efficiently produces tests to explore the coverage 
corners and helps to identify the weak spots in the circuit. However, nowadays ATPG 
demands more computational effort to obtain optimality due to the increasing design 
complexity. The performance of commercial APTG is inversely proportional to their 
size. Moreover, the technology in commercial ATPG today is fairly mature, and thus 
their performance improvement between subsequent releases largely relies on the 
performance trends of the underlying simulating hardware host. 
A traditional computer consists of a processor for performing the actions 
specified in a program with a memory. One way of increasing the computational 
power is to combine multiple processors into a single computer (multi-processor) or 
alternatively multiple computers, operating together on a signal problem. In both 
cases, the problem is split into different parts, each of which is performed by a 
 7 
we choose the algorithm as the baseline ATPG algorithm for parallelization. 
 
Year Algorithm Speedup over D-ALG 
1966 D-ALG 1 
1981 PODEM 7 
1983 FAN 23 
1987 TOPS 292 
1988 SOCRATES 1574 
1990 Waicukauski et al. 2189 
1991 EST 8765 
1993 TRAN 3005 
1995 Recursive Learning 485 
1997 Tafertshofer et al. 25057 
Table 3.2.1 Comparison of several ATPG algorithms 
 
3.2.1 Implementation Implication-graph ATPG 
This section will introduce the implementation details for implication-graph 
(IG) based ATPG. Figure 3.2.1.1 shows the flowchart of IG ATPG. 
 
Transfer CUT to IG
Break fault location
Activate fault
Propagate fault
Inject fault
Backtrack
Output pattern
Circuit under test
conflict
conflicttrue
true
 
Figure 3.2.1.1 flowchart of IG ATPG 
First, the circuit under test (CUT) will be transformed to corresponding IG. 
 9 
In order to include all structural information within the IG, the set of edges E is 
partitioned into three disjoint subsets. The set of forward edges EF comprises input 
to output implications, whereas the set of backward edges EB models the opposite 
direction, and all other implications are grouped in the set of other edges EO. In the 
IG these sets are denoted by edge tags f , b, and o. Figure 3.2.1.3 shows C17 with its 
structural as well as its IG model. 
 
tructural: 
 
 
G: 
G1 G2 G3 G4 G5 G5* G4* G3* G2* G1*
n9 n6 n10 n10* n6* n9*
n7 n7*
n8 n8*
G16 G17*G17 G16*
 
Figure 3.2.1.3 Structural description and Implication graph for C17 
 11 
 
Figure 3.2.1.4 Reconverge analysis for indirect implication on C17 
 
3.2.1.2  Break fault location 
In traditional IG implementation, we need to generate two IGs for good circuit 
and faulty circuit. In order to avoid the memory leak problem, we combine these two 
circuits into only one IG in our implementation. For combining two IGs into one IG, 
the fault location needs to be broken to avoid the implication error. For example, if 
the fault happens in the n10, G3 and G4 should be set for activating the fault. If we 
don’t break the fault in only one IG, the fault injection, which means n10* will be set, 
will result conflict. In order to avoid this situation, we obeying the following rule to 
break edge: 
RULE-2 (break fault location) Starting from fault node Fc  
 13 
 
3.2.1.3 Implication operation 
Implication graph based implication is simple and efficient, as it only requires a 
partial traversal of the implication graph. Implying from a signal assignment means 
that first the corresponding nodes are marked in the implication graph. Then, the 
implication procedure traverses the implication graph obeying the following rule: 
RULE-3 (direct implication) Starting from an initial set si of marked nodes, all 
successor nodes sj are marked if 
(1) node sj is a∧ -node and all its predecessors are marked. 
(2) node sj represents an encoding bit and at least one predecessor is marked . 
This rule is applied until no further propagation of marks is possible. 
Let us use the circuit of Figure 3.2.1.3 for explanation. Assigning logical value 
0 to signal G3 corresponds to setting node G3* in the IG. After the implication 
procedure, the following nodes are marked: n10*, n7, and n9*. To finally obtain the 
implied signal values with respect to the given logic, the marked nodes are decoded, 
i.e. we determine n10= 0, n7 =1, and n9 = 0. 
 
3.2.1.4 Justification operation 
In the context of ATPG, justification is the task that determines what value 
should be assigned at primary inputs so that an internal node can be forced to 
required value. Since the structural information is etched on edge tags of implication 
graph, the IG-based justification has the advantages of PODEM-based as well as 
SAT-based approach. The unjustified line is substituted by unjustified clause defined 
below: 
DEFINITION-2 (unjustified clause [Taf 2000]) 
A clause 1 2 ... nC c c c= ∨ ∨ ∨  is called unjustified iff all literals 1 2, ,..., nc c c  
do not evaluate to 1 and at least one complement c*i of a literal ci is 1.  
DEFINITION 3 (justification [Taf 2000]) 
Let 1 2, ,..., mc c c  be some unjustified literals in clause 1 2 ... mC c c c= ∨ ∨ ∨  
 15 
RULE-4 (backtracing [Taf 2000])  
Let the objective oi be driven to node iv V∈ , suc ( )S i Sv V⊆  and suc ( )iv V∧ ∧⊆  
denote the succeeding signal and ∧ -nodes in GB, respectively. Then the objective oi 
is driven to the following nodes: 
(1) all signal nodes suc ( )j S iv V∈  
(2) one ∧ -node suc ( )j iv V∧∈  which is selected according to a pre-computed 
controllability. 
Nodes vj, having successor (cx) whose complement node c*x is set, are not 
selected. This rule is applied until there is no possible backtracing of objectives, that 
is, all objectives have reached a primary input. 
As soon as reaching a signal node belonging to a primary input by backtracing, 
it is set and then implication procedure is invoked. If a confliction happens during 
implication, the assignment of the primary input has to be reversed (backtracking) by 
setting its complement node and restarting implication procedure. If a conflict still 
happens after reversion, it indicates that the examined signal node cannot be forced 
to certain required logic value. It is worth to mention that the IG-based approach to 
justification takes advantages of bit-parallelism in two different ways. First, several 
justiﬁcation problems can be solved simultaneously by processing a different 
justiﬁcation problem in each bit-slice (and-parallelism). This is exploited during fault 
parallel ATPG for easy-to-detect faults.  Second, alternative decisions can be 
examined simultaneously in different bit-slices (or- parallelism). This method is 
useful when dealing with redundant or hard-to-detect faults. 
 
3.2.1.5 Propagation operation 
Propagation is the task that makes a value change of an internal signal 
observable at certain primary output (at least one). It is achieved by finding a 
propagation path, sensitizing this path and finally justifying the injected sensitizing 
assignments. IG based propagation is as efficient as structure based approaches since 
the IG contains the complete topological information of a circuit. Similar to 
 17 
G1 G2 G3 G4 G5 G5* G4* G3* G2* G1*
n9 n6 n10 n10* n6* n9*
n7 n7*
n8 n8*
G16 G17*G17 G16*
 
Figure 3.2.1.7 Propagation in GF on C17 
 
3.3 Test Inflation Aware Parallel ATPG via Fault Ordering 
Parallel ATPG aims to partition ATPG process into non-overlapping 
subproblems where each subproblem is solved by a processor in parallel. In ideal 
situation, p processors could provide up to p times the computational speed of a 
single processor. Accordingly, substantial speedup improvement that can be achieved 
depends upon the problem itself and the amount of parallelism that can be explored. 
Conventional parallelism approaches include fault parallelism, heuristic parallelism, 
search space parallelism, algorithmic parallelism, and topological parallelism. 
However, these approaches often cannot perfectly divide ATPG process into 
independent subproblems and necessary interactions between processors including 
data transfer and computation synchronization hinder Parallel ATPG to achieve 
linear speedup. The inherent nature of Parallel ATPG also leads to the problem of 
Test Pattern Inflation; a critical problem where faults are detected multiple times by 
patterns from different processors. This problem inflates the amount of test patterns 
while achieving merely the same fault coverage, increases the test times of integrated 
circuits on Automatic Testing Equipment (ATE), results in higher test cost and longer 
time to market. 
To overcome the aforementioned difficulties, a Parallel ATPG implementation 
 19 
 
Figure 3.3.1.1 Differences between a process and threads 
 
3.3.1.2 Pthreads 
In Pthreads, the main program is a thread itself. The routines as follows are to 
create and terminated another thread: 
 
pthread_t thread1; 
  … 
pthread_create(&thread1, NULL, (void*) fun, (void*) 
&arg); 
  … 
pthread_join(thread1, NULL); 
 
A thread ID is assigned and obtained from &thread1 that can be used in 
subsequent references to the thread. The created thread is constructed through the 
function fun() and arg is the argument of fun() in pthread_create(). Note that the 
prototype of fun() and arg are unll pointer void*. If any prototype is used, casting is 
necessary to passing the arguments. The thread is destroyed when it terminates, 
releasing resources. The thread ID is used in pthread_join() to cause the calling 
thread to wait for new thread to terminate. Since it can only create one pthread in 
pthread_create(), a for loop is need for create when more than one thread is to be 
created. The program becomes, 
 
 
 
 
 21 
If a thread reach a mutex variable and the mutex is locked, it will wait for the 
lock to open. If more than one thread is waiting for the lock to open when it opens, 
the operating system will choose the front of thread in processing queue to be 
allowed to proceed. Moreover, only the thread that locks a mutex can unlock it. 
 
3.3.2 Flow of Test Inflation Aware Parallel ATPG 
The implemented Parallel ATPG considers parallelism in data-level, 
algorithm-level and architecture-level. The three major steps of an ATPG algorithm, 
test generation, fault simulation, and test compaction are optimally parallelized. New 
procedures including fault ordering, fan-in cone compaction, and concurrent 
interruption are incorporated into the implementation to decrease test generation time 
and lower test inflation. Three types of memories are used: shared read-only, shared 
writable memories, and private writable. The read-only memory includes 
information on logic gates, interconnects, and faults which cannot be altered under 
any circumstances. The shared writable memory refers to data structure like the 
fault-list which is shared among all slave processes and is editable by master and 
slave processes. To avoid data inaccuracy due to simultaneous modification to the 
same shared writable memory, mutually exclusive lock variables are used to ensure 
data integrity. Private writable memory may comprise necessary data structures like 
stacks, queues, or flags to facilitate ATPG progress. The private writable memory is 
privately used by each slave process and cannot be accessed or modified even by 
master process. The flow chart of Parallel ATPG is described in Figure 3.3.2.1. The 
highlighted section enumerates the parallelized ATPG procedures. An optimal fault 
sequence is first unveiled by sorting faults in fault-list according the number of 
undetected faults in the fan-in cone of each fault; then master processes initiates 
Parallel ATPG by creating slave processes.  
 23 
Setting a constraint on secondary fault limits the search space for test solution and 
speeds up ATPG process. If a test targeting secondary fault is found, the test would 
become the new constraint. The search for compatible secondary faults would iterate 
until all undetected faults in the fan-in cone of primary fault have been explored. 
 
           
  
 
 
 
 
Figure 3.3.2.3 Compatible and Incompatible Secondary Fault 
  
Once all undetected faults of primary fault have been explored, the slave process 
would perform Fault Simulation using the test pattern generated by the latest 
compatible fault. This test pattern is guaranteed to detect primary fault, compatible 
secondary faults, as well as some untargeted faults. When a fault is detected, slave 
process informs other slave processes about its newest discovery and performs 
concurrent interruption, interrupting other slave processes if they are generating a 
test for this fault concurrently. The slave process iterates the ATPG process 
independently until there are no more undetected faults in fault-list. The slave 
processes would then join the master process before the master process terminates 
the ATPG process.  
 
Chapter 4 Experimental Result 
We selected three ISCAS89 circuits with gate counts from 8k to 22k for our 
preliminary experiment. We will apply the implemented Parallel ATPG to circuits that 
have a more realistic scale in near future. The adopted ATPG algorithm is Fan-Out 
Oriented algorithm targeting stuck at faults. The multi-threaded programming 
language used is Pthreads. Table 4.1 lists number of gates and number of stuck at 
faults of these three circuits. Our experiments were conducted on a Multicore SMP 
Incompatible Secondary Fault 
Test for Primary Fault: 
1 1 X X 0 1 X X 
Test for Secondary Fault: 
1 0 0 1 0 0 1 X 
 
 
 
   
      
 
Compatible Secondary Fault 
Test for Primary Fault: 
1 1 X X 0 1 X X 
Test for Secondary Fault: 
1 1 0 1 0 1 1 X 
 
 
 
   
      
 
 25 
performance of parallel computing. Therefore, it is significant to carefully evaluate 
the capability of the hardware device before the design and implementation of a 
parallel program. 
 
Figure 4.2 Speedup on Intel Xeon machine  
 
Chapter 5  
Conclusion 
 
Implication-graph (IG) based ATPG is a fast and efficient framework for 
generating test pattern. In our implementation, we constructed the well-defined data 
structure for applying parallel techniques to three main procedures for ATPG, 
implication, justification, and propagation. Otherwise, we also combined the good 
and faulty IGs into only one IG to avoid the memory leak problem. In the next year, 
some structured-based and SAT-based ATPG techniques will be implemented into 
our IG ATPG, such as reach the headline in FAN instead of primary input in PODEM, 
the learning clauses in SAT-based ATPG for some additional information to IG. 
Moreover, pthread will first be implemented into IG ATPG for enhancing the 
performance of IG ATPG. 
ATPG (automatic test pattern generation) efficiently produces tests to explore 
the coverage corners and helps to identify the weak spots in the circuit. However, 
nowadays ATPG demands more computational effort to obtain optimality due to the 
increasing design complexity. The performance of commercial APTG is inversely 
 27 
[Fuj 1985]  H. Fujiwara. “FAN: A Fanout-Oriented Test Pattern Generation 
Algorithm,“ Proc. Int’l Symp. Circuits and Systems, p.p. 671–674, 1985. 
[Giz 2002] E. Gizdarski and H. Fujiwara. “SPIRIT: A highly robust combinational 
test generation algorithm,“ IEEE Trans. on CAD, 21(12):1446–1458, 2002. 
[Goe 1981]  P. Goel, “An implicit enumeration algorithm to generate tests for 
combinational logic circuits,” IEEE Trans. Comput.,30(3): 215–222, 1981. 
[Gul 2008] K. Gulati and S-P. Khatri, “Towards Acceleration of Fault Simulation 
using Graphics Processing Units,” Proc. Design Automation Conf., pp. 822-827, 
2008. 
[Koc 2010]  M-A. Kochte, M. Schaal, H-J. Wunderlich and C-G. Zoellin, C.G, 
“Efficient fault simulation on many-core processors,” Proc. Design Automation 
Conf., pp. 380-385, 2010. 
[Cai 2010] X. Cai, P. Wohl, J. Waicukauski, and P. Notiyath, “Highly Efficient 
Parallel ATPG Based on Shared Memory,“ Proc. Int’l Test Conf., 2010. 
[Lar 1989]  T. Larrabee. “Efficient Generation of Test Patterns Using Boolean 
Difference,“ Proc. Int’l. Test Conf., pp. 795–801, 1989. 
[Lee 1991]  H. K. Lee and D. S. Ha, “An efficient, forward fault simulation 
algorithm based on the parallel pattern single fault propagation,” Proc. Int’l. Test 
Conf., pp. 946–955, 1991. 
[Ham 1999]  I. Hamzaoglu and J.H. Patel. “New techniques for deterministic test 
pattern generation,“ Jour. of Electronic Testing: Theory and Applications, 15:63–73, 
1999. 
[Mar 1995] S. Marc, O. Steve, H-L. Steven, W. David and D. Jack. MPI: The 
Complete Reference. MIT Press Cambridge, MA, USA, 1995. 
[Nic 1996]  B. Nichols, D. Buttlar, J. P. Farell. Pthreads Programming. O'Reilly & 
Associates. 1996. 
[NVI 2007]  NVIDIA. CUDA Compute Unified Device Architecture, 2007. 
[Rot 1966]  J. P. Roth, “Diagnosis of automata failures: A calculus and a method,” 
IBM J. Res. Develop., 10(2): 278–281, July 1966. 
[Rao 1993] V.. Rao and V. Kumar, “On the Efficiency of Parallel 
Backtracking,“ IEEE Tran. Parallel and Distributed Systems, 4(4): 427-437, 1993. 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                         民國 101年 02月 07日 
報告人姓名 溫宏斌 服務機構 
及職稱 
交通大學電機工程學系 
助理教授 
會議時間 
會議地點 
January 30 - February 02, 
2012, Sydney, Australia 
本會核定 
補助文號 
計畫編號 
NSC 100-2220-E-009-048- 
會議 
名稱 
 (中文) 2012 ACM/IEEE 第 17屆國際泛亞太區設計自動化研討會 
 (英文) 2012 ACM/IEEE 17th Asia and South Pacific Design Automation 
Conference 
發表 
論文 
題目 
 (中文) 適用於深次微米晶片分類的 Iddq智慧型分析法 (榮獲最佳論文獎) 
 (英文) An intelligent analysis of Iddq data for chip classification in very deep 
submicron (VDSM) technology (best paper award) 
報告內容應包括下列各項： 
一、參加會議經過 
第 17屆國際泛亞太區設計自動化研討會(ASP-DAC)一向都是積體電路設計領域
最頂尖的國際研討會之一。今年 2012 ASP-DAC 選在澳洲的 Convention Center
舉辦，為其總共四天。其中共收錄九十五篇論文與數篇海報論文，主要來自國際
上知名的積體電路設計大廠，如 Intel、GlobalFoundries、IBM與 Freescale。除了
全程參與3場Keynote speeches和 technical sessions，包含Special Session 1: Robust 
and Resilient Designs from the Bottom-Up: Technology, CAD, Circuit and System 
Issues、Emerging Test Solutions、Yield and Manufacturability Enhancement、3D IC 
Layout、Timing, Thermal and Power Issues in High-Performance Design以及 Design 
for System Reliability。許多的論文報告都與我目前所從事的研究領域 Design for 
Yield(soft error)、data mining與 machine learning在 IC上的應用密切相關，而且
與會的過程中聆聽到現在業界在設計良率與測試上問題深入的討論，受益匪淺。
並且，在我自己這次論文報告與聆聽過程中，能與 Hiroto Yasuura、David Pan、
Yan Xie與 Sri Parameswaran等美加澳等地他校與業界設計良率與測試先進教授
學者直接對話，受益良多。其中，由於我第一次參與 ASP-DAC會議並報告論文，
有幸得到兩篇大會最佳論文獎的其中一篇(另外一篇來自美國德州奧斯丁大學由
Prof. David Pan 指導的論文)。大會主席 Dr. Sri Parameswaran 和議程主席 Dr. 
Yao-Wen Chang在第一天開幕式時介紹與頒獎，除了倍感幸運外也深感榮耀。 
 
與會心得 
國際泛亞太區設計自動化研討會(ASP-DAC)涵概研究領域創新，邀請的講者也夠
具知名度。今年本會參與人數與參業界代表還較歷年上升約 300 人，technical 
session中參與人數，討論比往常熱烈許多。其中，Special Session 1: Robust and 
Resilient Designs from the Bottom-Up: Technology, CAD, Circuit and System Issues
場次更是參與踴躍。另外，在我報告的論文討論中，聽眾族群以其不同的背景，提
出了對我們 Iddq 資料分析目前的瓶頸與提出許多建設性的建議(包含如何修正製程
變異問題設定使其更貼近真實面等)，也讓我們的研究方法更能拓廣其可實用性(包
含如何與現有 tester或 EDA 流程做整合)。整體而言收穫許多，也推薦其他國內
附件三
 
ASP-DAC 2012: Notification of Paper ID 1044
2 封郵件
溫宏斌 <opwen@g2.nctu.edu.tw> 
aspdac2012-tpc@mls.aspdac.com <aspdac2012-tpc@mls.aspdac.com> 2011年9月23日下午12:22 
收件者: chia.ching.cm98g@g2.nctu.edu.tw, hlchan@cs.nctu.edu.tw, tinger.cm98g@nctu.edu.tw, 
jayanta.bhadra@freescale.com, opwen@g2.nctu.edu.tw
Dear Chia-Ling Chang,
Congratulations!
It is our great pleasure to inform you that the paper below has been
accepted for regular presentation (25 minutes including Q&A) at
"The 17th Asia and South Pacific Design Automation Conference" (ASP-DAC 2012).
Paper ID:   1044
Title:      An Intelligent Analysis of Iddq Data for Chip Classification
Author(s):  Chia-Ling Chang, Chia-Ching Chang, Hui-Ling Chan, Charles H.-P. Wen (National Chiao Tung University, 
Taiwan), Jayanta Bhadra (Freescale Semiconductor Inc., U.S.A.)
ASP-DAC strives to maintain a high-quality technical program, and your paper
was one of only 99 regular papers accepted out of 288 papers submitted.
The reviewers and Program Committee chose your paper because it will
substantially add to the content and quality of the conference.
(1)
This acceptance is subject to submitting a camera-ready manuscript
by due date and making a presentation at ASP-DAC 2012.
Please follow the instructions in the guidelines on the web site below carefully
before preparing the final manuscript, and prepare all required materials by
each deadline.
http://www.cse.unsw.edu.au/~babaks/aspdac/author/
Important points are:
 a. Due date of manuscript and copyright transfer is 5 pm JST, Nov. 15th, 2011.
 b. At least one author is requested to register the full conference before
   final manuscript submission.
 c. Page limit is 6. This limit can be extended by two more pages
    with page charge of 15,000 Japanese Yen per page.
 d. If necessary, please improve your paper by considering attached reviewers'
   comments when you finalize your paper.
Paper ID and password to submit your final manuscript are as follows.
          Paper ID:    1044
          Password:    a5h72i
(2)
If you need VISA to come to Australia for presenting your paper,
please contact Conference Secretariat (aspdac2012-sec@mls.aspdac.com).
(3)
The title and authors of your final paper in PDF format *must* be
the same as those of your initial submission, which are shown
above. ASP-DAC 2012 does not allow any modifications of paper title
or addition/deletion of author names. Please check carefully the
title and authors of your final paper.
If you want to update authors' affiliations, please login at
http://www2.infonets.hiroshima-u.ac.jp/aspdac/cgi/ChangeAuthorship_final_submit.cgi
and update them by 5 pm JST, Oct. 6th (you can only update authors'
affiliations). You cannot update them after 5 pm JST, Oct. 6th.
(4)
Please also check the information of the contact author below.
results.
  c) It was seen in Table1 and Table2 that the accuracy keeps decreasing as the design size increases. Does it imply any 
scalability issue for the proposed method? What will happen for the proposed method if it is used for large industrial 
designs?
  d) One typo: page 5, line 22, "if a simple" => "if a sample"
Reviewer#3
[Overall score]
4. good
[Comment]
1. How do the authors obtain figures 1 and 2? From real designs, simulation, or just by imagination?&#8233;
2. The concept of "average-case" process parameter disregards intra-die variation which, as the&#8233;authors mention, 
contributes to 60% of leakage variation. What's the impact of this?
&#8233;3. The test circuits are too small. What's the background Iddq?
Reviewer#4
[Overall score]
4. good
[Comment]
This paper proposes an original Iddq testing technique called sigma-Iddq allowing to remove the process-variation but also 
the design sacling impacts from the Iddq data measurements.
The original idea consists in finding an 'uninfected pattern' and from this measurement deduce an average-case process 
conditions that can be used for the 'real' Iddq measurements.
Even if Iddq testing is today not so used, the idea is original a&nd the paper is well presented. The idea may be very 
interesting for the conference attendees.
Reviewer#5
[Overall score]
5. excellent
[Comment]
ó-Iddq testing is proposed to screen defective chips in this paper. The notion of ó-Iddq is similar to residual Iddq in ref. [10] 
but the estimated Iddq in equation (1) takes process variation into account. The well-known K-means clustering is applied 
to ó-Iddq data which are transformed from the original Iddq values and it classifies chips into good chips and bad chips. 
Experimental results show that the method has a high classification accuracy. To sum up, this paper combines data 
transformation and data mining to classify chips according to Iddq results effectively.
Several issues remain to be improved as follows:
1. Fig. 4 shows that ó-Iddq data under good chips and bad chips don $BCU (B overlap too much. Hence a threshold for ó-
Iddq may be determined to classify chips. In order to verify k-means clustering is really effective compared to other 
methods, it will be better and more convincing to provide chip classification results under  $BFu (B-Iddq + threshold 
value” for comparison.
2. K-means clustering, as a dynamic and iterative process, is not clearly clarified in Algorithm 1. Besides, how to determine 
the initial centers (like m1 and m2 in the paper) of clusters is not explained in this paper. The centers of clusters need to be 
obtained through samples.
Reviewer#6
[Overall score]
4. good
[Comment]
This paper presents a new Iddq test method called "sigma-Iddq" and the experimental results show the proposed method 
achieves a higher classification accuracy in a 45nm technology compared the an existing Iddq testing. The main idea is to 
find an uninfected pattern, which does not activate defects on a chip, and to deduce the parameters for average-case 
process variation.
This paper is well-organized and well-written. There is enough discussion about the related works, and the experimental 
An Intelligent Analysis of Iddq Data for Chip Classification
in Very Deep-Submicron (VDSM) CMOS Technology
Chia-Ling(Lynn) Chang1† , Chia-Ching(Austin) Chang1, Hui-Ling Chan2, Charles H.-P. Wen1, Jayanta Bhadra3
1 Dept. of Electrical Engineering, National Chiao Tung University, Hsinchu, Taiwan 300
2 Dept. of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 300
3 Freescale Semiconductor Inc., Austin, TX 78729
Abstract— Iddq testing has been a critical integral compo-
nent in test suites for screening unreliable devices. As the silicon
technology keeps shrinking, Iddq values and their variation
increase as well. Moreover, along with rapid design scaling,
defect-induced leakage currents become less significant when
compared to full-chip current and also make themselves less
distinguishable. Traditional Iddq methods become less effective
and cause more test escapes and yield loss. Therefore, in this
paper, a new test method named σ-Iddq testing is proposed
and integrates (1) a variation-aware full-chip leakage estimator
and (2) a clustering algorithm to classify chip without using
threshold values. Experimental result shows that σ-Iddq testing
achieves a higher classification accuracy in a 45nm technology
when compared to a single-threshold Iddq testing. As a result,
both the process-variation and design-scaling impacts are
successfully excluded and thus the defective chips can be
identified intelligently.
1. Introduction
Iddq (or leakage current) testing has been a critical inte-
gral component in test suites for screening unreliable devices
in a CMOS technology. It is a simple and cost-effective
test method which screens defective devices originated from
imperfect manufacturing. Traditional Iddq testing uses a
single threshold to classify chips. Many advanced variants
like [1][2][3][4] are proposed later to reduce variance in
fault-free Iddq, such as current signature, ∆-Iddq and cur-
rent ratio. Although these methods are simple and easy to
implement, the screening resolutions of these methods often
depend on the quality of Iddq measurement and determining
a threshold value to classify chips is not trivial for them.
Other methods proposed later, such as wafer-level spatial-
correlation methods [5][6][7], graphical Iddq [8], and
C△Iddq [9], continue to combat the decreasing effectiveness
of Iddq testing. Spatial-based methods [5][6] requires to
determine a threshold value to classify dies according to the
location of each die on the wafer. Nearest neighbors are used
as references to provide a basis during Iddq comparison.
However, the threshold value is wafer-dependent and hard
to be extended to other wafers. Graphical Iddq method [8]
removes outliers on current signatures to improve classifica-
tion result but requires visual identification by engineers.
C△Iddq [9] uses variance reduction against the process-
† Email: tinger.cm98g@nctu.edu.tw
variation effect. Additionally, test modification for Iddq
patterns is particularly required by C△Iddq.
F
re
q
u
e
n
c
y
Normalized Iddq Current
Good chips
Bad chips
test 
escape
yield
loss
threshold
F
re
q
u
e
n
c
y
Normalized Iddq Current
Good chips
Bad chips
test 
escape
threshold
(a) Iddq current on 90nm (b) Iddq current on 45nm
Figure 1. Iddq distributions in the VDSM regime
Moreover, along with process variation, the ever increas-
ing design complexity makes Iddq testing more challenging
from two aspects: (1) the leakage current due to a defect
becomes relatively small in a scaled design; (2) the leakages
of normal cells grow remarkably with the increasing process
variation. As a result, Iddq distributions of good chips and
bad chips are more indistinguishable and unavoidably induce
more test escapes and yield loss. Figure 1 illustrates this
situation under two different technologies (90nm and 45nm).
The distributions in Figure 1 represent Iddq simulation
on one ISCAS’85 benchmark circuit, where each bad chip
is caused by injecting one random defect. It is impossible
to prevent test escapes and/or yield loss in Figure 1(a)
once a threshold is decided. Moreover, all defective (bad)
chips in a 45nm technology all become test escapes as in
Figure 1(b) and cannot be separated because the bad-chip
Iddq distribution is entirely encompassed in the good-chip
Iddq distribution. Therefore, the increasing test-escape chips
may lead to an extra cost for circuit designs. To prevent this
problem, we need to analysis the Iddq data to pin point the
root cause.
When we further measure Iddq currents from one good
chip and one bad chip in the 45nm technology from same
Iddq data as shown in Figure 2, it is difficult to differentiate
them according to their Iddq distributions. For example,
Figure 2(a) and Figure 2(b) look no different if every
gray circle denoting one Iddq including the defect-induced
faulty leakage is not colored in Figure 2(b). Thus, a mis-
classification may lead to either a test escape or yield loss.
We also try ∆-Iddq and current ratio to handle the
inscrutable Iddq currents in Figure 2. ∆-Iddq attempts to
P activates the defect or not and is a Boolean variable. If is
the faulty leakage induced by an activated defect. A pattern
P is declared uninfected if and only if the λP = 0, i.e.
P does not activate any defect. Similarly, a measured Iddq
IPcut is declared uninfected if and only if the corresponding
pattern P is uninfected. For correctly decomposing one Iddq
value for the cut (either good or bad), σ-Iddq testing will
first find an uninfected pattern and then use such pattern to
disengage the process-variation impact.
Step 1A: Finding An Uninfected Pattern
Full-chip leakage estimation proposed in [11] helps fast
compute the full-chip leakage current Icut assuming that
all process parameters of each cell are given. However, in
reality, it is not the case. We need to figure out the fault-
free leakage as well as the process parameters of each cell
by ourselves. It is difficult to decide if a given pattern P
is uninfected, i.e. P does not activate a defect on a chip.
Figure 6 illustrates such difficulty on a sample circuit with
equal Iddq values (IP1cut=I
P2
cut) resulting from two different
patterns, P1 and P2. Note that such two Iddq values are
subject to the same set of active cells and the same process
variation with the only exception that P1 activates a defect
but P2 has higher variation-induced leakage on the bottom-
right corner.
variation-induced
leakage
=
+
+
nominal leakage
from active cells
+
+
no faulty
leakage
variation-induced
leakage
defect-induced
faulty leakage
infected
uninfected
P1
I cut
I cut
P2
Figure 6. Decompose Iddq data under two patterns
To overcome such problem, a simple strategy is taken in
this paper for finding a uninfected pattern. Basically, we rank
all patterns by their Iddq values and start to use the one with
the minimum leakage current to explore process parameters.
This idea comes from the observation that the pattern with
less leakage current activates fewer number of cells and is
less likely to have a defect activation (thus, not incurring
faulty leakage If ).
The pattern with the i-th minimum Iddq current is rep-
resented by P imin. P
1
min denotes the pattern with the first
minimum Iddq value among all and serves as the first
candidate for process-parameter exploration in Step 1B.
P 2min and P
3
min are used to validate the set of process
parameters deduced from P 1min. If P
1
min fails the validation,
then P 2min is sent to Step 1B for searching the next set of
process parameters.
Step 1A iterates until a satisfactory set of process param-
eters in Step 1B is found. Empirically, P 1min suffices to find
a feasible set of process parameters and thus the runtime
for this step is short. Furthermore, the number of validation
patterns can also be specified by the user and 2 validation
patterns are typically sufficient in our experiments.
Step 1B: Exploring Process Parameters
Given a pattern and a chip under test, the problem
of finding corresponding process parameters (e.g. effective
channel length Leff and gate oxide thickness Tox) on each
cell to estimate the full-chip leakage is named parameter-
deduction problem. Pmin found in Step 1A is the uninfected
pattern which assumes no defect activated and thus used to
deduce process parameters. In [14], the authors formulate
the process variation as a scaling factor on each cell,
model the characterization problem into linear equations and
solve equations by a linear programming engine. However,
runtime is the major bottleneck for this flavor of solutions.
Therefore, in this work, we propose a similar but slight
different strategy to tackle the parameter-deduction prob-
lem. Since it is almost impossible to deduce all process
parameters on every cell using only one pattern Pmin, the
notion of average-case comes into play for each type of
process parameters on all cells. During the derivation of the
average case, each type of process parameters among cells
is assumed to have the same value. For example, all cells on
one chip have the same amount of Leff variation (△Leff )
and the same amount of Tox variation (△Tox). Then, an
exhaustive search is performed over all possible parameter
combinations and formulated as:
arg min
△Leff ,△Tox
{IPmincut −
m∑
k=1
Icellk(△Leff ,△Tox)} (3)
where
∑m
k=1 Icellk(△Leff ,△Tox) indicates that the ideal
fault-free leakage with △Leff and △Tox under Pmin. The
objective of Equation 3 is to find a set of process parameters
that makes the least difference between measured Iddq and
average-case Iddq (such notion will be introducted in Sec-
tion 2.2). Since each parameter variation ranges from 10% to
20% of the nominal values according to [15], exploration of
each (△Leff ,△Tox) combination from −20% and +20%
stepping with 1% typically is not be a problem in terms of
time and space.
Given P imin, the best process-parameter combination is
the one with the least difference between the computed
fault-free leakage and the actual Iddq value measured from
the chip. As mentioned previously, P i+1min and P
i+2
min will be
used for validation. If two respective leakage differences for
P i+1min and P
i+2
min are both smaller than α% (we use 5% in
the experiments), the current set of process parameters is
valid and can be used as the average case onto each cell.
Otherwise, Step 1B iterates to find the next set from P i+1min .
Single-Threshold Iddq Clustering-based Auto-classification
(w/ golden answers) (w/o golden answers)
Circuit - direct clustering σ-Iddq + clustering
Name Positive Negative Accuracy Positive Negative Accuracy Positive Negative Accuracy
True False True False % True False True False % True False True False %
c499 800 200 0 0 80.00 800 4 196 0 99.60 800 0 200 0 100
c880 800 200 0 0 80.00 800 7 193 0 99.30 800 0 200 0 100
c1908 795 199 1 5 79.60 800 7 193 0 99.30 797 0 200 3 99.70
c2670 799 199 1 1 80.00 800 21 179 0 97.90 798 0 200 2 99.80
c3540 800 200 0 0 80.00 800 61 139 0 93.90 797 0 200 3 99.70
c6288 800 200 0 0 80.00 800 79 121 0 92.10 800 21 179 0 97.90
c5315 800 200 0 0 80.00 800 76 124 0 92.40 798 1 199 2 99.70
c7552 800 200 0 0 80.00 800 97 103 0 90.30 798 3 197 2 99.50
s5378 800 200 0 0 80.00 800 76 124 0 92.40 800 6 194 0 99.40
s9234 800 200 0 0 80.00 800 141 59 0 85.90 799 7 193 1 99.20
s13207 800 200 0 0 80.00 800 183 17 0 81.70 800 36 164 0 96.40
s15850 800 200 0 0 80.00 800 186 14 0 81.40 800 65 135 0 93.50
Average 79.97 92.18 98.73
Table 1. Chip-classification results for Iddq under a 5.5µA defective leakage
Leff and Tox are 50nm and 1.1nm, respectively, from the
specification of the Nangate library. In this work, the process
variation is set to 3σ = 20% and includes inter-die and intra-
die variations. With the same setting as in [11], inter-die
variation contributes 40% of overall process varation while
intra-die variation contributes the other 60%.
Pseudo stuck-at fault (PSA) model proven to detect all
leakage faults [16] is used in this work for experiments. A
pseudo stuck-at fault can be represented as a resistor short
between supply voltage and output of cell or short connect-
ing from the output of cell and ground. Corresponding to
real defective leakage currents in VDSM CMOS designs, big
resistors are used in our SPICE simulation to induce small
leakage currents. Therefore, two resistance values, 200KΩ
and 500KΩ, are used to induce 5.5µA and 2.2µA leakage
currents as the supply voltage is 1.1V .
1000 samples for each circuits with inter-die and intra-
die variations are generated to mimic the real manufacturing
process. A pseudo stuck-at fault is randomly injected onto
200 samples and Iddq patterns generated by Synopsys Tetra-
Max are simulated to derive the full-chip leakage. Single-
defect assumption is used in the experiments instead of
multiple defects since multiple defects induce larger leakage
currents than a single defect does. Therefore, the defective
chips with multiple defects are easier to be identified.
To demonstrate the power of our σ-Iddq testing, we use
an ideal single threshold method as a baseline approach.
Since it is difficult to determine the exact threshold value
for single-threshold Iddq, we refer the golden answers to
deriving the best performance. That is, the search of best
threshold started from the smallest value to the largest one
and terminated as the one with the least number of mis-
classifications is found. Therefore, each pattern can derive its
own golden threshold value to classify chips. Once a sample
is classified as bad chip by one pattern, it is marked as a bad
chip afterwards. Similarly, if a sample is claimed as good
chip, it must be good under all patterns. The classification
results of such single-threshold Iddq are shown in Table 1.
In Table 1, three types of classification results are listed.
True positive (TP ) denotes the number of samples that are
true fault-free samples and identified as good ones. False
positive (FP ) denotes the number of fault-free samples
claimed as good ones but actually are defective. True neg-
ative (TN ) denotes the number of true faulty samples and
also marked as bad ones. Last, false negative (FN ) denotes
the number of true fault-free samples mistakenly identifying
as bad ones. A good classification method is expected to
identify true positive and true negative samples as much
as possible. We use the following equation to define the
classification accuracy:
Accuracy =
TP + TN
TP + FP + TN + FN
where FP together with FN denotes the number of mis-
classification. As a result, single-threshold Iddq can only
reach the best accuracy as 79.97% on average for all circuits
in Table 1.
Before showing the results of our σ-Iddq testing, we
also apply our clustering algorithm directly onto original
Iddq data without referring golden answers (termed direct
clustering). Consequently, our algorithm can classify the
chips effectively and efficiently. As shown in Table 1,
the second major column lists the performance of direct
clustering and an accuracy of 92.18% on average is reached,
higher than single-threshold Iddq by 12%. More specifically,
based on original Iddq data, only 922 out of 1000 samples
can be successfully identified by direct clustering. The total
execution time including σ-Iddq and clustering for each
benchmark does not exceed 100 seconds.
Last, our σ-Iddq testing (termed σ-Iddq+clustering)
achieves the highest accuracy in Table 1. After transforming
國科會補助計畫衍生研發成果推廣資料表
日期:2012/03/03
國科會補助計畫
計畫名稱: 子計畫五：在處理器陣列與多電腦系統上的平行化測試向量產生演算法設計
(1/3)
計畫主持人: 溫宏斌
計畫編號: 100-2220-E-009-048- 學門領域: 智慧電子科技計畫-整合型學術研
究計畫
無研發成果推廣資料
已獲得件數 0 0 100%  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
這個計畫的第一年先要探索決定性 implication-graph ATPG 的可平行化空間，
針對多核心單晶片系統進行多引線(multi-threaded)程式開發。其後，在第二
年的目標會針對大量多核心(massively many-core)平台如 CUDA 進行程式設
計。第三年的發展目標是希冀結合多單晶片系統而開發分散式計算方法。而本
計劃之部分研究成果也將出版在教育部/Intel MTL 共同開發的多核心技術課程
模組：交換網路中邏輯閘層級模擬演算法平行化設計的開發，並舉薦為 100 年
度教育部顧問室「網路通訊人才培育先導型計畫」優良教材評選獲得佳作。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
