 I
行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   □期中進度報告 
仿人形運動與娛樂機器人之設計、研製及應用－總計畫(2/3) 
Design and Implementation of Android Systems for Soccer Games and Recreation 
計畫類別：□ 個別型計畫  ■ 整合型計畫 
計畫編號：NSC 97-2221-E-006-160-MY3 
執行期間：  98 年 8 月 1 日  至  100 年 7 月 31 日 
 
計畫主持人：李祖聖  特聘教授  國立成功大學電機工程學系 
共同主持人：劉立頌  教授      國立中正大學電機工程學系 
黃國勝  教授      國立中正大學電機工程學系 
計畫參與人員：胡伊欣 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          涉及專利或其他智慧財產權，□一年二年後可公開查詢 
          
執行單位：國立成功大學電機工程學系 
 III
then learns to improve its behavior in terms of stability. The main goals of sub-project 4 are 
twofold, one is to compare the new design of the android robot with the previous one, and the 
other is to investigate a simulator for humanoid robot soccer competition and its strategies. The 
simulator is developed for AndroSot in FIRA. Due to the robot soccer game presents a dynamic 
and complex environment, it provides a challenging platform for multi-agent research. However, 
if there were some problems occurred in the robot actions and image processing algorithm, it is 
very difficult to run or test strategy systems. In order to solve these issues, a humanoid robot 
soccer competition’s strategy simulation system is developed, which provides developer to test 
the feasibility and effectiveness of the game strategy.  
 
 
Keywords: reinforcement learning, balance control, simulator, robot soccer, AndroSot, FIRA.
  1
三、The reason and purpose of research project 
In recent years, the age of toy consumers is gradually expanding. It has been extended from 
infants and young children to adults and senior. The combination of technology and toys not only 
increases the growth rate but also extends the product life cycle. Furthermore, the entertainment 
robot will also be very popular in the future. On the other hand, the functions of the robot can be 
very diverse. Some robot can dance, sing, and some even can play the soccer games. Almost 
every family has one entertainment robot in the future. Therefore, watching the robot show will 
become one of the main entertainments of our family life. As a result, we can realize the market 
potential of entertainment robots. Thus, this project is to design an affordable multi-function 
humanoid robot that can play soccer and also participate in FIRA AndroSot [1] competitions. 
However, it needs high threshold to participate in the competition. And the examination of its 
strategy is also troublesome. In order to address these problems, development of simulator can be 
used for testing the feasibility and advancement of the game strategy of each team easily. And 
this simulator will be proposed in the following sub-themes. 
 
四、Research Steps and Methods 
There are two main sub-projects in this report. For sub-project 3, we use the Q-learning 
method to solve dynamic walking and balance control problem by reinforcement learning. For 
sub-project 4, in order to enhance the technical level of domestic robot, we develop the simulator 
and three android robots to attend AndroSot Competitions. The research methods and the 
procedures of subproject are described below. 
Sub-project 3 
Sub-theme 1: Introduction 
The robot is a linked system if we want to adjust the robot’s walking posture to keep balance. 
One of the methods is to calculate the center of mass of the robot. Some studies use trajectory 
planning [5], and some use a mathematical model to construct a dynamic equation of a biped 
robot [6], the method is complicated during the calculation. Some studies used the foot tactile 
  3
1. Initial Q(s, a) to be 0 for all states s and actions a. 
2. Perceives a current state s. 
3. Choose an action a according to the action value function. 
4. Carry out an action a in the environment. Let s' be a next state and r be an 
immediate reward. 
5. Update the action value function in terms of state s and the action a pair. 
 )]([),()1(),(1 sVrasQasQ tt    
 ),(max)( asQsV a A   
where α is a learning rate parameter and γ is a fixed discount factor between 0 and 1. 
Return to 2. 
 
Sub-theme 3: Proposed method 
The proposed method applies reinforcement learning algorithm to balance a walking biped 
robot by a variety of arms posture. The architecture of the proposed method shows in Fig.1. The 
architecture include three functional modules, a state space mapping sensory input from the 
environment into memory of the leaning process; the reward function feeding the evaluative 
feedback to the policy learning module; an action set containing various arms postures. 
Arms Posture 
Set
Policy Learning
Environment
Reward 
Function
Reward
Action
State
 
Fig.1 The architecture of the Q-learning for gait balance 
  5
One of the objectives of this work aims at the extension of the conventional Q-learning to 
generate a continuous effort from a set of actions. The architecture of the proposed algorithm can 
divided into two layers. The first layer is similar to a conventional Q-learning. But on the second 
layer, the concept of stochastic action generation method for reinforcement learning is applied 
[11]. The reward from the environment is based on the maximal Q value referred to the first layer. 
In other words, this algorithm uses the Gaussian distribution to produce a stochastic and 
real-valued output, and tries to record the maximum expected future reward and the action under 
this policy. The algorithm adjusts the mean and the variance of the Gaussian distribution so as to 
increase the probability of producing the optimal real-valued output for each input pattern. 
This paper combined the Arm Balance Algorithm to transfer robot’s discrete action into 
continuously. Before using Arm Balanced Algorithm it requires to discrete the action space in 
advance. It is illustrated in Fig. 2, each paradigm action has a corresponding action bias mean, 
Bm , and a action bias variance, Bv.  
 
 
Fig. 2 Definition of the action bias 
 
While system obtain the state value from the environment and  evaluate an action, Arm 
Balance Algorithm utilizes the normal distribution to find the action bias, B~N(Bma, Bva). The 
Bma and Bva are the action bias mean and action bias variance to the selected action. The action 
bias creates a suitable action as the system output. After that we update the value of action bias 
mean and action bias variance. The update method as follow: 
 
 Action Bias Mean 
  7
   max ,
a A
V s Q s a

  
6. Update the action bias mean by  
][ BmBQBmBm m    
and action bias variance by 
)( BvQBvBv v   . 
7. s  s'，a  a，Go back to step 2，until the 
episode is terminate. 
Fig. 3 The algorithm of the proposed learning method 
 
Sub-theme 4: Simulation 
A simulated Bioloid biped robot is shown in Fig. 4. Fig. 4 (a) illustrates its 
three-dimensional link structure. The robot is actuated by 18 RC servo motors. The mass of motor 
is about 1.31 kg in total. Each arm has three degrees of freedom (DOF) and each leg has six 
active DOF; three DOF are available at the hip, one at the knee and the other at the ankle joint. 
Each of the feet has four force sensors (two at the toe and two at the heel) to provide the contact 
forces between the feet and the ground, as shown on Fig. 4(b). 
The proposed Q-learning, in order to achieve the dynamic balance, ZMP is used as a 
criterion; therefore, the algorithm can learn to control two arms’ motions to help ZMP retained at 
the center of the feet as possible. 
 
Fig. 4(a) Fig. 4(b) 
Fig. 4 The simulation model of the Bioloid biped robot 
 
  9
C. Action Space 
In this work, an action set with 36 sequential distinct basic action is design a prior. The 
parameter in updating action bias is setting as follow: 
If Q  > 0 , m = 0.5 . If Q  < 0, m = 0.2, v  is learning rate, action bias variance must 
smaller than action bias mean. After obtain the m and v respectively, it utilizes a normal 
distribution to obtain a action bias as follow: 
Output = a + N( Bma , Bva ). 
 
D. Policy Update 
The policy determines proper action acting on the environment according to the current 
states. The only information received by the learning agent is the state of the environment in 
terms of state variables and the reward. The goal of training the policy is to take an optimal action 
that can maximize the Q value associated with that state. The learning rule used to adjust the 
parameter value can be described as follow  
)),())' ,'(Max(() ,() ,( oldoldnew asQasQrasQasQ    
where ),( asQ  denote the value of state-action pair in state s  taking action a . It has been 
proved that Q  value can asymptotic approach to the optimal one, *Q , after repeatedly updates. 
 
E. Result 
To evaluate biped dynamic balance a penalty signal should be given if the biped robot’s 
spontaneous ZMP is not within the supporting areas on the feet or it falls down on the plane. The 
evaluative feedback signals can only describe the simple walking states. It is less informative 
than the human evaluation on walking.  
Each simulation of learning processes repeats10000 times episodes, each of which is elapsed 
200 time steps of learning. A learning episode ends if the robot completes 200 time steps or falls 
down. 
  11
servo motors, the main controller and the batteries. 
There are 20 servo motors used in aiRobots-S1. Fig. 8 (a) denotes the complete structure if 
aiRobots-S1; Fig. 8 (b) shows the disposition of all motors on aiRobots-S1. There are 6 motors in 
each leg, 3 motors in each arm and 2 motors in the waist, totally 20 motors.  
The mechanism structure made with 2mm-thick aluminum-magnesium alloy is designed to 
connect all of the 20 servo motors of the aiRobots-S1, and also to protect the main controller, 
conforming to the rules of competition. Fig. 9 shows the actual appearance of aiRobots-S1. 
In aspect of aiRobots-S2, to handle the weight of the hardware and perform all actions 
smoothly, the aluminum-magnesium alloy is still a suitable material for the mechanism. On the 
other hand, the more weight the robot possesses, the more power the battery needs to consume. 
Therefore, in order to improve the moving speed of the robot, the weight of the robot must be 
reduced. To solve this problem, the design of the hollow frame is helpful.  
In additional, falling down or colliding with other robots are very frequent accidents in the 
games. The mechanism of the hand can help the robot getting up immediately. All the design of 
the mechanism is done with engineering software, Solid Works [2].  
The mechanism design and the disposition of motors on aiRobots-S2 are shown in Fig. 10. 
The degree of freedom makes aiRobots-S2 can perform all the motions that needed in the 
competition. Finally, the actual appearance of aiRobots-S2 is shown in Fig. 11. 
 
   
(a)                    (b) 
Fig. 8 (a) The complete structure if aiRobots-S1  (b) The disposition of motors on aiRobots-S1. 
  13
(1) Introduction 
To facilitate the strategy testing of these humanoid robot soccer competitions, a strategy 
simulator for AndroSot in FIRA is proposed. Simulate competition software is compiled using 
Embarcadero C++ Builder 2010 [3], and it can run under Windows XP and Windows 7. In this 
simulator, strategies compiled to DLL files may be explicitly loaded at run-time. In addition, 
another simulation mode, “Master Level” is presented. The healthy state of each robot is 
considered in this mode. If the robot collided with another robots or the wall, its value of the 
healthy state will be reduced. If the value of healthy state is zero, the robot cannot execute any 
action. 
(2) System architecture 
First of all, the system flowchart of the simulator is shown in Fig. 12. When the “Start” button 
is clicked by the user, the background music composed by ourselves is played. Next, the 
simulator will check whether the DLL files are loaded or not. If the DLL strategy files are loaded, 
the simulator will execute the strategies in the DLL files. On the contrary, if the DLL strategy 
files are not loaded, the simulator will execute the built-in strategies. After that, the pose of the 
robots will be set immediately. Then, if the simulation level selected by the user is “Master level”, 
the healthy point (HP) of each robot will be reduced or increased. The value of HP depends on 
the circumstances during the competition. Afterward, the location of the ball is updated. After all 
of the processes described above are completed, in order to show the circumstances in the playing 
field, the simulator will use computer graphics to simulate the field, the robots and the ball. 
Moreover, for the purpose of replaying the process, the current frame will be recorded in the 
memory. Then, the position of the ball will be checked for the post-process. That is, if the ball is 
outside, the simulator will show the dialog box to confirm whether the competition should be 
continued or not. On the other hand, if the ball is in the goal, the scores on the scoreboard will be 
adjusted. Then the simulator will show the dialog box to confirm whether the user wants to replay 
or not. If the user does not want to replay, the simulation will be stopped. However, the user also 
  15
 
Fig. 13 the interface of the simulator 
(4) Basic settings of the ball and the robots 
   The picture of the ball and the robots are shown in Fig. 14. Fig. 15 shows the form for setting 
of the robots and ball. In the robot setting form, the group box of the ball is on the top of the form. 
In this group box, the user can adjust the radius and the RGB value of the ball. 
The setting parameters of the robots are under the group box of the ball. The user can select 
the color of each robot. Furthermore, the length, width, and height of the robots are also can be 
adjusted. Next, the current position, the current angle and the initial position of the robots, are 
shown in this form. The user can realize the actual coordinates and the angles of the robots by 
opening the form. 
 
 
Fig. 14 The picture of the ball and the robots 
  17
4)  Monitoring board: Fig. 16 shows the monitoring board. All of the statuses of the robot 
are shown in monitoring board. Normally, the top of the label will show “Normal”. The label will 
show “Stuck!” if the robot is collided with other robots or the “walls” in the field. Next, the check 
box named “Power supply” denotes the state of the power (on/off). Furthermore, all of the actions 
executed by the robot are listed in the memo. 
5)  Scoreboard: The scoreboard is shown in Fig. 17. The score of each team will be 
changed automatically when one team is scored. However, the user still can change the score by 
clicking the up-down buttons. 
 
Fig. 16 The monitoring board 
 
 
Fig. 17 The scoreboard board 
 
(6) Extended mode - Master level 
  19
 
Fig. 19 The picture of the maintenance of the robots 
 
Sub-theme 3: tutorial on creating dll files 
If the user wants to test the feasibility and advancement of the game strategy, creating DLL 
files is essential. For the facilitation of teaching, the free software, Dev C++ [4] is chosen for the 
compiler. The picture of creating DLL files by Dev C++ is shown in Fig.. 20 First of all, the user 
must open the project file (as shown in the left rectangle in Fig. 20) of the strategy. Then start to 
implement the strategy programs. However, for the briefness of the contents, we are not going to 
illustrate the detail of the codes in here since we do not make much fix on it. After coding, we can 
click the button (as shown in the circle in Fig. 20) which is on the upper-left corner of the form to 
compile the DLL file. If the compilation is successful, The DLL file will be created in the folder, 
as shown in the right rectangle in Fig. 20. 
After DLL files are created by the user, they must be copied to the folder named “dll”. Next, 
open the form for loading the DLL files in the simulator. The form for loading the DLL files is 
shown in Fig. 21. All of the DLL files in the folder named “dll” are listed in the file list box. The 
user can select a file and assign it to the corresponding group box. 
 
  21
 
Fig. 22 Potential field navigation method 
 
Sub-theme 5: simulation and experimental results 
   Fig. 23 shows the simulation result. Firstly, the ball is on the left half field. Thus, the defender 
of Team B goes to the defending position and rotates for facing the ball. Then the attacker of 
Team A goes to the kicking position and shoots. However, the defender of Team B already stands 
on the defending position. So the ball is blocked by the defender successfully. Then, the ball is 
rebounded to the left half field. Therefore, the defender of Team B goes back to the defending 
position again. Then the attacker of Team B executes the rebound shooting. Also, the ball is 
blocked by the defender again. However, the ball is on the right half field. So the defender shoots, 
but the ball is blocked by the goalkeeper of Team A finally. 
 
(a) (b)
  23
(k) (l)
(m) (n)
(o) (p)
Fig. 23 the simulation result 
 
Fig. 24 shows the result of long shooting. Firstly, the ball was beside our robot. Our robot 
backward and shift left to approach the kick point. In order to make the path not blocked by 
enemy robots, out robot fixed the kicking angle. Then just kick forward powerfully. The ball 
passed through two enemy robots and goal. 
 
(a) (b)
 
  25
(q) (r)
 
(s) (t)
 
Fig. 24 The result of long shooting 
 
 
Fig. 25 shows the result of blocking the ball. Firstly, the ball was kicked directly toward our 
goal by enemy robot. In order not to goaled by enemy team, our goalkeeper dived for the ball 
immediately. 
 
(a) (b)
 
(c) (d)
 
(e) (f)
 
(g) (h)
 
  27
these events. 
The progress of the study results are shown as follows:  
1. The simulator for AndroSot in FIRA is developed. 
2. Another simulation mode, "Master Level" is also presented. 
3. Strategies which compiled to DLL files may be explicitly loaded at run-time in this simulator. 
4. Our team won the third place in FIRA 2008AndroSot competition. 
5. Our team won the champion in 2008 人工智慧單晶片電腦鼠暨機器人實作競賽 AndroSot 
competition. 
6. Our team won the champion in 2010 台灣智慧型機器人國內及國際邀請賽 AndroSot 
competition. 
7. Our team won the third place in FIRA 2010 AndroSot competition. 
  29
[14] J. Guldner, V. I. Utiken, H. Hashimoto and F. Harashima, Tracking gradients of artificial 
potential fields with non-holonomic mobile robots, submitted to ACC, 1995. 
[15] Min Gyu Park, Jae Hyun Jeon, and Min Cheol Lee, “Obstacle avoidance for mobile robots 
using artificial potential field approach with simulated annealing,” IEEE Conf. On IEEE 
International Symposium, Vol. 3, pp. 1530-1535, 2001. 
[16] J. Barraquand, J. C. Latombe, “Robot motion planning: a distributed representation 
approach”, International Journal Automation, Vol. 10, No. 6, 1991. 
[17] J. Guldner, V. I. Utiken, H. Hashimoto and F. Harashima, “Tracking gradients of artificial 
potential fields with non-holonomic mobile robots”, submitted to ACC, 1995. 
[18] R. C. Arkin. Behavior-Based Robotics. The MIT Press, 1998. 
  31
七、可供推廣之研發成果資料表 
■ 可申請專利  ■ 可技術移轉                          日期：100 年 01 月 08 日 
國科會補助計畫 
計畫名稱：仿人形運動與娛樂機器人之設計、研製及應用 (2/3) 
計畫主持人：李祖聖 教授 
計畫編號：NSC 97-2221-E-006-160-MY3  
學門領域：控制學門 
技術/創作名稱  
發明人/創作人  
技術說明 
中文： 
1. 利用加強式學習解決人形機器人動態行走與平衡控制的問題。
兩足機器人在開始行走時，無法穩定行走，經由演算法學習後，將
可改善其行走的穩定性。 
2. 小型人形機器人足球賽模擬器之開發與其策略演算系統。由於
人形機器人足球比賽提供了一個動態而且多樣化的環境，因此可以
做為多方面研究題材的測試平台。然而針對策略系統而言，如果在
沒有硬體架構與視覺系統整合完成情況之下，很難進行測試。為了
解決上述問題，本計畫提出了一套人形機器人足球比賽的策略模擬
系統，以供使用者方便測試策略之可行性與優缺點。在模擬器中，
會將使用者編譯完成的動態連結程式庫動態載入至程式中。 
 
