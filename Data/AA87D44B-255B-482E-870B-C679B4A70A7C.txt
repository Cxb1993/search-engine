1 
 
行政院國家科學委員會專題研究計畫成果報告 
下世代服務型機器人快速工作定義和全自主執行技術 
Rapid Job Definition and Autonomous Execution Systems for Next 
Generation Service Robots 
計畫編號：NSC 99-2221-E-011-011-094 
執行期限：99年 8月 01日至 100年 7月 31日 
主持人：林其禹 國立台灣科技大學機械工程系 
共同主持人：邱士軒 國立台灣科技大學材料科學與工程系 
 李維楨、徐繼聖、林紀穎 國立台灣科技大學機械工程系 
 郭重顯  國立台灣科技大學電機工程系 
 
I. Abstract 
本整合型研究計畫的目的係開發適用於下世代
實際可於家庭和廠房內全自主執行臨時指定工作之服
務型機器人之命令定義和全自主執行技術平台。該機
器人快速工作定義和全自主執行技術之成功開發將可
使智慧型機器人執行臨時指定服務工作所需的準備時
間由數月或數周之久縮短到 30 分鐘至一小時時間，
也間接使得服務型機器人得以擺脫僅能表演特定指定
工作之展示機器人宿命，這對比爾蓋茲總裁於 2006 
年的「在 2025年前，家家戶戶內都會有機器人」世紀
宣示，具有背書意義。本機器人快速工作定義和全自
主執行技術之開發，將可使得具有立體視覺和雙手臂
之服務型機器人，可以跟據使用者短時間內定義的臨
時性工作內容，在不需要撰寫任何視覺辨識軟體和手
臂操控程式的情況下，便能立即執行指定服務。經過
計畫預定開發的影像擷取和操作參數系統登錄後，機
器人便可以隨時辨識該些物件，再藉由創新的參數化
手臂運動控制副程式，機器人能立即使用手臂全自主
操作該些物件。仿生式視覺引導手臂運動控制技術和
精密手臂控制技術將能確保機器人得以迅速抓取並且
精密地操控物件，提升工作能力。 
 
關鍵字：服務型機器人、參數化手臂運動副程式、雙
手臂、立體視覺、全自主執行  
 
This integrated project aims to develop command 
definition and autonomous execution techniques for next 
generation service robots suitable for practical 
applications in family and factory environment. The 
successful development of the command definition and 
autonomous execution techniques can reduce the 
preparation time for conducting an assigned service from 
several weeks or even months to 30~60 minutes, and 
thereby a service robot can start to produce usable rapid 
service other than demonstration as a show only. This 
important technical advance can serve as a support to the 
century claim by Bill Gates in 2006, “By 2025, there will 
be robots in every family.” The development of the fast 
command definition and autonomous execution 
techniques enable a service robot with two-arms and 
stereo vision capabilities start to perform the requested 
assignment without any tedious object recognition 
algorithm generation and arms motion planning after the 
robot master spends a limited period of time in defining 
the content of the service and registering the related 
objects. After registering the objects to be operated and 
defining the parameters for operations in the system to be 
developed, a service robot can immediately recognize, 
grasp and operate the objects according to requests with 
the aid of the parameter-based arm operation subroutines 
to be developed. The bionic visual servoing techniques 
and the sensor-based precise arm control techniques 
enable robot for a fast object grasp and precise object 
operation. 
 
Keywords：Service robot, parameter-based arm motion 
subroutine, twin-arm, stereo vision, autonomous 
operation 
II. Introduction 
This project was formed by one integrated project 
and five sub-projects. According the Fig. 1, the 
integrated workflow and relationship between each 
sub-project are shown in bellow. Details of integrated 
project and five sub-projects will be addressed in the 
following chapters. 
 
 F
Fig. 4 Movin
step [4] 
In order t
the practicali
degrees of fr
The first deg
can make the
increase the 
degree of fre
change the 
useful and co
different heig
Fig. 
C. Robot H
We keep
movable ran
Because the 
hand is very
purposes ope
hand is refer
degrees of fr
It’s a simplif
finger’s joint
angle of fing
three fingers
 
 
 
 
ig. 3 Chassis’
g strategy on
o increase the
ty of the robo
eedom on the 
ree of freedom
 robot stoop. 
working spa
edom as sho
robot’s heigh
nvenient whe
hts. 
5 Two DOF a
and Design 
 the original 
ge of the tw
gripping abi
 limited, it 
ration. Degr
ence to the 
eedom on the
y model from
, these joints
ers. DOF-4~
. 
 
 degrees of fr
 rough terra
 work space 
t to fit our n
lower body a
 indicated by
The two arms
ce of the ha
wn in red arr
t. The degre
n the robot p
dded in the lo
design of th
o arms is li
lity of the p
is re-design
ee of freedom
real human h
 new hand as
 the humans.
 can change 
5 are for ope
eedom 
in of ascendin
of two hands
eeds, we add
s shown in Fi
 the cyan nar
 can significa
nds. The sec
ow in Fig. 5
e of freedom
ick up object 
 
wer body 
e robot arm. 
sted in Tabl
revious desig
ed for the m
 of new rob
and. There a
 shown in Fi
 DOF-1~2 for
the direction
ning and clo
3 
 
 
g a 
 and 
 two 
g. 5. 
row 
ntly 
ond 
 can 
 is 
with 
The 
e 2. 
ned 
ulti 
otic 
re 5 
g. 6. 
 the 
 and 
sing 
clev
this
colu
robo
 
 C
 Cyl
Fig.
desi
  
han
the 
ope
clos
han
  
deg
the 
are 
are 
chan
rang
Tab
han
Table 2
θ1 
θ2 
θ3 
θ4 
θ5 
θ6 
θ7 
DOF 2
Fig. 6 A
We focus the
er arrangeme
 robotic hand
mn, slice an
tic hand is 65
 
ube Bot
 
inder-3 Square
 7 Simulati
gned hand 
Another desi
d which is a 
grasping is 
rations, the fo
ely to the re
d to perform m
The new des
rees of freedo
same as the th
different to t
opening and c
ge the direct
es for each D
le 3 Moveabl
d 
θ1 
θ2 
θ3 
θ4 
θ5 
 Moveable ra
Movable R
-180
-17°
-180
-10°
-180
-47
-115
DOF 
DOF 
DOF 1
rrangement o
 design on th
nt of the 5 de
 can take cub
d wand. Th
0g. 
 
tle-1 Bottl
 
 column Ba
on results o
gn is differe
new type of f
used in mo
ur fingers ha
al human’s h
ore human o
ign of the f
m as show in
ree fingers d
he three finge
losing five fi
ion and angl
OF are listed
e range of ea
Movable R
90°
0°
0°
0°
90°
nge of two ar
ange of Arm
° ~ 180° 
 ~ 196° 
° ~ 180° 
 ~ 130° 
° ~ 180° 
° ~ 47° 
° ~ 47° 
5
4
f 5 DOF of h
e ability of 
grees of free
e, bottle, cyl
e maximum 
 
e-2 Cylinder-
 
r Screw 
f grasping 
nt from the 
our fingers h
st of the hu
nd is designe
and size whic
perable mani
our fingers 
 Fig. 8, and th
esign, but DO
rs hand desi
ngers, and the
e of fingers. 
 in Table 3. 
ch DOF of th
ange of Han
 ~ 160° 
 ~ 90° 
 ~ 90° 
 ~ 90° 
 ~ 180° 
ms 
 
DOF 3
 
and 
grapping. By
dom of hand,
inder, square
load of this
 
1 Cylinder-2
 
Key 
by 3-fingers
three fingers
and. Because
man’s daily
d to be more
h allows the
pulation. 
hand have 5
e degrees are
F definitions
gn, DOF-1~4
 DOF-5 can
The movable
e four fingers
d 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5 
 
could work 24 hours with highly precise and stable. 
There are many researches focusing on this application. 
[9-12] has focused on how to successfully grasp a 
defined object. However in the robot application robot is 
not only pick or grasp an object, after that process, the 
robot has to do the specific task. To do a specific task 
those robots discussed in [9-12] need to re-programming. 
This paper is different to those discussed paper. This 
paper will integrate a specific task in one program 
function called Parameter based function. Parameter 
based for industrial robot is the program that receives the 
output data of the image processing part and the data 
input from an interactive user and commands the task for 
industrial robot as shown in Fig. 12. By using the 
parameter based programming, the robot can reproduce 
or duplicate the same task with another complicated 
object without re-programming. The data of the image 
processing are the location (O), the grasping point (A) of 
the objects, the location of the target task (B) or the 
position of the circle obstacle (C). The data input from an 
interactive user are illustrated by the direction of the 
engagement or other commands. 
 
Fig. 12 Data input for the parameter based program 
After received the data input, by solving the inverse 
kinematic, the robot will grasp the object at A point and 
according to the direction command of the user, the robot 
will move the object to B point. On the way to approach 
the B point. The robot will automatically avoid the 
obstacle. The trajectory to move the robot to the B point 
and the method to avoid the obstacle are discussed in the 
next section. 
1) The Trajectory Planning and Avoidance the 
Obstacle 
The trajectory planning: For moving the object from A 
point to B point, the straight line trajectory is constructed 
as: 
 
 
(1) 
 
 
with x1, y1, z1 are the coordinate of the A point and x2, y2, 
z2 are the coordinate of the B point and t is the constant. 
2) Fuzzy avoidance obstacle:  
To avoid the obstacle during the moving from point 
A to B point the fuzzy control rule is constructed. The 
fuzzy control rules is one of the famous control 
algorithms with fast response and fairly good 
performance [7]. It is already presented and studied in 
[8], [9]. The fuzzy control rule in this research is 
constructed based on the relative position (epi) between 
the end-effector and the position of the obstacle as shown 
in Fig. 13. In this paper the fuzzy logic controller has two 
inputs, the position error (epi) and the change of the error 
(edi) and one output. The input and output (µ(epi), µ(edi), 
µ(Ku)) fuzzy membership functions were selected 
symmetrically as shown in Fig. 14 to 15. 
Fig. 13 Relative position between the end-effector and 
the position of the obstacle 
pie
( )pieμ
die
( )dieμ
 
Fig. 14 (Left) The membership function of the position 
error (Right) The membership function of the change 
rate of position error. 
( )Kuμ
u
 
Fig. 15 The membership function of output signal 
According to 3 fuzzy subsets of epi and 3 subsets edi, 
we have 9 rules and therefore the rule base is represented 
 
2 1 2 1
1 1 1
2 1 2 1
2 2 2
2 1 2 1
( )
( , , )
( )
( , , )
( )
x x t x x
A x y z
y y t y y
B x y z
z z t z z
= + −⎧⎫ ⎪⇒ = + −⎬ ⎨⎭ ⎪ = + −⎩
7 
 
 
 
 
 
 
 
 
 
Fig. 21 Loose fitting sub-routine 
V. A Preliminary Study on Robot Visual 
Servo with Obstacle Avoidance 
Visual servo, a popular technique widely used in 
robotics, is a vision based algorithm which can 
autonomously guide the robot effector to the desired 
location. Because of its effectiveness, visual servo has 
been applied to perform hand-eye coordination tasks 
such as grasping in humanoid robots and dual-arm 
service robotic manipulators. However, it is very difficult 
to perform tasks subject to obstacles if just using classic 
visual servo methods. To this aim several visual servo 
via advanced numerical methods have been recently 
reported and combining visual servo and obstacle 
avoidance algorithms together to solve this difficulty is 
one promising research direction. This article presents 
some initial results on using a classical position based 
visual servo method combined with an artificial potential 
field path planning method to avoid known obstacles and 
reach the goal position. Simulation based on a two link 
robot manipulator is provided to demonstrate the 
effectiveness of the presented method. 
A. Visual Servo 
1) Introductions of Visual Servo 
In order to develop the techniques of robotic obstacle 
avoidance based on machine vision. We control our 
robot hand under visual servo structure. Visual servo is a 
technique which uses feedback information extracted 
from a vision sensor to control the motion of a robot. 
There are two common classifications of control 
configuration named “Position Based Visual Servo” and 
“Imaged Based Visual Servo”. 
 The differences between the two control 
configurations are the reference inputs. Prior case uses 
coordinate error in 3D space as reference input as shown 
in Fig. 22, while the other uses error signals in 2D image 
plane as reference input as shown in Fig. 23. 
Fig. 22 Position based visual servo configuration. 
Fig. 23 Image based visual servo configuration. 
2) Position Based Visual Servo 
Position-based visual servo defines its reference input 
as the relative position and orientation between the 
object and the robot hand. The relative position and 
orientation are recovered from the images obtained by 
the fixed camera. Since the robot control problem itself 
is classical and well established, we adopt our system 
under PBVS. 
Usually, the object feature points are extracted from 
the image and feature point positions in the image plane 
are used for computing the object coordinates in 3D. The 
error signal between robot hand coordinate and camera 
coordinate is the reference input of the control loop. The 
feedback signal is the estimated hand position which is 
computed on the basis of joint angle measurement. With 
proper computation, robot hand moves to precise 
position when the joint controller receives position 
commands and finally reaches the goal position. 
3) Fixed Eyes Configuration 
II. System based on “Eye-to-Hand” configuration 
locates camera rigidly within robot manipulator’s 
workspace as shown in Fig. 24. That is to say, camera 
can’t change the position and orientation arbitrary. 
Since the transformation matrix between the original 
point of robot manipulator and camera is fixed, it’s 
relatively easy to estimate the pose of robot 
manipulator. 
 
Fig. 24 Fixed eyes configuration 
4)  System Description 
In order to apply path planning method to visual 
servo system, a simple example is given [13]. The aim of 
this simulation is to move the robot hand avoiding 
obstacles and to reach the goal position. 
The cameras are static. Assume that the optical axes 
of the cameras are parallel each other and let the 
direction of the optical axes be z . The image plane is in 
the x y− plane and the robot moves in the x y− plane. 
Let the base line between the cameras be b , the position 
of the hand and the goal in the left camera’s image be 
hl and gl  respectively. Also, let the positions of the 
hand and the goal in the right camera’s image be hr and
gr . Suppose the joint angles are 1 2( , )θ θ θ= , two link 
length are 1 2( , )a a , hand position ( , )x zh h h= , goal 
position ( , )x zg g g= as shown in Fig. 25. 
 
   
   
Surface A Surface B Surface C Surface D Loose fitting
9 
 
C. Simulation results 
1) Simulation setup 
This research combines Position based visual servo 
and Potential Field Methods. The whole system is under 
position based visual servo. With proper computations, 
system receives specific command which includes vector 
information and current position of the robot hand and 
then robot move to next position.  
For the purpose to realize the previous methods, we 
firstly simulate the system with Matlab Simulation as 
shown in Fig. 27. 
Fig. 27 System block diagram 
2) Simulation result 
First, simulating the visual servo system without 
potential field methods, any system coefficients are 
shown in Table II. Trajectory of end of effector is shown 
in Fig. 28. 
 
Table 6 
COORDINATE I 
Initial Point (1.1,0) 
Obstacle Point (0.55,0.55) 
Desire Goal Point (0,1.1) 
 
 
Fig. 28 Simulation result without potential field method. 
 
Robot hand without path planning moves toward the 
goal position and didn’t avoid obstacle. However, robot 
would bump into the obstacle in real condition. Then we 
combine system with potential field method. Any 
coefficients are been given as the previous cases. The 
results are shown in Fig. 29, Fig. 30. Robot hand actively 
avoids the obstacles before it touch them and then 
approaches the goal position as shown in Fig. 29. The 
angles change with time as shown in Fig. 30.  
 
Fig. 29 Simulation result with potential field method. 
 
Fig. 30 Robot joint angle plot (with an obstacle). 
 
Finally, we concern enormous obstacles occurring in 
next condition and set all the coefficients artificially as 
shown in Table 7. Robot hand effectively avoids all of 
the obstacles with the potential field methods and 
reaches the goal position as shown in Fig. 31. The joint 
angle is varying with time are shown in Fig. 32. 
Table 7 
COORDINATE II 
Initial Point (1.1,0) 
Obstacle Points (0.55,0.55)、(0.8,0.3) 
Desire Goal Point (0,1.1) 
 
Fig. 31 Simulation result with multi-obstacles. 
11 
 
The dice images are acquired in single channel. The 
gradient and its orientation is calculated at each pixel 
within a detected MSER region. The orientation is 
segmented into 8 major directions, forming 8 bins and 
each with gradient magnitudes summed up across the 
whole MSER region. This leads to an 8-bin histogram 
and thus an 8 dimensional vector for the OMSER. 
However, the OMSER obtained from non-dice area 
reveals a completely different pattern. Therefore, a 
threshold can be easily determined to remove the 
detected non-dice areas. Fig. 35 shows the result of dice 
detection after OMSER selection. Fig. 36 shows some 
example of dice detection with different types of dice 
and illumination conditions. The MSER detector extracts 
the regions relatively robust to illumination variation. 
When applied on a dice image, the dice regions, possible 
specular reflection and shadows on the dice roller or 
background are prone to be detected. It is almost always 
the case that the dice regions detected reveal a shape 
formed by one or, more often, multiple squares, 
especially when the dices are close to each other. To fast 
discriminate the dice regions from other MSER detected 
regions, the oriented MSER (OMSER) which extracts 
the gradient orientation from the detected MSER and 
forms an 8-dimensional feature vector is proposed. This 
feature is similar to HOG (Histogram of Oriented 
Gradient) [15] where an edge orientation histogram is 
obtained as the primary feature for justifying similarity. 
The dice images are acquired in single channel. The 
gradient and its orientation is calculated at each pixel 
within a detected MSER region. The orientation is 
segmented into 8 major directions, forming 8 bins and 
each with gradient magnitudes summed up across the 
whole MSER region. This leads to an 8-bin histogram 
and thus an 8 dimensional vector for the OMSER. 
However, the OMSER obtained from non-dice area 
reveals a completely different pattern. Therefore, a 
threshold can be easily determined to remove the 
detected non-dice areas. Fig. 35 shows the result of dice 
detection after OMSER selection. Fig. 36 shows some 
example of dice detection with different types of dice 
and illumination conditions. 
 
 
 
 
 
 
 
 
Fig. 35 (Left) All the OMSER detected in the image with 
its 8-bin orientation histogram. (Right) Result after 
OMSER selection. 
 
 
Fig. 36 Samples of dice detection on two types of dice 
with various illumination conditions. Top row is the 
original images and bottom row is the result of detection. 
2) Homography Rendering 
After the dice is localized by orientation-aided MSER, 
the aforementioned 8 corner-like and blob-like local 
invariant feature detectors are applied to find 
correspondences between different viewpoints. The 
performance of evaluation is compared by the ground 
truth obtained using manually selected correspondences. 
All of the invariant regions (or interest regions) are 
represented in the form of SIFT descriptor [16] as it is 
experimentally proven as one of the most effective 
descriptors among others [17]. The match of the 
invariant features across views is measured by the 
Euclidean distance between the feature descriptors, and a 
threshold on this distance measure is determined to select 
correspondences. Because a dot on a die in a given view 
can appear quite similar to a different dot in another view, 
the scale factor in the local feature detectors is first 
chosen as that comes with the maximum number of 
correct correspondences. RANSAC [18] is then applied 
to filter out outliers and determine the most appropriate 
homographies across different views with matched 
correspondences. Our experiments reveal that the 
multi-scale Harris-Hessian [19] detector gives the best 
performance. Fig. 37 shows an example of the 
correspondences across two viewpoints obtained using 
this detector. 
 
 
Fig. 37 The left column shows the dice images from two 
different views. MSER is applied to localize the dice 
region in the middle column and the correspondences of 
the local invariant features are detected by a multi-scale 
Harris-Hessian detector which is shown in the right 
column. 
3) Dice Segmentation 
Given N different viewpoints of dice images, 
N(N-1)/2 homographies would be obtained using the 
invariant feature correspondences. In most cases 2 ≤ N ≤ 
13 
 
illumination conditions. Dice identification rate and dice 
number recognition on the test set under 9 illumination 
conditions is performed. 
v. Homography Rendering Accuracy: 
In order to avoid false correspondences caused by the 
same dice pattern dice, we assume N dice are placed with 
different dot numbers. The error ܧி೔ is measured by the 
difference between the correspondences from the 
invariant-feature-based homography ܪி೔  and the 
ground-truth ܪீ  obtained using manually selected 
correspondences, i.e., 
 EF౟
ሺୟ.ୠሻ ൌ
ቛቀHF౟
ሺ౗.ౘሻିHG
ሺ౗.ౘሻቁxF౟
ౘ ቛ
NF౟
 (10) 
where ܪி೔
ሺ௔,௕ሻ  is the homography that transforms the 
invariant features xி೔
௕  detected by the invariant feature 
detector ܨ௜ in the image ܫ௕ to the corresponding ones 
in ܫ௔; ܪீ is the ground-truth homography obtained by 
manual selected correspondences between ܫ௔  are ܫ௕ , 
ிܰ೔ is the number of features detected by ܨ௜, and a, b 
denote two different viewpoints. 
Additionally, it is also desired that the 
correspondences from the feature-based homographies 
can be consistent across different scales, as some features 
change with scales. To investigate what features are 
better than others in rendering desired homographies 
across illumination and scale, the original images in 
320 ൈ 240 pixels are scaled down to smaller sizes, and 
the error is computed in each size and averaged over the 
three illumination conditions in the training set. Fig. 40 
shows this comparison, the smallest scale with 
128 ൈ 96 reveals relatively high errors, indicating that 
some details between the dice are lost in such a small 
scale and thus the accuracy in the homography 
estimation is degraded. The PC for the experiment is 
2.4GHz with 3.25GB RAM running under Windows 
MATLAB program. Process time for homography 
estimation in each image pair for 320 ൈ 240, 256ൈ 192, 
192ൈ 144  and 128ൈ 96  image resolution are 2.42 sec, 
2.27 sec, 1.77 sec and 0.55 sec respectively. 
Although many affine invariant detectors can handle 
the projection deformation caused by different 
viewpoints, it relies on an assumption that the camera is 
relatively far from the object, making the affine 
invariance valid. In our case the cameras are not far 
enough from the objects (dice), the non-coplanar features 
can be mistaken as coplanar ones by the affine invariant 
detectors, and therefore they are outperformed by other 
invariant detectors. Among the eight invariant feature 
detectors we tested, the multi-scale Harris-Hessian 
detector gives the lowest error at 0.87%, and it is about 
1.7 pixels in a 192ൈ 144  image. 
  
Fig. 40 Normalized error of feature-based homography 
across scales and three illumination conditions. 
Fig. 41 Identification rates in 9 illumination conditions, 
indexed from 1 to 9; at each index the black bar on the 
left shows the rate of dot identification, and the white bar 
on the right shows the rate of dice number identification. 
vi. Dots Identification and Dice Recognition 
The performance evaluation on the 9 test sets reveals 
the following observations and results: 
z As long as the correspondences from the 
feature-based homography are consistent over at 
least two scales, the average match error can be 
kept below or near 1%, and the top faces of dice 
can be perfectly segmented in all tested conditions. 
z Two identification rates are measured in each test 
illumination condition, one is the identification of 
the dots and the other is the identification of the dot 
number on each die. The former is shown by the 
bar on the left and the latter by the bar on the right 
at each indexed illumination condition in Fig. 8. 
The combination of size-constrained clustering and 
spatial pattern confirmation can effectively remove the 
false positives and yield superb dice recognition rates in 
all tested conditions, as shown by the right bar at each 
indexed illumination in Fig. 41. 
vii. Performance under various viewpoints 
From the previous experiment result, we have a 
superb recognition rate against various illumination 
conditions. In this experiment the comparison between 
different camera viewing angles is performed with 9 
various illumination conditions each. Dots identification 
is shown in ROC curve with varying MSER size 
constraint as shown in Fig. 41 to 47. 
15 
 
execute a task. The specifications of dual-arm robot are 
shown in table 8. 
The right arm consists of Mitsubishi and Maxon 
servo modules. Harmonic drivers were selected for 
reducers because of low backlash. The force sensor is 
installed in the wrist of right arm and optical sensors are 
used to initialize the home position. The left arm consists 
of Maxon and FAULHABER servo modules. Those 
motors are combined with planetary reducers. In the 
aspect of mechanism design of the left hand, spiral bevel 
gear was chose as transmission mechanism to change the 
driving direction of actuators which make it look more 
similar to a human arm. 
 
Fig. 48 Dual-arm robot system 
Table 8 
Specifications of the two arms 
 Right arm Left arm 
Weight (kgf) 51.0 12.5 
Length (mm) 690.5 630.0 
Payload (kg) 2.0 1.0 
B. Geometric kinematics 
1) Direct kinematics 
In order to control the position and orientation of the 
end-effector of manipulators, it is necessary to establish 
the kinematics model. Firstly, Denavit-Hartenberg (D-H) 
method was used to define the coordinate of each link as 
Fig. 49. Table 9 and Table 10 show link D-H parameters. 
 Fig. 49 Coordinate diagram of links 
Table 9 
Link D-H parameters of right arm 
i θR,i dR,i aR,i αR,i 
1 θR,1 0 0 -90∘ 
2 θR,2 0 aR,2 0∘ 
3 θR,3 dR,3 0 90∘ 
4 θR,4 dR,4 0 -90∘ 
5 θR,5 0 0 90∘ 
6 θR,6 dR,6 0 0∘ 
Table 10 
Link D-H parameters of left arm 
i θL,i dL,i aL,i αL,i 
1 θL,1 0 0 90∘ 
2 θL,2 0 aL,2 0∘ 
3 θL,3 0 0 90∘ 
4 θL,4 dL,4 0 -90∘ 
5 θL,5 0 0 90∘ 
6 θL,6 dL,6 0 0∘ 
 
After establishing link D-H parameters, direct 
kinematics of the left arm was developed by using the 
homogeneous transformation matrix. Direct kinematics 
was used to transform joint positions (joint space) into 
the position and orientation of end-effector (Cartesian 
space). The matrix Ti as (11) is a general type for 
transforming adjacent coordinate. 
 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
−
−
=
1000
cossin0
sincossincoscossin
cossinsinsincoscos
iii
iiiiiii
iiiiiii
i d
a
a
αα
θθαθαθ
θθαθαθ
T
 (11) 
Let 6,...,2,1=i , substitute i into (11). The result is 
shown as follow: 
 
17 
 
Fig. 50 Planned trajectory and checking calculation (left 
arm) 
Fig. 51 Position error of x and z coordinate (left arm) 
Fig. 12 Joint position of left arm 
Fig. 53 Planned trajectory and checking calculation 
(right arm) 
Fig. 54 Position error of x and z coordinate (right arm) 
Fig. 55 Joint position of left arm 
 
 
19 
 
The Features of Left-Hand -
Side (LHS) Web Camera
The Features of Right-Hand-
Side-Web (RHS) Camera
Neural Network 
Model of Stereo 
Vision System
Depth of Each 
Feature（Z）
Training Patterns from
Motion Capture 
Systems
Fig. 59 Neural network based stereoscopic vision system 
architecture. 
P*
P1
P2
OStereoscopic 
Vision Module
Motion Capture 
System
Fig. 60 Experimental setups for training stages. 
The coordinate system of the stereoscopic vision 
module is defined as shown in (18). The scalar projection 
of 
*
uuuuv
OP  onto the coordinate system of the stereoscopic 
vision module is indicated in (19). In this manner, the 
spatial coordinates (x, y, z) represent the position of 
point P* in the coordinate system of the stereoscopic 
vision module. It is noted that the global coordinate 
system of the motion capture system [22] is not directly 
used in the neural network model 
     1 2 1 2
1 2 1 2
, ,
×= = = ×
uuuv uuuuv uuuv uuuuvv v v
uuuv uuuuv uuuv uuuuvOP OP OP OPX Y Z
OP OP OP OP
 (18) 
      * * *, , .= ⋅ = ⋅ = ⋅uuuuv uuuuv uuuuvv v vx OP X y OP Y z OP Z  (19) 
 
The backpropagation neural network (BPN) is used 
to establish the nonlinear parametric relationships 
between the inputs (cameras’ coordinates) and outputs 
(spatial coordinates relative to the coordinate system of 
the stereoscopic vision module). The BPN is a 
supervised learning algorithm. It is popularly used in 
various neural network applications. In this paper, the 
BPN is formed as different layers, including input layer, 
hidden layers and output layer. A typical BPN structure 
is shown in Fig. 61. 
h
Inputs First layer Second layer N-th layer Outputs
Input layer Hidden layer Output layer
 
Fig. 61 Architecture of a typical BPN structure. 
The input of the neural network is the pixels of the 
target P* in both cameras’ coordinate systems and the 
output is the scalar projection of *
uuuuv
OP  in the vision 
coordinate system, which can be represented by  x, y 
and z. The experimental flow chart is shown in Fig. 62. 
From motion capture
From stereoscopic 
vision module
O (x, y, z)
P1 (x, y, z)
P2 (x, y, z)
P* (x, y, z) * 1000
(unit mm)
Camera 1 (px1, py1)
Camera 2 (px2, py2)
(unit pixel)
Relative coordinate 
of vision system
the scalar project of 
OP* onto X, Y and Z
* 1000
px1
py1
px2
py2
BPN
X’
Y’
Z’
 Fig. 62 Experimental flow chart. 
C. Experimental result and discussion 
1) Orientation sensor 
For testing the performance of the orientation sensor 
we developed in this research, we built a robot platform 
and installed the south-pointing chariot underneath the 
platform as shown in Fig. 63. Two toy servo motors were 
used to drive the two wheels of the differential drive. The 
wheels of the south-pointing chariot and the wheels of 
the differential touch the ground simultaneously. 
 
Fig. 63 Robot platform with the orientation sensor. 
 they are desc
neural netwo
consequence
testing of th
The BPN wa
Fig. 71 show
(MSE) of th
pixels resolu
is within 25 m
Fig. 67 
Layer 
1 
2 
3 
Fig
Fig. 69 Mea
ribed as show
rk architectu
, the regressi
e BPN mode
s also validat
s the test res
e near distan
tion, the posit
m in a 200 m
Robot platfor
Ta
Properties of
Number of Neu
5 
5 
3 
. 68 BPN mod
n squared er
n in table 1
re is shown 
ons of trainin
l are shown 
ed in terms o
ult of the m
ce data. Wit
ioning error o
m measurem
m with two w
ble 11 
 Different La
rons 
el for near di
ror of trainin
1. In addition
in Fig. 68. A
g, validation 
in Figs. 69 –
f the testing d
ean squared e
h the 320 × 
f a spatial ma
ent range. 
eb cameras.
yers 
Transfer Function
TANSIG 
PURELIN 
PURELIN 
stances. 
g, validation 
21 
, the 
s a 
and 
 70. 
ata. 
rror 
240 
rker 
 
 
 
 
and 
testi
uses
sam
The
show
the 
train
show
term
the 
reso
with
Fi
La
ng.  
With the far 
 the feed-forw
e training fun
 number of la
n in Table 
same as Fig. 
ing, validati
n in Figs. 7
s of the testi
MSE of the 
lution, the p
in 80 mm in 
g. 70 Regress
Fig. 71 Ne
Pro
yer Num
1 
2 
3 
distance set, 
ard backpro
ction and the
yers is also 3
12. The neur
13. As a cons
on and testin
2 – 73. The B
ng data. Fig. 
far distance d
ositioning err
a 1000 mm m
ions of trainin
ural network 
Table 
perties of Dif
ber of Neurons
10 
10 
5 
RMS (c
the neural ne
pagation. We
 adaption lear
, and they are
al network a
equence, the r
g of the BP
PN was also
74 shows the
ata. With the
or of a spat
easurement r
g, validation 
model testing
12 
ferent Layers 
 Trans
T
PU
PU
m)
twork model
 still used the
ning function
 described as
rchitecture is
egressions of
N model are
 validated in
 test result of
 same pixels
ial marker is
ange.  
and testing. 
 result. 
fer Function 
ANSIG 
RELIN 
RELIN 
 
 
. 
 
 
 
 
 
 
 
 
23 
 
[3] Fanuc’s M430-iA Robot, 
http://hk.youtube.com/watch?v=Q8WnAN9jmEc. 
[4] Nakajima, S., “Concept of a Novel 
Four-wheel-type Mobile Robot for Rough Terrain, 
RT-Mover,” Intelligent Robots and Systems, pp. 
23257–3264. 
[5] Nakajima, S., “Development of Four-wheel-type 
Mobile Robot for Rough Terrain and Verification 
of Its Fundamental Capability of Moving on 
Rough Terrain,” International conference on 
Robotics and Biomimetics, pp. 1968-1973. 
[6] Nakajima, S., “Proposal for Step-up Gait of 
RT-Mover, A Four-Wheel-Type Mobile Robot for 
Rough Terrain with Simple Leg Mechanism,” 
International Conference on Robotics and 
Biomimetics, pp. 351-356. 
[7] Nakajima, S., “Kinematics for A Rough Terrain 
Mobile Robot to Climb Up a Step,” Emerging 
Trends in Mobile Robotics, pp. 825-833 
[8] Takaki, T.,  Omata, T.,  "High-Performance 
Anthropomorphic Robot Hand With 
Grasping-Force-Magnification Mechanism",  
Mechatronics, IEEE/ASME Transactions on, 2010 
[9] Geoffrey. T and Lindsay. K, “Grasping Unknown 
Objects with a Humanoid Robot”, Proc. 2002 
Australian Conference on Robotics and 
Automation, Auckland, 2002. 
[10] Gary. M. Bone, Andrew. L, Mark. E, ”Automated 
Modeling and Robotics Grasping of Unknown 
Three-Dimensional Objects” ,IEEE International 
Conference on Robotics and Automation, USA, 
2008. 
[11] Benjamin. B, Stefano .C, “IEEE-RAS International 
Conference on Humanoids Robots”, USA, 2010. 
[12] Jeannette. B, Danica. K, “Learning grasping points 
with shape context”, Robotics and Autonomous 
system, 2010, pp. 362-377. 
[13] Koichi Hashimoto, “A review on vision-based 
control of robot manipulators”, Advanced Robotics, 
Vol. 17, No. 10, 2003, pp. 969–991 
[14] Khatib O, “Real-time obstacle avoidance for 
manipulators and mobile robots,” Int J Robot Res 
5(1),1985, pp. 90–8. 
[15] N.~Dalal, B.~Triggs, Histograms of oriented 
gradients for human detection, in: CVPR (1), 2005, 
pp. 886--893. 
[16] D.~G. Lowe, Distinctive image features from 
scale-invariant keypoints, International Journal of 
Computer Vision 60~(2) (2004) 91--110. 
[17] K. Mikolajczyk, C. Schmid, A performance 
evaluation of local descriptors, IEEE Trans. Pattern 
Anal. Mach. Intell. 27(10) (2005) 1615-1630. 
[18] M.~A. Fischler, R.~C. Bolles, Random sample 
consensus: A paradigm for model fitting with 
applications to image analysis and automated 
cartography, Commun. ACM 24~(6) (1981) 
381—395. 
[19] K.~Mikolajczyk, C.~Schmid, An affine invariant 
interest point detector, in:  ECCV, 2002, pp. 
128--142. 
[20] J.~Matas, O.~Chum, M.~Urban, T.~Pajdla, Robust 
wide baseline stereo from maximally stable 
extremal regions, in: BMVC, 2002. 
[21] N. John. "South pointing chariot", 
http://odts.de/southptr/gears/nuttall.htm. 
[22] A. Doulamis, N. Doulamis, K. Ntalianis, and S. 
Kollias, “An Efficient Fully Unsupervised Video 
Object Segmentation Scheme Using an Adaptive 
Neural-network Classifier Architecture,” IEEE 
Transactions on Neural Networks, Vol. 14, No. 3, 
pp. 616 – 630, 2003. 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：林其禹 計畫編號：99-2221-E-011-094- 
計畫名稱：下世代服務型機器人快速工作定義和全自主執行技術--總計畫：下世代服務型機器人快速
工作定義和全自主執行技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 1 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
