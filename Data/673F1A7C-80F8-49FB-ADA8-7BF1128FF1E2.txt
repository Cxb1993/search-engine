Z`
ÍijE5÷P¢ó£Ý]°×@~55÷P¢ó£ÎPa? ç­Ý'
TàÝ×ÍÃÍv¥Ý®ÞôÎÜæ*r§Ý×Í¥lP&Æ.0v;
5÷P¢ó£Ýí]0-mean squared errorÝó.îP9£T
fusion centerÎ¿àJÎ;TD¹£]Õ£ô®¢Âà¼f´¿à
;TD¹Ý£]ÿÝ£Q¡&Æ5¬ÿaØ°¿à×-®¢ó£Ý]
°9°5÷P¢ó£Ý]°&ðÊ)TàÕËÑÝPa? ç­
n"ÞPa? ç­5÷P¢ó£
Abstract
In this report we will analyze the distributed estimation that is a fundamental and important
problem in wireless sensor network. Our problem is basically a parameter estimation problem.
We derive some closed-forms of the mean-squared errors for which the observation is transmitted
to the fusion center without compression. These results serve a lower bound of the compressed
estimation. We then analyze and simulate two schemes of 1-bit estimation. Simulation results is
also provided.
Keywords: Wireless sensor network, distributed estimation
1 Introduction
The wireless sensor networks usually deploy many inexpensive sensors with very limited dynamic
range, resolution, power, reliability, and life time. Moreover, the communication link between
these sensors to a central sensor (the fusion center) may be also physically restricted. In such
situations, data compression or quantization carried out on the local sensors are necessary. In
fact, data compression must be considered and integrated into the design phase of the sensors as
well as the fusion centers. Besides, the data observed by the local sensors are usually corrupted
by noises whose distribution is hard to be analyzed in practically. As a result, a main challenge
in sensor network research is to develop distributed detection or estimation schemes for which the
sensor noise is unknown under very limited channel bandwidth.
The scheme of the distributed and the analysis of the performance have been reported in some
papers. In [1], the Crame`r-Rao lower bound is derived if the fusion center receives 1-bit message
from the local sensor. This paper also show that if the observation noise is within the range
of [−U,U ], then the mean-squared error of any one-bit distributed estimator is at least U2/N
where N is the sensor number. The distributed estimation is investigated from the view point
of information theory in [2]. A general distributed estimation of bandwidth constrained message
2
the fusion center uncompressed. That is, we do not need to design the local estimation function
mi because
mi(xi) = xi, i = 1, 2, . . . , N. (3)
Moreover, we assume a noise-free channel. When the fusion center receives these messages, it can
perform a simple estimation of θ by the following sample-mean estimation
θˆ =
1
N
N∑
i=1
xi. (4)
The mean squared error of the estimation is
E[(θˆ − θ)2] =
σ2
N
(5)
where σ2 is the variance of the noise random variable
σ2 =
∫
U
−U
u2p(n)dn. (6)
Suppose that our target mean-squared error is ǫ2 with ǫ > 0, then the sensor number that we need
to achieve the target based on the sample-mean estimator is
N ≥
σ2
ǫ2
. (7)
It is worth noting that the mean-squared error (5) is of order 1/N . In many cases, this is the
best we can do. The sample mean estimator is independent of the noise pdf p(u). That is, this
estimator is universal.
In the following, we will analyze the special case of that the noise pdf p(n) is uniformly
distributed over [−U,U ]. Although the estimator is not universal, the result may be served as a
lower bound of the universal estimator. Moreover, our analysis provides the results for which the
Crame`r-Rao lower bound can’t be applied.
2.2 Closed-form MSE of some estimators under uniformly distributed
noise
Suppose that the noise ni is uniformly distributed in [−U,U ] with p(n) = 1/(2U), −U ≤ n ≤ U .
We have the following results. Also assume that the channel is noise-free. We have yi = mi,
i = 1, 2 . . . , N .
4
We call the estimator as the sign estimator. Let us consider the case where the noise is uniformly
distributed again. It is easy to show that
P [mi = 1] =
U + θ
2U
P [mi = 0] =
U − θ
2U
and E[mi] = (U + θ)/2U . An unbiased estimation performed on the fusion center is
θˆ = −U +
2U
K
N∑
i=1
mi. (14)
It can be shown that the MSE is constrained by
E[(θˆ − θ)2] ≤
U2
N2
. (15)
Next we consider another estimation scheme. Let the sensor number be 2K for simplicity to
describe. Assume that the parameter to be estimated can be normalized and represented by a
B-bit fixed-point binary number. Specifically,
θˆ = A+ C × 2−B
B−1∑
n=0
bn2
n (16)
where A and C are constants for normalization. It is surely that there may exist difference between
θˆ and θ. However, it is rare for practical situation that an estimation of infinite precision is needed.
We divide the local sensors into B groups. Each group of sensors estimates one bit of θˆ. Suppose
that there are Pn sensors in the nth group of sensors that perform estimation on bn. The estimation
is based on a simple sample-mean estimation. We according to the following rule to assign the
number of sensors in each group: Pn = 2Pn−1. That is, the number of sensors used for estimating
bn is twice as the number of sensors used for estimating bn−1. Specially, we let P0 = P1. Therefore,
the number of sensors in each group satisfies
P0 + P0 + 2P0 + 4P0 + · · ·+ 2
B−2P0 = 2
K . (17)
We have
2B−1P0 = 2
K , P0 = 2
K−B+1. (18)
Before the local sensor performs the quantization of the observation, it must be normalized to
the rage of [0, 1] for representing by a binary number. Suppose that the parameter to be estimated
is within the range of [−V, V ] and the noise is distributed in [−U,U ]. Then the observation in
normalized by
x¯i =
xi + U + V
2U + 2V
. (19)
6
iWÝ
In this research we focus on the problem of distributed estimation that is fundamental and
important in wireless sensor network. We derive some analytical expressions for mean squared
error of estimators and obtain some simulated results. We can base on these results to perform
further research in the future.
8
−10 −8 −6 −4 −2 0 2 4 6 8 10
−2
10
−1
10
0
10
θ
M
SE
Figure 3: The MSE of the bit estimators by various bit numbers. “◦”: 6 bits. “△”:8 bits. “∗”:10
bits. The sensor number is 215. The noise is uniformly distributed over [−20, 20]. The MSE
decreases as the bit number increased. This is reasonable. However, from the figure we can see
that the MSE decreases less from 8 bits to 10 bits than from 6 bits to 8 bits.
10
Speech Processing 573
Image & Multidimensional Signal Processing 465
Signal Processing Theory and Methods 443
Signal Processing for Communications 382
Sensor Array & Multi-channel Signal Processing 166
Audio & Electroacoustics 161
Machine Learning for Signal Processing 141
Spoken Language Processing 99
Multimedia Signal Processing 97
Information Forensic Security 94
Bio Imaging and Signal Processing 91
Design & Implementation of SP Systems 69
Industry Technology Track 52
Signal Processing Education 11
ãî:9ó¡Zí/y*r§Ý§¡@~9ôÎÍºÈ©&Æôã
ºÈ¡ZsÝgH4:êG*r§@~]'
2
«&!×gÝbL§.oÝVaidyanathan>0sÞ“Some Properties of
IIR Power-Symmetric Filters”Ý¡ZVaidyanathan>0Î*r§Ý/sóyS
¡Z¬bÝhÌ4Q\ã¡Zÿá9/¬Î9ÎÏ×gÕÍ
ßVaidyanathan>0Í²áZ-b.ï±PÝýáyøÀ
Î½&9ßÊ8è®
ëÈ
Bã¢hg»jºÈs¡Z¡b|ì¿FÈ
1. È»Iº3Bð&ìÃ@~ßõ¢9g»jºÈs¡Z
2. ÍgICASSPºÈÝ sgb&9ßþ.ºÝt&Ý.ï?Ýéº
È®ßõôºSþ.Ý¡ZsïT&ºÅ(ÕÎ¼7FÝ;ÄÍ»/ÝÝ
.ïA¡Z»jºÈ#åÄ.ºÈs
°÷/£](ÌC/
hg÷/£]/ÀºÈ¡Z/Ãn×ù|CºÈW×Í
"!¡Z/
4
all functions f for which f (n) exists and is continuous every-
where. If x(t) ∈ CP+1(−∞,∞), then
x(t+ h) =
P∑
i=0
hi
i!
x(i)(t) +RP (t) (5)
where
RP (t) =
1
P !
∫ t+h
t
(t+ h− θ)Px(P+1)(θ) dθ. (6)
This is called Taylor’s theorem with integral remainder [5].
The remainder RP (t) can be rewritten as the following form
R(t) =
1
P !
∫ h
0
(h− θ)Px(P+1)(θ + t) dθ. (7)
which is more suitable for the derivation in the next section.
We restrict the increment value h ≥ 0 in the following deriva-
tion to keep the upper limit h is larger then the lower limit 0
in Eq. (7).
3. DEGREE OF EXACTNESS AND PEANO KERNEL
In this section, we will investigate Eq. (4) by Taylor series of
x(nT − kT − τ) and x(nT − mT ). The expanding center
is denoted by t0 = nT −KT where K is a positive integer.
That is, x(nT − kT − τ) and x(nT − mT ) are expanded
at past Kth samples for trying to keep the time increments
nT − kT − τ − t0 and nT −mT − t0 being positive. Let the
increments be αk and βk, we need
αk = (K − k)T − τ ≥ 0, 0 ≤ k ≤ N, (8)
βm = (K −m)T ≥ 0, 0 ≤ m ≤M. (9)
That is, we need K ≥ max(N + d,M) where
d ≡
τ
T
(10)
is the desired delay τ normalized by the sampling period T .
Now, x(nT − kT − τ) and x(nT − mT ) are ready to
expand into the Taylor series and are expressed by
x(nT −KT + αk) =
P∑
i=0
αik
i!
x(i)(t0) + rk(nT ) (11)
x(nT −KT + βm) =
P∑
i=0
βim
i!
x(i)(t0) + sm(nT ). (12)
Using the formula of integral remainder given in Eq. (7), the
remainders rk(nT ) and sm(nT ) can be explicitly expressed
as
rk(nT ) =
1
P !
∫ αk
0
(αk − θ)
Px(P+1)(θ + t0) dθ (13)
sm(nT ) =
1
P !
∫ βm
0
(βm − θ)
Px(P+1)(θ + t0) dθ. (14)
By introducing the unit step function u(θ)defined as u(θ) =
1, θ ≥ 0 and u(θ) = 0, θ < 0, we can express Eqs. (13) and
(14) by
rk(nT ) =
1
P !
∫ ∞
−∞
r˜k(θ)x
(P+1)(nT − θ) dθ (15)
sm(nT ) =
1
P !
∫ ∞
−∞
s˜m(θ)x
(P+1)(nT − θ) dθ (16)
where
r˜k(θ) = (θ− kT − τ)
P [u(θ− kT − τ)− u(θ−KT )] (17)
s˜m(θ) = (θ −mT )
P [u(θ −mT )− u(θ −KT )] (18)
The form of Eqs. (15) and (16) is more convenient than that
of Eqs. (13) and (14) for our derivation because we can add
these remainders together to represent the whole remainder of
the system.
Since Eq. (4) is an approximation to the ideal delay sys-
tem, the design error is represented by
e(nT ) =
N∑
k=0
akx(nT−kT−τ)−
M∑
m=0
bmx(nT−mT ). (19)
Substituting Eqs. (11) and (12) for Eq. (19), we can write it
as e(nT ) = a(nT ) + r(nT ) where
a(nT ) =
P∑
i=0
T iwi
i!
x(i)(nT −KT ), (20)
r(nT ) =
N∑
k=0
akrk(nT )−
M∑
m=0
bmsm(nT ). (21)
with
wi =
N∑
k=0
ak(K − k − d)
i −
M∑
m=0
bm(K −m)
i. (22)
Substituting Eqs. (15) and (16) for Eq. (21), we obtain
r(nT ) =
∫ ∞
−∞
K(θ)x(P+1)(nT − θ) dθ (23)
where
K(θ) =
1
P !
[
N∑
k=0
akr˜k(θ)−
M∑
m=0
bms˜m(θ)
]
. (24)
a(nT ) can be regarded as a primal term in the approx-
imation error e(nT ) and r(nT ) is the remainder term. If
a(nT ) = 0, the derivatives x(i)(nT −KT ), 0 ≤ i ≤ P van-
ish in e(nT ). This condition is called P -degree of exactness.
It is obvious that P -degree of exactness holds if
wi = 0, 0 ≤ i ≤ P. (25)
