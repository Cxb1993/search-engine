行政院國家科學委員會專題研究計畫成果報告 
多層波茲認知機 
的網路架構與學習法則 
Multilayer Potts perceptrons with Levenberg-Marquardt learning 
計畫編號：95-2221-E-259-012- 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
主持人：吳建銘    東華大學應用數學系 
 
 
中文摘要 
 本計劃提出新的波茲認知機(Potts perceptron)
的類神經計算組織，並探討多層波茲認知機的網路
架構與學習法則，據以解決資料驅動式的函數近似
問題。波茲認知機是傳統認知機的一般化版本，相
關計算包括將輸入向量投影在接收場向量
(receptive field)，並以新的轉換函數將線性投影值
轉換為波茲認知機的向量輸出。新的轉換函數使用
一組內建結點將函數定義域切割成多個非重疊區
間，再將函數輸出表示成投影值在各區間的常態化
高斯隸屬性(normalized Gaussian membership)。多層
波茲認知機包含多個波茲認知機，網路輸出初步設
定為各波茲認知機的加權輸出總和。因為波茲認知
機的輸出為向量型態，網路中的加權係數也以向量
型態表示。我們將證明傳統多層認知機的網路函數
是多層波茲認知機的網路函數的特例。本計劃將探
討多層波茲認知機的各式網路架構，並發展相關的
Levenberg-Marquardt(LM)學習法則，有效解決函數
近似問題。LM 方法是梯度法與高斯牛頓法的合成
方法，主要透過最小化網路實際輸出與目標輸出間
的平均平方誤差，求最佳網路參數。我們以數值模
擬探討多層波茲認知機的 LM 學習法則的有效性，
並與 MLP(multi-layer perceptrons)、RBF(radial basis 
function)、SVM(support vector machine)的既有學習
法則進行系統化評比。 
關鍵詞：資料驅動式函數近似，監督式學習，波茲
編碼，高斯陣列，類神經網路，後非線性投影，多
層認知機 
 
Abstract 
This work proposes neural organization of Potts 
perceptrons and addresses on how to organize and 
learn multi-layer Potts perceptrons for data driven 
function approximation, signal approximation and 
time-series prediction. The proposed Potts perceptron 
that is generalized from traditional perceptrons uses a 
novel transfer function to encode a scalar external 
field to a vector code, where the external field is 
formed by projecting an input vector on a receptive 
field. The novel transfer function employs a knot 
vector to partition its domain into non-overlapping 
intervals and activates a vector output to represent the 
normalized Gaussian membership of the scalar 
function input to each of partitioned intervals. A 
network of multiplayer Potts perceptrons is organized 
to sum up weighted activations of multiple Potts 
perceptrons. It will be shown that the function space 
spanned by networks of multilayer Potts perceptrons 
covers network functions of multiplayer perceptrons. 
Learning a network of multiplayer Potts perceptrons 
based on the Levenberg-Marquardt(LM) method is 
extensively explored for data driven function 
approximation. The LM method is employed to 
minimize the mean square error of approximating 
desired outputs by correspondent actual network 
outputs through iteratively updating the network 
parameter using a dynamical hybrid of searching 
directions determined by the Gauss-Newton method 
and the gradient method respectively. The LM 
learning method for multilayer Potts perceptrons is 
extensively studied by numerical simulations, 
systematically compared with popular existing 
supervised learning methods for multiplayer 
perceptrons, support vector machines and radial basis 
functions. 
Keywords: data driven function approximation, 
supervised learning, Potts encoding, Gaussian array, 
neural networks, post-nonlinear projection, multilayer 
perceptrons 
 
一、多層網路學習與非限制最佳化 
訓練集中的每一成對資料包含一個預測向
量(predictor)及一個目標值(target)，分別以
x[t]及 y[t]表示。當網路輸入為 x[t]時，網
路輸出的目標值為 y[t]，如果用 y(t|θ)表示
實際網路輸出，則下述的平均平方誤差
(mean square error)代表網路參數的成本函
數(cost function)， 
Thh
h
hh
h
h
)
)exp()exp(
)exp(
,
)exp()exp(
)exp(
(
),1) 1(;(
22
2
22
2
2T
σσ
σ
σσ
σ
σ
−+−+
−
=
−g
。 
既然轉換函數 g 可用來重構傳統認知機
的 S 型轉換函數，多層波茲認知機網路
也可以用來模擬多層認知機網路所執行
的對應函數。 
三、多層波茲認知網路 
本計劃將探討多層波茲認知機的網路架構
及有效的學習法則，以解決函數近似問
題，並應用在訊號處理問題上[14][20-22]。
以下介紹波茲認知機的相關研究主題及初
步方法。 
1. 多 層 波 茲 認 知 機 (multi-layer Potts 
perceptrons)的網路架構： 
A. 給定輸入向量 x，和傳統認知機一
樣，波茲認知機中的轉換函數 g
的輸入 h 設定為 x 在接收場向量
的投影，例如， 
xwTh =  
 其中 w 代表波茲認知機的接
收場向量。 
B. MLP 網路輸出通常設定為多個認
知機輸出的加權總和，依照這個
原則，我們初步設定多層波茲認
知機網路輸出為多個波茲認知機
輸出的加權總和。因為波茲認知
機的輸出為向量型態， 相關的權
重係數也以向量型態表示。多層
波茲認知機網路所執行的輸入輸
出對應函數初步表示如下 
∑
=
= M
m
mm
T
m
T
mG
1
2 ),;();( σcxwgrθx  (4)， 
其中 M 代表網路中波茲認知機的
個數，θ 代表網路參數， wm 及
rm分別代表第 m個波茲認知機的
接收場向量及權重係數。多層波
茲認知機網路和 MLP 網路[1-7]
同為以線性投影為基礎運算
(projective basis)的類神經網路，
顯然不同於以半徑距離為基礎運
算 RBF(radial basis function)網路
[15][25]。如果設定共同變異數
σm2 為 1/β，所有 cm為向量(-1 1)T
及 rm 為向量(-rm rm)T，則式(4)中
的多層波茲認知機網路函數等同
於 MLP 網路函數，表示如下 
)tanh();(
1
∑
=
= M
m
T
mmrG xwθx β
。 
證明 MLP 網路為多層波茲認知機
網路的特例。 
C. 轉換函數 g 的加權輸出可表示為
可調變的參數化非線性函數
(adaptive parametric nonlinear 
function)，例如 
),;(
),,;(
2
2
σ
σϕ
cgr
cr
h
h
T= 。 
ϕ  函數的輸出等同於高斯陣列
的常態化加權輸出 (weighted 
output of normalized Gaussian 
array)，其中r代表加權向量，c代
表由 K高斯變數的期望值所形成
的向量。給定訓練集，
])}[],[{( tyth ，如何有效求取ϕ函
數的最佳參數，使ϕ函數的輸入輸
出符合訓練集中成對資料的限制
關係，是最基本的波茲認知機學習
問題。我們已初步完成ϕ函數的學
習法則，數值模擬結果如圖二所
示。圖中的曲線就是所求得的近似
函數ϕ，各子圖中每一點代表一成
對資料在二維平面的對應座標，相
關的訓練集分別來自 log、exp、
cos、tanh 函數及多項式 xx −3 。 
階導數止，所得到的近似式是θ
的二次式。再令式中的θ-θi 為
Δθi，所得的近似式為Δθi的二
次式，求該二次式的最小值就可
以導出牛頓法所使用的網路參數
變動方向。如果將上述二次近似
式中稱為 Hessian 矩陣的權重矩
陣以下述矩陣取代 
)()()( iS
T
iSi EER θθθ ∇∇= ， 
並求出相關近似式的最小值，就
可以得到高斯牛頓法所使用的網
路參數變動方向。 
3. Levenberg-Marquardt 方法：LM
方法[8][9][16]是梯度法與高斯
牛頓法的合成方法，所使用的網
路參變動方向由以下的線性系統
決定， 
)())(( iSii EIR θθθ ∇=Δ− λ   (5) 
其中 I 為單位矩陣，λ為調節參
數。當λ=0，式(5)所決定的網路
參數變動方向和高斯牛頓法一
致；當λ為相對比較大的正數
時，式(5)所決定的網路參數變動
方向與梯度法一致。LM 方法以動
態方式決定λ值，根據成本函數
的實際下降量與預測量的比值來
調變λ值，平衡實際成本函數下
降的有效性及信賴度。當這個比
值很低時，代表目前使用的網路
參數變動方向對下降實際成本函
數是不可信賴的，LM 方法會自動
調高λ值，使網路參數變動方向
偏向梯度法所決定的方向。反過
來，當成本函數的實際下降量與
預測量的比值過高時，代表目前
使用的網路參數變動方向對下降
實際成本函數具高可信賴度，LM
方法會藉著調低λ值，使網路參
數變動方向偏向高斯牛頓法所決
定的方向，提高搜尋效率。 
 
五、數值模擬與結論 
圖一、一維函數近似 
 以 MATLAB 撰寫模擬程式進行數值實驗，
證實LM學習方法學習多層波茲認知機是有
效方法。圖一為一維函數近似結果，藍色
曲線是目標函數，藍點是訓練集，紅色曲
線是訓練所得的多層波茲認知機的網路函
數。圖二是以 LM 方法求得的二維近似函
數，藍色點代表成對的訓練資料，因為是
有雜訊的資料訓練集，所以藍色點不會剛
好落在近似函數的函數曲面上，圖二的函
數曲面就是用LM方法學習得到的多層波茲
認知機網路函數。函數輸入空間可以是多
維的，以五維的 sin 函數為例，LM 方法學
習得的多層波茲認知機網路函數曲面無法
繪製，但函數中的線性及非線性結構確可
以非常清楚繪製，如圖三所示為近似函數
的線行及非線性結構。 
 
圖二、二維函數近似 
