 2 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
□v   達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿□v 撰寫中 □無 
專利：□已獲得 □申請中□v 無 
技轉：□已技轉 □洽談中□v 無 
其他：（以 100 字為限） 
 
 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
 
本計畫的執行結果對於使用者要選擇雲端服務時，提供一套有用的參考流程
與工具，對於國內想要提供以及使用雲端服務的廠商與研究機構都有實用價
值，也有很多值得深入探討的議題，例如：如何讓使用者利用多個雲端服務
來達成像是容錯與備份的進階的效能。 
 
 
 
 
附件二 
 4 
照環境改變的系統，例如：有一篇論文是在談論電腦的記憶體如何在平日運作時依照軟體的
workload來自行挑戰來達成省電的效果。 
三、考察參觀活動(無是項活動者略) 
本人也藉由此次機會與德國 Karlshue Institute of Technology有交流研究的機會，參觀的他們
嵌入式系統實驗室的研究，並且與該實驗室的教授討論可能的合作機會，也參觀了與機器人相關
的實驗室。 
四、建議 
在參觀 Karlshue Institute of Technology時,與他們的教授聊到而對他們對於招收學生，以及
學生實務經驗訓練的過程發現有相當多值得我們學習的地方，國內在這兩個領域都有很多公司在
做研發，希望將可以建立可能的合作機會並且將人才培訓的概念帶入國內。 
五、攜回資料名稱及內容 
茲附上這次會議的網站，上面有各項會議相關統計數據，以及這次會議的論文
http://www.cis.fiu.edu/conferences/icac2011/index.shtm 
六、其他 
 
 
 6 
1 Introduction  
With the business opportunities in cloud computing, many companies are offering cloud services. This creates 
a market that provides users with many options and competitive pricing. At the same time, it is often unclear 
which provider would meet a user’s application needs in terms of cost and performance.  
To address this challenge, we developed CloudGuide tool suite that takes a given workload for legacy appli-
cations and outputs a guide with estimated cost based on the performance requirements and policy constraints 
from the users. CloudGuide includes an application pro-ﬁler that can estimate the computing resources 
required for an application to support a given workload. Users can deﬁne their workload by recording history 
traces for replay or use their own client emulator. We also survey and collect the pricing and speciﬁcations 
offered by different cloud providers. The central component of CloudGuide is an engine that takes the 
application proﬁles and offerings from cloud providers and calculates an estimated cost. 
 
2 Related work 
The closest related works to this proposal are Li et al [8] and. Hajjat et al.citeHajjat10. Li et al. [8] outline the 
total-cost-of-ownership (TCO) and utilization cost to spend and build an in-house infrastructure compared to a 
cloud. They include the amortized cost of eight factors in their calculation and provide a web interface for 
user to query. Their system require users to input their estimated numbers for the data center size, which can 
be cumbersome. Haijjat et al. [6] examined the problem of migrating client applications which are composed 
of several components into the cloud. They formulated their problem into an optimization problem by 
maximizing migration beneﬁt while satisfying client-imposed policy constraints. Our work differ from their 
work in that we also consider more ﬁne-grained application resource need when determining how to migrate 
and where to migrate. CloudCmp also examines selecting cloud vendor problem and ran a set of benchmark 
tools to evaluate cloud vendors. Our proposal beneﬁt from their benchmark work and further extend the 
concept of matching application needs to cloud vendor offerings.  
Tak et al. [12] applies Net Present Value (NPV) method to estimate and compare the cost of owning a local 
data center vs. renting cloud resources. They take Amazon AWS as an example of cloud provider and include 
detailed cost calculation for deploying locally. Their work is complementary to ours as we compare among 
multiple cloud providers, and we examine different applications.  
Conductor [20] considers the computation model based on MapReduce [2] and address resource management 
issues, such as dynamic pricing and resource diversity. Our work takes application resource requirement as an 
black-box and ﬁnds the best cloud provider based on the policy and requirements from users.  
Garﬁnkel [5] and Walker [18] documented their experiences in using Amazon for grid computing service and 
scientiﬁc computing respectively. Both of their work investigates scientiﬁc computing on one speciﬁc cloud 
service provider, while this proposal plans to across multiple cloud service providers and compare many types 
of applications. Ward [19] compared the performance and pricing of using UBUNTU Enterprise Cloud, a 
private cloud software suite, with services offered by Amazon EC2.  
Finally, Teregowda et al. [13] closely documented their migration process for a mature large-scale CiteSeer 
into the cloud and provided many insights on how application can be migrate in a hybrid cloud mode. Their 
work takes application-level knowledge into consideration for migration decision and we believe they can 
beneﬁt from CloudGuide to explore what-if scenarios. 
 
 8 
3.2 Benchmark results for provider 
We conducted a set of benchmark experiments on the selected cloud providers.  Specifically, we run 
SpecJVM to compare the CPU performance across providers and conducted micro-benchmarks for network 
and disk performance. 
To test the performance for network, we follow a similar approach as CloudCmp [7,8] by running ping, iperf 
across two instances.  Most IaaS cloud instances allow this kind of system testing easily, but interestingly, 
running such test on PaaS instances is not straightforward.  To overcome this limitation, we use 
application-level ping and throughput testing, and test this on Google AppEngine.  We used threaded App 
Engine urlfetch Python APIs to do the job. Two identical App Engine applications2, CloudCmp01 and 
CloudCmp02, are deployed to Google AppEngine (abbreviated as GAE in the following). The application 
uses urlfetch API to fetch specified size of data from its counterpart, which is essentially a ’ping’ over HTTP, 
while it can be also used to measure bandwidth by specifying a larger size to be transferred.  Since GAE 
scales the number of instances automatically and we cannot control its scaling behavior, in order to get its 
maximum ability, we used threads to squeeze each and every resource GAE can give us under our free quota.  
Figure 2 shows the latency results of our experiments on GAE. For Amazon EC2, on the other hand, is much 
simpler, for its instance number is always under our control. We used only one instance on the both ends of 
the Internet connections on Amazon EC2. 
 
Figure 2 AppEngine latency results 
 
Figure 3. Parallel downloads of GAE 
 10 
elapsed to run the workload. We calculate the network cost using: Costnetwork = ∑(Data ratei ∗ t ∗ 
Pricenetworki ), where data rate as average data rate received/transmitted from/to the Internet/Intranet per 
second (in Gb/sec),t as total elapsed time in seconds, and Pricenetwork as unit price of data transfer in $/per 
GB. Disk is calculated using this formula: CostDisk = CostStorage + CostTransaction, where CostStorage = 
Totaldatasize ∗ Unitprice and CostTransaction = RateTransaction ∗ t ∗ CostPertransaction,  
where RateTransaction is the average disk transaction rate in transactions per second and t is the total elapsed 
time. 
 
4. Evaluation 
This evaluation tries to answer the questions of “How well does CloudGuide choose cloud provider?” 
 
4.1 Methodology 
We chose multi-tiered Web services as our target application.  Two benchmarks are chosen for Web services: 
RUBiS [1] and TPC-W [14].  RUBiS is a three-tiered Web service modeled after an eBay-like e-commerce 
website.  It provides a platform for users to post items for sale, browse sale items, and place bides.  An 
emulator is also provided with RUBiS that enable easy emulation of a configurable number of concurrent 
clients exercising different operations using a transition table. TPC-W is a business-oriented transactional web 
benchmark, which exercises components commonly used in such environment.  Both of them are commonly 
used to evaluate the effectiveness of performance modeling and prediction.  
 
4.2 RUBiS 
We ﬁrst present out our results with hosting Enterprise Java Beans version of RUBiS in a cloud environment with 
each tier running in a separate instance, and a client emulator running on a different machine. Based on the 
calculation shown in subsection 3.4, CloudGuide recognizes Amazon as the most cost-effective cloud provider 
given similar performance.  Figure 5 shows a summary of the estimated cost per month for each provider chosen.  
From the figure, storage cost is the dominant cost, contributing to 70—90% cost.  In this case, the cloud provider 
with the lowest price in storage offering can therefore provide the most competitive cost.  Amazon is the clear 
winner here with its storage cost half of Azure and 20% less than GoGrid.  Between the two storage offering, 
EBS is even more competitive compared to S3, even though S3 provides a higher data availability because it 
replicates stored data.  However, if we only examine price and performance, the value of high availability 
provided by S3 cannot be reflected.  Therefore, this suggests that CloudGuide should also examine other factors 
than cost and performance to provide a guided report that demonstrates the strength by different offerings. 
 12 
come at the cost of monetary expense and performance, and their benefit only show when a disaster occurs.  
However, this benefit is not quantified here, and we would like to investigate such benefits. 
 
6. Conclusion  
We have taken the ﬁrst step in investigating choosing cloud provider from a user’s perspective. Our 
accomplishments presented in this final report include:  
First, a survey of cloud providers is conducted, and the survey also includes cloud providers’ offerings and pricing 
schemes. This survey helps us derive the suitable cost calculation function for user applications. Second, a basic 
application proﬁling tool is built that we were able to use when conduction experiments in our tested and on 
Amazon EC2. Third, an initial test-bed is setup and two application installations on our test-bed which we can 
transfer into a cloud environment. Finally, a benchmark methodology is developed so that we can easily compare 
the performance and performance to cost ratios across different cloud providers.  
 
References  
[1]  CECCHET, E., CHANDA, A., ELNIKETY, S., MARGUERTIE, J., AND ZWAENEPOEL, W. Performance comparison 
of middleware architectures for generating dynamie web content. In Proceedings of the 4th ACM/IFI?USENIX 
International Middleware Confernce (Rio de Janeiro, Brazil, June 2003).  
[2]  DEAN, J., AND GHEMAWAT, S. Mapreduce: Simpliﬁed data processing on large clusters. In Proceedings of the 6th 
Symposium on Operating Systems Design and Implementation (San Francisco, CA, December 2004).  
[3]  DU, J., SEHRAWAT, N., AND ZWAENEPOEL, W. Performance proﬁling of virtual machines, March 2011.  
[4]  eathena-mmorpg server package. http://code.google.com/ p/eathena-project.  
[5]  GARFINKEL, S. L. An evaluation of amazon’s grid computing services: Ec2, s3, and sqs. Tech. rep., Harvard University, 
2007.  
[6]  HAJJAT, M., SUN, X., AND)DAVID MALTZ, Y.-W. E. S., RAO, S., SRIPANIDKULCHAI, K., AND 
TAWARMALAFNI, M. Cloud-ward bound: Planning for beneﬁcial migration of enterprise applications to the cloud. In 
Proceedings of SIGCOMM 2010 (New Helhi, INDIA, 2010).  
[7]  LI, A., YANG, X., SANDULA, S., AND ZHANG, M. CloudCmp = Comparing Public Cloud Providers. In Internet 
Measurement Confernce (November 2010).  
[8]  LI, X., LI, Y., LIU, T., QIU, J., AND WANG, F. The method and tool of cost analysis for cloud computing. In 
Proceedings of 2009 IEEE International Conference on Cloud Computing (Bangalore, India, September 2009).  
[9]  Openkore. http://www.openkore.com/index.php/Main_ Page.  
[10]  SOBEL, W., SUBRAMANYAM, S., SUCHARITAKUL, A., NGUYEN, J., WONG, H., KLEPCHUKOV, A., PATIL, S., 
FOX, A., AND PATTERSON., D. Cloudstone: Multi-platform, multi-language benchmark and measurement tools for 
web 2.0. In Proc. 1st Workshop on Cloud Computing (CCA 08) (Chicago, IL, October 2008).  
[11]  STEWART, C., AND SHEN, K. Performance modeling and system management for multi-component online services. 
In Proceedings of the 2nd Symposium on Networked Systems Design and Implementation (NSDI ’05) (Boston, MA, May 
2005).  
[12]  TAK, B. C., URGAONKAR, B., AND SIVASUBRAMANIAM, A. To move or not to move: The economics of cloud 
computing. In Proceedings of 3rd USENIX Workshop on Hot Topics in Cloud Computing (Portland, OR, June 2011).  
[13]  TERGOWDA, P. B., URGAONKAR, B., AND GILES, C. L. Citeseerx: A cloud perspective. In Proceedings of 
HotCloud 2010 (Boston, MA, June 2010).  
 國科會補助專題研究計畫項下出席國際學術會議心
得報告 
                                     
日期：100年 10月 17日 
                                 
計畫編
號 
NSC  99－ 2218 － E － 002－ 038 
計畫名
稱 
以使用者為導向比較與選擇雲端服務提供者 
出國人
員姓名 
蘇雅韻 
服務機
構及職
稱 
台灣大學資訊工程學系所 
會議時
間 
100年 6月16
日至 
100年 6月 18
日 
會議地
點 
德國卡爾斯胡 
會議名
稱 
(中文)第八屆國際自主運算會議 
(英文) The 8th IEEE/ACM International Conference on 
Autonomic Conference 
發表論
文題目 
(中文) iPOEM:以 GPS介面之整合式虛擬數據中心管理
工具 
(英文) iPOEM: A GPS Tool for Integrated Management 
in Virtualized Data Centers 
附件四 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/17
國科會補助計畫
計畫名稱: 以使用者為導向比較與選擇雲端服務提供者
計畫主持人: 蘇雅韻
計畫編號: 99-2218-E-002-038- 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
