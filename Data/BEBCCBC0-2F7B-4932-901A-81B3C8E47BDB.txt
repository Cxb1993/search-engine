 II
中文摘要 
隨著網路的普遍與各種技術的進步，公司企業、政府機關與學校團體都將許多作業流
程藉由網頁應用方式來進行電子化作業，例如正如火如荼所進行的 e-Taiwan，e-Government, 
M-Taiwan計畫等等。但此同時網路入侵與破壞技術也不斷進行與衍生：包括病毒攻擊、網
路服務阻斷、分散式網路攻擊..等，常因此造成網路癱瘓與資料損毀等嚴重損失。因此本計
畫研究與開發『具智慧型網路檢測技術之 XML 安全交換器』的相關核心技術，來處理及
保護下一代網路所傳輸之 XML 資料並能夠適應未來網路環境。我們期許能夠以晶片、軟
體與系統(Packet In Silicon, Packet in Software, Packet in System)全面思考的角度來建構具決
策能力(Packet in Strategy)的新一代網路安全交換系統。 
本系統包含以下核心關鍵技術，分別為： 
(1) 網頁應用與 XML處理引擎(Web Application Processing Engine)：負責執行網頁應
用程式以及 XML語法處理之核心處理機制以提供加速與安全防禦能力。 
(2) 智慧型網路檢測引擎(Intelligent Network Inspection Engine)：利用軟硬體等各種技
術來實現可提供常規表示語法之網路內容檢測技術。 
(3) 網路異常行為決策引擎(Network Anomaly Detection & Dealing Engine)：負責根據
網路行為模型來判斷是否有網路異常行為之發生，並根據賽局理論所衍生之策略對
應機制來分析各種因應措施所帶來的效益分析，並根據所得之最佳策略來保障整體
網路環境。 
整個計畫用三年時間完成。第一年我們設計整體 XML 安全交換器細部架構，研究與
評估適用的核心處理平台與演算法。第二年我們以相關之軟硬體技術完成智慧型網路檢測
引擎的開發與實現。第三年完成網路異常行為決策引擎，並且整合上述三大部分，完成具
智慧型網路檢測技術之 XML安全交換器之研究與開發，並做相關測試演練。 
關鍵字：XML交換、網路安全、正規表示語法比對、賽局理論、特徵碼 
 1
一、 前言與研究目的 
網路交換技術發展至今，一方面除了朝向多兆赫(Multiple Gigabit)的高速發展，另一方
面則是往更網路更高階層來發展。以目前來說第七層的超高速交換技術也以達到商業化的
成熟。但是下一步呢？除了單純的交換功能外，隨著交換器技術的突破，多功能的交換器
隨之問世，最常見的便是在於安全範疇的涉足。如同我們過去所執行的計畫與研發成果，
安全交換器(Security Switch)這一類結合安全管理機制的交換設備將會是未來網路技術的發
展重心之一。 
此外，隨著網路應用的蓬勃發展，諸如 web service、web 2.0等技術與概念的提出，不
用網路應用程式間交換資訊的需求日增，而 XML(EXtensible Markup Language)就是一種用
於來表達資料結構的通用語法，可便利資料的傳輸與交換，在未來 internet中 XML資料所
佔的比率將逐漸增加。因此，如何處理及保護下一代網路所傳輸之 XML資料也是本計畫
著重的地方之一。 
而從網路安全的角度來分析，將眾多功能整合至單一設備之中，並且追尋更快速的處
理速度是發展的趨勢之一。從早期的防火牆、至入侵偵測與防禦等網路設備，發展至整合
了內容管理與網路防毒功能的 UTM。多種資安產品整合在單一設備，並朝向許多防火牆都
加入 IPS功能，隨著相關技術的成熟，目前已經出現許多整合型安全裝置，將多種資安產
品結合在一起。但是另一方面，駭客為了躲避現有的防禦機制（如防火牆、IPS、防毒），
都想從合法的管道找到入侵點。 
智慧型網路決策系統是一套智慧型多層網路決策的系統，本計畫開發的『具智慧型網
路檢測技術之 XML 安全交換器』不同於傳統的『單點防禦』系統，而是一個『全面』的
防禦系統架構，用以提供最完善且高效能的網路安全環境。並期許能夠以晶片、軟體與系
統全面思考的角度來建構智慧型網路決策系統的核心技術(Packet In Silicon, Packet in 
Software, Packet in System, Packet in Strategy)。 
二、 文獻探討 
在網路安全檢測的技術之中，字串比對(string matching)是最為重要的技術。一項統計
指出，在著名的 IDS(intrusion detection system) Snort系統運作時，string matching佔了 80%
以上的 CPU計算時間。因此，一個字串比對演算法的優劣，將大幅影響一個網路安全檢測
系統的穩定性及準確性。近年來各種字串比對的演算法及各種輔助的機制相繼被提出，而
其中一種基於狀態機(automata-based)的字串比對法被廣泛地運用，有大量的研究都根基於
此，以下將逐項介紹近年來相關之研究及文獻。 
Aho-Corasick 字串比對演算法 
這個演算法在 1975由 A. V. Aho and M. J. Corasick 提出[1]，主要是根據攻擊的特徵組
(signatures)，建立出一有限狀態機，再對任一字串輸入進行比對之運算。所謂攻擊特徵組，
是由各種已知的攻擊中，找出其特殊且可辨認之樣式(pattern)，蒐集而成一資料庫，再對於
任何的可疑輸入進行檢查，若有發現相同的樣式，即可依所存有的資訊做遭受何種攻擊之
判斷。為了有快速地進行字串比對的工作，AC演算法依據所蒐集之特徵組建立一有限狀態
機，並保有一當前狀態(初始化為零)，對於任一個輸入的字元，將依當前狀態決定下一狀態
 3
AC-Bitmap 
由表一可知，大部分的儲存內容皆相同(為零)。因此，若採用NFA表示法，即在每個狀
態中只紀錄有路徑往下走的輸入值之下一狀態，將可大幅地節省空間。但如何判定是否有
路徑繼續往下及如何得知下一狀態? N. Tuck, T. Sherwood, B. Calder, G. Varghes 在2004年
的 Infocom 中提出了一個解決此問題的演算法，即用一bitmap來紀錄某一狀態中輸入為某
一字元時是否有路徑，如表二所示，為對應到圖一之有限狀態機的bitmap。 
 
 
 
 
 
 
 
 
 當查詢 bitmap時，若結果為 0，則表示當前狀態對目前的輸入字元沒有路徑。反之，
若結果 1，可知有路徑往下一狀態。AC-bitmap中，會另外儲存一個下一狀態列，紀錄著所
有當前狀態有路徑往下走的下一狀態，即對每個 bitmap中為 1的位置，都要有一對應的下
一狀態，如表三所示。 
 
 
 
 
 
 
 
在執行狀態轉換的過程中，可藉由表三算出所對應的 bit之前有幾個 bit為 1(popcount
運算)，再查詢表三，即可得知下一狀態。例如，假設當前狀態為 1而輸入為 i時，由表二
可知，在 i之前有二個 bit為 1，因此下一狀態為表三中當前狀態為 1之下一狀態列 1 2  
4中的 4。此外，對於沒有路徑往下的情形，也須額外儲存一失敗路徑表(failure path)，但
由於失敗路徑表只須紀錄在每個狀態沒有路徑時應轉換到的下一狀態，所以對記憶體的消
耗量相對 DFA而言小的多。不過對一個輸入字元，可能有多次的 failure path轉換，最後才
找到有路徑可走的下一次狀態，因此 NFA的比對速度一般來說較 DFA為慢，而 AC-Bitmap
     輸入 
當前狀態 
h e r i m s 
0 1 0 0 0 0 0 
1 1 1 0 1 0 0 
2 1 0 1 0 0 0 
3 1 0 0 0 0 0 
4 1 0 0 0 1 1 
5 1 0 0 0 0 0 
6 1 0 0 0 0 0 
表二、圖一之有限狀態機的bitmap 
  當前狀態 
 
0 1 
1 1  2  4 
2 1  3 
3 1 
4 1  5  6 
5 1 
6 1 
表三、圖一之有限狀態機的下一狀態列 
 5
狀態轉換的過程中，有多少比率在matrix H 中的查詢值為1。我們發現，在壓縮率不斷提高
的情形下，記憶體被大幅的壓縮，而魔術命中率卻下降的很緩慢，說明了我們的壓縮方式
是很有效的。 
我們選定用Defcon9 會議中所收集的網路流量作為輸入，此一會議集合了世界各地的駭客
與攻擊專家，互相競賽誰會優先攻擊成功，因此這個網路流量對於測試防禦攻擊的網路安
全機制有一定的代表性。而攻擊樣式則是採用Snort2.4 的版本，先依AC 演算法建立出DFA 
table 後，再依我們的演算法建立出額外的資料結構。下圖所示為演算法中，不同的記憶體
使用率與魔術命中率之間的關係。 
 
圖三、記憶體使用率與魔術命中率間的關係 
硬體架構設計 
由於演算法中的魔術狀態表及matrix H 的記憶體需求較小，可置於高速的on-chip記憶體，
以m=4, n=0 來說，只需消耗42KByte 的on-chip 記憶體卻有高達85%的魔術命中率。至於
DFA table，因為需要約10.54 MByte 的記憶體空間，因此只能置於一般的off-chip記憶體(如
DRAM)之中。在我們的實驗過程中發現，存取一次off-chip 記憶體所需的時間，大約是
on-chip 記憶體時間的13 倍之多。 
我們的有三種不同的硬體設計：第一種為傳統的AC 字串比對演算法，即將整個DFAtable 
儲存於記憶體中，而沒有用到魔術狀態的特性 (baseline model)；第二種為我們的演算法
(MSH model)，在AC 演算法中加入魔術狀態查詢的機制：第三種(MSH-2 model)使用更為
進階的硬體，利用現在dual-ported 記憶體可同時讀寫的功能，放置二個字串比對引擎於硬
體中，在魔術命中率足夠高的情形下，這二個引擎同時去存取off-chip 記憶體的機會應該很
小，因此可大幅提升效能。下頁圖四為此三種不同的硬體架構圖。 
 
圖四、三種不同的硬體架構 
 7
第二年： 
基於魔術狀態的探索演算法 
由我們更進一步的統計發現，由Snort 2.4 提供之樣式所建立的DFA table 中，有高達95% 以
上的數值為魔術狀態，也讓我們更確信此一特殊性質是可以善加利用來加速字串比對之運
算。首先我們用一bitmap matrix B 來紀錄DFA 表中某一數值是否為魔術狀態(是則為1，否
則為0)，而對256 種不同的輸入將儲存其對應的魔術狀態於一魔術狀態表中。於字串比對
過程中，藉由查詢matrix B 來得知下一狀態是否為魔術狀態，若是，則從魔術狀態表中得
到下一狀態；若否，再去查詢DFA table。由於matrix B 及魔術狀態表所需的記憶體極小，
可以放在高速存取的記憶體中，加上有高達95%的魔術狀態現象存在，因此在一般情形下，
這個演算法可望有相當不錯的表現。 
為了更進一步減少記憶體的需求，我們將matrix B 依長2m 寬2n 之方塊做切割，對每個方
塊再做一個AND的邏輯運算，形成一大小只有原來bitmap 1/ 2m+n 的資料結構(稱為matrix 
H)。 
 
部分狀態重新編碼 
由 AC 字串比對演算法所建構出來的有限狀態機，其每個狀態是依照深度優先的方式來編
碼(如下圖六(a))，Toshio Nishimura於 2001年提出藉由將狀態重新以廣度優先來編碼的方式
以加速 AC演算法中字串比對的過程(我們稱之為 AC-Rearranging)，如下圖六(b)即為將圖六
(a)的狀態機重新編碼後的結果。 
0
139
151
802
269
.
.
.
.
.
.
.
.
.
.
      
0
56
57
58
55
.
.
.
.
.
.
.
.
.
.
 
圖六、 (a) 狀態重新編碼前                (b) 狀態重新編碼後 
由實驗的結果說明，經過重新編碼後的有限狀態機，因為具有提高快取命中率的特性，可
以有效地增加系統效能。然而，在一般情形下，要對狀態數目超過 2 萬的一個有限狀態機
進行重新編碼，是個十分耗時的工作。因此我們提出了「部分」重新編碼的想法，只要針
對最重要的幾個狀態(例: 前 128個狀態)進行重新編碼，一樣可以有效地增進效能，卻可以
大幅減少將所有狀態重新編碼的力氣。 
 
圖七、部分狀態重新編碼 
 
 9
 
圖九、演算法的流程 
 
轉換成最長字串比對問題 
另外，我們將『深層封包內容檢測』(Deep Packet Content Inspection)轉化為『最長前置字串
比對』(Longest Prefix Matching，簡稱LPM)的問題[6]。最長前置字串比對，是一個發展成
熟的領域，目前主要是使用在路由器，用來比對forwarding table，進而決定往哪個網路介面
送封包，因此，已經有許多相關的演算法被提出且實作。而封包檢測，屬於較新的研究領
域，主要是對封包內容做完整的字串比對，一般的多重字串比對演算法以AC(Aho-Corasick)
最為常見，AC是一個以自動機為基礎的演算法，同時也是目前最常被使用在封包檢測的演
算法。 
我們提出了一個一般性的轉換方式，SM as LPM，這個方法利用了兩種演算法的類比
性質，通過將自動機轉換成前置字串表(prefix table)的程序，來將最長前置字串比對演算法
運用到封包內容檢測的機制上。藉由這個轉換法，不同的最長前置字串比對演算法，都可
以轉換來處理封包內容檢測的問題，因此我們所提出的轉換方法，對封包內容檢測的研究
提供了新的可能性。 
    然而，這個轉換方法最大的困難在於，如何將已知的惡意內容的 signature字串轉換成
最長前置字串比對演算法中的 prefix。由於惡意內容的 signature 字串，通常數量極多，若
是直接轉換，會使得轉換後的 prefix table需要很大的儲存空間。而網路上的路由器，通常
記憶體有限，所以直接轉換在實際上是不可行的。 
 11 
以下為一個簡單的例子及此演算法的描述： 
 
圖十、此演算法架構，每個狀態中位於()內的數字，即為不同狀態的 magic number X 
 
機器學習即時分類演算法 
機器學習演算法的方法中，大部分的統計屬性都用於粹取整個 flow的特徵，也因此這些方
式只適用於離線分類或線上流量分析。另一方面，就即時分類的方法而言，目前所提出的
方法多只針對 TCP 連線進行辨識，UDP流量辨識仍屬未廣泛觸及的課題。也因此本方向研
究仍有許多待探討的課題。 
我們的出發點在於：若需要反映原來網路各點在應用層的實際互動，並達到支援線上即時
分類的目標，則應針對應用軟體初始協商的特徵，以應用層角度，粹取統計特徵。我們提
出方法 [7] 著重於即時在早期辨識出每一個 flow 屬於哪一個協定或應用程式，以支援未來
網路管理所需。本方法亦考慮 TCP與 UDP流量分類，以符合當下點對點軟體同時使用 TCP
與 UDP 協定進行通訊的現實狀況。為了反映出應用層的互動，我們將應用層的傳輸視為兩
邊持續地輪流傳輸一段資料，亦即一方傳輸一段資料後，另一方再回應一段資料。每一段
資料傳輸，我們稱為說話區段(Talk Block)。而從第一個區段開始，每兩個不同方向，一來
一往的說話區段，則組成一個互動回合(Interaction Round)。圖十一是一個簡單的示意圖。 
 
 
 
圖十一、應用層程式互動示意圖。 
我們以應用層角度出發，提出一系列對於說話區段與互動回合的統計屬性，包含傳輸資料
量(data size)、Throughput、經過時間(Elapsed Time)等等。為了驗證本方法的效能，我們特
別利用了圖十二所示的實驗架構進行準確率分析。我們使用了一個自宜蘭大學出入口端收
集了 24小時的校園流量(Traffic Dump)作為分析資料。圖十二中，流量首先經過流量預處理
(Flow Preprocessing)，以建立所需的對照組資料和統計屬性。實驗中我們不考慮只包含少於
400個 flow的協定，並使用逾時(Timeout)120 秒以區分不同的 flow session。 
… 
… 
Round I1 
TALK T1A TALK T1B 
Round I2 
 
TALK T2A TALK T2B 
 13
 
圖十三、經過 symbol-wise prefix壓縮後的前置字串分布 
 
 
圖十四、經過 symbol-wise prefix和 magic state壓縮後的前置字串分布 
 
機器學習即時分類演算法[7]實驗所需的資料集藉由流量取樣(Flow Sampling) 被建立，而此
處的取樣方式以 Uniform distribution 進行。為了計算平均辨識準確率，我們使用了交叉效
度 (cross validation)的方式進行分析，而此處使用的是 10 分交叉效度 (10-fold cross 
validation)，亦即做十次實驗而後計算平均，且資料集切割成 10 等份，每一等份作為每次
實驗的測試集(Test set)，其他九等份則作為訓練集(Train set)。為了更進一步了解本方法的
效能，我們採用了五種機器學習演算法進行實驗。這五種演算法包含：Naive Bayes、
Sequential Minimal Optimization(SMO)、Bayesian Network with hill climbing algorithm (K2)、
Partial decision tree(PART)、pruned C4.5 decision tree (J48)。 
 
 15
 
表八、各種方法辨識誤判率(False Positive)比較。 
 AppR 
(J48) 
AppR 
(Bayes 
Net) 
AppR 
Scale 
(J48) 
[Bernaille 
06-1] 
Kmeans 
[Bernaille 
06-2] 
No 
stdport 
[Bernaille  
06-2] 
With 
stdport 
[Erman07] 
Layer 2 
[Li07] 
Protocol 
Bittorrent 0.13% 0.23% 0.13% 1.76% 2.16% 1.20% 0.16% 0.50%
DNS 0.33% 0.75% 0.38% 2.91% 1.38% 0.17% 0.13% 0.59%
EDonkey 1.23% 0.96% 1.23% 5.17% 1.60% 0.70% 3.34% 3.03%
Fasttrack 1.04% 2.07% 1.11% 2.13% 1.45% 1.34% 3.25% 1.27%
FTP 0.02% 0.17% 0.02% 0.24% 0.96% 0.05% 0.97% 0.26%
Gnutella 0.32% 0.56% 0.35% 4.38% 3.85% 4.32% 0.02% 1.13%
HTTP 1.07% 3.00% 1.02% 2.32% 5.91% 4.61% 0.42% 1.81%
Msn IM 0.65% 0.46% 0.57% 1.59% 1.61% 0.00% 3.46% 1.20%
POP3 0.01% 0.07% 0.01% 4.60% 1.48% 0.00% 1.52% 1.50%
Skypetos
kype 0.32% 0.67% 0.44% 2.83% 3.45% 6.56% 0.07% 1.31%
SMTP 0.01% 0.27% 0.01% 1.24% 1.77% 0.01% 2.03% 0.92%
SSH 0.23% 0.04% 0.17% 0.12% 0.11% 0.03% 0.20% 0.84%
 
 17
圖十六到圖十八秀出了我們在三個軟體上所觀察到的三項變數，三個軟體分別為 eMule 
(P2P)、BT (Bit Torrent, P2P)、HTTP (一般正常應用程式)。我們可以發現 eMule有著高的 PC
和 CC值以及非常低的 DC值，然而，HTTP卻有著高的 DC值和低的 PC以及 CC值。從
這兩個圖來看，這兩種軟體可以很輕易的被分辨出來，但是在 BT上，卻不是那麼的明顯，
BT的三個變數的值都非常的接近，這是因為 BT在一開始是以 HTTP 的方式和 Tracker 通
訊的，因此，BT的啟始行為事實上是就是 HTTP而非 P2P，而其將會在稍後從 Tracker處
得到其他 peers的 IP位址才表現出 P2P的行為。 
 
圖十六: eMule觀察到的行為。 
 
圖十七: Bit Torrent觀察到的行為。 
 
圖十八: HTTP觀察到的行為。 
 19
 
圖二十: 以語音和影音為主的 P2P應用程式的 UDP流量所觀察到的 β值 
 
使用階層式的字串比對法的深度封包檢測技術 (EHMA) 
一般來說，Cache Memory 是比較快速的記憶體，但是通常它的 size 也比較小，如果
我們可以設計一個大小適當的 filter，使得它可以使用 Cache Memory 的好處，並且能精準
地過濾不必要的 DRAM access，勢必可以增加封包檢測的效率，這就是”使用階層式的字
串比對法的深度封包檢測技術”的基本想法。 
在架構上我們把 filters 分成兩個 Tier-1 與 Tier-2，要 Tier-1 命中後，才會進行 
Tier-2 的比對。在 Tier-1 filter 的建構方面，我們把所有的 patterns 當母集，尋找其”常用
共同子 pattern 集”，也就是說，我們假設在一群的 patterns 中間，必定有”常用共同子 pattern 
集”，我們把這個” 常用共同子 pattern 集” 找出來，用它來建構我們的 Tier-1 filter。由於
這個子 pattern 集是共通的，所以可以被這個子集所濾除的，表示它也必定不存在於母集
中，因此這個 Tier-1 filter 也可以看成一個 bloom filter。 
在 Tier-2 filter 的建構方面，假設我們不需要搜尋全部的 pattern 母集，應該可以減少
搜尋的時間，所以我們試著把母集依照共同子 pattern來分群 (clustering)，接著我們把分群
後的 pattern set 獨立建成個別的 WM 收尋結構。如此一來，封包就算通過 Tier-1 的 
filter，它到 Tier-2 的時候，也不用比全部的 pattern 母集，只要比部分即可，這樣就可以
大幅增加入侵偵測的效率。 
圖二十一為尋找常用常用共同子 pattern 集的演算法，圖二十二為 Tier-1 Filter與 Tier-2 
的比對演算法，圖二十三為在 pattern 數為 200、2500、5000 時的比對所需時間比較圖，
由圖可知，我們的 EHMA 技術所需要的時間不管在何種量級的 pattern 數環境下，所需用
的比對時間，都比其他演算法來得少，所以我們所提的方法是優於其他的。 
 
 
 21
 
圖二十三：在 pattern 數為 200、2500、5000 時的比對所需時間比較圖 
四、 結果與討論 
本計劃第一年度著重在提昇加速字串比對的效能，因為字串比對是最為核心也是影響
系統表現甚鉅的一項技術。我們在這一年中發現了 DFA table 中魔術狀態這個特性，並成
功地設計出演算法及硬體架構。以下列出本計畫第一年度的成果： 
 發現魔術狀態，並善加利用來加以增進效能。 
 提出較為便宜的硬體架構但不失高效能，適用於中小企業及一般家庭網路的環境。 
 演算法可依不用需求調整，可在硬體花費與效能間取得平衡，彈性大。 
 成功發明用以壓縮 DFA table 的方式。 
 所有演算法皆有實作軟體以評估其有效性及實作 verilog 模擬硬體環境。 
 於重要國際會議 IEEE ICC2007 發表兩篇論文 [4], [5]。 
字串比對是最為核心也是影響系統表現甚鉅的一項技術。第一年的方法中，針對所有
非魔術狀態，並沒有做任何的處理，而是靠著在外部記憶體中擺一個原始的 AC 狀態表來
完成非魔狀態時的狀態轉換。第二年我們對這個部分加以改進，以期減小記憶體的需求。
以下列出本計畫第二年度的成果： 
 結合「部分狀態重新編碼」與第一年我們所發現的魔術狀態，我們提出了一個適
用於家用分享器及嵌入式系統等資源較為受限的環境下的高速字串比對演算法。 
 將『深層封包內容檢測』(Deep Packet Content Inspection)轉化為『最長前置字串比
對』(Longest Prefix Matching，簡稱 LPM)的問題。 
 成功發明用以中國剩餘定理來改進 AC-Bitmap。 
 提出機器學習即時分類演算法。 
 於重要國際會議 IEEE GLOBECOM 2007發表一篇論文 [6]。 
 於重要國際會議 IEEE ICC 2008發表一篇論文 [7]。 
 23
六、參考文獻 
[1] T. Nishimura, S. Fukamachi, and T. Shinohara, “Speed-up of Aho-Corasick Pattern 
Matching Machines by Rearranging States,” SPIRE,  Nov.2001, pp. 175-185. 
[2] [Online]. Available: http://www.defcon.org 
[3] [Online]. Available: http://www.packetfactory.net/projects/ISIC/ 
[4] Nen-Fu Huang, Yen-Ming Chu, Chen-Ying Hsieh, Yih-Jou Tsang, “A Deterministic 
Cost-effective String Matching Algorithm for Network Intrusion Detection systems,” IEEE 
ICC2007, Glasgow, Scotland, UK, June 2007. 
[5] Nen-Fu Huang, Yen-Ming Chu, Chi-Hung Tsai, Yih-Jou Tsang, “A Novel Algorithm and 
Architecture for High Speed Pattern Matching in Resource-limited Silicon Solution,” IEEE 
ICC2007, Glasgow, Scotland, UK, June 2007. 
[6] Nen-Fu Huang, Yen-Ming Chu, Yen-Min Wu, “Performing Packet Content Inspection by 
Longest Prefix Matching Technology,“ IEEE GLOBECOM2007, Washington DC, USA, 
November 2007。  
[7] Nen-Fu Huang, Gin-Yuan Jai, and Han-Chieh Chao, “A High Accurate Machine-Learning 
Algorithm for Identifying Application Traffic in Early Stage,” IEEE ICC2008, Beijing, 
China, May 2008. 
[8] Tzu-Fang Sheu, Nen-Fu Huang, Hsiao-Ping Lee, “A Hierarchical Multi-pattern Matching 
Algorithm for Network Content Inspection,” Information Sciences (SCI), Vol. 174, Issue 14, 
July 2008, pp. 2880-2898. 
[9] Yen-Ming Chu, Nen-Fu Huang, Chi-Hung Tsai, and Chen-Ying Hsieh, ”A Software-based 
String Matching Algorithm for Resource-restricted Network System, IEEE Communications 
Letters (SCI), Vol.12., No.8, August 2008, pp.599-601. 
[10] Nen-Fu Huang, Hung-Shen Wu, and Guan-Hao Lin,"Identifying the Use of 
Data/Voice/Video-based P2P Traffic by DNS-query Behavior,”IEEE ICC2009, Dresden, 
Germany, June 2009. 
[11] Tzu-Fang Sheu, Nen-Fu Huang, Hsiao-Ping Lee, “In-depth Packet Inspection Using a 
Hierarchical Pattern Matching Algorithm,” IEEE Transactions on Dependable and Secure 
Computing (SCI), 2009. 
 25
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期： 年 月 日 
國科會補助計畫 
計畫名稱：具智慧型網路檢測技術之 XML安全交換器之研製 
計畫主持人：黃能富  教授 
計畫編號：NSC  95－2221－E－007－054－MY3          
學門領域：資訊學門一 
技術/創作名稱 針對資源受限的網路入侵偵測系統的字串比對演算法 
發明人/創作人 黃能富  朱彥明 
技術說明 
中文：字串比對演算法為網路入侵偵測系統的重要運作，我們提出
一個嶄新有效率使用記憶體的字串比對演算法，只需 AC演算法的
2%。 
英文：String matching is the most critical operation in network 
intrusion detection Systems (NIDS). This paper proposes a novel 
memory-efficient string matching algorithm that only requires around 
2% of the memory utilized in Aho-Corasick algorithm but has more 
than 4 times the throughput of state-of-the-art algorithm with very 
limited memory resource. The proposed algorithm is flexible to fit 
different resource constraints and performance requirements. 
可利用之產業 
及 
可開發之產品 
網路安全、入侵偵測、入侵防禦 
技術特點 
利用極少的記憶體紀錄魔術狀態的相關資訊，可大幅提高字串比對
的資源利用。 
推廣及運用的價值 
可用於入侵偵測系統，增快字串比對效能。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
a bitmap is used to record whether the next state exists or not 
for every symbol. In this way, the memory usage is decreased 
dramatically. Nevertheless, due to the time cost for popcount 
function and link list searching is expensive, the performance 
of AC-BITMAP implemented in software is bad. 
To accelerate AC algorithm, Toshio Nishimura proposed a 
method to rearrange states [13]. Since the states near to initial 
state are used more frequently, they will be gathered into the 
cache of CPU and therefore the CPU cache is utilized in a more 
efficient way. 
III. THE PATTERN-MATCHING WITH MAGIC STATE 
A. Automaton Mode 
Generally, the model of automaton-based model has two 
stages. Stage-1 is responsible for searching the next state in the 
state table by an index composed of current state and input 
symbol. Stage-2 takes the task that it identifies if the output 
next state from Stage-1 is an acceptance state and to fetch the 
corresponding matched pattern IDs. 
To fetch next state in the state table, it needs current state 
and input symbol. In this paper, all the symbols are ASCII 
codes, and there are 256 different symbols, so the bit size u of a 
symbol is 8. The bit size of state is determined by the number 
of states in this constructed automaton, and it is generally 16 or 
32. Take Snort2.4 pattern for example, there are 21584 states in 
the automaton, hence the bit size of state v is 16. 
Therefore, we can find that in an automata-based algorithm, 
the key point is the lookup of next state (Stage-1, the State 
Search). This paper focuses on this stage and tends to design 
cost effective tiny data structure. The state table in Stage-1 can 
be represented as the state transition matrix shown in Fig. 1. 
The variables u and v represent the bit sizes of the state and the 
symbol, respectively, and the rows and columns of the matrix 
represent the states in the machine and the symbols that are 
used by the strings and patterns, respectively. The element e(x,y) 
represents the appropriate next state when the current state y 
receives the input symbol x, and all the state transitions in the 
FSM can be stored in the matrix. In the remaining sections of 
this paper, the FSM will be represented in the form of matrix 
M. 
B. Data Structure Construction 
The data structures for our proposed algorithm (AC with 
Magic State Algorithm, denoted as ACMS) and the way to 
construct them are described as follows. The original AC 
algorithm constructs an NFA of given patterns, and then 
transforms the NFA into a DFA Transition Matrix M. A 
threshold state value is set to divide M into two parts that are 
respectively maintained by two data structures. Different from 
[13] which rearranges the whole NFA, if threshold state value 
is t, ACMS just rearranges state 0 to state t of a DFA. We call 
this process as renumbering, and the transformation process is 
presented as the Procedure R(t) shown in Fig. 2. Note that 
every time it renumbers a new state, it must trace the whole 
matrix M, and exchange new state and old state number. 














≡
−−−−−−
−−−
+
+−
−
−
−−
)12,12()12,22()12,1()12,0(
)22,12()22,0(
)1,(
),1(),(),1(
)1,(
)1,12()1,0(
)0,12()0,22()0,1()0,0(
vuvuvv
vuv
u
uu
eeee
ee
e
eee
e
ee
eeee
M
yx
yxyxyx
yx
"""
%%%%%
#%%%%#
#%%#
#%%%%#
%%%%%
"""
Figure 1.   Automaton Matrix M 
Procedure R(t) 
1:   new_state Å1; 
2:   y Å 0;   /* initial state */ 
3:   while new_state < threshold do 
4:      for x Å 0 to 2u – 1 do 
5:         old_state Å e(x, y); 
6:         for each element e(i, j) in M’ do/*old_state ↔ new_state */ 
7:            exchange_state(e(i, j), old_state, new_state); 
8 :         end for  
9:         for i Å 0 to 2u – 1 do   /* exchange row */ 
11:           exchange (e(i, old_state), e(i, new_state)); 
12:         end for 
13:      end for 
14:      new_state += 1; 
15:   end while 
Function exchange_state(state, old_state, new_state) 
16:   if e(i, j) == old_state 
17:     e(i, j) Å new_state; 
18:   else if e(i, j) == new_state 
19:      e(i, j) Å old_state; 
20:   end if 
Figure 2.   Pseudo code of Procedure R(t) 
12223415350080215281139269499
……
1852710598022995139269476
C
urrent State
1852710598021060139269500
1852710598021997139269475
……
2099852710598026065541419357
399652710598021527813926956
1852710598021511392690
……
185271059802151108882691
0x4D0x4C0x4B0x4A0x490x480x47
Input symbol
C
urrent State
 
3(a) State table before renumbering 
……
13586059585747555499
616050058499561317476
C
urrent State
……
61605958575655475
616059584845655500
616059585756550
476605958574755557
6160595857565556
……
130605912957561281
0x4D0x4C0x4B0x4A0x490x480x47
Input symbol
C
urrent State
 
3(b) State table after renumbering 
Figure 3.   The differences of Partial DFA table of Snort 2.4 before and after 
renumbering (t = 61) 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1293
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:18 from IEEE Xplore.  Restrictions apply. 
m : number of accesses to matrix MS. 
m  : number of accesses to matrix S. 
lD : latency of each access of matrix M’DFA. 
lMS : latency of each access of  MS 
lB : latency of each access of  B 
lS : latency of each access of S 
A. Memory Access Analysis 
Generally, the number of memory accesses is the key factor 
to evaluate the performance of string matching algorithms. We 
analyze this factor by considering software implementation 
and hardware implementation. 
1) Software implementation 
For software based implementation, algorithms are usually 
executed sequentially by the processor. Therefore, for the 
proposed ACMS algorithm, the total latency time required for 
memory accessing for an input string T of k symbols can be 
calculated as follows: 
mlmlnklnlL MSSBD ⋅+⋅+−+⋅= )( , where 
nlD ⋅ : latency of  querying the original DFA State table 
)( nklB − : latency of querying Bitmap Matrix B 
mlS ⋅ : latency of searching in matrix S 
mlMS ⋅ : latency of querying matrix MS 
This calculation only considers the memory access time 
without considering the instruction execution time of the 
accessed data. However, some of the instruction execution 
time is not negligible, especially for popcount function. Thus, 
if most of the current state values during state transitions are 
within the threshold state t, we can have a significantly 
improved performance. Furthermore, in case most of the state 
transitions are beyond t, we can still have good performance if 
most of them are magic states. Otherwise, we need to perform 
the popcount function which takes longer time and therefore 
the performance is impacted.  
2) Hardware implementation 
Considering the trade-off between performance and cost, 
here two possible hardware implementations are analyzed: off-
chip memory and on-chip memory. 
z Off-Chip memory: with hardware implementation, the 
accesses of Magic State Matrix MS and Bitmap Matrix 
B can be processed in the same time. Hence, as soon as 
next state is found as a magic state, these two matrices 
can be fetched simultaneously and the latency can be 
represented as: { }mlmlMax BMS ⋅⋅ , ---------------------- (1) 
If it is a non-magic state, the two matrices should be 
accessed one by one and the latency is mll SB ⋅+ )( . 
Consequently, the latency for off-chip memory 
implementation can be represented as 
{ } mllmlmlMaxnlL SBBMSD ⋅++⋅⋅+⋅= )(, .  
z On-Chip memory: assume the on-chip memory is large 
enough to store all the data structures used by the 
ACMS algorithm. Then lB will be equal to lM, 
and mlml BMS ⋅=⋅ . Refer to Equation (1), 
{ }mlmlMax BMS ⋅⋅ ,  can be briefly represented as mlB ⋅ . 
Therefore, the latency for on-chip memory 
implementation can be represented as: 
mllmlnlL SBBD ⋅++⋅+⋅= )(   ------------------------  (2) 
Although it takes a longer time to perform the popcount 
operation in software implementation, it can be performed 
rapidly by silicon circuit of hardware implementation. Thus, 
the performance of hardware implementation is dominated by 
the latency of accessing data structures. The hardware 
implementation also has the best performance when most of 
the current state values during state transitions are within the 
threshold state t. In case the on-chip memory is too small to 
store all the four matrices in ACMS, we can then arrange 
some of the matrices to be stored in off-chip memory to 
achieve as best performance as possible. 
B. Memory versus Throughput 
Usually to have better throughput, we need larger memory.  
In order to balance the time and the space (memory, especially 
the on-chip memory) of the proposed ACMS, we need to 
figure out the relation between the required memory space and 
its performance. A PC-based Windows XP system is used to 
evaluate this. The Snort 2.4 signatures with total 2389 patterns 
are employed and the Defcon9 traces [14] are adopted as the 
input strings. The evaluation results are demonstrated in Fig. 6; 
where we can see that the ACMS furnishes good scalability. 
The strategy of memory configuration can be adaptive for 
targeted speed demands. As expected, larger memory stands 
for higher throughput. In addition, a larger threshold state 
value t produces a larger DFA State Matrix and smaller 
matrices B and S. For t > 2048, the DFA State Matrix is the 
major memory contribution and therefore the total memory 
increases for larger value of t. For throughput, a larger 
threshold state value t offers a better throughput due to more 
state data can be stored in matrix M’DFA., and lD is the least 
latency compared with other parts. Also the renumbering 
process not only diminishes nodes stored in matrix S and 
memory usage, but also increases the frequency that state 
transits to magic state. This improves the performance too.  
V. IMPLEMENTATION AND SIMULATION 
A. Implementation Issues 
The ACMS algorithm is implemented by rewriting the AC 
implementation of Snort. The testing computer runs Windows 
XP and is a Pentium-4 3.0GHz with 1MB L2 Cache and 
768MB DDR SDRAM. Again, the Snort 2.4 signatures with 
0
2
4
6
8
10
12
0 128 256 512 1024 2048 4096 9192 21584
Threshold
M
em
or
y 
(M
By
te
)
0
20
40
60
80
100
120
140
160
180
Th
ro
ug
hp
ut
 (M
bp
s)
DFA State Matrix Bitmap Matrix
State List Throughput
 
Figure. 6 Comparisons between memory and throughput under different 
thresholds in ACMS 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1295
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:18 from IEEE Xplore.  Restrictions apply. 
hardware model with 2MB embedded SRAM and 200MHz 
clock frequency. The width of embedded memory is 128-Byte 
(1024-bit) that is the same with that used in [5]. As the memory 
usage of AC-DFA and AC-NFA both exceeded 2MB in 
SRAM, an additional memory module of DDR2-400 SDRAM 
with 200MHz clock frequency [15] is added. Fig. 9 depicts the 
hardware throughputs, measured by the number of memory 
accesses, from the viewpoint of electronic system level 
(ESL[16]), for different implementations. First of all, the 
performance of AC-DFA and AC-NFA does not increase a lot 
compared to their software implementations. This is because 
their data structures are too large to be stored into SRAM, and 
most of them are kept in slower SDRAM. Due to the data 
structure in AC-NFA contains extra fail state transitions 
compared that of AC-DFA, its throughput is clearly lower than 
that of AC-DFA. However, for AC-BITMAP, since its NFA 
data structures are compressed significantly and could be 
stored into the on-chip SRAM, it provides a much higher 
throughput (around 432Mbps) than that of AC-DFA. Based on 
previous analysis, the popcount function can be completed in 
very short time by hardware circuit. Therefore the performance 
of an algorithm is determined and dominated by the number of 
memory accesses and access latency. The proposed ACMS 
needs less number of memory accesses and most of these 
accesses can be completed within the fast on-chip memory. 
This also explains why the ACMS offers best throughput 
(1456Mbps) which is around 3.5 (1456/432) to 10 times faster 
than other algorithms. In summary, the ACMS is an efficient 
string matching algorithm that successfully balances memory 
and speed. 
VI. CONCLUSION 
This article introduces a novel string matching algorithm, 
named AC with Magic State Algorithm (ACMS), that using 
only a tiny memory and provides high throughput for either 
hardware or software implementations. ACMS is designed 
based on the observation of magic states in the deterministic 
finite state automata. Two features, renumbering and bitmap 
are employed to make a balance between the required memory 
and targeted speed.  
ACMS is a practical string matching algorithm and 
especially suitable for identifying network applications. 
Evaluation and experimental results show that the overall 
efficiency gain of ACMS is over at least 11 times better than 
that of the other researches. Through compacting mechanism, 
we show that the entire data structure constructed from the 
Snort2.4 (2389 patterns with 35K characters) is less than 2MB, 
which is less than 10% of the original memory size. Thus, the 
entire automaton not only can be stored into the on-chip 
memory for fast accessing, but also suitable to embedded 
systems with restricted resources. Compared to past NFA-
based string matching studies with similar memory 
requirement, the proposed ACMS algorithm achieves more 
than 3.5 times throughput in hardware performance. As for 
software implementation, the performance of the ACMS is 
over at least 21 times better than that of state-of-the-art 
researches. 
REFERENCES 
[1] [Online]. Available: http://www.snort.org 
[2] R.S. Boyer and J.S. Moore, “A fast string searching algorithm,” 
Communications of the ACM, vol. 20, Session 10, Oct. 1977, pp. 761–
772. 
[3] A.V. Aho and M.J. Corasick. “Efficient string matching: An aid to 
bibliographic search,” Communications of the ACM, 18(6), 1975, 
pp.333–340. 
[4] Sun Wu and Udi Manber, “A fast algorithm for multi-pattern searching,” 
Tech. Rep. TR94-17, Department of Computer Science, University of 
Arizona, May 1994. 
[5] N.Tuck, T.Sherwood, B. Calder, G. Varghese, “Deterministic memory-
efficient string matching algorithms for intrusion detection,” In 
Proceedings of the IEEE Infocom Conference, 2004, pp. 333–340. 
[6] Lin Tan and Timothy Sherwood, “Architectures for Bit-Split String 
Scanning in Intrusion Detection,” IEEE Micro: Micro's Top Picks from 
Computer Architecture Conferences (IEEE Micro - top pick), January-
February,2006, pp. 110-117. 
[7] S. Kumar, S. Dharmapurikar, F. Yu, P. Crowley and J. Turner,” 
Algorithms to accelerate multiple regular expressions matching for deep 
packet inspection,” ACM SIGCOMM Computer Communication Review, 
vol. 36, Issue 4, 2006, pp.339-350. 
[8] J. van Lunteren, "High-Performance Pattern-Matching for Intrusion 
Detection," Proceedings of IEEE INFOCOM'06 , Barcelona, Spain, 
April 2006. 
[9] S. Dharmapurikar, P. Krishnamurthy, T. Sproull, J. Lockwood, “Deep 
packet inspection using parallel Bloom filters,” IEEE Micro, vol. 24, No. 
1, 2004, pp. 52–61. 
[10] J. Moscola, J. Lockwood, R. P. Loui, and M. Pachos, “Implementation 
of a content-scanning module for an internet firewall,” Proceedings of 
IEEE Symposium on Field-Programmable Custom Computing Machines 
(FCCM), Napa, CA, April 9–11, 2003, pp. 31–38. 
[11] F. Yu, R.H. Katz , T.V. Lakshman, “Gigabit rate packet pattern-
matching using TCAM,” Proceedings of the network protocols, 12th 
IEEE International Conference on (ICNP’04), Oct. 5–8, 2004, pp.174–
183. 
[12] M. Norton, “Optimizing Pattern Matching for Intrusion Detection,” Jul. 
2004. [Online]. 
[13] T. Nishimura, S. Fukamachi, and T. Shinohara, "Speed-up of Aho-
Corasick Pattern Matching Machines by Rearranging States,"  
Proceedings of  Eighth International Symposium on  String Processing 
and Information Retrieval (SPIRE),  Nov. 13-15,2001,pp. 175-185 
[14] [Online]. Available: http://www.defcon.org 
[15] [Online]. Micron Tech. DDR2 SDRAM Components: 
MT47H16M16BG-37E,  
http://www.micron.com/products/partdetail?part=MT47H16M16BG-37E 
[16] SystemC [Online]. Available http://www.systemC.org 
[17] S. Berkovich, G. Lapir, M. Mack, "A bit-counting algorithm using the 
frequency division principle,"  Software - Practice And Experience, Vol. 
30, Issue 14, pp. 1531-1540, 2000. 
 
0
200
400
600
800
1000
1200
1400
1600
AC-DFA AC-NFA ACMS AC-Bitmap
Th
ro
ug
hp
ut
 (M
Bp
s)
 
Figure 9.   Comparison of throughput for different hardware implementation 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1297
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:18 from IEEE Xplore.  Restrictions apply. 
reference is needed, and the time-complexity is guaranteed to 
be O(n), where n is the length of input string. However, the 
demanded memory space is large. On the other hand, when 
NFA is adopted, for every state, not every symbol has the 
corresponding next state. In this way, failure path is needed, 
and time-complexity then becomes O(n + k), where k is the 
number of times which failure path is taken. 
B. The AC Algorithm with Bitmap (AC-BITMAP) 
N. Tuck, et al proposed a modified AC algorithm by 
applying IP routing lookup techniques [5]. According to the 
form of the NFA in the AC algorithm, they used bitmaps that 
correspond to symbols to record the state transition of the non-
failure path. In this way, every node in the finite automaton 
only uses a pointer pointing to the next state list instead of 
allocating all the pointers to the next state. Thus, AC-BITMAP 
can decrease a great deal of the demanded memory for 
implementing NFA. However, the performance of AC-
BITMAP implemented by software is bad because of the 
extremely heavy cost of popcount and the search time of 
linked list. And when it is implemented by hardware, all data 
structures must be stored in wide embedded memory for 
performance issue, which makes high cost. 
C. Parallel Bloom Filter 
Bloom Filter is an efficient data structure enabling fast 
membership query by computing multiple hashing functions 
with tunable false positive rate [9]. A hardware-based parallel 
Bloom filter (PBF) technique is also proposed by 
Dharmapurikar et al [6]. It makes extremely small data 
structures stored in embedded on-chip memory for fast packet 
filtration, and thus accelerates string matching. Such filtration 
method is also adopted by [10] for accelerating string 
matching with low-computation software solution. 
III. MAGIC STATE-BASED HEURISTIC (MSH) ALGORITHM 
A general automaton-based string matching model has two 
stages as shown in Fig. 1. The Stage-1 performs state 
transition by state table lookup. The Stage-2 searches the 
pattern ID if the output of the Stage-1 is an acceptance state. 
For illustration, Fig. 2 shows the execution of Stage-1. It 
takes the current state y and the input symbol x as an index = 
{x : y} to get the next state. Because all symbols used in this 
paper are ASCII codes, the bit size u of a symbol is 8. The bit 
size v of a state is decided by the number of all patterns and is 
generally equal to 16 or 32. For example, the total amount of 
states constructed from Snort 2.4 patterns is 21584 and hence 
the bit size v of a state is 16. 
Since the key operation to automaton-based model is state 
lookup in Stage-1, our research focus on this stage. The 
State_Table in Stage-1 can be represented as state transition 
matrix shown in Fig. 3. The variables u and v stand for the bit 
sizes of a symbol and a state, respectively. The rows of the 
matrix represent the states in the machine while the columns 
of the matrix represent the symbols used by the strings and 
patterns. The element a(x,y) represents the next state when the 
current state y receives the input symbol x, and all the state 
transitions in the automaton can be recorded in the matrix. 
A. Magic State 
It is interesting to see that when A is a DFA, for each 
symbol x, most of a(x,y) have the same value for different 
current state y. We call these elements “magic state”. Thus, we 
define that for each input symbol x, the magic state of x 
(denoted as ms(x)) is the next state that appears most 
frequently for all the next states. Moreover, some data 
structures are also designed to keep the magic state 
information to improve performance. When performing state 
transition, if we know that the next state is a magic state, then 
the state table lookup can be skipped. For example, a bitmap 
matrix B can be used to indicate whether an element in 
automaton matrix A is a magic state, as shown in Fig. 4. 
B. Matrices Construction 
There are three matrices in our proposed MSH algorithm: 
Automaton Transition Matrix A, Magic State Matrix M, and 
Heuristic Index Matrix H. As mentioned before, A stores the 
next state for every symbol and current state pair {x, y}. M 
stores the corresponding magic state ms(x) in the element m(x,0) 
 
Figure 1. Proposed model for automaton-based string matching algorithm. 
 
Figure 2. Illustration of lookup in Stage-1.














≡
−−−−−−
−−−
+
+−
−
−
−−
)12,12()12,22()12,1()12,0(
)22,12()22,0(
)1,(
),1(),(),1(
)1,(
)1,12()1,0(
)0,12()0,22()0,1()0,0(
vuvuvv
vuv
u
uu
aaaa
aa
a
aaa
a
aa
aaaa
A
yx
yxyxyx
yx
"""
%%%%%
#%%%%#
#%%#
#%%%%#
%%%%%
"""  
Figure 3. Automaton Matrix. 














≡
−−−−−−
−−−
+
+−
−
−
−−
)12,12()12,22()12,1()12,0(
)22,12()22,0(
)1,(
),1(),(),1(
)1,(
)1,12()1,0(
)0,12()0,22()0,1()0,0(
vuvuvv
vuv
u
uu
bbbb
bb
b
bbb
b
bb
bbbb
B
yx
yxyxyx
yx
"""
%%%%%
#%%%%#
#%%#
#%%%%#
%%%%%
"""
 
Figure 4. Bitmap Matrix corresponding to magic state for matrix A. 
b(x, y) Å 1, if a(x, y) = ms(x); 
b(x, y) Å 0, if a(x, y)≠ms(x); 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1287
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:22 from IEEE Xplore.  Restrictions apply. 
HitRate =
k
h
k
i
yx∑−
=
1
0
),(
, where x=  mit 2/ , y=  nistate 2/)( , 
state(i)= )( )1(,1 −− ii stateta when 0<i<k and state(0) = 0. Fig. 8 shows 
the evaluation results of memory size used by matrix M 
and,matrix H as well as the HitRate with different values of m 
(n is fixed to zero). As expected, with a smaller value of m, it 
consumes more memory space, but we have a higher HitRate. 
Nevertheless, it is interesting to see that the HitRate decreases 
slowly even when the memory size used is reduced 
dramatically for larger values of m. For example, with m = 0, 
the memory size used is 675KB with a hit rate of 95%.  With 
m = 4, the memory size used is now reduced to only 42KB, 
but we can still have a hit rate of 85%. Moreover, with m = 8, 
the hit rate is still up to 46% even now only a very tiny 
memory of 3 KB is used.  
A. Magic State 
The number of patterns in Snort2.4 is 21,584.  With 256 
input symbols, the total amount of elements in matrix A is 
21584*256 = 5,525,504. We found that among these elements, 
there are 5,243,748 magic states, i.e. 94.9% of the elements in 
A are magic states. It is expectable to have good benefit if this 
unique feature can be employed properly. 
B. HitRate vs Compression Ratio (CR) 
To see how different values of m and n impact the HitRate, 
a software is also implemented. It is clear that a higher CR 
conducts a lower hit rate. However, it is interesting to see that 
even with the same CR, we may have different hit rates for 
different combinations of m and n. The results for the case of 0 
≦ n ≦ 16, 0 ≦ m ≦ 8 with Snort2.4 patterns and Defcon9 
input strings are depicted as Fig. 9. For example, with m+n = 
4, we have five combinations of (m, n): (4,0), (3,1), (2,2), (1,3) 
and (0,4). And the HitRates for them are 85%, 70.2%, 68%, 
70.8% and 70.6%, respectively. The largest gap is 85%-
68%=17%. 
C. False Negative 
As described in the previous section, hit rate is 95% and 
85% when (m, n) = (0,0) and (4,0) respectively. So we know 
that when (m, n)=(4,0), there are 15% (100% - 85%) state 
transitions that we do not sure the next state is a magic state 
and need to access Automaton Transition Matrix A. However, 
among these 15% state transitions, only 5% (100%-95%) are 
non-magic states as we can see when (m, n) = (0, 0). Thus, 
10% (15% - 5%) state transitions is false negative due to the 
AND logic operation for every elements within the same block 
of size 2m+n when constructing matrix H. 
D. Theoretical Appraisal 
Let  ρM stand for the hit rate, and AHM AAA ,, be the time 
required to access an element of matrix M, matrix H and 
matrix A, respectively. For an input string of k symbols, the 
total time required for the state transitions (denoted as ST) in 
our proposed algorithm is as follows: 
mn
k
i
nA
k
i
k
i
MMH whereST ρρριριι −=×+×+= ∑∑ ∑ −
=
−
=
−
=
1,
1
0
1
0
1
0
 
If matrix M and matrix H can be accessed concurrently, the 
above equation is rewritten to: 
∑ ∑∑∑∑−
=
−
=
−
=
−
=
−
=
×+=×+×=
1
0
1
0
1
0
1
0
1
0
),(
k
i
k
i
nA
k
i
H
k
i
nA
k
i
MMHMAXST ριιριριι  
For algorithms using matrix A only without employing the 
unique feature of magic state, we have ∑−
=
=
1
0
k
i
AST ι
. 
Consequently, the proposed algorithm has a throughput gain 
of ( ∑ ∑∑ −
=
−
=
−
=
×+
1
0
1
0
1
0
)/(
k
i
k
i
nAH
k
i
A ριιι . 
V. PROTOTYPE AND SIMULATION 
A system prototype of the proposed algorithm is also 
constructed. Tables are used to store matrices A, M and H. 
Assume matrix M consists of r rows and c columns, and every 
element is 16-bit in length. Then a memory space of size 16-
bit * 2r * 2c is allocated to store the whole matrix M ordered by 
{x index: y index}. Let baseptr denote the base address of this 
continuous memory space. Assume the memory width is 16-
bit, then the element m(x,y) can be accessed with address 
baseptr+offset, where offset = yrx +×− )1( . Let these 
memory spaces storing matrices A, M and H be denoted as 
Magic State Table (MST), Heuristic Index Table (HIT) and 
Automaton Transition Table (ATT), respectively. As the main 
operation of our algorithm is table lookup, it is very suitable 
for hardware implementation. Also as the MST and HIT are 
tiny, they can be stored into the on-chip memory. The ATT is 
relative too large (10.54MB) to be stored in on-chip memory, 
and can be stored in DDR2 SDRAM. A simulation by verilog 
model is executed based on Altera EP2C35 FPGA and Micro 
MT47H16M16BG-37E DDR2 SDRAM [12][13] with (m, n) = 
Input string: Defcon9
Ketword patterns: Snort2.4
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8
m  (n =0)
H
it 
ra
te
(%
)
0
100
200
300
400
500
600
700
800
M
em
or
y 
(K
B)
hit rate(%)
memory(KB)
 
Figure 8. Evaluation of tradeoff between hit-rate and memory requirement.
Input String: Defcon9
Ketword Patterns: Snort 2.4
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8
m
hi
t r
at
e(
%
)
n=0
n=1
n=2
n=3
n=4
n=5
n=6
n=7
n=8
n=9
n=10
n=11
n=12
n=13
n=14
n=15
n=16  
Figure 9. Distribution of the hit rate with different values of m and n. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1289
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:22 from IEEE Xplore.  Restrictions apply. 
engine works at 200MHz, respectively. This arrangement is 
due to the trade off between clock speed and silicon area 
constrained by the FPGA synthesis tool. 
From the simulation result, we can see that the proposed 
MSH-2 outperforms than other solutions. Although the 
throughput of MSH-1 is slightly less than that of AC-BITMAP 
(571Mbps vs. 600 Mbps) under 200MHz clock rate, the MSH-
1 requires less on-chip memory (42KB vs. 2MB). The AC-
BITMAP mechanism compresses the AC finite state machine 
into 2MB which can be stored in on-chip memory of a high-
end FPGA [5]. Also this mechanism assumes that the on-chip 
memory width is up to 128*8=1024-bit. These provide a high 
performance solution. Nevertheless, since this design 
consumes large amount of silicon resources, especially in 
FPGA-based design, the FPGA-based solution is expensive. 
Of course, in case cost is sensitive, the solution can be 
implemented on off-chip high-speed memory (for example, 
2MB SSRAM). Unfortunately, this off-chip memory design 
faces the problem of very low throughput. The proposed 
MSH-1 scheme obtains the balance between the cost and 
performance due to only a tiny on-chip memory (42KB) is 
required. Thus, a very cheap FPGA is enough to implement 
the proposed MSH model. In the current MSH model, DDR2 
memory is also required to store the ATT (10MB). Actually, 
by utilizing the feature of Magic State more intelligently, the 
total memory size required by the proposed MSH scheme can 
be further reduced to less than 2MB [15] so that all the data 
structures are stored into on-chip memory. 
We can also see that the MSH-1 scheme outperforms than 
the PBF with the same required on-chip memory size. Take 
used memory and resources into consideration, since PBF is a 
search filter, its output still needs to be delivered to another 
string matching algorithm to find the matched patterns. So 
under the same performance, the required memory size 
depends on what string matching algorithm it applies. As 
shown in Fig. 13, MSH-1 has better performance than 
PBF(200-1) under the same environment, i.e. 42KB on-chip 
memory, one PME engine, with 200MHz clock rate. Besides, 
since the hardware implementation of PBF requires multiple 
hashing functions and more complicated circuit design, larger 
silicon resource are consumed and the clock rate is therefore 
limited. The principles and operations of the proposed PME 
engine are straightforward, i.e. it consumes less silicon 
resources and can support a higher clock rate compared to 
PBF in hardware implementation. 
One of the ways to upgrade the performance of MSH to 
multiple Gbps is to partition the HIT and stored them into 
different banks. This reduces the collisions when more than 
two PMEs try to access the on-chip data at the same time.  
VI. CONCLUSION 
An efficient and scalable pattern matching algorithm, 
named Magic State-based Heuristic Algorithm (MSH), has 
been proposed in this paper. Combining an architecture using 
the magic state observed from DFA with modified AC 
algorithm, the MSH algorithm requires only a tiny on-chip 
memory while providing high hardware performance.  
MSH is a practical algorithm that can provide Gbps pattern 
matching throughput while can be easily implemented on a 
very cheap FPGA (USD 25 for example). This is especially 
suitable for the low-cost network appliances in SMB/SOHO 
markets. Prototype and experimental results show the overall 
efficiency of MSH is at least 7 times faster than that of the 
baseline model. Consequently, MSH enables the design of 
cost effective FPGA-based accelerator to furnish over 1Gbps 
throughput in multi-session networks. 
REFERENCES 
[1] [Online]. Available: http://www.snort.org 
[2] R. S. Boyer and J. S. Moore, “A fast string searching algorithm,” 
Communications of the ACM, vol. 20, Session 10, Oct. 1977, pp. 761–
772. 
[3] A. V. Aho and M. J. Corasick. “Efficient string matching: An aid to 
bibliographic search.” Communications of the ACM, 18(6), 1975, 
pp.333–340. 
[4] Sun Wu and Udi Manber, “A fast algorithm for multi-pattern searching,” 
Tech. Rep. TR94-17, Department of Computer Science, University of 
Arizona, May 1994. 
[5] N. Tuck, T. Sherwood, B. Calder, G. Varghese, “Deterministic memory-
efficient string matching algorithms for intrusion detection,” In 
Proceedings of the IEEE INFOCOM Conference, 2004, pp. 333–340. 
[6] S. Dharmapurikar, P. Krishnamurthy, T. Sproull, J. Lockwood, “Deep 
packet inspection using parallel Bloom filters,” IEEE Micro, Vol. 24, No. 
1, 2004, pp. 52–61. 
[7] J. Moscola, J. Lockwood, R. P. Loui, and M. Pachos, “Implementation 
of a content-scanning module for an internet firewall,” Proceedings of 
IEEE Symposium on Field-Programmable Custom Computing Machines 
(FCCM), Napa, CA, April 2003, pp. 31–38. 
[8] F. Yu, R.H. Katz, T.V. Lakshman, “Gigabit rate packet pattern-matching 
using TCAM,” Proceedings of the network protocols, 12th IEEE 
International Conference on (ICNP’04), Oct. 2004, pp.174–183. 
[9] B. Bloom, “Space/time Trade-offs in Hash Coding with Allowable 
Errors,” Communications of the ACM, Vol. 13, No. 7, 1970. 
[10] N.F. Huang and Y. M. Chu et al, “A non-Computation Intensive Pre-
filter for String Pattern Matching in Network Intrusion Detection 
Systems,” IEEE GLOBECOM 2006, San Francisco, USA, November 
2006. 
[11] [Online]. Available: http://www.defcon.org 
[12] [Online]. Altera™ Cyclone™ FPGA, Available: 
http://www.altera.com/products/devices/cyclone2/cy2-index.jsp 
[13] [Online]. Micron Tech. DDR2 SDRAM Components: MT47H16M 
16BG-37E, http://www.micron.com/products/partdetail?part=MT47H16 
M16BG-37E 
[14] C. Courcoubetis and V. A. Siris, “Measurement and analysis of real 
network traffic,” Proceedings of the 7th Hellenic Conference on 
Informatics (HCI'99), Aug. 1999. 
[15] N. F. Huang and Y. M. Chu et al, “Deterministic Cost-efficient String 
Matching Algorithm for Resource-restricted System,” IEEE ICC 2007, 
Scotland, June 2007. 
0
200
400
600
800
1000
1200
Ba
sel
ine
AC
-B
IT
PM
AP
(20
0)
M
SH
-1
M
SH
-2
PB
F(1
00
-4)
PB
F(2
00
-1)
Algorithms
Throughput (Mbps)
 
Figure 13. Throughput of different algorithms. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2007 proceedings. 
1291
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:22 from IEEE Xplore.  Restrictions apply. 
 
 
solve the problem of LPM [15]. 
A. Automaton-based Algorithm 
Although the WM algorithm is superior to all other multiple- 
pattern matching algorithms in the average case, it only shifts 
one byte at a time in the worst case. Malicious elements (like 
hackers) can exploit this feature to attack systems. A research 
[16] employing a synthetic input string shows that the 
performance of such algorithms are considerably suffered from 
algorithm attacks, which can be even declined to that of single 
pattern string matching. Therefore, general network appliances, 
especially those related to network security, usually avoid 
adopting WM algorithm since the device may become a victim 
of denial of service attack.  
In order to avoid these problems, the well-known AC 
algorithm becomes a popular choice. It constructs an FSA based 
on the given keyword patterns and performs string matching by 
state transition. In the worst case, it still has a deterministic 
processing rate. In AC-based automata, each state requires 256 
next state pointers, a pattern pointer, and a corresponding 
pattern ID (PID). Besides, the NFA structure needs to maintain 
another failure pointer to record the failure path. To reduce the 
huge amount of memory space requirement, Norton [17] used 
special compression methods to decrease the memory space 
when implementing Snort rules on AC; this is the most popular 
AC implementation nowadays. However, it also lowers the 
search performance. Besides, Nishimura proposed a speed-up 
method for the AC algorithm by rearranging the states [18]. 
N. Tuck, et al modified the NFA-based AC algorithm and 
used bitmaps that correspond to symbols to record the state 
transition of the non-failure path [16]. In this way, every node in 
the finite automaton only uses a pointer pointing to the next 
state list instead of allocating all the pointers to the next state. 
Recently, J.V.Lunteren, et al [19] presented a BFSM-based 
pattern-matching (BFPM) scheme based on a BaRT routing 
table search algorithm. BFSM is based on state transition rules 
that include conditions for the current state and input symbols, 
which are assigned priorities. However, this scheme is not 
suitable for software implementation due to its multiple FSM 
data structure. 
III. STRING MATCHING AS LONGEST PREFIX MATCHING  
The proposed model for performing string matching by 
longest prefix matching consists of two stages. The first stage 
performs state transition by LPM-based table lookup. The 
second stage searches the pattern ID if the output of the first 
stage is an accepting state.  
Fig. 1 explains the execution of the first stage, state table 
lookup stage. Since the next state is generated from the current 
state ψ and the input symbol tr, Index = {tr:ψ}. Because all 
symbols used in this paper are ASCII codes, the bit size u of a 
symbol is 8. The bit size v of a state is decided by the number of 
whole characters of all patterns and is generally equals to 16 or 
32. For example, the total amount of states constructed from 
Snort 2.4 patterns is 21595 and hence the bit size v of a state is 
16. The bit size w of the index is then given by w = u + v = 8 + 16 
= 24 bits. The index is generated by a conjunction of the input 
symbol that enters the FSM and the current state. It is used to 
lookup the corresponding next state in state table. The major job 
of the second stage, pattern search, is to determine whether the 
output state of state table lookup is an accepting state or not. If it 
is, then find the matched pattern ID.  
We focus on state table lookup here, as it is the key operation 
to the automaton-based algorithm. In fact, the fundamentality of 
all kinds of FSA operations is based on state transitions. The 
state table in table lookup stage can be represented as a state 
transition matrix depicted in Fig. 2. The variables u and v 
represent the bit sizes of the symbol and state, respectively. The 
element e(x,y) represents the appropriate next state when the 
current state y receives the input symbol x, and all the state 
transitions in the FSM can be recorded in the matrix. In the 
remaining sections of this paper, the FSA will be represented in 
the form of matrix M. This study concentrates on problems 
involving M for the DFA. We observed two interesting key 
features: symbol-wise prefix and magic state, which are 
described as follows. 
A. Symbol-wise Prefix 
Among the IP lookup algorithms, an algorithm named 
DIR-24-8-BASIC was proposed by P. Gupta [13]. In simple 
words, the algorithm segments the 32-bit IP address a.b.c.d into 
two parts: the 24-bit part (a.b.c) and the 8-bit part (d). By a 
straightforward method, the first 24 bits of the IP address is 
employed to function as the index for 224 entries. The entry 
content of a prefix whose length equals to or less than 24 bits is 
the next hop, and for prefixes with length greater than 24 bits, 
the next hop will be found in the level-2 table whose index 
comprises the last 8 bits of the IP address. The concept of this IP 
lookup algorithm can be adopted straightforwardly to use 24 
bits as the index in our proposal. This makes both the proposed 
algorithm and that in [13] have almost the same table structure 
and operations. The original 224 elements in the automaton 
matrix can be rearranged as shown in Fig. 3. 
Conventionally, the next state is decided by the current state 
and the input symbol in the FSM. In the scheme shown in Fig. 1, 
we straightforwardly assume the combination of the current 
state and input symbol to be the index and determine the next 
state using direct-lookup mechanism. The next state and Index 
are seen as the next hop and IP address (most significant 24-bit 
portion) in the LPM, respectively. 
Now, let us have some critical definitions: Let Index(x,y) = 
{w23, w22,…, w16, w15,…, w1, w0}, where w23 represents the 24th 
bit, which is the most significant bit (MSB), and w0 represents 
1011100101011010111111111111111111111111
…
…
…
…
…
…
0000000000000001011010101111001100010010
………
0111010100011010000000000000000000000000
Next StateIndex
…
…
…
…
…
…
………
Symbol Current 
State
u bits v bits v bits
State Table
 
Fig. 1. Illustration of state table lookup. 
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.
12
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:24 from IEEE Xplore.  Restrictions apply. 
 
 
TCAM-based algorithms, have excellent performance [20]. For 
example, consider the approach proposed by Lim [21]; it 
divides a prefix into groups based on the prefix length and sends 
each group of prefixes to the corresponding engine. Since the 
prefixes are grouped based on a maximum of five lengths, we 
need only five engines to process the lookup operation. 
C. Demonstrations 
From the experiments in Fig. 5 and Fig. 6, we can see that the 
symbol-wise prefix is similar to the IP routing prefix and can 
effectively decrease the index requirement. We have 
straightforwardly used direct-lookup mechanism for string 
matching, and next, we would like to further prove that dealing 
with the symbol-wise prefix is the same as dealing with the IP 
prefix through applying a routing lookup algorithm.  
M. Degermark presented a data structure for forwarding table 
named Lulea algorithm designed for fast routing lookups [12]. 
By an ingenious design, it enables the IP routing table to be 
compressed for storing in the cache (500~600 KB for 40,000 
entries) of a processor through three elegant structures: code 
word array, base index array, and maptable. The algorithm got 
conceit and spawned an industry of follow-up articles [22]. 
Therefore, it is employed as the longest prefix matching 
algorithm without changing its algorithm and data structure. 
In Lulea [12], the routing prefix is segmented into three 
levels—one 16-bit level and two 8-bit levels. The data structure 
is mainly placed in level-1. Reviewing the discussion on the 
symbol-wise prefix, concerning Snort 2.4, there were 256 
symbols and no more than 65,536 total states and the longest 
prefix was 24-bit long. If we segment the symbol-wise prefix 
into two levels of 8 bits and 16 bits, we can find its 
advantageous like the case employing Huang’s NHA [14]. This 
means that the second level of the symbol-wise prefix is exactly 
suitable for the design of Lulea’s level-1. Therefore, we have to 
maintain 256 pairs of code word and base index arrays but just 
one maptable data structure.  
When processing the next state lookup, we use the first 8 bits 
(w23~w16) of Index to decide which code word array and base 
index array are needed for the lookup. We then execute level-1 
of the Lulea algorithm with the last 16 bits (w15~w0) to obtain 
the next state value. Considering Fig. 7 as an illustration, the 
first 8 bits representing the symbol that decides which code 
word array and base index array are used; these arrays are 
represented by hollow arrows in the figure. Next, we perform a 
lookup by employing the Lulea algorithm; this is represented in 
the dotted box on the right-hand side in the figure. By using the 
symbol-wise prefix, the string-matching problem can be 
successfully handled by the LPM algorithm. 
IV. EVALUATION AND ANALYSIS 
To evaluate the effectiveness of the proposed analogical 
relationship as well as the transformation procedure between 
string matching and LPM, the Snort 2.4 is taken as the patterns 
and the Defcon9 traces [23] are used as the input strings.  
Table I is the evaluation result for four implementations from 
the viewpoint of memory size Em (MB), software throughput 
rate Et (Mbps), and overall efficiency E. The efficiency value is 
defined as E = Et/Em, which represents throughput rate 
generated by each unit of memory. AC-DFA and AC-NFA 
represent the DFA and NFA in the traditional AC algorithm, 
respectively. AC-Bitmap is the algorithm for the NFA with 
bitmap that was proposed in [16]; while AC-Lulea refers to the 
implementation of the LPM algorithm [12] previous introduced. 
In Snort 2.4, the number of signatures (keyword patterns) is 
around 2390 with a total of 35K characters. Through Norton’s 
AC implementation, the content is transferred to a DFA of 
21,595 states and among them, 8,477 states are accepting states.  
The required memory space is approximately 56 MB according 
to the studies that have analyzed the AC algorithm [16][17]. In 
addition, by a comparison with several results of traces in 
various networks, we can find that the number of state 
transitions in a DFA is 30% less than that in an NFA. 
When Lulea algorithm is applied, the symbol-wise prefix is 
taken into consideration. Since the original 21595 × 256 indexes 
are reduced to 590,453 symbol-wise prefixes (also shown in Fig. 
5), the required memory space is decreased considerably. The 
memory space for Lulea algorithm is reduced to less than 1.4 
MB. Lulea algorithm is also implemented on a Windows XP 
platform to verify the correctness of the proposed mechanism. 
The software throughput for this implementation is also shown 
in TABLE I. Although the throughput for Lulea algorithm is not 
as good as traditional AC implementations because the 
proposed model involves the processing of the data structure 
(like popcount), the performance is still three times better than  
that of the compressed NFA with bitmap in [16]. Comparatively 
speaking, the AC-Lulea algorithm is much better than the 
AC-Bitmap algorithm. 
Based on TABLE I, we observed that there is a trade-off 
between the memory size requirement and the throughput rate. 
To make the comparison more practical and realistic, we take 
both throughput and memory usage into consideration, that is, 
overall efficiency (E). The higher E value represents better 
balance between performance and memory usage. Among all 
implementations listed in TABLE I, AC-Lulea has the best 
efficiency, about 7 times of that of AC-NFA, 3 times of that of 
AC-Bitmap, and more than 2 times of that of AC-DFA. The 
statistics indicate that through consumption of each memory 
unit, AC-Lulea could achieve the highest throughput rate. Since 
256
0 0 0 0 0 0 0 0 0 0 0 0
161
754
5411
266630
1
10
100
1000
10000
100000
1000000
8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Prefix Length
N
um
be
r o
f P
re
fix
es
 
Fig. 6. Length distribution of symbol-wise prefix with 256 magic states. 
0
50 54
272 328
416 448
839
1597
2801
4998
9876
19543
40011
83512
425708
1
10
100
1,000
10,000
100,000
1,000,000
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Prefix Length
N
um
be
r o
f P
re
fix
es
 
Fig. 5. Length distribution of symbol-wise prefix in DFA.  
1930-529X/07/$25.00 © 2007 IEEE
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE GLOBECOM 2007 proceedings.
14
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:24 from IEEE Xplore.  Restrictions apply. 
Early Identifying Application Traffic with 
Application Characteristics 
Nen-Fu Huang , Gin-Yuan Jai 
Department of Computer Science  
National Tsing Hua University  
Hsinchu, Taiwan 
{nfhuang@cs.nthu.edu.tw} 
Han-Chieh Chao 
Department of Electronics  
National Ilan University 
Ilan, Taiwan 
Abstract—To more accurately extract the characteristics of 
application flows, this paper proposes a set of flow attributes to 
characterize the possible negotiation behaviors of each flow in 
application layer perspective. The discriminators are available in 
the early stage, so they are suitable to support real-time based 
traffic classification and engineering. The ability of flow 
attributes was tested with several machine learning algorithms. 
On the other hand, we also compare the accuracy of our method 
with other related works that addressed real-time traffic 
classification problem based on the same sample traffic. The 
result shows that our method outperforms other previous works 
in protocol level identification with more than 8%~21% accuracy 
improvement based on fixed-ratio sample flow sets. Furthermore, 
the proposed method is also suitable to identify encrypted 
protocols. 
Keywords-P2P, Traffic identification/classification, Machine 
learning algorithm. 
I. INTRODUCTION
New techniques developed recent years break and improve 
the performance limitation of traditional transmitting 
architecture. The most impressive examples are: Peer to peer 
(P2P) architecture and network coding.  Most techniques bring 
the convenience to user and deeply affect the user behavior. 
However, the new transmission mechanisms they adopted 
also bring serious side effects to nowadays network 
management.  For example, bandwidth consumption problem 
resulted from P2P file sharing has been documented to 
understand affection in detail and characterize behaviors of 
specific P2P applications [1]-[3]. Unfortunately, the problems 
become more critical since the application developers adopt 
other techniques to prevent the management devices from 
correctly identifying traffic and shaping traffic. Two main 
challenges are dynamic communication mechanism and 
encrypted protocol. Dynamic communication protocols transfer 
data with dynamic parameters such as dynamic TCP/UDP 
ports or tunnel transferring mechanisms. In contrast to the 
traditional applications that adopt fixed port number, the 
unfixed ports used each time for those novel applications 
confuse port-based identification mechanisms. Although [1]-
[2], [4]-[5] developed signature-based classification to provide 
excellent accuracy for existing protocols, developing signature 
database becomes more expensive because of frequent revising 
of applications. Furthermore, with the fast improvement of 
hardware computing speed, more and more applications adopt 
encryption algorithms to protect secrets of user. The most 
prevalent example is Skype, an encrypted VoIP/IM software, 
protects user channels but still maintains voice quality. 
Nevertheless, adopting encryption algorithms prevents devices 
that employ string signatures from inspecting payload. 
To overcome these problems, [6]-[11] tried to classify 
traffic with L4 flow features. Unfortunately, most of them 
don’t satisfy the need of real-time network management. 
Although [12]-[16] addressed real-time classification, they do 
not fully support real-time classification since most P2P 
applications simultaneously adopt TCP and UDP protocols for 
communication but most methods only focused on identifying 
TCP traffic. On the other hand, discriminators of [14] are still 
available after the end of flow. 
To support real-time application filtering, we address the 
problem of early identifying application traffic in protocol level 
rather than category level. Both TCP and UDP flows are 
considered. Furthermore, to extract the possible characteristics 
of early negotiation stages of each flow, we define “interaction 
rounds” for communication flows and consider several types of 
statistical attributes of “interaction round” that are available in 
early stage: L7 transmitted size, throughput, inter-arrival time
(IAT) and response time. Compared with previous works, the 
proposed method provides higher accuracy and is suitable to 
early identify traditional or encryption-based protocols. 
The rest of this paper is organized as follows. Section II 
introduces previous works of real-time traffic identification. 
We discuss discriminators for early-stage traffic classification 
in section III and describe the preparation of employed traffic 
trace and simulation results in section IV. Finally, conclusion is 
given in section V. 
II. RELATED WORK
To support real-time traffic monitoring, [12]-[16] tried to 
develop suitable ways for online traffic identification. [13] 
applied wavelet analysis to inter-arrival time and packet size of 
applications. [13] also found that control flows are useful for 
clustering. However, [13] only considered FTP and eDonkey in 
their experiments. By applying unsupervised clustering to first 
k-data-packet size vector of each TCP flow, [12], [15] provide 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2008 proceedings.
978-1-4244-2075-9/08/$25.00 ©2008 IEEE 5788
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 23:29 from IEEE Xplore.  Restrictions apply. 
…Flow 
Preprocessing
ML 
Flow 
Sampling 
Random Split10-fold cross validation Average
Sample 
Set
Protocol Signatures Traffic Dump 
Flow Set
Test 1Average 
Result Result 1
Result 10 Train 10 Test 10
Train 1
… … … …
To early classify flows, first n rounds are chosen for each 
TCP/UDP flow. Each L4 flow is identified with five tuples -- 
(IPinitializer, IPlistener, Portinitializer, Portlistener, L4 protocol). To 
avoid delay brought by long data transmitting, our method 
computes the attributes of first 20 data packets for each long 
TALK block. If there are more than 200 data packets passed in 
a TALK after we get the attributes of 20 data packets, the block 
will be assumed as the final block and we finish to capture the 
attributes of that flow immediately. 
B. The Reconstruction of Talk Blocks 
To more accurately reflect the interaction between both 
sides of communication, we further consider the processing of 
packet order for UDP and TCP flows. TCP packets are sent 
with sequence and acknowledgement numbers of stream, so we 
use those numbers to reorder the packets and reconstruct the 
talk block. The TCP packets simultaneously sent from both 
sides are ordered according to their timestamps rather than 
SEQ/ACK numbers to avoid the possible challenges of 
computing time attributes. For the retransmitted stream bytes, 
the data size is not accounted for the flow. Nevertheless, it is 
difficult to reconstruct the order of packets for UDP flows 
since no sequence information can be taken from UDP header. 
Fortunately, most UDP flows are for flooding requests or 
voice/video data. So we simply extract the characteristics 
according to the original order of packets. 
IV. EXPERIMENT RESULTS AND ANALYSES
A. Traffic Traces for Experiment 
Traffic traces used for all experiments in this paper is 24-
hour campus traffic captured at edge link in National Ilan 
University of Taiwan. The traffic was captured from 1.30.2007 
13: 25 to 1.31.2007 13:25. To build training labels, traffic trace 
was pre-classified by payload inspection to get corresponding 
protocol label. Part of signatures defined in [8], [20] and 
additional signatures derived from documents or testing formed 
the pre-classification rules. To avoid possible noise for 
machine learning, protocols with fewer than 400 flows were 
not considered for data sampling and experiments. Flows with 
less than one interaction round were also not considered. Table 
II shows preprocessing result. In addition, timeout = 120s are 
used to identify all flows. 
B. Data Sampling and Evaluation Metrics 
Since original flow set includes too many instances to 
evaluate accuracy, we sampled flow instances to form instance 
set. Two kinds of traffic samples were employed. To reflect the 
original proportion, we sampled the first kind traffic sample 
that consists of about 100,000 instances according to original 
ratio of each protocol in preprocessed trace. To have more 
balanced view for learning accuracy, the other is fixed-number 
data set that consists of maximum N instances for each protocol. 
The features of our experiments are listed as follows: 
• Experiment architecture: Figure 3 shows the 
experiment architecture. After preprocessing steps that 
contain collecting attributes and sampling flows, the 
classifier was evaluated by 10 fold cross validation. 
• Sampling: uniform distribution was employed. 
For protocol Ci that contains N(Ci) instances, we considered 
false positive(FP) and false negative(FN) as evaluation metrics: 
 (1) 
 (2) 
Where N(FPCi) and N(TPCi) are the number of instances 
incorrectly and correctly classified as protocol Ci, respectively. 
Figure 3. Experiment architecture for evaluation. 
TABLE I. DISCRIMINATORS OF FIRST N = K ROUNDS DEFINED FOR 
TRAFFIC CLASSIFICATION.
 Discriminators 
Inter-arrival time between In-1 and In
Response time between TiA and TiB of Ii
Number of total bytes/packets sent during Ii
ith Interaction Round Ii
Data throughput of bytes/packets during Ii
Average inter-arrival time of packets of Tij
Number of total bytes/packets sent during Tij
Elapsed time of Tij
TALK block Tij of 
Interaction Round Ii
( j = A or B ) 
Data throughput of bytes/packets during Tij
Number of total bytes/packets sent by initializer 
or listener during first k rounds 
Sum of elapsed time from T1A to TkA
Sum of elapsed time from T1B to TkB
Data throughput of bytes/packets during first k
rounds (individually initializer or listener) 
L7 layer features of a 
TCP/UDP flow 
Data throughput of bytes/packets during first k
rounds (sum of initializer and listener) 
Initializer’s and Listener’s Port number 
Which side sends the first data packet 
L4 layer features of a 
TCP/UDP flow 
L4 protocol (TCP or UDP) 
TABLE II. CONSTITUENT OF PREPROCESSED TRAFFIC TRACE.
Protocol  # flows Protocol  # flows Protocol  # flows
Bittorrent(P1) 1009031 FTP(P5) 1341 POP3(P9) 4163
DNS(P2) 434879 Gnutella(P6) 1102414 Skypetoskype 
(P10) 
88506
EDonkey(P3) 386051 HTTP(P7) 1448803 SMTP(P11) 27418
Fasttrack(P4) 38137 Msn IM(P8) 3232 SSH(P12) 19961
¦
≠
=
ijall
j
C
C CN
FPN
FPRate i
i )(
)(
)(
)(
)(
1)(
i
C
C CN
TPN
FNRate i
i
−=
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2008 proceedings.
5790
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 23:29 from IEEE Xplore.  Restrictions apply. 
TABLE VIII. ACCURACY COMPARISON FOR ALL RELATED METHODS
Method Accuracy Method Accuracy Method Accuracy
AppR(J48) 95.11%
AppR 
(BayesNet) 90.87%
AppR 
(PART) 94.82%
AppR (J48) 
Scale, protocol 95.03%
AppR(J48) 
Scale, Category 97.15%
[15] 
No stdport 74.29%
[15] stdport 80.78% [12]Kmeans 73.28%[14] Protocol 86.90%
[14] Category 92.70% [16] Layer 1 82.94% [16] Layer 2 86.39%
TABLE IX. FALSE NEGATIVE RATES OF RELATED WORKS.
 AppR 
(J48) 
AppR 
(Bayes 
Net) 
AppR 
Scale 
(J48) 
[12] 
Kmeans 
[15] 
No 
stdport 
[15] 
With 
stdport 
[16] 
Layer 2
[14] 
Protocol
P1 1.57% 5.23% 1.70% 22.84% 21.12% 20.19% 93.89% 10.63%
P2 1.13% 5.50% 1.83% 46.23% 49.30% 41.17% 65.95% 7.27%
P3 13.57% 27.77% 14.93% 44.32% 42.55% 43.06% 45.32% 28.73%
P4 8.07% 7.20% 7.30% 15.77% 22.03% 24.00% 10.23% 12.03%
P5 0.15% 3.13% 0.30% 91.59% 55.89% 0.45% 26.24% 38.03%
P6 2.70% 5.93% 2.37% 12.80% 16.47% 13.07% 85.00% 3.57%
P7 13.70% 11.50% 13.30% 23.93% 12.40% 14.90% 75.68% 16.87%
P8 7.87% 17.50% 7.57% 20.09% 20.42% 18.01% 17.94% 14.67%
P9 0.47% 2.60% 0.43% 11.00% 21.10% 0.73% 3.07% 5.87%
P10 3.63% 10.93% 3.67% 50.07% 50.10% 39.07% 80.54% 20.27%
P11 0.00% 2.50% 0.00% 11.27% 7.50% 0.17% 8.19% 8.47%
P12 3.27% 6.40% 3.63% 6.60% 6.37% 5.50% 1.05% 4.63%
TABLE X. FALSE POSITIVE RATES OF RELATED WORKS.
 AppR 
(J48) 
AppR 
(Bayes 
Net) 
AppR 
Scale 
(J48) 
[12] 
Kmeans 
[15] 
No 
stdport 
[15] 
With 
stdport 
[16] 
Layer 2
[14] 
Protocol
P1 0.13% 0.23% 0.13% 1.76% 2.16% 1.20% 0.16% 0.50%
P2 0.33% 0.75% 0.38% 2.91% 1.38% 0.17% 0.13% 0.59%
P3 1.23% 0.96% 1.23% 5.17% 1.60% 0.70% 3.34% 3.03%
P4 1.04% 2.07% 1.11% 2.13% 1.45% 1.34% 3.25% 1.27%
P5 0.02% 0.17% 0.02% 0.24% 0.96% 0.05% 0.97% 0.26%
P6 0.32% 0.56% 0.35% 4.38% 3.85% 4.32% 0.02% 1.13%
P7 1.07% 3.00% 1.02% 2.32% 5.91% 4.61% 0.42% 1.81%
P8 0.65% 0.46% 0.57% 1.59% 1.61% 0.00% 3.46% 1.20%
P9 0.01% 0.07% 0.01% 4.60% 1.48% 0.00% 1.52% 1.50%
P10 0.32% 0.67% 0.44% 2.83% 3.45% 6.56% 0.07% 1.31%
P11 0.01% 0.27% 0.01% 1.24% 1.77% 0.01% 2.03% 0.92%
P12 0.23% 0.04% 0.17% 0.12% 0.11% 0.03% 0.20% 0.84%
TABLE XI. DISCRIMINATOR SELECTION AND RE-EVALUATION RESULTS.
J48 95.55 % NaiveBayes 37.5 % BayesNet 92.64 % 
PART 95.40 % SMO 38.57 %   
1. Total number of bytes sent by Initializer during round 1. 
2. Total number of bytes sent by Listener during round 1. 
3. The average inter-arrival time of packets sent by Listener during round 2.
4. Total number of bytes sent by Initializer during round 3. 
5. Listener’s port number.                          6. L4 protocol ID(TCP/UDP). 
ACKNOWLEDGEMENT
This work was supported by MOE Program for Promoting 
Academic Excellent of Universities (II) under the grant number NSC-
94-2752-E-007-002-PAE, and NSC project under the grant number 
NSC-95-2221-E007-054.
REFERENCES
[1] T. Karagiannis, A. Broido, N. Brownlee, K. Claffy, M. Faloutsos. “File-
sharing in the Internet: A characterization of P2P traffic in the 
backbone.” Technical Report, November, 2003. 
[2] T. Karagiannis, A. Broido, N. Brownlee, K. Claffy, M. Faloutsos.  “Is 
P2P dying or just hiding?” IEEE GLOBECOM2004, Volume 3, 
pp.1532- 1538. 
[3] Subhabrata Sen and Jia Wong. “Analyzing peer-to-peer traffic across 
large networks,” IEEE/ACM Transactions on Networking, Volume 12, 
Issue 2, pp. 219 - 232, April 2004. 
[4] Andrew W. Moore and Konstantina Papagiannaki, “Toward the accurate 
identification of network applications”, PAM2005(LNCS3431), Boston, 
MA, March/April 2005, pp. 41-54. 
[5] S. Sen, O. Spatscheck, and D. Wang., “Accurate scalable in network 
identification of p2p traffic using application signatures,” In Proc. of 
13th international conference on WWW,  pp. 512-521. 
[6] F. Constantinou, P. Mavrommatis, “Identifying known and unknown 
Peer-to-Peer traffic,” Fifth IEEE International Symposium on Network 
Computing and Applications, 2006, pp. 93-102. 
[7] T. Karagiannis, A. Broido, M. Faloutsos, and K. Claffy, “Transport layer 
identification of p2p traffic,” In Proc. of the 4th ACM SIGCOMM 
conference on Internet measurement (IMC2004), pp. 121-134. 
[8] T. Karagiannis, K. Papagiannaki, M. Faloutsos. “BLINC: multilevel 
traffic classification in the dark,” ACM SIGCOMM '05, pp. 229-240. 
[9] Andrew W. Moore and Denis Zuev, “Internet traffic classification using 
bayesian analysis techniques,” In Proc. of ACM SIGMETRICS 2005,
Banff, Canada, June 2005, pp. 50-60. 
[10] S. Zander, T. Nguyen, G. Armitage, “Automated traffic classification 
and application identification using machine learning,” IEEE LCN’05,
2005, pp. 250-257. 
[11] S. Zander, N. Williams, G. Armitage, “Internet archeology: estimating 
individual application trends in incomplete historic traffic traces,” CAIA 
Technical Report, March 2006. 
[12] L. Bernaille, R. Teixeira, I. Akodkenou, A. Soule, K. Salamatian, 
“Traffic classification on the fly,” ACM SIGCOMM Computer 
Communication Review, Volume 36, Issue 2, April 2006, pp. 23-26. 
[13] I. Dedinski, H. De Meer, L. Han, L. Mathy, D. P. Pezaros, J. S. Sventek, 
X.Y. Zhan, “Cross-layer Peer-to-Peer traffic identification and 
optimization based on active networking,” International Working 
Conference on Active and Programmable Networks, 2005. 
[14] Li, Z.; Yuan, R.; Guan, X., “Accurate classification of the internet traffic 
based on the SVM method, ” IEEE  ICC2007, pp. 1373-1378. 
[15] L. Bernaille, R. Teixeira, and K. Salamatian. “Early Application 
Identification.” In The 2nd ADETTI/ISCTE CoNEXT Conference, Lisboa, 
Portugal, December 2006. 
[16] Jeffrey Erman, Anirban Mahanti, Martin Arlitt, Ira Cohen, and Carey 
Williamson, “Offline/Realtime Network Traffic Classification Using 
Semi-Supervised Learning,” Technical Report, Department of Computer 
Science, University of Calgary, February 2007. 
[17] Ross Quinlan, “C4.5: Programs for Machine Learning,” Morgan 
Kaufmann, San Mateo, CA, 1993. 
[18] E. Frank, I. H. Witten, “Generating accurate rule sets without global 
optimization,” In Proc.15th International Conf. On Machine Learning,
1998, pp.144-151. 
[19] G. Cooper, E. Herskovits, “A Bayesian method for the induction of 
probabilistic networks from data.” Machine Learning, Volumn 9, Issue 
4, pp.309-347. October 1992. 
[20] L7 filter, http://l7-filter.sourceforge.net/ 
[21] George H. John, Pat Langley, “Estimating Continuous Distributions in 
Bayesian Classifiers.”,  The Eleventh Conference on Uncertainty in 
Artificial Intelligence, San Mateo, pp. 338-345, 1995. 
[22] J. Platt, “Sequential Minimal Optimization: A Fast Algorithm for 
Training Support Vector Machines”, Microsoft Research Technical 
Report MSR-TR-98-14, 1998. 
[23] Hall, M. A., “Correlation-based feature selection for machine learning,” 
Ph.D. dissertation at the University of Waikato, 1999. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the ICC 2008 proceedings.
5792
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 23:29 from IEEE Xplore.  Restrictions apply. 
the emerging high-layer network equipment needs an efﬁcient packet inspection engine to search the entire packet headers
and payloads for pattern matching. This study focuses on the nascent issues of payload inspection, and proposes a fast multi-
pattern matching algorithm.
The most important component of an inspection engine is a powerful multi-pattern matching algorithm, which can efﬁ-
ciently perform exact string matching to keep up with the growing data volume in the network. However, conventional
string matching algorithms are impractical for packet inspection [1,5]. Because of the large pattern database, an effective
inspection engine must be able to simultaneously search for a set of patterns, rather than iteratively performing the sin-
gle-pattern matching. The performance of processing packets is not only affected by the computation time, but also strongly
affected by the number of external memory accesses. It is well known that the rate of improvement in processor speed ex-
ceeds the improvement in memory speed. The gap has been the largest problem for system builders. For example, the
latency of one external memory access is about 150–250 times more than the latency of one instruction cycle in the Intel
IXP2x00 network processor systems [24]. Therefore, a high-speed multi-pattern matching algorithm should minimize the
number of external memory accesses.
This study proposes a novel hierarchical multi-pattern matching algorithm (HMA) for real-time packet inspection, which
simultaneously searches the packet payload for several patterns in a set. A small ﬁrst-tier table from the most frequent com-
mon-codes of patterns is used to narrow the searching scope. HMA signiﬁcantly reduces external memory accesses and pat-
tern comparisons by two-tier and cluster-wise matching strategies. HMA requires much less memory space than current
state-of-the-art multi-pattern matching algorithms [4,16,22,33,35]. For instance, HMA requires less than 350 KB to import
the current Snort database and it reveals small-scale and cost-effective implementations. The average number of external
memory accesses in HMA is about only 0.1–0.37 per byte, which efﬁcaciously improves the performance of the inspection
engine. Simulation results demonstrate that HMA performs about 0.9–410 times better than state-of-the-art algo-
rithms[16,22,33]. HMA has better best-case and average-case performance, in addition to manageable worst-case perfor-
mance. HMA also has an incremental pattern update mechanism to make it reliable and appropriate for on-line network
equipment. Consequently, HMA is a very cost-effective and efﬁcient mechanism that can be employed in real-time network
content inspection.
The increasing problem of network security threats means that NIDSs have become essential network applications
[15,18,26]. NIDSs protect network infrastructure from attacks and intrusions without modifying end-user software. To
ensure effective protection, NIDSs must be capable of real-time packet inspection, and be fast enough to keep up with the
ever-increasing data volume over the network. Hence, this study illustrates HMA with the promising NIDS that makes
use of a set of patterns describing known intrusions. A network-processor-based proactive NIDS is also developed to forestall
impending attacks.
The rest of this paper is organized as follows. Section 2 presents the background of pattern matching algorithms and
fundamental deﬁnitions. Section 3 describes in detail the proposed HMA and the incremental pattern updating mecha-
nism. Section 4 presents the performance analysis of HMA. The architecture of the network-processor-based NIDS is pre-
sented in Section 5. Section 6 shows the memory requirements and simulation results of HMA. At last, Section 7 draws
conclusions.
2. Background
This section describes the background to the state-of-the-art exact string matching algorithm. The fundamental deﬁni-
tions and notations used in this paper are ﬁrst presented.
2.1. General deﬁnitions and notations
An array is adopted to represent a string of characters from an alphabet set K. Namely, an element of string T at the posi-
tion i is T[t], where T[t] 2 K. The absolute value of an object signiﬁes the size of the object. For instance, jTj represents the
length of the string T, and jKj is the number of elements in the set K. Deﬁne a function sub(T, t, D), which is the substring
of T that starting from T[t] to T[t + D  1]. A string can also be given as a set of D-grams, where a gram is deﬁned as a group
of characters, and D is the number of grouped characters in a gram. For example, the string ‘‘green” can be translated into a
set of grams {‘gr’, ‘re’, ‘ee’, ‘en’} when D = 2.
Let P = {pi} denote a set of distinct patterns, where pi is a pattern with an identiﬁcation number (ID) i. Note that in the set
P, pi 6¼ pj when i 6¼ j. Assume that the payload of an input packet T and each pattern pi 2 P are both strings drawn over K.
A search request (jPj = 1) in a conventional exact string matching algorithm generally only contains one pattern. A single-
pattern matching algorithm is used to search a string (or text) T for the ﬁrst occurrence or all occurrences of one given pat-
tern. A multi-pattern matching algorithm is adopted to search the input T for all occurrences of any pattern pi 2 P where
jPj  1, or to conﬁrm that no pattern of P is in T. That is, the goal of the multi-pattern matching is to ﬁnd all the matched
patterns in T, say PM  P, such that PM = {pij "pi  T and pi 2 P}. PM can be applied to any high-level decision policy, such as
the high-priority-win, ﬁrst-matched-win or other state-concerned rules.
The notation e.f denotes the value of the ﬁeld (or offset) f at the entry (or address) e. If e is a table, then e.fmeans all ﬁelds
named f of the table e.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2881
in order to avoid missing any pattern. Hence, FV and WM are unfeasible for the inspection engines when the pattern set in-
cludes single-symbol patterns. The required memory space of the table for WM and WM-PH is O(jKjD). Generally, D = 3, and
the table requires 16 M entries when the alphabet size is 256. These large tables must be held in the external memory, which
leads to long access delay during the matching process. Furthermore, because the safety shift of a D-gram g in the BM-like
algorithms relates to all patterns containing g, it is a very complicated process to derive the shifts and updating the tables
when the pattern set is changed. The BM-like inspection engines must be suspended for table update, even when only one
pattern is added or removed.
2.2.2. The Aho–Corasick-based algorithms
The Aho–Corasick (AC) algorithm is a well-known algorithm that provides the best worst-case computational time com-
plexity [1,29]. AC is an automaton-based algorithm. By using a simple data structure, the memory space required to store the
transition matrixes of the states is in the order of O(jKj  S), where S is the number of states of the automaton. Using a com-
pressed structure, Tuck et al. modiﬁed AC (named AC-C), and lowered the required memory to about 2% of the original AC
[33]. However, the data structure of AC-C is still too large to be cached in the on-chip cache of general chips. Although the
AC-based algorithms have the best worst-case computational time complexity, the latency of external memory access dom-
inates the processing performance rather than the computational time. Even in the best-case scenario, AC still needs at least
two memory references per character. Additionally, even when only one pattern is removed, AC must rebuild the failure ta-
ble since AC’s failure table is built by correlating the entire pattern set. AC-C also needs to rebuild the entire state machine
when it adds or deletes a pattern, because the structures of AC-C are compressed. Consequently, the AC-based inspection
engine has to be suspended for pattern update, and the suspended time is proportional to the total length of all patterns
in P [1].
Coit et al. proposed a matching algorithm for Snort by combining AC and BM [10]. However, their algorithm requires three
times the memory of the standard version, and may yield inconsistent results.
2.2.3. Other approaches
In the case of hardware solutions, Li et al. developed an FPGA-based inspection engine for NIDSs, using the internal con-
tent addressable memory (CAM) to speed up multi-pattern matching [23]. Because the size of an internal CAM of FPGA is not
large enough to store all patterns, Li et al.’s engine dynamically reloads a block of patterns into the CAM, resulting in long
latency. Moreover, Li et al.’s approach does not solve this problem while the patterns of varied lengths complicate the
Fig. 1. The memory architecture of WM-PH, where the preﬁx size D = 3.
Table 2
Comparing the shifts of BM-based, FV, WM, and WM-PH algorithms
Shift value Maximum shift
BM-based J(a, p) jpj
FV min{J(a, pi)j " pi 2 P} min{jpijj "pi 2 P}
WM min{J(g, pi)jg  pi "pi 2 P}, where jgj = D min{jpijj "pi 2 P}  D + 1
WM-PH min{J(x, pi)j sub(pi, 1, D) = x "pi 2 P} D
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2883
The FCS algorithm is presented in Fig. 2, using a jKj vector M = (mi) and a jKj  jKj matrix R = (rij) as temporary memory,
where 0 6 i, j < jKj.M is a bit-map recording the occurrence of each character in a pattern. R is used to record the occurrence
frequency, where rij, i 6¼ j, indicates the relations of concurrent occurrence between two alphabets ai and aj in P, and rii
records the frequency of an alphabet ai 2 K occurring in different patterns. For example, rij = 2 means that currently two
patterns in P contain both ai and aj. Firstly, the FCS algorithm records the character occurrence of each pattern in the bit-
map M, and then accumulates the elements of M into the corresponding elements of R respectively (lines 2–4). Secondly,
FCS ﬁnds the largest occurrence frequency rff, and consequently the corresponding alphabet af is selected to be one of F. Then
the elements of R relating to af are subtracted accordingly to renew R (lines 6–9). FCS repeats until all elements on the diag-
onal of R become zero.
After FCS ﬁnds out F from P, F is used to construct a small index table, called the ﬁrst-tier table (H1). To speed up the pro-
cess, H1 uses a direct index table of jKj entries. The ath entry of H1 is denoted H1(a), where each entry has two ﬁelds: the
frequent code ID, say H1(a)ﬁd; and the single-symbol pattern ID, H1(a)pid. That is H1(a)ﬁd = {ij a = fi 2 F}, and H1(a)pid = {ij
jpij = 1, pi = ‘a’ and pi 2 P}. The unused ﬁelds of H1 are set as NULL. Since H1 is a small table, e.g. only 256 entries in the case of
one-byte coding, it can be stored in the on-chip cache. Later, H1 acts as a ﬁrst-tier ﬁlter in the on-line stage to quickly dis-
cover whether a packet contains a pattern. Namely, HMA makes use of F to narrow the searching scope to the most likely
subset of patterns (clusters).
3.2. The cluster balancing strategy (CBS)
Generally, most packets are innocent and a harmful packet may contain only few patterns. Hence, comparing all of the
patterns in the large P with each input packet is time consuming. If the patterns in P can be distributed into different small
clusters based on their similarity, then only the patterns in few clusters that are most similar to the input need to be com-
pared. Therefore, the efﬁciency of the matching process is improved. This subsection presents strategies to attain this goal.
First, the method of clustering a set P based on the similarity of patterns is described. Then a cluster balancing strategy (CBS)
is used to balance the cluster size, and ﬁnally a second-tier table (H2) for on-line matching based on the clustering results is
built.
Deﬁne the clustering pivots as the keys used to distribute patterns, where each clustering pivot is a common-code of pat-
terns deﬁned previously. Two common-codes are employed as a pair of clustering pivots, called a pivot pair and noted as
(a, b), where the ﬁrst pivot is a frequent common-code of F, and the second pivot is the code following the frequent com-
mon-code. Let Pa,b represent a cluster of selected patterns (a subset of patterns) with the pivot pair (a, b), which means that
Pa,b = {pij‘ab’  pi, a 2 F and b 2 K}, where ‘ab’ is the combination of two strings a and b, and is a substring of pi. Notably, a
pattern is assigned to only one cluster in the clustering strategy, although a pattern may have more than one pivot pair. That
is, the clusters have the following properties: any cluster Pa,b  P, [all a,bPa,b = P and \all a,bPa,b = ;. Since a pattern may have
several opportunities to select a cluster, a better assignment can lower the maximum cluster size, and thereby improving the
worst-case performance of HMA.
In order to lower the worst matching time, CBS is employed to balance the size of clusters. In CBS, an jFj  jKj matrix
N = (na,b) is used to record the current size of a cluster Pa,b. The algorithm is as the followings. Firstly, CBS reads one pattern
at a time from P and scans the pattern. According to FCS, for any given pi, there exists a character such that pi[k] 2 F, where
1 6 k < jpij. To balance the cluster size, CBS ﬁnds the smallest na,b among all available pivot pairs of pi, say (a, b), where a 2 F
and ‘ab’ 2 pi. After group pi into the smallest cluster Pa,b, the corresponding na,b is then incremented. All patterns are distrib-
uted sequentially into the designate clusters in the same way.
The second-tier table H2 is constructed based on the cluster assignments. H2 contains the pattern contents and the pat-
terns’ formatted information for fast on-line matching. Let H2(a, b) denote an entry of H2, storing the head pattern of a cluster
Pa,b, and deﬁned as
Fig. 2. The FCS algorithm.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2885
In the ﬁrst-tier matching, if H1(T[t])pid is not NULL, then T[t] is a single-symbol pattern, and this matched pattern will be
added into PM. Whether H1(T[t])pidis NULL or not, then the ﬁrst-tier matching procedure checks the ﬁd ﬁeld.
If H1(T[t])ﬁd is NULL, i.e., T[t] 62 F, T[t] will be skipped with no pattern comparison, and thereby no external memory is
necessary. Then the on-line matching stays in the ﬁrst-tier matching, proceeding to the next character T[t + 1] and checking
the H1(T [t + 1])pid as previous steps. Since jFj is much smaller than jKj, most characters of T can gain the skips and avoid the
second-tier matching. Consequently, both the number of character comparisons and costly memory accesses can be reduced.
If T[t] 2 F, Tmay contain a pattern pi 2 P, where T[t] 2 pi. That is, as H1(T[t])ﬁd is not NULL, Tmay have a pattern (or more
than one) belonging to the cluster PT[t],T[t+1]. Then the second-tier matching is activated to identify the pattern.
3.3.2. The second-tier matching
After the ﬁrst-tier matching, as long as H1(T[t])ﬁd is not NULL, the matching procedure proceeds to the second-tier
matching. H2(T[t], T[t + 1]) indicates the location of the corresponding cluster PT[t],T[t+1] according to the input T. As a clus-
ter-wise matching, HMA checks only the patterns in the small cluster PT[t],T[t+1], which are most similar to T.
In the second-tier matching, ﬁrstly the pid ﬁeld of H2 is checked. If H2(T[t], T[t + 1])pid is NULL, it means the cluster
PT[t],T[t+1] has no pattern. Afterward, the next character T[t + 1] is scanned, and the matching procedure returns to the
ﬁrst-tier matching. Otherwise, if H2(T[t], T[t + 1])pid is valid, it means the cluster PT[t],T[t+1] has patterns similar to T. Then,
HMA compares the pattern content in H2(T[t], T[t + 1]) with the suspected part of T, sub(T, T[t  H2(T[t], T[t + 1])offset],
H2(T[t], T[t + 1])size). If the pattern size H2(T[t], T[t + 1])size is larger than the width of a data ﬁeld, the next fragment of
the pattern at H2(T[t], T[t + 1])next is fetched and compared only when the current fragment gets a match. If the next ﬁeld
of the last pattern fragment points to a valid next pattern, say at H2(a, b), similarly the pattern in H2(a, b)datais compared
with the substring of T starting at T[t-H2(a, b)offset]. All matched patterns are added to PM.
Notably, if a pattern pi exists in T, then all characters of pi will appear in T. Deﬁnitely, the clustering pivot pair of pattern pi,
say pi[k] and pi[k + 1], will be found in T, say at T[t] and T[t + 1], where T[t] = pi[k] 2 F. When T compares with the patterns in
the cluster PT[t],T[t+1] during the matching procedure, pi will be recognized. Consequently, no patterns in the payload Twill be
missed.
The on-line matching procedure of HMA is presented in Fig. 4. Obviously, only few suspected patterns are loaded from
external memory, and the number of string comparisons is decreased. HMA can scan the packets rapidly by using H1 and
H2, since most packets in the network are generally innocent and the obtained F narrows the searching scope.
Fig. 3b demonstrates the on-line matching of HMA. Assume the H1 and H2 tables have been constructed as Fig. 3a,
where F = {e, a}. HMA scans the input T from left to right. If T = ‘pink’, after checking T with the on-cache H1 for four times
and ﬁnding that all characters of T do not belong to F, HMA knows that T contains no pattern and no external memory
access is required. If T = ‘black’, HMA stays in the ﬁrst-tier matching until ‘a’ is scanned, and ﬁnds that ‘a’ 2 F (H1(a)ﬁd
is valid) and ‘a’ is a single-symbol pattern (H1(a)pid = 1). Then, ‘a’ and its following ‘c’ are used as the index keys (pivot
pair), and the second-tier matching loads an entry from H2(a, c) for further checks. Because H2(a, c)pid (=6) is not NULL,
HMA compares the substring(s) of T with the pattern(s) in Pa,c, where H2(a, c)data = ‘black’, and a match is got. As H2(a,
c)next is NULL, the on-line matching process returns to the ﬁrst-tier matching as the previous steps. Since ‘c’ and ‘k’ 62 F,
the scanning process of this input is ﬁnished. For the input ‘black’, only one external memory access is required. The result
of this case is PM = {a, black}.
3.4. The incremental update
A packet inspection engine, just like most network equipment, must work persistently to avoid missing any packet.
When an inspection engine suspends, even for only 30 s, millions of packets will cut through it without any inspection.
Fig. 4. The on-line matching procedure of HMA.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2887
To delete a pattern pi, only one entry has to be cleared and at most two of H2next, H1ﬁd, and H1pid have to be modiﬁed.
The ﬁrst process is to ﬁnd out the pattern pi in H2 by using a matching process similar to the on-line matching. If jpij = 1, then
change H1(pi [j])pid to NULL. If jpij 6¼ 1 and pi is in the cluster Ppi ½j;pi ½jþ1, then (1) when pi is not the only pattern in the cluster
Ppi ½j;pi ½jþ1 link pi’s previous entry and its next one in H
2 before clearing the entry of pi (H2next is modiﬁed); (2) when pi is the
only one in the cluster Ppi ½j;pi ½jþ1, check whether any pattern exists in the subset Ppi ½j2F after clearing the entry of pi. If no, it
means the frequent common-code pi[j] = f is not used any more. Then, the code f can be removed from F and the ﬁeld H1(f)ﬁd
is set to NULL.
Obviously, the number of modiﬁed ﬁelds due to HMA’s pattern updates are constant (at most three), and thus the updat-
ing time is deterministic and negligible. Therefore, HMA provides more reliable inspection engines for real-time network
equipment.
4. An example: network intrusion detection system
HMA can be used in many novel network applications to inspect packets, such as NIDSs, anti-virus appliances, and layer-7
switches, which search for a set of patterns in packets. The only difference between these applications is the pattern format.
Most patterns of virus codes are binary codes; while most patterns of layer-7 switches are formed by English letters. The
patterns in NIDSs are written in mixed plain text and hex formatted bytecodes. In this section, we illustrate an application
of HMA with NIDSs.
Two complementary techniques are used to cope with the intrusion detection problem: anomaly detection and misuse
detection [20]. Anomaly detection techniques attempt to model normal behavior; while misuse detection techniques at-
tempt to model abnormal behavior. Anomaly-based IDSs are deployed based on machine learning, data mining or statistical
algorithms, which are more sensitive to new attacks than signature-based IDSs. However, anomaly-based IDSs usually trig-
ger up to 99% false positive alarms, and their complex normal models result in poor performance. Several researchers have
proposed new schemes to improve the anomaly detection [20,30,36]. Misuse detection is assumed to be more accurate and
efﬁcient than anomaly detection, and therefore signature-based IDSs are commonly used today. Some effort has focused on
automatic signature generation to improve the robustness of the signatures [30]. An example of signature-based NIDSs using
HMA is shown in this section.
As network processors have been widely used to develop novel network equipment [32], a network processor platform is
used to illustrate the HMA-base NIDS. A network processor development system usually consists of several on-chip multi-
context processing engines, each with a small on-chip cache, one host CPU, external memory, built-in Ethernet MACmodules
and queue modules, such as weighted fair queues (WFQs) [34]. A NIDS can be a sniffer for intrusion analyses, or can combine
with an embedded queue module (as in Fig. 6) to intercept malicious packets. To accelerate the performance of the network
equipment, the network-processor-based systems generally divide the tasks into two paths: the control path and the data
path. The host CPU processes the non-real-time tasks in the control path, including the table construction, the pattern set
management, the log analyses, and the user interface control. The on-chip microengines handle the real-time tasks in the
data path, including the packet parsing, header matching, content matching, decision control and queue management.
The content matching engine utilizes the proposed HMA, which is usually the most resource-intensive element; while
the header matching engine uses a hardware-supported classiﬁcation module. The H2 table of HMA is stored in the external
memory and H1 is in the on-chip cache of the content matching engines.
Fig. 6. The architecture of a network-processor-based NIDS.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2889
In the average case, assume that an input string T is drawn randomly from the alphabet set K. As deﬁned previously, H1 is
a direct indexing table for each character. The ﬁd ﬁeld of the entry H1(a) is assigned a valid ID, say i, for all a = fi 2 F. Thus, the
probability that an entry of H1 has a valid ﬁd is
PrfH1  fid 6¼ NULLg ¼ j F jj K j ; ð8Þ
where jFj is the number of frequent common-codes. In the matching process, if H1ﬁd 6¼ NULL, the next step is to check the
pid ﬁeld of the indexed H2 in the external memory, and proceed to the second-tier matching. Accordingly, the probability
that the on-line matching goes to the second-tier matching for any input character T[t] 2 K is deﬁned as CAVGtier2, and
CAVGtier2 ¼
XjKj1
0
1
j K j 
j F j
j K j ¼
j F j
j K j : ð9Þ
The ﬁrst step of the second-tier matching is to fetch the entry and check the pid ﬁeld, and thus one external memory
access is required. If there is more than one pattern in the cluster, additional external memory access will be needed to fetch
those patterns. We assume every pattern can be loaded into microprocessors within one external memory access. Let NAVGRAM
represent the average number of external memory accesses per one input character, and it is
NAVGRAM ¼ CAVGtier2  1þ
XjPj
k¼2
ðk 1Þ j P j
k
 
Ckp!clusterð1 Cp!clusterÞjPjk
 !
¼ j F jj K j ðNcluster þ ð1 Cp!clusterÞ
jPjÞ: ð10Þ
If the indexed H2(T[t], T[t+1])pid is valid, it means Tmay have a pattern pi 2 PT[t],T[t+1]. Then T has to be compared with the
patterns in the cluster PT[t], T[t+1], where the average number of patterns in PT[t],T[t+1] is Ncluster. Let N
AVG T
fetch be the average num-
ber of patterns fetching from an external memory for a given average-case input T. NAVG Tfetch can be derived from the previous
equations:
NAVG Tfetch ¼ ðj T j 1Þ  CAVGtier2  Ncluster: ð11Þ
Thereby, the number of XOR instructions used in string comparisons between T and the patterns for a given average-case
input T, denoted NAVG TXOR , can be obtained by
NAVG TXOR ¼ NAVG Tfetch
p
x
 
; ð12Þ
where x is the computer wordsize. In the average case of HMA, let NAVGXOR represent the average number of XOR comparisons
between Tand the patterns in the cluster for one input character, which can be derived by
NAVGXOR ¼
NAVG TXOR
j T j <
j F j
j K j
p
x
 
Ncluster: ð13Þ
5.2. Worst case
If a given string T is formed badly that has to do the exact string comparisons the most times, the performance of HMA for
the bad-formed T is the worst case. Assume the largest cluster size is Lc. When every character of T(T[t]) belongs to F, and
every corresponding indexed cluster is the largest (jPT[t],T[t+1]j = Lc), this is the worst scenario of HMA. As every character
T[t] 2 F, the probability to fetch the table H2 for the worst case is one. Thus, the number of external memory accesses per
character in the worst case is
NWSTRAM ¼
ðj T j 1Þ  Lc
j T j < Lc: ð14Þ
Assume the largest pattern size in P is Lp. When every input character points to the largest cluster, in which every pattern
has the longest size, the worst case requires the largest number of comparisons. Hence, the number of XOR character com-
parisons for one input character is
NWSTXOR < Lc
Lp
x
 
: ð15Þ
Obviously, the worst-case performance depends on Lc. To derive Lc, assume there is a largest cluster, say Px,y. Since Px,y is the
largest cluster, assume that the cluster size is always larger than one, and initially the probability that its cluster size in-
creases from 0 to 1 is one. That is
Prfj Px;y j¼ 0! 1g ¼ 1: ð16Þ
In the worst case, the patterns are assumed formed badly and have a bias on the pivot pair (x, y). Since Px,y is the largest
cluster, based on CBS, a given pattern pwill not be clustered into Px,y, unless all available pivot pairs of p are not in the
set F  K except (x, y). Therefore, the probability that jPx,yj increases from i to i + 1 is
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2891
The characters in a payload besides the patterns are called background characters. Two forms of background characters
are respectively used in the Model I and Model II. In the Model I, the payloads of random background are formed by characters
randomly drawn from K to imitate the normal packet contents. However, the random background may unconsciously con-
tain some patterns of P. To evaluate the impact of k on the performance of algorithms, pure background is used in the Model
II. The pure background is formed by the characters that never appeared in P.
6.2.2. Model III
Since different multi-pattern matching algorithms have different string forms that cause their best-case or worst-case
performance, all permutations of four-character input strings (232 strings) are used in the Model III to examine the extreme
performance of every algorithm. We choose the length of four characters because 24.5% of Snort’s patterns are less than or
equal to four characters (see Table 3), and the test pool of 232 input strings is large enough for simulations. Because it is very
difﬁcult to obtain the best-case and worst-case traces for every algorithm, it is quite feasible by using this model to evaluate
the extreme cases of every algorithm.
6.2.3. Model VI
To evaluate the performance of algorithms in an intense attack, a real trace from the Capture-the-Flag contest held at Def-
con9 was adopted as the input trafﬁc in the Model VI. The Defcon Capture-the-Flag contest is the largest security hacking
game. In this contest, competitors try to break into the servers of others while protecting their own servers, where each ser-
ver hides several security holes [9]. The summary of the trafﬁc models are shown in the Table 5 and the simulation param-
eters are listed in Table 6.
6.3. Memory requirements
The lookup information and patterns are generally saved in the memory using a tabular structure for fast lookup and
matching. Therefore, the memory requirements are shown in terms of the number of entries. Since the H1 of HMA is a direct
lookup table, the cache memory space (MI) of HMA is jKj entries. Based on the proposed schemes, FCS and CBS, the number of
entries in H2 is the total number of possible clusters. As all possible pivot pairs are in the space K  F, the maximum size of H2
is jFj  jKj entries along with a shared space of no larger than jPj entries for collisions. Thereby, the external memory space
(ME) of HMA is O(jFj  jKj + jPj). The lookup table of WM-PH is based on a direct preﬁx hash table with preﬁx length of D,
where D = 3 in the simulations. Accordingly, ME of WM-PH is jKjD + jPj entries for the index table and pattern contents. In
the BMH, every pattern has its own skip table of jKj entries, so that ME of BMH is O(jPj  jKj + j Pj). Since each skip table
of BMH is small enough to be loaded to the local memory, we allocate a cache memory space for BMH in the simulations
for fair comparisons. WM-PH and AC-C also need cache memory for loading one skip value or one state during matching
process. The required memory used in HMA, WM-PH, BMH and AC-C is summarized in Table 7 including lookup tables
and pattern contents.
Table 8 lists the relations between the pattern set size jPj and the number of frequent common-codes jFj in the HMA.
It shows that the growing rate of jFj is much slower than that of jPj. In the simulations with jPj = 1200 for example, the
Table 4
The measurements
Notation Meaning
NI The average number of RISC instructions per input character (including comparisons and calculations)
NL The average number of local memory accesses (including reading data from cache to registers)
NE The average number of external memory accesses for loading the packet, querying the entries of tables in the external memory, and fetching
the patterns
wI The time of one instruction or one local memory/register access
wE The time of one external memory access
wI The average computation cycles: wI = NI  wI
wM The average memory latency: wM = NE  wE + NL  wI
W Total average matching time: W = wI + wM
Table 5
The trafﬁc models
Packet format Packet length Number of packets
Background Number of patterns
Model I Random k 640 bytes 10 million
Model II Pure k 640 bytes 10 million
Model III All permutations 4 bytes 232
Model VI Real traces from Defcon
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2893
of HMA is about 26.5–68 (99.7–409.5) times less than that of BMH, 4.2–10.6 (2.8–10.6) times less than that of WM-PH, and
15.5–34.8 (9.1–34.7) times less than that of AC-C under different attack loads. In Fig. 7, when k is low, HMA signiﬁcantly
outperforms WM-PH, BMH, and AC-C. Consequently, HMA is very suitable for IDSs in a general network environment,
because most packets are innocent (k 
 0).
The simulation results shown in Figs. 8–10 use Model I as input trafﬁc. Fig. 8 comparesW of HMA, WM-PH, AC-C and BMH
with different attack loads k = 0 and k = 4 respectively. It also shows the impact of jPj on W. Simulation results reveal that
HMA outperforms WM-PH, BMH and AC-C even when jPj andk are increasing. For both k = 0 and k = 4, the matching costs
of HMA and WM-PH both rise with jPj. This is because while jPj rises, the number of patterns in P that have similar sub-
strings also rises. This leads to the increasing number of marked entries that request for comparisons in HMA and WM-
PH. Hence, HMA and WM-PH require more string comparisons and memory accesses with increasing jPj. HMA has slightly
higher growth rate than WM-PH, because the table size of HMA (H1 and H2) is about 830 times smaller than that of WM-PH.
The increasing jPj makes the matching time of BMH rise steeply, because the BMH is originally a single-pattern matching
algorithm that simply executes iteratively for every pattern. In the case of wE = 250 and k = 0 (k = 4), the matching time of
HMA is 14.5–35.8 (11.7–29.8) times less than that of BMH, 2–3.3 (1.9–2.8) times less than that of WM-PH, and 11.9–22.2
(9.5–24.3) times less than that of AC-C under different pattern set sizes. Fig. 8 reveals that HMA is quite stable due to slight
increment of its W while jPj increases.
The processing time W includes the computation time (wI) and memory access delay (wM). Fig. 9a–d illustrate the propor-
tion of wI toW and wM toW respectively for all approaches with jPj = 1200 and various k. In these ﬁgures, the upper and lower
part of the bar are represented as wM and wI respectively. The results show that HMA’s wI is close to WM-PH’s, but HMA’s wM
Fig. 8. The average matching cost (W) versus pattern set size (jPj) for HMA, WM-PH, BMH and AC-C with different attack loads (k), using Model I, and
(a) wE = 250, (b) wE = 100.
Fig. 9. wI and wM versus attack load (k), where jPj = 1200 and wE = 100, using Model I. The labeled value above each bar is W. (a) HMA, (b) WM-PH, (c) BMH
and (d) AC-C.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2895
Defcon trace (Model VI) contains a lot of malicious packets, Fig. 12 shows that HMA performs well and much better than
others. It also demonstrates that the memory access time of HMA is much smaller than others (note that ﬁgures are in log-
arithmic scale), which means HMA successfully reduces the number of memory accesses. In other words, the small ﬁrst-tier
ﬁlter of HMA can still work well even under heavy attacking loads.
7. Conclusions
The increasing variety of network applications and stakes held by various users are creating a strong demand for fast in-
depth packet inspection. The most important component of in-depth packet inspection is an efﬁcient multi-pattern match-
ing algorithm. This study has proposed a novel hierarchical multi-pattern matching algorithm (HMA) for network content
inspection. HMA applies the most frequent common-codes to quickly ﬁlter out innocent packets, and to reduce memory
accesses. The frequent common-codes are used to build small hierarchical index tables for simple and fast checks. The hier-
archical scheme improves the matching performance signiﬁcantly by reducing the average number of external memory
accesses to only 10–37%. The required memory of HMA is only about 350 KB including the pattern contents of Snort rules.
Particularly, HMA does not use special architecture and functions, and it can be easily implemented in both software and
hardware. This study also discussed and evaluated current multi-pattern matching algorithms for NIDSs. Simulation results
show that HMA performs about 0.9–409 times better than others. HMA signiﬁcantly improves the best-case and average-
case performance, and also provides moderately worst-case performance of the multi-pattern matching. Moreover, an incre-
mental pattern update mechanism was also proposed for HMA. In conclusion, HMA facilitates the creation of efﬁcient and
cost-effective network content inspection engines.
References
[1] A.V. Aho, M.J. Corasick, Efﬁcient string matching: an aid to bibliographic search, Communications of the ACM 18 (6) (1975) 330–340.
[2] S. Antonatos, K.G. Anagnostakis, E.P. Markatos, Generating realistic workloads for network intrusion detection systems, in: ACMWorkshop on Software
and Performance, 2004, pp. 207–215.
[3] S. Antonatos, M. Polychronakis, P. Akritidis, K.G. Anagnostakis, E.P. Markatos, Piranha: fast and memory-efﬁcient pattern matching for intrusion
detection, in: Proceedings of the 20th IFIP International Information Security Conference (SEC2005), 2005, pp. 393–408.
[4] Gordon Brebner, Delon Levi, Networking on chip with platform FPGAs, in: Proceedings of 2003 IEEE International Conference on Field-Programmable
Technology, 2003, pp. 13–20.
[5] R.A. Baeza-Yates, Improved string search, Software – Practice and Experience 19 (3) (1989) 57–271.
[6] R.S. Boyer, J.S. Moor, A fast string searching algorithm, Communications of the ACM 20 (10) (1977) 762–772.
[7] Brian Caswell, Jay Beale, James C. Foster, Jeremy Faircloth, Snort 2.0 Intrusion Detection, Syngress, February 2003.
Fig. 11. The pure costs of the matching algorithms in the worst-case and best-case situations using Model III.
Fig. 12. The processing time and the normalized costs using Model VI with wE = 100: (a) W and wM where jPj = 1200 (b) The matching costs normalized to
HMA where jPj = 200 and 1200.
T.-F. Sheu et al. / Information Sciences 178 (2008) 2880–2898 2897
IEEE COMMUNICATIONS LETTERS, VOL. 12, NO. 8, AUGUST 2008 599
A Software-Based String Matching Algorithm for
Resource-Restricted Network System
Yen-Ming Chu, Student Member, IEEE, Nen-Fu Huang, Senior Member, IEEE, Chi-Hung Tsai,
and Chen-Ying Hsieh
Abstract—String matching is the most critical operation in
network intrusion detection Systems (NIDS) [1]. This paper
proposes a novel memory-efficient string matching algorithm that
only requires around 2% of the memory utilized in Aho-Corasick
algorithm [2] but has more than 4 times the throughput of state-
of-the-art algorithm with very limited memory resource. The
proposed algorithm is flexible to fit different resource constraints
and performance requirements.
Index Terms—Embedded system security, computer network
security, network intrusion detection, string matching.
I. INTRODUCTION
VARIOUS new concepts and algorithms have been pro-posed to improve the performance of string matching
for NIDS[2-7]. However, most of them require very expensive
hardware and therefore are not suitable for small and medium-
sized enterprise (SME) and Small Office Home Office (SOHO)
environments that only have a very limited budget. Hence, this
study aims at developing an efficient string matching scheme
for NIDS in an embedded system platform.
II. STRING MATCHING WITH MAGIC STATE
The core of an automaton-based string matching model is
state lookup. The state transition information could be stored
in a matrix as follows:
A ≡ [A(0) A(1) · · · A(2v−1)]T
where A(i) ≡ [a(i,0) a(i,1) · · · a(i,2u−1)
]
The variables u and v denote the bit sizes of a symbol and
a state, respectively. The element a(y,x) represents the next
state when the current state y receives the input symbol x.
A. Magic State
It is interesting to see that when A is a deterministic finite
state automaton (DFA), for each symbol x, most elements
a(y,x) in A have the same value for different current states y.
We named these elements “magic states”. Thus, we define the
Manuscript received February 18, 2008. The associate editor coordinating
the review of this letter and approving it for publication was M. Ma. This work
was supported by the MOE Program for Promoting Academic Excellence of
Universities (II) under the grant number NSC-94-2752-E-007-002-PAE, and
NSC projects under the grant number NSC-95-2221-E007-054 and NSC96-
2219-E-009-013.
Y. M. Chu is with the Institute of Communication Engineering, National Ts-
ing Hua University, Hsinchu, Taiwan (e-mail: ymchu@totoro.cs.nthu.edu.tw).
N. F. Huang, C. H. Tsai, and C. Y. Hsieh are with the Department of
Computer Science, National Tsing Hua University, Hsinchu, Taiwan.
Digital Object Identifier 10.1109/LCOMM.2008.080253.
magic state of each symbol x (denoted as ms(x)) as the next
state that appears most frequently for different y.
With magic state, we could maintain a compact data struc-
ture instead of the enourmous matrix A to perform state
lookup: a bitmap matrix B indicates whether an element in
an automaton matrix A is a magic state or not, a vector MV
contains ms(x) of each symbol x and a vector NV stores
every non-magic state in matrix A with row-major order, all
are shown in Fig. 1(b). An element b(y,x) of matrix B is set
to 0 if a(y,x) equals ms(x), otherwise (a(y,x) = ms(x)), 1 is
assigned to b(y,x). Assuming that current state y receives input
symbol x during state lookup, then b(y,x) is examined first. If
it is 0 (a magic state), then the next state could be obtained in
MV [x]. If not (a non-magic state), we could perform a sum
(or popcount) up to the prior bits of b(y,x)with row-major order
in matrix B to get an offset f , then take NV [f ] as the next
state.
B. The Hierarchical Partition for Bitmap
To store the magic state information more compactly, the
hierarchical partition method for matrix B is proposed. Matrix
B is firstly partitioned into size(B)/(2m×2n) blocks, where
size(B) is the number of elements in B and 0 ≤ n ≤ v
and 0 ≤ m ≤ u. The width and height of each block
is 2m elements and 2n elements, respectively. (The value
of m and n can be flexibly tuned to fit different resource
constraints.) Next, the heuristic matrix H is constructed from
B by aggregating all the elements of the same block into one
bit based on a logical OR operation. Furthermore, for those
blocks that have at least one element equals 1, we store them
in BV with row-major order for precise magic state check and
the popcount operation for getting non-magic states in NV .
Notice that the order of non-magic states in NV need to be
adjusted for corresponding with the block bitmap stored in
BV . Fig. 1(c) illustrates these data structures in detail. (NV
in Fig. 1(b) differs from NV in Fig. 1(c) because the bitmap
now is stored in BV block by block)
C. Partial Renumbering with Magic State
T. Nishimura proposed a states rearrangement mechanism
to speedup the Aho-and-Corasick’s (AC) algorithm [2], where
the states of a DFA are renumbered in breadth-first order [7].
It is interesting to see that we found after this rearrangement,
the next states of the initial state (state 0) are much more
frequently accessed than other states during the state lookup
process. Therefore, unlike Nishimura’s method (renumbering
1089-7798/08$25.00 c© 2008 IEEE
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:47 from IEEE Xplore.  Restrictions apply. 
CHU et al.: A SOFTWARE-BASED STRING MATCHING ALGORITHM FOR RESOURCE-RESTRICTED NETWORK SYSTEM 601
TABLE I
MEMORY COMPARISONM AND THROUGHPUTT
Throughput (Mbps)
MByte Defcon9 ISIC Synthetic
Xscale ARM9 Xscale ARM9 Xscale ARM9
AC-Rearranging > 53 38.09 NA* 38.09 NA* 38.03 NA*
AC-Bitmap 1.09 37.59 0.27 33.23 0.24 29.02 0.2
Proposed (128, 8, 2) 1.12 139.44 0.35 152.57 0.3 17.81 0.18
Algorithm (128, 4, 16) 0.96 130.47 0.91 146.58 1.02 14.9 0.11
(t, m, n) (0, 8, 2) 1.06 49.56 0.35 41.92 0.3 17.14 0.18
1The 1st Platform: Intel’s Xscale 500 MHz with 2MB SRAM and 64MB
SDRAM;
2The 2nd Platform: ARM’s ARM922T 125MHz with 16MB SDRAM; 3The
implementation for AC-Rearranging was absent because of the restricted
memory space.
ρH =
∑k−1
i=0 [(state(i) ≥ t) ∧ (h(β,α)is 0)]
k
(2)
ρBV =
k−1∑
i=0
[(state(i) ≥ t) ∧ (h(β,α)is 1) ∧ (b(state(i),ti)is 0)]
k
where β = state(i)/2n	, and α = ti/2m	 (3)
Let λc, λH , λM , λBV and λNV represent the time required
to access an element ofA′Chief , H , M , BV and NV , respec-
tively. Thus, the average latency time required to perform a
state lookup is as follows:
ρc × λc+(1− ρC)× λH + (1− ρC − ρH)× λBV
+(1− ρC − ρH − ρBV )× λNV + (ρH + ρBV )× λM
(4)
Note that in formula (4), the instruction execution time is
not taken into consideration. Nevertheless, some of them could
not be negligible, such as the expensive popcount performed
in matrix H and vector BV . Thus, the proposed algorithm
would have high performance when ρ = (ρC + ρH) is
sufficiently large since it just accesses the content of A′Chief ,
H and M without executing any time-consuming instructions.
Also note that all the magic states are numbered less than
threshold t after renumbering when t is properly selected.
Consequently, if the proportion of magic states is sufficiently
large during state transitions, the proposed algorithm would
gain enormous advantages because larger ρ is conducted
and many popcount operations are omitted. Fortunately, an
amazing observation is that around 95% of the states are magic
states in the automaton constructed by the Snort [1] signatures.
The proposed algorithm is expected to have high performance
in such an automaton when input string is assumed to be real
world network traffic.
IV. IMPLEMENTATION AND EVALUATION
For evaluating the efficiency of the proposition, AC-
Rearranging [7], AC-Bitmap [3] and the proposed algorithm
were implemented on two general embedded system platforms
based on Snort rules. The first was Intel’s Xscale processor
at 500 MHz with 2MB SRAM and 64MB SDRAM. In this
platform, the data structure of AC is too large (> 53MB) to
be put into SDRAM, but that (about 1MB) of AC-Bitmap and
the proposed algorithm could reside in SRAM for speed-up.
Another platform was ARM922T processor at 125MHz with
16MB SDRAM and the implementation for AC-Rearranging
was absent because of the restricted memory space. To reveal
the effectiveness of partial renumbering, threshold t=0 and
t=128 were selected. For t=0, the partial renumbering was
discarded; for t=128, all the magic states were numbered less
than t after renumbering (more than half of magic states of
256 different symbols were equal to initial state 0). Also,
to evaluate the tradeoff between memory consumption and
throughput, (m, n) were set to (8, 2) and (4, 16), respectively.
Three different input flows: Defcon9 traces [8], the traffic
generated by ISIC [9] and a synthetic trace consisting of Snort
[1] signatures were adopted. The evaluation results are shown
in the Table I.
When (t,m,n) = (128,8,2), we have (ρC , ρH , ρBV )
= (95.37%, 2.39%, 1.61%), (95.30%, 1.51%, 0.77%) and
(5.14%, 1.73%, 3.14%) for ISIC, Defcon9 and the synthetic
trace, respectively. For (t,m,n)=(0,8,2), (ρC , ρH , ρBV ) be-
came (0%, 58.63%, 35.87%), (0%, 70.25%, 24.85%), and (0%,
1.73%, 3.17%) for ISIC, Defcon9 and the synthetic trace,
respectively. It is evident that higher ρ(ρC + ρH) conducts
better throughput. The dramatic drop (for example, from
152.57 Mbps to 41.92 Mbps in the first platform for ISIC) in
performance was caused by the absence of partial renumber-
ing and the popcount performed in matrix H . To evaluate the
worst case performance, a synthetic trace consisting of Snort
signatures was created. Although the performance decreased
enormously, the worst case throughput is deterministic and
it also occurred in many famous string matching algorithms,
such as Bloom filter [4, 5] and Super-symbol Pre-filter [6].
Following the analysis of [5] and traffic trace [8], network
traffic is not completely formed by attack signatures in real
world even if the attack behavior is occurring. In sum, the
proposed algorithm only consumes a very tiny memory space
which could be put into high-speed memory (the 1st platform)
for speed-up or be applicable in a severely resource-restricted
environment (the 2nd platform) but still achieves high perfor-
mance in real cases at the cost of poor worst case throughput.
REFERENCES
[1] [Online]. Available: http://www.snort.org
[2] A. V. Aho and M. J. Corasick. “Efficient string matching: an aid to
bibliographic search,” Commun. ACM, vol. 18, no. 6, pp. 333–340, 1975.
[3] N. Tuck, T. Sherwood, B. Calder, and G. Varghese, “Deterministic
memory-efficient string matching algorithms for intrusion detection,” in
Proc. IEEE INFOCOM 2004, pp. 333–340.
[4] T. Koack and I. Kya, “Low-power bloom filter architecture for deep
packet inspection,” IEEE Commun. Lett, vol. 10, no.3, Mar. 2006.
[5] S. Dharmapurikar, P. Krishnamurthy, T. S. Sproull, and J. W. Lockwood,
“Deep packet inspection using parallel bloom filters,” IEEE Micro, vol.
24, no. 1, pp. 52–61, Jan. 2004.
[6] N. F. Huang, Y. M. Chu, J. L. Chen, and K. J. Huang, “A non-
computation intensive pre-filter for string pattern matching in network
intrusion detection systems,” in Proc. IEEE GLOBECOM 2006, San
Francisco, USA, Nov. 2006.
[7] T. Nishimura, S. Fukamachi, and T. Shinohara, “Speed-up of Aho-
Corasick pattern matching machines by rearranging states,” in Proc.
SPIRE, Nov. 2001, pp. 175–185.
[8] [Online]. Available: http://www.defcon.org
[9] [Online]. Available: http://www.packetfactory.net/projects/ISIC/
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:47 from IEEE Xplore.  Restrictions apply. 
protocol analyzer uses a set of detection heuristics for the data it 
receives and plays as a dissector to extract the information 
when necessary. For example, the HTTP analyzer helps to 
ensure the traffic through port 80 adhere to the HTTP 
specification for detecting the tunneling of instant messaging 
and P2P software. 
By using the statistical analysis of traffic, the previous works 
[9, 10, 11] gave a different point of view on the traffic 
classification. These techniques use different parameters 
including the packet size distribution and the interactive 
relationship to classify the traffic into board categories. 
For identifying the application behaviors, the existing 
methods have the some limitations. First, each application has 
two phases that need to be tracked: the connections associated 
with the application and the packets with important information. 
However, previous works focused on the detection 
performance (evaluated by the false positives and false 
negatives) of connections and did not pay attentions on 
behavior detection. For instance, a network administrator might 
constitute a policy to block the file exchange by IM software for 
security and bandwidth management reason but allow the 
message exchange among the company's branches to save 
phone call cost. Therefore, to apply the different enforcement 
policies to different behaviors, a deeper visibility into the 
specific application is necessary. Second, the application 
detection by port-based, signature-based, or statistical-based 
approach only provides a coarse-grained information of 
whether a given protocol is in use or not. Nevertheless, this 
black-or-white result is not enough. It is essential for the 
behavior detection to have the capability of analyzing every 
application instance continuously and analyzing the parent 
connections to identify their children connections.  
III. METHODOLOGY 
From the observation of some famous P2P traffic 
(BT/eDonkey/Gnutella, etc), it is interesting to see that they 
have multiple long-term connections with different IP 
addresses simultaneously. This feature is quite different from 
normal applications which usually have few concurrent 
connections or many but short-term connections with the same 
host. Another more key observation is that normal applications 
are usually based on client-server architecture and the servers 
have domain names, but most of peers in P2P architecture do 
not have domain name. That is, the normal applications (clients) 
will first send DNS query and get DNS reply before the 
following communications while P2P applications send no or 
fewer DNS queries. Based on these two essential characteristics, 
criteria are developed to distinguish the P2P applications from 
general applications. 
To observe the mentioned behaviors, a tool is developed in 
Linux to record and parse the network traffic as well as 
behavior. In normal applications, a host can get the IP address 
of a domain name only when the DNS server returns a DNS 
reply. Here we define three metrics to measure: 
DC(DNS Count): Number of DNS replies received by a host. 
PC(Peer Count): Number of distinct remote connected hosts. 
CC(Connection Count): Number of active TCP connections. 
 
 Figure 1 demonstrates the experiment environment where a 
device with the developed tool is installed between a host and 
the Internet. All the traffic between the host to observe and the 
Internet are recorded and analyzed. Note that based on the 
defined metrics, a server in traditional client/server architecture 
will be treated as a peer. Since TCP is the major protocol used 
by most network applications, we first observe TCP behavior 
only.  
  
Figure 1: Experiment environment. 
Figures 2 to 4 illustrate the observed three metrics for famous 
eMule (P2P), BT (P2P), and HTTP (normal application), 
respectively. We can see that the eMule provides high PC and 
CC values with very low DC value. Nevertheless, the HTTP 
offers high DC value with lower PC and CC values. They can 
be obviously distinguished. But this is not clear for the BT, the 
introduced DC, PC and CC values are close. This is due to the 
BT peer initially communicates with tracker by HTTP and 
therefore its beginning behavior is just HTTP rather than P2P. It 
will perform P2P behavior later when the IP addresses of the 
peers are obtained from the tracker.  
 
Figure 2: Observed behavior of eMule. 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:49 from IEEE Xplore.  Restrictions apply. 
 
Figure 6: Observedβ value (UDP traffic) of voice-based and 
video-based P2P applications. 
 
IV. SYSTEM IMPLEMENTATION AND EXPERIMENTAL 
RESULTS 
A. Implementation 
We have implemented the proposed mechanism on the Linux 
with kernel 2.6.25.8 as a transparent device named P2P 
Detector. Figure 7 illustrates how the developed transparent 
device is working with gateway to identify the P2P applications. 
From the above observations we define a threshold of β to 
distinguish P2P applications. Packets are forwarded or blocked 
by ip_queue module in Linux. When a host triggers the 
threshold, it is identified as using the P2P applications, and 
therefore its packets are then blocked for evaluation purpose. 
  
 
 Figure 7: The experimental environment. 
 
B. Experimental Results 
To gather statistics, we first run 10 trials for each application 
and each trial lasts for 5 minutes. The average β of each trial is 
then calculated and listed in Table I. The average β can be 
referred as an index to distinguish the P2P applications later. 
We then verify our proposed mechanism by running P2P 
and HTTP applications with the P2P Detector. Two hosts with 
Windows XP sp2 behind the P2P Detector are used to run the 
applications. Apparently  other normal applications except 
HTTP will never be detected as P2P applications. It is expected 
that the HTTP applications should pass the P2P Detector and all 
the P2P applications should be correctly identified and blocked. 
It is easy to check if a P2P application is blocked by the client 
side as all its traffic will be dropped. 
Table 1. Average β of each application. 
Protocol
Trial 
BT eMule Gnutella Skype PPStream PPLive HTTP
TCP UDP TCP UDP TCP UDP UDP UDP UDP UDP 
1 79.98 80.33 681.23 502.40 241.50 1255.33 56.05 210.51 405.45 9.23 
2 54.58 84.59 612.60 449.03 195.06 427.61 67.32 476.78 433.21 11.30 
3 34.70 72.14 562.67 465.60 177.29 271.23 37.90 345.19 244.71 2.10 
4 61.49 52.78 547.83 458.20 194.32 1396.81 36.77 407.04 293.29 22.04 
5 59.20 63.86 594.17 474.77 182.51 296.29 37.42 348.14 911.81 4.81 
6 56.12 63.47 557.43 457.77 199.19 1364.94 37.38 307.24 599.75 3.19 
7 82.88 127.22 639.33 425.80 189.64 225.46 37.58 454.14 947.37 0.75 
8 21.00 74.79 490.07 442.83 190.23 1279.75 38.03 399.59 421.29 0.50 
9 31.29 193.78 495.73 449.46 193.68 83.78 35.85 435.62 637.24 9.73 
10 91.50 94.59 330.47 348.90 199.43 877.29 26.68 373.75 380.54 1.62 
Average 57.27 90.76 551.15 447.48 196.29 747.85 41.10 375.80 527.47 6.53 
 
We have 10 trials for each application and again, each trial 
lasts for 5 minutes. If a P2P application can’t be detected within 
5 minutes, it means that we fail at this trial and then have a false 
negative. The threshold of β is set as 80. If the β of a host is 
larger than 80, it is treated as using P2P. An HTTP application 
triggers the threshold within 5 minutes means a false positive as 
HTTP should not trigger the threshold. There are many 
programs developed for the same protocol, e.g. BT protocol has 
BitTorrent, BitComet, Azures, µTorrent, etc. We use programs 
from official website, i.e. BitTorrent for BT protocol and eMule 
for eMule protocol.  Each program uses default settings for test.  
The decision of threshold of β  is a tradeoff. A higher 
threshold produces lower false positive rate and longer 
detection time. On the contrary, a lower threshold obtains a 
shorter identification time but higher false positive rate. 
Apparently P2P applications with peers of public IP 
addresses perform better than that of private IP addresses 
(behind NAT). Thus is because initially the peers behind NAT 
are unable to communicate directly and need to be bridged via a 
super-node with public IP address. We evaluate the 
performance for both environments. Table2 demonstrates the 
time experienced to identify the use of P2P applications when 
hosts are with public IP addresses. BT and eMule can be 
detected by UDP because they implement DHT by UDP. 
Apparently, eMule soon can be detected while BT needs near 3 
minutes to be detected. The HTTP application didn’t trigger the 
threshold in each trial as we expected during experiment and 
then we don’t list in tables. Table 3 lists the same result as 
Table2 except that hosts are with private IP (behind NAT). It 
shows that the proposed mechanism still works well even with 
private IP. 
Table 4 lists the hit rate, false positive rate and false negative 
rate of each application when hosts are with public IP. The hit 
rates for all P2P applications are 100%. Table 5 is the same as 
Table 4 except that hosts are with private IP (behind NAT). BT 
has a false negative when threshold is 80. The hit rates for all 
other P2P applications are 100%, this indicates the proposed 
mechanism can accurately identify the P2P applications. Again, 
as we expected, the HTTP has no false positive under this 
This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE ICC 2009 proceedings
Authorized licensed use limited to: National Tsing Hua University. Downloaded on November 2, 2009 at 22:49 from IEEE Xplore.  Restrictions apply. 
IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING 1 
 
In-depth Packet Inspection Using a 
Hierarchical Pattern Matching Algorithm 
Tzu-Fang Sheu, Graduate Student Member, IEEE, Nen-Fu Huang, Member, IEEE and 
Hsiao-Ping Lee  
Abstract—Detection engines capable of inspecting packet payloads for application-layer network information are urgently 
required. The most important technology for fast payload inspection is an efficient multi-pattern matching algorithm, which 
performs exact string matching between packets and a large set of pre-defined patterns. This paper proposes a novel Enhanced 
Hierarchical Multi-pattern Matching Algorithm (EHMA) for packet inspection. Based on the occurrence frequency of grams, a 
small set of the most frequent grams is discovered and used in the EHMA. EHMA is a two-tier and cluster-wise matching 
algorithm, which significantly reduces the amount of external memory accesses and the capacity of memory. Using a skippable 
scan strategy, EHMA speeds up the scanning process. Furthermore, independent of parallel and special functions, EHMA is 
very simple and therefore practical for both software and hardware implementations. Simulation results reveal that EHMA 
significantly improves the matching performance. The speed of EHMA is about 0.89–1161 times faster than that of current 
matching algorithms. Even under real-life intense attack, EHMA still performs well. 
Index Terms—Network-level security and protection, Network Security, Intrusion Detection, Pattern Matching, Content 
Inspection. 
——————————
      —————————— 
1 INTRODUCTION
etwork services are extremely important since 
many companies provide services over the Internet. 
A variety of Internet-based applications have 
created a strong demand for content-aware services, 
network policy and security management. Furthermore, 
increasing amounts of important information exist in 
packet payloads. Therefore, low-layer network 
equipment is inadequate for checking the information, 
since it only checks specified fields of the packet headers. 
High-layer network equipment providing in-depth 
packet inspection, such as intrusion detection systems 
(IDSs), application firewalls, anti-virus appliances and 
layer-7 switches, is prerequisite in a network. Such 
equipment typically contains a policy or rule database 
applied to finding certain packets over the network. 
Every rule in the database consists of several patterns 
(also called signatures) and a matching action (or a series 
of actions). These patterns describe the fingerprints of 
packets.  
The network equipment applies the pre-defined 
patterns to identify and manage the monitored packets 
over the network. Different network equipment may have 
different pattern databases applied respectively to attack 
detection, bandwidth management, load balancing and 
virus blocking over the network. However, they have 
similar features in terms of patterns and matching 
procedures. The number of patterns is typically a few 
thousands, and the lengths of the patterns are varied. The 
patterns may appear anywhere in any packet payload. 
Consequently, the emerging high-layer network 
equipment needs a pattern detection engine capable of in-
depth packet inspection, which searches the entire packet 
headers and payloads for pattern matching. Network 
equipment then employs the detection results to manage 
network systems intelligently. For instance, Snort is an 
open-source network-based intrusion detection system 
(NIDS), and is adopted for detecting anomalous intruder 
behavior with a set of patterns, and generating logs and 
alerts from predefined actions [1]. One of the patterns of 
Nimda worm is described as “GET 
/scripts/root.exe?/c+dir”. When the detection engine of 
Snort finds this pattern existing in a packet, the 
corresponding alert is generated to warn network 
administrators. The pattern matching is considered as the 
most resource-intensive task in the Snort detection engine 
[2]. Hence, this study focuses on the nascent issues of the 
payload inspection. 
The most important part of a detection engine is a 
powerful multi-pattern matching algorithm, which can 
efficiently process the pattern matching task to keep up 
with the growing data volume in the network. However, 
conventional string-matching algorithms are impractical 
for packet inspection [3], [4], [5]. Due to the large pattern 
database, an effective detection engine must be able to 
search for a set of patterns simultaneously, rather than 
iteratively performing the single-pattern matching. While 
considering implementation issues of the network 
equipment, the performance of processing packets is not 
xxxx-xxxx/0x/$xx.00 © 200x IEEE 
———————————————— 
• T.F. Sheu is with the Institute of Communication Engineering, National 
Tsing Hua University, Taiwan, R.O.C. E-mail: sunnie@ieee.org. 
• N.F. Huang is with the Department of Computer Science and Institute of 
Communication Eng, National Tsing Hua University, Taiwan, R.O.C. 
(corresponding author. phone: +886-3-5731063; fax: +886-3-5723694; 
E-mail: nfhuang@cs.nthu.edu.tw). 
• H.P. Lee is with the Department of Applied Information Sciences, Chung 
Shan Medical University, Taiwan, R.O.C. E-mail: ping@csmu.edu.tw. 
Manuscript received 15 Aug. 2007; revised 11 Sep. 2008; accepted 17 Sep. 2008. 
N 
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 3 
 
iteration [11]. 
Several modifications to BM-based algorithms have 
been proposed for the multi-pattern matching. Risk and 
Varghese’s approach (RV) groups all patterns to pre-
calculate the number of safety shifts of each character [5]. 
Wu and Manber’s approach (WM), which assumes that 
all patterns are larger than M characters, groups B-grams 
of the M-character prefixes of all patterns to build a shift 
table [10]. The WM’s shift table contains the valid shifts of 
each B-gram. Liu et al’s algorithm (WM-PH) groups the B-
character prefixes of all patterns to build a large hash 
table, in which each entry contains valid shifts of the 
corresponding B-character prefix [12]. However, the 
maximum shift value of RV and WM must be not larger 
than the minimum pattern length in P, in order to avoid 
missing any pattern. Thus, RV and WM are unfeasible 
when the pattern set includes single-character patterns. 
The required memory space of the table in the algorithms 
WM and WM-PH is in the order of O(|Λ|B). Generally, B 
= 3, and the table consists of 16M entries when the 
alphabet size is 256 as in one-byte coding. The large tables 
must be stored in the external memory, which leads to 
long access delay during the matching process.  
It has been pointed out that the Aho-Corasick 
algorithm (AC) provides the best worst-case 
computational time complexity [4]. Using a compressed 
structure, Tuck et al proposed AC-C, a modification of AC, 
and reduced the required memory to about 2% of AC [8]. 
ACM applied a magic number derived from the Chinese 
Remainder Theorem to AC [14]. ACM reduced the 
required memory space and computation complexity, and 
thus improving the worst-case performance. However, 
the required memory of AC-C and ACM is typically too 
large to be cached in the on-chip memory of embedded 
systems, field programmable gate arrays (FPGAs) and 
network-processor-based platforms. Although the AC-
based algorithms have the best worst-case computational 
time complexity, the latency of external memory accesses 
dominates the processing performance rather than 
computational time. Coit et al proposed a matching 
algorithm for Snort that combines BM and AC [15]. 
However, this algorithm requires three times the memory 
of the standard version, and may produce inconsistent 
matching results.   
A Piranha algorithm was proposed based on an idea 
that if a least popular B-gram of a pattern exists in a packet, 
than this packet may have a pattern [16]. A least popular 
gram of a pattern was chosen as an index key of a pattern. 
However, the Piranha algorithm can not handle the 
patterns smaller than B, and the required memory space 
is very large (O(|Λ|B)). Although the idea of least 
popular index keys can reduce the collisions of patterns, 
the hit rate of index table is increased, thus increasing the 
number of external memory accesses and pattern 
comparisons.  
In the case of hardware solutions, Li et al presented an 
FPGA-based detection engine for NIDSs, using the 
internal content addressable memory (CAM) technology 
to speed up multi-pattern matching [17]. Since an internal 
CAM of FPGA is not large enough to store all patterns, Li 
et al’s approach has to dynamically reload a block of 
patterns into the CAM, causing long latency. Moreover, 
the patterns of varied lengths complicate the formulation 
of a CAM for exact matching, but Li et al’s approach does 
not mention the solution for patterns with varied lengths. 
Dharmapurikar et al used Bloom Filters (BFs) and Kim et 
al employed mask filters in the FPGA-based packet 
inspection [18], [19]. However, these two methods only 
act as pre-filters and have to cooperate with another 
string matching algorithm to verify a match, and 
furthermore this Bloom Filter based algorithm can be 
used only in the case that all patterns are longer than a 
certain length. Lu et al used several binary CAMs and BFs 
to implement parallel compressed deterministic finite 
automata (DFAs), and Dharmapurikar et al combined AC 
with BFs for packet inspection [20], [21]. These two 
methods applied parallel BFs and assumed that BFs can 
execute one query every clock cycle. However, these 
architectures and assumptions can only be established in 
some specific hardware implementations. BFs are 
inefficient in the software implementations, because one 
BF consists of several hash functions and the computation 
time of hash functions is usually expensive in software [6]. 
4 THE ENHANCED HIERARCHICAL MULTI-PATTERN 
MATCHING ALGORITHM 
Some network equipment is implemented by network 
processors, FPGAs, networking on chips (NOCs) or 
system-on-a-programmable-chips (SOPCs) to improve the 
performance. The embedded memory of these platforms 
is typically very small. For instance, the Intel IXP2x00 
network processor has only a 4KB instruction cache and a 
2KB data cache in each micro-engine, while the Vitesse 
IQ2000 network processor has a 4KB data cache (2KB for 
local storage and 2KB for reserved header buffers) [22], 
[23]. Although high-end FPGAs providing up to 1 MB 
embedded memory are available, linking many memory 
blocks degrades the chip performance. Nevertheless, the 
required memory of the previous pattern matching 
algorithms is generally larger than 300 KB for NIDSs. 
Hence, the patterns and the tables built by matching 
algorithms need to be stored in external memory.  
However, frequently accessing the external memory 
(to read patterns or tables) significantly decreases the 
matching efficiency due to the external memory access 
latency is very long and indeterminable. For example, 
Intel IXP2x00 needs about one cycle for one 
microprocessor instruction, but about 150 cycles for each 
access from SRAM (or 250~300 cycles from DRAM) [7]. 
The memory latency strongly affects the throughput of 
pattern matching. Therefore, reducing the number of 
required external memory accesses is more important 
than reducing the amount of computational time.  
This study proposes an Enhanced Hierarchical Multi-
Pattern Matching Algorithm (EHMA) based on a 
hierarchical and cluster-wise architecture. EHMA 
comprises two small index tables, namely the first-tier 
table (H1) and the second-tier table (H2). These two tables act 
as filters to avoid unnecessary external memory accesses 
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 5 
 
depending on the available size of on-chip cache. 
3.2 Cluster Balancing Strategy (CBS) 
Most packets are innocent in general situations. Even a 
harmful packet may contain only few patterns. Therefore, 
comparing all of the patterns in the large P with each 
input packet is time consuming. If the patterns in P can be 
distributed into different small clusters based on their 
similarity, then only the pattern in each cluster that is 
most similar to the suspected packet needs to be 
compared, thus improving the efficiency of the matching 
process. This subsection presents strategies to attain this 
goal. First, the method of clustering a set P based on the 
similarity of patterns is described. Then a cluster 
balancing strategy (CBS) is adopted to balance the cluster 
size. A second-tier table (H2) for on-line matching can be 
constructed based on the clusters. 
The clustering pivots are the keys used to distribute 
patterns, where each clustering pivot is a common gram 
of patterns defined previously. Two common grams are 
employed as a pair of clustering pivots, called a pivot pair, 
say (a, b), where the first pivot is a frequent-common 
gram, and the second pivot is the substring following the 
frequent-common gram. Let Pa,b represent a cluster of 
selected patterns (a subset of patterns) with the pivot pair 
(a, b), which means that Pa, b = {pi | ‘ab’ ⊂ pi, a∈ F and 
b∈ 2BΛ }, where ‘ab’ is the combination of two strings a 
and b and is a substring of pi; F is the result of GFGS, and 
B2 is the length of the second pivot. Notably, a pattern is 
assigned to only one cluster in the clustering strategy, 
although a pattern may have more than one pivot pair. 
That is, the clusters have the following properties: for any 
cluster Pa,b ⊂ P, that ∪ all a, b Pa,b = P, and ∩ all a, b Pa,b = ∅ . 
Since a pattern may have several opportunities to select a 
cluster, a better assignment can lower the maximum 
cluster size, and thereby improve the worst-case 
performance of EHMA.  
The pattern grouping is based on F. To lower the worst 
matching time, CBS is adopted to balance the size of all 
clusters. In CBS, an 2
BF Λ×  matrix N = (na, b) is used to 
record the current size of every cluster Pa, b during the 
pattern grouping procedure. The CBS is as follows. 
(1) First, read one pattern at a time from P and scan the 
pattern.  
(2) According to GFGS, for any given pi, there exists a B1-
gram g∈F, where B1 is the length of a frequent-common 
gram. To balance the cluster size, CBS finds the smallest 
na, b, given by nx, y, among all available pivot pairs (a, b)s of 
pi, for all a∈F and ‘ab’⊂ pi.  
(3) After grouping pi into the smallest cluster Px, y, the 
corresponding nx, y is also incremented.  
All patterns are distributed sequentially into the 
designate clusters. Accordingly, GFGS and CBS divide 
the large P into smaller subsets.  
3.3 Safety Shift Strategy 
This section presents a safety shift strategy to derive 
the values of the shift fields of H1 and H2. H1 and H2 can 
use the same strategy to derive their safety shifts 
respectively. As mentioned previously, as long as no 
frequent-common gram is matched in input strings, then 
no pattern exists. Therefore, if no frequent-common gram 
is missed, then no pattern will be missed. The safety shift 
strategy is based on a modified bad grouped character 
heuristic [7], named frequency-based bad gram heuristic in 
this study. The safety shift strategy ensures that no 
frequent-common gram is missed during a skippable 
scanning process. The proposed strategy helps EHMA to 
speed up the on-line matching process, since certain 
characters can be skipped unhesitatingly.  
Assume that x identifies all possible index keys, and 
that the length of x is B. Because the index keys of H1 and 
H2 are different, the parameters used to determine the 
shift fields of these two tables are different. For H1, as the 
length of a frequent-common gram is B1, thus x∈ 1BΛ  and 
B = B1. For H2, since x is all the possible of the pivot pairs 
(a, b), x∈F× 2BΛ  and B = B1+B2. The basic concept of the 
safety shift strategy is that: if x is not a gram of any 
pattern, and any suffix of x is not any prefix of any 
pattern in P, then it is safe to shift m when x is scanned; 
otherwise, the number of safety shifts is the offset 
between the rightmost occurrence position of x and the 
position of the frequent-common gram nearest to x. Two 
parameters are needed to derive the safety shifts, namely 
W, and m, as shown in Figure 2. Assume that B≤W≤m, 
and define the safety shifts of each entry (H(x).shift) as 
follows: 
(1) Initially, all shift fields of the table H are set as 
  If m > W, then  
H(x).shift = m - W + q, 
where q = min{q | ∃  sub(x, q+1, B - q) = sub(p, 1, B - q), 
∀  p∈P and 1 ≤  q < B} when B > 1 and q exists; 
otherwise q = B. 
 Else 
H(x).shift = r, 
where r = min{r | ∃  sub(x, r+1, B - r) = sub(f, 1, B - r), 
 
(a) Spectrum of 2-gram.                                                                                      (b) Spectrum of 1-gram. 
Figure 4. The pattern spectrum when |P| = 1200. 
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 7 
 
after scanning the first W-B2 characters of the sampling 
window of every pattern (the underlined characters of the 
patterns in Figure 5a), the matrix R is obtained and shown 
in the figure. In the first run, the maximum value on the 
diagonal of R is three, and thus the corresponding gram 
‘e’ is added into F. After refreshing the elements on the 
diagonal of R (lines 8 – 9 of Figure 3), GFGS finds that the 
maximum value on the diagonal of R is two in the second 
run, and the corresponding gram is ‘h’. GFGS stops while 
all elements on the diagonal of R are zero, and gets 
F={‘e’, ‘h’}. Figure 5b displays the logical architecture of 
the two-tier tables of EHMA. Because B1= 1, and the H1 
table has only 26 entries, the H1 table can be stored in the 
cache memory. The fid fields of H1 point to the 
corresponding offsets of H2. As the pattern ‘actress’ has 
‘e’∈ F and the pivot pair ‘es’, according to CBS it is 
grouped to the cluster Pe, s. The shift fields of H1 and H2 
are obtained from the proposed safety shift strategy. 
Initially, since B1 ≤ 1, H1.shift = 4. While B1+B2 > 1, H2.shift 
is set to 5 for those entries whose second pivot is not the 
prefix of any pattern (that is, b∉{‘a’, ‘f’, ‘t’}); otherwise, 
H2.shift is set to 4. When scanning the pattern ‘actress’, the 
shift fields of H1(‘a’), H1(‘c’) and H1(‘t’) are updated to 3, 2 
and 1 respectively (the 2nd safety shift strategy); the shift 
fields of H1(‘r’) and H1(‘s’) are both updated to 1, while 
the H1(‘e’).shift is updated to zero, because ‘e’∈F (the 3rd 
strategy). As for the table H2, only the existing entry 
H2(‘e’, ‘s’) has to be updated to two, because B = B1+B2 = 2, 
and no prefix of F is the suffix of ‘es’ (the 3rd strategy). 
The remainders of the patterns follow the same clustering 
and safety shift strategy. The shift fields of H1 and H2 
tables are updated when the new shift is less than the 
previous one. Let us see H1(‘a’) for example. When 
scanning the pattern ‘actress’, H1(‘a’).shift = 3 (as p1[i] = ‘a’, 
i = 1 and m-W-i+1 = 3); while scanning the pattern 
‘teacher’,  H1(‘a’).shift is updated to 1 (as ‘a’ is the third 
character of ‘teacher’: i = 3, then m-W-i+1 = 1), because the 
new value is smaller than the previous one (the 2nd 
strategy). Finally, H1(‘a’).shift = 1 is saved in the table 
because the remaining patterns do not have H1(‘a’).shift 
smaller than one.  Notably, the maximum shift of H1 and 
H2 is large (4 and 5 respectively). Consequently, the 
number of scans and comparisons can be significantly 
reduced.   
3.5 The On-line Hierarchical and Cluster-wise 
Matching 
The previous subsections presented the off-line stage 
of EHMA, which builds two index tables H1 and H2, 
holding the indexing and pattern information in the cache 
memory and external memory respectively. These two 
tables are regarded as the two-tier filters and indices for 
the on-line matching. This subsection presents the on-line 
matching procedure in detail.  
In network intrusion detection systems, an input 
packet is forwarded to a detection engine. The detection 
engine then returns the search results of matched patterns 
PM. This study focuses on the payload inspection, and 
assumes that each input is a packet payload T. As a 
hierarchical matching, the on-line matching procedure of 
EHMA is divided into two tiers: Tier-1 Matching and 
Tier-2 Matching. The hierarchical architecture is applied 
to decrease the number of external memory accesses. The 
small H1 is stored in the cache of the processing unit for 
Tier-1 Matching, while the H2 with pattern content is in 
the external memory for Tier-2 Matching. The external 
memory access is necessary only when the Tier-2 
Matching is invoked. This process is described in detail in 
the following subsections. 
3.5.1 Tier-1 Matching 
In on-line matching, the payload T is scanned from left 
to right, and each B1-gram of T is the key to fetch the 
entry H1(t1), where t1 = ][1 iT B . The H1 acts as the first-tier 
filter of EHMA, by checking whether T may likely contain 
patterns belonging the pattern set P. Because H1 is small 
enough to be stored in the on-chip memory during the 
on-line matching procedure, the latency of accessing H1 is 
very small.  
In the Tier-1 Matching, first the shift field is checked. If 
H1(t1).shift ≠ 0, i.e., t1∉ F, then no external memory is 
necessary. The obtained H1(t1).shift also determines the 
number of grams that can be skipped without further 
process. The next gram to check is then 
1 1
1[ ( ). ]BT i H t shift+ . After read the next gram, the 
matching process repeats as in the previous steps, and 
remains in the Tier-1 Matching. Because |F|≪ 1BΛ , the 
probability of t1∈F is small and most grams of T gain the 
shifts, thus avoiding the Tier-2 Matching. Consequently, 
both the number of string comparisons and the costly 
memory accesses can be significantly reduced.  
Otherwise, if t1∈ F, then T may contain a malicious 
pattern pk∈P, where t1 ⊂ pk. Simply stated, if H1(t1).shift = 
0, then T may have a pattern that belongs to the cluster of 
pivot pair (t1, t2), where t2 = ][ 12 BiT B + . Therefore, the 
matching procedure activates Tier-2 Matching to identify 
the pattern. If H1(t1).pid is not NULL, then the current 
gram t1 itself is a pattern, and this matched pattern is also 
added into PM.  
3.5.2 Tier-2 Matching 
After the Tier-1 Matching, if H1(t1).shift = 0, then the 
matching procedure proceeds to the Tier-2 Matching. The 
function H2(t1, t2) indicates the location of the 
corresponding cluster according to input T. Since EHMA 
is a cluster-wise matching algorithm, only the patterns in 
the small cluster of pivot pair (t1, t2), which are similar to 
T, are loaded to the processing unit for further checks.  
Tier-2 Matching first checks the pid field of H2. If 
H2(t1, t2).pid is NULL, then the cluster (t1, t2) contains no 
pattern, and no pattern comparison is necessary. 
Otherwise, if H2(t1, t2).pid is not NULL, then this cluster 
contains patterns. The pattern content in the H2(t1, t2).data 
is then compared with the corresponding substring of T: 
sub(T, i-H2(t1, t2).offset, H2(t1, t2).size). If H2(t1, t2).next is 
valid, and points to the next entry, here given by H2(a, b), 
then the cluster contains other patterns. Similarly, the 
pattern in H2(a, b).data is also fetched and compared with 
the substring of T starting at T[i−H2(a, b).offset] of length 
H2(a, b).size. Every matched pattern is added to the 
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 9 
 
entry in the H2, then a random B1-gram of p, say g, is 
chosen and added into F (H1(g) is modified accordingly), 
and a memory space is allocated for cluster set Pg in the 
H2. A random pivot pair of p, say (g, h), is chosen and then 
p is added into the cluster Pg, h. The shift fields of H1 and 
H2 may be modified because of the added p. Since the 
safety shift strategy scans the patterns one by one to 
calculate the shift values, no modification to the safety 
shift strategy is required for pattern addition. The added 
p can be recognized as the last scanned pattern of the 
safety shift strategy. At most |p|-B1+1 fields of H1 and 
|p|-B2+1 fields of H2 are modified for a pattern addition.  
To delete a pattern p from P, first step is to find the 
pattern. When p is found, just link p’s previous entry to 
p’s next entry by modified its next field in the H2, and 
delete p from tables. Then, subtract the count field of the 
cluster that p belongs to. The shift fields are not modified 
for pattern deletion. Because the shift values are universal 
minimum in the safety shift strategy, they may not be 
optimum after pattern deletion. However, no error will 
occur after pattern deletion, even while the shift fields are 
not modified. Consequently, EHMA needs not recalculate 
the whole index tables as long as the pattern database is 
changed. EHMA can refresh the index tables when the 
system is not busy. 
3.7 Worst Case 
If a given string T is formed badly that has to do the 
exact string comparisons the most times, and no character 
of T can be skipped during the on-line matching process, 
processing this bad-formed T is the worst case of EHMA. 
Assume the largest cluster size is Lc. When every 
character T[t] ∈ F, H1(T[t]).shift = 0, and each 
corresponding indexed cluster is the largest (|PT[t],T[t+1]| = 
Lc), T is a bad-formed string and this is the worst scenario 
of EHMA. As for all T[t], T[t]∈F and H1(T[t]).shift = 0, the 
probability to fetch the table H2 for the bad-formed T is 
one. Thus, the number of external memory accesses per 
character in the worst case is  
T
LBT
N cWSTRAM
×−
=
)( 2
< Lc, 
where assume that fetching one pattern needs one 
memory access. Define the largest pattern size in P as Lp. 
When every input character points to the largest cluster, 
in which every pattern has the longest size, this bad-
formed T requires the largest number of comparisons. 
Hence, the number of character comparisons per input 
character is 
pcp
WST
RAM
WST
CMP LLLNN ×<×= . 
Obviously, the worst-case performance depends on Lc. 
To derive Lc, assume there is a largest cluster, say Px, y. 
Since Px, y is the largest cluster, assume that the cluster 
size is always larger than one, and initially the probability 
that its cluster size increases from 0 to 1 is one.  
As Px, y is the largest cluster, based on CBS, a given 
pattern p will not be clustered into Px, y, unless all 
available pivot pairs of p are not in the set F×Λ except (x, 
y). Since the pattern database is usually predefined and 
static, assume the given patterns are uniformly 
distributed. Therefore, the probability that |Px, y| 
increases from i to i+1 is  
1
2
2
,
2
1}1Pr{
−−








Λ
+Λ×−Λ
=+→=
Bp
yx
F
iiP . 
As in the worst-case scenario, every pattern has the 
longest size Lp, the equation is rewritten  
1
2
2
,
2
1}1Pr{
−−








Λ
+Λ×−Λ
=+→=
BL
yx
p
F
iiP . 
Thereby, the probability that the cluster size of Px, y is 
maximum (Lc) is derived 
)1)(1(
2
2
,
2
1}Pr{
−−−








Λ
+Λ×−Λ
==
cp LBL
cyx
F
LP . 
When |P| is 1200 with |F| = 77, |Λ| = 256 and Lp = 
128, the probability that Lc = 4 is only 7× 10-79. When 
replacing Lp with the average pattern size, which is about 
eleven in the Snort, then the probability that Lc = 4 is 
about 3.6× 10-6. The probability that Lc = 4 is tiny, which 
infers that EHMA has a small Lc, and thus 
WST
RAMN  and 
WST
CMPN  are small. Consequently, the worst-case 
performance of EHMA is moderate and acceptable 
because Lc is much smaller than |P|. 
4. RESULTS 
As the number of network security threats rises, the 
NIDS has become one of the most important applications 
of packet inspection [24], [25]. Therefore, this study 
demonstrates the feasibility of integrating the proposed 
EHMA with the promising NIDS. This section presents 
the simulation results of EHMA deployed in the NIDS, 
compared with the original hierarchical matching 
algorithm (HMA) [9], the Boyer-Moore-Horspool 
algorithm (BMH) [3], the Wu-Manber algorithm (WM) 
[10], a variant of the Wu-Manber algorithm using a 
grouped prefix hash  (WM-PH) [12], and the Aho-
Corasick algorithm with memory compression (AC-C) [8]. 
In the simulations, the assembly-like microprograms were 
emulated for EHMA, BMH, WM, WM-PH and AC-C 
using RISC instructions of general network processors 
(such as ADD, XOR, MOV), and the number of 
instructions and the number of memory accesses needed 
to process a packet were calculated. To simplify the 
evaluation, the simulation assumed that one 
microprocessor was employed. 
4.1 Measurements 
Define I as the average number of RISC instructions 
(including comparisons and calculations), and L as the 
average number of local memory accesses (including 
reading data from the cache to the registers for further 
processes), for each payload character in the pattern 
matching. E represents the average number of external 
memory accesses per input character, which includes 
loading the input packets, querying the entries of tables in 
the external memory, and fetching the patterns. wI 
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 11 
 
Λ|3 entries. Every pattern in the BMH has its own skip table 
of |Λ| entries, so that the table of BMH has |P| × |Λ| entries. 
Because each skip table of BMH (for one pattern) is small 
enough to be loaded into the local memory, for fairness, a 
cache memory space was allocated to lower the number of 
external memory accesses. The BMH-O is the original BMH 
with no local cache and assesses the latency penalty. 
Notably, WM-PH, AC-C and BMH-O also require cache 
memory to store the skip value or one state during the 
matching process. TABLE IV lists the memory requirements 
of EHMA, HMA, WM, WM-PH, BMH and AC-C. The 
scale relation of the parameters is |F|< | Λ| ≪ |P| < S ≪|Λ|3. 
In the simulations using Model I, when |P| is 1200, the H1 
and H2 of EHMA needs 256 and 19712 entries respectively 
(about 768 bytes on-chip memory and 38.5KB external 
memory, including the shared memory pool); HMA has the 
same number of entries as EHMA, but needs smaller entry 
size as HMA has no shift field; the table of WM needs more 
than 16M entries (16MB external memory, in the case 
without using an additional prefix table); the table size of 
WM-PH is the same as that of WM; BMH and BMH-O need 
more than 300K entries (300KB external memory); and AC-
C needs 10731 states (461KB with each node of 44 bytes). 
The memory size of all algorithms listed previously 
excludes pattern content. Obviously, the required memory 
space of EHMA is quite small. 
4.4 Results and Discussion 
The minimum pattern length of the feeding patterns in 
Figures 10–13 is only one character, i.e., M = 1. Because 
the minimum pattern length of WM is restricted to be 
larger than the gram size, in this case three characters, 
WM is not compared in these figures. In Figures 10–13, 
the results labeling EHMA in the following simulations 
use the sampling window with parameters W = m = |pi|, 
which means that each pattern is sampled in its entirety.  
Figure 10 compares the average matching time (Ψ ) of 
EHMA, HMA, WM-PH, AC-C, BMH and BMH-O using 
Model I with different attack loads λ = 0 and λ = 4 
respectively. It also shows the impact of the number of 
patterns (|P|) on the matching time. Simulation results 
reveal that EHMA outperforms others even when |P| 
and λ  increase. EHMA has slightly higher growth rate 
than WM-PH, because it has a much smaller table size. 
WM-PH gains performance by having a large direct 
index table. Notably, the matching time of the original 
AC using basic structure is independent from |P| and λ . 
The curves of AC-C increase with |P| and λ  owing to 
the popsum used in the AC-C algorithm. The increasing 
|P| makes the matching time of BMH (BMH-O) rise 
steeply, because the BMH is originally a single-pattern 
matching algorithm that simply executes iteratively for 
multi-pattern matching.  
The case λ = 0 means that the traffic has no malicious 
packets. In this case, the proposed EHMA needs only 9.5–
19.9 cycles per character on average, which is about 0.9, 
3.3–5.3, 16.3–26.8, 40–117 and 408–1161 times less than the 
matching time of HMA, WM-PH, AC-C, BMH and BMH-
O, respectively, under various pattern set sizes. We can 
say that EHMA is very appropriate for network 
equipment, because generally most packets are innocent 
( 0≈λ ). The time available for the detection engine to 
process the malicious packets rises as the innocent 
packets are processed more quickly. 
When λ = 4, then the systems are under heavy attack, 
and the traffic contains many monitored patterns. In this 
situation, the matching time of EHMA is about 0.89–0.94, 
3.1–4.5, 14.1–24.9, 33.2–96.4 and 335–957 times less than 
that of HMA, WM-PH, AC-C, BMH and BMH-O 
respectively. Additionally, the performance of EHMA is 
quite stable, since Ψ  rises only slightly as λ  or |P| rises.  
The processing time of the pattern matching includes 
the time necessary for instructions ( Iψ ) and the time for 
memory accesses ( Mψ ). To investigate their impacts on 
the algorithms, these two measurements are separated 
from overall matching costs since different systems 
introduce different implementation overheads. Figure 11 
displays the proportion of Iψ  to Ψ  and Mψ  to Ψ  
respectively, for all approaches using Model I with |P| = 
1200, where Figure 11a shows the results under λ = 0, and 
Figure 11b shows the results under λ = 4. In Figure 11, the 
upper part of the bar is Iψ  and the lower part of the bar 
is Mψ . The results show that the Iψ  of EHMA is close to 
HMA’s and WM-PH’s, but Mψ  of EHMA is much less 
than others. The proportion of Mψ  to Ψ  of BMH seems 
smaller than others, because the whole skip table of a 
pattern is idealistically assumed to be loaded within one 
external memory access and kept in the cache during the 
matching process for each pattern. Because AC-C 
compresses the data structure of the state machine, it 
requires more time to derive the next state pointer. 
Therefore, AC-C does not have the smallest Iψ . 
Simulation results show that the Iψ  does not 
significantly rise with λ  in any of the experiments, 
because each algorithm has already tried to reduce the 
computation load ( Iψ ). However, Mψ  dominates the 
overall matching cost. This reveals that the number of 
external memory accesses is the bottleneck of almost all 
TABLE V. THE STATISTICS OF THE TRAFFIC TRACES. 
Statistics Model II Model III 
Average Packet Size (Byte) 467.71 896.1 
The Standard Deviation of the Size of each Packet (Byte) 651.06 690.99 
Data Transmission Rate (Kbps) 254.13 280.03 
Number of Packets per second 69.55 40 
 
1
10
100
1000
10000
100000
200 400 600 800 1000 1200
Number of Patterns
Cy
cle
 
Ti
m
e
EHMA     = 0
EHMA     = 4
HMA     = 0
HMA     = 4
WM-PH     = 0
WM-PH     = 4
AC-C     = 0
AC-C     = 4
BMH     = 0
BMH     = 4
BMH-O     = 0
BMH-O     = 4
 
Figure 10. The average matching time ( Ψ ) versus the number of 
patterns (|P|), using Model I with λ = 0 and λ = 4, where wE = 100.  
TZU-FANG SHEU ET AL.:  IN-DEPTH PACKET INSPECTION USING A HIERARCHICAL PATTERN MATCHING ALGORITHM 13 
 
spectrum is not normally distributed, the actual average 
number of shifts during matching process is not the same 
as the average of H1.shift and H2.shift. However, the trend 
is the same. E is effected by both |F| and the actual 
average shift. 
Figure 14a shows EHMA(W = 5) outperforms EHMA 
and others when |P| = 200; while EHMA performs better 
than EHMA(W = 5) and others when |P| = 500. 
Therefore, reducing |F| becomes more important than 
increasing the average number of shift values when |P| 
is large. Since all algorithms need a copy of the pattern 
contents, Figure 14b only displays the extra memory 
requirement of every algorithm for the index/hash tables. 
Figure 14b shows that the required memory of EHMA is 
only slightly larger than that of HMA but much smaller 
than that of others. The required memory of EHMA 
grows moderately with |P|. The memory of 
EHMA(W = 5) is greater than that of EHMA due to the 
larger |F|. As shown in Figure 14, EHMA is highly 
effective in reducing the required external memory, 
providing efficient performance even in the virus-
detection-like model.  
Figure 15 uses Model III as real-life normal traffic to 
show the performance of the algorithms. Meanwhile, to 
demonstrate the effect of the rising number of patterns on 
the matching performance, a more recent Snort ruleset R2 
of about 5000 patterns are used in Model III. Figure 15 
shows that EHMA performs better than others even when 
the patter set is very large. The matching time of EHMA 
only moderately increases with the rising |P|. 
5. CONCLUSIONS 
The increasing variety of network applications and 
stakes held by various users are creating a strong demand 
for fast in-depth packet inspection. The most important 
component of in-depth packet inspection is an efficient 
multi-pattern matching algorithm. This study proposes a 
novel Enhanced Hierarchical Multi-pattern Matching 
Algorithm (EHMA) for packet inspection. EHMA applies 
the frequent-common grams obtained by the proposed 
GFGS to narrowing the searching scope and to quickly 
filtering out the innocent packets. The matching process 
then focuses only on the most suspected packets. EHMA 
concentrates the patterns into a small on-chip table, and 
performs simple and fast checks. Additionally, EHMA 
uses the frequency-based bad gram heuristic to speed up 
the scanning process. The hierarchical matching 
significantly reduces the average number of external 
memory accesses to only 6%–19%, thus improving the 
matching performance. The required memory of EHMA 
is only about 40KB in additional to the pattern contents of 
Snort rules. Particularly, EHMA is very simple and can be 
easily implemented in both software-based and 
hardware-based platforms. This study also discusses and 
evaluates current multi-pattern matching algorithms for 
NIDSs. Simulation results show that EHMA performs 
about 0.89–1161 times better than others. Even under real-
life intense attack, EHMA significantly outperforms 
others. EHMA also works well for the systems with larger 
minimum pattern size, such as virus detection systems. In 
conclusion, EHMA facilitates the creation of efficient and 
cost-effective pattern detection engines for packet 
inspection. 
ACKNOWLEDGMENT 
This work was supported in part by MediaTek and 
National Science Council of the Republic of China, 
Taiwan, under Grants NSC -94-2213-E007-02 and NSC 95-
TABLE VI. THE IMPACT OF THE SIZE OF SAMPLING WINDOW (W) 
IN THE SHIFT VALUES OF TABLES, |F|, ACTUAL MATCHING SHIFTS 
AND E, USING MODEL II. 
|P| 200 500 
 
EHMA EHMA (W=7) 
EHMA 
(W=5) 
EHMA 
(W=3) EHMA 
EHMA 
(W=7) 
EHMA 
(W=5) 
EHMA 
(W=3) 
H1.shift 0.94 2.71 3.66 4.74 0.91 1.86 2.02 2.49 
H2.shift 1.99 4.89 6.79 8.71 1.99 4.84 6.72 8.65 
|F| 13 20 25 39 23 33 47 65 
Average Shift 1.5 1.74 1.79 1.84 1.49 1.68 1.74 1.8 
E 0.0377 0.0441 0.0431 0.0434 0.1243 0.16 0.1635 0.2512 
0
500
1000
1500
2000
2500
3000
3500
4000
EH
M
A
 (W
=5)
EH
M
A
H
M
A
W
M
 (B
=2)
W
M
 (B
=3)
W
M
-PH
 (B
=2)
W
M
-PH
 (B
=3)
A
C
-C
B
M
H
B
M
H
-O
Cy
cle
 
Ti
m
e
200 patterns 500 patterns
0
20
40
60
80
100
 
(a) Average matching time. 
1
10
100
1000
10000
100000
EH
M
A
 (W
=5)
EH
M
A
H
M
A
W
M
 (B
=2)
W
M
 (B
=3)
W
M
-PH
 (B
=2)
W
M
-PH
 (B
=3)
A
C
-C
B
M
H
B
M
H
-O
M
em
o
ry
 
Si
z
e 
(K
B
)
200 patterns 500 patterns
 
(b) Memory Requirement. 
Figure 14. The costs versus the number of patterns (|P|), using 
Model II, wE = 100 and M = 10: (a) Average matching time, (b) Extra 
memory requirement. 
1
10
100
1000
10000
100000
EHMA HMA WM-PH AC-C BMH BMH-O
Cy
cl
e 
Ti
m
e
200 2500 5000
 
Figure 15. The average matching time ( Ψ ) versus the number of 
patterns (|P|), using Model III, wE = 100. 
