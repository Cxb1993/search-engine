 行政院國家科學委員會補助專題研究計畫 ■成果報告   □期中進度報告 
 
（計畫名稱） 
 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 99－2221－E－001－010 
執行期間： 99 年 08 月 01 日至 100 年 07 月 31 日 
 
執行機構及系所：中央研究院資訊科學研究所 
 
計畫主持人：葉彌妍 
共同主持人： 
計畫參與人員： 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
 
中   華   民   國    年    月    日 
 
附件一 
Random Error Reduction in Similarity Search on
Time Series: A Statistical Approach
Wush Chi-Hsuan Wu #1, Mi-Yen Yeh #2, Jian Pei ∗3
#Institute of Information Science, Academia Sinica, Taipei, Taiwan
∗School of Computing Science, Simon Fraser University, Burnaby, BC, Canada
1wush978@gmail.com, 2miyen@iis.sinica.edu.tw, 3jpei@cs.sfu.ca
Abstract—
Errors in measurement can be categorized into two types: sys-
tematic errors that are predictable, and random errors that are
inherently unpredictable and have null expected value. Random
error is always present in a measurement. More often than not,
readings in time series may contain inherent random errors due
to causes like dynamic error, drift, noise, hysteresis, digitalization
error and limited sampling frequency. Random errors may affect
the quality of time series analysis substantially. Unfortunately,
most of the existing time series mining and analysis methods,
such as similarity search, clustering, and classiﬁcation tasks, do
not address random errors, possibly because random error in
a time series, which can be modeled as a random variable of
unknown distribution, is hard to handle. In this paper, we tackle
this challenging problem. Taking similarity search as an example,
which is an essential task in time series analysis, we develop
MISQ, a statistical approach for random error reduction in time
series analysis. The major intuition in our method is to use only
the readings at different time instants in a time series to reduce
random errors. We achieve a highly desirable property in MISQ:
it can ensure that the recall is above a user-speciﬁed threshold.
An extensive empirical study on 20 benchmark real data sets
clearly shows that our method can lead to better performance
than the baseline method without random error reduction in
real applications such as classiﬁcation. Moreover, MISQ achieves
good quality in similarity search.
I. INTRODUCTION
Time series analysis is widely used in many applications.
In general, a time series is a series of readings recorded at
a sequence of time instants. The quality of time series data
depends on the quality of the readings.
Due to limitation of data collection equipment and meth-
ods, readings in time series data are often prone to various
errors. In general, errors can be categorized into two types:
systematic errors that are predictable, and random errors that
are inherently unpredictable and have null expected value [1].
Often, systematic errors can be removed by calibration of
the measurement equipment. However, random error is always
present in a reading.
Since random errors have the property that the mean of
many separate measurements approaches 0, random errors can
be reduced by obtaining multiple independent measurements
and using the mean of them. In practice, it is however often
infeasible to obtain multiple independent measurements on one
target attribute at an instant. Consequently, we still need to
develop effective methods to reduce random errors in the data
processing and analysis phase.
● Su(t)  , the observation at time t
μu(t)  , the mean of  S~u(t)
probability density function (pdf) of   S~u   at time t
t
●
●
●
●
●
●
time
va
lu
e
Su(t)
μu(t)
Fig. 1: Modeling time series with random errors.
Most of the existing time series mining and analysis meth-
ods, such as similarity search, clustering, and classiﬁcation
tasks, do not address random errors, possibly because reducing
random errors in the data analysis phase is far from trivial.
Conceptually, random errors in a time series can be modeled
as a random variable of unknown distribution. It is very
challenging to systematically remove random errors.
In this paper, we tackle the problem of reducing random
errors in time series analysis. We take the similarity search on
time series as a concrete example, since it is essential for time
series analysis. To take random errors into account, we model a
time series S˜u, as illustrated in Fig. 1, as an ordered sequence
of continuous random variables. At timestamp t, the obser-
vation Su(t) recorded is a sample from an unknown random
variable S˜u(t) with an unknown probability density function
(pdf). The distance between two time series is consequently a
random variable with unknown distribution as well.
Technically, a time series can be further regarded as a series
of expected values (mean values), each being blurred with
a random variable of mean 0 and unknown variance. The
problem of measuring the similarity between two time series
becomes approaching the distance between the two time series
of mean values, as illustrated in Fig. 2. To make our discussion
concrete, we use Euclidean distance in this paper. In general,
any distance measure may be used, though some technical
details may need to be adjusted accordingly.
In this paper, we develop MISQ (for mean distance queries),
a similarity search method for time series. The major intuition
behind the feasibility of random error reduction is that, by
assuming that the readings in a time series are collected
through the same equipment under the proper working con-
Apparently, D(S˜Q, S˜u) is a random variable. Instead of
directly modeling the distribution of D(S˜Q, S˜u), we consider
the mean distance, which excludes the effect of random errors.
Deﬁnition 3 (Mean distance): The mean distance between
two uncertain time series S˜Q and S˜u of length l is.
MD(S˜Q, S˜u) = D(μQ, μu) =
l∑
t=1
(μQ(t)− μu(t))2. (3)
In this paper, we consider two types of query on time series.
Deﬁnition 4 (Queries): Given a reference time series S˜Q
and a set of time series T , an exact match query (also called
exact query) retrieves those time series S˜u ∈ T such that
D(μQ, μu) = 0; a threshold similarity query (also called
threshold query) retrieves those time series S˜u ∈ T such
that D(μQ, μu) ≤ r, where r > 0 is a user speciﬁed distance
threshold. For the sake of simplicity, we omit T hereafter if
it is clear from context.
Apparently, when the distance threshold r is set to 0, a
threshold similarity query becomes an exact match query. As
aforementioned, μQ and μu are unobserved values in practice.
Thus, we cannot compute D(μQ, μu) directly. Instead, we
can use only the observation values to estimate it, of which
the method will be introduced in later sections, and apply
statistical hypothesis testings to determine if a candidate time
series qualiﬁes a query.
We design the hypothesis testing procedure for exact queries
and threshold queries as follows.
Deﬁnition 5 (Null hypotheses): An exact query retrieves
those S˜u ∈ T that do not reject the null hypothesis H0 :
D(μQ, μu) ≤ 0. Here, we use ≤ instead of = is for the
convenience of the testing. Since distance is always non-
negative, there is no difference between using = and ≤.
A threshold query retrieves those S˜u that do not reject the
null hypothesis H0 : D(μQ, μu) ≤ r.
We consider two types of error.
Deﬁnition 6 (Two types of error): A type I error happens
if a statistical test rejects a true null hypothesis (H0). In our
case, a type I error happens if an exact match query fails
to retrieve a time series exactly matching the reference time
series, or a threshold similarity query fails to retrieve a time
series whose distance to the reference time series is less than
or equal to the threshold. In other words, a low type I error
rate implies a high recall.
A type II error happens when a test fails to reject a false
null hypothesis (H0). In our case, a type II error happens if
an exact match query retrieves a time series that is in fact
not exactly matching the reference time series, or a threshold
similarity query retrieves a time series whose distance to the
reference time series is in fact larger than the threshold. A low
type II error rate implies a high precision.
One major challenge is how we can estimate the mean
distance with only one observation at each timestamp for each
time series. One may think about using some common de-
noising methods, such as moving average, to guess the mean
values at different instants ﬁrst. However, those approaches
are usually heavily parameter-dependent. That is, one needs
to decide how many observations should be considered simul-
taneously to get the mean value. Such parameters are hard to
decide in practice. Different from those methods, in this paper
we develop a parameter-free, difference-based estimator for
mean distance using only observations in time series.
Another challenge is how we can ensure that the estimated
mean distance is controlled at a given conﬁdence level. In this
regard, we devise another estimator to compute the variance of
the mean distance estimator, which is to decide if the estimated
mean distance is signiﬁcantly above a given threshold r or not.
With the variance estimator and the user given conﬁdence
threshold 1 − α, we can compute the lower bound of the
conﬁdence level, LCI , of each estimated mean distance.
Beneﬁtting from the LCI , we can control the type I error
rates. Speciﬁcally, we only report the time series whose LCI
of the mean distance to the query time series is not greater
than r.
For both exact queries and threshold queries, we can
interpret the hypothesis testings using a conﬁdence interval
determined by a user given conﬁdence level of the estimated
D(μu, μQ). Since the testing is one-tailed, we can com-
pare the lower bound of the conﬁdence interval, denoted by
LCI(D(μQ, μu)), with the given distance threshold (0 for an
exact query and r for a threshold query). We will show that,
if the conﬁdence level is set to 1 − α, where α ∈ [0, 1], the
type I error rate most of the time is not greater than α.
Based on the above discussion, let us formally restate the
two types of query using hypothesis testing with type I error
rate controlled according to α as follows.
Deﬁnition 7 (Queries, using hypothesis testings): Given a
reference time series S˜Q, a set of time series T , a user
speciﬁed conﬁdence level 1 − α ∈ [0, 1], an exact match
query retrieves all time series S˜u ∈ T such that
LCI(D(μu, μQ)) ≤ 0. (4)
Moreover, a threshold similarity query retrieves all time
series S˜u ∈ T such that
LCI(D(μu, μQ)) ≤ r, (5)
where r > 0 is a given distance threshold.
B. Related Work
Similarity search in time series databases, as an important
function in many applications, has drawn wide attention in
the recent decades. Many studies investigate how to search
efﬁciently and accurately under the widely-used Euclidean
distance [3–5] and many other similarity measurements, such
as [5–9]. Those methods do not consider errors incurred in the
reading of each time instant yet.
Some recent studies tackle random errors in time series by
modeling time series as uncertain data. Speciﬁcally, in [10–
12], a type of probabilistic query is investigated that ﬁnds the
uncertain time series whose distances to a reference one are
not greater than a given distance threshold with a high enough
probability. Different from our study, those methods all assume
that both the mean and the variance of each uncertain variable
trials should be close to the expected value, V 2u and V
2
Q can
then be estimated by
Vˆ 2u =
1
2l
l∑
t=1
(Su(t)− Su(t− 1))2,
Vˆ 2Q =
1
2l
l∑
t=1
(SQ(t)− SQ(t− 1))2. (9)
Based on Eq. (6) and Eq. (9), we can estimate the mean
distance D(μu, μQ) by the following estimator.
Dˆ (μu, μQ)
=
l∑
t=1
(Su(t)− SQ(t))2 − 1
2
l∑
t=1
(Su(t)− Su(t− 1))2
−1
2
l∑
t=1
(SQ(t)− SQ(t− 1))2. (10)
From Eq. (10), we can see the main advantage of the mean
distance estimator Dˆ (μu, μQ): it is parameter-free and simple,
and needs only the observation values. The reliability of this
estimator depends on Eq. (7) and the i.i.d. assumption of ε˜u(t)
and ε˜Q(t). Next we show how to measure the reliability by
computing the variance of the mean distance estimator.
B. Variance and Asymptotic Distribution of the Mean Distance
Estimator
In order to understand the reliability of the mean distance
estimator Dˆ (μu, μQ), we compute its variance.
According to Eq. (10), the estimator is deﬁned as a function
of Su(t) and SQ(t), the observation values of S˜u(t) and S˜Q(t),
respectively. To evaluate its variance, we replace the obser-
vations Su and SQ by their corresponding random variables
S˜u and S˜Q, respectively. As a result, we have a variable
˜ˆ
D (μu, μQ) that models all possible values of Dˆ (μu, μQ).
Theorem 1: The variance of ˜ˆD (μu, μQ) can be evaluated
using the following estimator.
ˆV ar
(
˜ˆ
D (μu, μQ)
)
= 4
(
Vˆ 2u + Vˆ
2
Q
)
Dˆ (μu, μQ)
+l ·
(
Vˆ 4u + 4Vˆ
2
u Vˆ
2
Q + Vˆ
4
Q
)
, (11)
where Vˆ 2u and Vˆ
2
Q are deﬁned in Eq. (9), and Dˆ(μu, μQ) is
deﬁned in Eq. (10).
Proof: According to Eq. (10), we rewrite the estimator
variable in a vector form as follows.
˜ˆ
D (μu, μQ)
=
(
1 −1 −1 )
⎛
⎜⎜⎜⎜⎜⎜⎝
te∑
t=ts
(S˜u(t)− S˜Q(t))2
1
2
te∑
t=ts
(S˜u(t)− S˜u(t− 1))2
1
2
te∑
t=ts
(S˜Q(t)− S˜Q(t− 1))2
⎞
⎟⎟⎟⎟⎟⎟⎠
=
(
1 −1 −1 )X. (12)
Using Eq. (12) and following the basic property of covariance
matrix, the variance of ˜ˆD (μu, μQ) can be written as
V ar
(
˜ˆ
D (μu, μQ)
)
=
(
1 −1 −1 )Cov(X)
⎛
⎝ 1−1
−1
⎞
⎠ .
(13)
Based on Eq. (7) and the assumption that ε˜u(t) and ε˜Q(t))
are i.i.d., we can derive the covariance matrix Cov(X) =
E
(
(X− EX)(X− EX)T ). As a result, Eq. (13) is
V ar
(
˜ˆ
D (μu, μQ)
)
= 4(V 2u + V
2
Q)D(μu, μQ)
+l · (V 4u + 4V 2u V 2Q + V 4Q) .(14)
By substituting Vu, VQ and D(μu, μQ) with their estimators
in Eqs. (9) and (10), respectively, we prove the theorem.
Apparently, the variance is easy to compute according to
Theorem 1.
To control the type I error rate, we need to investigate the
asymptotic distribution of ˜ˆD(μQ, μu) as well.
Theorem 2: Suppose 1) μQ(t) and μu(t) are uniformly
bounded, 2) ε˜Q(t) and ε˜u(t) are i.i.d. and uniformly bounded
random variables, and 3) μQ(t) ≈ μQ(t − 1) and μu(t) ≈
μu(t− 1). Let l be the length of the time series. Then,
lim
l→∞
˜ˆ
D(μQ, μu)−D(μQ, μu)√
l
= N(0,
V ar
(
˜ˆ
D(μQ, μu)
)
l
),
(15)
where N(·) is the normal distribution.
Proof: According to the assumption deﬁned in Section II,
Eq. (7), and Eq. (10), Dˆ(μQ, μu) can be written as
Dˆ (μu, μQ)
=
l∑
t=1
(
S˜u(t)− S˜Q(t)
)2
− 1
2
l∑
t=1
(
S˜u(t)− S˜u(t− 1)
)2
−1
2
l∑
t=1
(
S˜Q(t)− S˜Q(t− 1)
)2
≈
l∑
t=1
(μQ(t)− μu(t))2
+2
l∑
t=1
(μQ(t)− μu(t)) (ε˜Q(t)− ε˜u(t))
+
l∑
t=1
(ε˜Q(t)− ε˜u(t))2 − 1
2
l∑
t=1
(ε˜Q(t)− ε˜Q(t− 1))2
−1
2
l∑
t=1
(ε˜u(t)− ε˜u(t− 1))2
≈ D(μQ, μu) +
l∑
t=1
[2 (μQ(t)− μu(t)) (ε˜Q(t)− ε˜u(t))
−2ε˜Q(t)ε˜u(t)]
+
l∑
t=1
[ε˜Q(t)ε˜Q(t− 1) + ε˜u(t)ε˜u(t− 1)]. (16)
be modeled as a normal distribution, of the mean distance
estimator under different types of uncertainty errors. The
information is the key concept used to control the type I error
rate of the query, which we will show in the next section.
IV. QUERY PROCESSING
In many existing methods for distance-based queries on
certain time series, ﬁnding a lower bound on the distance
between two time series plays an important role, since the
lower bound can help to guarantee a zero type I error rate,
that is, not missing any qualiﬁed time series in the answer set.
For queries on uncertain time series, missing some qualiﬁed
time series is inevitable due to the unknown uncertainty. To
solve the problem, we propose to control the type I error rate
to a given upper bound. Theorem 2 shows that the difference
between the estimated distance and the true distance tends to
be a normal distribution, which can be leveraged to ﬁnd the
conﬁdence interval of the estimated mean distance and the
lower bound of the conﬁdence interval.
A. Exact Match Queries
As stated in Deﬁnition 7, we need to ﬁnd the lower bound,
LCI(D(μu, μQ)), of the conﬁdence interval of D(μu, μQ)
given a conﬁdence level of 1 − α. According to Theorem 2,
we can compute the conﬁdence interval with a 1−α conﬁdence
level approximately as follows.
Suppose the time series length l is large enough. We have
Pr
⎛
⎜⎜⎝Dˆ(μQ, μu)−D(μQ, μu)√
V ar
(
Dˆ(μQ, μu)
) ≥ Φ−1 (1− α)
⎞
⎟⎟⎠ ≈ α, (21)
where Φ is the cumulative distribution function of the standard
normal distribution. After transposing the equation, we have
Pr(X) ≈ α, (22)
where X = D(μQ, μu) ≤ Dˆ(μQ, μu)
− Φ−1 (1− α)
√
V ar
(
Dˆ(μQ, μu)
)
In practice, we do not know the variance
V ar
(
Dˆ (μu, μQ)
)
. Thus, we replace it by its estimators in
Eq. (11). That is, LCI(D(μQ, μu)) corresponding to the
1− α conﬁdence level is deﬁned as
LCI(D(μQ, μu))
= Dˆ(μQ, μu)− Φ−1 (1− α)
√
ˆV ar
(
Dˆ(μQ, μu)
)
.(23)
With Eq. (23), we process the exact match query by
retrieving all uncertain time series S˜u that satisfy the inequality
Eq. (4).
In the following theorem, we prove that the type I error rate
of the exact match is controlled to α.
Theorem 3: When the length of the time series is large
enough, the type I error rate of the exact match query is up
to α if we retrieve all uncertain time series S˜u that satisfy
LCI(D(μQ, μu)) ≤ 0.
Proof: According to Deﬁnition 4, an exact match query
wants S˜u having D(μQ, μu) = 0. As a result, the type I error
rate is
Pr (LCI(D(μQ, μu)) > 0)
≈ Pr
(
Dˆ(μQ, μu)− Φ−1 (1− α)
√
V ar
(
Dˆ(μQ, μu)
)
> 0
)
= Pr
⎛
⎜⎜⎝ Dˆ(μQ, μu)√
V ar
(
Dˆ(μQ, μu)
) > Φ−1 (1− α)
⎞
⎟⎟⎠
≈ α. (24)
Therefore, we can approximately control the type I error rate
of the exact match query.
B. Threshold Similarity Query
Similarly, we use the same lower bound deﬁned in Eq. (23)
to process the threshold queries deﬁned in Deﬁnition 7. We
prove its type I error rate control as well.
Theorem 4: When the length of the reference time series is
large enough, the type I error rate of the threshold query is
no more than α if we retrieve all time series S˜u that satisfy
LCI(D(μQ, μu)) ≤ r, where r is a user speciﬁed distance
threshold.
Proof: Under condition D(μQ, μu) ≤ r, the type I error
rate is
Pr (LCI(D(μQ, μu)) > r)
≈ Pr
⎛
⎜⎜⎝ Dˆ(μQ, μu)− r√
V ar
(
Dˆ(μQ, μu)
) > Φ−1 (1− α)
⎞
⎟⎟⎠
≤ Pr
⎛
⎜⎜⎝Dˆ(μQ, μu)−D(μQ, μu)√
V ar
(
Dˆ(μQ, μu)
) > Φ−1 (1− α)
⎞
⎟⎟⎠
≈ α.
(25)
Therefore, the type I error rate of the threshold query is also
under controlled.
V. EXPERIMENT RESULTS
We conducted extensive experiments on 20 real data sets
in the UCR time series data repository [2] to evaluate MISQ.
All experiments were run on a PC with an Intel Core i7 3.07
GHz CPU and 12GB RAM using R 2.12.1 [18].
A. Settings
We compared MISQ with the conﬁdence band method on
exact queries, and with the moving average method on both
exact and threshold queries.
The idea of the conﬁdence band method works as follows.
Given two time series with their observations SQ and Su,
we tested if μQ − μu is equal to 0 or not. That is, the null
hypothesis was set to μQ−μu = 0. We applied the testing for
no effect in nonparametric regression via kernel smoothing
one as the query series S˜Q and the rest were the candidates.
Under the settings, any tested methods should retrieve all the
100 candidates.
For each data set, we picked 5 time series and repeated
the procedure above. As there were 20 data sets, we had 100
queries in total. If a query was processed with the type I error
rate no more than the given α, we call it is under-control.
We computed the under control rate of type I errors, and the
results are shown in Fig. 6. MISQ controlled the type I error
rate better than the conﬁdence band method. Movavg 5 and
movagv cv hardly controlled the type I error rates.
For the type II errors, the settings were the same except for
one difference: we let μu(t) = μQ(t) + 0.5 ×
√
V ar(μQ).
Under this setting, the mean of the reference time series was
different from the mean of all the candidates series, so the
tested methods shall retrieve no candidate. In this way, every
returned results from a method was regarded as a type II error.
From Fig. 7, we see that MISQ outperformed the conﬁdence
band method. As the uncertainty ratio increases, the type II
error rate of MISQ increases as well. The type II error rates
of movavg 5 and movagv cv were very small.
To understand the results, we note that MISQ directly
tested if D(μQ, μu) = 0 while the conﬁdence band method
estimated the curve of μu − μQ instead. Since the procedure
of the conﬁdence band method was indirect, its performance
was affected by parameters such as the bandwidth selector.
Consequently, it had poor type I and type II error rates.
Interestingly, we see the trade-offs between type I error rate
and type II error rate of MISQ and the moving average method.
The very low type II error rate of the moving average methods
was attributed to the poor performance in type I error rate
control, which failed to meet our goal. MISQ sacriﬁced the
type II error rate to control the type I error rate, since a low
type I error rate is more important in many applications.
C. Threshold Similarity Queries
For threshold queries, we only compared MISQ with
movavg 5 and movagv cv since the conﬁdence band method
is not able to be applied in the case when r 	= 0.
We introduced the uncertainty to the time series data as
follows. For each time series at each timestamp, it was blurred
by an i.i.d. random noise with a variance proportional to the
variance of the original time series, which is the same as in the
exact query experiments. To show that the proposed method
is not limited by a speciﬁc distribution of the noise, we tested
the normal distributed and uniformly distributed noise.
The parameters used were set as follows. For each data set,
we divided it into the training and the test sets as originally
deﬁned in the UCR time series data. The reference time series,
i.e., the query series S˜Q, was chosen from the testing set
and the candidates were all members in the training dataset.
The distance bound r was chosen from the 0.1, 0.2, ..., 0.9-
quantiles of the Euclidean distances between the reference
time series and the candidates. For under control rate, the
conﬁdence level, i.e., 1− α, was set to 0.95.
First, we compared the under-control rate of type I errors
of the three methods on 20 data sets. For a query time series
from the testing set, if the type I error rate was no more than
the given α, we counted it as an under-control. The under-
control rate is thus the percentage of queries from the testing
set that was under-control. Second, the type II error rates were
averaged from the query results of all the query time series in
the testing data set. Finally, since there is a trade-off between
the type I error rate and the type II error rate, we additionally
interpolated for each method the Equal Error Rate (EER), the
error rate where type I error rate equals to the type II error
rate, of each method for comparison.
The under-control rate of type I errors and the corresponding
type II error rates for both uniform and normal errors under
two uncertain ratio values, 0.2 and 2, of each data set are
shown in Fig. 8, Fig. 9, Fig. 10 and Fig. 11, respectively.
From these ﬁgures, we can see that MISQ outperformed the
other methods in controlling the type I error rate signiﬁcantly.
This supported the theoretical results derived in Section III
and Section IV. For the moving average method, sometimes
movavg cv worked better while the other times movavg 5 did.
This showed that the quality of moving average depended on
the bandwidth selection heavily. Moreover, as the uncertainty
ratio was large, e.g., when uncertainty ratio=2.0, the under-
control rate of type I errors of both two moving average
methods decreased signiﬁcantly while MISQ still guaranteed a
100% control. On the other hand, MISQ sacriﬁced more type
II error rates only when the uncertain ratio was large, such as
2. Note that the uncertainty ratio = 2 was quite large since
it means that the variation of noise is twice that of the mean
values of a time series. At a small uncertain ratio, the type II
error rate of MISQ was even smaller than that of the moving
average method in some data sets.
In addition, MISQ achieved good under control rates
(100%) all the time in our experiments, no matter which noise
distribution was used. However, the moving average method
had high under-control rates of type I error for normally
distributed noise but worked poorly for the uniform noise (See
data sets Two Pattern and wafer in Fig. 9(a) and Fig. 11(b).)
As there was a trade-off between the type I error rate and
type II error rate, we further calculated the EER values of
all methods. At different uncertainty ratios, we counted the
best (smallest) EER among the 20 data sets of each method
and plotted the results in Fig. 12. We can see that MISQ
performed best even when the uncertainty ratio was getting
larger. The movavg 5 worked better than movavg cv when
the uncertainty ratio became larger, but still it did not surpass
MISQ. Therefore, we can conclude that the MISQ is the best
method out of the three on the threshold query with type I
error rate controlled.
D. Computation Time
Here we compared the computation cost of each method to
show their efﬁciency. We show the computation cost on each
data set of all methods for threshold queries. For each dataset,
we computed the averaged processing time for each method
to get the answers of a query. Using the longest processing
time as a denominator, we computed the percentage of the
processing time of the other two methods. The results were
50
w
or
ds
A
di
ac
B
ee
f
C
B
F
C
of
fe
e
E
C
G
20
0
Fa
ce
A
ll
Fa
ce
Fo
ur
fis
h
G
un
_P
oi
nt
Li
gh
tin
g2
Li
gh
tin
g7
O
liv
eO
il
O
S
U
Le
af
S
w
ed
is
hL
ea
f
sy
nt
he
tic
_c
on
tro
l
Tr
ac
e
Tw
o_
Pa
tte
rn
s
w
af
er
yo
ga
movavg_5
movavg_cv
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
0%
25
%
50
%
75
%
10
0%
Data Set
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
U
nd
er
-c
on
tro
l r
at
e 
of
 ty
pe
 I 
er
ro
rs
MISQ movavg_5 movavg_cv
(a) The percentage of type I error under control on different datasets
50
w
or
ds
A
di
ac
B
ee
f
C
B
F
C
of
fe
e
E
C
G
20
0
Fa
ce
A
ll
Fa
ce
Fo
ur
fis
h
G
un
_P
oi
nt
Li
gh
tin
g2
Li
gh
tin
g7
O
liv
eO
il
O
S
U
Le
af
S
w
ed
is
hL
ea
f
sy
nt
he
tic
_c
on
tro
l
Tr
ac
e
Tw
o_
Pa
tte
rn
s
w
af
er
yo
ga
movavg_5
movavg_cv
ty
pe
 II
 e
rr
or
0%
25
%
50
%
75
%
10
0%
Data Set
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
Ty
pe
 II
 e
rr
or
 ra
te
s
MISQ movavg_5 movavg_cv
(b) The type II error rate on different datasets
Fig. 10: Uniformly distributed noise with uncertainty ratio=0.2.
50
w
or
ds
A
di
ac
B
ee
f
C
B
F
C
of
fe
e
E
C
G
20
0
Fa
ce
A
ll
Fa
ce
Fo
ur
fis
h
G
un
_P
oi
nt
Li
gh
tin
g2
Li
gh
tin
g7
O
liv
eO
il
O
S
U
Le
af
S
w
ed
is
hL
ea
f
sy
nt
he
tic
_c
on
tro
l
Tr
ac
e
Tw
o_
Pa
tte
rn
s
w
af
er
yo
ga
movavg_5
movavg_cv
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
0%
25
%
50
%
75
%
10
0%
Data Set
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
U
nd
er
-c
on
tro
l r
at
e 
of
 ty
pe
 I 
er
ro
rs
MISQ movavg_5 movavg_cv
(a) The percentage of type I error under control on different datasets
50
w
or
ds
A
di
ac
B
ee
f
C
B
F
C
of
fe
e
E
C
G
20
0
Fa
ce
A
ll
Fa
ce
Fo
ur
fis
h
G
un
_P
oi
nt
Li
gh
tin
g2
Li
gh
tin
g7
O
liv
eO
il
O
S
U
Le
af
S
w
ed
is
hL
ea
f
sy
nt
he
tic
_c
on
tro
l
Tr
ac
e
Tw
o_
Pa
tte
rn
s
w
af
er
yo
ga
movavg_5
movavg_cv
ty
pe
 II
 e
rr
or
0%
25
%
50
%
75
%
10
0%
Data Set
Pe
rc
en
ta
ge
 o
f U
nd
er
 C
on
tro
l
Ty
pe
 II
 e
rr
or
 ra
te
s
MISQ movavg_5 movavg_cv
(b) The type II error rate on different datasets
Fig. 11: Uniformly distributed noise with uncertainty ratio=2.
a pairwise manner. The k nearest neighbors are then those
having less-than-k time series that are signiﬁcantly closer to
the query time series after some multiple testing correction.
The main challenges are to reduce the signiﬁcantly high type
II error rates and the retrieving efﬁciency. Another direction is
to apply the same ideas in MISQ to compute the dynamic time
warping distance on time series, which is another important
and widely used distance measurement in many applications,
such as speech recognition.
ACKNOWLEDGMENT
The work was supported in part by the National Science
Council of Taiwan, R.O.C., under Contracts NSC99-2221-
E-001-010, and an NSERC Discovery Grant project. All
opinions, ﬁndings, conclusions and recommendations in this
paper are those of the authors and do not necessarily reﬂect
the views of the funding agencies.
REFERENCES
[1] “Random error - from wikipedia, the free
encyclopedia.” [Online]. Available: http://en.wikipedia.
org/wiki/Random error
[2] E. Keogh, X. Xi, L. Wei, and C. A. Ratanamahatana,
“The ucr time series classiﬁcation/clustering,” 2006.
[Online]. Available: http://www.cs.ucr.edu/∼eamonn/
time series data/
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：100 年 10 月 27 日 
一、參加會議經過 
 此次會議的第一天是由許多與主會議合辦之相關 Workshop 先行開場暖身，包含有
Ph.D. Symposium 、behavioral informatics 等與 data mining 的相關研究主題，
接下來的三天才開始進行主要會議的議程。 
 次日一大早，本屆 PAKDD 2011 會議正式開始，第一場 Keynote Speech 特別邀
請 Cloudera 的科技執行長 Dr. Amr Awadallah 以業界觀點介紹雲端環境中 Hadoop
計畫編號 NSC 99－2221－E－001－010 
計畫名稱 具不確定性時間序列與資料流之查詢處理 
出國人員
姓名 李兆殷、葉彌妍 
服務機構
及職稱 
中央研究院資訊所  
研究助理/助研究員 
會議時間 
100 年 5 月 24 日
至 
100 年 5 月 27 日 
會議地點 中國深圳 
會議名稱 
(中文)第十五屆亞太知識探索與資料探勘會議 
(英文) PAKDD2011: The 15th Pacific-Asia Conference on Knowledge 
Discovery and Data Mining 
發表論文
題目 
(中文)應用於異質性社群網絡上之抽樣演算法分析 
(英文) On Sampling Type Distribution from Heterogeneous Social 
Networks 
 3
 另外，本次會議舉辦地點為中國，因此與會人員中有很多來自大陸各地，如香港
科技大學、南京大學、中國科學院等學者。他們也帶來了許多有趣的研究以及對於
潛在或未來議題的獨到看法，令人印象深刻。 
 在本次會議中最大的收獲便是和眾多位學者交流。除了欣賞他們有趣的研究，更
學習如何在有限的時間內，清楚地表達並呈現自己的研究內容，如 Dr. Richi Nayak
的研究不但議題新穎，同時其口語表達與回應問題都相當清處易懂。此外，在此次
會議中，有幸與專注於機器學習方面研究的中村篤祥教授對話，與其互動的過程中
發現他在涉及統計方法和觀念時用字遣詞相當精確，對於數字資料也有很高的敏感
度，讓我獲益良多。 
三、建議 
感謝國科會准予補助，使本人對於參加此次會議能順利成行。希望爾後國科會能繼
續多鼓勵國內研究學者參加重要的國際會議，除了達到學術交流的目的，更能增加
國內學者的能見度與視野。 
四、攜回資料名稱及內容 
 大會論文集兩本(Lecture Note in Artificial Intelligence) 
 大會議程表含演講摘要乙本 
五、其他 
 另附上此次參與會議之官方網址: http://pakdd2011.pakdd.org/ 
This e mail address is being protected from spambots. You need
JavaScript enabled to view it , phone +49 6221 487 8706.
LNCS/LNAI/LNBI is published, in parallel to the printed books, in full-text
electronic form via our internet platform www.springerlink.com. We therefore
need all the electronic files, most importantly the source files, of all
parts of the manuscript (including front matter pages) as advised in the
instructions. We enter into the source files to smooth out any formatting
or capitalization discrepancies and to insert running heads, page numbers
and a reference line at the bottom of the first page of every paper. We
also require a final pdf of each paper created and checked by the authors.
Once data processing has been finished, Springer shall contact the corresponding
authors and ask them to check their papers.
REGISTRATION
It is important to note that to include your paper in the program as well as
in the LNCS/LANI proceedings, at least one author of your paper must attend the
conference to present the paper and pay the registration fee. No show papers
may be removed from the Springer digital library, and shown in the PAKDD2011
website.
The conference program as well as the online registration form will be available
on the conference Website shortly.
PAKDD2011 CONFERENCE INFORMATION
Please refer to PAKDD2011 website for more information about the main conference:
http://datamining.it.uts.edu.au/pakdd2011.
We look forward to seeing you and your presentation in Shenzhen, China on May 24-27, 2011.
Regards,
Joshua Huang, Longbing Cao, Jaideep Srivastava
PAKDD-11 PC Co-chairs
Gmail - PAKDD2011 notification for paper 227
112 J.-Y. Li and M.-Y. Yeh
hand, sampling is a way of data collection by selecting a subset of the original data. By
following some rules of sampling nodes and edges, a subgraph can be constructed with
the characteristics of the original graph preserved. In contrast to graph summarization,
a big advantage of sampling is that only a controlled number of nodes, instead of the
entire network, are visited. In this work, as a result, we want to focus on sampling from
large social networks.
Prior studies on graph sampling [7,8], however, focused only on preserving statis-
tics such as degree distribution, hop-plot, and clustering coefficient on homogeneous
graphs, where each node and link is treated equally. In reality, the social network is
heterogeneous, where each individual has its own attribute indicating a specific group
membership or type. For example, people are of different races or nationalities. The
link between individuals of the same or different types can thus be classified to intra-
connection and inter-connection. The type distribution of nodes and the proportion of
intra/inter-connection links is also key information that should be preserved to under-
stand the heterogeneous social network, which, to the best of our knowledge, has not yet
been addressed in the previous graph sampling works in the data mining community.
To this end, we propose two goals on the heterogeneous social network. First is the
type distribution preserving goal. Given a desired number of nodes of the sample size,
a subgraph Gs is generated by some sampling method. The type distribution of Gs,
Dist(Gs), is expected to be the same as that of the original graph G. The second goal is
the intra-relationship preserving goal. We expect that the ratio of the intra-connection
numbers to the total edges of Gs should be preserved.
In search of a better solution, we adopt five possible methods: Random Node Sam-
pling (RNS), Random Edge Sampling (RES), EgoCenteric Exploration Sampling(ECE)
[9], Multiple-Ego-Centric Sampling, (MES) and Respondent-Driven Sampling (RDS)
[10], to see their effects on sampling type distribution in the heterogeneous social net-
works. RNS and RES are two methods of selecting nodes and edges randomly until some
criteria are met. ECE is a chain-referral-based sampling proposed in [9]. Chain-referral
sampling usually starts from a node called ego and selects neighbor nodes uniformly at
random wave by wave [9]. MES is a variation of ECE we designed that the sampling
starts with multiple initial egos. Finally, we adopt RDS, which is a sampling method
used in social science for studying the hidden populations [10]. Many works on the
social network analysis focus on the majority, i.e., the greatest or the second greatest
connected components, of the network. However, sometimes the small or hidden group
of a network hints more interesting messages. For example, the population of drug users
or patients with rare diseases is usually hidden and relatively small. Essentially, RDS
is a method combining snowball sampling, of which the recruiting of future samples is
from acquaintances of the current subject, with a Markov Chain model to generate un-
biased samples. In our implementation, we adopt RDS for the simulation of the human
recruiting process and indicate how the Markov Chain is computed from the collected
samples.
To evaluate the sampling quality of the above five methods, we conduct experi-
ments on the Twitter data sets provided in [11]. We measure the difference of the type
distribution between the sampling results and the original network by two indexes:
error ratio and D-statistic of Kolmogorov-Smirnov Test. In addition, we measure the
114 J.-Y. Li and M.-Y. Yeh
3 Problem Statement
Given a graph G =< V,E >, V denotes a set of n vertexes (nodes, individuals) vi and
E is a set of m directed edges (link, relationships) ei. First, we define the heterogeneous
graph which models the heterogeneous social network we are interested in.
Definition 1. A heterogeneous graph G with k types is a graph where each node be-
longs to only one specific type out of k types. More specifically, given a finite set
L = {L1, ...Lk} denoting k types, the type of each node vi is T (vi) = Li, where Li ∈ L.
Suppose the number of vertex of G is n, and the number of nodes belonging to type Li is
Ni, then the condition ∑ki=1 Ni = n must be true. In other words, (nodes ∈ Li) ∩ (nodes
∈ Lj) =0, where i = j.
The edges between nodes of different types are defined as follows.
Definition 2. An edge ei connecting two nodes vi and v j is an intra-connection edge if
T (vi) = T (v j). Otherwise, it is an inter-connection edge.
With the above two definitions, our problem statements are presented as follows.
Problem 1. Type distribution preserving goal Given a desired number of nodes, i.e.,
the sample size, a subgraph Gs is generated by some sampling method. The type distri-
bution of Gs, Dist(Gs), is expected to be the same as that of the original graph G. That
is, d(Dist(Gs),Dist(G)) = 0, where d() denotes the difference between two distribu-
tions. In other words, the percentage of each Ni in Gs is expected to be the same as that
of G.
Problem 2. Intra-relationship preserving goal Given a desired number of nodes, i.e.,
the sample size, a subgraph Gs is generated by some sampling method. The ratio of the
intra-connection numbers to the total edges should be preserved. That is,
d(IR(Gs), IR(G)) = 0.
On the other hand, the inter-relationship is equal to 1− IR(Gs) which is also preserved.
An example is given to illustrate these two problems. Given a social network which
including 180 nodes (n = 180) and 320 edges (m = 320). Suppose there are totally
3 groups (k = 3) containing 20, 100, and 60 people respectively. Thus, the type dis-
tribution of the network Dist(G) is 0.11,0.56,0.33. Also suppose there are 200 intra-
connection edges, the intra-connection ratio is thus 0.625. Our goal is to find out a
sampling method that preserves the type distribution and the intra-connection ratio best.
Suppose that a subgraph Gs is sampled under the given 10% sampling rate, which is 18
nodes. If the number of nodes of group 1, 2, and 3 is 5, 8, and 5, then the type distri-
bution is 0.28,0.44,0.28. In addition, suppose there are 30 intra-connection edges out of
50 sampled edges, then the intra-connection ratio is 0.6. In the experiment section, we
will provide several indexes to compute the difference between these distributions and
ratios.
116 J.-Y. Li and M.-Y. Yeh
Although the chain-referral sampling algorithms can both produce a reasonable con-
nected subgraph and preserve community structure, the rich get richer flavor inherently
exists in this family of sampling techniques.
4.3 Respondent-Driven Sampling
To study the hidden population in social science, Respondent-Driven Sampling (RDS)
[10], a non-probability method, has been proposed. Generally, RDS contains two phases
that including the snowball sampling phase and the Markov Chain process. Snowball
sampling works similarly to ECE/MES that the recruiting of future samples is from
acquaintances of the current subject. To compensate for collecting the data in a non-
random way, the second phase of RDS, the Markov Chain process, helps to generate
unbiased samples. As opposed to the conventional sampling methods, the statistics are
not obtained directly from the samples, but indirectly inferred from the social network
information constructed through them.
We simulate the snowball sampling phase of RDS as follows. First, the initial seeds,
or individuals, must be chosen from a limited number of convenience samples. We just
randomly select these initial seeds. Originally in RDS, each chosen seed is rewarded
to encourage further recruiting. Here, we simply make all recruited nodes continue to
recruit their peers. In addition, we set a coupon limit, which is the number of peers an
individual can recruit, to prevent the sampling in favor of individuals who have many
acquaintances.
Then, we simulate the Markov Chain process. Suppose there are total k types of
people in the network we study. From the collected samples we can organize a k by k
recruitment matrix M, where the element Si, j of M represents the percentage of the type
j people among those recruited by the people of type i. An example is illustrated in the
following sample matrix.
M =
⎛
⎜⎝
S11 . . . S1 j
.
.
.
.
.
.
.
.
.
Si1 · · · Si j
⎞
⎟⎠
Suppose that the recruiting should reach to an equilibrium state if more samples are
recruited than currently we have. That is, the type distribution will stabilize at E =
(E1, ...,Ei, ...,Ek), where Ei is the proportion of the type i at equilibrium. The law of
large number of the regular Markov Chain process provides a way of computing that
equilibrium state of M. It is computed by solving the following linear equations.
E1 + E2 + ...+ Ek = 1
S1,1E1 + S2,1E2 + ...Sk,1Ek = E1
S1,2E1 + S2,2E2 + ...Sk,2Ek = E2
.
.
.
S1,k−1E1 + S2,k−1E2 + ...Sk,k−11Ek = Ek−1.
For instance, if there are only two groups in a social network, Male (m) and Female ( f ),
the solution is Em =
Sm f
1−Smm+S f m and E f = 1−Em, thus provide the information about
118 J.-Y. Li and M.-Y. Yeh
Table 1. Summary of the Twitter data sets
group count characteristics group 1 group 2 group 3 group 4 group 5 group 6 group 7
7
group ratio 0.24 0.246 0.149 0.196 0.142 0.023 0.004
node count 97053 99177 60318 79290 57357 9206 1473
intra-connection ratio 0.324 0.332 0.335 0.265 0.258 0.209 0.02
intra-edge count 55943 53170 33132 38360 21558 2798 70
edge count 185242 160094 98819 144920 83531 13378 3530
5
group ratio 0.486 0.149 0.338 0.023 0.004 — —
node count 196230 60318 136647 9206 1473 — —
intra-connection ratio 0.574 0.335 0.381 0.209 0.02 — —
intra-edge count 198306 33132 86943 2798 70 — —
edge count 345336 98819 228451 13378 3530 — —
3
group ratio 0.509 0.153 0.381 — — — —
node count 205436 61791 136647 — — — —
intra-connection ratio 0.598 0.334 0.381 — — — —
intra-edge count 214351 34149 86943 — — — —
edge count 358714 102349 228451 — — — —
the original graph G. First, the Error Ratio (ER) summed up the proportion difference
of all types. It is defined as ∑
k
i=1 |O(i)−E(i)|
2∗SN , where O(i) is the number of nodes in the i
th
group on Gs, E(i) is the theoretical number of nodes it should be in the sampled graph
according to the type i’s real proportion in G, and SN is the sample size. Another evalu-
ation statistic is the D-statistic for the Kolmogorov-Smirnov Test. We simply used it as
an index rather than conducting a hypothesis test. The D-statistic, which can measure
the agreement between two distributions, is defined as D = supx|F ′(x)−F(x)|, where
F ′(x) is the type distribution of Gs and F(x) is that of G. ER provided a percentage-
like form of the total errors between type distributions of Gs and G whereas D-statistic
provided the information about the cumulative errors within the structure of Gs and G.
For the intra-relationship preserving goal, we used the Intra-Relation Error (IRE) to
measure the difference of the intra-relationship ratio among Gs and G. It was defined
| I′
m′ − Im |, where I′ and I denoted the number of intra-connection edges in Gs and G
respectively, and m′ and m were the total number of edges in Gs and G respectively.
5.3 Results of the Type Distribution Preserving Goal
This goal is to preserve the type distribution of the sampled graph Gs as similar as that of
the original graph G. The sample size varied from 50 to 200000 nodes, i.e., about 0.1%
to 50% sampling rate. The experiment results in Fig. 1(a) and (b) showed the error in the
type distribution for the 7-groups Twitter data set. In general, the error decreased as the
sampling size increased. Fig. 1(a) showed the ER of all the five sampling methods, we
found that RDS performed best when the sampling size was very small, but improved
slowly in large sample size. Because the fast-converge rate in the Markov transition,
RDS can provide more accurate results even when the information from the samples
was limited. However, since the Markov process converged very fast, the result was
determined until an enough number of nodes was reached. Thus, the following selected
entities failed to improve the accuracy. On the other hand, MES outperformed ECE
120 J.-Y. Li and M.-Y. Yeh
For the 5-groups Twitter data set, all patterns from five methods were similar to those
of 7-group Twitter data set as shown in Fig. 1(c) and (d). This is also true for the setting
of 3-groups as shown in Fig. 1(e) and (f). Only at small sample sizes, the results showed
that the error decreased as the number of groups was getting smaller. We will further
discuss the results in Section 5.5.
It is noted that, the results were similar for both ER and D-statistic. This was because
the property of the Twitter data sets. Since ER is an index to measure the total error,
it was sensitive to the performance on the largest or relatively great groups in terms of
size. On the other hand, D-statistic measured the cumulative error that encountered on
the greater groups as well in most cases. According to those reasons, we observed some
similar patters between ER and D-statistic.
5.4 Results of the Intra-Relationship Preserving Goal
Our second goal is to preserve the relationship among different groups in a network.
Fig. 2 presented experimental results for this goal. We found that RDS produced the
best result even at small sample sizes. It indicated that the sampling phase of RDS not
only provided the network information to the Markov Chain process, but also somewhat
preserved the relationship information (different tie types) as well. Still, its improve-
ment slowed down when the sample size became very large. On the other hand, MES
had a little higher errors compared to that of the ECE at a small sample size. Since the
original concept of MES is to avoid sampling bias from the chain-referral procedure in
the type distribution, it did not consider the issues about the relationship among indi-
viduals (edges on the graph). However, we can observe the advantage of MES when
the sample size increased. RES outperformed RNS since it is an edge-based random
selection. Thus it had more advantages than the node-based random selection did. Fi-
nally, RNS failed to describe the relationships among individuals with a small sample
size since RNS tended to produce a set of nonconnective nodes, which was especially
true when the network was sparse, that mislead the intra-connection ratio to 0. How-
ever, the situation changed while the sample size increased. Because RNS performed a
vertex-induced procedure after sampling enough nodes into the sample pool, this pro-
cess included both in-edge and out-edges between two nodes. Therefore, more selected
edges resulted in the better performance of the intra-relationship preserving goal. We
omitted the results of the 5-group data set due to the space limit. Its IRE values were
between those of the 7-group and 3-group settings.
5.5 Analysis on the Effects of the Number of Groups and the Sample Size
Here we provide some remarks on the performance at different numbers of groups. The
sampling size chosen here was 100. We only presented ER in Fig. 3(a) and omitted
the results of D-statistic since they had similar patterns. We found that both ER and
D-statistic affected by the number of groups (k) positively. This is reasonable since the
more k existed in a social network the more errors we observed lead to a lower accuracy.
On the other hand, in Fig. 3(b), the number of groups k were almost independent of the
intra-relationship error. It is noted that since RNS cannot sample any edge in the small
122 J.-Y. Li and M.-Y. Yeh
sizes. MES helped ECE a little at a small sample size. In addition, the random-based
methods were sample size sensitive and failed to provide reasonable results at small
sample sizes. In preserving the link relationship goal, we had a similar conclusion while
some differences were discussed. Furthermore, we discussed the results under different
group sizes. Finally, we provided a rule of thumb that a 15% sample size should be
large enough on the type distribution preserving and the intra-relationship preserving
sampling problems in our findings.
References
1. Navlakha, S., Rastogi, R., Shrivastava, N.: Graph summarization with bounded error. In:
Proc. of ACM SIGMOD Int. Conf. on Management of Data, pp. 419–432 (2008)
2. Gibson, D., Kumar, R., Tomkins, A.: Discovering large dense subgraphs in massive graphs.
In: Proc. of Int. Conf. on Very Large Data Bases, p. 732 (2005)
3. Raghavan, S., Garcia-Molina, H.: Representing web graphs. In: Proc. of IEEE Int. Conf. on
Data Engineering, pp. 405–416 (2003)
4. Kumar, R., Raghavan, P., Rajagopalan, S., Tomkins, A.: Extracting large-scale knowledge
bases from the web. In: Proc. of Int. Conf. on Very Large Data Bases, pp. 639–650 (1999)
5. Li, C.T., Lin, S.D.: Egocentric Information Abstraction for Heterogeneous Social Networks.
In: Proc. of Int. Conf. on Advances in Social Network Analysis and Mining, pp. 255–260
(2009)
6. Tian, Y., Hankins, R., Patel, J.: Efficient aggregation for graph summarization. In: Proc. of
ACM SIGMOD Int. Conf. on Management of Data, pp. 567–580 (2008)
7. Leskovec, J., Faloutsos, C.: Sampling from large graphs. In: Proc. of ACM SIGKDD Int.
Conf. on Knowledge Discovery and Data Mining, p. 636 (2006)
8. Hübler, C., Kriegel, H., Borgwardt, K., Ghahramani, Z.: Metropolis algorithms for represen-
tative subgraph sampling. In: Proc. of IEEE Int. Conf. on Data Mining, pp. 283–292 (2008)
9. Ma, H., Gustafson, S., Moitra, A., Bracewell, D.: Ego-centric Network Sampling in Viral
Marketing Applications. In: Int. Conf. on Computational Science and Engineering, pp. 777–
781 (2009)
10. Heckathorn, D.: Respondent-driven sampling: a new approach to the study of hidden popu-
lations. Social problems 44, 174–199 (1997)
11. Choudhury, M.D.: Social datasets by munmun de choudhury (2010),
http://www.public.asu.edu/~mdechoud/datasets.html
12. Krishnamurthy, V., Faloutsos, M., Chrobak, M., Lao, L., Cui, J.-H., Percus, A.G.: Re-
ducing large internet topologies for faster simulations. In: Boutaba, R., Almeroth, K.C.,
Puigjaner, R., Shen, S., Black, J.P. (eds.) NETWORKING 2005. LNCS, vol. 3462, pp. 328–
341. Springer, Heidelberg (2005)
13. Heckathorn, D.: Respondent-driven sampling II: deriving valid population estimates from
chain-referral samples of hidden populations. Social Problems 49, 11–34 (2002)
14. Lovász, L.: Random walks on graphs: A survey. Combinatorics, Paul Erdos is Eighty 2, 1–46
(1993)
15. Kemeny, J.G., Snell, J.L.: Finite Markov Chains, pp. 69–72. Springer, Heidelberg (1960)
99 年度專題研究計畫研究成果彙整表 
計畫主持人：葉彌妍 計畫編號：99-2221-E-001-010- 
計畫名稱：具不確定性時間序列與資料流之查詢處理 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際
已達成數)
本計畫實
際貢獻百
分比 
單位 
備註（質化說明：如
數 個 計 畫 共 同 成
果、成果列為該期
刊 之 封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
Wush Chi-Hsuan Wu, 
Mi-Yen Yeh, and Jian 
Pei, ’’’’Random 
Error Reduction in 
Similarity Search on 
Time Series: A 
Statistical 
Approach,’’’’ 
To Appear in the 28th 
IEEE International 
Conference on Data 
Engineering 
(ICDE-2012), April 
1-5, 2012. 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
國外 
參與計畫人力 
（外國籍） 博士生 0 0 100% 
人次 
 
