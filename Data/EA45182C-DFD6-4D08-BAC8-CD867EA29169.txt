 I
（一） 計畫中文摘要 
強化學習法 (Reinforcement Learning) 於過去幾年備受矚目，特別對於智慧型代理
人技術(Intelligent Agent Technology)之應用，單一獨立的智慧型代理人擁有自主性、
前瞻性、 反應性、社交能力等特性。這些特性令其有能力相互配合來解決複雜的問題，強
化學習法之所以受到代理人技術研究學者的注意，主要在於此方法能夠協助有獨立自主權
的代理人做出合適的決策以達成代理人之目標，透過代理人與其外在環境的互動，進而衍
生出一套適用於代理人決策的模式。雖然在過去幾年有一些成功應用強化學習法的例子，
但是強化學習法在生產製造系統方面的成功應用卻仍相當缺乏，本研究計畫主要在研究應
用強化學習法中的 Q學習演算法 (Q-Learning Algorithm)來決定單一機器生產工作之順序 
(Job Sequencing)，一般而言，生產現場通常使用不同的工作派遣法則 (Dispatching Rule) 
來決定工作被處理的先後順序，本研究將探討是否 Q學習演算法能使機台代理人針對不同
的總體目標選擇出合適的工作派遣法則，下列三個追求不同總體目標的例子將被測試： 
1. 使最大工作完成的延遲時間最小化(Minimize Maximum Lateness)。 
2. 使被延遲工作的總數最小化 (Minimize Number of Tardy Jobs)。 
3. 使每個工作平均的延遲量最小化 (Minimize Mean Lateness)。 
Q 學習演算法是目前最被廣泛使用的強化學習法之一，目前雖然有一些成功應用的範例，
但於製造生產系統方面的應用，還未進入應用的階段，本研究針對單一機台的排程問題，
倘若能成功應用 Q 學習演算法於此排程問題，則研究結果將可做為未來實際應用於複雜生
產排程的基礎。 
 
關鍵字: 強化學習法、Q學習演算法、生產排程、智慧型代理人技術。 
 
 1
（二） 報告內容 
前言: 
生產排程為製造系統中非常重要的決策過程，它不僅將製程計畫具體轉換成製造生產
現場的作業時程表，且有效率的排程，更能為現場提供高輸出產能、降低存貨、且增高資
源的使用率。一般而言，生產排程的問題指的是運用有限的生產製造資源，在特定的條件
限制與目標函數下，將指定工作完成，許多複雜的排程問題屬於 NP-complete 型態的問題，
而有關於排程的研究，因為已有幾十年的歷史，許多演算法與啟發式法則已成功得解決某
些排程問題[3, 12, 15, 17]。基本上，排程問題可細分為兩個層次:工作路程的排定、工
作順序的安排，排定工作路徑乃關於決定分配工作於那些特定的機台作加工，工作路徑的
排定源於路徑的彈性的空間，而機台功能的多寡更是決定了路程彈性空間的重要因素，路
程彈性在近代的製造系統中被視為是一樣重要的議題，尤其是在一些關於彈性製造系統的
研究中。 
一旦工作的作業途程確定，工作物件依據所規劃的途程來運輸，當其被運送到所需的
機台，等待被加工時，接下來所要面臨的另一能層次的問題便是要決定在機台等待工件的
工作順序，工作順序經常是跟據某些派遣法則來做決策，這些派遣法則之所以實用因為其
簡單易懂且容易施行，在過去的幾十年裡，已有上百種派遣法則被研發提出[14]，不同的
派遣法則可針對不同的系統目標有特別的功用，然而，當多項系統目標同時被要求時，可
能很難找到某一種派遣法則同時可以合乎這些系統目標的要求，何況製造系統瞬息萬變，
針對系統狀況與需求的變更，不斷的改變與運用適當的派遣法則為一相當合理的作法。 
 
研究目的: 
本研究的主要目的在於研究應用強化學習法於以代理人為基礎之生產排程的可行性，
而本研究所採用之強化學習法為 Q學習演算法，於單一機台的工作派遣，研究 Q學習演算
法能否使機台代理人能針對不同的目標而選擇出最適當的工作派遣法則，本研究的結論預
期將可作為在未來在應用強化學習法於更複雜且動態的零工式生產排程問題的重要參考。  
 
文獻探討: 
 在過去的幾年，代理人技術被認定是一種能夠有效開發應用軟體的技術[2, 13]，從軟
體的角度來看，代理人可被視為一種具有運算、溝通能力，且能根據其自身的環境與自我
的目標自主作出判斷能力的模組 [16, 23]，這樣的特性十分符合地方分權的製造系統架
構，在此架構下的生產製造系統擁有彈性、可靠性、適應性、重新整合性等-未來製造系
統的特性。然而，應用代理人技術於此系統，每個獨立代理人可用來代表工作場所中的實
際個體如零件、機台、工具、作業員工等，每個代理人將負責資訊的收集、資料的儲存、
以及工作現場中決策的判斷，在自地方分權的架構下，獨立代理人需要協同合作以完成其
工作目標，Smith [18]於 1980 提出一種以協商談判為主的協定 – 契約網協定，契約網
協定能夠提供提供代理人一個資訊即時交流的機制使其完成製造系統中的作業的控制與
排程。 
 強化學習法，為一種於 90 年代初期被研發出的機械學習法[11, 20]，它能夠使得代理
人透過其與所處環境的互動，並根據代理人每個動作所造成對環境的正面或負面影響而習
得適當的決策機制。這樣的方法使代理人透過學習的過程以增強其作決策的能力進而對系
統整體有實質的助益，雖然現今有些應用強化學習法的例子[1, 4~10, 15, 17, 19, 21, 24]
的目標為最小化平均的延遲量，報償函數在此針對工件延遲量的多寡分為數個等級，工件
愈早被完成，獎勵愈大，工件愈晚被完成，懲罰愈大。 
 
表一. 三個案例之報償函數設置 
Case A Case B Case C 
 
If (L > ML) 
      ML = L; 
      Reward = -1;
Otherwise 
      Reward = 1; 
 
 
 
If (finished job is tardy)  
     Reward = -1; 
Otherwise 
     Reward = 1; 
If    L < -30),     Reward = 4; 
Else if (-30≤L <-20), Reward = 3; 
Else if (-20≤ L <-10), Reward = 2; 
Else if (-10≤ L < 0), Reward = 1; 
Else if ( 0 ≤ L < 10), Reward = -1; 
Else if ( 10≤ L < 20), Reward = -2; 
Else if ( 20≤ L < 40), Reward = -3; 
Else if ( 40≤ L < 60), Reward = -4; 
Else if ( 60≤ L ),    Reward = -5; 
 
結果與討論: 
本研究對上述三個案例分別模擬五十萬個工件，每個案例均以不同亂數為基礎重複執
行三次，在此三個案例中，從過去文獻中可獲知，EDD 為 case A 之最佳的法則，Hodgson’s 
演算法為 case B 之最佳法則，SPT 為 case C 最佳法則，表二為三個案例中機台代理人運
用強化學習法選擇各法則的比例，在 case B 與 case C 中機台代理人在五十萬個工件中選擇
了約 85%選擇了最佳的法則，顯示強化學習法具有讓機台代理人選擇了較佳派工法則的淺
力，在 case A 中最佳法則 EDD 雖然只被選用 45%，但於三個法則中，其被選擇率已是最
高。 
 針對 case A，代理人目標為使工作完成的最大延遲時間最小化，若分別實行 FIFO 與 
EDD，使用 EDD 與 FIFO 的最大延遲時間為 157.666 與 157.643，幾乎沒有差別，這也說
明為何 EDD 的被選擇率只有 45.2%，因為 FIFO 對最大延遲時間最小化有不錯的效果。對
於代理人學習的過程，圖一為 case C 中智慧型代理人選擇最佳派工法則之學習過程，其
中代理人經歷了十萬個工件的學習機會，代理人以超過八成的機率會選擇最佳的派工法則 
SPT。  
Case C.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Number of Jobs (x100,000)
Se
le
ct
io
n 
Pe
rc
en
ta
ge
EDD
SPT
FIFO
 
     圖一. 智慧型代理人選擇最佳派工法則之學習過程 
 
 3
 5
Cellular Telephone Systems. Advances in Neural Information Processing Systems: 
Proceedings of the 1996 Conference, MIT Press, Cambridge, MA, 974-980. 
18. Smith, R., 1980. The Contract Net Protocol: High Level Communication and Control in 
Distributed Problem Solver. IEEE Transactions on Computers 29 (12), 1104-1113. 
19. Sutton, R. S., 1996. Generalization in Reinforcement Learning: Successful Examples using 
Spare Coarse Coding. In: Touretzky, D.S., Mozer, M.C., Hasselmo, M.E. (eds.), Advances in 
Neural Information Processing Systems: Proceedings of the 1995 Conference, MIT Press, 
Cambridge, MA, 1038-1044. 
20. Sutton, R.S., Barto, A.G., 1999. Reinforcement Learning: An Introduction. The MIT Press. 
21. Tesauro, G., 1995. Temporal Difference Learning and TD-Gammon. Commun. of the ACM 
38(3), 58-67. 
22. Watkins, C.J.C.H., Dayan, P., 1992. Q-learning. Machine Learning 8, 279-292. 
23. Weiss, G., 1999. Multiagent Systems: A Modern Approach to Distributed Artificial 
Intelligence. MIT Press. 
24. Zhang, W., Dietterich, T.G. 1995. A Reinforcement Learning Approach to Job-Shop 
Scheduling. Proceedings of the 14th International Joint Conference on Artificial Intelligence, 
1114-1120. 
 
 
（四） 計畫成果自評 
強化學習法為一種新的機器學習方法，然而目前強化學習法還是處於待開發的階段，
而且在實務應用方面，過去的文獻只出現一些少許成功的案例，但是生產製造系統方面的
應用，仍然非常缺乏，本計畫乃針對製造系統裡的生產排程問題，將強化學習法應用於其
中並且整合代理人技術，會使得獨立代理人具有智慧決策的能力。本計畫的成果可供未來
整合代理人技術與強化學習法於複雜的生產排程問題上參考使用。本計畫部分成果已於發
表於 2005 年國際期刊 Engineering Application of Artificial Intelligence。 
 
 
 
