2 
 
3 移動式測圖系統相關文獻 
移動式測圖系統的概念最早是由加拿大 Calgary 大學所提出，整合多種感測器的移動式平台，藉
由感測器取得平台位置並收集物空間資訊，而真正成功運作則是於 1983 年的 MHIS(Mobile Highway 
Inventory System)系統(Zhang and Xiao, 2004)。而第一套將 GPS 整合至 MMS 的則是由美國俄亥俄州立
大學的製圖中心(Ohio State University's Center for Mapping)所研發的 GPSVan，該系統包含 GPS、航位
推算系統(Dead-Reckoning System, DRS)、數位相機及錄像機，主要以 GPS 提供相機位置，而航位推算
系統則提供相機姿態資訊及當 GPS 斷訊時的位置資訊，藉由量測相片共軛點並施行前方交會以獲致目
標物三維坐標資訊，而錄像機則是用以記錄屬性資訊，但在當時的技術下定位精度僅能達到數公尺等
級 ( 江 凱 偉 ， 2007) 。 近 幾 年 中 較 有 名 的 MMS 則 是 由 Calgary 大 學 發 展 的 VISAT TM 
(Video-Inertia-SATellite)系統，在發展者 El-Sheimy(1996)的博士論文中比較了在 VISAT 之前的 MMS，
而隨著測量技術的進步，VISAT 不斷的更新，其能即時產製三維圖形及俯視圖的功能，為其最大之特
色。另外， Grinstead et al.(2005)則以車載之雷射掃描資料建立三維建物模型；而進行道路的量測、更
新及管理亦為 MMS 的主要工作之一(Tao, 2000; Gilliéron et al., 2001; Gontran et al., 2003)，Hu and 
Uchimura(2006)則將 MMS 所得資訊直接用於視覺化的車用導航，Fruh and Zakhor(2003)則將 MMS 由
地面上所觀測之資訊與空載光達系統合併，建立出兼具屋頂及側面資訊之建物三維模型。 
4 研究方法 
針對 97 年度之研究項目如圖 4.1 所示，本報告將分述研究方法與測試成果於後續章節。 
 
圖 4.1、97 年度工作流程圖 
4.1 研究測試實驗場 
 由於整合計畫中之移動式測圖系統資料(子計畫二)尚待檢測，本研究中所研擬之演算法目前乃藉
由自行模擬之測試資料(實驗場一)與加拿大 Calgary 大學車載移動式測圖系統實測資料(實驗場二)進行
測試，以驗證演算法正確性與可行性。其中，以模擬移動式測圖系統量測之場景如圖 4.1.3 示，而實
際車載移動式測圖系統之施測場景如圖 4.1.4 示。 
4.1.1 模擬移動式測圖系統之測試資料(實驗場一)建置 
 本研究之實驗場一為由模擬(由人代替車體)移動式測圖系統針對台灣大學昆蟲系館外牆進行圖資
收集。圖 4.1.1 中，黃色線為模擬 MMS 系統行進軌跡，藍色圓點為 GPS 定位之取樣點。在測試場中
利用導線測量及 GPS 之 RTK 定位方式取得點位坐標，並同時以地面光達系統進行施測。在黃色線上
每隔一公尺對目標場景拍攝一張影像以建立 MMS 之時序影像資料。而綠色不規則形則表示樹木所在
之區域，為量測時造成遮蔽之重要因素。其中，RTK 所量測之點位為 WGS84 坐標系統，導線測量為
區域性坐標系統，因此需將兩坐標以七參數相似轉換，如式(4.1.1)所示，以利所有資料套合至共同的
WGS84 坐標基準。圖 4.1.2 展示套合成果。 
 
4 
 
收集，而本研究目前採用該系統中兩具影像感測器(感測器一及感測器二)所攝之時序影像進行測試，
圖 4.1.4 顯示所使用之影像。但由於該系統中並未具有光達量測系統，因此無法獲得目標場景之三維
點雲資料。VISAT TM 移動式測圖系統之量測架構可由式(4.1.2)之數學模式加以描述。 
࢘࢏
࡯ࡲ ൌ ࢘૙
࡯ࡲሺ࢚ሻ ൅ ࡾ࡯
࡯ࡲሺ࢚ሻൣࣅ࢏࢘࡯ െ ࢇ࡮
࡯ ൧                                                 (4.1.2) 
其中 ࢘࢏࡯ࡲ: 第 i 點之三維坐標向量；࢘૙࡯ࡲሺ࢚ሻ: t 時刻導航感測器(INS/DGPS)之坐標向量; 
     ࡾ࡯࡯ࡲሺ࢚ሻ: t 時刻從相機框架(Camera frame)轉至計算框架(Computational frame)之旋轉矩陣; 
     ࣅ࢏: 第 i 點之尺度因子; ሺ࢚ሻ: 影像拍攝之時刻; ࢘࡯: 校正後之影像量測坐標向量; 
     ࢇ࡮࡯ : 在相機框架中相機相對於載體框架(Body frame)之向量。 
 
圖 4.1.4、感測器一(左側兩張)、感測器二(右側兩張)之時序影像示意 
此實驗場主要場景為道路，而非如實驗場一含有特徵變化豐富之人造建物，可測試本研究採行之
演算法能適用於不同場景之效益。 
4.2 特徵萃取 
所謂特徵即為具有色彩或幾何上不連續特性之位置，利用這些位置可將真實世界中複雜之地物以
較簡單之方式呈現。而此類特徵因具有色彩或幾何的特性，亦可由特殊的演算法經自動化之方式將其
萃取出來。在影像的物像對應關係解算中需藉由物像與影像間之共軛資訊進行解算，而在 MMS 系統
中所拍攝之時序影像數量較大，若由人工進行量測則造成作業程序之困難，因此藉由影像中的特徵萃
取，再經由共軛特徵的匹配能有助於作業程序之簡化。 
4.2.1 影像特徵萃取 
影像為二維規則排列之資料，二維空間中之特徵物不外乎點、線、面的特徵，即使非直線之線型
特徵仍可由連續相連直線段表示，而面特徵則可邊界線段來描述其輪廓。為了避免過多類型之特徵造
成處理上之不便，本研究把影像中之特徵簡略分為點特徵及直線特徵兩種。 
4.2.1.1 影像中點特徵萃取 
 Harris 及 Stephens(1988)提出影像中點特徵萃取之演算法，以式(4.2.1)至(4.2.5)進行特徵萃取。 
    ࣬ ൌ ܦ݁ݐሺࣧሻ െ ݇ሺݐݎܽܿ݁ሺࣧሻሻଶ                                (4.2.1) 
    ࣧ ൌ ቂࣛ ࣝ
ࣝ ࣜ
ቃ                                   (4.2.2) 
    ࣛ ൌ ቀபூ
ப௫
ቁ
ଶ
ٔ इ, ࣜ ൌ ቀபூ
ப௬
ቁ
ଶ
ٔ इ, ࣝ ൌ ቀபூ
ப௫
ቁ ቀபூ
ப௬
ቁ ٔ इ                               (4.2.3) 
    ܦ݁ݐሺࣧሻ ൌ ࣛࣜ െ ࣝଶ                                   (4.2.4) 
    ݐݎܽܿ݁ሺࣧሻ ൌ ࣛ ൅ ࣜ                                 (4.2.5) 
其中，I 為目標影像，k 為 Harris 常數，一般常設為 0.04~0.15，इ為高斯濾波罩窗；計算所得的࣬即為
成果，特徵判斷的依據為當࣬ሺݔ, ݕሻ ൐ ܯ݁ܽ݊ሺ࣬ሻ ൅ 2 · ܵݐ݀ሺ࣬ሻ時則判斷為點特徵，若࣬ሺݔ, ݕሻ ൏ െܵݐ݀ሺ࣬ሻ
6 
 
ݔܿ݋ݏߠ ൅ ݕݏ݅݊ߠ ൌ ݎ)，轉換後每個邊緣像元會從 x-y 空間的一點變成ݎ െ ߠ空間的一條曲線，而在 x-y
空間中共線的邊緣像元，在ݎ െ ߠ空間會交於一點(如圖 4.2.4)，經由在ݎ െ ߠ空間的群集偵測(Cluster 
Detection)，則可反推得到在 x-y 空間中較長的直線。 
      
圖 4.2.4、霍夫轉換示意圖(彭念豪，2005)        圖 4.2.5、對邊緣偵測成果進行區域成長處理 
然而邊緣像元數量龐大，直線的群集之間常會互相干擾，又或是受到雜訊的影響，造成找到正確
直線的困難。由此，在本研究為了避免直線群集間的互相干擾，在影像邊緣偵測後加入區域成長處理
(圖 4.2.5)，將邊緣像元依照是否相鄰組成多個群集，再針對單一群集進行霍夫轉換。在單一群集中，
若有直線的存在，則屬於直線之像元比例將會較大於雜訊或不為直線的邊緣像元，如此則可減少對直
線偵測的影響。 但部份直線會因為影像中的雜訊，造成邊緣偵測所得之像元不為連續，使得後續區
域成長時被分為兩組，而被視為兩條不同之直線。因此在研究中再加入重新鏈結的機制，讓直線參數
相近且距離接近之直線鏈結為一條直線。同時亦考量直線像元較少之直線較不可靠且後續應用價值低
而將其剔除。依照前述流程對測試影像進行處理則可得到如圖 4.2.6 之成果。圖中大部份之直線特徵
皆可被偵測到，然後樹木中亦有被視為直線之特徵，此部份也可由前述之 GI 門檻方式剔除之。 
          
圖 4.2.6、測試場中影像之直線萃取成果；實驗場一(左)、實驗場二(右) 
4.2.2  光達點雲特徵萃取 
光達點雲特徵萃取是針對離散光達點雲處理的前端步驟，對於點雲資料運用是相當重要的環節。
在相關的研究方法中 Boyer and Sarkar(1999) 彙整三維離散點資料結構化之相關研究工作；Morgan and 
Habib(2002)以點雲產生不規則三角網配合區域成長法與霍夫轉換萃取建物邊線 ; Vosselman and 
Dijkman(2001)利用三維霍夫轉換萃取建物面特徵，然而目前現有之特徵萃取演算法對於計算執行效率、
萃取完整性、自動化程度仍是需待突破的重點。而上述相關文獻中的演算法大多具有自動化潛力但尚
未達到可完全自動處理的程度。本研究將選用三維霍夫轉換作為點雲特徵萃取工具，透過霍夫轉換萃
取三維平面特徵再藉由特徵交會的方式獲得空間中的直線與點特徵。 
4.2.2.1 三維平面特徵霍夫轉換 
研究中採用極坐標的三個角度參數ߠ, ߮, ߩ描述三維平面特徵如式(4.2.7)，而三維平面霍夫轉換示
意如圖 4.2.7 所示。 
ݏ݅݊ߠܿ݋ݏ߮ܺ ൅ ݏ݅݊ߠݏܻ݅݊߮ ൅ ܿ݋ݏߠܼ ൌ ߩ                                               (4.2.7) 
其中，ߠ: 為與 Z 軸之夾角; ߮: 為與 X 軸夾角; ߩ: 為原點至平面之距離。 
-2 0 2 4 6 8 10 12
-4
-2
0
2
4
6
0 50 100 150 200 250 300 350
-15
-10
-5
0
5
10
15
8 
 
  ܥ ൌ ൥
1 0 െݔ଴
0 1 െݕ଴
0 0 െ݂
൩                                                                                                                        (4.3.4) 
m 為目標點坐標ሾx y 1ሿT，ܾ௑, ܾ௒, ܾ௓為兩像片投影中心之基線分量，ݔ଴, ݕ଴, ݂為像主點偏移及像
主距，ܯ௅及ܯR分別為左右像之旋轉矩陣。核線雖可精確地約制基線方向，但垂直於基線之方向則無
約制能力，故藉助已知的方位參數精度來算點位在垂直核線方向的誤差量，以此約制垂直基線方向之
搜尋空間。在搜尋空間內利用影像匹配之方式取得目標點之共軛點，本工作由 NCC(Normalized Cross 
Correlation) 法取得近似之匹配點位置，再以 LSM(Least-Squares Matching)精化匹配成果，並比較 NCC
與 LSM 所得之匹配成果，若匹配位置差異過大(如超過兩像元)，則認為該成果不可靠並剔除該點。由
上述之匹配策略套用於兩組實驗場之匹配成果示意如圖 4.3.5 所示。在匹配效率上若可提供匹配合理
之搜尋位置則可使匹配有較高的成功機率且提升執行效率。 
 
(a)                                (b) 
 
                (c)                                (d) 
圖 4.3.1、兩實驗場影像之匹配成果示意圖 
圖 4.3.1(a)(c)分別為實驗場一及二之目標影像，圖中紅點為經匹配獲得之共軛點特徵，藍點為未
能找到共軛匹配之點特徵，黃點表示經人工檢核為錯誤之點特徵。(b)(d)為對應之為搜尋影像，圖中點
位則對應左圖中可找到共軛之點位。從圖中可得知，錯誤匹配點所匹配到的位置皆與合理位置有一定
之距離，因此若實際之 MMS 系統可提供較好方位近似值，應有助於避免或減少此類錯誤匹配。唯本
例實驗場二測試影像偵測得之點特徵及匹配成功之配對數量偏少，在偵測門檻的選擇上還須經由測試
找到最適值。 
4.3.2 點特徵物像對應之回饋 
物像對應之回饋意義在於精化(Refine)影像之方位參數品質。理論上，利用已知方位參數進行共軛
點、線特徵前方交會即可獲致所處理特徵之三維坐標或軌跡，然而考量直接定位系統中存在之誤差，
1
23
5
78 9 10
11
1416
18 1920
21
22
23
25
26
27
28
30
1 2
3
5
78 9
10
11
1416
18 1920
21
22
23
25
26
27
28
30
1 23
4
5 67 8
9
10
11
12
13
14
15
1 3
57
8
9 13
10 
 
1 像素(0.0074 公厘)，共軛光束交會一致性檢查成果如表 4.3.1，表中顯示共軛光束之 Y 視差相當小，
應伴隨有品質良好之方位參數品質，在此情況下，物像對應回饋改正助益應不顯著。 
 
5 參考文獻 
江凱偉，2007。新世代的測量車，科學發展 ，第416期，pp.6-12。 
彭念豪，2005。以控制直線進行影像外方位參數求解之自動化作業，國立台灣大學土木研究所碩士論文。 
彭念豪、趙鍵哲，2007。利用光達點雲作為地形特徵解算數位相機影像之外方位參數，第26屆測量及空間資訊
研討會，pp899—906。 
趙鍵哲、鄭傑中，2007。使用CSR演算法重建屋頂面模型，航測及遙測學刊，第十二卷，第四期，pp. 457-478。 
Boyer, K.L. and S. Sarkar, 1999. Perceptual organization in computer vision: status, challenges, and potential, 
Computer Vision and Image Understanding, 76(1):1-5.  
El-Sheimy, N., 1996. The Development of VISAT - A Mobile Survey System for GIS Applications, Ph.D. Dissertation, 
The University of Calgary, Calgary, Canada. 
Fruh, C. and A. Zakhor, 2003. Constructing 3D City Models by Merging Aerial and Ground Views. IEEE Computer 
Graphics and Applications, November/December, pp52—61 
Gilliéron, P.Y., J. Skaloud, D. Brugger, B. Merminod, 2001, Development of a Low Cost Mobile Mapping System for 
Road Database Management, The 3rd International Symposium on Mobile Mapping Technology 
Gontran, H., J. Skaloud, P.Y. Gilliéron, 2003. A Mobile Mapping System for Road Data Capture via a Single Camera, 
Proceedings of sixth optical 3D measurement techniques. 
Grinstead, B., A. Koschan, D. Page, A. Gribok, and M. A. Abidi, 2005. Vehicle-borne Scanning for Detailed 3D Terrain 
Model Generation, Society of Automotive Engineers Commercial Vehicle Engineering Congress, Chicago, SAE 
Technical Paper 2005-01-3557. 
Harris, C. and M. Stephens, 1988, A Combined Corner and Edge Detector, Proceedings of Alvey Vision Conference, 
Manchester, UK, pp.147-151. 
Hu, Z.C. and K. Uchimura, 2006. Fusion of Vision, GPS and 3D Gyro Data in Solving Camera Registration Problem 
for Direct Visual Navigation, International Journal of ITS Research, Vol. 4, No.1, pp3—12. 
Lin, S. H., and J. J. Jaw, 2004. Structuralization of LIDAR Point Cloud, 25th ACRS, Chiang Mai,Thailand, Vol I, pp. 
102-107. 
Morgan, M. and A. Habib, 2002. Interpolation of LIDAR Data and Automatic Building Extraction, ACSM-ASPRS 
2002 Annual Conference Proceedings, pp. 19-26. 
Nguyen, V., A. Martinelli, N. Tomatis, and R. Siegwart, 2005. A Comparison of Line Extraction Algorithms using 2D 
Laser Rangefinder for Indoor Mobile Robotics, Conference on Intelligent Robots and Systems, IROS’2005, 
Edmonton, Canada. 
Park, D. J., K. M. Nam and R. H. Park, 1995. Multi-resolution edge detection techniques, Pattern Recognition, Vol.28, 
No.2, pp.211-229. 
Schenk, T., 1999, Digital Photogrammetry, Volume I, TerraScience. 
Tao, C.V., 2000. Mobile Mapping Technology for Road Network Data Acquisition, Journal of Geospatial Engineering, 
Vol. 2, No.2, pp. 1-13 
Vosselman, G. and Dijkman, S., 2001. 3D building model reconstruction from point clouds and ground plans, 
International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 34(3):37-43. 
Wu, H., Yoshikawa, G.,  Shioyama, T.,  Lao, T., Kawade, T., Pattern Recognition, 2002. Proceedings. 16th 
International Conference on Volume 2, 11-15 Aug. 2002 Page(s):346 – 349 
Zhang, K.F. and B.L. Xiao, 2004. Current Status of Low Cost GPS and Mobile Mapping Systems. Department of 
Geospatial Science, RMIT University, Melbourne, Australia. 
 
6 計畫成果自評 
依據研究規劃時程，本年度預計之研究內容除了影像空間之直線特徵匹配與重建模式仍在進行中，
其餘各研究項目皆已建構完成，並透過實驗測試驗證本工作中所研擬之各演算模式確實可符合移動式
測圖系統之資料特性以及實際運用作業之需求。 
與本計畫相關之移動式測圖系統尚處於檢測階段，本工作報告中還未能引用前述系統收集之資料 
進行各項測試，此部份將於第二年執行進度報告中呈現完整測試結果及分析。 
2009 Urban Remote Sensing Joint Event 
978-1-4244-3461-9/09/$25.00 ©2009 IEEE
Data Fusion towards Building Roof Reconstruction 
based on CSR approach 
Jen-Jer Jaw, Chieh-Chung Cheng 
Dept. of Civil Eng.  
National Taiwan University 
Taipei 10617, Taiwan 
jejaw,d95521009@ntu.edu.tw 
Abstract—3D building modeling has emerged as a focus of 
research as well as development for many urban planning and 
geo-information related applications. To its most abstract form 
on geometric level, 3D building roofs are regarded as the 
fundamental elements to present outlines of 3D building models. 
The emphasis of this study is to demonstrate a novel algorithm of 
building roof reconstruction, termed CSR (Construct-Shape-
Refine). The proposed algorithm aims at reconstructing building 
roof models purely by employing 3D line features or integrating 
3D and 2D line features from existing data. In particular, the 
latter integration involving fusion from multiple data sources 
strengthens the reliability as well as the precision of 3D building 
roofs. The CSR approach is characterized through three stages. 
Geometric inferences are imposed at the stage of Construct 
where topological relationship of the 3D line features is 
established while 3D coordinates of roof corners are adjusted and 
estimated through Shape processes, apart from compensating the 
hidden boundaries, if any. The efficiencies of two fusion streams, 
including fusion of 3D line features that result from different 
sources or fusing 3D line features with photogrammetric line 
features, are gained as promoting the building roof quality via 
Refine workflow. Experiments of the Construct-Shape stages 
show that the proposed method is independent of building roof 
types, however constrained in polygons under current 
consideration, as well as whether 3D line features are complete 
for both successful and efficient performance of building roof 
reconstruction on a 3D line feature basis. Moreover, the Refine 
stage offers more up-to-date and satisfactory building roof 
reconstruction results as compared to the situation when only one 
single data set is considered. 
I. INTRODUCTION 
3D reconstruction of city model is a very active research 
topic in Digital Photogrammetry (DP) as well as Computer 
Vision (CV) community. The indispensable process of 3D city 
modeling is to construct building roof models. Traditionally, 
the generation of building roof models is mainly performed by 
measuring conjugate points on aerial stereo-paired images. On 
the other hand, line features of higher-order information, easier 
detected than point features, are the main evidence for building 
hypotheses and would serve as excellent feature primitives for 
building roof reconstruction when giving proper 
photogrammetric approaches [1]. As far as the data sources are 
concerned, line features can be extracted or measured from 
aerial images, topographic maps and laser range data (termed 
LIDAR data in this study), etc. Recent years, LIDAR system 
has emerged as a new technology for obtaining surface data 
potentially revealing detailed scene geometry, which, as 
compared to aerial images that contain abundant spectral 
information and scene information, renders a promising 
alternative for feature extraction and building roof 
reconstruction. Besides, vertical component accuracy is far 
better than horizontal component in airborne LIDAR system 
while photogrammetric means usually suggests the opposite 
result due to restricted base/height imaging geometry.  It is 
therefore found that the LIDAR point clouds and aerial 
imagery data possess mutually independent observations, 
suggesting complementary advantages if they are appropriately 
fused. Furthermore, two dimensional topographic maps, 
usually resulting from photogrammetric mapping, provide 
accurate building boundaries. Thus, it is highly desirable to 
fuse LIDAR data, aerial images and 2D topographic maps for 
the purpose of building roof reconstruction. 
Several data fusion algorithms have been developed to 
reconstruct the building model integrating different data 
sources, such as fusion of LIDAR and aerial images [2][3][4], 
fusion of LIDAR and 2D topographic maps[5][6] and fusion of 
LIDAR, 2D maps and aerial images[7]. In particular, upon 
realizing the flexibility of exploiting 3D line features from 
different data sources and the advantage of data fusion, the 
authors proposed, in this study, a scheme comprising 
Construct-Shape-Refine (CSR) steps for building roof 
reconstruction as follows: (1) 3D line features extraction, (2) 
building roof reconstruction (in Construct-Shape stages), and 
(3) data fusion (in Refine stages). In the first step, 3D line 
features from different sources are regarded as input data when 
initiating CSR approach. The Construct in the second step is to 
build the topological relationship among 3D line features, and 
then 3D building roof models are generated in Shape stage.  
The third step is to refine the 3D building roof model by fusing 
building roof models from different sources. The advantages of 
the refined step can not only improve the accuracy of building 
roof models but also increase the completeness and loyalty. Fig 
1 illustrates the flowchart of the proposed CSR method for the 
generation of 3D building roof models. The dashed and black 
rectangles depict the semi-automatic and automatic approach, 
respectively. The authors consider the proposed method a 
general approach without specifying any data set as a must, 
2009 Urban Remote Sensing Joint Event 
978-1-4244-3461-9/09/$25.00 ©2009 IEEE
                                                                 (e)                                                                 
Figure 4. Example of automatic mode 
                                        (a) 2D line features on one image. 
                                        (b) result of 2D geometric inference. 
                                        (c) line-based intersection. 
                                        (d) result of 3D geometric inference. 
                                        (e) 3D line fitting. 
2) Semi-Automatic Mode: In order to implement the 
process of building roof reconstruction in a more efficient as 
well as reliable fashion, the line-based user-interface module 
was designed. The approach of this module is to sequentially 
measure the conjugate 2D line features in, at least, a stereo-pair. 
And then 3D line features collected in this mode are the result 
of the intersections of conjugate 2D line features [9][10].  
C. For 2D Map System 
The existing 2D map data provide accurate building 
boundaries. However, a virtual value for each vertical 
component is assigned and a relatively large variance is given 
to reflect the deficiency of the height information. By doing so, 
the treatment of fusing building roof models on 3D basis can 
be common to all data sources.   
III. CONSTRCT-SHAPE-REFINE APPROACH
After 3D line features are extracted and regarded as input 
data for the CSR algorithm, then the procedures of building 
roof reconstruction are performed through the following 
geometric inferences: (1) constructing the topological 
relationship of 3D line features that belong to the same 
building roof by using the intersecting property of 3D line 
features when projected onto a plane; (2) shaping the building 
roof by means of adjusting the 3D line features, compensating 
missing parts by the shortest path algorithm and constructing 
roof polygons, if any, and reporting whether or not the 
investigated building roof is completed; (3) refining the 
building roof by integrating initial building roof models from 
the image system, map system and those from LIDAR system. 
The CSR approach is detailed in the following sections.
A. Constrct 
The objective of Construct step is to build the topological  
relationship of 3D line features that belong to the same 
building roof. There are three steps in the proposed scheme: (1) 
determination of reasonable junction points, (2) grouping the 
3D line features, and (3) completing the topology of every 
roof. 
1). Determination of Reasonable Junction Points: The core  
idea behind the Construct procedure is the characteristic of 
intersection of the 3D line features when projected onto XY-
plane [8]: When extended, a 3D line feature will be suspended 
as running into other 3D line features. For this reason, we 
claim that the 3D line features upon projecting onto the plane 
would find the most proper junction point which situates 
closest to the associated lines as they are extended. Fig. 5 
illustrates the above idea. The black dotted line in Fig. 5 is to 
be intersected with black line and then stop when the closest 
junction point is found. 
Figure 5. The property of 3D line features intersection 
In order to obtain the correct intersection of 3D line features 
that correspond to building corners, two thresholds of 
geometric inference are designed:  
a) Distance check: The distance threshold is set taking 
account of the random error of measurement and then used to 
filter out the improper junction points by buffering the end 
points of the line features.  
b) Topological threshold: When adjacent buildings get 
too close, the above distance check is inadequate to 
guaranteeing appropriate junction points. A topological check 
is therefore implemented to identify those ambiguities. The 
black points in Figs. 6 (a) and (b) show junction points before 
and after applying the aforementioned thresholds. Fig. 7 
depicts the algorithm of junction point determination. 
(a)
                                      (b) 
Figure 6. Junction points before (a) and after (b) applying 
thresholds 
            
Roof 1 Roof 2
Stop here ! 
2009 Urban Remote Sensing Joint Event 
978-1-4244-3461-9/09/$25.00 ©2009 IEEE
                 Figure 9. Example of compensation of hidden parts 
After the above-mentioned procedures, the wireframes of 
roof models are shown in Fig 10(a). Fig 10(b) is the 
magnification of one of those buildings where the blue corners 
are obtained by adjusting those visible 3D lines. Besides, roof 
corners in Fig 10(b) are non-coplanar. 
                                            (a)
                                           (b)
Figure 10. Illustration of building roof wireframe models 
(a) wireframe models. 
(b) a single roof model. 
3). Constructing the polygons: Due to the absence of polygon 
information, we developed a new algorithm for constructing all 
polygons of the building roof simultaneously. There are four 
steps involved:  
a)  Constructing 3D triangulated irregular networks (TIN) 
and projecting them onto X-Y plane, as shown in Fig 11(a).  
b) Filtering those unreasonable triangles by two thresholds 
of geometric inference, as shown in Figs 11(b) and (c).  Fig 
11(b) illustrates that when any 3D line feature intersects with a 
triangle, then this triangle is unreasonable and should be 
rejected. Fig 11(c) illustrates that when any roof corner is 
inside the triangle, then this triangle is unreasonable. Fig 11(d) 
illustrates those reasonable triangles. 
c) Grouping those reasonable triangles when they show 
common edges, and then forming polygons. 
d) Tracing the roof corers after deleting those virtual 
edges which do not belong to real boundaries of building roof. 
Fig 11(e) shows the result of polygon construction of the 
same building as in Fig 10(b).  
                                        (a) 
                   (b)                                 (c) 
                                   (d) 
                                           (e) 
Figure 11. Illustration of polygon construction 
                         (a) 3D triangles projected onto X-Y plane. 
                         (b) a triangle is intersected by a 3D line feature. 
                         (c) roof corners are inside a triangle. 
                         (d) reasonable triangles. 
                         (e) polygon construction of one building. 
                         
C. Refine 
The approach of Refine stage is aimed to fuse the 3D 
building roofs reconstructed from multi-data sources and to 
assess the precisions of roof corners. To this end, conjugate 
3D coordinates of roof corners from all sources are first of all 
identified and weighted averaging process by least-squares 
adjustment is employed to estimate the refined corners. The 
weights of coordinate observations are determined purely 
2009 Urban Remote Sensing Joint Event 
978-1-4244-3461-9/09/$25.00 ©2009 IEEE
Figure 14. Aerial image of the test area 
Figure 15. 2D map of the test area 
Figure 16.3D view of building roof models (LIDAR system) 
Fig 17 shows the reconstruction results for the whole test 
area after CSR approach. 
Figure 17. The building roof models after refinement 
The following case studies demonstrate the advantages by 
fusing multi-data sources: 
y Up-to-date:  If there exist changes or dissimilarities 
among LIDAR, image and 2D map, the result of building 
roof reconstruction performed under this study would 
suggest the most updated models through CSR approach. 
Figs 18 (a) and (b) depict the updated effect. The 
building model from LIDAR data is mistakenly 
reconstructed because of rougher data resolution, 
however, correct roof patterns are concluded as the result 
of fusion.  
y                    
y                                       (a)
(b)                                                         (c) 
Figure 18. Updating the building roof model through fusion
                                  (a)  building seen from aerial image.  
(b)  building modelling from LIDAR data. 
                                          (c)  building modelling upon fusion. 
y Enhancement of Reliability: Sometimes due to the 
inaccuracy of LIDAR data, the result of reconstruction 
may seem unsatisfied, as shown in Fig 19(a). A user 
interface would provide visual inspections for 
determining the most proper employment of line features 
on images, as shown in Fig 19(b).  
              (a)                                                                (b) 
Figure 19. Enhancing the building roof models 
    (a) a building roof outline acquired from LIDAR . 
    (b) a building roof outline refined by adding imagery. 
y Improvement of Theoretical Accuracy: In this 
study, we designated four check points, as numbered in 
Fig (13), and presented their standard deviations of 
coordinates as the indicators for accuracy assessment. 
The standard deviations of the corners are obtained by 
calculating the square roots of diagonal elements of 
variance-covariance matrix of estimated coordinates 
2009 Urban Remote Sensing Joint Event 
978-1-4244-3461-9/09/$25.00 ©2009 IEEE
REFERENCES
[1] T. Schenk, and B. Csatho, “ Fusion of LIDAR Data and Aerial Imagery 
for a More Complete Surface Description,” IAPRS, vol. XXXIII, 2002, 
pp. 310-317. 
[2] S. Seo, “Model-Based Automatic Building Extraction From LIDAR And 
Aerial Imagery,” PhD dissertation, Department of Civil and 
Environmental Engineering and Geodetic Science, The Ohio State 
University, Columbus, OH, USA, 2003, 139p. 
[3] F. Rottensteiner, and J. Jansa, “Automatic Extraction of Buildings from 
LIDAR Data and Aerial Images,” In: Proceedings of the ISPRS 
Commission IV Symposium in Ottawa, vol. XXXIV / 4, 2002, pp.569-
574. 
[4] R. Ma, “Building Model Reconstruction From LIDAR Data and Aerial 
Photographs,” PhD dissertation, Department of  Civil and 
Environmental Engineering and Geodetic Science, The Ohio State 
University, Columbus, OH, USA, 2004, 166p. 
[5] L.C. Chen, T.A. Teo, C.Y. Kuo, and J.Y. Rau, “Shaping Polyhedral 
Buildings by the Fusion of Vector Maps and LIDAR Point Clouds,” 
Photogrammetric Engineering and Remote Sensing, 74(5), 2008, 
pp.1147-1157. 
[6] N. Haala, and C. Brenner, “Virtual City Models from Laser Altimeter 
and 2D Map Data,” Photogrammetric Engineering & Remote Sensing,
65(7), 1999, pp.787–795. 
[7] L.C. Chen, and T.Y. Li, “Intergrating Multi-Source Data for the 
Generation of 3D Building Models,” Journal of photogrammetry and 
Remote Sensing(in Chinese), 13(3), 2008, pp.159-168.
[8] J.J. Jaw, and C.C. Cheng, “Building Roof Reconstruction by Using CSR 
Approach,” Journal of photogrammetry and Remote Sensing(in 
Chinese), 12(4), 2007, pp.457-478.
[9] J.J. Jaw, J.J., and C.C. Cheng, “Fusion of 3D Line Features and 2D 
Image Line Features for Building Roof Reconstruction,” CD-ROM 
Proceedings of the 27th Asian Conference on Remote Sensing, Kuala 
Lumpur, Malaysia, 2007. 
[10] J.J. Jaw, and C.C. Cheng, “Fusion of 3D Line Features and 2D Image 
Line Features for Building Roof Reconstruction,” Remote Sensing 
Symposium accross Taiwan Strait(in Chinese), on CD-Rom, 2007. 
