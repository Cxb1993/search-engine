)|()...|()|()()( 11
2
13
1
1211
−= mmn wwPwwPwwPwPwP
).|()(
2
1
11∏
=
−=
m
i
i
i wwPwP
有效改善語言模型中平滑方法的研究 
 
中文摘要 
     在本計劃中，我們研究平滑方法中著名的 Good-
Turing 法及其相關問題，我們提出一種以實際語料為
基礎的平滑方法，此方法可以反應出平滑方法的實際
統計上的行為。 
      相較於其它以直覺、隋機方式的機率分配方式，本
方法依據實際語料為主，我們對此方法進一步分析其
統計行為，亦比較其與 Good-Turing 法。最後，找出
不同語料下較好之 cut-off k 值。 
Abstract    
In this project, we study the well-known Good-
Turing smoothing technique, which has inherited issue and 
propose a novel method. The smoothing method is used to 
resolve the zero count problem in tranditional language 
models. We further proposed a novel smoothing methods 
based on real training data sets, which will reflect the 
physical behavior of smoothing method.  
With respect to the heuristic probability assignment 
used by most smoothing methods, our method is based on 
real training corpora. The curves of number for events 
with different counts can be plotted. The behavior of our 
method is analyzed. The empirical results are presented to 
compare the effectiveness of smoothing methods. 
Keywords: Language model, Smoothing method, Good-
Turing, Cross entropy. 
1  Introduction 
Stochastic language models (LMs) [1], [4] have been 
successively used in many fields of application; speech 
recognition, word segmentation, task prediction, and so 
on.and. In natural language processing (NLP), LMs can be 
used, for instance, to decide the correct target word 
sequence W. The conditional probability P(W), where 
W=w1w2w…wm is a possible translation of Str.                                                            
Chain rule of probability is used to decompose the 
probability calculation:    
                                                                                (1) 
 
In many applications, the models for n=1, 2 and 3 are 
called unigram, bigram and trigram models [1], [4], [6] 
and [11], respectively.   
      In Eq. (1), the probability for each event or token can 
be obtained by training the bigram model (for clarity, 
bigram model is illustrated). Therefore the probability of a 
word bigram will be written as: 
                               ,                                                          (2) 
 
where C(wi) is the count of word wi occurred in training 
corpus.   
As shown in Eq. (2), C( ) of event or word (in the 
paper) may be zero because of the limited training data 
and infinite resource language. It is hard for us us to 
collect large enough data.  The potential issue of MLE is 
that the probability for unseen events is exactly zero. This 
is so-called the zero-count problem . It is obvious zero 
count will leads to the zero probability of P(‧) in Eq. (2). 
Furthermore, it will degrade the performance of LMs.  
1.1 Smoothing Methods 
      The estimation of zero probability of certain event or 
object is unreliable and unfeasible for most applications, 
especially for language models. The smoothing techniques 
[2], [3], [12] and [13], are essential and employed by 
language mode to overcome the problems of traditional 
language models, as described above.  
There are many smoothing methods, such as Add-1, 
deleted interpolation [7], Good-Turing[5], Katz [8], etc. 
Among the smoothing methods, Good-Turing has been 
used extensively, used to decipher the German Enigma 
code during War II by I. J. Good and A. M. Turing [5]. 
There are several literatures discussing about smoothing 
methods, refrerring to papers of [2], [3], [9], [10], [11] [14] 
and [15].  We aim at the feature analysis on Good-Turing 
smoothing methodfor several language models in 
Mandarin text. 
1.2 Processes of Smoothing Methods  
The main idea of smoothing is to adjust the total 
probability of seen events to that of unseen events, leaving 
some probability mass (so-called escape probability, Pesc), 
for unseen events. Smoothing algorithms can be 
considered as discounting some counts of seen events in 
order to obtain the escape probability Pesc which will be 
assigned into the zero counts of unseen events. The 
adjustment of smoothed probability for all possibly 
occurred events involves discounting and redistributing 
processes: 
2 Good-Turing smoothing 
2.1 Basic Idea 
Good-Turing method was first described by I. J. 
Good and A. M. Turing in 1953 [6], which was used to 
decipher the German Enigma code during World War II. 
Some previous works are in [3], [7] and [12]. Notation nc 
denotes the number of n-grams with exactly c count in the 
corpus. For example, n0 represent that the number of n-
grams with zero count and n1 means the number of n-
grams which exactly occur once in training data. 
Therefore, nc will be described as: 
∑ −−− = w i
ii
ii wwC
wwC
wwP
)(
)(
)|(
1
1
1
,*,1
'
1 Nkkkk Qnnn ⋅+= ++
.
0
'
00*
,0 n
nn
Q N
−=
1
0
'
00
0
'
11
1
*
,00
'
11*
,1 n
n
nnnnn
n
Qnnn
Q NN
−⋅+−
=⋅+−=
1
'
00
'
11 )()(
n
nnnn −+−=
2
'
00
'
11
'
22*
,2
)()()(
n
nnnnnn
Q N
−+−+−=
.
)(
2
2
0
'
n
nn
i
ii∑
=
−
=
 
 
 
 
 
 
 
 
 
 
Fig. 2: the number nc of Mandarin char. bigrams with 
different count c on various training size N. up) for 
n0 of unseen events, down) for nc of seen events 
with count c from 1 to 12. 
Notations nc and 
'
cn are defined as numbers of 
bigrams b on data size N and N+1, respectively. Therefore, 
the numbers 'cn (c>=0) of bigrams with count c can be 
written as follows:   
                                                                                                          
(7) 
where *,00 NQn ⋅  is the number of incoming unseen bigrams, 
which is added to the number '1n of bigrams with count 1 
on training data N+1.  
The item *,00 NQn ⋅  in Eq. (7) is considered as the 
expected number of incoming unseen bigrams, which are 
originally with count 0 and will be with count 1. Therefore, 
it will be added to the number of bigrams with count 1. 
The number '1n  will be written as follows: 
                                          (8) 
  
where *,11 NQn ⋅  is the expected number of incoming seen 
bigrams with count 1, which is added to the number of 
bigrams with count 2. Similarly, '2n can be computed as 
follow :                                                                                         
.*,22
*
,112
'
2 NN QnQnnn ⋅−⋅+=                                            (9) 
As Shown in Eqs. (7)~(9), the general format for 
items 'cn can be written as follows: 
                                    (10) 
  
The numbers of bigrams with count k and k+1 on 
training data N and N+1 are written, respectively, as 
follows: 
,*,
*
,11
'
NkkNkkkk QnQnnn ⋅−⋅+= −−                                  (11) 
                                                               
(12) 
where nk+1 is zero (because the k is the max. count of seen 
bigrams occurred in corpus, all the number of bigrams 
with count larger than k are zero on training data N). 
Referring to Eq. (7), the smoothed probability for an 
unseen bigram can be written as follows: 
                                                                                       (13) 
 
  
  
                                                                   (14) 
 Similarly, we can calculate the smoothed 
probability *,2 NQ  from Eq.(9) as follows: 
 
                                                 
                                                                      (15)  
It is obvious that all the numbers of nc and n’ can be 
obtained from the curves in Figure 1 on various N, all the 
smoothed probabilities are also computed, in Eq. 
(13)~(15). As presented above, all other smoothed 
probability can be derived.  
We have proven that the total smoothed probability 
for possible events summed up to unity. For the limitation 
of paper size, the procedure of derivation is omitted.  
4  Evaluation  
4.1 Cross Entropy  
In the subsection, we introduce cross entropy, which 
has always been used to evaluate and compare different 
probabilistic model. For a testing data set T which 
contains a set of events, e1,e2,…,em, the probability for the 
testing set P(T) can be described as: 
 
                                                                                 (16) 
 
where m denotes the number of events in testing set T and 
P(ei) denotes the probability of event ei, obtained from n-
gram language model, assigning to event ei. 
     When we don’t know the actual probability distribution 
p that generated some data the cross entropy CH can be 
employed. For example, we use some M, which is a model 
of p. Therefore, the cross entropy CH of M on p can be 
regarded as: 
 
.                                                                             (17) 
 
The cross entropy CH is an upper bound on the 
entropy H(p). For a model M; H(p)<=CH(p,m). It means 
that some simplified model M can be used to estimate the 
true probability of a sequence of events based on the 
probability p. Between two models M1 and M2, the model 
with more accurate prediction is the one with lower CH.  
,)()(
1
∏
=
=
m
i
iePTP
)...(log)...(1lim),( 321321 n
LW
nn
wwwwMwwwwp
n
MpCH ∑
∈∞→
=
,*,000
'
0 NQnnn ⋅−=
,*,11
*
,001
'
1 NN QnQnnn ⋅−⋅+=
.*,
*
,11
'
NccNcccc QnQnnn ⋅−⋅+= −−
number nc  of count c for Mandarin bigrams on various N
0
50000
100000
150000
200000
250000
300000
350000
400000
1 2 3 4 5 6 7 8 9 10 11 12
training size N, M
see
n t
yp
es 
S
n1
n2
n3
n4
n5
n6
n7
n8
n9
n10
n11
n12
 Unseen type U of Mandarin char. Bigrams on various N
167800000
168000000
168200000
168400000
168600000
168800000
169000000
1 2 3 4 5 6 7 8 9 10 11 12
training size N, M
Un
se
en
 ty
pe
 U
n0
