???????????????? 
????????????????????????????? ICP(Iterative Closest 
Point)?(Besl & McKay,1992)????(???)???(Gruen & Akca,2004)???????
??(Akca,2003)??????????????(????????2002)????????
??????????????? ??????? Habib et al.(2003)????????
MIHT(Modified Iterated Hough Transform)??????????????????????
??; Shaker(2004)????????? LBTM(Line Based Transformation Model)????
???????Habib et al.(2005)?????????????????????????
???????????????????????????????????????
?????????????? 
?
?????? 
??????????????? 
(1?????????????? Optech??? ILRIS-3D????????????
800M(???? 10mm)????? IFOV 0.30???(Footprint)?? D = 2Rtan(IFOV/2)R?
????)???????? FOV? 400???????? 8~12mm(???,2004)? 
?????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
??????????????????????????????????????? 
??????????????????????????????????????
???????????????????????????????????????
???????????(? 10mm)????????????????????????
????????????????? 1? 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(2??????????????????????????????????????? 
??????????????????????????????????????? 
? 1????????????
????????????????????????????????(Rigid Body)? 
??????????????????????????????????????? 
??????????????????????????????????????? 
?????????????????????(???)?????????????? 
?????????? S????????ω?φ?κ ?????? XT ? YT ? ZT ???? 
????? 7????
 
? 7????????????????????? 
???????????? 8 ????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
??????????????????????????(Jaw and Chuang, 2007)? 
 
? 8??????????? 
?????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
? 9??? 
 
? 9??????????????? 
?????????????????????????????????????
??????????(Triplet-wise strategy)????????????????????
???????????????????????????????????????
???????????????????????????????????????
????????????(Jaw and Chuang, 2005)? 
 ? 11?CSI????????????? 
a. ??(Construct)????????????????????????????? 
?: 
(i).??????????? 
??????????????????????????????????
????????????????????????????????????
???????????? 12(a)?????????????????????
?????“Target line”????????? “Searching line”??? “Target line” ?
????????????????????????????????????
???????? 
???????????“Target line”???????????????“Target 
line”??????(????????)??“Target line”????????????
????????? ???????????????? ?????????  
“Searching line”?????????????????????“Searching line”?
???????????“Searching line”??????????????????
12(b)??????????????????????????????????
???????(?????)?????”Target line”?”Searching line”?????
????????????(????2006)?? 12(d)?? 12(a)????????
????????????????????????????????????
????????? 
 
(ii).??????????????????? 
??????????????????????????????????
????????????????????????(Cheng and Jaw, 2006)? 
(iii).???????????????????? 
??????????????????????????????? 13 ?
???????????(??????????)??????????????
????????????????????????????????????
???????????????????????????  
No.1 
No.2 
???????
(LiDAR?3D map) 
 
?? 
(Improve) 
?????
???? 
???
(Construct) 
?????? ??????? ??????? 
image 
???
(Shape) 
?        
    ?     (a)              (b)       ?     (c)??????? (d) 
?????????     ? 12??????????? 
?????? 1???????????????????????? 15?? 30o???
?? 0.6 o?40m????????(????????????????????????
????????????)? 
?????????????????????????????????????
???????????????????????? (Briese, 2004)?????????
???????????????????????????????????????
???????????????????????????????????????
?????????????? ????????????????????????
???????????????????????????????????????
????????????? 
? 1?????????? 
???o?? ?????o?? ???
10 o 0.2 o 20m 
20 o 0.4 o 30m 
30 o 0.6 o 40m 
40 o 0.8 o 50m 
50 o 1.0 o 60m 
 
 
 
 
 
 
 
 
 
 
 
?????????????????? 2?3???????? 16~18?? 2??? 
?????????Z10?Z20? Z30(?? 10o?20o? 30o)? DX?? DY???? DZ??
???????????????????????????????????????
?????????????????????????????( mm6.13± )??????
???????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????(??? 17? 18)?????????????????
????????????? 
?????????????????????????????????????
???????????????????????????????????(???
???????????)?????????????????? 
 
 
 
??? 15???? 30o????? 0.6 o????? 40m
85
90
95
100
105
90
95
100
105
110
40
60
80
85 90 95 100 105 110
90
95
100
105
110
?????
?????
?????
?????
?????
?????
?????
?????
?????
??? ??? ??? ??? ???
? ???
????
????
 
? 17???????????????????  
?????
?????
?????
?????
?????
?????
?????
?????
??? ??? ??? ??? ???
? ???
????
????
 
? 18???????????????????      
(2)????? 
????????????????????????????????????: 
? 19???? ILRIS-3D(?????? 0 8 ~ 12mmσ = ± ??????? ?????????
?? ??????????????????????????????? 4???????
????(?????????????????????????????????
?)?? 4??? 19(f)??????????(? 1~2mm)???????????????
???????????????????????????????????????
????????????????????????????? 
 
 
 
                 ? 19??????????????? 
 
 
 
(d) ?????? 
 
(e) ????? 
 
(f) ??????? 
(a) ???? 
 
(b) ???? (c) ?????? 
?
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
(3)????? 
?????????????????????????????????????
???????????????????????????????????????
???????????????????????? Optech ? ILRIS-3D ??????
??????? 30~100m????????????????? A??? G??????
?????????????????????????????????? A?B?C?
D?E?F?G???????????(open loop)????
? 24 ????????????????????????????????????
?????????????????????????????????????????
????????????????????????????? 
?
(a) ?? A          (b) ?? B              (c) ?? C            (d) ?? D 
? ?
(e) ?? E               (f) ?? F                (g) ?? G 
? 24?????????? 
? ??????????????????
? ????? ????? ???
B1B2 2.9875 2.9864 +0.0011 
D1D2 2.9875 2.9849 +0.0026 
F1F2 2.9875 2.9862 +0.0013 
B1D1 0.3466 0.3440 +0.0026 
B1F1 0.6927 0.6889 +0.0038 
D1F1 0.3461 0.3484 -0.0023 
B2D2 0.3302 0.3312 -0.0010 
B2F2 0.6812 0.6833 -0.0021 
D2F2 0.3511 0.3531 -0.0020 
? 21???????????? ???? ?? ? 22???????? 
A1 
B1 
C1 
D1 
E1 
F1 
B2 
C2 
D2 
E2 
F2 
? 23?????????????
? 25 ?(e)?(f)?(g)????????????????????????????
???????????????? 
(4)????? 
?????????????????????????????????????
???????????????????????????????????????
???????????????????????????????????????
???????????????? ???????????????? (??????
?)???????????????????-?????????????????? 
a. ???-???????????????????  
???????????????????????????????????
??????????????? 21 ?????????????????????
????????????????????????????? L????????
???????? 26????????? 26(c)??????????????? 26(a)
??????? 26(b)???????????? DSM??????????????
26(d)????????????????????????????????????
8 ???? 3 ???????????????????????????(????
?)? 
 
??????????? 42???????? 27(a)??????????? 27(b)
?????????????? 27(c)??????????????????? 27(d)
??????????????????????????????????????
?????????????????????? 
           
                (a)                               (b) 
         
                 (c)                                 (d) 
? 26?????????????????????? 
 ????????????-??-??(Construct-Shape-Improve, CSI)??????
??????????????????????????????????????
???(?????????????)?????????????-????????
??????????????????????????????????????
?????????????(???????????)?????????????
??????????????????????????????????????
?????????????????????????? 
???????
????????? (1).??????????????????????????
????????(2).??????????????????????????????
??(3).???????????????????????????????????
??(4).?????????????????????????????????????
?????
?????????????????????????????????????
????????????????????????????????????????
??????????????????????????????
 
???? 
Akca, D., 2003. Full Automatic Registration of Laser Scanner Point Clouds, In: Gruen, A., 
Kahmen, H. (Eds.), Optical 3D Measurement Techniques VI, pp.330-337. 
Ameri, B., 2000. Automatic Recognition and 3D Reconstruction of Buildings through 
Computer Vision and Digital Photogrammetry, Deutsche Geodätische Kommission, Reihe 
C, Nr. 526, München. 
Ayache, N., and Faugeras, O., 1989. Maintaining Representations of the Environment of a  
Mobile Robot, IEEE Trans. Robotics Automation, 5(6): 804-819. 
Besl, P.J. and N.D. McKay, 1992. A method for registration of 3D shape, IEEE Trans. Pattern 
Analysis and Machine Intelligence, 14(2):239-256. 
Boyer, K.L. and S. Sarkar, 1999. Perceptual organization in computer vision: status, challenges, 
and potential, Computer Vision and Image Understanding , 76(1):1-5.  
Brenner, C., 2005. Building reconstruction from images and laser scanning, International Journal  
of Applied Earth Observation and Geoinformation, 6(3-4):187-198. 
Briese, C., 2004. Breakline Modelling from Airborne Laser Scanner Data, Ph.D.  
Dissertation, Institute of Photogrammetry and Remote Sensing, Vienna University of  
Technology, Austria, 67p. 
Cheng, C. C and J.J Jaw, 2006. 3D Structure Line-based Building Roof Reconstruction,  
Proceedings of 27th Asian Conference on Remote Sensing, Oct. 9-13, Ulaanbaatar,  
Mongolia, CD-ROM. [??????????????] 
Filin, S., and Norbert, P., 2006. Segmentation of airborne laser scanning data using a slope  
adaptive neighborhood, ISPRS Journal of Photogrammetry & Remote Sensing 60 pp.71-80. 
Gruen, A. and D. Akca, 2004. Least Squares 3D Surface Matching, International Archives of the 
Photogrammetry, Remote Sensing and Spatial Information Sciences, Vol. 34, Part 5/W16. 
?????? 
????2004????????????????????????????????? 
???????? 
????????2005????????????????????????????? 
??????????????????pp.373-380?[???????????? 
??] 
????2006?95??????????????????????????????? 
??? 
????2006???????????????????????????????(I)?
????????????????????: NSC93-2211-E-002-054? 
 
 
 
 
 
?????? 
 
?????????????????????????????????????
????????????????????????????????????????
???????????????????????????????????????
???????????????: 
 
Title Journal Status 
Registration of LiDAR Point Clouds by means of 3D Line 
Features 
JCIE In Review 
Feature Extraction from LiDAR Point Cloud and Accuracy 
Assessment 
JCIE In Preparation 
CSI-based Building Roof Reconstruction PE&RS In Preparation 
 
AUTOMATIC REGISTRATION OF GROUND-BASED LIDAR DATA SETS  
BY USING 3D LINE FEATURES 
 
Jen-Jer Jaw1   Tzu-Yi Chuang 2  
Assistant Professor1   Ph.D Student 2  
Department of Civil Engineering, National Taiwan University 
1 Roosevelt Rd. Sec. 4, Taipei,10617, Taiwan 
Tel:886-2-23678645   Fax:886-2-23631558 
E-mail: jejaw@ntu.edu.tw1 yandevin@gmail.com 2  
 
 
KEY WORDS: 3D Line Features, 3D Line Extraction, 3D Line Matching, Registration 
 
ABSTRACT: In this paper the authors show how to extract the 3D line features from  
ground-based LiDAR data sets and develop the algorithms in which 3D line feature matching 
for registering overlapping scenes can be established by means of geometric constraints in an 
automatic course. The proposed working scheme consists of three major kernels including 3D 
line feature extraction, 3D line feature matching and the registration of ground-based LiDAR 
data sets. 3D line features are extracted by means of image process techniques and then input 
into the matching engine where geometric constraints are imposed and conjugated line features 
among stations are picked. The registration of consecutive overlapping data sets is performed 
through the two-step model of 3D spatial similarity transformation. The whole processing chains 
in this study are featured in a highly automatic fashion. The conducted experiments show that 
the proposed method seems to be the alternative or the complementary for LiDAR data sets 
registration apart from the point, surface and other features that are often realized as feature 
primitives for LiDAR data.  
1. INTRODUCTION 
To gain a complete scene by means of ground-based LiDAR systems, registering all the data 
sets generated from different stations onto the common coordinate system becomes an essential 
requirement. Among various alternatives, point-based approaches for establishing the 
transformation between data sets have been widely accepted. While in this study, the authors 
intend to perform the LiDAR point clouds registration task by 3D line features. The related 
work for employing linear features can be found in Shaker (2004) who applied LBTM (Line 
Based Transformation Model) for spatial similarity transformation by using line features in 
which the transformation is performed in a way from 3D to 2D fashion and Habib et al. (2005) 
who solved image orientation employing line features as control data and thereafter registered 
photogrammetric 3D lines with LiDAR 3D lines, just to name a few. 3D line features, are 
excellent primitives for feature matching and registering overlapping data sets by transforming 
one data set onto another. The strengths of matching conjugated 3D line features lie in the fact 
lower weights. The 3D line feature extraction model proposed in this study is illustrated in 
Figure 1. 
 
 Figure 1. The flowchart of 3D line feature extraction 
2.2  3D Line Feature Matching among Multiple Data Sets 
The 3D line feature matching model among multiple data sets can be established in a 
triplet-wise strategy (Jaw and Chuang, 2005), illustrated as in Figure 2. The central data set in 
the triplet is regarded as the reference data and then the left and right data sets, or simply the 
other two data sets, are matched simultaneously with the central data using the same matching 
approaches as in (Jaw and Chuang, 2005). The main consideration of designing triplet matching 
strategy is to include matching occurrence for the data sets that are situated on both sides of the 
central data set. Note that the transformation after the result of angle check (as seen in Figure 2) 
for each triplet is applied to all line features, instead of only candidate line features as in pair 
matching case, so that the line feature correspondences of left and right data sets, missing in the 
angle check may be compensated and added to the matching mates upon performing distance 
check. Surely enough, the consideration of employing triplet matching strategy is only authors’ 
choice for improving matching reliability based on implemental efficiency, other alternatives are 
also the possibilities. The matching strategy applies to each triplet and the matching results are 
combined for estimating transformation parameters. Upon all the matching mates are collected, 
the transformation can be performed by picking one, the most central one for the convenience, 
of the data sets as datum upon which all other data sets are registered as described in the 
following section. The flowchart of 3D line feature matching among multiple data sets can be 
seen in Figure 2. 
 
Figure 2. The flowchart of 3D line feature matching among multiple data 
2.3  Registration of Consecutive Overlapping Data Sets via 3D Line Features 
Table 1. The metadata of test fields 
Station A B C D E 
Number of points 322862 288125 206682 29821 50065 
Point density (pt./ 2m ) 157.816 196.087 110.655 54.810 89.716 
Scene distance (m) 29.489 29.144 29.597 17.262 13.216 
 
The range images of the LiDAR point clouds are given in Figure 4, while those 
superimposed lines indicate the approximate 3D line features that are extracted by Canny edge 
detection and Hough transformation operators. The refinement of 3D line features are followed 
and completed. 
 
 
(a). station A     (b). station B      (c). station C     (d). station D    (e). station E  
Figure 4. Results of approximate 3D line feature extraction. 
 
The conjugate line features upon the angle check are transformed onto the same reference 
coordinate system, and the spatial distance (shown in Table 2) of conjugate line features are 
calculated and filtered if exceeding the distance threshold. Through the 3D line feature matching, 
the proper matching mates are found. 
 
Table 2. The spatial distances of proper matching mates (unit: meter) 
Station A - B Distance  Station B - C Distance  Station C - D Distance  Station D - E Distance 
Line 1 0.018 Line 1 0.034 Line 1 0.036 Line 1 0.028 
Line 2 0.023 Line 2 0.031 Line 2 0.024 Line 2 0.035 
Line 3 0.014 Line 3 0.019 Line 3 0.018 Line 3 0.032 
Line 4 0.043 Line 4 0.028 Line 4 0.035 Line 4 0.043 
Line 5 0.022 Line 5 0.033 Line 5 0.021 Line 5 0.032 
Line 6 0.020 Line 6 0.025 Line 6 0.034 Line 6 0.022 
Line 7  Line 7 0.053 Line 7 0.024 Line 7  
Line 8  Line 8 0.023 Line 8  Line 8  
 
The matching mates are used to solve transformation parameters applying hybrid model of 
3D line feature similarity transformation (Jaw and Chuang, 2005). The internal accuracies after 
registering five successive LiDAR point cloud data sets are calculated and listed in Table 3. 
Table 3. Internal accuracy (unit: meter) 
Station A-B Station B-C Station C-D Station D-E 
0.030 0.031 0.027 0.032 
Apart from the internal accuracy that evaluates the registration performance, visual check on 
the registered scene can be found in Figure 5. 
3D STRUCTURE LINE-BASED BUILDING ROOF RECONSTRUCTION 
 
Chieh-Chung Cheng 1  Jen-Jer Jaw 2  
       Ph.D Student 1    Assistant Professor 2     
Department of Civil Engineering, National Taiwan University 
1 Roosevelt Rd. Sec. 4, Taipei,10617, Taiwan 
Tel:886-2-23678645   Fax:886-2-23631558 
E-mail: r92521124@ntu.edu.tw 1  jejaw@ntu.edu.tw 2  
 
 
KEY WORDS: 3D Structure Lines, Building Roof Reconstruction, Geometric Inferences,  
Topological Relationship 
 
ABSTRACT: 3D structure lines are geometric evidences of building roofs and can be resulted 
from photogrammetric mapping process, feature extraction of LiDAR data sets or other sources. 
Hence, 3D structure lines by nature are advantageous for inferring to building roof 
reconstruction. In this study, the authors developed a hierarchical algorithm of building roof 
reconstruction by employing 3D structure lines. The proposed algorithm starts by inputting 3D 
structure lines and runs the following geometric inferences: (1) computation of line intersections 
on the X-Y plane; (2) topological relationship check and establishment of 3D structure lines that 
belong to the same building; (3) completion and adjustment of the building roof including 
compensating missing parts. The primary experiment demonstrates that the proposed method is 
independent of building roof type as well as whether 3D structure lines are complete for both 
successful and efficient performance of building roof reconstruction on 3D line basis. 
 
1. INTRODUCTION 
 
Digital 3D building models are useful in true orthophoto generation (Rau and Chen, 2003), map 
revision, urban planning, etc. Automating data acquisition for 3D city models is an important 
research topic in photogrammetry. Line features, of higher-order information and easier detected 
than point features, are the main evidences for building hypotheses and reconstruction via 
photogrammetric approaches. In general, the strategy for modeling buildings can further be 
categorized into three classes according to the data flow of the algorithm. These are: the 
bottom-up approach, the top-down approach, and the hybrid approach (Faig and Widmer, 2000). 
Suveg and Vosselman(2004) developed a model-based approach to generate 3D building models 
from aerial images. The approach integrates the aerial image analysis with information from GIS 
database and domain knowledge. The building reconstruction process was described as a search 
tree and the developed system has been used in urban and suburban areas to reconstruct 
buildings and showed good results. Rau and Chen(2003) proposed a Split-Merge-Shape(SMS) 
are not parallel, they will intersect. Therefore, many junction points, including real corner points 
and virtual ones, will be produced as a result of intersection calculation for any potential line 
pairs.  
(b). Determination of proper junction points: Based on the afore-mentioned principle 
regarding the determination of junction points, determining the proper junction points can be 
approached as follows:  
First Step: One 3D structure line is chosen and regarded as “Target line”, and the rest of the lines 
are called “Searching line”. If the nearest junction point of any end of “Target line” is also the 
nearest junction point of “Searching line”, then this junction point is to be confirmed, as shown 
in Figure3(a) where the red dot point indicates the junction point while the blue line is the 
“Target line” and the black line, the “Searching line”. Furthermore, if the nearest junction point 
of any end of “Target line” is inside the “Searching line”, then this junction point is also 
confirmed (Figure3(b)). 
   
         (a)                        (b)       
Figure 3. First step of intersection               Figure 4. Second step of deletion 
Second step: If any end of “Target line” has identified the junction point, the other junction 
points on this side of the line should be deleted. In Figure4, the green point will be deleted 
because the “Target line” has already identified the most proper junction point. 
Third step (Inner distance check): Given one junction point inside “Target line”, if the distance 
from the junction points to the end point exceeds the distance buffer (the prediction area of 
potential junction points when considering data uncertainty) of “Target line”, then the junction 
point will be deleted. The meaning is that the inner junction point will not be identified as 
proper intersection, thus the “Target line” needs to be extended to find the proper junction point. 
In Figure 5, the red point will be deleted, because it exceeds the distance buffer of endpoint 
(gray point). The line on the gray point side should be further extended and search for the 
nearest junction point outside the “Target line”.  
Fourth step (Outer distance check), the distances between the following junction points and the 
first found junction point of “Target line” and “Searching line” will be calculated. Those 
distance values exceed the distance buffer (by error propagation) should be removed from 
further consideration. For instance, in Figure 6 (a), the red point will be reserved, because the 
distance is within the buffer of nearest junction point (green point) of “Searching line”. On the 
other hand, the red point in Figure 6 (b) will be deleted and the “Target line” will go on to check 
the next junction point, such as the purple point. Furthermore, the distance between the yellow 
point and the red point exceeds the distance buffer, so the “Target line” will not check on 
whether the yellow junction point should be considered or not.  
2L  1L
2L
1L
2L  1L  
can be described by two plane equations, there are at least four independent parameters of one 
3D line. The roof corners are regarded as the unknown parameter and the observations are the 
parameters of 3D lines. Equation1 is the observed equation. 
1111 0 ×××××× =−+ ccuucnnc WAeB ξ ? ),0(~ 120 −=∑ Pe σ                            (1) 
Where A matrix is obtained by the differentiation of unknown parameters; B matrix is obtained 
by the differentiation of observations; W matrix is the weight matrix from observations. ξ  are 
the unknown parameters. 
 
2.5 Compensation of Missing Parts 
 
Sometimes, due to the hiding situation, the roof corner and the roof edge may be invisible. The 
algorithm will compensate the hiding edges and roof corners by forecasting the higher edge of 
the neighboring building.  
 
3. CASE STUDY   
 
A test LiDAR data from Hsinchu, Taiwan was used to test the proposed algorithm. The 3D 
structure lines were extracted from LiDAR data sets by using the edge detection on range image. 
Figure8 (a) illustrates the aerial image of test area; (b) illustrates 3D structure lines extracted 
from DSM (black line); (c) illustrates the 3D structure lines of building projected onto plane. 
There are nine buildings in this test area, including nine flat roofs, two gable roofs and one 
hipped roof. The building of hipped roof is the virtual building roof (No.9) for testing the 
flexibility of the proposed algorithm. The prior accuracy of (X, Y, Z) in LiDAR data sets are 
( mmm 2.0,5.0,5.0 ±±± ).  
              
       (a)                        (b)                         (c)  
Figure 8. Test area and 3D structure lines 
Figure9 (a) illustrates the result of acquiring the proper junction points. Figure9 (b) illustrates 
the result of extraction of 3D structure lines that belong to the same building. In the final step of 
classification, the 3D structure lines of twelve building roofs are categorized into twelve groups. 
The structure lines belong to the twelve groups will be used for building roof reconstruction 
