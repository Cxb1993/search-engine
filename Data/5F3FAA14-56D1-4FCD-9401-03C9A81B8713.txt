 行政院國家科學委員會補助專題研究計畫
 □ 成 果 報 告  
□期中進度報告 
 
植基於視覺的人體/手部動作參數擷取系統 (2/2) 
 
計畫類別：□個別型計畫  □整合型計畫 
計畫編號：NSC 98 － 2221 － E007 － 090 － MY2 
執行期間： 99 年 8 月 1 日至 100  年 7 月 31 日 
 
計畫主持人：黃仲陵教授 
共同主持人： 
計畫參與人員： 曹益鐘、黃本軒、謝宗育、徐士中 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立清華大學電機系 
 
中   華   民   國  100  年  11   月   18   日 
 
附件一 
The motion state is recovered from 3-D configuration 
with the maximum similarity. 
In this report, we develop a multi-view-based 3-D 
hand pose analysis system to track the wrist rotation 
and finger bending movement, and reconstruct the 
corresponding 3-D hand in the virtual 3-D space. To 
estimate the movements, we develop a system which 
utilizes two different view angles (frontal view and side 
view) to overcome the self-occlusion problem. First, 
the wrist motion is analyzed with motion history image 
and the variation of fitted palm ellipse to determine 
whether the hand is in frontal view or side view. When 
the hand posed in frontal view, researches [2~4, 6, 7, 
11-~13] have been proposed to track the parameters of 
hand motion. Once the hand is rotated to a side view, 
the problem of occlusion among fingers will make the 
finger movement analysis difficult. Therefore, we 
employ different features including depth map, 
Chamfer distance and silhouette to deal with this issue. 
Due to the high degree of freedoms (DOFs) of a single 
hand, the high-dimensional problem is one of the 
concerns. Therefore, we propose the Separable State 
Based Particle Filter (SSBPF) to decrease the 
computation complexity.  
三、研究方法與成果  
A. Motion Constraint on Hand Model 
Construction of 3-D Hand Model 
Instead of using quadrics as shape primitives, 
cylinders, spheres and rectangles are the main elements 
to construct our 3-D hand model. The size of fingers 
and palm in the 3-D hand model are scaled proportional 
to the size of real human hand. The length and width of 
each patch should be calibrated for each individual 
person. Figure 1 shows the constructed 3-D hand model 
and the model contour onto the real hand. 
Motion Constraint 
Because of the high degree of freedom of hand 
motion (27 DOFs), we apply some reasonable moving 
constraints for the 3-D hand model to develop an 
effective 3-D hand gesture recognition system. In 
general, hand motion consists of wrist and finger 
movements. The wrist movement described with the 
transformation matrix MW consists of the 3-D 
translation matrix T and 3-D rotation matrix R where 
the fingers movement is described with the 
transformation matrix MF. The transformation matrices 
MW and MF are defined as: 
Translation:  -  3 DOF
: global information of hand
Rotation    :  -  3 DOF
: finger joint angles  of hand - 21 DOF
T
R
 

 


W
F
M
M
 
To reduce the DOF of 3-D hand model, the joint angles 
θmiddle and θfar (shown in Figure 2) are estimated 
according to the joint θnear with linear scaling [15]. In 
addition, Figure 2 illustrates the opening angles θopen 
between the fingers from two viewing angles. As a 
consequence, MW  and MF can be depicted as: 
( ){ }
( ){ }near middle far open
 : , | , ,    
 : , , , | 1 5
i i
i i i i
T R i x y z
iθ θ θ θ
 =

= 
W
F
M
M
 
Based on the state of hand motion described with 
the parameters of MW space and MF space, we can 
generate the 3-D hand model in the virtual environment 
and obtain the 2-D projection image of the 3-D hand 
model simultaneously. Here, the OpenGL[21] is used to 
generate the 2-D projection hand image for measuring 
the similarity between the 2-D projection hand image 
and the observed hand image. To improve the 
robustness and efficiency of the system, some 
constraints of hand motions are applied to reduce the 
estimation error caused by the finger occlusions. 
1) Structural Constraint. Some collocations of finger 
motions are not feasible, because two different fingers 
will never occupy the same physical 3-D space. To avoid 
this problem, the distance between two fingers’ axis, as 
shown in Figure 3, is used to detect the finger collision, 
which is calculated according to the angle θopen abducted 
within two fingers. 
The position of the ith finger, which can be 
estimated by using parameters, 
{ }near middle far open, , ,i i i iiF θ θ θ θ= , is applied to check the 
finger collision. Here, we use the function s
,i jψ to 
indicate the collision between the ith and jth fingers and 
it is defined as 
 
( )
i,j
1,  if , threshold
0,  otherwise
i js d F Fψ
 >= 

,      
where the threshold is determined by the finger width 
in the hand model. Then, by considering all the possible 
finger pairs, a prior function is defined as 
( ) i,j
,i j F
s
s
x x M
p ψ
∀ ∈
= ∏F .   
This constraint is used to describe the probability of 
finger collision while the hand joint configurations are 
physically unfeasible.  
2) Kinematics Constraint. Besides the structural 
constraint, there are some infeasible finger motions 
because of limitation of the finger movement. For 
instance, Figure 4 shows the hand gestures from two 
different view angles. When pinkie is bending, the ring 
finger is also moving due to the muscle constriction. 
More than 30,000 finger motion data obtained 
from DataGlove are collected to train the finger 
kinematics constraint information defined as 
( ) ( )| |i ji j near nearp F F p θ θ= ,   
where Fi and Fj are two different subset of MF, defined 
as { }, ,i i ii near middle farF θ θ θ= . Therefore, a prior function 
is represented as: 
( ) ki,j
,i j F
k
X X M
p ψ
∀ ∈
= ∏F ,   
where 
,
k
i jψ  is defined as  
,
1,  if ( | ) threshold
0,  otherwise
i jk
i j
p F F
ψ
≥
= 

. 
And F={Fi |i=1…5} represents the 5 different fingers. 
B. Separable State Based Particle Filter  
We propose a novel method called Separable State 
Based Particle filter (SSBPF) [25] which is a modified 
version of the Particle filters [22-24] to track the hand 
movement. Based-on the concept of divide-and-conquer, 
we employ a method to separate the high dimension 
image is used to represent the minimum distance from 
each edge pixel in the hand image to the hand template. 
The Chamfer score is derived by averaging the distance 
in the distance image.  
As a result, there are two hand silhouettes from the 
frontal and the side views which are used to generate 
the possible candidate hand models for tracking the 
hand postures. Based on the two observations obtained 
from two different view angles, we have two different 
observation likelihood functions as, frontal silh ( | )p Z X
and side DT ( | )p Z X , which are calculated by measuring 
the difference between the target image and the 
candidate model. The similarity between two binary 
hand silhouette images I1 and I2 with the same width 
and height is defined as 
( ) ( ) ( )silh 1 2 1 2, , ,
x y
D I I I x y I x y= ⊗∑∑ ,  (13) 
where ⊗  represents the XOR operation. Then, the 
observation likelihood can be defined as: 
 
( )
( )( ) 2silh silh
frontal silh
frontalfrontal
,1 1| exp
22
D Z C X
p Z X
σπσ
  − =      
,    (11) 
where Zsilh is the observation hand image. C(X) is the 
candidate hand model with given X.  
Similarly, the side likelihood function 
side DT ( | )p Z X  can be modeled by the Chamfer 
distance between the target image and the candidate 
model. Let the distance between the distance transform 
IDT and the candidate’s edge Iedge be defined as 
( ) ( ) ( )DT DT edge DT edge, , ,
x y
D I I I x y I x y= ⊕∑∑ (15) 
Therefore, the observation likelihood is defined as 
 
( )
( )( ) 2DT DT
side DT
DTDT
,1 1| exp
22
D Z C X
p Z X
σπσ
  − =      
,
      (16) 
where ZDT  is the observed hand image and C(X) is the 
candidate hand model with given X.  
E. Finger Occlusion 
To estimate the 3D hand parameters, the most 
serious problem is the finger occlusion. Here, we 
propose the joint angle correlation and the finger depth 
consistent measure to overcome the finger occlusion 
problem. 
1) Frontal-View-Based Observation probability 
Function. With the structural and kinematics 
constraints of ordinary human hand, the observation 
likelihood in the front view is written as  
( ) ( ) ( ) ( )frontal k silh s silh frontal silh| |p Z X p X p X p Z X= ⋅
.       (17) 
Where pk(Xsilh) and ps(Xsilh), the two constraint 
probability functions, which are used to solve the 
frontal view occlusion problem. Once occlusion occurs, 
two different hand postures may show the same 
silhouette.  
2) Side-View-Based Observation Probability 
Function. Here, we can employ the depth information 
to generate the observation likelihood in the side view 
as 
 
( ) ( ) ( )depth depthside depth side DT| | , |i jp Z X p Z X X p Z X= ⋅
      (18) 
where the likelihood function ( )depth depthdepth | ,i jp Z X X  is 
based on the correlation between two different finger 
joint (i.e., depthiX and depthjX ), which is proportional to 
the overlapped area between the real hand image and 
the depth image, 
 ( ) dep oridepth 2
 
| , exp
2
depth depth
i j
A A
p Z X X
σ
 
∝ − 
 

                
where Adep is the depth image by threshold processing 
and Aori is the original image in the region of Adep. Eq. 
(17) is applied to estimate the finger state in the frontal 
view, whereas, Eq. (18) is applied to estimate the finger 
state in the side view. 
四、實驗結果與討論 
The equipments include one depth camera, two 
ordinary CCD cameras. The whole system is 
implemented by using Standard C/C++ language and 
developed in Microsoft Visual Studio 2005 platform. 
We examine the proposed methodology with several 
video sequences. These video sequences include 
different users with the same initial gestures and each 
video sequence consists of more than one thousand 
frames with the frame rate of 25 FPS. 
A. Estimate Hand Wrist Rotation 
Due to our DataGlove does not contain the sensor 
of wrist rotation detection, we use the reconstructed 
2.5-D hand model image onto the original image to 
check the estimation result roughly as shown in Figure 
8. The estimated hand wrist rotation angle is shown in 
Figure 9. 
B. Comparison between Particle Filter and 
SSBPF 
Here, we do some comparison of Particle Filter 
(CONDENSATION [22]) and SSBPF, but we only 
consider one camera captured images. We use the same 
3-D hand model on a 21 DOF Particle Filter by using 
different samples. In Table 1, we can find out that with 
the same processing time, the average error of original 
Particle Filter is almost twice of proposed method. 
Table 1 shows the accuracy of two different methods. 
Furthermore, we discover that the increase of samples 
does not increase the accuracy conspicuously by the 
original Particle Filter. As shown in Table 1, the same 
processing time with different setting may improve 
10% on the average error. The average error value of 
the experiments can be determined in the following 
equation. 
Est GT4
0 5
i i
i
Finger Finger
Error
=
−
=∑ , 
where the fingers FingerEst represents the estimation 
results, FingerGT represents the ground truth and i 
means the different fingers. 
We also check the increase of samples does not 
"CONDENSATION—Conditional Density 
Propagation for Visual Tracking," International 
Journal of Computer Vision, vol. 29, pp. 5-28, 
1998. 
[23] M. Isard and A. Blake, "ICONDENSATION: 
Unifying Low-Level and High-Level Tracking 
in a Stochastic Framework," in Computer 
Vision — ECCV'98, 1998, p. 893. 
[24] K. Nummiaro, E. Koller-Meier, and L. Van Gool, 
"An adaptive color-based particle filter," in 
Image and Vision Computing. vol. 21: Elsevier 
Science, 2003, pp. 99-110. 
[25] 翁精佑, "基於視覺之手部動作參數擷取系
統," 清華大學碩士論文, 九十六年七月 
[26] M. Kolsch and M. Turk, "Fast 2D Hand 
Tracking with Flocks of Features and Multi-Cue 
Integration," in Computer Vision and Pattern 
Recognition Workshop, 2004 Conference on, 
2004, pp. 158-158. 
[27] Z. Haiting, W. Xiaojuan, and H. Hui, "Research 
of a Real-time Hand Tracking Algorithm," Int. 
Conference on Neural Networks and Brain,, 
2005, pp. 1233-1235. 
[28] H. Zhou, L. Xie, and X. Fang, "Visual Mouse: 
SIFT Detection and PCA 
Recognition,"Computational Intelligence and 
Security Workshops, 2007. pp. 263-266. 
[29] C. Shan, T. Tan, and Y. Wei, "Real-time hand 
tracking using a mean shift embedded particle 
filter," Pattern Recognition, vol. 40, pp. 
1958-1970, 2007. 
[30] Bumblebee2, "Point Grey Research " 
http://www.ptgrey.com/products/stereo.asp. 
[31] G. Amayeh, G. Bebis, A. Erol, and M. Nicolescu, 
"A Component-Based Approach to Hand 
Verification", IEEE Conference on CVPR, 2007, 
pp. 1-8. 
[32] B. Stenger, A. Thayananthan, P. H. S. Torr, and 
R. Cipolla, "Estimating 3D hand pose using 
hierarchical multi-label classification," Image 
and Vision Computing, vol. 25, pp. 1885-1894, 
2007. 
六 圖表 
 
圖 1. The constructed 3-D hand model. 
nearθ
middleθ
farθ
middle near
far middle
2
3
2  
3
θ θ
θ θ
 =

 =
   
openθopen
θ
(a) Frontal (b) Side
Different Angle of View
 
圖 2. The joint angles: (a) joint θnear , (b) finger open 
angle θopen between the index finger and the middle 
finger. 
(a) (b)  
圖 3. The measurement of finger axis collision or not. 
 
(a)  (b)  
圖 4. Unrealizable kinematics constraint: (a) view 1, (b) 
view 2. 
 
(a)  (b)  
圖 5. The depth images. 
 
 
圖 6. 3-D hand model with corresponding gray level 
indicating the depth. 
 
(a) (b) (c)
 
(a)       (b)       (c)       (d) 
圖 7. The finger occlusion inference. (a) The original 
image, (b) The depth image, (c) The edge image. (d) 
The overlapped image. 
 
(a)
(b) (c)
(e)(d)  
圖 8. Hand wrist rotates and the reconstructed model’s 
edge on it: (a) frontal view, (b) and (c) rotate clockwise, 
(d) and (e) rotate counterclockwise. 
 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期： 年 月 日 
國科會補助計畫 
計畫名稱： 
計畫主持人：         
計畫編號：             學門領域： 
技術/創作名稱  
發明人/創作人  
技術說明 
中文： 
 
 
（100~500字） 
英文： 
可利用之產業 
及 
可開發之產品 
 
技術特點 
 
推廣及運用的價值 
 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
附件二 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：黃仲陵 計畫編號：98-2221-E-007-090-MY2 
計畫名稱：植基於視覺的人體/手部動作參數擷取系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 5 5 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
