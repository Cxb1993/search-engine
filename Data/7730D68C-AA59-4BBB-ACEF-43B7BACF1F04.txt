ii
? ?
?? ???? ..................................................... 1
1. ?? .....................................................................................................................1
2. ???? .............................................................................................................1
3. ????? .........................................................................................................1
3.1. ???? ................................................ 1
3.2. ??????? .......................................... 2
3.3. ?????? ............................................ 2
?? ??????? ............................................... 4
?? ???? ..................................................... 5
2?(??(a))??????????????
???????????????????
???????????????????
??????(??(b)?????????
???????????????????
???????????????????
???????????????????
????????????????
?????????????????
???????????????????
???????????????????
??(??(a))?????????????
???????????????????
???????????????????
??????(??(b))?
????? 12 ??????????
??????(??)???????? 243
???????????????????
????????? k-means ??????
???????????????????
????? 20.8%???????????
???????????????????
???????????????????
????? 84.3%???????????
???????????????????
????????
3.2 ????????
?????????????????
???????????????????
???????????????????
???????(??)?????????
???????????????????
???????????????????
???????????????????
???????????????????
???????????????????
?????????????????
?????????????????
???????????????????
???????????????????
?????????????(Locally linear
embedding, LLE)[6][7]?????????
??????[6][8][9]??????????
???????????????????
??????? LLE ??????????
????????????LLE ?????
??????
?????????????????
???????????????????
???????????????????
???????????????????
???????????????????
???????????????????
?????(Class-conditional LLE, CLLE)?
???????????????????
???????????????????
???????????????????
???????????????????
???????????
???? 25 ????????????
??????? 7000 ??????????
???? 160 ?????????? 120 ?
????????? CLLE ???????
??????????????????
k-nearest neighbors ??????????(?
?)???? CLLE ??????????
??????
3.3 ??????
?????????????????
???????????????????
???????????????????
???????????????????
???????????????????
???????????????????
???(HMM)??[10][11]???????
???????????????????
???????????????????
???????????????????
??????????????????
???????????????????
???????????????????
??????????????
?????????????????
???????????????????
???????????????????
????(??(a))???????????
4??
CHMM ??????????
?? ???(%)
1 ?? 98.33
2 ?? 96.67
3 ?? 96.67
4 ? 95.00
5 ?? 86.67
6 ? 96.67
7 ? 81.67
8 ? 90.00
9 ? 66.67
10 ?? 95.00
11 ? 96.67
12 ? 63.33
Avg. acc. 89.45
(a)
(b)
???(a)???????????(b)HMM, LHMM,
CHMM ????????????????
?? ???????
??????????????????
???????????????????
???????????????????
???????????
1. ???????
??????????????????
?????????
2. ????????[12]
a. ????????????????
b. ??????????????
c. ?????????????????
??
d. ??????????????
e. ????????????????
??
3. ?????????[13]
a. ?????????????????
???
b. ????????????????
??
c. ???????????
d. ??????????????
4. ????????[14]
a. ???????????(?????
???????)
b. ?????????????
c. ?????????????????
d. ???????????
e. ???????????
x
y
z
x
z
y
x
z
x
y
x
z
y
x
z
y
ii
????????????????
?????
????? ???????(???)
????? NSC 96-2221-E-006-257
????? 96?8?1??97?7?31?
??????2008 ??????????????????
????(2008 IEEE Geoscience and Remote
Sensing Symposium, ?? IGARSS’08)
????? ?? 96 ? 7 ? 7 ?? 7 ? 10 ?
????? ??????????(?)
??? ??????(?????)
??? ???
????? 0911-225259
? ? ? ? ? ? ? ? ? ? ? ? ?
iii
iii
? ?
????.................................................................................................... 1
????.................................................................................................... 2
????.................................................................................................... 4
??????............................................................................................ 5
22
????
?????????????????”Kernel-based Nonlinear
Feature Extraction for Image Classification,”???????????
?(????????)?
???????????????????TU1.103: Student
Paper Prize Competition?????????????(Wei-Min Liu)
???TU1.103.1: Multiple-window anomaly detection for hyperspectral
imagery??????????????????????????
????????????????
???????????????????????????
44
????
2008??????????????????????????
??????????????????????????????
??????????????????????????????
??????????????????????????????
??????????????????????????????
???????????SAR?????????????????
??????????????????????????????
??????????????????????????????
??????????
????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
??????????????????????????????
??????????????????????????????
??????????????????????????????
?????????
66
? ? ? ????
? ? ? ????
2the contexture over all classes can be defined as a contextual
distance given by
22
1
( , ) ( ) ( )
L
C s td s t  x x r r . (4)
To explore another source of discriminative information, we
deliberately model the difference in class labels using the
membership of a sample belonging to a class. When
contextual information is available, the probability
(ω| ,ω )
ss
p x is used to represent the degree of class
membership of xs in . Let ω argmax (ω|,ω )tt tp  x be
the label assigned to xt. The value of (ω| ,ω )st sp x
reflects the relationship of xs to the class of xt with respect to
class membership. We present the following measure for
class label similarity between xs and xt,
s t s s tη(,)=[( |, ) ( |, )]/2s stp p     x x x x . (5)
If two samples are assigned the same label (x = y), then
1/L 1; otherwise, 0 1/L. A pair of samples with
the same label enjoys a larger value of label similarity than a
pair with different labels. This can be regarded as a
discriminative resource associated with class labels. A
contextual kernel function for samples xs and xt is thus
designed as
2 2
2 2
( , ) ( , )
( , ) ( , ) exp
σ σ
E s t C s t
s t s t
E C
d d
k
    
 
x x x x
x x x x , (6)
where E and C control the width of kernel associated with
the Euclidean and the contextual distances, respectively. If
contextual information is not available, the kernel function
for samples x and y is degenerated to
 2 2( , ) ( , ) exp ( , ) /σE Ek d x y x y x y . (7)
This kernel function can be generally applied to
classification problems. The application is not limited to
image data.
3. PROPOSED ALGORITHM
From an image, select training and test sets that are mutually
exclusive. Training samples xi i=1...n are located at sites si.
1. In the training stage, estimate the class-conditional
probability density function for each class. One may
model classes parametrically if the characteristic of the
data set permits.
2. Perform the ICM algorithm for an MRF classification of
the image. For each sample, compute (ω| ,ω )
ss
p x for
all classes. Update the labels of samples using
ω argmax (ω| ,ω )
ss s
p  x .
3. For each pair of training samples xi and xj, calculate the
kernel function, ( , )i jk x x . Construct the kernel matrix
with element ( , )ij i jkK x x .
4. For each sample x, find the feature vector, w 
1[ ( , ) ... ( , )]
T
nk kx x x x , by pairing the sample x with each
training sample xi.
5. For the chosen base algorithm, solve the corresponding
eigenvalue problem and generate optimum features.
Select m features to construct A.
6. Perform feature extraction z = ATw for each sample.
Generate an m-variate image data set.
7. In the test stage, assess the test classification accuracy
based on the test samples in the m-dimensional feature
space.
8. Repeat the above steps if iteration is requested. The
iteration may terminate if the test accuracy becomes
stable.
4. EXPERIMENTAL RESULTS
4.1. Simulated data sets
We illustrate the advantages of the proposed kernel function
using three simulated data sets. An image map in Fig. 1(a) is
included if discriminative spatial information is needed.
Compared methods include PCA/LDA and KPCA/KDA.
The proposed methods, denoted by MrfKPCA and MrfKDA,
are the algorithms obtained from applying the proposed
kernel function to PCA/LDA. We measure the effectiveness
of a kernel function in terms of classification accuracy.
To show that the proposed kernel function inherits the
property of the basic kernel function, Dataset 1 is used as a
typical case for the application of an RBF kernel trick. As
shown in Fig. 1(b), Class 1 is centered at the origin and
surrounded by Class 2. Class 1 has a probability density
function, N(0, I), and Class 2 is a mixture of eight
equiprobable spherical Gaussians. In this case, the linear
feature extraction methods (PCA and LDA) fail to produce
effective features, as shown in Fig. 2(a), because of the
nonlinear decision boundary between classes. In contrast, all
kernel-related methods perform relatively well. Consider
two spiral-shaped classes in Dataset 2, as shown in Fig. 1(c).
Data are separable in the original feature space. For each
class, 2000 samples were generated, 10% was used for
classifier-training, 5% for kernel-training, and the remaining
for testing. In spite of high class separability in the original
space, neither the RBF-based kernel methods (KDA, KPCA)
nor the linear methods (LDA, PCA) can preserve class
separability if only one or two features are used for
dimensionality reduction. For example, they all yield
classification accuracy of lower than 70% based on one
feature, as shown in Fig. 3(a). The proposed kernel function
provides a way to improve discrimination based on spatial
contextual information. If yi and yj have the same Euclidean
distance to a training sample x, the RBF kernel function fails
to distinguish yi and yj. If yi and yj have different contextual
distances to x, the degree of discrimination between yi and yj
can be increased. In the worst case, classes are severely
4(a) (b)
Fig. 2. Dataset 1: (a) Pixelwise, (b) MRF-based ML classifiers.
(a) (b)
Fig. 3. Dataset 2: (a) Pixelwise, (b) MRF-based ML classifiers.
(a) (b)
Fig. 4. Dataset 3: (a) Pixelwise, (b) MRF-based ML classifiers.
(a) (b) (c) (d)
Fig. 5. Dataset 4: (a) AVIRIS image. (b) Ground Truth. (c)(d) Two
and three iterations of MrfKDA and an MRF-based classifier.
Fig. 6. Dataset 4: Pixelwise ML classifier.
Fig. 7. Dataset 4: MRF-based ML classifier.
6. REFERENCE
[1] A. Ruiz and P.E. López-de-Teruel, “Nonlinear Kernel-
Based Statistical Pattern Analysis,” IEEE Trans Neural
Networks, vol. 12, pp. 16–32, Jan. 2001.
[2] G. Baudat and F. Anouar, “Generalized discriminant 
analysis using akernel approach,” Neural Comput., vol. 12,
no. 10, pp. 2385–2404, 2000.
[3] G. C. Valls and L. Bruzzone, “Kernel-based methods for
hyperspectral image classification,” IEEE Trans. Geosci.
Remote Sensing, vol. 43, no. 6, pp. 1351–1362, Jun. 2005.
[4] K. I. Kim, S. H. Park, and H. J. Kim, “Kernel principal
component analysis for texture classification,” IEEE Signal
Processing Letters, vol. 8, no. 2, pp. 39–41, Feb. 2001.
[5] K. I. Kim, K. Jung, and H. J. Kim, “Face recognition using
kernel principal component analysis,” IEEE Signal
Processing Letters, vol. 9, no. 2, pp. 40–42, Feb. 2002.
[6] M. Zhu and A. M. Martinez, “Subclass discriminant
analysis,” IEEE Trans. Pattern Anal. Machine Intell., vol.
28, no. 8, pp. 1274–1286, Aug., 2006.
[7] B. Scho¨lkopf, A. Smola, and K.R. Mu¨ler, “Nonlinear 
component analysis as a kernel eigenvalue problem,” Neural
Comput., vol. 10, no. 5, pp. 1299–1319, 1998.
[8] S. Mika, G. Ratsch, J. Weston, B. Scho¨lkopf, and K.
R.Mu¨ler, “Fisherdiscriminant analysis with kernels,” Proc.
IEEE Int. Workshop Neural Networks for Signal Processing
IX, pp. 41–48, Aug. 1999.
[9] G. Baudat and F. Anouar, “Generalized discriminant 
analysis using akernel approach,” Neural Comput., vol. 12,
no. 10, pp. 2385–2404, 2000.
[10] J. Ma, J. L. S. Gómez and S. C. Ahalt, “Nonlinear multiclass
discriminant analysis,” IEEE Signal Processing Letters,
vol0, no 7, pp. 196–199, Jul. 2003.
